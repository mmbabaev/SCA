<html><body><head><link rel="stylesheet" type="text/css" href="style.css" /><script src="map.js"></script><script src="jquery-1.7.1.min.js"></script></head>
<div class="dstPaperData">
N06-1020 <div class="dstPaperTitle">Effective Self-Training For Parsing</div><div class="dstPaperAuthors">McClosky, David;Charniak, Eugene;Johnson, Mark;</div>
</div>
<table cellspacing="0" cellpadding="0"><tr>
	<td class="srcData" >Source Paper</td>
	<td class="pp legend" ><input type="checkbox" id="cbIPositive" checked="true"/><label for="cbIPositive">Informal +<label></td>
	<td class="nn legend" ><input type="checkbox" id="cbINegative" checked="true"/><label for="cbINegative">Informal -<label></td>
	<td class="oo legend" ><input type="checkbox" id="cbIObjective" checked="true"/><label for="cbIObjective">Informal Neutral<label></td>
	<td class="ppc legend" ><input type="checkbox" id="cbEPositive" checked="true"/><label for="cbEPositive">Formal +</label></td>
	<td class="nnc legend" ><input type="checkbox" id="cbENegative" checked="true"/><label for="cbENegative">Formal -</label></td>
	<td class="ooc legend" ><input type="checkbox" id="cbEObjective" checked="true"/><label for="cbEObjective">Formal Neutral</label></td>
	<td class="lb"><input type="checkbox" id="cbSentenceBoundary"/><label for="cbSentenceBoundary">Sentence Boundary</label></td>
</tr></table>
<div class="dstPaper">
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-1043
Reranking And Self-Training For Parser Adaptation
McClosky, David;Charniak, Eugene;Johnson, Mark;"></td>
	<td class="line x" title="1:213	Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 337344, Sydney, July 2006." ></td>
	<td class="line x" title="2:213	c2006 Association for Computational Linguistics Reranking and Self-Training for Parser Adaptation David McClosky, Eugene Charniak, and Mark Johnson Brown Laboratory for Linguistic Information Processing (BLLIP) Brown University Providence, RI 02912 {dmcc|ec|mj}@cs.brown.edu Abstract Statistical parsers trained and tested on the Penn Wall Street Journal (WSJ) treebank have shown vast improvements over the last 10 years." ></td>
	<td class="line x" title="3:213	Much of this improvement, however, is based upon an ever-increasing number of features to be trained on (typically) the WSJ treebank data." ></td>
	<td class="line x" title="4:213	This has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres." ></td>
	<td class="line x" title="5:213	Such worries have merit." ></td>
	<td class="line x" title="6:213	The standard Charniak parser checks in at a labeled precisionrecall f-measure of 89.7% on the Penn WSJ test set, but only 82.9% on the test set from the Brown treebank corpus." ></td>
	<td class="line x" title="7:213	This paper should allay these fears." ></td>
	<td class="line x" title="8:213	In particular, we show that the reranking parser described in Charniak and Johnson (2005) improves performance of the parser on Brown to 85.2%." ></td>
	<td class="line oc" title="9:213	Furthermore, use of the self-training techniques described in (McClosky et al. , 2006) raise this to 87.8% (an error reduction of 28%) again without any use of labeled Brown data." ></td>
	<td class="line p" title="10:213	This is remarkable since training the parser and reranker on labeled Brown data achieves only 88.4%." ></td>
	<td class="line x" title="11:213	1 Introduction Modern statistical parsers require treebanks to train their parameters, but their performance declines when one parses genres more distant from the training datas domain." ></td>
	<td class="line x" title="12:213	Furthermore, the treebanks required to train said parsers are expensive and difficult to produce." ></td>
	<td class="line x" title="13:213	Naturally, one of the goals of statistical parsing is to produce a broad-coverage parser which is relatively insensitive to textual domain." ></td>
	<td class="line x" title="14:213	But the lack of corpora has led to a situation where much of the current work on parsing is performed on a single domain using training data from that domain  the Wall Street Journal (WSJ) section of the Penn Treebank (Marcus et al. , 1993)." ></td>
	<td class="line x" title="15:213	Given the aforementioned costs, it is unlikely that many significant treebanks will be created for new genres." ></td>
	<td class="line x" title="16:213	Thus, parser adaptation attempts to leverage existing labeled data from one domain and create a parser capable of parsing a different domain." ></td>
	<td class="line x" title="17:213	Unfortunately, the state of the art in parser portability (i.e. using a parser trained on one domain to parse a different domain) is not good." ></td>
	<td class="line x" title="18:213	The Charniak parser has a labeled precision-recall f-measure of 89.7% on WSJ but a lowly 82.9% on the test set from the Brown corpus treebank." ></td>
	<td class="line x" title="19:213	Furthermore, the treebanked Brown data is mostly general non-fiction and much closer to WSJ than, e.g., medical corpora would be." ></td>
	<td class="line x" title="20:213	Thus, most work on parser adaptation resorts to using some labeled in-domain data to fortify the larger quantity of outof-domain data." ></td>
	<td class="line x" title="21:213	In this paper, we present some encouraging results on parser adaptation without any in-domain data." ></td>
	<td class="line x" title="22:213	(Though we also present results with indomain data as a reference point)." ></td>
	<td class="line x" title="23:213	In particular we note the effects of two comparatively recent techniques for parser improvement." ></td>
	<td class="line x" title="24:213	The first of these, parse-reranking (Collins, 2000; Charniak and Johnson, 2005) starts with a standard generative parser, but uses it to generate the n-best parses rather than a single parse." ></td>
	<td class="line x" title="25:213	Then a reranking phase uses more detailed features, features which would (mostly) be impossible to incorporate in the initial phase, to reorder 337 the list and pick a possibly different best parse." ></td>
	<td class="line x" title="26:213	At first blush one might think that gathering even more fine-grained features from a WSJ treebank would not help adaptation." ></td>
	<td class="line x" title="27:213	However, we find that reranking improves the parsers performance from 82.9% to 85.2%." ></td>
	<td class="line x" title="28:213	The second technique is self-training  parsing unlabeled data and adding it to the training corpus." ></td>
	<td class="line pc" title="29:213	Recent work, (McClosky et al. , 2006), has shown that adding many millions of words of machine parsed and reranked LA Times articles does, in fact, improve performance of the parser on the closely related WSJ data." ></td>
	<td class="line x" title="30:213	Here we show that it also helps the father-afield Brown data." ></td>
	<td class="line x" title="31:213	Adding it improves performance yet-again, this time from 85.2% to 87.8%, for a net error reduction of 28%." ></td>
	<td class="line x" title="32:213	It is interesting to compare this to our results for a completely Brown trained system (i.e. one in which the first-phase parser is trained on just Brown training data, and the second-phase reranker is trained on Brown 50-best lists)." ></td>
	<td class="line x" title="33:213	This system performs at a 88.4% level  only slightly higher than that achieved by our system with only WSJ data." ></td>
	<td class="line x" title="34:213	2 Related Work Work in parser adaptation is premised on the assumption that one wants a single parser that can handle a wide variety of domains." ></td>
	<td class="line x" title="35:213	While this is the goal of the majority of parsing researchers, it is not quite universal." ></td>
	<td class="line x" title="36:213	Sekine (1997) observes that for parsing a specific domain, data from that domain is most beneficial, followed by data from the same class, data from a different class, and data from a different domain." ></td>
	<td class="line x" title="37:213	He also notes that different domains have very different structures by looking at frequent grammar productions." ></td>
	<td class="line x" title="38:213	For these reasons he takes the position that we should, instead, simply create treebanks for a large number of domains." ></td>
	<td class="line x" title="39:213	While this is a coherent position, it is far from the majority view." ></td>
	<td class="line x" title="40:213	There are many different approaches to parser adaptation." ></td>
	<td class="line x" title="41:213	Steedman et al.(2003) apply cotraining to parser adaptation and find that cotraining can work across domains." ></td>
	<td class="line x" title="43:213	The need to parse biomedical literature inspires (Clegg and Shepherd, 2005; Lease and Charniak, 2005)." ></td>
	<td class="line x" title="44:213	Clegg and Shepherd (2005) provide an extensive side-by-side performance analysis of several modern statistical parsers when faced with such data." ></td>
	<td class="line x" title="45:213	They find that techniques which combine differTraining Testing f-measureGildea Bacchiani WSJ WSJ 86.4 87.0 WSJ Brown 80.6 81.1 Brown Brown 84.0 84.7 WSJ+Brown Brown 84.3 85.6 Table 1: Gildea and Bacchiani results on WSJ and Brown test corpora using different WSJ and Brown training sets." ></td>
	<td class="line x" title="46:213	Gildea evaluates on sentences of length  40, Bacchiani on all sentences." ></td>
	<td class="line x" title="47:213	ent parsers such as voting schemes and parse selection can improve performance on biomedical data." ></td>
	<td class="line x" title="48:213	Lease and Charniak (2005) use the Charniak parser for biomedical data and find that the use of out-of-domain trees and in-domain vocabulary information can considerably improve performance." ></td>
	<td class="line x" title="49:213	However, the work which is most directly comparable to ours is that of (Ratnaparkhi, 1999; Hwa, 1999; Gildea, 2001; Bacchiani et al. , 2006)." ></td>
	<td class="line x" title="50:213	All of these papers look at what happens to modern WSJ-trained statistical parsers (Ratnaparkhis, Collins, Gildeas and Roarks, respectively) as training data varies in size or usefulness (because we are testing on something other than WSJ)." ></td>
	<td class="line x" title="51:213	We concentrate particularly on the work of (Gildea, 2001; Bacchiani et al. , 2006) as they provide results which are directly comparable to those presented in this paper." ></td>
	<td class="line x" title="52:213	Looking at Table 1, the first line shows us the standard training and testing on WSJ  both parsers perform in the 86-87% range." ></td>
	<td class="line x" title="53:213	The next line shows what happens when parsing Brown using a WSJ-trained parser." ></td>
	<td class="line x" title="54:213	As with the Charniak parser, both parsers take an approximately 6% hit." ></td>
	<td class="line x" title="55:213	It is at this point that our work deviates from these two papers." ></td>
	<td class="line x" title="56:213	Lacking alternatives, both (Gildea, 2001) and (Bacchiani et al. , 2006) give up on adapting a pure WSJ trained system, instead looking at the issue of how much of an improvement one gets over a pure Brown system by adding WSJ data (as seen in the last two lines of Table 1)." ></td>
	<td class="line x" title="57:213	Both systems use a model-merging (Bacchiani et al. , 2006) approach." ></td>
	<td class="line x" title="58:213	The different corpora are, in effect, concatenated together." ></td>
	<td class="line x" title="59:213	However, (Bacchiani et al. , 2006) achieve a larger gain by weighting the in-domain (Brown) data more heavily than the out-of-domain WSJ data." ></td>
	<td class="line x" title="60:213	One can imagine, for instance, five copies of the Brown data concatenated with just one copy of WSJ data." ></td>
	<td class="line x" title="61:213	338 3 Corpora We primarily use three corpora in this paper." ></td>
	<td class="line x" title="62:213	Selftraining requires labeled and unlabeled data." ></td>
	<td class="line x" title="63:213	We assume that these sets of data must be in similar domains (e.g. news articles) though the effectiveness of self-training across domains is currently an open question." ></td>
	<td class="line x" title="64:213	Thus, we have labeled (WSJ) and unlabeled (NANC) out-of-domain data and labeled in-domain data (BROWN)." ></td>
	<td class="line x" title="65:213	Unfortunately, lacking a corresponding corpus to NANC for BROWN, we cannot perform the opposite scenario and adapt BROWN to WSJ." ></td>
	<td class="line x" title="66:213	3.1 Brown The BROWN corpus (Francis and Kucera, 1979) consists of many different genres of text, intended to approximate a balanced corpus." ></td>
	<td class="line x" title="67:213	While the full corpus consists of fiction and nonfiction domains, the sections that have been annotated in Treebank II bracketing are primarily those containing fiction." ></td>
	<td class="line x" title="68:213	Examples of the sections annotated include science fiction, humor, romance, mystery, adventure, and popular lore. We use the same divisions as Bacchiani et al.(2006), who base their divisions on Gildea (2001)." ></td>
	<td class="line x" title="70:213	Each division of the corpus consists of sentences from all available genres." ></td>
	<td class="line x" title="71:213	The training division consists of approximately 80% of the data, while held-out development and testing divisions each make up 10% of the data." ></td>
	<td class="line x" title="72:213	The treebanked sections contain approximately 25,000 sentences (458,000 words)." ></td>
	<td class="line x" title="73:213	3.2 Wall Street Journal Our out-of-domain data is the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al. , 1993) which consists of about 40,000 sentences (one million words) annotated with syntactic information." ></td>
	<td class="line x" title="74:213	We use the standard divisions: Sections 2 through 21 are used for training, section 24 for held-out development, and section 23 for final testing." ></td>
	<td class="line x" title="75:213	3.3 North American News Corpus In addition to labeled news data, we make use of a large quantity of unlabeled news data." ></td>
	<td class="line x" title="76:213	The unlabeled data is the North American News Corpus, NANC (Graff, 1995), which is approximately 24 million unlabeled sentences from various news sources." ></td>
	<td class="line x" title="77:213	NANC contains no syntactic information and sentence boundaries are induced by a simple discriminative model." ></td>
	<td class="line x" title="78:213	We also perform some basic cleanups on NANC to ease parsing." ></td>
	<td class="line x" title="79:213	NANC contains news articles from various news sources including the Wall Street Journal, though for this paper, we only use articles from the LA Times portion." ></td>
	<td class="line oc" title="80:213	To use the data from NANC, we use self-training (McClosky et al. , 2006)." ></td>
	<td class="line x" title="81:213	First, we take a WSJ trained reranking parser (i.e. both the parser and reranker are built from WSJ training data) and parse the sentences from NANC with the 50-best (Charniak and Johnson, 2005) parser." ></td>
	<td class="line x" title="82:213	Next, the 50-best parses are reordered by the reranker." ></td>
	<td class="line pc" title="83:213	Finally, the 1-best parses after reranking are combined with the WSJ training set to retrain the firststage parser.1 McClosky et al.(2006) find that the self-trained models help considerably when parsing WSJ." ></td>
	<td class="line x" title="85:213	4 Experiments We use the Charniak and Johnson (2005) reranking parser in our experiments." ></td>
	<td class="line x" title="86:213	Unless mentioned otherwise, we use the WSJ-trained reranker (as opposed to a BROWN-trained reranker)." ></td>
	<td class="line x" title="87:213	To evaluate, we report bracketing f-scores.2 Parser f-scores reported are for sentences up to 100 words long, while reranking parser f-scores are over all sentences." ></td>
	<td class="line x" title="88:213	For simplicity and ease of comparison, most of our evaluations are performed on the development section of BROWN." ></td>
	<td class="line x" title="89:213	4.1 Adapting self-training Our first experiment examines the performance of the self-trained parsers." ></td>
	<td class="line x" title="90:213	While the parsers are created entirely from labeled WSJ data and unlabeled NANC data, they perform extremely well on BROWN development (Table 2)." ></td>
	<td class="line oc" title="91:213	The trends are the same as in (McClosky et al. , 2006): Adding NANC data improves parsing performance on BROWN development considerably, improving the f-score from 83.9% to 86.4%." ></td>
	<td class="line x" title="92:213	As more NANC data is added, the f-score appears to approach an asymptote." ></td>
	<td class="line x" title="93:213	The NANC data appears to help reduce data sparsity and fill in some of the gaps in the WSJ model." ></td>
	<td class="line x" title="94:213	Additionally, the reranker provides further benefit and adds an absolute 1-2% to the fscore." ></td>
	<td class="line x" title="95:213	The improvements appear to be orthogonal, as our best performance is reached when we use the reranker and add 2,500k self-trained sentences from NANC." ></td>
	<td class="line x" title="96:213	1We trained a new reranker from this data as well, but it does not seem to get significantly different performance." ></td>
	<td class="line x" title="97:213	2The harmonic mean of labeled precision (P) and labeled recall (R), i.e. f = 2PRP+R 339 Sentences added Parser Reranking Parser Baseline BROWN 86.4 87.4 Baseline WSJ 83.9 85.8 WSJ+50k 84.8 86.6 WSJ+250k 85.7 87.2 WSJ+500k 86.0 87.3 WSJ+750k 86.1 87.5 WSJ+1,000k 86.2 87.3 WSJ+1,500k 86.2 87.6 WSJ+2,000k 86.1 87.7 WSJ+2,500k 86.4 87.7 Table 2: Effects of adding NANC sentences to WSJ training data on parsing performance." ></td>
	<td class="line x" title="98:213	f-scores for the parser with and without the WSJ reranker are shown when evaluating on BROWN development." ></td>
	<td class="line x" title="99:213	For this experiment, we use the WSJ-trained reranker." ></td>
	<td class="line x" title="100:213	The results are even more surprising when we compare against a parser3 trained on the labeled training section of the BROWN corpus, with parameters tuned against its held-out section." ></td>
	<td class="line x" title="101:213	Despite seeing no in-domain data, the WSJ based parser is able to match the BROWN based parser." ></td>
	<td class="line x" title="102:213	For the remainder of this paper, we will refer to the model trained on WSJ+2,500k sentences of NANC as our best WSJ+NANC model." ></td>
	<td class="line x" title="103:213	We also note that this best parser is different from the best parser for parsing WSJ, which was trained on WSJ with a relative weight4 of 5 and 1,750k sentences from NANC." ></td>
	<td class="line x" title="104:213	For parsing BROWN, the difference between these two parsers is not large, though." ></td>
	<td class="line x" title="105:213	Increasing the relative weight of WSJ sentences versus NANC sentences when testing on BROWN development does not appear to have a significant effect." ></td>
	<td class="line pc" title="106:213	While (McClosky et al. , 2006) showed that this technique was effective when testing on WSJ, the true distribution was closer to WSJ so it made sense to emphasize it." ></td>
	<td class="line x" title="107:213	4.2 Incorporating In-Domain Data Up to this point, we have only considered the situation where we have no in-domain data." ></td>
	<td class="line x" title="108:213	We now 3In this case, only the parser is trained on BROWN." ></td>
	<td class="line x" title="109:213	In section 4.3, we compare against a fully BROWN-trained reranking parser as well." ></td>
	<td class="line x" title="110:213	4A relative weight of n is equivalent to using n copies of the corpus, i.e. an event that occurred x times in the corpus would occur xn times in the weighted corpus." ></td>
	<td class="line x" title="111:213	Thus, larger corpora will tend to dominate smaller corpora of the same relative weight in terms of event counts." ></td>
	<td class="line x" title="112:213	explore different ways of making use of labeled and unlabeled in-domain data." ></td>
	<td class="line x" title="113:213	Bacchiani et al.(2006) applies self-training to parser adaptation to utilize unlabeled in-domain data." ></td>
	<td class="line x" title="115:213	The authors find that it helps quite a bit when adapting from BROWN to WSJ." ></td>
	<td class="line x" title="116:213	They use a parser trained from the BROWN train set to parse WSJ and add the parsed WSJ sentences to their training set." ></td>
	<td class="line x" title="117:213	We perform a similar experiment, using our WSJtrained reranking parser to parse BROWN train and testing on BROWN development." ></td>
	<td class="line x" title="118:213	We achieved a boost from 84.8% to 85.6% when we added the parsed BROWN sentences to our training." ></td>
	<td class="line x" title="119:213	Adding in 1,000k sentences from NANC as well, we saw a further increase to 86.3%." ></td>
	<td class="line x" title="120:213	However, the technique does not seem as effective in our case." ></td>
	<td class="line x" title="121:213	While the self-trained BROWN data helps the parser, it adversely affects the performance of the reranking parser." ></td>
	<td class="line x" title="122:213	When self-trained BROWN data is added to WSJ training, the reranking parsers performance drops from 86.6% to 86.1%." ></td>
	<td class="line x" title="123:213	We see a similar degradation as NANC data is added to the training set as well." ></td>
	<td class="line x" title="124:213	We are not yet able to explain this unusual behavior." ></td>
	<td class="line x" title="125:213	We now turn to the scenario where we have some labeled in-domain data." ></td>
	<td class="line x" title="126:213	The most obvious way to incorporate labeled in-domain data is to combine it with the labeled out-of-domain data." ></td>
	<td class="line x" title="127:213	We have already seen the results (Gildea, 2001) and (Bacchiani et al. , 2006) achieve in Table 1." ></td>
	<td class="line x" title="128:213	We explore various combinations of BROWN, WSJ, and NANC corpora." ></td>
	<td class="line x" title="129:213	Because we are mainly interested in exploring techniques with self-trained models rather than optimizing performance, we only consider weighting each corpus with a relative weight of one for this paper." ></td>
	<td class="line x" title="130:213	The models generated are tuned on section 24 from WSJ." ></td>
	<td class="line x" title="131:213	The results are summarized in Table 3." ></td>
	<td class="line x" title="132:213	While both WSJ and BROWN models benefit from a small amount of NANC data, adding more than 250k NANC sentences to the BROWN or combined models causes their performance to drop." ></td>
	<td class="line x" title="133:213	This is not surprising, though, since adding too much NANC overwhelms the more accurate BROWN or WSJ counts." ></td>
	<td class="line x" title="134:213	By weighting the counts from each corpus appropriately, this problem can be avoided." ></td>
	<td class="line x" title="135:213	Another way to incorporate labeled data is to tune the parser back-off parameters on it." ></td>
	<td class="line x" title="136:213	Bacchiani et al.(2006) report that tuning on held-out BROWN data gives a large improvement over tun340 ing on WSJ data." ></td>
	<td class="line x" title="138:213	The improvement is mostly (but not entirely) in precision." ></td>
	<td class="line x" title="139:213	We do not see the same improvement (Figure 1) but this is likely due to differences in the parsers." ></td>
	<td class="line x" title="140:213	However, we do see a similar improvement for parsing accuracy once NANC data has been added." ></td>
	<td class="line x" title="141:213	The reranking parser generally sees an improvement, but it does not appear to be significant." ></td>
	<td class="line x" title="142:213	4.3 Reranker Portability We have shown that the WSJ-trained reranker is actually quite portable to the BROWN fiction domain." ></td>
	<td class="line x" title="143:213	This is surprising given the large number of features (over a million in the case of the WSJ reranker) tuned to adjust for errors made in the 50best lists by the first-stage parser." ></td>
	<td class="line x" title="144:213	It would seem the corrections memorized by the reranker are not as domain-specific as we might expect." ></td>
	<td class="line x" title="145:213	As further evidence, we present the results of applying the WSJ model to the Switchboard corpus  a domain much less similar to WSJ than BROWN." ></td>
	<td class="line x" title="146:213	In Table 4, we see that while the parsers performance is low, self-training and reranking provide orthogonal benefits." ></td>
	<td class="line x" title="147:213	The improvements represent a 12% error reduction with no additional in-domain data." ></td>
	<td class="line x" title="148:213	Naturally, in-domain data and speech-specific handling (e.g. disfluency modeling) would probably help dramatically as well." ></td>
	<td class="line x" title="149:213	Finally, to compare against a model fully trained on BROWN data, we created a BROWN reranker." ></td>
	<td class="line x" title="150:213	We parsed the BROWN training set with 20-fold cross-validation, selected features that occurred 5 times or more in the training set, and fed the 50-best lists from the parser to a numerical optimizer to estimate feature weights." ></td>
	<td class="line x" title="151:213	The resulting reranker model had approximately 700,000 features, which is about half as many as the WSJ trained reranker." ></td>
	<td class="line x" title="152:213	This may be due to the smaller size of the BROWN training set or because the feature schemas for the reranker were developed on WSJ data." ></td>
	<td class="line x" title="153:213	As seen in Table 5, the BROWN reranker is not a significant improvement over the WSJ reranker for parsing BROWN data." ></td>
	<td class="line x" title="154:213	5 Analysis We perform several types of analysis to measure some of the differences and similarities between the BROWN-trained and WSJ-trained reranking parsers." ></td>
	<td class="line x" title="155:213	While the two parsers agree on a large number of parse brackets (Section 5.2), there are categorical differences between them (as seen in Parser model Parser f-score Reranker f-score WSJ 74.0 75.9 WSJ+NANC 75.6 77.0 Table 4: Parser and reranking parser performance on the SWITCHBOARD development corpus." ></td>
	<td class="line x" title="156:213	In this case, WSJ+NANC is a model created from WSJ and 1,750k sentences from NANC." ></td>
	<td class="line x" title="157:213	Model 1-best 10-best 25-best 50-best WSJ 82.6 88.9 90.7 91.9 WSJ+NANC 86.4 92.1 93.5 94.3 BROWN 86.3 92.0 93.3 94.2 Table 6: Oracle f-scores of top n parses produced by baseline WSJ parser, a combined WSJ and NANC parser, and a baseline BROWN parser." ></td>
	<td class="line x" title="158:213	Section 5.3)." ></td>
	<td class="line x" title="159:213	5.1 Oracle Scores Table 6 shows the f-scores of an oracle reranker  i.e. one which would always choose the parse with the highest f-score in the n-best list." ></td>
	<td class="line x" title="160:213	While the WSJ parser has relatively low f-scores, adding NANC data results in a parser with comparable oracle scores as the parser trained from BROWN training." ></td>
	<td class="line nc" title="161:213	Thus, the WSJ+NANC model has better oracle rates than the WSJ model (McClosky et al. , 2006) for both the WSJ and BROWN domains." ></td>
	<td class="line x" title="162:213	5.2 Parser Agreement In this section, we compare the output of the WSJ+NANC-trained and BROWN-trained reranking parsers." ></td>
	<td class="line x" title="163:213	We use evalb to calculate how similar the two sets of output are on a bracket level." ></td>
	<td class="line x" title="164:213	Table 7 shows various statistics." ></td>
	<td class="line x" title="165:213	The two parsers achieved an 88.0% f-score between them." ></td>
	<td class="line x" title="166:213	Additionally, the two parsers agreed on all brackets almost half the time." ></td>
	<td class="line x" title="167:213	The part of speech tagging agreement is fairly high as well." ></td>
	<td class="line x" title="168:213	Considering they were created from different corpora, this seems like a high level of agreement." ></td>
	<td class="line x" title="169:213	5.3 Statistical Analysis We conducted randomization tests for the significance of the difference in corpus f-score, based on the randomization version of the paired sample ttest described by Cohen (1995)." ></td>
	<td class="line x" title="170:213	The null hypothesis is that the two parsers being compared are in fact behaving identically, so permuting or swapping the parse trees produced by the parsers for 341 WSJ tuned parser BROWN tuned parser WSJ tuned reranking parser BROWN tuned reranking parser NANC sentences added fsco re 2000k1750k1500k1250k1000k750k500k250k0k 87.8 87.0 86.0 85.0 83.8 Figure 1: Precision and recall f-scores when testing on BROWN development as a function of the number of NANC sentences added under four test conditions." ></td>
	<td class="line x" title="171:213	BROWN tuned indicates that BROWN training data was used to tune the parameters (since the normal held-out section was being used for testing)." ></td>
	<td class="line x" title="172:213	For WSJ tuned, we tuned the parameters from section 24 of WSJ." ></td>
	<td class="line x" title="173:213	Tuning on BROWN helps the parser, but not for the reranking parser." ></td>
	<td class="line x" title="174:213	Parser model Parser alone Reranking parser WSJ alone 83.9 85.8 WSJ+2,500k NANC 86.4 87.7 BROWN alone 86.3 87.4 BROWN+50k NANC 86.8 88.0 BROWN+250k NANC 86.8 88.1 BROWN+500k NANC 86.7 87.8 WSJ+BROWN 86.5 88.1 WSJ+BROWN+50k NANC 86.8 88.1 WSJ+BROWN+250k NANC 86.8 88.1 WSJ+BROWN+500k NANC 86.6 87.7 Table 3: f-scores from various combinations of WSJ, NANC, and BROWN corpora on BROWN development." ></td>
	<td class="line x" title="175:213	The reranking parser used the WSJ-trained reranker model." ></td>
	<td class="line x" title="176:213	The BROWN parsing model is naturally better than the WSJ model for this task, but combining the two training corpora results in a better model (as in Gildea (2001))." ></td>
	<td class="line x" title="177:213	Adding small amounts of NANC further improves the models." ></td>
	<td class="line x" title="178:213	Parser model Parser alone WSJ-reranker BROWN-reranker WSJ 82.9 85.2 85.2 WSJ+NANC 87.1 87.8 87.9 BROWN 86.7 88.2 88.4 Table 5: Performance of various combinations of parser and reranker models when evaluated on BROWN test." ></td>
	<td class="line x" title="179:213	The WSJ+NANC parser with the WSJ reranker comes close to the BROWN-trained reranking parser." ></td>
	<td class="line x" title="180:213	The BROWN reranker provides only a small improvement over its WSJ counterpart, which is not statistically significant." ></td>
	<td class="line x" title="181:213	342 Bracketing agreement f-score 88.03% Complete match 44.92% Average crossing brackets 0.94 POS Tagging agreement 94.85% Table 7: Agreement between the WSJ+NANC parser with the WSJ reranker and the BROWN parser with the BROWN reranker." ></td>
	<td class="line x" title="182:213	Complete match is how often the two reranking parsers returned the exact same parse." ></td>
	<td class="line x" title="183:213	the same test sentence should not affect the corpus f-scores." ></td>
	<td class="line x" title="184:213	By estimating the proportion of permutations that result in an absolute difference in corpus f-scores at least as great as that observed in the actual output, we obtain a distributionfree estimate of significance that is robust against parser and evaluator failures." ></td>
	<td class="line x" title="185:213	The results of this test are shown in Table 8." ></td>
	<td class="line x" title="186:213	The table shows that the BROWN reranker is not significantly different from the WSJ reranker." ></td>
	<td class="line x" title="187:213	In order to better understand the difference between the reranking parser trained on Brown and the WSJ+NANC/WSJ reranking parser (a reranking parser with the first-stage trained on WSJ+NANC and the second-stage trained on WSJ) on Brown data, we constructed a logistic regression model of the difference between the two parsers fscores on the development data using the R statistical package5." ></td>
	<td class="line x" title="188:213	Of the 2,078 sentences in the development data, 29 sentences were discarded because evalb failed to evaluate at least one of the parses.6 A Wilcoxon signed rank test on the remaining 2,049 paired sentence level f-scores was significant at p = 0.0003." ></td>
	<td class="line x" title="189:213	Of these 2,049 sentences, there were 983 parse pairs with the same sentence-level f-score." ></td>
	<td class="line x" title="190:213	Of the 1,066 sentences for which the parsers produced parses with different f-scores, there were 580 sentences for which the BROWN/BROWN parser produced a parse with a higher sentence-level f-score and 486 sentences for which the WSJ+NANC/WSJ parser produce a parse with a higher f-score." ></td>
	<td class="line x" title="191:213	We constructed a generalized linear model with a binomial link with BROWN/BROWN f-score > WSJ+NANC/WSJ f-score as the predicted variable, and sentence length, the number of prepositions (IN), the number of conjunctions (CC) and Brown 5http://www.r-project.org 6This occurs when an apostrophe is analyzed as a possessive marker in the gold tree and a punctuation symbol in the parse tree, or vice versa." ></td>
	<td class="line x" title="192:213	Feature Estimate z-value Pr(> |z|) (Intercept) 0.054 0.3 0.77 IN -0.134 -4.4 8.4e-06 *** ID=G 0.584 2.5 0.011 * ID=K 0.697 2.9 0.003 ** ID=L 0.552 2.3 0.021 * ID=M 0.376 0.9 0.33 ID=N 0.642 2.7 0.0055 ** ID=P 0.624 2.7 0.0069 ** ID=R 0.040 0.1 0.90 Table 9: The logistic model of BROWN/BROWN f-score > WSJ+NANC/WSJ f-score identified by model selection." ></td>
	<td class="line x" title="193:213	The feature IN is the number prepositions in the sentence, while ID identifies the Brown subcorpus that the sentence comes from." ></td>
	<td class="line x" title="194:213	Stars indicate significance level." ></td>
	<td class="line x" title="195:213	subcorpus ID as explanatory variables." ></td>
	<td class="line x" title="196:213	Model selection (using the step procedure) discarded all but the IN and Brown ID explanatory variables." ></td>
	<td class="line x" title="197:213	The final estimated model is shown in Table 9." ></td>
	<td class="line x" title="198:213	It shows that the WSJ+NANC/WSJ parser becomes more likely to have a higher f-score than the BROWN/BROWN parser as the number of prepositions in the sentence increases, and that the BROWN/BROWN parser is more likely to have a higher f-score on Brown sections K, N, P, G and L (these are the general fiction, adventure and western fiction, romance and love story, letters and memories, and mystery sections of the Brown corpus, respectively)." ></td>
	<td class="line x" title="199:213	The three sections of BROWN not in this list are F, M, and R (popular lore, science fiction, and humor)." ></td>
	<td class="line x" title="200:213	6 Conclusions and Future Work We have demonstrated that rerankers and selftrained models can work well across domains." ></td>
	<td class="line x" title="201:213	Models self-trained on WSJ appear to be better parsing models in general, the benefits of which are not limited to the WSJ domain." ></td>
	<td class="line x" title="202:213	The WSJtrained reranker using out-of-domain LA Times parses (produced by the WSJ-trained reranker) achieves a labeled precision-recall f-measure of 87.8% on Brown data, nearly equal to the performance one achieves by using a purely Brown trained parser-reranker." ></td>
	<td class="line x" title="203:213	The 87.8% f-score on Brown represents a 24% error reduction on the corpus." ></td>
	<td class="line x" title="204:213	Of course, as corpora differences go, Brown is relatively close to WSJ." ></td>
	<td class="line x" title="205:213	While we also find that our 343 WSJ+NANC/WSJ BROWN/WSJ BROWN/BROWN WSJ/WSJ 0.025 (0) 0.030 (0) 0.031 (0) WSJ+NANC/WSJ 0.004 (0.1) 0.006 (0.025) BROWN/WSJ 0.002 (0.27) Table 8: The difference in corpus f-score between the various reranking parsers, and the significance of the difference in parentheses as estimated by a randomization test with 106 samples." ></td>
	<td class="line x" title="206:213	x/y indicates that the first-stage parser was trained on data set x and the second-stage reranker was trained on data set y. best WSJ-parser-reranker improves performance on the Switchboard corpus, it starts from a much lower base (74.0%), and achieves a much less significant improvement (3% absolute, 11% error reduction)." ></td>
	<td class="line x" title="207:213	Bridging these larger gaps is still for the future." ></td>
	<td class="line x" title="208:213	One intriguing idea is what we call self-trained bridging-corpora. We have not yet experimented with medical text but we expect that the best WSJ+NANC parser will not perform very well." ></td>
	<td class="line x" title="209:213	However, suppose one does self-training on a biology textbook instead of the LA Times." ></td>
	<td class="line x" title="210:213	One might hope that such a text will split the difference between more normal newspaper articles and the specialized medical text." ></td>
	<td class="line x" title="211:213	Thus, a selftrained parser based upon such text might do much better than our standard best. This is, of course, highly speculative." ></td>
	<td class="line x" title="212:213	Acknowledgments This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-20001." ></td>
	<td class="line x" title="213:213	We would like to thank the BLLIP team for their comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-1109
An All-Subtrees Approach To Unsupervised Parsing
Bod, Rens;"></td>
	<td class="line x" title="1:166	Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 865872, Sydney, July 2006." ></td>
	<td class="line x" title="2:166	c2006 Association for Computational Linguistics An All-Subtrees Approach to Unsupervised Parsing Rens Bod School of Computer Science University of St Andrews North Haugh, St Andrews KY16 9SX Scotland, UK rb@dcs.st-and.ac.uk Abstract We investigate generalizations of the allsubtrees 'DOP' approach to unsupervised parsing." ></td>
	<td class="line x" title="3:166	Unsupervised DOP models assign all possible binary trees to a set of sentences and next use (a large random subset of) all subtrees from these binary trees to compute the most probable parse trees." ></td>
	<td class="line x" title="4:166	We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent." ></td>
	<td class="line x" title="5:166	We report state-ofthe-art results on English (WSJ), German (NEGRA) and Chinese (CTB) data." ></td>
	<td class="line x" title="6:166	To the best of our knowledge this is the first paper which tests a maximum likelihood estimator for DOP on the Wall Street Journal, leading to the surprising result that an unsupervised parsing model beats a widely used supervised model (a treebank PCFG)." ></td>
	<td class="line x" title="7:166	1 Introduction The problem of bootstrapping syntactic structure from unlabeled data has regained considerable interest." ></td>
	<td class="line x" title="8:166	While supervised parsers suffer from shortage of hand-annotated data, unsupervised parsers operate with unlabeled raw data of which unlimited quantities are available." ></td>
	<td class="line x" title="9:166	During the last few years there has been steady progress in the field." ></td>
	<td class="line x" title="10:166	Where van Zaanen (2000) achieved 39.2% unlabeled f-score on ATIS word strings, Clark (2001) reports 42.0% on the same data, and Klein and Manning (2002) obtain 51.2% f-score on ATIS part-of-speech strings using a constituent-context model called CCM." ></td>
	<td class="line x" title="11:166	On Penn Wall Street Journal po-s-strings  10 (WSJ10), Klein and Manning (2002) report 71.1% unlabeled f-score with CCM." ></td>
	<td class="line x" title="12:166	And the hybrid approach of Klein and Manning (2004), which combines constituency and dependency models, yields 77.6% f-score." ></td>
	<td class="line x" title="13:166	Bod (2006) shows that a further improvement on the WSJ10 can be achieved by an unsupervised generalization of the all-subtrees approach known as Data-Oriented Parsing (DOP)." ></td>
	<td class="line x" title="14:166	This unsupervised DOP model, coined U-DOP, first assigns all possible unlabeled binary trees to a set of sentences and next uses all subtrees from (a large subset of) these trees to compute the most probable parse trees." ></td>
	<td class="line x" title="15:166	Bod (2006) reports that U-DOP not only outperforms previous unsupervised parsers but that its performance is as good as a binarized supervised parser (i.e. a treebank PCFG) on the WSJ." ></td>
	<td class="line x" title="16:166	A possible drawback of U-DOP, however, is the statistical inconsistency of its estimator (Johnson 2002) which is inherited from the DOP1 model (Bod 1998)." ></td>
	<td class="line x" title="17:166	That is, even with unlimited training data, U-DOP's estimator is not guaranteed to converge to the correct weight distribution." ></td>
	<td class="line x" title="18:166	Johnson (2002: 76) argues in favor of a maximum likelihood estimator for DOP which is statistically consistent." ></td>
	<td class="line x" title="19:166	As it happens, in Bod (2000) we already developed such a DOP model, termed ML-DOP, which reestimates the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization." ></td>
	<td class="line x" title="20:166	Although crossvalidation is needed to avoid overlearning, ML-DOP outperforms DOP1 on the OVIS corpus (Bod 2000)." ></td>
	<td class="line x" title="21:166	This raises the question whether we can create an unsupervised DOP model which is also 865 statistically consistent." ></td>
	<td class="line x" title="22:166	In this paper we will show that an unsupervised version of ML-DOP can be constructed along the lines of U-DOP." ></td>
	<td class="line x" title="23:166	We will start out by summarizing DOP, U-DOP and ML-DOP, and next create a new unsupervised model called UML-DOP." ></td>
	<td class="line x" title="24:166	We report that UML-DOP not only obtains higher parse accuracy than U-DOP on three different domains, but that it also achieves this with fewer subtrees than U-DOP." ></td>
	<td class="line x" title="25:166	To the best of our knowledge, this paper presents the first unsupervised parser that outperforms a widely used supervised parser on the WSJ, i.e. a treebank PCFG." ></td>
	<td class="line x" title="26:166	We will raise the question whether the end of supervised parsing is in sight." ></td>
	<td class="line x" title="27:166	2 DOP The key idea of DOP is this: given an annotated corpus, use all subtrees, regardless of size, to parse new sentences." ></td>
	<td class="line x" title="28:166	The DOP1 model in Bod (1998) computes the probabilities of parse trees and sentences from the relative frequencies of the subtrees." ></td>
	<td class="line x" title="29:166	Although it is now known that DOP1's relative frequency estimator is statistically inconsistent (Johnson 2002), the model yields excellent empirical results and has been used in state-of-the-art systems." ></td>
	<td class="line x" title="30:166	Let's illustrate DOP1 with a simple example." ></td>
	<td class="line x" title="31:166	Assume a corpus consisting of only two trees, as given in figure 1." ></td>
	<td class="line x" title="32:166	NP VP S NP Mary V likes John NP VP S NPVPeter hates Susan Figure 1." ></td>
	<td class="line x" title="33:166	A corpus of two trees New sentences may be derived by combining fragments, i.e. subtrees, from this corpus, by means of a node-substitution operation indicated as ." ></td>
	<td class="line x" title="34:166	Node-substitution identifies the leftmost nonterminal frontier node of one subtree with the root node of a second subtree (i.e. , the second subtree is substituted on the leftmost nonterminal frontier node of the first subtree)." ></td>
	<td class="line x" title="35:166	Thus a new sentence such as Mary likes Susan can be derived by combining subtrees from this corpus, shown in figure 2." ></td>
	<td class="line x" title="36:166	NP VP S NPV likes NP Mary NP Susan NP VP S NPMary V likes Susan =  Figure 2." ></td>
	<td class="line x" title="37:166	A derivation for Mary likes Susan Other derivations may yield the same tree, e.g.: NP VP S NPV NP Mary NP VP S NPMary V likes Susan = Susan V likes   Figure 3." ></td>
	<td class="line x" title="38:166	Another derivation yielding same tree DOP1 computes the probability of a subtree t as the probability of selecting t among all corpus subtrees that can be substituted on the same node as t. This probability is computed as the number of occurrences of t in the corpus, | t |, divided by the total number of occurrences of all subtrees t' with the same root label as t.1 Let r(t) return the root label of t. Then we may write: P(t) = | t | t': r(t')= r(t) | t' | The probability of a derivation t1tn is computed by the product of the probabilities of its subtrees ti: P(t1tn) = i P(ti) As we have seen, there may be several distinct derivations that generate the same parse tree." ></td>
	<td class="line x" title="39:166	The probability of a parse tree T is the sum of the 1 This subtree probability is redressed by a simple correction factor discussed in Goodman (2003: 136) and Bod (2003)." ></td>
	<td class="line x" title="40:166	866 probabilities of its distinct derivations." ></td>
	<td class="line x" title="41:166	Let tid be the i-th subtree in the derivation d that produces tree T, then the probability of T is given by P(T) = di P(tid) Thus DOP1 considers counts of subtrees of a wide range of sizes: everything from counts of singlelevel rules to entire trees is taken into account to compute the most probable parse tree of a sentence." ></td>
	<td class="line x" title="42:166	A disadvantage of the approach may be that an extremely large number of subtrees (and derivations) must be considered." ></td>
	<td class="line x" title="43:166	Fortunately there exists a compact isomorphic PCFG-reduction of DOP1 whose size is linear rather than exponential in the size of the training set (Goodman 2003)." ></td>
	<td class="line x" title="44:166	Moreover, Collins and Duffy (2002) show how a tree kernel can be applied to DOP1's all-subtrees representation." ></td>
	<td class="line x" title="45:166	The currently most successful version of DOP1 uses a PCFG-reduction of the model with an n-best parsing algorithm (Bod 2003)." ></td>
	<td class="line x" title="46:166	3 U-DOP U-DOP extends DOP1 to unsupervised parsing (Bod 2006)." ></td>
	<td class="line x" title="47:166	Its key idea is to assign all unlabeled binary trees to a set of sentences and to next use (in principle) all subtrees from these binary trees to parse new sentences." ></td>
	<td class="line x" title="48:166	U-DOP thus proposes one of the richest possible models in bootstrapping trees." ></td>
	<td class="line x" title="49:166	Previous models like Klein and Manning's (2002, 2005) CCM model limit the dependencies to 'contiguous subsequences of a sentence'." ></td>
	<td class="line x" title="50:166	This means that CCM neglects dependencies that are non-contiguous such as between more and than in 'BA carried more people than cargo'." ></td>
	<td class="line x" title="51:166	Instead, UDOP's all-subtrees approach captures both contiguous and non-contiguous lexical dependencies." ></td>
	<td class="line x" title="52:166	As with most other unsupervised parsing models, U-DOP induces trees for p-o-s strings rather than for word strings." ></td>
	<td class="line x" title="53:166	The extension to word strings is straightforward as there exist highly accurate unsupervised part-of-speech taggers (e.g. Schtze 1995) which can be directly combined with unsupervised parsers." ></td>
	<td class="line x" title="54:166	To give an illustration of U-DOP, consider the WSJ p-o-s string NNS VBD JJ NNS which may correspond for instance to the sentence Investors suffered heavy losses." ></td>
	<td class="line x" title="55:166	U-DOP starts by assigning all possible binary trees to this string, where each root node is labeled S and each internal node is labeled X. Thus NNS VBD JJ NNS has a total of five binary trees shown in figure 4 -where for readability we add words as well." ></td>
	<td class="line x" title="56:166	NNS VBD JJ NNS Investors suffered heavy losses X X S NNS VBD JJ NNS Investors suffered heavy losses X X S NNS VBD JJ NNS Investors suffered heavy losses X X S NNS VBD JJ NNS Investors suffered heavy losses X X S NNS VBD JJ NNS Investors suffered heavy losses XX S Figure 4." ></td>
	<td class="line x" title="57:166	All binary trees for NNS VBD JJ NNS (Investors suffered heavy losses) While we can efficiently represent the set of all binary trees of a string by means of a chart, we need to unpack the chart if we want to extract subtrees from this set of binary trees." ></td>
	<td class="line x" title="58:166	And since the total number of binary trees for the small WSJ10 is almost 12 million, it is doubtful whether we can apply the unrestricted U-DOP model to such a corpus." ></td>
	<td class="line x" title="59:166	U-DOP therefore randomly samples a large subset from the total number of parse trees from the chart (see Bod 2006) and next converts the subtrees from these parse trees into a PCFG-reduction (Goodman 2003)." ></td>
	<td class="line x" title="60:166	Since the computation of the most probable parse tree is NP-complete (Sima'an 1996), U-DOP estimates the most probable tree from the 100 most probable derivations using Viterbi n-best parsing." ></td>
	<td class="line x" title="61:166	We could also have used the more efficient k-best hypergraph parsing technique by Huang and Chiang (2005), but we have not yet incorporated this into our implementation." ></td>
	<td class="line x" title="62:166	To give an example of the dependencies that U-DOP can take into account, consider the following subtrees in figure 5 from the trees in 867 figure 4 (where we again add words for readability)." ></td>
	<td class="line x" title="63:166	These subtrees show that U-DOP takes into account both contiguous and non-contiguous substrings." ></td>
	<td class="line x" title="64:166	NNS VBD Investors suffered X X S VBD suffered X X NNS NNS Investors losses X X S JJ NNS heavy losses XX S JJ NNS heavy losses X NNS VBD Investors suffered X VBD JJ suffered heavy X Figure 5." ></td>
	<td class="line x" title="65:166	Some subtrees from trees in figure 4 Of course, if we only had the sentence Investors suffered heavy losses in our corpus, there would be no difference in probability between the five parse trees in figure 4." ></td>
	<td class="line x" title="66:166	However, if we also have a different sentence where JJ NNS ( heavy losses) appears in a different context, e.g. in Heavy losses were reported, its covering subtree gets a relatively higher frequency and the parse tree where heavy losses occurs as a constituent gets a higher total probability." ></td>
	<td class="line x" title="67:166	4 ML-DOP ML-DOP (Bod 2000) extends DOP with a maximum likelihood reestimation technique based on the expectation-maximization (EM) algorithm (Dempster et al. 1977) which is known to be statistically consistent (Shao 1999)." ></td>
	<td class="line x" title="68:166	ML-DOP reestimates DOP's subtree probabilities in an iterative way until the changes become negligible." ></td>
	<td class="line x" title="69:166	The following exposition of ML-DOP is heavily based on previous work by Bod (2000) and Magerman (1993)." ></td>
	<td class="line x" title="70:166	It is important to realize that there is an implicit assumption in DOP that all possible derivations of a parse tree contribute equally to the total probability of the parse tree." ></td>
	<td class="line x" title="71:166	This is equivalent to saying that there is a hidden component to the model, and that DOP can be trained using an EM algorithm to determine the maximum likelihood estimate for the training data." ></td>
	<td class="line x" title="72:166	The EM algorithm for this ML-DOP model is related to the Inside-Outside algorithm for context-free grammars, but the reestimation formula is complicated by the presence of subtrees of depth greater than 1." ></td>
	<td class="line x" title="73:166	To derive the reestimation formula, it is useful to consider the state space of all possible derivations of a tree." ></td>
	<td class="line x" title="74:166	The derivations of a parse tree T can be viewed as a state trellis, where each state contains a partially constructed tree in the course of a leftmost derivation of T. st denotes a state containing the tree t which is a subtree of T. The state trellis is defined as follows." ></td>
	<td class="line x" title="75:166	The initial state, s0, is a tree with depth zero, consisting of simply a root node labeled with S. The final state, sT, is the given parse tree T. A state st is connected forward to all states stf such that tf = t  t', for some t'." ></td>
	<td class="line x" title="76:166	Here the appropriate t' is defined to be tf  t. A state st is connected backward to all states stb such that t = tb  t', for some t' . Again, t' is defined to be t  tb." ></td>
	<td class="line x" title="77:166	The construction of the state lattice and assignment of transition probabilities according to the ML-DOP model is called the forward pass." ></td>
	<td class="line x" title="78:166	The probability of a given state, P(s), is referred to as (s)." ></td>
	<td class="line x" title="79:166	The forward probability of a state st is computed recursively (st) =  (st ) P(t  tb).b stb The backward probability of a state, referred to as (s), is calculated according to the following recursive formula: (st) =  (st ) P(tf  t)f fst where the backward probability of the goal state is set equal to the forward probability of the goal state, (sT) = (sT)." ></td>
	<td class="line x" title="80:166	The update formula for the count of a subtree t is (where r(t) is the root label of t): 868 ct(t) =  (st )(st )P(t | r(t)) f b (sgoal)st :st,tbt=tf b f The updated probability distribution, P'(t | r(t)), is defined to be P'(t | r(t)) = ct(t)ct(r(t)) where ct(r(t)) is defined as ct(r(t)) =  ct(t') t': r(t')=r(t) In practice, ML-DOP starts out by assigning the same relative frequencies to the subtrees as DOP1, which are next reestimated by the formulas above." ></td>
	<td class="line x" title="81:166	We may in principle start out with any initial parameters, including random initializations, but since ML estimation is known to be very sensitive to the initialization of the parameters, it is convenient to start with parameters that are known to perform well." ></td>
	<td class="line x" title="82:166	To avoid overtraining, ML-DOP uses the subtrees from one half of the training set to be trained on the other half, and vice versa." ></td>
	<td class="line x" title="83:166	This crosstraining is important since otherwise UML-DOP would assign the training set trees their empirical frequencies and assign zero weight to all other subtrees (cf.Prescher et al. 2004)." ></td>
	<td class="line x" title="85:166	The updated probabilities are iteratively reestimated until the decrease in cross-entropy becomes negligible." ></td>
	<td class="line x" title="86:166	Unfortunately, no compact PCFG-reduction of MLDOP is known." ></td>
	<td class="line x" title="87:166	As a consequence, parsing with ML-DOP is very costly and the model has hitherto never been tested on corpora larger than OVIS (Bonnema et al. 1997)." ></td>
	<td class="line x" title="88:166	Yet, we will show that by clever pruning we can extend our experiments not only to the WSJ, but also to the German NEGRA and the Chinese CTB." ></td>
	<td class="line x" title="89:166	(Zollmann and Sima'an 2005 propose a different consistent estimator for DOP, which we cannot go into here)." ></td>
	<td class="line x" title="90:166	5 UML-DOP Analogous to U-DOP, UML-DOP is an unsupervised generalization of ML-DOP: it first assigns all unlabeled binary trees to a set of sentences and next extracts a large (random) set of subtrees from this tree set." ></td>
	<td class="line x" title="91:166	It then reestimates the initial probabilities of these subtrees by ML-DOP on the sentences from a held-out part of the tree set." ></td>
	<td class="line x" title="92:166	The training is carried out by dividing the tree set into two equal parts, and reestimating the subtrees from one part on the other." ></td>
	<td class="line x" title="93:166	As initial probabilities we use the subtrees' relative frequencies as described in section 2 (smoothed by Good-Turing -see Bod 1998), though it would also be interesting to see how the model works with other initial parameters, in particular with the usage frequencies proposed by Zuidema (2006)." ></td>
	<td class="line x" title="94:166	As with U-DOP, the total number of subtrees that can be extracted from the binary tree set is too large to be fully taken into account." ></td>
	<td class="line x" title="95:166	Together with the high computational cost of reestimation we propose even more drastic pruning than we did in Bod (2006) for U-DOP." ></td>
	<td class="line x" title="96:166	That is, while for sentences  7 words we use all binary trees, for each sentence  8 words we randomly sample a fixed number of 128 trees (which effectively favors more frequent trees)." ></td>
	<td class="line x" title="97:166	The resulting set of trees is referred to as the binary tree set." ></td>
	<td class="line x" title="98:166	Next, we randomly extract for each subtree-depth a fixed number of subtrees, where the depth of subtree is the longest path from root to any leaf." ></td>
	<td class="line x" title="99:166	This has roughly the same effect as the correction factor used in Bod (2003, 2006)." ></td>
	<td class="line x" title="100:166	That is, for each particular depth we sample subtrees by first randomly selecting a node in a random tree from the binary tree set after which we select random expansions from that node until a subtree of the particular depth is obtained." ></td>
	<td class="line x" title="101:166	For our experiments in section 6, we repeated this procedure 200,000 times for each depth." ></td>
	<td class="line x" title="102:166	The resulting subtrees are then given to MLDOP's reestimation procedure." ></td>
	<td class="line x" title="103:166	Finally, the reestimated subtrees are used to compute the most probable parse trees for all sentences using Viterbi n-best, as described in section 3, where the most probable parse is estimated from the 100 most probable derivations." ></td>
	<td class="line x" title="104:166	A potential criticism of (U)ML-DOP is that since we use DOP1's relative frequencies as initial parameters, ML-DOP may only find a local maximum nearest to DOP1's estimator." ></td>
	<td class="line x" title="105:166	But this is of course a criticism against any iterative ML approach: it is not guaranteed that the global maximum is found (cf.Manning and Schtze 1999: 401)." ></td>
	<td class="line x" title="107:166	Nevertheless we will see that our reestimation 869 procedure leads to significantly better accuracy compared to U-DOP (the latter would be equal to UML-DOP under 0 iterations)." ></td>
	<td class="line x" title="108:166	Moreover, in contrast to U-DOP, UML-DOP can be theoretically motivated: it maximizes the likelihood of the data using the statistically consistent EM algorithm." ></td>
	<td class="line x" title="109:166	6 Experiments: Can we beat supervised by unsupervised parsing?" ></td>
	<td class="line x" title="110:166	To compare UML-DOP to U-DOP, we started out with the WSJ10 corpus, which contains 7422 sentences  10 words after removing empty elements and punctuation." ></td>
	<td class="line x" title="111:166	We used the same evaluation metrics for unlabeled precision (UP) and unlabeled recall (UR) as defined in Klein (2005: 2122)." ></td>
	<td class="line x" title="112:166	Klein's definitions differ slightly from the standard PARSEVAL metrics: multiplicity of brackets is ignored, brackets of span one are ignored and the bracket labels are ignored." ></td>
	<td class="line x" title="113:166	The two metrics of UP and UR are combined by the unlabeled f score F1 which is defined as the harmonic mean of UP and UR: F1 = 2*UP*UR/(UP+UR)." ></td>
	<td class="line x" title="114:166	For the WSJ10, we obtained a binary tree set of 5.68 * 105 trees, by extracting the binary trees as described in section 5." ></td>
	<td class="line x" title="115:166	From this binary tree set we sampled 200,000 subtrees for each subtreedepth." ></td>
	<td class="line x" title="116:166	This resulted in a total set of roughly 1.7 * 106 subtrees that were reestimated by our maximum-likelihood procedure." ></td>
	<td class="line x" title="117:166	The decrease in cross-entropy became negligible after 14 iterations (for both halfs of WSJ10)." ></td>
	<td class="line x" title="118:166	After computing the most probable parse trees, UML-DOP achieved an f-score of 82.9% which is a 20.5% error reduction compared to U-DOP's f-score of 78.5% on the same data (Bod 2006)." ></td>
	<td class="line x" title="119:166	We next tested UML-DOP on two additional domains which were also used in Klein and Manning (2004) and Bod (2006): the German NEGRA10 (Skut et al. 1997) and the Chinese CTB10 (Xue et al. 2002) both containing 2200+ sentences  10 words after removing punctuation." ></td>
	<td class="line x" title="120:166	Table 1 shows the results of UML-DOP compared to U-DOP, the CCM model by Klein and Manning (2002), the DMV dependency learning model by Klein and Manning (2004) as well as their combined model DMV+CCM." ></td>
	<td class="line x" title="121:166	Table 1 shows that UML-DOP scores better than U-DOP and Klein and Manning's models in all cases." ></td>
	<td class="line x" title="122:166	It thus pays off to not only use subtrees rather than substrings (as in CCM) but to also reestimate the subtrees' probabilities by a maximum-likelihood procedure rather than using their (smoothed) relative frequencies (as in U-DOP)." ></td>
	<td class="line x" title="123:166	Note that UML-DOP achieves these improved results with fewer subtrees than U-DOP, due to UML-DOP's more drastic pruning of subtrees." ></td>
	<td class="line x" title="124:166	It is also noteworthy that UMLDOP, like U-DOP, does not employ a separate class for non-constituents, so-called distituents, while CCM and CCM+DMV do." ></td>
	<td class="line x" title="125:166	(Interestingly, the top 10 most frequently learned constituents by UMLDOP were exactly the same as by U-DOP -see the relevant table in Bod 2006)." ></td>
	<td class="line x" title="126:166	Model English German Chinese(WSJ10) (NEGRA10) (CTB10) CCM 71.9 61.6 45.0 DMV 52.1 49.5 46.7 DMV+CCM 77.6 63.9 43.3 U-DOP 78.5 65.4 46.6 UML-DOP 82.9 67.0 47.2 Table 1." ></td>
	<td class="line x" title="127:166	F-scores of UML-DOP compared to previous models on the same data We were also interested in testing UML-DOP on longer sentences." ></td>
	<td class="line x" title="128:166	We therefore included all WSJ sentences up to 40 words after removing empty elements and punctuation (WSJ40) and again sampled 200,000 subtrees for each depth, using the same method as before." ></td>
	<td class="line x" title="129:166	Furthermore, we compared UML-DOP against a supervised binarized PCFG, i.e. a treebank PCFG whose simple relative frequency estimator corresponds to maximum likelihood (Chi and Geman 1998), and which we shall refer to as 'ML-PCFG'." ></td>
	<td class="line x" title="130:166	To this end, we used a random 90%/10% division of WSJ40 into a training set and a test set." ></td>
	<td class="line x" title="131:166	The ML-PCFG had thus access to the Penn WSJ trees in the training set, while UML-DOP had to bootstrap all structure from the flat strings from the training set to next parse the 10% test set -clearly a much more challenging task." ></td>
	<td class="line x" title="132:166	Table 2 gives the results in terms of f-scores." ></td>
	<td class="line x" title="133:166	The table shows that UML-DOP scores better than U-DOP, also for WSJ40." ></td>
	<td class="line x" title="134:166	Our results on WSJ10 are somewhat lower than in table 1 due to the use of a smaller training set of 90% of the data." ></td>
	<td class="line x" title="135:166	But the most surprising result is that UML-DOP's f870 score is higher than the supervised binarized treebank PCFG (ML-PCFG) for both WSJ10 and WSJ40." ></td>
	<td class="line x" title="136:166	In order to check whether this difference is statistically significant, we additionally tested on 10 different 90/10 divisions of the WSJ40 (which were the same splits as in Bod 2006)." ></td>
	<td class="line x" title="137:166	For these splits, UML-DOP achieved an average f-score of 66.9%, while ML-PCFG obtained an average f-score of 64.7%." ></td>
	<td class="line x" title="138:166	The difference in accuracy between UMLDOP and ML-PCFG was statistically significant according to paired t-testing (p0.05)." ></td>
	<td class="line x" title="139:166	To the best of our knowledge this means that we have shown for the first time that an unsupervised parsing model (UML-DOP) outperforms a widely used supervised parsing model (a treebank PCFG) on the WSJ40." ></td>
	<td class="line x" title="140:166	Model WSJ10 WSJ40 U-DOP 78.1 63.9 UML-DOP 82.5 66.4 ML-PCFG 81.5 64.6 Table 2." ></td>
	<td class="line x" title="141:166	F-scores of U-DOP, UML-DOP and a supervised treebank PCFG (ML-PCFG) for a random 90/10 split of WSJ10 and WSJ40." ></td>
	<td class="line pc" title="142:166	We should keep in mind that (1) a treebank PCFG is not state-of-the-art: its performance is mediocre compared to e.g. Bod (2003) or McClosky et al.(2006), and (2) that our treebank PCFG is binarized as in Klein and Manning (2005) to make results comparable." ></td>
	<td class="line x" title="144:166	To be sure, the unbinarized version of the treebank PCFG obtains 89.0% average f-score on WSJ10 and 72.3% average f-score on WSJ40." ></td>
	<td class="line x" title="145:166	Remember that the Penn Treebank annotations are often exceedingly flat, and many branches have arity larger than two." ></td>
	<td class="line x" title="146:166	It would be interesting to see how UML-DOP performs if we also accept ternary (and wider) branches -though the total number of possible trees that can be assigned to strings would then further explode." ></td>
	<td class="line x" title="147:166	UML-DOP's performance still remains behind that of supervised (binarized) DOP parsers, such as DOP1, which achieved 81.9% average fscore on the 10 WSJ40 splits, and ML-DOP, which performed slightly better with 82.1% average fscore." ></td>
	<td class="line x" title="148:166	And if DOP1 and ML-DOP are not binarized, their average f-scores are respectively 90.1% and 90.5% on WSJ40." ></td>
	<td class="line x" title="149:166	However, DOP1 and ML-DOP heavily depend on annotated data whereas UML-DOP only needs unannotated data." ></td>
	<td class="line x" title="150:166	It would thus be interesting to see how close UML-DOP can get to ML-DOP's performance if we enlarge the amount of training data." ></td>
	<td class="line x" title="151:166	7 Conclusion: Is the end of supervised parsing in sight?" ></td>
	<td class="line x" title="152:166	Now that we have outperformed a well-known supervised parser by an unsupervised one, we may raise the question as to whether the end of supervised NLP comes in sight." ></td>
	<td class="line oc" title="153:166	All supervised parsers are reaching an asymptote and further improvement does not seem to come from more hand-annotated data but by adding unsupervised or semi-unsupervised techniques (cf.McClosky et al. 2006)." ></td>
	<td class="line x" title="155:166	Thus if we modify our question as: does the exclusively supervised approach to parsing come to an end, we believe that the answer is certainly yes." ></td>
	<td class="line x" title="156:166	Yet we should neither rule out the possibility that entirely unsupervised methods will in fact surpass semi-supervised methods." ></td>
	<td class="line x" title="157:166	The main problem is how to quantitatively compare these different parsers, as any evaluation on handannotated data (like the Penn treebank) will unreasonably favor semi-supervised parsers." ></td>
	<td class="line x" title="158:166	There is thus is a quest for designing an annotationindependent evaluation scheme." ></td>
	<td class="line x" title="159:166	Since parsers are becoming increasingly important in applications like syntax-based machine translation and structural language models for speech recognition, one way to go would be to compare these different parsing methods by isolating their contribution in improving a concrete NLP system, rather than by testing them against gold standard annotations which are inherently theory-dependent." ></td>
	<td class="line x" title="160:166	The initially disappointing results of inducing trees entirely from raw text was not so much due to the difficulty of the bootstrapping problem per se, but to (1) the poverty of the initial models and (2) the difficulty of finding theoryindependent evaluation criteria." ></td>
	<td class="line x" title="161:166	The time has come to fully reappraise unsupervised parsing models which should be trained on massive amounts of data, and be evaluated in a concrete application." ></td>
	<td class="line x" title="162:166	There is a final question as to how far the DOP approach to unsupervised parsing can be stretched." ></td>
	<td class="line x" title="163:166	In principle we can assign all possible syntactic categories, semantic roles, argument 871 structures etc. to a set of given sentences and let the statistics decide which assignments are most useful in parsing new sentences." ></td>
	<td class="line x" title="164:166	Whether such a massively maximalist approach is feasible can only be answered by empirical investigation in due time." ></td>
	<td class="line x" title="165:166	Acknowledgements Thanks to Willem Zuidema, David Tugwell and especially to three anonymous reviewers whose unanymous suggestions on DOP and EM considerably improved the original paper." ></td>
	<td class="line x" title="166:166	A substantial part of this research was carried out in the context of the NWO Exact project 'Unsupervised Stochastic Grammar Induction from Unlabeled Data', project number 612.066.405." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N07-1070
Towards Robust Semantic Role Labeling
Pradhan, Sameer S.;Ward, Wayne H.;Martin, James H.;"></td>
	<td class="line x" title="1:232	Proceedings of NAACL HLT 2007, pages 556563, Rochester, NY, April 2007." ></td>
	<td class="line x" title="2:232	c2007 Association for Computational Linguistics Towards Robust Semantic Role Labeling Sameer Pradhan BBN Technologies Cambridge, MA 02138 pradhan@bbn.com Wayne Ward, James H. Martin University of Colorado Boulder, CO 80303 {whw,martin}@colorado.edu Abstract Most research on semantic role labeling (SRL) has been focused on training and evaluating on the same corpus in order to develop the technology." ></td>
	<td class="line x" title="3:232	This strategy, while appropriate for initiating research, can lead to over-training to the particular corpus." ></td>
	<td class="line x" title="4:232	The work presented in this paper focuses on analyzing the robustness of an SRL system when trained on one genre of data and used to label a different genre." ></td>
	<td class="line x" title="5:232	Our state-of-the-art semantic role labeling system, while performing well on WSJ test data, shows significant performance degradation when applied to data from the Brown corpus." ></td>
	<td class="line x" title="6:232	We present a series of experiments designed to investigate the source of this lack of portability." ></td>
	<td class="line x" title="7:232	These experiments are based on comparisons of performance using PropBanked WSJ data and PropBanked Brown corpus data." ></td>
	<td class="line x" title="8:232	Our results indicate that while syntactic parses and argument identification port relatively well to a new genre, argument classification does not." ></td>
	<td class="line x" title="9:232	Our analysis of the reasons for this is presented and generally point to the nature of the more lexical/semantic features dominating the classification task and general structural features dominating the argument identification task." ></td>
	<td class="line x" title="10:232	1 Introduction Automatic, accurate and wide-coverage techniques that can annotate naturally occurring text with semantic argument structure play a key role in NLP applications such as Information Extraction (Surdeanu et al. , 2003; Harabagiu et al. , 2005), Question Answering (Narayanan and Harabagiu, 2004) and Machine Translation (Boas, 2002; Chen and Fung, 2004)." ></td>
	<td class="line x" title="11:232	Semantic Role Labeling (SRL) is the process of producing such a markup." ></td>
	<td class="line x" title="12:232	When presented with a sentence, a parser should, for each predicate in the sentence, identify and label the predicates semantic arguments." ></td>
	<td class="line x" title="13:232	In recent work, a number of researchers have cast this problem as a tagging problem and have applied various supervised machine learning techniques to it." ></td>
	<td class="line x" title="14:232	On the Wall Street Journal (WSJ) data, using correct syntactic parses, it is possible to achieve accuracies rivaling human interannotator agreement." ></td>
	<td class="line x" title="15:232	However, the performance gap widens when information derived from automatic syntactic parses is used." ></td>
	<td class="line x" title="16:232	So far, most of the work on SRL systems has been focused on improving the labeling performance on a test set belonging to the same genre of text as the training set." ></td>
	<td class="line x" title="17:232	Both the Treebank on which the syntactic parser is trained and the PropBank on which the SRL systems are trained represent articles from the year 1989 of the WSJ." ></td>
	<td class="line x" title="18:232	While all these systems perform quite well on the WSJ test data, they show significant performance degradation (approximately 10 point drop in F-score) when applied to label test data that is different than the genre that WSJ represents (Pradhan et al. , 2004; Carreras and M`arquez, 2005)." ></td>
	<td class="line x" title="19:232	556 Surprisingly, it does not matter much whether the data is from another newswire, or a completely different type of text  as in the Brown corpus." ></td>
	<td class="line x" title="20:232	These results indicate that the systems are being over-fit to the specific genre of text." ></td>
	<td class="line x" title="21:232	Many performance improvements on the WSJ PropBank corpus may reflect tuning to the corpus." ></td>
	<td class="line x" title="22:232	For the technology to be widely accepted and useful, it must be robust to change in genre of the data." ></td>
	<td class="line x" title="23:232	Until recently, data tagged with similar semantic argument structure was not available for multiple genres of text." ></td>
	<td class="line x" title="24:232	Recently, Palmer et al. , (2005), have PropBanked a significant portion of the Treebanked Brown corpus which enables us to perform experiments to analyze the reasons behind the performance degradation, and suggest potential solutions." ></td>
	<td class="line x" title="25:232	2 Semantic Annotation and Corpora In the PropBank1 corpus (Palmer et al. , 2005), predicate argument relations are marked for the verbs in the text." ></td>
	<td class="line x" title="26:232	PropBank was constructed by assigning semantic arguments to constituents of the handcorrected Treebank parses." ></td>
	<td class="line x" title="27:232	The arguments of a verb are labeled ARG0 to ARG5, where ARG0 is the PROTO-AGENT (usually the subject of a transitive verb) ARG1 is the PROTO-PATIENT (usually its direct object), etc. In addition to these CORE ARGUMENTS, 16 additional ADJUNCTIVE ARGUMENTS, referred to as ARGMs are also marked." ></td>
	<td class="line x" title="28:232	More recently the PropBanking effort has been extended to encompass multiple corpora." ></td>
	<td class="line x" title="29:232	In this study we use PropBanked versions of the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al. , 1994) and part of the Brown portion of the Penn Treebank." ></td>
	<td class="line x" title="30:232	The WSJ PropBank data comprise 24 sections of the WSJ, each section representing about 100 documents." ></td>
	<td class="line x" title="31:232	PropBank release 1.0 contains about 114,000 predicates instantiating about 250,000 arguments and covering about 3,200 verb lemmas." ></td>
	<td class="line x" title="32:232	Section 23, which is a standard test set and a test set in some of our experiments, comprises 5,400 predicates instantiating about 12,000 arguments." ></td>
	<td class="line x" title="33:232	The Brown corpus is a Standard Corpus of American English that consists of about one million words of English text printed in the calendar year 1961 1http://www.cis.upenn.edu/ace/ (Kucera and Francis, 1967)." ></td>
	<td class="line x" title="34:232	The corpus contains about 500 samples of 2000+ words each." ></td>
	<td class="line x" title="35:232	The idea behind creating this corpus was to create a heterogeneous sample of English text so that it would be useful for comparative language studies." ></td>
	<td class="line x" title="36:232	The Release 3 of the Penn Treebank contains the hand parsed syntactic trees of a subset of the Brown Corpus  sections F, G, K, L, M, N, P and R. Palmer et al. , (2005) have recently PropBanked a significant portion of this Treebanked Brown corpus." ></td>
	<td class="line x" title="37:232	In all, about 17,500 predicates are tagged with their semantic arguments." ></td>
	<td class="line x" title="38:232	For these experiments we used a limited release of PropBank dated September 2005." ></td>
	<td class="line x" title="39:232	A small portion of the predicates  about 8,000 have also been tagged with frame sense information." ></td>
	<td class="line x" title="40:232	3 SRL System Description We formulate the labeling task as a classification problem as initiated by Gildea and Jurafsky (2002) and use Support Vector Machine (SVM) classifiers (2005)." ></td>
	<td class="line x" title="41:232	We use TinySVM2 along with YamCha3 (Kudo and Matsumoto, 2000) (Kudo and Matsumoto, 2001) as the SVM training and classification software." ></td>
	<td class="line x" title="42:232	The system uses a polynomial kernel with degree 2; the cost per unit violation of the margin, C=1; and, tolerance of the termination criterion, e=0.001." ></td>
	<td class="line x" title="43:232	More details of this system can be found in Pradhan et al. , (2005)." ></td>
	<td class="line x" title="44:232	The performance of this system on section 23 of the WSJ when trained on sections 02-21 is shown in Table 1 ALL ARGs Task P R F A (%) (%) (%) TREEBANK Id. 97.5 96.1 96.8 Class." ></td>
	<td class="line x" title="45:232	93.0 Id. + Class." ></td>
	<td class="line x" title="46:232	91.8 90.5 91.2 AUTOMATIC Id. 86.9 84.2 85.5 Class." ></td>
	<td class="line x" title="47:232	92.0 Id. + Class." ></td>
	<td class="line x" title="48:232	82.1 77.9 79.9 Table 1: Performance of the SRL system on WSJ The performance of the SRL system is reported on three different tasks, all of which are with respect to a particular predicate: i) argument identification (ID), is the task of identifying the set of words (here, parse constituents) that represent a semantic role; ii) argument classification (Class.), is the task of classifying parse constituents known to represent some 2http://cl.aist-nara.ac.jp/talus-Au/software/TinySVM/ 3http://cl.aist-nara.ac.jp/taku-Au/software/yamcha/ 557 semantic role into one of the many semantic role types; and iii) argument identification and classification (ID + Class.), which involves both the identification of the parse constituents that represent semantic roles of the predicate and their classification into the respective semantic roles." ></td>
	<td class="line x" title="49:232	As usual, argument classification is measured as percent accuracy (A), whereas ID and ID + Class." ></td>
	<td class="line x" title="50:232	are measured in terms of precision (P), recall (R) and F-score (F)  the harmonic mean of P and R. The first three rows of Table 1 report performance for the system that uses hand-corrected Treebank parses, and the next three report performance for the SRL system that uses automatically generated  Charniak parser  parses, both during training and testing." ></td>
	<td class="line x" title="51:232	4 Robustness Experiments This section describes experiments that we performed using the PropBanked Brown corpus in an attempt to analyze the factors affecting the portability of SRL systems." ></td>
	<td class="line x" title="52:232	4.1 How does the SRL system trained on WSJ perform on Brown?" ></td>
	<td class="line x" title="53:232	In order to test the robustness of the SRL system, we used a system trained on the PropBanked WSJ corpus to label data from the Brown corpus." ></td>
	<td class="line x" title="54:232	We use the entire PropBanked Brown corpus (about 17,500 predicates) as a test set for this experiment and use the SRL system trained on WSJ sections 02-21 to tag its arguments." ></td>
	<td class="line x" title="55:232	Table 2 shows the performance for training and testing on WSJ, and for training on WSJ and testing on Brown." ></td>
	<td class="line x" title="56:232	There is a significant reduction in performance when the system trained on WSJ is used to label data from the Brown corpus." ></td>
	<td class="line x" title="57:232	The degradation in the Identification task is small compared to that of the combined Identification and Classification task." ></td>
	<td class="line x" title="58:232	A number of factors could be responsible for the loss of performance." ></td>
	<td class="line x" title="59:232	It is possible that the SRL models are tuned to the particular vocabulary and sense structure associated with the training data." ></td>
	<td class="line x" title="60:232	Also, since the syntactic parser that is used for generating the syntax parse trees (Charniak) is heavily lexicalized and is trained on WSJ, it could have decreased accuracy on the Brown data resulting in reduced accuracy for Semantic Role Labeling." ></td>
	<td class="line x" title="61:232	Since the SRL algorithm walks the syntax tree classifying each node, if no constituent node is present that corresponds to the correct argument, the system cannot produce a correct labeling for the argument." ></td>
	<td class="line x" title="62:232	Train Test Id. Id. + Class F F WSJ WSJ 85.5 79.9 WSJ Brown 82.4 65.1 Table 2: Performance of the SRL system on Brown." ></td>
	<td class="line x" title="63:232	In order to check the extent to which constituent nodes representing semantic arguments were deleted from the syntax tree due to parser error, we generated the performance numbers which are shown in Table 3." ></td>
	<td class="line x" title="64:232	These numbers are for top one parse for the Charniak parser, and represent not all parser errors, but deletion of argument bearing constituent nodes." ></td>
	<td class="line x" title="65:232	Total Misses % PropBank 12000 800 6.7 Brown 45880 3692 8.1 Table 3: Constituent deletions in WSJ and Brown." ></td>
	<td class="line x" title="66:232	The parser misses 6.7% of the argument-bearing nodes in the PropBank test set and about 8.1% in the Brown corpus." ></td>
	<td class="line x" title="67:232	This indicates that the errors in syntactic parsing account for a fairly small amount of the argument deletions and probably do not contributing significantly to the increased SRL error rate." ></td>
	<td class="line x" title="68:232	Obviously, just the presence of a argumentbearing constituent does not necessarily guarantee the correctness of the structural connections between itself and the predicate." ></td>
	<td class="line x" title="69:232	4.2 Identification vs Classification Performance Different features tend to dominate in the identification task vs the classification task." ></td>
	<td class="line x" title="70:232	For example, the path feature (representing the path in the syntax tree from the argument to the predicate) is the single most salient feature for the ID task and is not very important in the classification task." ></td>
	<td class="line x" title="71:232	In the next experiment we look at cross genre performance of the ID and Classification tasks." ></td>
	<td class="line x" title="72:232	We used gold standard syntactic trees from the Treebank so there are no errors in generating the syntactic structure." ></td>
	<td class="line x" title="73:232	In addition to training on the WSJ and testing on WSJ and Brown, we trained the SRL system on a Brown training set and tested it on a test set also from the Brown corpus." ></td>
	<td class="line x" title="74:232	In generating the Brown training and 558 SRL SRL Task P R F A Train Test (%) (%) (%) WSJ WSJ Id. 97.5 96.1 96.8 (104k) (5k) Class." ></td>
	<td class="line x" title="75:232	93.0 Id. + Class." ></td>
	<td class="line x" title="76:232	91.8 90.5 91.2 WSJ WSJ Id. 96.3 94.4 95.3 (14k) (5k) Class." ></td>
	<td class="line x" title="77:232	86.1 Id. + Class." ></td>
	<td class="line x" title="78:232	84.4 79.8 82.0 BROWN BROWN Id. 95.7 94.9 95.2 (14k) (1.6k) Class." ></td>
	<td class="line x" title="79:232	80.1 Id. + Class." ></td>
	<td class="line x" title="80:232	79.9 77.0 78.4 WSJ BROWN Id. 94.2 91.4 92.7 (14k) (1.6k) Class." ></td>
	<td class="line x" title="81:232	72.0 Id. + Class." ></td>
	<td class="line x" title="82:232	71.8 65.8 68.6 Table 4: Performance of the SRL system using correct Treebank parses." ></td>
	<td class="line x" title="83:232	test sets, we used stratified sampling, which is often used by the syntactic parsing community (Gildea, 2001)." ></td>
	<td class="line x" title="84:232	The test set was generated by selecting every 10th sentence in the Brown Corpus." ></td>
	<td class="line x" title="85:232	We also held out the development set used by Bacchiani et al. , (2006) to tune system parameters in the future." ></td>
	<td class="line x" title="86:232	This procedure resulted in a training set of approximately 14,000 predicates and a test set of about 1600 predicates." ></td>
	<td class="line x" title="87:232	We did not perform any parameter tuning for any of the following experiments, and used the parameter settings from the best performing version of the SRL system as reported in Table1." ></td>
	<td class="line x" title="88:232	We compare the performance on this test set with that obtained when the SRL system is trained using WSJ sections 02-21 and use section 23 for testing." ></td>
	<td class="line x" title="89:232	For a more balanced comparison, we retrained the SRL system on the same amount of data as used for training on Brown, and tested it on section 23." ></td>
	<td class="line x" title="90:232	As usual, trace information, and function tag information from the Treebank is stripped out." ></td>
	<td class="line x" title="91:232	Table 4 shows the results." ></td>
	<td class="line x" title="92:232	There is a fairly small difference in argument Identification performance when the SRL system is trained on 14,000 predicates vs 104,000 predicates from the WSJ (F-score 95.3 vs 96.8)." ></td>
	<td class="line x" title="93:232	However, there is a considerable drop in Classification accuracy (86.1% vs 93.0%)." ></td>
	<td class="line x" title="94:232	When the SRL system is trained and tested on Brown data, the argument Identification performance is not significantly different than that for the system trained and tested on WSJ data (F-score 95.2 vs 95.3)." ></td>
	<td class="line x" title="95:232	The drop in argument Classification accuracy is much more severe (86.1% vs 80.1%)." ></td>
	<td class="line x" title="96:232	This same trend between ID and Classification is even more pronounced when training on WSJ and testing on Brown." ></td>
	<td class="line x" title="97:232	For a system trained on WSJ, there is a fairly small drop in performance of the ID task when tested on Brown vs tested on WSJ (Fscore 92.7 vs 95.3)." ></td>
	<td class="line x" title="98:232	However, in this same condition, the Classification task has a very large drop in performance (72.0% vs 86.1%)." ></td>
	<td class="line x" title="99:232	So argument ID is not very sensitive to amount of training data in a corpus, or to the genre of the corpus, and ports well from WSJ to Brown." ></td>
	<td class="line x" title="100:232	This experiment supports the belief that there is no significant drop in the task of identifying the right syntactic constituents that are arguments  and this is intuitive since previous experiments have shown that the task of argument identification is more dependent on the structural features  one such feature being the path in the syntax tree." ></td>
	<td class="line x" title="101:232	Argument Classification seems to be the problem." ></td>
	<td class="line x" title="102:232	It requires more training data within the WSJ corpus, does not perform as well when trained and tested on Brown as it does for WSJ and does not port well from WSJ to Brown." ></td>
	<td class="line x" title="103:232	This suggests that the features it uses are being over-fit to the training data and are more idiosyncratic to a given dataset." ></td>
	<td class="line x" title="104:232	In particular, the predicate whose arguments are being identified, and the head word of the syntactic constituent being classified are both important features in the task of argument classification." ></td>
	<td class="line x" title="105:232	As a generalization, the features used by the Identification task reflect structure and port well." ></td>
	<td class="line x" title="106:232	The features used by the Classification task reflect specific lexical usage and semantics, and tend to require more training data and are more subject to overfitting." ></td>
	<td class="line x" title="107:232	Even when training and testing on Brown, Classification accuracy is considerably worse than 559 training and testing on WSJ (with comparable training set size)." ></td>
	<td class="line x" title="108:232	It is probably the case that the predicates and head words in a homogeneous corpus such as the WSJ are used more consistently, and tend to have single dominant word senses." ></td>
	<td class="line x" title="109:232	The Brown corpus probably has much more variety in its lexical usage and word senses." ></td>
	<td class="line x" title="110:232	4.3 How sensitive is semantic argument prediction to the syntactic correctness across genre?" ></td>
	<td class="line x" title="111:232	This experiment examines the same cross-genre effects as the last experiment, but uses automatically generated syntactic parses rather than gold standard ones." ></td>
	<td class="line x" title="112:232	For this experiment, we used the same amount of training data from WSJ as available in the Brown training set  that is about 14,000 predicates." ></td>
	<td class="line x" title="113:232	The examples from WSJ were selected randomly." ></td>
	<td class="line x" title="114:232	The Brown test set is the same as used in the previous experiment, and the WSJ test set is the entire section 23." ></td>
	<td class="line pc" title="115:232	Recently there have been some improvements to the Charniak parser, use n-best re-ranking as reported in (Charniak and Johnson, 2005) and selftraining and re-ranking using data from the North American News corpus (NANC) and adapts much better to the Brown corpus (McClosky et al. , 2006a; McClosky et al. , 2006b)." ></td>
	<td class="line x" title="116:232	The performance of these parsers as reported in the respective literature are shown in Table 6 shows the performance (as reported in the literature) of the Charniak parser: when trained and tested on WSJ, when trained on WSJ and tested on Brown, When trained and tested on Brown, and when trained on WSJ and adapted with NANC." ></td>
	<td class="line x" title="117:232	Train Test F WSJ WSJ 91.0 WSJ Brown 85.2 Brown Brown 88.4 WSJ+NANC Brown 87.9 Table 6: Charniak parser performance." ></td>
	<td class="line x" title="118:232	We describe the results of Semantic Role Labeling under the following five conditions: 1." ></td>
	<td class="line x" title="119:232	The SRL system is trained on features extracted from automatically generated parses of the PropBanked WSJ sentences." ></td>
	<td class="line x" title="120:232	The syntactic parser  Charniak parser  is itself trained on the WSJ training sections of the Treebank." ></td>
	<td class="line x" title="121:232	This is used for Semantic Role Labeling of section23 of WSJ." ></td>
	<td class="line x" title="122:232	2." ></td>
	<td class="line x" title="123:232	The SRL system is trained on features extracted from automatically generated parses of the PropBanked WSJ sentences." ></td>
	<td class="line x" title="124:232	The syntactic parser  Charniak parser  is itself trained on the WSJ training sections of the Treebank." ></td>
	<td class="line x" title="125:232	This is used to classify the Brown test set." ></td>
	<td class="line x" title="126:232	3." ></td>
	<td class="line x" title="127:232	The SRL system is trained on features extracted from automatically generated parses of the PropBanked Brown corpus sentences." ></td>
	<td class="line x" title="128:232	The syntactic parser is trained using the WSJ portion of the Treebank." ></td>
	<td class="line x" title="129:232	This is used to classify the Brown test set." ></td>
	<td class="line x" title="130:232	4." ></td>
	<td class="line x" title="131:232	The SRL system is trained on features extracted from automatically generated parses of the PropBanked Brown corpus sentences." ></td>
	<td class="line x" title="132:232	The syntactic parser is trained using the Brown training portion of the Treebank." ></td>
	<td class="line x" title="133:232	This is used to classify the Brown test set." ></td>
	<td class="line x" title="134:232	5." ></td>
	<td class="line x" title="135:232	The SRL system is trained on features extracted from automatically generated parses of the PropBanked Brown corpus sentences." ></td>
	<td class="line oc" title="136:232	The syntactic parser is the version that is selftrained using 2,500,000 sentences from NANC, and where the starting version is trained only on WSJ data (McClosky et al. , 2006b)." ></td>
	<td class="line x" title="137:232	This is used to classify the Brown test set." ></td>
	<td class="line x" title="138:232	Table 5 shows the results." ></td>
	<td class="line x" title="139:232	For simplicity of discussion we have tagged the five conditions as 1., 2., 3., 4., and 5." ></td>
	<td class="line x" title="144:232	Comparing conditions 2." ></td>
	<td class="line x" title="145:232	and 3." ></td>
	<td class="line x" title="146:232	shows that when the features used to train the SRL system are extracted using a syntactic parser that is trained on WSJ it performs at almost the same level on the task of Identification, regardless of whether it is trained on the PropBanked Brown corpus or the PropBanked WSJ corpus." ></td>
	<td class="line x" title="147:232	This, however, is significantly lower than when all the three  the syntactic parser training set, the SRL system training set, and the SRL system test set, are from the same genre (6 F-score points lower than condition 1, and 5 points lower than conditions 4 and 5)." ></td>
	<td class="line x" title="148:232	In case of the combined task, the gap between the performance for conditions 2 and 3 is about 10 points in F-score (59.1 vs 69.8)." ></td>
	<td class="line x" title="149:232	Looking at the argument classification accuracies, we see that using the SRL system 560 Setup Parser SRL SRL Task P R F A Train Train Test (%) (%) (%) 1." ></td>
	<td class="line x" title="150:232	WSJ WSJ WSJ Id. 87.3 84.8 86.0 (40k  sec:00-21) (14k) (5k) Class." ></td>
	<td class="line x" title="151:232	84.1 Id. + Class." ></td>
	<td class="line x" title="152:232	77.5 69.7 73.4 2." ></td>
	<td class="line x" title="153:232	WSJ WSJ Brown Id. 81.7 78.3 79.9 (40k  sec:00-21) (14k) (1.6k) Class." ></td>
	<td class="line x" title="154:232	72.1 Id. + Class." ></td>
	<td class="line x" title="155:232	63.7 55.1 59.1 3." ></td>
	<td class="line x" title="156:232	WSJ Brown Brown Id. 81.7 78.3 80.0 (40k  sec:00-21) (14k) (1.6k) Class." ></td>
	<td class="line x" title="157:232	79.2 Id. + Class." ></td>
	<td class="line x" title="158:232	78.2 63.2 69.8 4." ></td>
	<td class="line x" title="159:232	Brown Brown Brown Id. 87.6 82.3 84.8 (20k) (14k) (1.6k) Class." ></td>
	<td class="line x" title="160:232	78.9 Id. + Class." ></td>
	<td class="line x" title="161:232	77.4 62.1 68.9 5." ></td>
	<td class="line x" title="162:232	WSJ+NANC Brown Brown Id. 87.7 82.5 85.0 (2,500k) (14k) (1.6k) Class." ></td>
	<td class="line x" title="163:232	79.9 Id. + Class." ></td>
	<td class="line x" title="164:232	77.2 64.4 70.0 Table 5: Performance on WSJ and Brown using automatic syntactic parses trained on WSJ to test Brown sentences give a 12 point drop in F-score (84.1 vs 72.1)." ></td>
	<td class="line x" title="165:232	Using the SRL system trained on Brown using WSJ trained syntactic parser shows a drop in accuracy by about 5 Fscore points (84.1 to 79.2)." ></td>
	<td class="line x" title="166:232	When the SRL system is trained on Brown using syntactic parser also trained on Brown, we get a quite similar classification performance, which is again about 5 points lower than what we get using all WSJ data." ></td>
	<td class="line x" title="167:232	This shows lexical semantic features might be very important to get a better argument classification on Brown corpus." ></td>
	<td class="line x" title="168:232	4.4 How much data is required to adapt to a new genre?" ></td>
	<td class="line x" title="169:232	We would like to know how much data from a new genre we need to annotate and add to the training data of an existing corpus to adapt the system such that it gives the same level of performance as when it is trained on the new genre." ></td>
	<td class="line x" title="170:232	One section of the Brown corpus  section CK has about 8,200 predicates annotated." ></td>
	<td class="line x" title="171:232	We use six different conditions  two in which we use correct Treebank parses, and the four others in which we use automatically generated parses using the variations described before." ></td>
	<td class="line x" title="172:232	All training sets start with the same number of examples as in the Brown training set." ></td>
	<td class="line x" title="173:232	The part of this section used as a test set for the CoNLL 2005 shared task is used as the test set here." ></td>
	<td class="line x" title="174:232	It contains a total of about 800 predicates." ></td>
	<td class="line x" title="175:232	Table 7 shows a comparison of these conditions." ></td>
	<td class="line x" title="176:232	In all the six conditions, the performance on the task of Identification and Classification improves gradually until about 5625 examples of section CK which is about 75% of the total added, above which they improve very little." ></td>
	<td class="line x" title="177:232	In fact, even 50% of the new data accounts for 90% of the performance difference." ></td>
	<td class="line x" title="178:232	Even when the syntactic parser is trained on WSJ and the SRL is trained on WSJ, adding 7,500 instances of the new genres allows it to achieve almost the same performance as when all three are from the same genre (67.2 vs 69.9)." ></td>
	<td class="line x" title="179:232	Numbers for argument identification arent shown because adding more data does not have any statistically significant impact on its performance." ></td>
	<td class="line x" title="180:232	The system that uses self-trained syntactic parser seems to perform slightly better than the rest of the versions that use automatically generated syntactic parses." ></td>
	<td class="line x" title="181:232	The precision numbers are almost unaffected  except when the labeler is trained on WSJ PropBank data." ></td>
	<td class="line x" title="182:232	4.5 How much does verb sense information contribute?" ></td>
	<td class="line x" title="183:232	In order to find out how important the verb sense information is in the process of genre transfer, we used the subset of PropBanked Brown corpus that was tagged with verb sense information, ran an experiment similar to that of Experiment 1." ></td>
	<td class="line x" title="184:232	We used the oracle sense information and correct syntactic information for this experiment." ></td>
	<td class="line x" title="185:232	Table 8 shows the results of this experiment." ></td>
	<td class="line x" title="186:232	There is about 1 point F-score increase on using oracle sense information on the overall data." ></td>
	<td class="line x" title="187:232	We looked at predicates that had high perplexity in both the training and test sets, and whose sense distribu561 Parser SRL Id. + Class Parser SRL Id. + Class P R F P R F Train Train (%) (%) (%) (%) WSJ WSJ (14k) WSJ Brown (14k) (Treebank parses) (Treebank parses) +0 ex." ></td>
	<td class="line x" title="188:232	from CK 74.1 66.5 70.1 (40k) +0 ex." ></td>
	<td class="line x" title="189:232	from CK 74.4 57.0 64.5 +1875 ex." ></td>
	<td class="line x" title="190:232	from CK 77.6 71.3 74.3 +1875 ex." ></td>
	<td class="line x" title="191:232	from CK 75.1 58.7 65.9 +3750 ex." ></td>
	<td class="line x" title="192:232	from CK 79.1 74.1 76.5 +3750 ex." ></td>
	<td class="line x" title="193:232	from CK 76.1 59.6 66.9 +5625 ex." ></td>
	<td class="line x" title="194:232	from CK 80.4 76.1 78.1 +5625 ex." ></td>
	<td class="line x" title="195:232	from CK 76.9 60.5 67.7 +7500 ex." ></td>
	<td class="line x" title="196:232	from CK 80.2 76.1 78.1 +7500 ex." ></td>
	<td class="line x" title="197:232	from CK 76.8 59.8 67.2 Brown Brown (14k) Brown Brown (14k) (Treebank parses) (Treebank parses) +0 ex." ></td>
	<td class="line x" title="198:232	from CK 77.1 73.0 75.0 (20k) +0 ex." ></td>
	<td class="line x" title="199:232	from CK 76.0 59.2 66.5 +1875 ex." ></td>
	<td class="line x" title="200:232	from CK 78.8 75.1 76.9 +1875 ex." ></td>
	<td class="line x" title="201:232	from CK 76.1 60.0 67.1 +3750 ex." ></td>
	<td class="line x" title="202:232	from CK 80.4 76.9 78.6 +3750 ex." ></td>
	<td class="line x" title="203:232	from CK 77.7 62.4 69.2 +5625 ex." ></td>
	<td class="line x" title="204:232	from CK 80.4 77.2 78.7 +5625 ex." ></td>
	<td class="line x" title="205:232	from CK 78.2 63.5 70.1 +7500 ex." ></td>
	<td class="line x" title="206:232	from CK 81.2 78.1 79.6 +7500 ex." ></td>
	<td class="line x" title="207:232	from CK 78.2 63.2 69.9 WSJ WSJ (14k) WSJ+NANC Brown (14k) (40k) +0 ex." ></td>
	<td class="line x" title="208:232	from CK 65.2 55.7 60.1 (2,500k) +0 ex." ></td>
	<td class="line x" title="209:232	from CK 74.4 60.1 66.5 +1875 ex." ></td>
	<td class="line x" title="210:232	from CK 68.9 57.5 62.7 +1875 ex." ></td>
	<td class="line x" title="211:232	from CK 76.2 62.3 68.5 +3750 ex." ></td>
	<td class="line x" title="212:232	from CK 71.8 59.3 64.9 +3750 ex." ></td>
	<td class="line x" title="213:232	from CK 76.8 63.6 69.6 +5625 ex." ></td>
	<td class="line x" title="214:232	from CK 74.3 61.3 67.2 +5625 ex." ></td>
	<td class="line x" title="215:232	from CK 77.7 63.8 70.0 +7500 ex." ></td>
	<td class="line x" title="216:232	from CK 74.8 61.0 67.2 +7500 ex." ></td>
	<td class="line x" title="217:232	from CK 78.2 64.9 70.9 Table 7: Effect of incrementally adding data from a new genre Train Test Without Sense With Sense Id. Id. F F WSJ Brown (All) 69.1 69.9 WSJ Brown (predicate: go) 46.9 48.9 Table 8: Influence of verb sense feature." ></td>
	<td class="line x" title="218:232	tion was different." ></td>
	<td class="line x" title="219:232	One such predicate is go." ></td>
	<td class="line x" title="220:232	The improvement on classifying the arguments of this predicate was about 2 points (46.9 to 48.9), which suggests that verb sense is more important when the sense structure of the test corpus is more ambiguous and is different from the training." ></td>
	<td class="line x" title="221:232	Here we used oracle verb sense information, but one can train a classifier as done by Girju et al. , (2005) which achieves a disambiguation accuracy in the 80s for within the WSJ corpus." ></td>
	<td class="line x" title="222:232	5 Conclusions Our experimental results on robustness to change in genre can be summarized as follows:  There is a significant drop in performance when training and testing on different corpora  for both Treebank and Charniak parses  In this process the classification task is more disrupted than the identification task." ></td>
	<td class="line x" title="223:232	 There is a performance drop in classification even when training and testing on Brown (compared to training and testing on WSJ)  The syntactic parser error is not a large part of the degradation for the case of automatically generated parses." ></td>
	<td class="line x" title="224:232	An error analysis leads us to believe that some reasons for this behavior could be: i) lexical usages that are specific to WSJ, ii) variation in subcategorization across corpora, iii) variation in word sense distribution and iv) changes in topics and entities." ></td>
	<td class="line x" title="225:232	Training and testing on the same corpora tends to give a high weight to very specific semantic features." ></td>
	<td class="line x" title="226:232	Two possibilities remedies could be: i) using less homogeneous corpora and ii) less specific features, for eg., proper names are replaced with the name entities that they represent." ></td>
	<td class="line x" title="228:232	This way the system could be forced to use the more general features." ></td>
	<td class="line x" title="229:232	Both of these manipulations would most likely reduce performance on the training set, and on test sets of the same genre as the training data." ></td>
	<td class="line x" title="230:232	But they would be likely to generalize better." ></td>
	<td class="line xc" title="231:232	6 Acknowledgments We are extremely grateful to Martha Palmer for providing us with the PropBanked Brown corpus, and to David McClosky for providing us with hypotheses on the Brown test set as well as a cross-validated 562 version of the Brown training data for the various models reported in his work reported at HLT 2006." ></td>
	<td class="line x" title="232:232	This research was partially supported by the ARDA AQUAINT program via contract OCG4423B and by the NSF via grants IS-9978025 and ITR/HCI 0086132." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N07-2045
Language Modeling for Determiner Selection
Turner, Jenine;Charniak, Eugene;"></td>
	<td class="line x" title="1:82	Proceedings of NAACL HLT 2007, Companion Volume, pages 177180, Rochester, NY, April 2007." ></td>
	<td class="line x" title="2:82	c2007 Association for Computational Linguistics Language Modeling for Determiner Selection Jenine Turner and Eugene Charniak Department of Computer Science Brown Laboratory for Linguistic Information Processing (BLLIP) Brown University Providence, RI 02912 {jenine|ec}@cs.brown.edu Abstract We present a method for automatic determiner selection, based on an existing language model." ></td>
	<td class="line x" title="3:82	We train on the Penn Treebank and also use additional data from the North American News Text Corpus." ></td>
	<td class="line x" title="4:82	Our results are a significant improvement over previous best." ></td>
	<td class="line x" title="5:82	1 Introduction Determiner placement (choosing if a noun phrase needs a determiner, and if so, which one) is a non-trivial problem in several language processing tasks." ></td>
	<td class="line x" title="6:82	While context beyond that of the current sentence can sometimes be necessary, native speakers of languages with determiners can select determiners quite well for most NPs." ></td>
	<td class="line x" title="7:82	Native speakers of languages without determiners have a much more difficult time." ></td>
	<td class="line x" title="8:82	Automating determiner selection is helpful in several applications." ></td>
	<td class="line x" title="9:82	A determiner selection program can aid in Machine Translation of determiner-free languages (by adding determiners after the text has been translated), correct English text written by nonnative speakers (Lee, 2004), and choose determiners for text generation programs." ></td>
	<td class="line x" title="10:82	Early work on determiner selection focuses on rule-based systems (Gawronska, 1990; Murata and Nagao, 1993; Bond and Ogura, 1994; Heine, 1998)." ></td>
	<td class="line x" title="11:82	Knight and Chander (1994) use decision trees to choose between the and a/an, ignoring NPs with no determiner, and achieve 78% accuracy on their Wall Street Journal corpus." ></td>
	<td class="line x" title="12:82	(Deciding between a and an is a trivial postprocessing step.)" ></td>
	<td class="line x" title="13:82	Minnen et al.(2000) use a memory-based learner (Daelemans et al. , 2000) to choose determiners of base noun phrases." ></td>
	<td class="line x" title="15:82	They choose between no determiner (hencefore null), the, and a/an." ></td>
	<td class="line x" title="16:82	They use syntactic features (head of the NP, part-of-speech tag of the head of the NP, functional tag of the head of the NP, category of the constituent embedding the NP, and functional tag of the constituent embedding the NP), whether the head is a mass or count noun and semantic classes of the head of the NP (Ikehara et al. , 1991)." ></td>
	<td class="line x" title="17:82	They report 83.58% accuracy." ></td>
	<td class="line x" title="18:82	In this paper, we use the Charniak language model (Charniak, 2001) for determiner selection." ></td>
	<td class="line x" title="19:82	Our approach significantly improves upon the work of Minnen et al.(2000)." ></td>
	<td class="line x" title="21:82	We also use additional automatically parsed data from the North American News Text Corpus (Graff, 1995), further improving our results." ></td>
	<td class="line x" title="22:82	2 The Immediate-Head Parsing Model The language model we use is described in (Charniak, 2001)." ></td>
	<td class="line x" title="23:82	It is based upon a parser that, for a sentence s, tries to find the parse pi defined as: argmaxpip(pi | s) = argmaxpip(pi,s) (1) The parser can be turned into a language model p(s) describing the probability distribution over all possible strings s in the language, by considering all parses pi of s: p(s) = summationdisplay pi p(pi,s) (2) 177 Here p(pi,s) is zero if the yield of pi negationslash= s. The parsing model assigns a probability to a parse pi by a top-down process." ></td>
	<td class="line x" title="24:82	For each constituent c in pi it first guesses the pre-terminal of c, t(c) (t for tag), then the lexical head of c, h(c), and then the expansion of c into further constituents e(c)." ></td>
	<td class="line x" title="25:82	Thus the probability of a parse is given by the equation p(pi) = productdisplay cpi p(t(c) | l(c),H(c))  p(h(c) | t(c),l(c),H(c))  p(e(c) | l(c),t(c),h(c),H(c)) where l(c) is the label of c (e.g. , whether it is a noun phrase NP, verb phrase, etc)." ></td>
	<td class="line x" title="26:82	and H(c) is the relevant history of c  information outside c deemed important in determining the probability in question." ></td>
	<td class="line x" title="27:82	H(c) approximately consists of the label, head, and head-part-of-speech for the parent of c: m(c),i(c), and u(c) respectively and also a secondary head (e.g. , in Monday Night Football Monday would be conditioned on both the head of the noun-phrase Football and the secondary head Night)." ></td>
	<td class="line x" title="28:82	It is usually clear to which constituent we are referring and we omit the (c) in, e.g., h(c)." ></td>
	<td class="line x" title="29:82	In this notation the above equation takes the following form: p(pi) = productdisplay cpi p(t | l,m,u,i) p(h | t,l,m,u,i)  p(e | l,t,h,m,u)." ></td>
	<td class="line x" title="30:82	(3) Next we describe how we assign a probability to the expansion e of a constituent." ></td>
	<td class="line x" title="31:82	We break up a traditional probabilistic context-free grammar (PCFG) rule into a left-hand side with a label l(c) drawn from the non-terminal symbols of our grammar, and a right-hand side that is a sequence of one or more such symbols." ></td>
	<td class="line x" title="32:82	For each expansion we distinguish one of the right-hand side labels as the middle or head symbol M(c)." ></td>
	<td class="line x" title="33:82	M(c) is the constituent from which the head lexical item h is obtained according to deterministic rules that pick the head of a constituent from among the heads of its children." ></td>
	<td class="line x" title="34:82	To the left of M is a sequence of one or more left labels Li(c) including the special termination symbol , which indicates that there are no more symbols to the left." ></td>
	<td class="line x" title="35:82	We do the same for the labels to the right, Ri(c)." ></td>
	<td class="line x" title="36:82	Thus, an expansion e(c) looks like: l  LmL1MR1Rn." ></td>
	<td class="line x" title="37:82	(4) The expansion is generated first by guessing M, then in order L1 through Lm+1 (= ), and then, R1 through Rn+1." ></td>
	<td class="line x" title="38:82	Let us turn to how this works in the case of determiner recovery." ></td>
	<td class="line x" title="39:82	Consider a noun-phrase, which, missing a possible determiner, is simply FBI. The language model is interested in the probability of the strings the FBI, a/an FBI and FBI. The version with the highest probability will dictate the determiner, or lack thereof." ></td>
	<td class="line x" title="40:82	So, consider (most of) the probability calculation for the answer the FBI: p(NNP | H) p(FBI | NNP,H)  p(det | FBI,NNP,H)  p( | det,FBI,NNP,H)  p(the | det,FBI,NNP,H) (5) Of these, the first two terms, the probability that the head will be an NNP (a singular proper noun) and the probability that it will be FBI, are shared by all three competitors, null, the, and a/an." ></td>
	<td class="line x" title="41:82	These terms can therefore be ignored when we only wish to identify the competitor with the highest probability." ></td>
	<td class="line x" title="42:82	The next two probabilities state that the noun-phrase contains a determiner to the left of FBI and that the determiner is the last constituent of the left-hand side." ></td>
	<td class="line x" title="43:82	The last of the probabilities states that the determiner in question is the." ></td>
	<td class="line x" title="44:82	Ignoring the first two probabilities, the critical probabilities for the FBI are: p(det | FBI,NNP,H)  p( | det,FBI,NNP,H)  p(the | det,FBI,NNP,H) (6) Conversely, to evaluate the probability of the nounphrase FBI  i.e., no determiner, we evaluate: p( | FBI,NNP,H) (7) We ask the probability of the NP stopping immediately to the left of FBI. For a/an FBI we evaluate: p(det | FBI,NNP,H)  p( | det,FBI,NNP,H) (8)  (p(a | det,FBI,NNP,H) + p(an | det,FBI,NNP,H)) 178 Test Data Method Accuracy leave-one-out Minnen et al. 83.58% Language Model (LM) 86.74% tenfold on development LM 84.72% LM trained on WSJ + 3 million words of NANC 85.83% LM trained on WSJ + 10 million words of NANC 86.36% LM trained on WSJ + 20 million words of NANC 86.64% tenfold on test LM trained on WSJ + 20 million words of NANC 86.63% Table 1: Results of classification This equation is very similar to Equation 6 (the equation for the FBI, except the term for the probability of the is replaced by the sum of the probabilities for a and an." ></td>
	<td class="line x" title="45:82	To choose between null, the, or a/an, the language model in effect constructs Equations 6, 7 and 8 and we pick the one that has the highest probability." ></td>
	<td class="line x" title="46:82	2.1 Training the model As with (Minnen et al. , 2000), we train the language model on the Penn Treebank (Marcus et al. , 1993)." ></td>
	<td class="line oc" title="47:82	As far as we know, language modeling always improves with additional training data, so we add data from the North American News Text Corpus (NANC) (Graff, 1995) automatically parsed with the Charniak parser (McClosky et al. , 2006) to train our language model on up to 20 million additional words." ></td>
	<td class="line x" title="48:82	3 Results and Discussion The best results of Minnen et al.(2000) are using leave-one-out cross-validation." ></td>
	<td class="line x" title="50:82	We also test our language model using leave-one-out cross-validation on the Penn Treebank (Marcus et al. , 1993) (WSJ), giving us 86.74% accuracy (see Table 1)." ></td>
	<td class="line x" title="51:82	Leave-one-out cross-validation does not make sense in this case." ></td>
	<td class="line x" title="52:82	When choosing determiners, we can train a language model on similar data, but not on other NPs in the article." ></td>
	<td class="line x" title="53:82	Therefore, for the rest of our tests, we use tenfold cross-validation." ></td>
	<td class="line x" title="54:82	The difference between leave-one-out and tenfold crossvalidation is due to the co-occurrence of NPs within an article." ></td>
	<td class="line x" title="55:82	Church (2000) shows that a word appears with much higher probability when seen elsewhere in an article." ></td>
	<td class="line x" title="56:82	Thus, a rare NP might be unseen in tenfold cross-validation, but seen in leave-one-out." ></td>
	<td class="line x" title="57:82	For each of our sets in tenfold cross validation, we use 80% of the Penn Treebank for training, 10% for development, and 10% for testing." ></td>
	<td class="line x" title="58:82	The divisions occur at article boundaries." ></td>
	<td class="line x" title="59:82	On our development set with tenfold cross-validation, we get 84.72% accuracy using the language model (Table 1)." ></td>
	<td class="line x" title="60:82	As expected, we achieve significant improvement when adding NANC data over training on data from the Penn Treebank alone (Table 1)." ></td>
	<td class="line x" title="61:82	With 20 million additional words, we seem to be approaching an upper bound on the language model features." ></td>
	<td class="line x" title="62:82	We obtain improvement despite the fact that the parses were automatic, but there may have been errors in determiner selection due to parsing error." ></td>
	<td class="line x" title="63:82	Table 2 gives error examples." ></td>
	<td class="line x" title="64:82	Some errors are wrong (either grammatically or yielding a significantly different interpretation), but some incorrect answers are reasonable possibilities." ></td>
	<td class="line x" title="65:82	Furthermore, even all the text of the article is not enough for classification at times." ></td>
	<td class="line x" title="66:82	In particular note Example 5, where unless you know whether IBM was the world leader or simply one of the world leaders at the time of the article, no additional context would help." ></td>
	<td class="line x" title="67:82	4 Conclusions and Future Work With the Charniak (Charniak, 2001) language model, our results exceed those of the previous best (Minnen et al. , 2000) on the determiner selection task." ></td>
	<td class="line x" title="68:82	This shows the benefits of the language model features in determining the most grammatical determiner to use in a noun phrase." ></td>
	<td class="line x" title="69:82	Such a language model looks at much of the structure in individual sentences, but there may be additional features that could improve performance." ></td>
	<td class="line x" title="70:82	There is a high rate of ambiguity for many of the misclassified sentences." ></td>
	<td class="line x" title="71:82	The success of using a state-of-the-art language 179 Guess Correct Sentence the null (1) The computers were crude by todays standards." ></td>
	<td class="line x" title="72:82	null the (2) In addition, the Apple II was an affordable $1,298." ></td>
	<td class="line x" title="73:82	(3) Highway officials insist the ornamental railings on older bridges arent strong enough to prevent vehicles from crashing through." ></td>
	<td class="line x" title="74:82	a/an the (4) The new carrier can tote as many as four cups at once." ></td>
	<td class="line x" title="75:82	(5) IBM, the world leader in computers, didnt offer its first PC until August 1981 as many other companies entered the market." ></td>
	<td class="line x" title="76:82	the a/an (6) In addition, the Apple II was an affordable $1,298." ></td>
	<td class="line x" title="77:82	(7) The primary purpose of a railing is to contain a vehicle and not to provide a scenic view, says Jack White, a planner with the Indiana Highway Department." ></td>
	<td class="line x" title="78:82	a/an null (8) Crude as they were, these early PCs triggered explosive product development in desktop models for the home and office." ></td>
	<td class="line x" title="79:82	Table 2: Examples of errors model in determiner selection also suggests that one would be helpful in making other decisions in the surface realization stage of text generation." ></td>
	<td class="line x" title="80:82	This is an avenue worth exploring." ></td>
	<td class="line x" title="81:82	Acknowledgements This work was supported by NSF PIRE grant OISE-0530118." ></td>
	<td class="line x" title="82:82	We would also like to thank the BLLIP team for their comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-1036
Guiding Semi-Supervision with Constraint-Driven Learning
Chang, Ming-Wei;Ratinov, Lev;Roth, Dan;"></td>
	<td class="line x" title="1:270	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 280287, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:270	c2007 Association for Computational Linguistics Guiding Semi-Supervision with Constraint-Driven Learning Ming-Wei Chang Lev Ratinov Dan Roth Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 {mchang21, ratinov2, danr}@uiuc.edu Abstract Over the last few years, two of the main research directions in machine learning of natural language processing have been the study of semi-supervised learning algorithms as a way to train classi ers when the labeled data is scarce, and the study of ways to exploit knowledge and global information in structured learning tasks." ></td>
	<td class="line x" title="3:270	In this paper, we suggest a method for incorporating domain knowledge in semi-supervised learning algorithms." ></td>
	<td class="line x" title="4:270	Our novel framework uni es and can exploit several kinds of task speci c constraints." ></td>
	<td class="line x" title="5:270	The experimental results presented in the information extraction domain demonstrate that applying constraints helps the model to generate better feedback during learning, and hence the framework allows for high performance learning with significantly less training data than was possible before on these tasks." ></td>
	<td class="line x" title="6:270	1 Introduction Natural Language Processing (NLP) systems typically require large amounts of knowledge to achieve good performance." ></td>
	<td class="line x" title="7:270	Acquiring labeled data is a difcult and expensive task." ></td>
	<td class="line x" title="8:270	Therefore, an increasing attention has been recently given to semi-supervised learning, where large amounts of unlabeled data are used to improve the models learned from a small training set (Collins and Singer, 1999; Thelen and Riloff, 2002)." ></td>
	<td class="line x" title="9:270	The hope is that semi-supervised or even unsupervised approaches, when given enough knowledge about the structure of the problem, will be competitive with the supervised models trained on large training sets." ></td>
	<td class="line x" title="10:270	However, in the general case, semi-supervised approaches give mixed results, and sometimes even degrade the model performance (Nigam et al. , 2000)." ></td>
	<td class="line x" title="11:270	In many cases, improving semi-supervised models was done by seeding these models with domain information taken from dictionaries or ontology (Cohen and Sarawagi, 2004; Collins and Singer, 1999; Haghighi and Klein, 2006; Thelen and Riloff, 2002)." ></td>
	<td class="line x" title="12:270	On the other hand, in the supervised setting, it has been shown that incorporating domain and problem speci c structured information can result in substantial improvements (Toutanova et al. , 2005; Roth and Yih, 2005)." ></td>
	<td class="line x" title="13:270	This paper proposes a novel constraints-based learning protocol for guiding semi-supervised learning." ></td>
	<td class="line x" title="14:270	We develop a formalism for constraints-based learning that uni es several kinds of constraints: unary, dictionary based and n-ary constraints, which encode structural information and interdependencies among possible labels." ></td>
	<td class="line x" title="15:270	One advantage of our formalism is that it allows capturing different levels of constraint violation." ></td>
	<td class="line x" title="16:270	Our protocol can be used in the presence of any learning model, including those that acquire additional statistical constraints from observed data while learning (see Section 5." ></td>
	<td class="line x" title="17:270	In the experimental part of this paper we use HMMs as the underlying model, and exhibit signi cant reduction in the number of training examples required in two information extraction problems." ></td>
	<td class="line x" title="18:270	As is often the case in semi-supervised learning, the algorithm can be viewed as a process that improves the model by generating feedback through 280 labeling unlabeled examples." ></td>
	<td class="line x" title="19:270	Our algorithm pushes this intuition further, in that the use of constraints allows us to better exploit domain information as a way to label, along with the current learned model, unlabeled examples." ></td>
	<td class="line x" title="20:270	Given a small amount of labeled data and a large unlabeled pool, our framework initializes the model with the labeled data and then repeatedly: (1) Uses constraints and the learned model to label the instances in the pool." ></td>
	<td class="line x" title="21:270	(2) Updates the model by newly labeled data." ></td>
	<td class="line x" title="22:270	This way, we can generate better training examples during the semi-supervised learning process." ></td>
	<td class="line x" title="23:270	The core of our approach, (1), is described in Section 5." ></td>
	<td class="line x" title="24:270	The task is described in Section 3 and the Experimental study in Section 6." ></td>
	<td class="line x" title="25:270	It is shown there that the improvement on the training examples via the constraints indeed boosts the learned model and the proposed method signi cantly outperforms the traditional semi-supervised framework." ></td>
	<td class="line x" title="26:270	2 Related Work In the semi-supervised domain there are two main approaches for injecting domain speci c knowledge." ></td>
	<td class="line x" title="27:270	One is using the prior knowledge to accurately tailor the generative model so that it captures the domain structure." ></td>
	<td class="line x" title="28:270	For example, (Grenager et al. , 2005) proposes Diagonal Transition Models for sequential labeling tasks where neighboring words tend to have the same labels." ></td>
	<td class="line x" title="29:270	This is done by constraining the HMM transition matrix, which can be done also for other models, such as CRF." ></td>
	<td class="line x" title="30:270	However (Roth and Yih, 2005) showed that reasoning with more expressive, non-sequential constraints can improve the performance for the supervised protocol." ></td>
	<td class="line x" title="31:270	A second approach has been to use a small highaccuracy set of labeled tokens as a way to seed and bootstrap the semi-supervised learning." ></td>
	<td class="line x" title="32:270	This was used, for example, by (Thelen and Riloff, 2002; Collins and Singer, 1999) in information extraction, and by (Smith and Eisner, 2005) in POS tagging." ></td>
	<td class="line x" title="33:270	(Haghighi and Klein, 2006) extends the dictionarybased approach to sequential labeling tasks by propagating the information given in the seeds with contextual word similarity." ></td>
	<td class="line x" title="34:270	This follows a conceptually similar approach by (Cohen and Sarawagi, 2004) that uses a large named-entity dictionary, where the similarity between the candidate named-entity and its matching prototype in the dictionary is encoded as a feature in a supervised classi er." ></td>
	<td class="line x" title="35:270	In our framework, dictionary lookup approaches are viewed as unary constraints on the output states." ></td>
	<td class="line x" title="36:270	We extend these kinds of constraints and allow for more general, n-ary constraints." ></td>
	<td class="line x" title="37:270	In the supervised learning setting it has been established that incorporating global information can signi cantly improve performance on several NLP tasks, including information extraction and semantic role labeling." ></td>
	<td class="line x" title="38:270	(Punyakanok et al. , 2005; Toutanova et al. , 2005; Roth and Yih, 2005)." ></td>
	<td class="line x" title="39:270	Our formalism is most related to this last work." ></td>
	<td class="line x" title="40:270	But, we develop a semi-supervised learning protocol based on this formalism." ></td>
	<td class="line x" title="41:270	We also make use of soft constraints and, furthermore, extend the notion of soft constraints to account for multiple levels of constraints violation." ></td>
	<td class="line x" title="42:270	Conceptually, although not technically, the most related work to ours is (Shen et al. , 2005) that, in a somewhat ad-hoc manner uses soft constraints to guide an unsupervised model that was crafted for mention tracking." ></td>
	<td class="line x" title="43:270	To the best of our knowledge, we are the rst to suggest a general semi-supervised protocol that is driven by soft constraints." ></td>
	<td class="line x" title="44:270	We propose learning with constraints a framework that combines the approaches described above in a uni ed and intuitive way." ></td>
	<td class="line x" title="45:270	3 Tasks, Examples and Datasets In Section 4 we will develop a general framework for semi-supervised learning with constraints." ></td>
	<td class="line x" title="46:270	However, it is useful to illustrate the ideas on concrete problems." ></td>
	<td class="line x" title="47:270	Therefore, in this section, we give a brief introduction to the two domains on which we tested our algorithms." ></td>
	<td class="line x" title="48:270	We study two information extraction problems in each of which, given text, a set of pre-de ned elds is to be identi ed." ></td>
	<td class="line x" title="49:270	Since the elds are typically related and interdependent, these kinds of applications provide a good test case for an approach like ours.1 The rst task is to identify elds from citations (McCallum et al. , 2000)." ></td>
	<td class="line x" title="50:270	The data originally included 500 labeled references, and was later extended with 5,000 unannotated citations collected from papers found on the Internet (Grenager et al. , 2005)." ></td>
	<td class="line x" title="51:270	Given a citation, the task is to extract the 1The data for both problems is available at: http://www.stanford.edu/ grenager/data/unsupie.tgz 281 (a) [ AUTHOR Lars Ole Andersen . ] [ TITLE Program analysis and specialization for the C programming language . ] [ TECH-REPORT PhD thesis, ] [ INSTITUTION DIKU, University of Copenhagen, ] [ DATE May 1994 . ] (b) [ AUTHOR Lars Ole Andersen . Program analysis and ] [TITLE specialization for the ] [EDITOR C ] [ BOOKTITLE Programming language ] [ TECH-REPORT . PhD thesis, ] [ INSTITUTION DIKU, University of Copenhagen, May ] [ DATE 1994 . ] Figure 1: Error analysis of a HMM model." ></td>
	<td class="line x" title="52:270	The labels are annotated by underline and are to the right of each open bracket." ></td>
	<td class="line x" title="53:270	The correct assignment was shown in (a)." ></td>
	<td class="line x" title="54:270	While the predicted label assignment (b) is generally coherent, some constraints are violated." ></td>
	<td class="line x" title="55:270	Most obviously, punctuation marks are ignored as cues for state transitions." ></td>
	<td class="line x" title="56:270	The constraint Fields cannot end with stop words (such as the ) may be also good." ></td>
	<td class="line x" title="57:270	elds that appear in the given reference." ></td>
	<td class="line x" title="58:270	See Fig." ></td>
	<td class="line x" title="59:270	1." ></td>
	<td class="line x" title="60:270	There are 13 possible elds including author, title, location, etc. To gain an insight to how the constraints can guide semi-supervised learning, assume that the sentence shown in Figure 1 appears in the unlabeled data pool." ></td>
	<td class="line x" title="61:270	Part (a) of the gure shows the correct labeled assignment and part (b) shows the assignment labeled by a HMM trained on 30 labels." ></td>
	<td class="line x" title="62:270	However, if we apply the constraint that state transition can occur only on punctuation marks, the same HMM model parameters will result in the correct labeling (a)." ></td>
	<td class="line x" title="63:270	Therefore, by adding the improved labeled assignment we can generate better training samples during semi-supervised learning." ></td>
	<td class="line x" title="64:270	In fact, the punctuation marks are only some of the constraints that can be applied to this problem." ></td>
	<td class="line x" title="65:270	The set of constraints we used in our experiments appears in Table 1." ></td>
	<td class="line x" title="66:270	Note that some of the constraints are non-local and are very intuitive for people, yet it is very dif cult to inject this knowledge into most models." ></td>
	<td class="line x" title="67:270	The second problem we consider is extracting elds from advertisements (Grenager et al. , 2005)." ></td>
	<td class="line x" title="68:270	The dataset consists of 8,767 advertisements for apartment rentals in the San Francisco Bay Area downloaded in June 2004 from the Craigslist website." ></td>
	<td class="line x" title="69:270	In the dataset, only 302 entries have been labeled with 12 elds, including size, rent, neighborhood, features, and so on." ></td>
	<td class="line x" title="70:270	The data was preprocessed using regular expressions for phone numbers, email addresses and URLs." ></td>
	<td class="line x" title="71:270	The list of the constraints for this domain is given in Table 1." ></td>
	<td class="line x" title="72:270	We implement some global constraints and include unary constraints which were largely imported from the list of seed words used in (Haghighi and Klein, 2006)." ></td>
	<td class="line x" title="73:270	We slightly modi ed the seedwords due to difference in preprocessing." ></td>
	<td class="line x" title="74:270	4 Notation and De nitions Consider a structured classi cation problem, where given an input sequence x = (x1, . . ., xN), the task is to nd the best assignment to the output variables y = (y1, . . ., yM)." ></td>
	<td class="line x" title="75:270	We denote X to be the space of the possible input sequences and Y to be the set of possible output sequences." ></td>
	<td class="line x" title="76:270	We de ne a structured output classi er as a function h : X ! Y that uses a global scoring function f : X Y ! R to assign scores to each possible input/output pair." ></td>
	<td class="line x" title="77:270	Given an input x, a desired function f will assign the correct output y the highest score among all the possible outputs." ></td>
	<td class="line x" title="78:270	The global scoring function is often decomposed as a weighted sum of feature functions, f(x, y) = Msummationdisplay i=1 ifi(x, y) =  F(x, y)." ></td>
	<td class="line x" title="79:270	This decomposition applies both to discriminative linear models and to generative models such as HMMs and CRFs, in which case the linear sum corresponds to log likelihood assigned to the input/output pair by the model (for details see (Roth, 1999) for the classi cation case and (Collins, 2002) for the structured case)." ></td>
	<td class="line x" title="80:270	Even when not dictated by the model, the feature functions fi(x, y) used are local to allow inference tractability." ></td>
	<td class="line x" title="81:270	Local feature function can capture some context for each input or output variable, yet it is very limited to allow dynamic programming decoding during inference." ></td>
	<td class="line x" title="82:270	Now, consider a scenario where we have a set of constraints C1, . . ., CK." ></td>
	<td class="line x" title="83:270	We de ne a constraint C : X Y ! f0, 1g as a function that indicates whether the input/output sequence violates some desired properties." ></td>
	<td class="line x" title="84:270	When the constraints are hard, the solution is given by argmax y1C(x)  F(x, y), 282 (a)-Citations 1) Each eld must be a consecutive list of words, and can appear at most once in a citation." ></td>
	<td class="line x" title="85:270	2) State transitions must occur on punctuation marks." ></td>
	<td class="line x" title="86:270	3) The citation can only start with author or editor." ></td>
	<td class="line x" title="87:270	4) The words pp., pages correspond to PAGE." ></td>
	<td class="line x" title="89:270	5) Four digits starting with 20xx and 19xx are DATE." ></td>
	<td class="line x" title="90:270	6) Quotations can appear only in titles." ></td>
	<td class="line x" title="91:270	7) The words note, submitted, appear are NOTE." ></td>
	<td class="line x" title="92:270	8) The words CA, Australia, NY are LOCATION." ></td>
	<td class="line x" title="93:270	9) The words tech, technical are TECH REPORT." ></td>
	<td class="line x" title="94:270	10) The words proc, journal, proceedings, ACM are JOURNAL or BOOKTITLE." ></td>
	<td class="line x" title="95:270	11) The words ed, editors correspond to EDITOR." ></td>
	<td class="line x" title="96:270	(b)-Advertisements 1) State transitions can occur only on punctuation marks or the newline symbol." ></td>
	<td class="line x" title="97:270	2) Each eld must be at least 3 words long." ></td>
	<td class="line x" title="98:270	3) The words laundry, kitchen, parking are FEATURES." ></td>
	<td class="line x" title="99:270	4) The words sq, ft, bdrm are SIZE." ></td>
	<td class="line x" title="100:270	5) The word $, *MONEY* are RENT." ></td>
	<td class="line x" title="101:270	6) The words close, near, shopping are NEIGHBORHOOD." ></td>
	<td class="line x" title="102:270	7) The words laundry kitchen, parking are FEATURES." ></td>
	<td class="line x" title="103:270	8) The (normalized) words phone, email are CONTACT." ></td>
	<td class="line x" title="104:270	9) The words immediately, begin, cheaper are AVAILABLE." ></td>
	<td class="line x" title="105:270	10) The words roommates, respectful, drama are ROOMMATES." ></td>
	<td class="line x" title="106:270	11) The words smoking, dogs, cats are RESTRICTIONS." ></td>
	<td class="line x" title="107:270	12) The word http, image, link are PHOTOS." ></td>
	<td class="line x" title="108:270	13) The words address, carlmont, st, cross are ADDRESS." ></td>
	<td class="line x" title="109:270	14) The words utilities, pays, electricity are UTILITIES." ></td>
	<td class="line x" title="110:270	Table 1: The list of constraints for extracting elds from citations and advertisements." ></td>
	<td class="line x" title="111:270	Some constraints (represented in the rst block of each domain) are global and are relatively dif cult to inject into traditional models." ></td>
	<td class="line x" title="112:270	While all the constraints hold for the vast majority of the data, some of them are violated by some correct labeled assignments." ></td>
	<td class="line x" title="113:270	where 1C(x) is a subset of Y for which all Ci assign the value 1 for the given (x, y)." ></td>
	<td class="line x" title="114:270	When the constraints are soft, we want to incur some penalty for their violation." ></td>
	<td class="line x" title="115:270	Moreover, we want to incorporate into our cost function a measure for the amount of violation incurred by violating the constraint." ></td>
	<td class="line x" title="116:270	A generic way to capture this intuition is to introduce a distance function d(y, 1Ci(x)) between the space of outputs that respect the constraint,1Ci(x), and the given output sequence y. One possible way to implement this distance function is as the minimal Hamming distance to a sequence that respects the constraint Ci, that is: d(y, 1Ci(x)) = min(yprime1C(x)) H(y, yprime)." ></td>
	<td class="line x" title="117:270	If the penalty for violating the soft constraint Ci is i, we write the score function as: argmax y  F(x, y) Ksummationdisplay i=1 id(y, 1Ci(x)) (1) We refer to d(y, 1C(x)) as the valuation of the constraint C on (x, y)." ></td>
	<td class="line x" title="118:270	The intuition behind (1) is as follows." ></td>
	<td class="line x" title="119:270	Instead of merely maximizing the models likelihood, we also want to bias the model using some knowledge." ></td>
	<td class="line x" title="120:270	The rst term of (1) is used to learn from data." ></td>
	<td class="line x" title="121:270	The second term biases the mode by using the knowledge encoded in the constraints." ></td>
	<td class="line x" title="122:270	Note that we do not normalize our objective function to be a true probability distribution." ></td>
	<td class="line x" title="123:270	5 Learning and Inference with Constraints In this section we present a new constraint-driven learning algorithm (CODL) for using constraints to guide semi-supervised learning." ></td>
	<td class="line x" title="124:270	The task is to learn the parameter vector  by using the new objective function (1)." ></td>
	<td class="line x" title="125:270	While our formulation allows us to train also the coef cients of the constraints valuation, i, we choose not to do it, since we view this as a way to bias (or enforce) the prior knowledge into the learned model, rather than allowing the data to brush it away." ></td>
	<td class="line x" title="126:270	Our experiments demonstrate that the proposed approach is robust to inaccurate approximation of the prior knowledge (assigning the same penalty to all the i )." ></td>
	<td class="line x" title="127:270	We note that in the presence of constraints, the inference procedure (for nding the output y that maximizes the cost function) is usually done with search techniques (rather than Viterbi decoding, see (Toutanova et al. , 2005; Roth and Yih, 2005) for a discussion), we chose beamsearch decoding." ></td>
	<td class="line x" title="128:270	The semi-supervised learning with constraints is done with an EM-like procedure." ></td>
	<td class="line x" title="129:270	We initialize the model with traditional supervised learning (ignoring the constraints) on a small labeled set." ></td>
	<td class="line x" title="130:270	Given an unlabeled set U, in the estimation step, the traditional EM algorithm assigns a distribution over labeled assignmentsY of each x 2 U, and in the maximization step, the set of model parameters is learned from the distributions assigned in the estimation step." ></td>
	<td class="line x" title="131:270	However, in the presence of constraints, assigning the complete distributions in the estimation step is infeasible since the constraints reshape the distribution in an arbitrary way." ></td>
	<td class="line x" title="132:270	As in existing methods for training a model by maximizing a linear cost function (maximize likelihood or discriminative maxi283 mization), the distribution over Y is represented as the set of scores assigned to it; rather than considering the score assigned to all yprimes, we truncate the distribution to the top K assignments as returned by the search." ></td>
	<td class="line x" title="133:270	Given a set of K top assignments y1, . . ., yK, we approximate the estimation step by assigning uniform probability to the top K candidates, and zero to the other output sequences." ></td>
	<td class="line x" title="134:270	We denote this algorithm top-K hard EM." ></td>
	<td class="line x" title="135:270	In this paper, we use beamsearch to generate K candidates according to (1)." ></td>
	<td class="line x" title="136:270	Our training algorithm is summarized in Figure 2." ></td>
	<td class="line x" title="137:270	Several things about the algorithm should be claried: the Top-K-Inference procedure in line 7, the learning procedure in line 9, and the new parameter estimation in line 9." ></td>
	<td class="line x" title="138:270	The Top-K-Inference is a procedure that returns the K labeled assignments that maximize the new objective function (1)." ></td>
	<td class="line x" title="139:270	In our case we used the topK elements in the beam, but this could be applied to any other inference procedure." ></td>
	<td class="line x" title="140:270	The fact that the constraints are used in the inference procedure (in particular, for generating new training examples) allows us to use a learning algorithm that ignores the constraints, which is a lot more ef cient (although algorithms that do take the constraints into account can be used too)." ></td>
	<td class="line x" title="141:270	We used maximum likelihood estimation of  but, in general, perceptron or quasiNewton can also be used." ></td>
	<td class="line x" title="142:270	It is known that traditional semi-supervised training can degrade the learned models performance." ></td>
	<td class="line x" title="143:270	(Nigam et al. , 2000) has suggested to balance the contribution of labeled and unlabeled data to the parameters." ></td>
	<td class="line x" title="144:270	The intuition is that when iteratively estimating the parameters with EM, we disallow the parameters to drift too far from the supervised model." ></td>
	<td class="line x" title="145:270	The parameter re-estimation in line 9, uses a similar intuition, but instead of weighting data instances, we introduced a smoothing parameter  which controls the convex combination of models induced by the labeled and the unlabeled data." ></td>
	<td class="line x" title="146:270	Unlike the technique mentioned above which focuses on naive Bayes, our method allows us to weight linear models generated by different learning algorithms." ></td>
	<td class="line oc" title="147:270	Another way to look the algorithm is from the self-training perspective (McClosky et al. , 2006)." ></td>
	<td class="line x" title="148:270	Similarly to self-training, we use the current model to generate new training examples from the unlaInput: Cycles: learning cycles Tr = {x, y}: labeled training set." ></td>
	<td class="line x" title="149:270	U: unlabeled dataset F: set of feature functions." ></td>
	<td class="line x" title="150:270	{i}: set of penalties." ></td>
	<td class="line x" title="151:270	{Ci}: set of constraints." ></td>
	<td class="line x" title="152:270	: balancing parameter with the supervised model." ></td>
	<td class="line x" title="153:270	learn(Tr, F): supervised learning algorithm Top-K-Inference: returns top-K labeled scored by the cost function (1) CODL: 1." ></td>
	<td class="line x" title="154:270	Initialize 0 = learn(Tr, F)." ></td>
	<td class="line x" title="155:270	2." ></td>
	<td class="line x" title="156:270	= 0." ></td>
	<td class="line x" title="157:270	3. For Cycles iterations do: 4." ></td>
	<td class="line x" title="158:270	T =  5." ></td>
	<td class="line x" title="159:270	For each x  U 6." ></td>
	<td class="line x" title="160:270	{(x, y1), . . ., (x, yK)} = 7." ></td>
	<td class="line x" title="161:270	Top-K-Inference(x,, F,{Ci},{i}) 8." ></td>
	<td class="line x" title="162:270	T = T {(x, y1), . . ., (x, yK)} 9." ></td>
	<td class="line x" title="163:270	=  0 + (1  )learn(T, F) Figure 2: COnstraint Driven Learning (CODL)." ></td>
	<td class="line x" title="164:270	In Top-K-Inference, we use beamsearch to nd the Kbest solution according to Eq." ></td>
	<td class="line x" title="165:270	(1)." ></td>
	<td class="line x" title="166:270	beled set." ></td>
	<td class="line x" title="167:270	However, there are two important differences." ></td>
	<td class="line x" title="168:270	One is that in self-training, once an unlabeled sample was labeled, it is never labeled again." ></td>
	<td class="line x" title="169:270	In our case all the samples are relabeled in each iteration." ></td>
	<td class="line x" title="170:270	In self-training it is often the case that only high-con dence samples are added to the labeled data pool." ></td>
	<td class="line x" title="171:270	While we include all the samples in the training pool, we could also limit ourselves to the high-con dence samples." ></td>
	<td class="line x" title="172:270	The second difference is that each unlabeled example generates K labeled instances." ></td>
	<td class="line x" title="173:270	The case of one iteration of top-1 hard EM is equivalent to self training, where all the unlabeled samples are added to the labeled pool." ></td>
	<td class="line x" title="174:270	There are several possible bene ts to using K > 1 samples." ></td>
	<td class="line x" title="175:270	(1) It effectively increases the training set by a factor of K (albeit by somewhat noisy examples)." ></td>
	<td class="line x" title="176:270	In the structured scenario, each of the top-K assignments is likely to have some good components so generating top-K assignments helps leveraging the noise." ></td>
	<td class="line x" title="177:270	(2) Given an assignment that does not satisfy some constraints, using top-K allows for multiple ways to correct it." ></td>
	<td class="line x" title="178:270	For example, consider the output 11101000 with the constraint that it should belong to the language 10." ></td>
	<td class="line x" title="179:270	If the two top scoring corrections are 11111000 and 11100000, considering only one of those can negatively bias the model." ></td>
	<td class="line x" title="180:270	284 6 Experiments and Results In this section, we present empirical results of our algorithms on two domains: citations and advertisements." ></td>
	<td class="line x" title="181:270	Both problems are modeled with a simple token-based HMM." ></td>
	<td class="line x" title="182:270	We stress that token-based HMM cannot represent many of our constraints." ></td>
	<td class="line x" title="183:270	The function d(y, 1C(x)) used is an approximation of a Hamming distance function, discussed in Section 7." ></td>
	<td class="line x" title="184:270	For both domains, and all the experiments,  was set to 0.1." ></td>
	<td class="line x" title="185:270	The constraints violation penalty  is set to log 104 and log 101 for citations and advertisements, resp.2 Note that all constraints share the same penalty." ></td>
	<td class="line x" title="186:270	The number of semi-supervised training cycles (line 3 of Figure 2) was set to 5." ></td>
	<td class="line x" title="187:270	The constraints for the two domains are listed in Table 1." ></td>
	<td class="line x" title="188:270	We trained models on training sets of size varying from 5 to 300 for the citations and from 5 to 100 for the advertisements." ></td>
	<td class="line x" title="189:270	Additionally, in all the semi-supervised experiments, 1000 unlabeled examples are used." ></td>
	<td class="line x" title="190:270	We report token-based3 accuracy on 100 held-out examples (which do not overlap neither with the training nor with the unlabeled data)." ></td>
	<td class="line x" title="191:270	We ran 5 experiments in each setting, randomly choosing the training set." ></td>
	<td class="line x" title="192:270	The results reported below are the averages over these 5 runs." ></td>
	<td class="line x" title="193:270	To verify our claims we implemented several baselines." ></td>
	<td class="line x" title="194:270	The rst baseline is the supervised learning protocol denoted by sup." ></td>
	<td class="line x" title="195:270	The second baseline was a traditional top-1 Hard EM also known as truncated EM4 (denoted by H for Hard)." ></td>
	<td class="line x" title="196:270	In the third baseline, denoted H&W, we balanced the weight of the supervised and unsupervised models as described in line 9 of Figure 2." ></td>
	<td class="line x" title="197:270	We compare these baselines to our proposed protocol, H&W&C, where we added the constraints to guide the H&W protocol." ></td>
	<td class="line x" title="198:270	We experimented with two avors of the algorithm: the top-1 and the top-K version." ></td>
	<td class="line x" title="199:270	In the top-K version, the algorithm uses K-best predictions (K=50) for each instance in order to update the model as described in Figure 2." ></td>
	<td class="line x" title="200:270	The experimental results for both domains are in given Table 2." ></td>
	<td class="line x" title="201:270	As hypothesized, hard EM sometimes 2The guiding intuition is that F(x, y) corresponds to a loglikelihood of a HMM model and  to a crude estimation of the log probability that a constraint does not hold." ></td>
	<td class="line x" title="202:270	 was tuned on a development set and kept xed in all experiments." ></td>
	<td class="line x" title="203:270	3Each token (word or punctuation mark) is assigned a state." ></td>
	<td class="line x" title="204:270	4We also experimented with (soft) EM without constraints, but the results were generally worse." ></td>
	<td class="line x" title="205:270	(a)Citations N Inf." ></td>
	<td class="line x" title="206:270	sup." ></td>
	<td class="line x" title="207:270	H H&W H&W&C H&W&C (Top-1) (Top-K) 5 no I 55.1 60.9 63.6 70.6 71.0 I 66.6 69.0 72.5 76.0 77.8 10 no I 64.6 66.8 69.8 76.5 76.7 I 78.1 78.1 81.0 83.4 83.8 15 no I 68.7 70.6 73.7 78.6 79.4 I 81.3 81.9 84.1 85.5 86.2 20 no I 70.1 72.4 75.0 79.6 79.4 I 81.1 82.4 84.0 86.1 86.1 25 no I 72.7 73.2 77.0 81.6 82.0 I 84.3 84.2 86.2 87.4 87.6 300 no I 86.1 80.7 87.1 88.2 88.2 I 92.5 89.6 93.4 93.6 93.5 (b)-Advertisements N Inf." ></td>
	<td class="line x" title="208:270	sup." ></td>
	<td class="line x" title="209:270	H H&W H&W&C H&W&C (Top-1) (Top-K) 5 no I 55.2 61.8 60.5 66.0 66.0 I 59.4 65.2 63.6 69.3 69.6 10 no I 61.6 69.2 67.0 70.8 70.9 I 66.6 73.2 71.6 74.7 74.7 15 no I 66.3 71.7 70.1 73.0 73.0 I 70.4 75.6 74.5 76.6 76.9 20 no I 68.1 72.8 72.0 74.5 74.6 I 71.9 76.7 75.7 77.9 78.1 25 no I 70.0 73.8 73.0 74.9 74.8 I 73.7 77.7 76.6 78.4 78.5 100 no I 76.3 76.2 77.6 78.5 78.6 I 80.4 80.5 81.2 81.8 81.7 Table 2: Experimental results for extracting elds from citations and advertisements." ></td>
	<td class="line x" title="210:270	N is the number of labeled samples." ></td>
	<td class="line x" title="211:270	H is the traditional hard-EM and H&W weighs labeled and unlabeled data as mentioned in Sec." ></td>
	<td class="line x" title="212:270	5." ></td>
	<td class="line x" title="213:270	Our proposed model is H&W&C, which uses constraints in the learning procedure." ></td>
	<td class="line x" title="214:270	I refers to using constraints during inference at evaluation time." ></td>
	<td class="line x" title="215:270	Note that adding constraints improves the accuracy during both learning and inference." ></td>
	<td class="line x" title="216:270	degrade the performance." ></td>
	<td class="line x" title="217:270	Indeed, with 300 labeled examples in the citations domain, the performance decreases from 86.1 to 80.7." ></td>
	<td class="line x" title="218:270	The usefulness of injecting constraints in semi-supervised learning is exhibited in the two right most columns: using constraints H&W&C improves the performance over H&W quite signi cantly." ></td>
	<td class="line x" title="219:270	We carefully examined the contribution of using constraints to the learning stage and the testing stage, and two separate results are presented: testing with constraints (denoted I for inference) and without constraints (no I)." ></td>
	<td class="line x" title="220:270	The I results are consistently better." ></td>
	<td class="line x" title="221:270	And, it is also clear from Table 2, that using constraints in training always improves 285 the model and the amount of improvement depends on the amount of labeled data." ></td>
	<td class="line x" title="222:270	Figure 3 compares two protocols on the advertisements domain: H&W+I, where we rst run the H&W protocol and then apply the constraints during testing stage, and H&W&C+I, which uses constraints to guide the model during learning and uses it also in testing." ></td>
	<td class="line x" title="223:270	Although injecting constraints in the learning process helps, testing with constraints is more important than using constraints during learning, especially when the labeled data size is large." ></td>
	<td class="line x" title="224:270	This con rms results reported for the supervised learning case in (Punyakanok et al. , 2005; Roth and Yih, 2005)." ></td>
	<td class="line x" title="225:270	However, as shown, our proposed algorithm H&W&C for training with constraints is critical when the amount labeled data is small." ></td>
	<td class="line x" title="226:270	Figure 4 further strengthens this point." ></td>
	<td class="line x" title="227:270	In the citations domain, H&W&C+I achieves with 20 labeled samples similar performance to the supervised version without constraints with 300 labeled samples." ></td>
	<td class="line x" title="228:270	(Grenager et al. , 2005) and (Haghighi and Klein, 2006) also report results for semi-supervised learning for these domains." ></td>
	<td class="line x" title="229:270	However, due to different preprocessing, the comparison is not straightforward." ></td>
	<td class="line x" title="230:270	For the citation domain, when 20 labeled and 300 unlabeled samples are available, (Grenager et al. , 2005) observed an increase from 65.2% to 71.3%." ></td>
	<td class="line x" title="231:270	Our improvement is from 70.1% to 79.4%." ></td>
	<td class="line x" title="232:270	For the advertisement domain, they observed no improvement, while our model improves from 68.1% to 74.6% with 20 labeled samples." ></td>
	<td class="line x" title="233:270	Moreover, we successfully use out-of-domain data (web data) to improve our model, while they report that this data did not improve their unsupervised model." ></td>
	<td class="line x" title="234:270	(Haghighi and Klein, 2006) also worked on one of our data sets." ></td>
	<td class="line x" title="235:270	Their underlying model, Markov Random Fields, allows more expressive features." ></td>
	<td class="line x" title="236:270	Nevertheless, when they use only unary constraints they get 53.75%." ></td>
	<td class="line x" title="237:270	When they use their nal model, along with a mechanism for extending the prototypes to other tokens, they get results that are comparable to our model with 10 labeled examples." ></td>
	<td class="line x" title="238:270	Additionally, in their framework, it is not clear how to use small amounts of labeled data when available." ></td>
	<td class="line x" title="239:270	Our model outperforms theirs once we add 10 more examples." ></td>
	<td class="line x" title="240:270	0.65 0.7 0.75 0.8 0.85 100252015105 H+N+I H+N+C+I Figure 3: Comparison between H&W+I and H&W&C+I on the advertisements domain." ></td>
	<td class="line x" title="241:270	When there is a lot of labeled data, inference with constraints is more important than using constraints during learning." ></td>
	<td class="line x" title="242:270	However, it is important to train with constraints when the amount of labeled data is small." ></td>
	<td class="line x" title="243:270	0.7 0.75 0.8 0.85 0.9 0.95 100252015105 sup." ></td>
	<td class="line x" title="244:270	(300) H+N+C+I Figure 4: With 20 labeled citations, our algorithm performs competitively to the supervised version trained on 300 samples." ></td>
	<td class="line x" title="245:270	7 Soft Constraints This section discusses the importance of using soft constraints rather than hard constraints, the choice of Hamming distance for d(y, 1C(x)) and how we approximate it." ></td>
	<td class="line x" title="246:270	We use two constraints to illustrate the ideas." ></td>
	<td class="line x" title="247:270	(C1): state transitions can only occur on punctuation marks or newlines, and (C2): the eld TITLE must appear . First, we claim that de ning d(y, 1C(x)) to be the Hamming distance is superior to using a binary value, d(y, 1C(x)) = 0 if y 2 1C(x) and 1 otherwise." ></td>
	<td class="line x" title="248:270	Consider, for example, the constraint C1 in the advertisements domain." ></td>
	<td class="line x" title="249:270	While the vast majority of the instances satisfy the constraint, some violate it in more than one place." ></td>
	<td class="line x" title="250:270	Therefore, once the binary distance is set to 1, the algorithm looses the ability to discriminate constraint violations in other locations 286 of the same instance." ></td>
	<td class="line x" title="251:270	This may hurt the performance in both the inference and the learning stage." ></td>
	<td class="line x" title="252:270	Computing the Hamming distance exactly can be a computationally hard problem." ></td>
	<td class="line x" title="253:270	Furthermore, it is unreasonable to implement the exact computation for each constraint." ></td>
	<td class="line x" title="254:270	Therefore, we implemented a generic approximation for the hamming distance assuming only that we are given a boolean function C(yN) that returns whether labeling the token xN with state yN violates constraint with respect to an already labeled sequence (x1, . . ., xN1, y1, . . ., yN1)." ></td>
	<td class="line x" title="255:270	Then d(y, 1C(x)) = summationtextNi=1 C(yi)." ></td>
	<td class="line x" title="256:270	For example, consider the pre x x1, x2, x3, x4, which contains no punctuation or newlines and was labeled AUTH, AUTH, DATE, DATE." ></td>
	<td class="line x" title="257:270	This labeling violates C1, the minimal hamming distance is 2, and our approximation gives 1, (since there is only one transition that violates the constraint.)" ></td>
	<td class="line x" title="258:270	For constraints which cannot be validated based on pre x information, our approximation resorts to binary violation count." ></td>
	<td class="line x" title="259:270	For instance, the constraint C2 cannot be implemented with pre x information when the assignment is not complete." ></td>
	<td class="line x" title="260:270	Otherwise, it would mean that the eld TITLE should appear as early as possible in the assignment." ></td>
	<td class="line x" title="261:270	While (Roth and Yih, 2005) showed the significance of using hard constraints, our experiments show that using soft constraints is a superior option." ></td>
	<td class="line x" title="262:270	For example, in the advertisements domain, C1 holds for the large majority of the gold-labeled instances, but is sometimes violated." ></td>
	<td class="line x" title="263:270	In supervised training with 100 labeled examples on this domain, sup gave 76.3% accuracy." ></td>
	<td class="line x" title="264:270	When the constraint violation penalty  was in nity (equivalent to hard constraint), the accuracy improved to 78.7%, but when the penalty was set to log(0.1), the accuracy of the model jumped to 80.6%." ></td>
	<td class="line x" title="265:270	8 Conclusions and Future Work We proposed to use constraints as a way to guide semi-supervised learning." ></td>
	<td class="line x" title="266:270	The framework developed is general both in terms of the representation and expressiveness of the constraints, and in terms of the underlying model being learned HMM in the current implementation." ></td>
	<td class="line x" title="267:270	Moreover, our framework is a useful tool when the domain knowledge cannot be expressed by the model." ></td>
	<td class="line x" title="268:270	The results show that constraints improve not only the performance of the nal inference stage but also propagate useful information during the semisupervised learning process and that training with the constraints is especially signi cant when the number of labeled training data is small." ></td>
	<td class="line x" title="269:270	Acknowledgments: This work is supported by NSF SoDHCER-0613885 and by a grant from Boeing." ></td>
	<td class="line x" title="270:270	Part of this work was done while Dan Roth visited the Technion, Israel, supported by a Lady Davis Fellowship." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-1049
Fast Unsupervised Incremental Parsing
Seginer, Yoav;"></td>
	<td class="line x" title="1:181	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 384391, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:181	c2007 Association for Computational Linguistics Fast Unsupervised Incremental Parsing Yoav Seginer Institute for Logic, Language and Computation Universiteit van Amsterdam Plantage Muidergracht 24 1018TV Amsterdam The Netherlands yseginer@science.uva.nl Abstract This paper describes an incremental parser and an unsupervised learning algorithm for inducing this parser from plain text." ></td>
	<td class="line x" title="3:181	The parser uses a representation for syntactic structure similar to dependency links which is well-suited for incremental parsing." ></td>
	<td class="line x" title="4:181	In contrast to previous unsupervised parsers, the parser does not use part-of-speech tags and both learning and parsing are local and fast, requiring no explicit clustering or global optimization." ></td>
	<td class="line x" title="5:181	The parser is evaluated by converting its output into equivalent bracketing and improves on previously published results for unsupervised parsing from plain text." ></td>
	<td class="line x" title="6:181	1 Introduction Grammar induction, the learning of the grammar of a language from unannotated example sentences, has long been of interest to linguists because of its relevance to language acquisition by children." ></td>
	<td class="line x" title="7:181	In recent years, interest in unsupervised learning of grammar has also increased among computational linguists, as the difficulty and cost of constructing annotated corpora led researchers to look for ways to train parsers on unannotated text." ></td>
	<td class="line oc" title="8:181	This can either be semi-supervised parsing, using both annotated and unannotated data (McClosky et al. , 2006) or unsupervised parsing, training entirely on unannotated text." ></td>
	<td class="line x" title="9:181	The past few years have seen considerable improvement in the performance of unsupervised parsers (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006a; Bod, 2006b) and, for the first time, unsupervised parsers have been able to improve on the right-branching heuristic for parsing English." ></td>
	<td class="line x" title="10:181	All these parsers learn and parse from sequences of part-of-speech tags and select, for each sentence, the binary parse tree which maximizes some objective function." ></td>
	<td class="line x" title="11:181	Learning is based on global maximization of this objective function over the whole corpus." ></td>
	<td class="line x" title="12:181	In this paper I present an unsupervised parser from plain text which does not use parts-of-speech." ></td>
	<td class="line x" title="13:181	Learning is local and parsing is (locally) greedy." ></td>
	<td class="line x" title="14:181	As a result, both learning and parsing are fast." ></td>
	<td class="line x" title="15:181	The parser is incremental, using a new link representation for syntactic structure." ></td>
	<td class="line x" title="16:181	Incremental parsing was chosen because it considerably restricts the search space for both learning and parsing." ></td>
	<td class="line x" title="17:181	The representation the parser uses is designed for incremental parsing and allows a prefix of an utterance to be parsed before the full utterance has been read (see section 3)." ></td>
	<td class="line x" title="18:181	The representation the parser outputs can be converted into bracketing, thus allowing evaluation of the parser on standard treebanks." ></td>
	<td class="line x" title="19:181	To achieve completely unsupervised parsing, standard unsupervised parsers, working from partof-speech sequences, need first to induce the partsof-speech for the plain text they need to parse." ></td>
	<td class="line x" title="20:181	There are several algorithms for doing so (Schutze, 1995; Clark, 2000), which cluster words into classes based on the most frequent neighbors of each word." ></td>
	<td class="line x" title="21:181	This step becomes superfluous in the algorithm I present here: the algorithm collects lists of labels for each word, based on neighboring words, and then directly 384 uses these labels to parse." ></td>
	<td class="line x" title="22:181	No clustering is performed, but due to the Zipfian distribution of words, high frequency words dominate these lists and parsing decisions for words of similar distribution are guided by the same labels." ></td>
	<td class="line x" title="23:181	Section 2 describes the syntactic representation used, section 3 describes the general parser algorithm and sections 4 and 5 complete the details by describing the learning algorithm, the lexicon it constructs and the way the parser uses this lexicon." ></td>
	<td class="line x" title="24:181	Section 6 gives experimental results." ></td>
	<td class="line x" title="25:181	2 Common Cover Links The representation of syntactic structure which I introduce in this paper is based on links between pairs of words." ></td>
	<td class="line x" title="26:181	Given an utterance and a bracketing of that utterance, shortest common cover link sets for the bracketing are defined." ></td>
	<td class="line x" title="27:181	The original bracketing can be reconstructed from any of these link sets." ></td>
	<td class="line x" title="28:181	2.1 Basic Definitions An utterance is a sequence of words x1,,,xn and a bracket is any sub-sequence xi,,,xj of consecutive words in the utterance." ></td>
	<td class="line x" title="29:181	A set B of brackets over an utterance U is a bracketing of U if every word in U is in some bracket and for any X,Y  B either X  Y = , X  Y or Y  X (noncrossing brackets)." ></td>
	<td class="line x" title="30:181	The depth of a word x  U under a bracket B  B (x  B) is the maximal number of brackets X1,,,Xn  B such that x  X1   Xn  B. A word x is a generator of depth d of B in B if x is of minimal depth under B (among all words in B) and that depth is d. A bracket may have more than one generator." ></td>
	<td class="line x" title="31:181	2.2 Common Cover Link Sets A common cover link over an utterance U is a triple x d y where x,y  U, x negationslash= y and d is a nonnegative integer." ></td>
	<td class="line x" title="32:181	The word x is the base of the link, the word y is its head and d is the depth of the link." ></td>
	<td class="line x" title="33:181	The common cover link set RB associated with a bracketing B is the set of common cover links over U such that x d y  RB iff the word x is a generator of depth d of the smallest bracket B  B such that x,y  B (see figure 1(a))." ></td>
	<td class="line x" title="34:181	Given RB, a simple algorithm reconstructs the bracketing B: for each word x and depth 0  d, (a) [ [ w ] 1 d59d59 1 d60d60 1 d61d61[ x 1d122d122 0 d33d33 0 d32d32[ y d47d47 0 z ] ] ]d111d111 (b) [ [ w ] [ x 1d122d122 0 d33d33 0 d32d32[ y d47d47 0 z ] ] ]d111d111 (c) [ [ w ] [ x 1d122d122 0 d33d33 [ y d47d470 z ] ] ]d111d111 Figure 1: (a) The common cover link set RB of a bracketing B, (b) a representative subset R of RB, (c) the shortest common cover link set based on R. create a bracket covering x and all y such that for some dprime  d, x dprime y  RB." ></td>
	<td class="line x" title="35:181	Some of the links in the common cover link set RB are redundant." ></td>
	<td class="line x" title="36:181	The first redundancy is the result of brackets having more than one generator." ></td>
	<td class="line x" title="37:181	The bracketing reconstruction algorithm outlined above can construct a bracket from the links based at any of its generators." ></td>
	<td class="line x" title="38:181	The bracketing B can therefore be reconstructed from a subset R  RB if, for every bracket B  B, R contains the links based at least at one generator1 of B. Such a set R is a representative subset of RB (see figure 1(b))." ></td>
	<td class="line x" title="39:181	A second redundancy in the set RB follows from the linear transitivity of RB: Lemma 1 If y is between x and z, x d1 y  RB and y d2 z  RB then x d z  RB where if there is a link y dprime x  RB then d = max(d1,d2) and d = d1 otherwise." ></td>
	<td class="line x" title="40:181	This property implies that longer links can be deduced from shorter links." ></td>
	<td class="line x" title="41:181	It is, therefore, sufficient to leave only the shortest necessary links in the set." ></td>
	<td class="line x" title="42:181	Given a representative subset R of RB, a shortest common cover link set of RB is constructed by removing any link which can be deduced from shorter links by linear transitivity." ></td>
	<td class="line x" title="43:181	For each representative subset R  RB, this defines a unique shortest common cover link set (see figure 1(c))." ></td>
	<td class="line x" title="44:181	Given a shortest common cover link set S, the bracketing which it represents can be calculated by 1From the bracket reconstruction algorithm it can be seen that links of depth 0 may never be dropped." ></td>
	<td class="line x" title="45:181	385 [ [ I ] [ knowd123d123 d37d37[ [ the boy ]d111d111 [ sleeps ] ] ] ]d125d125 (a) dependency structure [ [ I ] [ know 1d123d123 0 d37d370 d34d34[ [ the 0 d47d47 boy ]d111d111 [ sleeps ] ] ] ]1d125d125 (b) shortest common cover link set Figure 2: A dependency structure and shortest common cover link set of the same sentence." ></td>
	<td class="line x" title="46:181	first using linear transitivity to deduce missing links and then applying the bracket reconstruction algorithm outlined above for RB." ></td>
	<td class="line x" title="47:181	2.3 Comparison with Dependency Structures Having defined a link-based representation of syntactic structure, it is natural to wonder what the relation is between this representation and standard dependency structures." ></td>
	<td class="line x" title="48:181	The main differences between the two representations can all be seen in figure 2." ></td>
	<td class="line x" title="49:181	The first difference is in the linking of the NP the boy." ></td>
	<td class="line x" title="50:181	While the shortest common cover link set has an exocentric construction for this NP (that is, links going back and forth between the two words), the dependency structure forces us to decide which of the two words in the NP is its head." ></td>
	<td class="line x" title="51:181	Considering that linguists have not been able to agree whether it is the determiner or the noun that is the head of an NP, it may be easier for a learning algorithm if it did not have to make such a choice." ></td>
	<td class="line x" title="52:181	The second difference between the structures can be seen in the link from know to sleeps." ></td>
	<td class="line x" title="53:181	In the shortest common cover link set, there is a path of links connecting know to each of the words separating it from sleeps, while in the dependency structure no such links exist." ></td>
	<td class="line x" title="54:181	This property, which I will refer to as adjacency plays an important role in incremental parsing, as explained in the next section." ></td>
	<td class="line x" title="55:181	The last main difference between the representations is the assignment of depth to the common cover links." ></td>
	<td class="line x" title="56:181	In the present example, this allows us to distinguish between the attachment of the external (subject) and the internal (object) arguments of the verb." ></td>
	<td class="line x" title="57:181	Dependencies cannot capture this difference without additional labeling of the links." ></td>
	<td class="line x" title="58:181	In what follows, I will restrict common cover links to having depth 0 or 1." ></td>
	<td class="line x" title="59:181	This restriction means that any tree represented by a shortest common cover link set will be skewed every subtree must have a short branch." ></td>
	<td class="line x" title="60:181	It seems that this is indeed a property of the syntax of natural languages." ></td>
	<td class="line x" title="61:181	Building this restriction into the syntactic representation considerably reduces the search space for both parsing and learning." ></td>
	<td class="line x" title="62:181	3 Incremental Parsing To calculate a shortest common cover link for an utterance, I will use an incremental parser." ></td>
	<td class="line x" title="63:181	Incrementality means that the parser reads the words of the utterance one by one and, as each word is read, the parser is only allowed to add links which have one of their ends at that word." ></td>
	<td class="line x" title="64:181	Words which have not yet been read are not available to the parser at this stage." ></td>
	<td class="line x" title="65:181	This restriction is inspired by psycholinguistic research which suggests that humans process language incrementally (Crocker et al. , 2000)." ></td>
	<td class="line x" title="66:181	If the incrementality of the parser roughly resembles that of human processing, the result is a significant restriction of parser search space which does not lead to too many parsing errors." ></td>
	<td class="line x" title="67:181	The adjacency property described in the previous section makes shortest common cover link sets especially suitable for incremental parsing." ></td>
	<td class="line x" title="68:181	Consider the example given in figure 2." ></td>
	<td class="line x" title="69:181	When the word the is read, the parser can already construct a link from know to the without worrying about the continuation of the sentence." ></td>
	<td class="line x" title="70:181	This link is part of the correct parse whether the sentence turns out to be I know the boy or I know the boy sleeps." ></td>
	<td class="line x" title="71:181	A dependency parser, on the other hand, cannot make such a decision before the end of the sentence is reached." ></td>
	<td class="line x" title="72:181	If the sentence is I know the boy then a dependency link has to be created from know to boy while if the sentence is I know the boy sleeps then such a link is wrong." ></td>
	<td class="line x" title="73:181	This problem is known in psycholinguistics as the problem of reanalysis (Sturt and Crocker, 1996)." ></td>
	<td class="line x" title="74:181	Assume the incremental parser is processing a prefix x1,,,xk of an utterance and has already deduced a set of links L for this prefix." ></td>
	<td class="line x" title="75:181	It can now only add links which have one of their ends at xk and it may never remove any links." ></td>
	<td class="line x" title="76:181	From the definitions in section 2.2 it is possible to derive an exact characterization of the links which may be added at each step such that the resulting link set represents some 386 bracketing." ></td>
	<td class="line x" title="77:181	It can be shown that any shortest common cover link set can be constructed incrementally under these conditions." ></td>
	<td class="line x" title="78:181	As the full specification of these conditions is beyond the scope of this paper, I will only give the main condition, which is based on adjacency." ></td>
	<td class="line x" title="79:181	It states that a link may be added from x to y only if for every z between x and y there is a path of links (in L) from x to z but no link from z to y. In the example in figure 2 this means that when the word sleeps is first read, a link to sleeps can be created from know, the and boy but not from I. Given these conditions, the parsing process is simple." ></td>
	<td class="line x" title="80:181	At each step, the parser calculates a nonnegative weight (section 5) for every link which may be added between the prefix x1,,,xk1 and xk." ></td>
	<td class="line x" title="81:181	It then adds the link with the strongest positive weight and repeats the process (adding a link can change the set of links which may be added)." ></td>
	<td class="line x" title="82:181	When all possible links are assigned a zero weight by the parser, the parser reads the next word of the utterance and repeats the process." ></td>
	<td class="line x" title="83:181	This is a greedy algorithm which optimizes every step separately." ></td>
	<td class="line x" title="84:181	4 Learning The weight function which assigns a weight to a candidate link is lexicalized: the weight is calculated based on the lexical entries of the words which are to be connected by the link." ></td>
	<td class="line x" title="85:181	It is the task of the learning algorithm to learn the lexicon." ></td>
	<td class="line x" title="86:181	4.1 The Lexicon The lexicon stores for each word x a lexical entry." ></td>
	<td class="line x" title="87:181	Each such lexical entry is a sequence of adjacency points, holding statistics relevant to the decision whether to link x to some other word." ></td>
	<td class="line x" title="88:181	These statistics are given as weights assigned to labels and linking properties." ></td>
	<td class="line x" title="89:181	Each adjacency point describes a different link based at x, similar to the specification of the arguments of a word in dependency parsing." ></td>
	<td class="line x" title="90:181	Let W be the set of words in the corpus." ></td>
	<td class="line x" title="91:181	The set of labels L(W) = W  {0,1} consists of two labels based on every word w: a class label (w,0) (denoted by [w]) and an adjacency label (w,1) (denoted by [w ] or [ w])." ></td>
	<td class="line x" title="92:181	The two labels (w,0) and (w,1) are said to be opposite labels and, for l  L(W), I write l1 for the opposite of l. In addition to the labels, there is also a finite set P = {Stop,In,In,Out} of linking properties." ></td>
	<td class="line x" title="93:181	The Stop specifies the strength of non-attachment, In and Out specify the strength of inbound and outbound links and In is an intermediate value in the induction of inbound and outbound strengths." ></td>
	<td class="line x" title="94:181	A lexicon L is a function which assigns each word w  W a lexical entry ((,Aw2,Aw1,Aw1,Aw2,)." ></td>
	<td class="line x" title="95:181	Each of the Awi is an adjacency point." ></td>
	<td class="line x" title="96:181	Each Awi is a function Awi : L(W)  P  R which assigns each label in L(W) and each linking property in P a real valued strength." ></td>
	<td class="line x" title="97:181	For each Awi, #(Awi ) is the count of the adjacency point: the number of times the adjacency point was updated." ></td>
	<td class="line x" title="98:181	Based on this count, I also define a normalized version of Awi : Awi (l) = Awi (l)/#(Awi )." ></td>
	<td class="line x" title="99:181	4.2 The Learning Process Given a sequence of training utterances (Ut)0t, the learner constructs a sequence of lexicons (Ls)0s beginning with the zero lexicon L0 (which assigns a zero strength to all labels and linking properties)." ></td>
	<td class="line x" title="100:181	At each step, the learner uses the parsing function PLs based on the previously learned lexicon Ls to extend the parse L of an utterance Ut. It then uses the result of this parse step (together with the lexicon Ls) to create a new lexicon Ls+1 (it may be that Ls = Ls+1)." ></td>
	<td class="line x" title="101:181	This operation is a lexicon update." ></td>
	<td class="line x" title="102:181	The process then continues with the new lexicon Ls+1." ></td>
	<td class="line x" title="103:181	Any of the lexicons Ls constructed by the learner may be used for parsing any utterance U, but as s increases, parsing accuracy should improve." ></td>
	<td class="line x" title="104:181	This learning process is open-ended: additional training text can always be added without having to re-run the learner on previous training data." ></td>
	<td class="line x" title="105:181	4.3 Lexicon Update To define a lexicon update, I extend the definition of an utterance to be U = l,x1,,,xn,r where l and r are boundary markers." ></td>
	<td class="line x" title="106:181	The property of adjacency can now be extended to include the boundary markers." ></td>
	<td class="line x" title="107:181	A symbol   U is adjacent to a word x relative to a set of links L over U if for every word z between x and  there is a path of links in L from x to z but there is no link from z to ." ></td>
	<td class="line x" title="108:181	In the following example, the adjacencies of x1 are l, x2 and x3: x1 0 d47d47 x2 x3 x4 387 If a link is added from x2 to x3, x4 becomes adjacent to x1 instead of x3 (the adjacencies of x1 are then l, x2 and x4): x1 0 d47d47 x2 0 d47d47 x3 x4 The positions in the utterance adjacent to a word x are indexed by an index i such that i < 0 to the left of x, i > 0 to the right of x and |i| increases with the distance from x. The parser may only add a link from a word x to a word y adjacent to x (relative to the set of links already constructed)." ></td>
	<td class="line x" title="109:181	Therefore, the lexical entry of x should collect statistics about each of the adjacency positions of x. As seen above, adjacency positions may move, so the learner waits until the parser completes parsing the utterance and then updates each adjacency point Axi with the symbol  at the ith adjacency position of x (relative to the parse generated by the parser)." ></td>
	<td class="line x" title="110:181	It should be stressed that this update does not depend on whether a link was created from x to ." ></td>
	<td class="line x" title="111:181	In particular, whatever links the parser assigns, Ax(1) and Ax1 are always updated by the symbols which appear immediately before and after x. The following example should clarify the picture." ></td>
	<td class="line x" title="112:181	Consider the fragment: put 0 d47d47 the d47d470 boxd111d111 on All the links in this example, including the absence of a link from box to on, depend on adjacency points of the form Ax(1) and Ax1 which are updated independently of any links." ></td>
	<td class="line x" title="113:181	Based on this alone and regardless of whether a link is created from put to on, Aput2 will be updated by the word on, which is indeed the second argument of the verb put." ></td>
	<td class="line x" title="114:181	4.4 Adjacency Point Update The update of Axi by  is given by operations Axi (p) += f(A(1),A1 ) which make the value of Axi (p) in the new lexicon Ls+1 equal to the sum Axi (p) + f(A(1),A1 ) in the old lexicon Ls." ></td>
	<td class="line x" title="115:181	Let Sign(i) be 1 if 0 < i and 1 otherwise." ></td>
	<td class="line x" title="116:181	Let A  i =    true if notexistentiall  L(W) : Ai (l) > Ai (Stop) false otherwise The update of Axi by  begins by incrementing the count: #(Axi ) += 1 If  is a boundary symbol (l or r) or if x and  are words separated by stopping punctuation (full stop, question mark, exclamation mark, semicolon, comma or dash): Axi (Stop) += 1 Otherwise, for every l  L(W): Axi (l1) += braceleftbigg 1 if l = [] ASign(i)(l) otherwise (In practice, only l = [] and the 10 strongest labels in ASign(i) are updated." ></td>
	<td class="line x" title="117:181	Because of the exponential decay in the strength of labels in ASign(i), this is a good approximation.)" ></td>
	<td class="line x" title="118:181	If i = 1,1 and  is not a boundary or blocked by punctuation, simple bootstrapping takes place by updating the following properties: Axi (In) +=    1 if ASign(i) +1 if ASign(i) ASign(i) 0 otherwise Axi (Out) += ASign(i)(In) Axi (In) += ASign(i)(Out) 4.5 Discussion To understand the way the labels and properties are calculated, it is best to look at an example." ></td>
	<td class="line x" title="119:181	The following table gives the linking properties and strongest labels for the determiner the as learned from the complete Wall Street Journal corpus (only Athe(1) and Athe1 are shown): the A1 A1 Stop 12897 Stop 8 In 14898 In 18914 In 8625 In 4764 Out -13184 Out 21922 [the] 10673 [the] 16461 [of ] 6871 [a] 3107 [in ] 5520 [ the] 2787 [a] 3407 [of] 2347 [for ] 2572 [ company] 2094 [to ] 2094 [s] 1686 A strong class label [w] indicates that the word w frequently appears in contexts which are similar to the." ></td>
	<td class="line x" title="120:181	A strong adjacency label [w ] (or [ w]) indicates 388 that w either frequently appears next to the or that w frequently appears in the same contexts as words which appear next to the." ></td>
	<td class="line x" title="121:181	The property Stop counts the number of times a boundary appeared next to the." ></td>
	<td class="line x" title="122:181	Because the can often appear at the beginning of an utterance but must be followed by a noun or an adjective, it is not surprising that Stop is stronger than any label on the left but weaker than all labels on the right." ></td>
	<td class="line x" title="123:181	In general, it is unlikely that a word has an outbound link on the side on which its Stop strength is stronger than that of any label." ></td>
	<td class="line x" title="124:181	The opposite is not true: a label stronger than Stop indicates an attachment but this may also be the result of an inbound link, as in the following entry for to, where the strong labels on the left are a result of an inbound link: to A1 A1 Stop 822 Stop 48 In -4250 In -981 In -57 In -1791 Out -3053 Out 4010 [to] 5912 [to] 7009 [% ] 848 [ the] 3851 [in] 844 [ be] 2208 [the] 813 [will] 1414 [of] 624 [ a] 1158 [a] 599 [the] 954 For this reason, the learning process is based on the property Axi which indicates where a link is not possible." ></td>
	<td class="line x" title="125:181	Since an outbound link on one word is inbound on the other, the inbound/outbound properties of each word are then calculated by a simple bootstrapping process as an average of the opposite properties of the neighboring words." ></td>
	<td class="line x" title="126:181	5 The Weight Function At each step, the parser must assign a non-negative weight to every candidate link x d y which may be added to an utterance prefix x1,,,xk, and the link with the largest (non-zero) weight (with a preference for links between xk1 and xk) is added to the parse." ></td>
	<td class="line x" title="127:181	The weight could be assigned directly based on the In and Out properties of either x or y but this method is not satisfactory for three reasons: first, the values of these properties on low frequency words are not reliable; second, the values of the properties on x and y may conflict; third, some words are ambiguous and require different linking in different contexts." ></td>
	<td class="line x" title="128:181	To solve these problems, the weight of the link is taken from the values of In and Out on the best matching label between x and y. This label depends on both words and is usually a frequent word with reliable statistics." ></td>
	<td class="line x" title="129:181	It serves as a prototype for the relation between x and y. 5.1 Best Matching Label A label l is a matching label between Axi and AySign(i) if Axi (l) > Axi (Stop) and either l = (y,1) or AySign(i)(l1) > 0." ></td>
	<td class="line x" title="130:181	The best matching label at Axi is the matching label l such that the match strength min( Axi (l), AySign(i)(l1)) is maximal (if l = (y,1) then AySign(i)(l1) is defined to be 1)." ></td>
	<td class="line x" title="131:181	In practice, as before, only the top 10 labels in Axi and AySign(i) are considered." ></td>
	<td class="line x" title="132:181	The best matching label from x to y is calculated between Axi and AySign(i) such that Axi is on the same side of x as y and was either already used to create a link or is the first adjacency point on that side of x which was not yet used." ></td>
	<td class="line x" title="133:181	This means that the adjacency points on each side have to be used one by one, but may be used more than once." ></td>
	<td class="line x" title="134:181	The reason is that optional arguments of x usually do not have an adjacency point of their own but have the same labels as obligatory arguments of x and can share their adjacency point." ></td>
	<td class="line x" title="135:181	The Axi with the strongest matching label is selected, with a preference for the unused adjacency point." ></td>
	<td class="line x" title="136:181	As in the learning process, label matching is blocked between words which are separated by stopping punctuation." ></td>
	<td class="line x" title="137:181	5.2 Calculating the Link Weight The best matching label l = (w,) from x to y can be either a class ( = 0) or an adjacency ( = 1) label at Axi . If it is a class label, w can be seen as taking the place of x and all words separating it from y (which are already linked to x)." ></td>
	<td class="line x" title="138:181	If l is an adjacency label, w can be seen to take the place of y. The calculation of the weight Wt(x d y) of the link from x to y is therefore based on the strengths of the In and Out properties of Aw where  = Sign(i) if l = (w,0) and  = Sign(i) if l = (w,1)." ></td>
	<td class="line x" title="139:181	In addition, the weight is bounded from above by the best label match strength, s(l):  If l = (w,0) and Aw (Out) > 0: Wt(x 0 y) = min(s(l), Aw (Out)) 389 WSJ10 WSJ40 Negra10 Negra40 Model UP UR UF1 UP UR UF1 UP UR UF1 UP UR UF1 Right-branching 55.1 70.0 61.7 35.4 47.4 40.5 33.9 60.1 43.3 17.6 35.0 23.4 Right-branching+punct." ></td>
	<td class="line x" title="140:181	59.1 74.4 65.8 44.5 57.7 50.2 35.4 62.5 45.2 20.9 40.4 27.6 Parsing from POS CCM 64.2 81.6 71.9 48.1 85.5 61.6 DMV+CCM(POS) 69.3 88.0 77.6 49.6 89.7 63.9 U-DOP 70.8 88.2 78.5 63.9 51.2 90.5 65.4 UML-DOP 82.9 66.4 67.0 Parsing from plain text DMV+CCM(DISTR)." ></td>
	<td class="line x" title="141:181	65.2 82.8 72.9 Incremental 75.6 76.2 75.9 58.9 55.9 57.4 51.0 69.8 59.0 34.8 48.9 40.6 Incremental (right to left) 75.9 72.5 74.2 59.3 52.2 55.6 50.4 68.3 58.0 32.9 45.5 38.2 Table 1: Parsing results on WSJ10, WSJ40, Negra10 and Negra40." ></td>
	<td class="line x" title="142:181	 If l = (w,1):  If Aw (In) > 0: Wt(x d y) = min(s(l), Aw (In))  Otherwise, if Aw (In)  |Aw (In)|: Wt(x d y) = min(s(l), Aw (In)) where if Aw (In) < 0 and Aw (Out)  0 then d = 1 and otherwise d = 0." ></td>
	<td class="line x" title="143:181	 If Aw (Out)  0 and Aw (In)  0 and either l = (w,1) or Aw (Out) = 0: Wt(x 0 y) = s(l)  In all other cases, Wt(x d y) = 0." ></td>
	<td class="line x" title="144:181	A link x 1 y attaches x to y but does not place y inside the smallest bracket covering x. Such links are therefore created in the second case above, when the attachment indication is mixed." ></td>
	<td class="line x" title="145:181	To explain the third case, recall that s(l) > 0 means that the label l is stronger than Stop on Axi . This implies a link unless the properties of w block it." ></td>
	<td class="line x" title="146:181	One way in which w can block the link is to have a positive strength for the link in the opposite direction." ></td>
	<td class="line x" title="147:181	Another way in which the properties of w can block the link is if l = (w,0) and Aw (Out) < 0, that is, if the learning process has explicitly determined that no outbound link from w (which represents x in this case) is possible." ></td>
	<td class="line x" title="148:181	The same conclusion cannot be drawn from a negative value for the In property when l = (w,1) because, as with standard dependencies, a word determines its outbound links much more strongly than its inbound links." ></td>
	<td class="line x" title="149:181	6 Experiments The incremental parser was tested on the Wall Street Journal and Negra Corpora.2 Parsing accuracy was evaluated on the subsets WSJX and NegraX of these corpora containing sentences of length at most X (excluding punctuation)." ></td>
	<td class="line x" title="150:181	Some of these subsets were used for scoring in (Klein and Manning, 2004; Bod, 2006a; Bod, 2006b)." ></td>
	<td class="line x" title="151:181	I also use the same precision and recall measures used in those papers: multiple brackets and brackets covering a single word were not counted, but the top bracket was." ></td>
	<td class="line x" title="152:181	The incremental parser learns while parsing, and it could, in principle, simply be evaluated for a single pass of the data." ></td>
	<td class="line x" title="153:181	But, because the quality of the parses of the first sentences would be low, I first trained on the full corpus and then measured parsing accuracy on the corpus subset." ></td>
	<td class="line x" title="154:181	By training on the full corpus, the procedure differs from that of Klein, Manning and Bod who only train on the subset of bounded length sentences." ></td>
	<td class="line x" title="155:181	However, this excludes the induction of parts-of-speech for parsing from plain text." ></td>
	<td class="line x" title="156:181	When Klein and Manning induce the parts-of-speech, they do so from a much larger corpus containing the full WSJ treebank together with additional WSJ newswire (Klein and Manning, 2002)." ></td>
	<td class="line x" title="157:181	The comparison between the algorithms remains, therefore, valid." ></td>
	<td class="line x" title="158:181	Table 1 gives two baselines and the parsing results for WSJ10, WSJ40, Negra10 and Negra40 for recent unsupervised parsing algorithms: CCM 2I also tested the incremental parser on the Chinese Treebank version 5.0, achieving an F1 score of 54.6 on CTB10 and 38.0 on CTB40." ></td>
	<td class="line x" title="159:181	Because this version of the treebank is newer and clearly different from that used by previous papers, the results are not comparable and only given here for completeness." ></td>
	<td class="line x" title="160:181	390 and DMV+CCM (Klein and Manning, 2004), UDOP (Bod, 2006b) and UML-DOP (Bod, 2006a)." ></td>
	<td class="line x" title="161:181	The middle part of the table gives results for parsing from part-of-speech sequences extracted from the treebank while the bottom part of the table given results for parsing from plain text." ></td>
	<td class="line x" title="162:181	Results for the incremental parser are given for learning and parsing from left to right and from right to left." ></td>
	<td class="line x" title="163:181	The first baseline is the standard right-branching baseline." ></td>
	<td class="line x" title="164:181	The second baseline modifies rightbranching by using punctuation in the same way as the incremental parser: brackets (except the top one) are not allowed to contain stopping punctuation." ></td>
	<td class="line x" title="165:181	It can be seen that punctuation accounts for merely a small part of the incremental parsers improvement over the right-branching heuristic." ></td>
	<td class="line x" title="166:181	Comparing the two algorithms parsing from plain text (of WSJ10), it can be seen that the incremental parser has a somewhat higher combined F1 score, with better precision but worse recall." ></td>
	<td class="line x" title="167:181	This is because Klein and Mannings algorithms (as well as Bods) always generate binary parse trees, while here no such condition is imposed." ></td>
	<td class="line x" title="168:181	The small difference between the recall (76.2) and precision (75.6) of the incremental parser shows that the number of brackets induced by the parser is very close to that of the corpus3 and that the parser captures the same depth of syntactic structure as that which was used by the corpus annotators." ></td>
	<td class="line x" title="169:181	Incremental parsing from right to left achieves results close to those of parsing from left to right." ></td>
	<td class="line x" title="170:181	This shows that the incremental parser has no built-in bias for right branching structures.4 The slight degradation in performance may suggest that language should not, after all, be processed backwards." ></td>
	<td class="line x" title="171:181	While achieving state of the art accuracy, the algorithm also proved to be fast, parsing (on a 1.86GHz Centrino laptop) at a rate of around 4000 words/sec." ></td>
	<td class="line x" title="172:181	and learning (including parsing) at a rate of 3200  3600 words/sec." ></td>
	<td class="line x" title="173:181	The effect of sentence length on parsing speed is small: the full WSJ corpus was parsed at 3900 words/sec." ></td>
	<td class="line x" title="174:181	while WSJ10 was parsed at 4300 words/sec." ></td>
	<td class="line x" title="175:181	3The algorithm produced 35588 brackets compared with 35302 brackets in the corpus." ></td>
	<td class="line x" title="176:181	4I would like to thank Alexander Clark for suggesting this test." ></td>
	<td class="line x" title="177:181	7 Conclusions The unsupervised parser I presented here attempts to make use of several universal properties of natural languages: it captures the skewness of syntactic trees in its syntactic representation, restricts the search space by processing utterances incrementally (as humans do) and relies on the Zipfian distribution of words to guide its parsing decisions." ></td>
	<td class="line x" title="178:181	It uses an elementary bootstrapping process to deduce the basic properties of the language being parsed." ></td>
	<td class="line x" title="179:181	The algorithm seems to successfully capture some of these basic properties, but can be further refined to achieve high quality parsing." ></td>
	<td class="line x" title="180:181	The current algorithm is a good starting point for such refinement because it is so very simple." ></td>
	<td class="line x" title="181:181	Acknowledgments I would like to thank Dick de Jongh for many hours of discussion, and Remko Scha, Reut Tsarfaty and Jelle Zuidema for reading and commenting on various versions of this paper." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-1051
Is the End of Supervised Parsing in Sight?
Bod, Rens;"></td>
	<td class="line x" title="1:173	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 400407, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:173	c2007 Association for Computational Linguistics Is the End of Supervised Parsing in Sight?" ></td>
	<td class="line x" title="3:173	Rens Bod School of Computer Science University of St Andrews, ILLC, University of Amsterdam rb@cs.st-and.ac.uk Abstract How far can we get with unsupervised parsing if we make our training corpus several orders of magnitude larger than has hitherto be attempted?" ></td>
	<td class="line x" title="4:173	We present a new algorithm for unsupervised parsing using an all-subtrees model, termed U-DOP*, which parses directly with packed forests of all binary trees." ></td>
	<td class="line x" title="5:173	We train both on Penns WSJ data and on the (much larger) NANC corpus, showing that U-DOP* outperforms a treebank-PCFG on the standard WSJ test set." ></td>
	<td class="line x" title="6:173	While U-DOP* performs worse than state-of-the-art supervised parsers on handannotated sentences, we show that the model outperforms supervised parsers when evaluated as a language model in syntax-based machine translation on Europarl." ></td>
	<td class="line x" title="7:173	We argue that supervised parsers miss the fluidity between constituents and non-constituents and that in the field of syntax-based language modeling the end of supervised parsing has come in sight." ></td>
	<td class="line x" title="8:173	1 Introduction A major challenge in natural language parsing is the unsupervised induction of syntactic structure." ></td>
	<td class="line nc" title="9:173	While most parsing methods are currently supervised or semi-supervised (McClosky et al. 2006; Henderson 2004; Steedman et al. 2003), they depend on hand-annotated data which are difficult to come by and which exist only for a few languages." ></td>
	<td class="line x" title="10:173	Unsupervised parsing methods are becoming increasingly important since they operate with raw, unlabeled data of which unlimited quantities are available." ></td>
	<td class="line x" title="11:173	There has been a resurgence of interest in unsupervised parsing during the last few years." ></td>
	<td class="line x" title="12:173	Where van Zaanen (2000) and Clark (2001) induced unlabeled phrase structure for small domains like the ATIS, obtaining around 40% unlabeled f-score, Klein and Manning (2002) report 71.1% f-score on Penn WSJ part-of-speech strings  10 words (WSJ10) using a constituentcontext model called CCM." ></td>
	<td class="line x" title="13:173	Klein and Manning (2004) further show that a hybrid approach which combines constituency and dependency models, yields 77.6% f-score on WSJ10." ></td>
	<td class="line x" title="14:173	While Klein and Mannings approach may be described as an all-substrings approach to unsupervised parsing, an even richer model consists of an all-subtrees approach to unsupervised parsing, called U-DOP (Bod 2006)." ></td>
	<td class="line x" title="15:173	U-DOP initially assigns all unlabeled binary trees to a training set, efficiently stored in a packed forest, and next trains subtrees thereof on a heldout corpus, either by taking their relative frequencies, or by iteratively training the subtree parameters using the EM algorithm (referred to as UML-DOP)." ></td>
	<td class="line x" title="16:173	The main advantage of an allsubtrees approach seems to be the direct inclusion of discontiguous context that is not captured by (linear) substrings." ></td>
	<td class="line x" title="17:173	Discontiguous context is important not only for learning structural dependencies but also for learning a variety of noncontiguous constructions such as nearest  to or take  by surprise." ></td>
	<td class="line x" title="18:173	Bod (2006) reports 82.9% unlabeled f-score on the same WSJ10 as used by Klein and Manning (2002, 2004)." ></td>
	<td class="line x" title="19:173	Unfortunately, his experiments heavily depend on a priori sampling of subtrees, and the model becomes highly inefficient if larger corpora are used or longer sentences are included." ></td>
	<td class="line x" title="20:173	In this paper we will also test an alternative model for unsupervised all-subtrees 400 parsing, termed U-DOP*, which is based on the DOP* estimator by Zollmann and Simaan (2005), and which computes the shortest derivations for sentences from a held-out corpus using all subtrees from all trees from an extraction corpus." ></td>
	<td class="line x" title="21:173	While we do not achieve as high an f-score as the UML-DOP model in Bod (2006), we will show that U-DOP* can operate without subtree sampling, and that the model can be trained on corpora that are two orders of magnitude larger than in Bod (2006)." ></td>
	<td class="line x" title="22:173	We will extend our experiments to 4 million sentences from the NANC corpus (Graff 1995), showing that an f-score of 70.7% can be obtained on the standard Penn WSJ test set by means of unsupervised parsing." ></td>
	<td class="line x" title="23:173	Moreover, U-DOP* can be directly put to use in bootstrapping structures for concrete applications such as syntax-based machine translation and speech recognition." ></td>
	<td class="line x" title="24:173	We show that U-DOP* outperforms the supervised DOP model if tested on the German-English Europarl corpus in a syntax-based MT system." ></td>
	<td class="line x" title="25:173	In the following, we first explain the DOP* estimator and discuss how it can be extended to unsupervised parsing." ></td>
	<td class="line x" title="26:173	In section 3, we discuss how a PCFG reduction for supervised DOP can be applied to packed parse forests." ></td>
	<td class="line x" title="27:173	In section 4, we will go into an experimental evaluation of UDOP* on annotated corpora, while in section 5 we will evaluate U-DOP* on unlabeled corpora in an MT application." ></td>
	<td class="line x" title="28:173	2 From DOP* to U-DOP* DOP* is a modification of the DOP model in Bod (1998) that results in a statistically consistent estimator and in an efficient training procedure (Zollmann and Simaan 2005)." ></td>
	<td class="line x" title="29:173	DOP* uses the allsubtrees idea from DOP: given a treebank, take all subtrees, regardless of size, to form a stochastic tree-substitution grammar (STSG)." ></td>
	<td class="line x" title="30:173	Since a parse tree of a sentence may be generated by several (leftmost) derivations, the probability of a tree is the sum of the probabilities of the derivations producing that tree." ></td>
	<td class="line x" title="31:173	The probability of a derivation is the product of the subtree probabilities." ></td>
	<td class="line x" title="32:173	The original DOP model in Bod (1998) takes the occurrence frequencies of the subtrees in the trees normalized by their root frequencies as subtree parameters." ></td>
	<td class="line x" title="33:173	While efficient algorithms have been developed for this DOP model by converting it into a PCFG reduction (Goodman 2003), DOPs estimator was shown to be inconsistent by Johnson (2002)." ></td>
	<td class="line x" title="34:173	That is, even with unlimited training data, DOP's estimator is not guaranteed to converge to the correct distribution." ></td>
	<td class="line x" title="35:173	Zollmann and Simaan (2005) developed a statistically consistent estimator for DOP which is based on the assumption that maximizing the joint probability of the parses in a treebank can be approximated by maximizing the joint probability of their shortest derivations (i.e. the derivations consisting of the fewest subtrees)." ></td>
	<td class="line x" title="36:173	This assumption is in consonance with the principle of simplicity, but there are also empirical reasons for the shortest derivation assumption: in Bod (2003) and Hearne and Way (2006), it is shown that DOP models that select the preferred parse of a test sentence using the shortest derivation criterion perform very well." ></td>
	<td class="line x" title="37:173	On the basis of this shortest-derivation assumption, Zollmann and Simaan come up with a model that uses held-out estimation: the training corpus is randomly split into two parts proportional to a fixed ratio: an extraction corpus EC and a held-out corpus HC." ></td>
	<td class="line x" title="38:173	Applied to DOP, held-out estimation would mean to extract fragments from the trees in EC and to assign their weights such that the likelihood of HC is maximized." ></td>
	<td class="line x" title="39:173	If we combine their estimation method with Goodmans reduction of DOP, Zollman and Simaans procedure operates as follows: (1) Divide a treebank into an EC and HC (2) Convert the subtrees from EC into a PCFG reduction (3) Compute the shortest derivations for the sentences in HC (by simply assigning each subtree equal weight and applying Viterbi 1-best) (4) From those shortest derivations, extract the subtrees and their relative frequencies in HC to form an STSG Zollmann and Simaan show that the resulting estimator is consistent." ></td>
	<td class="line x" title="40:173	But equally important is the fact that this new DOP* model does not suffer from a decrease in parse accuracy if larger subtrees are included, whereas the original DOP model needs to be redressed by a correction factor to maintain this property (Bod 2003)." ></td>
	<td class="line x" title="41:173	Moreover, DOP*s estimation procedure is very efficient, while the EM training procedure for UML-DOP 401 proposed in Bod (2006) is particularly time consuming and can only operate by randomly sampling trees." ></td>
	<td class="line x" title="42:173	Given the advantages of DOP*, we will generalize this model in the current paper to unsupervised parsing." ></td>
	<td class="line x" title="43:173	We will use the same allsubtrees methodology as in Bod (2006), but now by applying the efficient and consistent DOP*based estimator." ></td>
	<td class="line x" title="44:173	The resulting model, which we will call U-DOP*, roughly operates as follows: (1) Divide a corpus into an EC and HC (2) Assign all unlabeled binary trees to the sentences in EC, and store them in a shared parse forest (3) Convert the subtrees from the parse forests into a compact PCFG reduction (see next section) (4) Compute the shortest derivations for the sentences in HC (as in DOP*) (5) From those shortest derivations, extract the subtrees and their relative frequencies in HC to form an STSG (6) Use the STSG to compute the most probable parse trees for new test data by means of Viterbi n-best (see next section) We will use this U-DOP* model to investigate our main research question: how far can we get with unsupervised parsing if we make our training corpus several orders of magnitude larger than has hitherto be attempted?" ></td>
	<td class="line x" title="45:173	3 Converting shared parse forests into PCFG reductions The main computational problem is how to deal with the immense number of subtrees in U-DOP*." ></td>
	<td class="line x" title="46:173	There exists already an efficient supervised algorithm that parses a sentence by means of all subtrees from a treebank." ></td>
	<td class="line x" title="47:173	This algorithm was extensively described in Goodman (2003) and converts a DOP-based STSG into a compact PCFG reduction that generates eight rules for each node in the treebank." ></td>
	<td class="line x" title="48:173	The reduction is based on the following idea: every node in every treebank tree is assigned a unique number which is called its address." ></td>
	<td class="line x" title="49:173	The notation A@k denotes the node at address k where A is the nonterminal labeling that node." ></td>
	<td class="line x" title="50:173	A new nonterminal is created for each node in the training data." ></td>
	<td class="line x" title="51:173	This nonterminal is called Ak." ></td>
	<td class="line x" title="52:173	Let aj represent the number of subtrees headed by the node A@j, and let a represent the number of subtrees headed by nodes with nonterminal A, that is a = j aj." ></td>
	<td class="line x" title="53:173	Then there is a PCFG with the following property: for every subtree in the training corpus headed by A, the grammar will generate an isomorphic subderivation." ></td>
	<td class="line x" title="54:173	For example, for a node (A@j (B@k, C@l)), the following eight PCFG rules in figure 1 are generated, where the number following a rule is its weight." ></td>
	<td class="line x" title="55:173	Aj  BC (1/aj) A  BC (1/a) Aj  BkC (bk/aj) A  BkC (bk/a) Aj  BCl (cl/aj) A  BCl (cl/a) Aj  BkCl (bkcl/aj) A  BkCl (bkcl/a) Figure 1." ></td>
	<td class="line x" title="56:173	PCFG reduction of supervised DOP By simple induction it can be shown that this construction produces PCFG derivations isomorphic to DOP derivations (Goodman 2003: 130-133)." ></td>
	<td class="line x" title="57:173	The PCFG reduction is linear in the number of nodes in the corpus." ></td>
	<td class="line x" title="58:173	While Goodmans reduction method was developed for supervised DOP where each training sentence is annotated with exactly one tree, the method can be generalized to a corpus where each sentence is annotated with all possible binary trees (labeled with the generalized category X), as long as we represent these trees by a shared parse forest." ></td>
	<td class="line x" title="59:173	A shared parse forest can be obtained by adding pointers from each node in the chart (or tabular diagram) to the nodes that caused it to be placed in the chart." ></td>
	<td class="line x" title="60:173	Such a forest can be represented in cubic space and time (see Billot and Lang 1989)." ></td>
	<td class="line x" title="61:173	Then, instead of assigning a unique address to each node in each tree, as done by the PCFG reduction for supervised DOP, we now assign a unique address to each node in each parse forest for each sentence." ></td>
	<td class="line x" title="62:173	However, the same node may be part of more than one tree." ></td>
	<td class="line x" title="63:173	A shared parse forest is an AND-OR graph where AND-nodes correspond to the usual parse tree nodes, while OR-nodes correspond to distinct subtrees occurring in the same context." ></td>
	<td class="line x" title="64:173	The total number of nodes is cubic in sentence length n. This means that there are O(n3) many nodes that receive a unique address as described above, to which next our PCFG reduction is applied." ></td>
	<td class="line x" title="65:173	This is a huge reduction compared to Bod (2006) where 402 the number of subtrees of all trees increases with the Catalan number, and only ad hoc sampling could make the method work." ></td>
	<td class="line x" title="66:173	Since U-DOP* computes the shortest derivations (in the training phase) by combining subtrees from unlabeled binary trees, the PCFG reduction in figure 1 can be represented as in figure 2, where X refers to the generalized category while B and C either refer to part-of-speech categories or are equivalent to X. The equal weights follow from the fact that the shortest derivation is equivalent to the most probable derivation if all subtrees are assigned equal probability (see Bod 2000; Goodman 2003)." ></td>
	<td class="line x" title="67:173	Xj  BC 1 X  BC 0.5 Xj  BkC 1 X  BkC 0.5 Xj  BCl 1 X  BCl 0.5 Xj  BkCl 1 X  BkCl 0.5 Figure 2." ></td>
	<td class="line x" title="68:173	PCFG reduction for U-DOP* Once we have parsed HC with the shortest derivations by the PCFG reduction in figure 2, we extract the subtrees from HC to form an STSG." ></td>
	<td class="line x" title="69:173	The number of subtrees in the shortest derivations is linear in the number of nodes (see Zollmann and Simaan 2005, theorem 5.2)." ></td>
	<td class="line x" title="70:173	This means that UDOP* results in an STSG which is much more succinct than previous DOP-based STSGs." ></td>
	<td class="line x" title="71:173	Moreover, as in Bod (1998, 2000), we use an extension of Good-Turing to smooth the subtrees and to deal with unknown subtrees." ></td>
	<td class="line x" title="72:173	Note that the direct conversion of parse forests into a PCFG reduction also allows us to efficiently implement the maximum likelihood extension of U-DOP known as UML-DOP (Bod 2006)." ></td>
	<td class="line x" title="73:173	This can be accomplished by training the PCFG reduction on the held-out corpus HC by means of the expectation-maximization algorithm, where the weights in figure 1 are taken as initial parameters." ></td>
	<td class="line x" title="74:173	Both U-DOP*s and UML-DOPs estimators are known to be statistically consistent." ></td>
	<td class="line x" title="75:173	But while U-DOP*s training phase merely consists of the computation of the shortest derivations and the extraction of subtrees, UMLDOP involves iterative training of the parameters." ></td>
	<td class="line x" title="76:173	Once we have extracted the STSG, we compute the most probable parse for new sentences by Viterbi n-best, summing up the probabilities of derivations resulting in the same tree (the exact computation of the most probable parse is NP hard  see Simaan 1996)." ></td>
	<td class="line x" title="77:173	We have incorporated the technique by Huang and Chiang (2005) into our implementation which allows for efficient Viterbi n-best parsing." ></td>
	<td class="line x" title="78:173	4 Evaluation on hand-annotated corpora To evaluate U-DOP* against UML-DOP and other unsupervised parsing models, we started out with three corpora that are also used in Klein and Manning (2002, 2004) and Bod (2006): Penns WSJ10 which contains 7422 sentences  10 words after removing empty elements and punctuation, the German NEGRA10 corpus and the Chinese Treebank CTB10 both containing 2200+ sentences  10 words after removing punctuation." ></td>
	<td class="line x" title="79:173	As with most other unsupervised parsing models, we train and test on p-o-s strings rather than on word strings." ></td>
	<td class="line x" title="80:173	The extension to word strings is straightforward as there exist highly accurate unsupervised part-of-speech taggers (e.g. Schtze 1995) which can be directly combined with unsupervised parsers, but for the moment we will stick to p-o-s strings (we will come back to word strings in section 5)." ></td>
	<td class="line x" title="81:173	Each corpus was divided into 10 training/test set splits of 90%/10% (n-fold testing), and each training set was randomly divided into two equal parts, that serve as EC and HC and vice versa." ></td>
	<td class="line x" title="82:173	We used the same evaluation metrics for unlabeled precision (UP) and unlabeled recall (UR) as in Klein and Manning (2002, 2004)." ></td>
	<td class="line x" title="83:173	The two metrics of UP and UR are combined by the unlabeled f-score F1 = 2*UP*UR/(UP+UR)." ></td>
	<td class="line x" title="84:173	All trees in the test set were binarized beforehand, in the same way as in Bod (2006)." ></td>
	<td class="line x" title="85:173	For UML-DOP the decrease in crossentropy became negligible after maximally 18 iterations." ></td>
	<td class="line x" title="86:173	The training for U-DOP* consisted in the computation of the shortest derivations for the HC from which the subtrees and their relative frequencies were extracted." ></td>
	<td class="line x" title="87:173	We used the technique in Bod (1998, 2000) to include unknown subtrees." ></td>
	<td class="line x" title="88:173	Table 1 shows the f-scores for U-DOP* and UML-DOP against the f-scores for U-DOP reported in Bod (2006), the CCM model in Klein and Manning (2002), the DMV dependency model in Klein and Manning (2004) and their combined model DMV+CCM." ></td>
	<td class="line x" title="89:173	403 Model English (WSJ10) German (NEGRA10) Chinese (CTB10) CCM 71.9 61.6 45.0 DMV 52.1 49.5 46.7 DMV+CCM 77.6 63.9 43.3 U-DOP 78.5 65.4 46.6 U-DOP* 77.9 63.8 42.8 UML-DOP 79.4 65.2 45.0 Table 1." ></td>
	<td class="line x" title="90:173	F-scores of U-DOP* and UML-DOP compared to other models on the same data." ></td>
	<td class="line x" title="91:173	It should be kept in mind that an exact comparison can only be made between U-DOP* and UMLDOP in table 1, since these two models were tested on 90%/10% splits, while the other models were applied to the full WSJ10, NEGRA10 and CTB10 corpora." ></td>
	<td class="line x" title="92:173	Table 1 shows that U-DOP* performs worse than UML-DOP in all cases, although the differences are small and was statistically significant only for WSJ10 using paired t-testing." ></td>
	<td class="line x" title="93:173	As explained above, the main advantage of U-DOP* over UML-DOP is that it works with a more succinct grammar extracted from the shortest derivations of HC." ></td>
	<td class="line x" title="94:173	Table 2 shows the size of the grammar (number of rules or subtrees) of the two models for resp." ></td>
	<td class="line x" title="95:173	Penn WSJ10, the entire Penn WSJ and the first 2 million sentences from the NANC (North American News Text) corpus which contains a total of approximately 24 million sentences from different news sources." ></td>
	<td class="line x" title="96:173	Model Size of STSG for WSJ10 Size of STSG for Penn WSJ Size of STSG for 2,000K NANC U-DOP* 2.2 x 104 9.8 x 105 7.2 x 106 UML-DOP 1.5 x 106 8.1 x 107 5.8 x 109 Table 2." ></td>
	<td class="line x" title="97:173	Grammar size of U-DOP* and UML-DOP for WSJ10 (7,7K sentences), WSJ (50K sentences) and the first 2,000K sentences from NANC." ></td>
	<td class="line x" title="98:173	Note that while U-DOP* is about 2 orders of magnitudes smaller than UML-DOP for the WSJ10, it is almost 3 orders of magnitudes smaller for the first 2 million sentences of the NANC corpus." ></td>
	<td class="line x" title="99:173	Thus even if U-DOP* does not give the highest f-score in table 1, it is more apt to be trained on larger data sets." ></td>
	<td class="line x" title="100:173	In fact, a well-known advantage of unsupervised methods over supervised methods is the availability of almost unlimited amounts of text." ></td>
	<td class="line x" title="101:173	Table 2 indicates that U-DOP*s grammar is still of manageable size even for text corpora that are (almost) two orders of magnitude larger than Penns WSJ." ></td>
	<td class="line oc" title="102:173	The NANC corpus contains approximately 2 million WSJ sentences that do not overlap with Penns WSJ and has been previously used by McClosky et al.(2006) in improving a supervised parser by selftraining." ></td>
	<td class="line x" title="104:173	In our experiments below we will start by mixing subsets from the NANCs WSJ data with Penns WSJ data." ></td>
	<td class="line x" title="105:173	Next, we will do the same with 2 million sentences from the LA Times in the NANC corpus, and finally we will mix all data together for inducing a U-DOP* model." ></td>
	<td class="line x" title="106:173	From Penns WSJ, we only use sections 2 to 21 for training (just as in supervised parsing) and section 23 (100 words) for testing, so as to compare our unsupervised results with some binarized supervised parsers." ></td>
	<td class="line x" title="107:173	The NANC data was first split into sentences by means of a simple discriminitive model." ></td>
	<td class="line x" title="108:173	It was next p-o-s tagged with the the TnT tagger (Brants 2000) which was trained on the Penn Treebank such that the same tag set was used." ></td>
	<td class="line x" title="109:173	Next, we added subsets of increasing size from the NANC p-o-s strings to the 40,000 Penn WSJ p-o-s strings." ></td>
	<td class="line x" title="110:173	Each time the resulting corpus was split into two halfs and the shortest derivations were computed for one half by using the PCFGreduction from the other half and vice versa." ></td>
	<td class="line x" title="111:173	The resulting trees were used for extracting an STSG which in turn was used to parse section 23 of Penns WSJ." ></td>
	<td class="line x" title="112:173	Table 3 shows the results." ></td>
	<td class="line x" title="113:173	# sentences added f-score by adding WSJ data f-score by adding LA Times data 0 (baseline) 62.2 62.2 100k 64.7 63.0 250k 66.2 63.8 500k 67.9 64.1 1,000k 68.5 64.6 2,000k 69.0 64.9 Table 3." ></td>
	<td class="line x" title="114:173	Results of U-DOP* on section 23 from Penns WSJ by adding sentences from NANCs WSJ and NANCs LA Times 404 Table 3 indicates that there is a monotonous increase in f-score on the WSJ test set if NANC text is added to our training data in both cases, independent of whether the sentences come from the WSJ domain or the LA Times domain." ></td>
	<td class="line x" title="115:173	Although the effect of adding LA Times data is weaker than adding WSJ data, it is noteworthy that the unsupervised induction of trees from the LA Times domain still improves the f-score even if the test data are from a different domain." ></td>
	<td class="line x" title="116:173	We also investigated the effect of adding the LA Times data to the total mix of Penns WSJ and NANCs WSJ." ></td>
	<td class="line x" title="117:173	Table 4 shows the results of this experiment, where the baseline of 0 sentences thus starts with the 2,040k sentences from the combined Penn-NANC WSJ data." ></td>
	<td class="line x" title="118:173	Sentences added from LA Times to Penn-NANC WSJ f-score by adding LA Times data 0 69.0 100k 69.4 250k 69.9 500k 70.2 1,000k 70.4 2,000k 70.7 Table 4." ></td>
	<td class="line x" title="119:173	Results of U-DOP* on section 23 from Penns WSJ by mixing sentences from the combined Penn-NANC WSJ with additions from NANCs LA Times." ></td>
	<td class="line x" title="120:173	As seen in table 4, the f-score continues to increase even when adding LA Times data to the large combined set of Penn-NANC WSJ sentences." ></td>
	<td class="line x" title="121:173	The highest f-score is obtained by adding 2,000k sentences, resulting in a total training set of 4,040k sentences." ></td>
	<td class="line x" title="122:173	We believe that our result is quite promising for the future of unsupervised parsing." ></td>
	<td class="line x" title="123:173	In putting our best f-score in table 4 into perspective, it should be kept in mind that the gold standard trees from Penn-WSJ section 23 were binarized." ></td>
	<td class="line x" title="124:173	It is well known that such a binarization has a negative effect on the f-score." ></td>
	<td class="line x" title="125:173	Bod (2006) reports that an unbinarized treebank grammar achieves an average 72.3% f-score on WSJ sentences  40 words, while the binarized version achieves only 64.6% f-score." ></td>
	<td class="line x" title="126:173	To compare UDOP*s results against some supervised parsers, we additionally evaluated a PCFG treebank grammar and the supervised DOP* parser using the same test set." ></td>
	<td class="line x" title="127:173	For these supervised parsers, we employed the standard training set, i.e. Penns WSJ sections 2-21, but only by taking the p-o-s strings as we did for our unsupervised U-DOP* model." ></td>
	<td class="line x" title="128:173	Table 5 shows the results of this comparison." ></td>
	<td class="line x" title="129:173	Parser f-score U-DOP* 70.7 Binarized treebank PCFG 63.5 Binarized DOP* 80.3 Table 5." ></td>
	<td class="line x" title="130:173	Comparison between the (best version of) U-DOP*, the supervised treebank PCFG and the supervised DOP* for section 23 of Penns WSJ As seen in table 5, U-DOP* outperforms the binarized treebank PCFG on the WSJ test set." ></td>
	<td class="line x" title="131:173	While a similar result was obtained in Bod (2006), the absolute difference between unsupervised parsing and the treebank grammar was extremely small in Bod (2006): 1.8%, while the difference in table 5 is 7.2%, corresponding to 19.7% error reduction." ></td>
	<td class="line x" title="132:173	Our f-score remains behind the supervised version of DOP* but the gap gets narrower as more training data is being added to U-DOP*." ></td>
	<td class="line x" title="133:173	5 Evaluation on unlabeled corpora in a practical application Our experiments so far have shown that despite the addition of large amounts of unlabeled training data, U-DOP* is still outperformed by the supervised DOP* model when tested on handannotated corpora like the Penn Treebank." ></td>
	<td class="line x" title="134:173	Yet it is well known that any evaluation on hand-annotated corpora unreasonably favors supervised parsers." ></td>
	<td class="line x" title="135:173	There is thus a quest for designing an evaluation scheme that is independent of annotations." ></td>
	<td class="line x" title="136:173	One way to go would be to compare supervised and unsupervised parsers as a syntax-based language model in a practical application such as machine translation (MT) or speech recognition." ></td>
	<td class="line x" title="137:173	In Bod (2007), we compared U-DOP* and DOP* in a syntax-based MT system known as Data-Oriented Translation or DOT (Poutsma 2000; Groves et al. 2004)." ></td>
	<td class="line x" title="138:173	The DOT model starts with a bilingual treebank where each tree pair constitutes an example translation and where translationally equivalent constituents are linked." ></td>
	<td class="line x" title="139:173	Similar to DOP, 405 the DOT model uses all linked subtree pairs from the bilingual treebank to form an STSG of linked subtrees, which are used to compute the most probable translation of a target sentence given a source sentence (see Hearne and Way 2006)." ></td>
	<td class="line x" title="140:173	What we did in Bod (2007) is to let both DOP* and U-DOP* compute the best trees directly for the word strings in the German-English Europarl corpus (Koehn 2005), which contains about 750,000 sentence pairs." ></td>
	<td class="line x" title="141:173	Differently from UDOP*, DOP* needed to be trained on annotated data, for which we used respectively the Negra and the Penn treebank." ></td>
	<td class="line x" title="142:173	Of course, it is well-known that a supervised parsers f-score decreases if it is transferred to another domain: for example, the (non-binarized) WSJ-trained DOP model in Bod (2003) decreases from around 91% to 85.5% fscore if tested on the Brown corpus." ></td>
	<td class="line x" title="143:173	Yet, this score is still considerably higher than the accuracy obtained by the unsupervised U-DOP model, which achieves 67.6% unlabeled f-score on Brown sentences." ></td>
	<td class="line x" title="144:173	Our main question of interest is in how far this difference in accuracy on hand-annotated corpora carries over when tested in the context of a concrete application like MT. This is not a trivial question, since U-DOP* learns constituents for word sequences such as Ich mchte (I would like to) and There are (Bod 2007), which are usually hand-annotated as non-constituents." ></td>
	<td class="line x" title="145:173	While UDOP* is punished for this incorrect prediction if evaluated on the Penn Treebank, it may be rewarded for this prediction if evaluated in the context of machine translation using the Bleu score (Papineni et al. 2002)." ></td>
	<td class="line x" title="146:173	Thus similar to Chiang (2005), U-DOP can discover non-syntactic phrases, or simply phrases, which are typically neglected by linguistically syntax-based MT systems." ></td>
	<td class="line x" title="147:173	At the same time, U-DOP* can also learn discontiguous constituents that are neglected by phrase-based MT systems (Koehn et al. 2003)." ></td>
	<td class="line x" title="148:173	In our experiments, we used both U-DOP* and DOP* to predict the best trees for the GermanEnglish Europarl corpus." ></td>
	<td class="line x" title="149:173	Next, we assigned links between each two nodes in the respective trees for each sentence pair." ></td>
	<td class="line x" title="150:173	For a 2,000 sentence test set from a different part of the Europarl corpus we computed the most probable target sentence (using Viterbi n best)." ></td>
	<td class="line x" title="151:173	The Bleu score was used to measure translation accuracy, calculated by the NIST script with its default settings." ></td>
	<td class="line x" title="152:173	As a baseline we compared our results with the publicly available phrase-based system Pharaoh (Koehn et al. 2003), using the default feature set." ></td>
	<td class="line x" title="153:173	Table 6 shows for each system the Bleu score together with a description of the productive units." ></td>
	<td class="line x" title="154:173	U-DOT refers to Unsupervised DOT based on U-DOP*, while DOT is based on DOP*." ></td>
	<td class="line x" title="155:173	System Productive Units Bleu-score U-DOT / U-DOP* Constituents and Phrases 0.280 DOT / DOP* Constituents only 0.221 Pharaoh Phrases only 0.251 Table 6." ></td>
	<td class="line x" title="156:173	Comparing U-DOP* and DOP* in syntaxbased MT on the German-English Europarl corpus against the Pharaoh system." ></td>
	<td class="line x" title="157:173	The table shows that the unsupervised U-DOT model outperforms the supervised DOT model with 0.059." ></td>
	<td class="line x" title="158:173	Using Zhangs significance tester (Zhang et al. 2004), it turns out that this difference is statistically significant (p < 0.001)." ></td>
	<td class="line x" title="159:173	Also the difference between U-DOT and the baseline Pharaoh is statistically significant (p < 0.008)." ></td>
	<td class="line x" title="160:173	Thus even if supervised parsers like DOP* outperform unsupervised parsers like U-DOP* on hand-parsed data with >10%, the same supervised parser is outperformed by the unsupervised parser if tested in an MT application." ></td>
	<td class="line x" title="161:173	Evidently, U-DOPs capacity to capture both constituents and phrases pays off in a concrete application and shows the shortcomings of models that only allow for either constituents (such as linguistically syntax-based MT) or phrases (such as phrase-based MT)." ></td>
	<td class="line x" title="162:173	In Bod (2007) we also show that U-DOT obtains virtually the same Bleu score as Pharaoh after eliminating subtrees with discontiguous yields." ></td>
	<td class="line x" title="163:173	6 Conclusion: future of supervised parsing In this paper we have shown that the accuracy of unsupervised parsing under U-DOP* continues to grow when enlarging the training set with additional data." ></td>
	<td class="line x" title="164:173	However, except for the simple treebank PCFG, U-DOP* scores worse than supervised parsers if evaluated on hand-annotated data." ></td>
	<td class="line x" title="165:173	At the same time U-DOP* significantly outperforms the supervised DOP* if evaluated in a practical application like MT. We argued that this can be explained by the fact that U-DOP learns 406 both constituents and (non-syntactic) phrases while supervised parsers learn constituents only." ></td>
	<td class="line x" title="166:173	What should we learn from these results?" ></td>
	<td class="line x" title="167:173	We believe that parsing, when separated from a task-based application, is mainly an academic exercise." ></td>
	<td class="line x" title="168:173	If we only want to mimick a treebank or implement a linguistically motivated grammar, then supervised, grammar-based parsers are preferred to unsupervised parsers." ></td>
	<td class="line x" title="169:173	But if we want to improve a practical application with a syntaxbased language model, then an unsupervised parser like U-DOP* might be superior." ></td>
	<td class="line x" title="170:173	The problem with most supervised (and semi-supervised) parsers is their rigid notion of constituent which excludes constituents like the German Ich mchte or the French Il y a. Instead, it has become increasingly clear that the notion of constituent is a fluid which may sometimes be in agreement with traditional syntax, but which may just as well be in opposition to it." ></td>
	<td class="line x" title="171:173	Any sequence of words can be a unit of combination, including noncontiguous word sequences like closest X to Y. A parser which does not allow for this fluidity may be of limited use as a language model." ></td>
	<td class="line x" title="172:173	Since supervised parsers seem to stick to categorical notions of constituent, we believe that in the field of syntax-based language models the end of supervised parsing has come in sight." ></td>
	<td class="line x" title="173:173	Acknowledgements Thanks to Willem Zuidema and three anonymous reviewers for useful comments and suggestions on the future of supervised parsing." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-1078
Self-Training for Enhancement and Domain Adaptation of Statistical Parsers Trained on Small Datasets
Reichart, Roi;Rappoport, Ari;"></td>
	<td class="line x" title="1:197	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 616623, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:197	c2007 Association for Computational Linguistics Self-Training for Enhancement and Domain Adaptation of Statistical Parsers Trained on Small Datasets Roi Reichart ICNC Hebrew University of Jerusalem roiri@cs.huji.ac.il Ari Rappoport Institute of Computer Science Hebrew University of Jerusalem arir@cs.huji.ac.il Abstract Creating large amounts of annotated data to train statistical PCFG parsers is expensive, and the performance of such parsers declines when training and test data are taken from different domains." ></td>
	<td class="line x" title="3:197	In this paper we use selftraining in order to improve the quality of a parser and to adapt it to a different domain, using only small amounts of manually annotated seed data." ></td>
	<td class="line x" title="4:197	We report significant improvement both when the seed and test data are in the same domain and in the outof-domain adaptation scenario." ></td>
	<td class="line x" title="5:197	In particular, we achieve 50% reduction in annotation cost for the in-domain case, yielding an improvement of 66% over previous work, and a 20-33% reduction for the domain adaptation case." ></td>
	<td class="line x" title="6:197	This is the first time that self-training with small labeled datasets is applied successfully to these tasks." ></td>
	<td class="line x" title="7:197	We were also able to formulate a characterization of when selftraining is valuable." ></td>
	<td class="line x" title="8:197	1 Introduction State of the art statistical parsers (Collins, 1999; Charniak, 2000; Koo and Collins, 2005; Charniak and Johnson, 2005) are trained on manually annotated treebanks that are highly expensive to create." ></td>
	<td class="line x" title="9:197	Furthermore, the performance of these parsers decreases as the distance between the genres of their training and test data increases." ></td>
	<td class="line x" title="10:197	Therefore, enhancing the performance of parsers when trained on small manually annotated datasets is of great importance, both when the seed and test data are taken from the same domain (the in-domain scenario) and when they are taken from different domains (the outof-domain or parser adaptation scenario)." ></td>
	<td class="line x" title="11:197	Since the problem is the expense in manual annotation, we define small to be 100-2,000 sentences, which are the sizes of sentence sets that can be manually annotated by constituent structure in a few hours1." ></td>
	<td class="line x" title="12:197	Self-training is a method for using unannotated data when training supervised models." ></td>
	<td class="line x" title="13:197	The model is first trained using manually annotated (seed) data, then the model is used to automatically annotate a pool of unannotated (self-training) data, and then the manually and automatically annotated datasets are combined to create the training data for the final model." ></td>
	<td class="line x" title="14:197	Self-training of parsers trained on small datasets is of enormous potential practical importance, due to the huge amounts of unannotated data that are becoming available today and to the high cost of manual annotation." ></td>
	<td class="line x" title="15:197	In this paper we use self-training to enhance the performance of a generative statistical PCFG parser (Collins, 1999) for both the in-domain and the parser adaptation scenarios, using only small amounts of manually annotated data." ></td>
	<td class="line x" title="16:197	We perform four experiments, examining all combinations of in-domain and out-of-domain seed and self-training data." ></td>
	<td class="line x" title="17:197	Our results show that self-training is of substantial benefit for the problem." ></td>
	<td class="line x" title="18:197	In particular, we present:  50% reduction in annotation cost when the seed and test data are taken from the same domain, which is 66% higher than any previous result with small manually annotated datasets." ></td>
	<td class="line x" title="19:197	1We note in passing that quantitative research on the cost of annotation using various annotation schemes is clearly lacking." ></td>
	<td class="line x" title="20:197	616  The first time that self-training improves a generative parser when the seed and test data are from the same domain." ></td>
	<td class="line x" title="21:197	 20-33% reduction in annotation cost when the seed and test data are from different domains." ></td>
	<td class="line x" title="22:197	 The first time that self-training succeeds in adapting a generative parser between domains using a small manually annotated dataset." ></td>
	<td class="line x" title="23:197	 The first formulation (related to the number of unknown words in a sentence) of when selftraining is valuable." ></td>
	<td class="line x" title="24:197	Section 2 discusses previous work, and Section 3 compares in-depth our protocol to a previous one." ></td>
	<td class="line x" title="25:197	Sections 4 and 5 present the experimental setup and our results, and Section 6 analyzes the results in an attempt to shed light on the phenomenon of selftraining." ></td>
	<td class="line x" title="26:197	2 Related Work Self-training might seem a strange idea: why should a parser trained on its own output learn anything new?" ></td>
	<td class="line x" title="27:197	Indeed, (Clark et al. , 2003) applied selftraining to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance." ></td>
	<td class="line pc" title="28:197	Recently, (McClosky et al. , 2006a; McClosky et al. , 2006b) have successfully applied self-training to various parser adaptation scenarios using the reranking parser of (Charniak and Johnson, 2005)." ></td>
	<td class="line x" title="29:197	A reranking parser (see also (Koo and Collins, 2005)) is a layered model: the base layer is a generative statistical PCFG parser that creates a ranked list of k parses (say, 50), and the second layer is a reranker that reorders these parses using more detailed features." ></td>
	<td class="line oc" title="30:197	McClosky et al (2006a) use sections 2-21 of the WSJ PennTreebank as seed data and between 50K to 2,500K unlabeled NANC corpus sentences as self-training data." ></td>
	<td class="line o" title="31:197	They train the PCFG parser and the reranker with the manually annotated WSJ data, and parse the NANC data with the 50-best PCFG parser." ></td>
	<td class="line o" title="32:197	Then they proceed in two directions." ></td>
	<td class="line o" title="33:197	In the first, they reorder the 50-best parse list with the reranker to create a new 1-best list." ></td>
	<td class="line o" title="34:197	In the second, they leave the 1-best list produced by the generative PCFG parser untouched." ></td>
	<td class="line o" title="35:197	Then they combine the 1-best list (each direction has its own list) with the WSJ training set, to retrain the PCFG parser." ></td>
	<td class="line x" title="36:197	The final PCFG model and the reranker (trained only on annotated WSJ material) are then used to parse the test section (23) of WSJ." ></td>
	<td class="line o" title="37:197	There are two major differences between these papers and the current one, stemming from their usage of a reranker and of large seed data." ></td>
	<td class="line n" title="38:197	First, when their 1-best list of the base PCFG parser was used as self training data for the PCFG parser (the second direction), the performance of the base parser did not improve." ></td>
	<td class="line x" title="39:197	It had improved only when the 1best list of the reranker was used." ></td>
	<td class="line x" title="40:197	In this paper we show how the 1-best list of a base (generative) PCFG parser can be used as a self-training material for the base parser itself and enhance its performance, without using any reranker." ></td>
	<td class="line x" title="41:197	This reveals a noteworthy characteristic of generative PCFG models and offers a potential direction for parser improvement, since the quality of a parser-reranker combination critically depends on that of the base parser." ></td>
	<td class="line n" title="42:197	Second, these papers did not explore self-training when the seed is small, a scenario whose importance has been discussed above." ></td>
	<td class="line x" title="43:197	In general, PCFG models trained on small datasets are less likely to parse the self-training data correctly." ></td>
	<td class="line x" title="44:197	For example, the fscore of WSJ data parsed by the base PCFG parser of (Charniak and Johnson, 2005) when trained on the training sections of WSJ is between 89% to 90%, while the f-score of WSJ data parsed with the Collins model that we use, and a small seed, is between 40% and 80%." ></td>
	<td class="line nc" title="45:197	As a result, the good results of (McClosky et al, 2006a; 2006b) with large seed sets do not immediately imply success with small seed sets." ></td>
	<td class="line x" title="46:197	Demonstration of such success is a contribution of the present paper." ></td>
	<td class="line x" title="47:197	Bacchiani et al (2006) explored the scenario of out-of-domain seed data (the Brown training set containing about 20K sentences) and in-domain self-training data (between 4K to 200K sentences from the WSJ) and showed an improvement over the baseline of training the parser with the seed data only." ></td>
	<td class="line x" title="48:197	However, they did not explore the case of small seed datasets (the effort in manually annotating 20K is substantial) and their work addresses only one of our scenarios (OI, see below)." ></td>
	<td class="line x" title="49:197	617 A work closely related to ours is (Steedman et al. , 2003a), which applied co-training (Blum and Mitchell, 1998) and self-training to Collins parsing model using a small seed dataset (500 sentences for both methods and 1,000 sentences for co-training only)." ></td>
	<td class="line x" title="50:197	The seed, self-training and test datasets they used are similar to those we use in our II experiment (see below), but the self-training protocols are different." ></td>
	<td class="line x" title="51:197	They first train the parser with the seed sentences sampled from WSJ sections 2-21." ></td>
	<td class="line x" title="52:197	Then, iteratively, 30 sentences are sampled from these sections, parsed by the parser, and the 20 best sentences (in terms of parser confidence defined as probability of top parse) are selected and combined with the previously annotated data to retrain the parser." ></td>
	<td class="line x" title="53:197	The cotraining protocol is similar except that each parser is trained with the 20 best sentences of the other parser." ></td>
	<td class="line x" title="54:197	Self-training did not improve parser performance on the WSJ test section (23)." ></td>
	<td class="line x" title="55:197	Steedman et al (2003b) followed a similar co-training protocol except that the selection function (three functions were explored) considered the differences between the confidence scores of the two parsers." ></td>
	<td class="line x" title="56:197	In this paper we show a self-training protocol that achieves better results than all of these methods (Table 2)." ></td>
	<td class="line x" title="57:197	The next section discusses possible explanations for the difference in results." ></td>
	<td class="line x" title="58:197	Steedman et al (2003b) and Hwa et al, (2003) also used several versions of corrected co-training which are not comparable to ours and other suggested methods because their evaluation requires different measures (e.g. reviewed and corrected constituents are separately counted)." ></td>
	<td class="line x" title="59:197	As far as we know, (Becker and Osborne, 2005) is the only additional work that tries to improve a generative PCFG parsers using small seed data." ></td>
	<td class="line x" title="60:197	The techniques used are based on active learning (Cohn et al. , 1994)." ></td>
	<td class="line x" title="61:197	The authors test two novel methods, along with the tree entropy (TE) method of (Hwa, 2004)." ></td>
	<td class="line x" title="62:197	The seed, the unannotated and the test sets, as well as the parser used in that work, are similar to those we use in our II experiment." ></td>
	<td class="line x" title="63:197	Our results are superior, as shown in Table 3." ></td>
	<td class="line x" title="64:197	3 Self-Training Protocols There are many possible ways to do self-training." ></td>
	<td class="line x" title="65:197	A main goal of this paper is to identify a selftraining protocol most suitable for enhancement and domain adaptation of statistical parsers trained on small datasets." ></td>
	<td class="line x" title="66:197	No previous work has succeeded in identifying such a protocol for this task." ></td>
	<td class="line x" title="67:197	In this section we try to understand why." ></td>
	<td class="line x" title="68:197	In the protocol we apply, the self-training set contains several thousand sentences A parser trained with a small seed set parses the self-training set, and then the whole automatically annotated self-training set is combined with the manually annotated seed set to retrain the parser." ></td>
	<td class="line x" title="69:197	This protocol and that of Steedman et al (2003a) were applied to the problem, with the same seed, self-training and test sets." ></td>
	<td class="line x" title="70:197	As we show below (see Section 4 and Section 5), while Steedmans protocol does not improve over the baseline of using only the seed data, our protocol does." ></td>
	<td class="line x" title="71:197	There are four differences between the protocols." ></td>
	<td class="line x" title="72:197	First, Steedman et als seed set consists of consecutive WSJ sentences, while we select them randomly." ></td>
	<td class="line x" title="73:197	In the next section we show that this difference is immaterial." ></td>
	<td class="line x" title="74:197	Second, Steedman et als protocol looks for sentences of high quality parse, while our protocol prefers to use many sentences without checking their parse quality." ></td>
	<td class="line x" title="75:197	Third, their protocol is iterative while ours uses a single step." ></td>
	<td class="line x" title="76:197	Fourth, our selftraining set is orders of magnitude larger than theirs." ></td>
	<td class="line x" title="77:197	To examine the parse quality issue, we performed their experiment using their setting but selecting the high quality parse sentences using their f-score relative to the gold standard annotation from secs 221 rather than a quality estimate." ></td>
	<td class="line x" title="78:197	No improvement over the baseline was achieved even with this oracle." ></td>
	<td class="line x" title="79:197	Thus the problem with their protocol does not lie with the parse quality assessment function; no other function would produce results better than the oracle." ></td>
	<td class="line x" title="80:197	To examine the iteration issue, we performed their experiment in a single step, selecting at once the oracle-best 2,000 among 3,000 sentences2, which produced only a mediocre improvement." ></td>
	<td class="line x" title="81:197	We thus conclude that the size of the self-training set is a major factor responsible for the difference between the protocols." ></td>
	<td class="line x" title="82:197	4 Experimental Setup We used a reimplementation of Collins parsing model 2 (Bikel, 2004)." ></td>
	<td class="line x" title="83:197	We performed four experiments, II, IO, OI, and OO, two with in-domain seed 2Corresponding to a 100 iterations of 30 sentences each." ></td>
	<td class="line x" title="84:197	618 (II, IO) and two with out-of-domain seed (OI, OO), examining in-domain self-training (II, OI) and outof-domain self-training (IO, OO)." ></td>
	<td class="line x" title="85:197	Note that being in or out of domain is determined by the test data." ></td>
	<td class="line x" title="86:197	Each experiment contained 19 runs." ></td>
	<td class="line x" title="87:197	In each run a different seed size was used, from 100 sentences onwards, in steps of 100." ></td>
	<td class="line x" title="88:197	For statistical significance, we repeated each experiment five times, in each repetition randomly sampling different manually annotated sentences to form the seed dataset3." ></td>
	<td class="line x" title="89:197	The seed data were taken from WSJ sections 221." ></td>
	<td class="line x" title="90:197	For II and IO, the test data is WSJ section 23 (2416 sentences) and the self-training data are either WSJ sections 2-21 (in II, excluding the seed sentences) or the Brown training section (in IO)." ></td>
	<td class="line x" title="91:197	For OI and OO, the test data is the Brown test section (2424 sentences), and the self-training data is either the Brown training section (in OI) or WSJ sections 2-21 (in OO)." ></td>
	<td class="line x" title="92:197	We removed the manual annotations from the self-training sections before using them." ></td>
	<td class="line oc" title="93:197	For the Brown corpus, we based our division on (Bacchiani et al. , 2006; McClosky et al. , 2006b)." ></td>
	<td class="line x" title="94:197	The test and training sections consist of sentences from all of the genres that form the corpus." ></td>
	<td class="line x" title="95:197	The training division consists of 90% (9 of each 10 consecutive sentences) of the data, and the test section are the remaining 10% (We did not use any held out data)." ></td>
	<td class="line x" title="96:197	Parsing performance is measured by f-score, f = 2PRP+R, where P,R are labeled precision and recall." ></td>
	<td class="line x" title="97:197	To further demonstrate our results for parser adaptation, we also performed the OI experiment where seed data is taken from WSJ sections 2-21 and both self-training and test data are taken from the Switchboard corpus." ></td>
	<td class="line x" title="98:197	The distance between the domains of these corpora is much greater than the distance between the domains of WSJ and Brown." ></td>
	<td class="line x" title="99:197	The Brown and Switchboard corpora were divided to sections in the same way." ></td>
	<td class="line x" title="100:197	We have also performed all four experiments with the seed data taken from the Brown training section." ></td>
	<td class="line x" title="101:197	3 (Steedman et al. , 2003a) used the first 500 sentences of WSJ training section as seed data." ></td>
	<td class="line x" title="102:197	For direct comparison, we performed our protocol in the II scenario using the first 500 or 1000 sentences of WSJ training section as seed data and got similar results to those reported below for our protocol with random selection." ></td>
	<td class="line x" title="103:197	We also applied the protocol of Steedman et al to scenario II with 500 randomly selected sentences, getting no improvement over the random baseline." ></td>
	<td class="line x" title="104:197	The results were very similar and will not be detailed here due to space constraints." ></td>
	<td class="line x" title="105:197	5 Results 5.1 In-domain seed data In these two experiments we show that when the seed and test data are taken from the same domain, a very significant enhancement of parser performance can be achieved, whether the self-training material is in-domain (II) or out-of-domain (IO)." ></td>
	<td class="line x" title="106:197	Figure 1 shows the improvement in parser f-score when selftraining data is used, compared to when it is not used." ></td>
	<td class="line x" title="107:197	Table 1 shows the reduction in manually annotated seed data needed to achieve certain f-score levels." ></td>
	<td class="line x" title="108:197	The enhancement in performance is very impressive in the in-domain self-training data scenario  a reduction of 50% in the number of manually annotated sentences needed for achieving 75 and 80 fscore values." ></td>
	<td class="line x" title="109:197	A significant improvement is achieved in the out-of-domain self-training scenario as well." ></td>
	<td class="line x" title="110:197	Table 2 compares our results with self-training and co-training results reported by (Steedman et al, 20003a; 2003b)." ></td>
	<td class="line x" title="111:197	As stated earlier, the experimental setup of these works is similar to ours, but the selftraining protocols are different." ></td>
	<td class="line x" title="112:197	For self-training, our II improves an absolute 3.74% over their 74.3% result, which constitutes a 14.5% reduction in error (from 25.7%)." ></td>
	<td class="line x" title="113:197	The table shows that for both seed sizes our self training protocol outperforms both the selftraining and co-training protocols of (Steedman et al, 20003a; 2003b)." ></td>
	<td class="line x" title="114:197	Results are not included in the table only if they are not reported in the relevant paper." ></td>
	<td class="line x" title="115:197	The self-training protocol of (Steedman et al. , 2003a) does not actually improve over the baseline of using only the seed data." ></td>
	<td class="line x" title="116:197	Section 3 discussed a possible explanation to the difference in results." ></td>
	<td class="line x" title="117:197	In Table 3 we compare our results to the results of the methods tested in (Becker and Osborne, 2005) (including TE)4." ></td>
	<td class="line x" title="118:197	To do that, we compare the reduction in manually annotated data needed to achieve an f-score value of 80 on WSJ section 23 achieved by each method." ></td>
	<td class="line x" title="119:197	We chose this measure since it is 4The measure is constituents and not sentences because this is how results are reported in (Becker and Osborne, 2005)." ></td>
	<td class="line x" title="120:197	However, the same reduction is obtained when sentences are counted, because the number of constituents is averaged when taking many sentences." ></td>
	<td class="line x" title="121:197	619 f-score 75 80 Seed data only 600(0%) 1400(0%) II 300(50%) 700(50%) IO 500(17%) 1200(14.5%) Table 1: Number of in-domain seed sentences needed for achieving certain f-scores." ></td>
	<td class="line x" title="122:197	Reductions compared to no self-training (line 1) are given in parentheses." ></td>
	<td class="line x" title="123:197	Seed size our II our IO Steedman ST Steedman CT Steedman CT 2003a 2003b 500 sent." ></td>
	<td class="line x" title="124:197	78.04 75.81 74.3 76.9 1,000 sent." ></td>
	<td class="line x" title="125:197	81.43 79.49 79 81.2 Table 2: F-scores of our in-domain-seed selftraining vs. self-training (ST) and co-training (CT) of (Steedman et al, 20003a; 2003b)." ></td>
	<td class="line x" title="126:197	the only explicitly reported number in that work." ></td>
	<td class="line x" title="127:197	As the table shows, our method is superior: our reduction of 50% constitutes an improvement of 66% over their best reduction of 30.6%." ></td>
	<td class="line x" title="128:197	When applying self-training to a parser trained with a small dataset we expect the coverage of the parser to increase, since the combined training set should contain items that the seed dataset does not." ></td>
	<td class="line x" title="129:197	On the other hand, since the accuracy of annotation of such a parser is poor (see the no self-training curve in Figure 1) the combined training set surely includes inaccurate labels that might harm parser performance." ></td>
	<td class="line x" title="130:197	Figure 2 (left) shows the increase in coverage achieved for in-domain and out-of-domain self-training data." ></td>
	<td class="line x" title="131:197	The improvements induced by both methods are similar." ></td>
	<td class="line x" title="132:197	This is quite surprising given that the Brown sections we used as selftraining data contain science, fiction, humor, romance, mystery and adventure texts while the test section in these experiments, WSJ section 23, contains only news articles." ></td>
	<td class="line x" title="133:197	Figure 2 also compares recall (middle) and precision (right) for the different methods." ></td>
	<td class="line x" title="134:197	For II there is a significant improvement in both precision and recall even though many more sentences are parsed." ></td>
	<td class="line x" title="135:197	For IO, there is a large gain in recall and a much smaller loss in precision, yielding a substantial improvement in f-score (Figure 1)." ></td>
	<td class="line x" title="136:197	F score This work II Becker unparsed Becker entropy/unparsed Hwa TE 80 50% 29.4% 30.6% -5.7% Table 3: Reduction of the number of manually annotated constituents needed for achieving f score value of 80 on section 23 of the WSJ." ></td>
	<td class="line x" title="137:197	In all cases the seed and additional sentences selected to train the parser are taken from sections 02-21 of WSJ." ></td>
	<td class="line x" title="138:197	5.2 Out-of-domain seed data In these two experiments we show that self-training is valuable for adapting parsers from one domain to another." ></td>
	<td class="line x" title="139:197	Figure 3 compares out-of-domain seed data used with in-domain (OI) or out-of-domain (OO) self-training data against the baseline of training only with the out-of-domain seed data." ></td>
	<td class="line x" title="140:197	The left graph shows a significant improvement in f-score." ></td>
	<td class="line x" title="141:197	In the middle and right graphs we examine the quality of the parses produced by the model by plotting recall and precision vs. seed size." ></td>
	<td class="line x" title="142:197	Regarding precision, the difference between the three conditions is small relative to the f-score difference shown in the left graph." ></td>
	<td class="line x" title="143:197	The improvement in the recall measure is much greater than the precision differences, and this is reflected in the f-score result." ></td>
	<td class="line x" title="144:197	The gain in coverage achieved by both methods, which is not shown in the figure, is similar to that reported for the in-domain seed experiments." ></td>
	<td class="line x" title="145:197	The left graph along with the increase in coverage show the power of self-training in parser adaptation when small seed datasets are used: not only do OO and OI parse many more sentences than the baseline, but their f-score values are consistently better." ></td>
	<td class="line x" title="146:197	To see how much manually annotated data can be saved by using out-of-domain seed, we train the parsing model with manually annotated data from the Brown training section, as described in Section 4." ></td>
	<td class="line x" title="147:197	We assume that given a fixed number of training sentences the best performance of the parser without self-training will occur when these sentences are selected from the domain of the test section, the Brown corpus." ></td>
	<td class="line x" title="148:197	We compare the amounts of manually annotated data needed to achieve certain fscore levels in this condition with the corresponding amounts of data needed by OI and OO." ></td>
	<td class="line x" title="149:197	The results are summarized in Table 4." ></td>
	<td class="line x" title="150:197	We compare to two baselines using inand out-of-domain seed data without 620 0 200 400 600 800 100040 50 60 70 80 90 number of manually annotated sentences f score no self training wsj selftraining brown selftraining 1000 1200 1400 1600 1800 200078 79 80 81 82 83 84 number of manually annotated sentences f score no selftraining wsj selftraining brown selftraining Figure 1: Number of seed sentences vs. f-score, for the two in-domain seed experiments: II (triangles) and IO (squares), and for the no self-training baseline." ></td>
	<td class="line x" title="151:197	Self-training provides a substantial improvement." ></td>
	<td class="line x" title="152:197	0 500 1000 1500 20001000 1500 2000 2500 number of manually annotated sentences number of covered sentences no selftraining wsj selftraining brown selftraining 0 500 1000 1500 200020 40 60 80 100 number of manually annotated sentences recall no selftraining wsj selftraining brown selftraining 0 500 1000 1500 200065 70 75 80 85 number of manually annotated sentences precision no selftraining wsj selftraining brown selftraining Figure 2: Number of seed sentences vs. coverage (left), recall (middle) and precision (right) for the two in-domain seed experiments: II (triangles) and IO (squares), and for the no self-training baseline." ></td>
	<td class="line x" title="153:197	any self-training." ></td>
	<td class="line x" title="154:197	The second line (ID) serves as a reference to compute how much manual annotation of the test domain was saved, and the first line (OD) serves as a reference to show by how much selftraining improves the out-of-domain baseline." ></td>
	<td class="line x" title="155:197	The table stops at an f-score of 74 because that is the best that the baselines can do." ></td>
	<td class="line x" title="156:197	A significant reduction in annotation cost over the ID baseline is achieved where the seed size is between 100 and 1200." ></td>
	<td class="line x" title="157:197	Improvement over the OD baseline is for the whole range of seed sizes." ></td>
	<td class="line x" title="158:197	Both OO and OI achieve 20-33% reduction in manual annotation compared to the ID baseline and enhance the performance of the parser by as much as 42.9%." ></td>
	<td class="line x" title="159:197	The only previous work that adapts a parser trained on a small dataset between domains is that of (Steedman et al. , 2003a), which used co-training (no self-training results were reported there or elsewhere)." ></td>
	<td class="line x" title="160:197	In order to compare with that work, we performed OI with seed taken from the Brown corpus and self-training and test taken from WSJ, which is the setup they use, obtaining a similar improvement to that reported there." ></td>
	<td class="line x" title="161:197	However, co-training is a more complex method that requires an additional parser (LTAG in their case)." ></td>
	<td class="line x" title="162:197	To further substantiate our results for the parser adaptation scenario, we used an additional corpus, Switchboard." ></td>
	<td class="line x" title="163:197	Figure 4 shows the results of an OI experiment with WSJ seed and Switchboard selftraining and test data." ></td>
	<td class="line x" title="164:197	Although the domains of these two corpora are very different (more so than WSJ and Brown), self-training provides a substantial improvement." ></td>
	<td class="line x" title="165:197	We have also performed all four experiments with Brown and WSJ trading places." ></td>
	<td class="line x" title="166:197	The results obtained were very similar to those reported here, and will not be detailed due to lack of space." ></td>
	<td class="line x" title="167:197	6 Analysis In this section we try to better understand the benefit in using self-training with small seed datasets." ></td>
	<td class="line x" title="168:197	We formulate the following criterion: the number of words in a test sentence that do not appear in the seed data (unknown words) is a strong indicator 621 0 500 1000 1500 200030 40 50 60 70 80 number of manually annotated sentences f score no selftraining wsj selftraining brown selftraining 0 500 1000 1500 200020 30 40 50 60 70 80 number of manually annotated sentences recall no selftraining wsj selftraining brown selftraining 0 500 1000 1500 200072 74 76 78 80 82 number of manually annotated sentences precision no selftraining wsj selftraining brown selftraining Figure 3: Number of seed sentences vs. f-score (left), recall (middle) and precision (right), for the two out-of-domain seed data experiments: OO (triangles) and OI (squares), and for the no self-training baseline." ></td>
	<td class="line x" title="169:197	f-sc." ></td>
	<td class="line x" title="170:197	66 68 70 72 74 OD 600 800 1, 000 1, 400  ID 600 700 800 1, 000 1, 200 OO 400 500 600 800 1100 33, 33 28.6, 37.5 33, 40 20, 42.9 8,  OI 400 500 600 800 1, 300 33, 33 28.6, 37.5 33, 40 20, 42.9 8,  Table 4: Number of manually annotated seed sentences needed for achieving certain f-score values." ></td>
	<td class="line x" title="171:197	The first two lines show the out-of-domain and indomain seed baselines." ></td>
	<td class="line x" title="172:197	The reductions compared to the baselines is given as ID, OD." ></td>
	<td class="line x" title="173:197	0 500 1000 1500 200010 20 30 40 50 number of manually annotated sentences f score switchboard selftraining no selftraining Figure 4: Number of seed sentences vs. f-score, for the OI experiment using WSJ seed data and SwitchBoard self-training and test data." ></td>
	<td class="line x" title="174:197	In spite of the strong dissimilarity between the domains, selftraining provides a substantial improvement." ></td>
	<td class="line x" title="175:197	to whether it is worthwhile to use small seed selftraining." ></td>
	<td class="line x" title="176:197	Figure 5 shows the number of unknown words in a sentence vs. the probability that the selftraining model will parse a sentence no worse (upper curve) or better (lower curve) than the baseline model." ></td>
	<td class="line x" title="177:197	The upper curve shows that regardless of the 0 10 20 30 40 500 0.2 0.4 0.6 0.8 1 number of unknown words probability ST > baseline ST >= baseline Figure 5: For sentences having the same number of unknown words, we show the probability that the self-training model parses a sentence from the set no worse (upper curve) or better (lower curve) than the baseline model." ></td>
	<td class="line x" title="178:197	number of unknown words in the sentence, there is more than 50% chance that the self-training model will not harm the result." ></td>
	<td class="line x" title="179:197	This probability decreases from almost 1 for a very small number of unknown words to about 0.55 for 50 unknown words." ></td>
	<td class="line x" title="180:197	The lower curve shows that when the number of unknown words increases, the probability that the self-training model will do better than the baseline model increases from almost 0 (for a very small number of unknown words) to about 0.55." ></td>
	<td class="line x" title="181:197	Hence, the number of unknown words is an indication for the potential benefit (value on the lower curve) and risk (1 minus the value on the upper curve) in using the self-training model compared to using the baseline model." ></td>
	<td class="line oc" title="182:197	Unknown words were not identified in (McClosky et al. , 2006a) as a useful predictor for the benefit of self-training." ></td>
	<td class="line oc" title="183:197	622 We also identified a length effect similar to that studied by (McClosky et al. , 2006a) for self-training (using a reranker and large seed, as detailed in Section 2)." ></td>
	<td class="line o" title="184:197	Due to space limitations we do not discuss it here." ></td>
	<td class="line x" title="185:197	7 Discussion Self-training is usually not considered to be a valuable technique in improving the performance of generative statistical parsers, especially when the manually annotated seed sentence dataset is small." ></td>
	<td class="line oc" title="186:197	Indeed, in the II scenario, (Steedman et al. , 2003a; McClosky et al. , 2006a; Charniak, 1997) reported no improvement of the base parser for small (500 sentences, in the first paper) and large (40K sentences, in the last two papers) seed datasets respectively." ></td>
	<td class="line nc" title="187:197	In the II, OO, and OI scenarios, (McClosky et al, 2006a; 2006b) succeeded in improving the parser performance only when a reranker was used to reorder the 50-best list of the generative parser, with a seed size of 40K sentences." ></td>
	<td class="line x" title="188:197	Bacchiani et al (2006) improved the parser performance in the OI scenario but their seed size was large (about 20K sentences)." ></td>
	<td class="line x" title="189:197	In this paper we have shown that self-training can enhance the performance of generative parsers, without a reranker, in four inand out-of-domain scenarios using a small seed dataset." ></td>
	<td class="line x" title="190:197	For the II, IO and OO scenarios, we are the first to show improvement by self-training for generative parsers." ></td>
	<td class="line x" title="191:197	We achieved a 50% (20-33%) reduction in annotation cost for the in-domain (out-of-domain) seed data scenarios." ></td>
	<td class="line x" title="192:197	Previous work with small seed datasets considered only the II and OI scenarios." ></td>
	<td class="line x" title="193:197	Our results for the former are better than any previous method, and our results for the latter (which are the first reported self-training results) are similar to previous results for co-training, a more complex method." ></td>
	<td class="line x" title="194:197	We demonstrated our results using three corpora of varying degrees of domain difference." ></td>
	<td class="line x" title="195:197	A direction for future research is combining self-training data from various domains to enhance parser adaptation." ></td>
	<td class="line x" title="196:197	Acknowledgement." ></td>
	<td class="line x" title="197:197	We would like to thank Dan Roth for his constructive comments on this paper." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-1033
Reranking for Biomedical Named-Entity Recognition
Yoshida, Kazuhiro;Tsujii, Jun'ichi;"></td>
	<td class="line x" title="1:179	BioNLP 2007: Biological, translational, and clinical language processing, pages 209??16, Prague, June 2007." ></td>
	<td class="line x" title="2:179	c2007 Association for Computational Linguistics Reranking for Biomedical Named-Entity Recognition Kazuhiro Yoshida??Jun?ichi Tsujii??" ></td>
	<td class="line x" title="3:179	?Department of Computer Science, University of Tokyo ?School of Informatics, University of Manchester ?National Center for Text Mining Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN {kyoshida, tsujii}@is.s.u-tokyo.ac.jp Abstract This paper investigates improvement of automatic biomedical named-entity recognition by applying a reranking method to the COLING 2004 JNLPBA shared task of bioentity recognition." ></td>
	<td class="line x" title="4:179	Our system has a common reranking architecture that consists of a pipeline of two statistical classifiers which are based on log-linear models." ></td>
	<td class="line x" title="5:179	The architecture enables the reranker to take advantage of features which are globally dependent on the label sequences, and features from the labels of other sentences than the target sentence." ></td>
	<td class="line x" title="6:179	The experimental results show that our system achieves the labeling accuracies that are comparable to the best performance reported for the same task, thanks to the 1.55 points of F-score improvement by the reranker." ></td>
	<td class="line x" title="7:179	1 Introduction Difficulty and potential application of biomedical named-entity recognition has attracted many researchers of both natural language processing and bioinformatics." ></td>
	<td class="line x" title="8:179	The difficulty of the task largely stems from a wide variety of named entity expressions used in the domain." ></td>
	<td class="line x" title="9:179	It is common for practical protein or gene databases to contain hundreds of thousands of items." ></td>
	<td class="line x" title="10:179	Such a large variety of vocabulary naturally leads to long names with productive use of general words, making the task difficult to be solved by systems with naive Markov assumption of label sequences, because such systems must perform their prediction without seeing the entire string of the entities." ></td>
	<td class="line x" title="11:179	Importance of the treatment of long names might be implicitly indicated in the performance comparison of the participants of JNLPBA shared task (Kim et al. , 2004), where the best performing system (Zhou and Su, 2004) attains their scores by extensive post-processing, which enabled the system to make use of global information of the entity labels." ></td>
	<td class="line x" title="12:179	After the shared task, many researchers tackled the task by using conditional random fields (CRFs) (Lafferty et al. , 2001), which seemed to promise improvement over locally optimized models like maximum entropy Markov models (MEMMs) (McCallum et al. , 2000)." ></td>
	<td class="line x" title="13:179	However, many of the CRF systems developed after the shared task failed to reach the best performance achieved by Zhou et al. One of the reasons may be the deficiency of the dynamic programming-based systems, that the global information of sequences cannot be incorporated as features of the models." ></td>
	<td class="line x" title="14:179	Another reason may be that the computational complexity of the models prevented the developers to invent effective features for the task." ></td>
	<td class="line x" title="15:179	We had to wait until Tsai et al.(2006), who combine pattern-based postprocessing with CRFs, for CRF-based systems to achieve the same level of performance as Zhou et al. As such, a key to further improvement of the performance of bio-entity recognition has been to employ global features, which are effective to capture the features of long names appearing in the bio domain." ></td>
	<td class="line x" title="17:179	In this paper, we use reranking architecture, which was successfully applied to the task of natural language parsing (Collins, 2000; Charniak and 209 Johnson, 2005), to address the problem." ></td>
	<td class="line x" title="18:179	Reranking enables us to incorporate truly global features to the model of named entity tagging, and we aim to realize the state-of-the-art performance without depending on rule-based post-processes." ></td>
	<td class="line x" title="19:179	Use of global features in named-entity recognition systems is widely studied for sequence labeling including general named-entity tasks like CoNLL 2003 shared task." ></td>
	<td class="line x" title="20:179	Such systems may be classified into two kinds, one of them uses a single classifier which is optimized incorporating non-local features, and the other consists of pipeline of more than one classifiers." ></td>
	<td class="line x" title="21:179	The former includes Relational Markov Networks by Bunescu et al.(2004) and skip-edge CRFs by Sutton et al.(2004)." ></td>
	<td class="line x" title="24:179	A major drawback of this kind of systems may be heavy computational cost of inference both for training and running the systems, because non-local dependency forces such models to use expensive approximate inference instead of dynamic-programming-based exact inference." ></td>
	<td class="line x" title="25:179	The latter, pipelined systems include a recent study by Krishnan et al.(2006), as well as our reranking system." ></td>
	<td class="line x" title="27:179	Their method is a two stage model of CRFs, where the second CRF uses the global information of the output of the first CRF." ></td>
	<td class="line x" title="28:179	Though their method is effective in capturing various non-local dependencies of named entities like consistency of labels, we may be allowed to claim that reranking is likely to be more effective in bioentity tagging, where the treatment of long entity names is also a problem." ></td>
	<td class="line x" title="29:179	This paper is organized as follows." ></td>
	<td class="line x" title="30:179	First, we briefly overview the JNLPBA shared task of bioentity recognition and its related work." ></td>
	<td class="line x" title="31:179	Then we explain the components of our system, one of which is an MEMM n-best tagger, and the other is a reranker based on log-linear models." ></td>
	<td class="line x" title="32:179	Then we show the experiments to tune the performance of the system using the development set." ></td>
	<td class="line x" title="33:179	Finally, we compare our results with the existing systems, and conclude the paper with the discussion for further improvement of the system." ></td>
	<td class="line x" title="34:179	2 JNLPBA shared task and related work This section overviews the task of biomedical named entity recognition as presented in JNLPBA shared task held at COLING 2004, and the systems that were successfully applied to the task." ></td>
	<td class="line x" title="35:179	The training data provided by the shared task consisted of 2000 abstracts of biomedical articles taken from the GENIA corpus version 3 (Ohta et al. , 2002), which consists of the MEDLINE abstracts with publication years from 1990 to 1999." ></td>
	<td class="line x" title="36:179	The articles are annotated with named-entity BIO tags as an example shown in Table 1." ></td>
	<td class="line x" title="37:179	As usual, ?B??and ?I??tags are for beginning and internal words of named entities, and ?O??tags are for general English words that are not named entities." ></td>
	<td class="line x" title="38:179	?B??and ?I??tags are split into 5 sub-labels, each of which are used to represent proteins, genes, cell lines, DNAs, cell types, and RNAs." ></td>
	<td class="line x" title="39:179	The test set of the shared task consists of 404 MEDLINE abstracts whose publication years range from 1978 to 2001." ></td>
	<td class="line x" title="40:179	The difference of publication years between the training and test sets reflects the organizer?s intention to see the entity recognizers??portability with regard to the differences of the articles??publication years." ></td>
	<td class="line x" title="41:179	Kim et al.(Kim et al. , 2004) compare the 8 systems participated in the shared task." ></td>
	<td class="line x" title="43:179	The systems use various classification models including CRFs, hidden Markov models (HMMs), support vector machines (SVMs), and MEMMs, with various features and external resources." ></td>
	<td class="line x" title="44:179	Though it is impossible to observe clear correlation between the performance and classification models or resources used, an important characteristic of the best system by Zhou et al.(2004) seems to be extensive use of rule-based post processing they apply to the output of their classifier." ></td>
	<td class="line x" title="46:179	After the shared task, several researchers tackled the problem using the CRFs and their extensions." ></td>
	<td class="line x" title="47:179	Okanohara et al.(2006) applied semiCRFs (Sarawagi and Cohen, 2004), which can treat multiple words as corresponding to a single state." ></td>
	<td class="line x" title="49:179	Friedrich et al.(2006) used CRFs with features from the external gazetteer." ></td>
	<td class="line x" title="51:179	Current state-of-the-art for the shared-task is achieved by Tsai et al.(2006), whose improvement depends on careful design of features including the normalization of numeric expressions, and use of post-processing by automatically extracted patterns." ></td>
	<td class="line x" title="53:179	210 IL-2 gene expression requires reactive oxygen production by 5-lipoxygenase." ></td>
	<td class="line x" title="54:179	B-DNA I-DNA O O O O O O B-protein O Figure 1: Example sentence from the training data." ></td>
	<td class="line x" title="55:179	State name Possible next state BOS B-* or O B-protein I-protein, B-* or O B-cell type I-cell type, B-* or O B-DNA I-DNA, B-* or O B-cell line I-cell line, B-* or O B-RNA I-RNA, B-* or O I-protein I-protein, B-* or O I-cell type I-cell type, B-* or O I-DNA I-DNA, B-* or O I-cell line I-cell line, B-* or O I-RNA I-RNA, B-* or O O B-* or O Table 1: State transition of MEMM." ></td>
	<td class="line x" title="56:179	3 N-best MEMM tagger As our n-best tagger, we use a first order MEMM model (McCallum et al. , 2000)." ></td>
	<td class="line x" title="57:179	Though CRFs (Lafferty et al. , 2001) can be regarded as improved version of MEMMs, we have chosen MEMMs because MEMMs are usually much faster to train compared to CRFs, which enables extensive feature selection." ></td>
	<td class="line x" title="58:179	Training a CRF tagger with features selected using an MEMM may result in yet another performance boost, but in this paper we concentrate on the MEMM as our n-best tagger, and consider CRFs as one of our future extensions." ></td>
	<td class="line x" title="59:179	Table 1 shows the state transition table of our MEMM model." ></td>
	<td class="line x" title="60:179	Though existing studies suggest that changing the tag set of the original corpus, such as splitting of O tags, can contribute to the performances of named entity recognizers (Peshkin and Pfefer, 2003), our system uses the original tagset of the training data, except that the ?BOS??label is added to represent the state before the beginning of sentences." ></td>
	<td class="line x" title="61:179	Probability of state transition to the i-th label of a sentence is calculated by the following formula: P(li|li??,S) = exp( summationtext j jfj(li,li??,S))summationtext l exp( summationtext j jfj(l,li??,S)) ." ></td>
	<td class="line x" title="62:179	(1) Features used Forward tagging Backward tagging unigrams, bigrams and previous labels (62.43/71.77/66.78) (66.02/74.73/70.10) unigrams and bigrams (61.64/71.73/66.30) (65.38/74.87/69.80) unigrams and previous labels (62.17/71.67/66.58) (65.59/74.77/69.88) unigrams (61.31/71.81/66.15) (65.61/75.25/70.10) Table 2: (Recall/Precision/F-score) of forward and backward tagging." ></td>
	<td class="line x" title="63:179	where li is the next BIO tag, li??" ></td>
	<td class="line x" title="64:179	is the previous BIO tag, S is the target sentence, and fj and lj are feature functions and parameters of a log-linear model (Berger et al. , 1996)." ></td>
	<td class="line x" title="65:179	As a first order MEMM, the probability of a label li is dependent on the previous label li??, and when we calculate the normalization constant in the right hand side (i.e. the denominator of the fraction), we limit the range of l to the possible successors of the previous label." ></td>
	<td class="line x" title="66:179	This probability is multiplied to obtain the probability of a label sequence for a sentence: P(l1n|S) =productdisplay i P(li|li??)." ></td>
	<td class="line x" title="67:179	(2) The probability in Eq." ></td>
	<td class="line x" title="68:179	1." ></td>
	<td class="line x" title="69:179	is estimated as a single log-linear model, regardless to the types of the target labels." ></td>
	<td class="line x" title="70:179	N-best tag sequences of input sentences are obtained by well-known combination of the Viterbi algorithm and A* algorithm." ></td>
	<td class="line x" title="71:179	We implemented two methods for thresholding the best sequences: Nbest takes the sequences whose ranks are higher than N, and -best takes the sequences that have probability higher than that of the best sequences with a factor , where  is a real value between 0 and 1." ></td>
	<td class="line x" title="72:179	The -best method is used in combination with N-best to limit the maximum number of selected sequences." ></td>
	<td class="line x" title="73:179	3.1 Backward tagging There remains one significant choice when we develop an MEMM tagger, that is, the direction of tagging." ></td>
	<td class="line x" title="74:179	The results of the preliminary experiment with 211 forward and backward MEMMs with word unigram and bigram features are shown in Table 2." ></td>
	<td class="line x" title="75:179	(The evaluation is done using the same training and development set as used in Section 5)." ></td>
	<td class="line x" title="76:179	As can be seen, the backward tagging outperformed forward tagging by a margin larger than 3 points, in all the cases." ></td>
	<td class="line x" title="77:179	One of the reasons of these striking differences may be long names which appear in biomedical texts." ></td>
	<td class="line x" title="78:179	In order to recognize long entity names, forward tagging is preferable if we have strong clues of entities which appear around their left boundaries, and backward tagging is preferable if clues appear at right boundaries." ></td>
	<td class="line x" title="79:179	A common example of this effect is a gene expression like ?XXX YYY gene.??The right boundary of this expression is easy to detect because of the word ?gene.??For a backward tagger, the remaining decision is only ?where to stop??the entity." ></td>
	<td class="line x" title="80:179	But a forward tagger must decide not only ?where to start,??but also ?whether to start??the entity, before the tagger encounter the word ?gene.??In biomedical named-entity tagging, right boundaries are usually easier to detect, and it may be the reason of the superiority of the backward tagging." ></td>
	<td class="line x" title="81:179	We could have partially alleviated this effect by employing head-word triggers as done in Zhou et al.(2004), but we decided to use backward tagging because the results of a number of preliminary experiments, including the ones shown in Table 2 above, seemed to be showing that the backward tagging is preferable in this task setting." ></td>
	<td class="line x" title="83:179	3.2 Feature set In our system, features of log-linear models are generated by concatenating (or combining) the ?atomic??" ></td>
	<td class="line x" title="84:179	features, which belong to their corresponding atomic feature classes." ></td>
	<td class="line x" title="85:179	Feature selection is done by deciding whether to include combination of feature classes into the model." ></td>
	<td class="line x" title="86:179	We ensure that features in the same atomic feature class do not co-occur, so that a single feature-class combination generates only one feature for each event." ></td>
	<td class="line x" title="87:179	The following is a list of atomic feature classes implemented in our system." ></td>
	<td class="line x" title="88:179	Label features The target and previous labels." ></td>
	<td class="line x" title="89:179	We also include the coarse-grained label distinction to distinguish five ?I??labels of each entity classes from the other labels, expecting smoothing effect." ></td>
	<td class="line x" title="90:179	Word-based features Surface strings, base forms, parts-of-speech (POSs), word shapes1, suffixes and prefixes of words in input sentence." ></td>
	<td class="line x" title="91:179	These features are extracted from five words around the word to be tagged, and also from the words around NP-chunk boundaries as explained bellow." ></td>
	<td class="line x" title="92:179	Chunk-based features Features dependent on the output of shallow parser." ></td>
	<td class="line x" title="93:179	Word-based features of the beginning and end of noun phrases, and the distances of the target word from the beginning and end of noun phrases are used." ></td>
	<td class="line x" title="94:179	4 Reranker Our reranker is based on a log-linear classifier." ></td>
	<td class="line x" title="95:179	Given n-best tag sequences Li(1 ??i ??n), a loglinear model is used to estimate the probability P(Li|S) = exp( summationtext j jfj(Li,S))summationtext k exp( summationtext j jfj(Lk,S)) ." ></td>
	<td class="line x" title="96:179	(3) From the n-best sequences, reranker selects a sequence which maximize this probability." ></td>
	<td class="line x" title="97:179	The features used by the reranker are explained in the following sections." ></td>
	<td class="line x" title="98:179	Though most of the features are binary-valued (i.e. the value of fj in Eq." ></td>
	<td class="line x" title="99:179	3." ></td>
	<td class="line x" title="100:179	is exclusively 1 or 0), the logarithm of the probability of the sequence output by the n-best tagger is also used as a real-valued feature, to ensure the reranker?s improvement over the n-best tagger." ></td>
	<td class="line x" title="101:179	4.1 Basic features Basic features of the reranker are straightforward extension of the features used in the MEMM tagger." ></td>
	<td class="line x" title="102:179	The difference is that we do not have to care the locality of the features with regard to the labels." ></td>
	<td class="line x" title="103:179	Characteristics of words that are listed as wordbased features in the previous section is also used for the reranker." ></td>
	<td class="line x" title="104:179	Such features are chiefly extracted from around the left and right boundaries of entities." ></td>
	<td class="line x" title="105:179	In our experiments, we used five words around the leftmost and rightmost words of the entities." ></td>
	<td class="line x" title="106:179	We also use the entire string, affixes, word shape, concatenation of POSs, and length of entities." ></td>
	<td class="line x" title="107:179	Some of our 1The shape of a word is defined as a sequence of character types contained in the word." ></td>
	<td class="line x" title="108:179	Character types include uppercase letters, lowercase letters, numerics, space characters, and the other symbols." ></td>
	<td class="line x" title="109:179	212 features depend on two adjacent entities." ></td>
	<td class="line x" title="110:179	Such features include the word-based features of the words between the entities, and the verbs between the entities." ></td>
	<td class="line x" title="111:179	Most of the features are used in combination with entity types." ></td>
	<td class="line x" title="112:179	4.2 N-best distribution features N-best tags of sentences other than the target sentence is available to the rerankers." ></td>
	<td class="line x" title="113:179	This information is sometimes useful for recognizing the names in the target sentence." ></td>
	<td class="line x" title="114:179	For example, proteins are often written as ?XXX protein??where XXX is a protein name, especially when they are first introduced in an article, and thereafter referred to simply as ?XXX.??" ></td>
	<td class="line x" title="115:179	In such cases, the first appearance is easily identified as proteins only by local features, but the subsequent ones might not, and the information of the first appearance can be effectively used to identify the other appearances." ></td>
	<td class="line x" title="116:179	Our system uses the distribution of the tags of the 20 neighboring sentences of the target sentence to help the tagging of the target sentence." ></td>
	<td class="line x" title="117:179	Tag distributions are obtained by marginalizing the nbest tag sequences." ></td>
	<td class="line x" title="118:179	Example of an effective feature is a binary-valued feature which becomes 1 when the candidate entity names in the target sentence is contained in the marginal distribution of the neighboring sentences with a probability which is above some threshold." ></td>
	<td class="line x" title="119:179	We also use the information of overlapping named-entity candidates which appear in the target sentence." ></td>
	<td class="line x" title="120:179	When there is an overlap between the entities in the target sequence and any of the namedentity candidates in the marginal distribution of the target sentence, the corresponding features are used to indicate the existence of the overlapping entity and its entity type." ></td>
	<td class="line x" title="121:179	5 Experiments We evaluated the performance of the system on the data set provided by the COLING 2004 JNLPBA shared-task." ></td>
	<td class="line x" title="122:179	which consists of 2000 abstracts from the MEDLINE articles." ></td>
	<td class="line x" title="123:179	GENIA tagger 2, a biomedical text processing tool which automatically anno2http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/." ></td>
	<td class="line x" title="124:179	The tagger is trained on the GENIA corpus, so it is likely to show very good performance on both training and development sets, but not on the test set." ></td>
	<td class="line x" title="125:179	Features used (Recall/Precision/F-score) full set (73.90/77.58/75.69) w/o shallow parser (72.63/76.35/74.44) w/o previous labels (72.06/75.38/73.68) Table 3: Performance of MEMM tagger." ></td>
	<td class="line x" title="126:179	tates POS tags, shallow parses and named-entity tags is used to preprocess the corpus, and POS and shallow parse information is used in our experiments." ></td>
	<td class="line x" title="127:179	We divided the data into 20 contiguous and equally-sized sections, and used the first 18 sections for training, and the last 2 sections for testing while development (henceforth the training and development sets, respectively)." ></td>
	<td class="line x" title="128:179	The training data of the reranker is created by the n-best tagger, and every set of 17 sections from the training set is used to train the n-best tagger for the remaining section (The same technique is used by previous studies to avoid the n-best tagger?s ?unrealistically good??" ></td>
	<td class="line x" title="129:179	performance on the training set (Collins, 2000))." ></td>
	<td class="line x" title="130:179	Among the n-best sequences output by the MEMM tagger, the sequence with the highest F-score is used as the ?correct??sequence for training the reranker." ></td>
	<td class="line x" title="131:179	The two log-linear models for the MEMM tagger and reranker are estimated using a limited-memory BFGS algorithm implemented in an open-source software Amis3." ></td>
	<td class="line x" title="132:179	In both models, Gaussian prior distributions are used to avoid overfitting (Chen and Rosenfeld, 1999), and the standard deviations of the Gaussian distributions are optimized to maximize the performance on the development set." ></td>
	<td class="line x" title="133:179	We also used a thresholding technique which discards features with low frequency." ></td>
	<td class="line x" title="134:179	This is also optimized using the development set, and the best threshold was 4 for the MEMM tagger, and 50 for the reranker 4." ></td>
	<td class="line x" title="135:179	For both of the MEMM tagger and reranker, combinations of feature classes are manually selected to improve the accuracies on the development set." ></td>
	<td class="line x" title="136:179	Our final models include 49 and 148 feature class combinations for the MEMM tagger and reranker, respectively." ></td>
	<td class="line x" title="137:179	Table 3 shows the performance of the MEMM tagger on the development set." ></td>
	<td class="line x" title="138:179	As reported in many 3http://www-tsujii.is.s.u-tokyo.ac.jp/amis/." ></td>
	<td class="line x" title="139:179	4We treated feature occurrences both in positive and negative examples as one occurrence." ></td>
	<td class="line x" title="140:179	213 Features used (Recall/Precision/F-score) oracle (94.62/96.07/95.34) full set (75.46/78.85/77.12) w/o features that depend on two entities (74.67/77.99/76.29) w/o n-best distribution features (74.99/78.38/76.65) baseline (73.90/77.58/75.69) Table 4: Performance of the reranker." ></td>
	<td class="line x" title="141:179	of the previous studies (Kim et al. , 2004; Okanohara et al. , 2006; Tzong-Han Tsai et al. , 2006), features of shallow parsers had a large contribution to the performance." ></td>
	<td class="line x" title="142:179	The information of the previous labels was also quite effective, which indicates that label unigram models (i.e. 0th order Markov models, so to speak) would have been insufficient for good performance." ></td>
	<td class="line x" title="143:179	Then we developed the reranker, using the results of 50-best taggers as training data." ></td>
	<td class="line x" title="144:179	Table 4 shows the performance of the reranker pipelined with the 50best MEMM tagger, where the ?oracle??row shows the upper bound of reranker performance." ></td>
	<td class="line x" title="145:179	Here, we can observe that the reranker successfully improved the performance by 1.43 points from the baseline (i.e. the one-best of the MEMM tagger)." ></td>
	<td class="line x" title="146:179	It is also shown that the global features that depend on two adjacent entities, and the n-best distribution features from the outside of the target sentences, are both contributing to this performance improvement." ></td>
	<td class="line x" title="147:179	We also conducted experimental comparison of two thresholding methods which are described in Section 3." ></td>
	<td class="line x" title="148:179	Since we can train and test the reranker with MEMM taggers that use different thresholding methods, we could make a table of the performance of the reranker, changing the MEMM tagger used for both training and evaluation5." ></td>
	<td class="line x" title="149:179	Tables 5 and 6 show the F-scores obtained by various MEMM taggers, where the ?oracle??column again shows the performance upper bound." ></td>
	<td class="line x" title="150:179	(All of the -best methods are combined with 200-best thresholding)." ></td>
	<td class="line x" title="151:179	Though we can roughly state that the reranker can work better with n-best taggers which 5These results might not be a fair comparison, because the feature selection and hyper-parameter tuning are done using a reranker which is trained and tested with a 50-best tagger." ></td>
	<td class="line x" title="152:179	are more ambiguous than those used for their training, the differences are so slight to see clear tendencies (For example, the columns for the reranker trained using the 10-best MEMM tagger seems to be a counter example against the statement)." ></td>
	<td class="line x" title="153:179	We may also be able to say that the -best methods are generally performing slightly better, and it could be explained by the fact that we have better oracle performance with less ambiguity in -best methods." ></td>
	<td class="line x" title="154:179	However, the scores in the column corresponding to the 50-best training seems to be as high as any of the scores of the -best methods, and the best score is also achieved in that column." ></td>
	<td class="line x" title="155:179	The reason may be because our performance tuning is done exclusively using the 50-best-trained reranker." ></td>
	<td class="line x" title="156:179	Though we could have achieved better performance by doing feature selection and hyper-parameter tuning again using best MEMMs, we use the reranker trained on 50best tags run with 70-best MEMM tagger as the best performing system in the following." ></td>
	<td class="line x" title="157:179	5.1 Comparison with existing systems Table 7 shows the performance of our n-best tagger and reranker on the official test set, and the best reported results on the same task." ></td>
	<td class="line x" title="158:179	As naturally expected, our system outperformed the systems that cannot accommodate truly global features (Note that one point of F-score improvement is valuable in this task, because inter-annotator agreement rate of human experts in bio-entity recognition is likely to be about 80%." ></td>
	<td class="line x" title="159:179	For example, Krauthammer et al.(2004) report the inter-annotater agreement rate of 77.6% for the three way bio-entity classification task)." ></td>
	<td class="line x" title="161:179	and the performance can be said to be at the same level as the best systems." ></td>
	<td class="line x" title="162:179	However, in spite of our effort, our system could not outperform the best result achieved by Tsai et al. What makes Tsai et al.?s system perform better than ours might be the careful treatment of numeric expressions." ></td>
	<td class="line x" title="163:179	It is also notable that our MEMM tagger scored 71.10, which is comparable to the results of the systems that use CRFs." ></td>
	<td class="line x" title="164:179	Considering the fact that the tagger?s architecture is a simple first-order MEMM which is far from state-of-the-art, and it uses only POS taggers and shallow parsers as external resources, we can say that simple machine-learningbased method with carefully selected features could 214 Thresholding method for training Thresholding method for testing oracle avg." ></td>
	<td class="line x" title="165:179	# of an-swers 10-best 20-best 30-best 40-best 50-best 70-best 100-best 10-best 91.00 10 76.51 76.53 76.85 76.73 77.01 76.68 76.86 20-best 93.31 20 76.40 76.55 76.83 76.62 76.95 76.68 76.85 30-best 94.40 30 76.34 76.52 76.91 76.63 77.06 76.75 76.90 40-best 94.94 40 76.39 76.58 76.91 76.71 77.14 76.75 76.92 50-best 95.34 50 76.37 76.58 76.90 76.65 77.12 76.78 76.92 70-best 95.87 60 76.38 76.57 76.91 76.71 77.16 76.81 76.97 100-best 96.26 70 76.38 76.59 76.95 76.74 77.10 76.82 76.98 Table 5: Comparison of the F-scores of rerankers trained and evaluated with various N-best taggers." ></td>
	<td class="line x" title="166:179	Thresholding method for training Thresholding method for testing oracle avg." ></td>
	<td class="line x" title="167:179	# of answers 0.05-best 0.02-best 0.008-best 0.004-best 0.002-best 0.0005-best 0.0002-best 0.05-best 91.65 10.7 76.70 76.80 76.93 76.64 77.02 76.78 76.52 0.02-best 93.45 17.7 76.79 76.91 77.07 76.79 77.09 76.89 76.70 0.008-best 94.81 27.7 76.79 77.01 77.05 76.80 77.14 76.88 76.73 0.004-best 95.55 37.5 76.79 76.98 76.97 76.74 77.12 76.86 76.71 0.002-best 96.09 49.3 76.79 76.98 76.96 76.73 77.13 76.85 76.72 0.0005-best 96.82 77.7 76.79 76.98 76.96 76.73 77.13 76.85 76.70 0.0002-best 97.04 99.2 76.83 77.01 76.96 76.71 77.13 76.88 76.70 Table 6: Comparison of the F-scores of rerankers trained and evaluated with various -best taggers." ></td>
	<td class="line x" title="168:179	F-score Method 71.10 MEMMThis paper 72.65 reranking Tsai et al.(2006) 72.98 CRF, post-processing Zhou et al.(2004) 72.55 HMM, SVM, postprocessing, gazetteer Friedrich et al.(2006) 71.5 CRF,gazetteer Okanohara et al.(2006) 71.48 semi-CRF Table 7: Performance comparison on the test set." ></td>
	<td class="line x" title="173:179	be sufficient practical solutions for this kind of tasks." ></td>
	<td class="line x" title="174:179	6 Conclusion This paper showed that the named-entity recognition, which have usually been solved by dynamicprogramming-based sequence-labeling techniques with local features, can have innegligible performance improvement from reranking methods." ></td>
	<td class="line x" title="175:179	Our system showed clear improvement over many of the machine-learning-based systems reported to date, and also proved comparable to the existing state-ofthe-art systems that use rule-based post-processing." ></td>
	<td class="line x" title="176:179	Our future plans include further sophistication of features, such as the use of external gazetteers which is reported to improve the F-score by 1.0 and 2.7 points in (Zhou and Su, 2004) and (Friedrich et al. , 2006), respectively." ></td>
	<td class="line x" title="177:179	We expect that reranking architecture can readily accommodate dictionarybased features, because we can apply elaborated string-matching algorithms to the qualified candidate strings available at reranking phase." ></td>
	<td class="line pc" title="178:179	We also plan to apply self-training of n-best tagger which successfully boosted the performance of one of the best existing English syntactic parser (McClosky et al. , 2006)." ></td>
	<td class="line x" title="179:179	Since the test data of the shared-task consists of articles that represent the different publication years, the effects of the publication years of the texts used for self-training would be interesting to study." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1071
When is Self-Training Effective for Parsing?
McClosky, David;Charniak, Eugene;Johnson, Mark;"></td>
	<td class="line x" title="1:236	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 561568 Manchester, August 2008 When is Self-Training Effective for Parsing?" ></td>
	<td class="line pc" title="2:236	David McClosky, Eugene Charniak, and Mark Johnson Brown Laboratory for Linguistic Information Processing (BLLIP) Brown University Providence, RI 02912 {dmcc|ec|mj}@cs.brown.edu Abstract Self-training has been shown capable of improving on state-of-the-art parser performance (McClosky et al., 2006) despite the conventional wisdom on the matter and several studies to the contrary (Charniak, 1997; Steedman et al., 2003)." ></td>
	<td class="line x" title="3:236	However, it has remained unclear when and why selftraining is helpful." ></td>
	<td class="line x" title="4:236	In this paper, we test four hypotheses (namely, presence of a phase transition, impact of search errors, value of non-generative reranker features, and effects of unknown words)." ></td>
	<td class="line x" title="5:236	From these experiments, we gain a better understanding of why self-training works for parsing." ></td>
	<td class="line x" title="6:236	Since improvements from selftraining are correlated with unknown bigrams and biheads but not unknown words, the benefit of self-training appears most influenced by seeing known words in new combinations." ></td>
	<td class="line x" title="7:236	1 Introduction Supervised statistical parsers attempt to capture patterns of syntactic structure from a labeled set of examples for the purpose of annotating new sentences with their structure (Bod, 2003; Charniak and Johnson, 2005; Collins and Koo, 2005; Petrov et al., 2006; Titov and Henderson, 2007)." ></td>
	<td class="line x" title="8:236	These annotations can be used by various higher-level applications such as semantic role labeling (Pradhan et al., 2007) and machine translation (Yamada and Knight, 2001)." ></td>
	<td class="line x" title="9:236	c2008." ></td>
	<td class="line x" title="10:236	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="11:236	Some rights reserved." ></td>
	<td class="line x" title="12:236	However, labeled training data is expensive to annotate." ></td>
	<td class="line x" title="13:236	Given the large amount of unlabeled text available for many domains and languages, techniques which allow us to use both labeled and unlabeled text to train our models are desirable." ></td>
	<td class="line x" title="14:236	These methods are called semi-supervised." ></td>
	<td class="line x" title="15:236	Selftraining is a specific type of semi-supervised learning." ></td>
	<td class="line x" title="16:236	In self-training, first we train a model on the labeled data and use that model to label the unlabeled data." ></td>
	<td class="line x" title="17:236	From the combination of our original labeled data and the newly labeled data, we train a second model  our self-trained model." ></td>
	<td class="line x" title="18:236	The process can be iterated, where the self-trained model is used to label new data in the next iteration." ></td>
	<td class="line x" title="19:236	One can think of self-training as a simple case of cotraining (Blum and Mitchell, 1998) using a single learner instead of several." ></td>
	<td class="line x" title="20:236	Alternatively, one can think of it as one step of the Viterbi EM algorithm." ></td>
	<td class="line pc" title="21:236	Studies prior to McClosky et al.(2006) failed to show a benefit to parsing from self-training (Charniak, 1997; Steedman et al., 2003)." ></td>
	<td class="line n" title="23:236	While the recent success of self-training has demonstrated its merit, it remains unclear why self-training helps in some cases but not others." ></td>
	<td class="line x" title="24:236	Our goal is to better understand when and why self-training is beneficial." ></td>
	<td class="line x" title="25:236	In Section 2, we discuss the previous applications of self-training to parsing." ></td>
	<td class="line x" title="26:236	Section 3 describes our experimental setup." ></td>
	<td class="line x" title="27:236	We present and test four hypotheses of why self-training helps in Section 4 and conclude with discussion and future work in Section 5." ></td>
	<td class="line x" title="28:236	2 Previous Work To our knowledge, the first reported uses of selftraining for parsing are by Charniak (1997)." ></td>
	<td class="line x" title="29:236	He used his parser trained on the Wall Street Journal (WSJ, Mitch Marcus et al.(1993)) to parse 30 million words of unparsed WSJ text." ></td>
	<td class="line x" title="31:236	He then trained 561 a self-trained model from the combination of the newly parsed text with WSJ training data." ></td>
	<td class="line x" title="32:236	However, the self-trained model did not improve on the original model." ></td>
	<td class="line x" title="33:236	Self-training and co-training were subsequently investigated in the 2002 CLSP Summer Workshop at Johns Hopkins University (Steedman et al., 2003)." ></td>
	<td class="line x" title="34:236	They considered several different parameter settings, but in all cases, the number of sentences parsed per iteration of self-training was relatively small (30 sentences)." ></td>
	<td class="line x" title="35:236	They performed many iterations of self-training." ></td>
	<td class="line x" title="36:236	The largest seed size (amount of labeled training data) they used was 10,000 sentences from WSJ, though many experiments used only 500 or 1,000 sentences." ></td>
	<td class="line x" title="37:236	They found that under these parameters, self-training did not yield a significant gain." ></td>
	<td class="line x" title="38:236	Reichart and Rappoport (2007) showed that one can self-train with only a generative parser if the seed size is small." ></td>
	<td class="line x" title="39:236	The conditions are similar to Steedman et al.(2003), but only one iteration of self-training is performed (i.e. all unlabeled data is labeled at once).1 In this scenario, unknown words (words seen in the unlabeled data but not in training) were a useful predictor of when self-training improves performance." ></td>
	<td class="line oc" title="41:236	McClosky et al.(2006) showed that self-training improves parsing accuracy when the two-stage Charniak and Johnson (2005) reranking parser is used." ></td>
	<td class="line x" title="43:236	Using both stages (a generative parser and discriminative reranker) to label the unlabeled data set is necessary to improve performance." ></td>
	<td class="line x" title="44:236	Only retraining the first stage had a positive effect." ></td>
	<td class="line x" title="45:236	However, after retraining the first stage, both stages produced better parses." ></td>
	<td class="line x" title="46:236	Unlike Steedman et al.(2003), the training seed size is large and only one iteration of self-training is performed." ></td>
	<td class="line x" title="48:236	Error analysis revealed that most improvement comes from sentences with lengths between 20 and 40 words." ></td>
	<td class="line x" title="49:236	Surprisingly, improvements were also correlated with the number of conjunctions but not with the number of unknown words in the sentence." ></td>
	<td class="line x" title="50:236	To summarize, several factors have been identified as good predictors of when self-training improves performance, but a full explanation of why self-training works is lacking." ></td>
	<td class="line x" title="51:236	Previous work establishes that parsing all unlabeled sentences at once (rather than over many iterations) is important for successful self-training." ></td>
	<td class="line x" title="52:236	The full effect of 1Performing multiple iterations presumably fails because the parsing models become increasingly biased." ></td>
	<td class="line x" title="53:236	However, this remains untested in the large seed case." ></td>
	<td class="line x" title="54:236	seed size and the reranker on self-training is not well understood." ></td>
	<td class="line x" title="55:236	3 Experimental Setup We use the Charniak and Johnson reranking parser (outlined below), though we expect many of these results to generalize to other generative parsers and discriminative rerankers." ></td>
	<td class="line x" title="56:236	Our corpora consist of WSJ for labeled data and NANC (North American News Text Corpus, Graff (1995)) for unlabeled data." ></td>
	<td class="line x" title="57:236	We use the standard WSJ division for parsing: sections 2-21 for training (39,382 sentences) and section 24 for development (1,346 sentences)." ></td>
	<td class="line x" title="58:236	Given self-trainings varied performance in the past, many of our experiments use the concatenation of sections 1, 22, and 24 (5,039 sentences) rather than the standard development set for more robust testing." ></td>
	<td class="line x" title="59:236	A full description of the reranking parser can be found in Charniak and Johnson (2005)." ></td>
	<td class="line x" title="60:236	Briefly put, the reranking parser consists of two stages: A generative lexicalized PCFG parser which proposes a list of the n most probable parses (n-best list) followed by a discriminative reranker which reorders the n-best list." ></td>
	<td class="line x" title="61:236	The reranker uses about 1.3 million features to help score the trees, the most important of which is the first stage parsers probability." ></td>
	<td class="line x" title="62:236	In Section 4.3, we mention two classes of reranker features in more depth." ></td>
	<td class="line x" title="63:236	Since some of experiments rely on details of the first stage parser, we present a summary of the parsing model." ></td>
	<td class="line x" title="64:236	3.1 The Parsing Model The parsing model assigns a probability to a parse  by a top-down process of considering each constituent c in  and, for each c, first guessing the preterminal of c, t(c) then the lexical head of c, h(c), and then the expansion of c into further constituents e(c)." ></td>
	<td class="line x" title="65:236	Thus the probability of a parse is given by the equation p() = productdisplay c p(t(c) | l(c),R(c)) p(h(c) | t(c),l(c),R(c)) p(e(c) | l(c),t(c),h(c),R(c)) where l(c) is the label of c (e.g., whether it is a noun phrase (np), verb phrase, etc.) and R(c) is the relevant history of c information outside c that the probability model deems important in determining the probability in question." ></td>
	<td class="line x" title="66:236	562 For each expansion e(c) we distinguish one of the children as the middle child M(c)." ></td>
	<td class="line x" title="67:236	M(c) is the constituent from which the head lexical item h is obtained according to deterministic rules that pick the head of a constituent from among the heads of its children." ></td>
	<td class="line x" title="68:236	To the left of M is a sequence of one or more left labels Li(c) including the special termination symbol  and similarly for the labels to the right, Ri(c)." ></td>
	<td class="line x" title="69:236	Thus an expansion e(c) looks like: l  LmL1MR1Rn." ></td>
	<td class="line x" title="70:236	(1) The expansion is generated by guessing first M, then in order L1 through Lm+1 (= ), and similarly for R1 through Rn+1." ></td>
	<td class="line x" title="71:236	So the parser assigns a probability to the parse based upon five probability distributions, T (the part of speech of the head), H (the head), M (the child constituent which includes the head), L (children to the left of M), and R (children to the right of M)." ></td>
	<td class="line oc" title="72:236	4 Testing the Four Hypotheses The question of why self-training helps in some cases (McClosky et al., 2006; Reichart and Rappoport, 2007) but not others (Charniak, 1997; Steedman et al., 2003) has inspired various theories." ></td>
	<td class="line x" title="73:236	We investigate four of these to better understand when and why self-training helps." ></td>
	<td class="line x" title="74:236	At a high level, the hypotheses are (1) self-training helps after a phase transition, (2) self-training reduces search errors, (3) specific classes of reranker features are needed for self-training, and (4) selftraining improves because we see new combinations of words." ></td>
	<td class="line x" title="75:236	4.1 Phase Transition The phase transition hypothesis is that once a parser has achieved a certain threshold of performance, it can label data sufficiently accurately." ></td>
	<td class="line x" title="76:236	Once this happens, the labels will be good enough for self-training." ></td>
	<td class="line oc" title="77:236	To test the phase transition hypothesis, we use the same parser as McClosky et al.(2006) but train on only a fraction of WSJ to see if self-training is still helpful." ></td>
	<td class="line x" title="79:236	This is similar to some of the experiments by Reichart and Rappoport (2007) but with the use of a reranker and slightly larger seed sizes." ></td>
	<td class="line oc" title="80:236	The self-training protocol is the same as in (Charniak, 1997; McClosky et al., 2006; Reichart and Rappoport, 2007): we parse the entire unlabeled corpus in one iteration." ></td>
	<td class="line x" title="81:236	We start by taking a random subset of the WSJ training sections (2-21), accepting each sentence with 10% probability." ></td>
	<td class="line x" title="82:236	With the sampled training section and the standard development data, we train a parser and a reranker." ></td>
	<td class="line x" title="83:236	In Table 1, we show the performance of the parser with and without the reranker." ></td>
	<td class="line x" title="84:236	For reference, we show the performance when using the complete training division as well." ></td>
	<td class="line x" title="85:236	Unsurprisingly, both metrics drop as we decrease the amount of training data." ></td>
	<td class="line x" title="86:236	These scores represent our baselines for this experiment." ></td>
	<td class="line x" title="87:236	Using this parser model, we parse one million sentences from NANC, both with and without the reranker." ></td>
	<td class="line x" title="88:236	We combine these one million sentences with the sampled subsets of WSJ training and train new parser models from them.2 Finally, we evaluate these self-trained models (Table 2)." ></td>
	<td class="line x" title="89:236	The numbers in parentheses indicate the change from the corresponding non-self-trained model." ></td>
	<td class="line x" title="90:236	As in Reichart and Rappoport (2007), we see large improvements when self-training on a small seed size (10%) without using the reranker." ></td>
	<td class="line x" title="91:236	However, using the reranker to parse the selftraining and/or evaluation sentences further improves results." ></td>
	<td class="line oc" title="92:236	From McClosky et al.(2006), we know that when 100% of the training data is used, self-training does not improve performance without a reranker." ></td>
	<td class="line x" title="94:236	From this we conclude that there is no such threshold phase transition in this case." ></td>
	<td class="line x" title="95:236	High performance is not a requirement to successfully use self-training for parsing, since there are lower performing parsers which can self-train and higher performing parsers which cannot." ></td>
	<td class="line x" title="96:236	The higher performing Charniak and Johnson (2005) parser without reranker achieves an f-score of 89.0 on section 24 when trained on all of WSJ." ></td>
	<td class="line x" title="97:236	This parser does not benefit from self-training unless paired with a reranker." ></td>
	<td class="line x" title="98:236	Contrast this with the same parser trained on only 10% of WSJ, where it gets an f-score of 85.8 (Table 2) or the small seed models of Reichart and Rappoport (2007)." ></td>
	<td class="line x" title="99:236	Both of these lower performing parsers can successfully self-train." ></td>
	<td class="line x" title="100:236	Additionally, we now know that while a reranker is not required for self-training when the seed size is small, it still helps performance considerably (fscore improves from 87.7 to 89.0 in the 10% case)." ></td>
	<td class="line x" title="101:236	2We do not weight the original WSJ data, though our expectation is that performance would improve if WSJ were given a higher relative weight." ></td>
	<td class="line x" title="102:236	This is left as future work." ></td>
	<td class="line x" title="103:236	563 % WSJ # sentences Parser f-score Reranking parser f-score 10 3,995 85.8 87.0 100 39,832 89.9 91.5 Table 1: Parser and reranking parser performance on sentences  100 words in sections 1, 22, and 24 when trained on different amounts of training data." ></td>
	<td class="line x" title="104:236	% WSJ is the percentage of WSJ training data trained on (sampled randomly)." ></td>
	<td class="line x" title="105:236	Note that the full amount of development data is still used as held out data." ></td>
	<td class="line x" title="106:236	Parsed NANC with reranker?" ></td>
	<td class="line x" title="107:236	Parser f-score Reranking parser f-score No 87.7 (+1.9) 88.7 (+1.7) Yes 88.4 (+2.6) 89.0 (+2.0) Table 2: Effect of self-training using only 10% of WSJ as labeled data." ></td>
	<td class="line x" title="108:236	The parser model is trained from one million parsed sentences from NANC + WSJ training." ></td>
	<td class="line x" title="109:236	The first column indicates whether the million NANC sentences were parsed by the parser or reranking parser." ></td>
	<td class="line x" title="110:236	The second and third columns differ in whether the reranker is used to parse the test sentences (WSJ sections 1, 22, and 24, sentences 100 words and shorter)." ></td>
	<td class="line x" title="111:236	Numbers in parentheses are the improvements over the corresponding non-self-trained parser." ></td>
	<td class="line x" title="112:236	4.2 Search Errors Another possible explanation of self-trainings improvements is that seeing newly labeled data results in fewer search errors (Daniel Marcu, personal communication)." ></td>
	<td class="line x" title="113:236	A search error would indicate that the parsing model could have produced better (more probable) parses if not for heuristics in the search procedure." ></td>
	<td class="line x" title="114:236	The additional parse trees may help produce sharper distributions and reduce data sparsity, making the search process easier." ></td>
	<td class="line oc" title="115:236	To test this, first we present some statistics on the nbest lists (n = 50) from the baseline WSJ trained parser and self-trained model3 from McClosky et al.(2006)." ></td>
	<td class="line x" title="117:236	We use each model to parse sentences from held-out data (sections 1, 22, and 24) and examine the n-best lists." ></td>
	<td class="line x" title="118:236	We compute statistics of the WSJ and selftrained n-best lists with the goal of understanding how much they intersect and whether there are search errors." ></td>
	<td class="line x" title="119:236	On average, the n-best lists overlap by 66.0%." ></td>
	<td class="line x" title="120:236	Put another way, this means that about a third of the parses from each model are unique, so the parsers do find a fair number of different parses in their search." ></td>
	<td class="line x" title="121:236	The next question is where the differences in the n-best lists lie  if all the differences were near the bottom, this would be less meaningful." ></td>
	<td class="line x" title="122:236	Let W and S represent the n-best lists from the baseline WSJ and selftrained parsers, respectively." ></td>
	<td class="line x" title="123:236	The topm() function returns the highest scoring parse in the n-best list  according to the reranker and parser model 3http://bllip.cs.brown.edu/selftraining/ m.4 Almost 40% of the time, the top parse in the self-trained model is not in the WSJ models n-best list, (tops(S) / W) though the two models agree on the top parse roughly 42.4% of the time (tops(S) = topw(W))." ></td>
	<td class="line x" title="124:236	Search errors can be formulated as tops(S) / W  tops(S) = topw(W  S)." ></td>
	<td class="line x" title="125:236	This captures sentences where the parse that the reranker chose in the self-trained model is not present in the WSJ models n-best list, but if the parse were added to the WSJ models list, the parses probability in the WSJ model and other reranker features would have caused it to be chosen. These search errors occur in only 2.5% of the n-best lists." ></td>
	<td class="line x" title="126:236	At first glance, one might think that this could be enough to account for the differences, since the self-trained model is only several tenths better in f-score." ></td>
	<td class="line oc" title="127:236	However, we know from McClosky et al.(2006) that on average, parses do not change between the WSJ and self-trained models and when they do, they only improve slightly more than half the time." ></td>
	<td class="line x" title="129:236	For this reason, we run a second test more focused on performance." ></td>
	<td class="line x" title="130:236	For our second test we help the WSJ trained model find the parses that the self-trained model found." ></td>
	<td class="line x" title="131:236	For each sentence, we start with the n-best list (n = 500 here) from the WSJ trained parser, W. We then consider parses in the self-trained parsers n-best list, S, that are not present in W (S  W)." ></td>
	<td class="line x" title="132:236	For each of these parses, we determine its probability under the WSJ trained parsing 4Recall that the parsers probability is a reranker feature so the parsing model influences the ranking." ></td>
	<td class="line x" title="133:236	564 Model f-score WSJ 91.5 WSJ & search help 91.7 Self-trained 92.0 Table 3: Test of whether search help from the self-trained model impacts the WSJ trained model." ></td>
	<td class="line x" title="134:236	WSJ + search help is made by adding self-trained parses not proposed by the WSJ trained parser but to which the parser assigns a positive probability." ></td>
	<td class="line x" title="135:236	The WSJ reranker is used in all cases to select the best parse for sections 1, 22, and 24." ></td>
	<td class="line x" title="136:236	model." ></td>
	<td class="line x" title="137:236	If the probability is non-zero, we add the parse to the n-best list W, otherwise we ignore the parse." ></td>
	<td class="line x" title="138:236	In other words, we find parses that the WSJ trained model could have produced but didnt due to search heuristics." ></td>
	<td class="line x" title="139:236	In Table 3, we show the performance of the WSJ trained model, the model with search help as described above, and the selftrained model on WSJ sections 1, 22, and 24." ></td>
	<td class="line x" title="140:236	The WSJ reranker is used to pick the best parse from each n-best list." ></td>
	<td class="line x" title="141:236	WSJ with search help performs slightly better than WSJ alone but does not reach the level of the self-trained model." ></td>
	<td class="line x" title="142:236	From these experiments, we conclude that reduced search errors can only explain a small amount of self-trainings improvements." ></td>
	<td class="line x" title="143:236	4.3 Non-generative reranker features We examine the role of specific reranker features by training rerankers using only subsets of the features." ></td>
	<td class="line x" title="144:236	Our goal is to determine whether some classes of reranker features benefit self-training more than others." ></td>
	<td class="line x" title="145:236	We hypothesize that features which are not easily captured by the generative first-stage parser are the most beneficial for selftraining." ></td>
	<td class="line x" title="146:236	If we treat the parser and reranking parser as different (but clearly dependent) views, this is a bit like co-training." ></td>
	<td class="line x" title="147:236	If the reranker uses features which are captured by the first-stage, the views may be too similar for there to be an improvement." ></td>
	<td class="line x" title="148:236	We consider two classes of features (GEN and EDGE) and their complements (NON-GEN and NON-EDGE).5 GEN consists of features that are roughly captured by the first-stage generative parser: rule rewrites, head-child dependencies, etc. EDGE features describe items across constituent boundaries." ></td>
	<td class="line x" title="149:236	This includes the words and parts of 5A small number of features overlap hence these sizes do not add up." ></td>
	<td class="line x" title="150:236	Feature set # features f-score GEN 448,349 90.4 NON-GEN 885,492 91.1 EDGE 601,578 91.0 NON-EDGE 732,263 91.1 ALL 1,333,519 91.3 Table 4: Sizes and performance of reranker feature subsets." ></td>
	<td class="line x" title="151:236	Reranking parser f-scores are on all sentences in section 24." ></td>
	<td class="line x" title="152:236	speech of the tokens on the edges between constituents and the labels of these constituents." ></td>
	<td class="line x" title="153:236	This represents a specific class of features not captured by the first-stage." ></td>
	<td class="line x" title="154:236	These subsets and their sizes are shown in Table 4." ></td>
	<td class="line oc" title="155:236	For comparison, we also include the results of experiments using the full feature set, as in McClosky et al.(2006), labeled ALL." ></td>
	<td class="line x" title="157:236	The GEN features are roughly one third the size of the full feature set." ></td>
	<td class="line x" title="158:236	We evaluate the effect of these new reranker models on self-training (Table 4)." ></td>
	<td class="line x" title="159:236	For each feature set, we do the following: We parse one million NANC sentences with the reranking parser." ></td>
	<td class="line x" title="160:236	Combining the parses with WSJ training data, we train a new first-stage model." ></td>
	<td class="line x" title="161:236	Using this new first-stage model and the reranker subset, we evaluate on section 24 of WSJ." ></td>
	<td class="line x" title="162:236	GENs performance is weaker while the other three subsets achieve almost the same score as the full feature set." ></td>
	<td class="line x" title="163:236	This confirms our hypothesis that when the reranker helps in selftraining it is due to features which are not captured by the generative first-stage model." ></td>
	<td class="line x" title="164:236	4.4 Unknown Words Given the large size of the parsed self-training corpus, it contains an immense number of parsing events which never occur in the training corpus." ></td>
	<td class="line x" title="165:236	The most obvious of these events is words  the vocabulary grows from 39,548 to 265,926 words as we transition from the WSJ trained model to the self-trained model." ></td>
	<td class="line x" title="166:236	Slightly less obvious is bigrams." ></td>
	<td class="line x" title="167:236	There are roughly 330,000 bigrams in WSJ training data and approximately 4.8 million new bigrams in the self-training corpus." ></td>
	<td class="line x" title="168:236	One hypothesis (Mitch Marcus, personal communication) is that the parser is able to learn a lot of new bilexical head-to-head dependencies (biheads) from self-training." ></td>
	<td class="line x" title="169:236	The reasoning is as follows: Assume the self-training corpus is parsed in a mostly correct manner." ></td>
	<td class="line x" title="170:236	If there are not too many 565 new pairs of words in a sentence, there is a decent chance that we can tag these words correctly and bracket them in a reasonable fashion from context." ></td>
	<td class="line x" title="171:236	Thus, using these parses as part of the training data improves parsing because should we see these pairs of words together in the future, we will be more likely to connect them together properly." ></td>
	<td class="line x" title="172:236	We test this hypothesis in two ways." ></td>
	<td class="line oc" title="173:236	First, we perform an extension of the factor analysis similar to that in McClosky et al.(2006)." ></td>
	<td class="line o" title="175:236	This is done via a generalized linear regression model intended to determine which features of parse trees can predict when the self-training model will perform better." ></td>
	<td class="line x" title="176:236	We consider many of the same features (e.g. bucketed sentence length, number of conjunctions, and number of unknown words) but also consider two new features: unknown bigrams and unknown biheads." ></td>
	<td class="line x" title="177:236	Unknown items (words, bigrams, biheads) are calculated by counting the number of items which have never been seen in WSJ training but have been seen in the parsed NANC data." ></td>
	<td class="line x" title="178:236	Given these features, we take the f-scores for each sentence when parsed by the WSJ and self-trained models and look at the differences." ></td>
	<td class="line x" title="179:236	Our goal is to find out which features, if any, can predict these fscore differences." ></td>
	<td class="line x" title="180:236	Specifically, we ask the question of whether seeing more unknown items indicates whether we are more likely to see improvements when self-training." ></td>
	<td class="line x" title="181:236	The effect of unknown items on self-trainings relative performance is summarized in Figure 1." ></td>
	<td class="line x" title="182:236	For each item, we show the total number of incorrect parse nodes in sentences that contain the item." ></td>
	<td class="line x" title="183:236	We also show the change in the number of correct parse nodes in these sentences between the WSJ and self-trained models." ></td>
	<td class="line x" title="184:236	A positive change means that performance improved under self-training." ></td>
	<td class="line x" title="185:236	In other words, looking at Figure 1a, the greatest performance improvement occurs, perhaps surprisingly, when we have seen no unknown words." ></td>
	<td class="line x" title="186:236	As we see more unknown words, the improvement from self-training decreases." ></td>
	<td class="line x" title="187:236	This is a pretty clear indication that unknown words are not a good predictor of when self-training improves performance." ></td>
	<td class="line x" title="188:236	A possible objection that one might raise is that using unknown biheads as a regression feature will bias our results if they are counted from gold trees instead of parsed trees." ></td>
	<td class="line x" title="189:236	Seeing a bihead in training will cause the otherwise sparse biheads distribution to be extremely peaked around that bif-score Model 89.8  WSJ (baseline) 89.8  WSJ+NANC M 89.9  WSJ+NANC T 89.9  WSJ+NANC L 90.0  WSJ+NANC R 90.0 WSJ+NANC MT 90.1 WSJ+NANC H 90.2 WSJ+NANC LR 90.3 WSJ+NANC LRT 90.4 WSJ+NANC LMRT 90.4 WSJ+NANC LMR 90.5 WSJ+NANC LRH 90.7  WSJ+NANC LMRH 90.8  WSJ+NANC (fully self-trained) Table 5: Performance of the first-stage parser on various combinations of distributions WSJ and WSJ+NANC (self-trained) models on sections 1, 22, and 24." ></td>
	<td class="line x" title="190:236	Distributions are L (left expansion), R (right expansion), H (head word), M (head phrasal category), and T (head POS tag)." ></td>
	<td class="line x" title="191:236	 and  indicate the model is not significantly different from baseline and self-trained model, respectively." ></td>
	<td class="line x" title="192:236	head." ></td>
	<td class="line x" title="193:236	If we see the same pair of words in testing, we are likely to connect them in the same fashion." ></td>
	<td class="line x" title="194:236	Thus, if we count unknown biheads from gold trees, this feature may explain away other improvements: When gold trees contain a bihead found in our self-training data, we will almost always see an improvement." ></td>
	<td class="line x" title="195:236	However, given the similar trends in Figures 1b and 1c, we propose that unknown bigrams can be thought of as a rough approximation of unknown biheads." ></td>
	<td class="line x" title="196:236	The regression analysis reveals that unknown bigrams and unknown biheads are good predictors of f-score improvements." ></td>
	<td class="line pc" title="197:236	The significant predictors from McClosky et al.(2006) such as the number of conjunctions or sentence length continue to be helpful whereas unknown words are a weak predictor at best." ></td>
	<td class="line x" title="199:236	These results are apparent in Figure 1: as stated before, seeing more unknown words does not correlate with improvements." ></td>
	<td class="line x" title="200:236	However, seeing more unknown bigrams and biheads does predict these changes fairly well." ></td>
	<td class="line x" title="201:236	When we have seen zero or one new bigrams or biheads, selftraining negatively impacts performance." ></td>
	<td class="line x" title="202:236	After seeing two or more, we see positive effects until about six to ten after which improvements taper off." ></td>
	<td class="line x" title="203:236	566 To see the effect of biheads on performance more directly, we also experiment by interpolating between the WSJ and self-trained models on a distribution level." ></td>
	<td class="line x" title="204:236	To do this, we take specific distributions (see Section 3.1) from the self-trained model and have them override the corresponding distributions in a compatible WSJ trained model." ></td>
	<td class="line x" title="205:236	From this we hope to show which distributions self-training boosts." ></td>
	<td class="line x" title="206:236	According to the biheads hypothesis, the H distribution (which captures information about head-to-head dependencies) should account for most of the improvement." ></td>
	<td class="line x" title="207:236	The results of moving these distributions is shown in Table 5." ></td>
	<td class="line x" title="208:236	For each new model, we show whether the models performance is not significantly different than the baseline model (indicated by ) or not significantly different than the selftrained model ()." ></td>
	<td class="line x" title="209:236	H (biheads) is the strongest single feature and the only one to be significantly better than the baseline." ></td>
	<td class="line x" title="210:236	Nevertheless, it is only 0.3% higher, accounting for 30% of the full self-training improvement." ></td>
	<td class="line x" title="211:236	In general, the performance improvements from distributions are additive (+/ 0.1%)." ></td>
	<td class="line x" title="212:236	Self-training improves all distributions, so biheads are not the full picture." ></td>
	<td class="line x" title="213:236	Nevertheless, they remain the strongest single feature." ></td>
	<td class="line x" title="214:236	5 Discussion The experiments in this paper have clarified many details about the nature of self-training for parsing." ></td>
	<td class="line x" title="215:236	We have ruled out the phase transition hypothesis entirely." ></td>
	<td class="line x" title="216:236	Reduced search errors are responsible for some, but not all, of the improvements in selftraining." ></td>
	<td class="line x" title="217:236	We have confirmed that non-generative reranker features are more beneficial than generative reranker features since they make the reranking parser more different from the base parser." ></td>
	<td class="line x" title="218:236	Finally, we have found that while unknown bigrams and biheads are a significant source of improvement, they are not the sole source of it." ></td>
	<td class="line x" title="219:236	Since unknown words do not correlate well with selftraining improvements, there must be something about the unknown bigrams and biheads which are aid the parser." ></td>
	<td class="line x" title="220:236	Our belief is that new combinations of words we have already seen guides the parser in the right direction." ></td>
	<td class="line x" title="221:236	Additionally, these new combinations result in more peaked distributions which decreases the number of search errors." ></td>
	<td class="line x" title="222:236	However, while these experiments and others get us closer to understanding self-training, we still lack a complete explanation." ></td>
	<td class="line x" title="223:236	Naturally, the hy0 1 2 3 4 5 6 7 10 11 12 Total number of incorrect nodes 0 6000 0 1 2 3 4 5 6 7 10 11 12 Number of unknown words in tree Reduction in incorrect nodes 0 300 (a) Effect of unknown words on performance 0 2 4 6 8 10 12 14 16 18 20 Total number of incorrect nodes 0 1000 0 2 4 6 8 10 12 14 16 18 20 Number of unknown bigrams in tree Reduction in incorrect nodes 100 100 (b) Effect of unknown bigrams on performance 0 2 4 6 8 10 12 14 16 18 20 25 Total number of incorrect nodes 0 1000 0 2 4 6 8 10 12 14 16 18 20 25 Number of unknown biheads in tree Reduction in incorrect nodes 50 100 (c) Effect of unknown biheads on performance Figure 1: Change in the number of incorrect parse tree nodes between WSJ and self-trained models as a function of number of unknown items." ></td>
	<td class="line x" title="224:236	Seeing any number of unknown words results in fewer errors on average whereas seeing zero or one unknown bigrams or biheads is likely to hurt performance." ></td>
	<td class="line x" title="225:236	567 potheses tested are by no means exhaustive." ></td>
	<td class="line x" title="226:236	Additionally, we have only considered generative constituency parsers here and a good direction for future research would be to see if self-training generalizes to a broader class of parsers." ></td>
	<td class="line x" title="227:236	We suspect that using a generative parser/discriminative reranker paradigm should allow self-training to extend to other parsing formalisms." ></td>
	<td class="line x" title="228:236	Recall that in Reichart and Rappoport (2007) where only a small amount of labeled data was used, the number of unknown words in a sentence was a strong predictor of self-training benefits." ></td>
	<td class="line x" title="229:236	When a large amount of labeled data is available, unknown words are no longer correlated with these gains, but unknown bigrams and biheads are." ></td>
	<td class="line x" title="230:236	When using a small amount of training data, unknown words are useful since we have not seen very many words yet." ></td>
	<td class="line x" title="231:236	As the amount of training data increases, we see fewer new words but the number of new bigrams and biheads remains high." ></td>
	<td class="line x" title="232:236	We postulate that this difference may help explain the shift from unknown words to unknown bigrams and biheads." ></td>
	<td class="line x" title="233:236	We hope to further investigate the role of these unknown items by seeing how our analyses change under different amounts of labeled data relative to unknown item rates." ></td>
	<td class="line x" title="234:236	Acknowledgments This work was supported by DARPA GALE contract HR0011-06-2-0001." ></td>
	<td class="line x" title="235:236	We would like to thank Matt Lease, the rest of the BLLIP team, and our anonymous reviewers for their comments." ></td>
	<td class="line x" title="236:236	Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of DARPA." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D08-1071
Cross-Task Knowledge-Constrained Self Training
Daum III, Hal;"></td>
	<td class="line x" title="1:260	Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 680688, Honolulu, October 2008." ></td>
	<td class="line x" title="2:260	c2008 Association for Computational Linguistics Cross-Task Knowledge-Constrained Self Training Hal Daume III School of Computing University of Utah Salt Lake City, UT 84112 me@hal3.name Abstract We present an algorithmic framework for learning multiple related tasks." ></td>
	<td class="line x" title="3:260	Our framework exploits a form of prior knowledge that relates the output spaces of these tasks." ></td>
	<td class="line x" title="4:260	We present PAC learning results that analyze the conditions under which such learning is possible." ></td>
	<td class="line x" title="5:260	We present results on learning a shallow parser and named-entity recognition system that exploits our framework, showing consistent improvements over baseline methods." ></td>
	<td class="line x" title="6:260	1 Introduction When two NLP systems are run on the same data, we expect certain constraints to hold between their outputs." ></td>
	<td class="line x" title="7:260	This is a form of prior knowledge." ></td>
	<td class="line x" title="8:260	We propose a self-training framework that uses such information to significantly boost the performance of one of the systems." ></td>
	<td class="line x" title="9:260	The key idea is to perform self-training only on outputs that obey the constraints." ></td>
	<td class="line x" title="10:260	Our motivating example in this paper is the task pair: named entity recognition (NER) and shallow parsing (aka syntactic chunking)." ></td>
	<td class="line x" title="11:260	Consider a hidden sentence with known POS and syntactic structure below." ></td>
	<td class="line x" title="12:260	Further consider four potential NER sequences for this sentence." ></td>
	<td class="line x" title="13:260	POS: NNP NNP VBD TO NNP NN Chunk: [NP -][-VP-][-PP-][-NP-][-NP-] NER1: [Per -][O -][-Org-][0 -] NER2: [Per -][O -][O -][O -][O -] NER3: [Per -][O -][O -][Org -] NER4: [Per -][O -][O -][-Org-][O -] Without ever seeing the actual sentence, can we guess which NER sequence is correct?" ></td>
	<td class="line x" title="14:260	NER1 seems wrong because we feel like named entities should not be part of verb phrases." ></td>
	<td class="line x" title="15:260	NER2 seems wrong because there is an NNP1 (proper noun) that is not part of a named entity (word 5)." ></td>
	<td class="line x" title="16:260	NER3 is amiss because we feel it is unlikely that a single name should span more than one NP (last two words)." ></td>
	<td class="line x" title="17:260	NER4 has none of these problems and seems quite reasonable." ></td>
	<td class="line x" title="18:260	In fact, for the hidden sentence, NER4 is correct2." ></td>
	<td class="line x" title="19:260	The remainder of this paper deals with the problem of formulating such prior knowledge into a workable system." ></td>
	<td class="line x" title="20:260	There are similarities between our proposed model and both self-training and cotraining; background is given in Section 2." ></td>
	<td class="line x" title="21:260	We present a formal model for our approach and perform a simple, yet informative, analysis (Section 3)." ></td>
	<td class="line x" title="22:260	This analysis allows us to define what good and bad constraints are." ></td>
	<td class="line x" title="23:260	Throughout, we use a running example of NER using hidden Markov models to show the efficacy of the method and the relationship between the theory and the implementation." ></td>
	<td class="line x" title="24:260	Finally, we present full-blown results on seven different NER data sets (one from CoNLL, six from ACE), comparing our method to several competitive baselines (Section 4)." ></td>
	<td class="line x" title="25:260	We see that for many of these data sets, less than one hundred labeled NER sentences are required to get state-of-the-art performance, using a discriminative sequence labeling algorithm (Daume III and Marcu, 2005)." ></td>
	<td class="line x" title="26:260	2 Background Self-training works by learning a model on a small amount of labeled data." ></td>
	<td class="line x" title="27:260	This model is then evalu1When we refer to NNP, we also include NNPS." ></td>
	<td class="line x" title="28:260	2The sentence is: George Bush spoke to Congress today 680 ated on a large amount of unlabeled data." ></td>
	<td class="line x" title="29:260	Its predictions are assumed to be correct, and it is retrained on the unlabeled data according to its own predictions." ></td>
	<td class="line x" title="30:260	Although there is little theoretical support for self-training, it is relatively popular in the natural language processing community." ></td>
	<td class="line pc" title="31:260	Its success stories range from parsing (McClosky et al., 2006) to machine translation (Ueffing, 2006)." ></td>
	<td class="line x" title="32:260	In some cases, self-training takes into account model confidence." ></td>
	<td class="line x" title="33:260	Co-training (Yarowsky, 1995; Blum and Mitchell, 1998) is related to self-training, in that an algorithm is trained on its own predictions." ></td>
	<td class="line x" title="34:260	Where it differs is that co-training learns two separate models (which are typically assumed to be independent; for instance by training with disjoint feature sets)." ></td>
	<td class="line x" title="35:260	These models are both applied to a large repository of unlabeled data." ></td>
	<td class="line x" title="36:260	Examples on which these two models agree are extracted and treated as labeled for a new round of training." ></td>
	<td class="line x" title="37:260	In practice, one often also uses a notion of model confidence and only extracts agreed-upon examples for which both models are confident." ></td>
	<td class="line x" title="38:260	The original, and simplest analysis of cotraining is due to Blum and Mitchell (1998)." ></td>
	<td class="line x" title="39:260	It does not take into account confidence (to do so requires a significantly more detailed analysis (Dasgupta et al., 2001)), but is useful for understanding the process." ></td>
	<td class="line x" title="40:260	3 Model We define a formal PAC-style (Valiant, 1994) model that we call the hints model3." ></td>
	<td class="line x" title="41:260	We have an instance spaceX and two output spacesY1 andY2." ></td>
	<td class="line x" title="42:260	We assume two concept classesC1 andC2 for each output space respectively." ></td>
	<td class="line x" title="43:260	LetD be a distribution overX, andf1C1 (resp.,f2C2) be target functions." ></td>
	<td class="line x" title="44:260	The goal, of course, is to use a finite sample of examples drawn from D (and labeledperhaps with noise byf1 andf2) to learnh1C1 andh2C2, which are good approximations to f1 and f2." ></td>
	<td class="line x" title="45:260	So far we have not made use of any notion of constraints." ></td>
	<td class="line x" title="46:260	Our expectation is that if we constrain h1 and h2 to agree (vis-a-vis the example in the Introduction), then we should need fewer labeled examples to learn either." ></td>
	<td class="line x" title="47:260	(The agreement should shrink the size of the corresponding hypothesis spaces.)" ></td>
	<td class="line x" title="48:260	To formalize this, let  : Y1Y2 {0,1}be a con3The name comes from thinking of our knowledge-based constraints as hints to a learner as to what it should do." ></td>
	<td class="line x" title="49:260	straint function." ></td>
	<td class="line x" title="50:260	We say that two outputs y1 Y1 and y2 Y2 are compatible if (y1,y2) = 1." ></td>
	<td class="line x" title="51:260	We need to assume that  is correct: Definition 1." ></td>
	<td class="line x" title="52:260	We say that  is correct with respect to D,f1,f2 if whenever x has non-zero probability underD, then (f1(x),f2(x)) = 1." ></td>
	<td class="line x" title="53:260	RUNNING EXAMPLE In our example, Y1 is the space of all POS/chunk sequences and Y2 is the space of all NER sequences." ></td>
	<td class="line x" title="54:260	We assume that C1 and C2 are both represented by HMMs over the appropriate state spaces." ></td>
	<td class="line x" title="55:260	The functions we are trying to learn are f1, the true POS/chunk labeler and f2, the true NER labeler." ></td>
	<td class="line x" title="56:260	(Note that we assume f1  C1, which is obviously not true for language.)" ></td>
	<td class="line x" title="57:260	Our constraint function  will require the following for agreement: (1) any NNP must be part of a named entity; (2) any named entity must be a subsequence of a noun phrase." ></td>
	<td class="line x" title="58:260	This is precisely the set of constraints discussed in the introduction." ></td>
	<td class="line x" title="59:260	The question is: given this additional source of knowledge (i.e., ), has the learning problem become easier?" ></td>
	<td class="line x" title="60:260	That is, can we learnf2 (and/orf1) using significantly fewer labeled examples than if we did not have ?" ></td>
	<td class="line x" title="61:260	Moreover, we have assumed that  is correct, but is this enough?" ></td>
	<td class="line x" title="62:260	Intuitively, no: a functionthat returns 1 regardless of its inputs is clearly not useful." ></td>
	<td class="line x" title="63:260	Given this, what other constraints must be placed on ." ></td>
	<td class="line x" title="64:260	We address these questions in Sections 3.3." ></td>
	<td class="line x" title="65:260	However, first we define our algorithm." ></td>
	<td class="line x" title="66:260	3.1 One-sided Learning with Hints We begin by considering a simplified version of the learning with hints problem." ></td>
	<td class="line x" title="67:260	Suppose that all we care about is learningf2." ></td>
	<td class="line x" title="68:260	We have a small amount of data labeled byf2 (call thisD) and a large amount of data labeled by f1 (call this Dunlabunlab because as far as f2 is concerned, it is unlabeled)." ></td>
	<td class="line x" title="69:260	RUNNING EXAMPLE In our example, this means that we have a small amount of labeled NER data and a large amount of labeled POS/chunk data." ></td>
	<td class="line x" title="70:260	We use 3500 sentences from CoNLL (Tjong Kim Sang and De Meulder, 2003) as the NER data and section 20-23 of the WSJ (Marcus et al., 1993; Ramshaw and Marcus, 1995) as the POS/chunk data (8936 sentences)." ></td>
	<td class="line x" title="71:260	We are only interested in learning to do NER." ></td>
	<td class="line x" title="72:260	Details of the exact HMM setup are in Section 4.2." ></td>
	<td class="line x" title="73:260	681 We call the following algorithm One-Sided Learning with Hints, since it aims only to learn f2: 1: Learn h2 directly on D 2: For each example (x,y1)Dunlab 3: Compute y2 = h2(x) 4: If (y1,y2), add (x,y2) to D 5: Relearn h2 on the (augmented) D 6: Go to (2) if desired RUNNING EXAMPLE In step 1, we train an NER HMM on CoNLL." ></td>
	<td class="line x" title="74:260	On test data, this model achieves an F-score of 50.8." ></td>
	<td class="line x" title="75:260	In step 2, we run this HMM on all the WSJ data, and extract 3145 compatible examples." ></td>
	<td class="line x" title="76:260	In step 3, we retrain the HMM; the F-score rises to 58.9." ></td>
	<td class="line x" title="77:260	3.2 Two-sided Learning with Hints In the two-sided version, we assume that we have a small amount of data labeled by f1 (call this D1), a small amount of data labeled byf2 (call thisD2) and a large amount of unlabeled data (call this Dunlab)." ></td>
	<td class="line x" title="78:260	The algorithm we propose for learning hypotheses for both tasks is below: 1: Learn h1 on D1 and h2 on D2." ></td>
	<td class="line x" title="79:260	2: For each example xDunlab: 3: Compute y1 = h1(x) and y2 = h2(x) 4: If (y1,y2) add (x,y1) to D1, (x,y2) to D2 5: Relearn h1 on D1 and h2 on D2." ></td>
	<td class="line x" title="80:260	6: Go to (2) if desired RUNNING EXAMPLE We use 3500 examples from NER and 1000 from WSJ." ></td>
	<td class="line x" title="81:260	We use the remaining 18447 examples as unlabeled data." ></td>
	<td class="line x" title="82:260	The baseline HMMs achieve Fscores of 50.8 and 76.3, respectively." ></td>
	<td class="line x" title="83:260	In step 2, we add 7512 examples to each data set." ></td>
	<td class="line x" title="84:260	After step 3, the new models achieveF-scores of 54.6 and 79.2, respectively." ></td>
	<td class="line x" title="85:260	The gain for NER is lower than before as it is trained against noisy syntactic labels." ></td>
	<td class="line x" title="86:260	3.3 Analysis Our goal is to prove that one-sided learning with hints works. That is, if C2 is learnable from large amounts of labeled data, then it is also learnable from small amounts of labeled data and large amounts of f1-labeled data." ></td>
	<td class="line x" title="87:260	This is formalized in Theorem 1 (all proofs are in Appendix A)." ></td>
	<td class="line x" title="88:260	However, before stating the theorem, we must define an initial weakly-useful predictor (terminology from Blum and Mitchell(1998)), and the notion of noisy PAC-learning in the structured domain." ></td>
	<td class="line x" title="89:260	Definition 2." ></td>
	<td class="line x" title="90:260	We say that h is a weakly-useful predictor of f if for all y: PrD[h(x) = y]  epsilon1 and PrD[f(x) = y|h(x) = yprimenegationslash= y]  PrD[f(x) = y] +epsilon1." ></td>
	<td class="line x" title="91:260	This definition simply ensures that (1) h is nontrivial: it assigns some non-zero probability to every possible output; and (2) h is somewhat indicative of f. In practice, we use the hypothesis learned on the small amount of training data during step (1) of the algorithm as the weakly useful predictor." ></td>
	<td class="line x" title="92:260	Definition 3." ></td>
	<td class="line x" title="93:260	We say that C is PAC-learnable with noise (in the structured setting) if there exists an algorithm with the following properties." ></td>
	<td class="line x" title="94:260	For any c C, any distribution D over X, any 0    1/|Y|, any 0 < epsilon1 < 1, any 0 <  < 1 and any   0 < 1/|Y|, if the algorithm is given access to examples drawn EXSN(c,D) and inputs epsilon1, and 0, then with probability at least 1, the algorithm returns a hypothesis hC with error at most epsilon1." ></td>
	<td class="line x" title="95:260	Here, EXSN(c,D) is a structured noise oracle, which draws examples fromD, labels them by c and randomly replaces with another label with prob." ></td>
	<td class="line x" title="96:260	." ></td>
	<td class="line x" title="97:260	Note here the rather weak notion of noise: entire structures are randomly changed, rather than individual labels." ></td>
	<td class="line x" title="98:260	Furthermore, the error is 0/1 loss over the entire structure." ></td>
	<td class="line x" title="99:260	Collins (2001) establishes learnability results for the class of hyperplane models under 0/1 loss." ></td>
	<td class="line x" title="100:260	While not stated directly in terms of PAC learnability, it is clear that his results apply." ></td>
	<td class="line x" title="101:260	Taskar et al.(2005) establish tighter bounds for the case of Hamming loss." ></td>
	<td class="line x" title="103:260	This suggests that the requirement of 0/1 loss is weaker." ></td>
	<td class="line x" title="104:260	As suggested before, it is not sufficient for  to simply be correct (the constant 1 function is correct, but not useful)." ></td>
	<td class="line x" title="105:260	We need it to be discriminating, made precise in the following definition." ></td>
	<td class="line x" title="106:260	Definition 4." ></td>
	<td class="line x" title="107:260	We say the discrimination of  for h0 is PrD[(f1(x),h0(x))]1." ></td>
	<td class="line x" title="108:260	In other words, a constraint function is discriminating when it is unlikely that our weakly-useful predictor h0 chooses an output that satisfies the constraint." ></td>
	<td class="line x" title="109:260	This means that if we do find examples (in our unlabeled corpus) that satisfy the constraints, they are likely to be useful to learning." ></td>
	<td class="line x" title="110:260	682 RUNNING EXAMPLE In the NER HMM, let h0 be the HMM obtained by training on the small labeled NER data set and f1 is the true syntactic labels." ></td>
	<td class="line x" title="111:260	We approximate PrD by an empirical estimate over the unlabeled distribution." ></td>
	<td class="line x" title="112:260	This gives a discrimination is 41.6 for the constraint function defined previously." ></td>
	<td class="line x" title="113:260	However, if we compare against weaker constraint functions, we see the appropriate trend." ></td>
	<td class="line x" title="114:260	The value for the constraint based only on POS tags is 39.1 (worse) and for the NP constraint alone is 27.0 (much worse)." ></td>
	<td class="line x" title="115:260	Theorem 1." ></td>
	<td class="line x" title="116:260	Suppose C2 is PAC-learnable with noise in the structured setting, h02 is a weakly useful predictor of f2, and  is correct with respect to D,f1,f2,h02, and has discrimination2(|Y|1)." ></td>
	<td class="line x" title="117:260	ThenC2 is also PAC-learnable with one-sided hints." ></td>
	<td class="line x" title="118:260	The way to interpret this theorem is that it tells us that if the initial h2 we learn in step 1 of the onesided algorithm is good enough (in the sense that it is weakly-useful), then we can use it as specified by the remainder of the one-sided algorithm to obtain an arbitrarily good h2 (via iterating)." ></td>
	<td class="line x" title="119:260	The dependence on |Y| is the discrimination bound foris unpleasant for structured problems." ></td>
	<td class="line x" title="120:260	If we wish to find M unlabeled examples that satisfy the hints, well need a total of at least 2M(|Y|1) total." ></td>
	<td class="line x" title="121:260	This dependence can be improved as follows." ></td>
	<td class="line x" title="122:260	Suppose that our structure is represented by a graph over vertices V, each of which can take a label from a set Y. Then,|Y|= vextendsinglevextendsingleYVvextendsinglevextendsingle, and our result requires that  be discriminating on an order exponential in V. Under the assumption that  decomposes over the graph structure (true for our example) and that C2 is PAC-learnable with per-vertex noise, then the discrimination requirement drops to 2|V|(|Y|1)." ></td>
	<td class="line x" title="123:260	RUNNING EXAMPLE In NER, |Y| = 9 and |V|  26." ></td>
	<td class="line x" title="124:260	This means that the values from the previous example look not quite so bad." ></td>
	<td class="line x" title="125:260	In the 0/1 loss case, they are compared to 1025; in the Hamming case, they are compared to only 416." ></td>
	<td class="line x" title="126:260	The ability of the one-sided algorithm follows the same trends as the discrimination values." ></td>
	<td class="line x" title="127:260	Recall the baseline performance is 50.8." ></td>
	<td class="line x" title="128:260	With both constraints (and a discrimination value of 41.6), we obtain a score of 58.9." ></td>
	<td class="line x" title="129:260	With just the POS constraint (discrimination of 39.1), we obtain a score of 58.1." ></td>
	<td class="line x" title="130:260	With just the NP constraint (discrimination of 27.0, we obtain a score of 54.5." ></td>
	<td class="line x" title="131:260	The final question is how one-sided learning relates to two-sided learning." ></td>
	<td class="line x" title="132:260	The following definition and easy corollary shows that they are related in the obvious manner, but depends on a notion of uncorrelation between h01 and h02." ></td>
	<td class="line x" title="133:260	Definition 5." ></td>
	<td class="line x" title="134:260	We say that h1 and h2 are uncorrelated if PrD[h1(x) = y1|h2(x) = y2,x] = PrD[h1(x) = y1|x]." ></td>
	<td class="line x" title="135:260	Corollary 1." ></td>
	<td class="line x" title="136:260	Suppose C1 and C2 are both PAClearnable in the structured setting, h01 and h02 are weakly useful predictors of f1 and f2, and  is correct with respect to D,f1,f2,h01 and h02, and has discrimination  4(|Y|1)2 (for 0/1 loss) or 4|V|2 (|Y|1)2 (for Hamming loss), and thath01 and h02 are uncorrelated." ></td>
	<td class="line x" title="137:260	Then C1 and C2 are also PAC-learnable with two-sided hints." ></td>
	<td class="line x" title="138:260	Unfortunately, Corollary 1 depends quadratically on the discrimination term, unlike Theorem 1." ></td>
	<td class="line x" title="139:260	4 Experiments In this section, we describe our experimental results." ></td>
	<td class="line x" title="140:260	We have already discussed some of them in the context of the running example." ></td>
	<td class="line x" title="141:260	In Section 4.1, we briefly describe the data sets we use." ></td>
	<td class="line x" title="142:260	A full description of the HMM implementation and its results are in Section 4.2." ></td>
	<td class="line x" title="143:260	Finally, in Section 4.3, we present results based on a competitive, discriminativelylearned sequence labeling algorithm." ></td>
	<td class="line x" title="144:260	All results for NER and chunking are in terms of F-score; all results for POS tagging are accuracy." ></td>
	<td class="line x" title="145:260	4.1 Data Sets Our results are based on syntactic data drawn from the Penn Treebank (Marcus et al., 1993), specifically the portion used by CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000)." ></td>
	<td class="line x" title="146:260	Our NER data is from two sources." ></td>
	<td class="line x" title="147:260	The first source is the CoNLL 2003 shared task date (Tjong Kim Sang and De Meulder, 2003) and the second source is the 2004 NIST Automatic Content Extraction (Weischedel, 2004)." ></td>
	<td class="line x" title="148:260	The ACE data constitute six separate data sets from six domains: weblogs (wl), newswire (nw), broadcast conversations (bc), United Nations (un), direct telephone speech (dts) and broadcast news (bn)." ></td>
	<td class="line x" title="149:260	Of these, bc, dts and bn are all speech data sets." ></td>
	<td class="line x" title="150:260	All the examples from the previous sections have been limited to the CoNLL data." ></td>
	<td class="line x" title="151:260	683 4.2 HMM Results The experiments discussed in the preceding sections are based on a generative hidden Markov model for both the NER and syntactic chunking/POS tagging tasks." ></td>
	<td class="line x" title="152:260	The HMMs constructed use first-order transitions and emissions." ></td>
	<td class="line x" title="153:260	The emission vocabulary is pruned so that any word that appears1 time in the training data is replaced by a unique *unknown* token." ></td>
	<td class="line x" title="154:260	The transition and emission probabilities are smoothed with Dirichlet smoothing, = 0.001 (this was not-aggressively tuned by hand on one setting)." ></td>
	<td class="line x" title="155:260	The HMMs are implemented as finite state models in the Carmel toolkit (Graehl and Knight, 2002)." ></td>
	<td class="line x" title="156:260	The various compatibility functions are also implemented as finite state models." ></td>
	<td class="line x" title="157:260	We implement them as a transducer from POS/chunk labels to NER labels (though through the reverse operation, they can obviously be run in the opposite direction)." ></td>
	<td class="line x" title="158:260	The construction is with a single state with transitions:  (NNP,?)" ></td>
	<td class="line x" title="159:260	maps to B-* and I-*  (?,B-NP) maps to B-* and O  (?,I-NP) maps to B-*, I-* and O  Single exception: (NNP,x), where x is not an NP tag maps to anything (this is simply to avoid empty composition problems)." ></td>
	<td class="line x" title="160:260	This occurs in 100 of the 212k words in the Treebank data and more rarely in the automatically tagged data." ></td>
	<td class="line x" title="161:260	4.3 One-sided Discriminative Learning In this section, we describe the results of one-sided discriminative labeling with hints." ></td>
	<td class="line x" title="162:260	We use the true syntactic labels from the Penn Treebank to derive the constraints (this is roughly 9000 sentences)." ></td>
	<td class="line x" title="163:260	We use the LaSO sequence labeling software (Daume III and Marcu, 2005), with its built-in feature set." ></td>
	<td class="line x" title="164:260	Our goal is to analyze two things: (1) what is the effect of the amount of labeled NER data?" ></td>
	<td class="line x" title="165:260	(2) what is the effect of the amount of labeled syntactic data from which the hints are constructed?" ></td>
	<td class="line x" title="166:260	To answer the first question, we keep the amount of syntactic data fixed (at 8936 sentences) and vary the amount of NER data in N  {100,200,400,800,1600}." ></td>
	<td class="line x" title="167:260	We compare models with and without the default gazetteer information from the LaSO software." ></td>
	<td class="line x" title="168:260	We have the following models for comparison:  A default Baseline in which we simply train the NER model without using syntax." ></td>
	<td class="line x" title="169:260	Hints Self-T Hints vs Base vs Base vs Self-T Win 29 20 24 Tie 6 12 11 Lose 0 3 0 Table 1: Comparison between hints, self-training and the (best) baseline for varying amount of labeled data." ></td>
	<td class="line x" title="170:260	 In POS-feature, we do the same thing, but we first label the NER data using a tagger/chunker trained on the 8936 syntactic sentences." ></td>
	<td class="line x" title="171:260	These labels are used as features for the baseline." ></td>
	<td class="line x" title="172:260	 A Self-training setting where we use the 8936 syntactic sentences as unlabeled, label them with our model, and then train on the results." ></td>
	<td class="line x" title="173:260	(This is equivalent to a hints model where (,) = 1 is the constant 1 function.)" ></td>
	<td class="line x" title="174:260	We use model confidence as in Blum and Mitchell (1998).4 The results are shown in Figure 1." ></td>
	<td class="line x" title="175:260	The trends we see are the following:  More data always helps." ></td>
	<td class="line x" title="176:260	 Self-training usually helps over the baseline (though not always: for instance in wl and parts of cts and bn)." ></td>
	<td class="line x" title="177:260	 Adding the gazetteers help." ></td>
	<td class="line x" title="178:260	 Adding the syntactic features helps." ></td>
	<td class="line x" title="179:260	 Learning with hints, especially for  1000 training data points, helps significantly, even over self-training." ></td>
	<td class="line x" title="180:260	We further compare the algorithms by looking at how many training setting has each as the winner." ></td>
	<td class="line x" title="181:260	In particular, we compare both hints and self-training to the two baselines, and then compare hints to selftraining." ></td>
	<td class="line x" title="182:260	If results are not significant at the 95% level (according to McNemars test), we call it a tie." ></td>
	<td class="line x" title="183:260	The results are in Table 1." ></td>
	<td class="line x" title="184:260	In our second set of experiments, we consider the role of the syntactic data." ></td>
	<td class="line x" title="185:260	For this experiment, we hold the number of NER labeled sentences constant (at N = 200) and vary the amount of syntactic data in M {500,1000,2000,4000,8936}." ></td>
	<td class="line x" title="186:260	The results of these experiments are in Figure 2." ></td>
	<td class="line x" title="187:260	The trends are:  The POS feature is relatively insensitive to the amount of syntactic datathis is most likely because its weight is discriminatively adjusted 4Results without confidence were significantly worse." ></td>
	<td class="line x" title="188:260	684 0 1000 20000.2 0.3 0.4 0.5 0.6 0.7 wl 0 1000 20000.4 0.5 0.6 0.7 0.8 nw 0 1000 20000.4 0.5 0.6 0.7 0.8 0.9 conll 0 1000 20000.4 0.5 0.6 0.7 0.8 bc 0 1000 20000.2 0.3 0.4 0.5 0.6 0.7 un 0 1000 20000.75 0.8 0.85 0.9 0.95 cts 0 1000 20000 0.2 0.4 0.6 0.8 bn   POSfeature Hints (no gaz) Baseline (no gaz) Hints (w/ gaz) Baseline (w/ gaz) Selftrain (no gaz) Selftrain (w/ gaz) Figure 1: Results of varying the amount of NER labeled data, for a fixed amount (M = 8936) of syntactic data." ></td>
	<td class="line x" title="189:260	Hints Self-T Hints vs Base vs Base vs Self-T Win 34 28 15 Tie 1 7 20 Lose 0 0 0 Table 2: Comparison between hints, self-training and the (best) baseline for varying amount of unlabeled data." ></td>
	<td class="line x" title="190:260	by LaSO so that if the syntactic information is bad, it is relatively ignored." ></td>
	<td class="line x" title="191:260	 Self-training performance often degrades as the amount of syntactic data increases." ></td>
	<td class="line x" title="192:260	 The performance of learning with hints increases steadily with more syntactic data." ></td>
	<td class="line x" title="193:260	As before, we compare performance between the different models, declaring a tie if the difference is not statistically significant at the 95% level." ></td>
	<td class="line x" title="194:260	The results are in Table 2." ></td>
	<td class="line x" title="195:260	In experiments not reported here to save space, we experimented with several additional settings." ></td>
	<td class="line x" title="196:260	In one, we weight the unlabeled data in various ways: (1) to make it equal-weight to the labeled data; (2) at 10% weight; (3) according to the score produced by the first round of labeling." ></td>
	<td class="line x" title="197:260	None of these had a significant impact on scores; in a few cases performance went up bylessmuch1, in a few cases, performance went down about the same amount." ></td>
	<td class="line x" title="198:260	4.4 Two-sided Discriminative Learning In this section, we explore the use of two-sided discriminative learning to boost the performance of our syntactic chunking, part of speech tagging, and named-entity recognition software." ></td>
	<td class="line x" title="199:260	We continue to use LaSO (Daume III and Marcu, 2005) as the sequence labeling technique." ></td>
	<td class="line x" title="200:260	The results we present are based on attempting to improve the performance of a state-of-the-art system train on all of the training data." ></td>
	<td class="line x" title="201:260	(This is in contrast to the results in Section 4.3, in which the effect of using limited amounts of data was explored.)" ></td>
	<td class="line x" title="202:260	For the POS tagging and syntactic chunking, we being with all 8936 sentences of training data from CoNLL." ></td>
	<td class="line x" title="203:260	For the named entity recognition, we limit our presentation to results from the CoNLL 2003 NER shared task." ></td>
	<td class="line x" title="204:260	For this data, we have roughly 14k sentences of training data, all of which are used." ></td>
	<td class="line x" title="205:260	In both cases, we reserve 10% as development data." ></td>
	<td class="line x" title="206:260	The development data is use to do early stopping in LaSO." ></td>
	<td class="line x" title="207:260	As unlabeled data, we use 1msentences extracted from the North American National Corpus of En685 0 5000 10000 0.35 0.4 0.45 0.5 0.55 0.6 0.65 wl 0 5000 100000.55 0.6 0.65 0.7 0.75 0.8 nw 0 5000 100000.5 0.6 0.7 0.8 0.9 conll 0 5000 100000.5 0.6 0.7 0.8 0.9 bc 0 5000 100000.2 0.3 0.4 0.5 0.6 0.7 un 0 5000 100000.75 0.8 0.85 0.9 0.95 cts 0 5000 100000.2 0.4 0.6 0.8 1 bn   POSfeature Hints (no gaz) Baseline (no gaz) Hints (w/ gaz) Baseline (w/ gaz) Selftrain (no gaz) Selftrain (w/ gaz) Figure 2: Results of varying amount of syntactic data for a fixed amount of NER data (N = 200 sentences)." ></td>
	<td class="line oc" title="208:260	glish (previously used for self-training of parsers (McClosky et al., 2006))." ></td>
	<td class="line x" title="209:260	These 1m sentences were selected by dev-set relativization against the union of the two development data sets." ></td>
	<td class="line x" title="210:260	Following similar ideas to those presented by Blum and Mitchell (1998), we employ two slight modifications to the algorithm presented in Section 3.2." ></td>
	<td class="line x" title="211:260	First, in step (2b) instead of adding all allowable instances to the labeled data set, we only add the top R (for some hyper-parameter R), where top is determined by average model confidence for the two tasks." ></td>
	<td class="line x" title="212:260	Second, Instead of using the full unlabeled set to label at each iteration, we begin with a random subset of 10R unlabeled examples and another add random 10R every iteration." ></td>
	<td class="line x" title="213:260	We use the same baseline systems as in one-sided learning: a Baseline that learns the two tasks independently; a variant of the Baseline on which the output of the POS/chunker is used as a feature for the NER; a variant based on self-training; the hintsbased method." ></td>
	<td class="line x" title="214:260	In all cases, we do use gazetteers." ></td>
	<td class="line x" title="215:260	We run the hints-based model for 10 iterations." ></td>
	<td class="line x" title="216:260	For selftraining, we use 10R unlabeled examples (so that it had access to the same amount of unlabeled data as the hints-based learning after all 10 iterations)." ></td>
	<td class="line x" title="217:260	We used three values of R: 50, 100, 500." ></td>
	<td class="line x" title="218:260	We select the Chunking NER Baseline 94.2 87.5 w/POS N/A 88.0 Self-train R = 50 94.2 88.0 R = 100 94.3 88.6 R = 500 94.1 88.4 Hints R = 50 94.2 88.5 R = 100 94.3 89.1 R = 500 94.3 89.0 Table 3: Results on two-sided learning with hints." ></td>
	<td class="line x" title="219:260	best-performing model (by the dev data) over these ten iterations." ></td>
	<td class="line x" title="220:260	The results are in Table 3." ></td>
	<td class="line x" title="221:260	As we can see, performance for syntactic chunking is relatively stagnant: there are no significant improvements for any of the methods over the baseline." ></td>
	<td class="line x" title="222:260	This is not surprising: the form of the constraint function we use tells us a lot about the NER task, but relatively little about the syntactic chunking task." ></td>
	<td class="line x" title="223:260	In particular, it tells us nothing about phrases other than NPs." ></td>
	<td class="line x" title="224:260	On the other hand, for NER, we see that both self-training and learning with hints improve over the baseline." ></td>
	<td class="line x" title="225:260	The improvements are not 686 enormous, but are significant (at the 95% level, as measured by McNemars test)." ></td>
	<td class="line x" title="226:260	Unfortunately, the improvements for learning with hints over the selftraining model are only significant at the 90% level." ></td>
	<td class="line x" title="227:260	5 Discussion We have presented a method for simultaneously learning two tasks using prior knowledge about the relationship between their outputs." ></td>
	<td class="line x" title="228:260	This is related to joint inference (Daume III et al., 2006)." ></td>
	<td class="line x" title="229:260	However, we do not require that that a single data set be labeled for multiple tasks." ></td>
	<td class="line x" title="230:260	In all our examples, we use separate data sets for shallow parsing as for named-entity recognition." ></td>
	<td class="line x" title="231:260	Although all our experiments used the LaSO framework for sequence labeling, there is noting in our method that assumes any particular learner; alternatives include: conditional random fields (Lafferty et al., 2001), independent predictors (Punyakanok and Roth, 2001), maxmargin Markov networks (Taskar et al., 2005), etc. Our approach, both algorithmically and theoretically, is most related to ideas in co-training (Blum and Mitchell, 1998)." ></td>
	<td class="line x" title="232:260	The key difference is that in co-training, one assumes that the two views are on the inputs; here, we can think of the two output spaces as being the difference views and the compatibility function  being a method for reconciling these two views." ></td>
	<td class="line x" title="233:260	Like the pioneering work of Blum and Mitchell, the algorithm we employ in practice makes use of incrementally augmenting the unlabeled data and using model confidence." ></td>
	<td class="line x" title="234:260	Also like that work, we do not currently have a theoretical framework for this (more complex) model.5 It would also be interesting to explore soft hints, where the range of  is [0,1] rather than{0,1}." ></td>
	<td class="line x" title="235:260	Recently, Ganchev et al.(2008) proposed a coregularization framework for learning across multiple related tasks with different output spaces." ></td>
	<td class="line x" title="237:260	Their approach hinges on a constrained EM framework and addresses a quite similar problem to that addressed by this paper." ></td>
	<td class="line x" title="238:260	Chang et al.(2008) also propose a semisupervised learning approach quite similar to our own model." ></td>
	<td class="line x" title="240:260	The show very promising results in the context of semantic role labeling." ></td>
	<td class="line x" title="241:260	5Dasgupta et al.(2001) proved, three years later, that a formal model roughly equivalent to the actual Blum and Mitchell algorithm does have solid theoretical foundations." ></td>
	<td class="line x" title="243:260	Given the apparent (very!)" ></td>
	<td class="line x" title="244:260	recent interest in this problem, it would be ideal to directly compare the different approaches." ></td>
	<td class="line x" title="245:260	In addition to an analysis of the theoretical properties of the algorithm presented, the most compelling avenue for future work is to apply this framework to other task pairs." ></td>
	<td class="line x" title="246:260	With a little thought, one can imagine formulating compatibility functions between tasks like discourse parsing and summarization (Marcu, 2000), parsing and word alignment, or summarization and information extraction." ></td>
	<td class="line x" title="247:260	Acknowledgments Many thanks to three anonymous reviewers of this papers whose suggestions greatly improved the work and the presentation." ></td>
	<td class="line x" title="248:260	This work was partially funded by NSF grant IIS 0712764." ></td>
	<td class="line x" title="249:260	A Proofs The proof of Theorem 1 closes follows that of Blum and Mitchell (1998)." ></td>
	<td class="line x" title="250:260	Proof (Theorem 1, sketch)." ></td>
	<td class="line x" title="251:260	Use the following notation: ck = PrD[h(x) = k], pl = PrD[f(x) = l], ql|k = PrD[f(x) = l|h(x) = k]." ></td>
	<td class="line x" title="252:260	Denote by A the set of outputs that satisfy the constraints." ></td>
	<td class="line x" title="253:260	We are interested in the probability that h(x) is erroneous, given that h(x) satisfies the constraints: p(h(x)A\{l}|f(x) = l) = summationdisplay kA\{l} p(h(x) = k|f(x) = l) = summationdisplay kA\{l} ckql|k/pl  summationdisplay kA ck(|Y|1 +epsilon1 summationdisplay lnegationslash=k 1/pl)2 summationdisplay kA ck(|Y|1) Here, the second step is Bayes rule plus definitions, the third step is by the weak initial hypothesis assumption, and the last step is by algebra." ></td>
	<td class="line x" title="254:260	Thus, in order to get a probability of error at most , we needsummationtext kAck = Pr[h(x)A]/(2(|Y|1))." ></td>
	<td class="line x" title="255:260	The proof of Corollary 1 is straightforward." ></td>
	<td class="line x" title="256:260	Proof (Corollary 1, sketch)." ></td>
	<td class="line x" title="257:260	Write out the probability of error as a double sum over true labels y1,y2 and predicted labels y1,y2 subject to (y1,y2)." ></td>
	<td class="line x" title="258:260	Use the uncorrelation assumption and Bayes to split this into the product two terms as in the proof of Theorem 1." ></td>
	<td class="line x" title="259:260	Bound as before." ></td>
	<td class="line x" title="260:260	687" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="I08-2097
Learning Reliability of Parses for Domain Adaptation of Dependency Parsing
Kawahara, Daisuke;Uchimoto, Kiyotaka;"></td>
	<td class="line x" title="1:160	Learning Reliability of Parses for Domain Adaptation of Dependency Parsing Daisuke Kawahara and Kiyotaka Uchimoto National Institute of Information and Communications Technology, 3-5 Hikaridai Seika-cho Soraku-gun, Kyoto, 619-0289, Japan {dk, uchimoto}@nict.go.jp Abstract The accuracy of parsing has exceeded 90% recently, but this is not high enough to use parsing results practically in natural languageprocessing(NLP)applicationssuchas paraphrase acquisition and relation extraction." ></td>
	<td class="line x" title="2:160	We present a method for detecting reliable parses out of the outputs of a single dependency parser." ></td>
	<td class="line x" title="3:160	This technique is also applied to domain adaptation of dependency parsing." ></td>
	<td class="line x" title="4:160	Our goal was to improve the performance of a state-of-the-art dependency parser on the data set of the domain adaptation track of the CoNLL 2007 shared task, a formidable challenge." ></td>
	<td class="line x" title="5:160	1 Introduction Dependency parsing has been utilized in a variety of natural language processing (NLP) applications, such as paraphrase acquisition, relation extraction and machine translation." ></td>
	<td class="line x" title="6:160	For newspaper articles, the accuracy of dependency parsers exceeds 90% (for English), but it is still not sufcient for practical use in these NLP applications." ></td>
	<td class="line x" title="7:160	Moreover, the accuracy declinessignicantlyforout-of-domaintext, suchas weblogsandwebpages,whichhavecommonlybeen used as corpora." ></td>
	<td class="line x" title="8:160	From this point of view, it is important to consider the following points to use a parser practically in applications:  to select reliable parses, especially for knowledge acquisition,  and to adapt the parser to new domains." ></td>
	<td class="line x" title="9:160	This paper proposes a method for selecting reliable parses from parses output by a single dependency parser." ></td>
	<td class="line x" title="10:160	We do not use an ensemble method based on multiple parsers, but use only a single parser, because speed and efciency are important when processing a massive volume of text." ></td>
	<td class="line x" title="11:160	The resulting highly reliable parses would be useful to automatically construct dictionaries and knowledge bases, such as case frames (Kawahara and Kurohashi, 2006)." ></td>
	<td class="line x" title="12:160	Furthermore, we incorporate the reliable parses we obtained into the dependency parser to achieve domain adaptation." ></td>
	<td class="line x" title="13:160	The CoNLL 2007 shared task tackled domain adaptation of dependency parsers for the rst time (Nivre et al., 2007)." ></td>
	<td class="line x" title="14:160	Sagae and Tsujii applied an ensemble method to the domain adaptation track and achieved the highest score (Sagae and Tsujii, 2007)." ></td>
	<td class="line x" title="15:160	They rst parsed in-domain unlabeled sentences using two parsers trained on out-of-domain labeled data." ></td>
	<td class="line x" title="16:160	Then, they extracted identical parses that were produced by the two parsers and added them to the original (out-of-domain) training set to train a domain-adapted model." ></td>
	<td class="line x" title="17:160	Dredze et al. yielded the second highest score1 in the domain adaptation track (Dredze et al., 2007)." ></td>
	<td class="line x" title="18:160	However, their results were obtained without adaptation." ></td>
	<td class="line x" title="19:160	They concluded that it is very difcult to substantially improve the target domain performance over that of a state-of-the-art parser." ></td>
	<td class="line x" title="20:160	To conrm this, we parsed the test set (CHEM) of the domain adaptationtrackbyusingoneofthebestdependency parsers, second-order MSTParser (McDonald et al., 1Dredze et al. achieved the second highest score on the CHEM test set for unlabeled dependency accuracy." ></td>
	<td class="line x" title="21:160	709 2006)2." ></td>
	<td class="line x" title="22:160	Though this parser was trained on the provided out-of-domain (Penn Treebank) labeled data, surprisingly, its accuracy slightly outperformed the highest score achieved by Sagae and Tsujii (unlabeled dependency accuracy: 83.58 > 83.42 (Sagae and Tsujii, 2007))." ></td>
	<td class="line x" title="23:160	Our goal is to improve a stateof-the-art parser on this domain adaptation track." ></td>
	<td class="line x" title="24:160	Dredze et al. also indicated that unlabeled dependency parsing is not robust to domain adaptation (Dredze et al., 2007)." ></td>
	<td class="line x" title="25:160	This paper therefore focuses on unlabeled dependency parsing." ></td>
	<td class="line x" title="26:160	2 Related Work We have already described the domain adaptation track of the CoNLL 2007 shared task." ></td>
	<td class="line x" title="27:160	For the multilingual dependency parsing track, which was the other track of the shared task, Nilsson et al. achieved the best performance using an ensemble method (Hall et al., 2007)." ></td>
	<td class="line x" title="28:160	They used a method of combining several parsers outputs in the framework of MST parsing (Sagae and Lavie, 2006)." ></td>
	<td class="line x" title="29:160	This method does not select parses, but considers all the output parses with weights to decide a nal parse of a given sentence." ></td>
	<td class="line x" title="30:160	Reichart and Rappoport also proposed an ensemble method to select high-quality parses from the outputs of constituency parsers (Reichart and Rappoport, 2007a)." ></td>
	<td class="line x" title="31:160	They regarded parses as being of high quality if 20 different parsers agreed." ></td>
	<td class="line x" title="32:160	They did notapplytheirmethodtodomainadaptationorother applications." ></td>
	<td class="line x" title="33:160	Reranking methods for parsing have a relation to parse selection." ></td>
	<td class="line x" title="34:160	They rerank the n-best parses that are output by a generative parser using a lot of lexical and syntactic features (Collins and Koo, 2005; Charniak and Johnson, 2005)." ></td>
	<td class="line x" title="35:160	There are several related methods for 1-best outputs, such as revision learning (Nakagawa et al., 2002) and transformation-based learning (Brill, 1995) for partof-speech tagging." ></td>
	<td class="line x" title="36:160	Attardi and Ciaramita proposed a method of tree revision learning for dependency parsing (Attardi and Ciaramita, 2007)." ></td>
	<td class="line x" title="37:160	As for the use of unlabeled data, self-training methods have been successful in recent years." ></td>
	<td class="line x" title="38:160	McClosky et al. improved a state-of-the-art constituency parser by 1.1% using self-training (Mc2http://sourceforge.net/projects/mstparser/ Table 1: Labeled and unlabeled data provided for the shared task." ></td>
	<td class="line x" title="39:160	The labeled PTB data is used for training, and the labeled BIO data is used for development." ></td>
	<td class="line x" title="40:160	ThelabeledCHEMdataisusedforthenal test." ></td>
	<td class="line x" title="41:160	name source labeled unlabeled PTB Penn Treebank 18,577 1,625,606 BIO Penn BioIE 200 369,439 CHEM Penn BioIE 200 396,128 Closkyetal., 2006a)." ></td>
	<td class="line oc" title="42:160	Theyalsoappliedself-training to domain adaptation of a constituency parser (McClosky et al., 2006b)." ></td>
	<td class="line o" title="43:160	Their method simply adds parsed unlabeled data without selecting it to the training set." ></td>
	<td class="line x" title="44:160	Reichart and Rappoport applied selftraining to domain adaptation using a small set of in-domain training data (Reichart and Rappoport, 2007b)." ></td>
	<td class="line x" title="45:160	Van Noord extracted bilexical preferences from a Dutch parsed corpus of 500M words without selection(vanNoord, 2007)." ></td>
	<td class="line x" title="46:160	Headdedsomefeaturesinto an HPSG (head-driven phrase structure grammar) parser to consider the bilexical preferences, and obtained an improvement of 0.5% against a baseline." ></td>
	<td class="line x" title="47:160	Kawahara and Kurohashi extracted reliable dependencies from automatic parses of Japanese sentences on the web to construct large-scale case frames (Kawahara and Kurohashi, 2006)." ></td>
	<td class="line x" title="48:160	Then they incorporated the constructed case frames into a probabilistic dependency parser, and outperformed their baseline parser by 0.7%." ></td>
	<td class="line x" title="49:160	3 The Data Set This paper uses the data set that was used in the CoNLL 2007 shared task (Nivre et al., 2007)." ></td>
	<td class="line x" title="50:160	Table 1 lists the data set provided for the domain adaptation track." ></td>
	<td class="line x" title="51:160	We pre-processed all the unlabeled sentences using a conditional random elds (CRFs)-based partof-speech tagger." ></td>
	<td class="line x" title="52:160	This tagger is trained on the PTB training set that consists of 18,577 sentences." ></td>
	<td class="line x" title="53:160	The features are the same as those in (Ratnaparkhi, 1996)." ></td>
	<td class="line x" title="54:160	As an implementation of CRFs, we used CRF++3." ></td>
	<td class="line x" title="55:160	If a method of domain adaptation is applied to the tagger, the accuracy of parsing unlabeled sentences will improve (Yoshida et al., 2007)." ></td>
	<td class="line x" title="56:160	This 3http://crfpp.sourceforge.net/ 710 paper, however, does not deal with domain adaptation of a tagger but focuses on that of a parser." ></td>
	<td class="line x" title="57:160	4 Learning Reliability of Parses Our approach assesses automatic parses of a single parser in order to select only reliable parses from them." ></td>
	<td class="line x" title="58:160	We compare automatic parses and their goldstandard ones, and regard accurate parses as positive examples and the remainder as negative examples." ></td>
	<td class="line x" title="59:160	Based on these examples, we build a binary classier that classies each sentence as reliable or not." ></td>
	<td class="line x" title="60:160	To precisely detect reliable parses, we make use of several linguistic features inspired by the notion of controlled language (Mitamura et al., 1991)." ></td>
	<td class="line x" title="61:160	That is to say, the reliability of parses is judged based on the degree of sentence difculty." ></td>
	<td class="line x" title="62:160	Beforedescribingourbasedependencyparserand the algorithm for detecting reliable parses, we rst explain the data sets used for them." ></td>
	<td class="line x" title="63:160	We prepared the following three labeled data sets to train the base dependency parser and the reliability detector." ></td>
	<td class="line x" title="64:160	PTB base train: training set for the base parser: 14,862 sentences PTB rel train: training set for reliability detector: 2,500 sentences4 BIO rel dev: development set for reliability detector: 200 sentences (= labeled BIO data) PTB base train is used to train the base dependency parser, and PTB rel train is used to train our reliability detector." ></td>
	<td class="line x" title="65:160	BIO rel dev is used for tuning the parameters of the reliability detector." ></td>
	<td class="line x" title="66:160	4.1 Base Dependency Parser We used the MSTParser (McDonald et al., 2006), which achieved top results in the CoNLL 2006 (CoNLL-X) shared task, as a base dependency parser." ></td>
	<td class="line x" title="67:160	To enable second-order features, the parameter order was set to 2." ></td>
	<td class="line x" title="68:160	The other parameters were set to default." ></td>
	<td class="line x" title="69:160	We used PTB base train (14,862 sentences) to train this parser." ></td>
	<td class="line x" title="70:160	4.2 Algorithm to Detect Reliable Parses Webuiltabinaryclassierfordetectingreliablesentences from a set of automatic parses produced by 41,215 labeled PTB sentences are left as another development set for the reliability detector, but they are not used in this paper." ></td>
	<td class="line x" title="71:160	the base dependency parser." ></td>
	<td class="line x" title="72:160	We used support vector machines (SVMs) as a binary classier with a third-degree polynomial kernel." ></td>
	<td class="line x" title="73:160	We parsed PTB rel train (2,500 sentences) usingthebaseparser, andevaluatedeachsentencewith the metric of unlabeled dependency accuracy." ></td>
	<td class="line x" title="74:160	We regarded the sentences whose accuracy is better than a threshold, , as positive examples, and the others as negative ones." ></td>
	<td class="line x" title="75:160	In this experiment, we set the accuracy threshold  at 100%." ></td>
	<td class="line x" title="76:160	As a result, 736 out of 2,500 examples (sentences) were judged to be positive." ></td>
	<td class="line x" title="77:160	To evaluate the reliability of parses, we take advantage of the following features that can be related to the difculty of sentences." ></td>
	<td class="line x" title="78:160	sentence length: The longer the sentence is, the poorer the parser performs (McDonald and Nivre, 2007)." ></td>
	<td class="line x" title="79:160	We determine sentence length by the number of words." ></td>
	<td class="line x" title="80:160	dependency lengths: Long-distance dependencies exhibit bad performance (McDonald and Nivre, 2007)." ></td>
	<td class="line x" title="81:160	We calculate the average of the dependency length of each word." ></td>
	<td class="line x" title="82:160	difculty of vocabulary: It is hard for supervisedparserstolearndependenciesthatincludelowfrequency words." ></td>
	<td class="line x" title="83:160	We count word frequencies in the training data and make a word list in descending order of frequency." ></td>
	<td class="line x" title="84:160	For a given sentence, we calculate the average frequency rank of each word." ></td>
	<td class="line x" title="85:160	number of unknown words: Similarly, dependency accuracy for unknown words is notoriously poor." ></td>
	<td class="line x" title="86:160	We count the number of unknown words in a given sentence." ></td>
	<td class="line x" title="87:160	number of commas: Sentences with multiple commas are difcult to parse." ></td>
	<td class="line x" title="88:160	We count the number of commas in a given sentence." ></td>
	<td class="line x" title="89:160	number of conjunctions (and/or): Sentences with coordinate structures are also difcult to parse (Kurohashi and Nagao, 1994)." ></td>
	<td class="line x" title="90:160	We count the number of coordinate conjunctions (and/or) in a given sentence." ></td>
	<td class="line x" title="91:160	To apply these features to SVMs in practice, the numbers are binned at a certain interval for each feature." ></td>
	<td class="line x" title="92:160	For instance, the number of conjunctions is split into four bins: 0, 1, 2 and more than 2." ></td>
	<td class="line x" title="93:160	711 Table 2: Example BIO sentences judged as reliable." ></td>
	<td class="line x" title="94:160	The underlined words have incorrect modifying heads." ></td>
	<td class="line x" title="95:160	dep." ></td>
	<td class="line x" title="96:160	accuracy sentences judged as reliable 12/12 (100%) No mutations resulting in truncation of the APC protein were found . 12/13 (92%) Conventional imaging techniques did not show two in 10 of these patients . 6/6 (100%) Pancreatic juice was sampled endoscopically . 11/12 (92%) The specicity of p53 mutation for pancreatic cancer is very high . 9/10 (90%) K-ras mutations are early genetic changes in colon cancer .  0  10  20  30  40  50  60  70  80  90  100  80  82  84  86  88  90  92  94  96  98  100 Sentence coverage (%) Dependency accuracy (%) Figure 1: Accuracy-coverage curve on BIO rel dev." ></td>
	<td class="line x" title="97:160	4.3 Experiments on Detecting Reliable Parses We conducted an experiment on detecting the reliability of parses." ></td>
	<td class="line x" title="98:160	Our detector was applied to the automatic parses of BIO rel dev, and only reliable parses were selected from them." ></td>
	<td class="line x" title="99:160	When parsing this set, the POS tags contained in the set were substituted with automatic POS tags because it is preferable to have the same environment as when applying the parser to unlabeled data." ></td>
	<td class="line x" title="100:160	We evaluated unlabeled dependency accuracy of the extracted parses." ></td>
	<td class="line x" title="101:160	The accuracy-coverage curve shown in Figure 1 was obtained by changing the soft margin parameter C 5 of SVMs from 0.0001 to 10." ></td>
	<td class="line x" title="102:160	In this gure, the coverage is the ratio of selected sentences out of all the sentences (200 sentences), and the accuracy is unlabeled dependency accuracy." ></td>
	<td class="line x" title="103:160	A coverage of 100% indicates that the accuracy of 200 sentences without any selection was 80.85%." ></td>
	<td class="line x" title="104:160	If the soft margin parameter C is set to 0.001, we can obtain 19 sentences out of 200 at a dependency accuracy of 93.85% (183/195)." ></td>
	<td class="line x" title="105:160	The average sentence length was 10.3 words." ></td>
	<td class="line x" title="106:160	Out of obtained 19 sentences, 14 sentences achieved a dependency accuracy of 100%, and thus the precision of the reliability detector itself was 73.7% (14/19)." ></td>
	<td class="line x" title="107:160	Out of 200 sentences, 36 sentences were correctly parsed by the 5A higher soft margin value allows more classication errors, and thus leads to the increase of recall and the decrease of precision." ></td>
	<td class="line x" title="108:160	base parser, and thus the recall is 38.9% (14/36)." ></td>
	<td class="line x" title="109:160	Table 2 shows some sentences that were evaluated as reliable using the above setting (C = 0.001)." ></td>
	<td class="line x" title="110:160	Major errors were caused by prepositional phrase (PP)attachment." ></td>
	<td class="line x" title="111:160	To improve the accuracy of detecting reliable parses, it would be necessary to consider the numberofPP-attachmentambiguitiesinagivensentence as a feature." ></td>
	<td class="line x" title="112:160	5 Domain Adaptation of Dependency Parsing For domain adaptation, we adopt a self-training method." ></td>
	<td class="line x" title="113:160	We combine in-domain unlabeled (automatically labeled) data with out-of-domain labeled data to make a training set." ></td>
	<td class="line x" title="114:160	There are many possible methods for combining unlabeled and labeled data (Daume III, 2007), but we simply concatenate unlabeled data with labeled data to see the effectiveness of the selected reliable parses." ></td>
	<td class="line x" title="115:160	The in-domain unlabeled data to be added are selected by the reliability detector." ></td>
	<td class="line x" title="116:160	We set the soft margin parameter at 0.001 to extract highly reliable parses." ></td>
	<td class="line x" title="117:160	As mentioned in the previous section, the accuracy of selected parses was approximately 94%." ></td>
	<td class="line x" title="118:160	We parsed the unlabeled sentences of BIO and CHEM(approximately400Ksentencesforeach)using the base dependency parser that is trained on the entire PTB labeled data." ></td>
	<td class="line x" title="119:160	Then, we applied the reliability detector to these parsed sentences to obtain 31,266 sentences for BIO and 31,470 sentences for CHEM." ></td>
	<td class="line x" title="120:160	We call the two sets of obtained sentences  BIO pool and  CHEM pool . For each training set of the experiments described below, a certain number of sentences are randomly selected from the pool and combined with the entire out-of-domain (PTB) labeled data." ></td>
	<td class="line x" title="121:160	5.1 Experiment on BIO Development Data We rst conducted an experiment of domain adaptation using the BIO development set." ></td>
	<td class="line x" title="122:160	712  83  83.5  84  84.5  85  0  5000  10000  15000  20000  25000 Accuracy (%) Number of Unlabeled Sentences reliable parsesrandomly selected parses without addition Figure 2: Dependency accuracies on BIO when the number of added unlabeled data is changed." ></td>
	<td class="line x" title="123:160	Figure 2 shows how the accuracy changes when the number of added reliable parses is changed." ></td>
	<td class="line x" title="124:160	The solid line represents our proposed method, and the dotted line with points represents a baseline method." ></td>
	<td class="line x" title="125:160	This baseline is a self-training method that simply adds unlabeled data without selection to the PTB labeled data." ></td>
	<td class="line x" title="126:160	Each experimental result is the average of ve trials done to randomly select a certain number of parses from the BIO pool." ></td>
	<td class="line x" title="127:160	The horizontal dotted line (84.07%) represents the accuracy of the parser without adding unlabeled data (trained only on the PTB labeled data)." ></td>
	<td class="line x" title="128:160	From this gure, we can see that the proposed method always outperforms the baseline by approximately 0.4%." ></td>
	<td class="line x" title="129:160	The best accuracy was achieved when 18,000 unlabeled parses were added." ></td>
	<td class="line x" title="130:160	However, if more than 18,000 sentences are added, the accuracy declines." ></td>
	<td class="line x" title="131:160	This can be attributed to the balance of the number of labeled data and unlabeled data." ></td>
	<td class="line x" title="132:160	Since the number of added unlabeled data is more than the number of labeled data, the entire training set might be unreliable, though the accuracy of added unlabeled data is relatively high." ></td>
	<td class="line x" title="133:160	To address this problem, it is necessary to weigh labeled data or to change the way information from acquired unlabeled data is handled." ></td>
	<td class="line x" title="134:160	5.2 Experiment on CHEM Test Data The addition of 18,000 sentences showed the highest accuracy for the BIO development data." ></td>
	<td class="line x" title="135:160	To adapt the parser to the CHEM test set, we used 18,000 reliable unlabeled sentences from the CHEM pool with the PTB labeled sentences to train the parser." ></td>
	<td class="line x" title="136:160	Table 3 lists the experimental results." ></td>
	<td class="line x" title="137:160	In this table, the Table 3: Experimental results on CHEM test data." ></td>
	<td class="line x" title="138:160	system accuracy PTB+unlabel (18,000 sents.)" ></td>
	<td class="line x" title="139:160	84.12 only PTB (baseline) 83.58 1st (Sagae and Tsujii, 2007) 83.42 2nd (Dredze et al., 2007) 83.38 3rd (Attardi et al., 2007) 83.08 third row lists the three highest scores of the domain adaptation track of the CoNLL 2007 shared task." ></td>
	<td class="line x" title="140:160	The baseline parser was trained only on the PTB labeled data (as described in Section 1)." ></td>
	<td class="line x" title="141:160	The proposed method (PTB+unlabel (18,000 sents.)) outperformed the baseline by approximately 0.5%, and also beat all the systems submitted to the domain adaptation track." ></td>
	<td class="line x" title="142:160	These systems include an ensemble method (Sagae and Tsujii, 2007) and an approach of tree revision learning with a selection method of only using short training sentences (shorter than 30 words) (Attardi et al., 2007)." ></td>
	<td class="line x" title="143:160	6 Discussion and Conclusion This paper described a method for detecting reliable parses out of the outputs of a single dependency parser." ></td>
	<td class="line x" title="144:160	This technique was also applied to domain adaptation of dependency parsing." ></td>
	<td class="line x" title="145:160	To extract reliable parses, we did not adopt an ensemble method, but used a single-parser approach because speed and efciency are important in processing a gigantic volume of text to benet knowledge acquisition." ></td>
	<td class="line x" title="146:160	In this paper, we employed the MSTParser, which can process 3.9 sentences/s on a XEON 3.0GHz machine in spite of the time complexity of O(n3)." ></td>
	<td class="line x" title="147:160	If greater efciency is required, it is possible to apply a pre-lter that removes long sentences (e.g., longer than 30 words), which are seldom selected by the reliability detector." ></td>
	<td class="line x" title="148:160	In addition, our method does not depend on a particular parser, and can be applied to other state-of-theart parsers, such as Malt Parser (Nivre et al., 2006), which is a feature-rich linear-time parser." ></td>
	<td class="line x" title="149:160	In general, it is very difcult to improve the accuracy of the best performing systems by using unlabeled data." ></td>
	<td class="line pc" title="150:160	There are only a few successful studies, such as (Ando and Zhang, 2005) for chunking and (McClosky et al., 2006a; McClosky et al., 2006b) on constituency parsing." ></td>
	<td class="line x" title="151:160	We succeeded in boosting the accuracy of the second-order MST parser, which is 713 a state-of-the-art dependency parser, in the CoNLL 2007 domain adaptation task." ></td>
	<td class="line x" title="152:160	This was a difcult challenge as many participants in the task failed to obtain any meaningful gains from unlabeled data (Dredze et al., 2007)." ></td>
	<td class="line x" title="153:160	The key factor in our success was the extraction of only reliable information from unlabeled data." ></td>
	<td class="line x" title="154:160	However, that improvement was not satisfactory." ></td>
	<td class="line x" title="155:160	In order to achieve more gains, it is necessary to exploit a much larger number of unlabeled data." ></td>
	<td class="line x" title="156:160	In this paper, we adopted a simple method to combine unlabeled data with labeled data." ></td>
	<td class="line x" title="157:160	To use this method more effectively, we need to balance the labeled and unlabeled data very carefully." ></td>
	<td class="line x" title="158:160	However, this method is not scalable because the training time increases signicantlyasthesizeofatrainingsetexpands." ></td>
	<td class="line x" title="159:160	We can consider the information from more unlabeled data as features of machine learning techniques." ></td>
	<td class="line x" title="160:160	Another approach is to formalize a probabilistic model based on unlabeled data." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-1037
Improving Parsing and PP Attachment Performance with Sense Information
Agirre, Eneko;Baldwin, Timothy;Martinez, David;"></td>
	<td class="line x" title="1:210	Proceedings of ACL-08: HLT, pages 317325, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:210	c2008 Association for Computational Linguistics Improving Parsing and PP attachment Performance with Sense Information Eneko Agirre IXA NLP Group University of the Basque Country Donostia, Basque Country e.agirre@ehu.es Timothy Baldwin LT Group, CSSE University of Melbourne Victoria 3010 Australia tim@csse.unimelb.edu.au David Martinez LT Group, CSSE University of Melbourne Victoria 3010 Australia davidm@csse.unimelb.edu.au Abstract To date, parsers have made limited use of semantic information, but there is evidence to suggest that semantic features can enhance parse disambiguation." ></td>
	<td class="line x" title="3:210	This paper shows that semantic classes help to obtain signicant improvement in both parsing and PP attachment tasks." ></td>
	<td class="line x" title="4:210	We devise a gold-standard senseand parse tree-annotated dataset based on the intersection of the Penn Treebank and SemCor, and experiment with different approaches to both semantic representation and disambiguation." ></td>
	<td class="line x" title="5:210	For the Bikel parser, we achieved a maximal error reduction rate over the baseline parser of 6.9% and 20.5%, for parsing and PP-attachment respectively, using an unsupervised WSD strategy." ></td>
	<td class="line x" title="6:210	This demonstrates that word sense information can indeed enhance the performance of syntactic disambiguation." ></td>
	<td class="line x" title="7:210	1 Introduction Traditionally, parse disambiguation has relied on structural features extracted from syntactic parse trees, and made only limited use of semantic information." ></td>
	<td class="line x" title="8:210	There is both empirical evidence and linguistic intuition to indicate that semantic features can enhance parse disambiguation performance, however." ></td>
	<td class="line x" title="9:210	For example, a number of different parsers have been shown to benet from lexicalisation, that is, the conditioning of structural features on the lexical head of the given constituent (Magerman, 1995; Collins, 1996; Charniak, 1997; Charniak, 2000; Collins, 2003)." ></td>
	<td class="line x" title="10:210	As an example of lexicalisation, we may observe in our training data that knife often occurs as the manner adjunct of open in prepositional phrases headed by with (c.f. open with a knife), which would provide strong evidence for with (a) knife attaching to open and not box in open the box with a knife." ></td>
	<td class="line x" title="11:210	It would not, however, provide any insight into the correct attachment of with scissors in open the box with scissors, as the disambiguation model would not be able to predict that knife and scissors are semantically similar and thus likely to have the same attachment preferences." ></td>
	<td class="line x" title="12:210	In order to deal with this limitation, we propose to integrate directly the semantic classes of words into the process of training the parser." ></td>
	<td class="line x" title="13:210	This is done by substituting the original words with semantic codes that reect semantic classes." ></td>
	<td class="line x" title="14:210	For example, in the above example we could substitute both knife and scissors with the semantic class TOOL, thus relating the training and test instances directly." ></td>
	<td class="line x" title="15:210	We explore several models for semantic representation, based around WordNet (Fellbaum, 1998)." ></td>
	<td class="line x" title="16:210	Our approach to exploring the impact of lexical semantics on parsing performance is to take two state-of-the-art statistical treebank parsers and preprocess the inputs variously." ></td>
	<td class="line x" title="17:210	This simple method allows us to incorporate semantic information into the parser without having to reimplement a full statistical parser, and also allows for maximum comparability with existing results in the treebank parsing community." ></td>
	<td class="line x" title="18:210	We test the parsers over both a PP attachment and full parsing task." ></td>
	<td class="line x" title="19:210	In experimenting with different semantic representations,werequiresomestrategytodisambiguate the semantic class of polysemous words in context (e.g. determining for each instance of crane whether it refers to an animal or a lifting device)." ></td>
	<td class="line x" title="20:210	We explore a number of disambiguation strategies, including the use of hand-annotated (gold-standard) senses, the 317 use of the most frequent sense, and an unsupervised word sense disambiguation (WSD) system." ></td>
	<td class="line x" title="21:210	This paper shows that semantic classes help to obtain signicant improvements for both PP attachment and parsing." ></td>
	<td class="line x" title="22:210	We attain a 20.5% error reduction for PP attachment, and 6.9% for parsing." ></td>
	<td class="line x" title="23:210	These results are achieved using most frequent sense information, which surprisingly outperforms both goldstandard senses and automatic WSD." ></td>
	<td class="line x" title="24:210	The results are notable in demonstrating that very simple preprocessing of the parser input facilitates signicantimprovementsinparserperformance." ></td>
	<td class="line x" title="25:210	We provide the rst denitive results that word sense information can enhance Penn Treebank parser performance, building on earlier results of Bikel (2000) and Xiong et al.(2005)." ></td>
	<td class="line x" title="27:210	Given our simple procedure for incorporating lexical semantics into the parsing process, our hope is that this research will open the door to further gains using more sophisticated parsing models and richer semantic options." ></td>
	<td class="line x" title="28:210	2 Background This research is focused on applying lexical semantics in parsing and PP attachment tasks." ></td>
	<td class="line x" title="29:210	Below, we outline these tasks." ></td>
	<td class="line x" title="30:210	Parsing As our baseline parsers, we use two state-of-theart lexicalised parsing models, namely the Bikel parser (Bikel, 2004) and Charniak parser (Charniak, 2000)." ></td>
	<td class="line x" title="31:210	While a detailed description of the respective parsing models is beyond the scope of this paper, it is worth noting that both parsers induce a context free grammar as well as a generative parsing model from a training set of parse trees, and use a development set to tune internal parameters." ></td>
	<td class="line x" title="32:210	Traditionally, the two parsers have been trained and evaluated over the WSJ portion of the Penn Treebank (PTB: Marcus et al.(1993))." ></td>
	<td class="line x" title="34:210	We diverge from this norm in focusing exclusively on a sense-annotated subset of the Brown Corpus portion of the Penn Treebank, in order to investigate the upper bound performance of the models given gold-standard sense information." ></td>
	<td class="line x" title="35:210	PP attachment in a parsing context Prepositional phrase attachment (PP attachment) is the problem of determining the correct attachment site for a PP, conventionally in the form of the noun or verb in a V NP PP structure (Ratnaparkhi et al., 1994; Mitchell, 2004)." ></td>
	<td class="line x" title="36:210	For instance, in I ate a pizza with anchovies, the PP with anchovies could attach either to the verb (c.f. ate with anchovies) or to the noun (c.f. pizza with anchovies), of which the noun is the correct attachment site." ></td>
	<td class="line x" title="37:210	With I ate a pizza with friends, on the other hand, the verb is the correct attachment site." ></td>
	<td class="line x" title="38:210	PP attachment is a structural ambiguity problem, and as such, a subproblem of parsing." ></td>
	<td class="line x" title="39:210	Traditionally the so-called RRR data (Ratnaparkhi et al., 1994) has been used to evaluate PP attachment algorithms." ></td>
	<td class="line x" title="40:210	RRR consists of 20,081 training and 3,097 test quadruples of the form (v,n1,p,n2), where the attachment decision is either v or n1." ></td>
	<td class="line x" title="41:210	The best published results over RRR are those of Stetina and Nagao (1997), who employ WordNet sense predictions from an unsupervised WSD method within a decision tree classier." ></td>
	<td class="line x" title="42:210	Their work is particularly inspiring in that it signicantlyoutperformedtheplethoraoflexicalisedprobabilisticmodelsthathadbeenproposedtothatpoint, and has not been beaten in later attempts." ></td>
	<td class="line x" title="43:210	In a recent paper, Atterer and Schutze (2007) criticised the RRR dataset because it assumes that an oracle parser provides the two hypothesised structurestochoosebetween." ></td>
	<td class="line x" title="44:210	Thisisneededtoderivethe fact that there are two possible attachment sites, as well as information about the lexical phrases, which are typically extracted heuristically from gold standard parses." ></td>
	<td class="line x" title="45:210	Atterer and Schutze argue that the only meaningful setting for PP attachment is within a parser, and go on to demonstrate that in a parser setting, the Bikel parser is competitive with the bestperforming dedicated PP attachment methods." ></td>
	<td class="line x" title="46:210	Any improvementinPPattachmentperformanceoverthe baseline Bikel parser thus represents an advancement in state-of-the-art performance." ></td>
	<td class="line x" title="47:210	That we specically present results for PP attachment in a parsing context is a combination of us supporting the new research direction for PP attachment established by Atterer and Schutze, and us wishing to reinforce the ndings of Stetina and Nagao that word sense information signicantly enhances PP attachment performance in this new setting." ></td>
	<td class="line x" title="48:210	Lexical semantics in parsing There have been a number of attempts to incorporate word sense information into parsing tasks." ></td>
	<td class="line x" title="49:210	The 318 most closely related research is that of Bikel (2000), who merged the Brown portion of the Penn TreebankwithSemCor(similarlytoourapproachinSection 4.1), and used this as the basis for evaluation of agenerativebilexicalmodelforjointWSDandparsing." ></td>
	<td class="line x" title="50:210	He evaluated his proposed model in a parsing context both with and without WordNet-based sense information, andfoundthattheintroductionofsense information either had no impact or degraded parse performance." ></td>
	<td class="line x" title="51:210	Theonlysuccessfulapplicationsofwordsenseinformation to parsing that we are aware of are Xiong et al.(2005) and Fujita et al.(2007)." ></td>
	<td class="line x" title="54:210	Xiong et al.(2005) experimented with rst-sense and hypernym features from HowNet and CiLin (both WordNets for Chinese) in a generative parse model applied to the Chinese Penn Treebank." ></td>
	<td class="line x" title="56:210	The combination of word sense and rst-level hypernyms produced a signicant improvement over their basic model." ></td>
	<td class="line x" title="57:210	Fujita et al.(2007) extended this work in implementing a discriminative parse selection model incorporating word sense information mapped onto upper-level ontologies of differing depths." ></td>
	<td class="line x" title="59:210	Based on gold-standard sense information, they achieved large-scale improvements over a basic parse selection model in the context of the Hinoki treebank." ></td>
	<td class="line x" title="60:210	Othernotableexamplesofthesuccessfulincorporation of lexical semantics into parsing, not through word sense information but indirectly via selectional preferences, are Dowding et al.(1994) and Hektoen (1997)." ></td>
	<td class="line x" title="62:210	For a broader review of WSD in NLP applications, see Resnik (2006)." ></td>
	<td class="line x" title="63:210	3 Integrating Semantics into Parsing Our approach to providing the parsers with sense information is to make available the semantic denotation of each word in the form of a semantic class." ></td>
	<td class="line x" title="64:210	This is done simply by substituting the original words with semantic codes." ></td>
	<td class="line x" title="65:210	For example, in the earlier example of open with a knife we could substitute both knife and scissors with the class TOOL, and thus directly facilitate semantic generalisation within the parser." ></td>
	<td class="line x" title="66:210	There are three main aspects that we have to consider in this process: (i) the semantic representation, (ii) semantic disambiguation, and (iii) morphology." ></td>
	<td class="line x" title="67:210	There are many ways to represent semantic relationships between words." ></td>
	<td class="line x" title="68:210	In this research we opt for a class-based representation that will map semantically-related words into a common semantic category." ></td>
	<td class="line x" title="69:210	Our choice for this work was the WordNet 2.1lexicaldatabase, inwhichsynonymsaregrouped into synsets, which are then linked via an IS-A hierarchy." ></td>
	<td class="line x" title="70:210	WordNet contains other types of relations such as meronymy, but we did not use them in this research." ></td>
	<td class="line x" title="71:210	With any lexical semantic resource, we have to be careful to choose the appropriate level of granularity for a given task: if we limit ourselves to synsets we will not be able to capture broader generalisations, such as the one between knife and scissors;1 ontheotherhandbygroupingwordsrelatedat a higher level in the hierarchy we could nd that we make overly coarse groupings (e.g. mallet, square and steel-wool pad are also descendants of TOOL in WordNet, none of which would conventionally be used as the manner adjunct of cut)." ></td>
	<td class="line x" title="72:210	We will test different levels of granularity in this work." ></td>
	<td class="line x" title="73:210	The second problem we face is semantic disambiguation." ></td>
	<td class="line x" title="74:210	The more ne-grained our semantic representation, the higher the average polysemy and the greater the need to distinguish between these senses." ></td>
	<td class="line x" title="75:210	For instance, if we nd the word crane in a context such as demolish a house with the crane, the ability to discern that this corresponds to the DEVICE and not ANIMAL sense of word will allow us to avoid erroneous generalisations." ></td>
	<td class="line x" title="76:210	This problem of identifying the correct sense of a word in context is known as word sense disambiguation (WSD: Agirre and Edmonds (2006))." ></td>
	<td class="line x" title="77:210	Disambiguating each word relative to its context of use becomes increasingly difcult for ne-grained representations (Palmer et al., 2006)." ></td>
	<td class="line x" title="78:210	We experiment with different ways of tackling WSD, using both gold-standard data and automatic methods." ></td>
	<td class="line x" title="79:210	Finally, when substituting words with semantic tags we have to decide how to treat different word forms of a given lemma." ></td>
	<td class="line x" title="80:210	In the case of English, this pertains most notably to verb inection and noun number, a distinction which we lose if we opt to map all word forms onto semantic classes." ></td>
	<td class="line x" title="81:210	For our current purposes we choose to substitute all word 1In WordNet 2.1, knife and scissors are sister synsets, both of which have TOOL as their 4th hypernym." ></td>
	<td class="line x" title="82:210	Only by mapping them onto their 1st hypernym or higher would we be able to capture the semantic generalisation alluded to above." ></td>
	<td class="line x" title="83:210	319 forms, but we plan to look at alternative representations in the future." ></td>
	<td class="line x" title="84:210	4 Experimental setting We evaluate the performance of our approach in two settings: (1) full parsing, and (2) PP attachment within a full parsing context." ></td>
	<td class="line x" title="85:210	Below, we outline the dataset used in this research and the parser evaluation methodology, explain the methodology used to perform PP attachment, present the different options for semantic representation, and nally detail the disambiguation methods." ></td>
	<td class="line x" title="86:210	4.1 Dataset and parser evaluation One of the main requirements for our dataset is the availability of gold-standard sense and parse tree annotations." ></td>
	<td class="line x" title="87:210	The gold-standard sense annotations allow us to perform upper bound evaluation of the relative impact of a given semantic representation on parsing and PP attachment performance, to contrast with the performance in more realistic semantic disambiguation settings." ></td>
	<td class="line x" title="88:210	The gold-standard parse tree annotations are required in order to carry out evaluation of parser and PP attachment performance." ></td>
	<td class="line x" title="89:210	The only publicly-available resource with these two characteristics at the time of this work was the subset of the Brown Corpus that is included in both SemCor (Landes et al., 1998) and the Penn Treebank (PTB).2 This provided the basis of our dataset." ></td>
	<td class="line x" title="90:210	After sentenceand word-aligning the SemCor and PTB data (discarding sentences where there was a difference in tokenisation), we were left with a total of 8,669 sentences containing 151,928 words." ></td>
	<td class="line x" title="91:210	Note that this dataset is smaller than the one described by Bikel (2000) in a similar exercise, the reason being our simple and conservative approach taken when merging the resources." ></td>
	<td class="line x" title="92:210	We relied on this dataset alone for all the experiments in this paper." ></td>
	<td class="line x" title="93:210	In order to maximise reproducibility and encourage further experimentation in the direction pioneered in this research, we partitioned the data into 3 sets: 80% training, 10% development and 10% test data." ></td>
	<td class="line x" title="94:210	This dataset is available on request to the research community." ></td>
	<td class="line x" title="95:210	2OntoNotes (Hovy et al., 2006) includes large-scale treebank and (selective) sense data, which we plan to use for future experiments when it becomes fully available." ></td>
	<td class="line x" title="96:210	Weevaluatetheparsersvialabelledbracketingrecall (R), precision (P) and F-score (F1)." ></td>
	<td class="line x" title="97:210	We use Bikels randomized parsing evaluation comparator3 (with p < 0.05 throughout) to test the statistical signicance of the results using word sense information, relative to the respective baseline parser using only lexical features." ></td>
	<td class="line x" title="98:210	4.2 PP attachment task Following Atterer and Schutze (2007), we wrote a script that, given a parse tree, identies instances of PP attachment ambiguity and outputs the (v,n1,p,n2) quadruple involved and the attachment decision." ></td>
	<td class="line x" title="99:210	This extraction system uses Collins rules (based on TREEP (Chiang and Bikel, 2002)) to locate the heads of phrases." ></td>
	<td class="line x" title="100:210	Over the combined gold-standard parsing dataset, our script extracted a total of 2,541 PP attachment quadruples." ></td>
	<td class="line x" title="101:210	As with the parsing data, we partitioned the data into 3 sets: 80% training, 10% development and 10% test data." ></td>
	<td class="line x" title="102:210	Once again, this dataset and the script used to extract the quadruples are available on request to the research community." ></td>
	<td class="line x" title="103:210	In order to evaluate the PP attachment performance of a parser, we run our extraction script over the parser output in the same manner as for the goldstandard data, and compare the extracted quadruples to the gold-standard ones." ></td>
	<td class="line x" title="104:210	Note that there is no guarantee of agreement in the quadruple membership between the extraction script and the gold standard, as the parser may have produced a parse which is incompatible with either attachment possibility." ></td>
	<td class="line x" title="105:210	A quadruple is deemed correct if: (1) it exists in the gold standard, and (2) the attachment decision is correct." ></td>
	<td class="line x" title="106:210	Conversely, it is deemed incorrect if: (1) it exists in the gold standard, and (2) the attachment decision is incorrect." ></td>
	<td class="line x" title="107:210	Quadruples not found in the gold standard are discarded." ></td>
	<td class="line x" title="108:210	Precision was measuredasthenumberofcorrectquadruplesdividedby the total number of correct and incorrect quadruples (i.e. all quadruples which are not discarded), and recall as the number of correct quadruples divided by the total number of gold-standard quadruples in the testset." ></td>
	<td class="line x" title="109:210	Thisevaluationmethodologycoincideswith that of Atterer and Schutze (2007)." ></td>
	<td class="line x" title="110:210	Statistical signicance was calculated based on 3www.cis.upenn.edu/dbikel/software.html 320 a modied version of the Bikel comparator (see above), once again with p < 0.05." ></td>
	<td class="line x" title="111:210	4.3 Semantic representation We experimented with a range of semantic representations, all of which are based on WordNet 2.1." ></td>
	<td class="line x" title="112:210	As mentioned above, words in WordNet are organised into sets of synonyms, called synsets." ></td>
	<td class="line x" title="113:210	Each synset in turn belongs to a unique semanticle (SF)." ></td>
	<td class="line x" title="114:210	There are a total of 45 SFs (1 for adverbs, 3 for adjectives, 15 for verbs, and 26 for nouns), based on syntactic and semantic categories." ></td>
	<td class="line x" title="115:210	A selection of SFs is presented in Table 1 for illustration purposes." ></td>
	<td class="line x" title="116:210	We experiment with both full synsets and SFs as instances of ne-grained and coarse-grained semantic representation, respectively." ></td>
	<td class="line x" title="117:210	As an example of the difference in these two representations, knife in its tool sense is in the EDGE TOOL USED AS A CUTTING INSTRUMENT singleton synset, and also in the ARTIFACT SF along with thousands of other words including cutter." ></td>
	<td class="line x" title="118:210	Note that these are the two extremes of semantic granularity in WordNet, and we plan to experiment with intermediate representation levelsinfutureresearch(c.f.LiandAbe(1998),McCarthy and Carroll (2003), Xiong et al.(2005), Fujita et al.(2007))." ></td>
	<td class="line x" title="121:210	As a hybrid representation, we tested the effect of merging words with their corresponding SF (e.g. knife+ARTIFACT )." ></td>
	<td class="line x" title="122:210	This is a form of semantic specialisation rather than generalisation, and allows the parser to discriminate between the different senses of each word, but not generalise across words." ></td>
	<td class="line x" title="123:210	For each of these three semantic representations, we experimented with substituting each of: (1) all open-class POSs (nouns, verbs, adjectives and adverbs), (2) nouns only, and (3) verbs only." ></td>
	<td class="line x" title="124:210	There are thus a total of 9 combinations of representation type and target POS." ></td>
	<td class="line x" title="125:210	4.4 Disambiguation methods For a given semantic representation, we need some form of WSD to determine the semantics of each token occurrence of a target word." ></td>
	<td class="line x" title="126:210	We experimented with three options: 1." ></td>
	<td class="line x" title="127:210	Gold-standard: Gold-standard annotations from SemCor." ></td>
	<td class="line x" title="128:210	This gives us the upper bound performance of the semantic representation." ></td>
	<td class="line x" title="129:210	SF ID DEFINITION adj.all all adjective clusters adj.pert relational adjectives (pertainyms) adj.ppl participial adjectives adv.all all adverbs noun.act nouns denoting acts or actions noun.animal nouns denoting animals noun.artifact nouns denoting man-made objects  verb.consumption verbs of eating and drinking verb.emotion verbs of feeling verb.perception verbs of seeing, hearing, feeling  Table 1: A selection of WordNet SFs 2." ></td>
	<td class="line x" title="130:210	First Sense (1ST): All token instances of a given word are tagged with their most frequent sense in WordNet.4 Note that the rst sense predictions are based largely on the same dataset as we use in our evaluation, such that the predictions are tuned to our dataset and not fully unsupervised." ></td>
	<td class="line x" title="131:210	3." ></td>
	<td class="line x" title="132:210	Automatic Sense Ranking (ASR): First sense tagging as for First Sense above, except that an unsupervised system is used to automatically predict the most frequent sense for each word based on an independent corpus." ></td>
	<td class="line x" title="133:210	The method we use to predict the rst sense is that of McCarthy et al.(2004), which was obtained using a thesaurus automatically created from the British National Corpus (BNC) applying the method of Lin (1998), coupled with WordNetbased similarity measures." ></td>
	<td class="line x" title="135:210	This method is fully unsupervised and completely unreliant on any annotations from our dataset." ></td>
	<td class="line x" title="136:210	In the case of SFs, we perform full synset WSD based on one of the above options, and then map the prediction onto the corresponding (unique) SF." ></td>
	<td class="line x" title="137:210	5 Results We present the results for each disambiguation approach in turn, analysing the results for parsing and PP attachment separately." ></td>
	<td class="line x" title="138:210	4There are some differences with the most frequent sense in SemCor, due to extra corpora used in WordNet development, and also changes in WordNet from the original version used for the SemCor tagging." ></td>
	<td class="line x" title="139:210	321 CHARNIAK BIKELSYSTEM R P F1 R P F1 Baseline .857 .808 .832 .837 .845 .841 SF .855 .809 .831 .847 .854 .850 SFn .860 .808 .833 .847 .853 .850 SFv .861 .811 .835 .847 .856 .851 word + SF .865 .814 .839 .837 .846 .842 word + SFn .862 .809 .835 .841 .850 .846 word + SFv .862 .810 .835 .840 .851 .845 Syn .863 .812 .837 .845 .853 .849 Synn .860 .807 .832 .841 .849 .845 Synv .863 .813 .837 .843 .851 .847 Table 2: Parsing results with gold-standard senses ( indicates that the recall or precision is signicantly better than baseline; the best performing method in each column is shown in bold) 5.1 Gold standard We disambiguated each token instance in our corpus according to the gold-standard sense data, and trained both the Charniak and Bikel parsers over each semantic representation." ></td>
	<td class="line x" title="140:210	We evaluated the parsers in full parsing and PP attachment contexts." ></td>
	<td class="line x" title="141:210	The results for parsing are given in Table 2." ></td>
	<td class="line x" title="142:210	The rows represent the three semantic representations (including whether we substitute only nouns, only verbs or all POS)." ></td>
	<td class="line x" title="143:210	We can see that in almost all cases the semantically-enriched representations improve over the baseline parsers." ></td>
	<td class="line x" title="144:210	These results are statistically signicant in some cases (as indicated by )." ></td>
	<td class="line x" title="145:210	The SFv representation produces the best results for Bikel (F-score 0.010 above baseline), while for Charniak the best performance is obtained with word+SF (F-score 0.007 above baseline)." ></td>
	<td class="line x" title="146:210	Comparing the two baseline parsers, Bikel achieves better precision and Charniak better recall." ></td>
	<td class="line x" title="147:210	Overall, Bikel obtains a superior F-score in all congurations." ></td>
	<td class="line x" title="148:210	The results for the PP attachment experiments using gold-standard senses are given in Table 3, both for the Charniak and Bikel parsers." ></td>
	<td class="line x" title="149:210	Again, the Fscore for the semantic representations is better than the baseline in all cases." ></td>
	<td class="line x" title="150:210	We see that the improvement is signicant for recall in most cases (particularly when using verbs), but not for precision (only Charniak over Synv and word+SFv for Bikel)." ></td>
	<td class="line x" title="151:210	For both parsers the best results are achieved with SFv, which was also the best conguration for parsing with Bikel." ></td>
	<td class="line x" title="152:210	The performance gain obtained here is larger than in parsing, which is in accordance with the ndings of Stetina and Nagao that lexical semantics has a considerable effect on PP attachment CHARNIAK BIKELSYSTEM R P F1 R P F1 Baseline .667 .798 .727 .659 .820 .730 SF .710 .808 .756 .714 .809 .758 SFn .671 .792 .726 .706 .818 .758 SFv .729 .823 .773 .733 .827 .778 word + SF .710 .801 .753 .706 .837 .766 word + SFn .698 .813 .751 .706 .829 .763 word + SFv .714 .805 .757 .706 .837 .766 Syn .722 .814 .765 .702 .825 .758 Synn .678 .805 .736 .690 .822 .751 Synv .702 .817 .755 .690 .834 .755 Table 3: PP attachment results with gold-standard senses ( indicatesthattherecallorprecisionissignicantlybetter than baseline; the best performing method in each column is shown in bold) performance." ></td>
	<td class="line x" title="153:210	As in full-parsing, Bikel outperforms Charniak, but in this case the difference in the baselines is not statistically signicant." ></td>
	<td class="line x" title="154:210	5.2 First sense (1ST) For this experiment, we use the rst sense data from WordNet for disambiguation." ></td>
	<td class="line x" title="155:210	The results for full parsing are given in Table 4." ></td>
	<td class="line x" title="156:210	Again, the performance is signicantly better than baseline in most cases, and surprisingly the results are even better than gold-standard in some cases." ></td>
	<td class="line x" title="157:210	We hypothesise that this is due to the avoidance of excessive fragmentation, as occurs with ne-grained senses." ></td>
	<td class="line x" title="158:210	The results are signicantly better for nouns, with SFn performing best." ></td>
	<td class="line x" title="159:210	Verbs seem to suffer from lack of disambiguation precision, especially for Bikel." ></td>
	<td class="line x" title="160:210	Here again, Charniak trails behind Bikel." ></td>
	<td class="line x" title="161:210	The results for the PP attachment task are shown in Table 5." ></td>
	<td class="line x" title="162:210	The behaviour is slightly different here, with Charniak obtaining better results than Bikel in most cases." ></td>
	<td class="line x" title="163:210	As was the case for parsing, the performance with 1ST reaches and in many instances surpasses gold-standard levels, achieving statistical signicance over the baseline in places." ></td>
	<td class="line x" title="164:210	Comparing the semantic representations, the best results are achieved with SFv, as we saw in the gold-standard PP-attachment case." ></td>
	<td class="line x" title="165:210	5.3 Automatic sense ranking (ASR) The nal option for WSD is automatic sense ranking, which indicates how well our method performs in a completely unsupervised setting." ></td>
	<td class="line x" title="166:210	The parsing results are given in Table 6." ></td>
	<td class="line x" title="167:210	We can see that the scores are very similar to those from 322 CHARNIAK BIKELSYSTEM R P F1 R P F1 Baseline .857 .807 .832 .837 .845 .841 SF .851 .804 .827 .843 .850 .846 SFn .863 .813 .837 .850 .854 .852 SFv .857 .808 .832 .843 .853 .848 word + SF .859 .810 .834 .833 .841 .837 word + SFn .862 .811 .836 .844 .851 .848 word + SFv .857 .808 .832 .831 .839 .835 Syn .857 .810 .833 .837 .844 .840 Synn .863 .812 .837 .844 .851 .848 Synv .860 .810 .834 .836 .844 .840 Table 4: Parsing results with 1ST ( indicates that the recallorprecisionissignicantlybetterthanbaseline; the bestperformingmethodineachcolumnisshowninbold) CHARNIAK BIKELSYSTEM R P F1 R P F1 Baseline .667 .798 .727 .659 .820 .730 SF .710 .808 .756 .702 .806 .751 SFn .671 .781 .722 .702 .829 .760 SFv .737 .836 .783 .718 .821 .766 word + SF .706 .811 .755 .694 .823 .753 word + SFn .690 .815 .747 .667 .810 .731 word + SFv .714 .805 .757 .710 .819 .761 Syn .725 .833 .776 .698 .828 .757 Synn .698 .828 .757 .667 .817 .734 Synv .722 .811 .763 .706 .818 .758 Table 5: PP attachment results with 1ST ( indicates that therecallorprecisionissignicantlybetterthanbaseline; the best performing method in each column is shown in bold) 1ST, with improvements in some cases, particularly for Charniak." ></td>
	<td class="line x" title="168:210	Again, the results are better for nouns, except for the case of SFv with Bikel." ></td>
	<td class="line x" title="169:210	Bikel outperforms Charniak in terms of F-score in all cases." ></td>
	<td class="line x" title="170:210	The PP attachment results are given in Table 7." ></td>
	<td class="line x" title="171:210	The results are similar to 1ST, with signicant improvements for verbs." ></td>
	<td class="line x" title="172:210	In this case, synsets slightly outperform SF." ></td>
	<td class="line x" title="173:210	Charniak performs better than Bikel, and the results for Synv are higher than the best obtained using gold-standard senses." ></td>
	<td class="line x" title="174:210	6 Discussion The results of the previous section show that the improvements in parsing results are small but signicant, for all three word sense disambiguation strategies (gold-standard, 1ST and ASR)." ></td>
	<td class="line x" title="175:210	Table 8 summarises the results, showing that the error reduction rate (ERR) over the parsing F-score is up to 6.9%, which is remarkable given the relatively supercial strategy for incorporating sense information into the parser." ></td>
	<td class="line x" title="176:210	Note also that our baseline results for the CHARNIAK BIKELSYSTEM R P F1 R P F1 Baseline .857 .807 .832 .837 .845 .841 SF .863 .815 .838 .845 .852 .849 SFn .862 .810 .835 .845 .850 .847 SFv .859 .810 .833 .846 .856 .851 word + SF .859 .810 .834 .836 .844 .840 word + SFn .865 .813 .838 .844 .852 .848 word + SFv .856 .806 .830 .832 .839 .836 Syn .856 .807 .831 .840 .847 .843 Synn .864 .813 .838 .844 .851 .847 Synv .857 .806 .831 .837 .845 .841 Table 6: Parsing results with ASR ( indicates that the recallorprecisionissignicantlybetterthanbaseline; the bestperformingmethodineachcolumnisshowninbold) CHARNIAK BIKELSYSTEM R P F1 R P F1 Baseline .667 .798 .727 .659 .820 .730 SF .733 .824 .776 .698 .805 .748 SFn .682 .791 .733 .671 .807 .732 SFv .733 .813 .771 .710 .812 .757 word + SF .714 .798 .754 .675 .800 .732 word + SFn .690 .807 .744 .659 .804 .724 word + SFv .706 .800 .750 .702 .814 .754 Syn .733 .827 .778 .694 .805 .745 Synn .686 .810 .743 .667 .806 .730 Synv .714 .816 .762 .714 .816 .762 Table7: PPattachmentresultswith ASR ( indicatesthat therecallorprecisionissignicantlybetterthanbaseline; the best performance in each column is shown in bold) dataset are almost the same as previous work parsing the Brown corpus with similar models (Gildea, 2001), which suggests that our dataset is representative of this corpus." ></td>
	<td class="line x" title="177:210	The improvement in PP attachment was larger (20.5% ERR), and also statistically signicant." ></td>
	<td class="line x" title="178:210	The results for PP attachment are especially important, as we demonstrate that the sense information has high utility when embedded within a parser, where the parser needs to rst identify the ambiguity and heads correctly." ></td>
	<td class="line x" title="179:210	Note that Atterer and Schutze (2007) have shown that the Bikel parser performs as well as the state-of-the-art in PP attachment, which suggestsourmethodimprovesoverthecurrentstateof-the-art." ></td>
	<td class="line x" title="180:210	The fact that the improvement is larger for PP attachment than for full parsing is suggestive of PP attachment being a parsing subtask where lexical semantic information is particularly important, supporting the ndings of Stetina and Nagao (1997) over a standalone PP attachment task." ></td>
	<td class="line x" title="181:210	We also observed that while better PP-attachment usually improves parsing, there is some small variation." ></td>
	<td class="line x" title="182:210	This 323 WSD TASK PAR BASE SEM ERR BEST Pars." ></td>
	<td class="line x" title="183:210	C .832 .839  4.2% word+SF GoldB .841 .851 6.3% SFv standard PP C .727 .773 16.9% SFv B .730 .778 17.8% SFv Pars." ></td>
	<td class="line x" title="184:210	C .832 .837  3.0% SFn, Synn 1ST B .841 .852  6.9% SFn PP C .727 .783  20.5% SFv B .730 .766 13.3% SFv Pars." ></td>
	<td class="line x" title="185:210	C .832 .838  3.6% SF, word+SFn, Synn ASR B .841 .851  6.3% SFv PP C .727 .778  18.7% Syn B .730 .762 11.9% Synv Table 8: Summary of F-score results with error reduction rates and the best semantic representation(s) for each setting (C = Charniak, B = Bikel) means that the best conguration for PP-attachment does not always produce the best results for parsing One surprising nding was the strong performance of the automatic WSD systems, actually outperforming the gold-standard annotation overall." ></td>
	<td class="line x" title="186:210	Our interpretation of this result is that the approach of annotating all occurrences of the same word with the same sense allows the model to avoid the data sparsenessassociatedwiththegold-standarddistinctions, as well as supporting the merging of different words into single semantic classes." ></td>
	<td class="line x" title="187:210	While the results for gold-standard senses were intended as an upper bound for WordNet-based sense information, in practice there was very little difference between gold-standard senses and automatic WSD in all cases barring the Bikel parser and PP attachment." ></td>
	<td class="line x" title="188:210	Comparing the two parsers, Charniak performs better than Bikel on PP attachment when automatic WSDisused,whileBikelperformsbetteronparsing overall." ></td>
	<td class="line x" title="189:210	Regarding the choice of WSD system, the results for both approaches are very similar, showing that ASR performs well, even if it does not require sense frequency information." ></td>
	<td class="line x" title="190:210	The analysis of performance according to the semantic representation is not so clear cut." ></td>
	<td class="line x" title="191:210	Generalising only verbs to semantic les (SFv) was the best option in most of the experiments, particularly for PP-attachment." ></td>
	<td class="line x" title="192:210	This could indicate that semantic generalisation is particularly important for verbs, more so than nouns." ></td>
	<td class="line x" title="193:210	Our hope is that this paper serves as the bridgehead for a new line of research into the impact of lexical semantics on parsing." ></td>
	<td class="line x" title="194:210	Notably, more could be done to ne-tune the semantic representation between the two extremes of full synsets and SFs." ></td>
	<td class="line x" title="195:210	One could also imagine that the appropriate level of generalisation differs across POS and even the relative syntactic role, e.g. ner-grained semantics are needed for the objects than subjects of verbs." ></td>
	<td class="line x" title="196:210	On the other hand, the parsing strategy is very simple, as we just substitute words by their semantic class and then train statistical parsers on the transformed input." ></td>
	<td class="line x" title="197:210	The semantic class should be an informationsourcethattheparserstakeintoaccountin addition to analysing the actual words used." ></td>
	<td class="line pc" title="198:210	Tighter integration of semantics into the parsing models, possibly in the form of discriminative reranking models (Collins and Koo, 2005; Charniak and Johnson, 2005; McClosky et al., 2006), is a promising way forward in this regard." ></td>
	<td class="line x" title="199:210	7 Conclusions In this work we have trained two state-of-the-art statistical parsers on semantically-enriched input, where content words have been substituted with their semantic classes." ></td>
	<td class="line x" title="200:210	This simple method allows us to incorporate lexical semantic information into the parser, without having to reimplement a full statistical parser." ></td>
	<td class="line x" title="201:210	We tested the two parsers in both a full parsing and a PP attachment context." ></td>
	<td class="line x" title="202:210	This paper shows that semantic classes achieve signicant improvement both on full parsing and PP attachment tasks relative to the baseline parsers." ></td>
	<td class="line x" title="203:210	PP attachment achieves a 20.5% ERR, and parsing 6.9% without requiring hand-tagged data." ></td>
	<td class="line x" title="204:210	Theresultsarehighlysignicantindemonstrating that a simplistic approach to incorporating lexical semantics into a parser signicantly improves parser performance." ></td>
	<td class="line x" title="205:210	As far as we know, these are the rst results over both WordNet and the Penn Treebank to show that semantic processing helps parsing." ></td>
	<td class="line x" title="206:210	Acknowledgements We wish to thank Diana McCarthy for providing us with the sense rank for the target words." ></td>
	<td class="line x" title="207:210	This work was partially funded by the Education Ministry (project KNOW TIN2006-15049), the Basque Government (IT397-07), and the Australian Research Council (grant no." ></td>
	<td class="line x" title="208:210	DP0663879)." ></td>
	<td class="line x" title="209:210	Eneko Agirre participated in this research whilevisitingtheUniversityofMelbourne,basedonjoint funding from the Basque Government and HCSNet." ></td>
	<td class="line x" title="210:210	324" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-1067
Forest Reranking: Discriminative Parsing with Non-Local Features
Huang, Liang;"></td>
	<td class="line x" title="1:181	Proceedings of ACL-08: HLT, pages 586594, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:181	c2008 Association for Computational Linguistics Forest Reranking: Discriminative Parsing with Non-Local Features Liang Huang University of Pennsylvania Philadelphia, PA 19104 lhuang3@cis.upenn.edu Abstract Conventional n-best reranking techniques often suffer from the limited scope of the nbest list, which rules out many potentially good alternatives." ></td>
	<td class="line x" title="3:181	We instead propose forest reranking, a method that reranks a packed forest of exponentially many parses." ></td>
	<td class="line x" title="4:181	Since exact inference is intractable with non-local features, we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole Treebank." ></td>
	<td class="line x" title="5:181	Our final result, an F-score of 91.7, outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank." ></td>
	<td class="line x" title="6:181	1 Introduction Discriminative reranking has become a popular technique for many NLP problems, in particular, parsing (Collins, 2000) and machine translation (Shen et al., 2005)." ></td>
	<td class="line x" title="7:181	Typically, this method first generates a list of top-n candidates from a baseline system, and then reranks this n-best list with arbitrary features that are not computable or intractable to compute within the baseline system." ></td>
	<td class="line x" title="8:181	But despite its apparent success, there remains a major drawback: this method suffers from the limited scope of the nbest list, which rules out many potentially good alternatives." ></td>
	<td class="line x" title="9:181	For example 41% of the correct parses were not in the candidates of 30-best parses in (Collins, 2000)." ></td>
	<td class="line x" title="10:181	This situation becomes worse with longer sentences because the number of possible interpretations usually grows exponentially with the  Part of this work was done while I was visiting Institute of Computing Technology, Beijing, and I thank Prof. Qun Liu and his lab for hosting me. I am also grateful to Dan Gildea and Mark Johnson for inspirations, Eugene Charniak for help with his parser, and Wenbin Jiang for guidance on perceptron averaging." ></td>
	<td class="line x" title="11:181	This project was supported by NSF ITR EIA-0205456." ></td>
	<td class="line x" title="12:181	local non-local conventional reranking only at the root DP-based discrim." ></td>
	<td class="line x" title="13:181	parsing exact N/A this work: forest-reranking exact on-the-fly Table 1: Comparison of various approaches for incorporating local and non-local features." ></td>
	<td class="line x" title="14:181	sentence length." ></td>
	<td class="line x" title="15:181	As a result, we often see very few variations among the n-best trees, for example, 50best trees typically just represent a combination of 5 to 6 binary ambiguities (since 25 < 50 < 26)." ></td>
	<td class="line x" title="16:181	Alternatively, discriminative parsing is tractable with exact and efficient search based on dynamic programming (DP) if all features are restricted to be local, that is, only looking at a local window within the factored search space (Taskar et al., 2004; McDonald et al., 2005)." ></td>
	<td class="line x" title="17:181	However, we miss the benefits of non-local features that are not representable here." ></td>
	<td class="line x" title="18:181	Ideally, we would wish to combine the merits of both approaches, where an efficient inference algorithm could integrate both local and non-local features." ></td>
	<td class="line x" title="19:181	Unfortunately, exact search is intractable (at least in theory) for features with unbounded scope." ></td>
	<td class="line x" title="20:181	So we propose forest reranking, a technique inspired by forest rescoring (Huang and Chiang, 2007) that approximately reranks the packed forest of exponentially many parses." ></td>
	<td class="line x" title="21:181	The key idea is to compute non-local features incrementally from bottom up, so that we can rerank the n-best subtrees at all internal nodes, instead of only at the root node as in conventional reranking (see Table 1)." ></td>
	<td class="line x" title="22:181	This method can thus be viewed as a step towards the integration of discriminative reranking with traditional chart parsing." ></td>
	<td class="line x" title="23:181	Although previous work on discriminative parsing has mainly focused on short sentences ( 15 words) (Taskar et al., 2004; Turian and Melamed, 2007), our work scales to the whole Treebank, where 586 VP1,6 VBD1,2 blah NP2,6 NP2,3 blah PP3,6 be2 e1 Figure 1: A partial forest of the example sentence." ></td>
	<td class="line x" title="24:181	we achieved an F-score of 91.7, which is a 19% error reduction from the 1-best baseline, and outperforms both 50-best and 100-best reranking." ></td>
	<td class="line x" title="25:181	This result is also better than any previously reported systems trained on the Treebank." ></td>
	<td class="line x" title="26:181	2 Packed Forests as Hypergraphs Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar (Billot and Lang, 1989)." ></td>
	<td class="line x" title="27:181	For example, consider the following sentence 0 I 1 saw 2 him 3 with 4 a 5 mirror 6 where the numbers between words denote string positions." ></td>
	<td class="line x" title="28:181	Shown in Figure 1, this sentence has (at least) two derivations depending on the attachment of the prep." ></td>
	<td class="line x" title="29:181	phrase PP3,6 with a mirror: it can either be attached to the verb saw, VBD1,2 NP2,3 PP3,6 VP1,6 , (*) or be attached to him, which will be further combined with the verb to form the same VP as above." ></td>
	<td class="line x" title="30:181	These two derivations can be represented as a single forest by sharing common sub-derivations." ></td>
	<td class="line x" title="31:181	Such a forest has a structure of a hypergraph (Klein and Manning, 2001; Huang and Chiang, 2005), where items like PP3,6 are called nodes, and deductive steps like (*) correspond to hyperedges." ></td>
	<td class="line x" title="32:181	More formally, a forest is a pairV,E, where V is the set of nodes, and E the set of hyperedges." ></td>
	<td class="line x" title="33:181	For a given sentence w1:l = w1 wl, each node vV is in the form of Xi,j, which denotes the recognition of nonterminal X spanning the substring from positions i through j (that is, wi+1 wj)." ></td>
	<td class="line x" title="34:181	Each hyperedge e  E is a pair tails(e),head(e), where head(e)  V is the consequent node in the deductive step, and tails(e)V  is the list of antecedent nodes." ></td>
	<td class="line x" title="35:181	For example, the hyperedge for deduction (*) is notated: e1 =(VBD1,2, NP2,3, PP3,6), VP1,6 We also denote IN(v) to be the set of incoming hyperedges of node v, which represent the different ways of deriving v. For example, in the forest in Figure 1, IN(VP1,6) is {e1,e2}, with e2 = (VBD1,2, NP2,6), VP1,6." ></td>
	<td class="line x" title="36:181	We call|e|the arity of hyperedge e, which counts the number of tail nodes in e. The arity of a hypergraph is the maximum arity over all hyperedges." ></td>
	<td class="line x" title="37:181	A CKY forest has an arity of 2, since the input grammar is required to be binary branching (cf.Chomsky Normal Form) to ensure cubic time parsing complexity." ></td>
	<td class="line x" title="39:181	However, in this work, we use forests from a Treebank parser (Charniak, 2000) whose grammar is often flat in many productions." ></td>
	<td class="line x" title="40:181	For example, the arity of the forest in Figure 1 is 3." ></td>
	<td class="line x" title="41:181	Such a Treebank-style forest is easier to work with for reranking, since many features can be directly expressed in it." ></td>
	<td class="line x" title="42:181	There is also a distinguished root node TOP in each forest, denoting the goal item in parsing, which is simply S0,l where S is the start symbol and l is the sentence length." ></td>
	<td class="line x" title="43:181	3 Forest Reranking 3.1 Generic Reranking with the Perceptron We first establish a unified framework for parse reranking with both n-best lists and packed forests." ></td>
	<td class="line x" title="44:181	For a given sentence s, a generic reranker selects the best parse y among the set of candidates cand(s) according to some scoring function: y = argmax ycand(s) score(y) (1) In n-best reranking, cand(s) is simply a set of n-best parses from the baseline parser, that is, cand(s) = {y1,y2,,yn}." ></td>
	<td class="line x" title="45:181	Whereas in forest reranking, cand(s) is a forest implicitly representing the set of exponentially many parses." ></td>
	<td class="line x" title="46:181	As usual, we define the score of a parse y to be the dot product between a high dimensional feature representation and a weight vector w: score(y) = wf(y) (2) 587 where the feature extractor f is a vector of d functions f = (f1,,fd), and each feature fj maps a parse y to a real number fj(y)." ></td>
	<td class="line x" title="47:181	Following (Charniak and Johnson, 2005), the first feature f1(y) = logPr(y) is the log probability of a parse from the baseline generative parser, while the remaining features are all integer valued, and each of them counts the number of times that a particular configuration occurs in parse y. For example, one such feature f2000 might be a question how many times is a VP of length 5 surrounded by the word has and the period?" ></td>
	<td class="line x" title="48:181	 which is an instance of the WordEdges feature (see Figure 2(c) and Section 3.2 for details)." ></td>
	<td class="line x" title="49:181	Using a machine learning algorithm, the weight vector w can be estimated from the training data where each sentence si is labelled with its correct (gold-standard) parse yi . As for the learner, Collins (2000) uses the boosting algorithm and Charniak and Johnson (2005) use the maximum entropy estimator." ></td>
	<td class="line x" title="50:181	In this work we use the averaged perceptron algorithm (Collins, 2002) since it is an online algorithm much simpler and orders of magnitude faster than Boosting and MaxEnt methods." ></td>
	<td class="line x" title="51:181	Shown in Pseudocode 1, the perceptron algorithm makes several passes over the whole training data, and in each iteration, for each sentence si, it tries to predict a best parse yi among the candidates cand(si) using the current weight setting." ></td>
	<td class="line x" title="52:181	Intuitively, we want the gold parse yi to be picked, but in general it is not guaranteed to be within cand(si), because the grammar may fail to cover the gold parse, and because the gold parse may be pruned away due to the limited scope of cand(si)." ></td>
	<td class="line x" title="53:181	So we define an oracle parse y+i to be the candidate that has the highest Parseval F-score with respect to the gold tree yi :1 y+i defines argmax ycand(si) F(y,yi ) (3) where function F returns the F-score." ></td>
	<td class="line x" title="54:181	Now we train the reranker to pick the oracle parses as often as possible, and in case an error is made (line 6), perform an update on the weight vector (line 7), by adding the difference between two feature representations." ></td>
	<td class="line x" title="55:181	1If one uses the gold y i for oracle y + i , the perceptron willcontinue to make updates towards something unreachable even when the decoder has picked the best possible candidate." ></td>
	<td class="line x" title="56:181	Pseudocode 1 Perceptron for Generic Reranking 1: Input: Training examples{cand(si),y+i }Ni=1  y+i is the oracle tree for si among cand(si) 2: w0  initial weights 3: for t1T do  T iterations 4: for i1N do 5: y = argmaxycand(si) wf(y) 6: if ynegationslash= y+i then 7: ww +f(y+i )f(y) 8: return w In n-best reranking, since all parses are explicitly enumerated, it is trivial to compute the oracle tree.2 However, it remains widely open how to identify the forest oracle." ></td>
	<td class="line x" title="57:181	We will present a dynamic programming algorithm for this problem in Sec." ></td>
	<td class="line x" title="58:181	4.1." ></td>
	<td class="line x" title="59:181	We also use a refinement called averaged parameters where the final weight vector is the average of weight vectors after each sentence in each iteration over the training data." ></td>
	<td class="line x" title="60:181	This averaging effect has been shown to reduce overfitting and produce much more stable results (Collins, 2002)." ></td>
	<td class="line x" title="61:181	3.2 Factorizing Local and Non-Local Features A key difference between n-best and forest reranking is the handling of features." ></td>
	<td class="line x" title="62:181	In n-best reranking, all features are treated equivalently by the decoder, which simply computes the value of each one on each candidate parse." ></td>
	<td class="line x" title="63:181	However, for forest reranking, since the trees are not explicitly enumerated, many features can not be directly computed." ></td>
	<td class="line x" title="64:181	So we first classify features into local and non-local, which the decoder will process in very different fashions." ></td>
	<td class="line x" title="65:181	We define a feature f to be local if and only if it can be factored among the local productions in a tree, and non-local if otherwise." ></td>
	<td class="line x" title="66:181	For example, the Rule feature in Fig." ></td>
	<td class="line x" title="67:181	2(a) is local, while the ParentRule feature in Fig." ></td>
	<td class="line x" title="68:181	2(b) is non-local." ></td>
	<td class="line x" title="69:181	It is worth noting that some features which seem complicated at the first sight are indeed local." ></td>
	<td class="line x" title="70:181	For example, the WordEdges feature in Fig." ></td>
	<td class="line x" title="71:181	2(c), which classifies a node by its label, span length, and surrounding words, is still local since all these information are encoded either in the node itself or in the input sentence." ></td>
	<td class="line x" title="72:181	In contrast, it would become non-local if we replace the surrounding words by surrounding POS 2In case multiple candidates get the same highest F-score, we choose the parse with the highest log probability from the baseline parser to be the oracle parse (Collins, 2000)." ></td>
	<td class="line x" title="73:181	588 VP VBD NP PP S VP VBD NP PP VP VBZ has NP | 5 words| . . VP VBD saw NP DT the  (a) Rule (local) (b) ParentRule (non-local) (c) WordEdges (local) (d) NGramTree (non-local) VPVBD NP PP VPVBD NP PP|S NP 5 has . VP (VBD saw) (NP (DT the)) Figure 2: Illustration of some example features." ></td>
	<td class="line x" title="74:181	Shaded nodes denote information included in the feature." ></td>
	<td class="line x" title="75:181	tags, which are generated dynamically." ></td>
	<td class="line x" title="76:181	More formally, we split the feature extractor f = (f1,,fd) into f = (fL;fN) where fL and fN are the local and non-local features, respectively." ></td>
	<td class="line x" title="77:181	For the former, we extend their domains from parses to hyperedges, where f(e) returns the value of a local feature f fL on hyperedge e, and its value on a parsey factors across the hyperedges (local productions), fL(y) = summationdisplay ey fL(e) (4) and we can pre-compute fL(e) for each e in a forest." ></td>
	<td class="line x" title="78:181	Non-local features, however, can not be precomputed, but we still prefer to compute them as early as possible, which we call on-the-fly computation, so that our decoder can be sensitive to them at internal nodes." ></td>
	<td class="line x" title="79:181	For instance, the NGramTree feature in Fig." ></td>
	<td class="line x" title="80:181	2 (d) returns the minimum tree fragement spanning a bigram, in this case saw and the, and should thus be computed at the smallest common ancestor of the two, which is the VP node in this example." ></td>
	<td class="line x" title="81:181	Similarly, the ParentRule feature in Fig." ></td>
	<td class="line x" title="82:181	2 (b) can be computed when the S subtree is formed." ></td>
	<td class="line x" title="83:181	In doing so, we essentially factor non-local features across subtrees, where for each subtree y in a parse y, we define a unit feature f(y) to be the part of f(y) that are computable within y, but not computable in any (proper) subtree of y." ></td>
	<td class="line x" title="84:181	Then we have: fN(y) = summationdisplay yy fN(y) (5) Intuitively, we compute the unit non-local features at each subtree from bottom-up." ></td>
	<td class="line x" title="85:181	For example, for the binary-branching node Ai,k in Fig." ></td>
	<td class="line x" title="86:181	3, the Ai,k Bi,j wi  wj1 Cj,k wj wk1 Figure 3: Example of the unit NGramTree feature at node Ai,k:A (B wj1) (C wj)." ></td>
	<td class="line x" title="87:181	unit NGramTree instance is for the pairwj1,wj on the boundary between the two subtrees, whose smallest common ancestor is the current node." ></td>
	<td class="line x" title="88:181	Other unit NGramTree instances within this span have already been computed in the subtrees, except those for the boundary words of the whole node, wi and wk1, which will be computed when this node is further combined with other nodes in the future." ></td>
	<td class="line x" title="89:181	3.3 Approximate Decoding via Cube Pruning Before moving on to approximate decoding with non-local features, we first describe the algorithm for exact decoding when only local features are present, where many concepts and notations will be re-used later." ></td>
	<td class="line x" title="90:181	We will use D(v) to denote the top derivations of node v, where D1(v) is its 1-best derivation." ></td>
	<td class="line x" title="91:181	We also use the notatione,jto denote the derivation along hyperedge e, using the jith subderivation for tail ui, so e,1 is the best derivation along e. The exact decoding algorithm, shown in Pseudocode 2, is an instance of the bottom-up Viterbi algorithm, which traverses the hypergraph in a topological order, and at each node v, calculates its 1-best derivation using each incoming hyperedge e  IN(v)." ></td>
	<td class="line x" title="92:181	The cost of e, c(e), is the score of its 589 Pseudocode 2 Exact Decoding with Local Features 1: function VITERBI(V,E) 2: for vV in topological order do 3: for eIN(v) do 4: c(e)wfL(e) +Puitails(e) c(D1(ui)) 5: if c(e) > c(D1(v)) then  better derivation?" ></td>
	<td class="line x" title="93:181	6: D1(v)e,1 7: c(D1(v))c(e) 8: return D1(TOP) Pseudocode 3 Cube Pruning for Non-local Features 1: function CUBE(V,E) 2: for vV in topological order do 3: KBEST(v) 4: return D1(TOP) 5: procedure KBEST(v) 6: heap; buf  7: for eIN(v) do 8: c(e,1)EVAL(e,1)  extract unit features 9: appende,1to heap 10: HEAPIFY(heap)  prioritized frontier 11: while|heap|> 0 and|buf|< k do 12: itemPOP-MAX(heap)  extract next-best 13: append item to buf 14: PUSHSUCC(item,heap) 15: sort buf to D(v) 16: procedure PUSHSUCC(e,j,heap) 17: e is vu1 u|e| 18: for i in 1|e|do 19: j j+bi  bi is 1 only on the ith dim." ></td>
	<td class="line x" title="94:181	20: if|D(ui)|ji then  enough sub-derivations?" ></td>
	<td class="line x" title="95:181	21: c(e,j)EVAL(e,j)  unit features 22: PUSH(e,j,heap) 23: function EVAL(e,j) 24: e is vu1 u|e| 25: return wfL(e) +wfN(e,j) +Pi c(Dji(ui)) (pre-computed) local features wfL(e)." ></td>
	<td class="line x" title="96:181	This algorithm has a time complexity of O(E), and is almost identical to traditional chart parsing, except that the forest might be more than binary-branching." ></td>
	<td class="line x" title="97:181	For non-local features, we adapt cube pruning from forest rescoring (Chiang, 2007; Huang and Chiang, 2007), since the situation here is analogous to machine translation decoding with integrated language models: we can view the scores of unit nonlocal features as the language model cost, computed on-the-fly when combining sub-constituents." ></td>
	<td class="line x" title="98:181	Shown in Pseudocode 3, cube pruning works bottom-up on the forest, keeping a beam of at most k derivations at each node, and uses the k-best parsing Algorithm 2 of Huang and Chiang (2005) to speed up the computation." ></td>
	<td class="line x" title="99:181	When combining the subderivations along a hyperedge e to form a new subtree y = e,j, we also compute its unit non-local feature valuesfN(e,j) (line 25)." ></td>
	<td class="line x" title="100:181	A priority queue (heap in Pseudocode 3) is used to hold the candidates for the next-best derivation, which is initialized to the set of best derivations along each hyperedge (lines 7 to 9)." ></td>
	<td class="line x" title="101:181	Then at each iteration, we pop the best derivation (lines 12), and push its successors back into the priority queue (line 14)." ></td>
	<td class="line x" title="102:181	Analogous to the language model cost in forest rescoring, the unit feature cost here is a non-monotonic score in the dynamic programming backbone, and the derivations may thus be extracted out-of-order." ></td>
	<td class="line x" title="103:181	So a buffer buf is used to hold extracted derivations, which is sorted at the end (line 15) to form the list of top-k derivations D(v) of node v. The complexity of this algorithm is O(E + V klogkN) (Huang and Chiang, 2005), where O(N) is the time for on-the-fly feature extraction for each subtree, which becomes the bottleneck in practice." ></td>
	<td class="line x" title="104:181	4 Supporting Forest Algorithms 4.1 Forest Oracle Recall that the Parseval F-score is the harmonic mean of labelled precision P and labelled recall R: F(y,y) defines 2PRP + R = 2|yy | |y|+|y| (6) where|y|and|y|are the numbers of brackets in the test parse and gold parse, respectively, and|yy| is the number of matched brackets." ></td>
	<td class="line x" title="105:181	Since the harmonic mean is a non-linear combination, we can not optimize the F-scores on sub-forests independently with a greedy algorithm." ></td>
	<td class="line x" title="106:181	In other words, the optimal F-score tree in a forest is not guaranteed to be composed of two optimal F-score subtrees." ></td>
	<td class="line x" title="107:181	We instead propose a dynamic programming algorithm which optimizes the number of matched brackets for a given number of test brackets." ></td>
	<td class="line x" title="108:181	For example, our algorithm will ask questions like, when a test parse has 5 brackets, what is the maximum number of matched brackets? More formally, at each node v, we compute an oracle function ora[v] : NmapstoN, which maps an integer t to ora[v](t), the max." ></td>
	<td class="line x" title="109:181	number of matched brackets 590 Pseudocode 4 Forest Oracle Algorithm 1: function ORACLE(V,E, y) 2: for vV in topological order do 3: for eBS(v) do 4: e is vu1u2 u|e| 5: ora[v]ora[v](iora[ui]) 6: ora[v]ora[v](1,1vy) 7: return F(y+,y) = maxt 2ora[TOP](t)t+|y|  oracle F1 for all parses yv of node v with exactly t brackets: ora[v](t) defines max yv:|yv|=t |yvy| (7) When node v is combined with another node u along a hyperedge e =(v,u),w, we need to combine the two oracle functions ora[v] and ora[u] by distributing the test brackets of w between v and u, and optimize the number of matched bracktes." ></td>
	<td class="line x" title="110:181	To do this we define a convolution operatorbetween two functions f and g: (fg)(t) defines maxt 1+t2=t f(t1) + g(t2) (8) For instance: t f(t) 2 1 3 2  t g(t) 4 4 5 4 = t (fg)(t) 6 5 7 6 8 6 The oracle function for the head node w is then ora[w](t) = (ora[v]ora[u])(t1)+1wy (9) where 1 is the indicator function, returning 1 if node w is found in the gold tree y, in which case we increment the number of matched brackets." ></td>
	<td class="line x" title="111:181	We can also express Eq." ></td>
	<td class="line x" title="112:181	9 in a purely functional form ora[w] = (ora[v]ora[u])(1,1wy) (10) where  is a translation operator which shifts a function along the axes: (f (a,b))(t) defines f(ta) + b (11) Above we discussed the case of one hyperedge." ></td>
	<td class="line x" title="113:181	If there is another hyperedge e deriving node w, we also need to combine the resulting oracle functions from both hyperedges, for which we define a pointwise addition operator: (fg)(t) defines max{f(t),g(t)} (12) Shown in Pseudocode 4, we perform these computations in a bottom-up topological order, and finally at the root node TOP, we can compute the best global F-score by maximizing over different numbers of test brackets (line 7)." ></td>
	<td class="line x" title="114:181	The oracle tree y+ can be recursively restored by keeping backpointers for each ora[v](t), which we omit in the pseudocode." ></td>
	<td class="line x" title="115:181	The time complexity of this algorithm for a sentence of l words is O(|E|l2(a1)) where a is the arity of the forest." ></td>
	<td class="line x" title="116:181	For a CKY forest, this amounts to O(l3 l2) = O(l5), but for general forests like those in our experiments the complexities are much higher." ></td>
	<td class="line x" title="117:181	In practice it takes on average 0.05 seconds for forests pruned by p = 10 (see Section 4.2), but we can pre-compute and store the oracle for each forest before training starts." ></td>
	<td class="line x" title="118:181	4.2 Forest Pruning Our forest pruning algorithm (Jonathan Graehl, p.c.) is very similar to the method based on marginal probability (Charniak and Johnson, 2005), except that ours prunes hyperedges as well as nodes." ></td>
	<td class="line x" title="119:181	Basically, we use an Inside-Outside algorithm to compute the Viterbi inside cost (v) and the Viterbi outside cost (v) for each node v, and then compute the merit (e) for each hyperedge: (e) = (head(e)) + summationdisplay uitails(e) (ui) (13) Intuitively, this merit is the cost of the best derivation that traverses e, and the difference (e) = (e)(TOP) can be seen as the distance away from the globally best derivation." ></td>
	<td class="line x" title="120:181	We prune away all hyperedges that have (e) > p for a threshold p. Nodes with all incoming hyperedges pruned are also pruned." ></td>
	<td class="line x" title="121:181	The key difference from (Charniak and Johnson, 2005) is that in this algorithm, a node can partially survive the beam, with a subset of its hyperedges pruned." ></td>
	<td class="line x" title="122:181	In practice, this method prunes on average 15% more hyperedges than their method." ></td>
	<td class="line x" title="123:181	5 Experiments We compare the performance of our forest reranker against n-best reranking on the Penn English Treebank (Marcus et al., 1993)." ></td>
	<td class="line x" title="124:181	The baseline parser is the Charniak parser, which we modified to output a 591 Local instances Non-Local instances Rule 10, 851 ParentRule 18, 019 Word 20, 328 WProj 27, 417 WordEdges 454, 101 Heads 70, 013 CoLenPar 22 HeadTree 67, 836 Bigram 10, 292 Heavy 1, 401 Trigram 24, 677 NGramTree 67, 559 HeadMod 12, 047 RightBranch 2 DistMod 16, 017 Total Feature Instances: 800, 582 Table 2: Features used in this work." ></td>
	<td class="line x" title="125:181	Those with a  are from (Collins, 2000), and others are from (Charniak and Johnson, 2005), with simplifications." ></td>
	<td class="line x" title="126:181	packed forest for each sentence.3 5.1 Data Preparation We use the standard split of the Treebank: sections 02-21 as the training data (39832 sentences), section 22 as the development set (1700 sentences), and section 23 as the test set (2416 sentences)." ></td>
	<td class="line x" title="127:181	Following (Charniak and Johnson, 2005), the training set is split into 20 folds, each containing about 1992 sentences, and is parsed by the Charniak parser with a model trained on sentences from the remaining 19 folds." ></td>
	<td class="line x" title="128:181	The development set and the test set are parsed with a model trained on all 39832 training sentences." ></td>
	<td class="line x" title="129:181	We implemented both n-best and forest reranking systems in Python and ran our experiments on a 64bit Dual-Core Intel Xeon with 3.0GHz CPUs." ></td>
	<td class="line x" title="130:181	Our feature set is summarized in Table 2, which closely follows Charniak and Johnson (2005), except that we excluded the non-local features Edges, NGram, and CoPar, and simplified Rule and NGramTree features, since they were too complicated to compute.4 We also added four unlexicalized local features from Collins (2000) to cope with data-sparsity." ></td>
	<td class="line x" title="131:181	Following Charniak and Johnson (2005), we extracted the features from the 50-best parses on the training set (sec." ></td>
	<td class="line x" title="132:181	02-21), and used a cut-off of 5 to prune away low-count features." ></td>
	<td class="line x" title="133:181	There are 0.8M features in our final set, considerably fewer than that of Charniak and Johnson which has about 1.3M fea3This is a relatively minor change to the Charniak parser, since it implements Algorithm 3 of Huang and Chiang (2005) for efficient enumeration of n-best parses, which requires storing the forest." ></td>
	<td class="line x" title="134:181	The modified parser and related scripts for handling forests (e.g. oracles) will be available on my homepage." ></td>
	<td class="line x" title="135:181	4In fact, our Rule and ParentRule features are two special cases of the original Rule feature in (Charniak and Johnson, 2005)." ></td>
	<td class="line x" title="136:181	We also restricted NGramTree to be on bigrams only." ></td>
	<td class="line x" title="137:181	89.0 91.0 93.0 95.0 97.0 99.0  0  500  1000  1500  2000 Parseval F-score (%) average # of hyperedges or brackets per sentence p=10 p=20 n=10 n=50 n=100 1-best forest oracle n-best oracle Figure 4: Forests (shown with various pruning thresholds) enjoy higher oracle scores and more compact sizes than n-best lists (on sec 23)." ></td>
	<td class="line x" title="138:181	tures in the updated version.5 However, our initial experiments show that, even with this much simpler feature set, our 50-best reranker performed equally well as theirs (both with an F-score of 91.4, see Tables 3 and 4)." ></td>
	<td class="line x" title="139:181	This result confirms that our feature set design is appropriate, and the averaged perceptron learner is a reasonable candidate for reranking." ></td>
	<td class="line x" title="140:181	The forests dumped from the Charniak parser are huge in size, so we use the forest pruning algorithm in Section 4.2 to prune them down to a reasonable size." ></td>
	<td class="line x" title="141:181	In the following experiments we use a threshold of p = 10, which results in forests with an average number of 123.1 hyperedges per forest." ></td>
	<td class="line x" title="142:181	Then for each forest, we annotate its forest oracle, and on each hyperedge, pre-compute its local features.6 Shown in Figure 4, these forests have an forest oracle of 97.8, which is 1.1% higher than the 50-best oracle (96.7), and are 8 times smaller in size." ></td>
	<td class="line x" title="143:181	5.2 Results and Analysis Table 3 compares the performance of forest reranking against standard n-best reranking." ></td>
	<td class="line x" title="144:181	For both systems, we first use only the local features, and then all the features." ></td>
	<td class="line x" title="145:181	We use the development set to determine the optimal number of iterations for averaged perceptron, and report the F1 score on the test set." ></td>
	<td class="line x" title="146:181	With only local features, our forest reranker achieves an F-score of 91.25, and with the addition of non5http://www.cog.brown.edu/mj/software.htm." ></td>
	<td class="line x" title="147:181	We follow this version as it corrects some bugs from their 2005 paper which leads to a 0.4% increase in performance (see Table 4)." ></td>
	<td class="line x" title="148:181	6A subset of local features, e.g. WordEdges, is independent of which hyperedge the node takes in a derivation, and can thus be annotated on nodes rather than hyperedges." ></td>
	<td class="line x" title="149:181	We call these features node-local, which also include part of Word features." ></td>
	<td class="line x" title="150:181	592 baseline: 1-best Charniak parser 89.72 n-best reranking features n pre-comp." ></td>
	<td class="line x" title="151:181	training F1% local 50 1.7G / 16h 30.1h 91.28 all 50 2.4G / 19h 40.3h 91.43 all 100 5.3G / 44h 40.7h 91.49 forest reranking (p = 10) features k pre-comp." ></td>
	<td class="line x" title="152:181	training F1% local 1.2G / 2.9h 30.8h 91.25 all 15 46.1h 91.69 Table 3: Forest reranking compared to n-best reranking on sec." ></td>
	<td class="line x" title="153:181	23." ></td>
	<td class="line x" title="154:181	The pre-comp." ></td>
	<td class="line x" title="155:181	column is for feature extraction, and training column shows the number of perceptron iterations that achieved best results on the dev set, and average time per iteration." ></td>
	<td class="line x" title="156:181	local features, the accuracy rises to 91.69 (with beam size k = 15), which is a 0.26% absolute improvement over 50-best reranking.7 This improvement might look relatively small, but it is much harder to make a similar progress with n-best reranking." ></td>
	<td class="line x" title="157:181	For example, even if we double the size of the n-best list to 100, the performance only goes up by 0.06% (Table 3)." ></td>
	<td class="line x" title="158:181	In fact, the 100best oracle is only 0.5% higher than the 50-best one (see Fig." ></td>
	<td class="line x" title="159:181	4)." ></td>
	<td class="line x" title="160:181	In addition, the feature extraction step in 100-best reranking produces huge data files and takes 44 hours in total, though this part can be parallelized.8 On two CPUs, 100-best reranking takes 25 hours, while our forest-reranker can also finish in 26 hours, with a much smaller disk space." ></td>
	<td class="line x" title="161:181	Indeed, this demonstrates the severe redundancies as another disadvantage of n-best lists, where many subtrees are repeated across different parses, while the packed forest reduces space dramatically by sharing common sub-derivations (see Fig." ></td>
	<td class="line x" title="162:181	4)." ></td>
	<td class="line x" title="163:181	To put our results in perspective, we also compare them with other best-performing systems in Table 4." ></td>
	<td class="line x" title="164:181	Our final result (91.7) is better than any previously reported system trained on the Treebank, although 7It is surprising that 50-best reranking with local features achieves an even higher F-score of 91.28, and we suspect this is due to the aggressive updates and instability of the perceptron, as we do observe the learning curves to be non-monotonic." ></td>
	<td class="line x" title="165:181	We leave the use of more stable learning algorithms to future work." ></td>
	<td class="line x" title="166:181	8The n-best feature extraction already uses relative counts (Johnson, 2006), which reduced file sizes by at least a factor 4." ></td>
	<td class="line oc" title="167:181	type system F1% D Collins (2000) 89.7 Henderson (2004) 90.1 Charniak and Johnson (2005) 91.0 updated (Johnson, 2006) 91.4 this work 91.7 G Bod (2003) 90.7Petrov and Klein (2007) 90.1 S McClosky et al.(2006) 92.1 Table 4: Comparison of our final results with other best-performing systems on the whole Section 23." ></td>
	<td class="line x" title="169:181	Types D, G, and S denote discriminative, generative, and semi-supervised approaches, respectively." ></td>
	<td class="line pc" title="170:181	McClosky et al.(2006) achieved an even higher accuarcy (92.1) by leveraging on much larger unlabelled data." ></td>
	<td class="line x" title="172:181	Moreover, their technique is orthogonal to ours, and we suspect that replacing their n-best reranker by our forest reranker might get an even better performance." ></td>
	<td class="line x" title="173:181	Plus, except for n-best reranking, most discriminative methods require repeated parsing of the training set, which is generally impratical (Petrov and Klein, 2008)." ></td>
	<td class="line x" title="174:181	Therefore, previous work often resorts to extremely short sentences ( 15 words) or only looked at local features (Taskar et al., 2004; Henderson, 2004; Turian and Melamed, 2007)." ></td>
	<td class="line x" title="175:181	In comparison, thanks to the efficient decoding, our work not only scaled to the whole Treebank, but also successfully incorporated non-local features, which showed an absolute improvement of 0.44% over that of local features alone." ></td>
	<td class="line x" title="176:181	6 Conclusion We have presented a framework for reranking on packed forests which compactly encodes many more candidates than n-best lists." ></td>
	<td class="line x" title="177:181	With efficient approximate decoding, perceptron training on the whole Treebank becomes practical, which can be done in about a day even with a Python implementation." ></td>
	<td class="line x" title="178:181	Our final result outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank." ></td>
	<td class="line x" title="179:181	We also devised a dynamic programming algorithm for forest oracles, an interesting problem by itself." ></td>
	<td class="line x" title="180:181	We believe this general framework could also be applied to other problems involving forests or lattices, such as sequence labeling and machine translation." ></td>
	<td class="line x" title="181:181	593" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-2026
Self-Training for Biomedical Parsing
McClosky, David;Charniak, Eugene;"></td>
	<td class="line x" title="1:102	Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 101104, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:102	c2008 Association for Computational Linguistics Self-Training for Biomedical Parsing David McClosky and Eugene Charniak Brown Laboratory for Linguistic Information Processing (BLLIP) Brown University Providence, RI 02912 {dmcc|ec}@cs.brown.edu Abstract Parser self-training is the technique of taking an existing parser, parsing extra data and then creating a second parser by treating the extra data as further training data." ></td>
	<td class="line x" title="3:102	Here we apply this technique to parser adaptation." ></td>
	<td class="line x" title="4:102	In particular, we self-train the standard Charniak/Johnson Penn-Treebank parser using unlabeled biomedical abstracts." ></td>
	<td class="line x" title="5:102	This achieves an f-score of 84.3% on a standard test set of biomedical abstracts from the Genia corpus." ></td>
	<td class="line x" title="6:102	This is a 20% error reduction over the best previous result on biomedical data (80.2% on the same test set)." ></td>
	<td class="line x" title="7:102	1 Introduction Parser self-training is the technique of taking an existing parser, parsing extra data and then creating a second parser by treating the extra data as further training data." ></td>
	<td class="line x" title="8:102	While for many years it was thought not to help state-of-the art parsers, more recent work has shown otherwise." ></td>
	<td class="line x" title="9:102	In this paper we apply this technique to parser adaptation." ></td>
	<td class="line x" title="10:102	In particular we self-train the standard Charniak/Johnson Penn-Treebank (C/J) parser using unannotated biomedical data." ></td>
	<td class="line x" title="11:102	As is well known, biomedical data is hard on parsers because it is so far from more standard English." ></td>
	<td class="line x" title="12:102	To our knowledge this is the first application of self-training where the gap between the training and self-training data is so large." ></td>
	<td class="line x" title="13:102	In section two, we look at previous work." ></td>
	<td class="line x" title="14:102	In particular we note that there is, in fact, very little data on self-training when the corpora for self-training is so different from the original labeled data." ></td>
	<td class="line x" title="15:102	Section three describes our main experiment on standard test data (Clegg and Shepherd, 2005)." ></td>
	<td class="line x" title="16:102	Section four looks at some preliminary results we obtained on development data that show in slightly more detail how selftraining improved the parser." ></td>
	<td class="line x" title="17:102	We conclude in section five." ></td>
	<td class="line x" title="18:102	2 Previous Work While self-training has worked in several domains, the early results on self-training for parsing were negative (Steedman et al., 2003; Charniak, 1997)." ></td>
	<td class="line pc" title="19:102	However more recent results have shown that it can indeed improve parser performance (Bacchiani et al., 2006; McClosky et al., 2006a; McClosky et al., 2006b)." ></td>
	<td class="line x" title="20:102	One possible use for this technique is for parser adaptation  initially training the parser on one type of data for which hand-labeled trees are available (e.g., Wall Street Journal (M. Marcus et al., 1993)) and then self-training on a second type of data in order to adapt the parser to the second domain." ></td>
	<td class="line x" title="21:102	Interestingly, there is little to no data showing that this actually works." ></td>
	<td class="line oc" title="22:102	Two previous papers would seem to address this issue: the work by Bacchiani et al.(2006) and McClosky et al.(2006b)." ></td>
	<td class="line n" title="25:102	However, in both cases the evidence is equivocal." ></td>
	<td class="line x" title="26:102	Bacchiani and Roark train the Roark parser (Roark, 2001) on trees from the Brown treebank and then self-train and test on data from Wall Street Journal." ></td>
	<td class="line x" title="27:102	While they show some improvement (from 75.7% to 80.5% f-score) there are several aspects of this work which leave its re101 sults less than convincing as to the utility of selftraining for adaptation." ></td>
	<td class="line x" title="28:102	The first is the parsing results are quite poor by modern standards.1 Steedman et al.(2003) generally found that selftraining does not work, but found that it does help if the baseline results were sufficiently bad." ></td>
	<td class="line x" title="30:102	Secondly, the difference between the Brown corpus treebank and the Wall Street Journal corpus is not that great." ></td>
	<td class="line x" title="31:102	One way to see this is to look at out-of-vocabulary statistics." ></td>
	<td class="line x" title="32:102	The Brown corpus has an out-of-vocabulary rate of approximately 6% when given WSJ training as the lexicon." ></td>
	<td class="line x" title="33:102	In contrast, the out-of-vocabulary rate of biomedical abstracts given the same lexicon is significantly higher at about 25% (Lease and Charniak, 2005)." ></td>
	<td class="line x" title="34:102	Thus the bridge the selftrained parser is asked to build is quite short." ></td>
	<td class="line oc" title="35:102	This second point is emphasized by the second paper on self-training for adaptation (McClosky et al., 2006b)." ></td>
	<td class="line p" title="36:102	This paper is based on the C/J parser and thus its results are much more in line with modern expectations." ></td>
	<td class="line x" title="37:102	In particular, it was able to achieve an f-score of 87% on Brown treebank test data when trained and selftrained on WSJ-like data." ></td>
	<td class="line x" title="38:102	Note this last point." ></td>
	<td class="line x" title="39:102	It was not the case that it used the self-training to bridge the corpora difference." ></td>
	<td class="line x" title="40:102	It self-trained on NANC, not Brown." ></td>
	<td class="line x" title="41:102	NANC is a news corpus, quite like WSJ data." ></td>
	<td class="line x" title="42:102	Thus the point of that paper was that self-training a WSJ parser on similar data makes the parser more flexible, not better adapted to the target domain in particular." ></td>
	<td class="line x" title="43:102	It said nothing about the task we address here." ></td>
	<td class="line x" title="44:102	Thus our claim is that previous results are quite ambiguous on the issue of bridging corpora for parser adaptation." ></td>
	<td class="line x" title="45:102	Turning briefly to previous results on Medline data, the best comparative study of parsers is that of Clegg and Shepherd (2005), which evaluates several statistical parsers." ></td>
	<td class="line x" title="46:102	Their best result was an f-score of 80.2%." ></td>
	<td class="line x" title="47:102	This was on the Lease/Charniak (L/C) parser (Lease and Charniak, 2005).2 A close second (1% behind) was 1This is not a criticism of the work." ></td>
	<td class="line x" title="48:102	The results are completely in line with what one would expect given the base parser and the relatively small size of the Brown treebank." ></td>
	<td class="line x" title="49:102	2This is the standard Charniak parser (without the parser of Bikel (2004)." ></td>
	<td class="line x" title="50:102	The other parsers were not close." ></td>
	<td class="line x" title="51:102	However, several very good current parsers were not available when this paper was written (e.g., the Berkeley Parser (Petrov et al., 2006))." ></td>
	<td class="line x" title="52:102	However, since the newer parsers do not perform quite as well as the C/J parser on WSJ data, it is probably the case that they would not significantly alter the landscape." ></td>
	<td class="line x" title="53:102	3 Central Experimental Result We used as the base parser the standardly available C/J parser." ></td>
	<td class="line x" title="54:102	We then self-trained the parser on approximately 270,000 sentences  a random selection of abstracts from Medline.3 Medline is a large database of abstracts and citations from a wide variety of biomedical literature." ></td>
	<td class="line x" title="55:102	As we note in the next section, the number 270,000 was selected by observing performance on a development set." ></td>
	<td class="line x" title="56:102	We weighted the original WSJ hand annotated sentences equally with self-trained Medline data." ></td>
	<td class="line oc" title="57:102	So, for example, McClosky et al.(2006a) found that the data from the handannotated WSJ data should be considered at least five times more important than NANC data on an event by event level." ></td>
	<td class="line x" title="59:102	We did no tuning to find out if there is some better weighting for our domain than one-to-one." ></td>
	<td class="line x" title="60:102	The resulting parser was tested on a test corpus of hand-parsed sentences from the Genia Treebank (Tateisi et al., 2005)." ></td>
	<td class="line x" title="61:102	These are exactly the same sentences as used in the comparisons of the last section." ></td>
	<td class="line x" title="62:102	Genia is a corpus of abstracts from the Medline database selected from a search with the keywords Human, Blood Cells, and Transcription Factors." ></td>
	<td class="line x" title="63:102	Thus the Genia treebank data are all from a small domain within Biology." ></td>
	<td class="line x" title="64:102	As already noted, the Medline abstracts used for self-training were chosen randomly and thus span a large number of biomedical sub-domains." ></td>
	<td class="line x" title="65:102	The results, the central results of this paper, are shown in Figure 1." ></td>
	<td class="line x" title="66:102	Clegg and Shepherd (2005) do not provide separate precision and recall numbers." ></td>
	<td class="line x" title="67:102	However we can see that the reranker) modified to use an in-domain tagger." ></td>
	<td class="line x" title="68:102	3http://www.ncbi.nlm.nih.gov/PubMed/ 102 System Precision Recall f-score L/C   80.2% Self-trained 86.3% 82.4% 84.3% Figure 1: Comparison of the Medline self-trained parser against the previous best Medline self-trained parser achieves an f-score of 84.3%, which is an absolute reduction in error of 4.1%." ></td>
	<td class="line x" title="69:102	This corresponds to an error rate reduction of 20% over the L/C baseline." ></td>
	<td class="line x" title="70:102	4 Discussion Prior to the above experiment on the test data, we did several preliminary experiments on development data from the Genia Treebank." ></td>
	<td class="line x" title="71:102	These results are summarized in Figure 2." ></td>
	<td class="line x" title="72:102	Here we show the f-score for four versions of the parser as a function of number of self-training sentences." ></td>
	<td class="line x" title="73:102	The dashed line on the bottom is the raw C/J parser with no self-training." ></td>
	<td class="line x" title="74:102	At 80.4, it isclearly theworst ofthe lot." ></td>
	<td class="line x" title="75:102	On theother hand, it is already better than the 80.2% best previous result for biomedical data." ></td>
	<td class="line x" title="76:102	This is solely due to the introduction of the 50-best reranker which distinguishes the C/J parser from the preceding Charniak parser." ></td>
	<td class="line x" title="77:102	The almost flat line above it is the C/J parser with NANC self-training data." ></td>
	<td class="line x" title="78:102	As mentioned previously, NANC is a news corpus, quite like the original WSJ data." ></td>
	<td class="line x" title="79:102	At 81.4% it gives us a one percent improvement over the original WSJ parser." ></td>
	<td class="line x" title="80:102	The topmost line, is the C/J parser trained on Medline data." ></td>
	<td class="line x" title="81:102	As can be seen, even just a thousand lines of Medline is already enough to drive our results to a new level and it continues to improve until about 150,000 sentences at which point performance is nearly flat." ></td>
	<td class="line x" title="82:102	However, as 270,000 sentences is fractionally better than 150,000 sentences that is the number of self-training sentences we used for our results on the test set." ></td>
	<td class="line x" title="83:102	Lastly, the middle jagged line is for an interesting idea that failed to work." ></td>
	<td class="line x" title="84:102	We mention it in the hope that others might be able to succeed where we have failed." ></td>
	<td class="line x" title="85:102	We reasoned that textbooks would be a particularly good bridging corpus." ></td>
	<td class="line x" title="86:102	After all, they are written to introduce someone ignorant of a field to the ideas and terminology within it." ></td>
	<td class="line x" title="87:102	Thus one might expect that the English of a Biology textbook would be intermediate between the more typical English of a news article and the specialized English native to the domain." ></td>
	<td class="line x" title="88:102	To test this we created a corpus of seven texts (BioBooks) on various areas of biology that were available on the web." ></td>
	<td class="line x" title="89:102	We observe in Figure 2 that for all quantities of self-training data one does better with Medline than BioBooks." ></td>
	<td class="line x" title="90:102	For example, at 37,000 sentences the BioBook corpus is only able to achieve and an f-measure of 82.8% while the Medline corpus is at 83.4%." ></td>
	<td class="line x" title="91:102	Furthermore, BioBooks levels off in performance while Medline has significant improvement left in it." ></td>
	<td class="line x" title="92:102	Thus, while the hypothesis seems reasonable, we were unable to make it work." ></td>
	<td class="line x" title="93:102	5 Conclusion We self-trained the standard C/J parser on 270,000 sentences of Medline abstracts." ></td>
	<td class="line x" title="94:102	By doing so we achieved a 20% error reduction over the best previous result for biomedical parsing." ></td>
	<td class="line x" title="95:102	In terms of the gap between the supervised data and the self-trained data, this is the largest that has been attempted." ></td>
	<td class="line x" title="96:102	Furthermore, the resulting parser is of interest in its own right, being as it is the most accurate biomedical parser yet developed." ></td>
	<td class="line x" title="97:102	This parser is available on the web.4 Finally, there is no reason to believe that 84.3% is an upper bound on what can be achieved with current techniques." ></td>
	<td class="line x" title="98:102	Lease and Charniak(2005) achieve their results usingsmall amounts of hand-annotated biomedical part-ofspeech-tagged data and also explore other possible sources or information." ></td>
	<td class="line x" title="99:102	It is reasonable to assume that its use would result in further improvement." ></td>
	<td class="line x" title="100:102	Acknowledgments This work was supported by DARPA GALE contract HR0011-06-2-0001." ></td>
	<td class="line x" title="101:102	We would like to thank the BLLIP team for their comments." ></td>
	<td class="line x" title="102:102	4http://bllip.cs.brown.edu/biomedical/ 103 0 25000 50000 75000 100000 125000 150000 175000 200000 225000 250000 275000 Number of sentences added 80.0 80.2 80.4 80.6 80.8 81.0 81.2 81.4 81.6 81.8 82.0 82.2 82.4 82.6 82.8 83.0 83.2 83.4 83.6 83.8 84.0 84.2 84.4 Reranking parser f-score WSJ+Medline WSJ+BioBooks WSJ+NANC WSJ (baseline) Figure 2: Labeled Precision-Recall results on development data for four versions of the parser as a function of number of self-training sentences" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="Data not found"></td>
	<td class="line x" title="1:100	Parser-Based Retraining for Domain Adaptation of Probabilistic Generators Deirdre Hogan, Jennifer Foster, Joachim Wagner and Josef van Genabith National Centre for Language Technology School of Computing Dublin City University Ireland {dhogan, jfoster, jwagner, josef}@computing.dcu.ie Abstract While the effect of domain variation on Penntreebank-trainedprobabilisticparsershasbeen investigated in previous work, we study its effect on a Penn-Treebank-trained probabilistic generator." ></td>
	<td class="line x" title="2:100	We show that applying the generator to data from the British National Corpus results in a performance drop (from a BLEU score of 0.66 on the standard WSJ test set to a BLEU score of 0.54 on our BNC test set)." ></td>
	<td class="line x" title="3:100	We develop a generator retraining method where the domain-specific training data is automatically produced using state-of-the-art parser output." ></td>
	<td class="line x" title="4:100	The retraining method recovers a substantial portion of the performance drop, resulting in a generator which achieves a BLEU score of 0.61 on our BNC test data." ></td>
	<td class="line x" title="5:100	1 Introduction Grammars extracted from the Wall Street Journal (WSJ) section of the Penn Treebank have been successfully applied to natural language parsing, and more recently, to natural language generation." ></td>
	<td class="line x" title="6:100	It is clear that high-quality grammars can be extracted for the WSJ domain but it is not so clear how these grammars scale to other text genres." ></td>
	<td class="line x" title="7:100	Gildea (2001), for example, has shown that WSJ-trained parsers suffer a drop in performance when applied to the more varied sentences of the Brown Corpus." ></td>
	<td class="line x" title="8:100	We investigate the effect of domain variation in treebank-grammar-based generation by applying a WSJ-trained generator to sentences from the British National Corpus (BNC)." ></td>
	<td class="line x" title="9:100	As with probabilistic parsing, probabilistic generation aims to produce the most likely output(s) given the input." ></td>
	<td class="line x" title="10:100	We can distinguish three types of probabilistic generators, based on the type of probability model used to select the most likely sentence." ></td>
	<td class="line x" title="11:100	The first type uses an n-gram language model, e.g.(Langkilde, 2000), the second type uses a probability model defined over trees or feature-structureannotated trees, e.g.(Cahill and van Genabith, 2006), and the third type is a mixture of the first and second type, employing n-gram and grammarbased features, e.g.(Velldal and Oepen, 2005)." ></td>
	<td class="line x" title="15:100	The generator used in our experiments is an instance of the second type, using a probability model defined over Lexical Functional Grammar c-structure and f-structure annotations (Cahill and van Genabith, 2006; Hogan et al., 2007)." ></td>
	<td class="line x" title="16:100	In an initial evaluation, we apply our probabilistic WSJ-trained generator to BNC material, and show that the generator suffers a substantial performance degradation, with a drop in BLEU score from 0.66 to 0.54." ></td>
	<td class="line x" title="17:100	We then turn our attention to the problem of adapting the generator so that it can more accurately generate the 1,000 sentences in our BNC test set." ></td>
	<td class="line x" title="18:100	The problem of adapting any NLP system to a domain different from the domain upon which it has been trained and for which no gold standard training material is available is a very real one, and one which has been the focus of much recent research in parsing." ></td>
	<td class="line x" title="19:100	Somesuccesshasbeenachievedbytraining a parser, not on gold standard hand-corrected trees, but on parser output trees." ></td>
	<td class="line oc" title="20:100	These parser output trees can by produced by a second parser in a co-training scenario (Steedman et al., 2003), or by the same parser with a reranking component in a type of selftraining scenario(McCloskyetal., 2006)." ></td>
	<td class="line x" title="21:100	Wetackle 165 the problem of domain adaptation in generation in a similar way, by training the generator on domain specific parser output trees instead of manually corrected gold standard trees." ></td>
	<td class="line x" title="22:100	This experiment achieves promising results, with an increase in BLEU score from 0.54 to 0.61." ></td>
	<td class="line x" title="23:100	The method is generic and can be applied to other probabilistic generators (for which suitable training material can be automatically produced)." ></td>
	<td class="line x" title="24:100	2 Background The natural language generator used in our experiments is the WSJ-trained system described in Cahill and van Genabith (2006) and Hogan et al.(2007)." ></td>
	<td class="line x" title="26:100	Sentences are generated from Lexical Functional Grammar (LFG) f-structures (Kaplan and Bresnan, 1982)." ></td>
	<td class="line x" title="27:100	The f-structures are created automatically by annotating nodes in the gold standard WSJ trees with LFG functional equations and then passing these equations through a constraint solver (Cahill et al., 2004)." ></td>
	<td class="line x" title="28:100	The generation algorithm is a chartbased one which works by finding the most probable tree associated with the input f-structure." ></td>
	<td class="line x" title="29:100	The yield of the most probable tree is the output sentence." ></td>
	<td class="line x" title="30:100	An annotated PCFG, in which the nonterminal symbols are decorated with functional information, is used to generate the most probable tree from an f-structure." ></td>
	<td class="line x" title="31:100	Cahill and van Genabith (2006) attain 98.2% coverage and a BLEU score of 0.6652 on the standard WSJ test set (Section 23)." ></td>
	<td class="line x" title="32:100	Hogan et al.(2007) describe an extension to the system which replaces the annotated PCFG selection model with a more sophisticated history-based probabilistic model." ></td>
	<td class="line x" title="34:100	Instead of conditioning the righthand side of a rule on the lefthand non-terminal and its associated functional information alone, the new model includes non-local conditioning information in the form of functional information associated with ancestor nodes of the lefthand side category." ></td>
	<td class="line x" title="35:100	This system achieves a BLEU score of 0.6724 and 99.9% coverage." ></td>
	<td class="line x" title="36:100	Other WSJ-trained generation systems include Nakanishi et al.(2005) and White et al.(2007)." ></td>
	<td class="line x" title="39:100	Nakanishi et al.(2005) describe a generator trained on a HPSG grammar derived from the WSJ Section of the Penn Treebank." ></td>
	<td class="line x" title="41:100	On sentences of  20 words in length, their system attains coverage of 90.75% and a BLEU score of 0.7733." ></td>
	<td class="line x" title="42:100	White et al.(2007) describe a CCG-based realisation system which has been trained on logical forms derived from CCGBank (Hockenmaier and Steedman, 2005), achieving 94.3% coverage and a BLEU score of 0.5768 on WSJ23 for all sentence lengths." ></td>
	<td class="line x" title="44:100	The input structures upon which these systems are trained vary in form and specificity, but what the systems have in common is that their various input structures are derived from Penn Treebank trees." ></td>
	<td class="line x" title="45:100	3 The BNC Test Data The new English test set consists of 1,000 sentences taken from the British National Corpus (Burnard, 2000)." ></td>
	<td class="line x" title="46:100	The BNC is a one hundred million word balanced corpus of British English from the late twentieth century." ></td>
	<td class="line x" title="47:100	Ninety per cent of it is written text, and the remaining 10% consists of transcribed spontaneous and scripted spoken language." ></td>
	<td class="line x" title="48:100	The BNC sentences in the test set are not chosen completely at random." ></td>
	<td class="line x" title="49:100	Each sentence in the test set has the property of containing a word which appears as a verb in the BNC but not in the usual training sections of the Wall Street Journal section of the Penn Treebank (WSJ02-21)." ></td>
	<td class="line x" title="50:100	Sentences were chosen in this way so that the resulting test set would be a difficult one for WSJ-trained systems." ></td>
	<td class="line x" title="51:100	In order to produce input f-structures for the generator, the test sentences were manually parsed by one annotator, using as references the Penn Treebank trees themselves and the Penn Treebank bracketing guidelines (Bies et al., 1995)." ></td>
	<td class="line x" title="52:100	When the two references did not agree, the guidelines took precedence over the Penn Treebank trees." ></td>
	<td class="line x" title="53:100	Difficult parsing decisions were documented." ></td>
	<td class="line x" title="54:100	Due to time constraints, the annotator did not mark functional tags or traces." ></td>
	<td class="line x" title="55:100	The context-free gold standard parse trees were transformed into fstructures using the automatic procedure of Cahill et al.(2004)." ></td>
	<td class="line x" title="57:100	4 Experiments Experimental Setup In our first experiment, we apply the original WSJ-trained generator to our BNC test set." ></td>
	<td class="line x" title="58:100	The gold standard trees for our BNC test set differ from the gold standard Wall Street Journal trees, in that they do not contain Penn-II traces or functional tags." ></td>
	<td class="line x" title="59:100	The process which pro166 duces f-structures from trees makes use of trace and functional tag information, if available." ></td>
	<td class="line x" title="60:100	Thus, to ensure that the training and test input f-structures are created in the same way, we use a version of the generator which is trained using gold standard WSJ trees without functional tag or trace information." ></td>
	<td class="line x" title="61:100	When we test this system on the WSJ23 f-structures (produced in the same way as the WSJ training material), the BLEU score decreases slightly from 0.67 to 0.66." ></td>
	<td class="line x" title="62:100	This is our baseline system." ></td>
	<td class="line x" title="63:100	In a further experiment, we attempt to adapt the generator to BNC data by using BNC trees as training material." ></td>
	<td class="line x" title="64:100	Because we lack gold standard BNC trees (apart from those in our test set), we try instead to use parse trees produced by an accurate parser." ></td>
	<td class="line x" title="65:100	We choose the Charniak and Johnson reranking parser because it is freely available and achievesstate-of-the-artaccuracy(aParsevalf-score of 91.3%) on the WSJ domain (Charniak and Johnson, 2005)." ></td>
	<td class="line x" title="66:100	It is, however, affected by domain variation  Foster et al.(2007) report that its f-score drops by approximately 8 percentage points when applied to the BNC domain." ></td>
	<td class="line x" title="68:100	Our training size is 500,000 sentences." ></td>
	<td class="line x" title="69:100	We conduct two experiments: the first, in which 500,000 sentences are extracted randomly from the BNC (minus the test set sentences), and the second in which only shorter sentences, of length  20 words, are chosen as training material." ></td>
	<td class="line x" title="70:100	The rationale behind the second experiment is that shorter sentences are less likely to contain parser errors." ></td>
	<td class="line x" title="71:100	We use the BLEU evaluation metric for our experiments." ></td>
	<td class="line x" title="72:100	We measure both coverage and full coverage." ></td>
	<td class="line x" title="73:100	Coverage measures the number of cases for which the generator produced some kind of output." ></td>
	<td class="line x" title="74:100	Full coverage measures the number of cases for which the generator produced a tree spanning all of the words in the input." ></td>
	<td class="line x" title="75:100	Results The results of our experiments are shown in Fig." ></td>
	<td class="line x" title="76:100	1." ></td>
	<td class="line x" title="77:100	The first row shows the results we obtain when the baseline system is applied to the fstructures derived from the 1,000 BNC gold standard parse trees." ></td>
	<td class="line x" title="78:100	The second row shows the results on the same test set for a system trained on Charniak and Johnson parser output trees for 500,000 BNC sentences." ></td>
	<td class="line x" title="79:100	The results in the final row are obtained by training the generator on Charniak and Johnson parser output trees for 500,000 BNC sentences of length  20 words in length." ></td>
	<td class="line x" title="80:100	Discussion As expected, the performance of the baseline system degrades when faced with out-ofdomain test data." ></td>
	<td class="line x" title="81:100	The BLEU score drops from a 0.66 score for WSJ test data to a 0.54 score for the BNC test data, and full coverage drops from 85.97% to 68.77%." ></td>
	<td class="line x" title="82:100	There is a substantial improvement, however, when the generator is trained on BNC data." ></td>
	<td class="line x" title="83:100	The BLEU score jumps from 0.5358 to 0.6135." ></td>
	<td class="line x" title="84:100	There are at least two possible reasons why a BLEU score of 0.66 is not obtained: The first is that the quality of the f-structure-annotated trees upon which the generator has been trained has degraded." ></td>
	<td class="line x" title="85:100	For the baseline system, the generator is trained on f-structure-annotated trees derived from gold trees." ></td>
	<td class="line x" title="86:100	The new system is trained on f-structureannotated parser output trees, and the performance of Charniak and Johnsons parser degrades when applied to BNC data (Foster et al., 2007)." ></td>
	<td class="line x" title="87:100	The second reason has been suggested by Gildea (2001): WSJ dataiseasiertolearnthanthemorevarieddatainthe Brown Corpus or BNC." ></td>
	<td class="line x" title="88:100	Perhaps even if gold standard BNC parse trees were available for training, the system would not behave as well as it does for WSJ material." ></td>
	<td class="line x" title="89:100	It is interesting to note that training on 500,000 shorter sentences does not appear to help." ></td>
	<td class="line x" title="90:100	We hypothesized that it would improve results because shorter sentences are less likely to contain parser errors." ></td>
	<td class="line x" title="91:100	The drop in full coverage from 86.69% to 79.58% suggests that the number of short sentences needs to be increased so that the size of the training material stays constant." ></td>
	<td class="line x" title="92:100	5 Conclusion We have investigated the effect of domain variation on a LFG-based WSJ-trained generation system by testing the systems performance on 1,000 sentences from the British National Corpus." ></td>
	<td class="line x" title="93:100	PerformancedropsfromaBLEUscoreof0.66onWSJtest data to 0.54 on the BNC test set." ></td>
	<td class="line x" title="94:100	Encouragingly, we have also shown that domain-specific training material produced by a parser can be used to claw back a significant portion of this performance degradation." ></td>
	<td class="line x" title="95:100	Our method is general and could be applied to other WSJ-trained generators (e.g.(Nakanishi et 167 Train BLEU Coverage Full Coverage WSJ02-21 0.5358 99.1 68.77 BNC(500k) 0.6135 99.1 86.69 BNC(500k)  20 words 0.5834 99.1 79.58 Figure 1: Results for 1,000 BNC Sentences al., 2005; White et al., 2007))." ></td>
	<td class="line x" title="97:100	We intend to continue this research by training our generator on parse trees produced by a BNC-self-trained version of the CharniakandJohnsonrerankingparser(Fosteretal., 2007)." ></td>
	<td class="line x" title="98:100	Wealsohopetoextendtheevaluationbeyond the BLEU metric by carrying out a human judgement evaluation." ></td>
	<td class="line x" title="99:100	Acknowledgments This research has been supported by the Enterprise Ireland Commercialisation Fund (CFTD/2007/229), Science Foundation Ireland (04/IN/I527) and the IRCSET Embark Initative (P/04/232)." ></td>
	<td class="line x" title="100:100	We thank the Irish Centre for High End Computing for providing computing facilities." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1087
Self-Training PCFG Grammars with Latent Annotations Across Languages
Huang, Zhongqiang;Harper, Mary;"></td>
	<td class="line x" title="1:227	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832841, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:227	c 2009 ACL and AFNLP Self-Training PCFG Grammars with Latent Annotations Across Languages Zhongqiang Huang1 1Laboratory for Computational Linguistics and Information Processing Institute for Advanced Computer Studies University of Maryland, College Park zqhuang@umiacs.umd.edu Mary Harper1,2 2Human Language Technology Center of Excellence Johns Hopkins University mharper@umiacs.umd.edu Abstract We investigate the effectiveness of selftraining PCFG grammars with latent annotations (PCFG-LA) for parsing languages with different amounts of labeled training data." ></td>
	<td class="line x" title="3:227	Compared to Charniaks lexicalized parser, the PCFG-LA parser was more effectively adapted to a language for which parsing has been less well developed (i.e., Chinese) and benefited more from selftraining." ></td>
	<td class="line x" title="4:227	We show for the first time that self-training is able to significantly improve the performance of the PCFG-LA parser, a single generative parser, on both small and large amounts of labeled training data." ></td>
	<td class="line x" title="5:227	Our approach achieves stateof-the-art parsing accuracies for a single parser on both English (91.5%) and Chinese (85.2%)." ></td>
	<td class="line x" title="6:227	1 Introduction There is an extensive research literature on building high quality parsers for English (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Petrov et al., 2006), however, models for parsing other languages are less well developed." ></td>
	<td class="line x" title="7:227	Take Chinese for example; there have been several attempts to develop accurate parsers for Chinese (Bikel and Chiang, 2000; Levy and Manning, 2003; Petrov and Klein, 2007), but the state-of-the-art performance, around 83% F measure on Penn Chinese Treebank (achieved by the Berkeley parser (Petrov and Klein, 2007)) falls far short of performance on English (90-92%)." ></td>
	<td class="line x" title="8:227	As pointed out in (Levy and Manning, 2003), there are many linguistic differences between Chinese and English, as well as structural differences between their corresponding treebanks, and some of these make it a harder task to parse Chinese." ></td>
	<td class="line x" title="9:227	Additionally, the fact that the available treebanked Chinese materials are more limited than for English also increases the challenge of building high quality Chinese parsers." ></td>
	<td class="line x" title="10:227	Many of these differences would also tend to apply to other less well investigated languages." ></td>
	<td class="line x" title="11:227	In this paper, we focus on English and Chinese because the former is a language for which extensive parsing research has been conducted while the latter is a language that has been less extensively studied." ></td>
	<td class="line x" title="12:227	We adapt and improve the Berkeley parser, which learns PCFG grammars with latent annotations, and show through comparative studies that this parser significantly outperforms Charniaks parser, which was initially developed for English and subsequently ported to Chinese." ></td>
	<td class="line x" title="13:227	We focus on answering two questions: how well does a parser perform across languages and how much does it benefit from self-training?" ></td>
	<td class="line x" title="14:227	The first question is of special interest when choosing a parser that is designed for one language and adapting it to another less studied language." ></td>
	<td class="line x" title="15:227	We improve the PCFG-LA parser by adding a language-independent method for handling rare words and adapt it to another language, Chinese, by creating a method to better model Chinese unknown words." ></td>
	<td class="line x" title="16:227	Our results show that the PCFGLA parser performs significantly better than Charniaks parser on Chinese, and is also somewhat more accurate on English, although both parsers have high accuracy." ></td>
	<td class="line x" title="17:227	The second question is important because labeled training data is often quite limited, especially for less well investigated languages, while unlabeled data is ubiquitous." ></td>
	<td class="line x" title="18:227	Early investigations on self-training for parsing have had mixed results." ></td>
	<td class="line x" title="19:227	Charniak (1997) reported no improvements from self-training a PCFG parser on the standard WSJ training set." ></td>
	<td class="line x" title="20:227	Steedman et al.(2003) reported some degradation using a lexicalized tree adjoining grammar parser and minor improvement using Collins lexicalized PCFG parser; however, this gain was obtained only when the parser 832 was trained on a small labeled set." ></td>
	<td class="line x" title="22:227	Reichart and Rappoport (2007) obtained significant gains using Collins lexicalized parser with a different selftraining protocol, but again they only looked at small labeled sets." ></td>
	<td class="line nc" title="23:227	McClosky et al.(2006) effectively utilized unlabeled data to improve parsing accuracy on the standard WSJ training set, but they used a two-stage parser comprised of Charniaks lexicalized probabilistic parser with n-best parsing and a discriminative reranking parser (Charniak and Johnson, 2005), and thus it would be better categorized as co-training (McClosky et al., 2008)." ></td>
	<td class="line n" title="25:227	It is worth noting that their attempts at selftraining Charniaks lexicalized parser directly resulted in no improvement." ></td>
	<td class="line x" title="26:227	There are other successful semi-supervised training approaches for dependency parsing, such as (Koo et al., 2008; Wang et al., 2008), and it would be interesting to investigate how they could be applied to constituency parsing." ></td>
	<td class="line x" title="27:227	We show in this paper, for the first time, that self-training is able to significantly improve the performance of the PCFG-LA parser, a single generative parser, on both small and large amounts of labeled training data, for both English and Chinese." ></td>
	<td class="line x" title="28:227	With self-training, a fraction of the WSJ or CTB6 treebank training data is sufficient to train a PCFG-LA parser that is able to achieve or even beat the accuracies obtained using a single parser trained on the entire treebank without selftraining." ></td>
	<td class="line x" title="29:227	We conjecture based on our comparison of the PCFG-LA parser to Charniaks parser that the addition of self-training data helps the former parser learn more fine-grained latent annotations without over-fitting." ></td>
	<td class="line x" title="30:227	The rest of this paper is organized as follows." ></td>
	<td class="line x" title="31:227	We describe the PCFG-LA parser and several enhancements in Section 2, and discuss self-training in Section 3." ></td>
	<td class="line x" title="32:227	We then outline the experimental setup in Section 4, describe the results in Section 5, and present a detailed analysis in Section 6." ></td>
	<td class="line x" title="33:227	The last section draws conclusions and describes future work." ></td>
	<td class="line x" title="34:227	2 Parsing Model The Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007) is an efficient and effective parser that introduces latent annotations (Matsuzaki et al., 2005) to refine syntactic categories to learn better PCFG grammars." ></td>
	<td class="line x" title="35:227	In the example parse tree in Figure 1(a), each syntactic category is split into multiple latent subcategories, and accordingly the original parse tree is decomposed into many parse trees with latent annotations." ></td>
	<td class="line x" title="36:227	Figure 1(b) depicts one of such trees." ></td>
	<td class="line x" title="37:227	The grammar and lexical rules are split accordingly, e.g., NPPRP is split into different NP-iPRP-j rules." ></td>
	<td class="line x" title="38:227	The expansion probabilities of these split rules are the parameters of a PCFG-LA grammar." ></td>
	<td class="line x" title="39:227	S She PRP NP VP VBD heardDT NP NN the noise . . ." ></td>
	<td class="line x" title="40:227	NP2 VBD5PRP3 She heardDT2 the noise NN6 NP6 .1 S1 VP4 (a) (b) Figure 1: (a) original treebank tree, (b) after latent annotation." ></td>
	<td class="line x" title="41:227	The objective of training is to learn a grammar with latent annotations that maximizes the likelihood of the training trees, i.e., the sum of the likelihood of all parse trees with latent annotations." ></td>
	<td class="line x" title="42:227	Since the latent annotations are not available in the treebank, a variant of the EM algorithm is utilized to learn the rule probabilities for them." ></td>
	<td class="line x" title="43:227	The Berkeley parser employs a hierarchical split-merge method that gradually increases the number of latent annotations and adaptively allocates them to different treebank categories to best model the training data." ></td>
	<td class="line x" title="44:227	In this paper, we call a grammar trained after n split-merge steps an nth order grammar." ></td>
	<td class="line x" title="45:227	The order of a grammar is a step (not continuous) function of the number of latent annotations because the split-merge algorithm first splits each latent annotation into two and then merges some of the splits back based on their ability to increase training likelihood." ></td>
	<td class="line x" title="46:227	For this paper, we implemented1 our own version of Berkeley parser." ></td>
	<td class="line x" title="47:227	Updates include better handling of rare words across languages, as well as unknown Chinese words." ></td>
	<td class="line x" title="48:227	The parser is able to process difficult sentences robustly using adaptive beam expansion." ></td>
	<td class="line x" title="49:227	The training algorithm was updated to support a wide range of self-training experiments (e.g., posterior-weighted unlabeled data, introducing self-training in later iterations) and to make use of multiple processors to parallelize EM training." ></td>
	<td class="line x" title="50:227	The parallelization is crucial 1A major motivation for this implementation was to support some algorithms we are developing." ></td>
	<td class="line x" title="51:227	Most of our enhancements will be merged with a future release of the Berkeley parser." ></td>
	<td class="line x" title="52:227	833 for training a model with large volumes of data in a reasonable amount of time2." ></td>
	<td class="line x" title="53:227	We next describe the language-independent method to handle rare words, which is important for training better PCFG-LA grammars especially when the training data is limited in size, and our unknown Chinese word handling method, highlighting the importance of utilizing languagespecific features to enhance parsing performance." ></td>
	<td class="line x" title="54:227	As we will see later, both of these methods significantly improve parsing performance." ></td>
	<td class="line x" title="55:227	2.1 Rare Word Handling Whereas rule expansions are frequently observed in the treebank, word-tag co-occurrences are sparser and more likely to suffer from over-fitting." ></td>
	<td class="line x" title="56:227	Although the lexicon smoothing method in the Berkeley parser is able to make the word emission probabilities of different latent states of a POS tag more alike, the EM training algorithm still strongly discriminates among word identities." ></td>
	<td class="line x" title="57:227	Suppose word tag pairs w1,t and w2,t both appear the same number of times in the training data." ></td>
	<td class="line x" title="58:227	In a PCFG grammar without latent annotations, the probabilities of emitting these two words given tag t would be the same, i.e., p(w1|t) = p(w2|t)." ></td>
	<td class="line x" title="59:227	After introducing latent annotation x to tagt, the emission probabilities of these two words given a latent state tx may no longer be the same because p(w1|tx) and p(w2|tx) are two independent parameters that the EM algorithm optimizes on." ></td>
	<td class="line x" title="60:227	It is beneficial to learn subcategories of POS tags to model different types of words, especially for frequent words; however, it is not desirable to strongly discriminate among rare words because it could distract the model from learning about common phenomena." ></td>
	<td class="line x" title="61:227	To handle this problem, the probability of a latent state tx generating a rare word w is forced to be proportional to the emission probability of word w given the surface tag t. This is achieved by mapping all words with frequency less than threshold3  to the unk symbol, and for each latent statetx of a POS tagt, accumulating the word tag statistics of these rare words to cr(tx,unk) =summationtext w:c(w)<c(tx,w), and then redistributing them among the rare words to estimate their emission 2The parallel version is able to train our largest grammar on a 8-core machine within a week, while the non-parallel version is not able to finish even after 3 weeks." ></td>
	<td class="line x" title="62:227	3The value of  is tuned on the development set." ></td>
	<td class="line x" title="63:227	probabilities: c(tx,w) = cr(tx,unk) c(t,w)c r(t,unk) p(w|tx) = c(tx,w)/ summationdisplay wc(tx,w) 2.2 Chinese Unknown Word Handling The Berkeley parser utilizes statistics associated with rare words (e.g., suffix, capitalization) to estimate the emission probabilities of unknown words at decoding time." ></td>
	<td class="line x" title="64:227	This is adequate for for English, however, only a limited number of classes of unknown words, such as digits and dates, are handled for Chinese." ></td>
	<td class="line x" title="65:227	In this paper, we develop a characterbased unknown word model inspired by (Huang et al., 2007) that reflects the fact that characters in any position (prefix, infix, or suffix) can be predictive of the part-of-speech (POS) type for Chinese words." ></td>
	<td class="line x" title="66:227	In our model, the word emission probability, p(w|tx), of an unknown word w given the latent state tx of POS tag t is estimated by the geometric average of the emission probability of the characters ck in the word: P(w|tx) = n radicalbiggproductdisplay ckw,P(ck|t)negationslash=0P(ck|t) where n = |{ckw|P(ck|t)negationslash= 0}|." ></td>
	<td class="line x" title="67:227	Characters not seen in the training data are ignored in the computation of the geometric average." ></td>
	<td class="line x" title="68:227	We back off to use the rare word statistics regardless of word identity when the above equation cannot be used to compute the emission probability." ></td>
	<td class="line x" title="69:227	3 Parser Self-Training Our hypothesis is that combining automatically labeled parses with treebank trees will help the EM training of the PCFG-LA parser to make more informed decisions about latent annotations and thus generate more effective grammars." ></td>
	<td class="line x" title="70:227	In this section, we discuss how self-training is applied to train a PCFG-LA parser." ></td>
	<td class="line x" title="71:227	There are several ways to automatically label the data." ></td>
	<td class="line x" title="72:227	A fairly standard method is to parse the unlabeled sentences with a parser trained on labeled training data, and then combine the resulting parses with the treebank training data to retrain the parser." ></td>
	<td class="line x" title="73:227	This is the approach we chose for self-training." ></td>
	<td class="line x" title="74:227	An alternative approach is to run EM directly on the labeled treebank trees and the unlabeled sentences, without explicit parse trees for the unlabeled sentences." ></td>
	<td class="line x" title="75:227	However, because the 834 brackets would need to be determined for the unlabeled sentences together with the latent annotations, this would increase the running time from linear in the number of expansion rules to cubic in the length of the sentence." ></td>
	<td class="line x" title="76:227	Another important decision is how to weight the gold standard and automatically labeled data when training a new parser model." ></td>
	<td class="line x" title="77:227	Errors in the automatically labeled data could limit the accuracy of the self-trained model, especially when there is a much greater quantity of automatically labeled data than the gold standard training data." ></td>
	<td class="line x" title="78:227	To balance the gold standard and automatically labeled data, one could duplicate the treebank data to match the size of the automatically labeled data; however, the training of the PCFGLA parser would result in redundant applications of EM computations over the same data, increasing the cost of training." ></td>
	<td class="line x" title="79:227	Instead we weight the posterior probabilities computed for the gold and automatically labeled data, so that they contribute equally to the resulting grammar." ></td>
	<td class="line x" title="80:227	Our preliminary experiments show that balanced weighting is effective, especially for Chinese (about 0.4% absolute improvement) where the automatic parse trees have a relatively lower accuracy." ></td>
	<td class="line x" title="81:227	The training procedure of the PCFG-LA parser gradually introduces more latent annotations during each split-merge stage, and the self-labeled data can be introduced at any of these stages." ></td>
	<td class="line x" title="82:227	Introduction of the self-labeled data in later stages, after some important annotations are learned from the treebank, could result in more effective learning." ></td>
	<td class="line x" title="83:227	We have found that a middle stage introduction (after 3 split-merge iterations) of the automatically labeled data has an effect similar to balancing the weights of the gold and automatically labeled trees, possibly due to the fact that both methods place greater trust in the former than the latter." ></td>
	<td class="line x" title="84:227	In this study, we introduce the automatically labeled data at the outset and weight it equally with the gold treebank training data in order to focus our experiments to support a deeper analysis." ></td>
	<td class="line x" title="85:227	4 Experimental Setup For the English experiments, sections from the WSJ Penn Treebank are used as labeled training data: section 2-19 for training, section 22 for development, and section 23 as the test set." ></td>
	<td class="line x" title="86:227	We also used 210k4 sentences of unlabeled news articles in the BLLIP corpus for English self-training." ></td>
	<td class="line x" title="87:227	For the Chinese experiments, the Penn Chinese Treebank 6.0 (CTB6) (Xue et al., 2005) is used as labeled data." ></td>
	<td class="line x" title="88:227	CTB6 includes both news articles and transcripts of broadcast news." ></td>
	<td class="line x" title="89:227	We partitioned the news articles into train/development/test sets following Huang et al.(2007)." ></td>
	<td class="line x" title="91:227	The broadcast news section is added to the training data because it shares many of the characteristics of newswire text (e.g., fully punctuated, contains nonverbal expressions such as numbers and symbols)." ></td>
	<td class="line x" title="92:227	In addition, 210k sentences of unlabeled Chinese news articles are used for self-training." ></td>
	<td class="line x" title="93:227	Since the Chinese parsers in our experiments require wordsegmented sentences as input, the unlabeled sentences need to be word-segmented first." ></td>
	<td class="line x" title="94:227	As shown in (Harper and Huang, 2009), the accuracy of automatic word segmentation has a great impact on Chinese parsing performance." ></td>
	<td class="line x" title="95:227	We chose to use the Stanford segmenter (Chang et al., 2008) in our experiments because it is consistent with the treebank segmentation and provides the best performance among the segmenters that were tested." ></td>
	<td class="line x" title="96:227	To minimize the discrepancy between the selftraining data and the treebank data, we normalize both CTB6 and the self-training data using UW Decatur (Zhang and Kahn, 2008) text normalization." ></td>
	<td class="line x" title="97:227	Table 1 summarizes the data set sizes used in our experiments." ></td>
	<td class="line x" title="98:227	We used slightly modified versions of the treebanks; empty nodes and nonterminal-yield unary rules5, e.g., NPVP, are deleted using tsurgeon (Levy and Andrew, 2006)." ></td>
	<td class="line x" title="99:227	Train Dev Test Unlabeled English 39.8k 1.7k 2.4k 210k(950.0k) (40.1k) (56.7k) (5,082.1k) Chinese 24.4k 1.9k 2.0k 210k(678.8k) (51.2k) (52.9k) (6,254.9k) Table 1: The number of sentences (and words in parentheses) in our experiments." ></td>
	<td class="line x" title="100:227	We trained parsers on 20%, 40%, 60%, 80%, and 100% of the treebank training data to evaluate 4This amount was constrained based on both CPU and memory." ></td>
	<td class="line x" title="101:227	We plan to investigate cloud computing to exploit more unlabeled data." ></td>
	<td class="line x" title="102:227	5As nonterminal-yield unary rules are less likely to be posited by a statistical parser, it is common for parsers trained on the standard Chinese treebank to have substantially higher precision than recall." ></td>
	<td class="line x" title="103:227	This gap between bracket recall and precision is alleviated without loss of parse accuracy by deleting the nonterminal-yield unary rules." ></td>
	<td class="line x" title="104:227	This modification similarly benefits both parsers we study here." ></td>
	<td class="line x" title="105:227	835 the effect of the amount of labeled training data on parsing performance." ></td>
	<td class="line x" title="106:227	We also compare how selftraining impacts the models trained with different amounts of gold-standard training data." ></td>
	<td class="line x" title="107:227	This allows us to simulate scenarios where the language has limited human-labeled resources." ></td>
	<td class="line x" title="108:227	We compare models trained only on the gold labeled training data with those that utilize additional unlabeled data." ></td>
	<td class="line x" title="109:227	Self-training (PCFG-LA or Charniak) proceeds in two steps." ></td>
	<td class="line x" title="110:227	In the first step, the parser is first trained on the allocated labeled training data (e.g., 40%) and is then used to parse the unlabeled data." ></td>
	<td class="line x" title="111:227	In the second step, a new parser is trained on the weighted combination6 of the allocated labeled training data and the additional automatically labeled data." ></td>
	<td class="line x" title="112:227	The development set is used in each step to select the grammar order with the best accuracy for the PCFG-LA parser and to tune the smoothing parameters for Charniaks parser." ></td>
	<td class="line x" title="113:227	5 Results In this section, we first present the effect of unknown and rare word handling for the PCFG-LA parser, and then compare and discuss the performance of the PCFG-LA parser and Charniaks parser across languages with different amounts of labeled training, either with or without selftraining." ></td>
	<td class="line x" title="114:227	5.1 Rare and Unknown Word Handling Table 2 reports the effect of unknown and rare word handing for the PCFG-LA parser trained on 100%7 of the labeled training data." ></td>
	<td class="line x" title="115:227	The rare word handling improves the English parser by 0.68% and the Chinese parser by 0.56% over the Berkeley parser." ></td>
	<td class="line x" title="116:227	The Chinese unknown word handling method alone improves the Chinese parser by 0.47%." ></td>
	<td class="line x" title="117:227	The rare and unknown handling methods together improve the Chinese parser by 0.92%." ></td>
	<td class="line x" title="118:227	All the improvements are statistically significant8." ></td>
	<td class="line x" title="119:227	We found that the rare word handling method becomes more effective as the number of latent annotations increases, especially when there is not a 6We balance the size of manually and automatically labeled data by posterior weighting for the PCFG-LA parsers and by duplication for Charniaks parser." ></td>
	<td class="line x" title="120:227	7Greater improvements are obtained using smaller amounts of labeled training data." ></td>
	<td class="line x" title="121:227	8We use Bikels randomized parsing evaluation comparator to determine the significance (p < 0.05) of difference between two parsers output." ></td>
	<td class="line x" title="122:227	English Chinese PCFG-LA 89.95 83.23 +R 90.63 83.79 +U N/A 83.70 +R+U N/A 84.15 Table 2: Effects of rare word handling (+R) and Chinese unknown handling (+U) on the test set." ></td>
	<td class="line x" title="123:227	sufficient amount of labeled training data." ></td>
	<td class="line x" title="124:227	Sharing statistics of the rare words during training results in more robust grammars with better parsing performance." ></td>
	<td class="line x" title="125:227	The unknown word handling method also gives greater improvements on grammars trained on smaller amounts of training data, suggesting that it is quite helpful for modeling unseen words at decoding time." ></td>
	<td class="line x" title="126:227	However, it tends to be less effective when the number of latent annotations increases, probably because the probability estimation of unseen words based on surface tags is less reliable for finer-gained latent annotations." ></td>
	<td class="line x" title="127:227	5.2 Labeled Data Only When comparing the two parsers on both languages in Figure 2 with treebank training, it is clear that they perform much better on English than Chinese." ></td>
	<td class="line x" title="128:227	While this is probably due in part to the years of research on English, Chinese still appears to be more challenging than English." ></td>
	<td class="line x" title="129:227	The comparison between the two parsing approaches provides two interesting conclusions." ></td>
	<td class="line x" title="130:227	First, the PCFG-LA parser always performs significantly better than Charniaks parser on Chinese, although both model English well." ></td>
	<td class="line x" title="131:227	Admittedly Charniaks parser has not been optimized9 on Chinese, but neither has the PCFGLA parser10." ></td>
	<td class="line x" title="132:227	The lexicalized model in Charniaks parser was first optimized for English and required sophisticated smoothing to deal with sparseness; however, the lexicalized model developed for Chinese works less well." ></td>
	<td class="line x" title="133:227	In contrast, the PCFG-LA parser learns the latent annotations from the data, without any specification of what precisely should be modeled and how it should be modeled." ></td>
	<td class="line x" title="134:227	This flexibility may help it better model new languages." ></td>
	<td class="line x" title="135:227	Second, while both parsers benefit from increased amounts of gold standard training data, the PCFG-LA parser gains more." ></td>
	<td class="line x" title="136:227	The PCFG-LA parser is initially poorer than Charniaks parser 9The Chinese port includes modification of the head table, implementation of a Chinese punctuation model, etc. 10The PCFG-LA parser without the unknown word handling method still outperforms Charniaks parser on Chinese." ></td>
	<td class="line x" title="137:227	836  87  88  89  90  91  92  0.2  0.4  0.6  0.8  1 F score Number of Labeled WSJ Training Trees x 39,832 PCFG-LAPCFG-LA.ST CharniakCharniak.ST (a) English  76  78  80  82  84  86  0.2  0.4  0.6  0.8  1 F score Number of Labeled CTB Training Trees x 24,416 (b) Chinese Figure 2: The performance of the PCFG-LA parser and Charniaks parser evaluated on the test set, trained with different amounts of labeled training data, with and without self-training (ST)." ></td>
	<td class="line x" title="138:227	when trained on 20% WSJ training data, probably because the training data is too small for it to learn fine-grained annotations without over-fitting." ></td>
	<td class="line x" title="139:227	As more labeled training data becomes available, the performance of the PCFG-LA parser improves quickly and finally outperforms Charniaks parser significantly." ></td>
	<td class="line x" title="140:227	Moreover, performance of the PCFG-LA parser continues to grow when more labeled training data is available, while the performance of Charniaks parser levels out at around 80% of the labeled data." ></td>
	<td class="line x" title="141:227	The PCFG-LA parser improves by 3.5% when moving from 20% to 100% training data, compared to a 2.21% gain for Charniaks parser." ></td>
	<td class="line x" title="142:227	Similarly for Chinese, the PCFGLA parser also gains more (4.48% vs 3.63%)." ></td>
	<td class="line x" title="143:227	5.3 Labeled + Self-Labeled The PCFG-LA parser is also able to benefit more from self-training than Charniaks parser." ></td>
	<td class="line x" title="144:227	On the WSJ data set, Charniaks parser benefits from selftraining initially when there is little labeled training data, but the improvement levels out quickly as more labeled training trees become available." ></td>
	<td class="line x" title="145:227	In contrast, the PCFG-LA parser benefits consistently from self-training11, even when using 100% 11One may notice that the self-trained PCFG-LA parser with 100% labeled WSJ data has a slightly lower test accuof the labeled training set." ></td>
	<td class="line x" title="146:227	Similar trends are also found for Chinese." ></td>
	<td class="line x" title="147:227	It should be noted that the PCFG-LA parser trained on a fraction of the treebank training data plus a large amount of self-labeled training data, which comes with little or no cost, performs comparably or even better than grammars trained with additional labeled training data." ></td>
	<td class="line x" title="148:227	For example, the self-trained PCFG-LA parser with 60% labeled data is able to outperform the grammar trained with 100% labeled training data alone for both English and Chinese." ></td>
	<td class="line x" title="149:227	With self-training, even 40% labeled WSJ training data is sufficient to train a PCFG-LA parser that is comparable to the model trained on the entire WSJ training data alone." ></td>
	<td class="line x" title="150:227	This is of significant importance, especially for languages with limited human-labeled resources." ></td>
	<td class="line x" title="151:227	One might conjecture that the PCFG-LA parser benefits more from self-training than Charniaks parser because its self-labeled data has higher accuracy." ></td>
	<td class="line x" title="152:227	However, this is not true." ></td>
	<td class="line x" title="153:227	As shown in Figure 2 (a), the PCFG-LA parser trained with 40% of the WSJ training set alone has a much lower performance (88.57% vs 89.96%) than Charniaks parser trained on the full WSJ training set." ></td>
	<td class="line x" title="154:227	With the same amount of self-training data (labeled by each parser), the resulting PCFG-LA parser obtains a much higher F score than the self-trained Charniaks parser (90.52% vs 90.18%)." ></td>
	<td class="line x" title="155:227	Similar patterns can also be found for Chinese." ></td>
	<td class="line x" title="156:227	English Chinese PCFG-LA 90.63 84.15 + Self-training 91.46 85.18 Table 3: Final results on the test set." ></td>
	<td class="line x" title="157:227	Table 3 reports the final results on the test set when trained on the entire WSJ or CTB6 training set." ></td>
	<td class="line pc" title="158:227	For English, self-training contributes 0.83% absolute improvement to the PCFG-LA parser, which is comparable to the improvement obtained from using semi-supervised training with the twostage parser in (McClosky et al., 2006)." ></td>
	<td class="line n" title="159:227	Note that their improvement is achieved with the addition of 2,000k unlabeled sentences using the combination of a generative parser and a discriminative reranker, compared to using only 210k unlabeled sentences with a single generative parser in our approach." ></td>
	<td class="line x" title="160:227	For Chinese, self-training results in a racy than the self-trained PCFG-LA parser with 80% labeled WSJ data." ></td>
	<td class="line x" title="161:227	This is due to the variance in parser performance when initialized with different seeds and the fact that the development set is used to pick the best model for evaluation." ></td>
	<td class="line x" title="162:227	837  87 88  89 90  91 92  93 94  95 96  97 98  0.2  0.4  0.6  0.8  1 F score Number of Labeled WSJ Training Treesx 39,832Test Test.ST TrainTrain.ST 60  65  70  75  80  85  90  95  100  0  1  2  3  4  5  6  7103 104 105 106 107 F score Number of Rules (log scale) Split-Merge Roundsfewer latent states more latent statesTest Test.ST TrainTrain.ST RulesRules.ST  87 88  89 90  91 92  93 94  95 96  97  0.2  0.4  0.6  0.8  1 F score Number of Labeled WSJ Training Treesx 39,832Test Test.ST TrainTrain.ST 5 6 6 6 6 6 7 7 7 7 (a) Charniak (b) PCFG-LA (20% WSJ) (c) PCFG-LA Figure 3: (a) The training/test accuracy of Charniaks parser trained on varying amounts of labeled WSJ training data, with and without self-training (ST)." ></td>
	<td class="line x" title="163:227	(b) The training/test accuracy and the number of nonzero rules of the PCFG-LA grammars trained on 20% of the labeled WSJ training data, w/ and w/o ST." ></td>
	<td class="line x" title="164:227	(c) The training/test accuracy of the PCFG-LA parser trained on varying amount of labeled WSJ training data, w/ and w/o ST; the numbers along the training curves indicate the order of the grammars." ></td>
	<td class="line x" title="165:227	76  78  80  82  84  86  88  90  92  94  0.2  0.4  0.6  0.8  1 F score Number of Labeled CTB Training Treesx 24,416Test Test.ST TrainTrain.ST 55  60  65  70  75  80  85  90  95  100  0  1  2  3  4  5  6  7103 104 105 106 107 F score Nonzero Rules (log scale) Split-Merge Roundsfewer latent states more latent statesTest Test.ST TrainTrain.ST RulesRules.ST  78  80  82  84  86  88  90  92  94  0.2  0.4  0.6  0.8  1 F score Number of Labeled CTB Training Trees Train/Test Performance of the PCFG-LA Parser (CTB) x 24,416 TestTest.ST TrainTrain.ST 4 5 5 6 6 6 6 6 7 7 (a) Charniak (b) PCFG-LA (20% CTB) (c) PCFG-LA Figure 4: (a) The training/test accuracy of Charniaks parser trained on varying amounts of labeled CTB training data, with and without self-training (ST)." ></td>
	<td class="line x" title="166:227	(b) The training/test accuracy and the number of nonzero rules of the PCFG-LA grammars trained on 20% of the labeled CTB training data, w/ and w/o ST." ></td>
	<td class="line x" title="167:227	(c) The training/test accuracy of the PCFG-LA parser trained on varying amount of labeled CTB training data, w/ and w/o ST; the numbers along the training curves indicate the order of the grammars." ></td>
	<td class="line x" title="168:227	state-of-the-art parsing model with 85.18% accuracy (1.03% absolute improvement) on a representative test set." ></td>
	<td class="line x" title="169:227	Both improvements are statistically significant." ></td>
	<td class="line x" title="170:227	6 Analysis In this section, we perform a series of analyses, focusing on English (refer to Figure 3), to investigate why the PCFG-LA parser benefits more from additional data, most particularly automatically labeled data, when compared to Charniaks parser." ></td>
	<td class="line x" title="171:227	Similar analyses have been done for Chinese with similar results (refer to Figure 4)." ></td>
	<td class="line x" title="172:227	Charniaks parser is a lexicalized PCFG parser that models lexicalized dependencies explicitly observable in the training data and relies on smoothing to avoid over-fitting." ></td>
	<td class="line x" title="173:227	Although it is able to benefit from more training data because of broader lexicon and rule coverage and more robust estimation of parameters, its ability to benefit from the additional data is limited in the sense that it is not able to generate additional predictive features that are supported by this data." ></td>
	<td class="line x" title="174:227	As shown in figure 3(a), the parsing accuracy of Charniaks parser on the test set improves as the amount of labeled training data increases; however, the training accuracy12 degrades as more data is added." ></td>
	<td class="line x" title="175:227	Note that the training accuracy13 of Charniaks parser also 12The parser is tested on the treebank labeled set that the parser is trained on." ></td>
	<td class="line x" title="176:227	13The self-training data is combined with the labeled treebank trees in a weighted manner; otherwise, the training accuracy would be even lower." ></td>
	<td class="line x" title="177:227	838 decreases after the addition of self-training data." ></td>
	<td class="line x" title="178:227	This is expected for models like Charniaks parser with fixed model parameters; it is harder to model more data with greater diversity." ></td>
	<td class="line x" title="179:227	The addition of self-labeled data helps on the test set initially but it provides little gain when the labeled training data becomes relatively large." ></td>
	<td class="line x" title="180:227	In contrast, the PCFG-LA grammar is able to model the training data with different granularities." ></td>
	<td class="line x" title="181:227	Fewer latent annotations are employed when the training set is small." ></td>
	<td class="line x" title="182:227	As the size of the training data increases, it is able to allocate more latent annotations to better model the data." ></td>
	<td class="line x" title="183:227	As shown in Figure 3 (b), for a fixed amount (20%) of labeled training data, the accuracy of the model on training data continues to improve as the number of latent annotation increases." ></td>
	<td class="line x" title="184:227	Although it is important to limit the number of latent annotations to avoid over-fitting, the ability to model training data accurately given sufficient latent annotations is desirable when more training data is available." ></td>
	<td class="line x" title="185:227	When trained on the labeled data (20%) alone, the 5-th order grammar achieves its optimal generalization performance (based on the development set) and begins to degrade afterwords." ></td>
	<td class="line x" title="186:227	With the addition of self-training data, the 5-th order grammar achieves an even greater accuracy on the test set and its performance continues to increase14 when moving to the 6-th or even 7-th order grammar." ></td>
	<td class="line x" title="187:227	Figure 3 (c) plots the training and test curves of the English PCFG-LA parser with varying amounts of labeled training data, with and without self-training." ></td>
	<td class="line x" title="188:227	This figure differs substantially from Figure 3 (a)." ></td>
	<td class="line x" title="189:227	First, as mentioned earlier, the PCFG-LA parser benefits much more from selftraining than Charniaks parser with moderate to large amounts of labeled training data." ></td>
	<td class="line x" title="190:227	Second, in contrast to Charniaks parser for which training accuracy degrades consistently as the amount of labeled training data increases, the training accuracy of the PCFG-LA parser sometimes improves when trained on more labeled training data (e.g., the best model (at order 6) trained on 40%15 labeled train14Although the 20% self-trained grammar has a higher test accuracy at the 7-th round than the 6-th round, the development accuracy was better at the 6-th round, and thus we report the test accuracy of the 6-th round grammar in Figure 3 (c)." ></td>
	<td class="line x" title="191:227	15For models trained with greater amounts of labeled training data, although their training accuracy becomes lower (due to greater diversity) for the grammars (all at order 6) selected by the development set, their 7-th order grammars (not reported in the figure) actually have both higher training and test accuracies than the 6-th order grammar trained on less training data." ></td>
	<td class="line x" title="192:227	ing data alone has a higher training accuracy than the best model (at order 5) trained on 20% labeled training data)." ></td>
	<td class="line x" title="193:227	Third, the addition of self-labeled data supports more accurate PCFG-LA grammars with higher orders than those trained without selftraining, as evidenced by scores on both the training and test data." ></td>
	<td class="line x" title="194:227	This suggests that the selftrained grammars are able to utilize more latent annotations to learn deeper dependencies." ></td>
	<td class="line x" title="195:227	2 4  6 8  10 12  14 16  18 20  22  0  2  4  6  8  10  12  14  16 0  1.2  2.4  3.6  4.8  6 Relative reduction of F error (%) Number of brackets Span Length x 1e+4 +Labeled +Unlabled #Brackets Figure 5: The relative reduction of bracketing errors for different span lengths, evaluated on the test set." ></td>
	<td class="line x" title="196:227	The baseline model is the PCFG-LA parser trained on 20% of the WSJ training data." ></td>
	<td class="line x" title="197:227	The +Unlabeled curve corresponds to the parser trained with the additional automatically labeled data and the +Labeled curve corresponds to the parser trained with additional 20% labeled training data." ></td>
	<td class="line x" title="198:227	The counts of the brackets are computed on the gold reference." ></td>
	<td class="line x" title="199:227	Span length 0 is designated for the effect on preterminal POS tags to differentiate it from the non-terminal brackets spanning only one word." ></td>
	<td class="line x" title="200:227	Figure 5 compares the effect of additional treebank labeled and automatically labeled data on the relative reduction of bracketing errors for different span lengths." ></td>
	<td class="line x" title="201:227	It is clear from the figure that the improvement in parsing accuracy from self-training is the result of better bracketing across all span lengths16." ></td>
	<td class="line x" title="202:227	However, even though the automatically labeled training data provides more improvement than the additional treebank labeled data in terms of parsing accuracy, this data is less effective at improving tagging accuracy than the additional treebank labeled training data." ></td>
	<td class="line x" title="203:227	So, how could self-training improve rule estimation when training the PCFG-LA parser with more latent annotations?" ></td>
	<td class="line x" title="204:227	One possibility is that the automatically labeled data smooths the parameter 16There is a slight degradation in bracketing accuracy for some spans longer than 16 words, but the effect is negligible due to their low counts." ></td>
	<td class="line x" title="205:227	839 estimates in the EM algorithm, enabling effective training of models with more parameters to learn deeper dependencies." ></td>
	<td class="line x" title="206:227	Let p(a  b|e,t) be the posterior probability of expanding subcategoriesa to b given the event e, which is a rule expansion on a treebank parse tree t. Tl and Tu are the sets of gold and automatically labeled parse trees, respectively." ></td>
	<td class="line x" title="207:227	The update of the rule expansion probability p(ab) in self-training (with weighting parameter ) can be expressed as: P tTl P et p(ab|e,t) + P tTu P et p(ab|e,t) P b (P tTl P et p(ab|e,t) + P tTu P et p(ab|e,t)) Since the unlabeled data is parsed by a lower order grammar (with fewer latent annotations), the expected counts from the automatically labeled data can be thought of as counts from a lower-order grammar17 that smooth the higherorder (with more latent annotations) grammar." ></td>
	<td class="line x" title="208:227	We observe that many of the rule parameters of the grammar trained on WSJ training data alone have zero probabilities (rules with extremely low probabilities are also filtered to zero), as was also pointed out in (Petrov et al., 2006)." ></td>
	<td class="line x" title="209:227	On the one hand, this is what we want because the grammar should learn to avoid impossible rule expansions." ></td>
	<td class="line x" title="210:227	On the other hand, this might also be a sign of over-fitting of the labeled training data." ></td>
	<td class="line x" title="211:227	As shown in Figure 3 (b), the grammar obtained with the addition of automatically labeled data contains many more non-zero rules, and its performance continues to improve with more latent annotations." ></td>
	<td class="line x" title="212:227	Similar patterns also appear when using self-training for other amounts of labeled training data." ></td>
	<td class="line x" title="213:227	As is partially reflected by the zero probability rules, the addition of the automatically labeled data enables the exploration of a broader parameter space with less danger of over-fitting the data." ></td>
	<td class="line x" title="214:227	Also note that the benefit of the automatically labeled data is less clear in the early training stages (i.e., when there are fewer latent annotations), as can be seen in Figure 3 (b)." ></td>
	<td class="line x" title="215:227	This is probably because there is a small number of free parameters and the treebank data is sufficiently large for robust parameter estimation." ></td>
	<td class="line x" title="216:227	17We also trained models using only the automatically labeled data without combining it with human-labeled training data, but they were no more accurate than those trained on the human-labeled training data alone without self-training." ></td>
	<td class="line x" title="217:227	7 Conclusion In this paper, we showed that PCFG-LA parsers can be more effectively applied to languages where parsing is less well developed and that they are able to benefit more from self-training than lexicalized generative parsers." ></td>
	<td class="line x" title="218:227	We show for the first time that self-training is able to significantly improve the performance of a PCFG-LA parser, a single generative parser, on both small and large amounts of labeled training data." ></td>
	<td class="line x" title="219:227	We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting." ></td>
	<td class="line x" title="220:227	Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches (Charniak and Johnson, 2005; Huang, 2008) for self training." ></td>
	<td class="line x" title="221:227	Self-training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case." ></td>
	<td class="line x" title="222:227	In future work, we plan to scale up the training process with more unlabeled training data (e.g., gigaword) and investigate automatic selection of materials that are most suitable for self-training." ></td>
	<td class="line x" title="223:227	We also plan to investigate domain adaptation and apply the model to other languages with modest treebank resources." ></td>
	<td class="line x" title="224:227	Finally, it is also important to explore other ways to exploit the use of unlabeled data." ></td>
	<td class="line x" title="225:227	Acknowledgments This material is based upon work supported in part by the Defense Advanced Research Projects Agency (DARPA) under Contract No." ></td>
	<td class="line x" title="226:227	HR001106-C-0023 and NSF IIS-0703859." ></td>
	<td class="line x" title="227:227	Any opinions, findings and/or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the funding agencies or the institutions where the work was completed." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1160
Polynomial to Linear: Efficient Classification with Conjunctive Features
Yoshinaga, Naoki;Kitsuregawa, Masaru;"></td>
	<td class="line x" title="1:244	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 15421551, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:244	c 2009 ACL and AFNLP Polynomial to Linear: Efficient Classification with Conjunctive Features Naoki Yoshinaga Institute of Industrial Science University of Tokyo 4-6-1 Komaba, Meguro-ku, Tokyo ynaga@tkl.iis.u-tokyo.ac.jp Masaru Kitsuregawa Institute of Industrial Science University of Tokyo 4-6-1 Komaba, Meguro-ku, Tokyo kitsure@tkl.iis.u-tokyo.ac.jp Abstract This paper proposes a method that speeds up a classifier trained with many conjunctive features: combinations of (primitive) features." ></td>
	<td class="line x" title="3:244	The key idea is to precompute as partial results the weights of primitive feature vectors that appear frequently in the target NLP task." ></td>
	<td class="line x" title="4:244	A trie compactly stores the primitive feature vectors with their weights, and it enables the classifier to find for a given feature vector its longest prefix feature vector whose weight has already been computed." ></td>
	<td class="line x" title="5:244	Experimental results for a Japanese dependency parsing task show that our method speeded up the SVM and LLM classifiers of the parsers, which achieved accuracy of 90.84/90.71%, by a factor of 10.7/11.6." ></td>
	<td class="line oc" title="6:244	1 Introduction Deep and accurate text analysis based on discriminative models is not yet efficient enough as a component of real-time applications, and it is inadequate to process Web-scale corpora for knowledge acquisition (Pantel, 2007; Saeger et al., 2009) or semi-supervised learning (McClosky et al., 2006; Spoustov et al., 2009)." ></td>
	<td class="line x" title="7:244	One of the main reasons for this inefficiency is attributed to the inefficiency of core classifiers trained with many feature combinations(e.g., wordn-grams)." ></td>
	<td class="line x" title="8:244	Hereafter, werefer to features that explicitly represent combinations of features as conjunctive features and the other atomic features as primitive features." ></td>
	<td class="line pc" title="9:244	The feature combinations play an essential role in obtaining a classifier with state-of-the-art accuracy for several NLP tasks; recent examples include dependency parsing (Koo et al., 2008), parse re-ranking (McClosky et al., 2006), pronoun resolution (Nguyen and Kim, 2008), and semantic role labeling (Liu and Sarkar, 2007)." ></td>
	<td class="line x" title="10:244	However, explicit feature combinations significantly increase the feature space, which slows down not only training but also testing of the classifier." ></td>
	<td class="line x" title="11:244	Kernel-based methods such as support vector machines (SVMs) consider feature combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995)." ></td>
	<td class="line x" title="12:244	The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Goldberg and Elhadad, 2008)." ></td>
	<td class="line x" title="13:244	lscript1-regularized log-linear models (lscript1-LLMs), on the other hand, provide sparse solutions, in which weights of irrelevant features are exactly zero, by assumingaLaplacianpriorontheweights(Tibshirani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; Gao et al., 2007)." ></td>
	<td class="line x" title="14:244	However, as Kazama and Tsujii (2005) have reported in a text categorization task and we later confirm in a dependency parsing task, when most features regarded as irrelevant during training lscript1-LLMs appear rarely in the task, we cannot greatly reduce the number of active features in each classification." ></td>
	<td class="line x" title="15:244	In the end, when efficiency is a major concern, we must use exhaustive feature selection (Wu et al., 2007; Okanohara and Tsujii, 2009) or even restrict the order of conjunctive features at the expense of accuracy." ></td>
	<td class="line x" title="16:244	In this study, we provide a simple, but effective solution to the inefficiency of classifiers trained with higher-order conjunctive features (or polynomial kernel), by exploiting the Zipfian nature of language data." ></td>
	<td class="line x" title="17:244	The key idea is to precompute the weights of primitive feature vectors and use them as partial results to compute the weight of a given feature vector." ></td>
	<td class="line x" title="18:244	We use a trie called the feature sequence trie to efficiently find for a given feature vector its longest prefix feature vector whose weight has been computed." ></td>
	<td class="line x" title="19:244	The trie is built from featurevectorsgeneratedbyapplyingtheclassifier to actual data in the classification task." ></td>
	<td class="line x" title="20:244	The time complexity of the classifier approaches time that 1542 is linear with respect to the number of primitive features when the retrieved feature vector covers most of the features in the input feature vector." ></td>
	<td class="line x" title="21:244	We implemented our algorithm for SVM and LLM classifiers and evaluated the performance of the resulting classifiers in a Japanese dependency parsing task." ></td>
	<td class="line x" title="22:244	Experimental results show that it successfully speeded up classifiers trained with higher-orderconjunctivefeaturesbyafactorof10." ></td>
	<td class="line x" title="23:244	The rest of this paper is organized as follows." ></td>
	<td class="line x" title="24:244	Section 2 introduces LLMs and SVMs." ></td>
	<td class="line x" title="25:244	Section 3 proposes our classification algorithm." ></td>
	<td class="line x" title="26:244	Section 4 presents experimental results." ></td>
	<td class="line x" title="27:244	Section 5 concludes with a summary and addresses future directions." ></td>
	<td class="line x" title="28:244	2 Preliminaries Inthispaper, wefocusonlinearclassifiersthatcalculate the probability (or score) by summing up weights of individual features." ></td>
	<td class="line x" title="29:244	Examples include not only log-linear models but also support vector machines with kernel expansion (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003)." ></td>
	<td class="line x" title="30:244	Below, we introduce these two classifiers and their ways to consider feature combinations." ></td>
	<td class="line x" title="31:244	In classification-based NLP, the target task is modeled as one or more classification steps." ></td>
	<td class="line x" title="32:244	For example in part-of-speech (POS) tagging, each classification decides whether to assign a particular label (POS tag) to a given sample (each word in a given sentence)." ></td>
	<td class="line x" title="33:244	Each sample is then represented by a feature vector x, whose element xi is a value of a feature function fiF. Here, we assume a binary feature function fi(x) {0,1}, in which a non-zero value means that particular context data appears in the sample." ></td>
	<td class="line x" title="34:244	We say that a featurefi is active in samplexwhen xi = fi(x) = 1 and|x|represents the number of active features inx(|x|=|{fi|fi(x) = 1}|)." ></td>
	<td class="line x" title="35:244	2.1 Log-Linear Models The log-linear model (LLM), or also known as maximum-entropy model (Berger et al., 1996), is a linear classifier widely used in the NLP literature." ></td>
	<td class="line x" title="36:244	Let the training data of LLMs be {xi,yi}Li=1, where xi {0,1}n is a feature vector and yi is a class label associated withxi." ></td>
	<td class="line x" title="37:244	We assume a binary label yi{1}here to simplify the argument." ></td>
	<td class="line x" title="38:244	The classifier provides conditional probability p(y|x) for a given feature vectorxand a label y: p(y|x) = 1Z(x) exp summationdisplay i wi,yfi,y(x,y), (1) where fi,y(x,y) is a feature function that returns a non-zero value when fi(x) = 1 and the label is y, wi,y  R is a weight associated with fi,y, and Z(x) = summationtexty expsummationtexti wi,yfi,y(x,y) is the partition function." ></td>
	<td class="line x" title="39:244	We can consider feature combinations in LLMs by explicitly introducing a new conjunctive feature fFprime,y(x,y) that is activated when a particular set of featuresFprimeFto be combined is activated (namely, fFprime,y(x,y) = logicalandtextfi,yFprime fi,y(x,y))." ></td>
	<td class="line x" title="40:244	We then introduce an lscript1-regularized LLM (lscript1LLM), in which the weight vector w is tuned so as to maximize the logarithm of the a posteriori probability of the training data: L(w) = Lsummationdisplay i=1 logp(yi|xi)Cbardblwbardbl1." ></td>
	<td class="line x" title="41:244	(2) Hyper-parameter C thereby controls the degree of over-fitting (solution sparseness)." ></td>
	<td class="line x" title="42:244	Interested readers may refer to the cited literature (Andrew and Gao, 2007) for the optimization procedures." ></td>
	<td class="line x" title="43:244	2.2 Support Vector Machines A support vector machine (SVM) is a binary classifier (Cortes and Vapnik, 1995)." ></td>
	<td class="line x" title="44:244	Training with samples {xi,yi}Li=1 where xi  {0,1}n and yi{1}yields the following decision function: y(x) = sgn(g(x) +b) g(x) = summationdisplay xjSV yjj(xj)T(x), (3) where b  R,  : Rn mapsto RH and support vectors xj SV (subset of training samples), each of which is associated with weight j  R. We hereafter call g(x) the weight function." ></td>
	<td class="line x" title="45:244	Nonlinear mapping function  is chosen to make the training samples linearly separable in RH space." ></td>
	<td class="line x" title="46:244	Kernel function k(xj,x) = (xj)T(x) is then introduced to compute the dot product in RH space without mappingxto (x)." ></td>
	<td class="line x" title="47:244	To consider combinations of primitive features fj F, we use a polynomial kernel kd(xj,x) = (xTj x + 1)d. From Eq." ></td>
	<td class="line x" title="48:244	3, we obtain the weight function for the polynomial kernel as: g(x) = summationdisplay xjSV yjj(xTj x+ 1)d." ></td>
	<td class="line x" title="49:244	(4) Since we assumed that xi is a binary value representing whether a (primitive) feature fi is active in the sample, the polynomial kernel of degree d implies a mapping d from x to d(x) that has 1543 H = summationtextdk=0parenleftbignkparenrightbigdimensions." ></td>
	<td class="line x" title="50:244	Each dimension represents a (weighted) conjunction of d features in the original samplex.1 Kernel Expansion (SVM-KE) The time complexity of Eq." ></td>
	<td class="line x" title="51:244	4 is O(|x||SV|)." ></td>
	<td class="line x" title="52:244	This cost is usually high for classifiers used in NLP tasks because they often have many support vectors (|SV| > 10,000)." ></td>
	<td class="line x" title="53:244	Kernel expansion (KE) was proposed by Isozaki and Kazawa (2002) to convert Eq." ></td>
	<td class="line x" title="54:244	4 into the linear sum of the weights in the mapped feature space as in LLM (p(y|x) in Eq." ></td>
	<td class="line x" title="55:244	1): g(x) = wTxd = summationdisplay i wixdi, (5) wherexd is a binary feature vector whose element xdi has a non-zero value when (d(x))i > 0, w is the weight vector for xd in the expanded feature spaceFd and is precalculated from the support vectors xj and their weights j. Interested readers may refer to Kudo and Matsumoto (2003) for the detailed computation for obtainingw." ></td>
	<td class="line x" title="56:244	The time complexity of Eq." ></td>
	<td class="line x" title="57:244	5 (and Eq." ></td>
	<td class="line x" title="58:244	1) is O(|xd|), which is linear with respect to the number of active features in xd within the expanded feature spaceFd." ></td>
	<td class="line x" title="59:244	Heuristic Kernel Expansion (SVM-HKE) To make the weight vector sparse, Kudo and Matsumoto (2003) proposed a heuristic method that filters out less useful features whose absolute weight values are less than a pre-defined threshold .2 They reported that increased threshold value  resulted in a dramatically sparse feature spaceFd, which had the side-effects of accuracy degradation and classifier speed-up." ></td>
	<td class="line x" title="60:244	3 Proposed Method In this section, we propose a method that speeds up a classifier trained with many conjunctive features." ></td>
	<td class="line x" title="61:244	Below, we focus on a kernel-based classifier trained with a polynomial kernel of degreed(here, 1For example, given an input vector x = (x1,x2)T and a support vector xprime = (xprime1,xprime2)T, the 2nd-order polynomial kernel returns k2(xprime,x) = (xprime1x1 + xprime2x2 + 1)2 = 3xprime1x1 + 3xprime2x2 + 2xprime1x1xprime2x2 + 1 ( xprimei,xi  {0,1})." ></td>
	<td class="line x" title="62:244	This function thus implies a mapping 2(x) = (1,3x1,3x2,2x1x2)T. In the following argument, we ignore the dimension of the constant in the mapped space and assume constant b is set to include it." ></td>
	<td class="line x" title="63:244	2Precisely speaking, they set different thresholds to positive (j > 0) and negative (j < 0) support vectors, considering the proportion of positive and negative support vectors." ></td>
	<td class="line x" title="64:244	Figure 1: Efficient computation of g(x)." ></td>
	<td class="line x" title="65:244	SVMs), but an analogous argument is possible for linear classifiers (e.g., LLMs).3 We hereafter represent a binary feature vectorx as a set of active features{fi|fi(x) = 1}." ></td>
	<td class="line x" title="66:244	x can thereby be represented as an element of the power set 2F of the set of featuresF." ></td>
	<td class="line x" title="67:244	3.1 Idea Let us remember that weight function g(x) in Eq." ></td>
	<td class="line x" title="68:244	5 maps x 2F to W R. If we could calculate Wx = g(x) for all possible x in advance, we could obtain g(x) by simply checking|x|elements, namely, in O(|x|) time." ></td>
	<td class="line x" title="69:244	However, because |{x|x2F}|= 2|F| and|F|is likely to be very large (often|F|> 10,000 in NLP tasks), this calculation is impractical." ></td>
	<td class="line x" title="70:244	We then compute and store weight Wxprime = g(xprime) for xprime  Vc( 2F), a certain subset of the possible value space, and compute g(x) for x / Vc by using precalculated weight Wxc for xc4xin the following way: g(x) = Wxc + summationdisplay fixdxdc wi." ></td>
	<td class="line x" title="71:244	(6) Intuitively speaking, starting from partial weight Wxc, we add up remaining weights of primitive features f F that are not active in xc but active in x and conjunctive features that combine f and the other active features inx." ></td>
	<td class="line x" title="72:244	An example of this computation (d = 2) is depicted in Figure 1." ></td>
	<td class="line x" title="73:244	We can efficiently compute g(x) for a vector x that has four active features f1, f2, f3, and f4 (and x2 has their six conjunctive features) using precalculated weight W{1,2,3}; we should first check the three features f1, f2, and f3 to retrieve W{1,2,3} and next check the remaining four features related to f4, namely f4, f1,4, f2,4, and f3,4, in order to add up the remaining 3When a feature vector x includes (explicit) conjunctive features f  Fd, we assume weight function gprime(y|xprime) = g(y|x), where xprime is a projection of x (by 1d : Fd  F)." ></td>
	<td class="line x" title="74:244	4This means that all active features in xc are active in x. 1544 weights, while the normal computation in Eq." ></td>
	<td class="line x" title="75:244	5 should check the four primitive and six conjunctive features to get the individual weights." ></td>
	<td class="line x" title="76:244	Expected time complexity Counting the number of features to be checked in the computation, we obtain the time complexity f(x,d) of Eq." ></td>
	<td class="line x" title="77:244	6 as: f(x,d) = O(|xc|+|xd||xdc|), (7) where |xd|= dsummationdisplay k=1 parenleftbigg|x| k parenrightbigg (8) (e.g.,|x2|= |x|2+|x|2 and|x3|= |x|3+5|x|6 ).5 Note that when |xc| becomes close to |x|, this time complexity actually approaches O(|x|)." ></td>
	<td class="line x" title="78:244	Thus, to minimize this computational cost, xc is to be chosen fromVc as follows: xc = argmin xprimeVc,xprimex (|xprime|+|xd||xprimed|)." ></td>
	<td class="line x" title="79:244	(9) 3.2 Construction of Feature Sequence Trie There are two issues with speeding up the classifierbythecomputationshowninEq.6." ></td>
	<td class="line x" title="80:244	First, since we can store weights for only a small fraction of possible feature vectors (namely,|Vc|lessmuch2|F|), we should chooseVc so as to maximize its impact on the speed-up." ></td>
	<td class="line x" title="81:244	Second, we should quickly find an optimalxc fromVc for a given feature vectorx." ></td>
	<td class="line x" title="82:244	The solution to the first problem is to enumerate partial feature vectors that frequently appear in the target task." ></td>
	<td class="line x" title="83:244	Note that typical linguistic features used in NLP tasks usually consist of disjunctive sets of features (e.g., word surface and POS), in which each set is likely to follow Zipfs law (Zipf, 1949) and correlate with each other." ></td>
	<td class="line x" title="84:244	We can expect the distribution of feature vectors, the mixture of Zipf distributions, to be Zipfian." ></td>
	<td class="line x" title="85:244	This has been confirmed for word n-grams (Egghe, 2000) and itemset support distribution (Chuang et al., 2008)." ></td>
	<td class="line x" title="86:244	We can thereby expect that a small set of partial feature vectors commonly appear in the task." ></td>
	<td class="line x" title="87:244	To solve the second problem, we introduce a feature sequence trie (fstrie), which represents a hierarchy of feature vectors, to enable the classifier to efficiently retrieve (sub-)optimal xc (in Eq." ></td>
	<td class="line x" title="88:244	9) for a given feature vector x. We build an fstrie in the following steps: Step 1: Apply the target classifier to actual (raw) data in the task to enumerate possible feature vectors (hereafter, source feature vectors)." ></td>
	<td class="line x" title="89:244	5This is the maximum number of conjunctive features." ></td>
	<td class="line x" title="90:244	Figure 2: Feature sequence trie and completion of prefix feature vector weights." ></td>
	<td class="line x" title="91:244	Step 2: Sort the features in each source feature vector according to their frequency in the training data (in descending order)." ></td>
	<td class="line x" title="92:244	Step 3: Build a trie from the source feature vectorsbyregardingfeatureindicesascharacters and store weights of all prefix feature vectors." ></td>
	<td class="line x" title="93:244	An fstrie built from six source feature vectors is shown in Figure 2." ></td>
	<td class="line x" title="94:244	In fstries, a path from the root to another node represents a feature vector." ></td>
	<td class="line x" title="95:244	An important point here is that the fstrie stores the weights of all prefix feature vectors of the source feature vectors, and the trie structure enables us to retrieve for a given feature vector x the weight of its longest prefix vector xc x in O(|xc|) time." ></td>
	<td class="line x" title="96:244	To handle feature functions in LLMs (Eq." ></td>
	<td class="line x" title="97:244	1), we store partial weight Wxc,y = summationtexti wi,yfi,y(xc,y) for each label y on the node that expressesxc." ></td>
	<td class="line x" title="98:244	Since we sort the features in the source feature vectors according to their frequency, the prefix feature vectors exclude less frequent features in the source feature vectors." ></td>
	<td class="line x" title="99:244	Lexical features or finer-grained features (e.g., POS-subcategory) are usually less frequent than coarse-grained features (e.g., POS), so they lie in the latter part of the feature vectors." ></td>
	<td class="line x" title="100:244	This sorting helps us to retrieve longer feature vector xc for input feature vector x that will have diverse infrequent features." ></td>
	<td class="line x" title="101:244	It also minimizes the size of fstrie by sharing the common frequent prefix (e.g.,{f1,f2}in Figure 2)." ></td>
	<td class="line x" title="102:244	Pruning nodes from fstrie We have so far described the way to construct an fstrie from the source feature vectors." ></td>
	<td class="line x" title="103:244	However, a naive enumeration of source feature vectors will result in the explosion of the fstrie size, and we want to have a principled way to control the fstrie size rather than reducing the processed data size." ></td>
	<td class="line x" title="104:244	Below, we present a method that prunes useless prefix feature vectors (nodes) from the constructed fstrie to maximize its impact on the classifier efficiency." ></td>
	<td class="line x" title="105:244	1545 Algorithm 1 PRUNE NODES FROM FSTRIE Input: fstrie T, node_limit N N Output: fstrie T 1: while # of nodes in T > N do 2: xc argmin xprimeleaf(T) u(xprime) 3: removexc, T 4: end while 5: return T We adopt a greedy strategy that iteratively prunes a leaf node (one prefix feature vector and its weight) from the fstrie built from all the source feature vectors, according to a certain utility score calculated for each node." ></td>
	<td class="line x" title="106:244	In this study, we consider two metrics for each prefix feature vectorxc to calculate its utility score." ></td>
	<td class="line x" title="107:244	Probability p(xc), which denotes how often the stored weight Wxc will be used in the target task." ></td>
	<td class="line x" title="108:244	The maximum-likelihood estimation provides probability: p(xc) = summationtext xprimexc nxprimesummationtext xnx , (10) where nx  N is the frequency count of a source feature vectorxin the processed data." ></td>
	<td class="line x" title="109:244	Computation reduction d(xc), which denotes how much computation is reduced by Wxc to calculate a weight ofxxc." ></td>
	<td class="line x" title="110:244	This can be estimated by counting the number of conjunctive features we additionally have to check when we remove xc." ></td>
	<td class="line x" title="111:244	Since the fstrie stores the weight of a prefix feature vectorxc-xc such that|xc-|=|xc|1 (e.g., in Figure 2, xc= {f1,f2} for xc = {f1,f2,f4}), we can define the computation reduction as: d(xc) = (|xdc||xdc-|)(|xc||xc-|) = dsummationdisplay k=2 parenleftbigg|x c| k parenrightbigg  dsummationdisplay k=2 parenleftbigg|x c|1 k parenrightbigg ( Eq." ></td>
	<td class="line x" title="112:244	8)." ></td>
	<td class="line x" title="113:244	2(xc) =|xc|1 and 3(xc) = |xc|2|xc|2 . We calculate utility score of each nodexc in the fstrie as u(xc) = p(xc)d(xc), which means the expected computation reduction by xc in the target task, and prune the lowest-utility-score leaf nodes from the fstrie one by one (Algorithm 1)." ></td>
	<td class="line x" title="114:244	If several prefix vectors have the same utility score, we eliminate them in numerical descending order." ></td>
	<td class="line x" title="115:244	Algorithm 2 COMPUTE WEIGHT WITH FSTRIE Input: fstrie T, weight vectorwR|Fd| feature vectorx2F Output: weight W = g(x)R 1: xsort(x) 2: xc,Wxcprefix_search(T,x) 3: W Wxc 4: for all feature fj xdxdc do 5: W W +wj 6: end for 7: return W 3.3 Classification Algorithm Our classification algorithm is shown in detail in Algorithm 2." ></td>
	<td class="line x" title="116:244	The classifier first sorts the active featuresininputfeaturevectorxaccordingtotheir frequency in the training data." ></td>
	<td class="line x" title="117:244	Then, for x, it retrieves the longest common prefix vector xc from the fstrie (line 2 in Algorithm 2)." ></td>
	<td class="line x" title="118:244	It then adds the weights of the remaining features to partial weight Wxc (line 5 in Algorithm 2)." ></td>
	<td class="line x" title="119:244	Note that the remaining features whose weights we sum up (line 4 in Algorithm 2) are primitive and conjunctive features that relate to f xxc, which appear less frequently than fprime xc in the training data." ></td>
	<td class="line x" title="120:244	Thus, when we apply our algorithm to classifiers with the sparse solution (e.g., SVMHKEsorlscript1-LLMs),|xd||xdc|canbemuchsmaller than the theoretical expectation (Eq." ></td>
	<td class="line x" title="121:244	8)." ></td>
	<td class="line x" title="122:244	We confirmed this in the following experiments." ></td>
	<td class="line x" title="123:244	4 Evaluation We applied our algorithm to SVM-KE, SVM-HKE, and lscript1-LLM classifiers and evaluated the resulting classifiers in a Japanese dependency parsing task." ></td>
	<td class="line x" title="124:244	Tothebestofourknowledge, therearenoprevious reports of an exact weight calculation faster than linear summation (Eqs." ></td>
	<td class="line x" title="125:244	1 and 5)." ></td>
	<td class="line x" title="126:244	We also compared our SVM classifier with a classifier called polynomial kernel inverted (PKI: Kudo and Matsumoto (2003)), which uses the polynomial kernel (Eq." ></td>
	<td class="line x" title="127:244	4) and inverted indexing to support vectors." ></td>
	<td class="line x" title="128:244	4.1 Experimental Settings A Japanese dependency parser inputs bunsetsusegmented sentences and outputs the correct head (bunsetsu) for each bunsetsu; here, a bunsetsu is a grammatical unit in Japanese consisting of one or more content words followed by zero or more function words." ></td>
	<td class="line x" title="129:244	A parser generates a feature vec1546 Modifier, modifiee bunsetsu head word (surface-form, POS, POS-subcategory, inflection form), functional word (surface-form, POS, POS-subcategory, inflection form), brackets, quotation marks, punctuation marks, position in sentence (beginning, end) Between bunsetsus distance (1, 25, 6), case-particles, brackets, quotation marks, punctuation marks Table 1: Feature set used for experiments." ></td>
	<td class="line x" title="130:244	tor for a particular pair of bunsetsus (modifier and modifiee candidates) by exploiting the head-final and projective (Nivre, 2003) nature of dependency relations in Japanese." ></td>
	<td class="line x" title="131:244	The classifier then outputs label y = +1 (dependent) or 1 (independent)." ></td>
	<td class="line x" title="132:244	Since our classifier is independent of individual parsing algorithms, we targeted speeding up (a classifier in) the shift-reduce parser proposed by Sassano (2004), which has been reported to be the most efficient for this task, with almost stateof-the-art accuracy (Iwatate et al., 2008)." ></td>
	<td class="line x" title="133:244	This parser decreases the number of classification steps by using the fact that a bunsetsu is likely to modify a bunsetsu close to itself." ></td>
	<td class="line x" title="134:244	Due to space limitations, we omit the details of the parsing algorithm." ></td>
	<td class="line x" title="135:244	We used the standard feature set tailored for this task (Kudo and Matsumoto, 2002; Sassano, 2004; Iwatate et al., 2008) (Table 1)." ></td>
	<td class="line x" title="136:244	Note that features listed in the Between bunsetsus row represent contexts between the target pair of bunsetsus and appear independently from other features, which will become an obstacle to finding the longest prefix vector." ></td>
	<td class="line x" title="137:244	This task is therefore a better measure ofourmethodthansimplesequentiallabelingsuch as POS tagging or named-entity recognition." ></td>
	<td class="line x" title="138:244	For evaluation, we used Kyoto Text Corpus Version 4.0 (Kurohashi and Nagao, 2003), Mainichi news articles in 1995 that have been manually annotated with dependency relations.6 The training, development, and test sets included 24,283, 4833, and 9284 sentences, and 234,685, 47,571, and 89,874 bunsetsus, respectively." ></td>
	<td class="line x" title="139:244	The training samples generated from the training set included 150,064 positive and 146,712 negative samples." ></td>
	<td class="line x" title="140:244	The following experiments were performed on a server with an Intel R XeonTM 3.20-GHz CPU." ></td>
	<td class="line x" title="141:244	We used TinySVM7 and a simple C++ library for maximum entropy classification8 to train SVMs and lscript1-LLMs, respectively." ></td>
	<td class="line x" title="142:244	We used Darts-Clone,9 6http://nlp.kuee.kyoto-u.ac.jp/nl-resource/corpus-e.html 7http://chasen.org/taku/software/TinySVM/ 8http://www-tsujii.is.s.u-tokyo.ac.jp/tsuruoka/maxent/ 9http://code.google.com/p/darts-clone/ Model type Model statistics Dep." ></td>
	<td class="line x" title="143:244	Sent." ></td>
	<td class="line x" title="144:244	Model d  /  |Fd| |xd| acc." ></td>
	<td class="line x" title="145:244	acc." ></td>
	<td class="line x" title="146:244	SVM-KE 1 0 39712 27.3 88.29 46.49 SVM-KE 2 0 1478109 380.6 90.76 53.83 SVM-KE 3 0 26194354 3286.7 90.93greatermuch54.43greatermuch SVM-HKE 3 0.001 13247675 2725.9 90.92greatermuch54.39greatermuch SVM-HKE 3 0.002 2514385 2238.1 90.91greatermuch54.32> SVM-HKE 3 0.003 793195 1855.4 90.83 54.21 SVM-KE 4 0 293416102 20395.4 90.91greatermuch54.69greatermuch SVM-HKE 4 0.0002 96522236 15282.1 90.93greatermuch54.53> SVM-HKE 4 0.0004 19245076 11565.0 90.96greatermuch54.64greatermuch SVM-HKE 4 0.0006 7277592 8958.2 90.84 54.48> lscript1-LLM 1 1.0 9268 26.5 88.22 46.06 lscript1-LLM 2 2.0 32575 309.8 90.62 53.46 lscript1-LLM 3 3.0 129503 2088.3 90.71 54.09> lscript1-LLM 3 4.0 85419 1803.0 90.61 53.79 lscript1-LLM 3 5.0 63046 1699.5 90.59 53.55 Table 2: Specifications of LLMs and SVMs." ></td>
	<td class="line x" title="147:244	The accuracy marked with greatermuch or > was significantly better than the d = 2 counterpart (p < 0.01 or 0.01p < 0.05 by McNemars test)." ></td>
	<td class="line x" title="148:244	a double-array trie (Aoe, 1989; Yata et al., 2008), as a compact trie implementation." ></td>
	<td class="line x" title="149:244	All these libraries and algorithms are implemented in C++." ></td>
	<td class="line x" title="150:244	The code for building fstries occupies 100 lines, while the code for the classifier occupies 20 lines (except those for kernel expansion)." ></td>
	<td class="line x" title="151:244	4.2 Results Specifications of SVMs and LLMs used here are shown in Table 2;|Fd|is the number of active features, while|xd|is the average number of active features in each classification for the test corpus." ></td>
	<td class="line x" title="152:244	Dependency accuracy is the ratio of dependency relations correctly identified by the parser, while sentence accuracy is the exact match accuracy of complete dependency relations in a sentence." ></td>
	<td class="line x" title="153:244	For LLM training, we designed explicit conjunctive features for all the d or lower-order feature combinations to make the results comparable to those of SVMs." ></td>
	<td class="line x" title="154:244	We could not train d = 4 LLMs due to parameter explosion." ></td>
	<td class="line x" title="155:244	We varied SVM soft margin parametercfrom 0.1 to 0.000001 and LLM width factor parameter,10 which controls the impact of the prior, from 1.0 to 5.0, and adjusted the values to maximize dependency accuracy for the development set: (d,c) = (1,0.1), (2,0.005), (3,0.0001), (4,0.000005) for SVMs and (d,) = (1,1.0), (2,2.0), (3,4.0) for lscript1-LLMs." ></td>
	<td class="line x" title="156:244	The accuracy of around 90.9% (SVM-KE, d = 3,4) is close to the performance of state-of-the10The parameter C of lscript1-LLM in Eq." ></td>
	<td class="line x" title="157:244	2 was set to /L (referred to in Kazama and Tsujii (2003) as single width)." ></td>
	<td class="line x" title="158:244	1547 Model PKI Baseline Proposed w/ fstrieS Proposed w/ fstrieM Proposed w/ fstrieL Speed type d classify Mem." ></td>
	<td class="line x" title="159:244	Time [ms/sent.]" ></td>
	<td class="line x" title="160:244	Mem." ></td>
	<td class="line x" title="161:244	Time [ms/sent.]" ></td>
	<td class="line x" title="162:244	Mem." ></td>
	<td class="line x" title="163:244	Time [ms/sent.]" ></td>
	<td class="line x" title="164:244	Mem." ></td>
	<td class="line x" title="165:244	Time [ms/sent.]" ></td>
	<td class="line x" title="166:244	up [ms/sent.]" ></td>
	<td class="line x" title="167:244	(MB) classify (total) (MB) classify (total) (MB) classify (total) (MB) classify (total) SVM-KE 1 13.480 0.2 0.003 (0.015) +0.6 0.006 (0.018) +20.2 0.007 (0.018) +662.9 0.016 (0.029) NA SVM-KE 2 10.313 13.5 0.041 (0.054) +0.5 0.020 (0.032) +18.0 0.021 (0.034) +662.4 0.023 (0.036) 2.1 SVM-KE 3 10.945 142.2 0.345 (0.361) +0.5 0.163 (0.178) +18.2 0.108 (0.123) +667.0 0.079 (0.093) 4.4 SVM-KE 4 12.603 648.0 2.338 (2.363) +0.5 1.156 (1.178) +18.6 0.671 (0.690) +675.9 0.415 (0.432) 5.6 Table 3: Parsing results for test corpus: SVM-KE classifiers with dense feature space." ></td>
	<td class="line x" title="168:244	art parsers (Iwatate et al., 2008), and the model statistics are considered to be complex (or realistic) enough to evaluate our classifiers utility." ></td>
	<td class="line x" title="169:244	The number of support vectors of SVMs was 71,7669.2%, which is twice as many as those used by Kudo and Matsumoto (2003) (34,996) in their experiments on the same task." ></td>
	<td class="line x" title="170:244	We could clearly observe that the number of activefeatures|xd|increaseddramaticallyaccording to the order d of feature combinations." ></td>
	<td class="line x" title="171:244	The density of|xd|for SVMs was very high (e.g.,|x3| = 3286.7, close to the maximum shown in Eq." ></td>
	<td class="line x" title="172:244	8: (27.33 + 527.3)/6similarequal3414." ></td>
	<td class="line x" title="173:244	For d  3 models, we attempted to control the size of the feature space |Fd| by changing the models hyper-parameters: threshold  for the SVM-HKE and width factor  for the lscript1-LLM." ></td>
	<td class="line x" title="174:244	Although we successfully reduced the size of the feature space|Fd|, we could not dramatically reduce the average number of active features|xd|in each classification while keeping the accuracy advantage." ></td>
	<td class="line x" title="175:244	This confirms that the solution sparseness does not suffice to obtain an efficient classifier." ></td>
	<td class="line x" title="176:244	We obtained source feature vectors to build fstries by applying parsers with the target classifiers to a raw corpus in the target domain, 3,258,313 sentences of 199194 Mainichi news articles that were morphologically analyzed by JUMAN6 and segmented into bunsetsus by KNP.6 We first built fstrieL using all the source feature vectors." ></td>
	<td class="line x" title="177:244	We then attempted to reduce the number of prefix feature vectors in fstrieL to 1/2n the size by Algorithm 1." ></td>
	<td class="line x" title="178:244	We refer to fstries built from 1/32 and 1/1024 of the prefix feature vectors in fstrieL as fstrieM and fstrieS in the following experiments." ></td>
	<td class="line x" title="179:244	Because we exploited Algorithm 2 to calculate the weights of the prefix feature vectors, it took less than one hour (59 min." ></td>
	<td class="line x" title="180:244	29 sec.)" ></td>
	<td class="line x" title="181:244	on the 3.20-GHz server to build fstrieL (and calculate the utility score for all the nodes in it) for the slowest SVM-KE (d = 4) from the 40,409,190 source feature vectors (62,654,549 prefix feature vectors) generated by parsing the 3,258,313 sentences." ></td>
	<td class="line x" title="182:244	0 0.5 1 1.5 2 2.5 0 100 200 300 400 500 600 700A ve." ></td>
	<td class="line x" title="183:244	clas sific atio ntim e[m s/se nt.]" ></td>
	<td class="line x" title="184:244	Sizeoffstrie[MB] SVM-KE(d=1)SVM-KE(d=2) SVM-KE(d=3)SVM-KE(d=4) Figure 3: Average classification time per sentence plotted against size of fstrie: SVM-KE." ></td>
	<td class="line x" title="185:244	Results for SVM-KE with dense feature space The performances of parsers having SVM-KE classifiers with and without the fstrie are given in Table 3." ></td>
	<td class="line x" title="186:244	The speed-up column shows the speed-up factor of the most efficient classifier (bold) versus the baseline classifier without fstries." ></td>
	<td class="line x" title="187:244	Since each classifier solved a slightly different number of classification steps (112,8530.15%), we show the (average) cumulative classification time for a sentence." ></td>
	<td class="line x" title="188:244	The Mem." ></td>
	<td class="line x" title="189:244	columns show the size of weight vectors for SVM-KE classifiers and the size of fstriesS, fstriesM, and fstriesL, respectively." ></td>
	<td class="line x" title="190:244	The fstries successfully speeded up SVM-KE classifiers with the dense feature space.11 The SVM-KE classifiers without fstries were still faster than PKI, but as expected from a large|xd|value, the classifiers with higher conjunctive features were much slower than the classifier with only primitive features by factors of 13 (d = 2), 109 (d = 3) and 738 (d = 4) and the classification time accounted for most of the parsing time." ></td>
	<td class="line x" title="191:244	The average classification time of our classifiers plotted against fstrie size is shown in Figure 3." ></td>
	<td class="line x" title="192:244	Surprisingly, we obtained a significant speed-up even with tiny fstrie sizes of < 1 MB." ></td>
	<td class="line x" title="193:244	Furthermore, we naively controlled the fstrie size by sim11The inefficiency of the classifier (d = 1) results from the cost of the additional sort function (line 1 in Algorithm 2) and CPU cache failure due to random accesses to the huge fstries." ></td>
	<td class="line x" title="194:244	1548 Model Baseline Proposed w/ fstrieS Proposed w/ fstrieM Proposed w/ fstrieL Speed type d  /  Mem." ></td>
	<td class="line x" title="195:244	Time [ms/sent.]" ></td>
	<td class="line x" title="196:244	Mem." ></td>
	<td class="line x" title="197:244	Time [ms/sent.]" ></td>
	<td class="line x" title="198:244	Mem." ></td>
	<td class="line x" title="199:244	Time [ms/sent.]" ></td>
	<td class="line x" title="200:244	Mem." ></td>
	<td class="line x" title="201:244	Time [ms/sent.]" ></td>
	<td class="line x" title="202:244	up (MB) classify (total) (MB) classify (total) (MB) classify (total) (MB) classify (total) SVM-HKE 3 0.001 64.6 0.348 (0.363) +0.5 0.151 (0.166) +17.6 0.097 (0.111) +638.0 0.070 (0.084) 5.0 SVM-HKE 3 0.002 13.9 0.332 (0.346) +0.5 0.123 (0.137) +17.0 0.074 (0.088) +612.2 0.053 (0.067) 6.2 SVM-HKE 3 0.003 4.2 0.314 (0.328) +0.4 0.102 (0.115) +14.7 0.057 (0.070) +526.2 0.041 (0.054) 7.8 SVM-HKE 4 0.0002 235.0 2.258 (2.280) +0.5 1.022 (1.042) +17.7 0.558 (0.575) +637.1 0.330 (0.346) 6.8 SVM-HKE 4 0.0004 82.8 2.038 (2.058) +0.5 0.816 (0.835) +16.8 0.414 (0.430) +601.7 0.234 (0.249) 8.7 SVM-HKE 4 0.0006 32.2 1.802 (1.820) +0.4 0.646 (0.662) +15.7 0.311 (0.326) +558.9 0.168 (0.183) 10.7 lscript1-LLM 1 1.0 0.1 0.004 (0.016) +0.8 0.006 (0.018) +25.0 0.007 (0.019) +787.7 0.016 (0.029) NA lscript1-LLM 2 2.0 0.4 0.043 (0.055) +0.6 0.016 (0.028) +20.5 0.015 (0.027) +698.0 0.018 (0.030) 2.9 lscript1-LLM 3 3.0 1.0 0.314 (0.326) +0.5 0.091 (0.103) +17.8 0.041 (0.054) +601.0 0.027 (0.040) 11.6 lscript1-LLM 3 4.0 0.7 0.300 (0.313) +0.5 0.082 (0.094) +16.3 0.036 (0.049) +550.1 0.024 (0.037) 12.4 lscript1-LLM 3 5.0 0.5 0.290 (0.302) +0.5 0.076 (0.088) +15.1 0.032 (0.045) +510.7 0.022 (0.035) 13.3 Table 4: Parsing results for test corpus: SVM-HKE and lscript1-LLM classifiers with sparse feature space." ></td>
	<td class="line x" title="203:244	0 0.5 1 1.5 2 2.5 0 10 20 30 40 50 60 70 80A ve." ></td>
	<td class="line x" title="204:244	clas sific atio ntim e[m s/se nt.]" ></td>
	<td class="line x" title="205:244	Sizeoffstrie[MB] 0.671ms/sent.(18.6MB) 0.680ms/sent.(67.1MB) naiveutilityscore Figure 4: Fstrie reduction: utility score vs. processed sentence reduction for SVM-KE (d = 4)." ></td>
	<td class="line x" title="206:244	ply reducing the number of sentences processed to 1/2n." ></td>
	<td class="line x" title="207:244	The impact on the speed-up of the resulting fstries (naive) and the fstries constructed by our utility score (utility-score) on SVM-KE (d = 4) is shown in Figure 4." ></td>
	<td class="line x" title="208:244	The Zipfian nature of language data let us obtain a substantial speed-up even when we naively reduced the fstrie size, and the utility score further decreased the fstrie size required to obtain the same speed-up." ></td>
	<td class="line x" title="209:244	We needed less than 1/3 size fstries to achieve the same speedup: 0.671 ms./sent." ></td>
	<td class="line x" title="210:244	(18.6 MB) (utility-score) vs. 0.680 ms./sent." ></td>
	<td class="line x" title="211:244	(67.1 MB) (naive)." ></td>
	<td class="line x" title="212:244	Results for SVM-HKE and lscript1-LLM classifiers with sparse feature space The performances of parsers having SVM-HKE and lscript1-LLM classifiers with and without the fstrie are given in Table 4." ></td>
	<td class="line x" title="213:244	The fstries successfully speeded up the SVM-HKE and lscript1-LLM classifiers by factors of 10.7 (SVMHKE, d = 4,  = 0.0006) and 11.6 (lscript1-LLM, d = 3,  = 3.0)." ></td>
	<td class="line x" title="214:244	We obtained more speedup when we used fstries for classifiers with more sparse feature space Fd (Figures 5 and 6)." ></td>
	<td class="line x" title="215:244	The parsing speed with d = 3 models are now comparable to the parsing speed with d = 2 models." ></td>
	<td class="line x" title="216:244	0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 100 200 300 400 500 600 700A ve." ></td>
	<td class="line x" title="217:244	clas sific atio ntim e[m s/se nt.]" ></td>
	<td class="line x" title="218:244	Sizeoffstrie[MB] SVM-KE(d=3)SVM-HKE(d=3,=0.001) SVM-HKE(d=3,=0.002)SVM-HKE(d=3,=0.003) Figure 5: Average classification time per sentence plotted against size of fstrie: SVM-HKE (d = 3)." ></td>
	<td class="line x" title="219:244	0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 100 200 300 400 500 600 700A ve." ></td>
	<td class="line x" title="220:244	clas sific atio ntim e[m s/se nt.]" ></td>
	<td class="line x" title="221:244	Sizeoffstrie[MB] 1-LLM(d=3,=3.0) 1-LLM(d=3,=4.0) 1-LLM(d=3,=5.0) Figure 6: Average classification time per sentence plotted against size of fstrie: lscript1-LLM (d = 3)." ></td>
	<td class="line x" title="222:244	Without fstries, little speed-up of SVM-HKE classifiers versus the SVM-KE classifiers (in Table 3) was obtained due to the mild reduction in the average number of active features|xd|in the classification." ></td>
	<td class="line x" title="223:244	This result conforms to the results reported in (Kudo and Matsumoto, 2003)." ></td>
	<td class="line x" title="224:244	The parsing speed reached 14,937 sentences per second with accuracy of 90.91% (SVM-HKE, d = 3,  = 0.002)." ></td>
	<td class="line x" title="225:244	We used this parser to process 1,005,918 sentences (5,934,184 bunsetsus) randomly extracted from Japanese weblog feeds 1549 updated in November 2008, to see how much the impact of fstries lessens when the test data and the data processed to build fstries mismatch." ></td>
	<td class="line x" title="226:244	The parsing time was 156.4 sec." ></td>
	<td class="line x" title="227:244	without fstrieL, while it was just 35.9 sec." ></td>
	<td class="line x" title="228:244	with fstrieL." ></td>
	<td class="line x" title="229:244	The speed-up factor of 4.4 on weblog feeds was slightly worse than that on news articles (0.346/0.067 = 5.2) but still evident." ></td>
	<td class="line x" title="230:244	This implies that sorting features in building fstries yielded prefix features vectors that commonly appear in this task, by excluding domain-specific features such as lexical features." ></td>
	<td class="line x" title="231:244	In summary, our algorithm successfully minimized the efficiency gap among classifiers with different degrees of feature combinations and made accurate classifiers trained with higher-order feature combinations practical." ></td>
	<td class="line x" title="232:244	5 Conclusion and Future Work Our simple method speeds up a classifier trained with many conjunctive features by using precalculated weights of (partial) feature vectors stored in a feature sequence trie (fstrie)." ></td>
	<td class="line x" title="233:244	We experimentally demonstrated that it speeded up SVM and LLM classifiers for a Japanese dependency parsing task by a factor of 10." ></td>
	<td class="line x" title="234:244	We also confirmed that the sparse feature space provided by lscript1-LLMs and SVM-HKEs contributed much to size reduction of the fstrie required to achieve the same speed-up." ></td>
	<td class="line x" title="235:244	The implementations of the proposed algorithm for LLMsand SVMs(withapolynomialkernel)and the Japanese dependency parser will be available at http://www.tkl.iis.u-tokyo.ac.jp/ynaga/." ></td>
	<td class="line x" title="236:244	We plan to apply our method to wider range of classifiers used in various NLP tasks." ></td>
	<td class="line x" title="237:244	To speed up classifiers used in a real-time application, we can build fstries incrementally by using feature vectors generated from user inputs." ></td>
	<td class="line x" title="238:244	When we run our classifiers on resource-tight environments such as cell-phones, we can use a random feature mixing technique (Ganchev and Dredze, 2008) or a memory-efficient trie implementation based on a succinct data structure (Jacobson, 1989; Delpratt et al., 2006) to reduce required memory usage." ></td>
	<td class="line x" title="239:244	We will combine our method with other techniques that provide sparse solutions, for example, kernel methods on a budget (Dekel and Singer, 2007; Dekel et al., 2008; Orabona et al., 2008) or kernel approximation (surveyed in Kashima et al.(2009))." ></td>
	<td class="line x" title="241:244	It is also easy to combine our method with SVMs with partial kernel expansion (Goldberg and Elhadad, 2008), which will yield slower but more space-efficient classifiers." ></td>
	<td class="line x" title="242:244	We will in the future consider an issue of speeding up decoding with structured models (Lafferty et al., 2001; Miyao and Tsujii, 2002; Sutton et al., 2004)." ></td>
	<td class="line x" title="243:244	Acknowledgment The authors wish to thank Susumu Yata and Yoshimasa Tsuruoka for letting the authors to use their pre-release libraries." ></td>
	<td class="line x" title="244:244	The authors also thank Nobuhiro Kaji and the anonymous reviewers for their valuable comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1161
K-Best Combination of Syntactic Parsers
Zhang, Hui;Zhang, Min;Tan, Chew Lim;Li, Haizhou;"></td>
	<td class="line x" title="1:255	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 15521560, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:255	c 2009 ACL and AFNLP K-Best Combination of Syntactic Parsers  Hui Zhang 1, 2    Min Zhang 1    Chew Lim Tan 2   Haizhou Li 1  1 Institute for Infocomm Research 2 National University of Singapore zhangh1982@gmail.com   {mzhang, hli}@i2r.a-star.edu.sg   tancl@comp.nus.edu.sg    Abstract In this paper, we propose a linear model-based general framework to combine k-best parse outputs from multiple parsers." ></td>
	<td class="line x" title="3:255	The proposed framework leverages on the strengths of previous system combination and re-ranking techniques in parsing by integrating them into a linear model." ></td>
	<td class="line x" title="4:255	As a result, it is able to fully utilize both the logarithm of the probability of each k-best parse tree from each individual parser and any additional useful features." ></td>
	<td class="line x" title="5:255	For feature weight tuning, we compare the simulated-annealing algorithm and the perceptron algorithm." ></td>
	<td class="line x" title="6:255	Our experiments are carried out on both the Chinese and English Penn Treebank syntactic parsing task by combining two stateof-the-art parsing models, a head-driven lexicalized model and a latent-annotation-based un-lexicalized model." ></td>
	<td class="line x" title="7:255	Experimental results show that our F-Scores of 85.45 on Chinese and 92.62 on English outperform the previously best-reported systems by 1.21 and 0.52, respectively." ></td>
	<td class="line x" title="8:255	1 Introduction Statistical models have achieved great success in language parsing and obtained the state-of-theart results in a variety of languages." ></td>
	<td class="line x" title="9:255	In general, they can be divided into two major categories, namely lexicalized models (Collins 1997, 1999; Charniak 1997, 2000) and un-lexicalized models (Klein and Manning 2003; Matsuzaki et al. 2005; Petrov et al. 2006; Petrov and Klein 2007)." ></td>
	<td class="line x" title="10:255	In lexicalized models, word information play a key role in modeling grammar rule generation, while un-lexicalized models usually utilize latent information derived from the parse structure diversity." ></td>
	<td class="line x" title="11:255	Although the two models are different from each other in essence, both have achieved stateof-the-art results in a variety of languages and are complementary to each other (this will be empirically verified later in this paper)." ></td>
	<td class="line x" title="12:255	Therefore, it is natural to combine the two models for better parsing performance." ></td>
	<td class="line x" title="13:255	Besides individual parsing models, many system combination methods for parsing have been proposed (Henderson and Brill 1999; Zeman and abokrtsk 2005; Sagae and Lavie 2006) and promising performance improvements have been reported." ></td>
	<td class="line x" title="14:255	In addition, parsing re-ranking (Collins 2000; Riezler et al. 2002; Charniak and Johnson 2005; Huang 2008) has also been shown to be another effective technique to improve parsing performance." ></td>
	<td class="line x" title="15:255	This technique utilizes a bunch of linguistic features to re-rank the k-best (Huang and Chiang 2005) output on the forest level or tree level." ></td>
	<td class="line x" title="16:255	In prior work, system combination was applied on multiple parsers while re-ranking was applied on the k-best outputs of individual parsers." ></td>
	<td class="line x" title="17:255	In this paper, we propose a linear model-based general framework for multiple parsers combination." ></td>
	<td class="line x" title="18:255	The proposed framework leverages on the strengths of previous system combination and reranking methods and is open to any type of features." ></td>
	<td class="line x" title="19:255	In particular, it is capable of utilizing the logarithm of the parse tree probability from each individual parser while previous combination methods are unable to use this feature since the probabilities from different parsers are not comparable." ></td>
	<td class="line x" title="20:255	In addition, we experiment on k-best combination while previous methods are only verified on 1-best combination." ></td>
	<td class="line x" title="21:255	Finally, we apply our method in combining outputs from both the lexicalized and un-lexicalized parsers while previous methods only carry out experiments on multiple lexicalized parsers." ></td>
	<td class="line x" title="22:255	We also compare two learning algorithms in tuning the feature weights for the linear model." ></td>
	<td class="line x" title="23:255	We perform extensive experiments on the Chinese and English Penn Treebank corpus." ></td>
	<td class="line x" title="24:255	Experimental results show that our final results, an F-Score of 92.62 on English and 85.45 on Chinese, outperform the previously best-reported systems by 0.52 point and 1.21 point, respectively." ></td>
	<td class="line x" title="25:255	This convincingly demonstrates the effectiveness of our proposed framework." ></td>
	<td class="line x" title="26:255	Our study also shows that the simulated-annealing algorithm (Kirkpatrick et al. 1983) is more effective 1552 than the perceptron algorithm (Collins 2002) for feature weight tuning." ></td>
	<td class="line x" title="27:255	The rest of this paper is organized as follows." ></td>
	<td class="line x" title="28:255	Section 2 briefly reviews related work." ></td>
	<td class="line x" title="29:255	Section 3 discusses our method while section 4 presents the feature weight tuning algorithm." ></td>
	<td class="line x" title="30:255	In Section 5, we report our experimental results and then conclude in Section 6." ></td>
	<td class="line x" title="31:255	2 Related Work As discussed in the previous section, system combination and re-ranking are two techniques to improve parsing performance by postprocessing parsers k-best outputs." ></td>
	<td class="line x" title="32:255	Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees." ></td>
	<td class="line x" title="33:255	According to the second scheme, it breaks each parse tree into constituents, calculates the count of each constituent, then applies the majority voting to decide which constituent would appear in the final tree." ></td>
	<td class="line x" title="34:255	Sagae and Lavie (2006) improve this second scheme by introducing a threshold for the constituent count, and search for the tree with the largest number of count from all the possible constituent combination." ></td>
	<td class="line x" title="35:255	Zeman and abokrtsk (2005) study four combination techniques, including voting, stacking, unbalanced combining and switching, for constituent selection on Czech dependency parsing." ></td>
	<td class="line x" title="36:255	Promising results have been reported in all the above three prior work." ></td>
	<td class="line x" title="37:255	Henderson and Brill (1999) combine three parsers and obtained an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper." ></td>
	<td class="line x" title="38:255	Sagae and Lavie (2006) combine 5 parsers to obtain a score of 92.1, while they report a score of 91.0 for the best single parser in their paper." ></td>
	<td class="line x" title="39:255	Finally, Zeman and abokrtsk (2005) reports great improvements over each individual parsers and show that a parser with very low accuracy can also help to improve the performance of a highly accurate parser." ></td>
	<td class="line x" title="40:255	However, there are two major limitations in these prior works." ></td>
	<td class="line x" title="41:255	First, only one-best output from each individual parsers are utilized." ></td>
	<td class="line x" title="42:255	Second, none of these works uses the parse probability of each parse tree output from the individual parser." ></td>
	<td class="line x" title="43:255	Regarding the parser re-ranking, Collins (2000) proposes a dozen of feature types to re-rank kbest outputs of a single head-driven parser." ></td>
	<td class="line x" title="44:255	He uses these feature types to extract around half a million different features on the training set, and then examine two loss functions, MRF and Boosting, to do feature selection." ></td>
	<td class="line x" title="45:255	Charniak and Johnson (2005) generate a more accurate k-best output and adopt MaxEnt method to estimate the feature weights for more than one million features extracted from the training set." ></td>
	<td class="line x" title="46:255	Huang (2008) further improves the re-ranking work of Charniak and Johnson (2005) by re-ranking on packed forest, which could potentially incorporate exponential number of k-best list." ></td>
	<td class="line x" title="47:255	The reranking techniques also achieve great improvement over the original individual parser." ></td>
	<td class="line x" title="48:255	Collins (2002) improves the F1 score from 88.2% to 89.7%, while Charniak and Johnson (2005) improve from 90.3% to 91.4%." ></td>
	<td class="line x" title="49:255	This latter work was then further improved by Huang (2008) to 91.7%, by utilizing the benefit of forest structure." ></td>
	<td class="line x" title="50:255	However, one of the limitations of these techniques is the huge number of features which makes the training very expensive and inefficient in space and memory usage." ></td>
	<td class="line x" title="51:255	3 K-best Combination of Lexicalized and Un-Lexicalized Parsers with Model Probabilities In this section, we first introduce our proposed kbest combination framework." ></td>
	<td class="line x" title="52:255	Then we apply this framework to the combination of two state-ofthe-art lexicalized and un-lexicalized parsers with an additional feature inspired by traditional combination techniques." ></td>
	<td class="line x" title="53:255	3.1 K-best Combination Framework Our proposed framework consists of the following steps: 1) Given an input sentence and N different parsers, each parser generates K-best parse trees." ></td>
	<td class="line x" title="54:255	2) We combine the N*K output trees and remove any duplicates to obtain M unique tress." ></td>
	<td class="line x" title="55:255	3) For each of the M unique trees, we reevaluate it with all the N models which are used by the N parsers." ></td>
	<td class="line x" title="56:255	It is worth noting that this is the key point (i.e. one of the major advantages) of our method since some parse trees are only generated from one or I (I<N) parsers." ></td>
	<td class="line x" title="57:255	For example, if a tree is only generated from head-driven lexicalized model, then it only has the head-driven model score." ></td>
	<td class="line x" title="58:255	Now we reevaluate it with the latent-annotation unlexicalized model to reflect the latent1553 annotation models confidence for this tree." ></td>
	<td class="line x" title="59:255	This enables our method to effectively utilize the confidence measure of all the individual models without any bias." ></td>
	<td class="line x" title="60:255	Without this re-evaluation step, the previous combination methods are unable to utilize the various model scores." ></td>
	<td class="line x" title="61:255	4) Besides model scores, we also compute some additional feature scores for each tree, such as the widely-used constituent count feature." ></td>
	<td class="line x" title="62:255	5) Then we adopt the linear model to balance and combine these feature scores and generate an overall score for each parse tree." ></td>
	<td class="line x" title="63:255	6) Finally we re-rank the M best trees and output the one with the highest score." ></td>
	<td class="line x" title="64:255	null null null null null null null null   The above is the linear function used in our method, where t is the tree to be evaluated,  to  are the model confidence scores (in this paper, we use logarithm of the parse tree probability) from the N models,  to  are their weights,   to   are the L additional features,   to  are their weights." ></td>
	<td class="line x" title="65:255	In this paper, we employ two individual parsing model scores and only one additional feature." ></td>
	<td class="line x" title="66:255	Let  be the head-driven model score,  be the latent-annotation model score,   be the constituent count feature and   is the weight of feature  . 3.2 Confidences of Lexicalized and Unlexicalized Model The term confidence was used in prior parser combination studies to refer to the accuracy of each individual parser." ></td>
	<td class="line x" title="67:255	This reflects how much we can trust the parse output of each parser." ></td>
	<td class="line x" title="68:255	In this paper, we use the term confidence to refer to the logarithm of the tree probability computed by each model, which is a direct measurement of the models confidence on the target tree being the best or correct parse output." ></td>
	<td class="line x" title="69:255	In fact, the feature weight null  in our linear model functions similarly as the traditional confidence." ></td>
	<td class="line x" title="70:255	However, we do not directly use parsers accuracy as its value." ></td>
	<td class="line x" title="71:255	Instead we tune it automatically on development set to optimize it against the parsing performance directly." ></td>
	<td class="line x" title="72:255	In the following, we introduce the state-of-the-art head-driven lexicalized and latent-annotation un-lexicalized models (which are used as two individual models in this paper), and describe how they compute the tree probability briefly." ></td>
	<td class="line x" title="73:255	Head-driven model is one of the most representative lexicalized models." ></td>
	<td class="line x" title="74:255	It attaches the head word to each non-terminal and views the generation of each rule as a Markov process first from father to head child, and then to the head childs left and right siblings." ></td>
	<td class="line x" title="75:255	Take following rule r as example,     is the rules left hand side (i.e. father label),  is the head child,  is Ms left sibling and is Ms right sibling." ></td>
	<td class="line x" title="76:255	Let h be Ms head word, the probability of this rule is    The probability of a tree is just the product of the probabilities of all the rules in it." ></td>
	<td class="line x" title="77:255	The above is the general framework of head-driven model." ></td>
	<td class="line x" title="78:255	For a specific model, there may be some additional features and modification." ></td>
	<td class="line x" title="79:255	For example, the model2 in Collins (1999) introduces subcategorization and model3 introduces gap as additional features." ></td>
	<td class="line x" title="80:255	Charniak (2000)s model introduces pre-terminal as additional features." ></td>
	<td class="line x" title="81:255	The latent-annotation model (Matsuzaki et al. 2005; Petrov et al. 2006) is one of the most effective un-lexicalized models." ></td>
	<td class="line x" title="82:255	Briefly speaking, latent-annotation model views each non-terminal in the Treebank as a non-terminal followed by a set of latent variables, and uses EM algorithms to automatically learn the latent variables probability functions to maximize the probability of the given training data." ></td>
	<td class="line x" title="83:255	Take the following binarized rule as example,    could be viewed as the set of rules    The process of computing the probability of a normal tree is to first binarized all the rules in it, and then replace each rule to the corresponding set of rules with latent variables." ></td>
	<td class="line x" title="84:255	Now the previous tree becomes a packed forest (Klein and Manning 2001; Petrov et al. 2007) in the latentannotation model, and its probability is the inside probability of the root node." ></td>
	<td class="line x" title="85:255	This model is quite different from the head-driven model in which 1554 the probability of a tree is just the product all the rules probability." ></td>
	<td class="line x" title="86:255	3.3 Constituent Counts Besides the two model scores, we also adopt constituent count as an additional feature inspired by (Henderson and Brill 1999) and (Sagae and Lavie 2006)." ></td>
	<td class="line x" title="87:255	A constituent is a non-terminal node covering a special span." ></td>
	<td class="line x" title="88:255	For example, NP[2,4] means a constituent labelled as NP which covers the span from the second word to the fourth word." ></td>
	<td class="line x" title="89:255	If we have 100 trees and NP[2,4] appears in 60 of them, then its constituent count is 60." ></td>
	<td class="line x" title="90:255	For each tree, its constituent count is the sum of all the counts of its constituent." ></td>
	<td class="line x" title="91:255	However, as suggested in (Sagae and Lavie 2006), this feature favours precision over recall." ></td>
	<td class="line x" title="92:255	To solve this issue, Sagae and Lavie (2006) use a threshold to balance them." ></td>
	<td class="line x" title="93:255	For any constituent, we calculate its count if and only if it appears more than X times in the k-best trees; otherwise we set it as 0." ></td>
	<td class="line x" title="94:255	In this paper, we normalize this feature by dividing the constituent count by the number of k-best." ></td>
	<td class="line x" title="95:255	Note that the threshold value and the additional feature value are not independent." ></td>
	<td class="line x" title="96:255	Once the threshold changes, the feature value has to be recalculated." ></td>
	<td class="line x" title="97:255	In conclusion, we have four parameters to estimate: two model score weights, one additional feature weight and a threshold for the additional feature." ></td>
	<td class="line x" title="98:255	4 Parameter Estimation We adopt the minimum error rate principle to tune the feature weights by minimizing the error rate (i.e. maximizing the F1 score) on the development set." ></td>
	<td class="line x" title="99:255	In our study, we implement and compare two algorithms, the simulated-annealing style algorithm and the average perceptron algorithm." ></td>
	<td class="line x" title="100:255	4.1 Simulated Annealing Simulated-annealing algorithm has been proved to be a powerful and efficient algorithm in solving NP problem (ern 1985)." ></td>
	<td class="line x" title="101:255	Fig 1 is the pseudo code of the simulated-annealing algorithm that we apply." ></td>
	<td class="line x" title="102:255	In a single iteration (line 4-11), the simulated algorithm selects some random points (the Markov link) for hill climbing." ></td>
	<td class="line x" title="103:255	However, it accepts some bad points with a threshold probability controlled by the annealing temperature (line 710)." ></td>
	<td class="line x" title="104:255	The hill climbing nature gives this algorithm the ability of converging at local maximal point and the random nature offers it the chance to jump from some local maximal points to global maximal point." ></td>
	<td class="line x" title="105:255	We do a slight modification to save the best parameter so far across all the finished iterations and let it be the initial point for upcoming iterations (line 12-17)." ></td>
	<td class="line x" title="106:255	RandomNeighbour(p) is the function to generate a random neighbor for the p (the four-tuple parameter to be estimated)." ></td>
	<td class="line x" title="107:255	F1(p) is the function to calculate the F1 score over the entire test set." ></td>
	<td class="line x" title="108:255	Given a fixed parameter p, it selects the candidate tree with best score for each sentence and computes the F1 score with the PARSEVAL metrics." ></td>
	<td class="line x" title="109:255	Pseudo code 1." ></td>
	<td class="line x" title="110:255	Simulated-annealing algorithm Input: k-best trees combined from two model output Notation:    p: the current parameter value    F1(p): the F1 score with the parameter value p    TMF: the max F1 score of each iteration    TMp: the optimal parameter value during iteration    MaxF1: the max F1 score on dev set    Rp: the parameter value which maximizes the F1 score of the dev set    T: annealing temperature    L: length of Markov link Output: Rp  1." ></td>
	<td class="line x" title="111:255	MaxF1:= 0, Rp:= (0,0,0,0), T:=1, L=100 // initialize 2." ></td>
	<td class="line x" title="112:255	Repeat                                                       // iteration 3." ></td>
	<td class="line x" title="113:255	TMp :=Rp 4." ></td>
	<td class="line x" title="114:255	for  i := 1 to L  do 5." ></td>
	<td class="line x" title="115:255	p := RandomNeighbour(TMp) 6." ></td>
	<td class="line x" title="116:255	d= F1(p)TMF 7." ></td>
	<td class="line x" title="117:255	if d>0 or exp(d/T) > random[0,1) then 8." ></td>
	<td class="line x" title="118:255	TMF:=F1(p) 9." ></td>
	<td class="line x" title="119:255	TMp:=p 10." ></td>
	<td class="line x" title="120:255	end if 11." ></td>
	<td class="line x" title="121:255	end for 12." ></td>
	<td class="line x" title="122:255	if TMF > MaxF1 then 13." ></td>
	<td class="line x" title="123:255	MaxF:=TMF 14." ></td>
	<td class="line x" title="124:255	Rp:=TMp 15." ></td>
	<td class="line x" title="125:255	else 16." ></td>
	<td class="line x" title="126:255	TMp:=Rp 17." ></td>
	<td class="line x" title="127:255	end if 18." ></td>
	<td class="line x" title="128:255	T=T*0.9 19." ></td>
	<td class="line x" title="129:255	Until convergence  Fig 1." ></td>
	<td class="line x" title="130:255	Simulated Annealing Algorithm 4.2 Averaged Perceptron Another algorithm we apply is the averaged perceptron algorithm." ></td>
	<td class="line x" title="131:255	Fig 2 is the pseudo code of this algorithm." ></td>
	<td class="line x" title="132:255	Averaged perceptron is an online algorithm." ></td>
	<td class="line x" title="133:255	It iterates through each instance." ></td>
	<td class="line x" title="134:255	In each instance, it selects the candidate answer with the maximum function score." ></td>
	<td class="line x" title="135:255	Then it updates the weight by the margin of feature value between the select answer and the oracle answer (line 5-9)." ></td>
	<td class="line x" title="136:255	After each iteration, it does average to generate a new weight (line 10)." ></td>
	<td class="line x" title="137:255	The averaged 1555 perceptron has a solid theoretical fundamental and was proved to be effective across a variety of NLP tasks (Collins 2002)." ></td>
	<td class="line x" title="138:255	However, it needs a slightly modification to adapt to our problem." ></td>
	<td class="line x" title="139:255	Since the threshold and the constituent count are not independent, they are not linear separable." ></td>
	<td class="line x" title="140:255	In this case, the perceptron algorithm cannot be guaranteed to converge." ></td>
	<td class="line x" title="141:255	To solve this issue, we introduce an outer loop (line 2) to iterate through the value range of threshold with a fixed step length and in the inner loop we use perceptron to estimate the other three parameters." ></td>
	<td class="line x" title="142:255	Finally we select the final parameter which has maximum F1 score across all the iteration (line 14-17)." ></td>
	<td class="line x" title="143:255	Pseudo code 2." ></td>
	<td class="line x" title="144:255	Averaged perceptron algorithm Input: k-best trees combined from two model output Notation:    MaxF1, Rp: already defined in pseudo code 1    T: the max number of iterations    I: the number of instances    Threshold: the threshold for constituent count    w: the three feature weights other than threshold   : the candidate tree with max function score given a fixed weight w  null : the candidate tree with the max F1 score (since the oracle tree may not appeared in our candidate set, we choose this one as the pseudo orcale tree)    : the set of candidate tree for ith sentence Output: Rp  1." ></td>
	<td class="line x" title="145:255	MaxF1:=0, T=30 2." ></td>
	<td class="line x" title="146:255	for  Threshold :=0 to 1 with step 0.01 do 3." ></td>
	<td class="line x" title="147:255	Initialize w 4." ></td>
	<td class="line x" title="148:255	for iter : 1 to T do 5." ></td>
	<td class="line x" title="149:255	for  i := 1 to I  do 6." ></td>
	<td class="line x" title="150:255	 nullnullnullnullnullnullnullnullnull  7." ></td>
	<td class="line x" title="151:255	null   8." ></td>
	<td class="line x" title="152:255	null := w 9." ></td>
	<td class="line x" title="153:255	end for 10." ></td>
	<td class="line x" title="154:255	 null null I nullnullnull I  11." ></td>
	<td class="line x" title="155:255	if converged  then break 12." ></td>
	<td class="line x" title="156:255	end for 13." ></td>
	<td class="line x" title="157:255	p := (Threshold, w) 14." ></td>
	<td class="line x" title="158:255	if F1(p) > MaxF1 then 15." ></td>
	<td class="line x" title="159:255	MaxF1 := F1(p) 16." ></td>
	<td class="line x" title="160:255	Rp:=p 17." ></td>
	<td class="line x" title="161:255	end if 18." ></td>
	<td class="line x" title="162:255	end for  Fig 2." ></td>
	<td class="line x" title="163:255	Averaged Perceptron Algorithm 5 Experiments We evaluate our method on both Chinese and English syntactic parsing task with the standard division on Chinese Penn Treebank Version 5.0 and WSJ English Treebank 3.0 (Marcus et al. 1993) as shown in Table 1." ></td>
	<td class="line x" title="164:255	We use Satoshi Sekine and Michael Collins EVALB script modified by David Ellis for accuracy evaluation." ></td>
	<td class="line x" title="165:255	We use Charniaks parser (Charniak 2000) and Berkeleys parser (Petrov and Klein 2007) as the two individual parsers, where Charniaks parser represents the best performance of the lexicalized model and the Berkeleys parser represents the best performance of the un-lexicalized model." ></td>
	<td class="line x" title="166:255	We retrain both of them according to the division in Table." ></td>
	<td class="line x" title="167:255	1." ></td>
	<td class="line x" title="168:255	The number of EM iteration process for Berkeleys parser is set to 5 on English and 6 on Chinese." ></td>
	<td class="line x" title="169:255	Both the Charniaks parser and Berkeleys parser provide function to evaluate an input parse trees probability and output the logarithm of the probability." ></td>
	<td class="line x" title="170:255	Div." ></td>
	<td class="line x" title="171:255	Lang." ></td>
	<td class="line x" title="172:255	Train Dev Test English Sec.02-21 Sec." ></td>
	<td class="line x" title="173:255	22 Sec." ></td>
	<td class="line x" title="174:255	23  Chinese Art." ></td>
	<td class="line x" title="175:255	001-270, 400-1151 Art." ></td>
	<td class="line x" title="176:255	301-325 Art." ></td>
	<td class="line x" title="177:255	271-300            Table 1." ></td>
	<td class="line x" title="178:255	Data division 5.1 Effectiveness of our Combination Method This sub-section examines the effectiveness of our proposed methods." ></td>
	<td class="line x" title="179:255	The experiment is set up as follows: 1) for each sentence in the dev and test sets, we generate 50-best from Charniaks parser (Charniak 2000) and Berkeleys parser (Petrov and Klein 2007), respectively; 2) the two 50-best trees are merged together and duplication was removed; 3) we tune the parameters on the dev set and test on the test set." ></td>
	<td class="line x" title="180:255	(Without specific statement, we use simulated-annealing as default weight tuning algorithm.)" ></td>
	<td class="line x" title="181:255	The results are shown in Table 2 and Table 3." ></td>
	<td class="line x" title="182:255	P means precision, R means recall and F is the F1-measure (all is in % percentage metrics); Charniak represents the parser of (Charniak 2000), Berkeley represents the parser of (Petrov and Klein 2007), Comb. represents the combination of the two parsers." ></td>
	<td class="line x" title="183:255	parser accuracy Charniak Berkeley Comb." ></td>
	<td class="line x" title="184:255	<=40 words P 85.20 86.65 90.44 R 83.70 84.18 85.96 F 84.44 85.40 88.15 All P 82.07 84.63 87.76 R 79.66 81.69 83.27 F 80.85 83.13 85.45  Table 2." ></td>
	<td class="line x" title="185:255	Results on Chinese 1556          parser accuracy Charniak Berkeley Comb." ></td>
	<td class="line x" title="186:255	<=40 words P 90.45 90.27 92.36 R 90.14 89.76 91.42 F 90.30 90.02 91.89 All P 89.86 89.77 91.89 R 89.53 89.26 90.97 F 89.70 89.51 91.43  Table 3." ></td>
	<td class="line x" title="187:255	Results on English  From Table 2 and Table 3, we can see our method outperforms the single systems in all test cases with all the three evaluation metrics." ></td>
	<td class="line x" title="188:255	Using the entire Chinese test set, our method improves the performance by 2.3 (85.45-83.13) point in F1-Score, representing 13.8% error rate reduction." ></td>
	<td class="line x" title="189:255	Using the entire English test set, our method improves the performance by 1.7 (91.43-89.70) point in F1-Score, representing 16.5% error rate reduction." ></td>
	<td class="line x" title="190:255	These improvements convincingly demonstrate the effectiveness of our method." ></td>
	<td class="line x" title="191:255	5.2 Effectiveness of K Fig 3 and Fig." ></td>
	<td class="line x" title="192:255	4 show the relationship between F1 score and the number of K-best used when doing combination on Chinese and English respectively." ></td>
	<td class="line x" title="193:255	From Fig 3 and Fig." ></td>
	<td class="line x" title="194:255	4, we could see that the F1 score first increases with the increasing of K (there are some vibration points, this may due to statistical noise) and reach the peak when K is around 30-50, then it starts to drop." ></td>
	<td class="line x" title="195:255	It shows that k-best list did provide more information than one-best and thus can help improve the accuracy; however more k-best list may also contain more noises and these noises may hurt the final combination quality." ></td>
	<td class="line x" title="196:255	Fig 3." ></td>
	<td class="line x" title="197:255	F1-measure vs. K on Chinese           Fig 4." ></td>
	<td class="line x" title="198:255	F1-measure vs. K on English 5.3 Diversity on the K-best Output of the Head-driven and Latent-annotationdriven Model In this subsection, we examine how different of the 50-best trees generated from Charnriaks parser (head-driven model) (Charnriak, 2000) and Berkeleys parser (latent-annotation model) (Petrov and Klein, 2007)." ></td>
	<td class="line x" title="199:255	Table 4 reports the statistics on the 50-best output for Chinese and English test set." ></td>
	<td class="line x" title="200:255	Since for some short sentences the parser cannot generate up to 50 best trees, the average number of trees is less than 50 for each sentence." ></td>
	<td class="line x" title="201:255	Each cell reports the total number of trees generated over the entire test set followed by the average count for each sentence in bracket." ></td>
	<td class="line x" title="202:255	Total means simply combine the number of trees from the two parsers while Unique means the number after removing the duplicated trees for each sentence." ></td>
	<td class="line x" title="203:255	In the last row, we report the averaged redundant rate for each sentence, which is derived by dividing the figures in the row Duplicated by those in the row Total." ></td>
	<td class="line x" title="204:255	Chinese English Charniak 14577 (41.9) 120438 (49.9) Berkeley 14524 (41.7) 114299 (47.3) Total 29101 (83.6) 234737 (97.2) Unique 27747 (79.7) 221633 (91.7) Duplicated 1354 (3.9) 13104 (5.4) Redundant rate 4.65% 5.58%            Table 4." ></td>
	<td class="line x" title="205:255	The statistics on the 50-best output for Chinese and English test set." ></td>
	<td class="line x" title="206:255	The small redundant rate clearly suggests that the two parsing models are quite different and are complementary to each other." ></td>
	<td class="line x" title="207:255	1557          parser Oracle Charniak Berkeley Comb." ></td>
	<td class="line x" title="208:255	Chinese P 88.95 90.07 92.45 R 86.51 87.12 89.67 F 87.71 88.57 91.03 English P 97.06 95.86 98.10 R 96.57 95.53 97.68 F 96.82 95.70 97.89  Table 5." ></td>
	<td class="line x" title="209:255	The oracle over 50-best output for individual parser and our method  The k-best oracle score is the upper bound of the quality of the k-best trees." ></td>
	<td class="line x" title="210:255	Table 5 reports the oracle score for the 50-best of the two individual parsers and our method." ></td>
	<td class="line x" title="211:255	Similar to Table 4, Table 5 shows again that the two models are complementary to each other and our method is able to take the strength of the two models." ></td>
	<td class="line x" title="212:255	5.4 Effectiveness of Model Confidence One of the advantages of our method that we claim is that we can utilize the feature of the model confidence score (logarithm of the parse tree probability)." ></td>
	<td class="line x" title="213:255	Table 6 shows that all the three features contribute to the final accuracy improvement." ></td>
	<td class="line x" title="214:255	Even if we only use the B+C confidence scores, it also outperforms the baseline individual parser (as reported in Table 2 and Table 3) greatly." ></td>
	<td class="line x" title="215:255	All these together clearly verify the effective of the model confidence feature and our method can effectively utilize this feature." ></td>
	<td class="line x" title="216:255	Feat." ></td>
	<td class="line x" title="217:255	Lang    I B+C B+C+I Chinese 82.34 84.67 85.45 English 90.20 91.02 91.43  Table 6." ></td>
	<td class="line x" title="218:255	F1 score on 50-best combination with different feature configuration." ></td>
	<td class="line x" title="219:255	I means the constituent count, B means Berkeley parser confidence score and C means Charniak parser confidence score." ></td>
	<td class="line x" title="220:255	5.5 Comparison of the Weight Tuning Algorithms In this sub-section, we compare the two weight tuning algorithms on 50-best combination tasks on both Chinese and English." ></td>
	<td class="line x" title="221:255	Dan Bikels randomized parsing evaluation comparator (Bikel 2004) was used to do significant test on precision and recall metrics." ></td>
	<td class="line x" title="222:255	The results are shown in Table 7." ></td>
	<td class="line x" title="223:255	We can see, simulated annealing outperforms the averaged perceptron significantly in both precision (p<0.005) and recall (p<0.05) metrics of Chinese task and precision (p<0.005) metric of English task." ></td>
	<td class="line x" title="224:255	Though averaged perceptron got slightly better recall score on English task, it is not significant according to the p-value (p>0.2)." ></td>
	<td class="line x" title="225:255	From table 8, we could see the simulated annealing algorithm is around 2-4 times slower than averaged perceptron algorithm." ></td>
	<td class="line x" title="226:255	Algo." ></td>
	<td class="line x" title="227:255	Lang SA." ></td>
	<td class="line x" title="228:255	AP." ></td>
	<td class="line x" title="229:255	P-value Chinese P 87.76 86.85 0.003 R 83.27 82.90 0.030 English P 91.89 91.72 0.004 R 90.97 91.02 0.205  Table 7." ></td>
	<td class="line x" title="230:255	Precision and Recall score on 50-best combination by the two parameter estimation algorithms with significant test; SA. is simulated annealing, AP. is averaged perceptron, P-value is the significant test p-value." ></td>
	<td class="line x" title="231:255	Algo." ></td>
	<td class="line x" title="232:255	Lang Simulated Annealing Averaged Perceptron Chinese 2.3 0.6 English 12 6     Table 8." ></td>
	<td class="line x" title="233:255	Time taken (in minutes) on 50-best combination of the two parameter estimation algorithms 5.6 Performance-Enhanced Individual Parsers on English For Charniaks lexicalized parser, there are two techniques to improve its performance." ></td>
	<td class="line x" title="234:255	One is reranking as explained in section 2." ></td>
	<td class="line oc" title="235:255	The other is the self-training (McClosky et al. 2006) which first parses and reranks the NANC corpus, and then use them as additional training data to retrain the model." ></td>
	<td class="line x" title="236:255	In this sub-section, we apply our method to combine the Berkeley parser and the enhanced Charniak parser by using the new model confidence score output from the enhanced Charniak parser." ></td>
	<td class="line x" title="237:255	Table 9 and Table 10 show that the Charniak parser enhanced by re-ranking and self-training is able to help to further improve the performance of our method." ></td>
	<td class="line x" title="238:255	This is because that the enhanced Charniak parser provides more accurate model confidence score." ></td>
	<td class="line x" title="239:255	1558          parser accuracy reranking Comb." ></td>
	<td class="line x" title="240:255	baseline <=40 words P 92.34 93.41 92.36 R 91.61 92.15 91.42 F 91.97 92.77 91.89 All P 91.78 92.92 91.89 R 91.03 91.70 90.97 F 91.40 92.30 91.43  Table 9." ></td>
	<td class="line x" title="241:255	Performance with Charniak parser enhanced by re-ranking; baseline is the performance of the combination of Table 3." ></td>
	<td class="line x" title="242:255	parser accuracy self-train+ reranking Comb." ></td>
	<td class="line x" title="243:255	baseline <=40 words P 92.87 93.69 92.36 R 92.12 92.44 91.42 F 92.49 93.06 91.89 All P 92.41 93.25 91.89 R 91.64 92.00 90.97 F 92.02 92.62 91.43   Table 10." ></td>
	<td class="line pc" title="244:255	Performance with Charniak parser enhanced by re-ranking plus self-training 5.7 Comparison with Other State-of-the-art Results Table 11 and table 12 compare our method with the other state-of-the-art methods; we use I, B, R, S and C to denote individual model (Charniak 2000; Collins 2000; Bod 2003; Petrov and Klein 2007), bilingual-constrained model (Burkett and Klein 2008) 1 , re-ranking model (Charniak and Johnson 2005, Huang 2008), self-training model (David McClosky 2006) and combination model (Sagae and Lavie 2006) respectively." ></td>
	<td class="line n" title="245:255	The two tables clearly show that our method advance the state-of-the-art results on both Chinese and English syntax parsing." ></td>
	<td class="line x" title="246:255	System  F1-Measure I Charniak (2000) 80.85 Petrov and Klein (2007) 83.13 B Burkett and Klein (2008) 1  84.24 C Our method 85.45  Table 11." ></td>
	<td class="line x" title="247:255	Accuracy comparison on Chinese   1  Burkett and Klein (2008) use the additional knowledge from Chinese-English parallel Treebank to improve Chinese parsing accuracy." ></td>
	<td class="line oc" title="248:255	System  F1-Measure I Petrov and Klein (2007) 89.5 Charniak (2000) 89.7 Bod (2003) 90.7 R Collins (2000) 89.7 Charniak and Johnson (2005) 91.4 Huang (2008) 91.7 S David McClosky (2006) 92.1 C Sagae and Lavie (2006) 92.1 Our method 92.6    Table 12." ></td>
	<td class="line x" title="249:255	Accuracy comparison on English." ></td>
	<td class="line x" title="250:255	6 Conclusions In this paper2, we propose a linear model-based general framework for multiple parser combination." ></td>
	<td class="line x" title="251:255	Compared with previous methods, our method is able to use diverse features, including logarithm of the parse tree probability calculated by the individual systems." ></td>
	<td class="line x" title="252:255	We verify our method by combining the two representative parsing models, lexicalized model and un-lexicalized model, on both Chinese and English." ></td>
	<td class="line x" title="253:255	Experimental results show our method is very effective and advance the state-of-the-art results on both Chinese and English syntax parsing." ></td>
	<td class="line x" title="254:255	In the future, we will explore more features and study the forest-based combination methods for syntactic parsing." ></td>
	<td class="line x" title="255:255	Acknowledgement We would like to thank Prof. Hwee Tou Ng for his help and support; Prof. Charniak for his suggestion on doing the experiments with the selftrained parser and David McCloksy for his help on the self-trained model; Yee Seng Chan and the anonymous reviewers for their valuable comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E09-1033
Rich Bitext Projection Features for Parse Reranking
Fraser, Alexander;Wang, Renjing;Schtze, Hinrich;"></td>
	<td class="line x" title="1:247	Proceedings of the 12th Conference of the European Chapter of the ACL, pages 282290, Athens, Greece, 30 March  3 April 2009." ></td>
	<td class="line x" title="2:247	c2009 Association for Computational Linguistics Rich bitext projection features for parse reranking Alexander Fraser Renjing Wang Institute for Natural Language Processing University of Stuttgart {fraser,wangrg}@ims.uni-stuttgart.de Hinrich Schutze Abstract Many different types of features have been shown to improve accuracy in parse reranking." ></td>
	<td class="line x" title="3:247	A class of features that thus far has not been considered is based on a projection of the syntactic structure of a translation of the text to be parsed." ></td>
	<td class="line x" title="4:247	The intuition for using this type of bitext projection feature is that ambiguous structures in one language often correspond to unambiguous structures in another." ></td>
	<td class="line x" title="5:247	We show that reranking based on bitext projection features increases parsing accuracy significantly." ></td>
	<td class="line x" title="6:247	1 Introduction Parallel text or bitext is an important knowledge source for solving many problems such as machine translation, cross-language information retrieval, and the projection of linguistic resources from one language to another." ></td>
	<td class="line x" title="7:247	In this paper, we show that bitext-based features are effective in addressing another NLP problem, increasing the accuracy of statistical parsing." ></td>
	<td class="line x" title="8:247	We pursue this approach for a number of reasons." ></td>
	<td class="line x" title="9:247	First, one limiting factor for syntactic approaches to statistical machine translation is parse quality (Quirk and Corston-Oliver, 2006)." ></td>
	<td class="line x" title="10:247	Improved parses of bitext should result in improved machine translation." ></td>
	<td class="line x" title="11:247	Second, as more and more texts are available in several languages, it will be increasingly the case that a text to be parsed is itself part of a bitext." ></td>
	<td class="line oc" title="12:247	Third, we hope that the improved parses of bitext will serve as higher quality training data for improving monolingual parsing using a process similar to self-training (McClosky et al., 2006)." ></td>
	<td class="line x" title="13:247	It is well known that different languages encode different types of grammatical information (agreement, case, tense etc.) and that what can be left unspecified in one language must be made explicit NP NP NP DT a NN baby CC and NP DT a NN woman SBAR who had gray hair Figure 1: English parse with high attachment in another." ></td>
	<td class="line x" title="14:247	This information can be used for syntactic disambiguation." ></td>
	<td class="line x" title="15:247	However, it is surprisingly hard to do this well." ></td>
	<td class="line x" title="16:247	We use parses and alignments that are automatically generated and hence imperfect." ></td>
	<td class="line x" title="17:247	German parse quality is considered to be worse than English parse quality, and the annotation style is different, e.g., NP structure in German is flatter." ></td>
	<td class="line x" title="18:247	We conduct our research in the framework of N-best parse reranking, but apply it to bitext and add only features based on syntactic projection from German to English." ></td>
	<td class="line x" title="19:247	We test the idea that, generally, English parses with more isomorphism with respect to the projected German parse are better." ></td>
	<td class="line x" title="20:247	The system takes as input (i) English sentences with a list of automatically generated syntactic parses, (ii) a translation of the English sentences into German, (iii) an automatically generated parse of the German translation, and (iv) an automatically generated word alignment." ></td>
	<td class="line x" title="21:247	We achieve a significant improvement of 0.66 F1 (absolute) on test data." ></td>
	<td class="line x" title="22:247	The paper is organized as follows." ></td>
	<td class="line x" title="23:247	Section 2 outlines our approach and section 3 introduces the model." ></td>
	<td class="line x" title="24:247	Section 4 describes training and section 5 presents the data and experimental results." ></td>
	<td class="line x" title="25:247	In section 6, we discuss previous work." ></td>
	<td class="line x" title="26:247	Section 7 analyzes our results and section 8 concludes." ></td>
	<td class="line x" title="27:247	282 NP NP DT a NN baby CC and NP NP DT a NN woman SBAR who had gray hair Figure 2: English parse with low attachment CNP NP ART ein NN Baby KON und NP ART eine NN Frau , , S die Figure 3: German parse with low attachment 2 Approach Consider the English sentence He saw a baby and a woman who had gray hair." ></td>
	<td class="line x" title="28:247	Suppose that the baseline parser generates two parses, containing the NPs shown in figures 1 and 2, respectively, and that the semantically more plausible second parse in figure 2 is correct." ></td>
	<td class="line x" title="29:247	How can we determine that the second parse should be favored?" ></td>
	<td class="line x" title="30:247	Since we are parsing bitext, we can observe the German translation which is Er sah ein Baby und eine Frau, die graue Haare hatte (glossed: he saw a baby and a woman, who gray hair had)." ></td>
	<td class="line x" title="31:247	The singular verb in the subordinate clause (hatte: had) indicates that the subordinate S must be attached low to woman (Frau) as shown in figure 3." ></td>
	<td class="line x" title="32:247	We follow Collins (2000) approach to discriminative reranking (see also (Riezler et al., 2002))." ></td>
	<td class="line x" title="33:247	Given a new sentence to parse, we first select the best N parse trees according to a generative model." ></td>
	<td class="line x" title="34:247	Then we use new features to learn discriminatively how to rerank the parses in this N-best list." ></td>
	<td class="line x" title="35:247	We use features derived using projections of the 1-best German parse onto the hypothesized English parse under consideration." ></td>
	<td class="line x" title="36:247	In more detail, we take the 100 best English parses from the BitPar parser (Schmid, 2004) and rerank them." ></td>
	<td class="line x" title="37:247	We have a good chance of finding the optimal parse among the 100-best1." ></td>
	<td class="line x" title="38:247	An automatically generated word alignment determines translational correspondence between German and English." ></td>
	<td class="line x" title="39:247	We use features which measure syntactic di1Using an oracle to select the best parse results in an F1 of 95.90, an improvement of 8.01 absolute over the baseline." ></td>
	<td class="line x" title="40:247	vergence between the German and English trees to try to rank the English trees which have less divergence higher." ></td>
	<td class="line x" title="41:247	Our test set is 3718 sentences from the English Penn treebank (Marcus et al., 1993) which were translated into German." ></td>
	<td class="line x" title="42:247	We hold out these sentences, and train BitPar on the remaining Penn treebank training sentences." ></td>
	<td class="line x" title="43:247	The average F1 parsing accuracy of BitPar on this test set is 87.89%, which is our baseline2." ></td>
	<td class="line x" title="44:247	We implement features based on projecting the German parse to each of the English 100-best parses in turn via the word alignment." ></td>
	<td class="line x" title="45:247	By performing cross-validation and measuring test performance within each fold, we compare our new system with the baseline on the 3718 sentence set." ></td>
	<td class="line x" title="46:247	The overall test accuracy we reach is 88.55%, a statistically significant improvement over baseline of 0.66." ></td>
	<td class="line x" title="47:247	Given a word alignment of the bitext, the system performs the following steps for each English sentence to be parsed: (i) run BitPar trained on English to generate 100best parses for the English sentence (ii) run BitPar trained on German to generate the 1-best parse for the German sentence (iii) calculate feature function values which measure different kinds of syntactic divergence (iv) apply a model that combines the feature function values to score each of the 100-best parses (v) pick the best parse according to the model 3 Model We use a log-linear model to choose the best English parse." ></td>
	<td class="line x" title="48:247	The feature functions are functions on the hypothesized English parse e, the German parse g, and the word alignment a, and they assign a score (varying between 0 and infinity) that measures syntactic divergence." ></td>
	<td class="line x" title="49:247	The alignment of a sentence pair is a function that, for each English word, returns a set of German words that the English word is aligned with as shown here for the sentence pair from section 2: Er sah ein Baby und eine Frau , die graue Haare hatte He{1} saw{2} a{3} baby{4} and{5} a{6} woman{7} who{9} had{12} gray{10} hair{11} Feature function values are calculated either by taking the negative log of a probability, or by using a heuristic function which scales in a similar fash2The test set is very challenging, containing English sentences of up to 99 tokens." ></td>
	<td class="line x" title="50:247	283 ion3." ></td>
	<td class="line x" title="51:247	The form of the log-linear model is shown in eq." ></td>
	<td class="line x" title="52:247	1." ></td>
	<td class="line x" title="53:247	There are M feature functions h1,,hM." ></td>
	<td class="line x" title="54:247	The vector  is used to control the contribution of each feature function." ></td>
	<td class="line x" title="55:247	p(e|g,a) = exp( summationtext i ihi(e,g,a))summationtext e exp( summationtext i ihi(e,g,a)) (1) Given a vector of weights , the best English parse e can be found by solving eq." ></td>
	<td class="line x" title="56:247	2." ></td>
	<td class="line x" title="57:247	The model is trained by finding the weight vector  which maximizes accuracy (see section 4)." ></td>
	<td class="line x" title="58:247	e = argmax e p(e|g,a) = argmin e exp( summationdisplay i ihi(e,g,a)) (2) 3.1 Feature Functions The basic idea behind our feature functions is that any constituent in a sentence should play approximately the same syntactic role and have a similar span as the corresponding constituent in a translation." ></td>
	<td class="line x" title="59:247	If there is an obvious disagreement, it is probably caused by wrong attachment or other syntactic mistakes in parsing." ></td>
	<td class="line x" title="60:247	Sometimes in translation the syntactic role of a given semantic constitutent changes; we assume that our model penalizes all hypothesized parses equally in this case." ></td>
	<td class="line x" title="61:247	For the initial experiments, we used a set of 34 probabilistic and heuristic feature functions." ></td>
	<td class="line x" title="62:247	BitParLogProb (the only monolingual feature) is the negative log probability assigned by BitPar to the English parse." ></td>
	<td class="line x" title="63:247	If we set 1 = 1 and i = 0 for all i negationslash= 1 and evaluate eq." ></td>
	<td class="line x" title="64:247	2, we will select the parse ranked best by BitPar." ></td>
	<td class="line x" title="65:247	In order to define our feature functions, we first introduce auxiliary functions operating on individual word positions or sets of word positions." ></td>
	<td class="line x" title="66:247	Alignment functions take an alignment a as an argument." ></td>
	<td class="line x" title="67:247	In the descriptions of these functions we omit a as it is held constant for a sentence pair (i.e., an English sentence and its German translation)." ></td>
	<td class="line x" title="68:247	f(i) returns the set of word positions of German words aligned with an English word at position i. f(i) returns the leftmost word position of the German words aligned with an English word at position i, or zero if the English word is unaligned." ></td>
	<td class="line x" title="69:247	f1(i) returns the set of positions of English 3For example, a probability of 1 is a feature value of 0, while a low probability is a feature value which is 0." ></td>
	<td class="line x" title="70:247	words aligned with a German word at position i. f1(i) returns the leftmost word position of the English words aligned with a German word at position i, or zero if the German word is unaligned." ></td>
	<td class="line x" title="71:247	We overload the above functions to allow the argument i to be a set, in which case union is used, for example, f(i) = jif(j)." ></td>
	<td class="line x" title="72:247	Positions in a tree are denoted with integers." ></td>
	<td class="line x" title="73:247	First, the POS tags are numbered from 1 to the length of the sentence (i.e., the same as the word positions)." ></td>
	<td class="line x" title="74:247	Constituents higher in the tree are also indexed using consecutive integers." ></td>
	<td class="line x" title="75:247	We refer to the constituent that has been assigned index i in the tree t as constituent i in tree t or simply as constituent i." ></td>
	<td class="line x" title="76:247	The following functions have the English and German trees as an implicit argument; it should be obvious from the argument to the function whether the index i refers to the German tree or the English tree." ></td>
	<td class="line x" title="77:247	When we say constituents, we include nodes on the POS level of the tree." ></td>
	<td class="line x" title="78:247	Our syntactic trees are annotated with a syntactic head for each constituent." ></td>
	<td class="line x" title="79:247	Finally, the tag at position 0 is NULL." ></td>
	<td class="line x" title="80:247	mid2sib(i) returns 0 if i is 0, returns 1 if i has exactly two siblings, one on the left of i and one on the right, and otherwise returns 0." ></td>
	<td class="line x" title="81:247	head(i) returns the index of the head of i. The head of a POS tag is its own position." ></td>
	<td class="line x" title="82:247	tag(i) returns the tag of i. left(i) returns the index of the leftmost sibling of i. right(i) returns the index of the rightmost sibling." ></td>
	<td class="line x" title="83:247	up(i) returns the index of is parent." ></td>
	<td class="line x" title="84:247	(i) returns the set of word positions covered by i. If i is a set,  returns all word positions between the leftmost position covered by any constituent in the set and the rightmost position covered by any constituent in the set (inclusive)." ></td>
	<td class="line x" title="85:247	n(A) returns the size of the set A. c(A) returns the number of characters (including punctuation and excluding spaces) covered by the constituents in set A. llbracketpirrbracket is 1 if pi is true, and 0 otherwise." ></td>
	<td class="line x" title="86:247	l and m are the lengths in words of the English and German sentences, respectively." ></td>
	<td class="line x" title="87:247	3.1.1 Count Feature Functions Feature CrdBin counts binary events involving the heads of coordinated phrases." ></td>
	<td class="line x" title="88:247	If in the English parse we have a coordination where the English CC is aligned only with a German KON, and both have two siblings, then the value contributed to CrdBin is 1 (indicating a constraint violation) un284 less the head of the English left conjunct is aligned with the head of the German left conjunct and likewise the right conjuncts are aligned." ></td>
	<td class="line x" title="89:247	Eq." ></td>
	<td class="line x" title="90:247	3 calculates the value of CrdBin." ></td>
	<td class="line x" title="91:247	lsummationdisplay i=1 llbracket(tag(i) = CCrrbracketllbracket(n(f(i)) = 1rrbracket mid2sib(i) mid2sib(f(i)) llbrackettag(f(i)) = KON-CDrrbracket llbracket[head(left(f(i))) negationslash= f(head(left(i)))] OR [head(right(f(i))) negationslash= f(head(right(i)))]rrbracket (3) Feature Q simply captures a mismatch between questions and statements." ></td>
	<td class="line x" title="92:247	If an English sentence is parsed as a question but the parallel German sentence is not, or vice versa, the feature value is 1; otherwise the value is 0." ></td>
	<td class="line x" title="93:247	3.1.2 Span Projection Feature Functions Span projection features calculate the percentage difference between a constituents span and the span of its projection." ></td>
	<td class="line x" title="94:247	Span size is measured in characters or words." ></td>
	<td class="line x" title="95:247	To project a constituent in a parse, we use the word alignment to project all word positions covered by the constituent and then look for the smallest covering constituent in the parse of the parallel sentence." ></td>
	<td class="line x" title="96:247	CrdPrj is a feature that measures the divergence in the size of coordination constituents and their projections." ></td>
	<td class="line x" title="97:247	If we have a constituent (XP1 CC XP2) in English that is projected to a German coordination, we expect the English and German left conjuncts to span a similar percentage of their respective sentences, as should the right conjuncts." ></td>
	<td class="line x" title="98:247	The feature computes a character-based percentage difference as shown in eq." ></td>
	<td class="line x" title="99:247	4." ></td>
	<td class="line x" title="100:247	lsummationdisplay i=1 llbrackettag(i) = CCrrbracketllbracketn(f(i)) = 1rrbracket (4) llbrackettag(f(i)) = KON-CDrrbracket mid2sib(i)mid2sib(f(i)) (|c((left(i)))r  c((left(f (i)))) s | +|c((right(i)))r  c((right(f (i)))) s |) r and s are the lengths in characters of the English and German sentences, respectively." ></td>
	<td class="line x" title="101:247	In the English parse in figure 1, the left conjunct has 5 characters and the right conjunct has 6, while in figure 2 the left conjunct has 5 characters and the right conjunct has 20." ></td>
	<td class="line x" title="102:247	In the German parse (figure 3) the left conjunct has 7 characters and the right conjunct has 27." ></td>
	<td class="line x" title="103:247	Finally, r = 33 and s = 42." ></td>
	<td class="line x" title="104:247	Thus, the value of CrdPrj is 0.48 for the first hypothesized parse and 0.05 for the second, which captures the higher divergence of the first English parse from the German parse." ></td>
	<td class="line x" title="105:247	POSParentPrj is based on computing the span difference between all the parent constituents of POS tags in a German parse and their respective coverage in the corresponding hypothesized parse." ></td>
	<td class="line x" title="106:247	The feature value is the sum of all the differences." ></td>
	<td class="line x" title="107:247	POSPar(i) is true if i immediately dominates a POS tag." ></td>
	<td class="line x" title="108:247	The projection direction is from German to English, and the feature computes a percentage difference which is character-based." ></td>
	<td class="line x" title="109:247	The value of the feature is calculated in eq." ></td>
	<td class="line x" title="110:247	5, where M is the number of constituents (including POS tags) in the German tree." ></td>
	<td class="line x" title="111:247	Msummationdisplay i=1 llbracketPOSPar(i)rrbracket|c((i))s  c((f 1((i)))) r | (5) The right conjunct in figure 3 is a POSParent that corresponds to the coordination NP in figure 1, contributing a score of 0.21, and to the right conjunct in figure 2, contributing a score of 0.04." ></td>
	<td class="line x" title="112:247	For the two parses of the full sentences containing the NPs in figure 1 and figure 2, we sum over 7 POSParents and get a value of 0.27 for parse 1 and 0.11 for parse 2." ></td>
	<td class="line x" title="113:247	The lower value for parse 2 correctly captures the fact that the first English parse has higher divergence than the second due to incorrect high attachment." ></td>
	<td class="line x" title="114:247	AbovePOSPrj is similar to POSParentPrj, but it is word-based and the projection direction is from English to German." ></td>
	<td class="line x" title="115:247	Unlike POSParentPrj the feature value is calculated over all constituents above the POS level in the English tree." ></td>
	<td class="line x" title="116:247	Another span projection feature function is DTNNPrj, which projects English constituents of the form (NP(DT)(NN))." ></td>
	<td class="line x" title="117:247	DTNN(i) is true if i is an NP immediately dominating only DT and NN." ></td>
	<td class="line x" title="118:247	The feature computes a percentage difference which is word-based, shown in eq." ></td>
	<td class="line x" title="119:247	6." ></td>
	<td class="line x" title="120:247	Lsummationdisplay i=1 llbracketDTNN(i)rrbracket|n((i))l  n((f((i))))m | (6) L is the number of constituents in the English tree." ></td>
	<td class="line x" title="121:247	This feature is designed to disprefer parses 285 where constituents starting with DT NN, e.g., (NP (DT NN NN NN)), are incorrectly split into two NPs, e.g., (NP (DT NN)) and (NP (NN NN))." ></td>
	<td class="line x" title="122:247	This feature fires in this case, and projects the (NP (DT NN)) into German." ></td>
	<td class="line x" title="123:247	If the German projection is a surprisingly large number of words (as should be the case if the German also consists of a determiner followed by several nouns) then the penalty paid by this feature is large." ></td>
	<td class="line x" title="124:247	This feature is important as (NP (DT NN)) is a very common construction." ></td>
	<td class="line x" title="125:247	3.1.3 Probabilistic Feature Functions We use Europarl (Koehn, 2005), from which we extract a parallel corpus of approximately 1.22 million sentence pairs, to estimate the probabilistic feature functions described in this section." ></td>
	<td class="line x" title="126:247	For the PDepth feature, we estimate English parse depth probability conditioned on German parse depth from Europarl by calculating a simple probability distribution over the 1-best parse pairs for each parallel sentence." ></td>
	<td class="line x" title="127:247	A very deep German parse is unlikely to correspond to a flat English parse and we can penalize such a parse using PDepth." ></td>
	<td class="line x" title="128:247	The index i refers to a sentence pair in Europarl, as does j. Let li and mi be the depths of the top BitPar ranked parses of the English and German sentences, respectively." ></td>
	<td class="line x" title="129:247	We calculate the probability of observing an English tree of depth l given German tree of depth m as the maximum likelihood estimate, shown in eq." ></td>
	<td class="line x" title="130:247	7, where (z,z) = 1 if z = z and 0 otherwise." ></td>
	<td class="line x" title="131:247	To avoid noisy feature values due to outliers and parse errors, we bound the value of PDepth at 5 as shown in eq." ></td>
	<td class="line x" title="132:247	84." ></td>
	<td class="line x" title="133:247	p(l|m) = summationtext i (l ,li)(m,mi) summationtext j (m,mj) (7) min(5,log10(p(l|m))) (8) The full parse of the sentence containing the English high attachment has a parse depth of 8 while the full parse of the sentence containing the English low attachment has a depth of 9." ></td>
	<td class="line x" title="134:247	Their feature values given the German parse depth of 6 are log10(0.12) = 0.93 and log10(0.14) = 0.84." ></td>
	<td class="line x" title="135:247	The wrong parse is assigned a higher feature value indicating its higher divergence." ></td>
	<td class="line x" title="136:247	The feature PTagEParentGPOSGParent measures tagging inconsistency based on estimating 4Throughout this paper, assume log(0) =." ></td>
	<td class="line x" title="137:247	the probability that for an English word at position i, the parent of its POS tag has a particular label." ></td>
	<td class="line x" title="138:247	The feature value is calculated in eq." ></td>
	<td class="line x" title="139:247	10." ></td>
	<td class="line x" title="140:247	q(i,j) = p(tag(up(i))|tag(j),tag(up(j))) (9) lsummationdisplay i=1 min(5, summationtext jf(i) log10(q(i,j)) n(f(i)) ) (10) Consider (S(NP(NN fruit))(VP(V flies))) and (NP(NN fruit)(NNS flies)) with the translation (NP(NNS Fruchtfliegen))." ></td>
	<td class="line x" title="141:247	Assume that fruit and flies are aligned with the German compound noun Fruchtfliegen." ></td>
	<td class="line x" title="142:247	In the incorrect English parse the parent of the POS of fruit is NP and the parent of the POS of flies is VP, while in the correct parse the parent of the POS of fruit is NP and the parent of the POS of flies is NP." ></td>
	<td class="line x" title="143:247	In the German parse the compound noun is POS-tagged as an NNS and the parent is an NP." ></td>
	<td class="line x" title="144:247	The probabilities considered for the two English parses are p(NP|NNS,NP) for fruit in both parses, p(VP|NNS,NP) for flies in the incorrect parse, and p(NP|NNS,NP) for flies in the correct parse." ></td>
	<td class="line x" title="145:247	A German NNS in an NP has a higher probability of being aligned with a word in an English NP than with a word in an English VP, so the second parse will be preferred." ></td>
	<td class="line x" title="146:247	As with the PDepth feature, we use relative frequency to estimate this feature." ></td>
	<td class="line x" title="147:247	When an English word is aligned with two words, estimation is more complex." ></td>
	<td class="line x" title="148:247	We heuristically give each English and German pair one count." ></td>
	<td class="line x" title="149:247	The value calculated by the feature function is the geometric mean5 of the pairwise probabilities, see eq." ></td>
	<td class="line x" title="150:247	10." ></td>
	<td class="line x" title="151:247	3.1.4 Other Features Our best system uses the nine features we have described in detail so far." ></td>
	<td class="line x" title="152:247	In addition, we implemented the following 25 other features, which did not improve performance (see section 7): (i) 7 ptag features similar to PTagEParentGPOSGParent but predicting and conditioning on different combinations of tags (POS tag, parent of POS, grandparent of POS) (ii) 10 prj features similar to POSParentPrj measuring different combinations of character and word percentage differences at the POS parent and 5Each English word has the same weight regardless of whether it was aligned with one or with more German words." ></td>
	<td class="line x" title="153:247	286 POS grandparent levels, projecting from both English and German (iii) 3 variants of the DTNN feature function (iv) A NPPP feature function, similar to the DTNN feature function but trying to counteract a bias towards (NP (NP) (PP)) units (v) A feature function which penalizes aligning clausal units to non-clausal units (vi) The BitPar rank 4 Training Log-linear models are often trained using the Maximum Entropy criterion, but we train our model directly to maximize F1." ></td>
	<td class="line x" title="154:247	We score F1 by comparing hypothesized parses for the discriminative training set with the gold standard." ></td>
	<td class="line x" title="155:247	To try to find the optimal  vector, we perform direct accuracy maximization, meaning that we search for the  vector which directly optimizes F1 on the training set." ></td>
	<td class="line x" title="156:247	Och (2003) has described an efficient exact onedimensional accuracy maximization technique for a similar search problem in machine translation." ></td>
	<td class="line x" title="157:247	The technique involves calculating an explicit representation of the piecewise constant function gm(x) which evaluates the accuracy of the hypotheses which would be picked by eq." ></td>
	<td class="line x" title="158:247	2 from a set of hypotheses if we hold all weights constant, except for the weight m, which is set to x. This is calculated in one pass over the data." ></td>
	<td class="line x" title="159:247	The algorithm for training is initialized with a choice for  and is described in figure 4." ></td>
	<td class="line x" title="160:247	The function F1() returns F1 of the parses selected using ." ></td>
	<td class="line x" title="161:247	Due to space we do not describe step 8 in detail (see (Och, 2003))." ></td>
	<td class="line x" title="162:247	In step 9 the algorithm performs approximate normalization, where feature weights are forced towards zero." ></td>
	<td class="line x" title="163:247	The implementation of step 9 is straight-forward given the M explicit functions gm(x) created in step 8." ></td>
	<td class="line x" title="164:247	5 Data and Experiments We used the subset of the Wall Street Journal investigated in (Atterer and Schutze, 2007) for our experiments, which consists of all sentences that have at least one prepositional phrase attachment ambiguity." ></td>
	<td class="line x" title="165:247	This difficult subset of sentences seems particularly interesting when investigating the potential of information in bitext for improving parsing performance." ></td>
	<td class="line x" title="166:247	The first 500 sentences of this set were translated from English to German by a graduate student and an additional 3218 sen1: Algorithm TRAIN() 2: repeat 3: add  to the set s 4: let t be a set of 1000 randomly generated vectors 5: let  = argmax(st) F1() 6: let  =  7: repeat 8: repeatedly run one-dimensional error minimization step (updating a single scalar of the vector ) until no further error reduction 9: adjust each scalar of  in turn towards 0 such that there is no increase in error (if possible) 10: until no scalar in  changes in last two steps (8 and 9) 11: until  =  12: return  Figure 4: Sketch of the training algorithm tences by a translation bureau." ></td>
	<td class="line x" title="167:247	We withheld these 3718 English sentences (and an additional 1000 reserved sentences) when we trained BitPar on the Penn treebank." ></td>
	<td class="line x" title="168:247	Parses." ></td>
	<td class="line x" title="169:247	We use the BitPar parser (Schmid, 2004) which is based on a bit-vector implementation (cf.(Graham et al., 1980)) of the Cocke-Younger-Kasami algorithm (Kasami, 1965; Younger, 1967)." ></td>
	<td class="line x" title="171:247	It computes a compact parse forest for all possible analyses." ></td>
	<td class="line x" title="172:247	As all possible analyses are computed, any number of best parses can be extracted." ></td>
	<td class="line x" title="173:247	In contrast, other treebank parsers use sophisticated search strategies to find the most probable analysis without examining the set of all possible analyses (Charniak et al., 1998; Klein and Manning, 2003)." ></td>
	<td class="line x" title="174:247	BitPar is particularly useful for N-best parsing as the N-best parses can be computed efficiently." ></td>
	<td class="line x" title="175:247	For the 3718 sentences in the translated set, we created 100-best English parses and 1-best German parses." ></td>
	<td class="line x" title="176:247	The German parser was trained on the TIGER treebank." ></td>
	<td class="line x" title="177:247	For the Europarl corpus, we created 1-best parses for both languages." ></td>
	<td class="line x" title="178:247	Word Alignment." ></td>
	<td class="line x" title="179:247	We use a word alignment of the translated sentences from the Penn treebank, as well as a word alignment of the Europarl corpus." ></td>
	<td class="line x" title="180:247	We align these two data sets together with data from the JRC Acquis (Steinberger et al., 2006) to try to obtain better quality alignments (it is well known that alignment quality improves as the amount of data increases (Fraser and Marcu, 2007))." ></td>
	<td class="line x" title="181:247	We aligned approximately 3.08 million sentence pairs." ></td>
	<td class="line x" title="182:247	We tried to obtain better alignment quality as alignment quality is a problem in many cases where syntactic projection would otherwise work well (Fossum and Knight, 2008)." ></td>
	<td class="line x" title="183:247	287 System Train +base Test +base 1 Baseline 87.89 87.89 2 Contrastive 88.70 0.82 88.45 0.56 (5 trials/fold) 3 Contrastive 88.82 0.93 88.55 0.66 (greedy selection) Table 1: Average F1 of 7-way cross-validation To generate the alignments, we used Model 4 (Brown et al., 1993), as implemented in GIZA++ (Och and Ney, 2003)." ></td>
	<td class="line x" title="184:247	As is standard practice, we trained Model 4 with English as the source language, and then trained Model 4 with German as the source language, resulting in two Viterbi alignments." ></td>
	<td class="line x" title="185:247	These were combined using the Grow Diag Final And symmetrization heuristic (Koehn et al., 2003)." ></td>
	<td class="line x" title="186:247	Experiments." ></td>
	<td class="line x" title="187:247	We perform 7-way crossvalidation on 3718 sentences." ></td>
	<td class="line x" title="188:247	In each fold of the cross-validation, the training set is 3186 sentences, while the test set is 532 sentences." ></td>
	<td class="line x" title="189:247	Our results are shown in table 1." ></td>
	<td class="line x" title="190:247	In row 1, we take the hypothesis ranked best by BitPar." ></td>
	<td class="line x" title="191:247	In row 2, we train using the algorithm outlined in section 4." ></td>
	<td class="line x" title="192:247	To cancel out any effect caused by a particularly effective or ineffective starting  value, we perform 5 trials each time." ></td>
	<td class="line x" title="193:247	Columns 3 and 5 report the improvement over the baseline on train and test respectively." ></td>
	<td class="line x" title="194:247	We reach an improvement of 0.56 over the baseline using the algorithm as described in section 4." ></td>
	<td class="line x" title="195:247	Our initial experiments used many highly correlated features." ></td>
	<td class="line x" title="196:247	For our next experiment we use greedy feature selection." ></td>
	<td class="line x" title="197:247	We start with a  vector that is zero for all features, and then run the error minimization without the random generation of vectors (figure 4, line 4)." ></td>
	<td class="line x" title="198:247	This means that we add one feature at a time." ></td>
	<td class="line x" title="199:247	This greedy algorithm winds up producing a vector with many zero weights." ></td>
	<td class="line x" title="200:247	In row 3 of table 1, we used the greedy feature selection algorithm and trained using F1, resulting in a performance of 0.66 over the baseline which is our best result." ></td>
	<td class="line x" title="201:247	We performed a planned one-tailed paired t-test on the F1 scores of the parses selected by the baseline and this system for the 3718 sentences (parses were taken from the test portion of each fold)." ></td>
	<td class="line x" title="202:247	We found that there is a significant difference with the baseline (t(3717) = 6.42, p < .01)." ></td>
	<td class="line x" title="203:247	We believe that using the full set of 34 features (many of which are very similar to one another) made the training problem harder without improving the fit to the training data, and that greedy feature selection helps with this (see also section 7)." ></td>
	<td class="line x" title="204:247	6 Previous Work As we mentioned in section 2, work on parse reranking is relevant, but a vital difference is that we use features based only on syntactic projection of the two languages in a bitext." ></td>
	<td class="line x" title="205:247	For an overview of different types of features that have been used in parse reranking see Charniak and Johnson (2005)." ></td>
	<td class="line x" title="206:247	Like Collins (2000) we use cross-validation to train our model, but we have access to much less data (3718 sentences total, which is less than 1/10 of the data Collins used)." ></td>
	<td class="line x" title="207:247	We use rich feature functions which were designed by hand to specifically address problems in English parses which can be disambiguated using the German translation." ></td>
	<td class="line x" title="208:247	Syntactic projection has been used to bootstrap treebanks in resource poor languages." ></td>
	<td class="line x" title="209:247	Some examples of projection of syntactic parses from English to a resource poor language for which no parser is available are the works of Yarowsky and Ngai (2001), Hwa et al.(2005) and Goyal and Chatterjee (2006)." ></td>
	<td class="line x" title="211:247	Our work differs from theirs in that we are performing a parse reranking task in English using knowledge gained from German parses, and parsing accuracy is generally thought to be worse in German than in English." ></td>
	<td class="line x" title="212:247	Hopkins and Kuhn (2006) conducted research with goals similar to ours." ></td>
	<td class="line x" title="213:247	They showed how to build a powerful generative model which flexibly incorporates features from parallel text in four languages, but were not able to show an improvement in parsing performance." ></td>
	<td class="line x" title="214:247	After the submission of our paper for review, two papers outlining relevant work were published." ></td>
	<td class="line x" title="215:247	Burkett and Klein (2008) describe a system for simultaneously improving Chinese and English parses of a Chinese/English bitext." ></td>
	<td class="line x" title="216:247	This work is complementary to ours." ></td>
	<td class="line x" title="217:247	The system is trained using gold standard trees in both Chinese and English, in contrast with our system which only has access to gold standard trees in English." ></td>
	<td class="line x" title="218:247	Their system uses a tree alignment which varies within training, but this does not appear to make a large difference in performance." ></td>
	<td class="line x" title="219:247	They use coarsely defined features which are language independent." ></td>
	<td class="line x" title="220:247	We use several features similar to their two best performing sets of features, but in contrast with their work, we also define features which are specifically aimed at English disambiguation problems that we have observed can be resolved 288 using German parses." ></td>
	<td class="line x" title="221:247	They use an in-domain Chinese parser and out-of-domain English parser, while for us the English parser is in-domain and the German parser is out-of-domain, both of which make improving the English parse more difficult." ></td>
	<td class="line x" title="222:247	Their Maximum Entropy training is more appropriate for their numerous coarse features, while we use Minimum Error Rate Training, which is much faster." ></td>
	<td class="line x" title="223:247	Finally, we are projecting from a single German parse which is a more difficult problem." ></td>
	<td class="line x" title="224:247	Fossum and Knight (2008) outline a system for using Chinese/English word alignments to determine ambiguous English PP-attachments." ></td>
	<td class="line x" title="225:247	They first use an oracle to choose PP-attachment decisions which are ambiguous in the English side of a Chinese/English bitext, and then build a classifier which uses information from a word alignment to make PP-attachment decisions." ></td>
	<td class="line x" title="226:247	No Chinese syntactic information is required." ></td>
	<td class="line x" title="227:247	We use automatically generated German parses to improve English syntactic parsing, and have not been able to find a similar phenomenon for which only a word alignment would suffice." ></td>
	<td class="line x" title="228:247	7 Analysis We looked at the weights assigned during the cross-validation performed to obtain our best result." ></td>
	<td class="line x" title="229:247	The weights of many of the 34 features we defined were frequently set to zero." ></td>
	<td class="line x" title="230:247	We sorted the features by the number of times the relevant  scalar was zero (i.e., the number of folds of the cross-validation for which they were zero; the greedy feature selection is deterministic and so we do not run multiple trials)." ></td>
	<td class="line x" title="231:247	We then reran the same greedy feature selection algorithm as was used in table 1, row 3, but this time using only the top 9 feature values, which were the features which were active on 4 or more folds6." ></td>
	<td class="line x" title="232:247	The result was an improvement on train of 0.84 and an improvement on test of 0.73." ></td>
	<td class="line x" title="233:247	This test result may be slightly overfit, but the result supports the inference that these 9 feature functions are the most important." ></td>
	<td class="line x" title="234:247	We chose these feature functions to be described in detail in section 3." ></td>
	<td class="line x" title="235:247	We observed that the variants of the similar features POSParentPrj and AbovePOSPrj projected in opposite directions and measured character and word differences, respectively, and this complementarity seems to help." ></td>
	<td class="line x" title="236:247	6We saw that many features canceled one another out on different folds." ></td>
	<td class="line x" title="237:247	For instance either the word-based or the character-based version of DTNN was active in each fold, but never at the same time as one another." ></td>
	<td class="line x" title="238:247	We also tried to see if our results depended strongly on the log-linear model and training algorithm, by using the SVM-Light ranker (Joachims, 2002)." ></td>
	<td class="line x" title="239:247	In order to make the experiment tractable, we limited ourselves to the 8-best parses (rather than 100-best)." ></td>
	<td class="line x" title="240:247	Our training algorithm and model was 0.74 better than the baseline on train and 0.47 better on test, while SVM-Light was 0.54 better than baseline on train and 0.49 better on test (using linear kernels)." ></td>
	<td class="line x" title="241:247	We believe that the results are not unduly influenced by the training algorithm." ></td>
	<td class="line x" title="242:247	8 Conclusion We have shown that rich bitext projection features can improve parsing accuracy." ></td>
	<td class="line x" title="243:247	This confirms the hypothesis that the divergence in what information different languages encode grammatically can be exploited for syntactic disambiguation." ></td>
	<td class="line x" title="244:247	Improved parsing due to bitext projection features should be helpful in syntactic analysis of bitexts (by way of mutual syntactic disambiguation) and in computing syntactic analyses of texts that have translations in other languages available." ></td>
	<td class="line x" title="245:247	Acknowledgments This work was supported in part by Deutsche Forschungsgemeinschaft Grant SFB 732." ></td>
	<td class="line x" title="246:247	We would like to thank Helmut Schmid for support of BitPar and for his many helpful comments on our work." ></td>
	<td class="line x" title="247:247	We would also like to thank the anonymous reviewers." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E09-1090
Fast Full Parsing by Linear-Chain Conditional Random Fields
Tsuruoka, Yoshimasa;Tsujii, Jun'ichi;Ananiadou, Sophia;"></td>
	<td class="line x" title="1:207	Proceedings of the 12th Conference of the European Chapter of the ACL, pages 790798, Athens, Greece, 30 March  3 April 2009." ></td>
	<td class="line x" title="2:207	c2009 Association for Computational Linguistics Fast Full Parsing by Linear-Chain Conditional Random Fields Yoshimasa Tsuruoka Junichi Tsujii Sophia Ananiadou  School of Computer Science, University of Manchester, UK  National Centre for Text Mining (NaCTeM), UK  Department of Computer Science, University of Tokyo, Japan {yoshimasa.tsuruoka,j.tsujii,sophia.ananiadou}@manchester.ac.uk Abstract This paper presents a chunking-based discriminative approach to full parsing." ></td>
	<td class="line x" title="3:207	We convert the task of full parsing into a series of chunking tasks and apply a conditional random field (CRF) model to each level of chunking." ></td>
	<td class="line x" title="4:207	The probability of an entire parse tree is computed as the product of the probabilities of individual chunking results." ></td>
	<td class="line x" title="5:207	The parsing is performed in a bottom-up manner and the best derivation is efficiently obtained by using a depthfirst search algorithm." ></td>
	<td class="line x" title="6:207	Experimental results demonstrate that this simple parsing framework produces a fast and reasonably accurate parser." ></td>
	<td class="line x" title="7:207	1 Introduction Full parsing analyzes the phrase structure of a sentence and provides useful input for many kinds of high-level natural language processing such as summarization (Knight and Marcu, 2000), pronoun resolution (Yang et al., 2006), and information extraction (Miyao et al., 2008)." ></td>
	<td class="line x" title="8:207	One of the major obstacles that discourage the use of full parsing in large-scale natural language processing applications is its computational cost." ></td>
	<td class="line x" title="9:207	For example, the MEDLINE corpus, a collection of abstracts of biomedical papers, consists of 70 million sentences and would require more than two years of processing time if the parser needs one second to process a sentence." ></td>
	<td class="line x" title="10:207	Generative models based on lexicalized PCFGs enjoyed great success as the machine learning framework for full parsing (Collins, 1999; Charniak, 2000), but recently discriminative models attract more attention due to their superior accuracy (Charniak and Johnson, 2005; Huang, 2008) and adaptability to new grammars and languages (Buchholz and Marsi, 2006)." ></td>
	<td class="line x" title="11:207	A traditional approach to discriminative full parsing is to convert a full parsing task into a series of classification problems." ></td>
	<td class="line x" title="12:207	Ratnaparkhi (1997) performs full parsing in a bottom-up and left-toright manner and uses a maximum entropy classifier to make decisions to construct individual phrases." ></td>
	<td class="line x" title="13:207	Sagae and Lavie (2006) use the shiftreduce parsing framework and a maximum entropy model for local classification to decide parsing actions." ></td>
	<td class="line x" title="14:207	These approaches are often called history-based approaches." ></td>
	<td class="line x" title="15:207	A more recent approach to discriminative full parsing is to treat the task as a single structured prediction problem." ></td>
	<td class="line x" title="16:207	Finkel et al.(2008) incorporated rich local features into a tree CRF model and built a competitive parser." ></td>
	<td class="line x" title="18:207	Huang (2008) proposed to use a parse forest to incorporate non-local features." ></td>
	<td class="line x" title="19:207	They used a perceptron algorithm to optimize the weights of the features and achieved state-of-the-art accuracy." ></td>
	<td class="line x" title="20:207	Petrov and Klein (2008) introduced latent variables in tree CRFs and proposed a caching mechanism to speed up the computation." ></td>
	<td class="line x" title="21:207	In general, the latter whole-sentence approaches give better accuracy than history-based approaches because they can better trade off decisions made in different parts in a parse tree." ></td>
	<td class="line x" title="22:207	However, the whole-sentence approaches tend to require a large computational cost both in training and parsing." ></td>
	<td class="line x" title="23:207	In contrast, history-based approaches are less computationally intensive and usually produce fast parsers." ></td>
	<td class="line x" title="24:207	In this paper, we present a history-based parser using CRFs, by treating the task of full parsing as a series of chunking problems where it recognizes chunks in a flat input sequence." ></td>
	<td class="line x" title="25:207	We use the linear790 Estimated  volume  was   a   light  2.4  million  ounces  . VBN         NN    VBD DT  JJ    CD     CD NNS   . QPNP Figure 1: Chunking, the first (base) level." ></td>
	<td class="line x" title="26:207	volume          was   a   light    million       ounces . NP             VBD DT  JJ          QP            NNS   . NP Figure 2: Chunking, the 2nd level." ></td>
	<td class="line x" title="27:207	chain CRF model to perform chunking." ></td>
	<td class="line x" title="28:207	Although our parsing model falls into the category of history-based approaches, it is one step closer to the whole-sentence approaches because the parser uses a whole-sequence model (i.e. CRFs) for individual chunking tasks." ></td>
	<td class="line x" title="29:207	In other words, our parser could be located somewhere between traditional history-based approaches and whole-sentence approaches." ></td>
	<td class="line x" title="30:207	One of our motivations for this work was that our parsing model may achieve a better balance between accuracy and speed than existing parsers." ></td>
	<td class="line x" title="31:207	It is also worth mentioning that our approach is similar in spirit to supertagging for parsing with lexicalized grammar formalisms such as CCG and HPSG (Clark and Curran, 2004; Ninomiya et al., 2006), in which significant speed-ups for parsing time are achieved." ></td>
	<td class="line x" title="32:207	In this paper, we show that our approach is indeed appealing in that the parser runs very fast and gives competitive accuracy." ></td>
	<td class="line x" title="33:207	We evaluate our parser on the standard data set for parsing experiments (i.e. the Penn Treebank) and compare it with existing approaches to full parsing." ></td>
	<td class="line x" title="34:207	This paper is organized as follows." ></td>
	<td class="line x" title="35:207	Section 2 presents the overall chunk parsing strategy." ></td>
	<td class="line x" title="36:207	Section 3 describes the CRF model used to perform individual chunking steps." ></td>
	<td class="line x" title="37:207	Section 4 describes the depth-first algorithm for finding the best derivation of a parse tree." ></td>
	<td class="line x" title="38:207	The part-of-speech tagger used in the parser is described in section 5." ></td>
	<td class="line x" title="39:207	Experimental results on the Penn Treebank corpus are provided in Section 6." ></td>
	<td class="line x" title="40:207	Section 7 discusses possible improvements and extensions of our work." ></td>
	<td class="line x" title="41:207	Section 8 offers some concluding remarks." ></td>
	<td class="line x" title="42:207	volume          was                    ounces          . NP             VBD                    NP           . VP Figure 3: Chunking, the 3rd level." ></td>
	<td class="line x" title="43:207	volume                           was                   . NP                               VP                . S Figure 4: Chunking, the 4th level." ></td>
	<td class="line x" title="44:207	2 Full Parsing by Chunking This section describes the parsing framework employed in this work." ></td>
	<td class="line x" title="45:207	The parsing process is conceptually very simple." ></td>
	<td class="line x" title="46:207	The parser first performs chunking by identifying base phrases, and converts the identified phrases to non-terminal symbols." ></td>
	<td class="line x" title="47:207	It then performs chunking for the updated sequence and converts the newly recognized phrases into non-terminal symbols." ></td>
	<td class="line x" title="48:207	The parser repeats this process until the whole sequence is chunked as a sentence Figures 1 to 4 show an example of a parsing process by this framework." ></td>
	<td class="line x" title="49:207	In the first (base) level, the chunker identifies two base phrases, (NP Estimated volume) and (QP 2.4 million), and replaces each phrase with its non-terminal symbol and head1." ></td>
	<td class="line x" title="50:207	In the second level, the chunker identifies a noun phrase, (NP a light million ounces), and converts it into NP." ></td>
	<td class="line x" title="51:207	This process is repeated until the whole sentence is chunked at the fourth level." ></td>
	<td class="line x" title="52:207	The full parse tree is recovered from the chunking history in a straightforward way." ></td>
	<td class="line x" title="53:207	This idea of converting full parsing into a series of chunking tasks is not new by any means the history of this kind of approach dates back to 1950s (Joshi and Hopely, 1996)." ></td>
	<td class="line x" title="54:207	More recently, Brants (1999) used a cascaded Markov model to parse German text." ></td>
	<td class="line x" title="55:207	Tjong Kim Sang (2001) used the IOB tagging method to represent chunks and memory-based learning, and achieved an f-score of 80.49 on the WSJ corpus." ></td>
	<td class="line x" title="56:207	Tsuruoka and Tsujii (2005) improved upon their approach by using 1The head word is identified by using the headpercolation table (Magerman, 1995)." ></td>
	<td class="line x" title="57:207	791  0  1000  2000  3000  4000  5000  0  5  10  15  20  25  30 # sentences Height Figure 5: Distribution of tree height in WSJ sections 2-21." ></td>
	<td class="line x" title="58:207	a maximum entropy classifier and achieved an fscore of 85.9." ></td>
	<td class="line x" title="59:207	However, there is still a large gap between the accuracy of chunking-based parsers and that of widely-used practical parsers such as Collins parser and Charniak parser (Collins, 1999; Charniak, 2000)." ></td>
	<td class="line x" title="60:207	2.1 Heights of Trees A natural question about this parsing framework is how many levels of chunking are usually needed to parse a sentence." ></td>
	<td class="line x" title="61:207	We examined the distribution of the heights of the trees in sections 2-21 of the Wall Street Journal (WSJ) corpus." ></td>
	<td class="line x" title="62:207	The result is shown in Figure 5." ></td>
	<td class="line x" title="63:207	Most of the sentences have less than 20 levels." ></td>
	<td class="line x" title="64:207	The average was 10.0, which means we need to perform, on average, 10 chunking tasks to obtain a full parse tree for a sentence if the parsing is performed in a deterministic manner." ></td>
	<td class="line x" title="65:207	3 Chunking with CRFs The accuracy of chunk parsing is highly dependent on the accuracy of each level of chunking." ></td>
	<td class="line x" title="66:207	This section describes our approach to the chunking task." ></td>
	<td class="line x" title="67:207	A common approach to the chunking problem is to convert the problem into a sequence tagging task by using the BIO (B for beginning, I for inside, and O for outside) representation." ></td>
	<td class="line x" title="68:207	For example, the chunking process given in Figure 1 is expressed as the following BIO sequences." ></td>
	<td class="line x" title="69:207	B-NP I-NP O O O B-QP I-QP O O This representation enables us to use the linearchain CRF model to perform chunking, since the task is simply assigning appropriate labels to a sequence." ></td>
	<td class="line x" title="70:207	3.1 Linear Chain CRFs A linear chain CRF defines a single log-linear probabilistic distribution over all possible tag sequences y for the input sequence x: p(y|x) = 1Z(x) exp Tsummationdisplay t=1 Ksummationdisplay k=1 kfk(t,yt,yt1,x), where fk(t,yt,yt1,x) is typically a binary function indicating the presence of feature k, k is the weight of the feature, and Z(X) is a normalization function: Z(x) =summationdisplay y exp Tsummationdisplay t=1 Ksummationdisplay k=1 kfk(t,yt,yt1,x)." ></td>
	<td class="line x" title="71:207	This model allows us to define features on states and edges combined with surface observations." ></td>
	<td class="line x" title="72:207	The weights of the features are determined in such a way that they maximize the conditional loglikelihood of the training data: L = Nsummationdisplay i=1 logp(y(i)|x(i)) + R(), where R() is introduced for the purpose of regularization which prevents the model from overfitting the training data." ></td>
	<td class="line x" title="73:207	The L1 or L2 norm is commonly used in statistical natural language processing (Gao et al., 2007)." ></td>
	<td class="line x" title="74:207	We used L1-regularization, which is defined as R() = 1C Ksummationdisplay k=1 |k|, where C is the meta-parameter that controls the degree of regularization." ></td>
	<td class="line x" title="75:207	We used the OWL-QN algorithm (Andrew and Gao, 2007) to obtain the parameters that maximize the L1-regularized conditional log-likelihood." ></td>
	<td class="line x" title="76:207	3.2 Features Table 1 shows the features used in chunking for the base level." ></td>
	<td class="line x" title="77:207	Since the task is basically identical to shallow parsing by CRFs, we follow the feature sets used in the previous work by Sha and Pereira (2003)." ></td>
	<td class="line x" title="78:207	We use unigrams, bigrams, and trigrams of part-of-speech (POS) tags and words." ></td>
	<td class="line x" title="79:207	The difference between our CRF chunker and that in (Sha and Pereira, 2003) is that we could not use second-order CRF models, hence we could not use trigram features on the BIO states." ></td>
	<td class="line x" title="80:207	We 792 Symbol Unigrams s2, s1, s0, s+1, s+2 Symbol Bigrams s2s1, s1s0, s0s+1, s+1s+2 Symbol Trigrams s3s2s1, s2s1s0, s1s0s+1, s0s+1s+2, s+1s+2s+3 Word Unigrams h2, h1, h0, h+1, h+2 Word Bigrams h2h1, h1h0, h0h+1, h+1h+2 Word Trigrams h1h0h+1 Table 1: Feature templates used in the base level chunking." ></td>
	<td class="line x" title="81:207	s represents a terminal symbol (i.e. POS tag) and the subscript represents a relative position." ></td>
	<td class="line x" title="82:207	h represents a word." ></td>
	<td class="line x" title="83:207	found that using second order CRFs in our task was very difficult because of the computational cost." ></td>
	<td class="line x" title="84:207	Recall that the computational cost for CRFs is quadratic to the number of possible states." ></td>
	<td class="line x" title="85:207	In our task, we need to consider the states for all nonterminal symbols, whereas their work is only concerned with noun phrases." ></td>
	<td class="line x" title="86:207	Table 2 shows feature templates used in the nonbase levels of chunking." ></td>
	<td class="line x" title="87:207	In the non-base levels of chunking, we can use a richer set of features than the base-level chunking because the chunker has access to the information about the partial trees that have been already created." ></td>
	<td class="line x" title="88:207	In addition to the features listed in Table 1, the chunker looks into the daughters of the current non-terminal symbol and use them as features." ></td>
	<td class="line x" title="89:207	It also uses the words and POS tags around the edges of the region covered by the current non-terminal symbol." ></td>
	<td class="line x" title="90:207	We also added a special feature to better capture PP-attachment." ></td>
	<td class="line x" title="91:207	The chunker looks at the head of the second daughter of the prepositional phrase to incorporate the semantic head of the phrase." ></td>
	<td class="line x" title="92:207	4 Searching for the Best Parse The probability for an entire parse tree is computed as the product of the probabilities output by the individual CRF chunkers: score = hproductdisplay i=0 p(yi|xi), (1) where i is the level of chunking and h is the height of the tree." ></td>
	<td class="line x" title="93:207	The task of full parsing is then to choose the series of chunking results that maximizes this probability." ></td>
	<td class="line x" title="94:207	It should be noted that there are cases where different derivations (chunking histories) lead to the same parse tree (i.e. phrase structure)." ></td>
	<td class="line x" title="95:207	Strictly speaking, therefore, what we describe here as the probability of a parse tree is actually the probability of a single derivation." ></td>
	<td class="line x" title="96:207	The probabilities of the derivations should then be marginalized over to produce the probability of a parse tree, but in this paper we ignore this effect and simply focus only on the best derivation." ></td>
	<td class="line x" title="97:207	We use a depth-first search algorithm to find the highest probability derivation." ></td>
	<td class="line x" title="98:207	Figure 6 shows the algorithm in pseudo-code." ></td>
	<td class="line x" title="99:207	The parsing process is implemented with a recursive function." ></td>
	<td class="line x" title="100:207	In each level of chunking, the recursive function first invokes a CRF chunker to obtain chunking hypotheses for the given sequence." ></td>
	<td class="line x" title="101:207	For each hypothesis whose probability is high enough to have possibility of constituting the best derivation, the function calls itself with the sequence updated by the hypothesis." ></td>
	<td class="line x" title="102:207	The parsing process is performed in a bottom up manner and this recursive process terminates if the whole sequence is chunked as a sentence." ></td>
	<td class="line x" title="103:207	To extract multiple chunking hypotheses from the CRF chunker, we use a branch-and-bound algorithm rather than the A* search algorithm, which is perhaps more commonly used in previous studies." ></td>
	<td class="line x" title="104:207	We do not give pseudo code, but the basic idea is as follows." ></td>
	<td class="line x" title="105:207	It first performs the forward Viterbi algorithm to obtain the best sequence, storing the upper bounds that are used for pruning in branch-and-bound." ></td>
	<td class="line x" title="106:207	It then performs a branch-andbound algorithm in a backward manner to retrieve possible candidate sequences whose probabilities are greater than the given threshold." ></td>
	<td class="line x" title="107:207	Unlike A* search, this method is memory efficient because it is performed in a depth-first manner and does not require priority queues for keeping uncompleted hypotheses." ></td>
	<td class="line x" title="108:207	It is straightforward to introduce beam search in this search algorithmwe simply limit the number of hypotheses generated by the CRF chunker." ></td>
	<td class="line x" title="109:207	We examine how the width of the beam affects the parsing performance in the experiments." ></td>
	<td class="line x" title="110:207	793 Symbol Unigrams s2, s1, s0, s+1, s+2 Symbol Bigrams s2s1, s1s0, s0s+1, s+1s+2 Symbol Trigrams s3s2s1, s2s1s0, s1s0s+1, s0s+1s+2, s+1s+2s+3 Head Unigrams h2, h1, h0, h+1, h+2 Head Bigrams h2h1, h1h0, h0h+1, h+1h+2 Head Trigrams h1h0h+1 Symbol & Daughters s0d01,  s0d0m Symbol & Word/POS context s0wj1, s0pj1, s0wk+1 , s0pk+1 Symbol & Words on the edges s0wj, s0wk Freshness whether s0 has been created in the level just below PP-attachment h1h0m02 (only when s0 = PP) Table 2: Feature templates used in the upper level chunking." ></td>
	<td class="line x" title="111:207	s represents a non-terminal symbol." ></td>
	<td class="line x" title="112:207	h represents a head percolated from the bottom for each symbol." ></td>
	<td class="line x" title="113:207	d0i is the ith daughter of s0." ></td>
	<td class="line x" title="114:207	wj is the first word in the range covered by s0." ></td>
	<td class="line x" title="115:207	wj1 is the word preceding wj." ></td>
	<td class="line x" title="116:207	wk is the last word in the range covered by s0." ></td>
	<td class="line x" title="117:207	wk+1 is the word following wk." ></td>
	<td class="line x" title="118:207	p represents POS tags." ></td>
	<td class="line x" title="119:207	m02 represents the head of the second daughter of s0." ></td>
	<td class="line x" title="120:207	Word Unigram w2, w1, w0, w+1, wi+2 Word Bigram w1w0, w0w+1, w1w+1 Prefix, Suffix prefixes of w0 suffixes of w0 (up to length 10) Character features w0 has a hyphen w0 has a number w0 has a capital letter w0 is all capital Normalized word N(w0) Table 3: Feature templates used in the POS tagger." ></td>
	<td class="line x" title="121:207	w represents a word and the subscript represents a relative position." ></td>
	<td class="line x" title="122:207	5 Part-of-Speech Tagging We use the CRF model also for POS tagging." ></td>
	<td class="line x" title="123:207	The CRF-based POS tagger is incorporated in the parser in exactly the same way as the other layers of chunking." ></td>
	<td class="line x" title="124:207	In other words, the POS tagging process is treated like the bottom layer of chunking, so the parser considers multiple probabilistic hypotheses output by the tagger in the search algorithm described in the previous section." ></td>
	<td class="line x" title="125:207	5.1 Features Table 3 shows the feature templates used in the POS tagger." ></td>
	<td class="line x" title="126:207	Most of them are standard features commonly used in POS tagging for English." ></td>
	<td class="line x" title="127:207	We used unigrams and bigrams of neighboring words, prefixes and suffixes of the current word, and some characteristics of the word." ></td>
	<td class="line x" title="128:207	We also normalized the current word by lowering capital letters and converting all the numerals into #, and used the normalized word as a feature." ></td>
	<td class="line x" title="129:207	6 Experiments We ran parsing experiments using the Wall Street Journal corpus." ></td>
	<td class="line x" title="130:207	Sections 2-21 were used as the training data." ></td>
	<td class="line x" title="131:207	Section 22 was used as the development data, with which we tuned the feature set and parameters for learning and parsing." ></td>
	<td class="line x" title="132:207	Section 23 was reserved for the final accuracy report." ></td>
	<td class="line x" title="133:207	The training data for the CRF chunkers were created by converting each parse tree in the training data into a list of chunking sequences like the ones presented in Figures 1 to 4." ></td>
	<td class="line x" title="134:207	We trained three CRF models, i.e., the POS tagging model, the base chunking model, and the non-base chunking model." ></td>
	<td class="line x" title="135:207	The training took about two days on a single CPU." ></td>
	<td class="line x" title="136:207	We used the evalb script provided by Sekine and Collins for evaluating the labeled recall/precision of the parser outputs2." ></td>
	<td class="line x" title="137:207	All experiments were carried out on a server with 2.2 GHz AMD Opteron processors and 16GB memory." ></td>
	<td class="line x" title="138:207	6.1 Chunking Performance First, we describe the accuracy of individual chunking processes." ></td>
	<td class="line x" title="139:207	Table 4 shows the results for the ten most frequently occurring symbols on the development data." ></td>
	<td class="line x" title="140:207	Noun phrases (NP) are the 2The script is available at http://nlp.cs.nyu.edu/evalb/." ></td>
	<td class="line x" title="141:207	We used the parameter file COLLINS.prm." ></td>
	<td class="line x" title="142:207	794 1: procedure PARSESENTENCE(x) 2: PARSE(x, 1, 0) 3: 4: function PARSE(x, p, q) 5: if x is chunked as a complete sentence 6: return p 7: H PERFORMCHUNKING(x, q/p) 8: for hH in descending order of their probabilities do 9: rph.probability 10: if r > q then 11: xUPDATESEQUENCE(x, h) 12: sPARSE(x, r, q) 13: if s > q then 14: qs 15: return q 16: 17: function PERFORMCHUNKING(x, t) 18: perform chunking with a CRF chunker and 19: return a set of chunking hypotheses whose 20: probabilities are greater than t. 21: 22: function UPDATESEQUENCE(x, h) 23: update sequence x according to chunking 24: hypothesis h and return the updated 25: sequence." ></td>
	<td class="line x" title="143:207	Figure 6: Searching for the best parse with a depth-first search algorithm." ></td>
	<td class="line x" title="144:207	This pseudo-code illustrates how to find the highest probability parse, but in the real implementation, the function needs to keep track of chunking histories as well as probabilities." ></td>
	<td class="line x" title="145:207	most common symbol and consist of 55% of all phrases." ></td>
	<td class="line x" title="146:207	The accuracy of noun phrases recognition was relatively high, but it may be useful to design special features for this particular type of phrase, considering the dominance of noun phrases in the corpus." ></td>
	<td class="line x" title="147:207	Although not directly comparable, Sha and Pereira (2003) report almost the same level of accuracy (94.38%) on noun phrase recognition, using a much smaller training set." ></td>
	<td class="line x" title="148:207	We attribute their superior performance mainly to the use of second-order features on state transitions." ></td>
	<td class="line x" title="149:207	Table 4 also suggests that adverb phrases (ADVP) and adjective phrases (ADJP) are more difficult to recognize than other types of phrases, which coincides with the result reported in (Collins, 1999)." ></td>
	<td class="line x" title="150:207	It should be noted that the performance reported in this table was evaluated using the gold standard sequences as the input to the CRF chunkers." ></td>
	<td class="line x" title="151:207	In the Symbol # Samples Recall Prec." ></td>
	<td class="line x" title="152:207	F-score NP 317,597 94.79 94.16 94.47 VP 76,281 91.46 91.98 91.72 PP 66,979 92.84 92.61 92.72 S 33,739 91.48 90.64 91.06 ADVP 21,686 84.25 85.86 85.05 ADJP 14,422 77.27 78.46 77.86 QP 14,308 89.43 91.16 90.28 SBAR 11,603 96.42 96.97 96.69 WHNP 8,827 95.54 97.50 96.51 PRT 3,391 95.72 90.52 93.05 : : : : : all 579,253 92.63 92.62 92.63 Table 4: Chunking performance (section 22, all sentences)." ></td>
	<td class="line x" title="153:207	Beam Recall Prec." ></td>
	<td class="line x" title="154:207	F-score Time (sec) 1 86.72 87.83 87.27 16 2 88.50 88.85 88.67 41 3 88.69 89.08 88.88 61 4 88.72 89.13 88.92 92 5 88.73 89.14 88.93 119 10 88.68 89.19 88.93 179 Table 5: Beam width and parsing performance (section 22, all sentences)." ></td>
	<td class="line x" title="155:207	real parsing process, the chunkers have to use the output from the previous (one level below) chunker, so the quality of the input is not as good as that used in this evaluation." ></td>
	<td class="line x" title="156:207	6.2 Parsing Performance Next, we present the actual parsing performance." ></td>
	<td class="line x" title="157:207	The first set of experiments concerns the relationship between the width of beam and the parsing performance." ></td>
	<td class="line x" title="158:207	Table 5 shows the results obtained on the development data." ></td>
	<td class="line x" title="159:207	We varied the width of the beam from 1 to 10." ></td>
	<td class="line x" title="160:207	The beam width of 1 corresponds to deterministic parsing." ></td>
	<td class="line x" title="161:207	Somewhat unexpectedly, the parsing accuracy did not drop significantly even when we reduced the beam width to a very small number such as 2 or 3." ></td>
	<td class="line x" title="162:207	One of the interesting findings was that recall scores were consistently lower than precision scores throughout all experiments." ></td>
	<td class="line x" title="163:207	A possible reason is that, since the score of a parse is defined as the product of all chunking probabilities, the parser could prefer a parse tree that consists of a small number of chunk layers." ></td>
	<td class="line x" title="164:207	This may stem 795 from the history-based models inability of properly trading off decisions made by different chunkers." ></td>
	<td class="line x" title="165:207	Overall, the parsing speed was very high." ></td>
	<td class="line x" title="166:207	The deterministic version (beam width = 1) parsed 1700 sentences in 16 seconds, which means that the parser needed only 10 msec to parse one sentence." ></td>
	<td class="line x" title="167:207	The parsing speed decreases as we increase the beam width." ></td>
	<td class="line x" title="168:207	The parser was also memory efficient." ></td>
	<td class="line x" title="169:207	Thanks to L1 regularization, the training process did not result in many non-zero feature weights." ></td>
	<td class="line x" title="170:207	The numbers of non-zero weight features were 58,505 (for the base chunker), 263,889 (for the non-base chunker), and 42,201 (for the POS tagger)." ></td>
	<td class="line x" title="171:207	The parser required only 14MB of memory to run." ></td>
	<td class="line x" title="172:207	There was little accuracy difference between the beam width of 4 and 5, so we adopted the beam width of 4 for the final accuracy report on the test data." ></td>
	<td class="line x" title="173:207	6.3 Comparison with Previous Work Table 6 shows the performance of our parser on the test data and summarizes the results of previous work." ></td>
	<td class="line x" title="174:207	Our parser achieved an f-score of 88.4 on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al.(2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005)." ></td>
	<td class="line x" title="176:207	Our parser was more accurate than traditional history-based approaches such as Sagae & Lavie (2006) and Ratnaparkhi (1997), and was significantly better than previous cascaded chunking approaches such as Tsuruoka & Tsujii (2005) and Tjong Kim Sang (2001)." ></td>
	<td class="line x" title="177:207	Although the comparison presented in the table is not perfectly fair because of the differences in hardware platforms, the results show that our parsing model is a promising addition to the parsing frameworks for building a fast and accurate parser." ></td>
	<td class="line x" title="178:207	7 Discussion One of the obvious ways to improve the accuracy of our parser is to improve the accuracy of individual CRF models." ></td>
	<td class="line x" title="179:207	As mentioned earlier, we were not able to use second-order features on state transitions, which would have been very useful, due to the problem of computational cost." ></td>
	<td class="line x" title="180:207	Incremental feature selection methods such as grafting (Perkins et al., 2003) may help us to incorporate such higher-order features, but the problem of decreased efficiency of dynamic programming in the CRF would probably need to be addressed." ></td>
	<td class="line x" title="181:207	In this work, we treated the chunking problem as a sequence labeling problem by using the BIO representation for the chunks." ></td>
	<td class="line x" title="182:207	However, semiMarkov conditional random fields (semi-CRFs) can directly handle the chunking problem by considering all possible combinations of subsequences of arbitrary length (Sarawagi and Cohen, 2004)." ></td>
	<td class="line x" title="183:207	Semi-CRFs allow one to use a richer set of features than CRFs, so the use of semi-CRFs in our parsing framework should lead to improved accuracy." ></td>
	<td class="line x" title="184:207	Moreover, semi-CRFs would allow us to incorporate some useful restrictions in producing chunking hypotheses." ></td>
	<td class="line x" title="185:207	For example, we could naturally incorporate the restriction that every chunk has to contain at least one symbol that has just been created in the previous level3." ></td>
	<td class="line x" title="186:207	It is hard for the normal CRF model to incorporate such restrictions." ></td>
	<td class="line x" title="187:207	Introducing latent variables into the CRF model may be another promising approach." ></td>
	<td class="line x" title="188:207	This is the main idea of Petrov and Klein (2008), which significantly improved parsing accuracy." ></td>
	<td class="line oc" title="189:207	A totally different approach to improving the accuracy of our parser is to use the idea of selftraining described in (McClosky et al., 2006)." ></td>
	<td class="line o" title="190:207	The basic idea is to create a larger set of training data by applying an accurate parser (e.g. reranking parser) to a large amount of raw text." ></td>
	<td class="line x" title="191:207	We can then use the automatically created treebank as the additional training data for our parser." ></td>
	<td class="line x" title="192:207	This approach suggests that accurate (but slow) parsers and fast (but not-so-accurate) parsers can actually help each other." ></td>
	<td class="line x" title="193:207	Also, since it is not difficult to extend our parser to produce N-best parsing hypotheses, one could build a fast reranking parser by using the parser as the base (hypotheses generating) parser." ></td>
	<td class="line x" title="194:207	8 Conclusion Although the idea of treating full parsing as a series of chunking problems has a long history, there has not been a competitive parser based on this parsing framework." ></td>
	<td class="line x" title="195:207	In this paper, we have demonstrated that the framework actually enables us to 3For example, the sequence VBD DT JJ in Figure 2 cannot be a chunk in the current level because it would have been already chunked in the previous level if it were." ></td>
	<td class="line x" title="196:207	796 Recall Precision F-score Time (min) This work (deterministic) 86.3 87.5 86.9 0.5 This work (search, beam width = 4) 88.2 88.7 88.4 1.7 Huang (2008) 91.7 Unk Finkel et al.(2008) 87.8 88.2 88.0 >250* Petrov & Klein (2008) 88.3 3* Sagae & Lavie (2006) 87.8 88.1 87.9 17 Charniak & Johnson (2005) 90.6 91.3 91.0 Unk Tsuruoka & Tsujii (2005) 85.0 86.8 85.9 2 Collins (1999) 88.1 88.3 88.2 39** Tjong Kim Sang (2001) 78.7 82.3 80.5 Unk Charniak (2000) 89.6 89.5 89.5 23** Ratnaparkhi (1997) 86.3 87.5 86.9 Unk Table 6: Parsing performance on section 23 (all sentences)." ></td>
	<td class="line x" title="198:207	* estimated from the parsing time on the training data." ></td>
	<td class="line x" title="199:207	** reported in (Sagae and Lavie, 2006) where Pentium 4 3.2GHz was used to run the parsers." ></td>
	<td class="line x" title="200:207	build a competitive parser if we use CRF models for each level of chunking and a depth-first search algorithm to search for the highest probability parse." ></td>
	<td class="line x" title="201:207	Like other discriminative learning approaches, one of the advantages of our parser is its generality." ></td>
	<td class="line x" title="202:207	The design of our parser is very generic, and the features used in our parser are not particularly specific to the Penn Treebank." ></td>
	<td class="line x" title="203:207	We expect it to be straightforward to adapt the parser to other projective grammars and languages." ></td>
	<td class="line x" title="204:207	This parsing framework should be useful when one needs to process a large amount of text or when real time processing is required, in which the parsing speed is of top priority." ></td>
	<td class="line x" title="205:207	In the deterministic setting, our parser only needed about 10 msec to parse a sentence." ></td>
	<td class="line x" title="206:207	Acknowledgments This work described in this paper has been funded by the Biotechnology and Biological Sciences Research Council (BBSRC; BB/E004431/1) and the European BOOTStrep project (FP6 028099)." ></td>
	<td class="line x" title="207:207	The research team is hosted by the JISC/BBSRC/EPSRC sponsored National Centre for Text Mining." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E09-3005
Structural Correspondence Learning for Parse Disambiguation
Plank, Barbara;"></td>
	<td class="line x" title="1:229	Proceedings of the EACL 2009 Student Research Workshop, pages 3745, Athens, Greece, 2 April 2009." ></td>
	<td class="line x" title="2:229	c2009 Association for Computational Linguistics Structural Correspondence Learning for Parse Disambiguation Barbara Plank Alfa-informatica University of Groningen, The Netherlands b.plank@rug.nl Abstract The paper presents an application of Structural Correspondence Learning (SCL) (Blitzer et al., 2006) for domain adaptation of a stochastic attribute-value grammar (SAVG)." ></td>
	<td class="line x" title="3:229	So far, SCL has been applied successfully in NLP for Part-of-Speech tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007)." ></td>
	<td class="line x" title="4:229	An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa, 2007), however, without any clear conclusions." ></td>
	<td class="line x" title="5:229	We report on our exploration of applying SCL to adapt a syntactic disambiguation model and show promising initial results on Wikipedia domains." ></td>
	<td class="line x" title="6:229	1 Introduction Many current, effective natural language processing systems are based on supervised Machine Learning techniques." ></td>
	<td class="line x" title="7:229	The parameters of such systems are estimated to best reflect the characteristics of the training data, at the cost of portability: a system will be successful only as long as the training material resembles the input that the model gets." ></td>
	<td class="line x" title="8:229	Therefore, whenever we have access to a large amount of labeled data from some source (out-of-domain), but we would like a model that performs well on some new target domain (Gildea, 2001; Daume III, 2007), we face the problem of domain adaptation." ></td>
	<td class="line x" title="9:229	The need for domain adaptation arises in many NLP tasks: Part-of-Speech tagging, Sentiment Analysis, Semantic Role Labeling or Statistical Parsing, to name but a few." ></td>
	<td class="line x" title="10:229	For example, the performance of a statistical parsing system drops in an appalling way when a model trained on the Wall Street Journal is applied to the more varied Brown corpus (Gildea, 2001)." ></td>
	<td class="line oc" title="11:229	The problem itself has started to get attention only recently (Roark and Bacchiani, 2003; Hara et al., 2005; Daume III and Marcu, 2006; Daume III, 2007; Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007)." ></td>
	<td class="line x" title="12:229	We distinguish two main approaches to domain adaptation that have been addressed in the literature (Daume III, 2007): supervised and semi-supervised." ></td>
	<td class="line x" title="13:229	In supervised domain adaptation (Gildea, 2001; Roark and Bacchiani, 2003; Hara et al., 2005; Daume III, 2007), besides the labeled source data, we have access to a comparably small, but labeled amount of target data." ></td>
	<td class="line oc" title="14:229	In contrast, semi-supervised domain adaptation (Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007) is the scenario in which, in addition to the labeled source data, we only have unlabeled and no labeled target domain data." ></td>
	<td class="line n" title="15:229	Semi-supervised adaptation is a much more realistic situation, while at the same time also considerably more difficult." ></td>
	<td class="line x" title="16:229	Studies on the supervised task have shown that straightforward baselines (e.g. models based on source only, target only, or the union of the data) achieve a relatively high performance level and are surprisingly difficult to beat (Daume III, 2007)." ></td>
	<td class="line x" title="17:229	Thus, one conclusion from that line of work is that as soon as there is a reasonable (often even small) amount of labeled target data, it is often more fruitful to either just use that, or to apply simple adaptation techniques (Daume III, 2007; Plank and van Noord, 2008)." ></td>
	<td class="line nc" title="18:229	2 Motivation and Prior Work While several authors have looked at the supervised adaptation case, there are less (and especially less successful) studies on semi-supervised domain adaptation (McClosky et al., 2006; Blitzer et al., 2006; Dredze et al., 2007)." ></td>
	<td class="line oc" title="19:229	Of these, McClosky et al.(2006) deal specifically with selftraining for data-driven statistical parsing." ></td>
	<td class="line p" title="21:229	They show that together with a re-ranker, improvements 37 are obtained." ></td>
	<td class="line x" title="22:229	Similarly, Structural Correspondence Learning (Blitzer et al., 2006; Blitzer et al., 2007; Blitzer, 2008) has proven to be successful for the two tasks examined, PoS tagging and Sentiment Classification." ></td>
	<td class="line x" title="23:229	In contrast, Dredze et al.(2007) report on frustrating results on the CoNLL 2007 semi-supervised adaptation task for dependency parsing, i.e. no team was able to improve target domain performance substantially over a state of the art baseline." ></td>
	<td class="line x" title="25:229	In the same shared task, an attempt was made to apply SCL to domain adaptation for data-driven dependency parsing (Shimizu and Nakagawa, 2007)." ></td>
	<td class="line x" title="26:229	The system just ended up at rank 7 out of 8 teams." ></td>
	<td class="line x" title="27:229	However, based on annotation differences in the datasets (Dredze et al., 2007) and a bug in their system (Shimizu and Nakagawa, 2007), their results are inconclusive.1 Thus, the effectiveness of SCL is rather unexplored for parsing." ></td>
	<td class="line oc" title="28:229	So far, most previous work on domain adaptation for parsing has focused on data-driven systems (Gildea, 2001; Roark and Bacchiani, 2003; McClosky et al., 2006; Shimizu and Nakagawa, 2007), i.e. systems employing (constituent or dependency based) treebank grammars (Charniak, 1996)." ></td>
	<td class="line oc" title="29:229	Parse selection constitutes an important part of many parsing systems (Johnson et al., 1999; Hara et al., 2005; van Noord and Malouf, 2005; McClosky et al., 2006)." ></td>
	<td class="line x" title="30:229	Yet, the adaptation of parse selection models to novel domains is a far less studied area." ></td>
	<td class="line x" title="31:229	This may be motivated by the fact that potential gains for this task are inherently bounded by the underlying grammar." ></td>
	<td class="line x" title="32:229	The few studies on adapting disambiguation models (Hara et al., 2005; Plank and van Noord, 2008) have focused exclusively on the supervised scenario." ></td>
	<td class="line x" title="33:229	Therefore, the direction we explore in this study is semi-supervised domain adaptation for parse disambiguation." ></td>
	<td class="line x" title="34:229	We examine the effectiveness of Structural Correspondence Learning (SCL) (Blitzer et al., 2006) for this task, a recently proposed adaptation technique shown to be effective for PoS tagging and Sentiment Analysis." ></td>
	<td class="line x" title="35:229	The system used in this study is Alpino, a wide-coverage Stochastic Attribute Value Grammar (SAVG) for Dutch (van Noord and Malouf, 2005; van Noord, 2006)." ></td>
	<td class="line x" title="36:229	For our empirical eval1As shown in Dredze et al.(2007), the biggest problem for the shared task was that the provided datasets were annotated with different annotation guidelines, thus the general conclusion was that the task was ill-defined (Nobuyuki Shimizu, personal communication)." ></td>
	<td class="line x" title="38:229	uation we explore Wikipedia as primary test and training collection." ></td>
	<td class="line x" title="39:229	In the sequel, we first introduce the parsing system." ></td>
	<td class="line x" title="40:229	Section 4 reviews Structural Correspondence Learning and shows our application of SCL to parse selection, including all our design choices." ></td>
	<td class="line x" title="41:229	In Section 5 we present the datasets, introduce the process of constructing target domain data from Wikipedia, and discuss interesting initial empirical results of this ongoing study." ></td>
	<td class="line x" title="42:229	3 Background: Alpino parser Alpino (van Noord and Malouf, 2005; van Noord, 2006) is a robust computational analyzer for Dutch that implements the conceptual two-stage parsing approach." ></td>
	<td class="line x" title="43:229	The system consists of approximately 800 grammar rules in the tradition of HPSG, and a large hand-crafted lexicon, that together with a left-corner parser constitutes the generation component." ></td>
	<td class="line x" title="44:229	For parse selection, Alpino employs a discriminative approach based on Maximum Entropy (MaxEnt)." ></td>
	<td class="line x" title="45:229	The output of the parser is dependency structure based on the guidelines of CGN (Oostdijk, 2000)." ></td>
	<td class="line x" title="46:229	The Maximum Entropy model (Berger et al., 1996; Ratnaparkhi, 1997; Abney, 1997) is a conditional model that assigns a probability to every possible parse  for a given sentence s. The model consists of a set of m feature functions fj() that describe properties of parses, together with their associated weights j. The denominator is a normalization term where Y (s) is the set of parses with yield s: p(|s;) = exp( summationtextm j=1 jfj())summationtext yY (s) exp( summationtextm j=1 jfj(y))) (1) The parameters (weights) j can be estimated efficiently by maximizing the regularized conditional likelihood of a training corpus (Johnson et al., 1999; van Noord and Malouf, 2005):  = argmax  logL()  summationtextm j=1  2j 22 (2) where L() is the likelihood of the training data." ></td>
	<td class="line x" title="47:229	The second term is a regularization term (Gaussian prior on the feature weights with mean zero and variance )." ></td>
	<td class="line x" title="48:229	The estimated weights determine the contribution of each feature." ></td>
	<td class="line x" title="49:229	Features appearing in correct parses are given increasing (positive) weight, while features in incorrect parses are 38 given decreasing (negative) weight." ></td>
	<td class="line x" title="50:229	Once a model is trained, it can be applied to choose the parse with the highest sum of feature weights." ></td>
	<td class="line x" title="51:229	The MaxEnt model consists of a large set of features, corresponding to instantiations of feature templates that model various properties of parses." ></td>
	<td class="line x" title="52:229	For instance, Part-of-Speech tags, dependency relations, grammar rule applications, etc. The current standard model uses about 11,000 features." ></td>
	<td class="line x" title="53:229	We will refer to this set of features as original features." ></td>
	<td class="line x" title="54:229	They are used to train the baseline model on the given labeled source data." ></td>
	<td class="line x" title="55:229	4 Structural Correspondence Learning SCL (Structural Correspondence Learning) (Blitzer et al., 2006; Blitzer et al., 2007; Blitzer, 2008) is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains." ></td>
	<td class="line x" title="56:229	Before describing the algorithm in detail, let us illustrate the intuition behind SCL with an example, borrowed from Blitzer et al.(2007)." ></td>
	<td class="line x" title="58:229	Suppose we have a Sentiment Analysis system trained on book reviews (domain A), and we would like to adapt it to kitchen appliances (domain B)." ></td>
	<td class="line x" title="59:229	Features such as boring and repetitive are common ways to express negative sentiment in A, while not working or defective are specific to B. If there are features across the domains, e.g. dont buy, with which the domain specific features are highly correlated with, then we might tentatively align those features." ></td>
	<td class="line x" title="60:229	Therefore, the key idea of SCL is to identify automatically correspondences among features from different domains by modeling their correlations with pivot features." ></td>
	<td class="line x" title="61:229	Pivots are features occurring frequently and behaving similarly in both domains (Blitzer et al., 2006)." ></td>
	<td class="line x" title="62:229	They are inspired by auxiliary problems from Ando and Zhang (2005)." ></td>
	<td class="line x" title="63:229	Non-pivot features that correspond with many of the same pivot-features are assumed to correspond." ></td>
	<td class="line x" title="64:229	Intuitively, if we are able to find good correspondences among features, then the augmented labeled source domain data should transfer better to a target domain (where no labeled data is available) (Blitzer et al., 2006)." ></td>
	<td class="line x" title="65:229	The outline of the algorithm is given in Figure 1." ></td>
	<td class="line x" title="66:229	The first step is to identify m pivot features occurring frequently in the unlabeled data of both Input: labeled source data {(xs,ys)Nss=1} unlabeled data from both source and target domain xul = xs,xt 1." ></td>
	<td class="line x" title="67:229	Select m pivot features 2." ></td>
	<td class="line x" title="68:229	Train m binary classifiers (pivot predictors) 3." ></td>
	<td class="line x" title="69:229	Create matrix Wnm of binary predictor weight vectors W = [w1,,wm], where n is the number of nonpivot features in xul 4." ></td>
	<td class="line x" title="70:229	Apply SVD to W: Wnm = UnnDnmV Tmm where  = UT[1:h,:] are the h top left singular vectors of W. 5." ></td>
	<td class="line x" title="71:229	Apply projection xs and train a predictor on the original and new features obtained through the projection." ></td>
	<td class="line x" title="72:229	Figure 1: SCL algorithm (Blitzer et al., 2006)." ></td>
	<td class="line x" title="73:229	domains." ></td>
	<td class="line x" title="74:229	Then, a binary classifier is trained for each pivot feature (pivot predictor) of the form: Does pivot feature l occur in this instance?." ></td>
	<td class="line x" title="75:229	The pivots are masked in the unlabeled data and the aim is to predict them using non-pivot features." ></td>
	<td class="line x" title="76:229	In this way, we obtain a weight vector w for each pivot predictor." ></td>
	<td class="line x" title="77:229	Positive entries in the weight vector indicate that a non-pivot is highly correlated with the respective pivot feature." ></td>
	<td class="line x" title="78:229	Step 3 is to arrange the m weight vectors in a matrix W, where a column corresponds to a pivot predictor weight vector." ></td>
	<td class="line x" title="79:229	Applying the projection WTx (where x is a training instance) would give us m new features, however, for both computational and statistical reasons (Blitzer et al., 2006; Ando and Zhang, 2005) a low-dimensional approximation of the original feature space is computed by applying Singular Value Decomposition (SVD) on W (step 4)." ></td>
	<td class="line x" title="80:229	Let  = UThn be the top h left singular vectors of W (with h a dimension parameter and n the number of non-pivot features)." ></td>
	<td class="line x" title="81:229	The resulting  is a projection onto a lower dimensional space Rh, parameterized by h. The final step of SCL is to train a linear predictor on the augmented labeled source data x,x." ></td>
	<td class="line x" title="82:229	In more detail, the original feature space x is augmented with h new features obtained by applying the projection x. In this way, we can learn weights for domain-specific features, which otherwise would not have been observed." ></td>
	<td class="line x" title="83:229	If  contains meaningful correspondences, then the pre39 dictor trained on the augmented data should transfer well to the new domain." ></td>
	<td class="line x" title="84:229	4.1 SCL for Parse Disambiguation A property of the pivot predictors is that they can be trained from unlabeled data, as they represent properties of the input." ></td>
	<td class="line x" title="85:229	So far, pivot features on the word level were used (Blitzer et al., 2006; Blitzer et al., 2007; Blitzer, 2008), e.g. Does the bigram not buy occur in this document? (Blitzer, 2008)." ></td>
	<td class="line x" title="86:229	Pivot features are the key ingredient for SCL, and they should align well with the NLP task." ></td>
	<td class="line x" title="87:229	For PoS tagging and Sentiment Analysis, features on the word level are intuitively well-related to the problem at hand." ></td>
	<td class="line x" title="88:229	For the task of parse disambiguation based on a conditional model this is not the case." ></td>
	<td class="line x" title="89:229	Hence, we actually introduce an additional and new layer of abstraction, which, we hypothesize, aligns well with the task of parse disambiguation: we first parse the unlabeled data." ></td>
	<td class="line x" title="90:229	In this way we obtain full parses for given sentences as produced by the grammar, allowing access to more abstract representations of the underlying pivot predictor training data (for reasons of efficiency, we here use only the first generated parse as training data for the pivot predictors, rather than n-best)." ></td>
	<td class="line x" title="91:229	Thus, instead of using word-level features, our features correspond to properties of the generated parses: application of grammar rules (r1,r2 features), dependency relations (dep), PoS tags (f1,f2), syntactic features (s1), precedence (mf ), bilexical preferences (z), apposition (appos) and further features for unknown words, temporal phrases, coordination (h,in year and p1, respectively)." ></td>
	<td class="line x" title="92:229	This allows us to get a possibly noisy, but more abstract representation of the underlying data." ></td>
	<td class="line x" title="93:229	The set of features used in Alpino is further described in van Noord and Malouf (2005)." ></td>
	<td class="line x" title="94:229	Selection of pivot features As pivot features should be common across domains, here we restrict our pivots to be of the type r1,p1,s1 (the most frequently occurring feature types)." ></td>
	<td class="line x" title="95:229	In more detail, r1 indicates which grammar rule applied, p1 whether coordination conjuncts are parallel, and s1 whether topicalization or long-distance dependencies occurred." ></td>
	<td class="line x" title="96:229	We count how often each feature appears in the parsed source and target domain data, and select those r1,p1,s1 features as pivot features, whose count is > t, where t is a specified threshold." ></td>
	<td class="line x" title="97:229	In all our experiments, we set t = 5000." ></td>
	<td class="line x" title="98:229	In this way we obtained on average 360 pivot features, on the datasets described in Section 5." ></td>
	<td class="line x" title="99:229	Predictive features As pointed out by Blitzer et al.(2006), each instance will actually contain features which are totally predictive of the pivot features (i.e. the pivot itself)." ></td>
	<td class="line x" title="101:229	In our case, we additionally have to pay attention to more specific features, e.g. r2 is a feature that extends r1, in the sense that it incorporates more information than its parent (i.e. which grammar rules applied in the construction of daughter nodes)." ></td>
	<td class="line x" title="102:229	It is crucial to remove these predictive features when creating the training data for the pivot predictors." ></td>
	<td class="line x" title="103:229	Matrix and SVD Following Blitzer et al.(2006) (which follow Ando and Zhang (2005)), we only use positive entries in the pivot predictors weight vectors to compute the SVD." ></td>
	<td class="line x" title="105:229	Thus, when constructing the matrix W, we disregard all negative entries in W and compute the SVD (W = UDV T ) on the resulting non-negative sparse matrix." ></td>
	<td class="line x" title="106:229	This sparse representation saves both time and space." ></td>
	<td class="line x" title="107:229	4.2 Further practical issues of SCL In practice, there are more free parameters and model choices (Ando and Zhang, 2005; Ando, 2006; Blitzer et al., 2006; Blitzer, 2008) besides the ones discussed above." ></td>
	<td class="line x" title="108:229	Feature normalization and feature scaling." ></td>
	<td class="line x" title="109:229	Blitzer et al.(2006) found it necessary to normalize and scale the new features obtained by the projection , in order to allow them to receive more weight from a regularized discriminative learner." ></td>
	<td class="line x" title="111:229	For each of the features, they centered them by subtracting out the mean and normalized them to unit variance (i.e. x  mean/sd)." ></td>
	<td class="line x" title="112:229	They then rescaled the features by a factor  found on heldout data: x. Restricted Regularization." ></td>
	<td class="line x" title="113:229	When training the supervised model on the augmented feature space x,x, Blitzer et al.(2006) only regularize the weight vector of the original features, but not the one for the new low-dimensional features." ></td>
	<td class="line x" title="115:229	This was done to encourage the model to use the new low-dimensional representation rather than the higher-dimensional original representation (Blitzer, 2008)." ></td>
	<td class="line x" title="116:229	Dimensionality reduction by feature type." ></td>
	<td class="line x" title="117:229	An extension suggested in Ando and Zhang (2005) is 40 to compute separate SVDs for blocks of the matrix W corresponding to feature types (as illustrated in Figure 2), and then to apply separate projection for every type." ></td>
	<td class="line x" title="118:229	Due to the positive results in Ando (2006), Blitzer et al.(2006) include this in their standard setting of SCL and report results using block SVDs only." ></td>
	<td class="line x" title="120:229	Figure 2: Illustration of dimensionality reduction by feature type (Ando and Zhang, 2005)." ></td>
	<td class="line x" title="121:229	The grey area corresponds to a feature type (submatrix of W) on which the SVD is computed (block SVD); the white area is regarded as fixed to zero matrices." ></td>
	<td class="line x" title="122:229	5 Experiments and Results 5.1 Experimental design The base (source domain) disambiguation model is trained on the Alpino Treebank (van Noord, 2006) (newspaper text), which consists of approximately 7,000 sentences and 145,000 tokens." ></td>
	<td class="line x" title="123:229	For parameter estimation of the disambiguation model, in all reported experiments we use the TADM2 toolkit (toolkit for advanced discriminative training), with a Gaussian prior (2=1000) and the (default) limited memory variable metric estimation technique (Malouf, 2002)." ></td>
	<td class="line x" title="124:229	For training the binary pivot predictors, we use the MegaM3 Optimization Package with the socalled bernoulli implicit input format." ></td>
	<td class="line x" title="125:229	To compute the SVD, we use SVDLIBC.4 The output of the parser is dependency structure." ></td>
	<td class="line x" title="126:229	A standard evaluation metric is to measure the amount of generated dependencies that are identical to the stored dependencies (correct labeled dependencies), expressed as f-score." ></td>
	<td class="line x" title="127:229	An alternative measure is concept accuracy (CA), which is similar to f-score, but allows possible discrepancy between the number of returned dependencies (van Noord, 2006; Plank and van Noord, 2http://tadm.sourceforge.net/ 3http://www.cs.utah.edu/hal/megam/ 4http://tedlab.mit.edu/dr/svdlibc/ 2008)." ></td>
	<td class="line x" title="128:229	CA is usually slightly lower than f-score." ></td>
	<td class="line x" title="129:229	Let Dip be the number of dependencies produced by the parser for sentence i. Dig is the number of dependencies in the treebank parse, and Dio is the number of correct dependencies produced by the parser." ></td>
	<td class="line x" title="130:229	Then, CA = Dosummationtext i max(Dig,Dip) If we want to compare the performance of disambiguation models, we can employ the  measure (van Noord and Malouf, 2005; van Noord, 2007)." ></td>
	<td class="line x" title="131:229	Intuitively, it tells us how much of the disambiguation problem has been solved." ></td>
	<td class="line x" title="132:229	 = CAbaseoracle  base  100 In more detail, the  measure incorporates an upper and lower bound: base measures the accuracy of a model that simply selects the first parse for each sentence; oracle represents the accuracy achieved by a model that always selects the best parse from the set of potential parses (within the coverage of the parser)." ></td>
	<td class="line x" title="133:229	In addition, we also report relative error reduction (rel.er), which is the relative difference in  scores for two models." ></td>
	<td class="line x" title="134:229	As target domain, we consider the Dutch part of Wikipedia as data collection, described in the following." ></td>
	<td class="line x" title="135:229	5.2 Wikipedia as resource In our experiments, we exploit Wikipedia both as testset and as unlabeled data source." ></td>
	<td class="line x" title="136:229	We assume that in order to parse data from a very specific domain, say about the artist Prince, then data related to that domain, like information about the New Power Generation, the Purple rain movie, or other American singers and artists, should be of help." ></td>
	<td class="line x" title="137:229	Thus, we exploit Wikipedia and its category system to gather domain-specific target data." ></td>
	<td class="line x" title="138:229	Construction of target domain data In more detail, we use the Dutch part of Wikipedia provided by WikiXML,5 a collection of Wikipedia articles converted to XML format." ></td>
	<td class="line x" title="139:229	As the corpus is encoded in XML, we can exploit general purpose XML Query Languages, such as XQuery, Xslt and XPath, to extract relevant information from the Wikipedia corpus." ></td>
	<td class="line x" title="140:229	Given a wikipage p, with c  categories(p), we can identify pages related to p of various 5http://ilps.science.uva.nl/WikiXML/ 41 types of relatedness: directly related pages (those that share a category, i.e. all p where c  categories(p) such that c = c), or alternatively, pages that share a subor supercategory of p, i.e. p where c  categories(p) and c  sub categories(p) or c  super categories(p)." ></td>
	<td class="line x" title="141:229	For example, Figure 3 shows the categories extracted for the Wikipedia article about pope Johannes Paulus II." ></td>
	<td class="line x" title="142:229	<wikipage id='6677'> <cat t='direct' n='Categorie:Paus'/> <cat t='direct' n='Categorie:Pools_theoloog'/> <cat t='super' n='Categorie:Religieus leider'/> <cat t='super' n='Categorie:Rooms-katholiek persoon'/> <cat t='super' n='Categorie:Vaticaanstad'/> <cat t='super' n='Categorie:Bisschop'/> <cat t='super' n='Categorie:Kerkgeschiedenis'/> <cat t='sub' n='Categorie:Tegenpaus'/> <cat t='super' n='Categorie:Pools persoon'/> </wikipage> Figure 3: Example of extracted Wikipedia categories for a given article (direct, supand subcats)." ></td>
	<td class="line x" title="143:229	To create the set of related pages for a given article p, we proceed as follows: 1." ></td>
	<td class="line x" title="144:229	Find suband supercategories of p 2." ></td>
	<td class="line x" title="145:229	Extract all pages that are related to p (through sharing a direct, sub or super category) 3." ></td>
	<td class="line x" title="146:229	Optionally, filter out certain pages In our empirical setup, we followed Blitzer et al.(2006) and tried to balance the size of source and target data." ></td>
	<td class="line x" title="148:229	Thus, depending on the size of the resulting target domain dataset, and the broadness of the categories involved in creating it, we might wish to filter out certain pages." ></td>
	<td class="line x" title="149:229	We implemented a filter mechanism that excludes pages of a certain category (e.g. a supercategory that is hypothesized to be too broad)." ></td>
	<td class="line x" title="150:229	Alternatively, we might have used a filter mechanism that excludes certain pages directly." ></td>
	<td class="line x" title="151:229	In our experiments, we always included pages that are directly related to a page of interest, and those that shared a subcategory." ></td>
	<td class="line x" title="152:229	Of course, the page itself is not included in that dataset." ></td>
	<td class="line x" title="153:229	With regard to supercategories, we usually included all pages having a category c  super categories(p), unless stated otherwise." ></td>
	<td class="line x" title="154:229	Test collection Our testset consists of a selection of Wikipedia articles that have been manually corrected in the course of the D-Coi/LASSY project.6 6Ongoing project, see http://www.let.rug.nl/ vannoord/Lassy/ An overview of the testset including size indications is given in Table 1." ></td>
	<td class="line x" title="155:229	Table 2 provides information on the target domain datasets constructed from Wikipedia." ></td>
	<td class="line x" title="156:229	Wiki/DCOI ID Title Sents 6677/026563 Prince (musician) 358 6729/036834 Paus Johannes Paulus II 232 182654/041235 Augustus De Morgan 259 Table 1: Size of test datasets." ></td>
	<td class="line x" title="157:229	Related to Articles Sents Tokens Relationship Prince 290 9,772 145,504 filtered super Paus 445 8,832 134,451 all De Morgan 394 8,466 132,948 all Table 2: Size of related unlabeled data; relationship indicates whether all related pages are used or some are filtered out (see section 5.2)." ></td>
	<td class="line x" title="158:229	5.3 Empirical Results For all reported results, we randomly select n = 200 maximum number of parses per sentence for evaluation." ></td>
	<td class="line x" title="159:229	Baseline accuracies Table 3 shows the baseline performance (of the standard Alpino model) on the various Wikipedia testsets (CA, f-score)." ></td>
	<td class="line x" title="160:229	The third and fourth column indicate the upperand lower bound measures (defined in section 5.1)." ></td>
	<td class="line x" title="161:229	Title CA f-score base oracle Prince (musician) 85.03 85.38 71.95 88.70 Paus Johannes Paulus II 85.72 86.32 74.30 89.09 Augustus De Morgan 80.09 80.61 70.08 83.52 Table 3: Baseline results." ></td>
	<td class="line x" title="162:229	While the parser normally operates on an accuracy level of roughly 88-89% (van Noord, 2007) on its own domain (newspaper text), the accuracy on these subdomains drops to around 85%." ></td>
	<td class="line x" title="163:229	The biggest performance decrease (to 80%) was on the article about the British logician and mathematician De Morgan." ></td>
	<td class="line x" title="164:229	This confirms the intuition that this specific subdomain is the hardest, given that mathematical expressions might emerge in the data (e.g. Wet der distributiviteit : a(b+c) = ab+ac distributivity law)." ></td>
	<td class="line x" title="165:229	SCL results Table 4 shows the results of our instantiation of SCL for parse disambiguation, with varying h parameter (dimensionality parameter; 42 h = 25 means that applying the projection x resulted in adding 25 new features to every source domain instance)." ></td>
	<td class="line x" title="166:229	CA f-score  rel.er." ></td>
	<td class="line x" title="167:229	baseline Prince 85.03 85.38 78.06 0.00 SCL[+/-], h = 25 85.12 85.46 78.64 2.64 SCL[+/-], h = 50 85.29 85.63 79.66 7.29 SCL[+/-], h = 100 85.19 85.53 79.04 4.47 SCL[+/-], h = 200 85.21 85.54 79.18 5.10 baseline Paus 85.72 86.32 77.23 0.00 SCL[+/-], h = 25 85.87 86.48 78.26 4.52 SCL[+/-], h = 50 85.82 86.43 77.87 2.81 SCL[+/-], h = 100 85.87 86.49 78.26 4.52 SCL[+/-], h = 200 85.87 86.48 78.26 4.52 baseline DeMorgan 80.09 80.61 74.44 0.00 SCL[+/-], h = 25 80.15 80.67 74.92 1.88 SCL[+/-], h = 50 80.12 80.64 74.68 0.94 SCL[+/-], h = 100 80.12 80.64 74.68 0.94 SCL[+/-], h = 200 80.15 80.67 74.91 1.88 Table 4: Results of our instantiation of SCL (with varying h parameter and no feature normalization)." ></td>
	<td class="line x" title="168:229	The results show a (sometimes) small but consistent increase in absolute performance on all testsets over the baseline system (up to +0.26 absolute CA score), as well as an increase in  measure (absolute error reduction)." ></td>
	<td class="line x" title="169:229	This corresponds to a relative error reduction of up to 7.29%." ></td>
	<td class="line x" title="170:229	Thus, our first instantiation of SCL for parse disambiguation indeed shows promising results." ></td>
	<td class="line x" title="171:229	We can confirm that changing the dimensionality parameter h has rather little effect (Table 4), which is in line with previous findings (Ando and Zhang, 2005; Blitzer et al., 2006)." ></td>
	<td class="line x" title="172:229	Thus we might fix the parameter and prefer smaller dimensionalities, which saves space and time." ></td>
	<td class="line x" title="173:229	Note that these results were obtained without any of the additional normalization, rescaling, feature-specific regularization, or block SVD issues, etc.(discussed in section 4.2)." ></td>
	<td class="line x" title="175:229	We used the same Gaussian regularization term (2=1000) for all features (original and new features), and did not perform any feature normalization or rescaling." ></td>
	<td class="line x" title="176:229	This means our current instantiation of SCL is an actually simplified version of the original SCL algorithm, applied to parse disambiguation." ></td>
	<td class="line x" title="177:229	Of course, our results are preliminary and, rather than warranting many definite conclusions, encourage further exploration of SCL and related semi-supervised adaptation techniques." ></td>
	<td class="line x" title="178:229	5.4 Additional Empirical Results In the following, we describe additional results obtained by extensions and/or refinements of our current SCL instantiation." ></td>
	<td class="line x" title="179:229	Feature normalization." ></td>
	<td class="line x" title="180:229	We also tested feature normalization (as described in Section 4.2)." ></td>
	<td class="line x" title="181:229	While Blitzer et al.(2006) found it necessary to normalize (and scale) the projection features, we did not observe any improvement by normalizing them (actually, it slightly degraded performance in our case)." ></td>
	<td class="line x" title="183:229	Thus, we found this step unnecessary, and currently did not look at this issue any further." ></td>
	<td class="line x" title="184:229	A look at  To gain some insight of which kind of correspondences SCL learned in our case, we started to examine the rows of ." ></td>
	<td class="line x" title="185:229	Recall that applying a row of the projection matrix i to a training instance x gives us a new real-valued feature." ></td>
	<td class="line x" title="186:229	If features from different domains have similar entries (scores) in the projection row, they are assumed to correspond (Blitzer, 2008)." ></td>
	<td class="line x" title="187:229	Figure 4 shows example of correspondences that SCL found in the Prince dataset." ></td>
	<td class="line x" title="188:229	The first column represents the score of a feature." ></td>
	<td class="line x" title="189:229	The labels wiki and alp indicate the domain of the features, respectively." ></td>
	<td class="line x" title="190:229	For readability, we here grouped the features obtaining similar scores." ></td>
	<td class="line x" title="191:229	0.00010248|dep35(Chaka Khan,name(PER),hd/su,verb,ben)|wiki 0.00010248|dep35(de,det,hd/det,adj,Afro-Amerikaanse)|wiki 0.00010248|dep35(Yvette Marie Stevens,name(PER),hd/app, noun,zangeres)|wiki 0.000102772|dep34(leraar,noun,hd/su,verb)|alp 0.000161095|dep34(commissie,noun,hd/obj1,prep)|16|alp 0.00016113|dep34(Confessions Tour,name,hd/obj1,prep)|2|wiki 0.000161241|dep34(orgel,noun,hd/obj1,prep)|1|wiki 0.000217698|dep34(tournee,noun,hd/su,verb)|1|wiki 0.000223301|dep34(regisseur,noun,hd/su,verb)|15|wiki 0.000224517|dep34(voorsprong,noun,hd/su,verb)|2|alp 0.000224684|dep34(wetenschap,noun,hd/su,verb)|2|alp 0.000226617|dep34(pop_rock,noun,hd/su,verb)|1|wiki 0.000228918|dep34(plan,noun,hd/su,verb)|9|alp Figure 4: Example projection from  (row 2)." ></td>
	<td class="line x" title="192:229	SCL clustered information about Chaka Khan, an Afro-Amerikaanse zangeres (afro-american singer) whose real name is Yvette Marie Stevens." ></td>
	<td class="line x" title="193:229	She had close connections to Prince, who even wrote one of her singles." ></td>
	<td class="line x" title="194:229	These features got aligned to the Alpino feature leraar (teacher)." ></td>
	<td class="line x" title="195:229	Moreover, SCL finds that tournee, regisseur and pop rock in the Prince domain behave like voorsprong (advance), wetenschap (research) and plan as possible heads in a subject relation in the newspaper domain." ></td>
	<td class="line x" title="196:229	Similarly, correspon43 dences between the direct object features Confessions Tour and orgel (pipe organ) to commissie (commission) are discovered." ></td>
	<td class="line x" title="197:229	More unlabeled data In the experiments so far, we balanced the amount of source and target data." ></td>
	<td class="line x" title="198:229	We started to examine the effect of more unlabeled target domain data." ></td>
	<td class="line x" title="199:229	For the Prince dataset, we included all supercategories in constructing the related target domain data." ></td>
	<td class="line x" title="200:229	The so obtained dataset contains: 859 articles, 29,186 sentences and 385,289 tokens; hence, the size approximately tripled (w.r.t. Table 2)." ></td>
	<td class="line x" title="201:229	Table 5 shows the effect of using this larger dataset for SCL with h = 25." ></td>
	<td class="line x" title="202:229	The accuracy increases (from 85.12 to 85.25)." ></td>
	<td class="line x" title="203:229	Thus, there seems to be a positive effect (to be investigated further)." ></td>
	<td class="line x" title="204:229	CA f-score  rel.er." ></td>
	<td class="line x" title="205:229	baseline Prince 85.03 85.38 78.06 0.00 SCL[+/-], h = 25, all 85.25 85.58 79.42 6.20 Table 5: First result on increasing unlabeled data." ></td>
	<td class="line x" title="206:229	Dimensionality reduction by feature type We have started to implement the extension discussed in section 4.2, i.e. perform separate dimensionality reductions based on blocks of nonpivot features." ></td>
	<td class="line x" title="207:229	We clustered nonpivots (see section 4.1 for a description) into 9 types (ordered in terms of decreasing cluster size): dep, f1/f2 (pos), r1/r2 (rules), appos person, mf, z, h1, in year, dist. For each type, a separate SVD was computed on submatrix Wt (illustrated in Figure 2)." ></td>
	<td class="line x" title="208:229	Then, separate projections were applied to every training instance." ></td>
	<td class="line x" title="209:229	The results of these experiments on the Prince dataset are shown in Figure 5." ></td>
	<td class="line x" title="210:229	Applying SCL with dimensionality reduction by feature type (SCL block) results in a model that performs better (CA 85.27,  79.52, rel.er." ></td>
	<td class="line x" title="211:229	6.65%) than the model with no feature split (no block SVDs), thus obtaining a relative error reduction of 6.65% over the baseline." ></td>
	<td class="line x" title="212:229	The same figure also shows what happens if we remove a specific feature type at a time; the apposition features contribute the most on this Prince domain." ></td>
	<td class="line x" title="213:229	As a fact, one third of the sentences in the Prince testset contain constructions with appositions (e.g. about film-, albumand song titles)." ></td>
	<td class="line x" title="214:229	6 Conclusions and Future Work The paper presents an application of Structural Correspondence Learning (SCL) to parse disamFigure 5: Results of dimensionality reduction by feature type, h = 25; block SVD included all 9 feature types; the right part shows the accuracy when one feature type was removed." ></td>
	<td class="line x" title="215:229	biguation." ></td>
	<td class="line x" title="216:229	While SCL has been successfully applied to PoS tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007), its effectiveness for parsing was rather unexplored." ></td>
	<td class="line x" title="217:229	The empirical results show that our instantiation of SCL to parse disambiguation gives promising initial results, even without the many additional extensions on the feature level as done in Blitzer et al.(2006)." ></td>
	<td class="line x" title="219:229	We exploited Wikipedia as primary resource, both for collecting unlabeled target domain data, as well as test suite for empirical evaluation." ></td>
	<td class="line x" title="220:229	On the three examined datasets, SCL slightly but constantly outperformed the baseline." ></td>
	<td class="line x" title="221:229	Applying SCL involves many design choices and practical issues, which we tried to depict here in detail." ></td>
	<td class="line x" title="222:229	A novelty in our application is that we first actually parse the unlabeled data from both domains." ></td>
	<td class="line x" title="223:229	This allows us to get a possibly noisy, but more abstract representation of the underlying data on which the pivot predictors are trained." ></td>
	<td class="line x" title="224:229	In the near future, we plan to extend the work on semi-supervised domain adaptation for parse disambiguation, viz." ></td>
	<td class="line x" title="225:229	(1) further explore/refine SCL (block SVDs, varying amount of target domain data, other testsets, etc.), and (2) examine selftraining." ></td>
	<td class="line x" title="226:229	Studies on the latter have focused mainly on generative, constituent based, i.e. data-driven parsing systems." ></td>
	<td class="line x" title="227:229	Furthermore, from a machine learning point of view, it would be interesting to know a measure of corpus similarity to estimate the success of porting an NLP system from one domain to another." ></td>
	<td class="line x" title="228:229	This relates to the general question of what is meant by domain." ></td>
	<td class="line x" title="229:229	44" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1006
Exploiting Heterogeneous Treebanks for Parsing
Niu, Zheng-Yu;Wang, Haifeng;Wu, Hua;"></td>
	<td class="line x" title="1:212	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 4654, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:212	c2009 ACL and AFNLP Exploiting Heterogeneous Treebanks for Parsing Zheng-Yu Niu, Haifeng Wang, Hua Wu Toshiba (China) Research and Development Center 5/F., Tower W2, Oriental Plaza, Beijing, 100738, China {niuzhengyu,wanghaifeng,wuhua}@rdc.toshiba.com.cn Abstract We address the issue of using heterogeneous treebanks for parsing by breaking it down into two sub-problems, converting grammar formalisms of the treebanks to the same one, and parsing on these homogeneous treebanks." ></td>
	<td class="line x" title="3:212	First we propose to employ an iteratively trained target grammar parser to perform grammar formalism conversion, eliminating predefined heuristic rules as required in previous methods." ></td>
	<td class="line x" title="4:212	Then we provide two strategies to refine conversion results, and adopt a corpus weighting technique for parsing on homogeneous treebanks." ></td>
	<td class="line x" title="5:212	Results on the Penn Treebank show that our conversion method achieves 42% error reduction over the previous best result." ></td>
	<td class="line x" title="6:212	Evaluation on the Penn Chinese Treebank indicates that a converted dependency treebank helps constituency parsing and the use of unlabeled data by self-training further increases parsing f-score to 85.2%, resulting in 6% error reduction over the previous best result." ></td>
	<td class="line x" title="7:212	1 Introduction The last few decades have seen the emergence of multiple treebanks annotated with different grammar formalisms, motivated by the diversity of languages and linguistic theories, which is crucial to the success of statistical parsing (Abeille et al., 2000; Brants et al., 1999; Bohmova et al., 2003; Han et al., 2002; Kurohashi and Nagao, 1998; Marcus et al., 1993; Moreno et al., 2003; Xue et al., 2005)." ></td>
	<td class="line x" title="8:212	Availability of multiple treebanks creates a scenario where we have a treebank annotated with one grammar formalism, and another treebank annotated with another grammar formalism that we are interested in." ></td>
	<td class="line x" title="9:212	We call the first a source treebank, and the second a target treebank." ></td>
	<td class="line x" title="10:212	We thus encounter a problem of how to use these heterogeneous treebanks for target grammar parsing." ></td>
	<td class="line x" title="11:212	Here heterogeneous treebanks refer to two or more treebanks with different grammar formalisms, e.g., one treebank annotated with dependency structure (DS) and the other annotated with phrase structure (PS)." ></td>
	<td class="line x" title="12:212	It is important to acquire additional labeled data for the target grammar parsing through exploitation of existing source treebanks since there is often a shortage of labeled data." ></td>
	<td class="line x" title="13:212	However, to our knowledge, there is no previous study on this issue." ></td>
	<td class="line oc" title="14:212	Recently there have been some works on using multiple treebanks for domain adaptation of parsers, where these treebanks have the same grammar formalism (McClosky et al., 2006b; Roark and Bacchiani, 2003)." ></td>
	<td class="line x" title="15:212	Other related works focus on converting one grammar formalism of a treebank to another and then conducting studies on the converted treebank (Collins et al., 1999; Forst, 2003; Wang et al., 1994; Watkinson and Manandhar, 2001)." ></td>
	<td class="line x" title="16:212	These works were done either on multiple treebanks with the same grammar formalism or on only one converted treebank." ></td>
	<td class="line x" title="17:212	We see that their scenarios are different from ours as we work with multiple heterogeneous treebanks." ></td>
	<td class="line x" title="18:212	For the use of heterogeneous treebanks1, we propose a two-step solution: (1) converting the grammar formalism of the source treebank to the target one, (2) refining converted trees and using them as additional training data to build a target grammar parser." ></td>
	<td class="line x" title="19:212	For grammar formalism conversion, we choose the DS to PS direction for the convenience of the comparison with existing works (Xia and Palmer, 2001; Xia et al., 2008)." ></td>
	<td class="line x" title="20:212	Specifically, we assume that the source grammar formalism is dependency 1Here we assume the existence of two treebanks." ></td>
	<td class="line x" title="21:212	46 grammar, and the target grammar formalism is phrase structure grammar." ></td>
	<td class="line x" title="22:212	Previous methods for DS to PS conversion (Collins et al., 1999; Covington, 1994; Xia and Palmer, 2001; Xia et al., 2008) often rely on predefined heuristic rules to eliminate converison ambiguity, e.g., minimal projection for dependents, lowest attachment position for dependents, and the selection of conversion rules that add fewer number of nodes to the converted tree." ></td>
	<td class="line x" title="23:212	In addition, the validity of these heuristic rules often depends on their target grammars." ></td>
	<td class="line x" title="24:212	To eliminate the heuristic rules as required in previous methods, we propose to use an existing target grammar parser (trained on the target treebank) to generate N-best parses for each sentence in the source treebank as conversion candidates, and then select the parse consistent with the structure of the source tree as the converted tree." ></td>
	<td class="line x" title="25:212	Furthermore, we attempt to use converted trees as additional training data to retrain the parser for better conversion candidates." ></td>
	<td class="line x" title="26:212	The procedure of tree conversion and parser retraining will be run iteratively until a stopping condition is satisfied." ></td>
	<td class="line x" title="27:212	Since some converted trees might be imperfect from the perspective of the target grammar, we provide two strategies to refine conversion results: (1) pruning low-quality trees from the converted treebank, (2) interpolating the scores from the source grammar and the target grammar to select better converted trees." ></td>
	<td class="line x" title="28:212	Finally we adopt a corpus weighting technique to get an optimal combination of the converted treebank and the existing target treebank for parser training." ></td>
	<td class="line x" title="29:212	We have evaluated our conversion algorithm on a dependency structure treebank (produced from the Penn Treebank) for comparison with previous work (Xia et al., 2008)." ></td>
	<td class="line x" title="30:212	We also have investigated our two-step solution on two existing treebanks, the Penn Chinese Treebank (CTB) (Xue et al., 2005) and the Chinese Dependency Treebank (CDT)2 (Liu et al., 2006)." ></td>
	<td class="line x" title="31:212	Evaluation on WSJ data demonstrates that it is feasible to use a parser for grammar formalism conversion and the conversion benefits from converted trees used for parser retraining." ></td>
	<td class="line x" title="32:212	Our conversion method achieves 93.8% f-score on dependency trees produced from WSJ section 22, resulting in 42% error reduction over the previous best result for DS to PS conversion." ></td>
	<td class="line x" title="33:212	Results on CTB show that score interpolation is 2Available at http://ir.hit.edu.cn/." ></td>
	<td class="line x" title="34:212	more effective than instance pruning for the use of converted treebanks for parsing and converted CDT helps parsing on CTB." ></td>
	<td class="line x" title="35:212	When coupled with self-training technique, a reranking parser with CTB and converted CDT as labeled data achieves 85.2% f-score on CTB test set, an absolute 1.0% improvement (6% error reduction) over the previous best result for Chinese parsing." ></td>
	<td class="line x" title="36:212	The rest of this paper is organized as follows." ></td>
	<td class="line x" title="37:212	In Section 2, we first describe a parser based method for DS to PS conversion, and then we discuss possible strategies to refine conversion results, and finally we adopt the corpus weighting technique for parsing on homogeneous treebanks." ></td>
	<td class="line x" title="38:212	Section 3 provides experimental results of grammar formalism conversion on a dependency treebank produced from the Penn Treebank." ></td>
	<td class="line x" title="39:212	In Section 4, we evaluate our two-step solution on two existing heterogeneous Chinese treebanks." ></td>
	<td class="line x" title="40:212	Section 5 reviews related work and Section 6 concludes this work." ></td>
	<td class="line x" title="41:212	2 Our Two-Step Solution 2.1 Grammar Formalism Conversion Previous DS to PS conversion methods built a converted tree by iteratively attaching nodes and edges to the tree with the help of conversion rules and heuristic rules, based on current headdependent pair from a source dependency tree and the structure of the built tree (Collins et al., 1999; Covington, 1994; Xia and Palmer, 2001; Xia et al., 2008)." ></td>
	<td class="line x" title="42:212	Some observations can be made on these methods: (1) for each head-dependent pair, only one locally optimal conversion was kept during tree-building process, at the risk of pruning globally optimal conversions, (2) heuristic rules are required to deal with the problem that one head-dependent pair might have multiple conversion candidates, and these heuristic rules are usually hand-crafted to reflect the structural preference in their target grammars." ></td>
	<td class="line x" title="43:212	To overcome these limitations, we propose to employ a parser to generate N-best parses as conversion candidates and then use the structural information of source trees to select the best parse as a converted tree." ></td>
	<td class="line x" title="44:212	We formulate our conversion method as follows." ></td>
	<td class="line x" title="45:212	Let CDS be a source treebank annotated with DS and CPS be a target treebank annotated with PS." ></td>
	<td class="line x" title="46:212	Our goal is to convert the grammar formalism of CDS to that of CPS." ></td>
	<td class="line x" title="47:212	We first train a constituency parser on CPS 47 Input: CPS, CDS, Q, and a constituency parser Output: Converted trees CDSPS 1." ></td>
	<td class="line x" title="48:212	Initialize:  Set CDS,0PS as null, DevScore=0, q=0;  Split CPS into training set CPS,train and development set CPS,dev;  Train the parser on CPS,train and denote it by Pq1; 2." ></td>
	<td class="line x" title="49:212	Repeat:  Use Pq1 to generate N-best PS parses for each sentence in CDS, and convert PS to DS for each parse;  For each sentence in CDS Do diamondmath t=argmaxtScore(xi,t), and select the t-th parse as a converted tree for this sentence;  Let CDS,qPS represent these converted trees, and let Ctrain=CPS,trainuniontextCDS,qPS ;  Train the parser on Ctrain, and denote the updated parser by Pq;  Let DevScoreq be the f-score of Pq on CPS,dev;  If DevScoreq > DevScoreThen DevScore=DevScoreq, and CDSPS =CDS,qPS ;  Elsebreak;  q++; Untilq > Q Table 1: Our algorithm for DS to PS conversion." ></td>
	<td class="line x" title="50:212	(90% trees in CPS as training set CPS,train, and other trees as development set CPS,dev) and then let the parser generate N-best parses for each sentence in CDS." ></td>
	<td class="line x" title="51:212	Let n be the number of sentences (or trees) in CDS and ni be the number of N-best parses generated by the parser for the i-th (1  i  n) sentence in CDS." ></td>
	<td class="line x" title="52:212	Let xi,t be the t-th (1  t  ni) parse for the i-th sentence." ></td>
	<td class="line x" title="53:212	Let yi be the tree of the i-th (1  i  n) sentence in CDS." ></td>
	<td class="line x" title="54:212	To evaluate the quality of xi,t as a conversion candidate for yi, we convert xi,t to a dependency tree (denoted as xDSi,t ) and then use unlabeled dependency f-score to measure the similarity between xDSi,t and yi." ></td>
	<td class="line x" title="55:212	Let Score(xi,t) denote the unlabeled dependency f-score of xDSi,t against yi." ></td>
	<td class="line x" title="56:212	Then we determine the converted tree for yi by maximizing Score(xi,t) over the N-best parses." ></td>
	<td class="line x" title="57:212	The conversion from PS to DS works as follows: Step 1." ></td>
	<td class="line x" title="58:212	Use a head percolation table to find the head of each constituent in xi,t. Step 2." ></td>
	<td class="line x" title="59:212	Make the head of each non-head child depend on the head of the head child for each constituent." ></td>
	<td class="line x" title="60:212	Unlabeled dependency f-score is a harmonic mean of unlabeled dependency precision and unlabeled dependency recall." ></td>
	<td class="line x" title="61:212	Precision measures how many head-dependent word pairs found in xDSi,t are correct and recall is the percentage of headdependent word pairs defined in the gold-standard tree that are found in xDSi,t . Here we do not take dependency tags into consideration for evaluation since they cannot be obtained without more sophisticated rules." ></td>
	<td class="line x" title="62:212	To improve the quality of N-best parses, we attempt to use the converted trees as additional training data to retrain the parser." ></td>
	<td class="line x" title="63:212	The procedure of tree conversion and parser retraining can be run iteratively until a termination condition is satisfied." ></td>
	<td class="line x" title="64:212	Here we use the parsers f-score on CPS,dev as a termination criterion." ></td>
	<td class="line x" title="65:212	If the update of training data hurts the performance on CPS,dev, then we stop the iteration." ></td>
	<td class="line x" title="66:212	Table 1 shows this DS to PS conversion algorithm." ></td>
	<td class="line x" title="67:212	Q is an upper limit of the number of loops, and Q  0." ></td>
	<td class="line x" title="68:212	2.2 Target Grammar Parsing Through grammar formalism conversion, we have successfully turned the problem of using heterogeneous treebanks for parsing into the problem of parsing on homogeneous treebanks." ></td>
	<td class="line x" title="69:212	Before using converted source treebank for parsing, we present two strategies to refine conversion results." ></td>
	<td class="line x" title="70:212	InstancePruning For some sentences in CDS, the parser might fail to generate high quality N-best parses, resulting in inferior converted trees." ></td>
	<td class="line x" title="71:212	To clean the converted treebank, we can remove the converted trees with low unlabeled dependency f-scores (defined in Section 2.1) before using the converted treebank for parser training 48 Figure 1: A parse tree in CTB for a sentence of /.<world>  <every> I<country> < <people>  <all> r<with> 81<eyes>   <cast>  l<Hong Kong>0with /People from all over the world are casting their eyes on Hong Kong0as its English translation." ></td>
	<td class="line x" title="72:212	because these trees are/misleading0training instances." ></td>
	<td class="line x" title="73:212	The number of removed trees will be determined by cross validation on development set." ></td>
	<td class="line x" title="74:212	ScoreInterpolation Unlabeled dependency f-scores used in Section 2.1 measure the quality of converted trees from the perspective of the source grammar only." ></td>
	<td class="line x" title="75:212	In extreme cases, the top best parses in the N-best list are good conversion candidates but we might select a parse ranked quite low in the N-best list since there might be conflicts of syntactic structure definition between the source grammar and the target grammar." ></td>
	<td class="line x" title="76:212	Figure 1 shows an example for illustration of a conflict between the grammar of CDT and that of CTB." ></td>
	<td class="line x" title="77:212	According to Chinese head percolation tables used in the PS to DS conversion tool /Penn2Malt03 and Charniaks parser4, the head of VP-2 is the word /r0(a preposition, with /BA0as its POS tag in CTB), and the head of IP-OBJ is   0." ></td>
	<td class="line x" title="78:212	Therefore the word /  0depends on the word/r0." ></td>
	<td class="line x" title="79:212	But according to the annotation scheme in CDT (Liu et al., 2006), the word/r0is a dependent of the word/  0." ></td>
	<td class="line x" title="80:212	The conflicts between the two grammars may lead to the problem that the selected parses based on the information of the source grammar might not be preferred from the perspective of the 3Available at http://w3.msi.vxu.se/nivre/." ></td>
	<td class="line x" title="81:212	4Available at http://www.cs.brown.edu/ec/." ></td>
	<td class="line x" title="82:212	target grammar." ></td>
	<td class="line x" title="83:212	Therefore we modified the selection metric in Section 2.1 by interpolating two scores, the probability of a conversion candidate from the parser and its unlabeled dependency f-score, shown as follows: hatwidestScore(xi,t) = Prob(xi,t)+(1)Score(xi,t)." ></td>
	<td class="line x" title="84:212	(1) The intuition behind this equation is that converted trees should be preferred from the perspective of both the source grammar and the target grammar." ></td>
	<td class="line x" title="85:212	Here 0    1." ></td>
	<td class="line x" title="86:212	Prob(xi,t) is a probability produced by the parser for xi,t (0  Prob(xi,t)  1)." ></td>
	<td class="line x" title="87:212	The value of  will be tuned by cross validation on development set." ></td>
	<td class="line x" title="88:212	After grammar formalism conversion, the problem now we face has been limited to how to build parsing models on multiple homogeneous treebank." ></td>
	<td class="line x" title="89:212	A possible solution is to simply concatenate the two treebanks as training data." ></td>
	<td class="line x" title="90:212	However this method may lead to a problem that if the size of CPS is significantly less than that of converted CDS, converted CDS may weaken the effect CPS might have." ></td>
	<td class="line x" title="91:212	One possible solution is to reduce the weight of examples from converted CDS in parser training." ></td>
	<td class="line x" title="92:212	Corpus weighting is exactly such an approach, with the weight tuned on development set, that will be used for parsing on homogeneous treebanks in this paper." ></td>
	<td class="line x" title="93:212	3 Experiments of Grammar Formalism Conversion 3.1 Evaluation on WSJ section 22 Xia et al.(2008) used WSJ section 19 from the Penn Treebank to extract DS to PS conversion rules and then produced dependency trees from WSJ section 22 for evaluation of their DS to PS conversion algorithm." ></td>
	<td class="line x" title="95:212	They showed that their conversion algorithm outperformed existing methods on the WSJ data." ></td>
	<td class="line x" title="96:212	For comparison with their work, we conducted experiments in the same setting as theirs: using WSJ section 19 (1844 sentences) as CPS, producing dependency trees from WSJ section 22 (1700 sentences) as CDS5, and using labeled bracketing f-scores from the tool /EVALB0on WSJ section 22 for performance evaluation." ></td>
	<td class="line x" title="97:212	5We used the tool/Penn2Malt0to produce dependency structures from the Penn Treebank, which was also used for PS to DS conversion in our conversion algorithm." ></td>
	<td class="line x" title="98:212	49 All the sentences DevScore LR LP F Models (%) (%) (%) (%) The best result of Xia et al.(2008) 90.7 88.1 89.4 Q-0-method 86.8 92.2 92.8 92.5 Q-10-method 88.0 93.4 94.1 93.8 Table 2: Comparison with the work of Xia et al.(2008) on WSJ section 22." ></td>
	<td class="line x" title="101:212	All the sentences DevScore LR LP F Models (%) (%) (%) (%) Q-0-method 91.0 91.6 92.5 92.1 Q-10-method 91.6 93.1 94.1 93.6 Table 3: Results of our algorithm on WSJ section 218 and 2022." ></td>
	<td class="line x" title="102:212	We employed Charniaks maximum entropy inspired parser (Charniak, 2000) to generate N-best (N=200) parses." ></td>
	<td class="line x" title="103:212	Xia et al.(2008) used POS tag information, dependency structures and dependency tags in test set for conversion." ></td>
	<td class="line x" title="105:212	Similarly, we used POS tag information in the test set to restrict search space of the parser for generation of better N-best parses." ></td>
	<td class="line x" title="106:212	We evaluated two variants of our DS to PS conversion algorithm: Q-0-method: We set the value of Q as 0 for a baseline method." ></td>
	<td class="line x" title="107:212	Q-10-method: We set the value of Q as 10 to see whether it is helpful for conversion to retrain the parser on converted trees." ></td>
	<td class="line x" title="108:212	Table 2 shows the results of our conversion algorithm on WSJ section 22." ></td>
	<td class="line x" title="109:212	In the experiment of Q-10-method, DevScore reached the highest value of 88.0% when q was 1." ></td>
	<td class="line x" title="110:212	Then we used CDS,1PS as the conversion result." ></td>
	<td class="line x" title="111:212	Finally Q-10method achieved an f-score of 93.8% on WSJ section 22, an absolute 4.4% improvement (42% error reduction) over the best result of Xia et al.(2008)." ></td>
	<td class="line x" title="113:212	Moreover, Q-10-method outperformed Q0-method on the same test set." ></td>
	<td class="line x" title="114:212	These results indicate that it is feasible to use a parser for DS to PS conversion and the conversion benefits from the use of converted trees for parser retraining." ></td>
	<td class="line x" title="115:212	3.2 Evaluation on WSJ section 218 and 2022 In this experiment we evaluated our conversion algorithm on a larger test set, WSJ section 218 and 2022 (totally 39688 sentences)." ></td>
	<td class="line x" title="116:212	Here we also used WSJ section 19 as CPS." ></td>
	<td class="line x" title="117:212	Other settings for All the sentences LR LP F Training data (%) (%) (%) 1CTB +CDTPS 84.7 85.1 84.9 2CTB +CDTPS 85.1 85.6 85.3 5CTB +CDTPS 85.0 85.5 85.3 10CTB +CDTPS 85.3 85.8 85.6 20CTB +CDTPS 85.1 85.3 85.2 50CTB +CDTPS 84.9 85.3 85.1 Table 4: Results of the generative parser on the development set, when trained with various weighting of CTB training set and CDTPS." ></td>
	<td class="line x" title="118:212	this experiment are as same as that in Section 3.1, except that here we used a larger test set." ></td>
	<td class="line x" title="119:212	Table 3 provides the f-scores of our method with Q equal to 0 or 10 on WSJ section 218 and 2022." ></td>
	<td class="line x" title="120:212	With Q-10-method, DevScore reached the highest value of 91.6% when q was 1." ></td>
	<td class="line x" title="121:212	Finally Q10-method achieved an f-score of 93.6% on WSJ section 218 and 2022, better than that of Q-0method and comparable with that of Q-10-method in Section 3.1." ></td>
	<td class="line x" title="122:212	It confirms our previous finding that the conversion benefits from the use of converted trees for parser retraining." ></td>
	<td class="line x" title="123:212	4 Experiments of Parsing We investigated our two-step solution on two existing treebanks, CDT and CTB, and we used CDT as the source treebank and CTB as the target treebank." ></td>
	<td class="line x" title="124:212	CDT consists of 60k Chinese sentences, annotated with POS tag information and dependency structure information (including 28 POS tags, and 24 dependency tags) (Liu et al., 2006)." ></td>
	<td class="line x" title="125:212	We did not use POS tag information as inputs to the parser in our conversion method due to the difficulty of conversion from CDT POS tags to CTB POS tags." ></td>
	<td class="line x" title="126:212	We used a standard split of CTB for performance evaluation, articles 1-270 and 400-1151 as training set, articles 301-325 as development set, and articles 271-300 as test set." ></td>
	<td class="line x" title="127:212	We used Charniaks maximum entropy inspired parser and their reranker (Charniak and Johnson, 2005) for target grammar parsing, called a generative parser (GP) and a reranking parser (RP) respectively." ></td>
	<td class="line x" title="128:212	We reported ParseVal measures from the EVALB tool." ></td>
	<td class="line x" title="129:212	50 All the sentences LR LP F Models Training data (%) (%) (%) GP CTB 79.9 82.2 81.0 RP CTB 82.0 84.6 83.3 GP 10CTB +CDTPS 80.4 82.7 81.5 RP 10CTB +CDTPS 82.8 84.7 83.8 Table 5: Results of the generative parser (GP) and the reranking parser (RP) on the test set, when trained on only CTB training set or an optimal combination of CTB training set and CDTPS." ></td>
	<td class="line x" title="130:212	4.1 Results of a Baseline Method to Use CDT We used our conversion algorithm6 to convert the grammar formalism of CDT to that of CTB." ></td>
	<td class="line x" title="131:212	Let CDTPS denote the converted CDT by our method." ></td>
	<td class="line x" title="132:212	The average unlabeled dependency f-score of trees in CDTPS was 74.4%, and their average index in 200-best list was 48." ></td>
	<td class="line x" title="133:212	We tried the corpus weighting method when combining CDTPS with CTB training set (abbreviated as CTB for simplicity) as training data, by gradually increasing the weight (including 1, 2, 5, 10, 20, 50) of CTB to optimize parsing performance on the development set." ></td>
	<td class="line x" title="134:212	Table 4 presents the results of the generative parser with various weights of CTB on the development set." ></td>
	<td class="line x" title="135:212	Considering the performance on the development set, we decided to give CTB a relative weight of 10." ></td>
	<td class="line x" title="136:212	Finally we evaluated two parsing models, the generative parser and the reranking parser, on the test set, with results shown in Table 5." ></td>
	<td class="line x" title="137:212	When trained on CTB only, the generative parser and the reranking parser achieved f-scores of 81.0% and 83.3%." ></td>
	<td class="line x" title="138:212	The use of CDTPS as additional training data increased f-scores of the two models to 81.5% and 83.8%." ></td>
	<td class="line x" title="139:212	4.2 Results of Two Strategies for a Better Use of CDT 4.2.1 Instance Pruning We used unlabeled dependency f-score of each converted tree as the criterion to rank trees in CDTPS and then kept only the top M trees with high f-scores as training data for parsing, resulting in a corpus CDTPSM . M varied from 100%|CDTPS| to 10%|CDTPS| with 10%|CDTPS| as the interval." ></td>
	<td class="line x" title="140:212	|CDTPS| 6The setting for our conversion algorithm in this experiment was as same as that in Section 3.1." ></td>
	<td class="line x" title="141:212	In addition, we used CTB training set as CPS,train, and CTB development set as CPS,dev." ></td>
	<td class="line x" title="142:212	All the sentences LR LP F Models Training data (%) (%) (%) GP CTB +CDTPS 81.4 82.8 82.1 RP CTB +CDTPS 83.0 85.4 84.2 Table 6: Results of the generative parser and the reranking parser on the test set, when trained on an optimal combination of CTB training set and converted CDT." ></td>
	<td class="line x" title="143:212	is the number of trees in CDTPS." ></td>
	<td class="line x" title="144:212	Then we tuned the value of M by optimizing the parsers performance on the development set with 10CTB+CDTPSM as training data." ></td>
	<td class="line x" title="145:212	Finally the optimal value of M was 100%|CDT|." ></td>
	<td class="line x" title="146:212	It indicates that even removing very few converted trees hurts the parsing performance." ></td>
	<td class="line x" title="147:212	A possible reason is that most of non-perfect parses can provide useful syntactic structure information for building parsing models." ></td>
	<td class="line x" title="148:212	4.2.2 Score Interpolation We used hatwidestScore(xi,t)7 to replace Score(xi,t) in our conversion algorithm and then ran the updated algorithm on CDT." ></td>
	<td class="line x" title="149:212	Let CDTPS denote the converted CDT by this updated conversion algorithm." ></td>
	<td class="line x" title="150:212	The values of  (varying from 0.0 to 1.0 with 0.1 as the interval) and the CTB weight (including 1, 2, 5, 10, 20, 50) were simultaneously tuned on the development set8." ></td>
	<td class="line x" title="151:212	Finally we decided that the optimal value of  was 0.4 and the optimal weight of CTB was 1, which brought the best performance on the development set (an f-score of 86.1%)." ></td>
	<td class="line x" title="152:212	In comparison with the results in Section 4.1, the average index of converted trees in 200-best list increased to 2, and their average unlabeled dependency f-score dropped to 65.4%." ></td>
	<td class="line x" title="153:212	It indicates that structures of converted trees become more consistent with the target grammar, as indicated by the increase of average index of converted trees, further away from the source grammar." ></td>
	<td class="line x" title="154:212	Table 6 provides f-scores of the generative parser and the reranker on the test set, when trained on CTB and CDTPS . We see that the performance of the reranking parser increased to 7Before calculating hatwidestScore(xi,t), we normalized the values of Prob(xi,t) for each N-best list by (1) Prob(xi,t)=Prob(xi,t)-Min(Prob(xi,)), (2)Prob(xi,t)=Prob(xi,t)/Max(Prob(xi,)), resulting in that their maximum value was 1 and their minimum value was 0." ></td>
	<td class="line x" title="155:212	8Due to space constraint, we do not show f-scores of the parser with different values of  and the CTB weight." ></td>
	<td class="line x" title="156:212	51 All the sentences LR LP F Models Training data (%) (%) (%) Self-trained GP 10T+10D+P 83.0 84.5 83.7 Updated RP CTB+CDTPS 84.3 86.1 85.2 Table 7: Results of the self-trained generative parser and updated reranking parser on the test set." ></td>
	<td class="line x" title="157:212	10T+10D+P stands for 10CTB+10CDTPS +PDC." ></td>
	<td class="line x" title="158:212	84.2% f-score, better than the result of the reranking parser with CTB and CDTPS as training data (shown in Table 5)." ></td>
	<td class="line x" title="159:212	It indicates that the use of probability information from the parser for tree conversion helps target grammar parsing." ></td>
	<td class="line pc" title="160:212	4.3 Using Unlabeled Data for Parsing Recent studies on parsing indicate that the use of unlabeled data by self-training can help parsing on the WSJ data, even when labeled data is relatively large (McClosky et al., 2006a; Reichart and Rappoport, 2007)." ></td>
	<td class="line x" title="161:212	It motivates us to employ self-training technique for Chinese parsing." ></td>
	<td class="line x" title="162:212	We used the POS tagged People Daily corpus9 (Jan. 1998Jun. 1998, and Jan. 2000Dec. 2000) (PDC) as unlabeled data for parsing." ></td>
	<td class="line x" title="163:212	First we removed the sentences with less than 3 words or more than 40 words from PDC to ease parsing, resulting in 820k sentences." ></td>
	<td class="line x" title="164:212	Then we ran the reranking parser in Section 4.2.2 on PDC and used the parses on PDC as additional training data for the generative parser." ></td>
	<td class="line x" title="165:212	Here we tried the corpus weighting technique for an optimal combination of CTB, CDTPS and parsed PDC, and chose the relative weight of both CTB and CDTPS as 10 by cross validation on the development set." ></td>
	<td class="line x" title="166:212	Finally we retrained the generative parser on CTB, CDTPS and parsed PDC." ></td>
	<td class="line x" title="167:212	Furthermore, we used this self-trained generative parser as a base parser to retrain the reranker on CTB and CDTPS . Table 7 shows the performance of self-trained generative parser and updated reranker on the test set, with CTB and CDTPS as labeled data." ></td>
	<td class="line x" title="168:212	We see that the use of unlabeled data by self-training further increased the reranking parsers performance from 84.2% to 85.2%." ></td>
	<td class="line oc" title="169:212	Our results on Chinese data confirm previous findings on English data shown in (McClosky et al., 2006a; Reichart and Rappoport, 2007)." ></td>
	<td class="line x" title="170:212	9Available at http://icl.pku.edu.cn/." ></td>
	<td class="line x" title="171:212	4.4 Comparison with Previous Studies for Chinese Parsing Table 8 and 9 present the results of previous studies on CTB." ></td>
	<td class="line x" title="172:212	All the works in Table 8 used CTB articles 1-270 as labeled data." ></td>
	<td class="line x" title="173:212	In Table 9, Petrov and Klein (2007) trained their model on CTB articles 1-270 and 400-1151, and Burkett and Klein (2008) used the same CTB articles and parse trees of their English translation (from the English Chinese Translation Treebank) as training data." ></td>
	<td class="line x" title="174:212	Comparing our result in Table 6 with that of Petrov and Klein (2007), we see that CDTPS helps parsing on CTB, which brought 0.9% f-score improvement." ></td>
	<td class="line x" title="175:212	Moreover, the use of unlabeled data further boosted the parsing performance to 85.2%, an absolute 1.0% improvement over the previous best result presented in Burkett and Klein (2008)." ></td>
	<td class="line x" title="176:212	5 Related Work Recently there have been some studies addressing how to use treebanks with same grammar formalism for domain adaptation of parsers." ></td>
	<td class="line x" title="177:212	Roark and Bachiani (2003) presented count merging and model interpolation techniques for domain adaptation of parsers." ></td>
	<td class="line x" title="178:212	They showed that their system with count merging achieved a higher performance when in-domain data was weighted more heavily than out-of-domain data." ></td>
	<td class="line oc" title="179:212	McClosky et al.(2006b) used self-training and corpus weighting to adapt their parser trained on WSJ corpus to Brown corpus." ></td>
	<td class="line o" title="181:212	Their results indicated that both unlabeled in-domain data and labeled out-of-domain data can help domain adaptation." ></td>
	<td class="line o" title="182:212	In comparison with these works, we conduct our study in a different setting where we work with multiple heterogeneous treebanks." ></td>
	<td class="line x" title="183:212	Grammar formalism conversion makes it possible to reuse existing source treebanks for the study of target grammar parsing." ></td>
	<td class="line x" title="184:212	Wang et al.(1994) employed a parser to help conversion of a treebank from a simple phrase structure to a more informative phrase structure and then used this converted treebank to train their parser." ></td>
	<td class="line x" title="186:212	Collins et al.(1999) performed statistical constituency parsing of Czech on a treebank that was converted from the Prague Dependency Treebank under the guidance of conversion rules and heuristic rules, e.g., one level of projection for any category, minimal projection for any dependents, and fixed position of attachment." ></td>
	<td class="line x" title="188:212	Xia and Palmer (2001) adopted better heuristic rules to build converted trees, which 52  40 words All the sentences LR LP F LR LP F Models (%) (%) (%) (%) (%) (%) Bikel & Chiang (2000) 76.8 77.8 77.3 Chiang & Bikel (2002) 78.8 81.1 79.9 Levy & Manning (2003) 79.2 78.4 78.8 Bikels thesis (2004) 78.0 81.2 79.6 Xiong et." ></td>
	<td class="line x" title="189:212	al." ></td>
	<td class="line x" title="190:212	(2005) 78.7 80.1 79.4 Chen et." ></td>
	<td class="line x" title="191:212	al." ></td>
	<td class="line x" title="192:212	(2005) 81.0 81.7 81.2 76.3 79.2 77.7 Wang et." ></td>
	<td class="line x" title="193:212	al." ></td>
	<td class="line x" title="194:212	(2006) 79.2 81.1 80.1 76.2 78.0 77.1 Table 8: Results of previous studies on CTB with CTB articles 1-270 as labeled data." ></td>
	<td class="line x" title="195:212	 40 words All the sentences LR LP F LR LP F Models (%) (%) (%) (%) (%) (%) Petrov & Klein (2007) 85.7 86.9 86.3 81.9 84.8 83.3 Burkett & Klein (2008) 84.2 Table 9: Results of previous studies on CTB with more labeled data." ></td>
	<td class="line x" title="196:212	reflected the structural preference in their target grammar." ></td>
	<td class="line x" title="197:212	For acquisition of better conversion rules, Xia et al.(2008) proposed to automatically extract conversion rules from a target treebank." ></td>
	<td class="line x" title="199:212	Moreover, they presented two strategies to solve the problem that there might be multiple conversion rules matching the same input dependency tree pattern: (1) choosing the most frequent rules, (2) preferring rules that add fewer number of nodes and attach the subtree lower." ></td>
	<td class="line x" title="200:212	In comparison with the works of Wang et al.(1994) and Collins et al.(1999), we went further by combining the converted treebank with the existing target treebank for parsing." ></td>
	<td class="line x" title="203:212	In comparison with previous conversion methods (Collins et al., 1999; Covington, 1994; Xia and Palmer, 2001; Xia et al., 2008) in which for each headdependent pair, only one locally optimal conversion was kept during tree-building process, we employed a parser to generate globally optimal syntactic structures, eliminating heuristic rules for conversion." ></td>
	<td class="line x" title="204:212	In addition, we used converted trees to retrain the parser for better conversion candidates, while Wang et al.(1994) did not exploit the use of converted trees for parser retraining." ></td>
	<td class="line x" title="206:212	6 Conclusion We have proposed a two-step solution to deal with the issue of using heterogeneous treebanks for parsing." ></td>
	<td class="line x" title="207:212	First we present a parser based method to convert grammar formalisms of the treebanks to the same one, without applying predefined heuristic rules, thus turning the original problem into the problem of parsing on homogeneous treebanks." ></td>
	<td class="line x" title="208:212	Then we present two strategies, instance pruning and score interpolation, to refine conversion results." ></td>
	<td class="line x" title="209:212	Finally we adopt the corpus weighting technique to combine the converted source treebank with the existing target treebank for parser training." ></td>
	<td class="line x" title="210:212	The study on the WSJ data shows the benefits of our parser based approach for grammar formalism conversion." ></td>
	<td class="line x" title="211:212	Moreover, experimental results on the Penn Chinese Treebank indicate that a converted dependency treebank helps constituency parsing, and it is better to exploit probability information produced by the parser through score interpolation than to prune low quality trees for the use of the converted treebank." ></td>
	<td class="line x" title="212:212	Future work includes further investigation of our conversion method for other pairs of grammar formalisms, e.g., from the grammar formalism of the Penn Treebank to more deep linguistic formalism like CCG, HPSG, or LFG." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1108
K-Best A* Parsing
Pauls, Adam;Klein, Dan;"></td>
	<td class="line x" title="1:221	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 958966, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:221	c2009 ACL and AFNLP K-Best A Parsing Adam Pauls and Dan Klein Computer Science Division University of California, Berkeley {adpauls,klein}@cs.berkeley.edu Abstract A parsing makes 1-best search efficient by suppressing unlikely 1-best items." ></td>
	<td class="line x" title="3:221	Existing kbest extraction methods can efficiently search for top derivations, but only after an exhaustive 1-best pass." ></td>
	<td class="line x" title="4:221	We present a unified algorithm for k-best A parsing which preserves the efficiency of k-best extraction while giving the speed-ups of A methods." ></td>
	<td class="line x" title="5:221	Our algorithm produces optimalk-best parses under the same conditions required for optimality in a 1-best A parser." ></td>
	<td class="line x" title="6:221	Empirically, optimal k-best lists can be extracted significantly faster than with other approaches, over a range of grammar types." ></td>
	<td class="line x" title="7:221	1 Introduction Many situations call for a parser to return the kbest parses rather than only the 1-best." ></td>
	<td class="line oc" title="8:221	Uses for k-best lists include minimum Bayes risk decoding (Goodman, 1998; Kumar and Byrne, 2004), discriminative reranking (Collins, 2000; Charniak and Johnson, 2005), and discriminative training (Och, 2003; McClosky et al., 2006)." ></td>
	<td class="line x" title="9:221	The most efficient known algorithm for k-best parsing (Jimenez and Marzal, 2000; Huang and Chiang, 2005) performs an initial bottom-up dynamic programming pass before extracting thek-best parses." ></td>
	<td class="line x" title="10:221	In that algorithm, the initial pass is, by far, the bottleneck (Huang and Chiang, 2005)." ></td>
	<td class="line x" title="11:221	In this paper, we propose an extension of A parsing which integratesk-best search with an Abased exploration of the 1-best chart." ></td>
	<td class="line x" title="12:221	A parsing can avoid significant amounts of computation by guiding 1-best search with heuristic estimates of parse completion costs, and has been applied successfully in several domains (Klein and Manning, 2002; Klein and Manning, 2003c; Haghighi et al., 2007)." ></td>
	<td class="line x" title="13:221	Our algorithm extends the speedups achieved in the 1-best case to the k-best case and is optimal under the same conditions as a standard A algorithm." ></td>
	<td class="line x" title="14:221	The amount of work done in the k-best phase is no more than the amount of work done by the algorithm of Huang and Chiang (2005)." ></td>
	<td class="line x" title="15:221	Our algorithm is also equivalent to standard A parsing (up to ties) if it is terminated after the 1-best derivation is found." ></td>
	<td class="line x" title="16:221	Finally, our algorithm can be written down in terms of deduction rules, and thus falls into the well-understood view of parsing as weighted deduction (Shieber et al., 1995; Goodman, 1998; Nederhof, 2003)." ></td>
	<td class="line x" title="17:221	In addition to presenting the algorithm, we show experiments in which we extract k-best lists for three different kinds of grammars: the lexicalized grammars of Klein and Manning (2003b), the state-split grammars of Petrov et al.(2006), and the tree transducer grammars of Galley et al.(2006)." ></td>
	<td class="line x" title="20:221	We demonstrate that optimal k-best lists can be extracted significantly faster using our algorithm than with previous methods." ></td>
	<td class="line x" title="21:221	2 A k-Best A Parsing Algorithm We build up to our full algorithm in several stages, beginning with standard 1-best A parsing and making incremental modifications." ></td>
	<td class="line x" title="22:221	2.1 Parsing as Weighted Deduction Our algorithm can be formulated in terms of prioritized weighted deduction rules (Shieber et al., 1995; Nederhof, 2003; Felzenszwalb and McAllester, 2007)." ></td>
	<td class="line x" title="23:221	A prioritized weighted deduction rule has the form 1 : w1,,n : wn p(w1,,wn)0 : g(w1,,wn) where 1,,n are the antecedent items of the deduction rule and 0 is the conclusion item." ></td>
	<td class="line x" title="24:221	A deduction rule states that, given the antecedents 1,,n with weights w1,,wn, the conclusion 0 can be formed with weight g(w1,,wn) and priority p(w1,,wn)." ></td>
	<td class="line x" title="25:221	958 These deduction rules are executed within a generic agenda-driven algorithm, which constructs items in a prioritized fashion." ></td>
	<td class="line x" title="26:221	The algorithm maintains an agenda (a priority queue of unprocessed items), as well as a chart of items already processed." ></td>
	<td class="line x" title="27:221	The fundamental operation of the algorithm is to pop the highest priority item  from the agenda, put it into the chart with its current weight, and form using deduction rules any items which can be built by combining  with items already in the chart." ></td>
	<td class="line x" title="28:221	If new or improved, resulting items are put on the agenda with priority given by p()." ></td>
	<td class="line x" title="29:221	2.2 AParsing The A parsing algorithm of Klein and Manning (2003c) can be formulated in terms of weighted deduction rules (Felzenszwalb and McAllester, 2007)." ></td>
	<td class="line x" title="30:221	We do so here both to introduce notation and to build to our final algorithm." ></td>
	<td class="line x" title="31:221	First, we must formalize some notation." ></td>
	<td class="line x" title="32:221	Assume we have a PCFG1 G and an input sentence s1sn of length n. The grammarG has a set of symbols , including a distinguished goal (root) symbol G. Without loss of generality, we assume Chomsky normal form, so each non-terminal rule r inGhas the formr = AB C with weightwr (the negative log-probability of the rule)." ></td>
	<td class="line x" title="33:221	Edges are labeled spans e = (A,i,j)." ></td>
	<td class="line x" title="34:221	Inside derivations of an edge (A,i,j) are trees rooted at A and spanning si+1sj." ></td>
	<td class="line x" title="35:221	The total weight of the best (minimum) inside derivation for an edge e is called the Viterbi inside score (e)." ></td>
	<td class="line x" title="36:221	The goal of the 1-best A parsing algorithm is to compute the Viterbi inside score of the edge (G,0,n); backpointers allow the reconstruction of a Viterbi parse in the standard way." ></td>
	<td class="line x" title="37:221	The basic A algorithm operates on deduction items I(A,i,j) which represent in a collapsed way the possible inside derivations of edges (A,i,j)." ></td>
	<td class="line x" title="38:221	We call these items inside edge items or simply inside items where clear; a graphical representation of an inside item can be seen in Figure 1(a)." ></td>
	<td class="line x" title="39:221	The space whose items are inside edges is called the edge space." ></td>
	<td class="line x" title="40:221	These inside items are combined using the single IN deduction schema shown in Table 1." ></td>
	<td class="line x" title="41:221	This schema is instantiated for every grammar rule r 1While we present the algorithm specialized to parsing with a PCFG, it generalizes to a wide range of hypergraph search problems as shown in Klein and Manning (2001)." ></td>
	<td class="line x" title="42:221	VP s3 s4 s5 s1 s2 s6 sn VP VBZ NP DT NN s3 s4 s5 VP G (a) (b) (c) VP VBZ1 NP4 DT NN s3 s4 s5 (e) VP6 s3 s4 s5 VBZ NP DT NN (d) Figure 1: Representations of the different types of items used in parsing." ></td>
	<td class="line x" title="43:221	(a) An inside edge item: I(VP,2,5)." ></td>
	<td class="line x" title="44:221	(b) An outside edge item: O(VP,2,5)." ></td>
	<td class="line x" title="45:221	(c) An inside derivation item: D(TVP,2,5) for a tree TVP." ></td>
	<td class="line x" title="46:221	(d) A ranked derivation item: K(VP,2,5,6)." ></td>
	<td class="line x" title="47:221	(e) A modified inside derivation item (with backpointers to ranked items): D(VP,2,5,3,VP  VBZ NP,1,4)." ></td>
	<td class="line x" title="48:221	in G. For IN, the function g() simply sums the weights of the antecedent items and the grammar rule r, while the priority function p() adds a heuristic to this sum." ></td>
	<td class="line x" title="49:221	The heuristic is a bound on the Viterbi outside score (e) of an edge e; see Klein and Manning (2003c) for details." ></td>
	<td class="line x" title="50:221	A good heuristic allows A to reach the goal item I(G,0,n) while constructing few inside items." ></td>
	<td class="line x" title="51:221	If the heuristic is consistent, then A guarantees that whenever an inside item comes off the agenda, its weight is its true Viterbi inside score (Klein and Manning, 2003c)." ></td>
	<td class="line x" title="52:221	In particular, this guarantee implies that the goal item I(G,0,n) will be popped with the score of the 1-best parse of the sentence." ></td>
	<td class="line x" title="53:221	Consistency also implies that items are popped off the agenda in increasing order of bounded Viterbi scores: (e) +h(e) We will refer to this monotonicity as the ordering property of A (Felzenszwalb and McAllester, 2007)." ></td>
	<td class="line x" title="54:221	One final property implied by consistency is admissibility, which states that the heuristic never overestimates the true Viterbi outside score for an edge, i.e. h(e)  (e)." ></td>
	<td class="line x" title="55:221	For the remainder of this paper, we will assume our heuristics are consistent." ></td>
	<td class="line x" title="56:221	2.3 A Naivek-Best AAlgorithm Due to the optimal substructure of 1-best PCFG derivations, a 1-best parser searches over the space of edges; this is the essence of 1-best dynamic programming." ></td>
	<td class="line x" title="57:221	Although most edges can be built 959 Inside Edge Deductions (Used in A and KA) IN: I(B,i,l) : w1 I(C,l,j) : w2 w1+w2+wr+h(A,i,j) I(A,i,j) : w1 +w2 +wr Table 1: The deduction schema (IN) for building inside edge items, using a supplied heuristic." ></td>
	<td class="line x" title="58:221	This schema is sufficient on its own for 1-best A, and it is used in KA." ></td>
	<td class="line x" title="59:221	Here, r is the rule AB C. Inside Derivation Deductions (Used in NAIVE) DERIV: D(TB,i,l) : w1 D(TC,l,j) : w2 w1+w2+wr+h(A,i,j) D parenleftBigg A TB TC ,i,j parenrightBigg : w1 +w2 +wr Table 2: The deduction schema for building derivations, using a supplied heuristic." ></td>
	<td class="line x" title="60:221	TB and TC denote full tree structures rooted at symbols B and C. This schema is the same as the IN deduction schema, but operates on the space of fully specified inside derivations rather than dynamic programming edges." ></td>
	<td class="line x" title="61:221	This schema forms the NAIVE k-best algorithm." ></td>
	<td class="line x" title="62:221	Outside Edge Deductions (Used in KA) OUT-B: I(G,0,n) : w1 w1 O(G,0,n) : 0 OUT-L: O(A,i,j) : w1 I(B,i,l) : w2 I(C,l,j) : w3 w1+w3+wr+w2 O(B,i,l) : w1 +w3 +wr OUT-R: O(A,i,j) : w1 I(B,i,l) : w2 I(C,l,j) : w3 w1+w2+wr+w3 O(C,l,j) : w1 +w2 +wr Table 3: The deduction schemata for building ouside edge items." ></td>
	<td class="line x" title="63:221	The first schema is a base case that constructs an outside item for the goal (G,0,n) from the inside item I(G,0,n)." ></td>
	<td class="line x" title="64:221	The second two schemata build outside items in a top-down fashion." ></td>
	<td class="line x" title="65:221	Note that for outside items, the completion cost is the weight of an inside item rather than a value computed by a heuristic." ></td>
	<td class="line x" title="66:221	Delayed Inside Derivation Deductions (Used in KA) DERIV: D(TB,i,l) : w1 D(TC,l,j) : w2 O(A,i,j) : w3 w1+w2+wr+w3 D parenleftBigg A TB TC ,i,j parenrightBigg : w1 +w2 +wrTable 4: The deduction schema for building derivations, using exact outside scores computed using OUT deduc-tions." ></td>
	<td class="line x" title="67:221	The dependency on the outside item O(A,i,j) delays building derivation items until exact Viterbi outside scores have been computed." ></td>
	<td class="line x" title="68:221	This is the final search space for the KA algorithm." ></td>
	<td class="line x" title="69:221	Ranked Inside Derivation Deductions (Lazy Version of NAIVE) BUILD: K(B,i,l,u) : w1 K(C,l,j,v) : w2 w1+w2+wr+h(A,i,j) D(A,i,j,l,r,u,v) : w1 +w2 +wr RANK: D1(A,i,j,) : w1  Dk(A,i,j,) : wk maxmwm+h(A,i,j) K(A,i,j,k) : maxmwm Table 5: The schemata for simultaneously building and ranking derivations, using a supplied heuristic, for the lazier form of the NAIVE algorithm." ></td>
	<td class="line x" title="70:221	BUILD builds larger derivations from smaller ones." ></td>
	<td class="line x" title="71:221	RANK numbers derivations for each edge." ></td>
	<td class="line x" title="72:221	Note that RANK requires distinct Di, so a rank k RANK rule will first apply (optimally) as soon as the kth-best inside derivation item for a given edge is removed from the queue." ></td>
	<td class="line x" title="73:221	However, it will also still formally apply (suboptimally) for all derivation items dequeued after the kth." ></td>
	<td class="line x" title="74:221	In practice, the RANK schema need not be implemented explicitly  one can simply assign a rank to each inside derivation item when it is removed from the agenda, and directly add the appropriate ranked inside item to the chart." ></td>
	<td class="line x" title="75:221	Delayed Ranked Inside Derivation Deductions (Lazy Version of KA) BUILD:K(B,i,l,u) :w1 K(C,l,j,v) :w2 O(A,i,j) :w3 w1+w2+wr+w3 D(A,i,j,l,r,u,v) : w1 +w2 +wr RANK:D1(A,i,j,) :w1 Dk(A,i,j,) :wk O(A,i,j) :wk+1 maxmwm+wk+1 K(A,i,j,k) : maxmwm Table 6: The deduction schemata for building and ranking derivations, using exact outside scores computed from OUT deductions, used for the lazier form of the KA algorithm." ></td>
	<td class="line x" title="76:221	960 using many derivations, each inside edge item will be popped exactly once during parsing, with a score and backpointers representing its 1-best derivation." ></td>
	<td class="line x" title="77:221	However, k-best lists involve suboptimal derivations." ></td>
	<td class="line x" title="78:221	One way to compute k-best derivations is therefore to abandon optimal substructure and dynamic programming entirely, and to search over the derivation space, the much larger space of fully specified trees." ></td>
	<td class="line x" title="79:221	The items in this space are called inside derivation items, or derivation items where clear, and are of the form D(TA,i,j), specifying an entire tree TA rooted at symbol A and spanning si+1sj (see Figure 1(c))." ></td>
	<td class="line x" title="80:221	Derivation items are combined using the DERIV schema of Table 2." ></td>
	<td class="line x" title="81:221	The goals in this space, representing root parses, are any derivation items rooted at symbol G that span the entire input." ></td>
	<td class="line x" title="82:221	In this expanded search space, each distinct parse has its own derivation item, derivable only in one way." ></td>
	<td class="line x" title="83:221	If we continue to search long enough, we will pop multiple goal items." ></td>
	<td class="line x" title="84:221	The first k which come off the agenda will be thek-best derivations." ></td>
	<td class="line x" title="85:221	We refer to this approach as NAIVE." ></td>
	<td class="line x" title="86:221	It is very inefficient on its own, but it leads to the full algorithm." ></td>
	<td class="line x" title="87:221	The correctness of thisk-best algorithm follows from the correctness of A parsing." ></td>
	<td class="line x" title="88:221	The derivation space of full trees is simply the edge space of a much larger grammar (see Section 2.5)." ></td>
	<td class="line x" title="89:221	Note that the DERIV schemas priority includes a heuristic just like 1-best A." ></td>
	<td class="line x" title="90:221	Because of the context freedom of the grammar, any consistent heuristic for inside edge items usable in 1-best A is also consistent for inside derivation items (and vice versa)." ></td>
	<td class="line x" title="91:221	In particular, the 1-best Viterbi outside score for an edge is a perfect heuristic for any derivation of that edge." ></td>
	<td class="line x" title="92:221	While correct, NAIVE is massively inefficient." ></td>
	<td class="line x" title="93:221	In comparison with Aparsing overG, where there are O(n2) inside items, the size of the derivation space is exponential in the sentence length." ></td>
	<td class="line x" title="94:221	By the ordering property, we know that NAIVE will process all derivation items d with (d) +h(d)(gk) where gk is the kth-best root parse and () is the inside score of a derivation item (analogous to  for edges).2 Even for reasonable heuristics, this 2The new symbol emphasizes that  scores a specific derivation rather than a minimum over a set of derivations." ></td>
	<td class="line x" title="95:221	number can be very large; see Section 3 for empirical results." ></td>
	<td class="line x" title="96:221	This naive algorithm is, of course, not novel, either in general approach or specific computation." ></td>
	<td class="line x" title="97:221	Earlyk-best parsers functioned by abandoning dynamic programming and performing beam search on derivations (Ratnaparkhi, 1999; Collins, 2000)." ></td>
	<td class="line x" title="98:221	Huang (2005) proposes an extension of Knuths algorithm (Knuth, 1977) to produce k-best lists by searching in the space of derivations, which is essentially this algorithm." ></td>
	<td class="line x" title="99:221	While Huang (2005) makes no explicit mention of a heuristic, it would be easy to incorporate one into their formulation." ></td>
	<td class="line x" title="100:221	2.4 A Newk-Best AParser While NAIVE suffers severe performance degradation for loose heuristics, it is in fact very efficient if h() is perfect, i.e. h(e) = (e)e. In this case, the ordering property of A guarantees that only inside derivation items d satisfying (d) +(d)(gk) will be placed in the chart." ></td>
	<td class="line x" title="101:221	The set of derivation items d satisfying this inequality is exactly the set which appear in the k-best derivations of (G,0,n) (as always, modulo ties)." ></td>
	<td class="line x" title="102:221	We could therefore use NAIVE quite efficiently if we could obtain exact Viterbi outside scores." ></td>
	<td class="line x" title="103:221	One option is to compute outside scores with exhaustive dynamic programming over the original grammar." ></td>
	<td class="line x" title="104:221	In a certain sense, described in greater detail below, this precomputation of exact heuristics is equivalent to the k-best extraction algorithm of Huang and Chiang (2005)." ></td>
	<td class="line x" title="105:221	However, this exhaustive 1-best work is precisely what we want to use A to avoid." ></td>
	<td class="line x" title="106:221	Our algorithm solves this problem by integrating three searches into a single agenda-driven process." ></td>
	<td class="line x" title="107:221	First, an A search in the space of inside edge items with an (imperfect) external heuristic h() finds exact inside scores." ></td>
	<td class="line x" title="108:221	Second, exact outside scores are computed from inside and outside items." ></td>
	<td class="line x" title="109:221	Finally, these exact outside scores guide the search over derivations." ></td>
	<td class="line x" title="110:221	It can be useful to imagine these three operations as operating in phases, but they are all interleaved, progressing in order of their various priorities." ></td>
	<td class="line x" title="111:221	In order to calculate outside scores, we introduce outside items O(A,i,j), which represent best derivations of G  s1si A sj+1sn; see Figure 1(b)." ></td>
	<td class="line x" title="112:221	Where the weights of inside items 961 compute Viterbi inside scores, the weights of outside items compute Viterbi outside scores." ></td>
	<td class="line x" title="113:221	Table 3 shows deduction schemata for building outside items." ></td>
	<td class="line x" title="114:221	These schemata are adapted from the schemata used in the general hierarchical A algorithm of Felzenszwalb and McAllester (2007)." ></td>
	<td class="line x" title="115:221	In that work, it is shown that such schemata maintain the property that the weight of an outside item is the true Viterbi outside score when it is removed from the agenda." ></td>
	<td class="line x" title="116:221	They also show that outside items o follow an ordering property, namely that they are processed in increasing order of (o) +(o) This quantity is the score of the best root derivation which includes the edge corresponding to o. Felzenszwalb and McAllester (2007) also show that both inside and outside items can be processed on the same queue and the ordering property holds jointly for both types of items." ></td>
	<td class="line x" title="117:221	If we delay the construction of a derivation item until its corresponding outside item has been popped, then we can gain the benefits of using an exact heuristic h() in the naive algorithm." ></td>
	<td class="line x" title="118:221	We realize this delay by modifying the DERIV deduction schema as shown in Table 4 to trigger on and prioritize with the appropriate outside scores." ></td>
	<td class="line x" title="119:221	We now have our final algorithm, which we call KA." ></td>
	<td class="line x" title="120:221	It is the union of the IN, OUT, and new delayed DERIV deduction schemata." ></td>
	<td class="line x" title="121:221	In words, our algorithm functions as follows: we initialize the agenda with I(si,i1,i) and D(si,i1,i) for i = 1n." ></td>
	<td class="line x" title="122:221	We compute inside scores in standard A fashion using the IN deduction rule, using any heuristic we might provide to 1-best A." ></td>
	<td class="line x" title="123:221	Once the inside item I(G,0,n) is found, we automatically begin to compute outside scores via the OUT deduction rules." ></td>
	<td class="line x" title="124:221	Once O(si,i1,i) is found, we can begin to also search in the space of derivation items, using the perfect heuristics given by the just-computed outside scores." ></td>
	<td class="line x" title="125:221	Note, however, that all computation is done with a single agenda, so the processing of all three types of items is interleaved, with the k-best search possibly terminating without a full inside computation." ></td>
	<td class="line x" title="126:221	As with NAIVE, the algorithm terminates when ak-th goal derivation is dequeued." ></td>
	<td class="line x" title="127:221	2.5 Correctness We prove the correctness of this algorithm by a reduction to the hierarchical A (HA) algorithm of Felzenszwalb and McAllester (2007)." ></td>
	<td class="line x" title="128:221	The input to HA is a target grammarGm and a list of grammarsG0Gm1 in whichGt1 is a relaxed projection ofGt for all t = 1m." ></td>
	<td class="line x" title="129:221	A grammarGt1 is a projection ofGt if there exists some onto function pit : tmapstot1 defined for all symbols inGt." ></td>
	<td class="line x" title="130:221	We use At1 to represent pit(At)." ></td>
	<td class="line x" title="131:221	A projection is relaxed if, for every rule r = At  BtCt with weight wr there is a rule rprime = At1 Bt1Ct1 inGt1 with weight wrprimewr." ></td>
	<td class="line x" title="132:221	We assume that our external heuristic function h() is constructed by parsing our input sentence with a relaxed projection of our target grammar." ></td>
	<td class="line x" title="133:221	This assumption, though often true anyway, is to allow proof by reduction to Felzenszwalb and McAllester (2007).3 We construct an instance of HAas follows: Let G0 be the relaxed projection which computes the heuristic." ></td>
	<td class="line x" title="134:221	LetG1 be the input grammarG, and let G2, the target grammar of our HAinstance, be the grammar of derivations inGformed by expanding each symbol A in G to all possible inside derivationsTA rooted atA." ></td>
	<td class="line x" title="135:221	The rules inG2 have the form TA TB TC with weight given by the weight of the rule AB C. By construction, G1 is a relaxed projection of G2; by assumption G0 is a relaxed projection of G1." ></td>
	<td class="line x" title="136:221	The deduction rules that describe KA build the same items as HA with same weights and priorities, and so the guarantees from HA carry over to KA." ></td>
	<td class="line x" title="137:221	We can characterize the amount of work done using the ordering property." ></td>
	<td class="line x" title="138:221	Let gk be the kth-best derivation item for the goal edge g. Our algorithm processes all derivation items d, outside items o, and inside items i satisfying (d) +(d)  (gk) (o) +(o)  (gk) (i) +h(i)  (gk) We have already argued that the set of derivation items satisfying the first inequality is the set of subtrees that appear in the optimal k-best parses, modulo ties." ></td>
	<td class="line x" title="139:221	Similarly, it can be shown that the second inequality is satisfied only for edges that appear in the optimal k-best parses." ></td>
	<td class="line x" title="140:221	The last inequality characterizes the amount of work done in the bottom-up pass." ></td>
	<td class="line x" title="141:221	We compare this to 1-best A, which pops all inside items i satisfying (i) +h(i)(g) = (g1) 3KA is correct for any consistent heuristic but a nonreductive proof is not possible in the present space." ></td>
	<td class="line x" title="142:221	962 Thus, the extra inside items popped in the bottom-up pass duringk-best parsing as compared to 1-best parsing are those items i satisfying (g1)(i) +h(i)(gk) The question of how many items satisfy these inequalities is empirical; we show in our experiments that it is small for reasonable heuristics." ></td>
	<td class="line x" title="143:221	At worst, the bottom-up phase pops all inside items and reduces to exhaustive dynamic programming." ></td>
	<td class="line x" title="144:221	Additionally, it is worth noting that our algorithm is naturally online in that it can be stopped at any k without advance specification." ></td>
	<td class="line x" title="145:221	2.6 Lazy Successor Functions The global ordering property guarantees that we will only dequeue derivation fragments of top parses." ></td>
	<td class="line x" title="146:221	However, we will enqueue all combinations of such items, which is wasteful." ></td>
	<td class="line x" title="147:221	By exploiting a local ordering amongst derivations, we can be more conservative about combination and gain the advantages of a lazy successor function (Huang and Chiang, 2005)." ></td>
	<td class="line x" title="148:221	To do so, we represent inside derivations not by explicitly specifying entire trees, but rather by using ranked backpointers." ></td>
	<td class="line x" title="149:221	In this representation, inside derivations are represented in two ways, shown in Figure 1(d) and (e)." ></td>
	<td class="line x" title="150:221	The first way (d) simply adds a rank u to an edge, giving a tuple (A,i,j,u)." ></td>
	<td class="line x" title="151:221	The corresponding item is the ranked derivation item K(A,i,j,u), which represents the uth-best derivation of A over (i,j)." ></td>
	<td class="line x" title="152:221	The second representation (e) is a backpointer of the form (A,i,j,l,r,u,v), specifying the derivation formed by combining the uth-best derivation of (B,i,l) and the vth-best derivation of (C,l,j) using rule r = AB C. The corresponding items D(A,i,j,l,r,u,v) are the new form of our inside derivation items." ></td>
	<td class="line x" title="153:221	The modified deduction schemata for the NAIVE algorithm over these representations are shown in Table 5." ></td>
	<td class="line x" title="154:221	The BUILD schema produces new inside derivation items from ranked derivation items, while the RANK schema assigns each derivation item a rank; together they function like DERIV." ></td>
	<td class="line x" title="155:221	We can find the k-best list by searching until K(G,0,n,k) is removed from the agenda." ></td>
	<td class="line x" title="156:221	The k-best derivations can then be extracted by following the backpointers for K(G,0,n,1)  K(G,0,n,k)." ></td>
	<td class="line x" title="157:221	The KA algorithm can be modified in the same way, shown in Table 6." ></td>
	<td class="line x" title="158:221	1 5 50 500 Heuristic Derivation items pushed (millions) 5-split4-split3-split2-split1-split0-split NAIVE KA* Figure 2: Number of derivation items enqueued as a function of heuristic." ></td>
	<td class="line x" title="159:221	Heuristics are shown in decreasing order of tightness." ></td>
	<td class="line x" title="160:221	The y-axis is on a log-scale." ></td>
	<td class="line x" title="161:221	The actual laziness is provided by additionally delaying the combination of ranked items." ></td>
	<td class="line x" title="162:221	When an item K(B,i,l,u) is popped off the queue, a naive implementation would loop over items K(C,l,j,v) for all v, C, and j (and similarly for left combinations)." ></td>
	<td class="line x" title="163:221	Fortunately, little looping is actually necessary: there is a partial ordering of derivation items, namely, that D(A,i,j,l,r,u,v) will have a lower computed priority than D(A,i,j,l,r,u 1,v) and D(A,i,j,l,r,u,v  1) (Jimenez and Marzal, 2000)." ></td>
	<td class="line x" title="164:221	So, we can wait until one of the latter two is built before triggering the construction of the former." ></td>
	<td class="line x" title="165:221	This triggering is similar to the lazy frontier used by Huang and Chiang (2005)." ></td>
	<td class="line x" title="166:221	All of our experiments use this lazy representation." ></td>
	<td class="line x" title="167:221	3 Experiments 3.1 State-Split Grammars We performed our first experiments with the grammars of Petrov et al.(2006)." ></td>
	<td class="line x" title="169:221	The training procedure for these grammars produces a hierarchy of increasingly refined grammars through statesplitting." ></td>
	<td class="line x" title="170:221	We followed Pauls and Klein (2009) in computing heuristics for the most refined grammar from outside scores for less-split grammars." ></td>
	<td class="line x" title="171:221	We used the Berkeley Parser4 to learn such grammars from Sections 2-21 of the Penn Treebank (Marcus et al., 1993)." ></td>
	<td class="line x" title="172:221	We trained with 6 split-merge cycles, producing 7 grammars." ></td>
	<td class="line x" title="173:221	We tested these grammars on 100 sentences of length at most 30 of Section 23 of the Treebank." ></td>
	<td class="line x" title="174:221	Our target grammar was in all cases the most split grammar." ></td>
	<td class="line x" title="175:221	4http://berkeleyparser.googlecode.com 963 0 200040006000800010000 0 5000 15000 25000 KA* k Items pushed (millions) K Best Bottom-up Heuristic 0 200040006000800010000 0 5000 15000 25000 EXH k Items pushed (millions) K Best Bottom-up Figure 3: The cost of k-best extraction as a function of k for state-split grammars, for both KA and EXH." ></td>
	<td class="line x" title="176:221	The amount of time spent in the k-best phase is negligible compared to the cost of the bottom-up phase in both cases." ></td>
	<td class="line x" title="177:221	Heuristics computed from projections to successively smaller grammars in the hierarchy form successively looser bounds on the outside scores." ></td>
	<td class="line x" title="178:221	This allows us to examine the performance as a function of the tightness of the heuristic." ></td>
	<td class="line x" title="179:221	We first compared our algorithm KA against the NAIVE algorithm." ></td>
	<td class="line x" title="180:221	We extracted 1000-best lists using each algorithm, with heuristics computed using each of the 6 smaller grammars." ></td>
	<td class="line x" title="181:221	In Figure 2, we evaluate only the k-best extraction phase by plotting the number of derivation items and outside items added to the agenda as a function of the heuristic used, for increasingly loose heuristics." ></td>
	<td class="line x" title="182:221	We follow earlier work (Pauls and Klein, 2009) in using number of edges pushed as the primary, hardware-invariant metric for evaluating performance of our algorithms.5 While KA scales roughly linearly with the looseness of the heuristic, NAIVE degrades very quickly as the heuristics get worse." ></td>
	<td class="line x" title="183:221	For heuristics given by grammars weaker than the 4-split grammar, NAIVE ran out of memory." ></td>
	<td class="line x" title="184:221	Since the bottom-up pass of k-best parsing is the bottleneck, we also examine the time spent in the 1-best phase of k-best parsing." ></td>
	<td class="line x" title="185:221	As a baseline, we compared KA to the approach of Huang and Chiang (2005), which we will call EXH (see below for more explanation) since it requires exhaustive parsing in the bottom-up pass." ></td>
	<td class="line x" title="186:221	We performed the exhaustive parsing needed for EXH in our agenda-based parser to facilitate comparison." ></td>
	<td class="line x" title="187:221	For KA, we included the cost of computing the heuristic, which was done by running our agenda-based parser exhaustively on a smaller grammar to compute outside items; we chose the 5We found that edges pushed was generally well correlated with parsing time." ></td>
	<td class="line x" title="188:221	0 200040006000800010000 0 200 600 1000 KA* k Items pushed (millions) K Best Bottom-up Heuristic Figure 4: The performance of KA for lexicalized grammars." ></td>
	<td class="line x" title="189:221	The performance is dominated by the computation of the heuristic, so that both the bottom-up phase and the k-best phase are barely visible." ></td>
	<td class="line x" title="190:221	3-split grammar for the heuristic since it gives the best overall tradeoff of heuristic and bottom-up parsing time." ></td>
	<td class="line x" title="191:221	We separated the items enqueued into items enqueued while computing the heuristic (not strictly part of the algorithm), inside items (bottom-up), and derivation and outside items (together k-best)." ></td>
	<td class="line x" title="192:221	The results are shown in Figure 3." ></td>
	<td class="line x" title="193:221	The cost of k-best extraction is clearly dwarfed by the the 1-best computation in both cases." ></td>
	<td class="line x" title="194:221	However, KA is significantly faster over the bottom-up computations, even when the cost of computing the heuristic is included." ></td>
	<td class="line x" title="195:221	3.2 Lexicalized Parsing We also experimented with the lexicalized parsing model described in Klein and Manning (2003b)." ></td>
	<td class="line x" title="196:221	This model is constructed as the product of a dependency model and the unlexicalized PCFG model in Klein and Manning (2003a)." ></td>
	<td class="line x" title="197:221	We 964 0 200040006000800010000 0 500 1500 2500 KA* k Items pushed (millions) K Best Bottom-up Heuristic 0 200040006000800010000 0 500 1500 2500 EXH k Items pushed (millions) K Best Bottom-up Figure 5: k-best extraction as a function of k for tree transducer grammars, for both KA and EXH." ></td>
	<td class="line x" title="198:221	constructed these grammars using the Stanford Parser.6 The model was trained on Sections 2-20 of the Penn Treebank and tested on 100 sentences of Section 21 of length at most 30 words." ></td>
	<td class="line x" title="199:221	For this grammar, Klein and Manning (2003b) showed that a very accurate heuristic can be constructed by taking the sum of outside scores computed with the dependency model and the PCFG model individually." ></td>
	<td class="line x" title="200:221	We report performance as a function of k for KA in Figure 4." ></td>
	<td class="line x" title="201:221	Both NAIVE and EXH are impractical on these grammars due to memory limitations." ></td>
	<td class="line x" title="202:221	For KA, computing the heuristic is the bottleneck, after which bottom-up parsing and k-best extraction are very fast." ></td>
	<td class="line x" title="203:221	3.3 Tree Transducer Grammars Syntactic machine translation (Galley et al., 2004) uses tree transducer grammars to translate sentences." ></td>
	<td class="line x" title="204:221	Transducer rules are synchronous contextfree productions that have both a source and a target side." ></td>
	<td class="line x" title="205:221	We examine the cost of k-best parsing in the source side of such grammars with KA, which can be a first step in translation." ></td>
	<td class="line x" title="206:221	We extracted a grammar from 220 million words of Arabic-English bitext using the approach of Galley et al.(2006), extracting rules with at most 3 non-terminals." ></td>
	<td class="line x" title="208:221	These rules are highly lexicalized." ></td>
	<td class="line x" title="209:221	About 300K rules are applicable for a typical 30-word sentence; we filter the rest." ></td>
	<td class="line x" title="210:221	We tested on 100 sentences of length at most 40 from the NIST05 Arabic-English test set." ></td>
	<td class="line x" title="211:221	We used a simple but effective heuristic for these grammars, similar to the FILTER heuristic suggested in Klein and Manning (2003c)." ></td>
	<td class="line x" title="212:221	We projected the source projection to a smaller grammar by collapsing all non-terminal symbols to X, and 6http://nlp.stanford.edu/software/ also collapsing pre-terminals into related clusters." ></td>
	<td class="line x" title="213:221	For example, we collapsed the tags NN, NNS, NNP, and NNPS to N. This projection reduced the number of grammar symbols from 149 to 36." ></td>
	<td class="line x" title="214:221	Using it as a heuristic for the full grammar suppressed60% of the total items (Figure 5)." ></td>
	<td class="line x" title="215:221	4 Related Work While formulated very differently, one limiting case of our algorithm relates closely to the EXH algorithm of Huang and Chiang (2005)." ></td>
	<td class="line x" title="216:221	In particular, if all inside items are processed before any derivation items, the subsequent number of derivation items and outside items popped by KA is nearly identical to the number popped by EXH in our experiments (both algorithms have the same ordering bounds on which derivation items are popped)." ></td>
	<td class="line x" title="217:221	The only real difference between the algorithms in this limited case is that EXH places k-best items on local priority queues per edge, while KA makes use of one global queue." ></td>
	<td class="line x" title="218:221	Thus, in addition to providing a method for speeding up k-best extraction with A, our algorithm also provides an alternate form of Huang and Chiang (2005)s k-best extraction that can be phrased in a weighted deduction system." ></td>
	<td class="line x" title="219:221	5 Conclusions We have presented KA, an extension of A parsing that allows extraction of optimal k-best parses without the need for an exhaustive 1-best pass." ></td>
	<td class="line x" title="220:221	We have shown in several domains that, with an appropriate heuristic, our algorithm can extract kbest lists in a fraction of the time required by current approaches to k-best extraction, giving the best of both A parsing and efficientk-best extraction, in a unified procedure." ></td>
	<td class="line x" title="221:221	965" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-1008
On Statistical Parsing of French with Supervised and Semi-Supervised Strategies
Candito, Marie;Crabb, Benot;Seddah, Djam;"></td>
	<td class="line x" title="1:224	Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 4957, Athens, Greece, 30 March 2009." ></td>
	<td class="line x" title="2:224	c2009 Association for Computational Linguistics On statistical parsing of French with supervised and semi-supervised strategies Marie Candito*, Benot Crabb* and Djam Seddah * Universit Paris 7 UFRL et INRIA (Alpage) 30 rue du Chteau des Rentiers F-75013 Paris  France  Universit Paris 4 LALIC et INRIA (Alpage) 28 rue Serpente F-75006 Paris  France Abstract This paper reports results on grammatical induction for French." ></td>
	<td class="line x" title="3:224	We investigate how to best train a parser on the French Treebank (Abeill et al., 2003), viewing the task as a trade-off between generalizability and interpretability." ></td>
	<td class="line x" title="4:224	We compare, for French, a supervised lexicalized parsing algorithm with a semi-supervised unlexicalized algorithm (Petrov et al., 2006) along the lines of (Crabb and Candito, 2008)." ></td>
	<td class="line x" title="5:224	We report the best results known to us on French statistical parsing, that we obtained with the semi-supervised learning algorithm." ></td>
	<td class="line x" title="6:224	The reported experiments can give insights for the task of grammatical learning for a morphologically-rich language, with a relatively limited amount of training data, annotated with a rather flat structure." ></td>
	<td class="line x" title="7:224	1 Natural language parsing Despite the availability of annotated data, there have been relatively few works on French statistical parsing." ></td>
	<td class="line x" title="8:224	Together with a treebank, the availability of several supervised or semi-supervised grammatical learning algorithms, primarily set up on English data, allows us to figure out how they behave on French." ></td>
	<td class="line x" title="9:224	Before that, it is important to describe the characteristics of the parsing task." ></td>
	<td class="line x" title="10:224	In the case of statistical parsing, two different aspects of syntactic structures are to be considered : their capacity to capture regularities and their interpretability for further processing." ></td>
	<td class="line x" title="11:224	Generalizability Learning for statistical parsing requires structures that capture best the underlying regularities of the language, in order to apply these patterns to unseen data." ></td>
	<td class="line x" title="12:224	Since capturing underlying linguistic rules is also an objective for linguists, it makes sense to use supervised learning from linguisticallydefined generalizations." ></td>
	<td class="line x" title="13:224	One generalization is typically the use of phrases, and phrase-structure rules that govern the way words are grouped together." ></td>
	<td class="line x" title="14:224	It has to be stressed that these syntactic rules exist at least in part independently of semantic interpretation." ></td>
	<td class="line x" title="15:224	Interpretability But the main reason to use supervised learning for parsing, is that we want structures that are as interpretable as possible, in order to extract some knowledge from the analysis (such as deriving a semantic analysis from a parse)." ></td>
	<td class="line x" title="16:224	Typically, we need a syntactic analysis to reflect how words relate to each other." ></td>
	<td class="line x" title="17:224	This is our main motivation to use supervised learning : the learnt parser will output structures as defined by linguists-annotators, and thus interpretable within the linguistic theory underlying the annotation scheme of the treebank." ></td>
	<td class="line x" title="18:224	It is important to stress that this is more than capturing syntactic regularities : it has to do with the meaning of the words." ></td>
	<td class="line x" title="19:224	It is not certain though that both requirements (generalizability / interpretability) are best met in the same structures." ></td>
	<td class="line x" title="20:224	In the case of supervised learning, this leads to investigate different instantiations of the training trees, to help the learning, while keeping the maximum interpretability of the trees." ></td>
	<td class="line x" title="21:224	As we will see with some of our experiments, it may be necessary to find a trade-off between generalizability and interpretability." ></td>
	<td class="line x" title="22:224	Further, it is not guaranteed that syntactic rules infered from a manually annotated treebank produce the best language model." ></td>
	<td class="line x" title="23:224	This leads to 49 methods that use semi-supervised techniques on a treebank-infered grammar backbone, such as (Matsuzaki et al., 2005; Petrov et al., 2006)." ></td>
	<td class="line x" title="24:224	The plan of the paper is as follows : in the next section, we describe the available treebank for French, and how its structures can be interpreted." ></td>
	<td class="line x" title="25:224	In section 3, we describe the typical problems encountered when parsing using a plain probabilistic context-free grammar, and existing algorithmic solutions that try to circumvent these problems." ></td>
	<td class="line x" title="26:224	Next we describe experiments and results when training parsers on the French data." ></td>
	<td class="line x" title="27:224	Finally, we discuss related work and conclude." ></td>
	<td class="line x" title="28:224	2 Interpreting the French trees The French Treebank (Abeill et al., 2003) is a publicly available sample from the newspaper Le Monde, syntactically annotated and manually corrected for French." ></td>
	<td class="line x" title="29:224	<SENT> <NP fct='SUJ'> <w cat='D' lemma='le' mph='ms' subcat='def'>le</w> <w cat='N' lemma='bilan' mph='ms' subcat='C'>bilan</w> </NP> <VN> <w cat='ADV' lemma='ne' subcat='neg'>n</w> <w cat='V' lemma='tre' mph='P3s' subcat=''>est</w> </VN> <AdP fct='MOD'> <w compound='yes' cat='ADV' lemma='peut-tre'> <w catint='V'>peut</w> <w catint='PONCT'>-</w> <w catint='V'>tre</w> </w> <w cat='ADV' lemma='pas' subcat='neg'>pas</w> </AdP> <AP fct='ATS'> <w cat='ADV' lemma='aussi'>aussi</w> <w cat='A' lemma='sombre' mph='ms' subcat='qual'>sombre</w> </AP> <w cat='PONCT' lemma='.' subcat='S'>.</w> </SENT> Figure 1: Simplified example of the FTB To encode syntactic information, it uses a combination of labeled constituents, morphological annotations and functional annotation for verbal dependents as illustrated in Figure 1." ></td>
	<td class="line x" title="30:224	This constituent and functional annotation was performed in two successive steps : though the original release (Abeill et al., 2000) consists of 20,648 sentences (hereafter FTB-V0), the functional annotation was performed later on a subset of 12351 sentences (hereafter FTB)." ></td>
	<td class="line x" title="31:224	This subset has also been revised, and is known to be more consistently annotated." ></td>
	<td class="line x" title="32:224	This is the release we use in our experiments." ></td>
	<td class="line x" title="33:224	Its key properties, compared with the Penn Treebank, (hereafter PTB) are the following : Size : The FTB is made of 385 458 tokens and 12351 sentences, that is the third of the PTB." ></td>
	<td class="line x" title="34:224	The average length of a sentence is 31 tokens in the FTB, versus 24 tokens in the PTB." ></td>
	<td class="line x" title="35:224	Inflection : French morphology is richer than English and leads to increased data sparseness for statistical parsing." ></td>
	<td class="line x" title="36:224	There are 24098 types in the FTB, entailing an average of 16 tokens occurring for each type (versus 12 for the PTB)." ></td>
	<td class="line x" title="37:224	Flat structure : The annotation scheme is flatter in the FTB than in the PTB." ></td>
	<td class="line x" title="38:224	For instance, there are no VPs for finite verbs, and only one sentential level for sentences whether introduced by complementizer or not." ></td>
	<td class="line x" title="39:224	We can measure the corpus flatness using the ratio between tokens and non terminal symbols, excluding preterminals." ></td>
	<td class="line x" title="40:224	We obtain 0.69 NT symbol per token for FTB and 1.01 for the PTB." ></td>
	<td class="line x" title="41:224	Compounds : Compounds are explicitly annotated (see the compound peut-tre in Figure 1 ) and very frequent : 14,52% of tokens are part of a compound." ></td>
	<td class="line x" title="42:224	They include digital numbers (written with spaces in French 10 000), very frozen compounds pomme de terre (potato) but also named entities or sequences whose meaning is compositional but where insertion is rare or difficult (garde denfant (child care))." ></td>
	<td class="line x" title="43:224	Now let us focus on what is expressed in the French annotation scheme, and why syntactic information is split between constituency and functional annotation." ></td>
	<td class="line x" title="44:224	Syntactic categories and constituents capture distributional generalizations." ></td>
	<td class="line x" title="45:224	A syntactic category groups forms that share distributional properties." ></td>
	<td class="line x" title="46:224	Nonterminal symbols that label the constituents are a further generalizations over sequences of categories or constituents." ></td>
	<td class="line x" title="47:224	For instance about anywhere it is grammatical to have a given NP, it is implicitly assumed that it will also be grammatical though maybe nonsensical to have instead any other NPs." ></td>
	<td class="line x" title="48:224	Of course this is known to be false in many cases : for instance NPs with or without determiners have very different distributions in French (that may justify a different label) but they also share a lot." ></td>
	<td class="line x" title="49:224	Moreover, if words are taken into account, and not just sequences of categories, then constituent labels are a very coarse generalization." ></td>
	<td class="line x" title="50:224	Constituents also encode dependencies : for instance the different PP-attachment for the sentences I ate a cake with cream / with a fork reflects that with cream depends on cake, whereas with a fork depends on ate." ></td>
	<td class="line x" title="51:224	More precisely, a syntagmatic tree can be interpreted as a dependency structure using the following conventions : 50 for each constituent, given the dominating symbol and the internal sequence of symbols, (i) a head symbol can be isolated and (ii) the siblings of that head can be interpreted as containing dependents of that head." ></td>
	<td class="line x" title="52:224	Given these constraints, the syntagmatic structure may exhibit various degree of flatness for internal structures." ></td>
	<td class="line x" title="53:224	Functional annotation Dependencies are encoded in constituents." ></td>
	<td class="line x" title="54:224	While X-bar inspired constituents are supposed to contain all the syntactic information, in the FTB the shape of the constituents does not necessarily express unambiguously the type of dependency existing between a head and a dependent appearing in the same constituent." ></td>
	<td class="line x" title="55:224	Yet this is crucial for example to extract the underlying predicate-argument structures." ></td>
	<td class="line x" title="56:224	This has led to a flat annotation scheme, completed with functional annotations that inform on the type of dependency existing between a verb and its dependents." ></td>
	<td class="line x" title="57:224	This was chosen for French to reflect, for instance, the possibility to mix postverbal modifiers and complements (Figure 2), or to mix post-verbal subject and post-verbal indirect complements : a post verbal NP in the FTB can correspond to a temporal modifier, (most often) a direct object, or an inverted subject, and in the three cases other subcategorized complements may appear." ></td>
	<td class="line x" title="58:224	SENT NP-SUJ D une N lettre VN V avait V t V envoye NP-MOD D la N semaine A dernire PP-AOBJ P aux NP N salaris SENT NP-SUJ D Le N Conseil VN V a V notifi NP-OBJ D sa N dcision PP-AOBJ P  NP D la N banque Figure 2: Two examples of post-verbal NPs : a direct object and a temporal modifier 3 Algorithms for probabilistic grammar learning We propose here to investigate how to apply statistical parsing techniques mainly tested on English, to another language  French ." ></td>
	<td class="line x" title="59:224	In this section we briefly introduce the algorithms investigated." ></td>
	<td class="line x" title="60:224	Though Probabilistic Context Free Grammars (PCFG) is a baseline formalism for probabilistic parsing, it suffers a fundamental problem for the purpose of natural language parsing : the independence assumptions made by the model are too strong." ></td>
	<td class="line x" title="61:224	In other words all decisions are local to a grammar rule." ></td>
	<td class="line x" title="62:224	However as clearly pointed out by (Johnson, 1998) decisions have to take into account non local grammatical properties: for instance a noun phrase realized in subject position is more likely to be realized by a pronoun than a noun phrase realized in object position." ></td>
	<td class="line x" title="63:224	Solving this first methodological issue, has led to solutions dubbed hereafter as unlexicalized statistical parsing (Johnson, 1998; Klein and Manning, 2003a; Matsuzaki et al., 2005; Petrov et al., 2006)." ></td>
	<td class="line x" title="64:224	A second class of non local decisions to be taken into account while parsing natural languages are related to handling lexical constraints." ></td>
	<td class="line x" title="65:224	As shown above the subcategorization properties of a predicative word may have an impact on the decisions concerning the tree structures to be associated to a given sentence." ></td>
	<td class="line x" title="66:224	Solving this second methodological issue has led to solutions dubbed hereafter as lexicalized parsing (Charniak, 2000; Collins, 1999)." ></td>
	<td class="line x" title="67:224	In a supervised setting, a third and practical problem turns out to be critical: that of data sparseness since available treebanks are generally too small to get reasonable probability estimates." ></td>
	<td class="line oc" title="68:224	Three class of solutions are possible to reduce data sparseness: (1) enlarging the data manually or automatically (e.g.(McClosky et al., 2006) uses selftraining to perform this step) (2) smoothing, usually this is performed using a markovization procedure (Collins, 1999; Klein and Manning, 2003a) and (3) make the data more coarse (i.e. clustering)." ></td>
	<td class="line x" title="70:224	3.1 Lexicalized algorithm The first algorithm we use is the lexicalized parser of (Collins, 1999)." ></td>
	<td class="line x" title="71:224	It is called lexicalized, as it annotates non terminal nodes with an additional latent symbol: the head word of the subtree." ></td>
	<td class="line x" title="72:224	This additional information attached to the categories aims at capturing bilexical dependencies in order to perform informed attachment choices." ></td>
	<td class="line x" title="73:224	The addition of these numerous latent symbols to non terminals naturally entails an overspecialization of the resulting models." ></td>
	<td class="line x" title="74:224	To ensure generalization, it therefore requires to add additional simplifying assumptions formulated as a variant of usual nave Bayesian-style simplifying assumptions: the probability of emitting a non 51 head node is assumed to depend on the head and the mother node only, and not on other sibling nodes1." ></td>
	<td class="line x" title="75:224	Since Collins demonstrated his models to significantly improve parsing accuracy over bare PCFG, lexicalization has been thought as a major feature for probabilistic parsing." ></td>
	<td class="line x" title="76:224	However two problems are worth stressing here: (1) the reason why these models improve over bare PCFGs is not guaranteed to be tied to the fact that they capture bilexical dependencies and (2) there is no guarantee that capturing non local lexical constraints yields an optimal language model." ></td>
	<td class="line x" title="77:224	Concerning (1) (Gildea, 2001) showed that full lexicalization has indeed small impact on results : he reimplemented an emulation of Collins Model 1 and found that removing all references to bilexical dependencies in the statistical model2, resulted in a very small parsing performance decrease (PARSEVAL recall on WSJ decreased from 86.1 to 85.6)." ></td>
	<td class="line x" title="78:224	Further studies conducted by (Bikel, 2004a) proved indeed that bilexical information were used by the most probable parses." ></td>
	<td class="line x" title="79:224	The idea is that most bilexical parameters are very similar to their back-off distribution and have therefore a minor impact." ></td>
	<td class="line x" title="80:224	In the case of French, this fact can only be more true, with one third of training data compared to English, and with a much richer inflection that worsens lexical data sparseness." ></td>
	<td class="line x" title="81:224	Concerning (2) the addition of head word annotations is tied to the use of manually defined heuristics highly dependent on the annotation scheme of the PTB." ></td>
	<td class="line x" title="82:224	For instance, Collins models integrate a treatment of coordination that is not adequate for the FTB-like coordination annotation." ></td>
	<td class="line x" title="83:224	3.2 Unlexicalized algorithms Another class of algorithms arising from (Johnson, 1998; Klein and Manning, 2003a) attempts to attach additional latent symbols to treebank categories without focusing exclusively on lexical head words." ></td>
	<td class="line x" title="84:224	For instance the additional annotations will try to capture non local preferences like 1This short description cannot do justice to (Collins, 1999) proposal which indeed includes more fine grained informations and a backoff model." ></td>
	<td class="line x" title="85:224	We only keep here the key aspects of his work relevant for the current discussion." ></td>
	<td class="line x" title="86:224	2Let us consider a dependent constituent C with head word Chw and head tag Cht, and let C be governed by a constituent H, with head word Hhw and head tag Hht." ></td>
	<td class="line x" title="87:224	Gildea compares Collins model, where the emission of Chw is conditioned on Hhw, and a mono-lexical model, where the emission of Chw is not conditioned on Hhw." ></td>
	<td class="line x" title="88:224	the fact that an NP in subject position is more likely realized as a pronoun." ></td>
	<td class="line x" title="89:224	The first unlexicalized algorithms set up in this trend (Johnson, 1998; Klein and Manning, 2003a) also use language dependent and manually defined heuristics to add the latent annotations." ></td>
	<td class="line x" title="90:224	The specialization induced by this additional annotation is counterbalanced by simplifying assumptions, dubbed markovization (Klein and Manning, 2003a)." ></td>
	<td class="line x" title="91:224	Using hand-defined heuristics remains problematic since we have no guarantee that the latent annotations added in this way will allow to extract an optimal language model." ></td>
	<td class="line x" title="92:224	A further development has been first introduced by (Matsuzaki et al., 2005) who recasts the problem of adding latent annotations as an unsupervised learning problem: given an observed PCFG induced from the treebank, the latent grammar is generated by combining every non terminal of the observed grammar to a predefined set H of latent symbols." ></td>
	<td class="line x" title="93:224	The parameters of the latent grammar are estimated from the observed trees using a specific instantiation of EM." ></td>
	<td class="line x" title="94:224	This first procedure however entails a combinatorial explosion in the size of the latent grammar as |H| increases." ></td>
	<td class="line x" title="95:224	(Petrov et al., 2006) (hereafter BKY) overcomes this problem by using the following algorithm: given a PCFG G0 induced from the treebank, iteratively create n grammars G1 Gn (with n = 5 in practice), where each iterative step is as follows :  SPLIT Create a new grammar Gi from Gi1 by splitting every non terminal of Gi in two new symbols." ></td>
	<td class="line x" title="96:224	Estimate Gis parameters on the observed treebank using a variant of inside-outside." ></td>
	<td class="line x" title="97:224	This step adds the latent annotation to the grammar." ></td>
	<td class="line x" title="98:224	 MERGE For each pair of symbols obtained by a previous split, try to merge them back." ></td>
	<td class="line x" title="99:224	If the likelihood of the treebank does not get significantly lower (fixed threshold) then keep the symbol merged, otherwise keep the split." ></td>
	<td class="line x" title="100:224	 SMOOTH This step consists in smoothing the probabilities of the grammar rules sharing the same left hand side." ></td>
	<td class="line x" title="101:224	This algorithm yields state-of-the-art results on 52 English3." ></td>
	<td class="line x" title="102:224	Its key interest is that it directly aims at finding an optimal language model without (1) making additional assumptions on the annotation scheme and (2) without relying on hand-defined heuristics." ></td>
	<td class="line x" title="103:224	This may be viewed as a case of semisupervised learning algorithm since the initial supervised learning step is augmented with a second step of unsupervised learning dedicated to assign the latent symbols." ></td>
	<td class="line x" title="104:224	4 Experiments and Results We investigate how some treebank features impact learning." ></td>
	<td class="line x" title="105:224	We describe first the experimental protocol, next we compare results of lexicalized and unlexicalized parsers trained on various instantiations of the xml source files of the FTB, and the impact of training set size for both algorithms." ></td>
	<td class="line x" title="106:224	Then we focus on studying how words impact the results of the BKYalgorithm." ></td>
	<td class="line x" title="107:224	4.1 Protocol Treebank setting For all experiments, the treebank is divided into 3 sections : training (80%), development (10%) and test (10%), made of respectively 9881, 1235 and 1235 sentences." ></td>
	<td class="line x" title="108:224	We systematically report the results with the compounds merged." ></td>
	<td class="line x" title="109:224	Namely, we preprocess the treebank in order to turn each compound into a single token both for training and test." ></td>
	<td class="line x" title="110:224	Software and adaptation to French For the Collins algorithm, we use Bikels implementation (Bikel, 2004b) (hereafter BIKEL), and we report results using Collins model 1 and model 2, with internal tagging." ></td>
	<td class="line x" title="111:224	Adapting model 1 to French requires to design French specific head propagation rules." ></td>
	<td class="line x" title="112:224	To this end, we adapted those described by (Dybro-Johansen, 2004) for extracting a Stochastic Tree Adjoining Grammar parser on French." ></td>
	<td class="line x" title="113:224	And to adapt model 2, we have further designed French specific argument/adjunct identification rules." ></td>
	<td class="line x" title="114:224	For the BKY approach, we use the Berkeley implementation, with an horizontal markovization h=0, and 5 split/merge cycles." ></td>
	<td class="line x" title="115:224	All the required knowledge is contained in the treebank used for training, except for the treatment of unknown or rare words." ></td>
	<td class="line x" title="116:224	It clusters unknown words using typographical and morphological information." ></td>
	<td class="line x" title="117:224	We 3(Petrov et al., 2006) obtain an F-score=90.1 for sentences of less than 40 words." ></td>
	<td class="line x" title="118:224	adapted these clues to French, following (Arun and Keller, 2005)." ></td>
	<td class="line x" title="119:224	Finally we use as a baseline a standard PCFG algorithm, coupled with a trigram tagger (we refer to this setup as TNT/LNCKY algorithm4)." ></td>
	<td class="line x" title="120:224	Metrics For evaluation, we use the standard PARSEVAL metric of labeled precision/recall, along with unlabeled dependency evaluation, which is known as a more annotation-neutral metric." ></td>
	<td class="line x" title="121:224	Unlabeled dependencies are computed using the (Lin, 1995) algorithm, and the Dybro-Johansens head propagation rules cited above5." ></td>
	<td class="line x" title="122:224	The unlabeled dependency F-score gives the percentage of input words (excluding punctuation) that receive the correct head." ></td>
	<td class="line x" title="123:224	As usual for probabilistic parsing results, the results are given for sentences of the test set of less than 40 words (which is true for 992 sentences of the test set), and punctuation is ignored for F-score computation with both metrics." ></td>
	<td class="line x" title="124:224	4.2 Comparison using minimal tagsets We first derive from the FTB a minimallyinformed treebank, TREEBANKMIN, instantiated from the xml source by using only the major syntactic categories and no other feature." ></td>
	<td class="line x" title="125:224	In each experiment (Table 1) we observe that the BKY algorithm significantly outperforms Collins models, for both metrics." ></td>
	<td class="line x" title="126:224	parser BKY BIKEL BIKEL TNT/ metric M1 M2 LNCKY PARSEVAL LP 85.25 78.86 80.68 68.74 PARSEVAL LR 84.46 78.84 80.58 67.93 PARSEVAL F1 84.85 78.85 80.63 68.33 Unlab." ></td>
	<td class="line x" title="127:224	dep." ></td>
	<td class="line x" title="128:224	Prec." ></td>
	<td class="line x" title="129:224	90.23 85.74 87.60 79.50 Unlab." ></td>
	<td class="line x" title="130:224	dep." ></td>
	<td class="line x" title="131:224	Rec." ></td>
	<td class="line x" title="132:224	89.95 85.72 86.90 79.37 Unlab." ></td>
	<td class="line x" title="133:224	dep." ></td>
	<td class="line x" title="134:224	F1 90.09 85.73 87.25 79.44 Table 1: Results for parsers trained on FTB with minimal tagset 4The tagger is TNT (Brants, 2000), and the parser is LNCKY, that is distributed by Mark Johnson (http://www.cog.brown.edu/mj/Software.htm)." ></td>
	<td class="line x" title="135:224	Formally because of the tagger, this is not a strict PCFG setup." ></td>
	<td class="line x" title="136:224	Rather, it gives a practical trade-off, in which the tagger includes the lexical smoothing for unknown and rare words." ></td>
	<td class="line x" title="137:224	5For this evaluation, the gold constituent trees are converted into pseudo-gold dependency trees (that may contain errors)." ></td>
	<td class="line x" title="138:224	Then parsed constituent trees are converted into parsed dependency trees, that are matched against the pseudo-gold trees." ></td>
	<td class="line x" title="139:224	53 4.3 Impact of training data size How do the unlexicalized and lexicalized approaches perform with respect to size?" ></td>
	<td class="line x" title="140:224	We compare in figure 3 the parsing performance BKY and COLLINSM1, on increasingly large subsets of the FTB, in perfect tagging mode6 and using a more detailed tagset (CC tagset, described in the next experiment)." ></td>
	<td class="line x" title="141:224	The same 1235-sentences test set is used for all subsets, and the development sets size varies along with the training sets size." ></td>
	<td class="line x" title="142:224	BKY outperforms the lexicalized model even with small amount of data (around 3000 training sentences)." ></td>
	<td class="line x" title="143:224	Further, the parsing improvement that would result from more training data seems higher for BKY than for Bikel." ></td>
	<td class="line x" title="144:224	2000 4000 6000 8000 10000 76 78 80 82 84 86 88 Number of training sentences Fscore Bikel Berkeley Figure 3: Parsing Learning curve on FTB with CCtagset, in perfect-tagging This potential increase for BKY results if we had more French annotated data is somehow confirmed by the higher results reported for BKY training on the Penn Treebank (Petrov et al., 2006) : F1=90.2." ></td>
	<td class="line x" title="145:224	We can show though that the 4 points increase when training on English data is not only due to size : we extracted from the Penn Treebank a subset comparable to the FTB, with respect to number of tokens and average length of sentences." ></td>
	<td class="line x" title="146:224	We obtain F1=88.61 with BKY training." ></td>
	<td class="line x" title="147:224	4.4 Symbol refinements It is well-known that certain treebank transformations involving symbol refinements improve 6For BKY, we simulate perfect tagging by changing words into word+tag in training, dev and test sets." ></td>
	<td class="line x" title="148:224	We obtain around 99.8 tagging accuracy, errors are due to unknown words." ></td>
	<td class="line x" title="149:224	PCFGs (see for instance parent-transformation of (Johnson, 1998), or various symbol refinements in (Klein and Manning., 2003b))." ></td>
	<td class="line x" title="150:224	Lexicalization itself can be seen as symbol refinements (with backoff though)." ></td>
	<td class="line x" title="151:224	For BKY, though the key point is to automatize symbol splits, it is interesting to study whether manual splits still help." ></td>
	<td class="line x" title="152:224	We have thus experimented BKY training with various tagsets." ></td>
	<td class="line x" title="153:224	The FTB contains rich morphological information, that can be used to split preterminal symbols : main coarse category (there are 13), subcategory (subcatfeature refining the main cat), and inflectional information (mph feature)." ></td>
	<td class="line x" title="154:224	We report in Table 2 results for the four tagsets, where terminals are made of : MIN: main cat, SUBCAT: main cat + subcat feature, MAX: cat + subcat + all inflectional information, CC: cat + verbal mood + wh feature." ></td>
	<td class="line x" title="155:224	Tagset Nb of tags Parseval Unlab." ></td>
	<td class="line x" title="156:224	dep Tagging F1 F1 Acc MIN 13 84.85 90.09 97.35 SUBCAT 34 85.74  96.63 MAX 250 84.13  92.20 CC 28 86.41 90.99 96.83 Table 2: Tagset impact on learning with BKY (own tagging) The corpus instantiation with CC tagset is our best trade-off between tagset informativeness and obtained parsing performance7." ></td>
	<td class="line x" title="157:224	It is also the best result obtained for French probabilistic parsing." ></td>
	<td class="line x" title="158:224	This demonstrates though that the BKY learning is not optimal since manual a priori symbol refinements significantly impact the results." ></td>
	<td class="line x" title="159:224	We also tried to learn structures with functional annotation attached to the labels : we obtain PARSEVAL F1=78.73 with tags from the CC tagset + grammatical function." ></td>
	<td class="line x" title="160:224	This degradation, due to data sparseness and/or non local constraints badly captured by the model, currently constrains us to use a language model without functional informations." ></td>
	<td class="line x" title="161:224	As stressed in the introduction, this limits the interpretability of the parses and it is a tradeoff between generalization and interpretability." ></td>
	<td class="line x" title="162:224	4.5 Lexicon and Inflection impact French has a rich morphology that allows some degree of word order variation, with respect to 7The differences are statistically significant : using a standard t-test, we obtain p-value=0.015 between MIN and SUBCAT, and p-value=0.002 between CC and SUBCAT." ></td>
	<td class="line x" title="163:224	54 English." ></td>
	<td class="line x" title="164:224	For probabilistic parsing, this can have contradictory effects : (i) on the one hand, this induces more data sparseness : the occurrences of a French regular verb are potentially split into more than 60 forms, versus 5 for an English verb; (ii) on the other hand, inflection encodes agreements, that can serve as clues for syntactic attachments." ></td>
	<td class="line x" title="165:224	Experiment In order to measure the impact of inflection, we have tested to cluster word forms on a morphological basis, namely to partly cancel inflection." ></td>
	<td class="line x" title="166:224	Using lemmas as word form classes seems too coarse : it would not allow to distinguish for instance between a finite verb and a participle, though they exhibit different distributional properties." ></td>
	<td class="line x" title="167:224	Instead we use as word form classes, the couple lemma + syntactic category." ></td>
	<td class="line x" title="168:224	For example for verbs, given the CC tagset, this amounts to keeping 6 different forms (for the 6 moods)." ></td>
	<td class="line x" title="169:224	To test this grouping, we derive a treebank where words are replaced by the concatenation of lemma + category for training and testing the parser." ></td>
	<td class="line x" title="170:224	Since it entails a perfect tagging, it has to be compared to results in perfect tagging mode : more precisely, we simulate perfect tagging by replacing word forms by the concatenation form+tag." ></td>
	<td class="line x" title="171:224	Moreover, it is tempting to study the impact of a more drastic clustering of word forms : that of using the sole syntactic category to group word forms (we replace each word by its tag)." ></td>
	<td class="line x" title="172:224	This amounts to test a pure unlexicalized learning." ></td>
	<td class="line x" title="173:224	Discussion Results are shown in Figure 4." ></td>
	<td class="line x" title="174:224	We make three observations : First, comparing the terminal=tag curves with the other two, it appears that the parser does take advantage of lexical information to rank parses, even for this unlexicalized algorithm." ></td>
	<td class="line x" title="175:224	Yet the relatively small increase clearly shows that lexical information remains underused, probably because of lexical data sparseness." ></td>
	<td class="line x" title="176:224	Further, comparing terminal=lemma+tag and terminal=form+tag curves, we observe that grouping words into lemmas helps reducing this sparseness." ></td>
	<td class="line x" title="177:224	And third, the lexicon impact evolution (i.e. the increment between terminal=tag and terminal=form+tag curves) is stable, once the training size is superior to approx." ></td>
	<td class="line x" title="178:224	3000 sentences8." ></td>
	<td class="line x" title="179:224	This suggests that only very frequent words matter, otherwise words impact should be more and more important as training material augments." ></td>
	<td class="line x" title="180:224	0 2000 4000 6000 8000 10000 76 78 80 82 84 86 88 Number of training sentences Parseval Fscore Bky terminal=form+tag Bky terminal=lemma+tag Bky terminal=tag Figure 4: Impact of clustering word forms (training on FTB with CC-tagset, in perfect-tagging) 5 Related Work Previous works on French probabilistic parsing are those of (Arun and Keller, 2005), (Schluter and van Genabith, 2007), (Schluter and van Genabith, 2008)." ></td>
	<td class="line x" title="181:224	One major difficulty for comparison is that all three works use a different version of the training corpus." ></td>
	<td class="line x" title="182:224	Arun reports results on probabilistic parsing, using an older version of the FTB and using lexicalized models (Collins M1 and M2 models, and the bigram model)." ></td>
	<td class="line x" title="183:224	It is difficult to compare our results with Aruns work, since the treebank he has used is obsolete (FTB-V0)." ></td>
	<td class="line x" title="184:224	He obtains for Model 1 : LR=80.35 / LP=79.99, and for the bigram model : LR=81.15 / LP=80.84, with minimal tagset and internal tagging." ></td>
	<td class="line x" title="185:224	The results with FTB (revised subset of FTB-V0) with minimal 8 This is true for all points in the curves, except for the last step, i.e. when full training set is used." ></td>
	<td class="line x" title="186:224	We performed a 10-fold cross validation to limit sample effects." ></td>
	<td class="line x" title="187:224	For the BKYtraining with CC tagset, and own tagging, we obtain an average F-score of 85.44 (with a rather high standard deviation =1.14)." ></td>
	<td class="line x" title="188:224	For the clustering word forms experiment, using the full training set, we obtain : 86.64 for terminal=form+tag (=1.15), 87.33 for terminal=lemma+tag (=0.43), and 85.72 for terminal=tag (=0.43)." ></td>
	<td class="line x" title="189:224	Hence our conclusions (words help even with unlexicalized algorithm, and further grouping words into lemmas helps) hold independently of sampling." ></td>
	<td class="line x" title="190:224	55 tagset (Table 1) are comparable for COLLINSM1, and nearly 5 points higher for BKY." ></td>
	<td class="line x" title="191:224	It is also interesting to review (Arun and Keller, 2005) conclusion, built on a comparison with the German situation : at that time lexicalization was thought (Dubey and Keller, 2003) to have no sizable improvement on German parsing, trained on the Negra treebank, that uses a flat structures." ></td>
	<td class="line x" title="192:224	So (Arun and Keller, 2005) conclude that since lexicalization helps much more for parsing French, with a flat annotation, then word-order flexibility is the key-factor that makes lexicalization useful (if word order is fixed, cf.French and English) and useless (if word order is flexible, cf.German)." ></td>
	<td class="line x" title="195:224	This conclusion does not hold today." ></td>
	<td class="line x" title="196:224	First, it can be noted that as far as word order flexibility is concerned, French stands in between English and German. Second, it has been proven that lexicalization helps German probabilistic parsing (Kbler et al., 2006)." ></td>
	<td class="line x" title="197:224	Finally, these authors show that markovization of the unlexicalized Stanford parser gives almost the same increase in performance than lexicalization, both for the Negra treebank and the Tba-D/Z treebank." ></td>
	<td class="line x" title="198:224	This conclusion is reinforced by the results we have obtained : the unlexicalized, markovized, PCFG-LA algorithm outperforms the Collins lexicalized model." ></td>
	<td class="line x" title="199:224	(Schluter and van Genabith, 2007) aim at learning LFG structures for French." ></td>
	<td class="line x" title="200:224	To do so, and in order to learn first a Collins parser, N. Schluter created a modified treebank, the MFT, in order (i) to fit her underlying theoretical requirements, (ii) to increase the treebank coherence by error mining and (iii) to improve the performance of the learnt parser." ></td>
	<td class="line x" title="201:224	The MFT contains 4739 sentences taken from the FTB, with semi-automatic transformations." ></td>
	<td class="line x" title="202:224	These include increased rule stratification, symbol refinements (for information propagation), coordination raising with some manual re-annotation, and the addition of functional tags." ></td>
	<td class="line x" title="203:224	MFT has also undergone a phase of error mining, using the (Dickinson and Meurers, 2005) software, and following manual correction." ></td>
	<td class="line x" title="204:224	She reports a 79.95% F-score on a 400 sentence test set, which compares almost equally with Aruns results on the original 20000 sentence treebank." ></td>
	<td class="line x" title="205:224	So she attributes her results to the increased coherence of her smaller treebank." ></td>
	<td class="line x" title="206:224	Indeed, we ran the BKY training on the MFT, and we get Fscore=84.31." ></td>
	<td class="line x" title="207:224	While this is less in absolute than the BKY results obtained with FTB (cf.results in table 2), it is indeed very high if training data size is taken into account (cf.the BKY learning curve in figure 3)." ></td>
	<td class="line x" title="210:224	This good result raises the open question of identifying which modifications in the MFT (error mining and correction, tree transformation, symbol refinements) have the major impact." ></td>
	<td class="line x" title="211:224	6 Conclusion This paper reports results in statistical parsing for French with both unlexicalized (Petrov et al., 2006) and lexicalized parsers." ></td>
	<td class="line x" title="212:224	To our knowledge, both results are state of the art on French for each paradigm." ></td>
	<td class="line x" title="213:224	Both algorithms try to overcome PCFGs simplifying assumptions by some specialization of the grammatical labels." ></td>
	<td class="line x" title="214:224	For the lexicalized approach, the annotation of symbols with lexical head is known to be rarely fully used in practice (Gildea, 2001), what is really used being the category of the lexical head." ></td>
	<td class="line x" title="215:224	We observe that the second approach (BKY) constantly outperforms the lexicalist strategy  la (Collins, 1999)." ></td>
	<td class="line x" title="216:224	We observe however that (Petrov et al., 2006)s semi-supervised learning procedure is not fully optimal since a manual refinement of the treebank labelling turns out to improve the parsing results." ></td>
	<td class="line x" title="217:224	Finally we observe that the semi-supervised BKY algorithm does take advantage of lexical information : removing words degrades results." ></td>
	<td class="line x" title="218:224	The preterminal symbol splits percolates lexical distinctions." ></td>
	<td class="line x" title="219:224	Further, grouping words into lemmas helps for a morphologically rich language such as French." ></td>
	<td class="line x" title="220:224	So, an intermediate clustering standing between syntactic category and lemma is thought to yield better results in the future." ></td>
	<td class="line x" title="221:224	7 Acknowledgments We thank N. Schluter and J. van Genabith for kindly letting us run BKY on the MFT, and A. Arun for answering our questions." ></td>
	<td class="line x" title="222:224	We also thank the reviewers for valuable comments and references." ></td>
	<td class="line x" title="223:224	The work of the second author was partly funded by the Prix Diderot Innovation 2007, from University Paris 7." ></td>
	<td class="line x" title="224:224	56" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-1104
Data-Driven Dependency Parsing of New Languages Using Incomplete and Noisy Training Data
Spreyer, Kathrin;Kuhn, Jonas;"></td>
	<td class="line x" title="1:228	Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 1220, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:228	c 2009 Association for Computational Linguistics Data-Driven Dependency Parsing of New Languages Using Incomplete and Noisy Training Data Kathrin Spreyer and Jonas Kuhn Department of Linguistics University of Potsdam, Germany {spreyer,kuhn}@ling.uni-potsdam.de Abstract We present a simple but very effective approach to identifying high-quality data in noisy data sets for structured problems like parsing, by greedily exploiting partial structures." ></td>
	<td class="line x" title="3:228	We analyze our approach in an annotation projection framework for dependency trees, and show how dependency parsers from two different paradigms (graph-based and transition-based) can be trained on the resulting tree fragments." ></td>
	<td class="line x" title="4:228	We train parsers for Dutch to evaluate our method and to investigate to which degree graph-based and transitionbased parsers can benefit from incomplete training data." ></td>
	<td class="line x" title="5:228	We find that partial correspondence projection gives rise to parsers that outperform parsers trained on aggressively filtered data sets, and achieve unlabeled attachment scores that are only 5% behind the average UAS for Dutch in the CoNLL-X Shared Task on supervised parsing (Buchholz and Marsi, 2006)." ></td>
	<td class="line x" title="6:228	1 Introduction Many weakly supervised approaches to NLP rely on heuristics or filtering techniques to deal with noise in unlabeled or automatically labeled training data, e.g., in the exploitation of parallel corpora for crosslingual projection of morphological, syntactic or semantic information." ></td>
	<td class="line x" title="7:228	While heuristic approaches can implement (linguistic) knowledge that helps to detect noisy data (e.g., Hwa et al.(2005)), they are typically taskand language-specific and thus introduce a component of indirect supervision." ></td>
	<td class="line x" title="9:228	Non-heuristic filtering techniques, on the other hand, employ reliability measures (often unrelated to the task) to predict high-precision data points (e.g., Yarowsky et al.(2001))." ></td>
	<td class="line x" title="11:228	In order to reach a sufficient level of precision, filtering typically has to be aggressive, especially for highly structured tasks like parsing." ></td>
	<td class="line x" title="12:228	Such aggressive filtering techniques incur massive data loss and enforce trade-offs between the quality and the amount of usable data." ></td>
	<td class="line x" title="13:228	Ideally, a general filtering strategy for weakly supervised training of structured analysis tools should eliminate noisy subparts in the automatic annotation without discarding its high-precision aspects; thereby data loss would be kept to a minimum." ></td>
	<td class="line x" title="14:228	In this paper, we propose an extremely simple approach to noise reduction which greedily exploits partial correspondences in a parallel corpus, i.e., correspondences potentially covering only substructures of translated sentences." ></td>
	<td class="line x" title="15:228	We implemented this method in an annotation projection framework to create training data for two dependency parsers representing different parsing paradigms: The MSTParser (McDonald et al., 2005) as an instance of graph-based dependency parsing, and the MaltParser (Nivre et al., 2006) to represent transitionbased dependency parsing." ></td>
	<td class="line x" title="16:228	In an empirical evaluation, we investigate how they react differently to incomplete and noisy training data." ></td>
	<td class="line x" title="17:228	Despite its simplicity, the partial correspondence approach proves very effective and leads to parsers that achieve unlabeled attachment scores that are only 5% behind the average UAS for Dutch in the CoNLL-X Shared Task (Buchholz and Marsi, 2006)." ></td>
	<td class="line x" title="18:228	After a summary of related work in Sec." ></td>
	<td class="line x" title="19:228	2, we discuss dependency tree projection (Sec." ></td>
	<td class="line x" title="20:228	3) and partial correspondence (Sec." ></td>
	<td class="line x" title="21:228	4)." ></td>
	<td class="line x" title="22:228	In Sec." ></td>
	<td class="line x" title="23:228	5, we give an overview of graphand transition-based dependency parsing and describe how each can be adapted for training on partial training data in Sec." ></td>
	<td class="line x" title="24:228	6." ></td>
	<td class="line x" title="25:228	Experimental results are presented in Sec." ></td>
	<td class="line x" title="26:228	7, followed by an analysis in Sec." ></td>
	<td class="line x" title="27:228	8." ></td>
	<td class="line x" title="28:228	Sec." ></td>
	<td class="line x" title="29:228	9 concludes." ></td>
	<td class="line x" title="30:228	12 a. b. c. English (L1): I have two questions You are absolutely right You are absolutely right Dutch (L2): Ik heb twee vragen U heeft volkomen gelijk U heeft volkomen gelijk 1 2 3 Figure 1: Dependency tree projection from English to Dutch." ></td>
	<td class="line x" title="31:228	(a) Ideal scenario with bidirectional alignments." ></td>
	<td class="line x" title="32:228	(b) Projection fails due to weak alignments." ></td>
	<td class="line x" title="33:228	(c) Constrained fallback projection." ></td>
	<td class="line x" title="34:228	2 Related Work Annotation projection has been applied to many different NLP tasks." ></td>
	<td class="line x" title="35:228	On the word or phrase level, these include morphological analysis, part-of-speech tagging and NP-bracketing (Yarowsky et al., 2001), temporal analysis (Spreyer and Frank, 2008), or semantic role labeling (Pado and Lapata, 2006)." ></td>
	<td class="line x" title="36:228	In these tasks, word labels can technically be introduced in isolation, without reference to the rest of the annotation." ></td>
	<td class="line x" title="37:228	This means that an aggressive filter can be used to discard unreliable data points (words in a sentence) without necessarily affecting highprecision data points in the same sentence." ></td>
	<td class="line x" title="38:228	By using only the bidirectional word alignment links, one can implement a very robust such filter, as the bidirectional links are generally reliable, even though they have low recall for overall translational correspondences (Koehn et al., 2003)." ></td>
	<td class="line x" title="39:228	The bidirectional alignment filter is common practice (Pado and Lapata, 2006); a similar strategy is to discard entire sentences with low aggregated alignment scores (Yarowsky et al., 2001)." ></td>
	<td class="line x" title="40:228	On the sentence level, Hwa et al.(2005) were the first to project dependency trees from English to Spanish and Chinese." ></td>
	<td class="line x" title="42:228	They identify unreliable target parses (as a whole) on the basis of the number of unaligned or over-aligned words." ></td>
	<td class="line x" title="43:228	In addition, they manipulate the trees to accommodate for nonisomorphic sentences." ></td>
	<td class="line x" title="44:228	Systematic non-parallelisms between source and target language are then addressed by hand-crafted rules in a post-projection step." ></td>
	<td class="line x" title="45:228	These rules account for an enormous increase in the unlabeled f-score of the direct projections, from 33.9 to 65.7 for Spanish and from 26.3 to 52.4 for Chinese." ></td>
	<td class="line x" title="46:228	But they need to be designed anew for every target language, which is time-consuming and requires knowledge of that language." ></td>
	<td class="line oc" title="47:228	Research in the field of unsupervised and weakly supervised parsing ranges from various forms of EM training (Pereira and Schabes, 1992; Klein and Manning, 2004; Smith and Eisner, 2004; Smith and Eisner, 2005) over bootstrapping approaches like selftraining (McClosky et al., 2006) to feature-based enhancements of discriminative reranking models (Koo et al., 2008) and the application of semisupervised SVMs (Wang et al., 2008)." ></td>
	<td class="line x" title="48:228	The partial correspondence method we present in this paper is compatible with such approaches and can be combined with other weakly supervised machine learning schemes." ></td>
	<td class="line x" title="49:228	Our approach is similar to that of Clark and Curran (2006) who use partial training data (CCG lexical categories) for domain adaptation; however, they assume an existing CCG resource for the language in question to provide this data." ></td>
	<td class="line x" title="50:228	3 Projection of Dependency Trees Most state-of-the-art parsers for natural languages are data-driven and depend on the availability of sufficient amounts of labeled training data." ></td>
	<td class="line x" title="51:228	However, manual creation of treebanks is time-consuming and labour-intensive." ></td>
	<td class="line x" title="52:228	One way to avoid the expensive annotation process is to automatically label the training data using annotation projection (Yarowsky et al., 2001): Given a suitable resource (such as a parser) in language L1, and a word-aligned parallel corpus with languages L1 and L2, label the L1portion of the parallel text (with the parser) and copy the annotations to the corresponding (i.e., aligned) elements in language L2." ></td>
	<td class="line x" title="53:228	This is illustrated in Fig." ></td>
	<td class="line x" title="54:228	1a." ></td>
	<td class="line x" title="55:228	The arrows between English and Dutch words indicate the word alignment." ></td>
	<td class="line x" title="56:228	Assuming we have a parser to produce the dependency tree for the English sentence, we build the tree for the Dutch sentence by establishing arcs between words wD (e.g., Ik) and hD (heb) if there are aligned pairs (wD,wE) 13 #sents w/ avg." ></td>
	<td class="line x" title="57:228	sent vocab projected parse length (lemma) unfiltered (100,000) 24.92 19,066 bidirectional 2,112 6.39 1,905 fallback 6,426 9.72 4,801 bi+frags3 7,208 9.44 4,631 Table 1: Data reduction effect of noise filters." ></td>
	<td class="line x" title="58:228	(Ik and I) and (hD,hE) (heb and have) such that hE is the head of wE in the English tree." ></td>
	<td class="line x" title="59:228	Annotation projection assumes direct correspondence (Hwa et al., 2005) between languages (or annotations), whichalthough it is valid in many casesdoes not hold in general: non-parallelism between corresponding expressions in L1 and L2 causes errors in the target annotations." ></td>
	<td class="line x" title="60:228	The word alignment constitutes a further source for errors if it is established automaticallywhich is typically the case in large parallel corpora." ></td>
	<td class="line x" title="61:228	We have implemented a language-independent framework for dependency projection and use the Europarl corpus (Koehn, 2005) as the parallel text." ></td>
	<td class="line x" title="62:228	Europarl consists of the proceedings of the European Parliament, professionally translated in 11 languages (approx." ></td>
	<td class="line x" title="63:228	30mln words per language)." ></td>
	<td class="line x" title="64:228	The data was aligned on the word level with GIZA++ (Och and Ney, 2003).1 In the experiments reported here, we use the language pair English-Dutch, with English as the source for projection (L1) and Dutch as L2." ></td>
	<td class="line x" title="65:228	The English portion of the Europarl corpus was lemmatized and POS tagged with the TreeTagger (Schmid, 1994) and then parsed with MaltParser (which is described in Sec." ></td>
	<td class="line x" title="66:228	6), trained on a dependency-converted version of the WSJ part from the Penn Treebank (Marcus et al., 1994), but with the automatic POS tags." ></td>
	<td class="line x" title="67:228	The Dutch sentences were only POS tagged (with TreeTagger).2 3.1 Data Loss Through Filtering We quantitatively assess the impact of various filtering techniques on a random sample of 100,000 English-Dutch sentence pairs from Europarl (avg." ></td>
	<td class="line x" title="68:228	1Following standard practice, we computed word alignments in both directions (L1  L2 and L2  L1); this gives rise to two unidirectional alignments." ></td>
	<td class="line x" title="69:228	The bidirectional alignment is the intersection of the two unidirectional ones." ></td>
	<td class="line x" title="70:228	2The Dutch POS tags are used to train the monolingual parsers from the projected dependency trees (Sec." ></td>
	<td class="line x" title="71:228	7)." ></td>
	<td class="line x" title="72:228	24.9 words/sentence)." ></td>
	<td class="line x" title="73:228	The English dependency trees are projected to their Dutch counterparts as explained above for Fig." ></td>
	<td class="line x" title="74:228	1a." ></td>
	<td class="line x" title="75:228	The first filter we examine is the one that considers exclusively bidirectional alignments." ></td>
	<td class="line x" title="76:228	It admits dependency arcs to be projected only if the head hE and the dependent wE are each aligned bidirectionally with some word in the Dutch sentence." ></td>
	<td class="line x" title="77:228	This is indicated in Fig." ></td>
	<td class="line x" title="78:228	1b, where the English verb are is aligned with the Dutch translation heeft only in one direction." ></td>
	<td class="line x" title="79:228	This means that none of the dependencies involving are are projected, and the projected structure is not connected." ></td>
	<td class="line x" title="80:228	We will discuss in subsequent sections how less restricted projection methods can still incorporate such data." ></td>
	<td class="line x" title="81:228	Table 1 shows the quantitative effect of the bidirectional filter in the row labeled bidirectional." ></td>
	<td class="line x" title="82:228	The proportion of usable sentences is reduced to 2.11%." ></td>
	<td class="line x" title="83:228	Consequently, the vocabulary size diminishes by a factor of 10, and the average sentence length drops considerably from almost 25 to less than 7 words, suggesting that most non-trivial examples are lost." ></td>
	<td class="line x" title="84:228	3.2 Constrained Fallback Projection As an instance of a more relaxed projection of complete structures, we also implemented a fallback to unidirectional links which projects further dependencies after a partial structure has been built based on the more reliable bidirectional links." ></td>
	<td class="line x" title="85:228	That is, the dependencies established via unidirectional alignments are constrained by the existing subtrees, and are subject to the wellformedness conditions for dependency trees.3 Fig." ></td>
	<td class="line x" title="86:228	1c shows how the fallback mechanism, initialized with the unconnected structure built with the bidirectional filter, recovers a parse tree for the weakly aligned sentence pair in Fig." ></td>
	<td class="line x" title="87:228	1b." ></td>
	<td class="line x" title="88:228	Starting with the leftmost word in the Dutch sentence and its English translation (U and You), there is a unidirectional alignment for the head of You: are is aligned to heeft, so U is established as a dependent of heeft via fallback." ></td>
	<td class="line x" title="89:228	Likewise, heeft can now be identified as the root node." ></td>
	<td class="line x" title="90:228	Note that the (incorrect) alignment between heeft and You will not be pursued because it would lead to heeft being a dependent of itself and thus violating the wellformed3I.e., single headedness and acyclicity; we do not require the trees to be projective, but instead train pseudo-projective models (Nivre and Nilsson, 2005) on the projected data (cf.fn." ></td>
	<td class="line x" title="92:228	5)." ></td>
	<td class="line x" title="93:228	14 #frags 1 2 3 415 >15 #words <4 425 80 12   49 1,331 1,375 1,567 4,793  1019 339 859 1,503 27,910 522 2030 17 45 143 20,756 10,087 >30 0 5 5 4,813 23,362 Table 2: Fragmented parses projected with the alignment filter." ></td>
	<td class="line x" title="94:228	The sentences included in the data set bi+frags3 are in boldface." ></td>
	<td class="line x" title="95:228	ness conditions." ></td>
	<td class="line x" title="96:228	Finally, the subtree rooted in gelijk is incorporated as the second dependent of heeft." ></td>
	<td class="line x" title="97:228	As expected, the proportion of examples that pass this filter rises, to 6.42% (Table 1, fallback)." ></td>
	<td class="line x" title="98:228	However, we will see in Sec." ></td>
	<td class="line x" title="99:228	7 that parsers trained on this data do not improve over parsers trained on the bidirectionally aligned sentences alone." ></td>
	<td class="line x" title="100:228	This is presumably due to the noise that inevitably enters the training data through fallback." ></td>
	<td class="line x" title="101:228	4 Partial Correspondence Projection So far, we have only considered complete trees, i.e., projected structures with exactly one root node." ></td>
	<td class="line x" title="102:228	This is a rather strict requirement, given that even state-of-the-art parsers sometimes fail to produce plausible complete analyses for long sentences, and that non-sentential phrases such as complex noun phrases still contain valuable, non-trivial information." ></td>
	<td class="line x" title="103:228	We therefore propose partial correspondence projection which, in addition to the complete annotations produced by tree-oriented projection, yields partial structures: It admits fragmented analyses in case the tree-oriented projection cannot construct a complete tree." ></td>
	<td class="line x" title="104:228	Of course, the nature of those fragments needs to be restricted so as to exclude data with no (interesting) dependencies." ></td>
	<td class="line x" title="105:228	E.g., a sentence of five words with a parse consisting of five fragments provides virtually no information about dependency structure." ></td>
	<td class="line x" title="106:228	Hence, we impose a limit (fixed at 3 after quick preliminary tests on automatically labeled development data) on the number of fragments that can make up an analysis." ></td>
	<td class="line x" title="107:228	Alternatively, one could require a minimum fragment size." ></td>
	<td class="line x" title="108:228	As an example, consider again Fig." ></td>
	<td class="line x" title="109:228	1b." ></td>
	<td class="line x" title="110:228	This example would be discarded in strict tree projection, but under partial correspondence it is included as a partial analysis consisting of three fragments: U heeft volkomen gelijk Although the amount of information provided in this analysis is limited, the arc between gelijk and volkomen, which is strongly supported by the alignment, can be established without including potentially noisy data points that are only weakly aligned." ></td>
	<td class="line x" title="111:228	We use partial correspondence in combination with bidirectional projection.4 As can be seen in Table 1 (bi+frags3), this combination boosts the amount of usable data to a range similar to that of the fallback technique for trees; but unlike the latter, partial correspondence continues to impose a highprecision filter (bidirectionality) while improving recall through relaxed structural requirements (partial correspondence)." ></td>
	<td class="line x" title="112:228	Table 2 shows how fragment size varies with sentence length." ></td>
	<td class="line x" title="113:228	5 Data-driven Dependency Parsing Models for data-driven dependency parsing can be roughly divided into two paradigms: Graph-based and transition-based models (McDonald and Nivre, 2007)." ></td>
	<td class="line x" title="114:228	5.1 Graph-based Models In the graph-based approach, global optimization considers all possible arcs to find the tree T s.t. T = argmax TD s(T) = argmaxTD summationdisplay (i,j,l)AT s(i,j,l) where D is the set of all well-formed dependency trees for the sentence, AT is the set of arcs in T, and s(i,j,l) is the score of an arc between words wi and wj with label l. The specific graph-based parser we use in this paper is the MSTParser of McDonald et al.(2005)." ></td>
	<td class="line x" title="116:228	The MSTParser learns the scoring function s using an online learning algorithm (Crammer and Singer, 2003) which maximizes the margin between T and D \{T}, based on a loss function that counts the number of words with incorrect parents relative to the correct tree." ></td>
	<td class="line x" title="117:228	5.2 Transition-based Models In contrast to the global optimization employed in graph-based models, transition-based models construct a parse tree in a stepwise way: At each point, 4Fragments from fallback projection turned out not to be helpful as training data for dependency parsers." ></td>
	<td class="line x" title="118:228	15 the locally optimal parser action (transition) t is determined greedily on the basis of the current configuration c (previous actions plus local features): t = argmaxtT s(c,t) where T is the set of possible transitions." ></td>
	<td class="line x" title="119:228	As a representative of the transition-based paradigm, we use the MaltParser (Nivre et al., 2006)." ></td>
	<td class="line x" title="120:228	It implements incremental, deterministic parsing algorithms and employs SVMs to learn the transition scores s. 6 Parsing with Fragmented Trees To make effective use of the fragmented trees produced by partial correspondence projection, both parsing approaches need to be adapted for training on sentences with unconnected substructures." ></td>
	<td class="line x" title="121:228	Here we briefly discuss how we represent these structures, and then describe how we modified the parsers." ></td>
	<td class="line x" title="122:228	We use the CoNLL-X data format for dependency trees (Buchholz and Marsi, 2006) to encode partial structures." ></td>
	<td class="line x" title="123:228	Specifically, every fragment root specifies as its head an artificial root token w0 (distinguished from a true root dependency by a special relation FRAG)." ></td>
	<td class="line x" title="124:228	Thus, sentences with a fragmented parse are still represented as a single sentence, including all words; the difference from a fully parsed sentence is that unconnected substructures are attached directly under w0." ></td>
	<td class="line x" title="125:228	For instance, the partial parse in Fig." ></td>
	<td class="line x" title="126:228	1b would be represented as follows (details omitted): (1) 1 U pron 0 FRAG 2 heeft verb 0 ROOT 3 volkomen adj 4 mod 4 gelijk noun 0 FRAG 6.1 Graph-based Model: fMST In the training phase, the MSTParser tries to maximize the scoring margin between the correct parse and all other valid dependency trees for the sentence." ></td>
	<td class="line x" title="127:228	However, in the case of fragmented trees, the training example is not strictly speaking correct, in the sense that it does not coincide with the desired parse tree." ></td>
	<td class="line x" title="128:228	In fact, this desired tree is among the other possible trees that MST assumes to be incorrect, or at least suboptimal." ></td>
	<td class="line x" title="129:228	In order to relax this assumption, we have to ensure that the loss of the desired tree is zero." ></td>
	<td class="line x" title="130:228	While it is impossible to single out this one tree (since we do not know which one it is), we can steer the margin in the right direction with a loss function that assigns zero loss to all trees that are consistent with the training example, i.e., trees that differ from the training example at most on those words that are fragment roots (e.g., gelijk in Fig." ></td>
	<td class="line x" title="131:228	1)." ></td>
	<td class="line x" title="132:228	To reflect this notion of loss during optimization, we also adjust the definition of the score of a tree: s(T) = summationdisplay (i,j,l)AT: lnegationslash=FRAG s(i,j,l) We refer to this modified model as f(iltering)MST." ></td>
	<td class="line x" title="133:228	6.2 Transition-based Model: fMalt In the transition-based paradigm, it is particularly important to preserve the original context (including unattached words) of a partial analysis, because the parser partly bases its decisions on neighboring words in the sentence." ></td>
	<td class="line x" title="134:228	Emphasis of the role of isolated FRAG dependents as context rather than proper nodes in the tree can be achieved, as with the MSTParser, by eliminating their effect on the margin learned by the SVMs." ></td>
	<td class="line x" title="135:228	Since MaltParser scores local decisions, this simply amounts to suppressing the creation of SVM training instances for such nodes (U and gelijk in (1))." ></td>
	<td class="line x" title="136:228	That is, where the feature model refers to context information, unattached words provide this information (e.g., the feature vector for volkomen in (1) contains the form and POS of gelijk), but there are no instances indicating how they should be attached themselves." ></td>
	<td class="line x" title="137:228	This technique of excluding fragment roots during training will be referred to as fMalt." ></td>
	<td class="line x" title="138:228	7 Experiments 7.1 Setup We train instances of the graphand the transitionbased parser on projected dependencies, and occasionally refer to these as projected parsers.5 All results were obtained on the held-out CoNLL-X test set of 386 sentences (avg." ></td>
	<td class="line x" title="139:228	12.9 5The MaltParsers use the projective Nivre arc-standard parsing algorithm." ></td>
	<td class="line x" title="140:228	For SVM training, data are split on the coarse POS tag, with a threshold of 5,000 instances." ></td>
	<td class="line x" title="141:228	MSTParser instances use the projective Eisner parsing algorithm, and firstorder features." ></td>
	<td class="line x" title="142:228	The input for both systems is projectivized using the head+path schema (Nivre and Nilsson, 2005)." ></td>
	<td class="line x" title="143:228	16 Malt MST Alpino 80.05 82.43 EP 75.33 73.09 Alpino + EP 77.47 81.63 baseline 1 (previous) 23.65 baseline 2 (next) 27.63 Table 3: Upper and lower bounds (UAS)." ></td>
	<td class="line x" title="144:228	words/sentence) from the Alpino treebank (van der Beek et al., 2002)." ></td>
	<td class="line x" title="145:228	The Alpino treebank consists mostly of newspaper text, which means that we are evaluating the projected parsers, which are trained on Europarl, in an out-of-domain setting, in the absence of manually annotated Europarl test data." ></td>
	<td class="line x" title="146:228	Parsing performance is measured in terms of unlabeled attachment score (UAS), i.e., the proportion of tokens that are assigned the correct head, irrespective of the label.6 To establish upper and lower bounds for our task of weakly supervised dependency parsing, we proceed as follows." ></td>
	<td class="line x" title="147:228	We train MaltParsers and MSTParsers on (i) the CoNLL-X training portion of the Alpino treebank (195,000 words), (ii) 100,000 Europarl sentences parsed with the parser obtained from (i), and (iii) the concatenation of the data sets (i) and (ii)." ></td>
	<td class="line x" title="148:228	The first is a supervised upper bound (80.05/82.43% UAS)7 trained on manually labeled in-domain data, while the second constitutes a weaker bound (75.33/73.09%) subject to the same out-of-domain evaluation as the projected parsers, and the third (77.47%) is a self-trained version of (i)." ></td>
	<td class="line x" title="149:228	We note in passing that the supervised model does not benefit from self-training." ></td>
	<td class="line x" title="150:228	Two simple baselines provide approximations to a lower bound: Baseline 1 attaches every word to the preceding word, achieving 23.65%." ></td>
	<td class="line x" title="151:228	Analogously, baseline 2 attaches every word to the following word (27.63%)." ></td>
	<td class="line x" title="152:228	These systems are summarized in Table 3." ></td>
	<td class="line x" title="153:228	6The labeled accuracy of our parsers lags behind the UAS, because the Dutch dependency relations in the projected annotations arise from a coarse heuristic mapping from the original English labels." ></td>
	<td class="line x" title="154:228	We therefore report only UAS." ></td>
	<td class="line x" title="155:228	7The upper bound models are trained with the same parameter settings as the projected parsers (see fn." ></td>
	<td class="line x" title="156:228	5), which were adjusted for noisy training data." ></td>
	<td class="line x" title="157:228	Thus improvements are likely with other settings: Nivre et al.(2006) report 81.35% for a Dutch MaltParser with optimized parameter settings." ></td>
	<td class="line x" title="159:228	McDonald et al.(2006) report 83.57% with MST." ></td>
	<td class="line x" title="161:228	words Malt MST a. trees (bidirectional) 13,500 65.94 67.76 trees (fallback) 62,500 59.28 65.08 bi+frags3 68,000 55.09 57.14 bi+frags3 (fMalt/fMST) 68,000 69.15 70.02 b. trees (bidirectional) 100,000 61.86 69.91 trees (fallback) 100,000 60.05 64.84 bi+frags3 100,000 54.50 55.87 bi+frags3 (fMalt/fMST) 100,000 68.65 69.86 c. trees (bidirectional) 102,300 63.32 69.85 trees (fallback) 465,500 53.45 64.88 bi+frags3 523,000 51.48 57.20 bi+frags3 (fMalt/fMST) 523,000 69.52 70.33 Table 4: UAS of parsers trained on projected dependency structures for (a) a sample of 100,000 sentences, subject to filtering, (b) 10 random samples, each with 100,000 words after filtering (average scores given), and (c) the entire Europarl corpus, subject to filtering." ></td>
	<td class="line x" title="162:228	7.2 Results Table 4a summarizes the results of training parsers on the 100,000-sentence sample analyzed above." ></td>
	<td class="line x" title="163:228	Both the graph-based (MST) and the transitionbased (Malt) parsers react similarly to the more or less aggressive filtering methods, but to different degrees." ></td>
	<td class="line x" title="164:228	The first two rows of the table show the parsers trained on complete trees (trees (bidirectional) and trees (fallback))." ></td>
	<td class="line x" title="165:228	In spite of the additional training data gained by the fallback method, the resulting parsers do not achieve higher accuracy; on the contrary, there is a drop in UAS, especially in the transition-based model (6.66%)." ></td>
	<td class="line x" title="166:228	The increased level of noise in the fallback data has less (but significant)8 impact on the graph-based counterpart (2.68%)." ></td>
	<td class="line x" title="167:228	Turning to the parsers trained on partial correspondence data (bi+frags3), we observe even greater deterioration in both parsing paradigms if the data is used as is. However, in combination with the fMalt/fMST systems (bi+frags3 (fMalt/fMST)), both parsers significantly outperform the tree8Significance testing (p<.01) was performed by means of the t-test on the results of 10 training cycles (Table 4c trees (fb.) only 2 cycles due to time constraints)." ></td>
	<td class="line x" title="168:228	For the experiments in Table 4a and 4c, the cycles differed in terms of the order in which sentences where passed to the parser." ></td>
	<td class="line x" title="169:228	In Table 4b we base significance on 10 true random samples for training." ></td>
	<td class="line x" title="170:228	17 Recall Precision dep." ></td>
	<td class="line x" title="171:228	length 1 2 36 7 root 1 2 36 7 root a. trees (bi.)" ></td>
	<td class="line x" title="172:228	83.41 66.44 52.94 40.64 52.45 82.46 66.06 61.38 34.95 50.97 trees (fb.)" ></td>
	<td class="line x" title="173:228	82.20 64.21 54.59 37.95 55.72 82.64 61.41 54.39 31.96 68.55 bi+frags3 70.18 59.50 46.61 32.14 61.87 83.75 67.22 58.25 32.81 27.01 bi+frags3 (fMalt) 89.23 75.34 59.18 41.65 59.06 83.46 69.05 65.85 48.21 75.79 Alpino-Malt 92.81 84.94 75.11 65.44 66.15 89.71 81.08 77.56 62.57 84.58 b. trees (bi.)" ></td>
	<td class="line x" title="174:228	87.53 73.79 59.57 46.79 71.01 86.43 74.08 64.78 45.17 66.79 trees (fb.)" ></td>
	<td class="line x" title="175:228	82.53 69.37 55.77 37.46 70.24 85.31 69.29 59.85 40.14 53.99 bi+frags3 68.11 57.48 34.30 13.00 90.68 90.28 78.54 66.36 43.70 23.41 bi+frags3 (fMST) 87.73 72.84 62.55 50.15 67.78 86.94 71.60 66.05 48.48 68.20 Alpino-MST 94.13 86.60 76.91 65.14 71.60 91.76 82.49 76.23 71.96 85.38 Table 5: Performance relative to dependency length." ></td>
	<td class="line x" title="176:228	(a) Projected MaltParsers and (b) projected MSTParsers." ></td>
	<td class="line x" title="177:228	oriented models (trees (bidirectional)) by 3.21% (Malt) and 2.26% (MST)." ></td>
	<td class="line x" title="178:228	It would be natural to presume that the superiority of the partial correspondence filter is merely due to the amount of training data, which is larger by a factor of 5.04." ></td>
	<td class="line x" title="179:228	We address this issue by isolating the effect on the quality of the data, and hence the success at noise reduction: In Table 4b, we control for the amount of data that is effectively used in training, so that each filtered training set consists of 100,000 words." ></td>
	<td class="line x" title="180:228	Considering the Malt models, we find that the trends suggested in Table 4a are confirmed: The pattern of relative performance emerges even though any quantitative (dis-)advantages have been eliminated.9 10 Interestingly, the MSTParser does not appear to gain from the increased variety (cf.Table 1) in the partial data: it does not differ significantly from the trees (bi.) model." ></td>
	<td class="line x" title="182:228	Finally, Table 4c provides the results of training on the entire Europarl, or what remains of the corpus after the respective filters have applied." ></td>
	<td class="line x" title="183:228	The results corroborate those obtained for the smaller samples." ></td>
	<td class="line x" title="184:228	In summary, the results support our initial hypothesis that partial correspondence for sentences containing a highly reliable part is preferable to 9The degree of skewedness in the filtered data is not controlled, as it is an important characteristic of the filters." ></td>
	<td class="line x" title="185:228	10Some of the parsers trained on the larger data sets (Table 4b+c) achieve worse results than their smaller counterparts in Table 4a." ></td>
	<td class="line x" title="186:228	We conjecture that it is due to the thresholded POSbased data split, performed prior to SVM training: Larger training sets induce decision models with more specialized SVMs, which are more susceptible to tagging errors." ></td>
	<td class="line x" title="187:228	This could be avoided by increasing the threshold for splitting." ></td>
	<td class="line x" title="188:228	relaxing the reliability citerion, andin the case of the transition-based MaltParseralso to aggressively filtering out all but the reliable complete trees." ></td>
	<td class="line x" title="189:228	With UASs around 70%, both systems are only 5% behind the average 75.07% UAS achieved for Dutch in the CoNLL-X Shared Task." ></td>
	<td class="line x" title="190:228	8 Analysis We have seen that the graphand the transitionbased parser react similarly to the various filtering methods." ></td>
	<td class="line x" title="191:228	However, there are interesting differences in the magnitude of the performance changes." ></td>
	<td class="line x" title="192:228	If we compare the two tree-oriented filters trees (bi.) and trees (fb.), we observe that, although both Malt and MST suffer from the additional noise that is introduced via the unidirectional alignments, the drop in accuracy is much less pronounced in the latter, graph-based model." ></td>
	<td class="line x" title="193:228	Recall that in this paradigm, optimization is performed over the entire tree by scoring edges independenly; this might explain why noisy arcs in the training data have only a negligible impact." ></td>
	<td class="line x" title="194:228	Conversely, the transition-based MaltParser, which constructs parse trees in steps of locally optimal decisions, has an advantage when confronted with partial structures: The individual fragments provide exactly the local context, plus lexical information about the (unconnected) wider context." ></td>
	<td class="line x" title="195:228	To give a more detailed picture of the differences between predicted and actual annotations, we show the performance (of the parsers from Table 4b) separately for binned arc length (Table 5) and sentence length (Table 6)." ></td>
	<td class="line x" title="196:228	As expected, the performance of both the supervised upper bounds (Alpino18 sent." ></td>
	<td class="line x" title="197:228	length <4 49 1019 2030 > 30 a. trees (bi.)" ></td>
	<td class="line x" title="198:228	73.87 62.13 65.67 60.81 55.18 trees (fb.)" ></td>
	<td class="line x" title="199:228	69.91 57.84 62.29 60.04 55.47 bi+frags3 74.14 54.40 56.62 54.07 48.95 bi+fr3 (fMalt) 73.51 65.69 71.70 68.49 63.71 Alpino-Malt 81.98 69.81 81.11 82.82 76.02 b. trees (bi.)" ></td>
	<td class="line x" title="200:228	76.67 70.16 73.09 69.56 63.57 trees (fb.)" ></td>
	<td class="line x" title="201:228	73.24 64.93 67.79 64.98 57.70 bi+frags3 77.48 59.65 55.96 55.27 52.74 bi+fr3 (fMST) 73.24 67.84 73.46 70.04 62.92 Alpino-MST 81.98 72.24 85.10 83.86 78.51 Table 6: UAS relative to sentence length." ></td>
	<td class="line x" title="202:228	(a) Projected MaltParsers and (b) projected MSTParsers." ></td>
	<td class="line x" title="203:228	Malt/MST) and the projected parsers degrades as dependencies get longer, and the difference between the two grows." ></td>
	<td class="line x" title="204:228	Performance across sentence length remains relatively stable." ></td>
	<td class="line x" title="205:228	But note that both tables again reflect the pattern we saw in Table 4." ></td>
	<td class="line x" title="206:228	Importantly, the relative ranking (in terms of f-score, not shown, resp." ></td>
	<td class="line x" title="207:228	UAS) is still in place even in long distance dependencies and long sentences." ></td>
	<td class="line x" title="208:228	This indicates that the effects we have described are not artifacts of a bias towards short dependencies." ></td>
	<td class="line x" title="209:228	In addition, Table 5 sheds some light on the impact of fMalt/fMST in terms of the trade-off between precision and recall." ></td>
	<td class="line x" title="210:228	Without the specific adjustments to handle fragments, partial structures in the training data lead to an immense drop in recall." ></td>
	<td class="line x" title="211:228	By contrast, when the adapted parsers fMalt/fMST are applied, they boosts recall back to a level comparable to or even above that of the tree-oriented projection parsers, while maintaining precision." ></td>
	<td class="line x" title="212:228	Again, this effect can be observed across all arc lengths, except arcs to root, which naturally the bi+frags models are overly eager to predict." ></td>
	<td class="line x" title="213:228	Finally, the learning curves in Fig." ></td>
	<td class="line x" title="214:228	2 illustrate how much labeled data would be required to achieve comparable performance in a supervised setting." ></td>
	<td class="line x" title="215:228	The graph-based upper bound (AlpinoMST) reaches the performance of fMST (trained on the entire Europarl) with approx." ></td>
	<td class="line x" title="216:228	25,000 words of manually labeled treebank data; Alpino-Malt achieves the performance of fMalt with approx." ></td>
	<td class="line x" title="217:228	35,000 words." ></td>
	<td class="line x" title="218:228	The manual annotation of even these moderate amounts of data involves considerable efforts, including the creation of annotation guidelines Figure 2: Learning curves for the supervised upper bounds." ></td>
	<td class="line x" title="219:228	They reach the performance of the projected parsers with 25,000 (MST) resp." ></td>
	<td class="line x" title="220:228	35,000 (Malt) words." ></td>
	<td class="line x" title="221:228	and tools, the training of annotators etc. 9 Conclusion In the context of dependency parsing, we have proposed partial correspondence projection as a greedy method for noise reduction, and illustrated how it can be integrated with data-driven parsing." ></td>
	<td class="line x" title="222:228	Our experimental results show that partial tree structures are well suited to train transition-based dependency parsers." ></td>
	<td class="line x" title="223:228	Graph-based models do not benefit as much from additional partial structures, but instead are more robust to noisy training data, even when the training set is very small." ></td>
	<td class="line x" title="224:228	In future work, we will explore how well the techniques presented here for English and Dutch work for languages that are typologically further apart, e.g., English-Greek or English-Finnish." ></td>
	<td class="line x" title="225:228	Moreover, we are going to investigate how our approach, which essentially ignores unknown parts of the annotation, compares to approaches that marginalize over hidden variables." ></td>
	<td class="line x" title="226:228	We will also explore ways of combining graph-based and transition-based parsers along the lines of Nivre and McDonald (2008)." ></td>
	<td class="line x" title="227:228	Acknowledgments The research reported in this paper has been supported by the German Research Foundation DFG as part of SFB 632 Information structure (project D4; PI: Kuhn)." ></td>
	<td class="line x" title="228:228	19" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-2201
Coupling Semi-Supervised Learning of Categories and Relations
Carlson, Andrew;Betteridge, Justin;Hruschka Junior, Estevam Rafael;Mitchell, Tom M.;"></td>
	<td class="line x" title="1:177	Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 19, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:177	c 2009 Association for Computational Linguistics Coupling Semi-Supervised Learning of Categories and Relations Andrew Carlson1, Justin Betteridge1, Estevam R. Hruschka Jr.1,2 and Tom M. Mitchell1 1School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 {acarlson,jbetter,tom.mitchell}@cs.cmu.edu 2Federal University of Sao Carlos Sao Carlos, SP Brazil estevam@dc.ufscar.br Abstract We consider semi-supervised learning of information extraction methods, especially for extracting instances of noun categories (e.g., athlete, team) and relations (e.g., playsForTeam(athlete,team))." ></td>
	<td class="line x" title="3:177	Semisupervised approaches using a small number of labeled examples together with many unlabeled examples are often unreliable as they frequently produce an internally consistent, but nevertheless incorrect set of extractions." ></td>
	<td class="line x" title="4:177	We propose that this problem can be overcome by simultaneously learning classifiers for many different categories and relations in the presence of an ontology defining constraints that couple the training of these classifiers." ></td>
	<td class="line x" title="5:177	Experimental results show that simultaneously learning a coupled collection of classifiers for 30 categories and relations results in much more accurate extractions than training classifiers individually." ></td>
	<td class="line x" title="6:177	1 Introduction A great wealth of knowledge is expressed on the web in natural language." ></td>
	<td class="line x" title="7:177	Translating this into a structured knowledge base containing facts about entities (e.g., Disney) and relations between those entities (e.g. CompanyIndustry(Disney, entertainment)) would be of great use to many applications." ></td>
	<td class="line x" title="8:177	Although fully supervised methods for learning to extract such facts from text work well, the cost of collecting many labeled examples of each type of knowledge to be extracted is impractical." ></td>
	<td class="line x" title="9:177	Researchers have also explored semi-supervised learning methods that rely primarily on unlabeled data, Figure 1: We show that significant improvements in accuracy result from coupling the training of information extractors for many inter-related categories and relations (B), compared with the simpler but much more difficult task of learning a single information extractor (A)." ></td>
	<td class="line x" title="10:177	but these approaches tend to suffer from the fact that they face an under-constrained learning task, resulting in extractions that are often inaccurate." ></td>
	<td class="line x" title="11:177	We present an approach to semi-supervised learning that yields more accurate results by coupling the training of many information extractors." ></td>
	<td class="line x" title="12:177	The intuition behind our approach (summarized in Figure 1) is that semi-supervised training of a single type of extractor such as coach is much more difficult than simultaneously training many extractors that cover a variety of inter-related entity and relation types." ></td>
	<td class="line x" title="13:177	In particular, prior knowledge about the relationships between these different entities and relations (e.g., that coach(x) implies person(x) and not sport(x)) allows unlabeled data to become a much more useful constraint during training." ></td>
	<td class="line x" title="14:177	Although previous work has coupled the learning of multiple categories, or used static category recognizers to check arguments for learned relation ex1 tractors, our work is the first we know of to couple the simultaneous semi-supervised training of multiple categories and relations." ></td>
	<td class="line x" title="15:177	Our experiments show that this coupling results in more accurate extractions." ></td>
	<td class="line x" title="16:177	Based on our results reported here, we hypothesize that significant accuracy improvements in information extraction will be possible by coupling the training of hundreds or thousands of extractors." ></td>
	<td class="line x" title="17:177	2 Problem Statement It will be helpful to first explain our use of common terms." ></td>
	<td class="line x" title="18:177	An ontology is a collection of unary and binary predicates, also called categories and relations, respectively.1 An instance of a category, or a categoryinstance, is a noun phrase; an instance of a relation, or a relation instance, is a pair of noun phrases." ></td>
	<td class="line x" title="19:177	Instances can be positive or negative with respect to a specific predicate, meaning that the predicate holds or does not hold for that particular instance." ></td>
	<td class="line x" title="20:177	A promoted instance is an instance which our algorithm believes to be a positive instance of some predicate." ></td>
	<td class="line x" title="21:177	Also associated with both categories and relations are patterns: strings of tokens with placeholders (e.g., game against arg1 and arg1 , head coach of arg2)." ></td>
	<td class="line x" title="22:177	A promoted pattern is a pattern believed to be a high-probability indicator for some predicate." ></td>
	<td class="line x" title="23:177	The challenge addressed by this work is to learn extractors to automatically populate the categories and relations of a specified ontology with highconfidence instances, starting from a few seed positive instances and patterns for each predicate and a large corpus of sentences annotated with part-ofspeech (POS) tags." ></td>
	<td class="line x" title="24:177	We focus on extracting facts that are stated multiple times in the corpus, which we can assess probabilistically using corpus statistics." ></td>
	<td class="line x" title="25:177	We do not resolve strings to real-world entities the problems of synonym resolution and disambiguation of strings that can refer to multiple entities are left for future work." ></td>
	<td class="line x" title="26:177	3 Related Work Work on multitask learning has demonstrated that supervised learning of multiple related functions together can yield higher accuracy than learning the functions separately (Thrun, 1996; Caruana, 1997)." ></td>
	<td class="line x" title="27:177	Semi-supervised multitask learning has been shown 1We do not consider predicates of higher arity in this work." ></td>
	<td class="line x" title="28:177	to increase accuracy when tasks are related, allowing one to use a prior that encourages similar parameters (Liu et al., 2008)." ></td>
	<td class="line x" title="29:177	Our work also involves semi-supervised training of multiple coupled functions, but differs in that we assume explicit prior knowledge of the precise way in which our multiple functions are related (e.g., that the values of the functions applied to the same input are mutually exclusive, or that one implies the other)." ></td>
	<td class="line x" title="30:177	In this paper, we focus on a bootstrapping method for semi-supervised learning." ></td>
	<td class="line x" title="31:177	Bootstrapping approaches start with a small number of labeled seed examples, use those seed examples to train an initial model, then use this model to label some of the unlabeled data." ></td>
	<td class="line x" title="32:177	The model is then retrained, using the original seed examples plus the self-labeled examples." ></td>
	<td class="line x" title="33:177	This process iterates, gradually expanding the amount of labeled data." ></td>
	<td class="line pc" title="34:177	Such approaches have shown promise in applications such as web page classification (Blum and Mitchell, 1998), named entity classification (Collins and Singer, 1999), parsing (McClosky et al., 2006), and machine translation (Ueffing, 2006)." ></td>
	<td class="line x" title="35:177	Bootstrapping approaches to information extraction can yield impressive results with little initial human effort (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Pasca et al., 2006)." ></td>
	<td class="line x" title="36:177	However, after many iterations, they usually suffer from semantic drift, where errors in labeling accumulate and the learned concept drifts from what was intended (Curran et al., 2007)." ></td>
	<td class="line x" title="37:177	Coupling the learning of predicates by using positive examples of one predicate as negative examples for others has been shown to help limit this drift (Riloff and Jones, 1999; Yangarber, 2003)." ></td>
	<td class="line x" title="38:177	Additionally, ensuring that relation arguments are of certain, expected types can help mitigate the promotion of incorrect instances (Pasca et al., 2006; Rosenfeld and Feldman, 2007)." ></td>
	<td class="line x" title="39:177	Our work builds on these ideas to couple the simultaneous bootstrapped training of multiple categories and multiple relations." ></td>
	<td class="line x" title="40:177	Our approach to information extraction is based on using high precision contextual patterns (e.g., is mayor of arg1 suggests that arg1 is a city)." ></td>
	<td class="line x" title="41:177	An early pattern-based approach to information extraction acquired is a relations from text using generic contextual patterns (Hearst, 1992)." ></td>
	<td class="line x" title="42:177	This approach was later scaled up to the web by Etzioni et al.(2005)." ></td>
	<td class="line x" title="44:177	2 Other research explores the task of open information extraction, where the predicates to be learned are not specified in advance (Shinyama and Sekine, 2006; Banko et al., 2007), but emerge instead from analysis of the data." ></td>
	<td class="line x" title="45:177	In contrast, our approach relies strongly on knowledge in the ontology about the predicates to be learned, and relationships among them, in order to achieve high accuracy." ></td>
	<td class="line x" title="46:177	Chang et al.(2007) present a framework for learning that optimizes the data likelihood plus constraint-based penalty terms than capture prior knowledge, and demonstrate it with semi-supervised learning of segmentation models." ></td>
	<td class="line x" title="48:177	Constraints that capture domain knowledge guide bootstrap learning of a structured model by penalizing or disallowing violations of those constraints." ></td>
	<td class="line x" title="49:177	While similar in spirit, our work differs in that we consider learning many models, rather than one structured model, and that we are consider a much larger scale application in a different domain." ></td>
	<td class="line x" title="50:177	4 Approach 4.1 Coupling of Predicates As mentioned above, our approach hinges on the notion of coupling the learning of multiple functions in order to constrain the semi-supervised learning problem we face." ></td>
	<td class="line x" title="51:177	Our system learns four different types of functions." ></td>
	<td class="line x" title="52:177	For each category c: 1." ></td>
	<td class="line x" title="53:177	fc,inst : NP(C)[0,1] 2." ></td>
	<td class="line x" title="54:177	fc,patt : PattC(C)[0,1] and for each relation r: 1." ></td>
	<td class="line x" title="55:177	fr,inst : NP(C)NP(C)[0,1] 2." ></td>
	<td class="line x" title="56:177	fr,patt : PattR(C)[0,1] where C is the input corpus, NP(C) is the set of valid noun phrases inC, PattC(C) is the set of valid category patterns in C, and PattR(C) is the set of valid relation patterns in C. Valid noun phrases, category patterns, and relation patterns are defined in Section 4.2.2." ></td>
	<td class="line x" title="57:177	The learning of these functions is coupled in two ways: 1." ></td>
	<td class="line x" title="58:177	Sharing among same-arity predicates according to logical relations 2." ></td>
	<td class="line x" title="59:177	Relation argument type-checking These methods of coupling are made possible by prior knowledge in the input ontology, beyond the lists of categories and relations mentioned above." ></td>
	<td class="line x" title="60:177	We provide general descriptions of these methods of coupling in the next sections, while the details are given in section 4.2." ></td>
	<td class="line x" title="61:177	4.1.1 Sharing among same-arity predicates Each predicate P in the ontology has a list of other same-arity predicates with which P is mutually exclusive, where mutuallyExclusive(P,Pprime)  (P(arg1)  Pprime(arg1))  (Pprime(arg1)  P(arg1)), and similarly for relations." ></td>
	<td class="line x" title="62:177	These mutually exclusive relationships are used to carry out the following simple but crucial coupling: if predicate A is mutually exclusive with predicate B, As positive instances and patterns become negative instances and negative patterns for B. For example, if city, having an instance Boston and a pattern mayor of arg1, is mutually exclusive with scientist, then Boston and mayor of arg1 will become a negative instance and a negative pattern respectively for scientist. Such negative instances and patterns provide negative evidence to constrain the bootstrapping process and forestall divergence." ></td>
	<td class="line x" title="63:177	Some categories are declared to be a subset of one of the other categories being populated, where subset(P,Pprime)P(arg1)Pprime(arg1), (e.g., athlete is a subset of person)." ></td>
	<td class="line x" title="64:177	This prior knowledge is used to share instances and patterns of the subcategory (e.g., athlete) as positive instances and patterns for the super-category (e.g., person)." ></td>
	<td class="line x" title="65:177	4.1.2 Relation argument type-checking The last type of prior knowledge we use to couple the learning of functions is type checking information which couples the learning of relations with categories." ></td>
	<td class="line x" title="66:177	For example, the arguments of the ceoOf relation are declared to be of the categories person and company." ></td>
	<td class="line x" title="67:177	Our approach does not promote a pair of noun phrases as an instance of a relation unless the two noun phrases are classified as belonging to the correct argument types." ></td>
	<td class="line x" title="68:177	Additionally, when a relation instance is promoted, the arguments become promoted instances of their respective categories." ></td>
	<td class="line x" title="69:177	4.2 Algorithm Description In this section, we describe our algorithm, CBL (Coupled Bootstrap Learner), in detail." ></td>
	<td class="line x" title="70:177	The inputs to CBL are a large corpus of POStagged sentences and an initial ontology with pre3 Algorithm 1: CBL Algorithm Input: An ontologyO, and text corpus C Output: Trusted instances/patterns for each predicate SHARE initial instances/patterns among predicates; for i = 1,2,,do foreach predicate pOdo EXTRACT candidate instances/patterns; FILTER candidates; TRAIN instance/pattern classifiers; ASSESS candidates using classifiers; PROMOTE highest-confidence candidates; end SHARE promoted items among predicates; end defined categories, relations, mutually exclusive relationships between same-arity predicates, subset relationships between some categories, seed instances for all predicates, and seed patterns for the categories." ></td>
	<td class="line x" title="71:177	Categories in the input ontology also have a flag indicating whether instances must be proper nouns, common nouns, or whether they can be either (e.g., instances of city are proper nouns)." ></td>
	<td class="line x" title="72:177	Algorithm 1 gives a summary of the CBL algorithm." ></td>
	<td class="line x" title="73:177	First, seed instances and patterns are shared among predicates using the available mutual exclusion, subset, and type-checking relations." ></td>
	<td class="line x" title="74:177	Then, for an indefinite number of iterations, CBL expands the sets of promoted instances and patterns for each predicate, as detailed below." ></td>
	<td class="line x" title="75:177	CBL was designed to allow learning many predicates simultaneously from a large sample of text from the web." ></td>
	<td class="line x" title="76:177	In each iteration of the algorithm, the information needed from the text corpus is gathered in two passes through the corpus using the MapReduce framework (Dean and Ghemawat, 2008)." ></td>
	<td class="line x" title="77:177	This allows us to complete an iteration of the system in 1 hour using a corpus containing millions of web pages (see Section 5.3 for details on the corpus)." ></td>
	<td class="line x" title="78:177	4.2.1 Sharing At the start of execution, seed instances and patterns are shared among predicates according to the mutual exclusion, subset, and type-checking constraints." ></td>
	<td class="line x" title="79:177	Newly promoted instances and patterns are shared at the end of each iteration." ></td>
	<td class="line x" title="80:177	4.2.2 Candidate Extraction CBL finds new candidate instances by using newly promoted patterns to extract the noun phrases that co-occur with those patterns in the text corpus." ></td>
	<td class="line x" title="81:177	To keep the size of this set manageable, CBL limits the number of new candidate instances for each predicate to 1000 by selecting the ones that occur with the most newly promoted patterns." ></td>
	<td class="line x" title="82:177	An analogous procedure is used to extract candidate patterns." ></td>
	<td class="line x" title="83:177	Candidate extraction is performed for all predicates in a single pass through the corpus using the MapReduce framework." ></td>
	<td class="line x" title="84:177	The candidate extraction procedure has definitions for valid instances and patterns that limit extraction to instances that look like noun phrases and patterns that are likely to be informative." ></td>
	<td class="line x" title="85:177	Here we provide brief descriptions of those definitions." ></td>
	<td class="line x" title="86:177	Category Instances In the placeholder of a category pattern, CBL looks for a noun phrase." ></td>
	<td class="line x" title="87:177	It uses part-of-speech tags to segment noun phrases, ignoring determiners." ></td>
	<td class="line x" title="88:177	Proper nouns containing prepositions are segmented using a reimplementation of the Lex algorithm (Downey et al., 2007)." ></td>
	<td class="line x" title="89:177	Category instances are only extracted if they obey the proper/common noun specification of the category." ></td>
	<td class="line x" title="90:177	Category Patterns If a promoted category instance is found in a sentence, CBL extracts the preceding words as a candidate pattern if they are verbs followed by a sequence of adjectives, prepositions, or determiners (e.g., being acquired by arg1) or nouns and adjectives followed by a sequence of adjectives, prepositions, or determiners (e.g., former CEO of arg1)." ></td>
	<td class="line x" title="91:177	CBL extracts the words following the instance as a candidate pattern if they are verbs followed optionally by a noun phrase (e.g., arg1 broke the home run record), or verbs followed by a preposition (e.g., arg1 said that)." ></td>
	<td class="line x" title="92:177	Relation Instances If a promoted relation pattern (e.g., arg1 is mayor of arg2) is found, a candidate relation instance is extracted if both placeholders are valid noun phrases, and if they obey the proper/common specifications for their categories." ></td>
	<td class="line x" title="93:177	Relation Patterns If both arguments from a promoted relation instance are found in a sentence then 4 the intervening sequence of words is extracted as a candidate relation pattern if it contains no more than 5 tokens, has a content word, has an uncapitalized word, and has at least one non-noun." ></td>
	<td class="line x" title="94:177	4.2.3 Candidate Filtering Candidate instances and patterns are filtered to maintain high precision, and to avoid extremely specific patterns." ></td>
	<td class="line x" title="95:177	An instance is only considered for assessment if it co-occurs with at least two promoted patterns in the text corpus, and if its co-occurrence count with all promoted patterns is at least three times greater than its co-occurrence count with negative patterns." ></td>
	<td class="line x" title="96:177	Candidate patterns are filtered in the same manner using instances." ></td>
	<td class="line x" title="97:177	All co-occurrence counts needed by the filtering step are obtained with an additional pass through the corpus using MapReduce." ></td>
	<td class="line x" title="98:177	This implementation is much more efficient than one that relies on web search queries." ></td>
	<td class="line x" title="99:177	CBL typically requires cooccurrence counts of at least 10,000 instances with any of at least 10,000 patterns, which would require 100 million hit count queries." ></td>
	<td class="line x" title="100:177	4.2.4 Candidate Assessment Next, for each predicate CBL trains a discretized Nave Bayes classifier to classify the candidate instances." ></td>
	<td class="line x" title="101:177	Its features include pointwise mutual information (PMI) scores (Turney, 2001) of the candidate instance with each of the positive and negative patterns associated with the class." ></td>
	<td class="line x" title="102:177	The current sets of promoted and negative instances are used as training examples for the classifier." ></td>
	<td class="line x" title="103:177	Attributes are discretized based on information gain (Fayyad and Irani, 1993)." ></td>
	<td class="line x" title="104:177	Patterns are assessed using an estimate of the precision of each pattern p: Precision(p) = summationtext iI count(i,p) count(p) where I is the set of promoted instances for the predicate currently being considered, count(i,p) is the co-occurrence count of instance i with pattern p, and count(p) is the hit count of the pattern p. This is a pessimistic estimate because it assumes that the rest of the occurrences of pattern p are not with positive examples of the predicate." ></td>
	<td class="line x" title="105:177	We also penalize extremely rare patterns by thresholding the denominator using the 25th percentile candidate pattern hit count (McDowell and Cafarella, 2006)." ></td>
	<td class="line x" title="106:177	All of the co-occurrence counts needed for the assessment step are collected in the same MapReduce pass as those required for filtering candidates." ></td>
	<td class="line x" title="107:177	4.2.5 Candidate Promotion CBL then ranks the candidates according to their assessment scores and promotes at most 100 instances and 5 patterns for each predicate." ></td>
	<td class="line x" title="108:177	5 Experimental Evaluation We designed our experimental evaluation to try to answer the following questions: Can CBL iterate many times and still achieve high precision?" ></td>
	<td class="line x" title="109:177	How helpful are the types of coupling that we employ?" ></td>
	<td class="line x" title="110:177	Can we extend existing semantic resources?" ></td>
	<td class="line x" title="111:177	5.1 Configurations of the Algorithm We ran our algorithm in three configurations:  Full: The algorithm as described in Section 4.2." ></td>
	<td class="line x" title="112:177	 No Sharing Among Same-Arity Predicates (NS): This configuration couples predicates only using type-checking constraints." ></td>
	<td class="line x" title="113:177	It uses the full algorithm, except that predicates of the same arity do not share promoted instances and patterns with each other." ></td>
	<td class="line x" title="114:177	Seed instances and patterns are shared, though, so each predicate has a small, fixed pool of negative evidence." ></td>
	<td class="line x" title="115:177	 No Category/Relation coupling (NCR): This configuration couples predicates using mutual exclusion and subset constraints, but not typechecking." ></td>
	<td class="line x" title="116:177	It uses the full algorithm, except that relation instance arguments are not filtered or assessed using their specified categories, and arguments of promoted relations are not shared as promoted instances of categories." ></td>
	<td class="line x" title="117:177	The only type-checking information used is the common/proper noun specifications of arguments for filtering out implausible instances." ></td>
	<td class="line x" title="118:177	5.2 Initial ontology Our ontology contained categories and relations related to two domains: companies and sports." ></td>
	<td class="line x" title="119:177	Extra categories were added to provide negative evidence to the domain-related categories: hobby for economic sector; actor, politician, and scientist for athlete and coach; and board game for sport." ></td>
	<td class="line x" title="120:177	Table 1 lists each predicate in the leftmost column." ></td>
	<td class="line x" title="121:177	Categories were started with 1020 seed 5 5 iterations 10 iterations 15 iterations Predicate Full NS NCR Full NS NCR Full NS NCR Actor 93 100 100 93 97 100 100 97 100 Athlete 100 100 100 100 93 100 100 73 100 Board Game 93 76 93 89 27 93 89 30 93 City 100 100 100 100 97 100 100 100 100 Coach 100 63 73 97 53 43 97 47 47 Company 100 100 100 97 90 97 100 90 100 Country 60 40 60 30 43 27 40 23 40 Economic Sector 77 63 73 57 67 67 50 63 40 Hobby 67 63 67 40 40 57 20 23 30 Person 97 97 90 97 93 97 93 97 93 Politician 93 93 97 73 53 90 90 53 87 Product 97 87 90 90 87 100 97 90 77 Product Type 93 93 90 70 73 97 77 80 67 Scientist 100 90 97 97 63 97 93 60 100 Sport 100 90 100 93 67 83 97 27 90 Sports Team 100 97 100 97 70 100 90 50 100 Category Average 92 84 89 82 70 84 83 63 79 Acquired(Company, Company) 77 77 80 67 80 47 70 63 47 CeoOf(Person, Company) 97 87 100 90 87 97 90 80 83 CoachesTeam(Coach, Sports Team) 100 100 100 100 100 97 100 100 90 CompetesIn(Company, Econ." ></td>
	<td class="line x" title="122:177	Sector) 97 97 80 100 93 67 97 63 60 CompetesWith(Company, Company) 93 80 60 77 70 37 70 60 43 HasOfficesIn(Company, City) 97 93 40 93 90 27 93 57 30 HasOperationsIn(Company, Country) 100 95 50 100 97 40 90 83 13 HeadquarteredIn(Company, City) 77 90 20 70 77 27 70 60 7 LocatedIn(City, Country) 90 67 57 63 50 43 73 50 30 PlaysFor(Athlete, Sports Team) 100 100 0 100 97 7 100 43 0 PlaysSport(Athlete, Sport) 100 100 27 93 80 10 100 40 30 TeamPlaysSport(Sports Team, Sport) 100 100 77 100 97 80 93 83 67 Produces(Company, Product) 91 83 90 83 93 67 93 80 57 HasType(Product, Product Type) 73 63 17 33 67 33 40 57 27 Relation Average 92 88 57 84 84 48 84 66 42 All 92 86 74 83 76 68 84 64 62 Table 1: Precision (%) for each predicate." ></td>
	<td class="line x" title="123:177	Results are presented after 5, 10, and 15 iterations, for the Full, No Sharing (NS), and No Category/Relation Coupling (NCR) configurations of CBL . Note that we expect Full and NCR to perform similarly for categories, but for Full to outperform NCR on relations and for Full to outperform NS on both categories and relations." ></td>
	<td class="line x" title="124:177	6 instances and 5 seed patterns." ></td>
	<td class="line x" title="125:177	The seed instances were specified by a human, and the seed patterns were derived from the generic patterns of Hearst for each predicate (Hearst, 1992)." ></td>
	<td class="line x" title="126:177	Relations were started with similar numbers of seed instances, and no seed patterns (it is less obvious how to generate good seed patterns from relation names)." ></td>
	<td class="line x" title="127:177	Most predicates were declared as mutually exclusive with most others, except for special cases (e.g., hobby and sport; university and sports team; and has offices in and headquartered in)." ></td>
	<td class="line x" title="128:177	5.3 Corpus Our text corpus was from a 200-million page web crawl." ></td>
	<td class="line x" title="129:177	We parsed the HTML, filtered out nonEnglish pages using a stop word ratio threshold, then filtered out web spam and adult content using a bad word list." ></td>
	<td class="line x" title="130:177	The pages were then segmented into sentences, tokenized, and tagged with parts-of-speech using the OpenNLP package." ></td>
	<td class="line x" title="131:177	Finally, we filtered the sentences to eliminate those that were likely to be noisy and not useful for learning (e.g., sentences without a verb, without any lowercase words, with too many words that were all capital letters)." ></td>
	<td class="line x" title="132:177	This yielded a corpus of roughly 514-million sentences." ></td>
	<td class="line x" title="133:177	5.4 Experimental Procedure We ran each configuration for 15 iterations." ></td>
	<td class="line x" title="134:177	To evaluate the precision of promoted instances, we sampled 30 instances from the promoted set for each predicate in each configuration after 5, 10, and 15 iterations, pooled together the samples for each predicate, and then judged their correctness." ></td>
	<td class="line x" title="135:177	The judge did not know which run an instance was sampled from." ></td>
	<td class="line x" title="136:177	We estimated the precision of the promoted instances from each run after 5, 10, and 15 iterations as the number of correct promoted instances divided by the number sampled." ></td>
	<td class="line x" title="137:177	While samples of 30 instances do not produce tight confidence intervals around individual estimates, they are sufficient for testing for the effects in which we are interested." ></td>
	<td class="line x" title="138:177	5.5 Results Table 1 shows the precision of each of the three algorithm configurations for each category and relation after 5, 10, and 15 iterations." ></td>
	<td class="line x" title="139:177	As is apparent in this table, fully coupled training (Full) outperforms training when coupling is removed between categories and relations (NCR), and also when coupling is removed among predicates of the same arity (NS)." ></td>
	<td class="line x" title="140:177	The net effect is substantial, as is apparent from the bottom row of Table 1, which shows that the precision of Full outperforms NS by 6% and NCR by 18% after the first 5 iterations, and by an even larger 20% and 22% after 15 iterations." ></td>
	<td class="line x" title="141:177	This increasing gap in precision as iterations increase reflects the ability of coupled learning to constrain the system to reduce the otherwise common drift associated with self-trained classifiers." ></td>
	<td class="line x" title="142:177	Using Students paired t-test, we found that for categories, the difference in performance between Full and NS is statistically significant after 5, 10, and 15 iterations (p-value < 0.05).2 No significant difference was found between Full and NCR for categories, but this is not a surprise, because NCR still uses mutually exclusive and subset constraints." ></td>
	<td class="line x" title="143:177	The same test finds that the differences between Full and NS are significant for relations after 15 iterations, and the differences between Full and NCR are significant after 5, 10, and 15 iterations for relations." ></td>
	<td class="line x" title="144:177	The worst-performing categories after 15 iterations of Full are country, economic sector, and hobby. The Full configuration of CBL promoted 1637 instances for country, far more than the number of correct answers." ></td>
	<td class="line x" title="145:177	Many of these are general geographic regions like Bayfield Peninsula and Baltic Republics. In the hobby case, promoting patterns like the types of arg1 led to the category drifting into a general list of plural common nouns." ></td>
	<td class="line x" title="146:177	Economic sector drifted into academic fields like Behavioral Science and Political Sciences. We expect that the learning of these categories would be significantly better if there were even more categories being learned to provide additional negative evidence during the filtering and assessment steps of the algorithm." ></td>
	<td class="line x" title="147:177	At this stage of development, obtaining high recall is not a priority because our intent is to create a continuously running and continuously improving system; it is our hope that high recall will come with time." ></td>
	<td class="line x" title="148:177	However, to very roughly convey the completeness of the current results we show in Table 2 the average number of instances promoted for cate2Our selection of the paired t-test was motivated by the work of Smucker et al.(2007), but the Wilcoxon signed rank test gives the same results." ></td>
	<td class="line x" title="150:177	7 Categories Relations Configuration Instances Prec." ></td>
	<td class="line x" title="151:177	Instances Prec." ></td>
	<td class="line x" title="152:177	Full 970 83 191 84 NS 1337 63 307 66 NCR 916 79 458 42 Table 2: Average numbers of promoted category and relation instances and estimates of their precision for each configuration of CBL after 15 iterations." ></td>
	<td class="line x" title="153:177	Figure 2: Extracted facts for two companies discovered by CBL Full." ></td>
	<td class="line x" title="154:177	These two companies were extracted by the learned company extractor, and the relations shown were extracted by learned relation extractors." ></td>
	<td class="line x" title="155:177	gories and relations for each of the three configurations of CBL after 15 iterations." ></td>
	<td class="line x" title="156:177	For categories, not sharing examples results in fewer negative examples during the filtering and assessment steps." ></td>
	<td class="line x" title="157:177	This yields more promoted instances on average." ></td>
	<td class="line x" title="158:177	For relations, not using type checking yields higher relative recall, but at a much lower level of precision." ></td>
	<td class="line x" title="159:177	Figure 2 gives one view of the type of information extracted by the collection of learned category and relation classifiers." ></td>
	<td class="line x" title="160:177	Note the initial seed examples provided to CBL did not include information about either company or any of these relation instances.3 5.6 Comparison to an Existing Database To estimate the capacity of our algorithm to contribute additional facts to publicly available semantic resources, we compared the complete lists of instances promoted during the Full 15 iteration run for certain categories to corresponding lists in the Freebase database (Metaweb Technologies, 2009)." ></td>
	<td class="line x" title="161:177	Excluding the categories that did not have a directly corresponding Freebase list, we computed for each category: Precision|CBLInstances| |Matches|, where Precision is the estimated precision from our random sample of 30 instances, |CBLInstances| is the total number of instances promoted for that category, and |Matches| is the 3See http://rtw.ml.cmu.edu/sslnlp09 for results from a full run of the system." ></td>
	<td class="line x" title="162:177	Est." ></td>
	<td class="line x" title="163:177	CBL Freebase Est." ></td>
	<td class="line x" title="164:177	New Category Prec." ></td>
	<td class="line x" title="165:177	Instances Matches Instances Actor 100 522 465 57 Athlete 100 117 54 63 Board Game 89 18 6 10 City 100 1799 1665 134 Company 100 1937 995 942 Econ." ></td>
	<td class="line x" title="166:177	Sector 50 1541 137 634 Politician 90 962 74 792 Product 97 1259 0 1221 Sports Team 90 414 139 234 Sport 97 613 134 461 Table 3: Estimated numbers of new instances (correct instances promoted by CBL in the Full 15 iteration run which do not have a match in Freebase) and the values used in calculating them." ></td>
	<td class="line x" title="167:177	number of promoted instances that had an exact match in Freebase." ></td>
	<td class="line x" title="168:177	While exact matches may underestimate the number of matches, it should be noted that rather than make definitive claims, our intent here is simply to give rough estimates, which are shown in Table 3." ></td>
	<td class="line x" title="169:177	These approximate numbers indicate a potential to use CBL to extend existing semantic resources like Freebase." ></td>
	<td class="line x" title="170:177	6 Conclusion We have presented a method of coupling the semisupervised learning of categories and relations and demonstrated empirically that the coupling forestalls the problem of semantic drift associated with bootstrap learning methods." ></td>
	<td class="line x" title="171:177	We suspect that learning additional predicates simultaneously will yield even more accurate learning." ></td>
	<td class="line x" title="172:177	An approximate comparison with an existing repository of semantic knowledge, Freebase, suggests that our methods can contribute new facts to existing resources." ></td>
	<td class="line x" title="173:177	Acknowledgments This work is supported in part by DARPA, Google, a Yahoo!" ></td>
	<td class="line x" title="174:177	Fellowship to Andrew Carlson, and the Brazilian research agency CNPq." ></td>
	<td class="line x" title="175:177	We also gratefully acknowledge Jamie Callan for making available his collection of web pages, Yahoo!" ></td>
	<td class="line x" title="176:177	for use of their M45 computing cluster, and the anonymous reviewers for their comments." ></td>
	<td class="line x" title="177:177	8" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-2205
A Comparison of Structural Correspondence Learning and Self-training for Discriminative Parse Selection
Plank, Barbara;"></td>
	<td class="line x" title="1:131	Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 3742, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:131	c 2009 Association for Computational Linguistics A Comparison of Structural Correspondence Learning and Self-training for Discriminative Parse Selection Barbara Plank University of Groningen, The Netherlands b.plank@rug.nl Abstract This paper evaluates two semi-supervised techniques for the adaptation of a parse selection model to Wikipedia domains." ></td>
	<td class="line oc" title="3:131	The techniques examined are Structural Correspondence Learning (SCL) (Blitzer et al., 2006) and Self-training (Abney, 2007; McClosky et al., 2006)." ></td>
	<td class="line n" title="4:131	A preliminary evaluation favors the use of SCL over the simpler self-training techniques." ></td>
	<td class="line oc" title="5:131	1 Introduction and Motivation Parse selection constitutes an important part of many parsing systems (Hara et al., 2005; van Noord and Malouf, 2005; McClosky et al., 2006)." ></td>
	<td class="line x" title="6:131	Yet, there is little to no work focusing on the adaptation of parse selection models to novel domains." ></td>
	<td class="line x" title="7:131	This is most probably due to the fact that potential gains for this task are inherently bounded by the underlying grammar." ></td>
	<td class="line x" title="8:131	The few studies on adapting parse disambiguation models, like Hara et al.(2005), have focused exclusively on supervised domain adaptation, i.e. one has access to a comparably small, but labeled amount of target data." ></td>
	<td class="line x" title="10:131	In contrast, in semisupervised domain adaptation one has only unlabeled target data." ></td>
	<td class="line x" title="11:131	It is a more realistic situation, but at the same time also considerably more difficult." ></td>
	<td class="line x" title="12:131	In this paper we evaluate two semi-supervised approaches to domain adaptation of a discriminative parse selection model." ></td>
	<td class="line oc" title="13:131	We examine Structural Correspondence Learning (SCL) (Blitzer et al., 2006) for this task, and compare it to several variants of Self-training (Abney, 2007; McClosky et al., 2006)." ></td>
	<td class="line x" title="14:131	For empirical evaluation (section 4) we use the Alpino parsing system for Dutch (van Noord and Malouf, 2005)." ></td>
	<td class="line x" title="15:131	As target domain, we exploit Wikipedia as primary test and training collection." ></td>
	<td class="line x" title="16:131	2 Previous Work So far, Structural Correspondence Learning has been applied successfully to PoS tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007)." ></td>
	<td class="line x" title="17:131	An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa, 2007)." ></td>
	<td class="line x" title="18:131	However, the system just ended up at rank 7 out of 8 teams." ></td>
	<td class="line x" title="19:131	Based on annotation differences in the datasets (Dredze et al., 2007) and a bug in their system (Shimizu and Nakagawa, 2007), their results are inconclusive." ></td>
	<td class="line x" title="20:131	A recent attempt (Plank, 2009) shows promising results on applying SCL to parse disambiguation." ></td>
	<td class="line o" title="21:131	In this paper, we extend that line of work and compare SCL to bootstrapping approaches such as self-training." ></td>
	<td class="line oc" title="22:131	Studies on self-training have focused mainly on generative, constituent based parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007)." ></td>
	<td class="line x" title="23:131	Steedman et al.(2003) as well as Reichart and Rappoport (2007) examine self-training for PCFG parsing in the small seed case (< 1k labeled data), with different results." ></td>
	<td class="line oc" title="25:131	In contrast, McClosky et al.(2006) focus on large seeds and exploit a reranking-parser." ></td>
	<td class="line pc" title="27:131	Improvements are obtained (McClosky et al., 2006; McClosky and Charniak, 2008), showing that a reranker is necessary for successful self-training in such a high-resource scenario." ></td>
	<td class="line o" title="28:131	While they self-trained a generative model, we examine self-training and SCL for semi-supervised adaptation of a discriminative parse selection system." ></td>
	<td class="line x" title="29:131	37 3 Semi-supervised Domain Adaptation 3.1 Structural Correspondence Learning Structural Correspondence Learning (Blitzer et al., 2006) exploits unlabeled data from both source and target domain to find correspondences among features from different domains." ></td>
	<td class="line x" title="30:131	These correspondences are then integrated as new features in the labeled data of the source domain." ></td>
	<td class="line x" title="31:131	The outline of SCL is given in Algorithm 1." ></td>
	<td class="line x" title="32:131	The key to SCL is to exploit pivot features to automatically identify feature correspondences." ></td>
	<td class="line x" title="33:131	Pivots are features occurring frequently and behaving similarly in both domains (Blitzer et al., 2006)." ></td>
	<td class="line x" title="34:131	They correspond to auxiliary problems in Ando and Zhang (2005)." ></td>
	<td class="line x" title="35:131	For every such pivot feature, a binary classifier is trained (step 2 of Algorithm 1) by masking the pivot feature in the data and trying to predict it with the remaining non-pivot features." ></td>
	<td class="line x" title="36:131	Non-pivots that correlate with many of the same pivots are assumed to correspond." ></td>
	<td class="line x" title="37:131	These pivot predictor weight vectors thus implicitly align non-pivot features from source and target domain." ></td>
	<td class="line x" title="38:131	Intuitively, if we are able to find good correspondences through linking pivots, then the augmented source data should transfer better to a target domain (Blitzer et al., 2006)." ></td>
	<td class="line x" title="39:131	Algorithm 1 SCL (Blitzer et al., 2006) 1: Select m pivot features." ></td>
	<td class="line x" title="40:131	2: Train m binary classifiers (pivot predictors)." ></td>
	<td class="line x" title="41:131	Create matrix Wnm of binary predictor weight vectors W = [w1,,wm], with n number of nonpivots." ></td>
	<td class="line x" title="42:131	3: Dimensionality Reduction." ></td>
	<td class="line x" title="43:131	Apply SVD to W: Wnm = UnnDnmV Tmm and select  = UT[1:h,:] (the h top left singular vectors of W)." ></td>
	<td class="line x" title="44:131	4: Train a new model on the original and new features obtained by applying the projection x." ></td>
	<td class="line x" title="45:131	SCL for Discriminative Parse Selection So far, pivot features on the word level were used (Blitzer et al., 2006; Blitzer et al., 2007)." ></td>
	<td class="line x" title="46:131	However, for parse disambiguation based on a conditional model they are irrelevant." ></td>
	<td class="line x" title="47:131	Hence, we follow Plank (2009) and actually first parse the unlabeled data." ></td>
	<td class="line x" title="48:131	This allows a possibly noisy, but more abstract representation of the underlying data." ></td>
	<td class="line x" title="49:131	Features thus correspond to properties of parses: application of grammar rules (r1,r2 features), dependency relations (dep), PoS tags (f1,f2), syntactic features (s1), precedence (mf ), bilexical preferences (z), apposition (appos) and further features for unknown words, temporal phrases, coordination (h,in year and p1, respectively)." ></td>
	<td class="line x" title="50:131	These features are further described in van Noord and Malouf (2005)." ></td>
	<td class="line x" title="51:131	Selection of pivot features As pivot features should be common across domains, here we restrict our pivots to be of the type r1,p1,s1 (the most frequently occurring feature types)." ></td>
	<td class="line x" title="52:131	In more detail, r1 indicates which grammar rule applied, p1 whether coordination conjuncts are parallel, and s1 whether local/non-local extraction occurred." ></td>
	<td class="line x" title="53:131	We count how often each feature appears in the parsed source and target domain data, and select those r1,p1,s1 features as pivot features, whose count is > t, where t is a specified threshold." ></td>
	<td class="line x" title="54:131	In all our experiments, we set t = 5000." ></td>
	<td class="line x" title="55:131	In this way we obtained on average 360 pivot features, on the datasets described in Section 4." ></td>
	<td class="line x" title="56:131	3.2 Self-training Self-training (Algorithm 2) is a simple single-view bootstrapping algorithm." ></td>
	<td class="line x" title="57:131	In self-training, the newly labeled instances are taken at face value and added to the training data." ></td>
	<td class="line x" title="58:131	There are many possible ways to instantiate selftraining (Abney, 2007)." ></td>
	<td class="line x" title="59:131	One variant, introduced in Abney (2007) is the notion of (in)delibility: in the delible case the classifier relabels all of the unlabeled data from scratch in every iteration." ></td>
	<td class="line x" title="60:131	The classifier may become unconfident about previously selected instances and they may drop out (Steven Abney, personal communication)." ></td>
	<td class="line x" title="61:131	In contrast, in the indelible case, labels once assigned do not change again (Abney, 2007)." ></td>
	<td class="line x" title="62:131	In this paper we look at the following variants of self-training:  single versus multiple iterations,  selection versus no selection (taking all selflabeled data or selecting presumably higher quality instances); different scoring functions for selection,  delibility versus indelibility for multiple iterations." ></td>
	<td class="line x" title="63:131	38 Algorithm 2 Self-training (indelible) (Abney, 2007)." ></td>
	<td class="line x" title="64:131	1: L0 is labeled [seed] data, U is unlabeled data 2: ctrain(L0) 3: repeat 4: LL + select(label(UL,c)) 5: ctrain(L) 6: until stopping criterion is met Scoring methods We examine three simple scoring functions for instance selection: i) Entropy (summationtextyY (s) p(|s,)logp(|s,))." ></td>
	<td class="line x" title="65:131	ii) Number of parses (|Y (s)|); and iii) Sentence Length (|s|)." ></td>
	<td class="line x" title="66:131	4 Experiments and Results Experimental Design The system used in this study is Alpino, a two-stage dependency parser for Dutch (van Noord and Malouf, 2005)." ></td>
	<td class="line x" title="67:131	The first stage consists of a HPSG-like grammar that constitutes the parse generation component." ></td>
	<td class="line x" title="68:131	The second stage is a Maximum Entropy (MaxEnt) parse selection model." ></td>
	<td class="line x" title="69:131	To train the MaxEnt model, parameters are estimated based on informative samples (Osborne, 2000)." ></td>
	<td class="line x" title="70:131	A parse is added to the training data with a score indicating its goodness (van Noord and Malouf, 2005)." ></td>
	<td class="line x" title="71:131	The score is obtained by comparing it with the gold standard (if available; otherwise the score is approximated through parse probability)." ></td>
	<td class="line x" title="72:131	The source domain is the Alpino Treebank (van Noord and Malouf, 2005) (newspaper text; approx." ></td>
	<td class="line x" title="73:131	7,000 sentences; 145k tokens)." ></td>
	<td class="line x" title="74:131	We use Wikipedia both as testset and as unlabeled target data source." ></td>
	<td class="line x" title="75:131	We assume that in order to parse data from a very specific domain, say about the artist Prince, then data related to that domain, like information about the New Power Generation, the Purple rain movie, or other American singers and artists, should be of help." ></td>
	<td class="line x" title="76:131	Thus, we exploit Wikipedias category system to gather domain-specific target data." ></td>
	<td class="line x" title="77:131	In our empirical setup, we follow Blitzer et al.(2006) and balance the size of source and target data." ></td>
	<td class="line x" title="79:131	Thus, depending on the size of the resulting target domain dataset, and the broadness of the categories involved in creating it, we might wish to filter out certain pages." ></td>
	<td class="line x" title="80:131	We implemented a filter mechanism that excludes pages of a certain category (e.g. a supercategory that is hypothesized to be too broad)." ></td>
	<td class="line x" title="81:131	Further details about the dataset construction are given in (Plank, 2009)." ></td>
	<td class="line x" title="82:131	Table 1 provides information on the target domain datasets constructed from Wikipedia." ></td>
	<td class="line x" title="83:131	Related to Articles Sents Tokens Relationship Prince 290 9,772 145,504 filtered super Paus 445 8,832 134,451 all DeMorgan 394 8,466 132,948 all Table 1: Size of related unlabeled data; relationship indicates whether all related pages are used or some are filtered out." ></td>
	<td class="line x" title="84:131	The size of the target domain testsets is given in Table 2." ></td>
	<td class="line x" title="85:131	As evaluation measure concept accuracy (CA) (van Noord and Malouf, 2005) is used (similar to labeled dependency accuracy)." ></td>
	<td class="line x" title="86:131	The training data for the pivot predictors are the 1-best parses of source and target domain data as selected by the original Alpino model." ></td>
	<td class="line x" title="87:131	We report on results of SCL with dimensionality parameter set to h = 25, and remaining settings identical to Plank (2009) (i.e., no feature-specific regularization and no feature normalization and rescaling)." ></td>
	<td class="line x" title="88:131	Baseline Table 2 shows the baseline accuracies (model trained on labeled out-of-domain data) on the Wikipedia testsets (last column: size in number of sentences)." ></td>
	<td class="line x" title="89:131	The second and third column indicate lower (first parse) and upper(oracle) bounds." ></td>
	<td class="line x" title="90:131	Wikipedia article baseline first oracle sent Prince (musician) 85.03 71.95 88.70 357 Paus Johannes Paulus II 85.72 74.30 89.09 232 Augustus De Morgan 80.09 70.08 83.52 254 Table 2: Supervised Baseline results." ></td>
	<td class="line x" title="91:131	SCL and Self-training results The results for SCL (Table 3) show a small, but consistent increase in absolute performance on all testsets over the baselines (up to +0.27 absolute CA or 7.34% relative error reduction, which is significant at p < 0.05 according to sign test)." ></td>
	<td class="line x" title="92:131	In contrast, basic self-training (Table 3) achieves roughly only baseline accuracy and lower performance than SCL, with one exception." ></td>
	<td class="line x" title="93:131	On the DeMorgan testset, self-training scores slightly higher than SCL." ></td>
	<td class="line x" title="94:131	However, the improvements of both SCL and self-training are not significant on this rather 39 small testset." ></td>
	<td class="line x" title="95:131	Indeed, self-training scores better than the baseline on only 5 parses out of 254, while its performance is lower on 2, leaving only 3 parses that account for the difference." ></td>
	<td class="line x" title="96:131	CA  Rel.ER Prince baseline 85.03 78.06 0.00 SCL  85.30 79.67 7.34 Self-train (all-at-once) 85.08 78.38 1.46 Paus baseline 85.72 77.23 0.00 SCL 85.82 77.87 2.81 Self-train (all-at-once) 85.78 77.62 1.71 DeMorgan baseline 80.09 74.44 0.00 SCL 80.15 74.92 1.88 Self-train (all-at-once) 80.24 75.63 4.65 Table 3: Results of SCL and self-training (single iteration, no selection)." ></td>
	<td class="line x" title="97:131	Entries marked with  are statistically significant at p < 0.05." ></td>
	<td class="line x" title="98:131	The  score incorporates upperand lower-bounds." ></td>
	<td class="line x" title="99:131	To gauge whether other instantiations of selftraining are more effective, we evaluated the selftraining variants introduced in section 3.2 on the Prince dataset." ></td>
	<td class="line x" title="100:131	In the iterative setting, we follow Steedman et al.(2003) and parse 30 sentences from which 20 are selected in every iteration." ></td>
	<td class="line x" title="102:131	With regard to the comparison of delible versus indelible self-training (whether labels may change), our empirical findings shows that the two cases achieve very similar performance; the two curves highly overlap (Figure 1)." ></td>
	<td class="line x" title="103:131	The accuracies of both curves fluctuate around 85.13, showing no upward or downward trend." ></td>
	<td class="line x" title="104:131	In general, however, indelibility is preferred since it takes considerably less time (the classifier does not have to relabel U from scratch in every iteration)." ></td>
	<td class="line x" title="105:131	In addition, we tested EM (which uses all unlabeled data in each iteration)." ></td>
	<td class="line x" title="106:131	Its performance is consistently lower, varying around the baseline." ></td>
	<td class="line x" title="107:131	Figure 2 compares several self-training variants with the supervised baseline and SCL." ></td>
	<td class="line x" title="108:131	It summarizes the effect of i) selection versus no selection (and various selection techniques) as well as ii) single versus multiple iterations of self-training." ></td>
	<td class="line x" title="109:131	For clarity, the figure shows the learning curve of the best selection technique only, but depicts the performance of the various selection techniques in a single iteration (non-solid lines)." ></td>
	<td class="line x" title="110:131	In the iterative setting, taking the whole selflabeled data and not selecting certain instances (grey curve in Figure 2) degrades performance." ></td>
	<td class="line x" title="111:131	In contrast, selecting shorter sentences slightly improves accuracy, and is the best selection method among the ones tested (shorter sentences, entropy, fewer parses)." ></td>
	<td class="line x" title="112:131	For all self-training instantiations, running multiple iterations is on average just the same as running a single iteration (the non-solid lines are roughly the average of the learning curves)." ></td>
	<td class="line x" title="113:131	Thus there is no real need to run several iterations of self-training." ></td>
	<td class="line x" title="114:131	The main conclusion is that in contrast to SCL, none of the self-training instantiations achieves a significant improvement over the baseline." ></td>
	<td class="line oc" title="115:131	5 Conclusions and Future Work The paper compares Structural Correspondence Learning (Blitzer et al., 2006) with (various instances of) self-training (Abney, 2007; McClosky et al., 2006) for the adaptation of a parse selection model to Wikipedia domains." ></td>
	<td class="line n" title="116:131	The empirical findings show that none of the evaluated self-training variants (delible/indelible, single versus multiple iterations, various selection techniques) achieves a significant improvement over the baseline." ></td>
	<td class="line n" title="117:131	The more indirect exploitation of unlabeled data through SCL is more fruitful than pure self-training." ></td>
	<td class="line x" title="118:131	Thus, favoring the use of the more complex method, although the findings are not confirmed on all testsets." ></td>
	<td class="line x" title="119:131	Of course, our results are preliminary and, rather than warranting yet many definite conclusions, encourage further investigation of SCL (varying size of target data, pivots selection, bigger testsets as well as other domains etc.) as well as related semisupervised adaptation techniques." ></td>
	<td class="line x" title="120:131	Acknowledgments Thanks to Gertjan van Noord and the anonymous reviewers for their comments." ></td>
	<td class="line x" title="121:131	The Linux cluster of the High-Performance Computing Center of the University of Groningen was used in this work." ></td>
	<td class="line x" title="122:131	40 0 50 100 150 200 85.00 85.05 85.10 85.15 85.20 85.25 85.30 number of iterations accuracy Indelibility versus delibility baseline SCL Indelible SelfTrain Delible SelfTrain EM Figure 1: Delible versus Indelible self-training and EM." ></td>
	<td class="line x" title="123:131	Delible and indelible self-training achieve very similar performance." ></td>
	<td class="line x" title="124:131	However, indelibility is preferred over delibility since it is considerably faster." ></td>
	<td class="line x" title="125:131	0 50 100 150 200 85.00 85.05 85.10 85.15 85.20 85.25 85.30 number of iterations accuracy shorter sent entropy fewer parses / no selection baseline SCL Indelibility with different selection techniques select shorter sent no selection Figure 2: Self-training variants compared to supervised baseline and SCL." ></td>
	<td class="line x" title="126:131	The effect of various selection techniques (Sec." ></td>
	<td class="line x" title="127:131	3.2) in a single iteration is depicted (non-solid lines; fewer parses and no selection achieve identical results)." ></td>
	<td class="line x" title="128:131	For clarity, the figure shows the learning curve for the best selection technique only (shorter sent) versus no selection." ></td>
	<td class="line x" title="129:131	On average running multiple iterations is just the same as a single iteration." ></td>
	<td class="line x" title="130:131	In all cases SCL still performs best." ></td>
	<td class="line x" title="131:131	41" ></td>
</tr></table>
</div
</body></html>
