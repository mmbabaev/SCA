<html><body><head><link rel="stylesheet" type="text/css" href="style.css" /><script src="map.js"></script><script src="jquery-1.7.1.min.js"></script></head>
<div class="dstPaperData">
W06-1615 <div class="dstPaperTitle">Domain Adaptation With Structural Correspondence Learning</div><div class="dstPaperAuthors">Blitzer, John;McDonald, Ryan;Pereira, Fernando C. N.;</div>
</div>
<table cellspacing="0" cellpadding="0"><tr>
	<td class="srcData" >Source Paper</td>
	<td class="pp legend" ><input type="checkbox" id="cbIPositive" checked="true"/><label for="cbIPositive">Informal +<label></td>
	<td class="nn legend" ><input type="checkbox" id="cbINegative" checked="true"/><label for="cbINegative">Informal -<label></td>
	<td class="oo legend" ><input type="checkbox" id="cbIObjective" checked="true"/><label for="cbIObjective">Informal Neutral<label></td>
	<td class="ppc legend" ><input type="checkbox" id="cbEPositive" checked="true"/><label for="cbEPositive">Formal +</label></td>
	<td class="nnc legend" ><input type="checkbox" id="cbENegative" checked="true"/><label for="cbENegative">Formal -</label></td>
	<td class="ooc legend" ><input type="checkbox" id="cbEObjective" checked="true"/><label for="cbEObjective">Formal Neutral</label></td>
	<td class="lb"><input type="checkbox" id="cbSentenceBoundary"/><label for="cbSentenceBoundary">Sentence Boundary</label></td>
</tr></table>
<div class="dstPaper">
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D07-1070
Bootstrapping Feature-Rich Dependency Parsers with Entropic Priors
Smith, David A.;Eisner, Jason M.;"></td>
	<td class="line x" title="1:458	Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp." ></td>
	<td class="line x" title="2:458	667677, Prague, June 2007." ></td>
	<td class="line x" title="3:458	c2007 Association for Computational Linguistics Bootstrapping Feature-Rich Dependency Parsers with Entropic Priors David A. Smith and Jason Eisner Department of Computer Science Johns Hopkins University Balitmore, MD 21218, USA {dasmith,eisner}@jhu.edu Abstract One may need to build a statistical parser for a new language, using only a very small labeled treebank together with raw text." ></td>
	<td class="line x" title="4:458	We argue that bootstrapping a parser is most promising when the model uses a rich set of redundant features, as in recent models for scoring dependency parses (McDonald et al. , 2005)." ></td>
	<td class="line x" title="5:458	Drawing on Abneys (2004) analysis of the Yarowsky algorithm, we perform bootstrapping by entropy regularization: we maximize a linear combination of conditional likelihood on labeled data and confidence (negative Renyi entropy) on unlabeled data." ></td>
	<td class="line x" title="6:458	In initial experiments, this surpassed EM for training a simple feature-poor generative model, and also improved the performance of a feature-rich, conditionally estimated model where EM could not easily have been applied." ></td>
	<td class="line x" title="7:458	For our models and training sets, more peaked measures of confidence, measured by Renyi entropy, outperformed smoother ones." ></td>
	<td class="line x" title="8:458	We discuss how our feature set could be extended with cross-lingual or cross-domain features, to incorporate knowledge from parallel or comparable corpora during bootstrapping." ></td>
	<td class="line x" title="9:458	1 Motivation In this paper, we address the problem of bootstrapping new statistical parsers for new languages, genres, or domains." ></td>
	<td class="line x" title="10:458	Why is this problem important?" ></td>
	<td class="line x" title="11:458	Many applications of multilingual NLP require parsing in order to extract information, opinions, and answers from text, and to produce improved translations." ></td>
	<td class="line x" title="12:458	Yet an adequate labeled training corpusa large treebank of manually constructed parse trees of typical sentencesis rarely available and would be prohibitively expensive to develop." ></td>
	<td class="line x" title="13:458	We show how it is possible to train instead from a small hand-labeled treebank in the target domain, together with a large unannotated collection of indomain sentences." ></td>
	<td class="line x" title="14:458	Additional resources such as parsers for other domains or languages can be integrated naturally." ></td>
	<td class="line x" title="15:458	Dependency parsing is important as a key component in leading systems for information extraction (Weischedel, 2004)1 and question answering (Peng et al. , 2005)." ></td>
	<td class="line x" title="16:458	These systems rely on edges or paths in dependency parse trees to define their extraction patterns and classification features." ></td>
	<td class="line x" title="17:458	Parsing is also key to the latest advances in machine translation, which translate syntactic phrases (Galley et al. , 2006; Marcu et al. , 2006; Cowan et al. , 2006)." ></td>
	<td class="line x" title="18:458	2 Our Approach Our approach rests on three observations:  Recent feature-based parsing models are an excellent fit for bootstrapping, because the parse is often overdetermined by many redundant features." ></td>
	<td class="line x" title="19:458	 The feature-based framework is flexible enough to incorporate other sources of guidance during training or testingsuch as the knowledge contained in a parser for another language or domain." ></td>
	<td class="line x" title="20:458	 Maximizing a combination of likelihood on labeled data and confidence on unlabeled data is a principled approach to bootstrapping." ></td>
	<td class="line x" title="21:458	2.1 Feature-Based Parsing McDonald et al.(2005) introduced a simple, flexible framework for scoring dependency parses." ></td>
	<td class="line x" title="23:458	Each directed edge e in the dependency tree is described with a high-dimensional feature vector f(e)." ></td>
	<td class="line x" title="24:458	The edges score is the dot product f(e), where  is a learned weight vector." ></td>
	<td class="line x" title="25:458	The overall score of a dependency tree is the sum of the scores of all edges in the tree." ></td>
	<td class="line x" title="26:458	1Ralph Weischedel (p.c)." ></td>
	<td class="line x" title="27:458	reports that this systems performance degrades considerably when only phrase chunking is available rather than full parsing." ></td>
	<td class="line x" title="28:458	667 Givenann-wordinput sentence, theparser begins by scoring each of the O(n2) possible edges, and then seeks the highest-scoring legal dependency tree formed by any n1 of these edges, using an O(n3) dynamic programming algorithm (Eisner, 1996) for projective trees." ></td>
	<td class="line x" title="29:458	For non-projective parsing, O(n3), or with some trickery O(n2), greedy algorithms exist (Chu and Liu, 1965; Edmonds, 1967; Gabow et al. , 1986)." ></td>
	<td class="line x" title="30:458	The feature function f may pay attention to many properties of the directed edgee." ></td>
	<td class="line x" title="31:458	Of course, features may consider the parent and child words connected by e, and their parts of speech.2 But some features used by McDonald et al.(2005) also consider the parts of speech of words adjacent to the parent and child, or between the parent and child, as well as the number of words between the parent and child." ></td>
	<td class="line x" title="33:458	In general, these features are not available in a generative model such as a PCFG." ></td>
	<td class="line x" title="34:458	Although feature-based models are often trained purely discriminatively, we will see in2.6 how to train them to model conditional probabilities." ></td>
	<td class="line x" title="35:458	2.2 Feature-Based Parsing and Bootstrapping The above parsing model is robust, thanks to its many features." ></td>
	<td class="line x" title="36:458	On the Penn Treebank WSJ sections 0221, for example, McDonalds parser extracts 5.5 million feature types from supervised edges alone, with about 120 feature tokens firing per edge." ></td>
	<td class="line x" title="37:458	The highest-scoring parse tree represents a consensus among all features on all prospective edges." ></td>
	<td class="line x" title="38:458	Even if a prospective edge has some discouraging features (i.e. , with negative or zero weights), it may still have a relatively high score thanks to its other features." ></td>
	<td class="line x" title="39:458	Furthermore, even if the edge has a low total score, it may still appear in the consensus parse if the alternatives are even worse or are incompatible with other high-scoring edges." ></td>
	<td class="line x" title="40:458	Put another way, the parser is not able to include high-scoring features or edges independently of one another." ></td>
	<td class="line x" title="41:458	Selecting a good feature means accepting all other features on that edge." ></td>
	<td class="line x" title="42:458	It also means rejecting various other edges, because of the global constraints that a legal parse tree must give each word only one parent and must be free of cycles and, in 2Note that since we are not trying to predict parts of speech, wetreattheoutputofoneormoreautomatictaggersasyetmore inputs to edge feature functions." ></td>
	<td class="line x" title="43:458	the projective case, crossings." ></td>
	<td class="line x" title="44:458	Our observation is that this situation is ideal for so-called bootstrapping, co-training, or minimally supervised learning methods (Yarowsky, 1995; Blum and Mitchell, 1998; Yarowsky and Wicentowski, 2000)." ></td>
	<td class="line x" title="45:458	Such methods should thrive when the right answer is overdetermined owing to redundant features and/or global constraints." ></td>
	<td class="line x" title="46:458	Concretely, suppose we start by training a supervised parser on only 100 examples, using some regularization method to prevent overfitting to this set." ></td>
	<td class="line x" title="47:458	While many features might truly be relevant to the task, only a few appear often enough in this small training set to acquire significantly positive or negative weights." ></td>
	<td class="line x" title="48:458	Even this lightly trained parser may be quite sure of itself on some test sentences in a large unannotated corpus, when one parse scores far higher than all others." ></td>
	<td class="line x" title="49:458	More generally, the parser may be sure about part of a sentence: it may be certain that a particular edge is present (or absent), because that edge tends to be present (or absent) in all high-scoring parses." ></td>
	<td class="line x" title="50:458	Retraining the feature weights  on these highconfidence edges can learn about additional features that are correlated with an edges success or failure." ></td>
	<td class="line x" title="51:458	For example, it may now learn strong weights for lexically specific features that were never observed in the supervised training set." ></td>
	<td class="line x" title="52:458	The retrained parser may now be able to confidently parse even more of the unannotated examples; so we can iterate the process." ></td>
	<td class="line x" title="53:458	Our hope is that the model identifies new good and bad edges at each step, and does so correctly." ></td>
	<td class="line x" title="54:458	The more features and global constraints the model has,  the more power it will have to discriminate among edges even when  is insufficiently trained." ></td>
	<td class="line x" title="55:458	(Some feature weights may be too weak (i.e. , too close to zero) because the initial labeled set is small.)" ></td>
	<td class="line x" title="56:458	 the more robust it will be against errors even when  is incorrectly trained." ></td>
	<td class="line x" title="57:458	(Some feature weights may be too strong or have the wrong sign, because of overfitting or mistaken parses during bootstrapping.)" ></td>
	<td class="line x" title="58:458	668 Intheformercase, strongfeatureslendtheirstrength to weak ones." ></td>
	<td class="line x" title="59:458	In the latter case, a conflict among strong features weakens the ones that depart from the consensus, or discounts the example sentence if there is no consensus." ></td>
	<td class="line x" title="60:458	Previous work on parser bootstrapping has not been able to exploit this redundancy among features, becauseithasusedPCFG-likemodelswithfarfewer features (Steedman et al. , 2003)." ></td>
	<td class="line x" title="61:458	2.3 Adaptation and Projection via Features The previous section assumed that we had a small supervised treebank in the target language and domain (plus a large unsupervised corpus)." ></td>
	<td class="line x" title="62:458	We now consider other, more dubious, knowledge sources that might supplement or replace this small treebank." ></td>
	<td class="line x" title="63:458	In each case, we can use these knowledge sources to derive features that mayor may not prove trustworthy during bootstrapping." ></td>
	<td class="line x" title="64:458	Parses from a different domain." ></td>
	<td class="line x" title="65:458	One might have a treebank for a different domain or genre of the target language." ></td>
	<td class="line x" title="66:458	One could simply include these trees in the initial supervised training, and hope that bootstrapping corrects any learned weights that are inappropriate to the target domain, as discussed above." ></td>
	<td class="line x" title="67:458	In fact, McClosky et al.(2006) found a similar technique to be effectivethough only in a model with a large feature space (PCFG + reranking), as we would predict." ></td>
	<td class="line oc" title="69:458	However, another approach is to train a separate out-of-domain parser, and use this to generate additional features on the supervised and unsupervised in-domain data (Blitzer et al. , 2006)." ></td>
	<td class="line x" title="70:458	Bootstrapping now teaches us where to trust the out-of-domain parser." ></td>
	<td class="line x" title="71:458	If our basic model has 100 features, we could add features 101 through 200, where for example f123(e) = f23 log Pr(e) and Pr(e) is the posterior edge probability according to the out-of-domain parser." ></td>
	<td class="line x" title="72:458	Learning that this feature has a high weight means learning to trust the out-of-domain parsers decision on edges where in-domain feature 23 fires." ></td>
	<td class="line x" title="73:458	Even more sensibly, we could add features such as f201(e) =summationtext10i=1 fi(e)i, where f and  are the featureandweightvectorsfortheout-of-domainparser." ></td>
	<td class="line x" title="74:458	Learning that this feature has a high weight means learning to trust the out-of-domain parsers feature weights for a particular class of features (those numbered1through10)." ></td>
	<td class="line x" title="75:458	Thisaddressestheintuitionthat some linguistic phenomena remain stable across domains." ></td>
	<td class="line x" title="76:458	Parses of translations." ></td>
	<td class="line x" title="77:458	Suppose we have translationsintoEnglishof some ofoursupervisedorunsupervised sentences." ></td>
	<td class="line x" title="78:458	Good probabilistic dependency parsers already exist for English, so we run one over the English translation." ></td>
	<td class="line x" title="79:458	We can now derive many additional features on candidate edges on the target sentence." ></td>
	<td class="line x" title="80:458	For example, dependency edges in the target language of the form c poss p (this denotes a child-to-parent dependency with label possessor) might often correspond to dependency paths in the English translation of the form pprime prepof pobjcprime." ></td>
	<td class="line x" title="81:458	To discover whether this is so, we define a feature i by fi(c possp) def= logsummationdisplay cprime,pprime (Pr(c aligns with cprime) Pr(p aligns with pprime) Pr(pprime prepof pobjcprime)) (1) where cprime,pprime range over word tokens in the English translation, of is a literal English word, and the probabilities are posteriors provided by a probabilistic aligner and a probabilistic English parser." ></td>
	<td class="line x" title="82:458	Note that this is a single feature (not a feature family parameterized by c,p)." ></td>
	<td class="line x" title="83:458	It scores any candidate edge on whether it is a poss edge that seems to align to an English prepof pobj path." ></td>
	<td class="line x" title="84:458	This method is inspired by Hwa et al.(2005), who bootstrapped parsers for Spanish and Chinese by projecting dependencies from English translations and training a new parser on the resulting noisy treebank." ></td>
	<td class="line x" title="86:458	They used only 1-best translations, 1-best alignments, dependency paths of length 1, and no labeled data in Spanish or Chinese." ></td>
	<td class="line x" title="87:458	Hwa et al.(2005) used a manually written postprocessor to correct some of the many incorrect projections." ></td>
	<td class="line x" title="89:458	By contrast, our framework uses the projected dependencies only as one source of features." ></td>
	<td class="line x" title="90:458	They may be overridden by other features in particularcases, andwillbegivenahighweightonlyifthey tend to agree with other features during bootstrapping." ></td>
	<td class="line x" title="91:458	A similar soft projection of dependencies was used in supervised machine translation by Smith and Eisner (2006), who used a source sentences dependency paths to bias the generation of its translation." ></td>
	<td class="line x" title="92:458	669 Note that these bilingual features will only fire on those supervised or unsupervised sentences for which we have an English translation." ></td>
	<td class="line x" title="93:458	In particular, they will usually be unavailable on the test set." ></td>
	<td class="line x" title="94:458	However, we hope that they will seed and facilitate the bootstrapping process, by helping us confidently parse some unsupervised sentences that we would not be able to confidently parse without an English translation." ></td>
	<td class="line x" title="95:458	Parses of comparable English sentences." ></td>
	<td class="line x" title="96:458	World knowledge can be useful in parsing." ></td>
	<td class="line x" title="97:458	Suppose you see a French sentence that contains mangeons and pommes, and you know that manger=eat and pomme=apple." ></td>
	<td class="line x" title="98:458	You might reasonably guess that pommes is the direct object of mangeons, because you know that apple is a plausible direct object for eat." ></td>
	<td class="line x" title="99:458	We can discover this last bit of world knowledge from comparable English text." ></td>
	<td class="line x" title="100:458	Translation dictionaries can themselves be induced from comparable corpora (Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006), or extracted from bitext or digitized versions of human-readable dictionaries if these are available." ></td>
	<td class="line x" title="101:458	The above inference pattern can be captured by features similar to those in equation (1)." ></td>
	<td class="line x" title="102:458	For example, one can define a feature j by fi(c possp) def= logPr(pprime prepof pobjcprime |pprime translates p,cprime translates c) (2) where each event in the event space is a pair (cprime,pprime) of same-sentence tokens in comparable English text, all pairs being equally likely." ></td>
	<td class="line x" title="103:458	Thus, to estimate Pr( | ), the denominator counts same-sentence token pairs (cprime,pprime) in the comparable English corpus that translate into the types (c,p), and the numerator counts such pairs that are also related by a prep of pobj path." ></td>
	<td class="line x" title="104:458	Since the lexical translations and dependency paths are typically not labeled in the English corpus, a given pair must be counted fractionally according to its posterior probability of satisfying these conditions, given models of contextual translation and English parsing.3 3Similarly, Jansche (2005) imputes missing trees by using comparable corpora." ></td>
	<td class="line x" title="105:458	2.4 Bootstrapping as Optimization Section 2.2 assumed a relatively conventional kind of bootstrapping, where each iteration retrains the model on the examples where it is currently most confident." ></td>
	<td class="line x" title="106:458	This kind of confidence thresholding has been popular in previous bootstrapping work (as cited in 2.2)." ></td>
	<td class="line x" title="107:458	It attempts to maintain high accuracy while gradually expanding coverage." ></td>
	<td class="line x" title="108:458	The assumption is that throughout the training procedure, the parsers confidence is a trustworthy guide to its correctness." ></td>
	<td class="line x" title="109:458	Different bootstrapping procedures use different learners, smoothing methods, confidence measures, and procedures for forgetting the labelings from previous iterations." ></td>
	<td class="line x" title="110:458	In his analysis of Yarowsky (1995), Abney (2004) formulates several variants of bootstrapping." ></td>
	<td class="line x" title="111:458	These are shown to increase either the likelihood of the training data, or a lower bound on that likelihood." ></td>
	<td class="line x" title="112:458	In particular, Abney defines a function K that is an upper bound on the negative log-likelihood, and shows his bootstrapping algorithms locally minimize K. We now present a generalization of Abneys K function and relate it to another semi-supervised learning technique, entropy regularization (Brand, 1999; Grandvalet and Bengio, 2005; Jiao et al. , 2006)." ></td>
	<td class="line x" title="113:458	Our experiments will tune the feature weight vector, , to minimize our function." ></td>
	<td class="line x" title="114:458	We will do so simply by applying a generic function minimization method (stochastic gradient descent), rather than by crafting a new Yarowsky-style or Abney-style iterative procedure for our specific function." ></td>
	<td class="line x" title="115:458	Suppose we have examples xi and corresponding possible labelings yi,k. We are trying to learn a parametric model p(yi,k | xi)." ></td>
	<td class="line x" title="116:458	If p(yi,k | xi) is a labeling distribution that reflects our uncertainty about the true labels, then our expected negative loglikelihood of the model is K def= summationdisplay i summationdisplay k p(yi,k |xi)logp(yi,k |xi) = summationdisplay i summationdisplay k p(yi,k|xi)log p(yi,k|xi)p (yi,k|xi)p(yi,k|xi) = summationdisplay i D(pibardblp,i) + H(pi) (3) where pi() def= p(|xi) and p,i() def= p(|xi)." ></td>
	<td class="line x" title="117:458	Note that K is a function not only of  but also 670 of the labeling distribution p; a learner might be allowed to manipulate either in order to decrease K. The summands of K in equation (3) can be divided into two cases, according to whether xi is labeled or not." ></td>
	<td class="line x" title="118:458	For the labeled examplesxi : iL}, the labeling distribution pi is a point distribution that assigns all probability to the true, known label yi. Then H(pi) = 0." ></td>
	<td class="line x" title="119:458	The total contribution of these examples to K simplifies to summationtextiLlogp(yi | xi), i.e., just the negative log-likelihood on the labeled data." ></td>
	<td class="line x" title="120:458	But what is the labeling distribution for the unlabeled examplesxi : i negationslash L}?" ></td>
	<td class="line x" title="121:458	Abney simply uses a uniform distribution over labels (e.g. , parses), to reflect that the label is unknown." ></td>
	<td class="line x" title="122:458	If his bootstrapping algorithm labels xi, then i moves into L and H(pi) is thereby reduced from maximal to 0." ></td>
	<td class="line x" title="123:458	As a result, a method that labels the most confident examples may reduce K, and Abney shows that his method does so." ></td>
	<td class="line x" title="124:458	Our approach is different: we will take the labeling distribution pi to be our actual current belief p,i, and manipulate it through changing  rather than L. L remains the original set of supervised examples." ></td>
	<td class="line x" title="125:458	The total contribution of the unsupervised examples to K then simplifies tosummationtextinegationslashL H(p,i)." ></td>
	<td class="line x" title="126:458	We have no reason to believe that these two contributions (supervised and unsupervised) should be weighted equally." ></td>
	<td class="line x" title="127:458	We thus introduce a multiplier  to form the actual objective function that we minimize with respect to :4 summationdisplay iL logp,i(yi ) +  Nsummationdisplay inegationslashL H(p,i) (4) One may regard  as a Lagrange multiplier that is used to constrain the classifiers uncertainty H to be low, as presented in the work on entropy regularization (Brand, 1999; Grandvalet and Bengio, 2005; Jiao et al. , 2006)." ></td>
	<td class="line x" title="128:458	Conventional bootstrapping retrains on the most confident unsupervised examples, making them 4This function is not necessarily convex in , because of the additionoftheentropyterm(Jiaoetal., 2006)." ></td>
	<td class="line x" title="130:458	Onemighttryan annealingstrategy: start atzero(wherethefunctionisconvex) and gradually increase it, hoping to ride the global maximum." ></td>
	<td class="line x" title="131:458	Although we could increase  until the entropy term dominates the minimizations and we approach a completely deterministic classifier, it is preferable to use some labeled heldout data to evaluate a stopping criterion." ></td>
	<td class="line x" title="132:458	more confident." ></td>
	<td class="line x" title="133:458	Gradient descent on equation (4) essentially does the same, since unsupervised examples contribute to (4) only through H, and the shape of the H function means that it is most rapidly decreased by making the most confident unsupervised examples more confident." ></td>
	<td class="line x" title="134:458	Besidesfavoringmodelsthatareself-confidenton theunlabeleddata,theobjectivefunction(4)alsoexplicitly asks the model to continue to get the correct answers on the initial supervised corpus." ></td>
	<td class="line x" title="135:458	1/ controls the strength of this request." ></td>
	<td class="line x" title="136:458	One could obtain a similar effect in conventional bootstrapping by upweighting the initial labeled corpus when retraining." ></td>
	<td class="line x" title="137:458	2.5 Online Learning Minimizing equation (4) for parsing is more computationally intensive than in many other applications of bootstrapping, such as word sense disambiguation or document classification." ></td>
	<td class="line x" title="138:458	With millions of features, our objective could take many iterations to converge to a local optimum, if we were only to update our parameter vector  after each iteration through a large unsupervised corpus." ></td>
	<td class="line x" title="139:458	For many machine learning problems over large datasets, online learning methods such as stochastic gradient descent (SGD) have been empirically observed to converge in fewer iterations (Bottou, 2003)." ></td>
	<td class="line x" title="140:458	In SGD, instead of taking an optimization step in the direction of the gradient calculated over all unsupervised training examples, we parse each example, calculate the gradient of the objective function evaluated on that example alone, and then take a small step downhill." ></td>
	<td class="line x" title="141:458	The update rule is thus (t+1)  (t)F(t)((t)) (5) where (t) is the parameter vector at time t, F(t)() is the objective function specialized to the time-t example, and  > 0 is a learning rate that we choose." ></td>
	<td class="line x" title="142:458	We check for convergence after each pass through the example set." ></td>
	<td class="line x" title="143:458	2.6 Algorithms and Complexity To evaluate equation (4), we need a conditional model of trees given a sentence xi." ></td>
	<td class="line x" title="144:458	We define one by exponentiating and normalizing the tree scores: p,i(yi,k) def= exp(summationtexteyi,k f(e))/Zi." ></td>
	<td class="line x" title="145:458	With exponentially many parses of xi, does our objective function (4) now have prohibitive com671 putational complexity?" ></td>
	<td class="line x" title="146:458	The complexity is actually similar to that of the inside algorithm for parsing." ></td>
	<td class="line x" title="147:458	In fact, the first term of (4) for projective parsing is found by running the O(n3) inside algorithm on supervised data,5 and its gradient is found by the corresponding O(n3) outside algorithm." ></td>
	<td class="line x" title="148:458	For nonprojective parsing, the analogy to the inside algorithm is the O(n3) matrix-tree algorithm, which is dominated asymptotically by a matrix determinant (Smith and Smith, 2007; Koo et al. , 2007; McDonald and Satta, 2007)." ></td>
	<td class="line x" title="149:458	The gradient of a determinant may be computed by matrix inversion, so evaluating the gradient again has the same O(n3) complexity as evaluating the function." ></td>
	<td class="line x" title="150:458	The second term of (4) is the Shannon entropy of the posterior distribution over parses." ></td>
	<td class="line x" title="151:458	Computing this for projective parsing takes O(n3) time, using a dynamic programming algorithm that is closely related to the inside algorithm (Hwa, 2000).6 For nonprojective parsing, unfortunately, the runtime rises to O(n4), since it requires determinants of n distinct matrices (each incorporating a log factor in a different column; we omit the details)." ></td>
	<td class="line x" title="152:458	The gradient evaluation in both cases is again about as expensive as the function evaluation." ></td>
	<td class="line x" title="153:458	A convenient speedup is to replace Shannon entropy with Renyi entropy." ></td>
	<td class="line x" title="154:458	The family of Renyi entropy measures is parameterized by : R(p) = 11 log parenleftBiggsummationdisplay y p(y) parenrightBigg (6) In our setting, where p = p,i, the events y are the possible parses yi,k of xi." ></td>
	<td class="line x" title="155:458	Observe that under our definition of p, summationtexty p(y) = {summationtexty exp[summationtextey f(e) ()]}/Zi . We already have Zi from running the inside algorithm, and we can find the numerator by running the inside algorithm again with  scaled by ." ></td>
	<td class="line x" title="156:458	Thus with Renyi entropy, all computations and their gradients are O(n3)even in the nonprojective case." ></td>
	<td class="line x" title="157:458	Renyi entropy is also a theoretically attractive generalization." ></td>
	<td class="line x" title="158:458	It can be shown that lim1 R(p) 5The numerator of p,i(y i ) (see definition above) is trivial since yi is a single known parse." ></td>
	<td class="line x" title="159:458	But the denominator Zi is a normalizing constant that sums over all parses; it is found by a dependency-parsing variant of the inside algorithm, following (Eisner, 1996)." ></td>
	<td class="line x" title="160:458	6See also (Mann and McCallum, 2007) for similar results on conditional random fields." ></td>
	<td class="line x" title="161:458	is in fact the Shannon entropy H(p) and that limR(p) = logmaxy p(y), i.e. the negative log probability of the modal or Viterbi label (Arndt, 2001; Karakos et al. , 2007)." ></td>
	<td class="line x" title="162:458	The  = 2 case, widely used as a measure of purity in decision tree learning, is often called the Gini index. Finally, when  = 0, we get the log of the number of labels, which equals the H(uniform distribution) that Abney used in equation (3)." ></td>
	<td class="line x" title="163:458	3 Evaluation For this paper, we performed some initial bootstrapping experiments on small corpora, using the featuresfrom(McDonaldetal., 2005)." ></td>
	<td class="line x" title="165:458	Afterdiscussing experimental setup (3.1), we look at the correlation of confidence with accuracy and with oracle likelihood, and at the fine-grained behaviour of models dependency edge posteriors (3.2)." ></td>
	<td class="line x" title="166:458	We then compare our confidence-maximizing bootstrapping to EM, which has been widely used in semi-supervised learning (3.4)." ></td>
	<td class="line x" title="167:458	Section 3.3 presents overall bootstrapping accuracy." ></td>
	<td class="line x" title="168:458	3.1 Experimental Design We bootstrapped non-projective parsers for languages assembled for the CoNLL dependency parsing competitions (Buchholz and Marsi, 2006)." ></td>
	<td class="line x" title="169:458	We selected German, Spanish, and Czech (Brants et al. , 2002; Civit Torruella and Mart Antonn, 2002; Bohmova et al. , 2003)." ></td>
	<td class="line x" title="170:458	After removing sentences more than 60 words long, we randomly divided each corpus into small seed sets of 100 and 1000 trees; development and test sets of 200 trees each; and an unlabeled training set from the rest." ></td>
	<td class="line x" title="171:458	These treebanks contain strict dependency trees, in the sense that their only nodes are the words and a distinguished root node." ></td>
	<td class="line x" title="172:458	In the Czech dataset, more than one word can attach to the root; also, the trees in German, Spanish, and Czech may be nonprojective." ></td>
	<td class="line x" title="173:458	We use the MSTParser implementation described in McDonald et al.(2005) for feature extraction." ></td>
	<td class="line x" title="175:458	Since our seed sets are so small, we extracted features from all edges in both the seed and the unlabeled parts of our training data, not just the edges annotated as correct." ></td>
	<td class="line x" title="176:458	Since this produced many more features, we pruned our features to those with at least 10 occurrences over all edges." ></td>
	<td class="line x" title="177:458	672 Correlation of 100-tree model 1000-tree model Renyi  Acc." ></td>
	<td class="line x" title="178:458	Xent." ></td>
	<td class="line x" title="179:458	Acc." ></td>
	<td class="line x" title="180:458	Xent." ></td>
	<td class="line x" title="181:458	(uniform, Abney) 0 -0.254 0.980 -0.180 0.937 .5 -0.256 0.981 -0.203 0.955 (Shannon) 1 -0.260 0.983 -0.220 0.964 (Gini) 2 -0.266 0.985 -0.250 0.977 5 -0.291 0.992 -0.304 0.990 7 -0.301 0.993 -0.341 0.991 (Viterbi)  -0.317 0.995 -0.326 0.992 Xent." ></td>
	<td class="line x" title="182:458	-0.391 1.000 -0.410 1.000 Table 1: Correlation, on development sentences, of Renyi entropy with model accuracy and with cross-entropy (Xent.)." ></td>
	<td class="line x" title="183:458	Since these are measures of uncertainty, we see a negative correlation." ></td>
	<td class="line x" title="184:458	As  increases, we place more confidence in highprobability parses and correlate better with accuracy." ></td>
	<td class="line x" title="185:458	We used stochastic gradient descent first to minimize equation (4) on the labeled seed sets." ></td>
	<td class="line x" title="186:458	Then we continued to optimize over the labeled and unlabeleddatatogether." ></td>
	<td class="line x" title="187:458	Wetestedforconvergenceusing accuracy on development data." ></td>
	<td class="line x" title="188:458	3.2 Empirically Evaluating Entropy Bootstrapping assumes that where the parser is confident, it tends to be correct." ></td>
	<td class="line x" title="189:458	Standard bootstrapping methods retrain directly on confident links; similarly, our approach tries to make the parser even more confident on those links." ></td>
	<td class="line x" title="190:458	Is this assumption really true empirically?" ></td>
	<td class="line x" title="191:458	Yes: not only does confidence on unlabeled data correlate with cross-entropy, but both confidence and crossentropy correlate well with accuracy." ></td>
	<td class="line x" title="192:458	As we will see, some confidence measures correlate better than others." ></td>
	<td class="line x" title="193:458	In particular, measures that are more peaked around the one-best prediction of the parser, as in Viterbi re-estimation, perform well." ></td>
	<td class="line x" title="194:458	If we train a non-projective German parser on smallseedsetsof100and1000trees, only, howwell does its own confidence predict its performance?" ></td>
	<td class="line x" title="195:458	For 200 pointslabeled development sentences we measured the linear correlation of various Renyi entropies (6), normalized by sentence length, with treeaccuracy(Table1)." ></td>
	<td class="line x" title="196:458	Wealsomeasuredhowthese normalized Renyi entropies correlate with the posterior log-probability the model assigns to the true parse (the cross-entropy)." ></td>
	<td class="line x" title="197:458	Since Renyi entropy is a measure of uncertainty, we see a negative correlation with accuracy." ></td>
	<td class="line x" title="198:458	This correlation strengthens as we raise  to , so we might expect Viterbi re-estimation, or a differenFigure 1: Posterior probability of correct and incorrect edges in German test data under various models." ></td>
	<td class="line x" title="199:458	We show the distribution of posterior probabilities for correct edges, known from an oracle, in black and incorrect edges in gray." ></td>
	<td class="line x" title="200:458	In the upper row, learning on an initial supervised set raises the posterior probability of correct edges while dragging along some incorrect edges." ></td>
	<td class="line x" title="201:458	In the lower row, we see that adding unlabeled data with R2 entropy continues the pattern of the supervised learner." ></td>
	<td class="line x" title="202:458	R (Viterbi) training induces a second mode in correct posterior probabilities near 1 although it does shift more incorrect edges closer to 1." ></td>
	<td class="line x" title="203:458	Figure 2: Precision-recall curves for selecting edges according to their posterior probabilities: better bootstrapping puts more area under the curve." ></td>
	<td class="line x" title="204:458	tiable objective function with a very high , to perform best on held-out data." ></td>
	<td class="line x" title="205:458	Note also that the crossentropy, which looks at the true labels on the heldout data, does not itself correlate very much better with accuracy than the best unsupervised confidencemeasures." ></td>
	<td class="line x" title="206:458	Finally, weseethatRenyientropies with higher  are more stable: when calculated for a model trained on more data, they improve their correlation with accuracy." ></td>
	<td class="line x" title="207:458	From tree confidence, we now turn to edge confidence: what is the posterior probability that a model assigns to each of the n2 edges in the dependency graph?" ></td>
	<td class="line x" title="208:458	Figure 1 shows smoothed histograms of true edges(black)andfalseedges(gray)inheld-outdata, according to the posterior probabilities we assign to 673 them." ></td>
	<td class="line x" title="209:458	Since there are many more false edges, the figures are cropped to zoom in on the distribution of true edges." ></td>
	<td class="line x" title="210:458	As we start training on the labeled seed set, the posterior probabilities of true edges move towards one; many false edges also get greater mass, but not to the same extent." ></td>
	<td class="line x" title="211:458	As we add unlabeled data, we can see the different learning strategies of different confidence measures." ></td>
	<td class="line x" title="212:458	R2 gradually moves a few true and many fewer false edges towards 1, while R (Viterbi) learning is so confident as to induce a bimodal distribution in the posteriors of true edges." ></td>
	<td class="line x" title="213:458	Figure 2 visualizes the same data as four precision-recall curves, which show how noisy the highest-confidence edges are, across a range of confidence thresholds." ></td>
	<td class="line x" title="214:458	Although the very high precision end stays stable after 10 iterations on the seed set, the addition of unlabeled data puts more area under the curve." ></td>
	<td class="line x" title="215:458	Again, R dominates R2." ></td>
	<td class="line x" title="216:458	3.3 Bootstrapping Results We performed bootstrapping experiments on the full CoNLL sets for Czech, German, and Spanish using the non-projective model from McDonald et al.(2005)." ></td>
	<td class="line x" title="218:458	Performance confirms the results of our analysis above (Table 2)." ></td>
	<td class="line x" title="219:458	Adding unlabeled data improves performance over that of the seed set, with the exception of the Czech data with R2 bootstrapping." ></td>
	<td class="line x" title="220:458	As we saw in 3.2, bootstrapping with R dominates bootstrapping with R2 confidence." ></td>
	<td class="line x" title="221:458	For comparison, we also show the results obtained by supervised training on the combined seed and unlabeled sets." ></td>
	<td class="line x" title="222:458	Recall that we did not use the tree annotations to perform feature selection; models trained withonlysupportedfeaturesoughttoperformbetter." ></td>
	<td class="line x" title="223:458	Although we see statistically significant improvements (at the .05 level on a paired permutation test), the quality of the parsers is still quite poor, in contrast to other applications of bootstrapping which rival supervised methods (Yarowsky, 1995)." ></td>
	<td class="line x" title="224:458	Almost certainly, the CoNLL datasets, comprising at most some tens of thousands of sentences per language, are too small to afford qualitative improvements." ></td>
	<td class="line x" title="225:458	Also, at these relatively small training sizes, ourpreliminaryattemptstoleveragecomparableEnglish corpora did not improve performance." ></td>
	<td class="line x" title="226:458	What features were learned, and how dependent is performance on the seed set?" ></td>
	<td class="line x" title="227:458	We analyzed the performance of German bootstrapping on a develop% accuracy Seed trees  = 0 2  Czech 100 56.1 54.8 58.3 1000 68.1 68.2 68.2 71468 77.9   German 100 60.9 62.4 65.3 1000 74.6 74.5 75.0 37745 86.0   Spanish 100 63.6 64.1 64.4 2786 76.6   Table 2: Dependency accuracy of the McDonald model on 200 test sentences." ></td>
	<td class="line x" title="228:458	When  = 0, training only occurs on the supervised seed data." ></td>
	<td class="line x" title="229:458	As  increases, we train based on confidence in our models analysis of the unlabeled data." ></td>
	<td class="line x" title="230:458	Boldface results are the best in their rows in a permutation test at the .05 level." ></td>
	<td class="line x" title="231:458	ment set (Table 3)." ></td>
	<td class="line x" title="232:458	Using the parameters at the last iteration of supervised training on the seed set as a baseline, wetried updating to their bootstrapped values the weights of only those features that occurred in the seed set." ></td>
	<td class="line x" title="233:458	This achieved nearly the same accuracy as updating all the features." ></td>
	<td class="line x" title="234:458	As one would expect, using only the non-seed features weights performs abysmally." ></td>
	<td class="line x" title="235:458	This might be the case simply because the seed set is likely to contain frequently occurring features." ></td>
	<td class="line x" title="236:458	If, however, we use only the features occurring in an alternate training set of the same size (100 sentences), we get much worse performance." ></td>
	<td class="line x" title="237:458	These results indicate that our bootstrapped parser is still heavily dependent on the features that happened to fire in the seed set; we have not forgotten our initial conditions." ></td>
	<td class="line x" title="238:458	Similar experiments show that unlexicalized features contribute the most to bootstrapping performance." ></td>
	<td class="line x" title="239:458	Since in our log-linear models features have been trained to work together, we must not put too much weight on these ablation results." ></td>
	<td class="line x" title="240:458	These experiments do, however,suggestthatbootstrappingimprovedourresults byrefiningthevaluesofknown, non-lexicalizedfeatures." ></td>
	<td class="line x" title="241:458	3.4 Comparison with EM Perhaps the most popular statistical method for learning from incomplete data is the EM algorithm (Dempster et al. , 1977)." ></td>
	<td class="line x" title="242:458	Since we cannot try EM on McDonalds conditional model, we ran some pilot experiments using the generative dependency model with valence (DMV) of Klein and Manning (2004)." ></td>
	<td class="line x" title="243:458	As in their experiments, and unlike the other experiments in the current paper, we restricted ourselves 674 Updated M feat." ></td>
	<td class="line x" title="244:458	acc." ></td>
	<td class="line x" title="245:458	Updated M feat." ></td>
	<td class="line x" title="246:458	acc." ></td>
	<td class="line x" title="247:458	all 15.5 64.3 none 0 60.9 seed 1.4 64.1 non-seed 14.1 44.7 non-lexical 3.5 64.4 lexical 12.0 59.9 non-bilex." ></td>
	<td class="line x" title="248:458	12.6 64.4 bilexical 2.9 61.0 Table 3: Using all features, dependency accuracy on German development data rose to 64.3% on bootstrapping." ></td>
	<td class="line x" title="249:458	We show the contribution of different feature splits to the performance of this final model." ></td>
	<td class="line x" title="250:458	For example, although this model was trained by updating all 15.5M feature weights, it performs as well if we then keep only the 1.4M features that appeared at least once in theseedset, zeroingouttheweightsoftheothers." ></td>
	<td class="line x" title="251:458	Wedoaswell as the full feature set if we keep only the 3.5M non-lexicalized features." ></td>
	<td class="line x" title="252:458	% accuracy train Bulg." ></td>
	<td class="line x" title="253:458	German Spanish supervised ML 74.2 80.0 71.3 CL 77.5 79.3 75.0 semiEM 58.6 58.8 68.4 supervised Conf." ></td>
	<td class="line x" title="254:458	80.0 80.5 76.7 Table 4: Dependency accuracy of the DMV model (Klein and Manning, 2004)." ></td>
	<td class="line x" title="255:458	Maximizing confidence using R1 (Shannon) entropy improved performance over its own conditional likelihood (CL) baseline and over maximum likelihood (ML)." ></td>
	<td class="line x" title="256:458	EM degraded its ML baseline." ></td>
	<td class="line x" title="257:458	Since these models were only trained and tested on sentences of 10 words or fewer, accuracy is much higher than the full results in Table 2." ></td>
	<td class="line x" title="258:458	to sentences of ten words or fewer and to part-ofspeech sequences alone, without any lexical information." ></td>
	<td class="line x" title="259:458	Since the DMV models projective trees, we ran experiments on three CoNLL corpora that had augmented their primary non-projective parses with alternate projective annotations: Bulgarian (Simov et al. , 2005), German, and Spanish." ></td>
	<td class="line x" title="260:458	We performed supervised maximum likelihood and conditional likelihood estimation on a seed set of 100 sentences for each language." ></td>
	<td class="line x" title="261:458	These models respectively initialized EM and confidence training on unlabeled data." ></td>
	<td class="line x" title="262:458	We see (Table 4) that EM degrades the performance of its ML baseline." ></td>
	<td class="line x" title="263:458	Merialdo (1994) saw a similar degradation over small (and large) seed sets in HMM POS tagging." ></td>
	<td class="line x" title="264:458	We triedfixingandnotfixingthefeatureexpectationson the seed set during EM and show the former, better numbers." ></td>
	<td class="line x" title="265:458	Confidence maximization improved over both its own conditional likelihood initializer and also over ML." ></td>
	<td class="line x" title="266:458	We selected optimal smoothing parameters for all models and optimal  (equation (6)) and  (equation (4)) for the confidence model on labeled held-out data." ></td>
	<td class="line x" title="267:458	4 Future Work We hypothesize that qualitatively better bootstrapping results will require much larger unlabeled data sets." ></td>
	<td class="line x" title="268:458	In scaling up bootstrapping to larger unlabeled training sets, we must carefully weight tradeoffs between expanding coverage and introducing noise from out-of-domain data." ></td>
	<td class="line x" title="269:458	We could also better exploit the data we have with richer models of syntax." ></td>
	<td class="line x" title="270:458	In supervised dependency parsing, secondorder edge features provide improvements (McDonald and Pereira, 2006; Riedel and Clarke, 2006); moreover, the feature-based approach is not limited to dependency parsing." ></td>
	<td class="line x" title="271:458	Similar techniques could score parses in other formalisms, such as CFG or TAG." ></td>
	<td class="line x" title="272:458	In this case, f extracts features from each of the derivation trees rewrite rules (CFG) or elementary trees (TAG)." ></td>
	<td class="line x" title="273:458	In lexicalized formalisms, f will still be able to score lexical dependencies that are implicitly represented in the parse." ></td>
	<td class="line x" title="274:458	Finally, we want to investigate whether larger training sets will provide traction for sparser cross-lingual and crossdomain features." ></td>
	<td class="line x" title="275:458	5 Conclusions Feature-rich dependency models promise to help bootstrapping by providing many redundant features for the learner, and they can also cleanly incorporate cross-domain and cross-language information." ></td>
	<td class="line x" title="276:458	We explored bootstrapping feature-rich nonprojective dependency parsers for Czech, German, and Spanish." ></td>
	<td class="line x" title="277:458	Our bootstrapping method maximizes a linear combination of likelihood and confidence." ></td>
	<td class="line x" title="278:458	In initial experiments on small datasets, this surpassed EM for training a simple feature-poor generative model, and also improved the performance of a feature-rich, conditionally estimated model where EM could not easily have been applied." ></td>
	<td class="line x" title="279:458	For our models and training sets, more peaked measures of confidence, measured by Renyi entropy, outperformed smoother ones." ></td>
	<td class="line x" title="280:458	Acknowledgments The authors thank the anonymous reviewers, Noah A. Smith, and Keith Hall for helpful comments, and Ryan McDonald for making his parsing code publicly available." ></td>
	<td class="line x" title="281:458	This work was supported in part by NSF ITR grant IIS-0313193." ></td>
	<td class="line x" title="282:458	675 References Steven Abney." ></td>
	<td class="line x" title="283:458	2004." ></td>
	<td class="line x" title="284:458	Understanding the Yarowsky algorithm." ></td>
	<td class="line x" title="285:458	CL, 30(3):365395." ></td>
	<td class="line x" title="286:458	Cristoph Arndt." ></td>
	<td class="line x" title="287:458	2001." ></td>
	<td class="line x" title="288:458	Information Measures." ></td>
	<td class="line x" title="289:458	Springer." ></td>
	<td class="line x" title="290:458	John Blitzer, Ryan McDonald, and Fernando Pereira." ></td>
	<td class="line x" title="291:458	2006." ></td>
	<td class="line x" title="292:458	Domain adaptation with structural correspondence learning." ></td>
	<td class="line x" title="293:458	In EMNLP, pages 120128." ></td>
	<td class="line x" title="294:458	A. Blum and Tom Mitchell." ></td>
	<td class="line x" title="295:458	1998." ></td>
	<td class="line x" title="296:458	Combining labeled and unlabeled data with co-training." ></td>
	<td class="line x" title="297:458	In COLT." ></td>
	<td class="line x" title="298:458	A. Bohmova, J. Hajic, E. Hajicova, and B. Hladka. 2003." ></td>
	<td class="line x" title="299:458	The PDT: a 3-level annotation scenario." ></td>
	<td class="line x" title="300:458	In A. Abeille, editor, Treebanks: Building and Using Parsed Corpora, volume 20 of Text, Speech and Language Technology, chapter 7." ></td>
	<td class="line x" title="301:458	Kluwer." ></td>
	<td class="line x" title="302:458	Leon Bottou." ></td>
	<td class="line x" title="303:458	2003." ></td>
	<td class="line x" title="304:458	Stochastic learning." ></td>
	<td class="line x" title="305:458	In Advanced Lectures in Machine Learning, pages 146168." ></td>
	<td class="line x" title="306:458	Springer." ></td>
	<td class="line x" title="307:458	Matthew E. Brand." ></td>
	<td class="line x" title="308:458	1999." ></td>
	<td class="line x" title="309:458	Structure learning in conditional probability models via an entropic prior and parameter extinction." ></td>
	<td class="line x" title="310:458	Neural Computation, 11(5):11551182." ></td>
	<td class="line x" title="311:458	S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith." ></td>
	<td class="line x" title="312:458	2002." ></td>
	<td class="line x" title="313:458	The TIGER treebank." ></td>
	<td class="line x" title="314:458	In TLT." ></td>
	<td class="line x" title="315:458	S. Buchholz and E. Marsi." ></td>
	<td class="line x" title="316:458	2006." ></td>
	<td class="line x" title="317:458	CoNLL-X shared task on multilingual dependency parsing." ></td>
	<td class="line x" title="318:458	In CoNLL." ></td>
	<td class="line x" title="319:458	Y.J. Chu and T.H. Liu." ></td>
	<td class="line x" title="320:458	1965." ></td>
	<td class="line x" title="321:458	On the shortest arborescence of a directed graph." ></td>
	<td class="line x" title="322:458	Science Sinica, 14:13961400." ></td>
	<td class="line x" title="323:458	M. Civit Torruella and M. A. Mart Antonn. 2002." ></td>
	<td class="line x" title="324:458	Design principles for a Spanish treebank." ></td>
	<td class="line x" title="325:458	In TLT." ></td>
	<td class="line x" title="326:458	Brooke Cowan, Ivona Kucerova, and Michael Collins." ></td>
	<td class="line x" title="327:458	2006." ></td>
	<td class="line x" title="328:458	A discriminative model for tree-to-tree translation." ></td>
	<td class="line x" title="329:458	In EMNLP, pages 232241." ></td>
	<td class="line x" title="330:458	A. Dempster, N. Laird, and D. Rubin." ></td>
	<td class="line x" title="331:458	1977." ></td>
	<td class="line x" title="332:458	Maximum likelihood estimation from incomplete data via the EM algorithm." ></td>
	<td class="line x" title="333:458	Journal of the Royal Statistical Society B, 39:138." ></td>
	<td class="line x" title="334:458	J. Edmonds." ></td>
	<td class="line x" title="335:458	1967." ></td>
	<td class="line x" title="336:458	Optimum branchings." ></td>
	<td class="line x" title="337:458	Journal of Research of the National Bureau of Standards, 71B:233240." ></td>
	<td class="line x" title="338:458	Jason Eisner." ></td>
	<td class="line x" title="339:458	1996." ></td>
	<td class="line x" title="340:458	Three new probabilistic models for dependency parsing: An exploration." ></td>
	<td class="line x" title="341:458	In COLING, pages 340345." ></td>
	<td class="line x" title="342:458	H. N. Gabow, Z. Galil, T. H. Spencer, and R. E. Tarjan." ></td>
	<td class="line x" title="343:458	1986." ></td>
	<td class="line x" title="344:458	Efficient algorithms for finding minimum spanning trees in undirected and directed graphs." ></td>
	<td class="line x" title="345:458	Combinatorica, 6(2):109122." ></td>
	<td class="line x" title="346:458	Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer." ></td>
	<td class="line x" title="347:458	2006." ></td>
	<td class="line x" title="348:458	Scalable inference and training of context-rich syntactic translation models." ></td>
	<td class="line x" title="349:458	In ACL, pages 961968." ></td>
	<td class="line x" title="350:458	Yves Grandvalet and Yoshua Bengio." ></td>
	<td class="line x" title="351:458	2005." ></td>
	<td class="line x" title="352:458	Semi-supervised learning by entropy minimization." ></td>
	<td class="line x" title="353:458	In NIPS." ></td>
	<td class="line x" title="354:458	Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak." ></td>
	<td class="line x" title="355:458	2005." ></td>
	<td class="line x" title="356:458	Bootstrapping parsers via syntactic projection across parallel texts." ></td>
	<td class="line x" title="357:458	Natural Language Engineering, 11:311325." ></td>
	<td class="line x" title="358:458	Rebecca Hwa." ></td>
	<td class="line x" title="359:458	2000." ></td>
	<td class="line x" title="360:458	Sample selection for statistical grammar induction." ></td>
	<td class="line x" title="361:458	In EMNLP, pages 4552." ></td>
	<td class="line x" title="362:458	Martin Jansche." ></td>
	<td class="line x" title="363:458	2005." ></td>
	<td class="line x" title="364:458	Treebank transfer." ></td>
	<td class="line x" title="365:458	In IWPT." ></td>
	<td class="line x" title="366:458	Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell Greiner, and Dale Schuurmans." ></td>
	<td class="line x" title="367:458	2006." ></td>
	<td class="line x" title="368:458	Semi-supervised conditional random fields for improved sequence segmentation and labeling." ></td>
	<td class="line x" title="369:458	In COLING/ACL, pages 209216." ></td>
	<td class="line x" title="370:458	Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, and Carey E. Priebe." ></td>
	<td class="line x" title="371:458	2007." ></td>
	<td class="line x" title="372:458	Cross-instance tuning of unsupervised document clustering algorithms." ></td>
	<td class="line x" title="373:458	In HLT-NAACL, pages 252259." ></td>
	<td class="line x" title="374:458	Dan Klein and Christopher D. Manning." ></td>
	<td class="line x" title="375:458	2004." ></td>
	<td class="line x" title="376:458	Corpus-based induction of syntactic structure: Models of dependency and constituency." ></td>
	<td class="line x" title="377:458	In ACL, pages 479486." ></td>
	<td class="line x" title="378:458	Alexandre Klementiev and Dan Roth." ></td>
	<td class="line x" title="379:458	2006." ></td>
	<td class="line x" title="380:458	Weakly supervised named entity transliteration and discovery from multilingual comparable corpora." ></td>
	<td class="line x" title="381:458	In COLING-ACL, pages 817824." ></td>
	<td class="line x" title="382:458	T. Koo, A. Globerson, X. Carreras, and M. Collins." ></td>
	<td class="line x" title="383:458	2007." ></td>
	<td class="line x" title="384:458	Structured prediction models via the Matrix-Tree Theorem." ></td>
	<td class="line x" title="385:458	In EMNLP-CoNLL." ></td>
	<td class="line x" title="386:458	Gideon S. Mann and Andrew McCallum." ></td>
	<td class="line x" title="387:458	2007." ></td>
	<td class="line x" title="388:458	Efficient computation of entropy gradient for semi-supervised conditional random fields." ></td>
	<td class="line x" title="389:458	In HLT-NAACL." ></td>
	<td class="line x" title="390:458	Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight." ></td>
	<td class="line x" title="391:458	2006." ></td>
	<td class="line x" title="392:458	SPMT: Statistical machine translation with syntactified target language phrases." ></td>
	<td class="line x" title="393:458	In EMNLP, pages 4452, July." ></td>
	<td class="line x" title="394:458	David McClosky, Eugene Charniak, and Mark Johnson." ></td>
	<td class="line x" title="395:458	2006." ></td>
	<td class="line x" title="396:458	Reranking and self-training for parser adaptation." ></td>
	<td class="line x" title="397:458	In ACL, pages 337344." ></td>
	<td class="line x" title="398:458	Ryan McDonald and Fernando Pereira." ></td>
	<td class="line x" title="399:458	2006." ></td>
	<td class="line x" title="400:458	Online learning of approximate dependency parsing algorithms." ></td>
	<td class="line x" title="401:458	In EACL." ></td>
	<td class="line x" title="402:458	Ryan McDonald and Giorgio Satta." ></td>
	<td class="line x" title="403:458	2007." ></td>
	<td class="line x" title="404:458	On the complexity of non-projective data-driven dependency parsing." ></td>
	<td class="line x" title="405:458	In IWPT." ></td>
	<td class="line x" title="406:458	Ryan McDonald, Koby Crammer, and Fernando Pereira." ></td>
	<td class="line x" title="407:458	2005." ></td>
	<td class="line x" title="408:458	Online large-margin training of dependency parsers." ></td>
	<td class="line x" title="409:458	In ACL, pages 9198." ></td>
	<td class="line x" title="410:458	Bernardo Merialdo." ></td>
	<td class="line x" title="411:458	1994." ></td>
	<td class="line x" title="412:458	Tagging English text with a probabilistic model." ></td>
	<td class="line x" title="413:458	CL, 20(2):15572." ></td>
	<td class="line x" title="414:458	Fuchun Peng, Ralph Weischedel, Ana Licuanan, and Jinxi Xu." ></td>
	<td class="line x" title="415:458	2005." ></td>
	<td class="line x" title="416:458	Combining deep linguistics analysis and surface pattern learning: A hybrid approach to Chinese definitional question answering." ></td>
	<td class="line x" title="417:458	In HLT-EMNLP, pages 307314." ></td>
	<td class="line x" title="418:458	Sebastian Riedel and James Clarke." ></td>
	<td class="line x" title="419:458	2006." ></td>
	<td class="line x" title="420:458	Incremental integer linear programming for non-projective dependency parsing." ></td>
	<td class="line x" title="421:458	In EMNLP, pages 129137." ></td>
	<td class="line x" title="422:458	Charles Schafer and David Yarowsky." ></td>
	<td class="line x" title="423:458	2002." ></td>
	<td class="line x" title="424:458	Inducing translation lexicons via diverse similarity measures and bridge languages." ></td>
	<td class="line x" title="425:458	In CoNLL." ></td>
	<td class="line x" title="426:458	Charles Schafer." ></td>
	<td class="line x" title="427:458	2006." ></td>
	<td class="line x" title="428:458	Translation Discovery Using Diverse Smilarity Measures." ></td>
	<td class="line x" title="429:458	Ph.D. thesis, Johns Hopkins University." ></td>
	<td class="line x" title="430:458	K. Simov, P. Osenova, A. Simov, and M. Kouylekov." ></td>
	<td class="line x" title="431:458	2005." ></td>
	<td class="line x" title="432:458	Design and implementation of the Bulgarian HPSG-based treebank." ></td>
	<td class="line x" title="433:458	In Journal of Research on Language and Computation  Special Issue." ></td>
	<td class="line x" title="434:458	Kluwer." ></td>
	<td class="line x" title="435:458	David A. Smith and Jason Eisner." ></td>
	<td class="line x" title="436:458	2006." ></td>
	<td class="line x" title="437:458	Quasi-synchronous grammars: Alignment by soft projection of syntactic dependencies." ></td>
	<td class="line x" title="438:458	In Proceedings of the HLT-NAACL Workshop on Statistical Machine Translation, pages 2330." ></td>
	<td class="line x" title="439:458	David A. Smith and Noah A. Smith." ></td>
	<td class="line x" title="440:458	2007." ></td>
	<td class="line x" title="441:458	Probabilistic models of nonprojective dependency trees." ></td>
	<td class="line x" title="442:458	In EMNLP-CoNLL." ></td>
	<td class="line x" title="443:458	Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen, Steven Baker, and Jeremiah Crim." ></td>
	<td class="line x" title="444:458	2003." ></td>
	<td class="line x" title="445:458	Bootstrapping statistical parsers from small datasets." ></td>
	<td class="line x" title="446:458	In EACL." ></td>
	<td class="line x" title="447:458	Ralph Weischedel." ></td>
	<td class="line x" title="448:458	2004." ></td>
	<td class="line x" title="449:458	Extracting dynamic evidence networks." ></td>
	<td class="line x" title="450:458	Technical Report AFRL-IF-RS-TR-2004-246, BBN Technologies, Cambridge, MA, December." ></td>
	<td class="line x" title="451:458	David Yarowsky and Richard Wicentowski." ></td>
	<td class="line x" title="452:458	2000." ></td>
	<td class="line x" title="453:458	Minimally supervised morphological analysis by multi-modal alignment." ></td>
	<td class="line x" title="454:458	In ACL, pages 207216." ></td>
	<td class="line x" title="455:458	David Yarowsky." ></td>
	<td class="line x" title="456:458	1995." ></td>
	<td class="line x" title="457:458	Un supervised word sense disambiguation rivaling supervised methods." ></td>
	<td class="line x" title="458:458	In ACL, pages 189196 ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D07-1096
The CoNLL 2007 Shared Task on Dependency Parsing
Nivre, Joakim;Hall, Johan;Kübler, Sandra;McDonald, Ryan;Nilsson, Jens;Riedel, Sebastian;Yuret, Deniz;"></td>
	<td class="line x" title="1:410	Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp." ></td>
	<td class="line x" title="2:410	915932, Prague, June 2007." ></td>
	<td class="line x" title="3:410	c2007 Association for Computational Linguistics The CoNLL 2007 Shared Task on Dependency Parsing Joakim Nivre Johan Hall Sandra Kubler Ryan McDonald Jens Nilsson Sebastian Riedel Deniz Yuret Vaxjo University, School of Mathematics and Systems Engineering, first.last@vxu.se Uppsala University, Dept. of Linguistics and Philology, joakim.nivre@lingfil.uu.se Indiana University, Department of Linguistics, skuebler@indiana.edu Google Inc. , ryanmcd@google.com University of Edinburgh, School of Informatics, S.R.Riedel@sms.ed.ac.uk Koc University, Dept. of Computer Engineering, dyuret@ku.edu.tr Abstract The Conference on Computational Natural Language Learning features a shared task, in which participants train and test their learning systems on the same data sets." ></td>
	<td class="line x" title="4:410	In 2007, as in 2006, the shared task has been devoted to dependency parsing, this year with both a multilingual track and a domain adaptation track." ></td>
	<td class="line x" title="5:410	Inthispaper, wedefinethetasksofthe different tracks and describe how the data sets were created from existing treebanks for ten languages." ></td>
	<td class="line x" title="6:410	In addition, we characterize the different approaches of the participating systems, report the test results, and provide a first analysis of these results." ></td>
	<td class="line x" title="7:410	1 Introduction Previous shared tasks of the Conference on Computational Natural Language Learning (CoNLL) have been devoted to chunking (1999, 2000), clause identification (2001), named entity recognition (2002, 2003), and semantic role labeling (2004, 2005)." ></td>
	<td class="line x" title="8:410	In 2006 the shared task was multilingual dependency parsing, where participants had to train a single parser on data from thirteen different languages, which enabled a comparison not only of parsing and learning methods, but also of the performance that can be achieved for different languages (Buchholz and Marsi, 2006)." ></td>
	<td class="line x" title="9:410	In dependency-based syntactic parsing, the task is to derive a syntactic structure for an input sentence by identifying the syntactic head of each word in the sentence." ></td>
	<td class="line x" title="10:410	This defines a dependency graph, where the nodes are the words of the input sentence and the arcs are the binary relations from head to dependent." ></td>
	<td class="line x" title="11:410	Often, but not always, it is assumed that all words except one have a syntactic head, which means that the graph will be a tree with the single independent word as the root." ></td>
	<td class="line x" title="12:410	In labeled dependency parsing, we additionally require the parser to assign a specific type (or label) to each dependency relation holding between a head word and a dependent word." ></td>
	<td class="line x" title="13:410	In this years shared task, we continue to explore data-driven methods for multilingual dependency parsing, but we add a new dimension by also introducing the problem of domain adaptation." ></td>
	<td class="line x" title="14:410	The way this was done was by having two separate tracks: a multilingual track using essentially the same setup as last year, but with partly different languages, and a domain adaptation track, where the task was to use machine learning to adapt a parser for a single language to a new domain." ></td>
	<td class="line x" title="15:410	In total, test results were submitted for twenty-three systems in the multilingual track, and ten systems in the domain adaptation track (six of which also participated in the multilingual track)." ></td>
	<td class="line x" title="16:410	Not everyone submitted papers describing their system, and some papers describe more than one system (or the same system in both tracks), which explains why there are only ()!" ></td>
	<td class="line x" title="17:410	twenty-one papers in the proceedings." ></td>
	<td class="line x" title="18:410	In this paper, we provide task definitions for the two tracks (section 2), describe data sets extracted from available treebanks (section 3), report results for all systems in both tracks (section 4), give an overview of approaches used (section 5), provide a first analysis of the results (section 6), and conclude with some future directions (section 7)." ></td>
	<td class="line x" title="19:410	915 2 Task Definition In this section, we provide the task definitions that were used in the two tracks of the CoNLL 2007 Shard Task, the multilingual track and the domain adaptation track, together with some background and motivation for the design choices made." ></td>
	<td class="line x" title="20:410	First of all, we give a brief description of the data format and evaluation metrics, which were common to the two tracks." ></td>
	<td class="line x" title="21:410	2.1 Data Format and Evaluation Metrics The data sets derived from the original treebanks (section 3) were in the same column-based format as for the 2006 shared task (Buchholz and Marsi, 2006)." ></td>
	<td class="line x" title="22:410	In this format, sentences are separated by a blank line; a sentence consists of one or more tokens, each one starting on a new line; and a token consists of the following ten fields, separated by a single tab character: 1." ></td>
	<td class="line x" title="23:410	ID: Token counter, starting at 1 for each new sentence." ></td>
	<td class="line x" title="24:410	2." ></td>
	<td class="line x" title="25:410	FORM: Word form or punctuation symbol." ></td>
	<td class="line x" title="26:410	3." ></td>
	<td class="line x" title="27:410	LEMMA: Lemma or stem of word form, or an underscore if not available." ></td>
	<td class="line x" title="28:410	4." ></td>
	<td class="line x" title="29:410	CPOSTAG: Coarse-grained part-of-speech tag, where the tagset depends on the language." ></td>
	<td class="line x" title="30:410	5." ></td>
	<td class="line x" title="31:410	POSTAG: Fine-grained part-of-speech tag, where the tagset depends on the language, or identical to the coarse-grained part-of-speech tag if not available." ></td>
	<td class="line x" title="32:410	6." ></td>
	<td class="line x" title="33:410	FEATS: Unordered set of syntactic and/or morphological features (depending on the particular language), separated by a vertical bar (|), or an underscore if not available." ></td>
	<td class="line x" title="34:410	7." ></td>
	<td class="line x" title="35:410	HEAD: Head of the current token, which is either a value of ID or zero (0)." ></td>
	<td class="line x" title="36:410	Note that, depending on the original treebank annotation, there may be multiple tokens with HEAD=0." ></td>
	<td class="line x" title="37:410	8. DEPREL: Dependency relation to the HEAD." ></td>
	<td class="line x" title="38:410	The set of dependency relations depends on the particular language." ></td>
	<td class="line x" title="39:410	Note that, depending on the original treebank annotation, the dependency relation when HEAD=0 may be meaningful or simply ROOT." ></td>
	<td class="line x" title="40:410	9." ></td>
	<td class="line x" title="41:410	PHEAD: Projective head of current token, which is either a value of ID or zero (0), or an underscore if not available." ></td>
	<td class="line x" title="42:410	10." ></td>
	<td class="line x" title="43:410	PDEPREL: Dependency relation to the PHEAD, or an underscore if not available." ></td>
	<td class="line x" title="44:410	The PHEAD and PDEPREL were not used at all in this years data sets (i.e. , they always contained underscores) but were maintained for compatibility with last years data sets." ></td>
	<td class="line x" title="45:410	This means that, in practice, the first six columns can be considered as input to the parser, while the HEAD and DEPREL fields are the output to be produced by the parser." ></td>
	<td class="line x" title="46:410	Labeled training sets contained all ten columns; blind test sets only contained the first six columns; and gold standard test sets (released only after the end of the testperiod)againcontainedalltencolumns." ></td>
	<td class="line x" title="47:410	Alldata files were encoded in UTF-8." ></td>
	<td class="line x" title="48:410	The official evaluation metric in both tracks was the labeled attachment score (LAS), i.e., the percentage of tokens for which a system has predicted the correct HEAD and DEPREL, but results were also reported for unlabeled attachment score (UAS), i.e., the percentage of tokens with correct HEAD, and the label accuracy (LA), i.e., the percentage of tokens with correct DEPREL." ></td>
	<td class="line x" title="49:410	One important difference compared to the 2006 shared task is that all tokens were counted as scoring tokens, including in particular all punctuation tokens." ></td>
	<td class="line x" title="50:410	The official evaluation script, eval07.pl, is available from the shared task website.1 2.2 Multilingual Track The multilingual track of the shared task was organized in the same way as the 2006 task, with annotated training and test data from a wide range of languages to be processed with one and the same parsing system." ></td>
	<td class="line x" title="51:410	This system must therefore be able to learn from training data, to generalize to unseen test data, and to handle multiple languages, possibly by adjusting a number of hyper-parameters." ></td>
	<td class="line x" title="52:410	Participants in the multilingual track were expected to submit parsing results for all languages involved." ></td>
	<td class="line x" title="53:410	1http://depparse.uvt.nl/depparse-wiki/SoftwarePage 916 One of the claimed advantages of dependency parsing, as opposed to parsing based on constituent analysis, is that it extends naturally to languages with free or flexible word order." ></td>
	<td class="line x" title="54:410	This explains the interest in recent years for multilingual evaluation of dependency parsers." ></td>
	<td class="line x" title="55:410	Even before the 2006 shared task, the parsers of Collins (1997) and Charniak (2000), originally developed for English, had been adapted for dependency parsing of Czech, and the parsing methodology proposed by Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) had been evaluated on both Japanese and English." ></td>
	<td class="line x" title="56:410	The parser of McDonald and Pereira (2006) had been applied to English, Czech and Danish, and the parser of Nivre et al.(2007) to ten different languages." ></td>
	<td class="line x" title="58:410	But by far the largest evaluation of multilingual dependency parsing systems so far was the 2006 shared task, where nineteen systems were evaluatedondatafromthirteenlanguages(Buchholzand Marsi, 2006)." ></td>
	<td class="line x" title="59:410	One of the conclusions from the 2006 shared task was that parsing accuracy differed greatly between languages and that a deeper analysis of the factors involved in this variation was an important problem for future research." ></td>
	<td class="line x" title="60:410	In order to provide an extended empirical foundation for such research, we tried to select the languages and data sets for this years task based on the following desiderata:  The selection of languages should be typologically varied and include both new languages and old languages (compared to 2006)." ></td>
	<td class="line x" title="61:410	 The creation of the data sets should involve as little conversion as possible from the original treebank annotation, meaning that preference should be given to treebanks with dependency annotation." ></td>
	<td class="line x" title="62:410	 The training data sets should include at least 50,000 tokens and at most 500,000 tokens.2 The final selection included data from Arabic, Basque, Catalan, Chinese, Czech, English, Greek, Hungarian, Italian, and Turkish." ></td>
	<td class="line x" title="63:410	The treebanks from 2The reason for having an upper bound on the training set size was the fact that, in 2006, some participants could not train on all the data for some languages because of time limitations." ></td>
	<td class="line x" title="64:410	Similar considerations also led to the decision to have a smaller number of languages this year (ten, as opposed to thirteen)." ></td>
	<td class="line x" title="65:410	which the data sets were extracted are described in section 3." ></td>
	<td class="line x" title="66:410	2.3 Domain Adaptation Track One well known characteristic of data-driven parsing systems is that they typically perform much worse on data that does not come from the training domain (Gildea, 2001)." ></td>
	<td class="line x" title="67:410	Due to the large overhead in annotating text with deep syntactic parse trees, the need to adapt parsers from domains with plentiful resources (e.g. , news) to domains with little resources is an important problem." ></td>
	<td class="line x" title="68:410	This problem is commonly referred to as domain adaptation, where the goal is to adapt annotated resources from a source domain to a target domain of interest." ></td>
	<td class="line x" title="69:410	Almost all prior work on domain adaptation assumes one of two scenarios." ></td>
	<td class="line x" title="70:410	In the first scenario, there are limited annotated resources available in the target domain, and many studies have shown that this may lead to substantial improvements." ></td>
	<td class="line x" title="71:410	This includes the work of Roark and Bacchiani (2003), Florian et al.(2004), Chelba and Acero (2004), Daume and Marcu (2006), and Titov and Henderson (2006)." ></td>
	<td class="line x" title="73:410	Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing." ></td>
	<td class="line x" title="74:410	The second scenario assumes that there are no annotated resources in the target domain." ></td>
	<td class="line x" title="75:410	This is a more realistic situation and is considerably more difficult." ></td>
	<td class="line oc" title="76:410	Recent work by McClosky et al.(2006) and Blitzer et al.(2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation." ></td>
	<td class="line x" title="79:410	For this shared-task, we are assuming the latter setting  no annotated resources in the target domain." ></td>
	<td class="line x" title="80:410	Obtaining adequate annotated syntactic resources formultiplelanguagesisalreadyachallengingproblem, whichisonlyexacerbatedwhentheseresources must be drawn from multiple and diverse domains." ></td>
	<td class="line x" title="81:410	As a result, the only language that could be feasibly tested in the domain adaptation track was English." ></td>
	<td class="line x" title="82:410	The setup for the domain adaptation track was as follows." ></td>
	<td class="line x" title="83:410	Participants were provided with a large annotated corpus from the source domain, in this case sentences from the Wall Street Journal." ></td>
	<td class="line x" title="84:410	Participants were also provided with data from three different target domains: biomedical abstracts (development data), chemical abstracts (test data 1), and parentchild dialogues (test data 2)." ></td>
	<td class="line x" title="85:410	Additionally, a large 917 unlabeled corpus for each data set (training, development, test) was provided." ></td>
	<td class="line x" title="86:410	The goal of the task was to use the annotated source data, plus any unlabeled data, to produce a parser that is accurate for each of the test sets from the target domains.3 Participants could submit systems in either the open or closed class (or both)." ></td>
	<td class="line x" title="87:410	The closed class requires a system to use only those resources provided as part of the shared task." ></td>
	<td class="line x" title="88:410	The open class allows a system to use additional resources provided those resources are not drawn from the same domain as the development or test sets." ></td>
	<td class="line x" title="89:410	An example might be a part-of-speech tagger trained on the entire Penn Treebank and not just the subset provided as training data, or a parser that has been hand-crafted or trained on a different training set." ></td>
	<td class="line x" title="90:410	3 Treebanks In this section, we describe the treebanks used in the shared task and give relevant information about the data sets created from them." ></td>
	<td class="line x" title="91:410	3.1 Multilingual Track Arabic The analytical syntactic annotation of the Prague Arabic Dependency Treebank (PADT) (Hajic et al. , 2004) can be considered a pure dependency annotation." ></td>
	<td class="line x" title="92:410	The conversion, done by Otakar Smrz, from the original format to the column-based format described in section 2.1 was therefore relatively straightforward, although not all the information in the original annotation could be transfered to the new format." ></td>
	<td class="line x" title="93:410	PADT was one of the treebanks used in the 2006 shared task but then only contained about 54,000 tokens." ></td>
	<td class="line x" title="94:410	Since then, the size of the treebank has more than doubled, with around 112,000 tokens." ></td>
	<td class="line x" title="95:410	In addition, the morphological annotation has been made more informative." ></td>
	<td class="line x" title="96:410	It is also worth noting that the parsing units in this treebank are in many cases larger than conventional sentences, which partly explains the high average number of tokens per sentence (Buchholz and Marsi, 2006)." ></td>
	<td class="line x" title="97:410	3Note that annotated development data for the target domain was only provided for the development domain, biomedical abstracts." ></td>
	<td class="line x" title="98:410	For the two test domains, chemical abstracts and parentchild dialogues, the only annotated data sets were the gold standard test sets, released only after test runs had been submitted." ></td>
	<td class="line x" title="99:410	Basque For Basque, we used the 3LB Basque treebank (Aduriz et al. , 2003)." ></td>
	<td class="line x" title="100:410	At present, the treebank consists of approximately 3,700 sentences, 334 of which were used as test data." ></td>
	<td class="line x" title="101:410	The treebank comprises literary and newspaper texts." ></td>
	<td class="line x" title="102:410	It is annotated in a dependency format and was converted to the CoNLL format by a team led by Koldo Gojenola." ></td>
	<td class="line x" title="103:410	Catalan The Catalan section of the CESS-ECE Syntactically and Semantically Annotated Corpora (Mart et al. , 2007) is annotated with, among other things, constituent structure and grammatical functions." ></td>
	<td class="line x" title="104:410	A head percolation table was used for automatically converting the constituent trees into dependency trees." ></td>
	<td class="line x" title="105:410	The original data only contains functions related to the verb, and a function table was used for deriving the remaining syntactic functions." ></td>
	<td class="line x" title="106:410	The conversion was performed by a team led by Llus M`arquez and Ant`onia Mart." ></td>
	<td class="line x" title="107:410	Chinese The Chinese data are taken from the Sinica treebank (Chen et al. , 2003), which contains both syntactic functions and semantic functions." ></td>
	<td class="line x" title="108:410	The syntactic head was used in the conversion to the CoNLL format, carried out by Yu-Ming Hsieh and the organizers of the 2006 shared task, and the syntactic functions were used wherever it was possible." ></td>
	<td class="line x" title="109:410	The training data used is basically the same as for the 2006 shared task, except for a few corrections, but the test data is new for this years shared task." ></td>
	<td class="line x" title="110:410	It is worth noting that the parsing units in this treebank are sometimes smaller than conventional sentence units, which partly explains the low average number of tokens per sentence (Buchholz and Marsi, 2006)." ></td>
	<td class="line x" title="111:410	Czech The analytical syntactic annotation of the Prague Dependency Treebank (PDT) (Bohmova et al. , 2003) is a pure dependency annotation, just as for PADT." ></td>
	<td class="line x" title="112:410	It was also used in the shared task 2006, but there are two important changes compared to last year." ></td>
	<td class="line x" title="113:410	First, version 2.0 of PDT was used instead of version 1.0, and a conversion script was created by Zdenek Zabokrtsky, using the new XMLbased format of PDT 2.0." ></td>
	<td class="line x" title="114:410	Secondly, due to the upper bound on training set size, only sections 13 of PDT constitute the training data, which amounts to some 450,000 tokens." ></td>
	<td class="line x" title="115:410	The test data is a small subset of the development test set of PDT." ></td>
	<td class="line x" title="116:410	918 English For English we used the Wall Street Journal section of the Penn Treebank (Marcus et al. , 1993)." ></td>
	<td class="line x" title="117:410	In particular, we used sections 2-11 for training and a subset of section 23 for testing." ></td>
	<td class="line x" title="118:410	As a preprocessing stage we removed many functions tags from the non-terminals in the phrase structure representation to make the representations more uniform with out-of-domain test sets for the domain adaptation track (see section 3.2)." ></td>
	<td class="line x" title="119:410	The resulting data set was then converted to dependency structures using the procedure described in Johansson and Nugues (2007a)." ></td>
	<td class="line x" title="120:410	This work was done by Ryan McDonald." ></td>
	<td class="line x" title="121:410	Greek The Greek Dependency Treebank (GDT) (Prokopidis et al. , 2005) adopts a dependency structure annotation very similar to those of PDT and PADT, which means that the conversion by Prokopis Prokopidis was relatively straightforward." ></td>
	<td class="line x" title="122:410	GDT is one of the smallest treebanks in this years shared task (about 65,000 tokens) and contains sentences of Modern Greek." ></td>
	<td class="line x" title="123:410	Just like PDT and PADT, the treebank contains more than one level of annotation, but we only used the analytical level of GDT." ></td>
	<td class="line x" title="124:410	Hungarian For the Hungarian data, the Szeged treebank (Csendes et al. , 2005) was used." ></td>
	<td class="line x" title="125:410	The treebank is based on texts from six different genres, ranging from legal newspaper texts to fiction." ></td>
	<td class="line x" title="126:410	The original annotation scheme is constituent-based, following generative principles." ></td>
	<td class="line x" title="127:410	It was converted into dependencies by Zoltan Alexin based on heuristics." ></td>
	<td class="line x" title="128:410	Italian The data set used for Italian is a subset of the balanced section of the Italian SyntacticSemantic Treebank (ISST) (Montemagni et al. , 2003) and consists of texts from the newspaper Corriere della Sera and from periodicals." ></td>
	<td class="line x" title="129:410	A team led by Giuseppe Attardi, Simonetta Montemagni, and Maria Simi converted the annotation to the CoNLL format, using information from two different annotation levels, the constituent structure level and the dependency structure level." ></td>
	<td class="line x" title="130:410	Turkish For Turkish we used the METU-Sabanc Turkish Treebank (Oflazer et al. , 2003), which was also used in the 2006 shared task." ></td>
	<td class="line x" title="131:410	A new test set of about 9,000 tokens was provided by Gulsen Eryigit (Eryigit, 2007), who also handled the conversion to the CoNLL format, which means that we could use all the approximately 65,000 tokens of the original treebank for training." ></td>
	<td class="line x" title="132:410	The rich morphology of Turkish requires the basic tokens in parsing to be inflectionalgroups(IGs)ratherthanwords." ></td>
	<td class="line x" title="133:410	IGsofasingle word are connected to each other deterministically using dependency links labeled DERIV, referred to as word-internal dependencies in the following, and the FORM and the LEMMA fields may be empty (they contain underscore characters in the data files)." ></td>
	<td class="line x" title="134:410	Sentences do not necessarily have a unique root; most internal punctuation and a few foreign words also have HEAD=0." ></td>
	<td class="line x" title="135:410	3.2 Domain Adaptation Track As mentioned previously, the source data is drawn from a corpus of news, specifically the Wall Street Journal section of the Penn Treebank (Marcus et al. , 1993)." ></td>
	<td class="line x" title="136:410	This data set is identical to the English training set from the multilingual track (see section 3.1)." ></td>
	<td class="line x" title="137:410	For the target domains we used three different labeled data sets." ></td>
	<td class="line x" title="138:410	The first two were annotated as part of the PennBioIE project (Kulick et al. , 2004) and consist of sentences drawn from either biomedical or chemical research abstracts." ></td>
	<td class="line x" title="139:410	Like the source WSJ corpus, this data is annotated using the Penn Treebank phrase structure scheme." ></td>
	<td class="line x" title="140:410	To convert these sets to dependency structures we used the same procedure as before (Johansson and Nugues, 2007a)." ></td>
	<td class="line x" title="141:410	Additional care was taken to remove sentences that contained non-WSJ part-of-speech tags or non-terminals (e.g. , HYPH part-of-speech tag indicating a hyphen)." ></td>
	<td class="line x" title="142:410	Furthermore, the annotation schemeforgapsandtraceswasmadeconsistentwith the Penn Treebank wherever possible." ></td>
	<td class="line x" title="143:410	As already mentioned, the biomedical data set was distributed as a development set for the training phase, while the chemical data set was only used for final testing." ></td>
	<td class="line x" title="144:410	The third target data set was taken from the CHILDES database (MacWhinney, 2000), in particular the EVE corpus (Brown, 1973), which has been annotated with dependency structures." ></td>
	<td class="line x" title="145:410	Unfortunately the dependency labels of the CHILDES data were inconsistent with those of the WSJ, biomedical and chemical data sets, and we therefore opted to only evaluate unlabeled accuracy for this data set." ></td>
	<td class="line x" title="146:410	Furthermore, there was an inconsistency in how mainandauxiliaryverbswereannotatedforthisdata set relative to others." ></td>
	<td class="line x" title="147:410	As a result of this, submitting 919 Multilingual Domain adaptation Ar Ba Ca Ch Cz En Gr Hu It Tu PCHEM CHILDES Language family Sem." ></td>
	<td class="line x" title="148:410	Isol." ></td>
	<td class="line x" title="149:410	Rom." ></td>
	<td class="line x" title="150:410	Sin." ></td>
	<td class="line x" title="151:410	Sla." ></td>
	<td class="line x" title="152:410	Ger." ></td>
	<td class="line x" title="153:410	Hel." ></td>
	<td class="line x" title="154:410	F.-U." ></td>
	<td class="line x" title="155:410	Rom." ></td>
	<td class="line x" title="156:410	Tur." ></td>
	<td class="line x" title="157:410	Ger." ></td>
	<td class="line x" title="158:410	Annotation d d c+f c+f d c+f d c+f c+f d c+f d Training data Development data Tokens (k) 112 51 431 337 432 447 65 132 71 65 5 Sentences (k) 2.9 3.2 15.0 57.0 25.4 18.6 2.7 6.0 3.1 5.6 0.2 Tokens/sentence 38.3 15.8 28.8 5.9 17.0 24.0 24.2 21.8 22.9 11.6 25.1 LEMMA Yes Yes Yes No Yes No Yes Yes Yes Yes No No." ></td>
	<td class="line x" title="159:410	CPOSTAG 15 25 17 13 12 31 18 16 14 14 25 No." ></td>
	<td class="line x" title="160:410	POSTAG 21 64 54 294 59 45 38 43 28 31 37 No." ></td>
	<td class="line x" title="161:410	FEATS 21 359 33 0 71 0 31 50 21 78 0 No." ></td>
	<td class="line x" title="162:410	DEPREL 29 35 42 69 46 20 46 49 22 25 18 No." ></td>
	<td class="line x" title="163:410	DEPREL H=0 18 17 1 1 8 1 22 1 1 1 1 % HEAD=0 8.7 9.7 3.5 16.9 11.6 4.2 8.3 4.6 5.4 12.8 4.0 % HEAD left 79.2 44.5 60.0 24.7 46.9 49.0 44.8 27.4 65.0 3.8 50.0 % HEAD right 12.1 45.8 36.5 58.4 41.5 46.9 46.9 68.0 29.6 83.4 46.0 HEAD=0/sentence 3.3 1.5 1.0 1.0 2.0 1.0 2.0 1.0 1.2 1.5 1.0 % Non-proj." ></td>
	<td class="line x" title="164:410	arcs 0.4 2.9 0.1 0.0 1.9 0.3 1.1 2.9 0.5 5.5 0.4 % Non-proj." ></td>
	<td class="line x" title="165:410	sent." ></td>
	<td class="line x" title="166:410	10.1 26.2 2.9 0.0 23.2 6.7 20.3 26.4 7.4 33.3 8.0 Punc." ></td>
	<td class="line x" title="167:410	attached S S A S S A S A A S A DEPRELS for punc." ></td>
	<td class="line x" title="168:410	10 13 6 29 16 13 15 1 10 12 8 Test data PCHEM CHILDES Tokens 5124 5390 5016 5161 4724 5003 4804 7344 5096 4513 5001 4999 Sentences 131 334 167 690 286 214 197 390 249 300 195 666 Tokens/sentence 39.1 16.1 30.0 7.5 16.5 23.4 24.4 18.8 20.5 15.0 25.6 12.9 % New words 12.44 24.98 4.35 9.70 12.58 3.13 12.43 26.10 15.07 36.29 31.33 6.10 % New lemmas 2.82 11.13 3.36 n/a 5.28 n/a 5.82 14.80 8.24 9.95 n/a n/a Table 1: Characteristics of the data sets for the 10 languages of the multilingual track and the development set and the two test sets of the domain adaptation track." ></td>
	<td class="line x" title="169:410	920 results for the CHILDES data was considered optional." ></td>
	<td class="line x" title="170:410	Like the chemical data set, this data set was only used for final testing." ></td>
	<td class="line x" title="171:410	Finally, a large corpus of unlabeled in-domain data was provided for each data set and made availablefortraining." ></td>
	<td class="line x" title="172:410	ThisdatawasdrawnfromtheWSJ, PubMed.com (specific to biomedical and chemical research literature), and the CHILDES data base." ></td>
	<td class="line x" title="173:410	The data was tokenized to be as consistent as possible with the WSJ training set." ></td>
	<td class="line x" title="174:410	3.3 Overview Table 1 describes the characteristics of the data sets." ></td>
	<td class="line x" title="175:410	For the multilingual track, we provide statistics over the training and test sets; for the domain adaptation track, the statistics were extracted from the development set." ></td>
	<td class="line x" title="176:410	Following last years shared task practice (Buchholz and Marsi, 2006), we use the following definition of projectivity: An arc (i, j) is projective iff all nodes occurring between i and j are dominated by i (where dominates is the transitive closure of the arc relation)." ></td>
	<td class="line x" title="177:410	In the table, the languages are abbreviated to their first two letters." ></td>
	<td class="line x" title="178:410	Language families are: Semitic, Isolate, Romance, Sino-Tibetan, Slavic, Germanic, Hellenic, Finno-Ugric, and Turkic." ></td>
	<td class="line x" title="179:410	The type of the original annotation is either constituents plus (some) functions (c+f) or dependencies (d)." ></td>
	<td class="line x" title="180:410	For the training data, the number of words and sentences are given in multiples of thousands, and the average length of a sentence in words (including punctuation tokens)." ></td>
	<td class="line x" title="181:410	The following rows contain information about whether lemmas are available, the number of coarseand fine-grained part-of-speech tags, the number of feature components, and the number of dependency labels." ></td>
	<td class="line x" title="182:410	Then information is given on how many different dependency labels can co-occur with HEAD=0, the percentage of HEAD=0 dependencies, and the percentage of heads preceding (left) or succeeding (right) a token (giving an indication of whether a language is predominantly head-initial or head-final)." ></td>
	<td class="line x" title="183:410	This is followed by the average number of HEAD=0 dependencies per sentence and the percentage of non-projective arcs and sentences." ></td>
	<td class="line x" title="184:410	The last two rows show whether punctuation tokens are attached as dependents of other tokens (A=Always, S=Sometimes) and specify the number of dependency labels that exist for punctuation tokens." ></td>
	<td class="line x" title="185:410	Note that punctuation is defined as any token belonging to the UTF-8 category of punctuation." ></td>
	<td class="line x" title="186:410	This means, for example, that any token having an underscore in the FORM field (which happens for word-internal IGs in Turkish) is also counted as punctuation here." ></td>
	<td class="line x" title="187:410	For the test sets, the number of words and sentences as well as the ratio of words per sentence are listed, followed by the percentage of new words and lemmas (if applicable)." ></td>
	<td class="line x" title="188:410	For the domain adaptation sets, the percentage of new words is computed with regard to the training set (Penn Treebank)." ></td>
	<td class="line x" title="189:410	4 Submissions and Results As already stated in the introduction, test runs were submitted for twenty-three systems in the multilingual track, and ten systems in the domain adaptation track (six of which also participated in the multilingual track)." ></td>
	<td class="line x" title="190:410	In the result tables below, systems are identifiedbythelastnameoftheteammemberlisted first when test runs were uploaded for evaluation." ></td>
	<td class="line x" title="191:410	In general, this name is also the first author of a paper describing the system in the proceedings, but there are a few exceptions and complications." ></td>
	<td class="line x" title="192:410	First of all, for four out of twenty-seven systems, no paper was submitted to the proceedings." ></td>
	<td class="line x" title="193:410	This is the case for the systems of Jia, Maes et al. , Nash, and Zeman, which is indicated by the fact that these names appear in italics in all result tables." ></td>
	<td class="line x" title="194:410	Secondly, two teams submitted two systems each, which are described in a single paper by each team." ></td>
	<td class="line x" title="195:410	Thus, the systems called Nilsson and Hall, J. are both described in Hall et al.(2007a), while the systems called Duan (1) and Duan (2) are both described in Duan et al.(2007)." ></td>
	<td class="line x" title="198:410	Finally, please pay attention to the fact that there are two teams, where the first authors last name is Hall." ></td>
	<td class="line x" title="199:410	Therefore, we use Hall, J. and Hall, K., to disambiguate between the teams involving Johan Hall (Hall et al. , 2007a) and Keith Hall (Hall et al. , 2007b), respectively." ></td>
	<td class="line x" title="200:410	Tables 2 and 3 give the scores for the multilingual track in the CoNLL 2007 shared task." ></td>
	<td class="line x" title="201:410	The Average column contains the average score for all ten languages, which determines the ranking in this track." ></td>
	<td class="line x" title="202:410	Table 4 presents the results for the domain adaptation track, where the ranking is determined based on the PCHEM results only, since the CHILDES data set was optional." ></td>
	<td class="line x" title="203:410	Note also that there are no labeled 921 Team Average Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian Turkish Nilsson 80.32(1) 76.52(1) 76.94(1) 88.70(1) 75.82(15) 77.98(3) 88.11(5) 74.65(2) 80.27(1) 84.40(1) 79.79(2) Nakagawa 80.29(2) 75.08(2) 72.56(7) 87.90(3) 83.84(2) 80.19(1) 88.41(3) 76.31(1) 76.74(8) 83.61(3) 78.22(5) Titov 79.90(3) 74.12(6) 75.49(3) 87.40(6) 82.14(7) 77.94(4) 88.39(4) 73.52(10) 77.94(4) 82.26(6) 79.81(1) Sagae 79.90(4) 74.71(4) 74.64(6) 88.16(2) 84.69(1) 74.83(8) 89.01(2) 73.58(8) 79.53(2) 83.91(2) 75.91(10) Hall, J. 79.80(5)* 74.75(3) 74.99(5) 87.74(4) 83.51(3) 77.22(6) 85.81(12) 74.21(6) 78.09(3) 82.48(5) 79.24(3) Carreras 79.09(6)* 70.20(11) 75.75(2) 87.60(5) 80.86(10) 78.60(2) 89.61(1) 73.56(9) 75.42(9) 83.46(4) 75.85(11) Attardi 78.27(7) 72.66(8) 69.48(12) 86.86(7) 81.50(8) 77.37(5) 85.85(10) 73.92(7) 76.81(7) 81.34(8) 76.87(7) Chen 78.06(8) 74.65(5) 72.39(8) 86.66(8) 81.24(9) 73.69(10) 83.81(13) 74.42(3) 75.34(10) 82.04(7) 76.31(9) Duan (1) 77.70(9)* 69.91(13) 71.26(9) 84.95(10) 82.58(6) 75.34(7) 85.83(11) 74.29(4) 77.06(5) 80.75(9) 75.03(12) Hall, K. 76.91(10)* 73.40(7) 69.81(11) 82.38(14) 82.77(4) 72.27(12) 81.93(15) 74.21(5) 74.20(11) 80.69(10) 77.42(6) Schiehlen 76.18(11) 70.08(12) 66.77(14) 85.75(9) 80.04(11) 73.86(9) 86.21(9) 72.29(12) 73.90(12) 80.46(11) 72.48(15) Johansson 75.78(12)* 71.76(9) 75.08(4) 83.33(12) 76.30(14) 70.98(13) 80.29(17) 72.77(11) 71.31(13) 77.55(14) 78.46(4) Mannem 74.54(13)* 71.55(10) 65.64(15) 84.47(11) 73.76(17) 70.68(14) 81.55(16) 71.69(13) 70.94(14) 78.67(13) 76.42(8) Wu 73.02(14)* 66.16(14) 70.71(10) 81.44(15) 74.69(16) 66.72(16) 79.49(18) 70.63(14) 69.08(15) 78.79(12) 72.52(14) Nguyen 72.53(15)* 63.58(16) 58.18(17) 83.23(13) 79.77(12) 72.54(11) 86.73(6) 70.42(15) 68.12(17) 75.06(16) 67.63(17) Maes 70.66(16)* 65.12(15) 69.05(13) 79.21(16) 70.97(18) 67.38(15) 69.68(21) 68.59(16) 68.93(16) 73.63(18) 74.03(13) Canisius 66.99(17)* 59.13(18) 63.17(16) 75.44(17) 70.45(19) 56.14(17) 77.27(19) 60.35(18) 64.31(19) 75.57(15) 68.09(16) Jia 63.00(18)* 63.37(17) 57.61(18) 23.35(20) 76.36(13) 54.95(18) 82.93(14) 65.45(17) 66.61(18) 74.65(17) 64.68(18) Zeman 54.87(19) 46.06(20) 50.61(20) 62.94(19) 54.49(20) 50.21(20) 53.59(22) 55.29(19) 55.24(20) 62.13(19) 58.10(19) Marinov 54.55(20)* 54.00(19) 51.24(19) 69.42(18) 49.87(21) 53.47(19) 52.11(23) 54.33(20) 44.47(21) 59.75(20) 56.88(20) Duan (2) 24.62(21)* 82.64(5) 86.69(7) 76.89(6) Nash 8.65(22)* 86.49(8) Shimizu 7.20(23) 72.02(20) Table 2: Labeled attachment score (LAS) for the multilingual track in the CoNLL 2007 shared task." ></td>
	<td class="line x" title="204:410	Teams are denoted by the last name of their first member, with italics indicating that there is no corresponding paper in the proceedings." ></td>
	<td class="line x" title="205:410	The number in parentheses next to each score gives the rank." ></td>
	<td class="line x" title="206:410	A star next to a score in the Average column indicates a statistically significant difference with the next lower rank." ></td>
	<td class="line x" title="207:410	Team Average Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian Turkish Nakagawa 86.55(1)* 86.09(1) 81.04(5) 92.86(4) 88.88(2) 86.28(1) 90.13(2) 84.08(1) 82.49(3) 87.91(1) 85.77(3) Nilsson 85.71(2) 85.81(2) 82.84(1) 93.12(3) 84.52(12) 83.59(4) 88.93(5) 81.22(4) 83.55(1) 87.77(2) 85.77(2) Titov 85.62(3) 83.18(7) 81.93(2) 93.40(1) 87.91(4) 84.19(3) 89.73(4) 81.20(5) 82.18(4) 86.26(6) 86.22(1) Sagae 85.29(4)* 84.04(4) 81.19(3) 93.34(2) 88.94(1) 81.27(8) 89.87(3) 80.37(11) 83.51(2) 87.68(3) 82.72(9) Carreras 84.79(5) 81.48(10) 81.11(4) 92.46(5) 86.20(9) 85.16(2) 90.63(1) 81.37(3) 79.92(9) 87.19(4) 82.41(10) Hall, J. 84.74(6)* 84.21(3) 80.61(6) 92.20(6) 87.60(5) 82.35(6) 86.77(12) 80.66(9) 81.71(6) 86.26(5) 85.04(5) Attardi 83.96(7)* 82.53(8) 76.88(11) 91.41(7) 86.73(8) 83.40(5) 86.99(10) 80.75(8) 81.81(5) 85.54(8) 83.56(7) Chen 83.22(8) 83.49(5) 78.65(8) 90.87(8) 85.91(10) 80.14(11) 84.91(13) 81.16(6) 79.25(11) 85.91(7) 81.92(12) Hall, K. 83.08(9) 83.45(6) 78.55(9) 87.80(15) 87.91(3) 78.47(12) 83.21(15) 82.04(2) 79.34(10) 84.81(9) 85.18(4) Duan (1) 82.77(10) 79.04(13) 77.59(10) 89.71(12) 86.88(7) 80.82(10) 86.97(11) 80.77(7) 80.66(7) 84.20(11) 81.03(13) Schiehlen 82.42(11)* 81.07(11) 73.30(14) 90.79(10) 85.45(11) 81.73(7) 88.91(6) 80.47(10) 78.61(12) 84.54(10) 79.33(15) Johansson 81.13(12)* 80.91(12) 80.43(7) 88.34(13) 81.30(15) 77.39(13) 81.43(18) 79.58(12) 75.53(15) 81.55(15) 84.80(6) Mannem 80.30(13) 81.56(9) 72.88(15) 89.81(11) 78.84(17) 77.20(14) 82.81(16) 78.89(13) 75.39(16) 82.91(12) 82.74(8) Nguyen 80.00(14)* 73.46(18) 69.15(18) 88.12(14) 84.05(13) 80.91(9) 88.01(7) 77.56(15) 78.13(13) 80.40(16) 80.19(14) Jia 78.46(15) 74.20(17) 70.24(16) 90.83(9) 83.39(14) 70.41(18) 84.37(14) 75.65(16) 77.19(14) 82.36(14) 75.96(17) Wu 78.44(16)* 77.05(14) 75.77(12) 85.85(16) 79.71(16) 73.07(16) 81.69(17) 78.12(14) 72.39(18) 82.57(13) 78.15(16) Maes 76.60(17)* 75.47(16) 75.27(13) 84.35(17) 76.57(18) 74.03(15) 71.62(21) 75.19(17) 72.93(17) 78.32(18) 82.21(11) Canisius 74.83(18)* 76.89(15) 70.17(17) 81.64(18) 74.81(19) 72.12(17) 78.23(19) 72.46(18) 67.80(19) 79.08(17) 75.14(18) Zeman 62.02(19)* 58.55(20) 57.42(20) 68.50(20) 62.93(20) 59.19(20) 58.33(22) 62.89(19) 59.78(20) 68.27(19) 64.30(19) Marinov 60.83(20)* 64.27(19) 58.55(19) 74.22(19) 56.09(21) 59.57(19) 54.33(23) 61.18(20) 50.39(21) 65.52(20) 64.13(20) Duan (2) 25.53(21)* 86.94(6) 87.87(8) 80.53(8) Nash 8.77(22)* 87.71(9) Shimizu 7.79(23) 77.91(20) Table 3: Unlabeled attachment scores (UAS) for the multilingual track in the CoNLL 2007 shared task." ></td>
	<td class="line x" title="208:410	Teams are denoted by the last name of their first member, with italics indicating that there is no corresponding paper in the proceedings." ></td>
	<td class="line x" title="209:410	The number in parentheses next to each score gives the rank." ></td>
	<td class="line x" title="210:410	A star next to a score in the Average column indicates a statistically significant difference with the next lower rank." ></td>
	<td class="line x" title="211:410	922 LAS UAS Team PCHEM-c PCHEM-o PCHEM-c PCHEM-o CHILDES-c CHILDES-o Sagae 81.06(1) 83.42(1) Attardi 80.40(2) 83.08(3) 58.67(3) Dredze 80.22(3) 83.38(2) 61.37(1) Nguyen 79.50(4)* 82.04(4)* Jia 76.48(5)* 78.92(5)* 57.43(5) Bick 71.81(6)* 78.48(1)* 74.71(6)* 81.62(1)* 58.07(4) 62.49(1) Shimizu 64.15(7)* 63.49(2) 71.25(7)* 70.01(2)* Zeman 50.61(8) 54.57(8) 58.89(2) Schneider 63.01(3)* 66.53(3)* 60.27(2) Watson 55.47(4) 62.79(4) 45.61(3) Wu 52.89(6) Table 4: Labeled (LAS) and unlabeled (UAS) attachment scores for the closed (-c) and open (-o) classes of the domain adaptation track in the CoNLL 2007 shared task." ></td>
	<td class="line x" title="212:410	Teams are denoted by the last name of their first member, with italics indicating that there is no corresponding paper in the proceedings." ></td>
	<td class="line x" title="213:410	The number in parentheses next to each score gives the rank." ></td>
	<td class="line x" title="214:410	A star next to a score in the PCHEM columns indicates a statistically significant difference with the next lower rank." ></td>
	<td class="line x" title="215:410	attachmentscoresfortheCHILDESdataset, forreasons explained in section 3.2." ></td>
	<td class="line x" title="216:410	The number in parentheses next to each score gives the rank." ></td>
	<td class="line x" title="217:410	A star next to a score indicates that the difference with the next lower rank is significant at the 5% level using a ztest for proportions." ></td>
	<td class="line x" title="218:410	A more complete presentation of the results, including the significance results for all the tasks and their p-values, can be found on the shared task website.4 Looking first at the results in the multilingual track, we note that there are a number of systems performing at almost the same level at the top of the ranking." ></td>
	<td class="line x" title="219:410	For the average labeled attachment score, the difference between the top score (Nilsson) and the fifth score (Hall, J)." ></td>
	<td class="line x" title="220:410	is no more than half a percentage point, and there are generally very few significant differences among the five or six best systems, regardless of whether we consider labeled or unlabeled attachment score." ></td>
	<td class="line x" title="221:410	For the closed class of the domain adaptation track, we see a very similar pattern, with the top system (Sagae) being followed very closely by two other systems." ></td>
	<td class="line x" title="222:410	For the open class, the results are more spread out, but then there are very few results in this class." ></td>
	<td class="line x" title="223:410	It is also worth noting that the top scores in the closed class, somewhat unexpectedly, are higher than the top scores in the 4http://nextens.uvt.nl/depparse-wiki/AllScores openclass." ></td>
	<td class="line x" title="224:410	Butbeforeweproceedtoamoredetailed analysis of the results (section 6), we will make an attempt to characterize the approaches represented by the different systems." ></td>
	<td class="line x" title="225:410	5 Approaches In this section we give an overview of the models, inference methods, and learning methods used in the participating systems." ></td>
	<td class="line x" title="226:410	For obvious reasons the discussion is limited to systems that are described by a paper in the proceedings." ></td>
	<td class="line x" title="227:410	But instead of describing the systems one by one, we focus on the basic methodological building blocks that are often found in several systems although in different combinations." ></td>
	<td class="line x" title="228:410	For descriptions of the individual systems, we refer to the respective papers in the proceedings." ></td>
	<td class="line x" title="229:410	Section 5.1 is devoted to system architectures." ></td>
	<td class="line x" title="230:410	We then describe the two main paradigms for learning and inference, in this years shared task as well as in last years, which we call transition-based parsers (section 5.2) and graph-based parsers (section 5.3), adopting the terminology of McDonald and Nivre (2007).5 Finally, we give an overview of the domain adaptation methods that were used (section 5.4)." ></td>
	<td class="line x" title="231:410	5This distinction roughly corresponds to the distinction made by Buchholz and Marsi (2006) between stepwise and all-pairs approaches." ></td>
	<td class="line x" title="232:410	923 5.1 Architectures Most systems perform some amount of preand post-processing, making the actual parsing component part of a sequential workflow of varying length and complexity." ></td>
	<td class="line x" title="233:410	For example, most transitionbased parsers can only build projective dependency graphs." ></td>
	<td class="line x" title="234:410	For languages with non-projective dependencies, graphs therefore need to be projectivized for training and deprojectivized for testing (Hall et al. , 2007a; Johansson and Nugues, 2007b; Titov and Henderson, 2007)." ></td>
	<td class="line x" title="235:410	Instead of assigning HEAD and DEPREL in a single step, some systems use a two-stage approach for attaching and labeling dependencies (Chen et al. , 2007; Dredze et al. , 2007)." ></td>
	<td class="line x" title="236:410	In the first step unlabeled dependencies are generated, in the second step these are labeled." ></td>
	<td class="line x" title="237:410	This is particularly helpful for factored parsing models, in which label decisions cannot be easily conditioned on larger parts of the structure due to the increased complexity of inference." ></td>
	<td class="line x" title="238:410	One system(Halletal., 2007b)extendsthistwo-stageapproach to a three-stage architecture where the parser and labeler generate ann-best list of parses which in turn is reranked.6 In ensemble-based systems several base parsers provide parsing decisions, which are added together for a combined score for each potential dependency arc. The tree that maximizes the sum of these combined scores is taken as the final output parse." ></td>
	<td class="line x" title="240:410	This technique is used by Sagae and Tsujii (2007) and in the Nilsson system (Hall et al. , 2007a)." ></td>
	<td class="line x" title="241:410	It is worth noting that both these systems combine transitionbased base parsers with a graph-based method for parser combination, as first described by Sagae and Lavie (2006)." ></td>
	<td class="line x" title="242:410	Data-driven grammar-based parsers, such as Bick (2007), Schneider et al.(2007), and Watson and Briscoe (2007), need preand post-processing in order to map the dependency graphs provided as training data to a format compatible with the grammar used, and vice versa." ></td>
	<td class="line x" title="244:410	5.2 Transition-Based Parsers Transition-based parsers build dependency graphs by performing sequences of actions, or transitions." ></td>
	<td class="line x" title="245:410	Both learning and inference is conceptualized in 6They also flip the order of the labeler and the reranker." ></td>
	<td class="line x" title="246:410	terms of predicting the correct transition based on the current parser state and/or history." ></td>
	<td class="line x" title="247:410	We can further subclassify parsers with respect to the model (or transition system) they adopt, the inference method they use, and the learning method they employ." ></td>
	<td class="line x" title="248:410	5.2.1 Models The most common model for transition-based parsers is one inspired by shift-reduce parsing, where a parser state contains a stack of partially processed tokens and a queue of remaining input tokens, and where transitions add dependency arcs and perform stack and queue operations." ></td>
	<td class="line x" title="249:410	This type of model is used by the majority of transition-based parsers (Attardi et al. , 2007; Duan et al. , 2007; Hall et al. , 2007a; Johansson and Nugues, 2007b; Mannem, 2007; Titov and Henderson, 2007; Wu et al. , 2007)." ></td>
	<td class="line x" title="250:410	Sometimes it is combined with an explicit probability model for transition sequences, which may be conditional (Duan et al. , 2007) or generative (Titov and Henderson, 2007)." ></td>
	<td class="line x" title="251:410	An alternative model is based on the list-based parsing algorithm described by Covington (2001), which iterates over the input tokens in a sequential manner and evaluates for each preceding token whether it can be linked to the current token or not." ></td>
	<td class="line x" title="252:410	This model is used by Marinov (2007) and in component parsers of the Nilsson ensemble system (Hall etal., 2007a)." ></td>
	<td class="line x" title="254:410	Finally, twosystemsusemodelsbased on LR parsing (Sagae and Tsujii, 2007; Watson and Briscoe, 2007)." ></td>
	<td class="line x" title="255:410	5.2.2 Inference The most common inference technique in transitionbased dependency parsing is greedy deterministic search, guided by a classifier for predicting the next transition given the current parser state and history, processing the tokens of the sentence in sequential left-to-right order7 (Hall et al. , 2007a; Mannem, 2007; Marinov, 2007; Wu et al. , 2007)." ></td>
	<td class="line x" title="256:410	Optionally multiple passes over the input are conducted until no tokens are left unattached (Attardi et al. , 2007)." ></td>
	<td class="line x" title="257:410	As an alternative to deterministic parsing, several parsers use probabilistic models and maintain a heap or beam of partial transition sequences in order to pick the most probable one at the end of the sentence 7For diversity in parser ensembles, right-to-left parsers are also used." ></td>
	<td class="line x" title="258:410	924 (Duan et al. , 2007; Johansson and Nugues, 2007b; SagaeandTsujii, 2007; TitovandHenderson, 2007)." ></td>
	<td class="line x" title="259:410	One system uses as part of their parsing pipeline a neighbor-parser that attaches adjacent words and a root-parser that identifies the root word(s) of a sentence (Wu et al. , 2007)." ></td>
	<td class="line x" title="260:410	In the case of grammarbased parsers, a classifier is used to disambiguate in cases where the grammar leaves some ambiguity (Schneider et al. , 2007; Watson and Briscoe, 2007) 5.2.3 Learning Transition-based parsers either maintain a classifier that predicts the next transition or a global probabilistic model that scores a complete parse." ></td>
	<td class="line x" title="261:410	To train these classifiers and probabilitistic models several approaches were used: SVMs (Duan et al. , 2007; Hall et al. , 2007a; Sagae and Tsujii, 2007), modified finite Newton SVMs (Wu et al. , 2007), maximum entropy models (Sagae and Tsujii, 2007), multiclass averaged perceptron (Attardi et al. , 2007) and maximum likelihood estimation (Watson and Briscoe, 2007)." ></td>
	<td class="line x" title="262:410	In order to calculate a global score or probability for a transition sequence, two systems used a Markov chain approach (Duan et al. , 2007; Sagae and Tsujii, 2007)." ></td>
	<td class="line x" title="263:410	Here probabilities fromthe output ofaclassifieraremultipliedoverthewholesequence of actions." ></td>
	<td class="line x" title="264:410	This results in a locally normalized model." ></td>
	<td class="line x" title="265:410	Two other entries used MIRA (Mannem, 2007) or online passive-aggressive learning (Johansson and Nugues, 2007b) to train a globally normalized model." ></td>
	<td class="line x" title="266:410	Titov and Henderson (2007) used an incremental sigmoid Bayesian network to model the probability of a transition sequence and estimated model parameters using neural network learning." ></td>
	<td class="line x" title="267:410	5.3 Graph-Based Parsers While transition-based parsers use training data to learn a process for deriving dependency graphs, graph-based parsers learn a model of what it means to be a good dependency graph given an input sentence." ></td>
	<td class="line x" title="268:410	They define a scoring or probability function over the set of possible parses." ></td>
	<td class="line x" title="269:410	At learning time they estimate parameters of this function; at parsing time they search for the graph that maximizes this function." ></td>
	<td class="line x" title="270:410	These parsers mainly differ in the type and structure of the scoring function (model), the search algorithm that finds the best parse (inference), and the method to estimate the functions parameters (learning)." ></td>
	<td class="line x" title="271:410	5.3.1 Models The simplest type of model is based on a sum of local attachment scores, which themselves are calculated based on the dot product of a weight vector and a feature representation of the attachment." ></td>
	<td class="line x" title="272:410	This type of scoring function is often referred to as a firstorder model.8 Several systems participating in this yearssharedtaskusedfirst-ordermodels(Schiehlen and Spranger, 2007; Nguyen et al. , 2007; Shimizu and Nakagawa, 2007; Hall et al. , 2007b)." ></td>
	<td class="line x" title="273:410	Canisius and Tjong Kim Sang (2007) cast the same type of arc-based factorization as a weighted constraint satisfaction problem." ></td>
	<td class="line x" title="274:410	Carreras (2007) extends the first-order model to incorporate a sum over scores for pairs of adjacent arcs in the tree, yielding a second-order model." ></td>
	<td class="line x" title="275:410	In contrasttopreviousworkwherethiswasconstrained to sibling relations of the dependent (McDonald and Pereira, 2006), here head-grandchild relations can be taken into account." ></td>
	<td class="line x" title="276:410	In all of the above cases the scoring function is decomposed into functions that score local properties (arcs, pairs of adjacent arcs) of the graph." ></td>
	<td class="line x" title="277:410	By contrast, the model of Nakagawa (2007) considers global properties of the graph that can take multiple arcs into account, such as multiple siblings and children of a node." ></td>
	<td class="line x" title="278:410	5.3.2 Inference Searching for the highest scoring graph (usually a tree) in a model depends on the factorization chosen and whether we are looking for projective or non-projective trees." ></td>
	<td class="line x" title="279:410	Maximum spanning tree algorithms can be used for finding the highest scoring non-projective tree in a first-order model (Hall et al. , 2007b; Nguyen et al. , 2007; Canisius and Tjong Kim Sang, 2007; Shimizu and Nakagawa, 2007), while Eisners dynamic programming algorithm solves the problem for a first-order factorization in the projective case (Schiehlen and Spranger, 2007)." ></td>
	<td class="line x" title="280:410	Carreras (2007) employs his own extension of Eisners algorithm for the case of projective trees and second-order models that include headgrandparent relations." ></td>
	<td class="line x" title="281:410	8It is also known as an edge-factored model." ></td>
	<td class="line x" title="282:410	925 The methods presented above are mostly efficient and always exact." ></td>
	<td class="line x" title="283:410	However, for models that take global properties of the tree into account, they cannot be applied." ></td>
	<td class="line x" title="284:410	Instead Nakagawa (2007) uses Gibbs sampling to obtain marginal probabilities of arcs being included in the tree using his global model and then applies a maximum spanning tree algorithm to maximize the sum of the logs of these marginals and return a valid cycle-free parse." ></td>
	<td class="line x" title="285:410	5.3.3 Learning Most of the graph-based parsers were trained using an online inference-based method such as passiveaggressive learning (Nguyen et al. , 2007; Schiehlen and Spranger, 2007), averaged perceptron (Carreras, 2007), or MIRA (Shimizu and Nakagawa, 2007), while some systems instead used methods based on maximum conditional likelihood (Nakagawa, 2007; Hall et al. , 2007b)." ></td>
	<td class="line x" title="286:410	5.4 Domain Adaptation 5.4.1 Feature-Based Approaches Onewayofadaptingalearnertoanewdomainwithout using any unlabeled data is to only include features that are expected to transfer well (Dredze et al. , 2007)." ></td>
	<td class="line x" title="287:410	In structural correspondence learning a transformation from features in the source domain to features of the target domain is learnt (Shimizu and Nakagawa, 2007)." ></td>
	<td class="line x" title="288:410	The original source features along with their transformed versions are then used to train a discriminative parser." ></td>
	<td class="line x" title="289:410	5.4.2 Ensemble-Based Approaches Dredze et al.(2007) trained a diverse set of parsers in order to improve cross-domain performance by incorporating their predictions as features for another classifier." ></td>
	<td class="line x" title="291:410	Similarly, two parsers trained with different learners and search directions were used in the co-learning approach of Sagae and Tsujii (2007)." ></td>
	<td class="line x" title="292:410	Unlabeled target data was processed with both parsers." ></td>
	<td class="line x" title="293:410	Sentences that both parsers agreed on were then added to the original training data." ></td>
	<td class="line x" title="294:410	This combined data set served as training data for one of the original parsers to produce the final system." ></td>
	<td class="line x" title="295:410	In a similar fashion, Watson and Briscoe (2007) used a variant of self-training to make use of the unlabeled target data." ></td>
	<td class="line x" title="296:410	5.4.3 Other Approaches Attardi et al.(2007) learnt tree revision rules for the target domain by first parsing unlabeled target data using a strong parser; this data was then combined with labeled source data; a weak parser was applied to this new dataset; finally tree correction rules are collected based on the mistakes of the weak parser with respect to the gold data and the output of the strong parser." ></td>
	<td class="line x" title="298:410	Another technique used was to filter sentences of the out-of-domain corpus based on their similarity to the target domain, as predicted by a classifier (Dredze et al. , 2007)." ></td>
	<td class="line x" title="299:410	Only if a sentence was judged similar to target domain sentences was it included in the training set." ></td>
	<td class="line x" title="300:410	Bick(2007)usedahybridapproach, whereadatadriven parser trained on the labeled training data was given access to the output of a Constraint Grammar parser for English run on the same data." ></td>
	<td class="line x" title="301:410	Finally, Schneider et al.(2007) learnt collocations and relational nouns from the unlabeled target data and used these in their parsing algorithm." ></td>
	<td class="line x" title="303:410	6 Analysis Having discussed the major approaches taken in the two tracks of the shared task, we will now return to the test results." ></td>
	<td class="line x" title="304:410	For the multilingual track, we compare results across data sets and across systems, and report results from a parser combination experiment involving all the participating systems (section 6.1)." ></td>
	<td class="line x" title="305:410	Forthedomainadaptationtrack, wesumupthemost important findings from the test results (section 6.2)." ></td>
	<td class="line x" title="306:410	6.1 Multilingual Track 6.1.1 Across Data Sets The average LAS over all systems varies from 68.07 for Basque to 80.95 for English." ></td>
	<td class="line x" title="307:410	Top scores vary from 76.31 for Greek to 89.61 for English." ></td>
	<td class="line x" title="308:410	In general, there is a good correlation between the top scores and the average scores." ></td>
	<td class="line x" title="309:410	For Greek, Italian, and Turkish, the top score is closer to the average score than the average distance, while for Czech, the distance is higher." ></td>
	<td class="line x" title="310:410	The languages that produced the most stable results in terms of system ranks with respect to LAS are Hungarian and Italian." ></td>
	<td class="line x" title="311:410	For UAS, Catalan also falls into this group." ></td>
	<td class="line x" title="312:410	The language that 926 Setup Arabic Chinese Czech Turkish 2006 without punctuation 66.9 90.0 80.2 65.7 2007 without punctuation 75.5 84.9 80.0 71.6 2006 with punctuation 67.0 90.0 80.2 73.8 2007 with punctuation 76.5 84.7 80.2 79.8 Table 5: A comparison of the LAS top scores from 2006 and 2007." ></td>
	<td class="line x" title="313:410	Official scoring conditions in boldface." ></td>
	<td class="line x" title="314:410	For Turkish, scores with punctuation also include word-internal dependencies." ></td>
	<td class="line x" title="315:410	produced the most unstable results with respect to LAS is Turkish." ></td>
	<td class="line x" title="316:410	In comparison to last years languages, the languages involved in the multilingual track this year can be more easily separated into three classes with respect to top scores:  Low (76.3176.94): Arabic, Basque, Greek  Medium (79.1980.21): Czech, Hungarian, Turkish  High (84.4089.61): Catalan, Chinese, English, Italian It is interesting to see that the classes are more easily definable via language characteristics than via characteristics of the data sets." ></td>
	<td class="line x" title="317:410	The split goes across training set size, original data format (constituent vs. dependency), sentence length, percentage of unknown words, number of dependency labels, and ratio of (C)POSTAGS and dependency labels." ></td>
	<td class="line x" title="318:410	The class with the highest top scores contains languages with a rather impoverished morphology." ></td>
	<td class="line x" title="319:410	Medium scores are reached by the two agglutinative languages, Hungarian and Turkish, as well as by Czech." ></td>
	<td class="line x" title="320:410	The most difficult languages are those that combine a relatively free word order with a high degree of inflection." ></td>
	<td class="line x" title="321:410	Based on these characteristics, one would expect to find Czech in the last class." ></td>
	<td class="line x" title="322:410	However, the Czech training set is four times the size of the training set for Arabic, which is the language with the largest training set of the difficult languages." ></td>
	<td class="line x" title="323:410	However, it would be wrong to assume that training set size alone is the deciding factor." ></td>
	<td class="line x" title="324:410	A closer look at table 1 shows that while Basque and Greek in fact have small training data sets, so do Turkish and Italian." ></td>
	<td class="line x" title="325:410	Another factor that may be associated with the above classification is the percentage of new words (PNW) in the test set." ></td>
	<td class="line x" title="326:410	Thus, the expectation would be that the highly inflecting languages have a high PNW while the languages with little morphology have a low PNW." ></td>
	<td class="line x" title="327:410	But again, there is no direct correspondence." ></td>
	<td class="line x" title="328:410	Arabic, Basque, Catalan, English, and Greek agree with this assumption: Catalan and English have the smallest PNW, and Arabic, Basque, and Greek have a high PNW." ></td>
	<td class="line x" title="329:410	But the PNW for Italian is higher than for Arabic and Greek, and this is also true for the percentage of new lemmas." ></td>
	<td class="line x" title="330:410	Additionally, the highest PNW can be found in Hungarian and Turkish, which reach higher scores than Arabic, Basque, and Greek." ></td>
	<td class="line x" title="331:410	These considerations suggest that highly inflected languages with (relatively) free word order need more training data, a hypothesis that will have to be investigated further." ></td>
	<td class="line x" title="332:410	There are four languages which were included in the shared tasks on multilingual dependency parsing both at CoNLL 2006 and at CoNLL 2007: Arabic, Chinese, Czech, and Turkish." ></td>
	<td class="line x" title="333:410	For all four languages, the same treebanks were used, which allows a comparison of the results." ></td>
	<td class="line x" title="334:410	However, in some cases the size of the training set changed, and at least one treebank, Turkish, underwent a thorough correction phase." ></td>
	<td class="line x" title="335:410	Table 5 shows the top scores for LAS." ></td>
	<td class="line x" title="336:410	Since the official scores excluded punctuation in 2006 but includes it in 2007, we give results both with and without punctuation for both years." ></td>
	<td class="line x" title="337:410	For Arabic and Turkish, we see a great improvement of approximately 9 and 6 percentage points." ></td>
	<td class="line x" title="338:410	For Arabic, the number of tokens in the training set doubled, and the morphological annotation was made more informative." ></td>
	<td class="line x" title="339:410	The combined effect of these changes can probably account for the substantial improvement in parsing accuracy." ></td>
	<td class="line x" title="340:410	For Turkish, thetrainingsetgrewinsizeaswell, althoughonlyby 600 sentences, but part of the improvement for Turkishmayalsobeduetocontinuingeffortsinerrorcor927 rection and consistency checking." ></td>
	<td class="line x" title="341:410	We see that the choice to include punctuation or not makes a large difference for the Turkish scores, since non-final IGs of a word are counted as punctuation (because they have the underscore character as their FORM value), which means that word-internal dependency links are included if punctuation is included.9 However, regardless of whether we compare scores with or without punctuation, we see a genuine improvement of approximately 6 percentage points." ></td>
	<td class="line x" title="342:410	For Chinese, the same training set was used." ></td>
	<td class="line x" title="343:410	Therefore, the drop from last years top score to this years is surprising." ></td>
	<td class="line x" title="344:410	However, last years top scoring system for Chinese (Riedel et al. , 2006), which did not participate this year, had a score that was more than 3 percentage points higher than the second best system for Chinese." ></td>
	<td class="line x" title="345:410	Thus, if we compare this years results to the second best system, the difference is approximately 2 percentage points." ></td>
	<td class="line x" title="346:410	This final difference may be attributed to the properties of thetestsets." ></td>
	<td class="line x" title="347:410	Whilelastyearstestsetwastakenfrom the treebank, this years test set contains texts from other sources." ></td>
	<td class="line x" title="348:410	The selection of the textual basis also significantly changed average sentence length: The Chinese training set has an average sentence length of 5.9." ></td>
	<td class="line x" title="349:410	Last years test set also had an average sentence length of 5.9." ></td>
	<td class="line x" title="350:410	However, this year, the average sentence length is 7.5 tokens, which is a significant increase." ></td>
	<td class="line x" title="351:410	Longer sentences are typically harder to parse due to the increased likelihood of ambiguous constructions." ></td>
	<td class="line x" title="352:410	Finally, we note that the performance for Czech is almost exactly the same as last year, despite the fact that the size of the training set has been reduced to approximately one third of last years training set." ></td>
	<td class="line x" title="353:410	It is likely that this in fact represents a relative improvement compared to last years results." ></td>
	<td class="line x" title="354:410	6.1.2 Across Systems The LAS over all languages ranges from 80.32 to 54.55." ></td>
	<td class="line x" title="355:410	The comparison of the system ranks averaged over all languages with the ranks for single lan9The decision to include word-internal dependencies in this way can be debated on the grounds that they can be parsed deterministically." ></td>
	<td class="line x" title="356:410	On the other hand, they typically correspond to regular dependencies captured by function words in other languages, which are often easy to parse as well." ></td>
	<td class="line x" title="357:410	It is therefore unclear whether scores are more inflated by including wordinternal dependencies or deflated by excluding them." ></td>
	<td class="line x" title="358:410	guages show considerably more variation than last years systems." ></td>
	<td class="line x" title="359:410	Buchholz and Marsi (2006) report that [f]or most parsers, their ranking differs at most a few places from their overall ranking." ></td>
	<td class="line x" title="360:410	This year, for all of the ten best performing systems with respecttoLAS,thereisatleastonelanguageforwhich their rank is at least 5 places different from their overall rank." ></td>
	<td class="line x" title="361:410	The most extreme case is the top performing Nilsson system (Hall et al. , 2007a), which reached rank 1 for five languages and rank 2 for two more languages." ></td>
	<td class="line x" title="362:410	Their only outlier is for Chinese, where the system occupies rank 14, with a LAS approximately 9 percentage points below the top scoring system for Chinese (Sagae and Tsujii, 2007)." ></td>
	<td class="line x" title="363:410	However, Hall et al.(2007a) point out that the official results for Chinese contained a bug, and the true performance of their system was actually much higher." ></td>
	<td class="line x" title="365:410	The greatest improvement of a system with respect to its average rank occurs for English, for which the system by Nguyen et al.(2007) improved from the average rank 15 to rank 6." ></td>
	<td class="line x" title="367:410	Two more outliers can be observed in the system of Johansson and Nugues (2007b), which improves from its average rank 12 to rank 4 for Basque and Turkish." ></td>
	<td class="line x" title="368:410	The authors attribute this high performance to their parsers good performance on small training sets." ></td>
	<td class="line x" title="369:410	However, this hypothesis is contradicted by their results for Greek and Italian, the other two languages with small training sets." ></td>
	<td class="line x" title="370:410	For these two languages, the systems rank is very close to its average rank." ></td>
	<td class="line x" title="371:410	6.1.3 An Experiment in System Combination Having the outputs of many diverse dependency parsers for standard data sets opens up the interesting possibility of parser combination." ></td>
	<td class="line x" title="372:410	To combine the outputs of each parser we used the method of Sagae and Lavie (2006)." ></td>
	<td class="line x" title="373:410	This technique assigns to each possible labeled dependency a weight that is equal to the number of systems that included the dependency in their output." ></td>
	<td class="line x" title="374:410	This can be viewed as an arc-based voting scheme." ></td>
	<td class="line x" title="375:410	Using these weights it is possible to search the space of possible dependency trees using directed maximum spanning tree algorithms (McDonald et al. , 2005)." ></td>
	<td class="line x" title="376:410	The maximum spanning tree in this case is equal to the tree that on average contains the labeled dependencies that most systems voted for." ></td>
	<td class="line x" title="377:410	It is worth noting that variants of this scheme were used in two of the participating 928 5 10 15 20 Number of Systems 80 82 84 86 88 Accuracy Unlabeled Accuracy Labeled Accuracy Figure 1: System Combination systems, the Nilsson system (Hall et al. , 2007a) and the system of Sagae and Tsujii (2007)." ></td>
	<td class="line x" title="378:410	Figure 1 plots the labeled and unlabeled accuracies when combining an increasing number of systems." ></td>
	<td class="line x" title="379:410	The data used in the plot was the output of all competing systems for every language in the multilingual track." ></td>
	<td class="line x" title="380:410	The plot was constructed by sorting the systems based on their average labeled accuracy scores over all languages, and then incrementally adding each system in descending order.10 We can see that both labeled and unlabeled accuracy are significantly increased, even when just the top three systems are included." ></td>
	<td class="line x" title="381:410	Accuracy begins to degrade gracefully after about ten different parsers have been added." ></td>
	<td class="line x" title="382:410	Furthermore, the accuracy never falls below the performance of the top three systems." ></td>
	<td class="line x" title="383:410	6.2 Domain Adaptation Track For this task, the results are rather surprising." ></td>
	<td class="line x" title="384:410	A look at the LAS and UAS for the chemical research abstracts shows that there are four closed systems that outperform the best scoring open system." ></td>
	<td class="line x" title="385:410	The best system (Sagae and Tsujii, 2007) reaches an LAS of 81.06 (in comparison to their LAS of 89.01 for the English data set in the multilingual track)." ></td>
	<td class="line x" title="386:410	Considering that approximately one third of the words of the chemical test set are new, the results are noteworthy." ></td>
	<td class="line x" title="387:410	The next surprise is to be found in the relatively low UAS for the CHILDES data." ></td>
	<td class="line x" title="388:410	At a first glance, this data set has all the characteristics of an easy 10The reason that there is no data point for two parsers is that the simple voting scheme adopted only makes sense with at least three parsers voting." ></td>
	<td class="line x" title="389:410	set; the average sentence is short (12.9 words), and the percentage of new words is also small (6.10%)." ></td>
	<td class="line x" title="390:410	Despite these characteristics, the top UAS reaches 62.49 and is thus more than 10 percentage points below the top UAS for the chemical data set." ></td>
	<td class="line x" title="391:410	One major reason for this is that auxiliary and main verb dependencies are annotated differently in the CHILDES data than in the WSJ training set." ></td>
	<td class="line x" title="392:410	As a result of this discrepancy, participants were not required to submit results for the CHILDES data." ></td>
	<td class="line x" title="393:410	The best performing system on the CHILDES corpus is an open system (Bick, 2007), but the distance to the top closed system is approximately 1 percentage point." ></td>
	<td class="line x" title="394:410	In this domain, it seems more feasible to use general language resources than for the chemical domain." ></td>
	<td class="line x" title="395:410	However, the results prove that the extra effort may be unnecessary." ></td>
	<td class="line x" title="396:410	7 Conclusion Two years of dependency parsing in the CoNLL shared task has brought an enormous boost to the developmentofdependencyparsersformultiplelanguages (and to some extent for multiple domains)." ></td>
	<td class="line x" title="397:410	But even though nineteen languages have been covered by almost as many different parsing and learning approaches, we still have only vague ideas about the strengths and weaknesses of different methods for languages with different typological characteristics." ></td>
	<td class="line x" title="398:410	Increasing our knowledge of the multi-causal relationship between language structure, annotation scheme, and parsing and learning methods probably remains the most important direction for future research in this area." ></td>
	<td class="line x" title="399:410	The outputs of all systems for all data sets from the two shared tasks are freely availableforresearchandconstituteapotentialgoldmine for comparative error analysis across languages and systems." ></td>
	<td class="line x" title="400:410	For domain adaptation we have barely scratched the surface so far." ></td>
	<td class="line x" title="401:410	But overcoming the bottleneck of limited annotated resources for specialized domains will be as important for the deployment of human language technology as being able to handle multiple languages in the future." ></td>
	<td class="line x" title="402:410	One result from the domain adaptation track that may seem surprising at first is the fact that closed class systems outperformed open class systems on the chemical abstracts." ></td>
	<td class="line x" title="403:410	However, it seems that the major problem in 929 adapting pre-existing parsers to the new domain was not the domain as such but the mapping from the native output of the parser to the kind of annotation provided in the shared task data sets." ></td>
	<td class="line x" title="404:410	Thus, finding ways of reusing already invested development efforts by adapting the outputs of existing systems to new requirements, without substantial loss in accuracy, seems to be another line of research that may be worth pursuing." ></td>
	<td class="line x" title="405:410	Acknowledgments First and foremost, we want to thank all the people and organizations that generously provided us with treebank data and helped us prepare the data sets and without whom the shared task would have been literally impossible: Otakar Smrz, Charles University, and the LDC (Arabic); Maxux Aranzabe, Kepa Bengoetxea, Larraitz Uria, Koldo Gojenola, and the University of the Basque Country (Basque); Ma." ></td>
	<td class="line x" title="406:410	Ant`onia Mart Antonn, Llus M`arquez, Manuel Bertran, Mariona Taule, Difda Monterde, Eli Comelles, and CLiC-UB (Catalan); Shih-Min Li, Keh-Jiann Chen, Yu-Ming Hsieh, and Academia Sinica (Chinese); Jan Hajic, Zdenek Zabokrtsky, Charles University, and the LDC (Czech); Brian MacWhinney, Eric Davis, the CHILDES project, the Penn BioIE project, and the LDC (English); Prokopis Prokopidis and ILSP (Greek); Csirik Janos and Zoltan Alexin (Hungarian); Giuseppe Attardi, Simonetta Montemagni, Maria Simi, Isidoro Barraco, Patrizia Topi, Kiril Ribarov, Alessandro Lenci, Nicoletta Calzolari, ILC, and ELRA (Italian); Gulsen Eryigit, Kemal Oflazer, and Ruket Cakc (Turkish)." ></td>
	<td class="line x" title="407:410	Secondly, we want to thank the organizers of last years shared task, Sabine Buchholz, Amit Dubey, Erwin Marsi, and Yuval Krymolowski, who solved all the really hard problems for us and answered all our questions, as well as our colleagues who helped review papers: Jason Baldridge, Sabine Buchholz, James Clarke, Gulsen Eryigit, Kilian Evang, Julia Hockenmaier, Yuval Krymolowski, Erwin Marsi, Beata Megyesi, Yannick Versley, and Alexander Yeh." ></td>
	<td class="line x" title="408:410	Special thanks to Bertjan Busser and Erwin Marsi for help with the CoNLL shared task website and many other things, and to Richard Johansson for letting us use his conversion tool for English." ></td>
	<td class="line x" title="409:410	Thirdly, we want to thank the program chairs for EMNLP-CoNLL 2007, Jason Eisner and Taku Kudo, the publications chair, Eric Ringger, the SIGNLL officers, Antal van den Bosch, Hwee Tou Ng, and Erik Tjong Kim Sang, and members of the LDC staff, Tony Castelletto and Ilya Ahtaridis, for great cooperation and support." ></td>
	<td class="line x" title="410:410	Finally, we want to thank the following people, who in different ways assisted us in the organization of the CoNLL 2007 shared task: Giuseppe Attardi, Eckhard Bick, Matthias Buch-Kromann, Xavier Carreras, Tomaz Erjavec, Svetoslav Marinov, Wolfgang Menzel, Xue Nianwen, Gertjan van Noord, Petya Osenova, Florian Schiel, Kiril Simov, Zdenka Uresova, and Heike Zinsmeister." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D07-1129
Structural Correspondence Learning for Dependency Parsing
Shimizu, Nobuyuki;Nakagawa, Hiroshi;"></td>
	<td class="line x" title="1:93	Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp." ></td>
	<td class="line x" title="2:93	11661169, Prague, June 2007." ></td>
	<td class="line oc" title="3:93	c2007 Association for Computational Linguistics Structural Correspondence Learning for Dependency Parsing Nobuyuki Shimizu Information Technology Center University of Tokyo Tokyo, Japan shimizu@r.dl.itc.u-tokyo.ac.jp Hiroshi Nakagawa Information Technology Center University of Tokyo Tokyo, Japan nakagawa@dl.itc.u-tokyo.ac.jp Abstract Following (Blitzer et al. , 2006), we present an application of structural correspondence learning to non-projective dependency parsing (McDonald et al. , 2005)." ></td>
	<td class="line x" title="4:93	To induce the correspondences among dependency edges from different domains, we looked at every two tokens in a sentence and examined whether or not there is a preposition, a determiner or a helping verb between them." ></td>
	<td class="line x" title="5:93	Three binary linear classifiers were trained to predict the existence of a preposition, etc, on unlabeled data and we used singular value decomposition to induce new features." ></td>
	<td class="line x" title="6:93	During the training, the parser was trained with these additional features in addition to these described in (McDonald et al. , 2005)." ></td>
	<td class="line x" title="7:93	We discriminatively trained our parser in an on-line fashion using a variant of the voted perceptron (Collins, 2002; Collins and Roark, 2004; Crammer and Singer, 2003)." ></td>
	<td class="line x" title="8:93	1 Introduction We have recently seen growing popularity of dependency parsing." ></td>
	<td class="line x" title="9:93	It is no longer rare to see dependency relations used as features, in tasks such as machine translation (Ding and Palmer, 2005) and relation extraction (Bunescu and Mooney, 2005)." ></td>
	<td class="line x" title="10:93	However, there is one factor that prevents the use of dependency parsing: sparseness of annotated corpora outside Wall Street Journal." ></td>
	<td class="line x" title="11:93	In many situations we need to parse sentences from a target domain with no labeled data, which is a different distribution from a source domain where plentiful labeled training data is available." ></td>
	<td class="line oc" title="12:93	In this paper, we investigate the effectiveness of structural correspondence learning (SCL) (Blitzer et al. , 2006) in the domain adaptation task given by the CoNLL 2007." ></td>
	<td class="line o" title="13:93	They hypothesize that a model trained in the source domain using this common feature representation will generalize better to the target domain, and focus on using unlabeled data from both the source and target domains to learn a common feature representation that is meaningful across both domains." ></td>
	<td class="line x" title="14:93	The paper is structured as follows: in section 2, we review the decoding and learning aspects of (McDonald et al. , 2005), in section 3, structural correspondence learning applied to dependency parsing, and in section 4, we describe the experiments and the features needed for the CoNLL 2006 shared task." ></td>
	<td class="line x" title="15:93	2 Non-Projective Dependency Parsing 2.1 Dependency Structure Let us define x to be a generic sequence of input tokens together with their POS tags and other morphological features, and y to be a generic dependency structure, that is, a set of edges for x. A labeled edge is a tuple DEPREL,i  j where i is the start point of the edge, j is the end point, and DEPREL is the label of the edge." ></td>
	<td class="line x" title="16:93	The token at i is the head of the token at j. Table 1 shows our formulation of a structured prediction problem." ></td>
	<td class="line x" title="17:93	Given x, the input tokens and their features (column 2 and 3, Table 1), the task is to pre1166 Index Token POS Labeled Edge 1 John NN SUBJ,2  1 2 saw VBD PRED,0  2 3 a DT DET,4  3 4 dog NN OBJ,2  4 5 yesterday RB ADJU,2  5 6 which WDT MODWH,7  6 7 was VBD MODPRED,4  7 8 a DT DET,10  8 9 Yorkshire NN MODN,10  9 10 Terrier NN OBJ,7  10 11." ></td>
	<td class="line x" title="18:93	.,10  11 Table 1: Example Edges dict y, the set of labeled edges (column 4, Table 1)." ></td>
	<td class="line x" title="20:93	In this paper we use the common method of factoring the score of the dependency structure as the sum of the scores of all the labeled edges." ></td>
	<td class="line x" title="21:93	A dependency structure is characterized by its labeled edges, and for each labeled edge, we have features and corresponding weights." ></td>
	<td class="line x" title="22:93	The score of a dependency structure is the sum of these weights." ></td>
	<td class="line x" title="23:93	For example, let us say we would like to find the score of the labeled edge OBJ,2  4." ></td>
	<td class="line x" title="24:93	This is the edge going to the 4th token dog in Table 1." ></td>
	<td class="line x" title="25:93	The features for this edge could be:  There is an edge starting at saw, with the POS tag VBD, and the distance between the head and the child is 2." ></td>
	<td class="line x" title="26:93	( head = wordj, headPOS = posj, dist(i,j) = |ij| )  There is an edge ending at dog, with the POS tag NN, and the distance between the head and the child is 2." ></td>
	<td class="line x" title="27:93	( child = wordi, childPOS = posi, dist(i,j) = |ij| ) In the upcoming section, we explain a decoding algorithm for the dependency structures, and later we give a method for learning the weight vector used in the decoding." ></td>
	<td class="line x" title="28:93	2.2 Maximum Spanning Tree Algorithm As in (McDonald et al. , 2005), we use Chu-LiuEdmonds (CLE) algorithm (Chu and Liu, 1965; Edmonds, 1967) for decoding." ></td>
	<td class="line x" title="29:93	CLE finds the Maximum Spanning Tree in a directed graph." ></td>
	<td class="line x" title="30:93	The following is a summary given in (McDonald et al. , 2005)." ></td>
	<td class="line x" title="31:93	Informally, the algorithm has each vertex in the graph greedily select the incoming edge with highest weight." ></td>
	<td class="line x" title="32:93	Note that the edge is coming from the parent to the child." ></td>
	<td class="line x" title="33:93	That is, given a child node wordj, we are finding the parent, or the head wordi such that the edge (i,j) has the highest weight among all i, i negationslash= j. If a tree results, then this must be the maximum spanning tree." ></td>
	<td class="line x" title="34:93	If not, there must be a cycle." ></td>
	<td class="line x" title="35:93	The procedure identifies a cycle and contracts it into a single vertex and recalculates edge weights going into and out of the cycle." ></td>
	<td class="line x" title="36:93	It can be shown that a maximum spanning tree on the contracted graph is equivalent to a maximum spanning tree in the original graph (Leonidas, 2003)." ></td>
	<td class="line x" title="37:93	Hence the algorithm can recursively call itself on the new graph." ></td>
	<td class="line x" title="38:93	2.3 Online Learning Again following (McDonald et al. , 2005), we have used the single best MIRA (Crammer and Singer, 2003), which is a margin aware variant of perceptron (Collins, 2002; Collins and Roark, 2004) for structured prediction." ></td>
	<td class="line x" title="39:93	In short, the update is executed when the decoder fails to predict the correct parse, and we compare the correct parse yt and the incorrect parse y suggested by the decoding algorithm." ></td>
	<td class="line x" title="40:93	The weights of the features in y will be lowered, and the weights of the features in yt will be increased accordingly." ></td>
	<td class="line oc" title="41:93	3 Domain Adaptation Following (Blitzer et al. , 2006), we present an application of structural correspondence learning (SCL) to non-projective dependency parsing (McDonald et al. , 2005)." ></td>
	<td class="line o" title="42:93	SCL is a method for adapting a classifier learned in a source domain to a target domain." ></td>
	<td class="line x" title="43:93	We assume that both domains have unlabeled data, but only the source domain has labeled training data." ></td>
	<td class="line o" title="44:93	SCL works as follows: 1." ></td>
	<td class="line x" title="45:93	Define a set of pivot features on the unlabeled data from both domains." ></td>
	<td class="line x" title="46:93	2." ></td>
	<td class="line x" title="47:93	Use these pivot features to learn a mapping from the original feature spaces of both domains to a shared, low-dimensional real-valued feature space." ></td>
	<td class="line x" title="48:93	A high inner product in this new space indicates a high degree of correspondence." ></td>
	<td class="line x" title="49:93	3." ></td>
	<td class="line x" title="50:93	Use both the transformed and original features from the source domain." ></td>
	<td class="line x" title="51:93	4." ></td>
	<td class="line x" title="52:93	Again using both the transformed and original features, test the samples from the target domain." ></td>
	<td class="line x" title="53:93	If we learned a good mapping, then the effectiveness of the classifier in the source domain should transfer to the target domain." ></td>
	<td class="line x" title="54:93	To induce the correspondences among dependency edges in the source domain and the target domain, we looked at every two tokens in a sentence and examined whether or not there is a preposition, a determiner or a helping verb between them." ></td>
	<td class="line x" title="55:93	Although no edge is present in unlabeled data, the 1167 presence of a preposition indicates that this edge between the tokens, if existed, will not be a noun modifier (in English corpus, this label is NMOD)." ></td>
	<td class="line x" title="56:93	Thus, this induced feature should correlate with the label of an edge candidate." ></td>
	<td class="line x" title="57:93	We postulate that the label of an edge candidate, if known, may allow the supervised learner to choose the correct edge among the edge candidates in the target domain." ></td>
	<td class="line x" title="58:93	In the first step, we chose the presence of a preposition, a determiner or a helping verb between tokens as pivot features." ></td>
	<td class="line x" title="59:93	Then three binary linear classifiers were trained to predict the existence of a preposition (prep), determiner (det) and helping verb (hv) on unlabeled data and obtained a weight vector for each classifier." ></td>
	<td class="line x" title="60:93	classifierprep(e) = sign(wprep(e)) classifierdet(e) = sign(wdet(e)) classifierhv(e) = sign(whv(e)) The input to the above classifiers is an edge e instead of a whole sentence x.  is a mapping from an edge to a feature vector." ></td>
	<td class="line x" title="61:93	Since POS tags were not available in unlabeled data, for pivot predictors, we took the subset of the features given by an edge." ></td>
	<td class="line x" title="62:93	The features for pivot predictors are listed in Table 2." ></td>
	<td class="line x" title="63:93	The reminder of the features are the same as ones used in (McDonald et al. , 2005)." ></td>
	<td class="line x" title="64:93	Using each weight vector as a column, we created a weight matrix." ></td>
	<td class="line x" title="65:93	W = [wprep|wdet|whv]." ></td>
	<td class="line x" title="66:93	And run a singular value decomposition to induce a lower dimensional feature space." ></td>
	<td class="line x" title="67:93	W = UV . We then took the transpose of the resulting unitary matrix, U which maps the original data to the space spanned by the principal components, and applied it to the feature vector of every potential edge." ></td>
	<td class="line x" title="68:93	The original feature vector is parenleftbigg fsubset freminder parenrightbigg . We argument the feature vector with the additional feature induced by U." ></td>
	<td class="line x" title="69:93	The augmented feature vectors parenleftBigg f subset freminder Ufsubset parenrightBigg were used throughout the training and testing of the dependency parser." ></td>
	<td class="line x" title="70:93	4 Experiments Our experiments were conducted on CoNLL-2007 shared task domain adaptation track (Nivre et al. , 2007) using treebanks (Marcus et al. , 1993; Johansson and Nugues, 2007; Kulick et al. , 2004)." ></td>
	<td class="line x" title="71:93	Given an edge DEPREL,i,j head1 = wordi1 head = wordi head+1 = wordi+1 child1 = wordj1 child = wordj child+1 = wordj+1 Table 2: Binary Features for Pivot Predictors 4.1 Dependency Relation The CLE algorithm works on a directed graph with unlabeled edges." ></td>
	<td class="line x" title="72:93	Since the CoNLL shared task requires the labeling of edges, as a preprocessing stage, we created a directed complete graph." ></td>
	<td class="line x" title="73:93	Then we labeled each edge with the highest scoring dependency relation." ></td>
	<td class="line x" title="74:93	This complete graph was given to the CLE algorithm and the edge labels were never altered in the course of finding the maximum spanning tree." ></td>
	<td class="line x" title="75:93	4.2 Features The features we used for pivot predictors to classify each edge DEPREL,i,j are shown in Table 2." ></td>
	<td class="line x" title="76:93	The index i is the position of the parent and j is that of the child." ></td>
	<td class="line x" title="77:93	wordj = the word token at the position j. posj = the coarse part-of-speech at j. No other features were used beyond the combinations of the word token in Table 2." ></td>
	<td class="line x" title="78:93	The hardware used was an Intel CPU at 3.0 Ghz with 32 GB of memory, and the software was written in C++." ></td>
	<td class="line x" title="79:93	While more iterations should help, due to the time constraints, we were unable to complete more training." ></td>
	<td class="line x" title="80:93	The parser required a few days to train." ></td>
	<td class="line x" title="81:93	5 Results Unfortunately, we have discovered a bug in our codes after submitting our results for the blind tests, and the reported results in (Nivre et al. , 2007) were not representative of our approach." ></td>
	<td class="line x" title="82:93	The current results (closed class) are shown in Table 3." ></td>
	<td class="line x" title="83:93	For the explanations of Labeled Attachment Score, Unlabeled Attachment Score and Label Accuracy, the readers are suggested to refer to the shared task introductory paper (Nivre et al. , 2007)." ></td>
	<td class="line o" title="84:93	WSJ represents the application of the parser without SCL to the source domain test set, and WSJ-SCL the parser with SCL to the same test set." ></td>
	<td class="line o" title="85:93	Similarily 1168 Domain LAS UAS Label Accuracy WSJ 83.01%  83.43% 86.43%  86.81% 88.77%  88.99% WSJ-SCL 83.43%  83.59% 86.87%  86.93% 88.75%  89.01% Chem 74.75%  75.18% 80.74%  81.24% 82.34%  82.70% Chem-SCL 75.04%  74.91% 81.02%  80.82% 82.18%  82.18% Table 3: Labeled Attachment Score, Unlabeled Attachment Score and Label Accuracy Chem and Chem-SCL represents the application of the parser without SCL and with SCL to the source domain test set respectively." ></td>
	<td class="line x" title="86:93	We did batch learning by running the online algorithm 4 times." ></td>
	<td class="line x" title="87:93	An arrow  indicates how the results after 2nd iteration changed at the end of 4th iteration." ></td>
	<td class="line n" title="88:93	Contrary to our expectations, we seem to see SCL overfitting to the source domain WSJ in this experiment." ></td>
	<td class="line x" title="89:93	Due to the lack of POS tags in unlabeled data, our feature set for pivot predictors uses tokens extensively unlike that for the dependency parser." ></td>
	<td class="line x" title="90:93	Since tokens are not as abstract as POS tags, we suspect induced features may have caused overfitting." ></td>
	<td class="line x" title="91:93	6 Conclusion We presented an application of structural correspondence learning to non-projective dependency parsing." ></td>
	<td class="line n" title="92:93	Effectiveness of SCL for domain adaptation is mixed in this experiment perhaps due to the mismatch between feature sets." ></td>
	<td class="line x" title="93:93	Future work includes use of more sophisticated features such as POS and other morphological features, possibly a joint domain adaptation of POS tagging and dependency parsing for unlabeled data as well as re-examination of pivot features." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N07-3002
Learning Structured Classifiers for Statistical Dependency Parsing
Wang, Qin Iris;"></td>
	<td class="line x" title="1:74	Proceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 58, Rochester, April 2007." ></td>
	<td class="line x" title="2:74	c2007 Association for Computational Linguistics Learning Structured Classi ers for Statistical Dependency Parsing Qin Iris Wang Department of Computing Science University of Alberta Edmonton, Canada T6G 2E8 wqin@cs.ualberta.ca Abstract My research is focused on developing machine learning algorithms for inferring dependency parsers from language data." ></td>
	<td class="line x" title="3:74	By investigating several approaches I have developed a unifying perspective that allows me to share advances between both probabilistic and non-probabilistic methods." ></td>
	<td class="line x" title="4:74	First, I describe a generative technique that uses a strictly lexicalised parsing model, where all the parameters are based on words and do not use any partof-speech (POS) tags nor grammatical categories." ></td>
	<td class="line x" title="5:74	Then, I incorporate two ideas from probabilistic parsing word similarity smoothing and local estimation to improve the large margin approach." ></td>
	<td class="line x" title="6:74	Finally, I present a simpler and more efcient approach to training dependency parsers by applying a boosting-like procedure to standard training methods." ></td>
	<td class="line x" title="7:74	1 Introduction Over the past decade, there has been tremendous progress on learning parsing models from treebank data (Magerman, 1995; Collins, 1999; Charniak, 1997; Ratnaparkhi, 1999; Charniak, 2000; Wang et al. , 2005; McDonald et al. , 2005)." ></td>
	<td class="line x" title="8:74	Most of the early work in this area was based on postulating generative probability models of language that included parse structures (Magerman, 1995; Collins, 1997; Charniak, 1997)." ></td>
	<td class="line x" title="9:74	Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems (Collins, 1997; Bikel, 2004)." ></td>
	<td class="line x" title="10:74	Subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed discriminative training techniques such as maximum conditional likelihood (i.e. maximum entropy ) to be applied (Ratnaparkhi, 1999; Charniak, 2000)." ></td>
	<td class="line x" title="11:74	Currently, the work on conditional parsing models appears to have culminated in large margin training approaches (Taskar et al. , 2004; McDonald et al. , 2005), which demonstrates the state of the art performance in English dependency parsing." ></td>
	<td class="line x" title="12:74	Despite the realization that maximum margin training is closely related to maximum conditional likelihood for conditional models (McDonald et al. , 2005), a suf ciently uni ed view has not yet been achieved that permits the easy exchange of improvements between the probabilistic and nonprobabilistic approaches." ></td>
	<td class="line x" title="13:74	For example, smoothing methods have played a central role in probabilistic approaches (Collins, 1997; Wang et al. , 2005), and yet they are not being used in current large margin training algorithms." ></td>
	<td class="line x" title="14:74	Another unexploited connection is that probabilistic approaches pay closer attention to the individual errors made by each component of a parse, whereas the training error minimized in the large margin approach the structured margin loss (McDonald et al. , 2005) is a coarse measure that only assesses the total error of an entire parse rather than focusing on the error of any particular component." ></td>
	<td class="line x" title="15:74	I have addressed both of these issues, as well as others in my work." ></td>
	<td class="line x" title="16:74	2 Dependency Parsing Model Given a sentence a0 a1a3a2a5a4a7a6a9a8a11a10a12a10a12a10a12a8a13a4a15a14a17a16, I consider the problem of computing an accurate directed depen5 dency tree, a18, over a0." ></td>
	<td class="line x" title="17:74	Note that a18 consists of ordered pairs of words a2a5a4a20a19a22a21a23a4a25a24a26a16 in a0 such that each word appears in at least one pair and each word has in-degree at most one." ></td>
	<td class="line x" title="18:74	Dependency trees are usually assumed to be projective (no crossing arcs), which means that if there is an arc a2a5a4a20a19a15a21a27a4a25a24a28a16, then a4a29a19 is an ancestor of all the words between a4a30a19 and a4a25a24 . Let a31a7a2a32a0a33a16 denote the set of all the directed, projective trees that span a0 . From an input sentence a0, one would like to be able to compute the best parse; that is, a projective tree, a18a35a34 a31a7a2a32a0a33a16, that obtains the highest score . In particular, I follow Eisner (1996) and McDonald et al.(2005) and assume that the score of a complete spanning tree a18 for a given sentence, whether probabilistically motivated or not, can be decomposed as a sum of local scores for each link (a word pair)." ></td>
	<td class="line x" title="20:74	In which case, the parsing problem reduces to a18a37a36 a1a39a38a41a40a43a42a45a44a46a38a48a47 a49a22a50a41a51a53a52a55a54a57a56 a58 a52a60a59a62a61a5a63a64a59a66a65a43a56a67a50a26a49 sa2a5a4a29a19a22a21a23a4a25a24a28a16 (1) where the score sa2a5a4 a19 a21 a4 a24 a16 can depend on any measurable property of a4a20a19 and a4a25a24 within the tree a18 . This formulation is suf ciently general to capture most dependency parsing models, including probabilistic dependency models (Wang et al. , 2005; Eisner, 1996) as well as non-probabilistic models (McDonald et al. , 2005; Wang et al. , 2006)." ></td>
	<td class="line x" title="21:74	For the purpose of learning, the score of each link can be expressed as a weighted linear combination of features sa2a5a4 a19 a21a68a4 a24 a16a45a1 a69a53a70a72a71a73a2a5a4 a19 a21a74a4 a24 a16 (2) where a69 are the weight parameters to be estimated during training." ></td>
	<td class="line x" title="22:74	3 Lexicalised Dependency Parsing To learn an accurate dependency parser from data, the rst approach I investigated is based on a strictly lexical parsing model where all the parameters are based on words (Wang et al. , 2005)." ></td>
	<td class="line x" title="23:74	The advantage of this approach is that it does not rely on part-ofspeech tags nor grammatical categories." ></td>
	<td class="line x" title="24:74	Furthermore, I based training on maximizing the conditional probability of a parse tree given a sentence, unlike most previous generative models (Magerman, 1995; Collins, 1997; Charniak, 1997), which focus on maximizing the joint probability of the parse tree and the sentence." ></td>
	<td class="line x" title="25:74	An ef cient training algorithm can be achieved by maximizing the conditional probability of each parsing decision, hence minimizing a loss based on each local link decision independently." ></td>
	<td class="line x" title="26:74	Importantly, inter-dependence between links can still be accommodated by exploiting dynamic features in training features that take into account the labels of (some) of the surrounding components when predicting the label of a target component." ></td>
	<td class="line x" title="27:74	To cope with the sparse data problem, I use distributional word similarity (Pereira et al. , 1993; Grefenstette, 1994; Lin, 1998) to generalize the observed frequency counts in the training corpus." ></td>
	<td class="line x" title="28:74	The experimental results on the Chinese Treebank 4.0 show that the accuracy of the conditional model is 13.6% higher than corresponding joint models, while similarity smoothing also allows the strictly lexicalised approach to outperform corresponding models based on part-of-speech tags." ></td>
	<td class="line x" title="29:74	4 Extensions to Large Margin Parsing The approach presented above has a limitation: it uses a local scoring function instead of a global scoring function to compute the score for a candidate tree." ></td>
	<td class="line x" title="30:74	The structured large margin approach, on the other hand, uses a global scoring function by minimizing a training loss the structured margin loss (McDonald et al. , 2005) which is directly coordinated with the global tree." ></td>
	<td class="line x" title="31:74	However, the training error minimized in the large margin approach is a coarse measure that only assesses the total error of an entire parse rather than focusing on the error of any particular component." ></td>
	<td class="line x" title="32:74	Also, smoothing methods, which have been widely used in probabilistic approaches, are not currently being used in large margin training algorithms." ></td>
	<td class="line x" title="33:74	In the second approach, I improve structured large margin training for parsing in two ways (Wang et al. , 2006)." ></td>
	<td class="line x" title="34:74	First, I incorporate local constraints that enforce the correctness of each individual link, rather than just scoring the global parse tree." ></td>
	<td class="line x" title="35:74	Second, to cope with sparse data and generalize to unseen words, I smooth the lexical parameters according to their underlying word similarities." ></td>
	<td class="line x" title="36:74	To smooth parameters in the large margin framework, I introduce the technique of Laplacian 6 regularization in large margin parsing." ></td>
	<td class="line x" title="37:74	Finally, to demonstrate the bene ts of my approach, I reconsider the problem of parsing Chinese treebank data using only lexical features, as in Section 3." ></td>
	<td class="line x" title="38:74	My results improve current large margin approaches and show that similarity smoothing combined with local constraint enforcement leads to state of the art performance, while only requiring word-based features that do not rely on part-of-speech tags nor grammatical categories in any way." ></td>
	<td class="line x" title="39:74	5 Training via Structured Boosting Finally, I have recently demonstrated the somewhat surprising result that state of the art dependency parsing performance can be achieved through the use of conventional, local classi cation methods." ></td>
	<td class="line x" title="40:74	In particular, I show how a simple form of structured boosting can be used to improve the training of standard local classi cation methods, in the context of structured predictions, without modifying the underlying training method (Wang et al. , 2007)." ></td>
	<td class="line x" title="41:74	The advantage of this approach is that one can use off-theshelf classi cation techniques, such as support vector machines or logistic regression, to achieve competitive parsing results with little additional effort." ></td>
	<td class="line x" title="42:74	The idea behind structured boosting is very simple." ></td>
	<td class="line x" title="43:74	To produce an accurate parsing model, one combines the local predictions of multiple weak predictors to obtain a score for each link, which a parser can then use to compute the maximum score tree for a given sentence." ></td>
	<td class="line x" title="44:74	Structured boosting proceeds in rounds." ></td>
	<td class="line x" title="45:74	On each round a local link predictor is trained merely to predict the existence and orientation of a link between two words given input features encoding context without worrying about coordinating the predictions in a coherent global parse." ></td>
	<td class="line x" title="46:74	Once a weak predictor is learned, it is added to the ensemble of weak hypotheses, the training corpus is re-parsed using the new predictor, and the local training contexts are re-weighted based on errors made by the parsers output." ></td>
	<td class="line x" title="47:74	Thus, a wrapper approach is used to successively modify the training data so that the training algorithm is encouraged to facilitate improved global parsing accuracy." ></td>
	<td class="line x" title="48:74	Table 1: Comparison with State of the Art (Dependency Accuracy) Model Chinese English Yamada&Matsumoto 03 90.3 Nivre&Scholz 04 87.3 Wang et al. 05 (Sec." ></td>
	<td class="line x" title="49:74	3) 79.9* McDonald et al. 05 90.9 McDonald&Pereira 06 82.5* 91.5 Corston-Oliver et al. 06 73.3a75 90.8 Structured 86.6* 89.3 Boosting (Sec." ></td>
	<td class="line x" title="50:74	5) 77.6a75 a76 Obtained with Chinese Treebank 4.0 using the data split reported in Wang et al.(2005)." ></td>
	<td class="line x" title="52:74	a77 Obtained with Chinese Treebank 5.0 using the data split reported in Corston-Olivr et al.(2006)." ></td>
	<td class="line x" title="54:74	6 Current Results Table 1 compares my results1 with those obtained by other researchers, on both English and Chinese data.2 The English results are obtained using the same standard training and test set splits from English Penn Treebank 3.0." ></td>
	<td class="line x" title="55:74	The results on Chinese are obtained on two different data sets, Chinese Treebank 4.0 and Chinese Treebank 5.0 as noted.3 Table 1 shows that the results I am able to achieve on English are competitive with the state of the art, but are still behind the best results of (McDonald and Pereira, 2006)." ></td>
	<td class="line x" title="56:74	However, perhaps surprisingly, Table 1 also shows that the structured boosting approach actually surpasses state of the art accuracy on Chinese parsing for both treebank collections." ></td>
	<td class="line x" title="57:74	7 Future Work Although the three pieces of my work above look very different super cially, they are actually closely related by the scoring formulation and, more 1I did not include the results of the technique described in Section 4, because we were only able to conveniently train on sentences with less than or equal to 15 words." ></td>
	<td class="line x" title="58:74	2McDonald et al.(2005) have tried MIRA on Chinese Treebank 4.0 with the same data split reported here, obtaining a dependency accuracy score of 82.5 (Ryan McDonald, personal communication)." ></td>
	<td class="line x" title="60:74	3The results on Chinese Treebank 5.0 are generally worse than on Chinese Treebank 4.0, since the former is a superset of the latter, and moreover the additional sentences come entirely from a Taiwanese Chinese source that is more dif cult to parse than the rest of the data." ></td>
	<td class="line x" title="61:74	7 speci cally, by the equations introduced in Section 2." ></td>
	<td class="line x" title="62:74	In other words, they all compute a linear classi er.4 The only differences among them are: (1) What features are used?" ></td>
	<td class="line x" title="63:74	(2) How are the parameters a69 estimated?" ></td>
	<td class="line x" title="64:74	A general perspective I bring to my investigation is the desire to delineate the effects of domain engineering (choosing good features for representing and learning parsing models) from the general machine learning principles (training criteria, regularization and smoothing techniques) that permit good results." ></td>
	<td class="line x" title="65:74	In fact, combined features have been proved to be useful in dependency parsing with support vector machines (Yamada and Matsumoto, 2003), and I have already obtained some preliminary results on generating useful feature combinations via boosting." ></td>
	<td class="line x" title="66:74	Therefore, I will consider combining all the projects I presented above." ></td>
	<td class="line x" title="67:74	That is, I plan to incorporate all the useful features, the morphological features and the combined features as discussed above, into the training algorithms presented in Section 4 or Section 5, to train a dependency parser globally." ></td>
	<td class="line x" title="68:74	Then I am going to augment the training with the existing smoothing and regularization techniques (as described in Section 4), or new developed ones." ></td>
	<td class="line x" title="69:74	I expect the resulting parser to have better performance than those I have presented above." ></td>
	<td class="line x" title="70:74	There are a lot of other ideas which can be explored in my future work." ></td>
	<td class="line x" title="71:74	First and most important, I plan to investigate new advanced machine learning methods (e.g. , structured boosting or unsupervised / semi-supervised algorithms (Xu et al. , 2006)) and apply them to the dependency parsing problem generally, since the goal of my research is to learn natural language parsers in an elegant and principled manner." ></td>
	<td class="line x" title="72:74	Next, I am going to apply my approaches to parse other languages, such as Czech, German, Spanish and French, and analyze the performance of my parsers on these different languages." ></td>
	<td class="line oc" title="73:74	Furthermore, I plan to apply my parsers in other domains (e.g. , biomedical data) (Blitzer et al. , 2006) besides treebank data, to investigate the effectiveness and generality of my approaches." ></td>
	<td class="line x" title="74:74	4In general, for any probabilistic model, the product of probabilities can be converted to sums of scores in the log space, which makes the search identical to a score based discriminative model." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-1034
Instance Weighting for Domain Adaptation in NLP
Jiang, Jing;Zhai, ChengXiang;"></td>
	<td class="line x" title="1:210	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 264271, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:210	c2007 Association for Computational Linguistics Instance Weighting for Domain Adaptation in NLP Jing Jiang and ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801, USA {jiang4,czhai}@cs.uiuc.edu Abstract Domain adaptation is an important problem in natural language processing (NLP) due to the lack of labeled data in novel domains." ></td>
	<td class="line x" title="3:210	In this paper, we study the domain adaptation problem from the instance weighting perspective." ></td>
	<td class="line x" title="4:210	We formally analyze and characterize the domain adaptation problem from a distributional view, and show that there are two distinct needs for adaptation, corresponding to the different distributions of instances and classification functions in the source and the target domains." ></td>
	<td class="line x" title="5:210	We then propose a general instance weighting framework for domain adaptation." ></td>
	<td class="line x" title="6:210	Our empirical results on three NLP tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective." ></td>
	<td class="line x" title="7:210	1 Introduction Many natural language processing (NLP) problems such as part-of-speech (POS) tagging, named entity (NE) recognition, relation extraction, and semantic role labeling, are currently solved by supervised learning from manually labeled data." ></td>
	<td class="line x" title="8:210	A bottleneck problem with this supervised learning approach is the lack of annotated data." ></td>
	<td class="line x" title="9:210	As a special case, we often face the situation where we have a sufficient amount of labeled data in one domain, but have little or no labeled data in another related domain which we are interested in." ></td>
	<td class="line x" title="10:210	We thus face the domain adaptation problem." ></td>
	<td class="line oc" title="11:210	Following (Blitzer et al. , 2006), we call the first the source domain, and the second the target domain." ></td>
	<td class="line x" title="12:210	The domain adaptation problem is commonly encountered in NLP." ></td>
	<td class="line x" title="13:210	For example, in POS tagging, the source domain may be tagged WSJ articles, and the target domain may be scientific literature that contains scientific terminology." ></td>
	<td class="line x" title="14:210	In NE recognition, the source domain may be annotated news articles, and the target domain may be personal blogs." ></td>
	<td class="line x" title="15:210	Another example is personalized spam filtering, where we may have many labeled spam and ham emails from publicly available sources, but we need to adapt the learned spam filter to an individual users inbox because the user has her own, and presumably very different, distribution of emails and notion of spams." ></td>
	<td class="line x" title="16:210	Despite the importance of domain adaptation in NLP, currently there are no standard methods for solving this problem." ></td>
	<td class="line x" title="17:210	An immediate possible solution is semi-supervised learning, where we simply treat the target instances as unlabeled data but do not distinguish the two domains." ></td>
	<td class="line x" title="18:210	However, given that the source data and the target data are from different distributions, we should expect to do better by exploiting the domain difference." ></td>
	<td class="line oc" title="19:210	Recently there have been some studies addressing domain adaptation from different perspectives (Roark and Bacchiani, 2003; Chelba and Acero, 2004; Florian et al. , 2004; Daume III and Marcu, 2006; Blitzer et al. , 2006)." ></td>
	<td class="line x" title="20:210	However, there have not been many studies that focus on the difference between the instance distributions in the two domains." ></td>
	<td class="line x" title="21:210	A detailed discussion on related work is given in Section 5." ></td>
	<td class="line x" title="22:210	In this paper, we study the domain adaptation problem from the instance weighting perspective." ></td>
	<td class="line x" title="23:210	264 In general, the domain adaptation problem arises when the source instances and the target instances are from two different, but related distributions." ></td>
	<td class="line x" title="24:210	We formally analyze and characterize the domain adaptation problem from this distributional view." ></td>
	<td class="line x" title="25:210	Such an analysis reveals that there are two distinct needs for adaptation, corresponding to the different distributions of instances and the different classification functions in the source and the target domains." ></td>
	<td class="line x" title="26:210	Based on this analysis, we propose a general instance weighting method for domain adaptation, which can be regarded as a generalization of an existing approach to semi-supervised learning." ></td>
	<td class="line x" title="27:210	The proposed method implements several adaptation heuristics with a unified objective function: (1) removing misleading training instances in the source domain; (2) assigning more weights to labeled target instances than labeled source instances; (3) augmenting training instances with target instances with predicted labels." ></td>
	<td class="line x" title="28:210	We evaluated the proposed method with three adaptation problems in NLP, including POS tagging, NE type classification, and spam filtering." ></td>
	<td class="line x" title="29:210	The results show that regular semi-supervised and supervised learning methods do not perform as well as our new method, which explicitly captures domain difference." ></td>
	<td class="line x" title="30:210	Our results also show that incorporating and exploiting more information from the target domain is much more useful for improving performance than excluding misleading training examples from the source domain." ></td>
	<td class="line x" title="31:210	The rest of the paper is organized as follows." ></td>
	<td class="line x" title="32:210	In Section 2, we formally analyze the domain adaptation problem and distinguish two types of adaptation." ></td>
	<td class="line x" title="33:210	In Section 3, we then propose a general instance weighting framework for domain adaptation." ></td>
	<td class="line x" title="34:210	In Section 4, we present the experiment results." ></td>
	<td class="line x" title="35:210	Finally, we compare our framework with related work in Section 5 before we conclude in Section 6." ></td>
	<td class="line x" title="36:210	2 Domain Adaptation In this section, we define and analyze domain adaptation from a theoretical point of view." ></td>
	<td class="line x" title="37:210	We show that the need for domain adaptation arises from two factors, and the solutions are different for each factor." ></td>
	<td class="line x" title="38:210	We restrict our attention to those NLP tasks that can be cast into multiclass classification problems, and we only consider discriminative models for classification." ></td>
	<td class="line x" title="39:210	Since both are common practice in NLP, our analysis is applicable to many NLP tasks." ></td>
	<td class="line x" title="40:210	Let X be a feature space we choose to represent the observed instances, and let Y be the set of class labels." ></td>
	<td class="line x" title="41:210	In the standard supervised learning setting, we are given a set of labeled instances {(xi,yi)}Ni=1, where xi  X, yi  Y, and (xi,yi) are drawn from an unknown joint distribution p(x,y)." ></td>
	<td class="line x" title="42:210	Our goal is to recover this unknown distribution so that we can predict unlabeled instances drawn from the same distribution." ></td>
	<td class="line x" title="43:210	In discriminative models, we are only concerned with p(y|x)." ></td>
	<td class="line x" title="44:210	Following the maximum likelihood estimation framework, we start with a parameterized model family p(y|x;), and then find the best model parameter  that maximizes the expected log likelihood of the data:  = argmax  integraldisplay X summationdisplay yY p(x,y)logp(y|x;)dx." ></td>
	<td class="line x" title="45:210	Since we do not know the distribution p(x,y), we maximize the empirical log likelihood instead:   argmax  integraldisplay X summationdisplay yY p(x,y)logp(y|x;)dx = argmax  1 N Nsummationdisplay i=1 logp(yi|xi;)." ></td>
	<td class="line x" title="46:210	Note that since we use the empirical distribution p(x,y) to approximate p(x,y), the estimated  is dependent on p(x,y)." ></td>
	<td class="line x" title="47:210	In general, as long as we have sufficient labeled data, this approximation is fine because the unlabeled instances we want to classify are from the same p(x,y)." ></td>
	<td class="line x" title="48:210	2.1 Two Factors for Domain Adaptation Let us now turn to the case of domain adaptation where the unlabeled instances we want to classify are from a different distribution than the labeled instances." ></td>
	<td class="line x" title="49:210	Let ps(x,y) and pt(x,y) be the true underlying distributions for the source and the target domains, respectively." ></td>
	<td class="line x" title="50:210	Our general idea is to use ps(x,y) to approximate pt(x,y) so that we can exploit the labeled examples in the source domain." ></td>
	<td class="line x" title="51:210	If we factor p(x,y) into p(x,y) = p(y|x)p(x), we can see that pt(x,y) can deviate from ps(x,y) in two different ways, corresponding to two different kinds of domain adaptation: 265 Case 1 (Labeling Adaptation): pt(y|x) deviates from ps(y|x) to a certain extent." ></td>
	<td class="line x" title="52:210	In this case, it is clear that our estimation of ps(y|x) from the labeled source domain instances will not be a good estimation of pt(y|x), and therefore domain adaptation is needed." ></td>
	<td class="line x" title="53:210	We refer to this kind of adaptation as function/labeling adaptation." ></td>
	<td class="line x" title="54:210	Case 2 (Instance Adaptation): pt(y|x) is mostly similar to ps(y|x), but pt(x) deviates from ps(x)." ></td>
	<td class="line x" title="55:210	In this case, it may appear that our estimated ps(y|x) can still be used in the target domain." ></td>
	<td class="line x" title="56:210	However, as we have pointed out, the estimation of ps(y|x) depends on the empirical distribution ps(x,y), which deviates from pt(x,y) due to the deviation of ps(x) from pt(x)." ></td>
	<td class="line x" title="57:210	In general, the estimation of ps(y|x) would be more influenced by the instances with high ps(x,y) (i.e. , high ps(x))." ></td>
	<td class="line x" title="58:210	If pt(x) is very different from ps(x), then we should expect pt(x,y) to be very different from ps(x,y), and therefore different from ps(x,y)." ></td>
	<td class="line x" title="59:210	We thus cannot expect the estimated ps(y|x) to work well on the regions where pt(x,y) is high, but ps(x,y) is low." ></td>
	<td class="line x" title="60:210	Therefore, in this case, we still need domain adaptation, which we refer to as instance adaptation." ></td>
	<td class="line x" title="61:210	Because the need for domain adaptation arises from two different factors, we need different solutions for each factor." ></td>
	<td class="line x" title="62:210	2.2 Solutions for Labeling Adaptation If pt(y|x) deviates from ps(y|x) to some extent, we have one of the following choices: Change of representation: It may be the case that if we change the representation of the instances, i.e., if we choose a feature space Xprime different from X, we can bridge the gap between the two distributions ps(y|x) and pt(y|x)." ></td>
	<td class="line x" title="63:210	For example, consider domain adaptive NE recognition where the source domain contains clean newswire data, while the target domain contains broadcast news data that has been transcribed by automatic speech recognition and lacks capitalization." ></td>
	<td class="line x" title="64:210	Suppose we use a naive NE tagger that only looks at the word itself." ></td>
	<td class="line x" title="65:210	If we consider capitalization, then the instance Bush is represented differently from the instance bush." ></td>
	<td class="line x" title="66:210	In the source domain, ps(y = Person|x = Bush) is high while ps(y = Person|x = bush) is low, but in the target domain, pt(y = Person|x = bush) is high." ></td>
	<td class="line x" title="67:210	If we ignore the capitalization information, then in both domains p(y = Person|x = bush) will be high provided that the source domain contains much fewer instances of bush than Bush." ></td>
	<td class="line x" title="68:210	Adaptation through prior: When we use a parameterized model p(y|x;) to approximate p(y|x) and estimate  based on the source domain data, we can place some prior on the model parameter  so that the estimated distribution p(y|x; ) will be closer to pt(y|x)." ></td>
	<td class="line x" title="69:210	Consider again the NE tagging example." ></td>
	<td class="line x" title="70:210	If we use capitalization as a feature, in the source domain where capitalization information is available, this feature will be given a large weight in the learned model because it is very useful." ></td>
	<td class="line x" title="71:210	If we place a prior on the weight for this feature so that a large weight will be penalized, then we can prevent the learned model from relying too much on this domain specific feature." ></td>
	<td class="line x" title="72:210	Instance pruning: If we know the instances x for which pt(y|x) is different from ps(y|x), we can actively remove these instances from the training data because they are misleading." ></td>
	<td class="line x" title="73:210	For all the three solutions given above, we need either some prior knowledge about the target domain, or some labeled target domain instances; from only the unlabeled target domain instances, we would not know where and why pt(y|x) differs from ps(y|x)." ></td>
	<td class="line x" title="74:210	2.3 Solutions for Instance Adaptation In the case where pt(y|x) is similar to ps(y|x), but pt(x) deviates from ps(x), we may use the (unlabeled) target domain instances to bias the estimate of ps(x) toward a better approximation of pt(x), and thus achieve domain adaptation." ></td>
	<td class="line x" title="75:210	We explain the idea below." ></td>
	<td class="line x" title="76:210	Our goal is to obtain a good estimate of t that is optimized according to the target domain distribution pt(x,y)." ></td>
	<td class="line x" title="77:210	The exact objective function is thus t = argmax  integraldisplay X summationdisplay yY pt(x,y)logp(y|x;)dx = argmax  integraldisplay X pt(x) summationdisplay yY pt(y|x)logp(y|x;)dx." ></td>
	<td class="line x" title="78:210	266 Our idea of domain adaptation is to exploit the labeled instances in the source domain to help obtain t. Let Ds = {(xsi,ysi)}Nsi=1 denote the set of labeled instances we have from the source domain." ></td>
	<td class="line x" title="79:210	Assume that we have a (small) set of labeled and a (large) set of unlabeled instances from the target domain, denoted by Dt,l = {(xt,lj,yt,lj )}Nt,lj=1 and Dt,u = {xt,uk }Nt,uk=1, respectively." ></td>
	<td class="line x" title="80:210	We now show three ways to approximate the objective function above, corresponding to using three different sets of instances to approximate the instance space X. Using Ds: Using ps(y|x) to approximate pt(y|x), we obtain t  argmax  integraldisplay X pt(x) ps(x)ps(x) summationdisplay yY ps(y|x)logp(y|x;)dx  argmax  integraldisplay X pt(x) ps(x)ps(x) summationdisplay yY ps(y|x)logp(y|x;)dx = argmax  1 Ns Nssummationdisplay i=1 pt(xsi) ps(xsi) logp(y s i|x s i;)." ></td>
	<td class="line x" title="81:210	Here we use only the labeled instances in Ds but we adjust the weight of each instance by pt(x)ps(x)." ></td>
	<td class="line x" title="82:210	The major difficulty is how to accurately estimate pt(x)ps(x)." ></td>
	<td class="line x" title="83:210	Using Dt,l: t  argmax  integraldisplay X pt,l(x) summationdisplay yY pt,l(y|x)logp(y|x;)dx = argmax  1 Nt,l Nt,lsummationdisplay j=1 logp(yt,lj |xt,lj ;) Note that this is the standard supervised learning method using only the small amount of labeled target instances." ></td>
	<td class="line x" title="84:210	The major weakness of this approximation is that when Nt,l is very small, the estimation is not accurate." ></td>
	<td class="line x" title="85:210	Using Dt,u: t  argmax  integraldisplay X pt,u(x) summationdisplay yY pt(y|x)logp(y|x;)dx = argmax  1 Nt,u Nt,usummationdisplay k=1 summationdisplay yY pt(y|xt,uk )logp(y|xt,uk ;), The challenge here is that pt(y|xt,uk ;) is unknown to us, thus we need to estimate it." ></td>
	<td class="line x" title="86:210	One possibility is to approximate it with a model  learned from Ds and Dt,l. For example, we can set pt(y|x,) = p(y|x; )." ></td>
	<td class="line x" title="87:210	Alternatively, we can also set pt(y|x,) to 1 if y = argmaxyprime p(yprime|x; ) and 0 otherwise." ></td>
	<td class="line x" title="88:210	3 A Framework of Instance Weighting for Domain Adaptation The theoretical analysis we give in Section 2 suggests that one way to solve the domain adaptation problem is through instance weighting." ></td>
	<td class="line x" title="89:210	We propose a framework that incorporates instance pruning in Section 2.2 and the three approximations in Section 2.3." ></td>
	<td class="line x" title="90:210	Before we show the formal framework, we first introduce some weighting parameters and explain the intuitions behind these parameters." ></td>
	<td class="line x" title="91:210	First, for each (xsi,ysi)  Ds, we introduce a parameter i to indicate how likely pt(ysi|xsi) is close to ps(ysi|xsi)." ></td>
	<td class="line x" title="92:210	Large i means the two probabilities are close, and therefore we can trust the labeled instance (xsi,ysi) for the purpose of learning a classifier for the target domain." ></td>
	<td class="line x" title="93:210	Small i means these two probabilities are very different, and therefore we should probably discard the instance (xsi,ysi) in the learning process." ></td>
	<td class="line x" title="94:210	Second, again for each (xsi,ysi)  Ds, we introduce another parameter i that ideally is equal to pt(xsi) ps(xsi)." ></td>
	<td class="line x" title="95:210	From the approximation in Section 2.3 that uses only Ds, it is clear that such a parameter is useful." ></td>
	<td class="line x" title="96:210	Next, for each xt,ui  Dt,u, and for each possible label y  Y, we introduce a parameter i(y) that indicates how likely we would like to assign y as a tentative label to xt,ui and include (xt,ui,y) as a training example." ></td>
	<td class="line x" title="97:210	Finally, we introduce three global parameters s, t,l and t,u that are not instance-specific but are associated with Ds, Dt,l and Dt,u, respectively." ></td>
	<td class="line x" title="98:210	These three parameters allow us to control the contribution of each of the three approximation methods in Section 2.3 when we linearly combine them together." ></td>
	<td class="line x" title="99:210	We now formally define our instance weighting framework." ></td>
	<td class="line x" title="100:210	Given Ds, Dt,l and Dt,u, to learn a classifier for the target domain, we find a parameter  that optimizes the following objective function: 267  = argmax  bracketleftbigg s  1C s Nssummationdisplay i=1 ii logp(ysi|xsi;) +t,l  1C t,l Nt,lsummationdisplay j=1 logp(yt,lj |xt,lj ;) +t,u  1C t,u Nt,usummationdisplay k=1 summationdisplay yY k(y)logp(y|xt,uk ;) +logp() bracketrightbigg, where Cs = summationtextNsi=1 ii, Ct,l = Nt,l, Ct,u =summationtext Nt,u k=1 summationtext yY k(y), and s + t,l + t,u = 1." ></td>
	<td class="line x" title="101:210	The last term, logp(), is the log of a Gaussian prior distribution of , commonly used to regularize the complexity of the model." ></td>
	<td class="line x" title="102:210	In general, we do not know the optimal values of these parameters for the target domain." ></td>
	<td class="line x" title="103:210	Nevertheless, the intuitions behind these parameters serve as guidelines for us to design heuristics to set these parameters." ></td>
	<td class="line x" title="104:210	In the rest of this section, we introduce several heuristics that we used in our experiments to set these parameters." ></td>
	<td class="line x" title="105:210	3.1 Setting  Following the intuition that if pt(y|x) differs much from ps(y|x), then (x,y) should be discarded from the training set, we use the following heuristic to set s. First, with standard supervised learning, we train a model t,l from Dt,l. We consider p(y|x; t,l) to be a crude approximation of pt(y|x)." ></td>
	<td class="line x" title="106:210	Then, we classify {xsi}Nsi=1 using t,l. The top k instances that are incorrectly predicted by t,l (ranked by their prediction confidence) are discarded." ></td>
	<td class="line x" title="107:210	In another word, si of the top k instances for which ysi negationslash= argmaxy p(y|xsi; t,l) are set to 0, and i of all the other source instances are set to 1." ></td>
	<td class="line x" title="108:210	3.2 Setting  Accurately setting  involves accurately estimating ps(x) and pt(x) from the empirical distributions." ></td>
	<td class="line x" title="109:210	For many NLP classification tasks, we do not have a good parametric model for p(x)." ></td>
	<td class="line x" title="110:210	We thus need to resort to non-parametric density estimation methods." ></td>
	<td class="line x" title="111:210	However, for many NLP tasks, x resides in a high dimensional space, which makes it hard to apply standard non-parametric density estimation methods." ></td>
	<td class="line x" title="112:210	We have not explored this direction, and in our experiments, we set  to 1 for all source instances." ></td>
	<td class="line x" title="113:210	3.3 Setting  Setting  is closely related to some semi-supervised learning methods." ></td>
	<td class="line x" title="114:210	One option is to set k(y) = p(y|xt,uk ;)." ></td>
	<td class="line x" title="115:210	In this case,  is no longer a constant but is a function of ." ></td>
	<td class="line x" title="116:210	This way of setting  corresponds to the entropy minimization semi-supervised learning method (Grandvalet and Bengio, 2005)." ></td>
	<td class="line x" title="117:210	Another way to set  corresponds to bootstrapping semi-supervised learning." ></td>
	<td class="line x" title="118:210	First, let (n) be a model learned from the previous round of training." ></td>
	<td class="line x" title="119:210	We then select the top k instances from Dt,u that have the highest prediction confidence." ></td>
	<td class="line x" title="120:210	For these instances, we set k(y) = 1 for y = argmaxyprime p(yprime|xt,uk ; (n)), and k(y) = 0 for all other y. In another word, we select the top k confidently predicted instances, and include these instances together with their predicted labels in the training set." ></td>
	<td class="line x" title="121:210	All other instances in Dt,u are not considered." ></td>
	<td class="line x" title="122:210	In our experiments, we only considered this bootstrapping way of setting ." ></td>
	<td class="line x" title="123:210	3.4 Setting  s, t,l and t,u control the balance among the three sets of instances." ></td>
	<td class="line x" title="124:210	Using standard supervised learning, s and t,l are set proportionally to Cs and Ct,l, that is, each instance is weighted the same whether it is in Ds or in Dt,l, and t,u is set to 0." ></td>
	<td class="line x" title="125:210	Similarly, using standard bootstrapping, t,u is set proportionally to Ct,u, that is, each target instance added to the training set is also weighted the same as a source instance." ></td>
	<td class="line x" title="126:210	In neither case are the target instances emphasize more than source instances." ></td>
	<td class="line x" title="127:210	However, for domain adaptation, we want to focus more on the target domain instances." ></td>
	<td class="line x" title="128:210	So intuitively, we want to make t,l and t,u somehow larger relative to s. As we will show in Section 4, this is indeed beneficial." ></td>
	<td class="line x" title="129:210	In general, the framework provides great flexibility for implementing different adaptation strategies through these instance weighting parameters." ></td>
	<td class="line x" title="130:210	4 Experiments 4.1 Tasks and Data Sets We chose three different NLP tasks to evaluate our instance weighting method for domain adaptation." ></td>
	<td class="line x" title="131:210	The first task is POS tagging, for which we used 268 6166 WSJ sentences from Sections 00 and 01 of Penn Treebank as the source domain data, and 2730 PubMed sentences from the Oncology section of the PennBioIE corpus as the target domain data." ></td>
	<td class="line x" title="132:210	The second task is entity type classification." ></td>
	<td class="line x" title="133:210	The setup is very similar to Daume III and Marcu (2006)." ></td>
	<td class="line x" title="134:210	We assume that the entity boundaries have been correctly identified, and we want to classify the types of the entities." ></td>
	<td class="line x" title="135:210	We used ACE 2005 training data for this task." ></td>
	<td class="line x" title="136:210	For the source domain, we used the newswire collection, which contains 11256 examples, and for the target domains, we used the weblog (WL) collection (5164 examples) and the conversational telephone speech (CTS) collection (4868 examples)." ></td>
	<td class="line x" title="137:210	The third task is personalized spam filtering." ></td>
	<td class="line x" title="138:210	We used the ECML/PKDD 2006 discovery challenge data set." ></td>
	<td class="line x" title="139:210	The source domain contains 4000 spam and ham emails from publicly available sources, and the target domains are three individual users inboxes, each containing 2500 emails." ></td>
	<td class="line x" title="140:210	For each task, we consider two experiment settings." ></td>
	<td class="line x" title="141:210	In the first setting, we assume there are a small number of labeled target instances available." ></td>
	<td class="line x" title="142:210	For POS tagging, we used an additional 300 Oncology sentences as labeled target instances." ></td>
	<td class="line x" title="143:210	For NE typing, we used 500 labeled target instances and 2000 unlabeled target instances for each target domain." ></td>
	<td class="line x" title="144:210	For spam filtering, we used 200 labeled target instances and 1800 unlabeled target instances." ></td>
	<td class="line x" title="145:210	In the second setting, we assume there is no labeled target instance." ></td>
	<td class="line x" title="146:210	We thus used all available target instances for testing in all three tasks." ></td>
	<td class="line x" title="147:210	We used logistic regression as our model of p(y|x;) because it is a robust learning algorithm and widely used." ></td>
	<td class="line x" title="148:210	We now describe three sets of experiments, corresponding to three heuristic ways of setting , t,l and t,u. 4.2 Removing Misleading Source Domain Instances In the first set of experiments, we gradually remove misleading labeled instances from the source domain, using the small number of labeled target instances we have." ></td>
	<td class="line x" title="149:210	We follow the heuristic we described in Section 3.1, which sets the  for the top k misclassified source instances to 0, and the  for all the other source instances to 1." ></td>
	<td class="line x" title="150:210	We also set t,l and t,l to 0 in order to focus only on the effect of removing misleading instances." ></td>
	<td class="line x" title="151:210	We compare with a baseline method which uses all source instances with equal weight but no target instances." ></td>
	<td class="line x" title="152:210	The results are shown in Table 1." ></td>
	<td class="line x" title="153:210	From the table, we can see that in most experiments, removing these predicted misleading examples improved the performance over the baseline." ></td>
	<td class="line x" title="154:210	In some experiments (Oncology, CTS, u00, u01), the largest improvement was achieved when all misclassified source instances were removed." ></td>
	<td class="line x" title="155:210	In the case of weblog NE type classification, however, removing the source instances hurt the performance." ></td>
	<td class="line x" title="156:210	A possible reason for this is that the set of labeled target instances we use is a biased sample from the target domain, and therefore the model trained on these instances is not always a good predictor of misleading source instances." ></td>
	<td class="line x" title="157:210	4.3 Adding Labeled Target Domain Instances with Higher Weights The second set of experiments is to add the labeled target domain instances into the training set." ></td>
	<td class="line x" title="158:210	This corresponds to setting t,l to some non-zero value, but still keeping t,u as 0." ></td>
	<td class="line x" title="159:210	If we ignore the domain difference, then each labeled target instance is weighted the same as a labeled source instance (u,ls = Cu,lCs ), which is what happens in regular supervised learning." ></td>
	<td class="line x" title="160:210	However, based on our theoretical analysis, we can expect the labeled target instances to be more representative of the target domain than the source instances." ></td>
	<td class="line x" title="161:210	We can therefore assign higher weights for the target instances, by adjusting the ratio between t,l and s. In our experiments, we set t,ls = aCt,lCs, where a ranges from 2 to 20." ></td>
	<td class="line x" title="162:210	The results are shown in Table 2." ></td>
	<td class="line x" title="163:210	As shown from the table, adding some labeled target instances can greatly improve the performance for all tasks." ></td>
	<td class="line x" title="164:210	And in almost all cases, weighting the target instances more than the source instances performed better than weighting them equally." ></td>
	<td class="line x" title="165:210	We also tested another setting where we first removed the misleading source examples as we showed in Section 4.2, and then added the labeled target instances." ></td>
	<td class="line x" title="166:210	The results are shown in the last row of Table 2." ></td>
	<td class="line x" title="167:210	However, although both removing misleading source instances and adding labeled 269 POS NE Type Spam k Oncology k CTS k WL k u00 u01 u02 0 0.8630 0 0.7815 0 0.7045 0 0.6306 0.6950 0.7644 4000 0.8675 800 0.8245 600 0.7070 150 0.6417 0.7078 0.7950 8000 0.8709 1600 0.8640 1200 0.6975 300 0.6611 0.7228 0.8222 12000 0.8713 2400 0.8825 1800 0.6830 450 0.7106 0.7806 0.8239 16000 0.8714 3000 0.8825 2400 0.6795 600 0.7911 0.8322 0.8328 all 0.8720 all 0.8830 all 0.6600 all 0.8106 0.8517 0.8067 Table 1: Accuracy on the target domain after removing misleading source domain instances." ></td>
	<td class="line x" title="168:210	POS NE Type Spam method Oncology method CTS WL method u00 u01 u02 Ds only 0.8630 Ds only 0.7815 0.7045 Ds only 0.6306 0.6950 0.7644 Ds + Dt,l 0.9349 Ds + Dt,l 0.9340 0.7735 Ds + Dt,l 0.9572 0.9572 0.9461 Ds + 5Dt,l 0.9411 Ds + 2Dt,l 0.9355 0.7810 Ds + 2Dt,l 0.9606 0.9600 0.9533 Ds + 10Dt,l 0.9429 Ds + 5Dt,l 0.9360 0.7820 Ds + 5Dt,l 0.9628 09611 0.9601 Ds + 20Dt,l 0.9443 Ds + 10Dt,l 0.9355 0.7840 Ds + 10Dt,l 0.9639 0.9628 0.9633 Dprimes + 20Dt,l 0.9422 Dprimes + 10Dt,l 0.8950 0.6670 Dprimes + 10Dt,l 0.9717 0.9478 0.9494 Table 2: Accuracy on the unlabeled target instances after adding the labeled target instances." ></td>
	<td class="line x" title="169:210	target instances work well individually, when combined, the performance in most cases is not as good as when no source instances are removed." ></td>
	<td class="line x" title="170:210	We hypothesize that this is because after we added some labeled target instances with large weights, we already gained a good balance between the source data and the target data." ></td>
	<td class="line x" title="171:210	Further removing source instances would push the emphasis more on the set of labeled target instances, which is only a biased sample of the whole target domain." ></td>
	<td class="line oc" title="172:210	The POS data set and the CTS data set have previously been used for testing other adaptation methods (Daume III and Marcu, 2006; Blitzer et al. , 2006), though the setup there is different from ours." ></td>
	<td class="line o" title="173:210	Our performance using instance weighting is comparable to their best performance (slightly worse for POS and better for CTS)." ></td>
	<td class="line x" title="174:210	4.4 Bootstrapping with Higher Weights In the third set of experiments, we assume that we do not have any labeled target instances." ></td>
	<td class="line x" title="175:210	We tried two bootstrapping methods." ></td>
	<td class="line x" title="176:210	The first is a standard bootstrapping method, in which we gradually added the most confidently predicted unlabeled target instances with their predicted labels to the training set." ></td>
	<td class="line x" title="177:210	Since we believe that the target instances should in general be given more weight because they better represent the target domain than the source instances, in the second method, we gave the added target instances more weight in the objective function." ></td>
	<td class="line x" title="178:210	In particular, we set t,u = s such that the total contribution of the added target instances is equal to that of all the labeled source instances." ></td>
	<td class="line x" title="179:210	We call this second method the balanced bootstrapping method." ></td>
	<td class="line x" title="180:210	Table 3 shows the results." ></td>
	<td class="line x" title="181:210	As we can see, while bootstrapping can generally improve the performance over the baseline where no unlabeled data is used, the balanced bootstrapping method performed slightly better than the standard bootstrapping method." ></td>
	<td class="line x" title="182:210	This again shows that weighting the target instances more is a right direction to go for domain adaptation." ></td>
	<td class="line x" title="183:210	5 Related Work There have been several studies in NLP that address domain adaptation, and most of them need labeled data from both the source domain and the target domain." ></td>
	<td class="line x" title="184:210	Here we highlight a few representative ones." ></td>
	<td class="line x" title="185:210	For generative syntactic parsing, Roark and Bacchiani (2003) have used the source domain data to construct a Dirichlet prior for MAP estimation of the PCFG for the target domain." ></td>
	<td class="line x" title="186:210	Chelba and Acero (2004) use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target data." ></td>
	<td class="line x" title="187:210	Florian et al.(2004) first train a NE tagger on the source domain, and then use the taggers predictions as features for training and testing on the target domain." ></td>
	<td class="line x" title="189:210	The only work we are aware of that directly mod270 POS NE Type Spam method Oncology CTS WL u00 u01 u02 supervised 0.8630 0.7781 0.7351 0.6476 0.6976 0.8068 standard bootstrap 0.8728 0.8917 0.7498 0.8720 0.9212 0.9760 balanced bootstrap 0.8750 0.8923 0.7523 0.8816 0.9256 0.9772 Table 3: Accuracy on the target domain without using labeled target instances." ></td>
	<td class="line x" title="190:210	In balanced bootstrapping, more weights are put on the target instances in the objective function than in standard bootstrapping." ></td>
	<td class="line x" title="191:210	els the different distributions in the source and the target domains is by Daume III and Marcu (2006)." ></td>
	<td class="line x" title="192:210	They assume a truly source domain distribution, a truly target domain distribution, and a general domain distribution." ></td>
	<td class="line x" title="193:210	The source (target) domain data is generated from a mixture of the truly source (target) domain distribution and the general domain distribution." ></td>
	<td class="line x" title="194:210	In contrast, we do not assume such a mixture model." ></td>
	<td class="line x" title="195:210	None of the above methods would work if there were no labeled target instances." ></td>
	<td class="line x" title="196:210	Indeed, all the above methods do not make use of the unlabeled instances in the target domain." ></td>
	<td class="line x" title="197:210	In contrast, our instance weighting framework allows unlabeled target instances to contribute to the model estimation." ></td>
	<td class="line oc" title="198:210	Blitzer et al.(2006) propose a domain adaptation method that uses the unlabeled target instances to infer a good feature representation, which can be regarded as weighting the features." ></td>
	<td class="line x" title="200:210	In contrast, we weight the instances." ></td>
	<td class="line x" title="201:210	The idea of using pt(x)ps(x) to weight instances has been studied in statistics (Shimodaira, 2000), but has not been applied to NLP tasks." ></td>
	<td class="line x" title="202:210	6 Conclusions and Future Work Domain adaptation is a very important problem with applications to many NLP tasks." ></td>
	<td class="line x" title="203:210	In this paper, we formally analyze the domain adaptation problem and propose a general instance weighting framework for domain adaptation." ></td>
	<td class="line x" title="204:210	The framework is flexible to support many different strategies for adaptation." ></td>
	<td class="line x" title="205:210	In particular, it can support adaptation with some target domain labeled instances as well as that without any labeled target instances." ></td>
	<td class="line x" title="206:210	Experiment results on three NLP tasks show that while regular semi-supervised learning methods and supervised learning methods can be applied to domain adaptation without considering domain difference, they do not perform as well as our new method, which explicitly captures domain difference." ></td>
	<td class="line x" title="207:210	Our results also show that incorporating and exploiting more information from the target domain is much more useful than excluding misleading training examples from the source domain." ></td>
	<td class="line x" title="208:210	The framework opens up many interesting future research directions, especially those related to how to more accurately set/estimate those weighting parameters." ></td>
	<td class="line x" title="209:210	Acknowledgments This work was in part supported by the National Science Foundation under award numbers 0425852 and 0428472." ></td>
	<td class="line x" title="210:210	We thank the anonymous reviewers for their valuable comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-1056
Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification
Blitzer, John;Dredze, Mark;Pereira, Fernando C. N.;"></td>
	<td class="line x" title="1:215	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440447, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:215	c2007 Association for Computational Linguistics Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification John Blitzer Mark Dredze Department of Computer and Information Science University of Pennsylvania {blitzer|mdredze|pereria@cis.upenn.edu} Fernando Pereira Abstract Automatic sentiment classification has been extensively studied and applied in recent years." ></td>
	<td class="line x" title="3:215	However, sentiment is expressed differently in different domains, and annotating corporaforeverypossibledomainofinterest is impractical." ></td>
	<td class="line x" title="4:215	We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products." ></td>
	<td class="line n" title="5:215	First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline." ></td>
	<td class="line x" title="6:215	Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another." ></td>
	<td class="line x" title="7:215	This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains." ></td>
	<td class="line x" title="8:215	1 Introduction Sentiment detection and classification has received considerable attention recently (Pang et al. , 2002; Turney, 2002; Goldberg and Zhu, 2004)." ></td>
	<td class="line x" title="9:215	While movie reviews have been the most studied domain, sentiment analysis has extended to a number of new domains, ranging from stock message boards to congressional floor debates (Das and Chen, 2001; Thomas et al. , 2006)." ></td>
	<td class="line x" title="10:215	Research results have been deployed industrially in systems that gauge market reaction and summarize opinion from Web pages, discussion boards, and blogs." ></td>
	<td class="line x" title="11:215	With such widely-varying domains, researchers and engineers who build sentiment classification systems need to collect and curate data for each new domain they encounter." ></td>
	<td class="line x" title="12:215	Even in the case of market analysis, if automatic sentiment classification were to be used across a wide range of domains, the effort to annotate corpora for each domain may become prohibitive, especially since product features change over time." ></td>
	<td class="line x" title="13:215	We envision a scenario in which developers annotate corpora for a small number of domains, train classifiers on those corpora, and then apply them to other similar corpora." ></td>
	<td class="line x" title="14:215	However, this approach raises two important questions." ></td>
	<td class="line x" title="15:215	First, it is well known that trained classifiers lose accuracy when the test data distribution is significantly differentfromthetrainingdatadistribution 1." ></td>
	<td class="line x" title="16:215	Second, itis not clear which notion of domain similarity should be used to select domains to annotate that would be good proxies for many other domains." ></td>
	<td class="line x" title="17:215	We propose solutions to these two questions and evaluate them on a corpus of reviews for four different types of products from Amazon: books, DVDs, electronics, and kitchen appliances2." ></td>
	<td class="line x" title="18:215	First, we show how to extend the recently proposed structural cor1For surveys of recent research on domain adaptation, see the ICML 2006 Workshop on Structural Knowledge Transfer for Machine Learning (http://gameairesearch.uta." ></td>
	<td class="line x" title="19:215	edu/) and the NIPS 2006 Workshop on Learning when test and training inputs have different distribution (http://ida. first.fraunhofer.de/projects/different06/) 2The dataset will be made available by the authors at publication time." ></td>
	<td class="line oc" title="20:215	440 respondence learning (SCL) domain adaptation algorithm (Blitzer et al. , 2006) for use in sentiment classification." ></td>
	<td class="line o" title="21:215	A key step in SCL is the selection of pivot features thatare usedtolink thesourceandtarget domains." ></td>
	<td class="line x" title="22:215	We suggest selecting pivots based not only on their common frequency but also according to their mutual information with the source labels." ></td>
	<td class="line n" title="23:215	For data as diverse as product reviews, SCL can sometimes misalign features, resulting in degradation when we adapt between domains." ></td>
	<td class="line x" title="24:215	In our second extensionweshowhowtocorrectmisalignmentsusing a very small number of labeled instances." ></td>
	<td class="line x" title="25:215	Second, we evaluate the A-distance (Ben-David et al. , 2006) between domains as measure of the loss due to adaptation from one to the other." ></td>
	<td class="line x" title="26:215	The Adistancecanbemeasuredfromunlabeleddata, andit was designed to take into account only divergences which affect classification accuracy." ></td>
	<td class="line x" title="27:215	We show that it correlates well with adaptation loss, indicating that we can use the A-distance to select a subset of domains to label as sources." ></td>
	<td class="line o" title="28:215	In the next section we briefly review SCL and introduce our new pivot selection method." ></td>
	<td class="line x" title="29:215	Section 3 describes datasets and experimental method." ></td>
	<td class="line o" title="30:215	Section 4 gives results for SCL and the mutual information method for selecting pivot features." ></td>
	<td class="line x" title="31:215	Section 5 shows how to correct feature misalignments using a small amount of labeled target domain data." ></td>
	<td class="line x" title="32:215	Section 6 motivates the A-distance and shows that it correlates well with adaptability." ></td>
	<td class="line x" title="33:215	We discuss related work in Section 7 and conclude in Section 8." ></td>
	<td class="line o" title="34:215	2 Structural Correspondence Learning Before reviewing SCL, we give a brief illustrative example." ></td>
	<td class="line x" title="35:215	Suppose that we are adapting from reviews of computers to reviews of cell phones." ></td>
	<td class="line x" title="36:215	While many of the features of a good cell phone review are the same as a computer review  the words excellent and awful for example  many words are totally new, like reception." ></td>
	<td class="line x" title="37:215	At the same time, many features which were useful for computers, such as dual-core are no longer useful for cell phones." ></td>
	<td class="line x" title="38:215	Our key intuition is that even when good-quality reception and fast dual-core are completely distinct for each domain, if they both have high correlation with excellent and low correlation with awful on unlabeled data, then we can tentatively align them." ></td>
	<td class="line x" title="39:215	After learning a classifier for computer reviews, when we see a cell-phone feature like goodquality reception, we know it should behave in a roughly similar manner to fast dual-core." ></td>
	<td class="line o" title="40:215	2.1 Algorithm Overview Given labeled data from a source domain and unlabeled data from both source and target domains, SCLfirstchoosesasetofmpivotfeatureswhichoccur frequently in both domains." ></td>
	<td class="line oc" title="41:215	Then, it models the correlations between the pivot features and all other features by training linear pivot predictors to predict occurrences of each pivot in the unlabeled data from both domains (Ando and Zhang, 2005; Blitzer et al. , 2006)." ></td>
	<td class="line x" title="42:215	The lscriptth pivot predictor is characterized by its weight vector wlscript; positive entries in that weight vector mean that a non-pivot feature (like fast dualcore) is highly correlated with the corresponding pivot (like excellent)." ></td>
	<td class="line x" title="43:215	The pivot predictor column weight vectors can be arranged into a matrix W = [wlscript]nlscript=1." ></td>
	<td class="line x" title="44:215	Let   Rkd be the top k left singular vectors of W (here d indicatesthetotalnumberoffeatures)." ></td>
	<td class="line x" title="45:215	Thesevectorsare the principal predictors for our weight space." ></td>
	<td class="line x" title="46:215	If we chose our pivot features well, then we expect these principal predictors to discriminate among positive and negative words in both domains." ></td>
	<td class="line x" title="47:215	At training and test time, suppose we observe a feature vector x. We apply the projection x to obtain k new real-valued features." ></td>
	<td class="line x" title="48:215	Now we learn a predictor for the augmented instance x,x." ></td>
	<td class="line x" title="49:215	If  contains meaningful correspondences, then the predictor which uses  will perform well in both source and target domains." ></td>
	<td class="line o" title="50:215	2.2 Selecting Pivots with Mutual Information The efficacy of SCL depends on the choice of pivot features." ></td>
	<td class="line oc" title="51:215	For the part of speech tagging problem studied by Blitzer et al.(2006), frequently-occurring words in both domains were good choices, since they often correspond to function words such as prepositions and determiners, which are good indicators of parts of speech." ></td>
	<td class="line x" title="53:215	This is not the case for sentiment classification, however." ></td>
	<td class="line x" title="54:215	Therefore, we require that pivot features also be good predictors of the source label." ></td>
	<td class="line x" title="55:215	Among those features, we then choose the ones with highest mutual information to the source label." ></td>
	<td class="line o" title="56:215	Table 1 shows the set-symmetric 441 SCL, not SCL-MI SCL-MI, not SCL book one <num> so all a must a wonderful loved it very about they like weak dont waste awful good when highly recommended and easy Table 1: Top pivots selected by SCL, but not SCLMI (left) and vice-versa (right) differencesbetweenthetwomethodsforpivotselectionwhenadaptingaclassifierfrombookstokitchen appliances." ></td>
	<td class="line x" title="57:215	Wereferthroughouttherest ofthiswork to our method for selecting pivots as SCL-MI." ></td>
	<td class="line x" title="58:215	3 Dataset and Baseline We constructed a new dataset for sentiment domain adaptation by selecting Amazon product reviews for fourdifferentproducttypes: books, DVDs, electronics and kitchen appliances." ></td>
	<td class="line x" title="59:215	Each review consists of a rating (0-5 stars), a reviewer name and location, a product name, a review title and date, and the review text." ></td>
	<td class="line x" title="60:215	Reviews with rating > 3 were labeled positive, those with rating < 3 were labeled negative, and the rest discarded because their polarity was ambiguous." ></td>
	<td class="line x" title="61:215	After this conversion, we had 1000 positive and 1000 negative examples for each domain, the same balanced composition as the polarity dataset (Pang et al. , 2002)." ></td>
	<td class="line x" title="62:215	In addition to the labeled data, we included between 3685 (DVDs) and 5945 (kitchen)instancesofunlabeleddata." ></td>
	<td class="line x" title="63:215	Thesizeofthe unlabeled data was limited primarily by the number of reviews we could crawl and download from the Amazon website." ></td>
	<td class="line x" title="64:215	Since we were able to obtain labels for all of the reviews, we also ensured that they were balanced between positive and negative examples, as well." ></td>
	<td class="line x" title="65:215	While the polarity dataset is a popular choice in the literature, we were unable to use it for our task." ></td>
	<td class="line x" title="66:215	Our method requires many unlabeled reviews and despite a large number of IMDB reviews available online, the extensive curation requirements made preparing a large amount of data difficult 3." ></td>
	<td class="line x" title="67:215	For classification, we use linear predictors on unigram and bigram features, trained to minimize the Huber loss with stochastic gradient descent (Zhang, 3For a description of the construction of the polarity dataset, see http://www.cs.cornell.edu/people/ pabo/movie-review-data/." ></td>
	<td class="line x" title="68:215	2004)." ></td>
	<td class="line x" title="69:215	On the polarity dataset, this model matches the results reported by Pang et al.(2002)." ></td>
	<td class="line o" title="71:215	When we reportresultswithSCLandSCL-MI,werequirethat pivots occur in more than five documents in each domain." ></td>
	<td class="line x" title="72:215	Wesetk,thenumberofsingularvectorsofthe weight matrix, to 50." ></td>
	<td class="line o" title="73:215	4 Experiments with SCL and SCL-MI Each labeled dataset was split into a training set of 1600 instances and a test set of 400 instances." ></td>
	<td class="line x" title="74:215	All the experiments use a classifier trained on the training set of one domain and tested on the test set of a possibly different domain." ></td>
	<td class="line x" title="75:215	The baseline is a linear classifier trained without adaptation, while the gold standard is an in-domain classifier trained on the same domain as it is tested." ></td>
	<td class="line x" title="76:215	Figure 1 gives accuracies for all pairs of domain adaptation." ></td>
	<td class="line x" title="77:215	The domains are ordered clockwise from the top left: books, DVDs, electronics, and kitchen." ></td>
	<td class="line x" title="78:215	For each set of bars, the first letter is the source domain and the second letter is the target domain." ></td>
	<td class="line x" title="79:215	The thick horizontal bars are the accuracies of the in-domain classifiers for these domains." ></td>
	<td class="line x" title="80:215	Thus the first set of bars shows that the baseline achieves 72.8% accuracy adapting from DVDs to books." ></td>
	<td class="line x" title="81:215	SCL-MI achieves 79.7% and the in-domain gold standard is 80.4%." ></td>
	<td class="line x" title="82:215	We say that the adaptation loss for the baseline model is 7.6% and the adaptationlossfortheSCL-MImodelis0.7%." ></td>
	<td class="line x" title="83:215	Therelative reduction in error due to adaptation of SCL-MI for this test is 90.8%." ></td>
	<td class="line x" title="84:215	We can observe from these results that there is a rough grouping of our domains." ></td>
	<td class="line x" title="85:215	Books and DVDs are similar, as are kitchen appliances and electronics, but the two groups are different from one another." ></td>
	<td class="line x" title="86:215	Adapting classifiers from books to DVDs, for instance, is easier than adapting them from books to kitchen appliances." ></td>
	<td class="line x" title="87:215	We note that when transferring from kitchen to electronics, SCL-MI actually outperforms the in-domain classifier." ></td>
	<td class="line x" title="88:215	This is possiblesincetheunlabeleddatamaycontaininformation that the in-domain classifier does not have access to." ></td>
	<td class="line x" title="89:215	At the beginning of Section 2 we gave examples of how features can change behavior across domains." ></td>
	<td class="line x" title="90:215	The first type of behavior is when predictive features from the source domain are not predictive or do not appear in the target domain." ></td>
	<td class="line o" title="91:215	The second is 442 65 70 75 80 85 90 D->BE->BK->BB->DE->DK->D baselineSCLSCL-MIbooks 72.8 76.8 79.7 70.7 75.475.4 70.9 66.1 68.6 80.4 82.4 77.2 74.0 75.8 70.6 74.3 76.2 72.7 75.4 76.9 dvd 65 70 75 80 85 90 B->ED->EK->EB->KD->KE->K electronics kitchen 70.8 77.5 75.9 73.0 74.1 74.1 82.7 83.7 86.8 84.4 87.7 74.5 78.778.9 74.0 79.4 81.484.0 84.4 85.9 Figure 1: Accuracy results for domain adaptation between all pairs using SCL and SCL-MI." ></td>
	<td class="line x" title="92:215	Thick black lines are the accuracies of in-domain classifiers." ></td>
	<td class="line o" title="93:215	domain\polarity negative positive books plot <num> pages predictable reader grisham engaging reading this page <num> must read fascinating kitchen the plastic poorly designed excellent product espresso leaking awkward to defective are perfect years now a breeze Table 2: Correspondences discovered by SCL for books and kitchen appliances." ></td>
	<td class="line x" title="94:215	The top row shows features that only appear in books and the bottom features that only appear in kitchen appliances." ></td>
	<td class="line x" title="95:215	The left and right columns show negative and positive features in correspondence, respectively." ></td>
	<td class="line x" title="96:215	when predictive features from the target domain do not appear in the source domain." ></td>
	<td class="line o" title="97:215	To show how SCL deals with those domain mismatches, we look at the adaptation from book reviews to reviews of kitchen appliances." ></td>
	<td class="line x" title="98:215	We selected the top 1000 most informative features in both domains." ></td>
	<td class="line x" title="99:215	In both cases, between 85 and 90% of the informative features from one domain were not among the most informative of the other domain4." ></td>
	<td class="line p" title="100:215	SCL addresses both of these issues simultaneously by aligning features from the two domains." ></td>
	<td class="line x" title="101:215	4There is a third type, features which are positive in one domain but negative in another, but they appear very infrequently in our datasets." ></td>
	<td class="line x" title="102:215	Table 2 illustrates one row of the projection matrix for adapting from books to kitchen appliances; the features on each row appear only in the corresponding domain." ></td>
	<td class="line x" title="103:215	A supervised classifier trained on book reviews cannot assign weight to the kitchen features in the second row of table 2." ></td>
	<td class="line o" title="104:215	In contrast, SCL assigns weight to these features indirectly through the projection matrix." ></td>
	<td class="line x" title="105:215	When we observe the feature predictable with a negative book review, we update parameters corresponding to the entire projection, including the kitchen-specific features poorly designed and awkward to." ></td>
	<td class="line n" title="106:215	While some rows of the projection matrix  are 443 useful for classification, SCL can also misalign features." ></td>
	<td class="line x" title="107:215	This causes problems when a projection is discriminative in the source domain but not in the target." ></td>
	<td class="line x" title="108:215	This is the case for adapting from kitchen appliances to books." ></td>
	<td class="line x" title="109:215	Since the book domain is quite broad, many projections in books model topic distinctions such as between religious and political books." ></td>
	<td class="line x" title="110:215	These projections, which are uninformative as to the target label, are put into correspondence with the fewer discriminating projections in the much narrower kitchen domain." ></td>
	<td class="line x" title="111:215	When we adapt from kitchen to books, we assign weight to these uninformative projections, degrading target classification accuracy." ></td>
	<td class="line x" title="112:215	5 Correcting Misalignments We now show how to use a small amount of target domain labeled data to learn to ignore misaligned projections from SCL-MI." ></td>
	<td class="line x" title="113:215	Using the notation of AndoandZhang(2005),wecanwritethesupervised training objective of SCL on the source domain as minw,v summationdisplay i Lparenleftbigwprimexi +vprimexi,yiparenrightbig+ ||w||2 + ||v||2, where y is the label." ></td>
	<td class="line x" title="114:215	The weight vector w  Rd weighs the original features, while v  Rk weighs the projected features." ></td>
	<td class="line oc" title="115:215	Ando and Zhang (2005) and Blitzeretal.(2006)suggest = 104, = 0,which we have used in our results so far." ></td>
	<td class="line x" title="116:215	Suppose now that we have trained source model weight vectors ws and vs. A small amount of target domain data is probably insufficient to significantly change w, but we can correct v, which is much smaller." ></td>
	<td class="line oc" title="117:215	We augment each labeled target instance xj with the label assigned by the source domain classifier (Florian et al. , 2004; Blitzer et al. , 2006)." ></td>
	<td class="line x" title="118:215	Then we solve minw,vsummationtextj L(wprimexj +vprimexj,yj) + ||w||2 +||vvs||2." ></td>
	<td class="line x" title="119:215	Sincewedontwanttodeviatesignificantlyfromthe source parameters, we set  =  = 101." ></td>
	<td class="line x" title="120:215	Figure 2 shows the corrected SCL-MI model using 50 target domain labeled instances." ></td>
	<td class="line x" title="121:215	We chose this number since we believe it to be a reasonable amount for a single engineer to label with minimal effort." ></td>
	<td class="line x" title="122:215	For reasons of space, for each target domain dom \ model base base scl scl-mi scl-mi +targ +targ books 8.9 9.0 7.4 5.8 4.4 dvd 8.9 8.9 7.8 6.1 5.3 electron 8.3 8.5 6.0 5.5 4.8 kitchen 10.2 9.9 7.0 5.6 5.1 average 9.1 9.1 7.1 5.8 4.9 Table 3: For each domain, we show the loss due to transfer for each method, averaged over all domains." ></td>
	<td class="line x" title="123:215	The bottom row shows the average loss over all runs." ></td>
	<td class="line x" title="124:215	we show adaptation from only the two domains on which SCL-MI performed the worst relative to the supervised baseline." ></td>
	<td class="line x" title="125:215	For example, the book domain shows only results from electronics and kitchen, but not DVDs." ></td>
	<td class="line o" title="126:215	As a baseline, we used the label of the sourcedomainclassifierasafeatureinthetarget, but did not use any SCL features." ></td>
	<td class="line x" title="127:215	We note that the baseline is very close to just using the source domain classifier, because with only 50 target domain instances we do not have enough data to relearn all of the parameters inw." ></td>
	<td class="line x" title="128:215	As we can see, though, relearning the 50 parameters in v is quite helpful." ></td>
	<td class="line x" title="129:215	The corrected model always improves over the baseline for every possible transfer, including those not shown in the figure." ></td>
	<td class="line x" title="130:215	The idea of using the regularizer of a linear model to encourage the target parameters to be close to the source parameters has been used previously in domain adaptation." ></td>
	<td class="line x" title="131:215	In particular, Chelba and Acero (2004) showed how this technique can be effective for capitalization adaptation." ></td>
	<td class="line x" title="132:215	The major difference between our approach and theirs is that we only penalize deviation from the source parameters for the weights v of projected features, while they work with the weights of the original features only." ></td>
	<td class="line x" title="133:215	For our small amount of labeled target data, attempting to penalize w using ws performed no better than our baseline." ></td>
	<td class="line x" title="134:215	Because we only need to learn to ignore projections that misalign features, we can make much better use of our labeled data by adapting only 50 parameters, rather than 200,000." ></td>
	<td class="line x" title="135:215	Table 3 summarizes the results of sections 4 and 5." ></td>
	<td class="line x" title="136:215	Structural correspondence learning reduces the error due to transfer by 21%." ></td>
	<td class="line x" title="137:215	Choosing pivots by mutual information allows us to further reduce the error to 36%." ></td>
	<td class="line x" title="138:215	Finally, by adding 50 instances of target domain data and using this to correct the misaligned projections, we achieve an average relative 444 65 70 75 80 85 90 E->BK->BB->DK->DB->ED->EB->KE->K base+50-targSCL-MI+50-targ books kitchen 70.9 76.0 70.7 76.8 78.5 72.7 80.4 87.7 76.6 70.8 76.6 73.0 77.9 74.3 80.7 84.3 dvd electronics 82.4 84.4 73.2 85.9 Figure 2: Accuracy results for domain adaptation with 50 labeled target domain instances." ></td>
	<td class="line x" title="139:215	reduction in error of 46%." ></td>
	<td class="line x" title="140:215	6 Measuring Adaptability Sections 2-5 focused on how to adapt to a target domain when you had a labeled source dataset." ></td>
	<td class="line x" title="141:215	We now take a step back to look at the problem of selecting source domain data to label." ></td>
	<td class="line x" title="142:215	We study a setting where an engineer knows roughly her domains of interest but does not have any labeled data yet." ></td>
	<td class="line x" title="143:215	In that case, she can ask the question Which sources should I label to obtain the best performance over all my domains? On our product domains, for example, if we are interested in classifying reviews of kitchen appliances, we know from sections 4-5 that it would be foolish to label reviews of books or DVDs rather than electronics." ></td>
	<td class="line o" title="144:215	Here we show how to select source domains using only unlabeled data and the SCL representation." ></td>
	<td class="line o" title="145:215	6.1 The A-distance We propose to measure domain adaptability by using the divergence of two domains after the SCL projection." ></td>
	<td class="line x" title="146:215	We can characterize domains by their induced distributions on instance space: the more different the domains, the more divergent the distributions." ></td>
	<td class="line x" title="147:215	Here we make use of the A-distance (BenDavid et al. , 2006)." ></td>
	<td class="line x" title="148:215	The key intuition behind the A-distance is that while two domains can differ in arbitrary ways, we are only interested in the differences that affect classification accuracy." ></td>
	<td class="line x" title="149:215	Let A be the family of subsets of Rk corresponding to characteristic functions of linear classifiers (sets on which a linear classifier returns positive value)." ></td>
	<td class="line x" title="150:215	ThentheAdistancebetweentwoprobability distributions is dA(D,Dprime) = 2 sup AA |PrD [A]  PrDprime [A]| . That is, we find the subset in A on which the distributions differ the most in the L1 sense." ></td>
	<td class="line x" title="151:215	Ben-David et al.(2006) show that computing the A-distance for a finite sample is exactly the problem of minimizing the empirical risk of a classifier that discriminatesbetweeninstancesdrawnfromD andinstances drawn from Dprime." ></td>
	<td class="line x" title="153:215	This is convenient for us, since it allows us to use classification machinery to compute the A-distance." ></td>
	<td class="line x" title="154:215	6.2 Unlabeled Adaptability Measurements We follow Ben-David et al.(2006) and use the Huber loss as a proxy for the A-distance." ></td>
	<td class="line o" title="156:215	Our procedure is as follows: Given two domains, we compute the SCL representation." ></td>
	<td class="line x" title="157:215	Then we create a data set where each instance x is labeled with the identity of the domain from which it came and train a linear classifier." ></td>
	<td class="line x" title="158:215	For each pair of domains we compute the empirical average per-instance Huber loss, subtract it from 1, and multiply the result by 100." ></td>
	<td class="line x" title="159:215	We refer to this quantity as the proxy A-distance." ></td>
	<td class="line x" title="160:215	When it is 100, the two domains are completely distinct." ></td>
	<td class="line x" title="161:215	When it is 0, the two domains are indistinguishable using a linear classifier." ></td>
	<td class="line x" title="162:215	Figure 3 is a correlation plot between the proxy A-distance and the adaptation error." ></td>
	<td class="line x" title="163:215	Suppose we wantedtolabeltwodomainsoutofthefourinsucha 445 0 2 4 6 8 10 12 14 6065707580859095100 Proxy A-distance Adaptation Loss EK BD DE DK BE, BK Figure 3: The proxy A-distance between each domain pair plotted against the average adaptation loss of as measured by our baseline system." ></td>
	<td class="line x" title="164:215	Each pair of domains is labeled by their first letters: EK indicates the pair electronics and kitchen." ></td>
	<td class="line x" title="165:215	way asto minimizeour erroron all thedomains." ></td>
	<td class="line x" title="166:215	Using the proxy A-distance as a criterion, we observe that we would choose one domain from either books or DVDs, but not both, since then we would not be abletoadequatelycoverelectronicsorkitchenappliances." ></td>
	<td class="line x" title="167:215	Similarly we would also choose one domain fromeitherelectronicsorkitchenappliances, butnot both." ></td>
	<td class="line x" title="168:215	7 Related Work Sentiment classification has advanced considerably since the work of Pang et al.(2002), which we use as our baseline." ></td>
	<td class="line x" title="170:215	Thomas et al.(2006) use discourse structurepresentincongressionalrecordstoperform more accurate sentiment classification." ></td>
	<td class="line x" title="172:215	Pang and Lee (2005) treat sentiment analysis as an ordinal ranking problem." ></td>
	<td class="line x" title="173:215	In our work we only show improvement for the basic model, but all of these new techniques also make use of lexical features." ></td>
	<td class="line x" title="174:215	Thus webelievethatouradaptationmethodscouldbealso applied to those more refined models." ></td>
	<td class="line x" title="175:215	While work on domain adaptation for sentiment classifiers is sparse, it is worth noting that other researchers have investigated unsupervised and semisupervised methods for domain adaptation." ></td>
	<td class="line x" title="176:215	The work most similar in spirit to ours that of Turney (2002)." ></td>
	<td class="line x" title="177:215	He used the difference in mutual information with two human-selected features (the words excellent and poor) to score features in a completely unsupervised manner." ></td>
	<td class="line x" title="178:215	Then he classified documents according to various functions of these mutual information scores." ></td>
	<td class="line x" title="179:215	We stress that our method improves a supervised baseline." ></td>
	<td class="line x" title="180:215	While we do not have a direct comparison, we note that Turney (2002) performs worse on movie reviews than on his other datasets, the same type of data as the polarity dataset." ></td>
	<td class="line x" title="181:215	We also note the work of Aue and Gamon (2005), who performed a number of empirical tests on domain adaptation of sentiment classifiers." ></td>
	<td class="line x" title="182:215	Most of these tests were unsuccessful." ></td>
	<td class="line x" title="183:215	We briefly note their results on combining a number of source domains." ></td>
	<td class="line x" title="184:215	They observed that source domains closer to the target helped more." ></td>
	<td class="line x" title="185:215	In preliminary experiments we confirmed these results." ></td>
	<td class="line x" title="186:215	Adding more labeled data always helps, but diversifying training data does not." ></td>
	<td class="line x" title="187:215	When classifying kitchen appliances, for any fixed amount of labeled data, it is always better to draw from electronics as a source than use some combination of all three other domains." ></td>
	<td class="line x" title="188:215	Domain adaptation alone is a generally wellstudied area, and we cannot possibly hope to cover all of it here." ></td>
	<td class="line nc" title="189:215	As we noted in Section 5, we are able to significantly outperform basic structural correspondence learning (Blitzer et al. , 2006)." ></td>
	<td class="line oc" title="190:215	We also note that while Florian et al.(2004) and Blitzer et al.(2006) observe that including the label of a source classifier asa featureon smallamounts of target data tends to improve over using either the source alone or the target alone, we did not observe that for our data." ></td>
	<td class="line x" title="193:215	We believe the most important reason for this is that they explore structured prediction problems, where labels of surrounding words from the source classifier may be very informative, even if the current label is not." ></td>
	<td class="line x" title="194:215	In contrast our simple binary predictionproblemdoesnotexhibitsuchbehavior." ></td>
	<td class="line x" title="195:215	This may also be the reason that the model of Chelba and Acero (2004) did not aid in adaptation." ></td>
	<td class="line oc" title="196:215	Finally we note that while Blitzer et al.(2006) did combine SCL with labeled target domain data, they only compared using the label of SCL or non-SCL source classifiers as features, following the work of Florian et al.(2004)." ></td>
	<td class="line n" title="199:215	By only adapting the SCLrelated part of the weight vector v, we are able to make better use of our small amount of unlabeled data than these previous techniques." ></td>
	<td class="line x" title="200:215	446 8 Conclusion Sentiment classification has seen a great deal of attention." ></td>
	<td class="line x" title="201:215	Its application to many different domains of discourse makes it an ideal candidate for domain adaptation." ></td>
	<td class="line x" title="202:215	This work addressed two important questions of domain adaptation." ></td>
	<td class="line nc" title="203:215	First, we showed that for a given source and target domain, we can significantly improve for sentiment classification the structural correspondence learning model of Blitzer et al.(2006)." ></td>
	<td class="line x" title="205:215	We chose pivot features using not only common frequency among domains but also mutual information with the source labels." ></td>
	<td class="line x" title="206:215	We also showed how to correct structural correspondence misalignments by using a small amount of labeled target domain data." ></td>
	<td class="line x" title="207:215	Second, we provided a method for selecting those source domains most likely to adapt well to given target domains." ></td>
	<td class="line x" title="208:215	The unsupervised A-distance measure of divergence between domains correlates well with loss due to adaptation." ></td>
	<td class="line x" title="209:215	Thus we can use the Adistance to select source domains to label which will give low target domain error." ></td>
	<td class="line x" title="210:215	In the future, we wish to include some of the more recent advances in sentiment classification, as well as addressing the more realistic problem of ranking." ></td>
	<td class="line x" title="211:215	We are also actively searching for a larger and morevariedsetofdomainsonwhichtotestourtechniques." ></td>
	<td class="line x" title="212:215	Acknowledgements We thank Nikhil Dinesh for helpful advice throughout the course of this work." ></td>
	<td class="line x" title="213:215	This material is based upon work partially supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No." ></td>
	<td class="line x" title="214:215	NBCHD03001." ></td>
	<td class="line x" title="215:215	Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA or the Department of Interior-National BusinessCenter (DOI-NBC)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-2202
Evaluating Impact of Re-training a Lexical Disambiguation Model on Domain Adaptation of an HPSG Parser
Hara, Tadayoshi;Miyao, Yusuke;Tsujii, Jun'ichi;"></td>
	<td class="line x" title="1:258	Proceedings of the 10th Conference on Parsing Technologies, pages 1122, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:258	c2007 Association for Computational Linguistics Evaluating Impact of Re-training a Lexical Disambiguation Model on Domain Adaptation of an HPSG Parser Tadayoshi HaraBD Yusuke MiyaoBD Junichi TsujiiBDBNBEBNBF BDDepartment of Computer Science, University of Tokyo Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan BESchool of Computer Science, University of Manchester POBox 88, Sackville St, MANCHESTER M60 1QD, UK BFNaCTeM(National Center for Text Mining) Manchester Interdisciplinary Biocentre, University of Manchester 131 Princess St, MANCHESTER M1 7DN, UK E-mail: CUharasan, yusuke, tsujiiCV@is.s.u-tokyo.ac.jp Abstract This paper describes an effective approach to adapting an HPSG parser trained on the Penn Treebank to a biomedical domain." ></td>
	<td class="line x" title="3:258	In this approach, we train probabilities of lexical entry assignments to words in a target domain and then incorporate them into the original parser." ></td>
	<td class="line x" title="4:258	Experimental results show that this method can obtain higher parsing accuracy than previous work on domain adaptation for parsing the same data." ></td>
	<td class="line x" title="5:258	Moreover, the results show that the combination of the proposed method and the existing method achieves parsing accuracy that is as high as that of an HPSG parser retrained from scratch, but with much lower training cost." ></td>
	<td class="line x" title="6:258	We also evaluated our method in the Brown corpus to show the portability of our approach in another domain." ></td>
	<td class="line x" title="7:258	1 Introduction Domain portability is an important aspect of the applicability of NLP tools to practical tasks." ></td>
	<td class="line x" title="8:258	Therefore, domain adaptation methods have recently been proposed in several NLP areas, e.g., word sense disambiguation (Chan and Ng, 2006), statistical parsing (Lease and Charniak, 2005; McClosky et al. , 2006), and lexicalized-grammar parsing (Johnson and Riezler, 2000; Hara et al. , 2005)." ></td>
	<td class="line x" title="9:258	Their aim was to re-train a probabilistic model for a new domain at low cost, and more or less successfully improved the accuracy for the domain." ></td>
	<td class="line x" title="10:258	In this paper, we propose a method for adapting an HPSG parser (Miyao and Tsujii, 2002; Ninomiya et al. , 2006) trained on the WSJ section of the Penn Treebank (Marcus et al. , 1994) to a biomedical domain." ></td>
	<td class="line x" title="11:258	Our method re-trains a probabilistic model of lexical entry assignments to words in a target domain, and incorporates it into the original parser." ></td>
	<td class="line x" title="12:258	The model of lexical entry assignments is a loglinear model re-trained with machine learning features only of word n-grams." ></td>
	<td class="line x" title="13:258	Hence, the cost for the re-training is much lower than the cost of training the entire disambiguation model from scratch." ></td>
	<td class="line x" title="14:258	In the experiments, we used an HPSG parser originally trained with the Penn Treebank, and evaluated a disambiguation model re-trained with the GENIA treebank (Kim et al. , 2003), which consists of abstracts of biomedical papers." ></td>
	<td class="line x" title="15:258	We varied the size of a training corpus, and measured the transition of the parsing accuracy and the cost required for parameter estimation." ></td>
	<td class="line x" title="16:258	For comparison, we also examined other possible approaches to adapting the same parser." ></td>
	<td class="line x" title="17:258	In addition, we applied our approach to the Brown corpus (Kucera and Francis, 1967) in order to examine portability of our approach." ></td>
	<td class="line x" title="18:258	The experimental results revealed that by simply re-training the probabilistic model of lexical entry assignments we achieve higher parsing accuracy than with a previously proposed adaptation method." ></td>
	<td class="line x" title="19:258	In addition, combined with the existing adaptation method, our approach achieves accuracy as high as that obtained by re-training the original parser from scratch, but with much lower training cost." ></td>
	<td class="line x" title="20:258	In this paper, we report these experimental results in detail, and discuss how disambiguation models of lexical entry assignments contribute to domain adaptation." ></td>
	<td class="line x" title="21:258	In recent years, it has been shown that lexical in11 formation plays a very important role for high accuracy of lexicalized grammar parsing." ></td>
	<td class="line x" title="22:258	Bangalore and Joshi (1999) indicated that, correct disambiguation with supertagging, i.e., assignment of lexical entries before parsing, enabled effective LTAG (Lexicalized Tree-Adjoining Grammar) parsing." ></td>
	<td class="line x" title="23:258	Clark and Curran (2004a) showed that supertagging reduced cost for training and execution of a CCG (Combinatory Categorial Grammar) parser while keeping accuracy." ></td>
	<td class="line x" title="24:258	Clark and Curran (2006) showed that a CCG parser trained on data derived from lexical category sequences alone was only slightly less accurate than one trained on complete dependency structures." ></td>
	<td class="line x" title="25:258	Ninomiya et al.(2006) also succeeded in significantly improving speed and accuracy of HPSG parsing by using supertagging probabilities." ></td>
	<td class="line x" title="27:258	These results indicate that the probability of lexical entry assignments is essential for parse disambiguation." ></td>
	<td class="line x" title="28:258	Such usefulness of lexical information has also been shown for domain adaptation methods." ></td>
	<td class="line x" title="29:258	Lease and Charniak (2005) showed how existing domainspecific lexical resources on a target domain may be leveraged to augment PTB-training: part-of-speech tags, dictionary collocations, and named-entities." ></td>
	<td class="line x" title="30:258	Our findings basically follow the above results." ></td>
	<td class="line x" title="31:258	The contribution of this paper is to provide empirical results of the relationships among domain variation, probability of lexical entry assignment, training data size, and training cost." ></td>
	<td class="line x" title="32:258	In particular, this paper empirically shows how much in-domain corpus is required for satisfiable performance." ></td>
	<td class="line x" title="33:258	In Section 2, we introduce an HPSG parser and describe an existing method for domain adaptation." ></td>
	<td class="line x" title="34:258	In Section 3, we show our methods of re-training a lexical disambiguation model and incorporating it into the original model." ></td>
	<td class="line x" title="35:258	In Section 4, we examine our method through experiments on the GENIA treebank." ></td>
	<td class="line x" title="36:258	In Section 5, we examine the portability of our method through experiments on the Brown corpus." ></td>
	<td class="line x" title="37:258	In Section 6, we showed several recent researches related to domain adaptation." ></td>
	<td class="line x" title="38:258	2 An HPSG Parser HPSG (Pollard and Sag, 1994) is a syntactic theory based on lexicalized grammar formalism." ></td>
	<td class="line x" title="39:258	In HPSG, a small number of grammar rules describe general construction rules, and a large number of HEAD noun SUBCAT <> HEAD verb SUBCAT <verb> HEAD verb SUBCAT <noun> Grammar Rule 3 1 Unification HEAD SUBCAT < > 1 2 HEAD SUBCAT < > 3 2 HEAD SUBCAT < > HEAD noun SUBCAT <> HEAD verb SUBCAT <verb> HEAD verb SUBCAT <noun> John has come HEAD verb SUBCAT <noun> HEAD noun SUBCAT <> HEAD verb SUBCAT <verb> HEAD verb SUBCAT <noun> Lexical Entries John has come John has come Figure 1: Parsing a sentence John has come. HEAD verb SUBCAT <noun> HEAD noun SUBCAT <> HEAD verb SUBCAT <verb> HEAD verb SUBCAT <noun> John has come HEAD verb SUBCAT <> Figure 2: An HPSG parse tree for a sentence John has come. lexical entries express word-specific characteristics." ></td>
	<td class="line x" title="40:258	The structures of sentences are explained using combinations of grammar rules and lexical entries." ></td>
	<td class="line x" title="41:258	Figure 1 shows an example of HPSG parsing of the sentence John has come. First, as shown at the top of the figure, an HPSG parser assigns a lexical entry to each word in this sentence." ></td>
	<td class="line x" title="42:258	Next, a grammar rule is assigned and applied to lexical entries." ></td>
	<td class="line x" title="43:258	At the middle of this figure, the grammar rule is applied to the lexical entries for has and come. We then obtain the structure represented at the bottom of the figure." ></td>
	<td class="line x" title="44:258	After that, the application of grammar rules is done iteratively, and then we can finally obtain the parse tree as is shown in Figure 2." ></td>
	<td class="line x" title="45:258	In practice, since two or more parse candidates can be given for one sentence, a disambiguation model gives probabilities to these candidates, and a candidate given the highest probability is then chosen as a correct parse." ></td>
	<td class="line x" title="46:258	12 The HPSG parser used in this study is Ninomiya et al.(2006), which is based on Enju (Miyao and Tsujii, 2005)." ></td>
	<td class="line x" title="48:258	Lexical entries of Enju were extracted from the Penn Treebank (Marcus et al. , 1994), which consists of sentences collected from The Wall Street Journal (Miyao et al. , 2004)." ></td>
	<td class="line x" title="49:258	The disambiguation model of Enju was trained on the same treebank." ></td>
	<td class="line x" title="50:258	The disambiguation model of Enju is based on a feature forest model (Miyao and Tsujii, 2002), which is a log-linear model (Berger et al. , 1996) on packed forest structure." ></td>
	<td class="line x" title="51:258	The probability, D4 BX B4D8CYDBB5, of producing the parse result D8 for a given sentence DB BP CWDB BD BNBMBMBMBNDB D9 CX is defined as D4 BX B4D8CYDBB5 BP BD CI D7 CH CX D4 D0CTDC B4D0 CX CYDBBNCXB5 A1 D5 D7DDD2 B4D8CYD0B5BN CI D7 BP CG D8BECCB4DBB5 CH CX D4 D0CTDC B4D0 CX CYDBBNCXB5 A1 D5 D7DDD2 B4D8CYD0B5 where D0 BP CWD0 BD BNBMBMBMBND0 D9 CX is a list of lexical entries assigned to DB, D4 D0CTDC B4D0 CX CYDBBNCXB5 is a probabilistic model giving the probability that lexical entry D0 CX is assigned to word DB CX, D5 D7DDD2 B4D8CYD0B5 is an unnormalized log-linear model of tree construction and gives the possibility that parse candidate D8 is produced from lexical entries D0, and CCB4DBB5 is a set of parse candidates assigned to DB." ></td>
	<td class="line x" title="52:258	With a treebank of a target domain as training data, model parameters of D4 D0CTDC and D5 D7DDD2 are estimated so as to maximize the log-likelihood of the training data." ></td>
	<td class="line x" title="53:258	Probabilistic model D4 D0CTDC is defined as a log-linear model as follows." ></td>
	<td class="line x" title="54:258	D4 D0CTDC B4D0 CX CYDBBNCXB5 BP BD CI DB CX CTDCD4 AW CG CY AL CY CU CY B4D0 CX BNDBBNCXB5 AX BN CI DB CX BP CG D0 CX BEC4B4DB CX B5 CTDCD4 AW CG CY AL CY CU CY B4D0 CX BNDBBNCXB5 AX BN where C4B4DB CX B5 is a set of lexical entries which can be assigned to word DB CX." ></td>
	<td class="line x" title="55:258	Before training this model, C4B4DB CX B5 for all DB CX are extracted from the training treebank." ></td>
	<td class="line x" title="56:258	The feature function CU CY B4D0 CX BNDBBNCXB5 represents the characteristics of D0 CX, DB and DB CX, while corresponding AL CY is its weight." ></td>
	<td class="line x" title="57:258	For the feature functions, instead of using unigram features adopted in Miyao and Tsujii (2005), Ninomiya et al.(2006) used word trigram and POS 5-gram features which are listed in Table 1." ></td>
	<td class="line x" title="59:258	With the revised Enju model, they achieved Table 1: Features for the probabilities of lexical entry selection surrounding words DB A0BD DB BC DB BD (word trigram) surrounding POS tags D4 A0BE D4 A0BD D4 BC D4 BD D4 BE (POS 5-gram) combinations DB A0BD DB BC BNDB BC DB BD BND4 A0BD DB BC BND4 BC DB BC BN D4 BD DB BC BND4 BC D4 BD D4 BE D4 BF BND4 A0BE D4 A0BD D4 BC BN D4 A0BD D4 BC D4 BD BND4 BC D4 BD D4 BE BND4 A0BE D4 A0BD BN D4 A0BD D4 BC BND4 BC D4 BD BND4 BD D4 BE parsing accuracy as high as Miyao and Tsujii (2005), with around four times faster parsing speed." ></td>
	<td class="line x" title="60:258	Johnson and Riezler (2000) suggested the possibility of the method for adapting a stochastic unification-based grammar including HPSG to another domain." ></td>
	<td class="line x" title="61:258	They incorporated auxiliary distributions as additional features for an original log-linear model, and then attempted to assign proper weights to the new features." ></td>
	<td class="line x" title="62:258	With this approach, they succeeded in decreasing to a degree indistinguishable sentences for a target grammar." ></td>
	<td class="line x" title="63:258	Our previous work proposed a method for adapting an HPSG parser trained on the Penn Treebank to a biomedical domain (Hara et al. , 2005)." ></td>
	<td class="line x" title="64:258	We re-trained a disambiguation model of tree construction, i.e., D5 D7DDD2, for the target domain." ></td>
	<td class="line x" title="65:258	In this approach, D5 D7DDD2 of the original parser was used as a reference distribution (Jelinek, 1998) of another loglinear model, and the new model was trained using a target treebank." ></td>
	<td class="line x" title="66:258	Since re-training used only a small treebank of the target domain, the cost was small and parsing accuracy was successfully improved." ></td>
	<td class="line x" title="67:258	3 Re-training of a Disambiguation Model of Lexical Entry Assignments Our idea of domain adaptation is to train a disambiguation model of lexical entry assignments for the target domain and then incorporate it into the original parser." ></td>
	<td class="line x" title="68:258	Since Enju includes the disambiguation model of lexical entry assignments as D4 D0CTDC, we can implement our method in Enju by training another disambiguation model D4 BC D0CTDC B4D0 CX CYDBBNCXB5 of lexical entry assignments for the biomedical domain, and then replacing the original D4 D0CTDC with the newly trained D4 BC D0CTDC . In this paper, for D4BC D0CTDC, we train a disambiguation model D4 D0CTDCA0D1CXDC B4D0 CX CYDBBNCXB5 of lexical entry assignments." ></td>
	<td class="line x" title="69:258	D4 D0CTDCA0D1CXDC is a maximum entropy model and the feature functions for it is the same as D4 D0CTDC as 13 given in Table 1." ></td>
	<td class="line x" title="70:258	With these feature functions, we train D4 D0CTDCA0D1CXDC on the treebanks both of the original and biomedical domains." ></td>
	<td class="line x" title="71:258	In the experiments, we examine the contribution of our method to parsing accuracy." ></td>
	<td class="line x" title="72:258	In addition, we implement several other possible methods for comparison of the performances." ></td>
	<td class="line x" title="73:258	baseline: use the original model of Enju GENIA only: execute the same method of training the disambiguation model of Enju, using only the GENIA treebank Mixture: execute the same method of training the disambiguation model of Enju, using both of the Penn Treebank and the GENIA treebank (a kind of smoothing method) HMT05: execute the method proposed in our previous work (Hara et al. , 2005) Our method: replace D4 D0CTDC in the original model with D4 D0CTDCA0D1CXDC, while leaving D5 D7DDD2 as it is Our method (GENIA): replace D4 D0CTDC in the original model with D4 D0CTDCA0CVCTD2CXCP, which is a probabilistic model of lexical entry assignments trained only with the GENIA treebank, while leaving D5 D7DDD2 as it is Our method + GENIA: replace D4 D0CTDC in the original model with D4 D0CTDCA0D1CXDC and D5 D7DDD2 with D5 D7DDD2A0CVCTD2CXCP, which is a disambiguation model of tree construction trained with the GENIA treebank Our method + HMT05: replace D4 D0CTDC in the original model with D4 D0CTDCA0D1CXDC and D5 D7DDD2 with the model re-trained with our previous method (Hara et al. , 2005) (the combination of our method and the HMT05 method) baseline (lex): use only D4 D0CTDC as a disambiguation model GENIA only (lex): use only D4 D0CTDCA0CVCTD2CXCP as a disambiguation model, which is a probabilistic model of lexical entry assignments trained only with the GENIA treebank Mixture (lex): use only D4 D0CTDCA0D1CXDC as a disambiguation model The baseline method does no adaptation to the biomedical domain, and therefore gives lower parsing accuracy for the domain than for the original domain." ></td>
	<td class="line x" title="74:258	This method is regarded as the baseline of the experiments." ></td>
	<td class="line x" title="75:258	The GENIA only method relies solely on the treebank for the biomedical domain, and therefore it cannot work well with the small treebank." ></td>
	<td class="line x" title="76:258	The Mixture method is a kind of smoothing method using all available training data at the same time, and therefore the method can give the highest accuracy of the three, which would be regarded as the ideal accuracy with the naive methods." ></td>
	<td class="line x" title="77:258	However, training this model is expected to be very costly." ></td>
	<td class="line x" title="78:258	The baseline (lex), GENIA only (lex), and Mixture (lex) approaches rely solely on models of lexical entry assignments, and show lower accuracy than those that contain both of models of lexical entry assignments and tree constructions." ></td>
	<td class="line x" title="79:258	These approaches can be utilized as indicators of importance of combining the two types of models." ></td>
	<td class="line x" title="80:258	Our previous work (Hara et al. , 2005) showed that the model trained with the HMT05 method can give higher accuracy than the baseline method, even with the small amount of the treebanks in the biomedical domain." ></td>
	<td class="line x" title="81:258	The model also takes much less cost to train than with the Mixture method." ></td>
	<td class="line x" title="82:258	However, they reported that the method could not give as high accuracy as the Mixture method." ></td>
	<td class="line x" title="83:258	4 Experiments with the GENIA Corpus 4.1 Experimental Settings We implemented the models shown in Section 3, and then evaluated the performance of them." ></td>
	<td class="line x" title="84:258	The original parser, Enju, was developed on Section 0221 of the Penn Treebank (39,832 sentences) (Miyao and Tsujii, 2005; Ninomiya et al. , 2006)." ></td>
	<td class="line x" title="85:258	For training those models, we used the GENIA treebank (Kim et al. , 2003), which consisted of 1,200 abstracts (10,848 sentences) extracted from MEDLINE." ></td>
	<td class="line x" title="86:258	We divided it into three sets of 900, 150, and 150 abstracts (8,127, 1,361, and 1,360 sentences), and these sets were used respectively as training, development, and final evaluation data." ></td>
	<td class="line x" title="87:258	The method of Gaussian MAP estimation (Chen and Rosenfeld, 1999) was used for smoothing." ></td>
	<td class="line x" title="88:258	The meta parameter AR of the Gaussian distribution was determined so as to maximize the accuracy on the development set." ></td>
	<td class="line x" title="89:258	14 a0a1 a0a2 a0a3 a0a4 a0a5 a0a0 a0a6 a6a7 a7 a8a7a7a7 a2a7a7a7 a4a7a7a7 a0a7a7a7 a9 a10a11 a12a13a14a15 a13a14a16a17a14a16a18a14a13 a19a16 a17a20a14 a21a22a23a24a25 a18a10a26a27a12a13 a28 a29 a30 a31 a32 a33 a34 a35a36a13a14a37a19a16a14 a21a22a23a24a25 a10a16a37 a38 a39a19a40a17a12a26a14 a41a39a42a7a3 a43a12a26 a44a14a17a20a10a15 a43a12a26 a44a14a17a20a10a15 a45a21a22a23a24a25a46 a43a12a26 a44a14a17a20a10a15 a47 a21a22a23a24a25 a43a12a26 a44a14a17a20a10a15 a47 a41a39a42a7a3 a35a36a13a14a37a19a16a14 a45a37a14a40a46 a21a22a23a24a25 a10a16a37 a38 a45a37a14a40a46 a39a19a40a17a12a26a14 a45a37a14a40a46 Figure 3: Corpus size vs. accuracy for various methods In the following experiments, we measured the accuracy of predicate-argument dependencies on the evaluation set." ></td>
	<td class="line x" title="90:258	The measure is labeled precision/recall (LP/LR), which is the same measure as previous work (Clark and Curran, 2004b; Miyao and Tsujii, 2005) that evaluated the accuracy of lexicalized grammars on the Penn Treebank." ></td>
	<td class="line x" title="91:258	The features for the examined approaches were all the same as the original disambiguation model." ></td>
	<td class="line x" title="92:258	In our previous work, the features for HMT05 were tuned to some extent." ></td>
	<td class="line x" title="93:258	We evened out the features in order to compare various approaches under the same condition." ></td>
	<td class="line x" title="94:258	The lexical entries for training each model were extracted from the treebank used for training the model of lexical entry assignments." ></td>
	<td class="line x" title="95:258	We compared the performances of the given models from various angles, by focusing mainly on the accuracy against the cost." ></td>
	<td class="line x" title="96:258	For each of the models, we measured the accuracy transition according to the size of the GENIA treebank for training and according to the training time." ></td>
	<td class="line x" title="97:258	We changed the size of the GENIA treebank for training: 100, 200, 300, 400, 500, 600, 700, 800, and 900 abstracts." ></td>
	<td class="line x" title="98:258	Figure 3 and 4 show the F-score transition according to the size of the training set and the training time among the given models respectively." ></td>
	<td class="line x" title="99:258	Table 2 and Table 3 show the parsing performance and the training cost obtained when using 900 abstracts of the GENIA treebank." ></td>
	<td class="line x" title="100:258	Note that Figure 4 does not include the results of the Mixture method because only the method took too much training cost as shown in Table 3." ></td>
	<td class="line x" title="101:258	It should also be noted that training time in Figure 4 includes time required for both training and development tests." ></td>
	<td class="line x" title="102:258	In Table 2, accuracies with models other than baseline showed the significant differences from baseline according to stratified shuffling test (Cohen, 1995) with p-value BO BCBMBCBH." ></td>
	<td class="line x" title="103:258	In the rest of this section we analyze these experimental results by focusing mainly on the contribution of re-training lexical entry assignment models." ></td>
	<td class="line x" title="104:258	We first observe the results with the naive or existing approaches." ></td>
	<td class="line x" title="105:258	On the basis of these results, we evaluate the impact of our method." ></td>
	<td class="line x" title="106:258	We then explore the combination of our method with other methods, and analyze the errors for our future research." ></td>
	<td class="line x" title="107:258	4.2 Exploring Naive or Existing Approaches Without adaptation, Enju gave the parsing accuracy of 86.39 in F-score, which was 3.42 point lower than 15 a0a1 a0a2 a0a3 a0a4 a0a5 a0a0 a0a6 a6a7 a7 a8a3a7a7a7 a3a7a7a7a7 a9a10a11a12a13a12a13a14 a15a12a16a17 a18a19a17a20a21a22 a23 a24 a25 a26 a27 a28 a29 a30a31a32a33a34 a35a13a36 a37 a38a39a9a7a3 a40a41a10 a16a17a15a42a35a43 a40a41a10 a16a17a15a42a35a43 a18a30a31a32a33a34a22 a40a41a10 a16a17a15a42a35a43 a44 a30a31a32a33a34 a40a41a10 a16a17a15a42a35a43 a44 a38a39a9a7a3 a30a31a32a33a34 a35a13a36 a37 a18a36a17a45a22 a39a12a45a15a41a10a17 a18a36a17a45a22 Figure 4: Training time vs. accuracy for various methods that Enju gave for the original domain, the Penn Treebank." ></td>
	<td class="line x" title="108:258	This is the baseline of the experiments." ></td>
	<td class="line x" title="109:258	Figure 3 shows that, for less than about 4,500 training sentences, the GENIA only method could not obtain as high parsing accuracy as the baseline method." ></td>
	<td class="line x" title="110:258	This result would indicate that the training data would not be sufficient for re-training the whole disambiguation model from scratch." ></td>
	<td class="line x" title="111:258	However, if we prepared more than about 4,500 sentences, the method could give higher accuracy than baseline with low training cost (see Figure 4)." ></td>
	<td class="line x" title="112:258	On the other hand, the Mixture method could obtain the highest level of the parsing accuracy for any size of the GENIA treebank." ></td>
	<td class="line x" title="113:258	However, Table 3 shows that this method required too much training cost." ></td>
	<td class="line x" title="114:258	It would be a major barrier for further challenges for improvement with various additional parameters." ></td>
	<td class="line x" title="115:258	The HMT05 method could give higher accuracy than the baseline method for any size of the training sentences although the accuracy was lower than the Mixture method." ></td>
	<td class="line x" title="116:258	The method could also be carried out in much smaller training time and lower cost than the Mixture method." ></td>
	<td class="line x" title="117:258	These points would be the benefits of the HMT05 method." ></td>
	<td class="line x" title="118:258	On the other hand, when we compared the HMT05 method with the GENIA only method, for the larger size of the training corpus, the HMT05 method was defeated by the GENIA only method in parsing accuracy and training cost." ></td>
	<td class="line x" title="119:258	4.3 Impact of Re-training a Lexical Disambiguation Model When we focused on our method, it could constantly give higher accuracy than the baseline and the HMT05 methods." ></td>
	<td class="line x" title="120:258	These results would indicate that, for an individual method, re-training a model of lexical entry assignments might be more critical to domain adaptation than re-training that of tree construction." ></td>
	<td class="line x" title="121:258	In addition, for the small treebank, our method could give as high accuracy as the Mixture method with much lower training cost." ></td>
	<td class="line x" title="122:258	Our method would be a very satisfiable approach when applied with a small treebank." ></td>
	<td class="line x" title="123:258	It should be noted that the retrained lexical model could not solely give the accuracy as high as our method (see Mixture (lex) in Figure 3)." ></td>
	<td class="line x" title="124:258	The combination of a re-trained lexical model and a tree construction model would have given such a high performance." ></td>
	<td class="line x" title="125:258	When we compared the training time for our 16 Table 2: Parsing accuracy and time for various methods For GENIA Corpus For Penn Treebank LP LR F-score Time LP LR F-score Time baseline 86.71 86.08 86.39 476 sec." ></td>
	<td class="line x" title="126:258	89.99 89.63 89.81 675 sec." ></td>
	<td class="line x" title="127:258	GENIA only 88.99 87.91 88.45 242 sec." ></td>
	<td class="line x" title="128:258	72.07 45.78 55.99 2,441 sec." ></td>
	<td class="line x" title="129:258	Mixture 90.01 89.87 89.94 355 sec." ></td>
	<td class="line x" title="130:258	89.93 89.60 89.77 767 sec." ></td>
	<td class="line x" title="131:258	HMT05 88.47 87.89 88.18 510 sec." ></td>
	<td class="line x" title="132:258	88.92 88.61 88.76 778 sec." ></td>
	<td class="line x" title="133:258	Our method 89.11 88.97 89.04 327 sec." ></td>
	<td class="line x" title="134:258	89.96 89.63 89.79 713 sec." ></td>
	<td class="line x" title="135:258	Our method (GENIA) 86.06 85.15 85.60 542 sec." ></td>
	<td class="line x" title="136:258	70.18 44.88 54.75 3,290 sec." ></td>
	<td class="line x" title="137:258	Our method + GENIA 90.02 89.88 89.95 320 sec." ></td>
	<td class="line x" title="138:258	88.11 87.77 87.94 718 sec." ></td>
	<td class="line x" title="139:258	Our method + HMT05 90.23 90.08 90.15 377 sec." ></td>
	<td class="line x" title="140:258	89.31 88.98 89.14 859 sec." ></td>
	<td class="line x" title="141:258	baseline (lex) 85.93 85.27 85.60 377 sec." ></td>
	<td class="line x" title="142:258	87.52 87.13 87.33 553 sec." ></td>
	<td class="line x" title="143:258	GENIA only (lex) 87.42 86.28 86.85 197 sec." ></td>
	<td class="line x" title="144:258	71.49 45.41 55.54 1,928 sec." ></td>
	<td class="line x" title="145:258	Mixture (lex) 88.43 88.18 88.31 258 sec." ></td>
	<td class="line x" title="146:258	87.49 87.12 87.30 585 sec." ></td>
	<td class="line x" title="147:258	Table 3: Training cost of various methods Training time Memory used baseline 0 sec." ></td>
	<td class="line x" title="148:258	0.00 GByte GENIA only 14,695 sec." ></td>
	<td class="line x" title="149:258	1.10 GByte Mixture 238,576 sec." ></td>
	<td class="line x" title="150:258	5.05 GByte HMT05 21,833 sec." ></td>
	<td class="line x" title="151:258	1.10 GByte Our method 12,957 sec." ></td>
	<td class="line x" title="152:258	4.27 GByte Our method (GENIA) 1,419 sec." ></td>
	<td class="line x" title="153:258	0.94 GByte Our method + GENIA 42,475 sec." ></td>
	<td class="line x" title="154:258	4.27 GByte Our method + HMT05 31,637 sec." ></td>
	<td class="line x" title="155:258	4.27 GByte baseline (lex) 0 sec." ></td>
	<td class="line x" title="156:258	0.00 GByte GENIA only (lex) 1,434 sec." ></td>
	<td class="line x" title="157:258	1.10 GByte Mixture (lex) 13,595 sec." ></td>
	<td class="line x" title="158:258	4.27 GByte method with the one for the HMT05 method, our method required less time than the HMT05 method." ></td>
	<td class="line x" title="159:258	This would be because our method required only the re-training of the very simple model, that is, a probabilistic model of lexical entry assignments." ></td>
	<td class="line x" title="160:258	It should be noted that our method would not work only with in-domain treebank." ></td>
	<td class="line x" title="161:258	The Our method (GENIA) and the GENIA only (lex) methods could hardly give as high parsing accuracy as the baseline method." ></td>
	<td class="line x" title="162:258	Although, for the larger size of the GENIA treebank, the methods could obtain a little higher accuracy than the baseline method, the benefit was very little." ></td>
	<td class="line x" title="163:258	These results would indicate that only the treebank in the target domain would be insufficient for adaptation." ></td>
	<td class="line x" title="164:258	Figure 5 shows the coverage of each training corpus for the GENIA treebank, which would also support the above observation." ></td>
	<td class="line x" title="165:258	It shows that the GENIA treebank could not solely cover so much sentences in the GENIA corpus as the combination of the Penn Treebank and the GENIA treebank." ></td>
	<td class="line x" title="166:258	g14986g14983 g14987g14983 g14988g14983 g14989g14983 g14990g14983 g14991g14983 g14992g14983 g14983 g14985g14983g14983g14983 g14987g14983g14983g14983 g14989g14983g14983g14983 g14991g14983g14983g14983 g14970g14967g15046g15037g14967g15052g15050g15036g15035g14967g15050g15036g15045g15051g15036g15045g15034g15036g15050 g15002g15046 g15053g15036 g15049g15032g15038 g15036g14967g14975 g14972g14976 g15015g15019g15001 g15006g15004g15013g15008g15000 g15006g15004g15013g15008g15000g14967g14978g14967g15015g15019g15001 Figure 5: Corpus size vs. coverage of each training set for the GENIA corpus Table 4: Coverage of each training set % of covered sentencesTraining set for GENIA for PTB GENIA treebank 77.54 % 25.66 % PTB treebank 70.45 % 84.12 % GENIA treebank + PTB treebank 82.74 % 84.86 % 4.4 Effectiveness of Combining Lexical and Syntactic Disambiguation Models When we compared the Our method + HMT05 and Our method + GENIA methods with the Mixture method, the former two models could give as the high parsing accuracies as the latter one for any size of the training corpus." ></td>
	<td class="line x" title="167:258	In particular, for the maximum size, the Our method + HMT05 models could give a little higher parsing accuracy than the Mixture method." ></td>
	<td class="line x" title="168:258	This difference was 17 Table 5: Errors in various methods Total errors = Common errors with baseline + Specific errors GENIA only 2,889 = 1,906 (65.97%) + 983 (34.03%) Mixture 2,653 = 2,177 (82.06%) + 476 (17.94%) HMT05 3,063 = 2,470 (80.64%) + 593 (19.36%) Our method 2,891 = 2,405 (83.19%) + 486 (16.81%) Our method (GENIA) 3,153 = 2,070 (65.65%) + 1,083 (34.35%) Our method + GENIA 2,650 = 2,056 (77.58%) + 594 (22.42%) Our method + HMT05 2,597 = 1,943 (74.82%) + 654 (25.18%) baseline 3,542 Total errors = Common errors with baseline (lex) + Specific errors GENIA only (lex) 3,320 = 2,509 (75.57%) + 811 (24.43%) Mixture (lex) 3,100 = 2,769 (89.32%) + 331 (10.68%) baseline (lex) 3,757 Table 6: Types of disambiguation errors # of errors Only forError cause Common Baseline Adapted Attachment ambiguity prepositional phrase 12 12 6 relative clause 0 1 0 adjective 4 2 2 adverb 1 3 1 verb phrase 10 3 1 subordinate clause 0 2 0 Argument/modifier distinction to-infinitive 0 0 7 Lexical ambiguity preposition/modifier 0 3 0 verb subcategorization frame 5 0 6 participle/adjective 0 2 0 Test set errors Errors of treebank 2 0 0 Other types of error causes Comma 10 8 4 Noun phrase identification 21 5 8 Coordination/insertion 6 3 5 Zero-pronoun resolution 8 1 0 Others 1 1 2 shown to be significant according to stratified shuffling test with p-value BO 0.10, which might suggest the beneficial impact of the Our method + HMT05 method." ></td>
	<td class="line x" title="169:258	In addition, Figure 4 and Table 3 show that training the Our method + HMT05 or Our method + GENIA model required much less time and PC memory than training the Mixture model." ></td>
	<td class="line x" title="170:258	According to the above observation, we would be able to say that the Our method + HMT05 method might be the most ideal among the given methods." ></td>
	<td class="line x" title="171:258	The Our method + HMT05 and Our method + GENIA methods showed the different performances in the point that the former could obtain high parsing accuracy with less training time than the latter." ></td>
	<td class="line x" title="172:258	This would come from the fact that the latter method trained D5 D7DDD2A0CVCTD2CXCP solely with lexical entries in the GENIA treebank, while the former one trained D5 D7DDD2 with rich lexical entries borrowed from D5 D0CTDCA0D1CXDC . Rich lexical entries would decrease unknown lexical entries, and therefore would improve the effectiveness of making the feature forest model." ></td>
	<td class="line x" title="173:258	On the other hand, the difference in lexical entries would not seem to affect so much on the contribution of tree construction model to the parsing accuracy." ></td>
	<td class="line x" title="174:258	In our experiments, the parameters for a tree construction model such as feature functions were not adjusted thoroughly, which might possibly blur the benefits of the rich lexical entries." ></td>
	<td class="line x" title="175:258	4.5 Error Analysis Table 5 shows the comparison of the number of errors for various models with that for the original model in parsing the GENIA corpus." ></td>
	<td class="line x" title="176:258	For each of the methods, the table gives the numbers of common errors with the original Enju model and the ones specific to that method." ></td>
	<td class="line x" title="177:258	If possible, we would like our methods to decrease the errors in the original Enju model while not increasing new errors." ></td>
	<td class="line x" title="178:258	The table shows that our method gave the least percentage of newly added errors among the approaches except for the methods utilizing only lexical entry assignments models." ></td>
	<td class="line x" title="179:258	On the other hand, the Our method + HMT05 approach gave over 25 % of newly added errors, although we considered above that the approach gave the best performance." ></td>
	<td class="line x" title="180:258	In order to explore this phenomenon, we observed 18 the errors for the Our method + HMT05 and the baseline models, and then classified them into several types." ></td>
	<td class="line x" title="181:258	Table 6 shows manual classification of causes of errors for the two models in 50 sentences." ></td>
	<td class="line x" title="182:258	In the classification, one error often propagated and resulted in multiple errors of predicate argument dependencies." ></td>
	<td class="line x" title="183:258	The numbers in the table include such double counting." ></td>
	<td class="line x" title="184:258	It would be desirable that the errors in the rightmost column were less than the ones in the middle column, which means that the Our method + HMT05 approach did not produce more errors specific to the approach than the baseline." ></td>
	<td class="line x" title="185:258	With the Our method + HMT05 approach, errors for attachment ambiguity decreased as a whole." ></td>
	<td class="line x" title="186:258	Errors for comma and lexical ambiguities of preposition/modifier and participle/adjective also decreased." ></td>
	<td class="line x" title="187:258	For these attributes, the approach could learn in the training phase lexical properties of continuous words with the lexical entry assignment model, and syntactic relations of separated words with the tree construction model." ></td>
	<td class="line x" title="188:258	On the other hand, the errors for to-infinitive argument/modifier distinction and verb subcategorization frame ambiguity considerably increased." ></td>
	<td class="line x" title="189:258	These two types of errors have close relation to each other because the failure to recognize verb subcategorization frames tends to cause the failure to recognize the syntactic role of the to-infinitives." ></td>
	<td class="line x" title="190:258	We must research further on these errors in our future work." ></td>
	<td class="line x" title="191:258	When we focused on noun phrase identification, most of the errors did not differ between the two models." ></td>
	<td class="line x" title="192:258	In the biomedical domain, there would be many technical terms which could not be correctly identified solely with the disambiguation model, which would possibly result in such many untouched errors." ></td>
	<td class="line x" title="193:258	In order to properly cope with these terms, we might have to introduce some kinds of dictionaries or named entity recognition methods." ></td>
	<td class="line x" title="194:258	5 Experiments with the Brown Corpus 5.1 Brown Corpus We applied our methods to the Brown corpus (Kucera and Francis, 1967) and examined the portability of our method." ></td>
	<td class="line x" title="195:258	The Brown corpus consists of 15 domains, and the Penn Treebank gives bracketed version of the corpus for the 8 domains containing 19,395 sentences (Table 7)." ></td>
	<td class="line x" title="196:258	Table 7: Domains in the Brown corpus label domain sentences CF popular lore 2,420 CG belles lettres 2,546 CK general fiction 3,172 CL mystery and detective fiction 2,745 CM science fiction 615 CN adventure and western fiction 3,521 CP romance and love story 3,089 CR humor 812 All total of all the above domains 19,395 For the target of adaptation, we utilized the domain containing all of these 8 domains as a total fiction domain (labelled All) as well as the individual ones." ></td>
	<td class="line x" title="197:258	As in the experiments with the GENIA Treebank, we divided sentences for each domain into three parts, 80% for training, 10% for develepment test, and 10% for final test." ></td>
	<td class="line x" title="198:258	For the All domain, we merged all training sets, all development test sets, and all final test sets for the 8 domains respectively." ></td>
	<td class="line x" title="199:258	Table 8 and 9 show the parsing accuracy and training time for each domain with the various methods shown in Section 3." ></td>
	<td class="line x" title="200:258	The methods are fundamentally the same as in the experiments with the GENIA corpus except that the target corpus is replaced with the Brown corpus." ></td>
	<td class="line x" title="201:258	In order to avoid confusion, we replaced GENIA in the names of the methods with Brown. Each of the bold numbers in Table 8 means that it was the best accuracy given for the target domain." ></td>
	<td class="line x" title="202:258	It should be noted that the CM and CR domain contains very small treebank, and therefore we must consider that the results with these domains would not be so useful." ></td>
	<td class="line x" title="203:258	5.2 Evaluation of Portability of Our Method When we focus on the ALL domain, the approaches other than the baseline succeeded to give higher parsing accuracy than the baseline." ></td>
	<td class="line x" title="204:258	This would show that these approaches were effective not only for the GENIA corpus but also for the Brown corpus." ></td>
	<td class="line x" title="205:258	The Mixture method gave the highest accuracy which was 3.41 point higher than the baseline." ></td>
	<td class="line x" title="206:258	The Our method + HMT05 approach also gave the accuracy as high as the Mixture method." ></td>
	<td class="line x" title="207:258	In addition, as is the case with the GENIA corpus, the approach could be trained with much less time than the Mixture method." ></td>
	<td class="line x" title="208:258	Not only for these two 19 Table 8: Parsing accuracy for the Brown corpus F-score ALL CF CG CK CL CM CN CP CR baseline 83.09 85.75 85.38 81.12 77.53 85.30 82.84 85.18 76.63 Brown only 84.84 77.65 78.92 75.72 70.56 50.02 78.38 79.10 50.34 Mixture 86.50 86.59 85.94 82.49 78.66 84.82 84.28 86.85 76.45 HMT05 83.79 85.80 84.98 81.48 76.91 85.25 83.50 85.66 77.15 Our method 86.14 86.73 85.74 82.77 77.95 85.40 84.23 86.90 76.71 Our method (GENIA) 84.71 78.49 79.63 75.43 70.86 50.24 78.49 79.69 51.82 Our method + GENIA 86.00 86.12 85.41 83.22 77.10 83.39 84.21 85.77 76.91 Our method + HMT05 86.44 86.76 85.85 82.90 77.70 85.61 84.43 86.87 77.48 baseline (lex) 82.19 84.69 83.85 80.25 76.32 83.42 81.29 84.13 77.33 Brown only (lex) 83.92 77.12 77.81 75.06 70.35 49.95 77.06 78.84 50.63 Mixture (lex) 85.29 85.47 84.18 81.88 77.22 83.98 82.67 85.65 77.58 Table 9: Consumed time for various methods for the Brown corpus Consumed time for training (sec.)" ></td>
	<td class="line x" title="209:258	ALL CF CG CK CL CM CN CP CR baseline 0 0 0 0 0 0 0 0 0 Brown only 42,614 4,115 3,763 2,478 2,162 925 2,362 2,695 1,226 Mixture 383,557 190,449 159,490 156,299 210,357 131,335 170,108 224,045 184,251 HMT05 30,933 6,003 4,830 4,186 5,010 1,681 4,411 5,069 1,588 Our method 15,912 11,053 10,988 11,151 10,782 10,158 11,075 10,594 10,284 Our method (Brown) 3,273 312 373 310 249 46 321 317 86 Our method + Brown 130,434 24,633 21,848 20,171 19,184 11,995 19,164 20,922 13,461 Our method + HMT05 54,355 17,722 16,627 15,229 14,914 12,226 15,760 16,175 11,724 baseline (lex) 0 0 0 0 0 0 0 0 0 Brown only (lex) 3,001 317 373 308 251 47 321 317 86 Mixture (lex) 21,148 11,128 11,251 11,094 10,728 10,466 11,151 10,897 10,537 methods, the experimental results for the All domain showed the tendency similar to the GENIA corpus as a whole, except for the less improvement with the HMT05 method." ></td>
	<td class="line x" title="210:258	When we focus on the individual domains, our method could successfully obtain higher parsing accuracy than the baseline for all the domains." ></td>
	<td class="line x" title="211:258	Moreover, for the CP domain, our method could give the highest parsing accuracy among the methods." ></td>
	<td class="line x" title="212:258	These results would support the portability of retraining the model for lexical entry assignment." ></td>
	<td class="line x" title="213:258	The Our method + HMT05 approach, which gave the highest performance for the GENIA corpus, also gave accuracy improvement for the all domains while it did not give so much impact for the CL domain." ></td>
	<td class="line x" title="214:258	The Mixture approach, which utilized the same lexical entry assignment model, could obtain 0.94 point higher parsing accuracy than the Our method + HMT05 approach." ></td>
	<td class="line x" title="215:258	Table 10, which shows the lexical coverage with each domains, does not seem to indicate the noteworthy difference in lexical entry coverage between the CL and the other domains." ></td>
	<td class="line x" title="216:258	As mentioned in the error analysis in Section 4, the model of tree construction might affect the performance in some way." ></td>
	<td class="line x" title="217:258	In our future work, we must clarify the mechanism of this result and would like to further improve the performance." ></td>
	<td class="line x" title="218:258	6 Related Work For recent years, domain adaptation has been studied extensively." ></td>
	<td class="line x" title="219:258	This section explores how our research is relevant to the previous works." ></td>
	<td class="line x" title="220:258	Our previous work (Hara et al. , 2005) and this research mainly focused on how to draw as much benefit from a smaller amount of in-domain annotated data as possible." ></td>
	<td class="line x" title="221:258	Titov and Henderson (2006) also took this type of approach." ></td>
	<td class="line x" title="222:258	They first trained a probabilistic model on original and target treebanks and used it to define a kernel over parse trees." ></td>
	<td class="line x" title="223:258	This kernel was used in a large margin classifier trained on a small set of data only from the target domain, and the classifier was then used for reranking the top 20 Table 10: Coverage of each training set for the Brown corpus % of covered sentences for the target corpusTraining set ALL CF CG CK CL CM CN CP CR Target treebank 74.99 % 49.13 % 50.00 % 47.97 % 49.08 % 29.66 % 53.51 % 64.01 % 8.57% PTB treebank 70.02 % 72.09 % 68.93 % 66.42 % 68.87 % 78.62 % 70.00 % 77.59 % 47.14 % Target + PTB 79.77 % 74.71 % 71.47 % 71.59 % 70.45 % 80.00 % 72.70 % 80.39 % 52.86 % parses on the target domain." ></td>
	<td class="line x" title="224:258	On the other hand, several studies have explored how to draw useful information from unlabelled indomain data." ></td>
	<td class="line x" title="225:258	Roark and Bacchiani (2003) adapted a lexicalized PCFG by using maximum a posteriori (MAP) estimation for handling unlabelled adaptation data." ></td>
	<td class="line oc" title="226:258	In the field of classifications, Blitzer et al.(2006) utilized unlabelled corpora to extract features of structural correspondences, and then adapted a POS-tagger to a biomedical domain." ></td>
	<td class="line x" title="228:258	Steedman et al.(2003) utilized a co-training parser for adaptation and showed that co-training is effective even across domains." ></td>
	<td class="line x" title="230:258	McClosky et al.(2006) adapted a re-ranking parser to a target domain by self-training the parser with unlabelled data in the target domain." ></td>
	<td class="line x" title="232:258	Clegg and Shepherd (2005) combined several existing parsers with voting schemes or parse selection, and then succeeded to gain the improvement of performance for a biomedical domain." ></td>
	<td class="line n" title="233:258	Although unsupervised methods can exploit large in-domain data, the above studies could not obtain the accuracy as high as that for an original domain, even with the sufficient size of the unlabelled corpora." ></td>
	<td class="line x" title="234:258	On the other hand, we showed that our approach could achieve this goal with about 6,500 labelled sentences." ></td>
	<td class="line x" title="235:258	However, when 6,500 labelled can not be prepared, it might be worth while to explore the potentiality of combining the above unsupervised and our supervised methods." ></td>
	<td class="line x" title="236:258	When we focuses on biomedical domains, there have also been various works which coped with domain adaptation." ></td>
	<td class="line x" title="237:258	Biomedical sentences contain many technical terms which cannot be easily recognized without expert knowledge, and this damages performances of NLP tools directly." ></td>
	<td class="line x" title="238:258	In order to solve this problem, two types of approaches have been suggested." ></td>
	<td class="line x" title="239:258	The first approach is to utilize existing domain-specific lexical resources." ></td>
	<td class="line x" title="240:258	Lease and Charniak (2005) utilized POS tags, dictionary collocations, and named entities for parser adaptation, and then succeeded to achieve accuracy improvement." ></td>
	<td class="line x" title="241:258	The second approach is to expand lexical entries for a target domain." ></td>
	<td class="line x" title="242:258	Szolovits (2003) extended a lexical dictionary for a target domain by predicting lexical information for words." ></td>
	<td class="line x" title="243:258	They transplanted lexical indiscernibility of words in an original domain into a target domain." ></td>
	<td class="line x" title="244:258	Pyysalo et al.(2004) showed the experimental results that this approach improved the performance of a parser for Link Grammar." ></td>
	<td class="line x" title="246:258	Since our re-trained model of lexical entry assignments was shown to be unable to cope with this problem properly (shown in Section 4), the combination of the above approaches with our approach would be expected to bring further improvement." ></td>
	<td class="line x" title="247:258	7 Conclusions This paper presented an effective approach to adapting an HPSG parser trained on the Penn Treebank to a biomedical domain." ></td>
	<td class="line x" title="248:258	We trained a probabilistic model of lexical entry assignments in a target domain and then incorporated it into the original parser." ></td>
	<td class="line x" title="249:258	The experimental results showed that this approach obtains higher parsing accuracy than the existing approach of adapting the structural model alone." ></td>
	<td class="line x" title="250:258	Moreover, the results showed that, the combination of our method and the existing approach could achieve parsing accuracy that is as high as that obtained by re-training an HPSG parser for the target domain from scratch, but with much lower training cost." ></td>
	<td class="line x" title="251:258	With this model, the parsing accuracy for the target domain improved by 3.84 f-score points, using a domain-specific treebank of 8,127 sentences." ></td>
	<td class="line x" title="252:258	Experiments showed that 6,500 sentences are sufficient for achieving as high parsing accuracy as the baseline for the original domain." ></td>
	<td class="line x" title="253:258	In addition, we applied our method to the Brown corpus in order to evaluate the portability of our method." ></td>
	<td class="line x" title="254:258	Experimental results showed that the parsing accuracy for the target domain improved by 3.35 f-score points." ></td>
	<td class="line x" title="255:258	On the other hand, when we focused 21 on some individual domains, that combination approach could not give the desirable results." ></td>
	<td class="line x" title="256:258	In future work, we would like to explore further performance improvement of our approach." ></td>
	<td class="line x" title="257:258	For the first step, domain-specific features such as named entities could be much help for solving unsuccessful recognition of technical terms." ></td>
	<td class="line x" title="258:258	Acknowledgment This research was partially supported by Grant-inAid for Specially Promoted Research 18002007." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-1029
Exploiting Feature Hierarchy for Transfer Learning in Named Entity Recognition
Arnold, Andrew;Nallapati, Ramesh;Cohen, William W.;"></td>
	<td class="line x" title="1:161	Proceedings of ACL-08: HLT, pages 245253, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:161	c2008 Association for Computational Linguistics Exploiting Feature Hierarchy for Transfer Learning in Named Entity Recognition Andrew Arnold, Ramesh Nallapati and William W. Cohen Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, USA {aarnold, nmramesh, wcohen}@cs.cmu.edu Abstract We present a novel hierarchical prior structure for supervised transfer learning in named entity recognition, motivated by the common structure of feature spaces for this task across natural language data sets." ></td>
	<td class="line x" title="3:161	The problem of transfer learning, where information gained in one learning task is used to improve performance in another related task, is an important new area of research." ></td>
	<td class="line x" title="4:161	In the subproblem of domain adaptation, a model trained over a source domain is generalized to perform well on a related target domain, where the two domains data are distributed similarly, but not identically." ></td>
	<td class="line x" title="5:161	We introduce the concept of groups of closely-related domains, called genres, and show how inter-genre adaptation is related to domain adaptation." ></td>
	<td class="line x" title="6:161	We also examine multitask learning, where two domains may be related, but where the concept to be learned in each case is distinct." ></td>
	<td class="line x" title="7:161	We show that our prior conveys useful information across domains, genres and tasks, while remaining robust to spurious signals not related to the target domain and concept." ></td>
	<td class="line x" title="8:161	We further show that our model generalizes a class of similar hierarchical priors, smoothed to varying degrees, and lay the groundwork for future exploration in this area." ></td>
	<td class="line x" title="9:161	1 Introduction 1.1 Problem definition Consider the task of named entity recognition (NER)." ></td>
	<td class="line x" title="10:161	Specifically, you are given a corpus of news articles in which all tokens have been labeled as either belonging to personal name mentions or not." ></td>
	<td class="line x" title="11:161	The standard supervised machine learning problem is to learn a classifier over this training data that will successfully label unseen test data drawn from the same distribution as the training data, where same distribution could mean anything from having the train and test articles written by the same author to having them written in the same language." ></td>
	<td class="line x" title="12:161	Having successfully trained a named entity classifier on this news data, now consider the problem of learning to classify tokens as names in e-mail data." ></td>
	<td class="line x" title="13:161	An intuitive solution might be to simply retrain the classifier, de novo, on the e-mail data." ></td>
	<td class="line x" title="14:161	Practically, however, large, labeled datasets are often expensive to build and this solution would not scale across a large number of different datasets." ></td>
	<td class="line x" title="15:161	Clearly the problems of identifying names in news articles and e-mails are closely related, and learning to do well on one should help your performance on the other." ></td>
	<td class="line x" title="16:161	At the same time, however, there are serious differences between the two problems that need to be addressed." ></td>
	<td class="line x" title="17:161	For instance, capitalization, which will certainly be a useful feature in the news problem, may prove less informative in the e-mail data since the rules of capitalization are followed less strictly in that domain." ></td>
	<td class="line x" title="18:161	These are the problems we address in this paper." ></td>
	<td class="line x" title="19:161	In particular, we develop a novel prior for named entity recognition that exploits the hierarchical feature space often found in natural language domains (1.2) and allows for the transfer of information from labeled datasets in other domains (1.3)." ></td>
	<td class="line x" title="20:161	2 introduces the maximum entropy (maxent) and conditional random field (CRF) learning techniques employed, along with specifications for the design and training of our hierarchical prior." ></td>
	<td class="line x" title="21:161	Finally, in 3 we present an empirical investigation of our priors performance against a number of baselines, demonstrating both its effectiveness and robustness." ></td>
	<td class="line x" title="22:161	1.2 Hierarchical feature trees In many NER problems, features are often constructed as a series of transformations of the input training data, performed in sequence." ></td>
	<td class="line x" title="23:161	Thus, if our task is to identify tokens as either being (O)utside or (I)nside person names, and we are given the labeled 245 sample training sentence: O O O O O I Give the book to Professor Caldwell (1) one such useful feature might be: Is the token one slot to the left of the current token Professor?" ></td>
	<td class="line x" title="24:161	We can represent this symbolically as L.1.Professor where we describe the whole space of useful features of this form as: {direction = (L)eft, (C)urrent, (R)ight}.{distance = 1, 2, 3, }.{value = Professor, book, }." ></td>
	<td class="line x" title="25:161	We can conceptualize this structure as a tree, where each slot in the symbolic name of a feature is a branch and each period between slots represents another level, going from root to leaf as read left to right." ></td>
	<td class="line x" title="26:161	Thus a subsection of the entire feature tree for the token Caldwell could be drawn as in Figure 1 (zoomed in on the section of the tree where the L.1.Professor feature resides)." ></td>
	<td class="line x" title="27:161	direction L C R distance 1 2    value Professor book   true false  Figure 1: Graphical representation of a hierarchical feature tree for token Caldwell in example Sentence 1." ></td>
	<td class="line x" title="28:161	Representing feature spaces with this kind of tree, besides often coinciding with the explicit language used by common natural language toolkits (Cohen, 2004), has the added benefit of allowing a model to easily back-off, or smooth, to decreasing levels of specificity." ></td>
	<td class="line x" title="29:161	For example, the leaf level of the feature tree for our sample Sentence 1 tells us that the word Professor is important, with respect to labeling person names, when located one slot to the left of the current word being classified." ></td>
	<td class="line x" title="30:161	This may be useful in the context of an academic corpus, but might be less useful in a medical domain where the word Professor occurs less often." ></td>
	<td class="line x" title="31:161	Instead, we might want to learn the related feature L.1.Dr. In fact, it might be useful to generalize across multiple domains the fact that the word immediately preceding the current word is often important with respect LeftToken.* LeftToken.IsWord.* LeftToken.IsWord.IsTitle.* LeftToken.IsWord.IsTitle.equals.* LeftToken.IsWord.IsTitle.equals.mr Table 1: A few examples of the feature hierarchy to the named entity status of the current word." ></td>
	<td class="line x" title="32:161	This is easily accomplished by backing up one level from a leaf in the tree structure to its parent, to represent a class of features such as L.1.*." ></td>
	<td class="line x" title="33:161	It has been shown empirically that, while the significance of particular features might vary between domains and tasks, certain generalized classes of features retain their importance across domains (Minkov et al., 2005)." ></td>
	<td class="line x" title="34:161	By backing-off in this way, we can use the feature hierarchy as a prior for transferring beliefs about the significance of entire classes of features across domains and tasks." ></td>
	<td class="line x" title="35:161	Some examples illustrating this idea are shown in table 1." ></td>
	<td class="line x" title="36:161	1.3 Transfer learning When only the type of data being examined is allowed to vary (from news articles to e-mails, for example), the problem is called domain adaptation (Daume III and Marcu, 2006)." ></td>
	<td class="line x" title="37:161	When the task being learned varies (say, from identifying person names to identifying protein names), the problem is called multi-task learning (Caruana, 1997)." ></td>
	<td class="line x" title="38:161	Both of these are considered specific types of the overarching transfer learning problem, and both seem to require a way of altering the classifier learned on the first problem (called the source domain, or source task) to fit the specifics of the second problem (called the target domain, or target task)." ></td>
	<td class="line x" title="39:161	More formally, given an example x and a class label y, the standard statistical classification task is to assign a probability, p(y|x), to x of belonging to class y. In the binary classification case the labels are Y  {0,1}." ></td>
	<td class="line x" title="40:161	In the case we examine, each example xi is represented as a vector of binary features (f1(xi),,fF(xi)) where F is the number of features." ></td>
	<td class="line x" title="41:161	The data consists of two disjoint subsets: the training set (Xtrain,Ytrain) = {(x1,y1),(xN,yN)}, available to the model for its training and the test set Xtest = (x1,,xM), upon which we want to use our trained classifier to make predictions." ></td>
	<td class="line x" title="42:161	246 In the paradigm of inductive learning, (Xtrain,Ytrain) are known, while both Xtest and Ytest are completely hidden during training time." ></td>
	<td class="line x" title="43:161	In this cases Xtest and Xtrain are both assumed to have been drawn from the same distribution, D. In the setting of transfer learning, however, we would like to apply our trained classifier to examples drawn from a distribution different from the one upon which it was trained." ></td>
	<td class="line x" title="44:161	We therefore assume there are two different distributions,Dsource andDtarget, from which data may be drawn." ></td>
	<td class="line x" title="45:161	Given this notation we can then precisely state the transfer learning problem as trying to assign labels Y targettest to test data Xtargettest drawn from Dtarget, given training data (Xsourcetrain ,Y sourcetrain ) drawn fromDsource." ></td>
	<td class="line x" title="46:161	In this paper we focus on two subproblems of transfer learning:  domain adaptation, where we assume Y (the set of possible labels) is the same for both Dsource and Dtarget, while Dsource and Dtarget themselves are allowed to vary between domains." ></td>
	<td class="line x" title="47:161	 multi-task learning (Ando and Zhang, 2005; Caruana, 1997; Sutton and McCallum, 2005; Zhang et al., 2005) in which the task (and label set) is allowed to vary from source to target." ></td>
	<td class="line x" title="48:161	Domain adaptation can be further distinguished by the degree of relatedness between the source and target domains." ></td>
	<td class="line x" title="49:161	For example, in this work we group data collected in the same medium (e.g., all annotated e-mails or all annotated news articles) as belonging to the same genre." ></td>
	<td class="line x" title="50:161	Although the specific boundary between domain and genre for a particular set of data is often subjective, it is nevertheless a useful distinction to draw." ></td>
	<td class="line x" title="51:161	One common way of addressing the transfer learning problem is to use a prior which, in conjunction with a probabilistic model, allows one to specify a priori beliefs about a distribution, thus biasing the results a learning algorithm would have produced had it only been allowed to see the training data (Raina et al., 2006)." ></td>
	<td class="line x" title="52:161	In the example from1.1, our belief that capitalization is less strict in e-mails than in news articles could be encoded in a prior that biased the importance of the capitalization feature to be lower for e-mails than news articles." ></td>
	<td class="line x" title="53:161	In the next section we address the problem of how to come up with a suitable prior for transfer learning across named entity recognition problems." ></td>
	<td class="line x" title="54:161	2 Models considered 2.1 Basic Conditional Random Fields In this work, we will base our work on Conditional Random Fields (CRFs) (Lafferty et al., 2001), which are now one of the most preferred sequential models for many natural language processing tasks." ></td>
	<td class="line x" title="55:161	The parametric form of the CRF for a sentence of length n is given as follows: p(Y = y|x) = 1Z(x) exp( nsummationdisplay i=1 Fsummationdisplay j=1 fj(x,yi)j) (2) where Z(x) is the normalization term." ></td>
	<td class="line x" title="56:161	CRF learns a model consisting of a set of weights  ={1F} over the features so as to maximize the conditional likelihood of the training data, p(Ytrain|Xtrain), given the model p." ></td>
	<td class="line x" title="57:161	2.2 CRF with Gaussian priors To avoid overfitting the training data, these s are often further constrained by the use of a Gaussian prior (Chen and Rosenfeld, 1999) with diagonal covariance,N(,2), which tries to maximize: argmax  Nsummationdisplay k=1 parenleftbigg log p(yk|xk) parenrightbigg  Fsummationdisplay j (jj)2 22j where  > 0 is a parameter controlling the amount of regularization, and N is the number of sentences in the training set." ></td>
	<td class="line x" title="58:161	2.3 Source trained priors One recently proposed method (Chelba and Acero, 2004) for transfer learning in Maximum Entropy models 1 involves modifying the s of this Gaussian prior." ></td>
	<td class="line x" title="59:161	First a model of the source domain, source, is learned by training on{Xsourcetrain ,Y sourcetrain }." ></td>
	<td class="line x" title="60:161	Then a model of the target domain is trained over a limited set of labeled target data braceleftBig Xtargettrain ,Y targettrain bracerightBig , but instead of regularizing this target to be near zero (i.e. setting  = 0), target is instead regularized towards the previously learned source values source (by setting  = source, while 2 remains 1) and thus minimizing (targetsource)2." ></td>
	<td class="line x" title="61:161	1Maximum Entropy models are special cases of CRFs that use the I.I.D. assumption." ></td>
	<td class="line x" title="62:161	The method under discussion can also be extended to CRF directly." ></td>
	<td class="line x" title="63:161	247 Note that, since this model requires Y targettrain in order to learn target, it, in effect, requires two distinct labeled training datasets: one on which to train the prior, and another on which to learn the models final weights (which we call tuning), using the previously trained prior for regularization." ></td>
	<td class="line x" title="64:161	If we are unable to find a match between features in the training and tuning datasets (for instance, if a word appears in the tuning corpus but not the training), we backoff to a standardN(0,1) prior for that feature." ></td>
	<td class="line x" title="65:161	3 y x i i (1) (1) (1)M w (1)1 y x i i ( M y x i i ( M (2) 2) (2) (3) 3) (3) w w (1) w (1) w1 w w w1 w(1)2 3 4 (2) (2) (2)2 3 (3) (3)2 z z z 1 2 Figure 2: Graphical representation of the hierarchical transfer model." ></td>
	<td class="line x" title="66:161	2.4 New model: Hierarchical prior model In this section, we will present a new model that learns simultaneously from multiple domains, by taking advantage of our feature hierarchy." ></td>
	<td class="line x" title="67:161	We will assume that there are D domains on which we are learning simultaneously." ></td>
	<td class="line x" title="68:161	Let there be Md training data in each domain d. For our experiments with non-identically distributed, independent data, we use conditional random fields (cf.2.1)." ></td>
	<td class="line x" title="70:161	However, this model can be extended to any discriminative probabilistic model such as the MaxEnt model." ></td>
	<td class="line x" title="71:161	Let (d) = ((d)1 ,,(d)Fd ) be the parameters of the discriminative model in the domain d where Fd represents the number of features in the domain d. Further, we will also assume that the features of different domains share a common hierarchy represented by a treeT, whose leaf nodes are the features themselves (cf.Figure 1)." ></td>
	<td class="line x" title="73:161	The model parameters (d), then, form the parameters of the leaves of this hierarchy." ></td>
	<td class="line x" title="74:161	Each non-leaf node n  non-leaf(T) of the tree is also associated with a hyper-parameter zn." ></td>
	<td class="line x" title="75:161	Note that since the hierarchy is a tree, each node n has only one parent, represented by pa(n)." ></td>
	<td class="line x" title="76:161	Similarly, we represent the set of children nodes of a node n as ch(n)." ></td>
	<td class="line x" title="77:161	The entire graphical model for an example consisting of three domains is shown in Figure 2." ></td>
	<td class="line x" title="78:161	The conditional likelihood of the entire training data (y,x) ={(y(d)1 ,x(d)1 ),,(y(d)Md,x(d)Md)}Dd=1 is given by: P(y|x,w,z) = braceleftBigg Dproductdisplay d=1 Mdproductdisplay k=1 P(y(d)k |x(d)k ,(d)) bracerightBigg     Dproductdisplay d=1 Fdproductdisplay f=1 N((d)f |zpa(f(d)),1)        productdisplay nTnonleaf N(zn|zpa(n),1)    (3) where the terms in the first line of eq." ></td>
	<td class="line x" title="79:161	(3) represent the likelihood of data in each domain given their corresponding model parameters, the second line represents the likelihood of each model parameter in each domain given the hyper-parameter of its parent in the tree hierarchy of features and the last term goes over the entire treeT except the leaf nodes." ></td>
	<td class="line x" title="80:161	Note that in the last term, the hyper-parameters are shared across the domains, so there is no product over d. We perform a MAP estimation for each model parameter as well as the hyper-parameters." ></td>
	<td class="line x" title="81:161	Accordingly, the estimates are given as follows: (d)f = Mdsummationdisplay i=1  (d)f parenleftBig logP(ydi|x(d)i ,(d)) parenrightBig + zpa(f(d)) zn = zpa(n) +summationtextich(n)(|z)i 1 +|ch(n)| (4) where we used the notation (|z)i because node i, the child node of n, could be a parameter node or a hyper-parameter node depending on the position of the node n in the hierarchy." ></td>
	<td class="line x" title="82:161	Essentially, in this model, the weights of the leaf nodes (model parameters) depend on the log-likelihood as well as the prior weight of its parent." ></td>
	<td class="line x" title="83:161	Additionally, the weight 248 of each hyper-parameter node in the tree is computed as the average of all its children nodes and its parent, resulting in a smoothing effect, both up and down the tree." ></td>
	<td class="line x" title="84:161	2.5 An approximate Hierarchical prior model The Hierarchical prior model is a theoretically well founded model for transfer learning through feature heirarchy." ></td>
	<td class="line x" title="85:161	However, our preliminary experiments indicated that its performance on real-life data sets is not as good as expected." ></td>
	<td class="line x" title="86:161	Although a more thorough investigation needs to be carried out, our analysis indicates that the main reason for this phenomenon is over-smoothing." ></td>
	<td class="line x" title="87:161	In other words, by letting the information propagate from the leaf nodes in the hierarchy all the way to the root node, the model loses its ability to discriminate between its features." ></td>
	<td class="line x" title="88:161	As a solution to this problem, we propose an approximate version of this model that weds ideas from the exact heirarchical prior model and the Chelba model." ></td>
	<td class="line x" title="89:161	As with the Chelba prior method in2.3, this approximate hierarchical method also requires two distinct data sets, one for training the prior and another for tuning the final weights." ></td>
	<td class="line x" title="90:161	Unlike Chelba, we smooth the weights of the priors using the featuretree hierarchy presented in1.1, like the hierarchical prior model." ></td>
	<td class="line x" title="91:161	For smoothing of each feature weight, we chose to back-off in the tree as little as possible until we had a large enough sample of prior data (measured as M, the number of subtrees below the current node) on which to form a reliable estimate of the mean and variance of each feature or class of features." ></td>
	<td class="line x" title="92:161	For example, if the tuning data set is as in Sentence 1, but the prior contains no instances of the word Professor, then we would back-off and compute the prior mean and variance on the next higher level in the tree." ></td>
	<td class="line x" title="93:161	Thus the prior for L.1.Professor would be N(mean(L.1.*), variance(L.1.*)), where mean() and variance() of L.1.* are the sample mean and variance of all the features in the prior dataset that match the pattern L.1.*  or, put another way, all the siblings of L.1.Professor in the feature tree." ></td>
	<td class="line x" title="94:161	If fewer than M such siblings exist, we continue backing-off, up the tree, until an ancestor with sufficient descendants is found." ></td>
	<td class="line x" title="95:161	A detailed description of the approximate hierarchical algorithm is shown in table 2." ></td>
	<td class="line x" title="96:161	Input:Dsource = (Xsourcetrain ,Y sourcetrain ) Dtarget = (Xtargettrain ,Y targettrain ); Feature setsFsource,Ftarget; Feature HierarchiesHsource,Htarget Minimum membership size M Train CRF usingDsource to obtain feature weights source For each feature f Ftarget Initialize: node n = f While (n /Hsource or|Leaves(Hsource(n))|M) and nnegationslash= root(Htarget) nPa(Htarget(n)) Compute f and f using the sample {sourcei |iLeaves(Hsource(n))} Train Gaussian prior CRF usingDtarget as data and{f}and{f}as Gaussian prior parameters." ></td>
	<td class="line x" title="97:161	Output:Parameters of the new CRF target." ></td>
	<td class="line x" title="98:161	Table 2: Algorithm for approximate hierarchical prior: Pa(Hsource(n)) is the parent of node n in feature hierarchy Hsource; |Leaves(Hsource(n))| indicates the number of leaf nodes (basic features) under a node n in the hierarchyHsource." ></td>
	<td class="line x" title="99:161	It is important to note that this smoothed tree is an approximation of the exact model presented in 2.4 and thus an important parameter of this method in practice is the degree to which one chooses to smooth up or down the tree." ></td>
	<td class="line x" title="100:161	One of the benefits of this model is that the semantics of the hierarchy (how to define a feature, a parent, how and when to back-off and up the tree, etc.) can be specified by the user, in reference to the specific datasets and tasks under consideration." ></td>
	<td class="line x" title="101:161	For our experiments, the semantics of the tree are as presented in1.1." ></td>
	<td class="line x" title="102:161	The Chelba method can be thought of as a hierarchical prior in which no smoothing is performed on the tree at all." ></td>
	<td class="line x" title="103:161	Only the leaf nodes of the priors feature tree are considered, and, if no match can be found between the tuning and priors training datasets features, a N(0,1) prior is used instead." ></td>
	<td class="line x" title="104:161	However, in the new approximate hierarchical model, even if a certain feature in the tuning dataset does not have an analog in the training dataset, we can always back-off until an appropriate match is found, even to the level of the root." ></td>
	<td class="line x" title="105:161	Henceforth, we will use only the approximate hierarchical model in our experiments and discussion." ></td>
	<td class="line x" title="106:161	249 Table 3: Summary of data used in experiments Corpus Genre Task UTexas Bio Protein Yapex Bio Protein MUC6 News Person MUC7 News Person CSPACE E-mail Person 3 Investigation 3.1 Data, domains and tasks For our experiments, we have chosen five different corpora (summarized in Table 3)." ></td>
	<td class="line x" title="107:161	Although each corpus can be considered its own domain (due to variations in annotation standards, specific task, date of collection, etc), they can also be roughly grouped into three different genres." ></td>
	<td class="line x" title="108:161	These are: abstracts from biological journals [UT (Bunescu et al., 2004), Yapex (Franzen et al., 2002)]; news articles [MUC6 (Fisher et al., 1995), MUC7 (Borthwick et al., 1998)]; and personal e-mails [CSPACE (Kraut et al., 2004)]." ></td>
	<td class="line x" title="109:161	Each corpus, depending on its genre, is labeled with one of two name-finding tasks:  protein names in biological abstracts  person names in news articles and e-mails We chose this array of corpora so that we could evaluate our hierarchical priors ability to generalize across and incorporate information from a variety of domains, genres and tasks." ></td>
	<td class="line x" title="110:161	In each case, each item (abstract, article or e-mail) was tokenized and each token was hand-labeled as either being part of a name (protein or person) or not, respectively." ></td>
	<td class="line x" title="111:161	We used a standard natural language toolkit (Cohen, 2004) to compute tens of thousands of binary features on each of these tokens, encoding such information as capitalization patterns and contextual information from surrounding words." ></td>
	<td class="line x" title="112:161	This toolkit produces features of the type described in1.2 and thus was amenable to our hierarchical prior model." ></td>
	<td class="line x" title="113:161	In particular, we chose to use the simplest default, out-of-the-box feature generator and purposefully did not use specifically engineered features, dictionaries, or other techniques commonly employed to boost performance on such tasks." ></td>
	<td class="line x" title="114:161	The goal of our experiments was to see to what degree named entity recognition problems naturally conformed to hierarchical methods, and not just to achieve the highest performance possible." ></td>
	<td class="line x" title="115:161	0.1 0.2 0.3 0.4 0.5 0.6 0.7 0 20 40 60 80 100 F1 Percent of target-domain data used for tuning Intra-genre transfer performance evaluated on MUC6 (a) GAUSS: tuned on MUC6 (b) CAT: tuned on MUC6+7 (c) HIER: MUC6+7 prior, tuned on MUC6 (d) CHELBA: MUC6+7 prior, tuned on MUC6 Figure 3: Adding a relevant HIER prior helps compared to the GAUSS baseline ((c) > (a)), while simply CATing or using CHELBA can hurt ((d)(b) < (a), except with very little data), and never beats HIER ((c) > (b)(d))." ></td>
	<td class="line x" title="116:161	3.2 Experiments & results We evaluated the performance of various transfer learning methods on the data and tasks described in3.1." ></td>
	<td class="line x" title="117:161	Specifically, we compared our approximate hierarchical prior model (HIER), implemented as a CRF, against three baselines:  GAUSS: CRF model tuned on a single domains data, using a standardN(0,1) prior  CAT: CRF model tuned on a concatenation of multiple domains data, using aN(0,1) prior  CHELBA: CRF model tuned on one domains data, using a prior trained on a different, related domains data (cf.2.3) We use token-level F1 as our main evaluation measure, combining precision and recall into one metric." ></td>
	<td class="line x" title="118:161	3.2.1 Intra-genre, same-task transfer learning Figure 3 shows the results of an experiment in learning to recognize person names in MUC6 news articles." ></td>
	<td class="line x" title="119:161	In this experiment we examined the effect of adding extra data from a different, but related domain from the same genre, namely, MUC7." ></td>
	<td class="line x" title="120:161	Line a shows the F1 performance of a CRF model tuned only on the target MUC6 domain (GAUSS) across a range of tuning data sizes." ></td>
	<td class="line x" title="121:161	Line b shows the same experiment, but this time the CRF model has been tuned on a dataset comprised of a simple concatenation of the training MUC6 data from (a), along with a different training set from MUC7 (CAT)." ></td>
	<td class="line x" title="122:161	We can see that adding extra data in this way, though 250 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 20 40 60 80 100 F1 Percent of target-domain data used for tuning Inter-genre transfer performance evaluated on MUC6 (e) HIER: MUC6+7 prior, tuned on MUC6 (f) CAT: tuned on all domains (g) HIER: all domains prior, tuned on MUC6 (h) CHELBA: all domains prior, tuned on MUC6 Figure 4: Transfer aware priors CHELBA and HIER effectively filter irrelevant data." ></td>
	<td class="line x" title="123:161	Adding more irrelevant data to the priors doesnt hurt ((e)  (g)  (h)), while simply CATing it, in this case, is disastrous ((f) << (e)." ></td>
	<td class="line x" title="124:161	the data is closely related both in domain and task, has actually hurt the performance of our recognizer for training sizes of moderate to large size." ></td>
	<td class="line x" title="125:161	This is most likely because, although the MUC6 and MUC7 datasets are closely related, they are still drawn from different distributions and thus cannot be intermingled indiscriminately." ></td>
	<td class="line x" title="126:161	Line c shows the same combination of MUC6 and MUC7, only this time the datasets have been combined using the HIER prior." ></td>
	<td class="line x" title="127:161	In this case, the performance actually does improve, both with respect to the single-dataset trained baseline (a) and the naively trained double-dataset (b)." ></td>
	<td class="line x" title="128:161	Finally, line d shows the results of the CHELBA prior." ></td>
	<td class="line x" title="129:161	Curiously, though the domains are closely related, it does more poorly than even the non-transfer GAUSS." ></td>
	<td class="line x" title="130:161	One possible explanation is that, although much of the vocabulary is shared across domains, the interpretation of the features of these words may differ." ></td>
	<td class="line x" title="131:161	Since CHELBA doesnt model the hierarchy among features like HIER, it is unable to smooth away these discrepancies." ></td>
	<td class="line x" title="132:161	In contrast, we see that our HIER prior is able to successfully combine the relevant parts of data across domains while filtering the irrelevant, and possibly detrimental, ones." ></td>
	<td class="line x" title="133:161	This experiment was repeated for other sets of intra-genre tasks, and the results are summarized in3.2.3." ></td>
	<td class="line x" title="134:161	3.2.2 Inter-genre, multi-task transfer learning In Figure 4 we see that the properties of the hierarchical prior hold even when transferring across tasks." ></td>
	<td class="line x" title="135:161	Here again we are trying to learn to recognize person names in MUC6 e-mails, but this time, instead of adding only other datasets similarly labeled with person names, we are additionally adding biological corpora (UT & YAPEX), labeled not with person names but with protein names instead, along with the CSPACE e-mail and MUC7 news article corpora." ></td>
	<td class="line x" title="136:161	The robustness of our prior prevents a model trained on all five domains (g) from degrading away from the intra-genre, same-task baseline (e), unlike the model trained on concatenated data (f )." ></td>
	<td class="line x" title="137:161	CHELBA (h) performs similarly well in this case, perhaps because the domains are so different that almost none of the features match between prior and tuning data, and thus CHELBA backs-off to a standardN(0,1) prior." ></td>
	<td class="line x" title="138:161	This robustness in the face of less similarly related data is very important since these types of transfer methods are most useful when one possesses only very little target domain data." ></td>
	<td class="line x" title="139:161	In this situation, it is often difficult to accurately estimate performance and so one would like assurance than any transfer method being applied will not have negative effects." ></td>
	<td class="line x" title="140:161	3.2.3 Comparison of HIER prior to baselines Each scatter plot in Figure 5 shows the relative performance of a baseline method against HIER." ></td>
	<td class="line x" title="141:161	Each point represents the results of two experiments: the y-coordinate is the F1 score of the baseline method (shown on the y-axis), while the xcoordinate represents the score of the HIER method in the same experiment." ></td>
	<td class="line x" title="142:161	Thus, points lying below the y = x line represent experiments for which HIER received a higher F1 value than did the baseline." ></td>
	<td class="line x" title="143:161	While all three plots show HIER outperforming each of the three baselines, not surprisingly, the non-transfer GAUSS method suffers the worst, followed by the naive concatenation (CAT) baseline." ></td>
	<td class="line x" title="144:161	Both methods fail to make any explicit distinction between the source and target domains and thus suffer when the domains differ even slightly from each other." ></td>
	<td class="line x" title="145:161	Although the differences are more subtle, the right-most plot of Figure 5 suggests HIER is likewise able to outperform the nonhierarchical CHELBA prior in certain transfer scenarios." ></td>
	<td class="line x" title="146:161	CHELBA is able to avoid suffering as much as the other baselines when faced with large difference between domains, but is still unable to capture 251 0 .2 .4 .6 .8 1 0 .2 .4 .6 .8 1 GA US S( F1 ) HIER (F1) 0 .2 .4 .6 .8 1 0 .2 .4 .6 .8 1 CA T( F1 ) HIER (F1) .4 .6 .8 .4 .6 .8 CH EL BA (F1 ) HIER (F1)  y = x MUC6@3% MUC6@6% MUC6@13% MUC6@25% MUC6@50% MUC6@100% CSPACE@3% CSPACE@6% CSPACE@13% CSPACE@25% CSPACE@50% CSPACE@100% Figure 5: Comparative performance of baseline methods (GAUSS, CAT, CHELBA) vs. HIER prior, as trained on nine prior datasets (both pure and concatenated) of various sample sizes, evaluated on MUC6 and CSPACE datasets." ></td>
	<td class="line x" title="147:161	Points below the y = x line indicate HIER outperforming baselines." ></td>
	<td class="line x" title="148:161	as many dependencies between domains as HIER." ></td>
	<td class="line x" title="149:161	4 Conclusions, related & future work In this work we have introduced hierarchical feature tree priors for use in transfer learning on named entity extraction tasks." ></td>
	<td class="line x" title="150:161	We have provided evidence that motivates these models on intuitive, theoretical and empirical grounds, and have gone on to demonstrate their effectiveness in relation to other, competitive transfer methods." ></td>
	<td class="line x" title="151:161	Specifically, we have shown that hierarchical priors allow the user enough flexibility to customize their semantics to a specific problem, while providing enough structure to resist unintended negative effects when used inappropriately." ></td>
	<td class="line x" title="152:161	Thus hierarchical priors seem a natural, effective and robust choice for transferring learning across NER datasets and tasks." ></td>
	<td class="line x" title="153:161	Some of the first formulations of the transfer learning problem were presented over 10 years ago (Thrun, 1996; Baxter, 1997)." ></td>
	<td class="line x" title="154:161	Other techniques have tried to quantify the generalizability of certain features across domains (Daume III and Marcu, 2006; Jiang and Zhai, 2006), or tried to exploit the common structure of related problems (Ben-David et al., 2007; Scholkopf et al., 2005)." ></td>
	<td class="line oc" title="155:161	Most of this prior work deals with supervised transfer learning, and thus requires labeled source domain data, though there are examples of unsupervised (Arnold et al., 2007), semi-supervised (Grandvalet and Bengio, 2005; Blitzer et al., 2006), and transductive approaches (Taskar et al., 2003)." ></td>
	<td class="line x" title="156:161	Recent work using so-called meta-level priors to transfer information across tasks (Lee et al., 2007), while related, does not take into explicit account the hierarchical structure of these meta-level features often found in NLP tasks." ></td>
	<td class="line x" title="157:161	Daume allows an extra degree of freedom among the features of his domains, implicitly creating a two-level feature hierarchy with one branch for general features, and another for domain specific ones, but does not extend his hierarchy further (Daume III, 2007))." ></td>
	<td class="line x" title="158:161	Similarly, work on hierarchical penalization (Szafranski et al., 2007) in two-level trees tries to produce models that rely only on a relatively small number of groups of variable, as structured by the tree, as opposed to transferring knowledge between branches themselves." ></td>
	<td class="line x" title="159:161	Our future work is focused on designing an algorithm to optimally choose a smoothing regime for the learned feature trees so as to better exploit the similarities between domains while neutralizing their differences." ></td>
	<td class="line x" title="160:161	Along these lines, we are working on methods to reduce the amount of labeled target domain data needed to tune the prior-based models, looking forward to semi-supervised and unsupervised transfer methods." ></td>
	<td class="line x" title="161:161	252" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1058
An Empirical Study of Semi-supervised Structured Conditional Models for Dependency Parsing
Suzuki, Jun;Isozaki, Hideki;Carreras, Xavier;Collins, Michael John;"></td>
	<td class="line x" title="1:216	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 551560, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:216	c 2009 ACL and AFNLP An Empirical Study of Semi-supervised Structured Conditional Models for Dependency Parsing Jun Suzuki, Hideki Isozaki NTT CS Lab., NTT Corp. Kyoto, 619-0237, Japan jun@cslab.kecl.ntt.co.jp isozaki@cslab.kecl.ntt.co.jp Xavier Carreras, and Michael Collins MIT CSAIL Cambridge, MA 02139, USA carreras@csail.mit.edu mcollins@csail.mit.edu Abstract This paper describes an empirical study of high-performance dependency parsers based on a semi-supervised learning approach." ></td>
	<td class="line x" title="3:216	We describe an extension of semisupervised structured conditional models (SS-SCMs) to the dependency parsing problem, whose framework is originally proposed in (Suzuki and Isozaki, 2008)." ></td>
	<td class="line x" title="4:216	Moreover, we introduce two extensions related to dependency parsing: The first extension is to combine SS-SCMs with another semi-supervised approach, described in (Koo et al., 2008)." ></td>
	<td class="line x" title="5:216	The second extension is to apply the approach to secondorder parsing models, such as those described in (Carreras, 2007), using a twostage semi-supervised learning approach." ></td>
	<td class="line x" title="6:216	We demonstrate the effectiveness of our proposed methods on dependency parsing experiments using two widely used test collections: the Penn Treebank for English, and the Prague Dependency Treebank for Czech." ></td>
	<td class="line x" title="7:216	Our best results on test data in the above datasets achieve 93.79% parent-prediction accuracy for English, and 88.05% for Czech." ></td>
	<td class="line x" title="8:216	1 Introduction Recent work has successfully developed dependency parsing models for many languages using supervised learning algorithms (Buchholz and Marsi, 2006; Nivre et al., 2007)." ></td>
	<td class="line x" title="9:216	Semi-supervised learning methods, which make use of unlabeled data in addition to labeled examples, have the potential to give improved performance over purely supervised methods for dependency parsing." ></td>
	<td class="line x" title="10:216	It is often straightforward to obtain large amounts of unlabeled data, making semi-supervised approaches appealing; previous work on semisupervised methods for dependency parsing includes (Smith and Eisner, 2007; Koo et al., 2008; Wang et al., 2008)." ></td>
	<td class="line x" title="11:216	In particular, Koo et al.(2008) describe a semi-supervised approach that makes use of cluster features induced from unlabeled data, and gives state-of-the-art results on the widely used dependency parsing test collections: the Penn Treebank (PTB) for English and the Prague Dependency Treebank (PDT) for Czech." ></td>
	<td class="line x" title="13:216	This is a very simple approach, but provided significant performance improvements comparing with the stateof-the-art supervised dependency parsers such as (McDonald and Pereira, 2006)." ></td>
	<td class="line x" title="14:216	This paper introduces an alternative method for semi-supervised learning for dependency parsing." ></td>
	<td class="line x" title="15:216	Our approach basically follows a framework proposed in (Suzuki and Isozaki, 2008)." ></td>
	<td class="line x" title="16:216	We extend it for dependency parsing, which we will refer to as a Semi-supervised Structured Conditional Model (SS-SCM)." ></td>
	<td class="line x" title="17:216	In this framework, a structured conditional model is constructed by incorporating a series of generative models, whose parameters are estimated from unlabeled data." ></td>
	<td class="line x" title="18:216	This paper describes a basic method for learning within this approach, and in addition describes two extensions." ></td>
	<td class="line x" title="19:216	The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al., 2008)." ></td>
	<td class="line x" title="20:216	The second extension is to apply the approach to second-order parsing models, more specifically the model of (Carreras, 2007), using a two-stage semi-supervised learning approach." ></td>
	<td class="line x" title="21:216	We conduct experiments on dependency parsing of English (on Penn Treebank data) and Czech (on the Prague Dependency Treebank)." ></td>
	<td class="line x" title="22:216	Our experiments investigate the effectiveness of: 1) the basic SS-SCM for dependency parsing; 2) a combination of the SS-SCM with Koo et al.(2008)s semisupervised approach (even in the case we used the same unlabeled data for both methods); 3) the twostage semi-supervised learning approach that in551 corporates a second-order parsing model." ></td>
	<td class="line x" title="24:216	In addition, we evaluate the SS-SCM for English dependency parsing with large amounts (up to 3.72 billion tokens) of unlabeled data . 2 Semi-supervised Structured Conditional Models for Dependency Parsing Suzuki et al.(2008) describe a semi-supervised learning method for conditional random fields (CRFs) (Lafferty et al., 2001)." ></td>
	<td class="line x" title="26:216	In this paper we extend this method to the dependency parsing problem." ></td>
	<td class="line x" title="27:216	We will refer to this extended method as Semi-supervised Structured Conditional Models (SS-SCMs)." ></td>
	<td class="line x" title="28:216	The remainder of this section describes our approach." ></td>
	<td class="line x" title="29:216	2.1 The Basic Model Throughout this paper we will use x to denote an input sentence, and y to denote a labeled dependency structure." ></td>
	<td class="line x" title="30:216	Given a sentence x with n words, a labeled dependency structure y is a set of n dependencies of the form (h,m,l), where h is the index of the head-word in the dependency, m is the index of the modifier word, and l is the label of the dependency." ></td>
	<td class="line x" title="31:216	We use h = 0 for the root of the sentence." ></td>
	<td class="line x" title="32:216	We assume access to a set of labeled training examples, {xi,yi}Ni=1, and in addition a set of unlabeled examples, {xprimei}Mi=1." ></td>
	<td class="line x" title="33:216	In conditional log-linear models for dependency parsing (which are closely related to conditional random fields (Lafferty et al., 2001)), a distribution over dependency structures for a sentence x is defined as follows: p(y|x) = 1Z(x) exp{g(x,y)}, (1) where Z(x) is the partition function, w is a parameter vector, and g(x,y) = summationdisplay (h,m,l)y wf(x,h,m,l) Here f(x,h,m,l) is a feature vector representing the dependency (h,m,l) in the context of the sentence x (see for example (McDonald et al., 2005a))." ></td>
	<td class="line x" title="34:216	In this paper we extend the definition of g(x,y) to include features that are induced from unlabeled data." ></td>
	<td class="line x" title="35:216	Specifically, we define g(x,y)= summationdisplay (h,m,l)y wf(x,h,m,l) + summationdisplay (h,m,l)y ksummationdisplay j=1 vjqj(x,h,m,l)." ></td>
	<td class="line x" title="36:216	(2) In this model v1,,vk are scalar parameters that may be positive or negative; q1 qk are functions (in fact, generative models), that are trained on unlabeled data." ></td>
	<td class="line x" title="37:216	The vj parameters will dictate the relative strengths of the functions q1 qk, and will be trained on labeled data." ></td>
	<td class="line x" title="38:216	For convenience, we will use v to refer to the vector of parameters v1 vk, and q to refer to the set of generative models q1 qk." ></td>
	<td class="line x" title="39:216	The full model is specified by values for w,v, and q. We will write p(y|x;w,v,q) to refer to the conditional distribution under parameter values w,v,q. We will describe a three-step parameter estimation method that: 1) initializes the q functions (generative models) to be uniform distributions, and estimates parameter values w and v from labeled data; 2) induces new functions q1 qk from unlabeled data, based on the distribution defined by the w,v,q values from step (1); 3) re-estimates w and v on the labeled examples, keeping the q1 qk from step (2) fixed." ></td>
	<td class="line x" title="40:216	The end result is a model that combines supervised training with generative models induced from unlabeled data." ></td>
	<td class="line x" title="41:216	2.2 The Generative Models We now describe how the generative models q1 qk are defined, and how they are induced from unlabeled data." ></td>
	<td class="line x" title="42:216	These models make direct use of the feature-vector definition f(x,y) used in the original, fully supervised, dependency parser." ></td>
	<td class="line x" title="43:216	The first step is to partition the d features in f(x,y) into k separate feature vectors, r1(x,y)rk(x,y) (with the result that f is the concatenation of the k feature vectorsr1 rk)." ></td>
	<td class="line x" title="44:216	In our experiments on dependency parsing, we partitioned f into up to over 140 separate feature vectors corresponding to different feature types." ></td>
	<td class="line x" title="45:216	For example, one feature vector rj might include only those features corresponding to word bigrams involved in dependencies (i.e., indicator functions tied to the word bigram (xm,xh) involved in a dependency (x,h,m,l))." ></td>
	<td class="line x" title="46:216	We then define a generative model that assigns a probability qprimej(x,h,m,l) = djproductdisplay a=1 rj,a(x,h,m,l)j,a (3) to the dj-dimensional feature vectorrj(x,h,m,l)." ></td>
	<td class="line x" title="47:216	The parameters of this model are j,1 j,dj; 552 they form a multinomial distribution, with the constraints that j,a  0, and summationtexta j,a = 1." ></td>
	<td class="line x" title="48:216	This model can be viewed as a very simple (naiveBayes) model that defines a distribution over feature vectors rj  Rdj." ></td>
	<td class="line x" title="49:216	The next section describes how the parameters j,a are trained on unlabeled data." ></td>
	<td class="line x" title="50:216	Given parameters j,a, we can simply define the functions q1 qk to be log probabilities under the generative model: qj(x,h,m,l)=logqprimej(x,h,m,l) = djsummationdisplay a=1 rj,a(x,h,m,l)logj,a. We modify this definition slightly, be introducing scaling factors cj,a > 0, and defining qj(x,h,m,l) = djsummationdisplay a=1 rj,a(x,h,m,l)log j,ac j,a (4) In our experiments, cj,a is simply a count of the number of times the feature indexed by (j,a) appears in unlabeled data." ></td>
	<td class="line x" title="51:216	Thus more frequent features have their contribution down-weighted in the model." ></td>
	<td class="line x" title="52:216	We have found this modification to be beneficial." ></td>
	<td class="line x" title="53:216	2.3 Estimating the Parameters of the Generative Models We now describe the method for estimating the parameters j,a of the generative models." ></td>
	<td class="line x" title="54:216	We assume initial parameters w,v,q, which define a distribution p(y|xprimei;w,v,q) over dependency structures for each unlabeled example xprimei." ></td>
	<td class="line x" title="55:216	We will re-estimate the generative models q, based on unlabeled examples." ></td>
	<td class="line x" title="56:216	The likelihood function on unlabeled data is defined as Msummationdisplay i=1 summationdisplay y p(y|xprimei;w,v,q) summationdisplay (h,m,l)y logqprimej(xprimei,h,m,l), (5) where qprimej is as defined in Eq." ></td>
	<td class="line x" title="57:216	3." ></td>
	<td class="line x" title="58:216	This function resembles the Q function used in the EM algorithm, where the hidden labels (in our case, dependency structures), are filled in using the conditional distribution p(y|xprimei;w,v,q)." ></td>
	<td class="line x" title="59:216	It is simple to show that the estimates j,a that maximize the function in Eq." ></td>
	<td class="line x" title="60:216	5 can be defined as follows." ></td>
	<td class="line x" title="61:216	First, define a vector of expected counts based on w,v,q as rj = Msummationdisplay i=1 summationdisplay y p(y|xprimei;w,v,q) summationdisplay (h,m,l)y rj(xprimei,h,m,l)." ></td>
	<td class="line x" title="62:216	Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm (Baker, 1979) applied to the (Eisner, 1996) dependency-parsing data structures (Paskin, 2001) for projective dependency structures, or the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for nonprojective dependency structures." ></td>
	<td class="line x" title="63:216	The estimates that maximize Eq." ></td>
	<td class="line x" title="64:216	5 are then j,a = rj,asummationtextd j a=1 rj,a . In a slight modification, we employ the following estimates in our model, where  > 1 is a parameter of the model: j,a = ( 1)+ rj,a dj ( 1)+summationtextdja=1 rj,a ." ></td>
	<td class="line x" title="65:216	(6) This corresponds to a MAP estimate under a Dirichlet prior over the j,a parameters." ></td>
	<td class="line x" title="66:216	2.4 The Complete Parameter-Estimation Method This section describes the full parameter estimation method." ></td>
	<td class="line x" title="67:216	The input to the algorithm is a set of labeled examples {xi,yi}Ni=1, a set of unlabeled examples {xprimei}Mi=1, a feature-vector definition f(x,y), and a partition of f into k feature vectors r1 rk which underly the generative models." ></td>
	<td class="line x" title="68:216	The output from the algorithm is a parameter vector w, a set of generative models q1 qk, and parameters v1 vk, which define a probabilistic dependency parsing model through Eqs." ></td>
	<td class="line x" title="69:216	1 and 2." ></td>
	<td class="line x" title="70:216	The learning algorithm proceeds in three steps: Step 1: Estimation of a Fully Supervised Model." ></td>
	<td class="line x" title="71:216	We choose the initial value q0 of the generative models to be the uniform distribution, i.e., we set j,a = 1/dj for all j,a. We then define the regularized log-likelihood function for the labeled examples, with the generative model fixed at q0, to be: L(w,v;q0)= nsummationdisplay i=1 logp(yi|xi;w,v,q0) C2 parenleftBig ||w||2 +||v||2 parenrightBig 553 This is a conventional regularized log-likelihood function, as commonly used in CRF models." ></td>
	<td class="line x" title="72:216	The parameter C > 0 dictates the level of regularization in the model." ></td>
	<td class="line x" title="73:216	We define the initial parameters (w0,v0) = argmaxw,v L(w,v;q0)." ></td>
	<td class="line x" title="74:216	These parameters can be found using conventional methods for estimating the parameters of regularized log-likelihood functions (in our case we use LBFGS (Liu and Nocedal, 1989))." ></td>
	<td class="line x" title="75:216	Note that the gradient of the log-likelihood function can be calculated using the inside-outside algorithm applied to projective dependency parse structures, or the matrix-tree theorem applied to non-projective structures." ></td>
	<td class="line x" title="76:216	Step 2: Estimation of the Generative Models." ></td>
	<td class="line x" title="77:216	In this step, expected count vectors r1 rk are first calculated, based on the distribution p(y|x;w0,v0,q0)." ></td>
	<td class="line x" title="78:216	Generative model parameters j,a are calculated through the definition in Eq." ></td>
	<td class="line x" title="79:216	6; these estimates define updated generative models q1j for j = 1k through Eq." ></td>
	<td class="line x" title="80:216	4." ></td>
	<td class="line x" title="81:216	We refer to the new values for the generative models as q1." ></td>
	<td class="line x" title="82:216	Step 3: Re-estimation of w and v. In the final step, w1 and v1 are estimated as argmaxw,v L(w,v;q1) where L(w,v;q1) is defined in an analogous way to L(w,v;q0)." ></td>
	<td class="line x" title="83:216	Thus w and v are re-estimated to optimize log-likelihood of the labeled examples, with the generative models q1 estimated in step 2." ></td>
	<td class="line x" title="84:216	The final output from the algorithm is the set of parameters (w1,v1,q1)." ></td>
	<td class="line x" title="85:216	Note that it is possible to iterate the methodsteps 2 and 3 can be repeated multiple times (Suzuki and Isozaki, 2008)but in our experiments we only performed these steps once." ></td>
	<td class="line x" title="86:216	3 Extensions 3.1 Incorporating Cluster-Based Features Koo et al.(2008) describe a semi-supervised approach that incorporates cluster-based features, and that gives competitive results on dependency parsing benchmarks." ></td>
	<td class="line x" title="88:216	The method is a two-stage approach." ></td>
	<td class="line x" title="89:216	First, hierarchical word clusters are derived from unlabeled data using the Brown et al. clustering algorithm (Brown et al., 1992)." ></td>
	<td class="line x" title="90:216	Second, a new feature set is constructed by representing words by bit-strings of various lengths, corresponding to clusters at different levels of the hierarchy." ></td>
	<td class="line x" title="91:216	These features are combined with conventional features based on words and part-of-speech tags." ></td>
	<td class="line x" title="92:216	The new feature set is then used within a conventional discriminative, supervised approach, such as the averaged perceptron algorithm." ></td>
	<td class="line x" title="93:216	The important point is that their approach uses unlabeled data only for the construction of a new feature set, and never affects to learning algorithms." ></td>
	<td class="line x" title="94:216	It is straightforward to incorporate clusterbased features within the SS-SCM approach described in this paper." ></td>
	<td class="line x" title="95:216	We simply use the clusterbased feature-vector representation f(x,y) introduced by (Koo et al., 2008) as the basis of our approach." ></td>
	<td class="line x" title="96:216	3.2 Second-order Parsing Models Previous work (McDonald and Pereira, 2006; Carreras, 2007) has shown that second-order parsing models, which include information from sibling or grandparent relationships between dependencies, can give significant improvements in accuracy over first-order parsing models." ></td>
	<td class="line x" title="97:216	In principle it would be straightforward to extend the SS-SCM approach that we have described to second-order parsing models." ></td>
	<td class="line x" title="98:216	In practice, however, a bottleneck for the method would be the estimation of the generative models on unlabeled data." ></td>
	<td class="line x" title="99:216	This step requires calculation of marginals on unlabeled data." ></td>
	<td class="line x" title="100:216	Second-order parsing models generally require more costly inference methods for the calculation of marginals, and this increased cost may be prohibitive when large quantities of unlabeled data are employed." ></td>
	<td class="line x" title="101:216	We instead make use of a simple two-stage approach for extending the SS-SCM approach to the second-order parsing model of (Carreras, 2007)." ></td>
	<td class="line x" title="102:216	In the first stage, we use a first-order parsing model to estimate generative models q1 qk from unlabeled data." ></td>
	<td class="line x" title="103:216	In the second stage, we incorporate these generative models as features within a second-order parsing model." ></td>
	<td class="line x" title="104:216	More precisely, in our approach, we first train a first-order parsing model by Step 1 and 2, exactly as described in Section 2.4, to estimate w0, v0 and q1." ></td>
	<td class="line x" title="105:216	Then, we substitute Step 3 as a supervised learning such as MIRA with a second-order parsing model (McDonald et al., 2005a), which incorporates q1 as a real-values features." ></td>
	<td class="line x" title="106:216	We refer this two-stage approach to as two-stage SS-SCM." ></td>
	<td class="line x" title="107:216	In our experiments we use the 1-best MIRA algorithm (McDonald and Pereira, 2006)1 as a 1We used a slightly modified version of 1-best MIRA, whose difference can be found in the third line in Eq." ></td>
	<td class="line x" title="108:216	7, namely, including L(yi,y)." ></td>
	<td class="line x" title="109:216	554 (a) English dependency parsing Data set (WSJ Sec." ></td>
	<td class="line x" title="110:216	IDs) # of sentences # of tokens Training (0221) 39,832 950,028 Development (22) 1,700 40,117 Test (23) 2,012 47,377 Unlabeled 1,796,379 43,380,315 (b) Czech dependency parsing Data set # of sentences # of tokens Training 73,088 1,255,590 Development 7,507 126,030 Test 7,319 125,713 Unlabeled 2,349,224 39,336,570 Table 1: Details of training, development, test data (labeled data sets) and unlabeled data used in our experiments parameter-estimation method for the second-order parsing model." ></td>
	<td class="line x" title="111:216	In particular, we perform the following optimizations on each update t = 1,,T for re-estimating w and v: min||w(t+1) w(t)||+||v(t+1) v(t)|| s.t. S(xi,yi)S(xi,y)  L(yi,y) y = argmaxy S(xi,y)+L(yi,y), (7) where L(yi,y) represents the loss between correct output of ith sample yi and y. Then, the scoring function S for each y can be defined as follows: S(x,y)=w(f1(x,y)+f2(x,y)) +B ksummationdisplay j=1 vjqj(x,y), (8) where B represents a tunable scaling factor, and f1 and f2 represent the feature vectors of first and second-order parsing parts, respectively." ></td>
	<td class="line x" title="112:216	4 Experiments We now describe experiments investigating the effectiveness of the SS-SCM approach for dependency parsing." ></td>
	<td class="line x" title="113:216	The experiments test basic, firstorder parsing models, as well as the extensions to cluster-based features and second-order parsing models described in the previous section." ></td>
	<td class="line x" title="114:216	4.1 Data Sets We conducted experiments on both English and Czech data." ></td>
	<td class="line x" title="115:216	We used the Wall Street Journal sections of the Penn Treebank (PTB) III (Marcus et al., 1994) as a source of labeled data for English, and the Prague Dependency Treebank (PDT) 1.0 (Hajic, 1998) for Czech." ></td>
	<td class="line x" title="116:216	To facilitate comparisons with previous work, we used exactly the same training, development and test sets Corpus article name (mm/yy) # of sent." ></td>
	<td class="line x" title="117:216	# of tokens BLLIP wsj 00/8700/89 1,796,379 43,380,315 Tipster wsj 04/9003/92 1,550,026 36,583,547 North wsj 07/9412/96 2,748,803 62,937,557 American reu 04/9407/96 4,773,701 110,001,109 Reuters reu 09/9608/97 12,969,056 214,708,766 English afp 05/9412/06 21,231,470 513,139,928 Gigaword apw 11/9412/06 46,978,725 960,733,303 ltw 04/9412/06 10,524,545 230,370,454 nyt 07/9412/06 60,752,363 1,266,531,274 xin 01/9512/06 12,624,835 283,579,330 total 175,949,903 3,721,965,583 Table 2: Details of the larger unlabeled data set used in English dependency parsing: sentences exceeding 128 tokens in length were excluded for computational reasons." ></td>
	<td class="line x" title="118:216	as those described in (McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Koo et al., 2008)." ></td>
	<td class="line x" title="119:216	The English dependencyparsing data sets were constructed using a standard set of head-selection rules (Yamada and Matsumoto, 2003) to convert the phrase structure syntax of the Treebank to dependency tree representations." ></td>
	<td class="line x" title="120:216	We split the data into three parts: sections 02-21 for training, section 22 for development and section 23 for test." ></td>
	<td class="line x" title="121:216	The Czech data sets were obtained from the predefined training/development/test partition in the PDT." ></td>
	<td class="line x" title="122:216	The unlabeled data for English was derived from the Brown Laboratory for Linguistic Information Processing (BLLIP) Corpus (LDC2000T43)2, giving a total of 1,796,379 sentences and 43,380,315 tokens." ></td>
	<td class="line x" title="123:216	The raw text section of the PDT was used for Czech, giving 2,349,224 sentences and 39,336,570 tokens." ></td>
	<td class="line x" title="124:216	These data sets are identical to the unlabeled data used in (Koo et al., 2008), and are disjoint from the training, development and test sets." ></td>
	<td class="line x" title="125:216	The datasets used in our experiments are summarized in Table 1." ></td>
	<td class="line x" title="126:216	In addition, we will describe experiments that make use of much larger amounts of unlabeled data." ></td>
	<td class="line x" title="127:216	Unfortunately, we have no data available other than PDT for Czech, this is done only for English dependency parsing." ></td>
	<td class="line x" title="128:216	Table 2 shows the detail of the larger unlabeled data set used in our experiments, where we eliminated sentences that have more than 128 tokens for computational reasons." ></td>
	<td class="line x" title="129:216	Note that the total size of the unlabeled data reaches 3.72G (billion) tokens, which is approxi2We ensured that the sentences used in the PTB were excluded from the unlabeled data, since sentences used in BLLIP corpus are a super-set of the PTB." ></td>
	<td class="line x" title="130:216	555 mately 4,000 times larger than the size of labeled training data." ></td>
	<td class="line x" title="131:216	4.2 Features 4.2.1 Baseline Features In general we will assume that the input sentences include both words and part-of-speech (POS) tags." ></td>
	<td class="line x" title="132:216	Our baseline features (baseline) are very similar to those described in (McDonald et al., 2005a; Koo et al., 2008): these features track word and POS bigrams, contextual features surrounding dependencies, distance features, and so on." ></td>
	<td class="line x" title="133:216	English POS tags were assigned by MXPOST (Ratnaparkhi, 1996), which was trained on the training data described in Section 4.1." ></td>
	<td class="line x" title="134:216	Czech POS tags were obtained by the following two steps: First, we used feature-based tagger included with the PDT3, and then, we used the method described in (Collins et al., 1999) to convert the assigned rich POS tags into simplified POS tags." ></td>
	<td class="line x" title="135:216	4.2.2 Cluster-based Features In a second set of experiments, we make use of the feature set used in the semi-supervised approach of (Koo et al., 2008)." ></td>
	<td class="line x" title="136:216	We will refer to this as the cluster-based feature set (CL)." ></td>
	<td class="line x" title="137:216	The BLLIP (43M tokens) and PDT (39M tokens) unlabeled data sets shown in Table 1 were used to construct the hierarchical clusterings used within the approach." ></td>
	<td class="line x" title="138:216	Note that when this feature set is used within the SSSCM approach, the same set of unlabeled data is used to both induce the clusters, and to estimate the generative models within the SS-SCM model." ></td>
	<td class="line x" title="139:216	4.2.3 Constructing the Generative Models As described in section 2.2, the generative models in the SS-SCM approach are defined through a partition of the original feature vector f(x,y) into k feature vectors r1(x,y)rk(x,y)." ></td>
	<td class="line x" title="140:216	We follow a similar approach to that of (Suzuki and Isozaki, 2008) in partitioning f(x,y), where the k different feature vectors correspond to different feature types or feature templates." ></td>
	<td class="line x" title="141:216	Note that, in general, we are not necessary to do as above, this is one systematic way of a feature design for this approach." ></td>
	<td class="line x" title="142:216	4.3 Other Experimental Settings All results presented in our experiments are given in terms of parent-prediction accuracy on unla3Training, development, and test data in PDT already contains POS tags assigned by the feature-based tagger." ></td>
	<td class="line x" title="143:216	beled dependency parsing." ></td>
	<td class="line x" title="144:216	We ignore the parentpredictions of punctuation tokens for English, while we retain all the punctuation tokens for Czech." ></td>
	<td class="line x" title="145:216	These settings match the evaluation setting in previous work such as (McDonald et al., 2005a; Koo et al., 2008)." ></td>
	<td class="line x" title="146:216	We used the method proposed by (Carreras, 2007) for our second-order parsing model." ></td>
	<td class="line x" title="147:216	Since this method only considers projective dependency structures, we projectivized the PDT training data in the same way as (Koo et al., 2008)." ></td>
	<td class="line x" title="148:216	We used a non-projective model, trained using an application of the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for the first-order Czech models, and projective parsers for all other models." ></td>
	<td class="line x" title="149:216	As shown in Section 2, SS-SCMs with 1st-order parsing models have two tunable parameters, C and , corresponding to the regularization constant, and the Dirichlet prior for the generative models." ></td>
	<td class="line x" title="150:216	We selected a fixed value  = 2, which was found to work well in preliminary experiments.4 The value of C was chosen to optimize performance on development data." ></td>
	<td class="line x" title="151:216	Note that C for supervised SCMs were also tuned on development data." ></td>
	<td class="line x" title="152:216	For the two-stage SS-SCM for incorporating second-order parsing model, we have additional one tunable parameter B shown in Eq." ></td>
	<td class="line x" title="153:216	8." ></td>
	<td class="line x" title="154:216	This was also chosen by the value that provided the best performance on development data." ></td>
	<td class="line x" title="155:216	In addition to providing results for models trained on the full training sets, we also performed experiments with smaller labeled training sets." ></td>
	<td class="line x" title="156:216	These training sets were either created through random sampling or by using a predefined subset of document IDs from the labeled training data." ></td>
	<td class="line x" title="157:216	5 Results and Discussion Table 3 gives results for the SS-SCM method under various configurations: for first and secondorder parsing models, with and without the cluster features of (Koo et al., 2008), and for varying amounts of labeled data." ></td>
	<td class="line x" title="158:216	The remainder of this section discusses these results in more detail." ></td>
	<td class="line x" title="159:216	5.1 Effects of the Quantity of Labeled Data We can see from the results in Table 3 that our semi-supervised approach consistently gives gains 4An intuitive meaning of  = 2 is that this adds one pseudo expected count to every feature when estimating new parameter values." ></td>
	<td class="line x" title="160:216	556 (a) English dependency parsing: w/ 43M token unlabeled data (BLLIP) WSJ sec." ></td>
	<td class="line x" title="161:216	IDs wsj 21 random selection random selection wsj 1518 wsj 02-21(all) # of sentences / tokens 1,671 / 40,039 2,000 / 48,577 8,000 / 190,958 8,936 / 211,727 39,832 / 950,028 feature type baseline CL baseline CL baseline CL baseline CL baseline CL Supervised SCM (1od) 85.63 86.80 87.02 88.05 89.23 90.45 89.43 90.85 91.21 92.53 SS-SCM (1od) 87.16 88.40 88.07 89.55 90.06 91.45 90.23 91.63 91.72 93.01 (gain over Sup." ></td>
	<td class="line x" title="162:216	SCM) (+1.53) (+1.60) (+1.05) (+1.50) (+0.83) (+1.00) (+0.80) (+0.78) (+0.51) (+0.48) Supervised MIRA (2od) 87.99 89.05 89.20 90.06 91.20 91.75 91.50 92.14 93.02 93.54 2-stage SS-SCM(+MIRA) (2od) 88.88 89.94 90.03 90.90 91.73 92.51 91.95 92.73 93.45 94.13 (gain over Sup." ></td>
	<td class="line x" title="163:216	MIRA) (+0.89) (+0.89) (+0.83) (+0.84) (+0.53) (+0.76) (+0.45) (+0.59) (+0.43) (+0.59) (b) Czech dependency parsing: w/ 39M token unlabeled data (PDT) PDT Doc." ></td>
	<td class="line x" title="164:216	IDs random selection c[0-9]* random selection l[a-i]* (all) # of sentences / tokens 2,000 / 34,722 3,526 / 53,982 8,000 / 140,423 14,891 / 261,545 73,008 /1,225,590 feature type baseline CL baseline CL baseline CL baseline CL baseline CL Supervised SCM (1od) 75.67 77.82 76.88 79.24 80.61 82.85 81.94 84.47 84.43 86.72 SS-SCM (1od) 76.47 78.96 77.61 80.28 81.30 83.49 82.74 84.91 85.00 87.03 (gain over Sup." ></td>
	<td class="line x" title="165:216	SCM) (+0.80) (+1.14) (+0.73) (+1.04) (+0.69) (+0.64) (+0.80) (+0.44) (+0.57) (+0.31) Supervised MIRA (2od) 78.19 79.60 79.58 80.77 83.15 84.39 84.27 85.75 86.82 87.76 2-stage SS-SCM(+MIRA) (2od) 78.71 80.09 80.37 81.40 83.61 84.87 84.95 86.00 87.03 88.03 (gain over Sup." ></td>
	<td class="line x" title="166:216	MIRA) (+0.52) (+0.49) (+0.79) (+0.63) (+0.46) (+0.48) (+0.68) (+0.25) (+0.21) (+0.27) Table 3: Dependency parsing results for the SS-SCM method with different amounts of labeled training data." ></td>
	<td class="line x" title="167:216	Supervised SCM (1od) and Supervised MIRA (2od) are the baseline first and second-order approaches; SS-SCM (1od) and 2-stage SS-SCM(+MIRA) (2od) are the first and second-order approaches described in this paper." ></td>
	<td class="line x" title="168:216	Baseline refers to models without cluster-based features, CL refers to models which make use of cluster-based features." ></td>
	<td class="line x" title="169:216	in performance under various sizes of labeled data." ></td>
	<td class="line x" title="170:216	Note that the baseline methods that we have used in these experiments are strong baselines." ></td>
	<td class="line x" title="171:216	It is clear that the gains from our method are larger for smaller labeled data sizes, a tendency that was also observed in (Koo et al., 2008)." ></td>
	<td class="line x" title="172:216	5.2 Impact of Combining SS-SCM with Cluster Features One important observation from the results in Table 3 is that SS-SCMs can successfully improve the performance over a baseline method that uses the cluster-based feature set (CL)." ></td>
	<td class="line x" title="173:216	This is in spite of the fact that the generative models within the SS-SCM approach were trained on the same unlabeled data used to induce the cluster-based features." ></td>
	<td class="line x" title="174:216	5.3 Impact of the Two-stage Approach Table 3 also shows the effectiveness of the twostage approach (described in Section 3.2) that integrates the SS-SCM method within a second-order parser." ></td>
	<td class="line x" title="175:216	This suggests that the SS-SCM method can be effective in providing features (generative models) used within a separate learning algorithm, providing that this algorithm can make use of realvalued features." ></td>
	<td class="line x" title="176:216	91.5 92.0 92.5 93.0 93.5 10 100 1,000 10,000 CL baseline 43.4M 143M 468M 1.38G3.72G (Mega tokens)Unlabeled data size: [Log-scale]Pa rent -pre dict ion Acc urac y (BLLIP) Figure 1: Impact of unlabeled data size for the SSSCM on development data of English dependency parsing." ></td>
	<td class="line x" title="177:216	5.4 Impact of the Amount of Unlabeled Data Figure 1 shows the dependency parsing accuracy on English as a function of the amount of unlabeled data used within the SS-SCM approach." ></td>
	<td class="line x" title="178:216	(As described in Section 4.1, we have no unlabeled data other than PDT for Czech, hence this section only considers English dependency parsing.)" ></td>
	<td class="line x" title="179:216	We can see that performance does improve as more unlabeled data is added; this trend is seen both with and without cluster-based features." ></td>
	<td class="line x" title="180:216	In addition, Table 4 shows the performance of our proposed method using 3.72 billion tokens of unla557 feature type baseline CL SS-SCM (1st-order) 92.23 93.23 (gain over Sup." ></td>
	<td class="line x" title="181:216	SCM) (+1.02) (+0.70) 2-stage SS-SCM(+MIRA) (2nd-order) 93.68 94.26 (gain over Sup." ></td>
	<td class="line x" title="182:216	MIRA) (+0.66) (+0.72) Table 4: Parent-prediction accuracies on development data with 3.72G tokens unlabeled data for English dependency parsing." ></td>
	<td class="line x" title="183:216	beled data." ></td>
	<td class="line x" title="184:216	Note, however, that the gain in performance as unlabeled data is added is not as sharp as might be hoped, with a relatively modest difference in performance for 43.4 million tokens vs. 3.72 billion tokens of unlabeled data." ></td>
	<td class="line x" title="185:216	5.5 Computational Efficiency The main computational challenge in our approach is the estimation of the generative models q = q1 qk from unlabeled data, particularly when the amount of unlabeled data used is large." ></td>
	<td class="line x" title="186:216	In our implementation, on the 43M token BLLIP corpus, using baseline features, it takes about 5 hours to compute the expected counts required to estimate the parameters of the generative models on a single 2.93GHz Xeon processor." ></td>
	<td class="line x" title="187:216	It takes roughly 18 days of computation to estimate the generative models from the larger (3.72 billion word) corpus." ></td>
	<td class="line x" title="188:216	Fortunately it is simple to parallelize this step; our method takes a few hours on the larger data set when parallelized across around 300 separate processes." ></td>
	<td class="line x" title="189:216	Note that once the generative models have been estimated, decoding with the model, or training the model on labeled data, is relatively inexpensive, essentially taking the same amount of computation as standard dependency-parsing approaches." ></td>
	<td class="line x" title="190:216	5.6 Results on Test Data Finally, Table 5 displays the final results on test data." ></td>
	<td class="line x" title="191:216	There results are obtained using the best setting in terms of the development data performance." ></td>
	<td class="line x" title="192:216	Note that the English dependency parsing results shown in the table were achieved using 3.72 billion tokens of unlabeled data." ></td>
	<td class="line x" title="193:216	The improvements on test data are similar to those observed on the development data." ></td>
	<td class="line x" title="194:216	To determine statistical significance, we tested the difference of parent-prediction error-rates at the sentence level using a paired Wilcoxon signed rank test." ></td>
	<td class="line x" title="195:216	All eight comparisons shown in Table 5 are significant with (a) English dependency parsing: w/ 3.72G token ULD feature set baseline CL SS-SCM (1st-order) 91.89 92.70 (gain over Sup." ></td>
	<td class="line x" title="196:216	SCM) (+0.92) (+0.58) 2-stage SS-SCM(+MIRA) (2nd-order) 93.41 93.79 (gain over Sup." ></td>
	<td class="line x" title="197:216	MIRA) (+0.65) (+0.48) (b) Czech dependency parsing: w/ 39M token ULD (PDT) feature set baseline CL SS-SCM (1st-order) 84.98 87.14 (gain over Sup." ></td>
	<td class="line x" title="198:216	SCM) (+0.58) (+0.39) 2-stage SS-SCM(+MIRA) (2nd-order) 86.90 88.05 (gain over Sup." ></td>
	<td class="line x" title="199:216	MIRA) (+0.15) (+0.36) Table 5: Parent-prediction accuracies on test data using the best setting in terms of development data performances in each condition." ></td>
	<td class="line x" title="200:216	(a) English dependency parsers on PTB dependency parser test description (McDonald et al., 2005a) 90.9 1od (McDonald and Pereira, 2006) 91.5 2od (Koo et al., 2008) 92.23 1od, 43M ULD SS-SCM (w/ CL) 92.70 1od, 3.72G ULD (Koo et al., 2008) 93.16 2od, 43M ULD 2-stage SS-SCM(+MIRA, w/ CL) 93.79 2od, 3.72G ULD (b) Czech dependency parsers on PDT dependency parser test description (McDonald et al., 2005b) 84.4 1od (McDonald and Pereira, 2006) 85.2 2od (Koo et al., 2008) 86.07 1od, 39M ULD (Koo et al., 2008) 87.13 2od, 39M ULD SS-SCM (w/ CL) 87.14 1od, 39M ULD 2-stage SS-SCM(+MIRA, w/ CL) 88.05 2od, 39M ULD Table 6: Comparisons with the previous top systems: (1od, 2od: 1stand 2nd-order parsing model, ULD: unlabeled data)." ></td>
	<td class="line x" title="201:216	p < 0.01." ></td>
	<td class="line x" title="202:216	6 Comparison with Previous Methods Table 6 shows the performance of a number of state-of-the-art approaches on the English and Czech data sets." ></td>
	<td class="line x" title="203:216	For both languages our approach gives the best reported figures on these datasets." ></td>
	<td class="line x" title="204:216	Our results yield relative error reductions of roughly 27% (English) and 20% (Czech) over McDonald and Pereira (2006)s second-order supervised dependency parsers, and roughly 9% (English) and 7% (Czech) over the previous best results provided by Koo et." ></td>
	<td class="line x" title="205:216	al." ></td>
	<td class="line x" title="206:216	(2008)s secondorder semi-supervised dependency parsers." ></td>
	<td class="line oc" title="207:216	Note that there are some similarities between our two-stage semi-supervised learning approach and the semi-supervised learning method introduced by (Blitzer et al., 2006), which is an extension of the method described by (Ando and Zhang, 558 2005)." ></td>
	<td class="line o" title="208:216	In particular, both methods use a two-stage approach; They first train generative models or auxiliary problems from unlabeled data, and then, they incorporate these trained models into a supervised learning algorithm as real valued features." ></td>
	<td class="line o" title="209:216	Moreover, both methods make direct use of existing feature-vector definitions f(x,y) in inducing representations from unlabeled data." ></td>
	<td class="line x" title="210:216	7 Conclusion This paper has described an extension of the semi-supervised learning approach of (Suzuki and Isozaki, 2008) to the dependency parsing problem." ></td>
	<td class="line x" title="211:216	In addition, we have described extensions that incorporate the cluster-based features of Koo et al.(2008), and that allow the use of second-order parsing models." ></td>
	<td class="line x" title="213:216	We have described experiments that show that the approach gives significant improvements over state-of-the-art methods for dependency parsing; performance improves when the amount of unlabeled data is increased from 43.8 million tokens to 3.72 billion tokens." ></td>
	<td class="line x" title="214:216	The approach should be relatively easily applied to languages other than English or Czech." ></td>
	<td class="line x" title="215:216	We stress that the SS-SCM approach requires relatively little hand-engineering: it makes direct use of the existing feature-vector representation f(x,y) used in a discriminative model, and does not require the design of new features." ></td>
	<td class="line x" title="216:216	The main choice in the approach is the partitioning of f(x,y) into components r1(x,y)rk(x,y), which in our experience is straightforward." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1158
Domain adaptive bootstrapping for named entity recognition
Wu, Dan;Lee, Wee Sun;Ye, Nan;Chieu, Hai Leong;"></td>
	<td class="line x" title="1:234	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 15231532, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:234	c 2009 ACL and AFNLP Domain adaptive bootstrapping for named entity recognition Dan Wu1, Wee Sun Lee2, Nan Ye2 1Singapore MIT Alliance 2Department of Computer Science National University of Singapore {dwu@,leews@comp,g0701171@}nus.edu.sg Hai Leong Chieu DSO National Laboratories chaileon@dso.org.sg Abstract Bootstrapping is the process of improving the performance of a trained classifier by iteratively adding data that is labeled by the classifier itself to the training set, and retraining the classifier." ></td>
	<td class="line x" title="3:234	It is often used in situations where labeled training data is scarce but unlabeled data is abundant." ></td>
	<td class="line x" title="4:234	In this paper, we consider the problem of domain adaptation: the situation where training data may not be scarce, but belongs to a different domain from the target application domain." ></td>
	<td class="line x" title="5:234	As the distribution of unlabeled data is different from the training data, standard bootstrapping often has difficulty selecting informative data to add to the training set." ></td>
	<td class="line x" title="6:234	We propose an effective domain adaptive bootstrapping algorithm that selects unlabeled target domain data that are informative about the target domain and easy to automatically label correctly." ></td>
	<td class="line x" title="7:234	We call these instances bridges, as they are used to bridge the source domain to the target domain." ></td>
	<td class="line x" title="8:234	We show that the method outperforms supervised, transductive and bootstrapping algorithms on the named entity recognition task." ></td>
	<td class="line x" title="9:234	1 Introduction Most recent researches on natural language processing (NLP) problems are based on machine learning algorithms." ></td>
	<td class="line x" title="10:234	High performance can often be achieved if the system is trained and tested on data from the same domain." ></td>
	<td class="line x" title="11:234	However, the performance of NLP systems often degrades badly when the test data is drawn from a source that is different from the labeled data used to train the system." ></td>
	<td class="line x" title="12:234	For named entity recognition (NER), for example, Ciaramita and Altun (2005) reported that a system trained on a labeled Reuters corpus achieved an F-measure of 91% on a Reuters test set, but only 64% on a Wall Street Journal test set." ></td>
	<td class="line x" title="13:234	The task of adapting a system trained on one domain (called the source domain) to a new domain (called the target domain) is called domain adaptation." ></td>
	<td class="line x" title="14:234	In domain adaptation, it is generally assumed that we have labeled data in the source domain while labeled data may or may not be available in the target domain." ></td>
	<td class="line oc" title="15:234	Previous work in domain adaptation can be classified into two categories: [S+T+], where a small, labeled target domain data is available, e.g.(Blitzer et al., 2006; Jiang and Zhai, 2007; Daume III, 2007; Finkel and Manning, 2009), or [S+T-], where no labeled target domain data is available, e.g.(Blitzer et al., 2006; Jiang and Zhai, 2007)." ></td>
	<td class="line o" title="18:234	In both cases, and especially for [S+T-], domain adaptation can leverage on large amounts of unlabeled data in the target domain." ></td>
	<td class="line x" title="19:234	In practice, it is often unreasonable to expect labeled data for every new domain that we come across, such as blogs, emails, a different newspaper agency, or simply articles from a different topic or period in time." ></td>
	<td class="line p" title="20:234	Thus although [S+T+] is easier to handle, [S+T-] is of higher practical importance." ></td>
	<td class="line o" title="21:234	In this paper, we propose a domain adaptive bootstrapping (DAB) approach to tackle the domain adaptation problem under the setting [S+T-]." ></td>
	<td class="line x" title="22:234	Bootstrapping is an iterative process that uses a trained classifier to label and select unlabeled instances to add to the training set for retraining the classifier." ></td>
	<td class="line x" title="23:234	It is often used when labeled training data is scarce but unlabeled data is abundant." ></td>
	<td class="line x" title="24:234	In contrast, for domain adaptation problems, we may have a lot of training data but the target application domain has a different data distribution." ></td>
	<td class="line x" title="25:234	Standard bootstrapping usually selects instances that are most confidently labeled from the unlabeled data." ></td>
	<td class="line x" title="26:234	In domain adaptation situations, usually the most confidently labeled instances are the ones that are most similar to the source domain in1523 stances these instances tend to contain very little information about the target domain." ></td>
	<td class="line x" title="27:234	For domain adaptive bootstrapping, we propose a selection criterion that selects instances that are informative and easy to automatically label correctly." ></td>
	<td class="line x" title="28:234	In addition, we propose a criterion for stopping the process of bootstrapping before it adds uninformative and incorrectly labeled instances that can reduce performance." ></td>
	<td class="line x" title="29:234	Our approach leverages on instances in the target domain called bridges." ></td>
	<td class="line x" title="30:234	These instances contain domain-independent features, as well as features specific to the target domain." ></td>
	<td class="line x" title="31:234	As they contain domain-independent features, they can be classified correctly by classifiers trained on the source domain labeled data." ></td>
	<td class="line x" title="32:234	We argue that these instances act as a bridge between the source and the target domain." ></td>
	<td class="line x" title="33:234	We show that, on the NER task, DAB outperforms supervised, transductive and standard bootstrapping algorithms, as well as a bootstrapping variant, called balanced bootstrapping (Jiang and Zhai, 2007), that has recently been proposed for domain adaptation." ></td>
	<td class="line x" title="34:234	2 Related work One general class of approaches to domain adaptation is to consider that the instances from the source and the target domain are drawn from different distributions." ></td>
	<td class="line o" title="35:234	Bickel et al.(Bickel et al., 2007) discriminatively learns a scaling factor for source domain training data, so as to adapt the source domain data distribution to resemble the target domain data distribution, under the [S+T-] setting." ></td>
	<td class="line x" title="37:234	Daume III and Marcu (Daume III and Marcu, 2006) considers that the data distribution is a mixture distribution over general, source domain and target domain data." ></td>
	<td class="line o" title="38:234	They learn the underlying mixture distribution using the conditional expectation maximization algorithm, under the [S+T+] setting." ></td>
	<td class="line o" title="39:234	Jiang and Zhai (2007) proposed an instance re-weighting framework that handles both the [S+T+] and [S+T-] settings." ></td>
	<td class="line p" title="40:234	For [S+T-], the resulting algorithm is a balanced bootstrapping algorithm, which was shown to outperform the standard bootstrapping algorithm." ></td>
	<td class="line p" title="41:234	In this paper, we assume the [S+T-] settings, and we show that the approach proposed in this paper, domain adaptive bootstrapping (DAB), outperforms the balanced bootstrapping algorithm on NER." ></td>
	<td class="line x" title="42:234	Another class of approaches to domain adaptation is feature-based." ></td>
	<td class="line x" title="43:234	Daume III (Daume III, 2007) divided features into three classes: domainindependent features, source-domain features and target-domain features." ></td>
	<td class="line o" title="44:234	He assumed the existence of training data in the target-domain (under the setting [S+T+]), so that the three classes of features can be jointly trained using source and target domain labeled data." ></td>
	<td class="line o" title="45:234	This cannot be done in the setting [S+T-], where no training data is available in the target domain." ></td>
	<td class="line oc" title="46:234	Using a different approach, Blitzer et al.(2006) induces correspondences between feature spaces in different domains, by detecting pivot features." ></td>
	<td class="line x" title="48:234	Pivot features are features that occur frequently and behave similarly in different domains." ></td>
	<td class="line x" title="49:234	Pivot features are used to put domain-specific features in correspondence." ></td>
	<td class="line x" title="50:234	In this paper, instead of pivot features, we attempt to leverage on pivot instances that we call bridges, which are instances that bridge the source and target domain." ></td>
	<td class="line x" title="51:234	This will be illustrated in Section 3." ></td>
	<td class="line x" title="52:234	It is generally recognized that adding informative and correctly labeled instances is more useful for learning." ></td>
	<td class="line x" title="53:234	Active learning queries the user for labels of most informative or relevant instances." ></td>
	<td class="line x" title="54:234	Active learning, which has been applied to the problem of NER in (Shen et al., 2004), is used in situations where a large amount of unlabeled data exists and data labeling is expensive." ></td>
	<td class="line x" title="55:234	It has also been applied to the problem of domain adaptation for word sense disambiguation in (Chan and Ng, 2007)." ></td>
	<td class="line x" title="56:234	However, active learning requires human intervention." ></td>
	<td class="line x" title="57:234	Here, we want to achieve the same goal without human intervention." ></td>
	<td class="line o" title="58:234	3 Bootstrapping for domain adaptation We first define the notations used for domain adaptation in the [S+T-] setting." ></td>
	<td class="line x" title="59:234	A set of training data DS = {xi,yi}1i|DS| is given in the source domain, where the notation |X| denotes the size of a set X. Each instance xi in DS has been manually annotated with a label, yi, from a given set of labels Y . The objective of domain adaptation is to label a set of unlabeled data, DT = {xi}1i|DT| with labels from Y . A machine learning algorithm will take a labeled data set (for e.g. DS) and outputs a classifier, which can then be used to classify unlabeled data, i.e. assign labels to unlabeled instances." ></td>
	<td class="line x" title="60:234	A special class of machine learning algorithms, called transductive learning algorithms, is able to take the unlabeled data DT into account during the learning process (see e.g.(Joachims, 1999))." ></td>
	<td class="line x" title="62:234	1524 However, such algorithms do not take into account the shift in domain of the test data." ></td>
	<td class="line x" title="63:234	Jiang and Zhai (2007) recently proposed an instance re-weighting framework to take domain shift into account." ></td>
	<td class="line o" title="64:234	For [S+T-], the resulting algorithm is a balanced bootstrapping algorithm, which we describe below." ></td>
	<td class="line x" title="65:234	3.1 Standard and balanced bootstrapping We define a general bootstrapping algorithm in Algorithm 1." ></td>
	<td class="line x" title="66:234	The algorithm can be applied to any machine learning algorithm that allows training instances to be weighted, and that gives confidence scores for the labels when used to classify test data." ></td>
	<td class="line x" title="67:234	The bootstrapping procedure iteratively improves the performance of a classifier SCt over a number of iterations." ></td>
	<td class="line x" title="68:234	In Algorithm 1, we have left a number of parameters unspecified." ></td>
	<td class="line x" title="69:234	These parameters are (1) the selection-criterion for instances to be added to the training data, (2) the terminationcriterion for the bootstrapping process, and (3) the weights (wS,wT) given to the labeled and bootstrapped training sets." ></td>
	<td class="line x" title="70:234	Standard bootstrapping: (Jiang and Zhai, 2007) the selection-criterion is based on selecting the top k most-confidently labeled instances in Rt." ></td>
	<td class="line x" title="71:234	The weight wSt is equal to wTt . The value of k is a parameter for the bootstrapping algorithm." ></td>
	<td class="line x" title="72:234	Balanced bootstrapping: (Jiang and Zhai, 2007) the selection-criterion is still based on selecting the top k most-confidently labeled instances in Rt." ></td>
	<td class="line x" title="73:234	Balanced bootstrapping was formulated for domain adaptation, and hence they set the weights to satisfy the ratio wStwT t = |Tt||DS|." ></td>
	<td class="line x" title="74:234	This allows the small amount of target data added, Tt, to have an equal weight to the large source domain training set DS." ></td>
	<td class="line x" title="75:234	In this paper, we formulate a selection-criterion and a termination-criterion which are better than those used in standard and balanced bootstrapping." ></td>
	<td class="line x" title="76:234	Regarding the selection-criterion, standard and balanced bootstrapping both select instances which are confidently labeled by SCt to be used for training SCt+1, in the hope of avoiding using wrongly labeled data in bootstrapping." ></td>
	<td class="line x" title="77:234	However, instances that are already confidently labeled by SCt may not contain sufficient information which is not in DS, and using them to train SCt+1 may result in SCt+1 performing similarly to SCt." ></td>
	<td class="line x" title="78:234	This motivates us to select samples which are both informative and easy to automatically label correctly." ></td>
	<td class="line x" title="79:234	Regarding the termination-criterion, which Algorithm 1 Bootstrapping algorithm Input: labeled data DS, test data DT and a machine learning algorithm." ></td>
	<td class="line x" title="80:234	Output: the predicted labels of the set DT . Set T0 = , R0 = DT , and t = 0 Repeat 1." ></td>
	<td class="line x" title="81:234	learn a classifier SCt with (DS,Tt) with weights (wSt ,wTt ) 2." ></td>
	<td class="line x" title="82:234	label the set Rt with SCt 3." ></td>
	<td class="line x" title="83:234	select St  Rt based on selection-criterion 4." ></td>
	<td class="line x" title="84:234	Tt+1 = Tt St, and Rt+1 = Rt \St. Until termination-criterion Output the predicted labels of DT by SCt." ></td>
	<td class="line x" title="85:234	is not mentioned in the paper (Jiang and Zhai, 2007), we assume that bootstrapping is simply run for either a single iteration, or a small and fixed number of iterations." ></td>
	<td class="line x" title="86:234	However, it is known that such simple criterion may result in stopping too early or too late, leading to sub-optimal performance." ></td>
	<td class="line x" title="87:234	We propose a more effective terminationcriterion here." ></td>
	<td class="line x" title="88:234	3.2 Domain adaptive bootstrapping (DAB) Our selection-criterion relies on the observation that in domain adaptation, instances (from the source or the target domain) can be divided into three types according to their information content: generalists are instances that contain only domainindependent information and are present in all domains; specialists are instances containing only domain-specific information and are present only in their respective domains; bridges are instances containing both domain-independent and domainspecific information, also present only in their respective domains but are useful as a bridge between the source and the target domains." ></td>
	<td class="line x" title="89:234	The implication of the above observation is that when choosing unlabeled target domain data for bootstrapping, we should exploit the bridges, because the generalists are not likely to contain much information not in DS due to their domainindependence, and the specialists are difficult to be labeled correctly due to their domain-specificity." ></td>
	<td class="line x" title="90:234	In contrast, the bridges are informative and easier to label correctly." ></td>
	<td class="line x" title="91:234	Choosing confidently classified instances for bootstrapping, as in standard bootstrapping and balanced bootstrapping, is simple, but results in choosing mostly generalists, and is too conservative." ></td>
	<td class="line x" title="92:234	We design a scoring function 1525 on instances, which has high value when the instance is informative and sufficiently likely to be correctly labeled in order to identify correctly labeled bridges." ></td>
	<td class="line x" title="93:234	Intuitively, informativeness of an instance can be measured by the prediction results of the ideal classifier IS for the source domain and the ideal classifier IT for the target domain." ></td>
	<td class="line x" title="94:234	If IS and IT are both probabilistic classifiers, IS should return a noninformative distribution while IT should return an informative one." ></td>
	<td class="line x" title="95:234	The ideal classifier for the source domain is approximated with a source classifier SC trained on DS, while the ideal classifier for the target domain is approximated by training a classifier, TC, on target domain instances labeled by the source classifier." ></td>
	<td class="line x" title="96:234	We also try to ensure that instances that are selected are correctly classified." ></td>
	<td class="line x" title="97:234	As the label used is provided by the target classifier, we estimate the precision of the target classification." ></td>
	<td class="line x" title="98:234	The final ranking function is constructed by combining this estimate with the informativeness of the instance." ></td>
	<td class="line x" title="99:234	We show the algorithm for the instance selection in Algorithm 2." ></td>
	<td class="line x" title="100:234	The notations used follow those used in Algorithm 1." ></td>
	<td class="line x" title="101:234	For simplicity, we assume that wSt = wTt = 1 for all t. We expect TC to be a reasonable classifier on DT due to the presence of generalists and bridges." ></td>
	<td class="line x" title="102:234	Note that the target classifier is constructed by randomly splitting DT into two partitions, training a classifier on each partition and using the prediction of the trained classifier on the partition it is not trained on." ></td>
	<td class="line x" title="103:234	This is because classifiers tend to fit the data that they have been trained on too well making the probability estimates on their training data unreliable." ></td>
	<td class="line x" title="104:234	Also, a random partition is used to ensure that the data in each partition is representative of Du." ></td>
	<td class="line x" title="105:234	3.3 The scoring function: score(p(s),p(t)) The scoring function score(p(s),p(t)) in Algorithm 2 is simply implemented as the product of two components: a measure of the informativeness and the probability that SCs label is correct." ></td>
	<td class="line x" title="106:234	We show how the intuitive ideas (described above) behind these two components are formalized." ></td>
	<td class="line x" title="107:234	Informativeness of a distribution p on a set of discrete labels Y is measured by its entropy h(p) defined by h(p) = summationdisplay yY p(y)logp(y)." ></td>
	<td class="line x" title="108:234	Algorithm 2 Algorithm for selecting instances for bootstrapping at iteration t Input: Labeled source domain data DS, target domain training data Tt, remaining data Rt, the classifier SCt trained on DS Tt, and a scoring function score(p(s),p(t)) Output: k instances for bootstrapping." ></td>
	<td class="line x" title="109:234	1." ></td>
	<td class="line x" title="110:234	Label Rt with SCt, and to each instance xi  Rt, SCt outputs a distribution p(s)i (yi) over its labels." ></td>
	<td class="line x" title="111:234	2." ></td>
	<td class="line x" title="112:234	Randomly split Rt into two partitions, R0t and R1t with their labels assigned by SCt." ></td>
	<td class="line x" title="113:234	3." ></td>
	<td class="line x" title="114:234	Train each target classifier, TCxt with the data Rxt , for x = {0,1}." ></td>
	<td class="line x" title="115:234	4." ></td>
	<td class="line x" title="116:234	Label R(1x)t with the classifier TCxt , which to each instance xi  Rt, outputs a distribution p(t)i (yi) over its labels." ></td>
	<td class="line x" title="117:234	5." ></td>
	<td class="line x" title="118:234	Score each instance from xi  Rt with the function score(p(s)i ,p(t)i )." ></td>
	<td class="line x" title="119:234	6." ></td>
	<td class="line x" title="120:234	Select top k instances from Rt with the highest scores." ></td>
	<td class="line x" title="121:234	h(p) is nonnegative; h(p) = 0 if and only if p has probability 1 on one of the labels; h(p) attains its maximum value when the distribution p is uniform over all labels." ></td>
	<td class="line x" title="122:234	Hence, an instance is classified with high confidence when the distribution over its labels has low entropy." ></td>
	<td class="line x" title="123:234	We measure the informativeness of an instance using h(p(s))h(p(t)), where p(s) and p(t) are as in Algorithm 2." ></td>
	<td class="line x" title="124:234	We argue that a larger value of this expression implies that the instance is more likely to be a bridge instance." ></td>
	<td class="line x" title="125:234	This expression has a high value when the source classifier is uncertain, and the target classifier is certain." ></td>
	<td class="line x" title="126:234	Uncertain classification by the source classifier indicates that the instance is unlikely to be a generalist." ></td>
	<td class="line x" title="127:234	Moreover, if the target classifier is certain on xi, it means that instances similar to the instance xi are consistently labeled with the same label by the source classifier SCt, indicating that it is likely to be a bridge instance." ></td>
	<td class="line x" title="128:234	The probability that TCs label is correct cannot be estimated directly because we do not have labeled target domain data." ></td>
	<td class="line x" title="129:234	Instead, we use the source domain to give an estimate." ></td>
	<td class="line x" title="130:234	We do this with a simple pre-processing step: we split the data DS into two partitions of equal size, train a classifier on each partition, and test each classifier on the 1526 other partition." ></td>
	<td class="line x" title="131:234	We then measure the resulting accuracy given each label: (y) = # correctly labeled instances of label y# total instances of label y . Summarizing the above discussion, the scoring function is as shown below." ></td>
	<td class="line x" title="132:234	score(p(s),p(t)) = (y) bracketleftBig h(p(s))h(p(t)) bracketrightBig , where y = argmaxyY p(s)(y) The scoring function has a high value when the information content of the example is high and the label has high precision." ></td>
	<td class="line x" title="133:234	3.4 The termination criterion Intuitively, our algorithm terminates when there are not enough informative instances." ></td>
	<td class="line x" title="134:234	Formally, we define the termination criterion as follows: we terminate the bootstrapping process when, there exists an instance xi in the top k instances satisfying the following condition: 1." ></td>
	<td class="line x" title="135:234	h(p(s)i ) < h(p(t)i ), or 2." ></td>
	<td class="line x" title="136:234	maxyY p(s)i (y) > maxyY p(t)i (y) The second case is used to check for instances where the classifier SCt is more confident than the target classifiers TCxt , on their respective predicted labels." ></td>
	<td class="line x" title="137:234	This shows that the instance xi is more of a generalist than a bridge." ></td>
	<td class="line x" title="138:234	4 NER task and implementation The algorithm described in Section 3 is not specific to any particular application." ></td>
	<td class="line x" title="139:234	In this paper, we apply it to the problem of named entity recognition (NER)." ></td>
	<td class="line x" title="140:234	In this section, we describe the NER classifier and the features used in our experiments." ></td>
	<td class="line x" title="141:234	4.1 NER features We used the features generated by the CRF package (Finkel et al., 2005)." ></td>
	<td class="line x" title="142:234	These features include the word string feature, the case feature for the current word, the context words for the current word and their cases, the presence in dictionaries for the current word, the position of the current word in the sentence, prefix and suffix of the current word as well as the case information of the multiple occurrences of the current word." ></td>
	<td class="line x" title="143:234	We use the same set of features for all classifiers used in the bootstrapping process, and for all baselines used in the experimental section." ></td>
	<td class="line x" title="144:234	4.2 Machine learning algorithms A base machine learning algorithm is required in bootstrapping approaches." ></td>
	<td class="line x" title="145:234	We describe the two machine learning algorithms used in this paper." ></td>
	<td class="line x" title="146:234	We chose these algorithms for their good performance on the NER task." ></td>
	<td class="line x" title="147:234	Maximum entropy classification (MaxEnt): The MaxEnt approach, or logistic regression, is one of the most competitive methods for named entity recognition (Tjong and Meulder, 2003)." ></td>
	<td class="line x" title="148:234	MaxEnt is a discriminative method that learns a distribution, p(yi|xi), over the labels, yi, given the vector of features, xi." ></td>
	<td class="line x" title="149:234	We used the implementation of MaxEnt classifier described in (Manning and Klein, 2003)." ></td>
	<td class="line x" title="150:234	For NER, each instance represents a single word token within a sentence, with the feature vector xi derived from the sentence as described in the previous section." ></td>
	<td class="line x" title="151:234	MaxEnt is not designed for sequence classification." ></td>
	<td class="line x" title="152:234	To deal with sequences, each name-class (e.g. PERSON) is divided into sub-classes: first token (e.g. PERSON-begin), unique token (e.g. PERSONunique), or subsequent tokens (e.g. PERSONcontinue) in the name-class." ></td>
	<td class="line x" title="153:234	To ensure that the results returned by MaxEnt is coherent, we define deterministic transition probabilities that disallow transitions such as one from PERSON-begin to LOCATION-continue." ></td>
	<td class="line x" title="154:234	A Viterbi parse is used to find the valid sequence of name-classes with the highest probability." ></td>
	<td class="line x" title="155:234	Support vector machines (SVM): The basic idea behind SVM for binary classification problems is to consider the data points in their feature space, and to separate the two classes with a hyper-plane, by maximizing the shortest distance between the data points and the hyper-plane." ></td>
	<td class="line x" title="156:234	If there exists no hyperplane that can split the two labels, the soft margin version of SVM will choose a hyperplane that splits the examples as cleanly as possible, while still maximizing the distance to the nearest cleanly split examples (Joachims, 2002)." ></td>
	<td class="line x" title="157:234	We used the SVMlight package for our experiments (Joachims, 2002)." ></td>
	<td class="line x" title="158:234	For the multi-label NER classification with N classes, we learn N SVM classifiers, and use a softmax function to obtain the distribution." ></td>
	<td class="line x" title="159:234	Formally, denoting by s(y) the confidence returned by the classifier for each label y  Y , the probability of the label yi is given by p(yi|xi) = exp(s(yi))summationtext yY exp(s(y)) 1527 Similarly to MaxEnt, we subdivide name-classes into begin, continue, and unique sub-classes, and use a Viterbi parse for the sequence of highest probability." ></td>
	<td class="line x" title="160:234	The SVMlight package also implements a transductive version of the SVM algorithm." ></td>
	<td class="line x" title="161:234	We also compare our approach with the transductive SVM (Joachims, 1999) in our experimental results." ></td>
	<td class="line x" title="162:234	5 Experimental results In this paper, we use the annotated data provided by the Automatic Content Extraction (ACE) program." ></td>
	<td class="line x" title="163:234	The ACE data set is annotated for an Entity Detection task, and the annotation consists of the labeling of entity names (e.g. Powell) and mentions for each entity (e.g. pronouns such as he)." ></td>
	<td class="line x" title="164:234	In this paper, we are interested in the problem of recognition of the proper names (the named entity recognition task), and hence use only entities labeled with the type NAM (LDC, 2005)." ></td>
	<td class="line x" title="165:234	Entities are classified into seven types: Person entities are humans mentioned in a document; Organization entities are limited to established associations of people; Geo-political entities are geographical areas defined by political and/or social groups; Location entities are geographical items like landmasses and bodies of water; Facility entities refer to buildings and real estate improvements; Vehicle entities are devices used for transportation; and Weapon entities are devices used for harming or destruction." ></td>
	<td class="line x" title="166:234	We compare performances of a few algorithms: MaxEnt classifier (MaxEnt); MaxEnt classifier with standard bootstrapping (MaxEnt-SB); balanced bootstrapping based on MaxEnt classifier (MaxEnt-BB); MaxEnt with DAB (MaxEntDAB); SVM classifier (SVM); transductive SVM classifier (SVM-Trans); and DAB based on SVM classifier (SVM-DAB)." ></td>
	<td class="line x" title="167:234	No regularization is used for MaxEnt classifiers." ></td>
	<td class="line x" title="168:234	SVM classifiers use a value of 10 for parameter C (trade-off between training error and margin)." ></td>
	<td class="line x" title="169:234	Bootstrapping based algorithms are run for 30 iterations and 100 instances are selected in every iteration." ></td>
	<td class="line x" title="170:234	The evaluation measure used is the F-measure." ></td>
	<td class="line x" title="171:234	F-measure is the harmonic mean of precision and recall, and is commonly used to evaluate NER systems." ></td>
	<td class="line x" title="172:234	We use the scorer for CONLL 2003 shared task (Tjong and Meulder, 2003) where the F-measure is computed by averaging F-measures for name-classes, weighted by the number of ocCode Source Num docs NW Newswire 81 BC Broadcast conversation 52 WL Weblog 114 CTS Conversational Telephone Speech 34 Table 1: The sources, and the number of documents in each source, in the ACE 2005 data set." ></td>
	<td class="line x" title="173:234	currences." ></td>
	<td class="line x" title="174:234	5.1 Cross-source transfer The ACE 2005 data set consists of articles drawn from a variety of sources." ></td>
	<td class="line x" title="175:234	We use the four categories shown in Table 1." ></td>
	<td class="line x" title="176:234	Each category is considered to be a domain, and we consider each pair of categories as the source and the target domain in turn." ></td>
	<td class="line x" title="177:234	Figure 1 compares the performance of MaxEntSB, MaxEnt-BB and MaxEnt-DAB over multiple iterations." ></td>
	<td class="line x" title="178:234	Figure 2 compares the performance of SVM, SVM-Trans and SVM-DAB." ></td>
	<td class="line x" title="179:234	Each line in the figures represents the average F-measure across all the domains over many iterations." ></td>
	<td class="line x" title="180:234	When the termination condition is met for one domain, its F-measure remains at the value of the final iteration." ></td>
	<td class="line x" title="181:234	Despite a large number of iterations, both standard and balanced bootstrapping fail to improve performance." ></td>
	<td class="line x" title="182:234	Supervised learning performance on each domain is shown in Table 3 (by 2-fold crossvalidation with random ordering) as a reference." ></td>
	<td class="line x" title="183:234	In Table 5, we compare the F-measures obtained by different algorithms at the last iteration they were run." ></td>
	<td class="line x" title="184:234	We will discuss more on this in Section 5.3." ></td>
	<td class="line x" title="185:234	5.2 Cross-topic transfer This data set is constructed from 175 articles from the ACE 2005 corpus." ></td>
	<td class="line x" title="186:234	The data set is used to evaluate transfer across topics." ></td>
	<td class="line x" title="187:234	We manually classify the articles into 4 categories: military operations (MO), political relationship or politicians (POL), terrorism-related (TER), and those which are not in the above categories (OTH)." ></td>
	<td class="line x" title="188:234	A detailed breakdown of the number of documents in the each topic is given in Table 2." ></td>
	<td class="line x" title="189:234	Supervised learning performance on each domain is shown in Table 4 (by 2-fold crossvalidation with random ordering) as a reference." ></td>
	<td class="line x" title="190:234	Experimental results on cross-topic evaluation are shown in Table 6." ></td>
	<td class="line x" title="191:234	Figure 3 compares the performance of MaxEnt-SB, MaxEnt-BB and MaxEnt1528 57.558.0 58.559.0 59.560.0 60.561.0 61.562.0  0  5  10 15 20 25 30 35 F-measure Number of iterations MaxEnt-DABMaxEnt-SBMaxEnt-BB Figure 1: Average performance on the crosssource transfer using MaxEnt classifier." ></td>
	<td class="line x" title="192:234	35.040.0 45.050.0 55.060.0 65.070.0  1  2  3  4  5  6 F-measure Number of iterations SVM-DABSVMSVM-Trans Figure 2: Average performance on the crosssource transfer using SVM classifier." ></td>
	<td class="line x" title="193:234	66.266.4 66.666.8 67.067.2 67.467.6 67.868.0 68.2  0  5  10 15 20 25 30 35 F-measure Number of iterations MaxEnt-DABMaxEnt-SBMaxEnt-BB Figure 3: Average performance on the cross-topic transfer using MaxEnt classifier." ></td>
	<td class="line x" title="194:234	56.058.0 60.062.0 64.066.0 68.070.0 72.0  1  2  3  4 F-measure Number of iterations SVM-DABSVMSVM-Trans Figure 4: Average performance on the cross-topic transfer using SVM classifier." ></td>
	<td class="line x" title="195:234	Topic Topic description # docs MO Military operations 92 POL Political relationships 40 TER Terrorist-related 28 OTH None of the above 15 Table 2: The topics, their descriptions, and the number of training and test documents in each topic." ></td>
	<td class="line x" title="196:234	Domain MaxEnt SVM NW 82.47 82.32 BC 78.21 77.91 WL 71.41 71.84 CTS 93.90 94.01 Table 3: F-measure of supervised learning on the cross-source target domains." ></td>
	<td class="line x" title="197:234	DAB over multiple iterations." ></td>
	<td class="line x" title="198:234	Figure 4 compares the performance of SVM, SVM-Trans and SVMDAB." ></td>
	<td class="line x" title="199:234	Similar to cross-source transfer, standard and balanced bootstrapping perform badly." ></td>
	<td class="line x" title="200:234	This will be discussed in Section 5.3." ></td>
	<td class="line x" title="201:234	Domain MaxEnt SVM MO 80.52 80.6 POL 77.99 79.05 TER 81.74 82.12 OTH 71.33 72.08 Table 4: F-measure of supervised learning on the cross-topic target domains." ></td>
	<td class="line x" title="202:234	5.3 Discussion We show in our experiments that DAB outperforms standard and balanced bootstrapping, as well as the transductive SVM." ></td>
	<td class="line x" title="203:234	We have also shown DAB to be robust across two state-of-the-art classifiers, MaxEnt and SVM." ></td>
	<td class="line x" title="204:234	Balanced bootstrapping has been shown to be more effective for domain adaptation than standard bootstrapping (Jiang and Zhai, 2007) for named entity classification on a subset of the dataset used here." ></td>
	<td class="line x" title="205:234	In contrast, we found that both methods perform poorly on domain adaptation for NER." ></td>
	<td class="line x" title="206:234	In named entity classification, the names have already been segmented out and only need to be classified with the appropriate class." ></td>
	<td class="line x" title="207:234	However, for NER, the names also 1529 Train Test MaxEnt MaxEnt-SB MaxEnt-BB MaxEnt-DAB SVM SVM-Trans SVM-DAB BC CTS 74.26 74.19 74.16 81.03 72.47 43.27 75.43 BC NW 64.81 64.76 64.80 66.20 64.08 43.01 64.39 BC WL 47.81 47.80 47.76 49.52 47.98 36.58 47.93 CTS BC 46.19 46.12 46.40 54.62 46.02 40.44 49.64 CTS NW 54.25 54.15 54.26 53.07 55.63 23.61 58.99 CTS WL 40.42 40.43 40.72 41.27 39.96 29.05 42.04 NW BC 59.90 59.83 59.80 60.55 59.89 45.71 58.42 NW CTS 66.64 66.48 66.59 66.73 68.28 28.80 73.47 NW WL 52.52 52.53 52.47 53.44 52.19 36.39 52.30 WL BC 58.58 58.79 58.65 56.00 58.43 52.64 58.64 WL CTS 64.63 63.89 64.50 80.45 65.96 45.04 81.04 WL NW 67.79 67.72 67.92 68.46 68.38 43.40 69.33 Average 58.15 58.06 58.17 60.95 58.27 39.00 60.97 Table 5: F-measure of the cross-source transfer." ></td>
	<td class="line x" title="208:234	Train Test MaxEnt MaxEnt-SB MaxEnt-BB MaxEnt-DAB SVM SVM-Trans SVM-DAB MO OTH 81.70 81.48 81.57 81.95 81.78 75.68 81.94 MO POL 73.21 73.11 73.28 74.97 72.56 58.13 72.66 MO TER 68.13 68.07 68.24 69.89 69.40 65.02 69.38 OTH MO 63.30 63.80 63.94 63.91 64.18 61.03 65.45 OTH POL 67.96 68.05 67.86 69.13 68.29 56.50 70.67 OTH TER 45.34 44.82 45.30 51.06 45.71 48.77 52.87 POL MO 62.14 62.12 61.95 61.94 61.98 51.67 62.32 POL OTH 77.91 77.72 77.79 76.58 78.11 65.71 78.13 POL TER 66.55 66.38 66.08 66.38 66.44 51.29 67.24 TER MO 58.35 58.62 58.02 57.29 58.30 49.80 58.14 TER OTH 66.83 67.61 66.83 68.97 66.28 58.25 68.12 TER POL 67.34 66.94 67.16 72.00 67.54 50.55 70.65 Average 66.56 66.56 66.50 67.84 66.71 57.70 68.13 Table 6: F-measure of the cross-topic transfer." ></td>
	<td class="line x" title="209:234	need to be separated from not-a-name instances." ></td>
	<td class="line x" title="210:234	We find that the addition of not-a-name instances changes the problem the not-a-names form most of the instances classified with high confidence." ></td>
	<td class="line x" title="211:234	As a result, we find that both standard and balanced bootstrapping fail to improve performance: the selection of the most confident instances no longer provide sufficient new information to improve performance." ></td>
	<td class="line x" title="212:234	We also find that transductive SVM performs poorly on this task." ></td>
	<td class="line x" title="213:234	This is because it assumes that the unlabeled data comes from the same distribution as the labeled data." ></td>
	<td class="line n" title="214:234	In general, applying semi-supervised learning methods directly to [S+T-] type domain adaptation problems do not work and appropriate modifications need to be made to the methods." ></td>
	<td class="line x" title="215:234	The ACE 2005 data set also contains a set of ariticles from the broadcast news (BN) source which is written entirely in lower case." ></td>
	<td class="line x" title="216:234	This makes NER much more difficult." ></td>
	<td class="line x" title="217:234	However, when BN is the source domain, the capitalization information can be discovered by DAB." ></td>
	<td class="line x" title="218:234	Figures 5 and 6 show the average performance when BN is used as the source domain and all other domains in Table 1 as the target domains." ></td>
	<td class="line x" title="219:234	The source domain classifier tends to have high precision and low recall, DAB results in an increase in recall, with a small decrease in precision." ></td>
	<td class="line x" title="220:234	Testing the significance of the F-measure is not trivial because the named entities wrongly labeled by two classifiers are not directly comparable." ></td>
	<td class="line x" title="221:234	We tested the labeling disagreements instead, using a McNemar paired test." ></td>
	<td class="line x" title="222:234	The significance test is performed on the improvement of MaxEnt-DAB over MaxEnt and SVM-DAB over SVM." ></td>
	<td class="line x" title="223:234	In most of the domains for the cross-source transfer, the improvements are significant at a significance level of 0.05, using MaxEnt classifier." ></td>
	<td class="line x" title="224:234	The exceptional train-test pairs are NW-WL and WL-BC." ></td>
	<td class="line x" title="225:234	In the case of WL-BC, this means the slight decrement in performance is not statistically significant." ></td>
	<td class="line x" title="226:234	Similar result is achieved for the cross-source transfer using SVM classifier." ></td>
	<td class="line x" title="227:234	In the cross-topic transfer, the source domain and the target domain are not very different." ></td>
	<td class="line x" title="228:234	When we have a large amount of training data and little testing data, the gain of DAB can be not statistically significant, as in the case when we train with MO and POL domains." ></td>
	<td class="line x" title="229:234	1530 20.025.0 30.035.0 40.045.0 50.0  1  2  3  4  5  6  7  8  9 F-measure Number of iterations MaxEnt-DABMaxEnt-SBMaxEnt-BB Figure 5: Performance on recovering capitalization using MaxEnt classifier." ></td>
	<td class="line x" title="230:234	28.030.0 32.034.0 36.038.0 40.042.0 44.046.0  1  2  3  4 F-measure Number of iterations SVM-DABSVMSVM-Trans Figure 6: Performance on recovering capitalization using SVM classifier." ></td>
	<td class="line x" title="231:234	6 Conclusion We proposed a bootstrapping approach for domain adaptation, and we applied it to the named entity recognition task." ></td>
	<td class="line x" title="232:234	Our approach leverages on instances that serve as bridges between the source and target domain." ></td>
	<td class="line x" title="233:234	Empirically, our method outperforms baseline approaches including supervised, transductive and standard bootstrapping approaches." ></td>
	<td class="line x" title="234:234	It also outperforms balanced bootstrapping, an approach designed for domain adaptation (Jiang and Zhai, 2007)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E09-3005
Structural Correspondence Learning for Parse Disambiguation
Plank, Barbara;"></td>
	<td class="line x" title="1:229	Proceedings of the EACL 2009 Student Research Workshop, pages 3745, Athens, Greece, 2 April 2009." ></td>
	<td class="line oc" title="2:229	c2009 Association for Computational Linguistics Structural Correspondence Learning for Parse Disambiguation Barbara Plank Alfa-informatica University of Groningen, The Netherlands b.plank@rug.nl Abstract The paper presents an application of Structural Correspondence Learning (SCL) (Blitzer et al., 2006) for domain adaptation of a stochastic attribute-value grammar (SAVG)." ></td>
	<td class="line pc" title="3:229	So far, SCL has been applied successfully in NLP for Part-of-Speech tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007)." ></td>
	<td class="line n" title="4:229	An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa, 2007), however, without any clear conclusions." ></td>
	<td class="line p" title="5:229	We report on our exploration of applying SCL to adapt a syntactic disambiguation model and show promising initial results on Wikipedia domains." ></td>
	<td class="line x" title="6:229	1 Introduction Many current, effective natural language processing systems are based on supervised Machine Learning techniques." ></td>
	<td class="line x" title="7:229	The parameters of such systems are estimated to best reflect the characteristics of the training data, at the cost of portability: a system will be successful only as long as the training material resembles the input that the model gets." ></td>
	<td class="line x" title="8:229	Therefore, whenever we have access to a large amount of labeled data from some source (out-of-domain), but we would like a model that performs well on some new target domain (Gildea, 2001; Daume III, 2007), we face the problem of domain adaptation." ></td>
	<td class="line x" title="9:229	The need for domain adaptation arises in many NLP tasks: Part-of-Speech tagging, Sentiment Analysis, Semantic Role Labeling or Statistical Parsing, to name but a few." ></td>
	<td class="line x" title="10:229	For example, the performance of a statistical parsing system drops in an appalling way when a model trained on the Wall Street Journal is applied to the more varied Brown corpus (Gildea, 2001)." ></td>
	<td class="line oc" title="11:229	The problem itself has started to get attention only recently (Roark and Bacchiani, 2003; Hara et al., 2005; Daume III and Marcu, 2006; Daume III, 2007; Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007)." ></td>
	<td class="line x" title="12:229	We distinguish two main approaches to domain adaptation that have been addressed in the literature (Daume III, 2007): supervised and semi-supervised." ></td>
	<td class="line x" title="13:229	In supervised domain adaptation (Gildea, 2001; Roark and Bacchiani, 2003; Hara et al., 2005; Daume III, 2007), besides the labeled source data, we have access to a comparably small, but labeled amount of target data." ></td>
	<td class="line oc" title="14:229	In contrast, semi-supervised domain adaptation (Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007) is the scenario in which, in addition to the labeled source data, we only have unlabeled and no labeled target domain data." ></td>
	<td class="line x" title="15:229	Semi-supervised adaptation is a much more realistic situation, while at the same time also considerably more difficult." ></td>
	<td class="line x" title="16:229	Studies on the supervised task have shown that straightforward baselines (e.g. models based on source only, target only, or the union of the data) achieve a relatively high performance level and are surprisingly difficult to beat (Daume III, 2007)." ></td>
	<td class="line x" title="17:229	Thus, one conclusion from that line of work is that as soon as there is a reasonable (often even small) amount of labeled target data, it is often more fruitful to either just use that, or to apply simple adaptation techniques (Daume III, 2007; Plank and van Noord, 2008)." ></td>
	<td class="line nc" title="18:229	2 Motivation and Prior Work While several authors have looked at the supervised adaptation case, there are less (and especially less successful) studies on semi-supervised domain adaptation (McClosky et al., 2006; Blitzer et al., 2006; Dredze et al., 2007)." ></td>
	<td class="line x" title="19:229	Of these, McClosky et al.(2006) deal specifically with selftraining for data-driven statistical parsing." ></td>
	<td class="line x" title="21:229	They show that together with a re-ranker, improvements 37 are obtained." ></td>
	<td class="line pc" title="22:229	Similarly, Structural Correspondence Learning (Blitzer et al., 2006; Blitzer et al., 2007; Blitzer, 2008) has proven to be successful for the two tasks examined, PoS tagging and Sentiment Classification." ></td>
	<td class="line x" title="23:229	In contrast, Dredze et al.(2007) report on frustrating results on the CoNLL 2007 semi-supervised adaptation task for dependency parsing, i.e. no team was able to improve target domain performance substantially over a state of the art baseline." ></td>
	<td class="line o" title="25:229	In the same shared task, an attempt was made to apply SCL to domain adaptation for data-driven dependency parsing (Shimizu and Nakagawa, 2007)." ></td>
	<td class="line x" title="26:229	The system just ended up at rank 7 out of 8 teams." ></td>
	<td class="line n" title="27:229	However, based on annotation differences in the datasets (Dredze et al., 2007) and a bug in their system (Shimizu and Nakagawa, 2007), their results are inconclusive.1 Thus, the effectiveness of SCL is rather unexplored for parsing." ></td>
	<td class="line x" title="28:229	So far, most previous work on domain adaptation for parsing has focused on data-driven systems (Gildea, 2001; Roark and Bacchiani, 2003; McClosky et al., 2006; Shimizu and Nakagawa, 2007), i.e. systems employing (constituent or dependency based) treebank grammars (Charniak, 1996)." ></td>
	<td class="line o" title="29:229	Parse selection constitutes an important part of many parsing systems (Johnson et al., 1999; Hara et al., 2005; van Noord and Malouf, 2005; McClosky et al., 2006)." ></td>
	<td class="line x" title="30:229	Yet, the adaptation of parse selection models to novel domains is a far less studied area." ></td>
	<td class="line x" title="31:229	This may be motivated by the fact that potential gains for this task are inherently bounded by the underlying grammar." ></td>
	<td class="line x" title="32:229	The few studies on adapting disambiguation models (Hara et al., 2005; Plank and van Noord, 2008) have focused exclusively on the supervised scenario." ></td>
	<td class="line x" title="33:229	Therefore, the direction we explore in this study is semi-supervised domain adaptation for parse disambiguation." ></td>
	<td class="line pc" title="34:229	We examine the effectiveness of Structural Correspondence Learning (SCL) (Blitzer et al., 2006) for this task, a recently proposed adaptation technique shown to be effective for PoS tagging and Sentiment Analysis." ></td>
	<td class="line x" title="35:229	The system used in this study is Alpino, a wide-coverage Stochastic Attribute Value Grammar (SAVG) for Dutch (van Noord and Malouf, 2005; van Noord, 2006)." ></td>
	<td class="line x" title="36:229	For our empirical eval1As shown in Dredze et al.(2007), the biggest problem for the shared task was that the provided datasets were annotated with different annotation guidelines, thus the general conclusion was that the task was ill-defined (Nobuyuki Shimizu, personal communication)." ></td>
	<td class="line x" title="38:229	uation we explore Wikipedia as primary test and training collection." ></td>
	<td class="line x" title="39:229	In the sequel, we first introduce the parsing system." ></td>
	<td class="line o" title="40:229	Section 4 reviews Structural Correspondence Learning and shows our application of SCL to parse selection, including all our design choices." ></td>
	<td class="line x" title="41:229	In Section 5 we present the datasets, introduce the process of constructing target domain data from Wikipedia, and discuss interesting initial empirical results of this ongoing study." ></td>
	<td class="line x" title="42:229	3 Background: Alpino parser Alpino (van Noord and Malouf, 2005; van Noord, 2006) is a robust computational analyzer for Dutch that implements the conceptual two-stage parsing approach." ></td>
	<td class="line x" title="43:229	The system consists of approximately 800 grammar rules in the tradition of HPSG, and a large hand-crafted lexicon, that together with a left-corner parser constitutes the generation component." ></td>
	<td class="line x" title="44:229	For parse selection, Alpino employs a discriminative approach based on Maximum Entropy (MaxEnt)." ></td>
	<td class="line x" title="45:229	The output of the parser is dependency structure based on the guidelines of CGN (Oostdijk, 2000)." ></td>
	<td class="line x" title="46:229	The Maximum Entropy model (Berger et al., 1996; Ratnaparkhi, 1997; Abney, 1997) is a conditional model that assigns a probability to every possible parse  for a given sentence s. The model consists of a set of m feature functions fj() that describe properties of parses, together with their associated weights j. The denominator is a normalization term where Y (s) is the set of parses with yield s: p(|s;) = exp( summationtextm j=1 jfj())summationtext yY (s) exp( summationtextm j=1 jfj(y))) (1) The parameters (weights) j can be estimated efficiently by maximizing the regularized conditional likelihood of a training corpus (Johnson et al., 1999; van Noord and Malouf, 2005):  = argmax  logL()  summationtextm j=1  2j 22 (2) where L() is the likelihood of the training data." ></td>
	<td class="line x" title="47:229	The second term is a regularization term (Gaussian prior on the feature weights with mean zero and variance )." ></td>
	<td class="line x" title="48:229	The estimated weights determine the contribution of each feature." ></td>
	<td class="line x" title="49:229	Features appearing in correct parses are given increasing (positive) weight, while features in incorrect parses are 38 given decreasing (negative) weight." ></td>
	<td class="line x" title="50:229	Once a model is trained, it can be applied to choose the parse with the highest sum of feature weights." ></td>
	<td class="line x" title="51:229	The MaxEnt model consists of a large set of features, corresponding to instantiations of feature templates that model various properties of parses." ></td>
	<td class="line x" title="52:229	For instance, Part-of-Speech tags, dependency relations, grammar rule applications, etc. The current standard model uses about 11,000 features." ></td>
	<td class="line x" title="53:229	We will refer to this set of features as original features." ></td>
	<td class="line x" title="54:229	They are used to train the baseline model on the given labeled source data." ></td>
	<td class="line oc" title="55:229	4 Structural Correspondence Learning SCL (Structural Correspondence Learning) (Blitzer et al., 2006; Blitzer et al., 2007; Blitzer, 2008) is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains." ></td>
	<td class="line o" title="56:229	Before describing the algorithm in detail, let us illustrate the intuition behind SCL with an example, borrowed from Blitzer et al.(2007)." ></td>
	<td class="line x" title="58:229	Suppose we have a Sentiment Analysis system trained on book reviews (domain A), and we would like to adapt it to kitchen appliances (domain B)." ></td>
	<td class="line x" title="59:229	Features such as boring and repetitive are common ways to express negative sentiment in A, while not working or defective are specific to B. If there are features across the domains, e.g. dont buy, with which the domain specific features are highly correlated with, then we might tentatively align those features." ></td>
	<td class="line o" title="60:229	Therefore, the key idea of SCL is to identify automatically correspondences among features from different domains by modeling their correlations with pivot features." ></td>
	<td class="line oc" title="61:229	Pivots are features occurring frequently and behaving similarly in both domains (Blitzer et al., 2006)." ></td>
	<td class="line x" title="62:229	They are inspired by auxiliary problems from Ando and Zhang (2005)." ></td>
	<td class="line x" title="63:229	Non-pivot features that correspond with many of the same pivot-features are assumed to correspond." ></td>
	<td class="line oc" title="64:229	Intuitively, if we are able to find good correspondences among features, then the augmented labeled source domain data should transfer better to a target domain (where no labeled data is available) (Blitzer et al., 2006)." ></td>
	<td class="line o" title="65:229	The outline of the algorithm is given in Figure 1." ></td>
	<td class="line x" title="66:229	The first step is to identify m pivot features occurring frequently in the unlabeled data of both Input: labeled source data {(xs,ys)Nss=1} unlabeled data from both source and target domain xul = xs,xt 1." ></td>
	<td class="line x" title="67:229	Select m pivot features 2." ></td>
	<td class="line x" title="68:229	Train m binary classifiers (pivot predictors) 3." ></td>
	<td class="line x" title="69:229	Create matrix Wnm of binary predictor weight vectors W = [w1,,wm], where n is the number of nonpivot features in xul 4." ></td>
	<td class="line x" title="70:229	Apply SVD to W: Wnm = UnnDnmV Tmm where  = UT[1:h,:] are the h top left singular vectors of W. 5." ></td>
	<td class="line x" title="71:229	Apply projection xs and train a predictor on the original and new features obtained through the projection." ></td>
	<td class="line oc" title="72:229	Figure 1: SCL algorithm (Blitzer et al., 2006)." ></td>
	<td class="line x" title="73:229	domains." ></td>
	<td class="line x" title="74:229	Then, a binary classifier is trained for each pivot feature (pivot predictor) of the form: Does pivot feature l occur in this instance?." ></td>
	<td class="line x" title="75:229	The pivots are masked in the unlabeled data and the aim is to predict them using non-pivot features." ></td>
	<td class="line x" title="76:229	In this way, we obtain a weight vector w for each pivot predictor." ></td>
	<td class="line x" title="77:229	Positive entries in the weight vector indicate that a non-pivot is highly correlated with the respective pivot feature." ></td>
	<td class="line x" title="78:229	Step 3 is to arrange the m weight vectors in a matrix W, where a column corresponds to a pivot predictor weight vector." ></td>
	<td class="line oc" title="79:229	Applying the projection WTx (where x is a training instance) would give us m new features, however, for both computational and statistical reasons (Blitzer et al., 2006; Ando and Zhang, 2005) a low-dimensional approximation of the original feature space is computed by applying Singular Value Decomposition (SVD) on W (step 4)." ></td>
	<td class="line x" title="80:229	Let  = UThn be the top h left singular vectors of W (with h a dimension parameter and n the number of non-pivot features)." ></td>
	<td class="line o" title="81:229	The resulting  is a projection onto a lower dimensional space Rh, parameterized by h. The final step of SCL is to train a linear predictor on the augmented labeled source data x,x." ></td>
	<td class="line x" title="82:229	In more detail, the original feature space x is augmented with h new features obtained by applying the projection x. In this way, we can learn weights for domain-specific features, which otherwise would not have been observed." ></td>
	<td class="line x" title="83:229	If  contains meaningful correspondences, then the pre39 dictor trained on the augmented data should transfer well to the new domain." ></td>
	<td class="line o" title="84:229	4.1 SCL for Parse Disambiguation A property of the pivot predictors is that they can be trained from unlabeled data, as they represent properties of the input." ></td>
	<td class="line oc" title="85:229	So far, pivot features on the word level were used (Blitzer et al., 2006; Blitzer et al., 2007; Blitzer, 2008), e.g. Does the bigram not buy occur in this document? (Blitzer, 2008)." ></td>
	<td class="line o" title="86:229	Pivot features are the key ingredient for SCL, and they should align well with the NLP task." ></td>
	<td class="line x" title="87:229	For PoS tagging and Sentiment Analysis, features on the word level are intuitively well-related to the problem at hand." ></td>
	<td class="line x" title="88:229	For the task of parse disambiguation based on a conditional model this is not the case." ></td>
	<td class="line x" title="89:229	Hence, we actually introduce an additional and new layer of abstraction, which, we hypothesize, aligns well with the task of parse disambiguation: we first parse the unlabeled data." ></td>
	<td class="line x" title="90:229	In this way we obtain full parses for given sentences as produced by the grammar, allowing access to more abstract representations of the underlying pivot predictor training data (for reasons of efficiency, we here use only the first generated parse as training data for the pivot predictors, rather than n-best)." ></td>
	<td class="line x" title="91:229	Thus, instead of using word-level features, our features correspond to properties of the generated parses: application of grammar rules (r1,r2 features), dependency relations (dep), PoS tags (f1,f2), syntactic features (s1), precedence (mf ), bilexical preferences (z), apposition (appos) and further features for unknown words, temporal phrases, coordination (h,in year and p1, respectively)." ></td>
	<td class="line x" title="92:229	This allows us to get a possibly noisy, but more abstract representation of the underlying data." ></td>
	<td class="line x" title="93:229	The set of features used in Alpino is further described in van Noord and Malouf (2005)." ></td>
	<td class="line x" title="94:229	Selection of pivot features As pivot features should be common across domains, here we restrict our pivots to be of the type r1,p1,s1 (the most frequently occurring feature types)." ></td>
	<td class="line x" title="95:229	In more detail, r1 indicates which grammar rule applied, p1 whether coordination conjuncts are parallel, and s1 whether topicalization or long-distance dependencies occurred." ></td>
	<td class="line x" title="96:229	We count how often each feature appears in the parsed source and target domain data, and select those r1,p1,s1 features as pivot features, whose count is > t, where t is a specified threshold." ></td>
	<td class="line x" title="97:229	In all our experiments, we set t = 5000." ></td>
	<td class="line x" title="98:229	In this way we obtained on average 360 pivot features, on the datasets described in Section 5." ></td>
	<td class="line oc" title="99:229	Predictive features As pointed out by Blitzer et al.(2006), each instance will actually contain features which are totally predictive of the pivot features (i.e. the pivot itself)." ></td>
	<td class="line x" title="101:229	In our case, we additionally have to pay attention to more specific features, e.g. r2 is a feature that extends r1, in the sense that it incorporates more information than its parent (i.e. which grammar rules applied in the construction of daughter nodes)." ></td>
	<td class="line x" title="102:229	It is crucial to remove these predictive features when creating the training data for the pivot predictors." ></td>
	<td class="line oc" title="103:229	Matrix and SVD Following Blitzer et al.(2006) (which follow Ando and Zhang (2005)), we only use positive entries in the pivot predictors weight vectors to compute the SVD." ></td>
	<td class="line x" title="105:229	Thus, when constructing the matrix W, we disregard all negative entries in W and compute the SVD (W = UDV T ) on the resulting non-negative sparse matrix." ></td>
	<td class="line x" title="106:229	This sparse representation saves both time and space." ></td>
	<td class="line nc" title="107:229	4.2 Further practical issues of SCL In practice, there are more free parameters and model choices (Ando and Zhang, 2005; Ando, 2006; Blitzer et al., 2006; Blitzer, 2008) besides the ones discussed above." ></td>
	<td class="line x" title="108:229	Feature normalization and feature scaling." ></td>
	<td class="line oc" title="109:229	Blitzer et al.(2006) found it necessary to normalize and scale the new features obtained by the projection , in order to allow them to receive more weight from a regularized discriminative learner." ></td>
	<td class="line o" title="111:229	For each of the features, they centered them by subtracting out the mean and normalized them to unit variance (i.e. x  mean/sd)." ></td>
	<td class="line o" title="112:229	They then rescaled the features by a factor  found on heldout data: x. Restricted Regularization." ></td>
	<td class="line oc" title="113:229	When training the supervised model on the augmented feature space x,x, Blitzer et al.(2006) only regularize the weight vector of the original features, but not the one for the new low-dimensional features." ></td>
	<td class="line o" title="115:229	This was done to encourage the model to use the new low-dimensional representation rather than the higher-dimensional original representation (Blitzer, 2008)." ></td>
	<td class="line x" title="116:229	Dimensionality reduction by feature type." ></td>
	<td class="line x" title="117:229	An extension suggested in Ando and Zhang (2005) is 40 to compute separate SVDs for blocks of the matrix W corresponding to feature types (as illustrated in Figure 2), and then to apply separate projection for every type." ></td>
	<td class="line oc" title="118:229	Due to the positive results in Ando (2006), Blitzer et al.(2006) include this in their standard setting of SCL and report results using block SVDs only." ></td>
	<td class="line x" title="120:229	Figure 2: Illustration of dimensionality reduction by feature type (Ando and Zhang, 2005)." ></td>
	<td class="line x" title="121:229	The grey area corresponds to a feature type (submatrix of W) on which the SVD is computed (block SVD); the white area is regarded as fixed to zero matrices." ></td>
	<td class="line x" title="122:229	5 Experiments and Results 5.1 Experimental design The base (source domain) disambiguation model is trained on the Alpino Treebank (van Noord, 2006) (newspaper text), which consists of approximately 7,000 sentences and 145,000 tokens." ></td>
	<td class="line x" title="123:229	For parameter estimation of the disambiguation model, in all reported experiments we use the TADM2 toolkit (toolkit for advanced discriminative training), with a Gaussian prior (2=1000) and the (default) limited memory variable metric estimation technique (Malouf, 2002)." ></td>
	<td class="line x" title="124:229	For training the binary pivot predictors, we use the MegaM3 Optimization Package with the socalled bernoulli implicit input format." ></td>
	<td class="line x" title="125:229	To compute the SVD, we use SVDLIBC.4 The output of the parser is dependency structure." ></td>
	<td class="line x" title="126:229	A standard evaluation metric is to measure the amount of generated dependencies that are identical to the stored dependencies (correct labeled dependencies), expressed as f-score." ></td>
	<td class="line x" title="127:229	An alternative measure is concept accuracy (CA), which is similar to f-score, but allows possible discrepancy between the number of returned dependencies (van Noord, 2006; Plank and van Noord, 2http://tadm.sourceforge.net/ 3http://www.cs.utah.edu/hal/megam/ 4http://tedlab.mit.edu/dr/svdlibc/ 2008)." ></td>
	<td class="line x" title="128:229	CA is usually slightly lower than f-score." ></td>
	<td class="line x" title="129:229	Let Dip be the number of dependencies produced by the parser for sentence i. Dig is the number of dependencies in the treebank parse, and Dio is the number of correct dependencies produced by the parser." ></td>
	<td class="line x" title="130:229	Then, CA = Dosummationtext i max(Dig,Dip) If we want to compare the performance of disambiguation models, we can employ the  measure (van Noord and Malouf, 2005; van Noord, 2007)." ></td>
	<td class="line x" title="131:229	Intuitively, it tells us how much of the disambiguation problem has been solved." ></td>
	<td class="line x" title="132:229	 = CAbaseoracle  base  100 In more detail, the  measure incorporates an upper and lower bound: base measures the accuracy of a model that simply selects the first parse for each sentence; oracle represents the accuracy achieved by a model that always selects the best parse from the set of potential parses (within the coverage of the parser)." ></td>
	<td class="line x" title="133:229	In addition, we also report relative error reduction (rel.er), which is the relative difference in  scores for two models." ></td>
	<td class="line x" title="134:229	As target domain, we consider the Dutch part of Wikipedia as data collection, described in the following." ></td>
	<td class="line x" title="135:229	5.2 Wikipedia as resource In our experiments, we exploit Wikipedia both as testset and as unlabeled data source." ></td>
	<td class="line x" title="136:229	We assume that in order to parse data from a very specific domain, say about the artist Prince, then data related to that domain, like information about the New Power Generation, the Purple rain movie, or other American singers and artists, should be of help." ></td>
	<td class="line x" title="137:229	Thus, we exploit Wikipedia and its category system to gather domain-specific target data." ></td>
	<td class="line x" title="138:229	Construction of target domain data In more detail, we use the Dutch part of Wikipedia provided by WikiXML,5 a collection of Wikipedia articles converted to XML format." ></td>
	<td class="line x" title="139:229	As the corpus is encoded in XML, we can exploit general purpose XML Query Languages, such as XQuery, Xslt and XPath, to extract relevant information from the Wikipedia corpus." ></td>
	<td class="line x" title="140:229	Given a wikipage p, with c  categories(p), we can identify pages related to p of various 5http://ilps.science.uva.nl/WikiXML/ 41 types of relatedness: directly related pages (those that share a category, i.e. all p where c  categories(p) such that c = c), or alternatively, pages that share a subor supercategory of p, i.e. p where c  categories(p) and c  sub categories(p) or c  super categories(p)." ></td>
	<td class="line x" title="141:229	For example, Figure 3 shows the categories extracted for the Wikipedia article about pope Johannes Paulus II." ></td>
	<td class="line x" title="142:229	<wikipage id='6677'> <cat t='direct' n='Categorie:Paus'/> <cat t='direct' n='Categorie:Pools_theoloog'/> <cat t='super' n='Categorie:Religieus leider'/> <cat t='super' n='Categorie:Rooms-katholiek persoon'/> <cat t='super' n='Categorie:Vaticaanstad'/> <cat t='super' n='Categorie:Bisschop'/> <cat t='super' n='Categorie:Kerkgeschiedenis'/> <cat t='sub' n='Categorie:Tegenpaus'/> <cat t='super' n='Categorie:Pools persoon'/> </wikipage> Figure 3: Example of extracted Wikipedia categories for a given article (direct, supand subcats)." ></td>
	<td class="line x" title="143:229	To create the set of related pages for a given article p, we proceed as follows: 1." ></td>
	<td class="line x" title="144:229	Find suband supercategories of p 2." ></td>
	<td class="line x" title="145:229	Extract all pages that are related to p (through sharing a direct, sub or super category) 3." ></td>
	<td class="line oc" title="146:229	Optionally, filter out certain pages In our empirical setup, we followed Blitzer et al.(2006) and tried to balance the size of source and target data." ></td>
	<td class="line x" title="148:229	Thus, depending on the size of the resulting target domain dataset, and the broadness of the categories involved in creating it, we might wish to filter out certain pages." ></td>
	<td class="line x" title="149:229	We implemented a filter mechanism that excludes pages of a certain category (e.g. a supercategory that is hypothesized to be too broad)." ></td>
	<td class="line x" title="150:229	Alternatively, we might have used a filter mechanism that excludes certain pages directly." ></td>
	<td class="line x" title="151:229	In our experiments, we always included pages that are directly related to a page of interest, and those that shared a subcategory." ></td>
	<td class="line x" title="152:229	Of course, the page itself is not included in that dataset." ></td>
	<td class="line x" title="153:229	With regard to supercategories, we usually included all pages having a category c  super categories(p), unless stated otherwise." ></td>
	<td class="line x" title="154:229	Test collection Our testset consists of a selection of Wikipedia articles that have been manually corrected in the course of the D-Coi/LASSY project.6 6Ongoing project, see http://www.let.rug.nl/ vannoord/Lassy/ An overview of the testset including size indications is given in Table 1." ></td>
	<td class="line x" title="155:229	Table 2 provides information on the target domain datasets constructed from Wikipedia." ></td>
	<td class="line x" title="156:229	Wiki/DCOI ID Title Sents 6677/026563 Prince (musician) 358 6729/036834 Paus Johannes Paulus II 232 182654/041235 Augustus De Morgan 259 Table 1: Size of test datasets." ></td>
	<td class="line x" title="157:229	Related to Articles Sents Tokens Relationship Prince 290 9,772 145,504 filtered super Paus 445 8,832 134,451 all De Morgan 394 8,466 132,948 all Table 2: Size of related unlabeled data; relationship indicates whether all related pages are used or some are filtered out (see section 5.2)." ></td>
	<td class="line x" title="158:229	5.3 Empirical Results For all reported results, we randomly select n = 200 maximum number of parses per sentence for evaluation." ></td>
	<td class="line x" title="159:229	Baseline accuracies Table 3 shows the baseline performance (of the standard Alpino model) on the various Wikipedia testsets (CA, f-score)." ></td>
	<td class="line x" title="160:229	The third and fourth column indicate the upperand lower bound measures (defined in section 5.1)." ></td>
	<td class="line x" title="161:229	Title CA f-score base oracle Prince (musician) 85.03 85.38 71.95 88.70 Paus Johannes Paulus II 85.72 86.32 74.30 89.09 Augustus De Morgan 80.09 80.61 70.08 83.52 Table 3: Baseline results." ></td>
	<td class="line x" title="162:229	While the parser normally operates on an accuracy level of roughly 88-89% (van Noord, 2007) on its own domain (newspaper text), the accuracy on these subdomains drops to around 85%." ></td>
	<td class="line x" title="163:229	The biggest performance decrease (to 80%) was on the article about the British logician and mathematician De Morgan." ></td>
	<td class="line x" title="164:229	This confirms the intuition that this specific subdomain is the hardest, given that mathematical expressions might emerge in the data (e.g. Wet der distributiviteit : a(b+c) = ab+ac distributivity law)." ></td>
	<td class="line o" title="165:229	SCL results Table 4 shows the results of our instantiation of SCL for parse disambiguation, with varying h parameter (dimensionality parameter; 42 h = 25 means that applying the projection x resulted in adding 25 new features to every source domain instance)." ></td>
	<td class="line x" title="166:229	CA f-score  rel.er." ></td>
	<td class="line o" title="167:229	baseline Prince 85.03 85.38 78.06 0.00 SCL[+/-], h = 25 85.12 85.46 78.64 2.64 SCL[+/-], h = 50 85.29 85.63 79.66 7.29 SCL[+/-], h = 100 85.19 85.53 79.04 4.47 SCL[+/-], h = 200 85.21 85.54 79.18 5.10 baseline Paus 85.72 86.32 77.23 0.00 SCL[+/-], h = 25 85.87 86.48 78.26 4.52 SCL[+/-], h = 50 85.82 86.43 77.87 2.81 SCL[+/-], h = 100 85.87 86.49 78.26 4.52 SCL[+/-], h = 200 85.87 86.48 78.26 4.52 baseline DeMorgan 80.09 80.61 74.44 0.00 SCL[+/-], h = 25 80.15 80.67 74.92 1.88 SCL[+/-], h = 50 80.12 80.64 74.68 0.94 SCL[+/-], h = 100 80.12 80.64 74.68 0.94 SCL[+/-], h = 200 80.15 80.67 74.91 1.88 Table 4: Results of our instantiation of SCL (with varying h parameter and no feature normalization)." ></td>
	<td class="line x" title="168:229	The results show a (sometimes) small but consistent increase in absolute performance on all testsets over the baseline system (up to +0.26 absolute CA score), as well as an increase in  measure (absolute error reduction)." ></td>
	<td class="line x" title="169:229	This corresponds to a relative error reduction of up to 7.29%." ></td>
	<td class="line p" title="170:229	Thus, our first instantiation of SCL for parse disambiguation indeed shows promising results." ></td>
	<td class="line oc" title="171:229	We can confirm that changing the dimensionality parameter h has rather little effect (Table 4), which is in line with previous findings (Ando and Zhang, 2005; Blitzer et al., 2006)." ></td>
	<td class="line x" title="172:229	Thus we might fix the parameter and prefer smaller dimensionalities, which saves space and time." ></td>
	<td class="line x" title="173:229	Note that these results were obtained without any of the additional normalization, rescaling, feature-specific regularization, or block SVD issues, etc.(discussed in section 4.2)." ></td>
	<td class="line x" title="175:229	We used the same Gaussian regularization term (2=1000) for all features (original and new features), and did not perform any feature normalization or rescaling." ></td>
	<td class="line o" title="176:229	This means our current instantiation of SCL is an actually simplified version of the original SCL algorithm, applied to parse disambiguation." ></td>
	<td class="line o" title="177:229	Of course, our results are preliminary and, rather than warranting many definite conclusions, encourage further exploration of SCL and related semi-supervised adaptation techniques." ></td>
	<td class="line o" title="178:229	5.4 Additional Empirical Results In the following, we describe additional results obtained by extensions and/or refinements of our current SCL instantiation." ></td>
	<td class="line x" title="179:229	Feature normalization." ></td>
	<td class="line x" title="180:229	We also tested feature normalization (as described in Section 4.2)." ></td>
	<td class="line oc" title="181:229	While Blitzer et al.(2006) found it necessary to normalize (and scale) the projection features, we did not observe any improvement by normalizing them (actually, it slightly degraded performance in our case)." ></td>
	<td class="line x" title="183:229	Thus, we found this step unnecessary, and currently did not look at this issue any further." ></td>
	<td class="line o" title="184:229	A look at  To gain some insight of which kind of correspondences SCL learned in our case, we started to examine the rows of ." ></td>
	<td class="line x" title="185:229	Recall that applying a row of the projection matrix i to a training instance x gives us a new real-valued feature." ></td>
	<td class="line x" title="186:229	If features from different domains have similar entries (scores) in the projection row, they are assumed to correspond (Blitzer, 2008)." ></td>
	<td class="line o" title="187:229	Figure 4 shows example of correspondences that SCL found in the Prince dataset." ></td>
	<td class="line x" title="188:229	The first column represents the score of a feature." ></td>
	<td class="line x" title="189:229	The labels wiki and alp indicate the domain of the features, respectively." ></td>
	<td class="line x" title="190:229	For readability, we here grouped the features obtaining similar scores." ></td>
	<td class="line x" title="191:229	0.00010248|dep35(Chaka Khan,name(PER),hd/su,verb,ben)|wiki 0.00010248|dep35(de,det,hd/det,adj,Afro-Amerikaanse)|wiki 0.00010248|dep35(Yvette Marie Stevens,name(PER),hd/app, noun,zangeres)|wiki 0.000102772|dep34(leraar,noun,hd/su,verb)|alp 0.000161095|dep34(commissie,noun,hd/obj1,prep)|16|alp 0.00016113|dep34(Confessions Tour,name,hd/obj1,prep)|2|wiki 0.000161241|dep34(orgel,noun,hd/obj1,prep)|1|wiki 0.000217698|dep34(tournee,noun,hd/su,verb)|1|wiki 0.000223301|dep34(regisseur,noun,hd/su,verb)|15|wiki 0.000224517|dep34(voorsprong,noun,hd/su,verb)|2|alp 0.000224684|dep34(wetenschap,noun,hd/su,verb)|2|alp 0.000226617|dep34(pop_rock,noun,hd/su,verb)|1|wiki 0.000228918|dep34(plan,noun,hd/su,verb)|9|alp Figure 4: Example projection from  (row 2)." ></td>
	<td class="line o" title="192:229	SCL clustered information about Chaka Khan, an Afro-Amerikaanse zangeres (afro-american singer) whose real name is Yvette Marie Stevens." ></td>
	<td class="line x" title="193:229	She had close connections to Prince, who even wrote one of her singles." ></td>
	<td class="line x" title="194:229	These features got aligned to the Alpino feature leraar (teacher)." ></td>
	<td class="line o" title="195:229	Moreover, SCL finds that tournee, regisseur and pop rock in the Prince domain behave like voorsprong (advance), wetenschap (research) and plan as possible heads in a subject relation in the newspaper domain." ></td>
	<td class="line x" title="196:229	Similarly, correspon43 dences between the direct object features Confessions Tour and orgel (pipe organ) to commissie (commission) are discovered." ></td>
	<td class="line x" title="197:229	More unlabeled data In the experiments so far, we balanced the amount of source and target data." ></td>
	<td class="line x" title="198:229	We started to examine the effect of more unlabeled target domain data." ></td>
	<td class="line x" title="199:229	For the Prince dataset, we included all supercategories in constructing the related target domain data." ></td>
	<td class="line x" title="200:229	The so obtained dataset contains: 859 articles, 29,186 sentences and 385,289 tokens; hence, the size approximately tripled (w.r.t. Table 2)." ></td>
	<td class="line o" title="201:229	Table 5 shows the effect of using this larger dataset for SCL with h = 25." ></td>
	<td class="line x" title="202:229	The accuracy increases (from 85.12 to 85.25)." ></td>
	<td class="line x" title="203:229	Thus, there seems to be a positive effect (to be investigated further)." ></td>
	<td class="line x" title="204:229	CA f-score  rel.er." ></td>
	<td class="line o" title="205:229	baseline Prince 85.03 85.38 78.06 0.00 SCL[+/-], h = 25, all 85.25 85.58 79.42 6.20 Table 5: First result on increasing unlabeled data." ></td>
	<td class="line x" title="206:229	Dimensionality reduction by feature type We have started to implement the extension discussed in section 4.2, i.e. perform separate dimensionality reductions based on blocks of nonpivot features." ></td>
	<td class="line x" title="207:229	We clustered nonpivots (see section 4.1 for a description) into 9 types (ordered in terms of decreasing cluster size): dep, f1/f2 (pos), r1/r2 (rules), appos person, mf, z, h1, in year, dist. For each type, a separate SVD was computed on submatrix Wt (illustrated in Figure 2)." ></td>
	<td class="line x" title="208:229	Then, separate projections were applied to every training instance." ></td>
	<td class="line x" title="209:229	The results of these experiments on the Prince dataset are shown in Figure 5." ></td>
	<td class="line o" title="210:229	Applying SCL with dimensionality reduction by feature type (SCL block) results in a model that performs better (CA 85.27,  79.52, rel.er." ></td>
	<td class="line x" title="211:229	6.65%) than the model with no feature split (no block SVDs), thus obtaining a relative error reduction of 6.65% over the baseline." ></td>
	<td class="line x" title="212:229	The same figure also shows what happens if we remove a specific feature type at a time; the apposition features contribute the most on this Prince domain." ></td>
	<td class="line x" title="213:229	As a fact, one third of the sentences in the Prince testset contain constructions with appositions (e.g. about film-, albumand song titles)." ></td>
	<td class="line o" title="214:229	6 Conclusions and Future Work The paper presents an application of Structural Correspondence Learning (SCL) to parse disamFigure 5: Results of dimensionality reduction by feature type, h = 25; block SVD included all 9 feature types; the right part shows the accuracy when one feature type was removed." ></td>
	<td class="line x" title="215:229	biguation." ></td>
	<td class="line nc" title="216:229	While SCL has been successfully applied to PoS tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007), its effectiveness for parsing was rather unexplored." ></td>
	<td class="line pc" title="217:229	The empirical results show that our instantiation of SCL to parse disambiguation gives promising initial results, even without the many additional extensions on the feature level as done in Blitzer et al.(2006)." ></td>
	<td class="line x" title="219:229	We exploited Wikipedia as primary resource, both for collecting unlabeled target domain data, as well as test suite for empirical evaluation." ></td>
	<td class="line p" title="220:229	On the three examined datasets, SCL slightly but constantly outperformed the baseline." ></td>
	<td class="line o" title="221:229	Applying SCL involves many design choices and practical issues, which we tried to depict here in detail." ></td>
	<td class="line x" title="222:229	A novelty in our application is that we first actually parse the unlabeled data from both domains." ></td>
	<td class="line x" title="223:229	This allows us to get a possibly noisy, but more abstract representation of the underlying data on which the pivot predictors are trained." ></td>
	<td class="line x" title="224:229	In the near future, we plan to extend the work on semi-supervised domain adaptation for parse disambiguation, viz." ></td>
	<td class="line o" title="225:229	(1) further explore/refine SCL (block SVDs, varying amount of target domain data, other testsets, etc.), and (2) examine selftraining." ></td>
	<td class="line x" title="226:229	Studies on the latter have focused mainly on generative, constituent based, i.e. data-driven parsing systems." ></td>
	<td class="line x" title="227:229	Furthermore, from a machine learning point of view, it would be interesting to know a measure of corpus similarity to estimate the success of porting an NLP system from one domain to another." ></td>
	<td class="line x" title="228:229	This relates to the general question of what is meant by domain." ></td>
	<td class="line x" title="229:229	44" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-2046
Improving SCL Model for Sentiment-Transfer Learning
Tan, Songbo;Cheng, Xue-Qi;"></td>
	<td class="line x" title="1:84	Proceedings of NAACL HLT 2009: Short Papers, pages 181184, Boulder, Colorado, June 2009." ></td>
	<td class="line p" title="2:84	c 2009 Association for Computational Linguistics Improving SCL Model for Sentiment-Transfer Learning  Songbo Tan Institute of Computing Technology Beijing, China tansongbo@software.ict.ac.cn Xueqi Cheng Institute of Computing Technology Beijing, China cxq@ict.ac.cn  ABSTRACT In recent years, Structural Correspondence Learning (SCL) is becoming one of the most promising techniques for sentiment-transfer learning." ></td>
	<td class="line o" title="3:84	However, SCL model treats each feature as well as each instance by an equivalent-weight strategy." ></td>
	<td class="line x" title="4:84	To address the two issues effectively, we proposed a weighted SCL model (W-SCL), which weights the features as well as the instances." ></td>
	<td class="line x" title="5:84	More specifically, W-SCL assigns a smaller weight to high-frequency domain-specific (HFDS) features and assigns a larger weight to instances with the same label as the involved pivot feature." ></td>
	<td class="line x" title="6:84	The experimental results indicate that proposed W-SCL model could overcome the adverse influence of HFDS features, and leverage knowledge from labels of instances and pivot features." ></td>
	<td class="line x" title="7:84	1   Introduction In the community of sentiment analysis (Turney 2002; Pang et al., 2002; Tang et al., 2009), transferring a sentiment classifier from one source domain to another target domain is still far from a trivial work, because sentiment expression often behaves with strong domain-specific nature." ></td>
	<td class="line x" title="8:84	Up to this time, many researchers have proposed techniques to address this problem, such as classifiers adaptation, generalizable features detection and so on (DaumeIII et al., 2006; Jiang et al., 2007; Tan et al., 2007; Tan et al., 2008; Tan et al., 2009)." ></td>
	<td class="line pc" title="9:84	Among these techniques, SCL (Structural Correspondence Learning) (Blitzer et al., 2006) is regarded as a promising method to tackle transfer-learning problem." ></td>
	<td class="line o" title="10:84	The main idea behind SCL model is to identify correspondences among features from different domains by modeling their correlations with pivot features (or generalizable features)." ></td>
	<td class="line x" title="11:84	Pivot features behave similarly in both domains." ></td>
	<td class="line x" title="12:84	If non-pivot features from different domains are correlated with many of the same pivot features, then we assume them to be corresponded with each other, and treat them similarly when training a sentiment classifier." ></td>
	<td class="line o" title="13:84	However, SCL model treats each feature as well as each instance by an equivalent-weight strategy." ></td>
	<td class="line n" title="14:84	From the perspective of feature, this strategy fails to overcome the adverse influence of highfrequency domain-specific (HFDS) features." ></td>
	<td class="line x" title="15:84	For example, the words stock or market occurs frequently in most of stock reviews, so these nonsentiment features tend to have a strong correspondence with pivot features." ></td>
	<td class="line x" title="16:84	As a result, the representative ability of the other sentiment features will inevitably be weakened to some degree." ></td>
	<td class="line x" title="17:84	To address this issue, we proposed Frequently Exclusively-occurring Entropy (FEE) to pick out HFDS features, and proposed a feature-weighted SCL model (FW-SCL) to adjust the influence of HFDS features in building correspondence." ></td>
	<td class="line x" title="18:84	The main idea of FW-SCL is to assign a smaller weight to HFDS features so that the adverse influence of HFDS features can be decreased." ></td>
	<td class="line n" title="19:84	From the other perspective, the equivalentweight strategy of SCL model ignores the labels (positive or negative) of labeled instances." ></td>
	<td class="line n" title="20:84	Obviously, this is not a good idea." ></td>
	<td class="line x" title="21:84	In fact, positive pivot features tend to occur in positive instances, so the correlations built on positive instances are more reliable than that built on negative instances; and vice versa." ></td>
	<td class="line x" title="22:84	Consequently, utilization of labels of instances and pivot features can decrease the adverse influence of some co-occurrences, such as co-occurrences involved with positive pivot features and negative instances, or involved with negative pivot features and positive instances." ></td>
	<td class="line x" title="23:84	In order to take into account the labels of labeled instances, we proposed an instanceweighted SCL model (IW-SCL), which assigns a larger weight to instances with the same label as the involved pivot feature." ></td>
	<td class="line x" title="24:84	In this time, we obtain a combined model: feature-weighted and instanceweighted SCL model (FWIW-SCL)." ></td>
	<td class="line x" title="25:84	For the sake 181 of convenience, we simplify FWIW-SCL as W-SCL in the rest of this paper." ></td>
	<td class="line o" title="26:84	2   Structural Correspondence Learning In the section, we provide the detailed procedures for SCL model." ></td>
	<td class="line x" title="27:84	First we need to pick out pivot features." ></td>
	<td class="line x" title="28:84	Pivot features occur frequently in both the source and the target domain." ></td>
	<td class="line x" title="29:84	In the community of sentiment analysis, generalizable sentiment words are good candidates for pivot features, such as good and excellent." ></td>
	<td class="line x" title="30:84	In the rest of this paper, we use K to stand for the number of pivot features." ></td>
	<td class="line x" title="31:84	Second, we need to compute the pivot predictors (or mapping vectors) using selected pivot features." ></td>
	<td class="line o" title="32:84	The pivot predictors are the key job, because they directly decide the performance of SCL." ></td>
	<td class="line oc" title="33:84	For each pivot feature k, we use a loss function L k , () 2 1)( wxwxpL i i T ikk +=         (1) where the function p k (x i ) indicates whether the pivot feature k occurs in the instance x i , otherwise xif xp ik ik 0 1 1 )( >     = , where the weight vector w encodes the correspondence of the non-pivot features with the pivot feature k (Blitzer et al., 2006)." ></td>
	<td class="line x" title="34:84	Finally we use the augmented space [x T ,  x T W] T  to train the classifier on the source labeled data and predict the examples on the target domain, where W=[w 1 ,w 2 , , w K ]." ></td>
	<td class="line x" title="35:84	3   Feature-Weighted SCL Model 3.1 Measure to pick out HFDS features In order to pick out HFDS features, we proposed Frequently Exclusively-occurring Entropy (FEE)." ></td>
	<td class="line x" title="36:84	Our measure includes two criteria: occur in one domain as frequently as possible, while occur on another domain as rarely as possible." ></td>
	<td class="line x" title="37:84	To satisfy this requirement, we proposed the following formula: () ()         += )(),(min )(),(max log)(),(maxlog wPwP wPwP wPwPf no no now (2) where P o (w) and P n (w) indicate the probability of word w in the source domain and the target domain respectively:  () ()  + + = 2 )( )( o o o N wN wP                     (3) ( ) ()  + + = 2 )( )( n n n N wN wP                     (4) where N o (w) and N n (w) is the number of examples with word w in the source domain and the target domain respectively; N o  and N n  is the number of examples in the source domain and the target domain respectively." ></td>
	<td class="line x" title="38:84	In order to overcome overflow, we set  =0.0001 in our experiment reported in section 5." ></td>
	<td class="line x" title="39:84	To better understand this measure, lets take a simple example (see Table 1)." ></td>
	<td class="line x" title="40:84	Given a source dataset with 1000 documents and a target dataset with 1000 documents, 12 candidate features, and a task to pick out 2 HFDS features." ></td>
	<td class="line x" title="41:84	According to our understanding, the best choice is to pick out w 4  and w 8 .  According to formula (2), fortunately, we successfully pick out w 4 , and w 8 . This simple example validates the effectiveness of proposed FEE formula." ></td>
	<td class="line x" title="42:84	Table 1: A simple example for FEE FEE Words N o (w) N n (w) Score Rank w 1  100 100 -2.3025 6 w 2  100 90 -2.1971 4 w 3  100 45 -1.5040 3 w 4  100 4 0.9163 1 w 5  50 50 -2.9956 8 w 6  50 45 -2.8903 7 w 7  50 23 -2.2192 5 w 8  50 2 0.2231 2 w 9  4 4 -5.5214 11 w 10 4 3 -5.2337 10 w 11 4 2 -4.8283 9 w 12 1 1 -6.9077 12 3.2 Feature-Weighted SCL model To adjust the influence of HFDS features in building correspondence, we proposed featureweighted SCL model (FW-SCL), ( ) 2 1)( wxwxpL i il l llikk  +=      (5) where the function p k (x i ) indicates whether the pivot feature k occurs in the instance x i ; otherwise xif xp ik ik 0 1 1 )( >     = , and  l  is the parameter to control the weight of the HFDS feature l, 182  otherwise Zlif HFDS l     =  1    where Z HFDS  indicates the HFDS feature set and  is located in the range [0,1]." ></td>
	<td class="line o" title="43:84	When  =0, it indicates that no HFDS features are used to build the correspondence vectors; while  =1 indicates that all features are equally used to build the correspondence vectors, that is to say, proposed FW-SCL algorithm is simplified as traditional SCL algorithm." ></td>
	<td class="line o" title="44:84	Consequently, proposed FW-SCL algorithm could be regarded as a generalized version of traditional SCL algorithm." ></td>
	<td class="line n" title="45:84	4 Instance-Weighted SCL Model The traditional SCL model does not take into account the labels (positive or negative) of instances on the source domain and pivot features." ></td>
	<td class="line x" title="46:84	Although the labels of pivot features are not given at first, it is very easy to obtain these labels because the number of pivot features is typically very small." ></td>
	<td class="line x" title="47:84	Obviously, positive pivot features tend to occur in positive instances, so the correlations built on positive instances are more reliable than the correlations built on negative instances; and vice versa." ></td>
	<td class="line x" title="48:84	As a result, the ideal choice is to assign a larger weight to the instances with the same label as the involved pivot feature, while assign a smaller weight to the instances with the different label as the involved pivot feature." ></td>
	<td class="line x" title="49:84	This strategy can make correlations more reliable." ></td>
	<td class="line x" title="50:84	This is the key idea of instance-weighted SCL model (IWSCL)." ></td>
	<td class="line x" title="51:84	Combining the idea of feature-weighted SCL model (FW-SCL), we obtain the featureweighted and instance-weighted SCL model (FWIW-SCL), ()()() () ()()()   ++= 1),(11 1),( 2 jl l lljkj il l llikik xwxpxk wxwxpxkL   (6) where   is the instance weight and the function p k (x i ) indicates whether the pivot feature k occurs in the instance x i ; otherwise xif xp ik ik 0 1 1 )( >     = and  l  is the parameter to control the weight of the HFDS feature l,  otherwise Zlif HFDS l     =  1   , where Z HFDS  indicates the HFDS feature set and  is located in the range [0,1]." ></td>
	<td class="line x" title="52:84	In equation (6), the function  (z,y) indicates whether the two variables z and y have the same non-zero value, () otherwise 0y and zzif z,y =    =  0 1  ; and the function  (z) is a hinge function, whose variables are either pivot features or instances, labelnegativez has aif unknown labelpositivez has aif z   1 0 1 )(       = . For the sake of convenience, we simplify FWIW-SCL as W-SCL." ></td>
	<td class="line x" title="53:84	5   Experimental Results 5.1 Datasets We collected three Chinese domain-specific datasets: Education Reviews (Edu, from http://blog.sohu.com/learning/), Stock Reviews (Sto, from http://blog.sohu.com/stock/) and Computer Reviews (Comp, from http://detail.zol.com.cn/)." ></td>
	<td class="line x" title="54:84	All of these datasets are annotated by three linguists." ></td>
	<td class="line x" title="55:84	We use ICTCLAS (a Chinese text POS tool, http://ictclas.org/) to parse Chinese words." ></td>
	<td class="line x" title="56:84	The dataset Edu includes 1,012 negative reviews and 254 positive reviews." ></td>
	<td class="line x" title="57:84	The average size of reviews is about 600 words." ></td>
	<td class="line x" title="58:84	The dataset Sto consists of 683 negative reviews and 364 positive reviews." ></td>
	<td class="line x" title="59:84	The average length of reviews is about 460 terms." ></td>
	<td class="line x" title="60:84	The dataset Comp contains 390 negative reviews and 544 positive reviews." ></td>
	<td class="line x" title="61:84	The average length of reviews is about 120 words." ></td>
	<td class="line x" title="62:84	5.2 Comparison Methods In our experiments, we run one supervised baseline, i.e., Nave Bayes (NB), which only uses one source-domain labeled data as training data." ></td>
	<td class="line oc" title="63:84	For transfer-learning baseline, we implement traditional SCL model (T-SCL) (Blitzer et al., 2006)." ></td>
	<td class="line o" title="64:84	Like TSVM, it makes use of the sourcedomain labeled data as well as the target-domain unlabeled data." ></td>
	<td class="line x" title="65:84	5.3 Does proposed method work?" ></td>
	<td class="line x" title="66:84	To conduct our experiments, we use sourcedomain data as unlabeled set or labeled training set, and use target-domain data as unlabeled set or testing set." ></td>
	<td class="line o" title="67:84	Note that we use 100 manualannotated pivot features for T-SCL, FW-SCL and W-SCL in the following experiments." ></td>
	<td class="line x" title="68:84	We select 183 pivot features use three criteria: a) is a sentiment word; b) occurs frequently in both domains; c) has similar occurring probability." ></td>
	<td class="line o" title="69:84	For T-SCL, FWSCL and W-SCL, we use prototype classifier (Sebastiani, 2002) to train the final model." ></td>
	<td class="line o" title="70:84	Table 2 shows the results of experiments comparing proposed method with supervised learning, transductive learning and T-SCL." ></td>
	<td class="line x" title="71:84	For FW-SCL, the Z HFDS  is set to 200 and   is set to 0.1; For W-SCL, the Z HFDS  is set to 200,   is set to 0.1, and   is set to 0.9." ></td>
	<td class="line n" title="72:84	As expected, proposed method FW-SCL does indeed provide much better performance than supervised baselines, TSVM and T-SCL model." ></td>
	<td class="line n" title="73:84	For example, the average accuracy of FW-SCL beats supervised baselines by about 12 percents, beats TSVM by about 11 percents and beats TSCL by about 10 percents." ></td>
	<td class="line x" title="74:84	This result indicates that proposed FW-SCL model could overcome the shortcomings of HFDS features in building correspondence vectors." ></td>
	<td class="line x" title="75:84	More surprisingly, instance-weighting strategy can further boost the performance of FW-SCL by about 4 percents." ></td>
	<td class="line x" title="76:84	This result indicates that the labels of instances and pivot features are very useful in building the correlation vectors." ></td>
	<td class="line x" title="77:84	This result also verifies our analysis in section 4: positive pivot features tend to occur in positive instances, so the correlations built on positive instances are more reliable than the correlations built on negative instances, and vice versa." ></td>
	<td class="line n" title="78:84	Table 2: Accuracy of different methods  NB T-SCL FW-SCL W-SCL Edu->Sto 0.6704 0.7965 0.7917 0.8108 Edu->Comp 0.5085 0.8019 0.8993 0.9025 Sto->Edu 0.6824 0.7712 0.9072 0.9368 Sto->Comp 0.5053 0.8126 0.8126 0.8693 Comp->Sto 0.6580 0.6523 0.7010 0.7717 Comp->Edu 0.6114 0.5976 0.9112 0.9408 Average 0.6060 0.7387 0.8372 0.8720 Although SCL is a method designed for transfer learning, but it cannot provide better performance than TSVM." ></td>
	<td class="line x" title="79:84	This result verifies the analysis in section 3: a small amount of HFDS features occupy a large amount of weight in classification model, but hardly carry corresponding sentiment." ></td>
	<td class="line o" title="80:84	In another word, very few top-frequency words degrade the representative ability of SCL model for sentiment classification." ></td>
	<td class="line x" title="81:84	6 Conclusion Remarks In this paper, we proposed a weighted SCL model (W-SCL) for domain adaptation in the context of sentiment analysis." ></td>
	<td class="line x" title="82:84	On six domaintransfer tasks, W-SCL consistently produces much better performance than the supervised, semisupervised and transfer-learning baselines." ></td>
	<td class="line x" title="83:84	As a result, we can say that proposed W-SCL model offers a better choice for sentiment-analysis applications that require high-precision classification but hardly have any labeled training data." ></td>
	<td class="line x" title="84:84	7 Acknowledgments This work was mainly supported by two funds, i.e., 0704021000 and 60803085, and one another project, i.e., 2004CB318109." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1001
Heterogeneous Transfer Learning for Image Clustering via the SocialWeb
Yang, Qiang;Chen, Yuqiang;Xue, Gui-Rong;Dai, Wenyuan;Yu, Yong;"></td>
	<td class="line x" title="1:190	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 19, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:190	c2009 ACL and AFNLP Heterogeneous Transfer Learning for Image Clustering via the Social Web Qiang Yang Hong Kong University of Science and Technology, Clearway Bay, Kowloon, Hong Kong qyang@cs.ust.hk Yuqiang Chen Gui-Rong Xue Wenyuan Dai Yong Yu Shanghai Jiao Tong University, 800 Dongchuan Road, Shanghai 200240, China {yuqiangchen,grxue,dwyak,yyu}@apex.sjtu.edu.cn Abstract In this paper, we present a new learning scenario, heterogeneous transfer learning, which improves learning performance when the data can be in different feature spaces and where no correspondence between data instances in these spaces is provided." ></td>
	<td class="line x" title="3:190	In the past, we have classified Chinese text documents using English training data under the heterogeneous transfer learning framework." ></td>
	<td class="line x" title="4:190	In this paper, we present image clustering as an example to illustrate how unsupervised learning can be improved by transferring knowledge from auxiliary heterogeneous data obtained from the social Web." ></td>
	<td class="line x" title="5:190	Image clustering is useful for image sense disambiguation in query-based image search, but its quality is often low due to imagedata sparsity problem." ></td>
	<td class="line x" title="6:190	We extend PLSA to help transfer the knowledge from social Web data, which have mixed feature representations." ></td>
	<td class="line x" title="7:190	Experiments on image-object clustering and scene clustering tasks show that our approach in heterogeneous transfer learning based on the auxiliary data is indeed effective and promising." ></td>
	<td class="line x" title="8:190	1 Introduction Traditional machine learning relies on the availability of a large amount of data to train a model, which is then applied to test data in the same feature space." ></td>
	<td class="line x" title="9:190	However, labeled data are often scarce and expensive to obtain." ></td>
	<td class="line oc" title="10:190	Various machine learning strategies have been proposed to address this problem, including semi-supervised learning (Zhu, 2007), domain adaptation (Wu and Dietterich, 2004; Blitzer et al., 2006; Blitzer et al., 2007; Arnold et al., 2007; Chan and Ng, 2007; Daume, 2007; Jiang and Zhai, 2007; Reichart and Rappoport, 2007; Andreevskaia and Bergler, 2008), multi-task learning (Caruana, 1997; Reichart et al., 2008; Arnold et al., 2008), self-taught learning (Raina et al., 2007), etc. A commonality among these methods is that they all require the training data and test data to be in the same feature space." ></td>
	<td class="line o" title="11:190	In addition, most of them are designed for supervised learning." ></td>
	<td class="line x" title="12:190	However, in practice, we often face the problem where the labeled data are scarce in their own feature space, whereas there may be a large amount of labeled heterogeneous data in another feature space." ></td>
	<td class="line x" title="13:190	In such situations, it would be desirable to transfer the knowledge from heterogeneous data to domains where we have relatively little training data available." ></td>
	<td class="line x" title="14:190	To learn from heterogeneous data, researchers have previously proposed multi-view learning (Blum and Mitchell, 1998; Nigam and Ghani, 2000) in which each instance has multiple views in different feature spaces." ></td>
	<td class="line x" title="15:190	Different from previous works, we focus on the problem of heterogeneous transfer learning, which is designed for situation when the training data are in one feature space (such as text), and the test data are in another (such as images), and there may be no correspondence between instances in these spaces." ></td>
	<td class="line x" title="16:190	The type of heterogeneous data can be very different, as in the case of text and image." ></td>
	<td class="line x" title="17:190	To consider how heterogeneous transfer learning relates to other types of learning, Figure 1 presents an intuitive illustration of four learning strategies, including traditional machine learning, transfer learning across different distributions, multi-view learning and heterogeneous transfer learning." ></td>
	<td class="line x" title="18:190	As we can see, an important distinguishing feature of heterogeneous transfer learning, as compared to other types of learning, is that more constraints on the problem are relaxed, such that data instances do not need to correspond anymore." ></td>
	<td class="line x" title="19:190	This allows, for example, a collection of Chinese text documents to be classified using another collection of English text as the 1 training data (c.f.(Ling et al., 2008) and Section 2.1)." ></td>
	<td class="line x" title="21:190	In this paper, we will give an illustrative example of heterogeneous transfer learning to demonstrate how the task of image clustering can benefit from learning from the heterogeneous social Web data." ></td>
	<td class="line x" title="22:190	A major motivation of our work is Web-based image search, where users submit textual queries and browse through the returned result pages." ></td>
	<td class="line x" title="23:190	One problem is that the user queries are often ambiguous." ></td>
	<td class="line x" title="24:190	An ambiguous keyword such as Apple might retrieve images of Apple computers and mobile phones, or images of fruits." ></td>
	<td class="line x" title="25:190	Image clustering is an effective method for improving the accessibility of image search result." ></td>
	<td class="line x" title="26:190	Loeff et al.(2006) addressed the image clustering problem with a focus on image sense discrimination." ></td>
	<td class="line x" title="28:190	In their approach, images associated with textual features are used for clustering, so that the text and images are clustered at the same time." ></td>
	<td class="line x" title="29:190	Specifically, spectral clustering is applied to the distance matrix built from a multimodal feature set associated with the images to get a better feature representation." ></td>
	<td class="line x" title="30:190	This new representation contains both image and text information, with which the performance of image clustering is shown to be improved." ></td>
	<td class="line x" title="31:190	A problem with this approach is that when images contained in the Web search results are very scarce and when the textual data associated with the images are very few, clustering on the images and their associated text may not be very effective." ></td>
	<td class="line x" title="32:190	Different from these previous works, in this paper, we address the image clustering problem as a heterogeneous transfer learning problem." ></td>
	<td class="line x" title="33:190	We aim to leverage heterogeneous auxiliary data, social annotations, etc. to enhance image clustering performance." ></td>
	<td class="line x" title="34:190	We observe that the World Wide Web has many annotated images in Web sites such as Flickr (http://www.flickr.com), which can be used as auxiliary information source for our clustering task." ></td>
	<td class="line x" title="35:190	In this work, our objective is to cluster a small collection of images that we are interested in, where these images are not sufficient for traditional clustering algorithms to perform well due to data sparsity and the low level of image features." ></td>
	<td class="line x" title="36:190	We investigate how to utilize the readily available socially annotated image data on the Web to improve image clustering." ></td>
	<td class="line x" title="37:190	Although these auxiliary data may be irrelevant to the images to be clustered and cannot be directly used to solve the data sparsity problem, we show that they can still be used to estimate a good latent feature representation, which can be used to improve image clustering." ></td>
	<td class="line x" title="38:190	2 Related Works 2.1 Heterogeneous Transfer Learning Between Languages In this section, we summarize our previous work on cross-language classification as an example of heterogeneous transfer learning." ></td>
	<td class="line x" title="39:190	This example is related to our image clustering problem because they both rely on data from different feature spaces." ></td>
	<td class="line x" title="40:190	As the World Wide Web in China grows rapidly, it has become an increasingly important problem to be able to accurately classify Chinese Web pages." ></td>
	<td class="line x" title="41:190	However, because the labeled Chinese Web pages are still not sufficient, we often find it difficult to achieve high accuracy by applying traditional machine learning algorithms to the Chinese Web pages directly." ></td>
	<td class="line x" title="42:190	Would it be possible to make the best use of the relatively abundant labeled English Web pages for classifying the Chinese Web pages?" ></td>
	<td class="line x" title="43:190	To answer this question, in (Ling et al., 2008), we developed a novel approach for classifying the Web pages in Chinese using the training documents in English." ></td>
	<td class="line x" title="44:190	In this subsection, we give a brief summary of this work." ></td>
	<td class="line x" title="45:190	The problem to be solved is: we are given a collection of labeled English documents and a large number of unlabeled Chinese documents." ></td>
	<td class="line x" title="46:190	The English and Chinese texts are not aligned." ></td>
	<td class="line x" title="47:190	Our objective is to classify the Chinese documents into the same label space as the English data." ></td>
	<td class="line x" title="48:190	Our key observation is that even though the data use different text features, they may still share many of the same semantic information." ></td>
	<td class="line x" title="49:190	What we need to do is to uncover this latent semantic information by finding out what is common among them." ></td>
	<td class="line x" title="50:190	We did this in (Ling et al., 2008) by using the information bottleneck theory (Tishby et al., 1999)." ></td>
	<td class="line x" title="51:190	In our work, we first translated the Chinese document into English automatically using some available translation software, such as Google translate." ></td>
	<td class="line x" title="52:190	Then, we encoded the training text as well as the translated target text together, in terms of the information theory." ></td>
	<td class="line x" title="53:190	We allowed all the information to be put through a bottleneck and be represented by a limited number of code2 Figure 1: An intuitive illustration of different kinds learning strategies using classification/clustering of image apple and banana as the example." ></td>
	<td class="line x" title="54:190	words (i.e. labels in the classification problem)." ></td>
	<td class="line x" title="55:190	Finally, information bottleneck was used to maintain most of the common information between the two data sources, and discard the remaining irrelevant information." ></td>
	<td class="line x" title="56:190	In this way, we can approximate the ideal situation where similar training and translated test pages shared in the common part are encoded into the same codewords, and are thus assigned the correct labels." ></td>
	<td class="line x" title="57:190	In (Ling et al., 2008), we experimentally showed that heterogeneous transfer learning can indeed improve the performance of cross-language text classification as compared to directly training learning models (e.g., Naive Bayes or SVM) and testing on the translated texts." ></td>
	<td class="line x" title="58:190	2.2 Other Works in Transfer Learning In the past, several other works made use of transfer learning for cross-feature-space learning." ></td>
	<td class="line x" title="59:190	Wu and Oard (2008) proposed to handle the crosslanguage learning problem by translating the data into a same language and applying kNN on the latent topic space for classification." ></td>
	<td class="line x" title="60:190	Most learning algorithms for dealing with cross-language heterogeneous data require a translator to convert the data to the same feature space." ></td>
	<td class="line x" title="61:190	For those data that are in different feature spaces where no translator is available, Davis and Domingos (2008) proposed a Markov-logic-based transfer learning algorithm, which is called deep transfer, for transferring knowledge between biological domains and Web domains." ></td>
	<td class="line x" title="62:190	Dai et al.(2008a) proposed a novel learning paradigm, known as translated learning, to deal with the problem of learning heterogeneous data that belong to quite different feature spaces by using a risk minimization framework." ></td>
	<td class="line x" title="64:190	2.3 Relation to PLSA Our work makes use of PLSA." ></td>
	<td class="line x" title="65:190	Probabilistic latent semantic analysis (PLSA) is a widely used probabilistic model (Hofmann, 1999), and could be considered as a probabilistic implementation of latent semantic analysis (LSA) (Deerwester et al., 1990)." ></td>
	<td class="line x" title="66:190	An extension to PLSA was proposed in (Cohn and Hofmann, 2000), which incorporated the hyperlink connectivity in the PLSA model by using a joint probabilistic model for connectivity and content." ></td>
	<td class="line x" title="67:190	Moreover, PLSA has shown a lot of applications ranging from text clustering (Hofmann, 2001) to image analysis (Sivic et al., 2005)." ></td>
	<td class="line x" title="68:190	2.4 Relation to Clustering Compared to many previous works on image clustering, we note that traditional image clustering is generally based on techniques such as Kmeans (MacQueen, 1967) and hierarchical clustering (Kaufman and Rousseeuw, 1990)." ></td>
	<td class="line x" title="69:190	However, when the data are sparse, traditional clustering algorithms may have difficulties in obtaining high-quality image clusters." ></td>
	<td class="line x" title="70:190	Recently, several researchers have investigated how to leverage the auxiliary information to improve target clustering 3 performance, such as supervised clustering (Finley and Joachims, 2005), semi-supervised clustering (Basu et al., 2004), self-taught clustering (Dai et al., 2008b), etc. 3 Image Clustering with Annotated Auxiliary Data In this section, we present our annotation-based probabilistic latent semantic analysis algorithm (aPLSA), which extends the traditional PLSA model by incorporating annotated auxiliary image data." ></td>
	<td class="line x" title="71:190	Intuitively, our algorithm aPLSA performs PLSA analysis on the target images, which are converted to an image instance-to-feature cooccurrence matrix." ></td>
	<td class="line x" title="72:190	At the same time, PLSA is also applied to the annotated image data from social Web, which is converted into a text-to-imagefeature co-occurrence matrix." ></td>
	<td class="line x" title="73:190	In order to unify those two separate PLSA models, these two steps are done simultaneously with common latent variables used as a bridge linking them." ></td>
	<td class="line x" title="74:190	Through these common latent variables, which are now constrained by both target image data and auxiliary annotation data, a better clustering result is expected for the target data." ></td>
	<td class="line x" title="75:190	3.1 Probabilistic Latent Semantic Analysis LetF = {fi}|F|i=1 be an image feature space, and V = {vi}|V|i=1 be the image data set." ></td>
	<td class="line x" title="76:190	Each image vi V is represented by a bag-of-features{f|f  vif F}." ></td>
	<td class="line x" title="77:190	Based on the image data set V, we can estimate an image instance-to-feature co-occurrence matrix A|V||F|  R|V||F|, where each element Aij (1i|V|and 1j |F|) in the matrix A is the frequency of the feature fj appearing in the instance vi." ></td>
	<td class="line x" title="78:190	LetW ={wi}|W|i=1 be a text feature space." ></td>
	<td class="line x" title="79:190	The annotated image data allow us to obtain the cooccurrence information between images v and text features w W. An example of annotated image data is the Flickr (http://www.flickr." ></td>
	<td class="line x" title="80:190	com), which is a social Web site containing a large number of annotated images." ></td>
	<td class="line x" title="81:190	By extracting image features from the annotated images v, we can estimate a text-to-image feature co-occurrence matrix B|W||F|  R|W||F|, where each element Bij (1  i  |W| and 1  j |F|) in the matrix B is the frequency of the text feature wi and the image feature fj occurring together in the annotated image data set." ></td>
	<td class="line x" title="82:190	V Z F P(z|v) P(f|z) Figure 2: Graphical model representation ofPLSA model." ></td>
	<td class="line x" title="83:190	LetZ ={zi}|Z|i=1 be the latent variable set in our aPLSA model." ></td>
	<td class="line x" title="84:190	In clustering, each latent variable ziZ corresponds to a certain cluster." ></td>
	<td class="line x" title="85:190	Our objective is to estimate a clustering function g : V mapsto Z with the help of the two cooccurrence matrices A and B as defined above." ></td>
	<td class="line x" title="86:190	To formally introduce the aPLSA model, we start from the probabilistic latent semantic analysis (PLSA) (Hofmann, 1999) model." ></td>
	<td class="line x" title="87:190	PLSA is a probabilistic implementation of latent semantic analysis (LSA) (Deerwester et al., 1990)." ></td>
	<td class="line x" title="88:190	In our image clustering task, PLSA decomposes the instance-feature co-occurrence matrix A under the assumption of conditional independence of image instancesV and image featuresF, given the latent variablesZ." ></td>
	<td class="line x" title="89:190	P(f|v) = summationdisplay zZ P(f|z)P(z|v)." ></td>
	<td class="line x" title="90:190	(1) The graphical model representation of PLSA is shown in Figure 2." ></td>
	<td class="line x" title="91:190	Based on the PLSA model, the log-likelihood can be defined as: L= summationdisplay i summationdisplay j Aijsummationtext jprime Aijprime logP(fj|vi) (2) where A|V||F| R|V||F| is the image instancefeature co-occurrence matrix." ></td>
	<td class="line x" title="92:190	The term AijP jprime Aijprimein Equation (2) is a normalization term ensuring each image is giving the same weight in the loglikelihood." ></td>
	<td class="line x" title="93:190	Using EM algorithm (Dempster et al., 1977), which locally maximizes the log-likelihood of the PLSA model (Equation (2)), the probabilities P(f|z) and P(z|v) can be estimated." ></td>
	<td class="line x" title="94:190	Then, the clustering function is derived as g(v) = argmax zZ P(z|v)." ></td>
	<td class="line x" title="95:190	(3) Due to space limitation, we omit the details for the PLSA model, which can be found in (Hofmann, 1999)." ></td>
	<td class="line x" title="96:190	3.2 aPLSA: Annotation-based PLSA In this section, we consider how to incorporate a large number of socially annotated images in a 4 V W Z F P(z|v ) P(z|w ) P(f|z) Figure 3: Graphical model representation of aPLSA model." ></td>
	<td class="line x" title="97:190	unified PLSA model for the purpose of utilizing the correlation between text features and image features." ></td>
	<td class="line x" title="98:190	In the auxiliary data, each image has certain textual tags that are attached by users." ></td>
	<td class="line x" title="99:190	The correlation between text features and image features can be formulated as follows." ></td>
	<td class="line x" title="100:190	P(f|w) = summationdisplay zZ P(f|z)P(z|w)." ></td>
	<td class="line x" title="101:190	(4) It is clear that Equations (1) and (4) share a same term P(f|z)." ></td>
	<td class="line x" title="102:190	So we design a new PLSA model by joining the probabilistic model in Equation (1) and the probabilistic model in Equation (4) into a unified model, as shown in Figure 3." ></td>
	<td class="line x" title="103:190	In Figure 3, the latent variables Z depend not only on the correlation between image instancesV and image featuresF, but also the correlation between text featuresWand image featuresF." ></td>
	<td class="line x" title="104:190	Therefore, the auxiliary socially-annotated image data can be used to help the target image clustering performance by estimating good set of latent variablesZ." ></td>
	<td class="line x" title="105:190	Based on the graphical model representation in Figure 3, we derive the log-likelihood objective function, in a similar way as in (Cohn and Hofmann, 2000), as follows L= summationdisplay j bracketleftBigg  summationdisplay i Aijsummationtext jprime Aijprime logP(fj|vi) +(1) summationdisplay l Bljsummationtext jprime Bljprime logP(fj|wl) bracketrightBigg , (5) where A|V||F| R|V||F| is the image instancefeature co-occurrence matrix, and B|W||F|  R|W||F| is the text-to-image feature-level cooccurrence matrix." ></td>
	<td class="line x" title="106:190	Similar to Equation (2), AijP jprime Aijprime and BljP jprime Bljprime in Equation (5) are the normalization terms to prevent imbalanced cases." ></td>
	<td class="line x" title="107:190	Furthermore,  acts as a trade-off parameter between the co-occurrence matrices A and B. In the extreme case when  = 1, the log-likelihood objective function ignores all the biases from the text-to-image occurrence matrix B. In this case, the aPLSA model degenerates to the traditional PLSA model." ></td>
	<td class="line x" title="108:190	Therefore, aPLSA is an extension to the PLSA model." ></td>
	<td class="line x" title="109:190	Now, the objective is to maximize the loglikelihoodLof the aPLSA model in Equation (5)." ></td>
	<td class="line x" title="110:190	Then we apply the EM algorithm (Dempster et al., 1977) to estimate the conditional probabilities P(f|z), P(z|w) and P(z|v) with respect to each dependence in Figure 3 as follows." ></td>
	<td class="line x" title="111:190	 E-Step: calculate the posterior probability of each latent variable z given the observation of image features f, image instances v and text features w based on the old estimate of P(f|z), P(z|w) and P(z|v): P(zk|vi,fj) = P(fj|zk)P(zk|vi)summationtext kprime P(fj|zkprime)P(zkprime|vi) (6) P(zk|wl,fj) = P(fj|zk)P(zk|wl)summationtext kprime P(fj|zkprime)P(zkprime|wl) (7)  M-Step: re-estimates conditional probabilities P(zk|vi) and P(zk|wl): P(zk|vi) = summationdisplay j Aijsummationtext jprime Aijprime P(zk|vi,fj) (8) P(zk|wl) = summationdisplay j Bljsummationtext jprime Bljprime P(zk|wl,fj) (9) and conditional probability P(fj|zk), which is a mixture portion of posterior probability of latent variables P(fj|zk) summationdisplay i Aijsummationtext jprime Aijprime P(zk|vi,fj) + (1) summationdisplay l Bljsummationtext jprime Bljprime P(zk|wl,fj) (10) Finally, the clustering function for a certain image v is g(v) = argmax zZ P(z|v)." ></td>
	<td class="line x" title="112:190	(11) From the above equations, we can derive our annotation-based probabilistic latent semantic analysis (aPLSA) algorithm." ></td>
	<td class="line x" title="113:190	As shown in Algorithm 1, aPLSA iteratively performs the E-Step and the M-Step in order to seek local optimal points based on the objective functionLin Equation (5)." ></td>
	<td class="line x" title="114:190	5 Algorithm 1 Annotation-based PLSA Algorithm (aPLSA) Input: TheV-F co-occurrence matrix A andWF co-occurrence matrix B. Output: A clustering (partition) function g :Vmapsto Z, which maps an image instance vVto a latent variable zZ. 1: Initial Z so that |Z| equals the number clusters desired." ></td>
	<td class="line x" title="115:190	2: Initialize P(z|v), P(z|w), P(f|z) randomly." ></td>
	<td class="line x" title="116:190	3: while the change ofLin Eq." ></td>
	<td class="line x" title="117:190	(5) between two sequential iterations is greater than a predefined threshold do 4: E-Step: Update P(z|v,f) and P(z|w,f) based on Eq." ></td>
	<td class="line x" title="118:190	(6) and (7) respectively." ></td>
	<td class="line x" title="119:190	5: M-Step: Update P(z|v), P(z|w) and P(f|z) based on Eq." ></td>
	<td class="line x" title="120:190	(8), (9) and (10) respectively." ></td>
	<td class="line x" title="121:190	6: end while 7: for all v inV do 8: g(v)argmax z P(z|v)." ></td>
	<td class="line x" title="122:190	9: end for 10: Return g. 4 Experiments In this section, we empirically evaluate theaPLSA algorithm together with some state-of-art baseline methods on two widely used image corpora, to demonstrate the effectiveness of our algorithm aPLSA." ></td>
	<td class="line x" title="123:190	4.1 Data Sets In order to evaluate the effectiveness of our algorithm aPLSA, we conducted experiments on several data sets generated from two image corpora, Caltech-256 (Griffin et al., 2007) and the fifteenscene (Lazebnik et al., 2006)." ></td>
	<td class="line x" title="124:190	The Caltech-256 data set has 256 image objective categories, ranging from animals to buildings, from plants to automobiles, etc. The fifteen-scene data set contains 15 scenes such as store and forest." ></td>
	<td class="line x" title="125:190	From these two corpora, we randomly generated eleven image clustering tasks, including seven 2way clustering tasks, two 4-way clustering task, one 5-way clustering task and one 8-way clustering task." ></td>
	<td class="line x" title="126:190	The detailed descriptions for these clustering tasks are given in Table 1." ></td>
	<td class="line x" title="127:190	In these tasks, bi7 and oct1 were generated from fifteen-scene data set, and the rest were from Caltech-256 data set." ></td>
	<td class="line x" title="128:190	DATA SET INVOLVED CLASSES DATA SIZE bi1 skateboard, airplanes 102, 800 bi2 billiards, mars 278, 155 bi3 cd, greyhound 102, 94 bi4 electric-guitar, snake 122, 112 bi5 calculator, dolphin 100, 106 bi6 mushroom, teddy-bear 202, 99 bi7 MIThighway, livingroom 260, 289 quad1 calculator, diamond-ring, dolphin,microscope 100, 118, 106, 116 quad2 bonsai, comet, frog, saddle 122, 120, 115, 110 quint1 frog, kayak, bear, jesus-christ, watch 115, 102, 101, 87,201 oct1 MIThighway, MITmountain, kitchen, MITcoast, PARoffice, MITtallbuilding, livingroom, bedroom 260, 374, 210, 360, 215, 356, 289, 216 tune1 coin, horse 123, 270 tune2 socks, spider 111, 106 tune3 galaxy, snowmobile 80, 112 tune4 dice, fern 98, 110 tune5 backpack, lightning, mandolin, swan 151, 136, 93, 114 Table 1: The descriptions of all the image clustering tasks used in our experiment." ></td>
	<td class="line x" title="129:190	Among these data sets, bi7 and oct1 were generated from fifteen-scene data set, and the rest were from Caltech-256 data set." ></td>
	<td class="line x" title="130:190	To empirically investigate the parameter  and the convergence of our algorithm aPLSA, we generated five more date sets as the development sets." ></td>
	<td class="line x" title="131:190	The detailed description of these five development sets, namely tune1 to tune5 is listed in Table 1 as well." ></td>
	<td class="line x" title="132:190	The auxiliary data were crawled from the Flickr (http://www.flickr.com/) web site during August 2007." ></td>
	<td class="line x" title="133:190	Flickr is an internet community where people share photos online and express their opinions as social tags (annotations) attached to each image." ></td>
	<td class="line x" title="134:190	From Flicker, we collected 19,959 images and 91,719 related annotations, among which 2,600 words are distinct." ></td>
	<td class="line x" title="135:190	Based on the method described in Section 3, we estimated the co-occurrence matrix B between text features and image features." ></td>
	<td class="line x" title="136:190	This co-occurrence matrix B was used by all the clustering tasks in our experiments." ></td>
	<td class="line x" title="137:190	For data preprocessing, we adopted the bag-offeatures representation of images (Li and Perona, 2005) in our experiments." ></td>
	<td class="line x" title="138:190	Interesting points were found in the images and described via the SIFT descriptors (Lowe, 2004)." ></td>
	<td class="line x" title="139:190	Then, the interesting points were clustered to generate a codebook to form an image feature space." ></td>
	<td class="line x" title="140:190	The size of codebook was set to 2,000 in our experiments." ></td>
	<td class="line x" title="141:190	Based on the codebook, which serves as the image feature space, each image can be represented as a corresponding feature vector to be used in the next step." ></td>
	<td class="line x" title="142:190	To set our evaluation criterion, we used the 6 Data Set KMeans PLSA STC aPLSAseparate combined separate combined bi1 0.6450.064 0.5480.031 0.5440.074 0.5370.033 0.5860.139 0.4820.062 bi2 0.6870.003 0.6620.014 0.4640.074 0.6920.001 0.5770.016 0.4550.096 bi3 1.2940.060 1.3000.015 1.0850.073 1.1260.036 1.1030.108 1.0290.074 bi4 1.2270.080 1.1640.053 0.9760.051 1.0380.068 1.0240.089 0.9190.065 bi5 1.4500.058 1.4170.045 1.4260.025 1.4050.040 1.4110.043 1.3770.040 bi6 1.9690.078 1.8520.051 1.5140.039 1.7090.028 1.5890.121 1.5030.030 bi7 0.6860.006 0.6830.004 0.6430.058 0.6320.037 0.6510.012 0.6240.066 quad1 0.5910.094 0.6750.017 0.4880.071 0.6620.013 0.5800.115 0.4320.085 quad2 0.6480.036 0.6460.045 0.6140.062 0.6260.026 0.5910.087 0.5150.098 quint1 0.5570.021 0.5080.104 0.5470.060 0.5390.051 0.5380.100 0.5020.067 oct1 0.6590.031 0.6800.012 0.3400.147 0.6910.002 0.4110.089 0.3060.101 average 0.9470.029 0.9220.017 0.7860.009 0.8780.006 0.8240.036 0.7410.018 Table 2: Experimental result in term of entropy for all data sets and evaluation methods." ></td>
	<td class="line x" title="143:190	entropy to measure the quality of our clustering results." ></td>
	<td class="line x" title="144:190	In information theory, entropy (Shannon, 1948) is a measure of the uncertainty associated with a random variable." ></td>
	<td class="line x" title="145:190	In our problem, entropy serves as a measure of randomness of clustering result." ></td>
	<td class="line x" title="146:190	The entropy of g on a single latent variable z is defined to be H(g,z) defines summationtextcC P(c|z)log2 P(c|z), where C is the class label set of V and P(c|z) = |{v|g(v)=zt(v)=c}||{v|g(v)=z}| , in which t(v) is the true class label of image v. Lower entropy H(g,Z) indicates less randomness and thus better clustering result." ></td>
	<td class="line x" title="147:190	4.2 Empirical Analysis We now empirically analyze the effectiveness of our aPLSA algorithm." ></td>
	<td class="line x" title="148:190	Because, to our best of knowledge, few existing methods addressed the problem of image clustering with the help of social annotation image data, we can only compare our aPLSA with several state-of-the-art clustering algorithms that are not directly designed for our problem." ></td>
	<td class="line x" title="149:190	The first baseline is the well-known KMeans algorithm (MacQueen, 1967)." ></td>
	<td class="line x" title="150:190	Since our algorithm is designed based on PLSA (Hofmann, 1999), we also included PLSA for clustering as a baseline method in our experiments." ></td>
	<td class="line x" title="151:190	For each of the above two baselines, we have two strategies: (1) separated: the baseline method was applied on the target image data only; (2) combined: the baseline method was applied to cluster the combined data consisting of both target image data and the annotated image data." ></td>
	<td class="line x" title="152:190	Clustering results on target image data were used for evaluation." ></td>
	<td class="line x" title="153:190	Note that, in the combined data, all the annotations were thrown away since baseline methods evaluated in this paper do not leverage annotation information." ></td>
	<td class="line x" title="154:190	In addition, we compared our algorithm aPLSA to a state-of-the-art transfer clustering strategy, known as self-taught clustering (STC) (Dai et al., 2008b)." ></td>
	<td class="line x" title="155:190	STC makes use of auxiliary data to estimate a better feature representation to benefit the target clustering." ></td>
	<td class="line x" title="156:190	In these experiments, the annotated image data were used as auxiliary data in STC, which does not use the annotation text." ></td>
	<td class="line x" title="157:190	In our experiments, the performance is in the form of the average entropy and variance of five repeats by randomly selecting 50 images from each of the categories." ></td>
	<td class="line x" title="158:190	We selected only 50 images per category, since this paper is focused on clustering sparse data." ></td>
	<td class="line x" title="159:190	Table 2 shows the performance with respect to all comparison methods on each of the image clustering tasks measured by the entropy criterion." ></td>
	<td class="line x" title="160:190	From the tables, we can see that our algorithm aPLSA outperforms the baseline methods in all the data sets." ></td>
	<td class="line x" title="161:190	We believe that is because aPLSA can effectively utilize the knowledge from the socially annotated image data." ></td>
	<td class="line x" title="162:190	On average, aPLSA gives rise to 21.8% of entropy reduction and as compared to KMeans, 5.7% of entropy reduction as compared to PLSA, and 10.1% of entropy reduction as compared to STC." ></td>
	<td class="line x" title="163:190	4.2.1 Varying Data Size We now show how the data size affects aPLSA, with two baseline methods KMeans and PLSA as reference." ></td>
	<td class="line x" title="164:190	The experiments were conducted on different amounts of target image data, varying from 10 to 80." ></td>
	<td class="line x" title="165:190	The corresponding experimental results in average entropy over all the 11 clustering tasks are shown in Figure 4(a)." ></td>
	<td class="line x" title="166:190	From this figure, we observe that aPLSA always yields a significant reduction in entropy as compared with two baseline methodsKMeansandPLSA, regardless of the size of target image data that we used." ></td>
	<td class="line x" title="167:190	7 10 20 30 40 50 60 70 800.7 0.75 0.8 0.85 0.9 0.95 1 Data size per category Entropy   KMeans PLSA aPLSA (a) 0 0.2 0.4 0.6 0.8 10.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75  Entropy   average over 5 development sets (b) 0 50 100 150 200 250 3000.5 0.55 0.6 0.65 0.7 0.75 Number of Iteration Entropy   average over 5 development sets (c) Figure 4: (a) The entropy curve as a function of different amounts of data per category." ></td>
	<td class="line x" title="168:190	(b) The entropy curve as a function of different number of iterations." ></td>
	<td class="line x" title="169:190	(c) The entropy curve as a function of different trade-off parameter ." ></td>
	<td class="line x" title="170:190	4.2.2 Parameter Sensitivity In aPLSA, there is a trade-off parameter  that affects how the algorithm relies on auxiliary data." ></td>
	<td class="line x" title="171:190	When  = 0, the aPLSA relies only on annotated image data B. When  = 1, aPLSA relies only on target image data A, in which case aPLSA degenerates to PLSA." ></td>
	<td class="line x" title="172:190	Smaller  indicates heavier reliance on the annotated image data." ></td>
	<td class="line x" title="173:190	We have done some experiments on the development sets to investigate how different  affect the performance of aPLSA." ></td>
	<td class="line x" title="174:190	We set the number of images per category to 50, and tested the performance of aPLSA." ></td>
	<td class="line x" title="175:190	The result in average entropy over all development sets is shown in Figure 4(b)." ></td>
	<td class="line x" title="176:190	In the experiments described in this paper, we set  to 0.2, which is the best point in Figure 4(b)." ></td>
	<td class="line x" title="177:190	4.2.3 Convergence In our experiments, we tested the convergence property of our algorithm aPLSA as well." ></td>
	<td class="line x" title="178:190	Figure 4(c) shows the average entropy curve given by aPLSA over all development sets." ></td>
	<td class="line x" title="179:190	From this figure, we see that the entropy decreases very fast during the first 100 iterations and becomes stable after 150 iterations." ></td>
	<td class="line x" title="180:190	We believe that 200 iterations is sufficient for aPLSA to converge." ></td>
	<td class="line x" title="181:190	5 Conclusions In this paper, we proposed a new learning scenario called heterogeneous transfer learning and illustrated its application to image clustering." ></td>
	<td class="line x" title="182:190	Image clustering, a vital component in organizing search results for query-based image search, was shown to be improved by transferring knowledge from unrelated images with annotations in a social Web." ></td>
	<td class="line x" title="183:190	This is done by first learning the high-quality latent variables in the auxiliary data, and then transferring this knowledge to help improve the clustering of the target image data." ></td>
	<td class="line x" title="184:190	We conducted experiments on two image data sets, using the Flickr data as the annotated auxiliary image data, and showed that our aPLSA algorithm can greatly outperform several state-of-the-art clustering algorithms." ></td>
	<td class="line x" title="185:190	In natural language processing, there are many future opportunities to apply heterogeneous transfer learning." ></td>
	<td class="line x" title="186:190	In (Ling et al., 2008) we have shown how to classify the Chinese text using English text as the training data." ></td>
	<td class="line x" title="187:190	We may also consider clustering, topic modeling, question answering, etc., to be done using data in different feature spaces." ></td>
	<td class="line x" title="188:190	We can consider data in different modalities, such as video, image and audio, as the training data." ></td>
	<td class="line x" title="189:190	Finally, we will explore the theoretical foundations and limitations of heterogeneous transfer learning as well." ></td>
	<td class="line x" title="190:190	Acknowledgement Qiang Yang thanks Hong Kong CERG grant 621307 for supporting the research." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1056
Distributional Representations for Handling Sparsity in Supervised Sequence-Labeling
Huang, Fei;Yates, Alexander;"></td>
	<td class="line x" title="1:216	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 495503, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:216	c2009 ACL and AFNLP Distributional Representations for Handling Sparsity in Supervised Sequence-Labeling Fei Huang Temple University 1805 N. Broad St. Wachman Hall 324 tub58431@temple.edu Alexander Yates Temple University 1805 N. Broad St. Wachman Hall 324 yates@temple.edu Abstract Supervised sequence-labeling systems in natural language processing often suffer from data sparsity because they use word types as features in their prediction tasks." ></td>
	<td class="line x" title="3:216	Consequently, they have difficulty estimating parameters for types which appear in the test set, but seldom (or never) appear in the training set." ></td>
	<td class="line x" title="4:216	We demonstrate that distributional representations of word types, trained on unannotated text, can be used to improve performance on rare words." ></td>
	<td class="line x" title="5:216	We incorporate aspects of these representations into the feature space of our sequence-labeling systems." ></td>
	<td class="line x" title="6:216	In an experiment on a standard chunking dataset, our best technique improves a chunker from 0.76 F1 to 0.86 F1 on chunks beginning with rare words." ></td>
	<td class="line x" title="7:216	On the same dataset, it improves our part-of-speech tagger from 74% to 80% accuracy on rare words." ></td>
	<td class="line x" title="8:216	Furthermore, our system improves significantly over a baseline system when applied to text from a different domain, and it reduces the sample complexity of sequence labeling." ></td>
	<td class="line x" title="9:216	1 Introduction Data sparsity and high dimensionality are the twin curses of statistical natural language processing (NLP)." ></td>
	<td class="line x" title="10:216	In many traditional supervised NLP systems, the feature space includes dimensions for each word type in the data, or perhaps even combinations of word types." ></td>
	<td class="line x" title="11:216	Since vocabularies can be extremely large, this leads to an explosion in the number of parameters." ></td>
	<td class="line x" title="12:216	To make matters worse, language is Zipf-distributed, so that a large fraction of any training data set will be hapax legomena, very many word types will appear only a few times, and many word types will be left out of the training set altogether." ></td>
	<td class="line x" title="13:216	As a consequence, for many word types supervised NLP systems have very few, or even zero, labeled examples from which to estimate parameters." ></td>
	<td class="line x" title="14:216	The negative effects of data sparsity have been well-documented in the NLP literature." ></td>
	<td class="line x" title="15:216	The performance of state-of-the-art, supervised NLP systems like part-of-speech (POS) taggers degrades significantly on words that do not appear in the training data, or out-of-vocabulary (OOV) words (Lafferty et al., 2001)." ></td>
	<td class="line oc" title="16:216	Performance also degrades when the domain of the test set differs from the domain of the training set, in part because the test set includes more OOV words and words that appear only a few times in the training set (henceforth, rare words) (Blitzer et al., 2006; Daume III and Marcu, 2006; Chelba and Acero, 2004)." ></td>
	<td class="line x" title="17:216	We investigate the use of distributional representations, which model the probability distribution of a words context, as techniques for finding smoothed representations of word sequences." ></td>
	<td class="line x" title="18:216	That is, we use the distributional representations to share information across unannotated examples of the same word type." ></td>
	<td class="line x" title="19:216	We then compute features of the distributional representations, and provide them as input to our supervised sequence labelers." ></td>
	<td class="line x" title="20:216	Our technique is particularly well-suited to handling data sparsity because it is possible to improve performance on rare words by supplementing the training data with additional unannotated text containing more examples of the rare words." ></td>
	<td class="line x" title="21:216	We provide empirical evidence that shows how distributional representations improve sequencelabeling in the face of data sparsity." ></td>
	<td class="line x" title="22:216	Specifically, we investigate empirically the effects of our smoothing techniques on two sequence-labeling tasks, POS tagging and chunking, to answer the following: 1." ></td>
	<td class="line x" title="23:216	What is the effect of smoothing on sequencelabeling accuracy for rare word types?" ></td>
	<td class="line x" title="24:216	Our best smoothing technique improves a POS tagger by 11% on OOV words, and a chunker by an impressive 21% on OOV words." ></td>
	<td class="line x" title="25:216	495 2." ></td>
	<td class="line x" title="26:216	Can smoothing improve adaptability to new domains?" ></td>
	<td class="line x" title="27:216	After training our chunker on newswire text, we apply it to biomedical texts." ></td>
	<td class="line x" title="28:216	Remarkably, we find that the smoothed chunker achieves a higher F1 on the new domain than the baseline chunker achieves on a test set from the original newswire domain." ></td>
	<td class="line x" title="29:216	3." ></td>
	<td class="line x" title="30:216	How does our smoothing technique affect sample complexity?" ></td>
	<td class="line x" title="31:216	We show that smoothing drastically reduces sample complexity: our smoothed chunker requires under 100 labeled samples to reach 85% accuracy, whereas the unsmoothed chunker requires 3500 samples to reach the same level of performance." ></td>
	<td class="line x" title="32:216	The remainder of this paper is organized as follows." ></td>
	<td class="line x" title="33:216	Section 2 discusses the smoothing problem for word sequences, and introduces three smoothing techniques." ></td>
	<td class="line x" title="34:216	Section 3 presents our empirical study of the effects of smoothing on two sequencelabeling tasks." ></td>
	<td class="line x" title="35:216	Section 4 describes related work, and Section 5 concludes and suggests items for future work." ></td>
	<td class="line x" title="36:216	2 Smoothing Natural Language Sequences To smooth a dataset is to find an approximation of it that retains the important patterns of the original data while hiding the noise or other complicating factors." ></td>
	<td class="line x" title="37:216	Formally, we define the smoothing task as follows: let D = {(x,z)|x is a word sequence, z is a label sequence} be a labeled dataset of word sequences, and let M be a machine learning algorithm that will learn a function f to predict the correct labels." ></td>
	<td class="line x" title="38:216	The smoothing task is to find a function g such that when M is applied to Dprime = {(g(x),z)|(x,z)  D}, it produces a function fprime that is more accurate than f. For supervised sequence-labeling problems in NLP, the most important complicating factor that we seek to avoid through smoothing is the data sparsity associated with word-based representations." ></td>
	<td class="line x" title="39:216	Thus, the task is to find g such that for every word x, g(x) is much less sparse, but still retains the essential features of x that are useful for predicting its label." ></td>
	<td class="line x" title="40:216	As an example, consider the string Researchers test reformulated gasolines on newer engines. In a common dataset for NP chunking, the word reformulated never appears in the training data, but appears four times in the test set as part of the NP reformulated gasolines. Thus, a learning algorithm supplied with word-level features would have a difficult time determining that reformulated is the start of a NP." ></td>
	<td class="line x" title="41:216	Character-level features are of little help as well, since the -ed suffix is more commonly associated with verb phrases." ></td>
	<td class="line x" title="42:216	Finally, context may be of some help, but test is ambiguous between a noun and verb, and gasolines is only seen once in the training data, so there is no guarantee that context is sufficient to make a correct judgment." ></td>
	<td class="line x" title="43:216	On the other hand, some of the other contexts in which reformulated appears in the test set, such as testing of reformulated gasolines, provide strong evidence that it can start a NP, since of is a highly reliable indicator that a NP is to follow." ></td>
	<td class="line x" title="44:216	This example provides the intuition for our approach to smoothing: we seek to share information about the contexts of a word across multiple instances of the word, in order to provide more information about words that are rarely or never seen in training." ></td>
	<td class="line x" title="45:216	In particular, we seek to represent each word by a distribution over its contexts, and then provide the learning algorithm with features computed from this distribution." ></td>
	<td class="line x" title="46:216	Importantly, we seek distributional representations that will provide features that are common in both training and test data, to avoid data sparsity." ></td>
	<td class="line x" title="47:216	In the next three sections, we develop three techniques for smoothing text using distributional representations." ></td>
	<td class="line x" title="48:216	2.1 Multinomial Representation In its simplest form, the context of a word may be represented as a multinomial distribution over the terms that appear on either side of the word." ></td>
	<td class="line x" title="49:216	If V is the vocabulary, or the set of word types, and X is a sequence of random variables over V, the left and right context of Xi = v may each be represented as a probability distribution over V: P(Xi1|Xi = v) and P(Xi+1|X = v) respectively." ></td>
	<td class="line x" title="50:216	We learn these distributions from unlabeled texts in two different ways." ></td>
	<td class="line x" title="51:216	The first method computes word count vectors for the left and right contexts of each word type in the vocabulary of the training and test texts." ></td>
	<td class="line x" title="52:216	We also use a large collection of additional text to determine the vectors." ></td>
	<td class="line x" title="53:216	We then normalize each vector to form a probability distribution." ></td>
	<td class="line x" title="54:216	The second technique first applies TF-IDF weighting to each vector, where the context words of each word type constitute a document, before applying normalization." ></td>
	<td class="line x" title="55:216	This gives greater weight to words with more idiosyncratic distributions and may improve the informativeness of a distributional representation." ></td>
	<td class="line x" title="56:216	We refer to these techniques as TF and TF-IDF." ></td>
	<td class="line x" title="57:216	496 To supply a sequence-labeling algorithm with information from these distributional representations, we compute real-valued features of the context distributions." ></td>
	<td class="line x" title="58:216	In particular, for every word xi in a sequence, we provide the sequence labeler with a set of features of the left and right contexts indexed by v  V: Fleftv (xi) = P(Xi1 = v|xi) and Frightv (xi) = P(Xi+1 = v|xi)." ></td>
	<td class="line x" title="59:216	For example, the left context for reformulated in our example above would contain a nonzero probability for the word of. Using the features F(xi), a sequence labeler can learn patterns such as, if xi has a high probability of following of, it is a good candidate for the start of a noun phrase." ></td>
	<td class="line x" title="60:216	These features provide smoothing by aggregating information across multiple unannotated examples of the same word." ></td>
	<td class="line x" title="61:216	2.2 LSA Model One drawback of the multinomial representation is that it does not handle sparsity well enough, because the multinomial distributions themselves are so high-dimensional." ></td>
	<td class="line x" title="62:216	For example, the two phrases red lamp and magenta tablecloth share no words in common." ></td>
	<td class="line x" title="63:216	If magenta is never observed in training, the fact that tablecloth appears in its right context is of no help in connecting it with the phrase red lamp. But if we can group similar context words together, putting lamp and tablecloth into a category for household items, say, then these two adjectives will share that category in their context distributions." ></td>
	<td class="line x" title="64:216	Any patterns learned for the more common red lamp will then also apply to the less common magenta tablecloth. Our second distributional representation aggregates information from multiple context words by grouping together the distributions P(xi1 = v|xi = w) and P(xi1 = vprime|xi = w) if v and vprime appear together with many of the same words w. Aggregating counts in this way smooths our representations even further, by supplying better estimates when the data is too sparse to estimate P(xi1|xi) accurately." ></td>
	<td class="line x" title="65:216	Latent Semantic Analysis (LSA) (Deerwester et al., 1990) is a widely-used technique for computing dimensionality-reduced representations from a bag-of-words model." ></td>
	<td class="line x" title="66:216	We apply LSA to the set of right context vectors and the set of left context vectors separately, to find compact versions of each vector, where each dimension represents a combination of several context word types." ></td>
	<td class="line x" title="67:216	We normalize each vector, and then calculate features as above." ></td>
	<td class="line x" title="68:216	After experimenting with different choices for the number of dimensions to reduce our vectors to, we choose a value of 10 dimensions as the one that maximizes the performance of our supervised sequence labelers on held-out data." ></td>
	<td class="line x" title="69:216	2.3 Latent Variable Language Model Representation To take smoothing one step further, we present a technique that aggregates context distributions both for similar context words xi1 = v and vprime, and for similar words xi = w and wprime." ></td>
	<td class="line x" title="70:216	Latent variable language models (LVLMs) can be used to produce just such a distributional representation." ></td>
	<td class="line x" title="71:216	We use Hidden Markov Models (HMMs) as the main example in the discussion and as the LVLMs in our experiments, but the smoothing technique can be generalized to other forms of LVLMs, such as factorial HMMs and latent variable maximum entropy models (Ghahramani and Jordan, 1997; Smith and Eisner, 2005)." ></td>
	<td class="line x" title="72:216	An HMM is a generative probabilistic model that generates each word xi in the corpus conditioned on a latent variable Yi." ></td>
	<td class="line x" title="73:216	Each Yi in the model takes on integral values from 1 to S, and each one is generated by the latent variable for the preceding word, Yi1." ></td>
	<td class="line x" title="74:216	The distribution for a corpus x = (x1,,xN) given a set of state vectors y = (y1,,yN) is given by: P(x|y) = productdisplay i P(xi|yi)P(yi|yi1) Using Expectation-Maximization (Dempster et al., 1977), it is possible to estimate the distributions for P(xi|yi) and P(yi|yi1) from unlabeled data." ></td>
	<td class="line x" title="75:216	We use a trained HMM to determine the optimal sequence of latent states yi using the wellknown Viterbi algorithm (Rabiner, 1989)." ></td>
	<td class="line x" title="76:216	The output of this process is an integer (ranging from 1 to S) for every word xi in the corpus; we include a new boolean feature for each possible value of yi in our sequence labelers." ></td>
	<td class="line x" title="77:216	To compare our models, note that in the multinomial representation we directly model the probability that a word v appears before a word w: P(xi1 = v|xi = w))." ></td>
	<td class="line x" title="78:216	In our LSA model, we find latent categories of context words z, and model the probability that a category appears before the current word w: P(xi1 = z|xi = w)." ></td>
	<td class="line x" title="79:216	The HMM finds (probabilistic) categories Y for both the current word xi and the context word xi1, and models the probability that one category follows the 497 other: P(Yi|Yi1)." ></td>
	<td class="line x" title="80:216	Thus the HMM is our most extreme smoothing model, as it aggregates information over the greatest number of examples: for a given consecutive pair of words xi1,xi in the test set, it aggregates over all pairs of consecutive words xprimei1,xprimei where xprimei1 is similar to xi1 and xprimei is similar to xi." ></td>
	<td class="line x" title="81:216	3 Experiments We tested the following hypotheses in our experiments: 1." ></td>
	<td class="line x" title="82:216	Smoothing can improve the performance of a supervised sequence labeling system on words that are rare or nonexistent in the training data." ></td>
	<td class="line x" title="83:216	2." ></td>
	<td class="line x" title="84:216	A supervised sequence labeler achieves greater accuracy on new domains with smoothing." ></td>
	<td class="line x" title="85:216	3." ></td>
	<td class="line x" title="86:216	A supervised sequence labeler has a better sample complexity with smoothing." ></td>
	<td class="line x" title="87:216	3.1 Experimental Setup We investigate the use of smoothing in two test systems, conditional random field (CRF) models for POS tagging and chunking." ></td>
	<td class="line x" title="88:216	To incorporate smoothing into our models, we follow the following general procedure: first, we collect a set of unannotated text from the same domain as the test data set." ></td>
	<td class="line x" title="89:216	Second, we train a smoothing model on the text of the training data, the test data, and the additional collection." ></td>
	<td class="line x" title="90:216	We then automatically annotate both the training and test data with features calculated from the distributional representation." ></td>
	<td class="line x" title="91:216	Finally, we train the CRF model on the annotated training set and apply it to the test set." ></td>
	<td class="line x" title="92:216	We use an open source CRF software package designed by Sunita Sajarwal and William W. Cohen to implement our CRF models.1 We use a set of boolean features listed in Table 1." ></td>
	<td class="line x" title="93:216	Our baseline CRF system for POS tagging follows the model described by Lafferty et al.(2001)." ></td>
	<td class="line x" title="94:216	We include transition features between pairs of consecutive tag variables, features between tag variables and words, and a set of orthographic features that Lafferty et al. found helpful for performance on OOV words." ></td>
	<td class="line x" title="95:216	Our smoothed models add features computed from the distributional representations, as discussed above." ></td>
	<td class="line x" title="96:216	Our chunker follows the system described by Sha and Pereira (2003)." ></td>
	<td class="line x" title="97:216	In addition to the transition, word-level, and orthographic features, we include features relating automatically-generated POS tags and the chunk labels." ></td>
	<td class="line x" title="98:216	Unlike Sha and 1Available from http://sourceforge.net/projects/crf/ CRF Feature Set Transition zi=z zi=z and zi1=zprime Word xi=w and zi=z POS ti=t and zi=z Orthography for every s  {-ing, -ogy, ed, -s, -ly, -ion, -tion, -ity}, suffix(xi)= s and zi=z xi is capitalized and zi = z xi has a digit and zi = z TF, TF-IDF, and LSA features for every context type v, Fleftv (xi) and Frightv (xi) HMM features yi=y and zi = z Table 1: Features used in our CRF systems." ></td>
	<td class="line x" title="99:216	zi variables represent labels to be predicted, ti represent tags (for the chunker), and xi represent word tokens." ></td>
	<td class="line x" title="100:216	All features are boolean except for the TF, TF-IDF, and LSA features." ></td>
	<td class="line x" title="101:216	Pereira, we exclude features relating consecutive pairs of words and a chunk label, or features relating consecutive tag labels and a chunk label, in order to expedite our experiments." ></td>
	<td class="line x" title="102:216	We found that including such features does improve chunking F1 by approximately 2%, but it also significantly slows down CRF training." ></td>
	<td class="line x" title="103:216	3.2 Rare Word Accuracy For these experiments, we use the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993)." ></td>
	<td class="line x" title="104:216	Following the CoNLL shared task from 2000, we use sections 15-18 of the Penn Treebank for our labeled training data for the supervised sequence labeler in all experiments (Tjong et al., 2000)." ></td>
	<td class="line x" title="105:216	For the tagging experiments, we train and test using the gold standard POS tags contained in the Penn Treebank." ></td>
	<td class="line x" title="106:216	For the chunking experiments, we train and test with POS tags that are automatically generated by a standard tagger (Brill, 1994)." ></td>
	<td class="line x" title="107:216	We tested the accuracy of our models for chunking and POS tagging on section 20 of the Penn Treebank, which corresponds to the test set from the CoNLL 2000 task." ></td>
	<td class="line x" title="108:216	Our distributional representations are trained on sections 2-22 of the Penn Treebank." ></td>
	<td class="line x" title="109:216	Because we include the text from the train and test sets in our training data for the distributional representations, we do not need to worry about smoothing them  when they are decoded on the test set, they 498 Freq: 0 1 2 0-2 all #Samples 438 508 588 1534 46661 Baseline .62 .77 .81 .74 .93 TF .76 .72 .77 .75 .92 TF-IDF .82 .75 .76 .78 .94 LSA .78 .80 .77 .78 .94 HMM .73 .81 .86 .80 .94 Table 2: POS tagging accuracy: our HMM-smoothed tagger outperforms the baseline tagger by 6% on rare words." ></td>
	<td class="line x" title="110:216	Differences between the baseline and the HMM are statistically significant at p < 0.01 for the OOV, 0-2, and all cases using the two-tailed Chi-squared test with 1 degree of freedom." ></td>
	<td class="line x" title="111:216	will not encounter any previously unseen words." ></td>
	<td class="line x" title="112:216	However, to speed up training during our experiments and, in some cases, to avoid running out of memory, we replaced words appearing twice or fewer times in the data with the special symbol *UNKNOWN*." ></td>
	<td class="line x" title="113:216	In addition, all numbers were replaced with another special symbol." ></td>
	<td class="line x" title="114:216	For the LSA model, we had to use a more drastic cutoff to fit the singular value decomposition computation into memory: we replaced words appearing 10 times or fewer with the *UNKNOWN* symbol." ></td>
	<td class="line x" title="115:216	We initialize our HMMs randomly." ></td>
	<td class="line x" title="116:216	We run EM ten times and take the model with the best cross-entropy on a held-out set." ></td>
	<td class="line x" title="117:216	After experimenting with different variations of HMM models, we settled on a model with 80 latent states as a good compromise between accuracy and efficiency." ></td>
	<td class="line x" title="118:216	For our POS tagging experiments, we measured the accuracy of the tagger on rare words, or words that appear at most twice in the training data." ></td>
	<td class="line x" title="119:216	For our chunking experiments, we focus on chunks that begin with rare words, as we found that those were the most difficult for the chunker to identify correctly." ></td>
	<td class="line x" title="120:216	So we define rare chunks as those that begin with words appearing at most twice in training data." ></td>
	<td class="line x" title="121:216	To ensure that our smoothing models have enough training data for our test set, we further narrow our focus to those words that appear rarely in the labeled training data, but appear at least ten times in sections 2-22." ></td>
	<td class="line x" title="122:216	Tables 2 and 3 show the accuracy of our smoothed models and the baseline model on tagging and chunking, respectively." ></td>
	<td class="line x" title="123:216	The line for all in both tables indicates results on the complete test set." ></td>
	<td class="line x" title="124:216	Both our baseline tagger and chunker achieve respectable results on their respective tasks for all words, and the results were good enough for Freq: 0 1 2 0-2 all #Samples 133 199 231 563 21900 Baseline .69 .75 .81 .76 .90 TF .70 .82 .79 .77 .89 TF-IDF .77 .77 .80 .78 .90 LSA .84 .82 .83 .84 .90 HMM .90 .85 .85 .86 .93 Table 3: Chunking F1: our HMM-smoothed chunker outperforms the baseline CRF chunker by 0.21 on chunks that begin with OOV words, and 0.10 on chunks that begin with rare words." ></td>
	<td class="line x" title="125:216	us to be satisfied that performance on rare words closely follows how a state-of-the-art supervised sequence-labeler behaves." ></td>
	<td class="line x" title="126:216	The chunkers accuracy is roughly in the middle of the range of results for the original CoNLL 2000 shared task (Tjong et al., 2000) . While several systems have achieved slightly higher accuracy on supervised POS tagging, they are usually trained on larger training sets." ></td>
	<td class="line x" title="127:216	As expected, the drop-off in the baseline systems performance from all words to rare words is impressive for both tasks." ></td>
	<td class="line x" title="128:216	Comparing performance on all terms and OOV terms, the baseline taggers accuracy drops by 0.31, and the baseline chunkers F1 drops by 0.21." ></td>
	<td class="line x" title="129:216	Comparing performance on all terms and rare terms, the drop is less severe but still dramatic: 0.19 for tagging and 0.15 for chunking." ></td>
	<td class="line x" title="130:216	Our hypothesis that smoothing would improve performance on rare terms is validated by these experiments." ></td>
	<td class="line x" title="131:216	In fact, the more aggregation a smoothing model performs, the better it appears to be at smoothing." ></td>
	<td class="line x" title="132:216	The HMM-smoothed system outperforms all other systems in all categories except tagging on OOV words, where TF-IDF performs best." ></td>
	<td class="line x" title="133:216	And in most cases, the clear trend is for HMM smoothing to outperform LSA, which in turn outperforms TF and TF-IDF." ></td>
	<td class="line x" title="134:216	HMM tagging performance on OOV terms improves by 11%, and chunking performance by 21%." ></td>
	<td class="line x" title="135:216	Tagging performance on all of the rare terms improves by 6%, and chunking by 10%." ></td>
	<td class="line x" title="136:216	In chunking, there is a clear trend toward larger increases in performance as words become rarer in the labeled data set, from a 0.02 improvement on words of frequency 2, to an improvement of 0.21 on OOV words." ></td>
	<td class="line x" title="137:216	Because the test data for this experiment is drawn from the same domain (newswire) as the 499 training data, the rare terms make up a relatively small portion of the overall dataset (approximately 4% of both the tagged words and the chunks)." ></td>
	<td class="line x" title="138:216	Still, the increased performance by the HMMsmoothed model on the rare-word subset contributes in part to an increase in performance on the overall dataset of 1% for tagging and 3% for chunking." ></td>
	<td class="line x" title="139:216	In our next experiment, we consider a common scenario where rare terms make up a much larger fraction of the test data." ></td>
	<td class="line x" title="140:216	3.3 Domain Adaptation For our experiment on domain adaptation, we focus on NP chunking and POS tagging, and we use the labeled training data from the CoNLL 2000 shared task as before." ></td>
	<td class="line x" title="141:216	For NP chunking, we use 198 sentences from the biochemistry domain in the Open American National Corpus (OANC) (Reppen et al., 2005) as or our test set." ></td>
	<td class="line x" title="142:216	We manually tagged the test set with POS tags and NP chunk boundaries." ></td>
	<td class="line x" title="143:216	The test set contains 5330 words and a total of 1258 NP chunks." ></td>
	<td class="line x" title="144:216	We used sections 15-18 of the Penn Treebank as our labeled training set, including the gold standard POS tags." ></td>
	<td class="line x" title="145:216	We use our best-performing smoothing model, the HMM, and train it on sections 13 through 19 of the Penn Treebank, plus the written portion of the OANC that contains journal articles from biochemistry (40,727 sentences)." ></td>
	<td class="line x" title="146:216	We focus on chunks that begin with words appearing 0-2 times in the labeled training data, and appearing at least ten times in the HMMs training data." ></td>
	<td class="line x" title="147:216	Table 4 contains our results." ></td>
	<td class="line oc" title="148:216	For our POS tagging experiments, we use 561 MEDLINE sentences (9576 words) from the Penn BioIE project (PennBioIE, 2005), a test set previously used by Blitzer et al.(2006)." ></td>
	<td class="line o" title="149:216	We use the same experimental setup as Blitzer et al.: 40,000 manually tagged sentences from the Penn Treebank for our labeled training data, and all of the unlabeled text from the Penn Treebank plus their MEDLINE corpus of 71,306 sentences to train our HMM." ></td>
	<td class="line x" title="150:216	We report on tagging accuracy for all words and OOV words in Table 5." ></td>
	<td class="line oc" title="151:216	This table also includes results for two previous systems as reported by Blitzer et al.(2006): the semi-supervised Alternating Structural Optimization (ASO) technique and the Structural Correspondence Learning (SCL) technique for domain adaptation." ></td>
	<td class="line x" title="153:216	Note that this test set for NP chunking contains a much higher proportion of rare and OOV words: 23% of chunks begin with an OOV word, and 29% begin with a rare word, as compared with Baseline HMM Freq." ></td>
	<td class="line x" title="154:216	# R P F1 R P F1 0 284 .74 .70 .72 .80 .89 .84 1 39 .85 .87 .86 .92 .88 .90 2 39 .79 .86 .83 .92 .90 .91 0-2 362 .75 .73 .74 .82 .89 .85 all 1258 .86 .87 .86 .91 .90 .91 Table 4: On biochemistry journal data from the OANC, our HMM-smoothed NP chunker outperforms the baseline CRF chunker by 0.12 (F1) on chunks that begin with OOV words, and by 0.05 (F1) on all chunks." ></td>
	<td class="line x" title="155:216	Results in bold are statistically significantly different from the baseline results at p < 0.05 using the two-tailed Fishers exact test." ></td>
	<td class="line x" title="156:216	We did not perform significance tests for F1." ></td>
	<td class="line n" title="157:216	All Unknown Model words words Baseline 88.3 67.3 ASO 88.4 70.9 SCL 88.9 72.0 HMM 90.5 75.2 Table 5: On biomedical data from the Penn BioIE project, our HMM-smoothed tagger outperforms the SCL tagger by 3% (accuracy) on OOV words, and by 1.6% (accuracy) on all words." ></td>
	<td class="line o" title="158:216	Differences between the smoothed tagger and the SCL tagger are significant at p < .001 for all words and for OOV words, using the Chi-squared test with 1 degree of freedom." ></td>
	<td class="line x" title="159:216	1% and 4%, respectively, for NP chunks in the test set from the original domain." ></td>
	<td class="line x" title="160:216	The test set for tagging also contains a much higher proportion: 23% OOV words, as compared with 1% in the original domain." ></td>
	<td class="line x" title="161:216	Because of the increase in the number of rare words, the baseline chunkers overall performance drops by 4% compared with performance on WSJ data, and the baseline taggers overall performance drops by 5% in the new domain." ></td>
	<td class="line n" title="162:216	The performance improvements for both the smoothed NP chunker and tagger are again impressive: there is a 12% improvement on OOV words, and a 10% overall improvement on rare words for chunking; the tagger shows an 8% improvement on OOV words compared to out baseline and a 3% improvement on OOV words compared to the SCL model." ></td>
	<td class="line x" title="163:216	The resulting performance of the smoothed NP chunker is almost identical to its performance on the WSJ data." ></td>
	<td class="line x" title="164:216	Through smoothing, the chunker not only improves by 5% 500 in F1 over the baseline system on all words, it in fact outperforms our baseline NP chunker on the WSJ data." ></td>
	<td class="line x" title="165:216	60% of this improvement comes from improved accuracy on rare words." ></td>
	<td class="line x" title="166:216	The performance of our HMM-smoothed chunker caused us to wonder how well the chunker could work without some of its other features." ></td>
	<td class="line x" title="167:216	We removed all tag features and all features for word types that appear fewer than 20 times in training." ></td>
	<td class="line x" title="168:216	This chunker achieves 0.91 F1 on OANC data, and 0.93 F1 on WSJ data, outperforming the baseline system in both cases." ></td>
	<td class="line x" title="169:216	It has only 20% as many features as the baseline chunker, greatly improving its training time." ></td>
	<td class="line x" title="170:216	Thus our smoothing features are more valuable to the chunker than features from POS tags and features for all but the most common words." ></td>
	<td class="line x" title="171:216	Our results point to the exciting possibility that with smoothing, we may be able to train a sequence-labeling system on a small labeled sample, and have it apply generally to other domains." ></td>
	<td class="line x" title="172:216	Exactly what size training set we need is a question that we address next." ></td>
	<td class="line x" title="173:216	3.4 Sample Complexity Our complete system consists of two learned components, a supervised CRF system and an unsupervised smoothing model." ></td>
	<td class="line x" title="174:216	We measure the sample complexity of each component separately." ></td>
	<td class="line x" title="175:216	To measure the sample complexity of the supervised CRF, we use the same experimental setup as in the chunking experiment on WSJ text, but we vary the amount of labeled data available to the CRF." ></td>
	<td class="line x" title="176:216	We take ten random samples of a fixed size from the labeled training set, train a chunking model on each subset, and graph the F1 on the labeled test set, averaged over the ten runs, in Figure 1." ></td>
	<td class="line x" title="177:216	To measure the sample complexity of our HMM with respect to unlabeled text, we use the full labeled training set and vary the amount of unlabeled text available to the HMM." ></td>
	<td class="line x" title="178:216	At minimum, we use the text available in the labeled training and test sets, and then add random subsets of the Penn Treebank, sections 2-22." ></td>
	<td class="line x" title="179:216	For each subset size, we take ten random samples of the unlabeled text, train an HMM and then a chunking model, and graph the F1 on the labeled test set averaged over the ten runs in Figure 2." ></td>
	<td class="line x" title="180:216	The results from our labeled sample complexity experiment indicate that sample complexity is drastically reduced by HMM smoothing." ></td>
	<td class="line x" title="181:216	On rare chunks, the smoothed system reaches 0.78 F1 using only 87 labeled training sentences, a level that the baseline system never reaches, even with 6933 baseline (all) HMM (all) HMM (rare) 0.6 0.7 0.8 0.9 1 F1  (C hu nk ing ) Labeled Sample Complexity baseline (rare) 0.2 0.3 0.4 0.5 1 10 100 1000 10000 F1  (C hu nk ing ) Number of Labeled Sentences (log scale) Figure 1: The smoothed NP chunker requires less than 10% of the samples needed by the baseline chunker to achieve .83 F1, and the same for .88 F1." ></td>
	<td class="line x" title="182:216	Baseline (all) HMM (all) HMM (rare) 0.80 0.85 0.90 0.95 F1  (C hu nk ing ) Unlabeled Sample Complexity Baseline (rare)0.70 0.75 0 10000 20000 30000 40000 F1  (C hu nk ing ) Number of Unannotated Sentences Figure 2: By leveraging plentiful unannotated text, the smoothed chunker soon outperforms the baseline." ></td>
	<td class="line x" title="183:216	labeled sentences." ></td>
	<td class="line x" title="184:216	On the overall data set, the smoothed system reaches 0.83 F1 with 50 labeled sentences, which the baseline does not reach until it has 867 labeled sentences." ></td>
	<td class="line x" title="185:216	With 434 labeled sentences, the smoothed system reaches 0.88 F1, which the baseline system does not reach until it has 5200 labeled samples." ></td>
	<td class="line x" title="186:216	Our unlabeled sample complexity results show that even with access to a small amount of unlabeled text, 6000 sentences more than what appears in the training and test sets, smoothing using the HMM yields 0.78 F1 on rare chunks." ></td>
	<td class="line x" title="187:216	However, the smoothed system requires 25,000 more sentences before it outperforms the baseline system on all chunks." ></td>
	<td class="line x" title="188:216	No peak in performance is reached, so further improvements are possible with more unlabeled data." ></td>
	<td class="line x" title="189:216	Thus smoothing is optimizing performance for the case where unlabeled data is plentiful and labeled data is scarce, as we would hope." ></td>
	<td class="line x" title="190:216	4 Related Work To our knowledge, only one previous system  the REALM system for sparse information extrac501 tion  has used HMMs as a feature representation for other applications." ></td>
	<td class="line x" title="191:216	REALM uses an HMM trained on a large corpus to help determine whether the arguments of a candidate relation are of the appropriate type (Downey et al., 2007)." ></td>
	<td class="line x" title="192:216	We extend and generalize this smoothing technique and apply it to common NLP applications involving supervised sequence-labeling, and we provide an in-depth empirical analysis of its performance." ></td>
	<td class="line x" title="193:216	Several researchers have previously studied methods for using unlabeled data for tagging and chunking, either alone or as a supplement to labeled data." ></td>
	<td class="line x" title="194:216	Ando and Zhang develop a semisupervised chunker that outperforms purely supervised approaches on the CoNLL 2000 dataset (Ando and Zhang, 2005)." ></td>
	<td class="line x" title="195:216	Recent projects in semisupervised (Toutanova and Johnson, 2007) and unsupervised (Biemann et al., 2007; Smith and Eisner, 2005) tagging also show significant progress." ></td>
	<td class="line x" title="196:216	Unlike these systems, our efforts are aimed at using unlabeled data to find distributional representations that work well on rare terms, making the supervised systems more applicable to other domains and decreasing their sample complexity." ></td>
	<td class="line x" title="197:216	HMMs have been used many times for POS tagging and chunking, in supervised, semisupervised, and in unsupervised settings (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Johnson, 2007; Zhou, 2004)." ></td>
	<td class="line x" title="198:216	We take a novel perspective on the use of HMMs by using them to compute features of each token in the data that represent the distribution over that tokens contexts." ></td>
	<td class="line x" title="199:216	Our technique lets the HMM find parameters that maximize cross-entropy, and then uses labeled data to learn the best mapping from the HMM categories to the POS categories." ></td>
	<td class="line x" title="200:216	Smoothing in NLP usually refers to the problem of smoothing n-gram models." ></td>
	<td class="line x" title="201:216	Sophisticated smoothing techniques like modified Kneser-Ney and Katz smoothing (Chen and Goodman, 1996) smooth together the predictions of unigram, bigram, trigram, and potentially higher n-gram sequences to obtain accurate probability estimates in the face of data sparsity." ></td>
	<td class="line x" title="202:216	Our task differs in that we are primarily concerned with the case where even the unigram model (single word) is rarely or never observed in the labeled training data." ></td>
	<td class="line x" title="203:216	Sparsity for low-order contexts has recently spurred interest in using latent variables to represent distributions over contexts in language models." ></td>
	<td class="line x" title="204:216	While n-gram models have traditionally dominated in language modeling, two recent efforts develop latent-variable probabilistic models that rival and even surpass n-gram models in accuracy (Blitzer et al., 2005; Mnih and Hinton, 2007)." ></td>
	<td class="line x" title="205:216	Several authors investigate neural network models that learn not just one latent state, but rather a vector of latent variables, to represent each word in a language model (Bengio et al., 2003; Emami et al., 2003; Morin and Bengio, 2005)." ></td>
	<td class="line x" title="206:216	One of the benefits of our smoothing technique is that it allows for domain adaptation, a topic that has received a great deal of attention from the NLP community recently." ></td>
	<td class="line x" title="207:216	Unlike our technique, in most cases researchers have focused on the scenario where labeled training data is available in both the source and the target domain (e.g., (Daume III, 2007; Chelba and Acero, 2004; Daume III and Marcu, 2006))." ></td>
	<td class="line x" title="208:216	Our technique uses unlabeled training data from the target domain, and is thus applicable more generally, including in web processing, where the domain and vocabulary is highly variable, and it is extremely difficult to obtain labeled data that is representative of the test distribution." ></td>
	<td class="line x" title="209:216	When labeled target-domain data is available, instance weighting and similar techniques can be used in combination with our smoothing technique to improve our results further, although this has not yet been demonstrated empirically." ></td>
	<td class="line nc" title="210:216	HMM-smoothing improves on the most closely related work, the Structural Correspondence Learning technique for domain adaptation (Blitzer et al., 2006), in experiments." ></td>
	<td class="line x" title="211:216	5 Conclusion and Future Work Our study of smoothing techniques demonstrates that by aggregating information across many unannotated examples, it is possible to find accurate distributional representations that can provide highly informative features to supervised sequence labelers." ></td>
	<td class="line x" title="212:216	These features help improve sequence labeling performance on rare word types, on domains that differ from the training set, and on smaller training sets." ></td>
	<td class="line x" title="213:216	Further experiments are of course necessary to investigate distributional representations as smoothing techniques." ></td>
	<td class="line x" title="214:216	One particularly promising area for further study is the combination of smoothing and instance weighting techniques for domain adaptation." ></td>
	<td class="line x" title="215:216	Whether the current techniques are applicable to structured prediction tasks, like parsing and relation extraction, also deserves future attention." ></td>
	<td class="line x" title="216:216	502" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1114
Multi-Task Transfer Learning for Weakly-Supervised Relation Extraction
Jiang, Jing;"></td>
	<td class="line x" title="1:210	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 10121020, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:210	c2009 ACL and AFNLP Multi-Task Transfer Learning for Weakly-Supervised Relation Extraction Jing Jiang School of Information Systems Singapore Management University 80 Stamford Road, Singapore 178902 jingjiang@smu.edu.sg Abstract Creating labeled training data for relation extraction is expensive." ></td>
	<td class="line x" title="3:210	In this paper, we study relation extraction in a special weakly-supervised setting when we have only a few seed instances of the target relation type we want to extract but we also have a large amount of labeled instances of other relation types." ></td>
	<td class="line x" title="4:210	Observing that different relation types can share certain common structures, we propose to use a multi-task learning method coupled with human guidance to address this weakly-supervised relation extraction problem." ></td>
	<td class="line x" title="5:210	The proposed framework models the commonality among different relation types through a shared weight vector, enables knowledge learned from the auxiliary relation types to be transferred to the target relation type, and allows easy control of the tradeoff between precision and recall." ></td>
	<td class="line x" title="6:210	Empirical evaluation on the ACE 2004 data set shows that the proposed method substantially improves over two baseline methods." ></td>
	<td class="line x" title="7:210	1 Introduction Relation extraction is the task of detecting and characterizing semantic relations between entities from free text." ></td>
	<td class="line x" title="8:210	Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008)." ></td>
	<td class="line x" title="9:210	However, supervised learning heavily relies on a sufficient amount of labeled data for training, which is not always available in practice due to the labor-intensive nature of human annotation." ></td>
	<td class="line x" title="10:210	This problem is especially serious for relation extraction because the types of relations to be extracted are highly dependent on the application domain." ></td>
	<td class="line x" title="11:210	For example, when working in the financial domain we may be interested in the employment relation, but when moving to the terrorism domain we now may be interested in the ethnic and ideology affiliation relation, and thus have to create training data for the new relation type." ></td>
	<td class="line x" title="12:210	However, is the old training data really useless?" ></td>
	<td class="line x" title="13:210	Inspired by recent work on transfer learning and domain adaptation, in this paper, we study how we can leverage labeled data of some old relation types to help the extraction of a new relation type in a weakly-supervised setting, where only a few seed instances of the new relation type are available." ></td>
	<td class="line oc" title="14:210	While transfer learning was proposed more than a decade ago (Thrun, 1996; Caruana, 1997), its application in natural language processing is still a relatively new territory (Blitzer et al., 2006; Daume III, 2007; Jiang and Zhai, 2007a; Arnold et al., 2008; Dredze and Crammer, 2008), and its application in relation extraction is still unexplored." ></td>
	<td class="line x" title="15:210	Our idea of performing transfer learning is motivated by the observation that different relation types share certain common syntactic structures, which can possibly be transferred from the old types to the new type." ></td>
	<td class="line x" title="16:210	We therefore propose to use a general multi-task learning framework in which classification models for a number of related tasks are forced to share a common model component and trained together." ></td>
	<td class="line x" title="17:210	By treating classification of different relation types as related tasks, the learning framework can naturally model the common syntactic structures among different relation types in a principled manner." ></td>
	<td class="line x" title="18:210	It also allows us to introduce human guidance in separating the common model component from the type-specific components." ></td>
	<td class="line x" title="19:210	The framework naturally transfers the knowledge learned from the old relation types to the new relation type and helps improve the recall of the relation extractor." ></td>
	<td class="line x" title="20:210	We also exploit ad1012 ditional human knowledge about the entity type constraints on the relation arguments, which can usually be derived from the definition of a relation type." ></td>
	<td class="line x" title="21:210	Imposing these constraints further improves the precision of the final relation extractor." ></td>
	<td class="line x" title="22:210	Empirical evaluation on the ACE 2004 data set shows that our proposed method largely outperforms two baseline methods, improving the average F1 measure from 0.1532 to 0.4132 when only 10 seed instances of the new relation type are used." ></td>
	<td class="line x" title="23:210	2 Related work Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods." ></td>
	<td class="line x" title="24:210	Zhou et al.(2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction." ></td>
	<td class="line x" title="26:210	We systematically explored the feature space for relation extraction (Jiang and Zhai, 2007b) . Kernel methods allow a large set of features to be used without being explicitly extracted." ></td>
	<td class="line x" title="27:210	A number of relation extraction kernels have been proposed, including dependency tree kernels (Culotta and Sorensen, 2004), shortest dependency path kernels (Bunescu and Mooney, 2005) and more recently convolution tree kernels (Zhang et al., 2006; Qian et al., 2008)." ></td>
	<td class="line x" title="28:210	However, in both feature-based and kernel-based studies, availability of sufficient labeled training data is always assumed." ></td>
	<td class="line x" title="29:210	Chen et al.(2006) explored semi-supervised learning for relation extraction using label propagation, which makes use of unlabeled data." ></td>
	<td class="line x" title="31:210	Zhou et al.(2008) proposed a hierarchical learning strategy to address the data sparseness problem in relation extraction." ></td>
	<td class="line x" title="33:210	They also considered the commonality among different relation types, but compared with our work, they had a different problem setting and a different way of modeling the commonality." ></td>
	<td class="line x" title="34:210	Banko and Etzioni (2008) studied open domain relation extraction, for which they manually identified several common relation patterns." ></td>
	<td class="line x" title="35:210	In contrast, our method obtains common patterns through statistical learning." ></td>
	<td class="line x" title="36:210	Xu et al.(2008) studied the problem of adapting a rule-based relation extraction system to new domains, but the types of relations to be extracted remain the same." ></td>
	<td class="line x" title="38:210	Transfer learning aims at transferring knowledge learned from one or a number of old tasks to a new task." ></td>
	<td class="line x" title="39:210	Domain adaptation is a special case of transfer learning where the learning task remains the same but the distribution of data changes." ></td>
	<td class="line x" title="40:210	There has been an increasing amount of work on transfer learning and domain adaptation in natural language processing recently." ></td>
	<td class="line oc" title="41:210	Blitzer et al.(2006) proposed a structural correspondence learning method for domain adaptation and applied it to part-of-speech tagging." ></td>
	<td class="line x" title="43:210	Daume III (2007) proposed a simple feature augmentation method to achieve domain adaptation." ></td>
	<td class="line x" title="44:210	Arnold et al.(2008) used a hierarchical prior structure to help transfer learning and domain adaptation for named entity recognition." ></td>
	<td class="line x" title="46:210	Dredze and Crammer (2008) proposed an online method for multi-domain learning and adaptation." ></td>
	<td class="line x" title="47:210	Multi-task learning is another learning paradigm in which multiple related tasks are learned simultaneously in order to achieve better performance for each individual task (Caruana, 1997; Evgeniou and Pontil, 2004)." ></td>
	<td class="line x" title="48:210	Although it was not originally proposed to transfer knowledge to a particular new task, it can be naturally used to achieve this goal because it models the commonality among tasks, which is the knowledge that should be transferred to a new task." ></td>
	<td class="line x" title="49:210	In our work, transfer learning is done through a multi-task learning framework similar to Evgeniou and Pontil (2004)." ></td>
	<td class="line x" title="50:210	3 Task definition Our study is conducted using data from the Automatic Content Extraction (ACE) program1." ></td>
	<td class="line x" title="51:210	We focus on extracting binary relation instances between two relation arguments occurring in the same sentence." ></td>
	<td class="line x" title="52:210	Some example relation instances and their corresponding relation types as defined by ACE can be found in Table 1." ></td>
	<td class="line x" title="53:210	We consider the following weakly-supervised problem setting." ></td>
	<td class="line x" title="54:210	We are interested in extracting instances of a target relation type T , but this relation type is only specified by a small set of seed instances." ></td>
	<td class="line x" title="55:210	We may possibly have some additional knowledge about the target type not in the form of labeled instances." ></td>
	<td class="line x" title="56:210	For example, we may be given the entity type restrictions on the two relation arguments." ></td>
	<td class="line x" title="57:210	In addition to such limited information about the target relation type, we also have a large amount of labeled instances for K auxiliary relation types A1,,AK." ></td>
	<td class="line x" title="58:210	Our goal is to learn a relation extractor for T , leveraging all the data and information we have." ></td>
	<td class="line x" title="59:210	1http://projects.ldc.upenn.edu/ace/ 1013 Syntactic Pattern Relation Instance Relation Type (Subtype) arg-2 arg-1 Arab leaders OTHER-AFF (Ethnic) his father PER-SOC (Family) South Jakarta Prosecution Office GPE-AFF (Based-In) arg-1 of arg-2 leader of a minority government EMP-ORG (Employ-Executive) the youngest son of ex-director Suharto PER-SOC (Family) the Socialist Peoples Party of Montenegro GPE-AFF (Based-In) arg-1 [verb] arg-2 Yemen [sent] planes to Baghdad ART (User-or-Owner) his wife [had] three young children PER-SOC (Family) Jody Scheckter [paced] Ferrari to both victories EMP-ORG (Employ-Staff) Table 1: Examples of similar syntactic structures across different relation types." ></td>
	<td class="line x" title="60:210	The head words of the first and the second arguments are shown in italic and bold, respectively." ></td>
	<td class="line x" title="61:210	Before introducing our transfer learning solution, let us first briefly explain our basic classification approach and the features we use, as well as two baseline solutions." ></td>
	<td class="line x" title="62:210	3.1 Feature configuration We treat relation extraction as a classification problem." ></td>
	<td class="line x" title="63:210	Each pair of entities within a single sentence is considered a candidate relation instance, and the task becomes predicting whether or not each candidate is a true instance of T . We use feature-based logistic regression classifiers." ></td>
	<td class="line x" title="64:210	Following our previous work (Jiang and Zhai, 2007b), we extract features from a sequence representation and a parse tree representation of each relation instance." ></td>
	<td class="line x" title="65:210	Each node in the sequence or the parse tree is augmented by an argument tag that indicates whether the node subsumes arg-1, arg2, both or neither." ></td>
	<td class="line x" title="66:210	Nodes that represent the arguments are also labeled with the entity type, subtype and mention type as defined by ACE." ></td>
	<td class="line x" title="67:210	Based on the findings of Qian et al.(2008), we trim the parse tree of a relation instance so that it contains only the most essential components." ></td>
	<td class="line x" title="69:210	We extract unigram features (consisting of a single node) and bigram features (consisting of two connected nodes) from the graphic representations." ></td>
	<td class="line x" title="70:210	An example of the graphic representation of a relation instance is shown in Figure 1 and some features extracted from this instance are shown in Table 2." ></td>
	<td class="line x" title="71:210	This feature configuration gives state-of-the-art performance (F1 = 0.7223) on the ACE 2004 data set in a standard setting with sufficient data for training." ></td>
	<td class="line x" title="72:210	3.2 Baseline solutions We consider two baseline solutions to the weaklysupervised relation extraction problem." ></td>
	<td class="line x" title="73:210	In the first NP NPB 3 PP1 leaderNN PER ofIN governmentNN ORG NPB 1 0 2 2 2Figure 1: The combined sequence and parse treerepresentation of the relation instance leader of a minority government. The non-essential nodes for a and for minority are removed based on the algorithm from Qian et al.(2008)." ></td>
	<td class="line x" title="75:210	Feature Explanation ORG2 arg-2 is an ORG entity." ></td>
	<td class="line x" title="76:210	of0 government2 arg-2 is government and follows the word of. NP3  PP2 There is a noun phrase containing both arguments, with arg-2 contained in a prepositional phrase inside the noun phrase." ></td>
	<td class="line x" title="77:210	Table 2: Examples of unigram and bigram features extracted from Figure 1." ></td>
	<td class="line x" title="78:210	baseline, we use only the few seed instances of the target relation type together with labeled negative relation instances (i.e. pairs of entities within the same sentence but having no relation) to train a binary classifier." ></td>
	<td class="line x" title="79:210	In the second baseline, we take the union of the positive instances of both the target relation type and the auxiliary relation types as our positive training set, and together with the negative instances we train a binary classifier." ></td>
	<td class="line x" title="80:210	Note that the second baseline method essentially learns 1014 a classifier for any relation type." ></td>
	<td class="line x" title="81:210	Another existing solution to weakly-supervised learning problems is semi-supervised learning, e.g. bootstrapping." ></td>
	<td class="line x" title="82:210	However, because our proposed transfer learning method can be combined with semi-supervised learning, here we do not include semi-supervised learning as a baseline." ></td>
	<td class="line x" title="83:210	4 A multi-task transfer learning solution We now present a multi-task transfer learning solution to the weakly-supervised relation extraction problem, which makes use of the labeled data from the auxiliary relation types." ></td>
	<td class="line x" title="84:210	4.1 Syntactic similarity between relation types To see why the auxiliary relation types may help the identification of the target relation type, let us first look at how different relation types may be related and even similar to each other." ></td>
	<td class="line x" title="85:210	Based on our inspection of a sample of the ACE data, we find that instances of different relation types can share certain common syntactic structures." ></td>
	<td class="line x" title="86:210	For example, the syntactic pattern arg-1 of arg-2 strongly indicates that there exists some relation between the two arguments, although the nature of the relation may be well dependent on the semantic meanings of the two arguments." ></td>
	<td class="line x" title="87:210	More examples are shown in Table 1." ></td>
	<td class="line x" title="88:210	This observation suggests that some of the syntactic patterns learned from the auxiliary relation types may be transferable to the target relation type, making it easier to learn the target relation type and thus alleviating the insufficient training data problem with the target type." ></td>
	<td class="line x" title="89:210	How can we incorporate this desired knowledge transfer process into our learning method?" ></td>
	<td class="line x" title="90:210	While one can make explicit use of these general syntactic patterns in a rule-based relation extraction system, here we restrict our attention to feature-based linear classifiers." ></td>
	<td class="line x" title="91:210	We note that in feature-based linear classifiers, a useful syntactic pattern is translated into large weights for features related to the syntactic pattern." ></td>
	<td class="line x" title="92:210	For example, if arg-1 of arg-2 is a useful pattern, in the learned linear classifier we should have relatively large weights for features such as the word of occurs before arg-2 or a preposition occurs before arg-2, or even more complex features such as there is a prepositional phrase containing arg-2 attached to arg-1. It is the weights of these generally useful features that are transferable from the auxiliary relation types to the target relation type." ></td>
	<td class="line x" title="93:210	4.2 Statistical learning model As we have discussed, we want to force the linear classifiers for different relation types to share their model weights for those features that are related to the common syntactic patterns." ></td>
	<td class="line x" title="94:210	Formally, we consider the following statistical learning model." ></td>
	<td class="line x" title="95:210	Let k denote the weight vector of the linear classifier that separates positive instances of auxiliary type Ak from negative instances, and let T denote a similar weight vector for the target type T . If different relation types are totally unrelated, these weight vectors should also be independent of each other." ></td>
	<td class="line x" title="96:210	But because we observe similar syntactic structures across different relation types, we now assume that these weight vectors are related through a common component : T = T +, k = k + for k = 1,,K. If we assume that only weights of certain general features can be shared between different relation types, we can force certain dimensions of  to be 0." ></td>
	<td class="line x" title="97:210	We express this constraint by introducing a matrix F and setting F = 0." ></td>
	<td class="line x" title="98:210	Here F is a square matrix with all entries set to 0 except that Fi,i = 1 if we want to force i = 0." ></td>
	<td class="line x" title="99:210	Now we can learn these weight vectors in a multi-task learning framework." ></td>
	<td class="line x" title="100:210	Let x represent the feature vector of a candidate relation instance, and y  {+1,1} represent a class label." ></td>
	<td class="line x" title="101:210	Let DT = {(xTi ,yTi )}NTi=1 denote the set of labeled instances for the target type T ." ></td>
	<td class="line x" title="102:210	(Note that the number of positive instances in DT is very small.)" ></td>
	<td class="line x" title="103:210	And let Dk = {(xki,yki )}Nki=1 denote the labeled instances for the auxiliary type Ak." ></td>
	<td class="line x" title="104:210	We learn the optimal weight vectors {k}Kk=1, T and  by optimizing the following objective function: parenleftbigg {k}Kk=1, T, parenrightbigg = argmin {k},T ,,F=0 bracketleftBigg L(DT,T +) + Ksummationdisplay k=1 L(Dk,k +) +TbardblTbardbl2 + Ksummationdisplay k=1 kbardblkbardbl2 +bardblbardbl2 bracketrightBigg ." ></td>
	<td class="line x" title="105:210	(1) 1015 The objective function follows standard empirical risk minimization with regularization." ></td>
	<td class="line x" title="106:210	Here L(D,) is the aggregated loss of labeling x with y for all (x,y) in D, using weight vector ." ></td>
	<td class="line x" title="107:210	In logistic regression models, the loss function is the negative log likelihood, that is, L(D,) =  summationdisplay (x,y)D logp(y|x,), p(y|x,) = exp(y x)summationtext yprime{+1,1}exp(yprime x) . T , k and  are regularization parameters." ></td>
	<td class="line x" title="108:210	By adjusting their values, we can control the degree of weight sharing among the relation types." ></td>
	<td class="line x" title="109:210	The larger the ratio T/ (or k/) is, the more we believe that the model for T (or Ak) should conform to the common model, and the smaller the type-specific weight vector T (or k) will be." ></td>
	<td class="line x" title="110:210	The model presented above is based on our previous work (Jiang and Zhai, 2007c), which bears the same spirit of some other recent work on multitask learning (Ando and Zhang, 2005; Evgeniou and Pontil, 2004; Daume III, 2007)." ></td>
	<td class="line x" title="111:210	It is general for any transfer learning problem with auxiliary labeled data from similar tasks." ></td>
	<td class="line x" title="112:210	Here we are mostly interested in the models applicability and effectiveness on the relation extraction problem." ></td>
	<td class="line x" title="113:210	4.3 Feature separation Recall that we impose a constraint F = 0 when optimizing the objective function." ></td>
	<td class="line x" title="114:210	This constraint gives us the freedom to force only the weights of a subset of the features to be shared among different relation types." ></td>
	<td class="line x" title="115:210	A remaining question is how to set this matrix F, that is, how to determine the set of general features to use." ></td>
	<td class="line x" title="116:210	We propose two ways of setting this matrix F. Automatically setting F One way is to fix the number of non-zero entries in  to be a pre-defined number H of general features, and allow F to change during the optimization process." ></td>
	<td class="line x" title="117:210	This can be done by repeating the following two steps until F converges: 1." ></td>
	<td class="line x" title="118:210	Fix F, and optimize the objective function as in Equation (1)." ></td>
	<td class="line x" title="119:210	2." ></td>
	<td class="line x" title="120:210	Fix parenleftbigT + parenrightbig and parenleftbigk + parenrightbig, and search for T , {k} and  that minimizes parenleftbigTbardblTbardbl2 +summationtext K k=1  kbardblkbardbl2 + bardblbardbl2parenrightbig, subject to the constraint that at most H entries of  are nonzero." ></td>
	<td class="line x" title="121:210	Human guidance Another way to select the general features is to follow some guidance from human knowledge." ></td>
	<td class="line x" title="122:210	Recall that in Section 4.1 we find that the commonality among different relation types usually lies in the syntactic structures between the two arguments." ></td>
	<td class="line x" title="123:210	This observation gives some intuition about how to separate general features from typespecific features." ></td>
	<td class="line x" title="124:210	In particular, here we consider two hypotheses regarding the generality of different kinds of features." ></td>
	<td class="line x" title="125:210	Argument word features: We hypothesize that the head words of the relation arguments are more likely to be strong indicators of specific relation types rather than any relation type." ></td>
	<td class="line x" title="126:210	For example, if an argument has the head word sister, it strongly indicates a family relation." ></td>
	<td class="line x" title="127:210	We refer to the set of features that contain any head word of an argument as arg-word features." ></td>
	<td class="line x" title="128:210	Entity type features: We hypothesize that the entity types and subtypes of the relation arguments are also more likely to be associated with specific relation types." ></td>
	<td class="line x" title="129:210	For example, arguments that are location entities may be strongly correlated with physical proximity relations." ></td>
	<td class="line x" title="130:210	We refer to the set of features that contain the entity type or subtype of an argument as arg-NE features." ></td>
	<td class="line x" title="131:210	We hypothesize that the arg-word and arg-NE features are type-specific and therefore should be excluded from the set of general features." ></td>
	<td class="line x" title="132:210	We can force the weights of these hypothesized typespecific features to be 0 in the shared weight vector , i.e. we can set the matrix F to achieve this feature separation." ></td>
	<td class="line x" title="133:210	Combined method We can also combine the automatic way of setting F with human guidance." ></td>
	<td class="line x" title="134:210	Specifically, we still follow the first automatic procedure to choose general features, but we then filter out any hypothesized type-specific feature from the set of general features chosen by the automatic procedure." ></td>
	<td class="line x" title="135:210	4.4 Imposing entity type constraints Finally, we consider how we can exploit additional human knowledge about the target relation type T to further improve the classifier." ></td>
	<td class="line x" title="136:210	We note that usually when a relation type is defined, we often have strong preferences or even hard constraints on the types of entities that can possibly be the two relation arguments." ></td>
	<td class="line x" title="137:210	These type constraints can help us 1016 Target Type T BL BL-A TL-auto TL-guide TL-comb TL-NE P 0.0000 0.1692 0.2920 0.2934 0.3325 0.5056 Physical R 0.0000 0.0848 0.1696 0.1722 0.2383 0.2316 F 0.0000 0.1130 0.2146 0.2170 0.2777 0.3176 Personal P 1.0000 0.0804 0.1005 0.3069 0.3214 0.6412 /Social R 0.0386 0.1708 0.1598 0.7245 0.7686 0.7631 F 0.0743 0.1093 0.1234 0.4311 0.4533 0.6969 Employment P 0.9231 0.3561 0.5230 0.5428 0.5973 0.7145 /Membership R 0.0075 0.1850 0.2617 0.2648 0.3632 0.3601 /Subsidiary F 0.0148 0.2435 0.3488 0.3559 0.4518 0.4789 AgentP 0.8750 0.0603 0.1813 0.1825 0.1835 0.1967 Artifact R 0.0343 0.2353 0.6471 0.6225 0.6422 0.6373 F 0.0660 0.0960 0.2833 0.2822 0.2854 0.3006 PER/ORG P 0.8889 0.0838 0.1510 0.1592 0.1667 0.1844 Affiliation R 0.0567 0.4965 0.6950 0.8369 0.8794 0.8723 F 0.1067 0.1434 0.2481 0.2676 0.2802 0.3045 GPE P 1.0000 0.2530 0.3904 0.3604 0.3560 0.5824 Affiliation R 0.0077 0.4509 0.6416 0.5992 0.6166 0.6127 F 0.0153 0.3241 0.4854 0.4501 0.4513 0.5972 P 1.0000 0.0298 0.0503 0.0471 0.1370 0.1370 Discourse R 0.0036 0.0789 0.1075 0.1147 0.3477 0.3477 F 0.0071 0.0433 0.0685 0.0668 0.1966 0.1966 P 0.8124 0.1475 0.2412 0.2703 0.2992 0.4231 Average R 0.0212 0.2432 0.3832 0.4764 0.5509 0.5464 F 0.0406 0.1532 0.2532 0.2958 0.3423 0.4132 Table 3: Comparison of different methods on ACE 2004 data set." ></td>
	<td class="line x" title="138:210	P, R and F stand for precision, recall and F1, respectively." ></td>
	<td class="line x" title="139:210	remove some false positive instances." ></td>
	<td class="line x" title="140:210	We therefore manually identify the entity type constraints for each target relation type based on the definition of the relation type given in the ACE annotation guidelines, and impose these type constraints as a final refinement step on top of the predicted positive instances." ></td>
	<td class="line x" title="141:210	5 Experiments 5.1 Data set and experiment setup We used the ACE 2004 data set to evaluate our proposed methods." ></td>
	<td class="line x" title="142:210	There are seven relation types defined in ACE 2004." ></td>
	<td class="line x" title="143:210	After data cleaning, we obtained 4290 positive instances among 48614 candidate relation instances." ></td>
	<td class="line x" title="144:210	We took each relation type as the target type and used the remaining types as auxiliary types." ></td>
	<td class="line x" title="145:210	This gave us seven sets of experiments." ></td>
	<td class="line x" title="146:210	In each set of experiments for a single target relation type, we randomly divided all the data into five subsets, and used each subset for testing while using the other four subsets for training, i.e. each experiment was repeated five times with different training and test sets." ></td>
	<td class="line x" title="147:210	Each time, we removed most of the positive instances of the target type from the training set except only a small number S of seed instances." ></td>
	<td class="line x" title="148:210	This gave us the weakly-supervised setting." ></td>
	<td class="line x" title="149:210	We kept all the positive instances of the target type in the test set." ></td>
	<td class="line x" title="150:210	In order to concentrate on the classification accuracy for the target relation type, we removed the positive instances of the auxiliary relation types from the test set, although in practice we need to extract these auxiliary relation instances using learned classifiers for these relation types." ></td>
	<td class="line x" title="151:210	5.2 Comparison of different methods We first show the comparison of our proposed multi-task transfer learning methods with the two baseline methods described in Section 3.2." ></td>
	<td class="line x" title="152:210	The performance on each target relation type and the average performance across seven types are shown in Table 3." ></td>
	<td class="line x" title="153:210	BL refers to the first baseline and BLA refers to the second baseline which uses auxil1017 T 100 1000 10000 P 0.6265 0.3162 0.2992 R 0.1170 0.3959 0.5509 F 0.1847 0.2983 0.3423 Table 4: The average performance of TL-comb with different T ." ></td>
	<td class="line x" title="154:210	(k = 104 and  = 1.)" ></td>
	<td class="line x" title="155:210	iary relation instances." ></td>
	<td class="line x" title="156:210	The four TL methods are all based on the multi-task transfer learning framework." ></td>
	<td class="line x" title="157:210	TL-auto sets F automatically within the optimization problem itself." ></td>
	<td class="line x" title="158:210	TL-guide chooses all features except arg-word and arg-NE features as general features and sets F accordingly." ></td>
	<td class="line x" title="159:210	TL-comb combines TL-auto and TL-guide, as described in Section 4.3." ></td>
	<td class="line x" title="160:210	Finally, TL-NE builds on top of TLcomb and uses the entity type constraints to refine the predictions." ></td>
	<td class="line x" title="161:210	In this set of experiments, the number of seed instances for each target relation type was set to 10." ></td>
	<td class="line x" title="162:210	The parameters were set to their optimal values (T = 104, k = 104,  = 1, and H = 500)." ></td>
	<td class="line x" title="163:210	As we can see from the table, first of all, BL generally has high precision but very low recall." ></td>
	<td class="line x" title="164:210	BL-A performs better than BL in terms of F1 because it gives better recall." ></td>
	<td class="line x" title="165:210	However, BL-A still cannot achieve as high recall as the TL methods." ></td>
	<td class="line x" title="166:210	This is probably because the model learned by BLA still focuses more on type-specific features for each relation type rather than on the commonly useful general features, and therefore does not help much in classifying the target relation type." ></td>
	<td class="line x" title="167:210	The four TL methods all outperform the two baseline methods." ></td>
	<td class="line x" title="168:210	TL-comb performs better than both TL-auto and TL-guide, which shows that while we can either choose general features automatically by the learning algorithm or manually with human knowledge, it is more effective to combine human knowledge with the multi-task learning framework." ></td>
	<td class="line x" title="169:210	Not surprisingly, TL-NE improves the precision over TL-comb without hurting the recall much." ></td>
	<td class="line x" title="170:210	Ideally, TL-NE should not decrease recall if the type constraints are strictly observed in the data." ></td>
	<td class="line x" title="171:210	We find that it is not always the case with the ACE data, leading to the small decrease of recall from TL-comb to TL-NE." ></td>
	<td class="line x" title="172:210	5.3 The effect of T Let us now take a look at the effect of using different T . As we can see from Table 4, smaller T gives higher precision while larger T gives  0.1  0.15  0.2  0.25  0.3  0.35  0.4  0.45  0.5  100  1000  10000 avg F1 H TL-comb TL-auto BL-A Figure 2: Performance of TL-comb and TL-auto as H changes." ></td>
	<td class="line x" title="173:210	higher recall." ></td>
	<td class="line x" title="174:210	These results make sense because the larger T is, the more we penalize large weights of T . As a result, the model for the target type is forced to conform to the shared model  and prevented from overfitting the few seed target instances." ></td>
	<td class="line x" title="175:210	T is therefore a useful parameter to help us control the tradeoff between precision and recall for the target type." ></td>
	<td class="line x" title="176:210	While varying k also gives similar effect for typeAk, we found that setting k to smaller values would not helpT because in this case the auxiliary relation instances would be used more for training the type-specific component k rather than the common component ." ></td>
	<td class="line x" title="177:210	5.4 Sensitivity of H Another parameter in the multi-task transfer learning framework is the number of general features H, i.e. the number of non-zero entries in the shared weight vector ." ></td>
	<td class="line x" title="178:210	To see how the performance may vary as H changes, we plot the performance of TL-comb and TL-auto in terms of the average F1 across the seven target relation types, with H ranging from 100 to 50000." ></td>
	<td class="line x" title="179:210	As we can see in Figure 2, the performance is relatively stable, and always above BL-A." ></td>
	<td class="line x" title="180:210	This suggests that the performance of TL-comb and TL-auto is not very sensitive to the value of H. 5.5 Hypothesized type-specific features In Section 4.3, we showed two sets of hypothesized type-specific features, namely, arg-word features and arg-NE features." ></td>
	<td class="line x" title="181:210	We also experimented with each set separately to see whether both sets are useful." ></td>
	<td class="line x" title="182:210	The comparison is shown in Table 5." ></td>
	<td class="line x" title="183:210	As we can see, using either set of typespecific features in either TL-guide or TL-comb can improve the performance over BL-A, but the 1018 arg-word arg-NE union TL-guide 0.2095 0.2983 0.2958 TL-comb 0.2215 0.3331 0.3423 BL-A 0.1532 Table 5: Average F1 using different hypothesized type-specific features." ></td>
	<td class="line x" title="184:210	0  0.1  0.2  0.3  0.4  0.5  0.6  10  100  1000 avg F1 S TL-NE (104) TL-NE (102) BL BL-A Figure 3: Performance of TL-NE, BL and BL-A as the number of seed instances S of the target type increases." ></td>
	<td class="line x" title="185:210	(H = 500." ></td>
	<td class="line x" title="186:210	T was set to 104 and 102)." ></td>
	<td class="line x" title="187:210	arg-NE features are probably more type-specific than arg-word features because they give better performance." ></td>
	<td class="line x" title="188:210	Using the union of the two sets is still the best for TL-comb." ></td>
	<td class="line x" title="189:210	5.6 Changing the number of seed instances Finally, we compare TL-NE with BL and BL-A when the number of seed instances increases." ></td>
	<td class="line x" title="190:210	We set S from 5 up to 1000." ></td>
	<td class="line x" title="191:210	When S is large, the problem becomes more like traditional supervised learning, and our setting of T = 104 is no longer optimal because we are now not afraid of overfitting the large set of seed target instances." ></td>
	<td class="line x" title="192:210	Therefore we also included another TL-NE experiment with T set to 102." ></td>
	<td class="line x" title="193:210	The comparison of the performance is shown in Figure 3." ></td>
	<td class="line x" title="194:210	We see that as S increases, both BL and BL-Acatch up, and BL overtakes BL-A when S is sufficiently large because BL uses positive training examples only from the target type." ></td>
	<td class="line x" title="195:210	Overall, TL-NE still outperforms the two baselines in most of the cases over the wide range of values of S, but the optimal value for T decreases as S increases, as we have suspected." ></td>
	<td class="line x" title="196:210	The results show that if T is set appropriately, our multi-task transfer learning method is robust and advantageous over the baselines under both the weakly-supervised setting and the traditional supervised setting." ></td>
	<td class="line x" title="197:210	6 Conclusions and future work In this paper, we applied multi-task transfer learning to solve a weakly-supervised relation extraction problem, leveraging both labeled instances of auxiliary relation types and human knowledge including hypotheses on feature generality and entity type constraints." ></td>
	<td class="line x" title="198:210	In the multi-task learning framework that we introduced, different relation types are treated as different but related tasks that are learned together, with the common structures among the relation types modeled by a shared weight vector." ></td>
	<td class="line x" title="199:210	The shared weight vector corresponds to the general features across different relation types." ></td>
	<td class="line x" title="200:210	We proposed to choose the general features either automatically inside the learning algorithm or guided by human knowledge." ></td>
	<td class="line x" title="201:210	We also leveraged additional human knowledge about the target relation type in the form of entity type constraints." ></td>
	<td class="line x" title="202:210	Experiment results on the ACE 2004 data show that the multi-task transfer learning method achieves the best performance when we combine human guidance with automatic general feature selection, followed by imposing the entity type constraints." ></td>
	<td class="line x" title="203:210	The final method substantially outperforms two baseline methods, improving the average F1 measure from 0.1532 to 0.4132 when only 10 seed target instances are used." ></td>
	<td class="line x" title="204:210	Our work is the first to explore transfer learning for relation extraction, and we have achieved very promising results." ></td>
	<td class="line x" title="205:210	Because of the practical importance of transfer learning and adaptation for relation extraction due to lack of training data in new domains, we hope our study and findings will lead to further investigation into this problem." ></td>
	<td class="line x" title="206:210	There are still many issues that remain unsolved." ></td>
	<td class="line x" title="207:210	For example, we have not looked at the degrees of relatedness between different pairs of relation types." ></td>
	<td class="line x" title="208:210	Presumably, when adapting to a specific target relation type, we want to choose the most similar auxiliary relation types to use." ></td>
	<td class="line x" title="209:210	Our current study is based on ACE relation types." ></td>
	<td class="line x" title="210:210	It would also be interesting to study similar problems in other domains, for example, the protein-protein interaction extraction problem in biomedical text mining." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-2205
A Comparison of Structural Correspondence Learning and Self-training for Discriminative Parse Selection
Plank, Barbara;"></td>
	<td class="line x" title="1:131	Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 3742, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:131	c 2009 Association for Computational Linguistics A Comparison of Structural Correspondence Learning and Self-training for Discriminative Parse Selection Barbara Plank University of Groningen, The Netherlands b.plank@rug.nl Abstract This paper evaluates two semi-supervised techniques for the adaptation of a parse selection model to Wikipedia domains." ></td>
	<td class="line oc" title="3:131	The techniques examined are Structural Correspondence Learning (SCL) (Blitzer et al., 2006) and Self-training (Abney, 2007; McClosky et al., 2006)." ></td>
	<td class="line p" title="4:131	A preliminary evaluation favors the use of SCL over the simpler self-training techniques." ></td>
	<td class="line x" title="5:131	1 Introduction and Motivation Parse selection constitutes an important part of many parsing systems (Hara et al., 2005; van Noord and Malouf, 2005; McClosky et al., 2006)." ></td>
	<td class="line x" title="6:131	Yet, there is little to no work focusing on the adaptation of parse selection models to novel domains." ></td>
	<td class="line x" title="7:131	This is most probably due to the fact that potential gains for this task are inherently bounded by the underlying grammar." ></td>
	<td class="line x" title="8:131	The few studies on adapting parse disambiguation models, like Hara et al.(2005), have focused exclusively on supervised domain adaptation, i.e. one has access to a comparably small, but labeled amount of target data." ></td>
	<td class="line x" title="10:131	In contrast, in semisupervised domain adaptation one has only unlabeled target data." ></td>
	<td class="line x" title="11:131	It is a more realistic situation, but at the same time also considerably more difficult." ></td>
	<td class="line x" title="12:131	In this paper we evaluate two semi-supervised approaches to domain adaptation of a discriminative parse selection model." ></td>
	<td class="line oc" title="13:131	We examine Structural Correspondence Learning (SCL) (Blitzer et al., 2006) for this task, and compare it to several variants of Self-training (Abney, 2007; McClosky et al., 2006)." ></td>
	<td class="line x" title="14:131	For empirical evaluation (section 4) we use the Alpino parsing system for Dutch (van Noord and Malouf, 2005)." ></td>
	<td class="line x" title="15:131	As target domain, we exploit Wikipedia as primary test and training collection." ></td>
	<td class="line pc" title="16:131	2 Previous Work So far, Structural Correspondence Learning has been applied successfully to PoS tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007)." ></td>
	<td class="line o" title="17:131	An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa, 2007)." ></td>
	<td class="line n" title="18:131	However, the system just ended up at rank 7 out of 8 teams." ></td>
	<td class="line x" title="19:131	Based on annotation differences in the datasets (Dredze et al., 2007) and a bug in their system (Shimizu and Nakagawa, 2007), their results are inconclusive." ></td>
	<td class="line p" title="20:131	A recent attempt (Plank, 2009) shows promising results on applying SCL to parse disambiguation." ></td>
	<td class="line o" title="21:131	In this paper, we extend that line of work and compare SCL to bootstrapping approaches such as self-training." ></td>
	<td class="line x" title="22:131	Studies on self-training have focused mainly on generative, constituent based parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007)." ></td>
	<td class="line x" title="23:131	Steedman et al.(2003) as well as Reichart and Rappoport (2007) examine self-training for PCFG parsing in the small seed case (< 1k labeled data), with different results." ></td>
	<td class="line x" title="25:131	In contrast, McClosky et al.(2006) focus on large seeds and exploit a reranking-parser." ></td>
	<td class="line x" title="27:131	Improvements are obtained (McClosky et al., 2006; McClosky and Charniak, 2008), showing that a reranker is necessary for successful self-training in such a high-resource scenario." ></td>
	<td class="line o" title="28:131	While they self-trained a generative model, we examine self-training and SCL for semi-supervised adaptation of a discriminative parse selection system." ></td>
	<td class="line oc" title="29:131	37 3 Semi-supervised Domain Adaptation 3.1 Structural Correspondence Learning Structural Correspondence Learning (Blitzer et al., 2006) exploits unlabeled data from both source and target domain to find correspondences among features from different domains." ></td>
	<td class="line x" title="30:131	These correspondences are then integrated as new features in the labeled data of the source domain." ></td>
	<td class="line o" title="31:131	The outline of SCL is given in Algorithm 1." ></td>
	<td class="line o" title="32:131	The key to SCL is to exploit pivot features to automatically identify feature correspondences." ></td>
	<td class="line oc" title="33:131	Pivots are features occurring frequently and behaving similarly in both domains (Blitzer et al., 2006)." ></td>
	<td class="line x" title="34:131	They correspond to auxiliary problems in Ando and Zhang (2005)." ></td>
	<td class="line x" title="35:131	For every such pivot feature, a binary classifier is trained (step 2 of Algorithm 1) by masking the pivot feature in the data and trying to predict it with the remaining non-pivot features." ></td>
	<td class="line x" title="36:131	Non-pivots that correlate with many of the same pivots are assumed to correspond." ></td>
	<td class="line x" title="37:131	These pivot predictor weight vectors thus implicitly align non-pivot features from source and target domain." ></td>
	<td class="line oc" title="38:131	Intuitively, if we are able to find good correspondences through linking pivots, then the augmented source data should transfer better to a target domain (Blitzer et al., 2006)." ></td>
	<td class="line oc" title="39:131	Algorithm 1 SCL (Blitzer et al., 2006) 1: Select m pivot features." ></td>
	<td class="line x" title="40:131	2: Train m binary classifiers (pivot predictors)." ></td>
	<td class="line x" title="41:131	Create matrix Wnm of binary predictor weight vectors W = [w1,,wm], with n number of nonpivots." ></td>
	<td class="line x" title="42:131	3: Dimensionality Reduction." ></td>
	<td class="line x" title="43:131	Apply SVD to W: Wnm = UnnDnmV Tmm and select  = UT[1:h,:] (the h top left singular vectors of W)." ></td>
	<td class="line x" title="44:131	4: Train a new model on the original and new features obtained by applying the projection x." ></td>
	<td class="line oc" title="45:131	SCL for Discriminative Parse Selection So far, pivot features on the word level were used (Blitzer et al., 2006; Blitzer et al., 2007)." ></td>
	<td class="line n" title="46:131	However, for parse disambiguation based on a conditional model they are irrelevant." ></td>
	<td class="line x" title="47:131	Hence, we follow Plank (2009) and actually first parse the unlabeled data." ></td>
	<td class="line x" title="48:131	This allows a possibly noisy, but more abstract representation of the underlying data." ></td>
	<td class="line x" title="49:131	Features thus correspond to properties of parses: application of grammar rules (r1,r2 features), dependency relations (dep), PoS tags (f1,f2), syntactic features (s1), precedence (mf ), bilexical preferences (z), apposition (appos) and further features for unknown words, temporal phrases, coordination (h,in year and p1, respectively)." ></td>
	<td class="line x" title="50:131	These features are further described in van Noord and Malouf (2005)." ></td>
	<td class="line x" title="51:131	Selection of pivot features As pivot features should be common across domains, here we restrict our pivots to be of the type r1,p1,s1 (the most frequently occurring feature types)." ></td>
	<td class="line x" title="52:131	In more detail, r1 indicates which grammar rule applied, p1 whether coordination conjuncts are parallel, and s1 whether local/non-local extraction occurred." ></td>
	<td class="line x" title="53:131	We count how often each feature appears in the parsed source and target domain data, and select those r1,p1,s1 features as pivot features, whose count is > t, where t is a specified threshold." ></td>
	<td class="line x" title="54:131	In all our experiments, we set t = 5000." ></td>
	<td class="line x" title="55:131	In this way we obtained on average 360 pivot features, on the datasets described in Section 4." ></td>
	<td class="line x" title="56:131	3.2 Self-training Self-training (Algorithm 2) is a simple single-view bootstrapping algorithm." ></td>
	<td class="line x" title="57:131	In self-training, the newly labeled instances are taken at face value and added to the training data." ></td>
	<td class="line x" title="58:131	There are many possible ways to instantiate selftraining (Abney, 2007)." ></td>
	<td class="line x" title="59:131	One variant, introduced in Abney (2007) is the notion of (in)delibility: in the delible case the classifier relabels all of the unlabeled data from scratch in every iteration." ></td>
	<td class="line x" title="60:131	The classifier may become unconfident about previously selected instances and they may drop out (Steven Abney, personal communication)." ></td>
	<td class="line x" title="61:131	In contrast, in the indelible case, labels once assigned do not change again (Abney, 2007)." ></td>
	<td class="line x" title="62:131	In this paper we look at the following variants of self-training:  single versus multiple iterations,  selection versus no selection (taking all selflabeled data or selecting presumably higher quality instances); different scoring functions for selection,  delibility versus indelibility for multiple iterations." ></td>
	<td class="line x" title="63:131	38 Algorithm 2 Self-training (indelible) (Abney, 2007)." ></td>
	<td class="line x" title="64:131	1: L0 is labeled [seed] data, U is unlabeled data 2: ctrain(L0) 3: repeat 4: LL + select(label(UL,c)) 5: ctrain(L) 6: until stopping criterion is met Scoring methods We examine three simple scoring functions for instance selection: i) Entropy (summationtextyY (s) p(|s,)logp(|s,))." ></td>
	<td class="line x" title="65:131	ii) Number of parses (|Y (s)|); and iii) Sentence Length (|s|)." ></td>
	<td class="line x" title="66:131	4 Experiments and Results Experimental Design The system used in this study is Alpino, a two-stage dependency parser for Dutch (van Noord and Malouf, 2005)." ></td>
	<td class="line x" title="67:131	The first stage consists of a HPSG-like grammar that constitutes the parse generation component." ></td>
	<td class="line x" title="68:131	The second stage is a Maximum Entropy (MaxEnt) parse selection model." ></td>
	<td class="line x" title="69:131	To train the MaxEnt model, parameters are estimated based on informative samples (Osborne, 2000)." ></td>
	<td class="line x" title="70:131	A parse is added to the training data with a score indicating its goodness (van Noord and Malouf, 2005)." ></td>
	<td class="line x" title="71:131	The score is obtained by comparing it with the gold standard (if available; otherwise the score is approximated through parse probability)." ></td>
	<td class="line x" title="72:131	The source domain is the Alpino Treebank (van Noord and Malouf, 2005) (newspaper text; approx." ></td>
	<td class="line x" title="73:131	7,000 sentences; 145k tokens)." ></td>
	<td class="line x" title="74:131	We use Wikipedia both as testset and as unlabeled target data source." ></td>
	<td class="line x" title="75:131	We assume that in order to parse data from a very specific domain, say about the artist Prince, then data related to that domain, like information about the New Power Generation, the Purple rain movie, or other American singers and artists, should be of help." ></td>
	<td class="line x" title="76:131	Thus, we exploit Wikipedias category system to gather domain-specific target data." ></td>
	<td class="line oc" title="77:131	In our empirical setup, we follow Blitzer et al.(2006) and balance the size of source and target data." ></td>
	<td class="line x" title="79:131	Thus, depending on the size of the resulting target domain dataset, and the broadness of the categories involved in creating it, we might wish to filter out certain pages." ></td>
	<td class="line x" title="80:131	We implemented a filter mechanism that excludes pages of a certain category (e.g. a supercategory that is hypothesized to be too broad)." ></td>
	<td class="line x" title="81:131	Further details about the dataset construction are given in (Plank, 2009)." ></td>
	<td class="line x" title="82:131	Table 1 provides information on the target domain datasets constructed from Wikipedia." ></td>
	<td class="line x" title="83:131	Related to Articles Sents Tokens Relationship Prince 290 9,772 145,504 filtered super Paus 445 8,832 134,451 all DeMorgan 394 8,466 132,948 all Table 1: Size of related unlabeled data; relationship indicates whether all related pages are used or some are filtered out." ></td>
	<td class="line x" title="84:131	The size of the target domain testsets is given in Table 2." ></td>
	<td class="line x" title="85:131	As evaluation measure concept accuracy (CA) (van Noord and Malouf, 2005) is used (similar to labeled dependency accuracy)." ></td>
	<td class="line x" title="86:131	The training data for the pivot predictors are the 1-best parses of source and target domain data as selected by the original Alpino model." ></td>
	<td class="line o" title="87:131	We report on results of SCL with dimensionality parameter set to h = 25, and remaining settings identical to Plank (2009) (i.e., no feature-specific regularization and no feature normalization and rescaling)." ></td>
	<td class="line x" title="88:131	Baseline Table 2 shows the baseline accuracies (model trained on labeled out-of-domain data) on the Wikipedia testsets (last column: size in number of sentences)." ></td>
	<td class="line x" title="89:131	The second and third column indicate lower (first parse) and upper(oracle) bounds." ></td>
	<td class="line x" title="90:131	Wikipedia article baseline first oracle sent Prince (musician) 85.03 71.95 88.70 357 Paus Johannes Paulus II 85.72 74.30 89.09 232 Augustus De Morgan 80.09 70.08 83.52 254 Table 2: Supervised Baseline results." ></td>
	<td class="line p" title="91:131	SCL and Self-training results The results for SCL (Table 3) show a small, but consistent increase in absolute performance on all testsets over the baselines (up to +0.27 absolute CA or 7.34% relative error reduction, which is significant at p < 0.05 according to sign test)." ></td>
	<td class="line p" title="92:131	In contrast, basic self-training (Table 3) achieves roughly only baseline accuracy and lower performance than SCL, with one exception." ></td>
	<td class="line n" title="93:131	On the DeMorgan testset, self-training scores slightly higher than SCL." ></td>
	<td class="line p" title="94:131	However, the improvements of both SCL and self-training are not significant on this rather 39 small testset." ></td>
	<td class="line x" title="95:131	Indeed, self-training scores better than the baseline on only 5 parses out of 254, while its performance is lower on 2, leaving only 3 parses that account for the difference." ></td>
	<td class="line o" title="96:131	CA  Rel.ER Prince baseline 85.03 78.06 0.00 SCL  85.30 79.67 7.34 Self-train (all-at-once) 85.08 78.38 1.46 Paus baseline 85.72 77.23 0.00 SCL 85.82 77.87 2.81 Self-train (all-at-once) 85.78 77.62 1.71 DeMorgan baseline 80.09 74.44 0.00 SCL 80.15 74.92 1.88 Self-train (all-at-once) 80.24 75.63 4.65 Table 3: Results of SCL and self-training (single iteration, no selection)." ></td>
	<td class="line x" title="97:131	Entries marked with  are statistically significant at p < 0.05." ></td>
	<td class="line x" title="98:131	The  score incorporates upperand lower-bounds." ></td>
	<td class="line x" title="99:131	To gauge whether other instantiations of selftraining are more effective, we evaluated the selftraining variants introduced in section 3.2 on the Prince dataset." ></td>
	<td class="line x" title="100:131	In the iterative setting, we follow Steedman et al.(2003) and parse 30 sentences from which 20 are selected in every iteration." ></td>
	<td class="line x" title="102:131	With regard to the comparison of delible versus indelible self-training (whether labels may change), our empirical findings shows that the two cases achieve very similar performance; the two curves highly overlap (Figure 1)." ></td>
	<td class="line x" title="103:131	The accuracies of both curves fluctuate around 85.13, showing no upward or downward trend." ></td>
	<td class="line x" title="104:131	In general, however, indelibility is preferred since it takes considerably less time (the classifier does not have to relabel U from scratch in every iteration)." ></td>
	<td class="line x" title="105:131	In addition, we tested EM (which uses all unlabeled data in each iteration)." ></td>
	<td class="line x" title="106:131	Its performance is consistently lower, varying around the baseline." ></td>
	<td class="line o" title="107:131	Figure 2 compares several self-training variants with the supervised baseline and SCL." ></td>
	<td class="line x" title="108:131	It summarizes the effect of i) selection versus no selection (and various selection techniques) as well as ii) single versus multiple iterations of self-training." ></td>
	<td class="line x" title="109:131	For clarity, the figure shows the learning curve of the best selection technique only, but depicts the performance of the various selection techniques in a single iteration (non-solid lines)." ></td>
	<td class="line x" title="110:131	In the iterative setting, taking the whole selflabeled data and not selecting certain instances (grey curve in Figure 2) degrades performance." ></td>
	<td class="line x" title="111:131	In contrast, selecting shorter sentences slightly improves accuracy, and is the best selection method among the ones tested (shorter sentences, entropy, fewer parses)." ></td>
	<td class="line x" title="112:131	For all self-training instantiations, running multiple iterations is on average just the same as running a single iteration (the non-solid lines are roughly the average of the learning curves)." ></td>
	<td class="line x" title="113:131	Thus there is no real need to run several iterations of self-training." ></td>
	<td class="line p" title="114:131	The main conclusion is that in contrast to SCL, none of the self-training instantiations achieves a significant improvement over the baseline." ></td>
	<td class="line oc" title="115:131	5 Conclusions and Future Work The paper compares Structural Correspondence Learning (Blitzer et al., 2006) with (various instances of) self-training (Abney, 2007; McClosky et al., 2006) for the adaptation of a parse selection model to Wikipedia domains." ></td>
	<td class="line x" title="116:131	The empirical findings show that none of the evaluated self-training variants (delible/indelible, single versus multiple iterations, various selection techniques) achieves a significant improvement over the baseline." ></td>
	<td class="line p" title="117:131	The more indirect exploitation of unlabeled data through SCL is more fruitful than pure self-training." ></td>
	<td class="line x" title="118:131	Thus, favoring the use of the more complex method, although the findings are not confirmed on all testsets." ></td>
	<td class="line o" title="119:131	Of course, our results are preliminary and, rather than warranting yet many definite conclusions, encourage further investigation of SCL (varying size of target data, pivots selection, bigger testsets as well as other domains etc.) as well as related semisupervised adaptation techniques." ></td>
	<td class="line x" title="120:131	Acknowledgments Thanks to Gertjan van Noord and the anonymous reviewers for their comments." ></td>
	<td class="line x" title="121:131	The Linux cluster of the High-Performance Computing Center of the University of Groningen was used in this work." ></td>
	<td class="line o" title="122:131	40 0 50 100 150 200 85.00 85.05 85.10 85.15 85.20 85.25 85.30 number of iterations accuracy Indelibility versus delibility baseline SCL Indelible SelfTrain Delible SelfTrain EM Figure 1: Delible versus Indelible self-training and EM." ></td>
	<td class="line x" title="123:131	Delible and indelible self-training achieve very similar performance." ></td>
	<td class="line x" title="124:131	However, indelibility is preferred over delibility since it is considerably faster." ></td>
	<td class="line o" title="125:131	0 50 100 150 200 85.00 85.05 85.10 85.15 85.20 85.25 85.30 number of iterations accuracy shorter sent entropy fewer parses / no selection baseline SCL Indelibility with different selection techniques select shorter sent no selection Figure 2: Self-training variants compared to supervised baseline and SCL." ></td>
	<td class="line x" title="126:131	The effect of various selection techniques (Sec." ></td>
	<td class="line x" title="127:131	3.2) in a single iteration is depicted (non-solid lines; fewer parses and no selection achieve identical results)." ></td>
	<td class="line x" title="128:131	For clarity, the figure shows the learning curve for the best selection technique only (shorter sent) versus no selection." ></td>
	<td class="line x" title="129:131	On average running multiple iterations is just the same as a single iteration." ></td>
	<td class="line p" title="130:131	In all cases SCL still performs best." ></td>
	<td class="line x" title="131:131	41" ></td>
</tr></table>
</div
</body></html>
