<html><body><head><link rel="stylesheet" type="text/css" href="style.css" /><script src="map.js"></script><script src="jquery-1.7.1.min.js"></script></head>
<div class="dstPaperData">
P04-1015 <div class="dstPaperTitle">Incremental Parsing With The Perceptron Algorithm</div><div class="dstPaperAuthors">Collins, Michael John;Roark, Brian;</div>
</div>
<table cellspacing="0" cellpadding="0"><tr>
	<td class="srcData" >Source Paper</td>
	<td class="pp legend" ><input type="checkbox" id="cbIPositive" checked="true"/><label for="cbIPositive">Informal +<label></td>
	<td class="nn legend" ><input type="checkbox" id="cbINegative" checked="true"/><label for="cbINegative">Informal -<label></td>
	<td class="oo legend" ><input type="checkbox" id="cbIObjective" checked="true"/><label for="cbIObjective">Informal Neutral<label></td>
	<td class="ppc legend" ><input type="checkbox" id="cbEPositive" checked="true"/><label for="cbEPositive">Formal +</label></td>
	<td class="nnc legend" ><input type="checkbox" id="cbENegative" checked="true"/><label for="cbENegative">Formal -</label></td>
	<td class="ooc legend" ><input type="checkbox" id="cbEObjective" checked="true"/><label for="cbEObjective">Formal Neutral</label></td>
	<td class="lb"><input type="checkbox" id="cbSentenceBoundary"/><label for="cbSentenceBoundary">Sentence Boundary</label></td>
</tr></table>
<div class="dstPaper">
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W04-0303
Efficient Incremental Beam-Search Parsing With Generative And Discriminative Models
Roark, Brian;"></td>
	<td class="line x" title="1:41	Efficient incremental beam-search parsing with generative and discriminative models Brian Roark Center for Spoken Language Understanding OGI School of Science & Engineering, Oregon Health & Science University Extended Abstract: This talk will present several issues related to incremental (left-to-right) beam-search parsing of natural language using generative or discriminative models, either individually or in combination." ></td>
	<td class="line x" title="2:41	The first part of the talk will provide background in incremental top-down and (selective) left-corner beamsearch parsing algorithms, and in stochastic models for such derivation strategies." ></td>
	<td class="line x" title="3:41	Next, the relative benefits and drawbacks of generative and discriminative models with respect to heuristic pruning and search will be discussed." ></td>
	<td class="line x" title="4:41	A range of methods for using multiple models during incremental parsing will be detailed." ></td>
	<td class="line x" title="5:41	Finally, we will discuss the potential for effective use of fast, finite-state processing, e.g. partof-speech tagging, to reduce the parsing search space without accuracy loss." ></td>
	<td class="line x" title="6:41	POS-tagging is shown to improve efficiency by as much as 20-25 percent with the same accuracy, largely due to the treatment of unknown words." ></td>
	<td class="line x" title="7:41	In contrast, an islands-of-certainty approach, which quickly annotates labeled bracketing over low-ambiguity word sequences, is shown to provide little or no efficiency gain over the existing beam-search." ></td>
	<td class="line x" title="8:41	The basic parsing approach that will be described in this talk is stochastic incremental top-down parsing, using a beam-search to prune the search space." ></td>
	<td class="line x" title="9:41	Grammar induction occurs from an annotated treebank, and non-local features are extracted from each derivation to enrich the stochastic model." ></td>
	<td class="line x" title="10:41	Left-corner grammar and tree transforms can be applied to the treebank or the induced grammar, either fully or selectively, to change the derivation order while retaining the same underlying parsing algorithm." ></td>
	<td class="line pc" title="11:41	This approach has been shown to be accurate, relatively efficient, and robust using both generative and discriminative models (Roark, 2001; Roark, 2004; Collins and Roark, 2004)." ></td>
	<td class="line x" title="12:41	The key to effective beam-search parsing is comparability of analyses when the pruning is done." ></td>
	<td class="line x" title="13:41	If two competing parses are at different points in their respective derivations, e.g. one is near the end of the derivation and another is near the beginning, then it will be difficult to evaluate which of the two is likely to result in a better parse." ></td>
	<td class="line x" title="14:41	With a generative model, comparability can be accomplished by the use of a look-ahead statistic, which estimates the amount of probability mass required to extend a given derivation to include the word(s) in the look-ahead." ></td>
	<td class="line x" title="15:41	Every step in the derivation decreases the probability of the derivation, but also takes the derivation one step closer to attaching to the look-ahead." ></td>
	<td class="line x" title="16:41	For good parses, the look-ahead statistic should increase with each step of the derivation, ensuring a certain degree of comparability among competing parses with the same look-ahead." ></td>
	<td class="line oc" title="17:41	Beam-search parsing using an unnormalized discriminative model, as in Collins and Roark (2004), requires a slightly different search strategy than the original generative model described in Roark (2001; 2004)." ></td>
	<td class="line x" title="18:41	This alternate search strategy is closer to the approach taken in Costa et al.(2001; 2003), in that it enumerates a set of possible ways of attaching the next word before evaluating with the model." ></td>
	<td class="line x" title="20:41	This ensures comparability for models that do not have the sort of behavior described above for the generative models, rendering look-ahead statistics difficult to estimate." ></td>
	<td class="line x" title="21:41	This approach is effective, although somewhat less so than when a look-ahead statistic is used." ></td>
	<td class="line oc" title="22:41	A generative parsing model can be used on its own, and it was shown in Collins and Roark (2004) that a discriminative parsing model can be used on its own." ></td>
	<td class="line x" title="23:41	Most discriminative parsing approaches, e.g.(Johnson et al. , 1999; Collins, 2000; Collins and Duffy, 2002), are re-ranking approaches, in which another model (typically a generative model) presents a relatively small set of candidates, which are then re-scored using a second, discriminatively trained model." ></td>
	<td class="line x" title="25:41	There are other ways to combine a generative and discriminative model apart from waiting for the former to provide a set of completed candidates to the latter." ></td>
	<td class="line x" title="26:41	For example, the scores can be used simultaneously; or the generative model can present candidates to the discriminative model at intermediate points in the string, rather than simply at the end." ></td>
	<td class="line x" title="27:41	We discuss these options and their potential benefits." ></td>
	<td class="line x" title="28:41	Finally, we discuss and present a preliminary evaluation of the use of rapid finite-state tagging to reduce the parsing search space, as was done in (Ratnaparkhi, 1997; Ratnaparkhi, 1999)." ></td>
	<td class="line x" title="29:41	When the parsing algorithm is integrated with model training, such efficiency improvements can be particularly important." ></td>
	<td class="line x" title="30:41	POS-tagging using a simple bi-tag model improved parsing efficiency by nearly 25 percent without a loss in accuracy, when 1.2 tags per word were produced on average by the tagger." ></td>
	<td class="line x" title="31:41	Producing a single tag sequence for each string resulted in further speedups, but at the loss of 1-2 points of accuracy." ></td>
	<td class="line x" title="32:41	We show that much, but not all, of the speedup from POS-tagging is due to more constrained tagging of unknown words." ></td>
	<td class="line x" title="33:41	In a second set of trials, we make use of what we are calling syntactic collocations, i.e. collocations that are (nearly) unambiguously associated with a particular syntactic configuration." ></td>
	<td class="line x" title="34:41	For example, a chain of auxiliaries in English will always combine in a particular syntactic configuration, modulo noise in the annotation." ></td>
	<td class="line x" title="35:41	In our approach, the labeled bracketing spanning the sub-string is treated as a tag for the sequence." ></td>
	<td class="line x" title="36:41	A simple, finite-state method for finding such collocations, and an efficient longest match algorithm for labeling strings will be presented." ></td>
	<td class="line x" title="37:41	The labeled-bracketing tags are integrated with the parse search as follows: when a derivation reaches the first word of such a collocation, the remaining words are attached in the given configuration." ></td>
	<td class="line x" title="38:41	This has the effect of extending the look-ahead beyond the collocation, as well as potentially reducing the amount of search required to extend the derivations to include the words in the collocation." ></td>
	<td class="line x" title="39:41	However, while POS-tagging improved efficiency, we find that using syntactic collocations does not, indicating that islands-of-certainty approaches are not what is needed from shallow processing; rather genuine dis-ambiguation of the sort provided by the POS-tagger." ></td>
	<td class="line x" title="40:41	Acknowledgments Most of this work was done while the author was at AT&T Labs Research." ></td>
	<td class="line x" title="41:41	Some of it was in collaboration with Michael Collins." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="H05-1102
Incremental LTAG Parsing
Shen, Libin;Joshi, Aravind K.;"></td>
	<td class="line x" title="1:366	Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 811818, Vancouver, October 2005." ></td>
	<td class="line x" title="2:366	c2005 Association for Computational Linguistics Incremental LTAG Parsing Libin Shen and Aravind K. Joshi Department of Computer and Information Science University of Pennsylvania Philadelphia, PA 19104, USA a0 libin,joshi a1 @linc.cis.upenn.edu Abstract We present a very efficient statistical incremental parser for LTAG-spinal, a variant of LTAG." ></td>
	<td class="line x" title="3:366	The parser supports the full adjoining operation, dynamic predicate coordination, and non-projective dependencies, with a formalism of provably stronger generative capacity as compared to CFG." ></td>
	<td class="line x" title="4:366	Using gold standard POS tags as input, on section 23 of the PTB, the parser achieves an f-score of 89.3% for syntactic dependency defined on LTAG derivation trees, which are deeper than the dependencies extracted from PTB alone with head rules (for example, in Magermans style)." ></td>
	<td class="line x" title="5:366	1 Introduction Lexicalized Tree Adjoining Grammar (LTAG) is a formalism motived by both linguistic and computational perspectives (for a relatively recent review, see (Joshi and Schabes, 1997))." ></td>
	<td class="line x" title="6:366	Because of the introduction of the adjoining operation, the TAG formalism is provably stronger than Context Free Grammar (CFG) both in the weak and the strong generative power." ></td>
	<td class="line x" title="7:366	The TAG formalism provides linguistically attractive analysis of natural language (Frank, 2002)." ></td>
	<td class="line x" title="8:366	Recent psycholinguistic experiments (Sturt and Lombardo, 2005) demonstrate that the adjoining operation of LTAG is required for eager incremental processing." ></td>
	<td class="line x" title="9:366	Vijay-Shanker and Joshi (1985) introduced the first TAG parser in a CYK-like algorithm." ></td>
	<td class="line x" title="10:366	Because of the adjoining operation, the time complexity of LTAG parsing is as large as a2a4a3a6a5a8a7a10a9, compared with a2a11a3a6a5a13a12a14a9 of CFG parsing, where a5 is the length of the sentence to be parsed." ></td>
	<td class="line x" title="11:366	Many LTAG parsers were proposed, such as the head-driven Earley style parser (Lavelli and Satta, 1991) and the head-corner parser (van Noord, 1994)." ></td>
	<td class="line x" title="12:366	The high time complexity prevents LTAG parsing from real-time applications." ></td>
	<td class="line x" title="13:366	In this paper, we work on LTAG-spinal (Shen and Joshi, 2005), an interesting subset of LTAG, which preserves almost all of the strong generative power of LTAG, and it is both weakly and strongly more powerful than CFG 1." ></td>
	<td class="line x" title="14:366	We will present a statistical incremental parsing for LTAG-spinal." ></td>
	<td class="line x" title="15:366	As far as we know, this parser is the first comprehensive attempt of efficient statistical parsing with a formal grammar with provably stronger generative power than CFG, supporting the full adjoining operation, dynamic predicate coordination, as well as non-projective dependencies 2." ></td>
	<td class="line x" title="16:366	2 LTAG-spinal and the Treebank We first briefly describe the LTAG-spinal formalism and the LTAG-spinal treebank to be used in this paper." ></td>
	<td class="line x" title="17:366	More details are reported in (Shen and Joshi, 2005)." ></td>
	<td class="line x" title="18:366	In LTAG-spinal, we have two different kinds of elementary trees, initial trees and auxiliary trees, which are the same as in LTAG." ></td>
	<td class="line x" title="19:366	However, as the name implies, an initial tree in LTAG-spinal only contains the spine from the root to the anchor, and an auxiliary tree only contains the spine and the foot node directly connected to a node on the spine." ></td>
	<td class="line x" title="20:366	Three types of operations are used to connect the elementary trees into a derivation tree, which are attachment, adjunction and conjunction." ></td>
	<td class="line x" title="21:366	We show LTAG-spinal elementary trees and operations with an example in Figure 1." ></td>
	<td class="line x" title="22:366	In Figure 1, each arc is associated with a character which represents the type of operation." ></td>
	<td class="line x" title="23:366	We use T for attach, A for adjoin, and C for conjoin." ></td>
	<td class="line x" title="24:366	1Further formal results are described in (Shen and Joshi, 2005)." ></td>
	<td class="line x" title="25:366	There is also some relationship of LTAG-spinal to the spinal form context-free tree grammar, as in (Fujiyoshi and Kasai, 2000) 2In (Riezler et al. , 2002), the MaxEnt model was used to rerank the K-best parses generated by a rule-based LFG parser." ></td>
	<td class="line x" title="26:366	811 JJ VP S JJ VP S TO CC VP *VBZNNDT XP XP WDT PRP interestingnew andseemswhichparsera meto T T T T T A T C VP XPXP XPXP Figure 1: An example in LTAG-spinal." ></td>
	<td class="line x" title="27:366	A=adjoin, T=attach, C=conjoin." ></td>
	<td class="line x" title="28:366	Attachment in LTAG-spinal is similar to sister adjunction (Chiang, 2000) in Tree Insertion Grammar (TIG) (Schabes and Waters, 1995)." ></td>
	<td class="line x" title="29:366	It represents a combination of substitution and sister adjunction." ></td>
	<td class="line x" title="30:366	The attachment operation is designed to encode the ambiguity of an argument and an adjunct." ></td>
	<td class="line x" title="31:366	Adjunction inserts part of the spine and the foot node of an auxiliary tree into to the spine of another tree." ></td>
	<td class="line x" title="32:366	The adjunction operation can effectively do wrapping, which distinguishes itself from sister adjunction." ></td>
	<td class="line x" title="33:366	It is not difficult to see that adjunction only happens on the spine of a tree." ></td>
	<td class="line x" title="34:366	This property will be exploited in the incremental parser." ></td>
	<td class="line x" title="35:366	Conjunction is similar to what was originally proposed in (Sarkar and Joshi, 1996)." ></td>
	<td class="line x" title="36:366	However, in LTAG-spinal, the conjunction operation is much easier to handle, since we only conjoin spinal elementary trees and we do not need to enumerate contraction sets for conjunction." ></td>
	<td class="line x" title="37:366	In our formalization, conjunction can be treated as a special adjunction, however, this is beyond the scope of this paper." ></td>
	<td class="line x" title="38:366	We use the LTAG-spinal treebank described in (Shen and Joshi, 2005), which was extracted from the Penn Treebank (PTB) (Marcus et al. , 1994) with Propbank (Palmer et al. , 2005) annotations." ></td>
	<td class="line x" title="39:366	2.1 Relation to Traditional LTAG LTAG-spinal preserves most of the strong generative power of LTAG." ></td>
	<td class="line x" title="40:366	It can be shown that LTAGspinal with adjoining restrictions (Joshi and Schabes, 1997) has stronger generative capacity as compared to CFG." ></td>
	<td class="line x" title="41:366	For example, there exists an LTAGspinal grammar that generates a15a10a16a18a17a20a19a21a17a23a22a18a24a25a17a27a26a28a17a30a29a31a5a33a32 a34a36a35, which is not a context-free language." ></td>
	<td class="line x" title="42:366	A spinal elementary tree is smaller than a tradition LTAG elementary tree which contains all the substitution nodes of the arguments." ></td>
	<td class="line x" title="43:366	In the LTAGspinal formalism, both arguments and adjuncts are expected to be directly attached or adjoined onto a spine." ></td>
	<td class="line x" title="44:366	In this sense, LTAG-spinal roughly satisfies the fundamental TAG hypothesis: Every syntactic dependency is expressed locally within a single elementary tree (Frank, 2002)." ></td>
	<td class="line x" title="45:366	The only difference is that, in LTAG-spinal, syntactic dependencies are represented via direct or local connections." ></td>
	<td class="line x" title="46:366	To better understand the meaning of this difference, we relate it to Franks (2002) model for how the LTAG elementary trees are constructed." ></td>
	<td class="line x" title="47:366	In Franks model, all the elementary trees are built via Marge and Move operations, starting with a local lexical array." ></td>
	<td class="line x" title="48:366	The resulting LTAG elementary trees are then combined with adjunction and substitution to build a derivation tree." ></td>
	<td class="line x" title="49:366	Thus, in a sense, the LTAG-spinal grammar opens a door to a parallel mechanism of building the elementary trees and the derivation tree." ></td>
	<td class="line x" title="50:366	The spinal templates in LTAG-spinal only contain the path of projection from the anchor to the top node." ></td>
	<td class="line x" title="51:366	A spinal template plus the root nodes of the subtrees attached to this template can be viewed as a traditional LTAG elementary tree." ></td>
	<td class="line x" title="52:366	More specifically, it encodes a set of possible elementary trees if we distinguish substitution from sister adjunction." ></td>
	<td class="line x" title="53:366	Thus, the LTAGspinal parsing model to be proposed in Section 3 can be viewed as a parser at the meta-grammar (Candito, 1998; Kinyon and Prolo, 2002) level for traditional LTAG." ></td>
	<td class="line x" title="54:366	Derivation tree construction and fullsize elementary tree filtering are processed in parallel." ></td>
	<td class="line x" title="55:366	Researches in statistical CFG parsing (Ratnaparkhi, 1997; Collins, 1999) and psycholinguistics 812 (Shieber and Johnson, 1993) showed that this strategy is desirable for NLP." ></td>
	<td class="line x" title="56:366	Furthermore, the way that we split a traditional LTAG elementary tree along the spine is similar to the method with which Evans and Weir (1997) compiled the XTAG English Grammar into finite state automata." ></td>
	<td class="line x" title="57:366	In their work, this method was designed to employ shared structure in a rule-based parser." ></td>
	<td class="line x" title="58:366	But here we extend this technique to statistical LTAG parsing." ></td>
	<td class="line x" title="59:366	2.2 Relation to Propbank In building the LTAG-spinal Treebank, the Propbank information is used in the treebank extraction." ></td>
	<td class="line x" title="60:366	As reported in (Shen and Joshi, 2005), tree transformation on PTB are employed to make it more compatible with the Propbank annotations." ></td>
	<td class="line x" title="61:366	It was shown that 8 simple patterns of the path from a predicate to an argument account for 95.5% of the total pred-arg pairs." ></td>
	<td class="line x" title="62:366	Thus, our high-quality parsing output will be very useful for semantic role labeling." ></td>
	<td class="line x" title="63:366	Arguments in Propbank are not obligatory complements." ></td>
	<td class="line x" title="64:366	Therefore, we cannot treat the Propbank arguments as the arguments in LTAG." ></td>
	<td class="line x" title="65:366	The ambiguity of argument and adjunct is reflected in the similarity of substitution and sister adjunction." ></td>
	<td class="line x" title="66:366	This is one of the reasons that we do not distinguish substitution and sister adjunction in LTAG-spinal." ></td>
	<td class="line x" title="67:366	3 Incremental Parsing We are especially interested in incremental parsing for the following two reasons." ></td>
	<td class="line x" title="68:366	Firstly, the left to right strategy used in incremental parsing gives rise to a drastic boost in speed." ></td>
	<td class="line x" title="69:366	Furthermore, there is also a strong connection between incremental parsing and psycholinguistics, and this connection is also observed in the LTAG formalism (Ferreira, 2000; Sturt and Lombardo, 2005)." ></td>
	<td class="line x" title="70:366	In recent years, there have been many interesting works on incremental or semi-incremental parsing." ></td>
	<td class="line x" title="71:366	By semi-incremental we mean the parsers that allow several rounds of left to right scans instead of one." ></td>
	<td class="line oc" title="72:366	Both left-corner strategy (Ratnaparkhi, 1997; Roark, 2001; Prolo, 2003; Henderson, 2003; Collins and Roark, 2004) and head-corner strategy (Henderson, 2000; Yamada and Matsumoto, 2003) were employed in incremental parsing." ></td>
	<td class="line x" title="73:366	The head-corner approach is more natural to the LTAG formalism (Evans and Weir, 1997)." ></td>
	<td class="line x" title="74:366	In our approach, we use a stack of derivation treelets to represent the partial parsing result." ></td>
	<td class="line x" title="75:366	Furthermore, the LTAG formalism allows us to handle non-projectivity dependencies, which cannot be generated by a CFG or a Dependency parser." ></td>
	<td class="line x" title="76:366	In fact, the idea of incremental parsing with LTAG is closely related to the work on Supertagging (Joshi and Srinivas, 1994)." ></td>
	<td class="line x" title="77:366	A supertager first assigns the correct LTAG elementary tree to each word." ></td>
	<td class="line x" title="78:366	Then a Lightweight Dependency Analyzer (LDA) (Srinivas, 1997) composes the whole derivation tree with these elementary trees." ></td>
	<td class="line x" title="79:366	We use incremental parsing to incorporate supertager and LDA dynamically." ></td>
	<td class="line x" title="80:366	The model of incremental LTAG parsing is also similar to Structured Language Modeling (SLM) in (Chelba and Jelinek, 2000)." ></td>
	<td class="line x" title="81:366	In SLM, the left context of history is represented with a stack of binary trees." ></td>
	<td class="line x" title="82:366	At each step, one computes the likelihood of the current word, its tag and the operations over the new context trees." ></td>
	<td class="line x" title="83:366	3.1 Treatment of Coordination Predicate coordination appears in about 1/6 of the sentences in PTB, therefore proper treatment of coordination, especially predicate coordination, is important to parsing of PTB." ></td>
	<td class="line x" title="84:366	Some recent results in psycholinguistic experiments (Sturt and Lombardo, 2005) showed a high degree of eagerness in building coordination structures which is absent in a bottom-up approach; A bottom-up parser waits for the second conjunct to be completed before combining the two conjuncts as for example in VP coordination, and then combine the coordinated VP with the subject of the left conjunct." ></td>
	<td class="line x" title="85:366	Psycholinguistic results suggest that the right conjunct has to have access to the subject NP of the left conjunct." ></td>
	<td class="line x" title="86:366	This can be achieved by first building the entire S on the left and then adjoining the right VP conjunct to the VP node of the left conjunct (Sturt and Lombardo, 2005)." ></td>
	<td class="line x" title="87:366	We follow the strategy suggested by the psycholinguistic experiments, treating conjoining as a special adjoining operation." ></td>
	<td class="line x" title="88:366	813 3.2 The Parsing Algorithm There are four different types of operations in our parser." ></td>
	<td class="line x" title="89:366	Three of them are described in Section 2." ></td>
	<td class="line x" title="90:366	The fourth operation is generation, which is used to generate a possible spine for a given word according to the context and the lexicon." ></td>
	<td class="line x" title="91:366	Our left to right parsing algorithm is a variant of the shift-reduce algorithm with beam-search." ></td>
	<td class="line x" title="92:366	We use a stack of disconnected derivation treelets to represent the left context." ></td>
	<td class="line x" title="93:366	When the parser reads a word, it first generates a list of possible spinal elementary trees for this word, For each elementary tree, we first push it into the stack." ></td>
	<td class="line x" title="94:366	Then we recursively pop the top two treelets from the stack and push the combined tree into the stack until we choose not to combine the top two treelets with one of the three combination operations (we can also choose not to pop anything at the beginning)." ></td>
	<td class="line x" title="95:366	Then we shift to the next word." ></td>
	<td class="line x" title="96:366	This model is called the Flex Model in this paper." ></td>
	<td class="line x" title="97:366	A potential problem with the Flex Model is that a single LTAG derivation tree can be generated by several shift-reduce derivation steps, which only differ in the order of operations." ></td>
	<td class="line x" title="98:366	For example, we have three trees a37, a38 and a39." ></td>
	<td class="line x" title="99:366	In LTAG derivation, a37 adjoins to a38, and a38 adjoins to a39 . Then we have two different shift-reduce derivations, which are a3a40a37a42a41a43a3a40a38a44a41a45a39a46a9a47a9 and a3a47a3a40a37a42a41a48a38a11a9a49a41a45a39a50a9 . Now we introduce the Eager Model, an eager evaluation strategy." ></td>
	<td class="line x" title="100:366	Any two elementary trees which are directly connected in the LTAG derivation tree are combined immediately when they can be combined in some context." ></td>
	<td class="line x" title="101:366	Furthermore, they cannot be combined afterwards, if they miss the first chance." ></td>
	<td class="line x" title="102:366	In the previous example, the parser will generate a3a47a3a40a37a44a41a51a38a11a9a52a41a53a39a50a9, while a3a40a37a54a41 a3a40a38a55a41a56a39a46a9a47a9 is ruled out." ></td>
	<td class="line x" title="103:366	Then for each LTAG derivation tree, there exists a unique left-to-right derivation." ></td>
	<td class="line x" title="104:366	The Eager Model is motived by the treatment of coordination in (Sturt and Lombardo, 2005), as we discussed in the previous section." ></td>
	<td class="line x" title="105:366	For example, we have the following two sentences." ></td>
	<td class="line x" title="106:366	1." ></td>
	<td class="line x" title="107:366	Quimby knows Tom likes Philly steak." ></td>
	<td class="line x" title="108:366	2." ></td>
	<td class="line x" title="109:366	Quimby knows Tom likes Philly steak and Jerry likes pizza." ></td>
	<td class="line x" title="110:366	Suppose we are parsing these two sentences, and for each case the current word is likes, the fourth word." ></td>
	<td class="line x" title="111:366	Now we have just the same local contexts for both cases." ></td>
	<td class="line x" title="112:366	According to the Eager Model, the parser takes the same action according to the context, which is to combine the knows tree and the likes tree." ></td>
	<td class="line x" title="113:366	For sentence 2, the second likes tree will be conjoined with the first likes tree later." ></td>
	<td class="line x" title="114:366	This is compatible with the psycholinguistic preference." ></td>
	<td class="line x" title="115:366	In the following section, we will explain the parsing mechanism for the Eager Model with an example." ></td>
	<td class="line x" title="116:366	The Flex Model is similar except that the order of operations is flexible to some extent." ></td>
	<td class="line x" title="117:366	3.3 An Example Figure 2 shows the left to right parsing of the phrase a parser which seems new and interesting to me with the Eager Model." ></td>
	<td class="line x" title="118:366	In Figure 2, each arc is associated with a number and a character." ></td>
	<td class="line x" title="119:366	The number represents the order of operation, and the character stands for the type of operation as in Figure 1." ></td>
	<td class="line x" title="120:366	Furthermore we use G to represent Generate." ></td>
	<td class="line x" title="121:366	In step 1 and 2, two disconnected spines are generated for a and parser." ></td>
	<td class="line x" title="122:366	The spine for a is attached to the spine for parser on the NP node in step 3." ></td>
	<td class="line x" title="123:366	In step 6, the spine for new, the first conjunct of the predicate coordination, is generated." ></td>
	<td class="line x" title="124:366	Then the auxiliary tree for seems is adjoined to the spine for new at the node VP." ></td>
	<td class="line x" title="125:366	the latter is further combined with which, and is attached to the tree for parser." ></td>
	<td class="line x" title="126:366	In step 13, the conjoin operation is used to combine the treelet anchored on new and the treelet anchored on interesting." ></td>
	<td class="line x" title="127:366	Alignments between the two spines are built, through which argument sharing is implemented in an implicit and underspecified way." ></td>
	<td class="line x" title="128:366	In step 15, for the spine for to, the visible nodes of the conjoined treelet include nodes on some auxiliary trees adjoined on the left of the spines, like the root VP node for seems." ></td>
	<td class="line x" title="129:366	In this way, a non-projective structure is generated, which is just the same as the wrapping adjoining in LTAG." ></td>
	<td class="line x" title="130:366	3.4 Machine Learning Algorithm Many machine learning algorithms have been successfully applied to parsing, incremental parsing, or shallow parsing (Ratnaparkhi, 1997; Punyakanok and Roth, 2001; Lafferty et al. , 2001; Taskar et al. , 2003), which can be applied to our incremental parsing algorithm." ></td>
	<td class="line x" title="131:366	814 JJ VP S JJ VP S TO PRP CC VP *VBZNN WDTDT XP XP interestingnew andseemswhichparsera meto 1 2 4 5 6 11 16 17 GG G G G 14 G G T T T T 9 8 7 G 10 G 12 T 3 A 15 T 13 C VP XPXP XP XP Figure 2: Incremental parsing with Eager Model." ></td>
	<td class="line x" title="132:366	A=adjoin, T=attach, C=conjoin, G=generate In this paper, we use the perceptron-like algorithm proposed in (Collins, 2002) which does not suffer from the label bias problem, and is fast in training." ></td>
	<td class="line oc" title="133:366	We also employ the voted perceptron algorithm (Freund and Schapire, 1999) and the early update technique as in (Collins and Roark, 2004)." ></td>
	<td class="line x" title="134:366	3.5 Features Features are defined in the format of (operation, main spine, child spine, spine node, context), where the spine node is the node on the main spine onto which the child spine is attached or adjoined." ></td>
	<td class="line x" title="135:366	For generate, child spine and spine node are undefined, and for conjoin spine node is undefined." ></td>
	<td class="line x" title="136:366	context describes the constituent label or lexical item associated with a certain node." ></td>
	<td class="line x" title="137:366	The context of an operation includes the top two treelets involved in the operation as well as the two closest words on both sides of the current word." ></td>
	<td class="line x" title="138:366	a57 Context for generate : The (-2, 2) window in the flat sentence." ></td>
	<td class="line x" title="139:366	The visible 3 spines on the topmost treelet." ></td>
	<td class="line x" title="140:366	a57 Context for attach and adjoin : The (0, 2) window in the flat sentence." ></td>
	<td class="line x" title="141:366	The most recent spine previously attached or adjoined to the same location on the main spine." ></td>
	<td class="line x" title="142:366	The leftmost child spine attached to the child spines." ></td>
	<td class="line x" title="143:366	The spines that are visible before the operation and become invisible after the operation." ></td>
	<td class="line x" title="144:366	a57 Context for conjoin : The (0, 2) window in the flat sentence." ></td>
	<td class="line x" title="145:366	3The details are presented in (Shen, 2005)." ></td>
	<td class="line x" title="146:366	The leftmost child spine attached to the main spine, which is the first adjunct." ></td>
	<td class="line x" title="147:366	The two leftmost children spines attached to the child spine, which is the current adjunct." ></td>
	<td class="line x" title="148:366	We have about 1.4M features extracted from the gold-standard parses, and about 600K features dynamically extracted from the generated parses in 10 rounds of training with the Eager Model." ></td>
	<td class="line x" title="149:366	4 Experiments and Analysis We use the LTAG-spinal treebank reported in (Shen and Joshi, 2005)." ></td>
	<td class="line x" title="150:366	The LTAG-spinal parse for the 39434 sentences extracted from WSJ section 2-21 are used as the training data." ></td>
	<td class="line x" title="151:366	Section 24 is used as the development data." ></td>
	<td class="line x" title="152:366	Section 23 are used for test4." ></td>
	<td class="line x" title="153:366	We use syntactic dependency for evaluation." ></td>
	<td class="line x" title="154:366	It is worth mentioning that, for predicate coordination, we define the dependency on the parent of the coordination structure and each of the conjunct predicate." ></td>
	<td class="line x" title="155:366	For example, in Figure 1, we have dependency relation on (parser, new) and (parser, interesting)." ></td>
	<td class="line x" title="156:366	Compared with other dependency parsers on PTB, the dependency defined on LTAG-spinal reveals deeper relations because of the treatment of traditional adjoining and predicate coordination described above." ></td>
	<td class="line x" title="157:366	In the community of parsing, labeled recall and labeled precision on phrase structures are often used for evaluation." ></td>
	<td class="line x" title="158:366	However, in our experiments we cannot evaluate our parser with respect to the phrase structures in PTB." ></td>
	<td class="line x" title="159:366	As shown in (Shen and Joshi, 2005), various irrecoverable tree transformations 4The LTAG-spinal treebank contains 2401 out of 2416 sentences in section 23." ></td>
	<td class="line x" title="160:366	815 0.7 0.75 0.8 0.85 0.9 0 2 4 6 8 10 f-score a58 # iteration dev-voted dev train Figure 3: f-score of syntactic dependency on the development data with the Eager Model were used to extract the LTAG-spinal treebank according the Propbank annotation on PTB." ></td>
	<td class="line x" title="161:366	Therefore, we use syntactic dependency for evaluation." ></td>
	<td class="line x" title="162:366	4.1 Eager vs. Flex We first train our incremental parser with Eager Model and Flex Model respectively." ></td>
	<td class="line x" title="163:366	In the training, beam width is set to 10." ></td>
	<td class="line x" title="164:366	Lexical features are limited to words appearing for at least 5 times in the training data." ></td>
	<td class="line x" title="165:366	Figure 3 and Figure 4 show the learning curves on the training and the development data." ></td>
	<td class="line x" title="166:366	The X axis represents the number of iterations of training, and the Y axis represents the f-score of dependency with respected to the LTAG derivation tree." ></td>
	<td class="line x" title="167:366	Since early update is used, the f-score on the training data is very low at the beginning." ></td>
	<td class="line x" title="168:366	In both cases, the voted weights provide an f-score which is more than 3% higher." ></td>
	<td class="line x" title="169:366	The voted results converge faster and are more stable." ></td>
	<td class="line x" title="170:366	The result with Flex Model is 0.6% higher than the one with Eager Model, but the parsing time is much longer with Flex Model as we will show later." ></td>
	<td class="line x" title="171:366	We use the voted weights obtained after 10 rounds of iteration for the evaluation on the test data." ></td>
	<td class="line x" title="172:366	We achieve an f-score of 88.7% on dependency with the Eager Model, and 89.3% with the Flex Model." ></td>
	<td class="line x" title="173:366	The Flex Model achieves better performance because it allows the decision of operation to be delayed until there is enough context information." ></td>
	<td class="line x" title="174:366	4.2 K-Best Parsing The next experiment is on K-best parsing." ></td>
	<td class="line x" title="175:366	As a first attempt, we just use the same algorithm as in the pre0.7 0.75 0.8 0.85 0.9 0 2 4 6 8 10 f-score a58 # iteration dev-voted dev train Figure 4: f-score of syntactic dependency on the development data with the Flex Model Table 1: F-score of the oracle parse in the 10best parses on the development data with the Eager Model algorithm f-score% top (eager) 87.3 oracle (eager) 88.5 top (eager+combined parses) 87.4 oracle (eager+combined parses) 91.0 vious section, except that we study the oracle parse, or the best parse, among the top 10 parses." ></td>
	<td class="line x" title="176:366	The fscore on the oracle in top 10 in the development data is 88.5%, while the f-score of the top candidate is 87.3%, as shown in Table 1." ></td>
	<td class="line x" title="177:366	However, we are not satisfied with the score on oracle, which is not good enough for post-processing, i.e. parse reranking." ></td>
	<td class="line x" title="178:366	We notice that from a single partial derivation we can generate a large set of different partial derivations, just by combining the elementary tree of the next word." ></td>
	<td class="line x" title="179:366	It is easy to see that these similar derivations may use up the search beam quickly, which is not good for parse search." ></td>
	<td class="line x" title="180:366	Many of the new derivations share the same dependency structure." ></td>
	<td class="line x" title="181:366	So we revised our learning procedure by combining derivations with the same dependency structure before each shift operation." ></td>
	<td class="line x" title="182:366	We repeated the K-best parsing experiments by using Combined Parses as described above, and achieved significant improvement on the oracle, as shown in Table 1." ></td>
	<td class="line x" title="183:366	Figure 5 shows the f-score of the oracle on K-best parsing using combined parses on the test data." ></td>
	<td class="line x" title="184:366	For each K-best oracle test, we set the beam width to K 816 0.9 0.905 0.91 0.915 0.92 0.925 0.93 0.935 0.94 0.945 0.95 0 20 40 60 80 100 f-score a58 k-best eager + combined flex + combined Figure 5: f-score of the oracle on the test data Table 2: Speed of parsing on the test data set." ></td>
	<td class="line x" title="185:366	Here cp?" ></td>
	<td class="line x" title="186:366	= whether the method of Combined Parses is used; sen/sec = sentence per second; top = top candidate given by the parser; oracle = oracle of the Kbest parses where K equals the width of the beam." ></td>
	<td class="line x" title="187:366	model cp?" ></td>
	<td class="line x" title="188:366	beam sen/sec f-score% single best top flex no 10 0.37 89.3 eager no 10 0.79 88.7 K-best oracle eager yes 10 0.62 92.2 eager yes 20 0.31 92.9 eager yes 30 0.22 93.2 eager yes 50 0.13 93.7 eager yes 100 0.07 94.2 in parsing." ></td>
	<td class="line x" title="189:366	The f-score of oracle in 100-best parsing is 94.2% with the Eager Model + Combined Parses." ></td>
	<td class="line x" title="190:366	4.3 Speed of Parsing Efficiency is important to the application of incremental parsing." ></td>
	<td class="line x" title="191:366	This set of experiments is related to the speed of our parser on single best and Kbest parsing with both the Eager Model and the Flex Model." ></td>
	<td class="line x" title="192:366	All the experiments are performed on a Linux node with two 1.13GHz PIII CPUs and 2GB RAM." ></td>
	<td class="line x" title="193:366	The parser is coded in Java." ></td>
	<td class="line x" title="194:366	Table 2 shows that the Eager Model is more than two times faster than the Flex Model, as we expected." ></td>
	<td class="line x" title="195:366	The time spent on K-best parsing is proportional to the beam width." ></td>
	<td class="line x" title="196:366	5 Discussion and Future Work The parser proposed in this paper is an incremental parser, so the accuracy on dependency is lower than that for chart parsers, for example like those reported in (Collins, 1999; Charniak, 2000)." ></td>
	<td class="line x" title="197:366	5 However, it should be noted that the dependencies computed by our parser are deeper than those calculated by parsers working directly on PTB." ></td>
	<td class="line x" title="198:366	This is due to the treatment of adjunction and coordination." ></td>
	<td class="line x" title="199:366	On the other hand, the LTAG-spinal treebank used in this paper shows a high degree of compatibility with the Propbank, as shown in (Shen and Joshi, 2005), so the LTAG derivations given by the parser are very useful for predicate-argument recognition." ></td>
	<td class="line x" title="200:366	We plan to improve the parsing performance by reranking and extend our work to semantic parsing (Mooney, 2004)." ></td>
	<td class="line x" title="201:366	Another interesting topic is whether this parser can be applied to languages which have various long-distance scrambling, as in German." ></td>
	<td class="line x" title="202:366	It appears that by carefully modifying the definition of visible spines, we can represent scrambling structures, which at present can only be represented by MultiComponent TAG (Becker et al. , 1991)." ></td>
	<td class="line x" title="203:366	6 Conclusions In this paper, we present an efficient incremental parser for LTAG-spinal, a variant of LTAG which is both linguistically and psycholinguistically motivated." ></td>
	<td class="line x" title="204:366	As far as we know, the statistical incremental parser proposed in this paper is the first comprehensive attempt of efficient statistical parsing with a formal grammar with provably stronger generative power than CFG, supporting the adjoining operation, dynamic predicate coordination, as well as non-projective dependencies." ></td>
	<td class="line x" title="205:366	We have trained and tested our parser on the LTAG-spinal treebank, extracted from the Penn Treebank with Propbank annotation, Using gold standard POS tags as part of the input, the parser achieves an f-score of 89.3% for syntactic dependency on section 23 of PTB." ></td>
	<td class="line x" title="206:366	Because of the treatment of adjunction and predicate coordination, These dependencies, which are defined on LTAGspinal derivation trees, are deeper than the dependencies extracted from PTB alone with head rules." ></td>
	<td class="line x" title="207:366	5We plan to work on a chart parser for LTAG-spinal." ></td>
	<td class="line x" title="208:366	817 References T. Becker, A. K. Joshi, and O. Rambow." ></td>
	<td class="line x" title="209:366	1991." ></td>
	<td class="line x" title="210:366	Long distance scrambling and Tree Adjoining Grammars." ></td>
	<td class="line x" title="211:366	In EACL 1991." ></td>
	<td class="line x" title="212:366	M. Candito." ></td>
	<td class="line x" title="213:366	1998." ></td>
	<td class="line x" title="214:366	Building parallel ltag for french and italian." ></td>
	<td class="line x" title="215:366	In ACL-COLING 1998." ></td>
	<td class="line x" title="216:366	E. Charniak." ></td>
	<td class="line x" title="217:366	2000." ></td>
	<td class="line x" title="218:366	A maximum-entropy-inspired parser." ></td>
	<td class="line x" title="219:366	In NAACL 2000." ></td>
	<td class="line x" title="220:366	C. Chelba and F. Jelinek." ></td>
	<td class="line x" title="221:366	2000." ></td>
	<td class="line x" title="222:366	Structured language modeling." ></td>
	<td class="line x" title="223:366	Computer Speech and Language, 14(4):283332." ></td>
	<td class="line x" title="224:366	D. Chiang." ></td>
	<td class="line x" title="225:366	2000." ></td>
	<td class="line x" title="226:366	Statistical Parsing with an AutomaticallyExtracted Tree Adjoining Grammar." ></td>
	<td class="line x" title="227:366	In ACL 2000." ></td>
	<td class="line x" title="228:366	M. Collins and B. Roark." ></td>
	<td class="line x" title="229:366	2004." ></td>
	<td class="line x" title="230:366	Incremental parsing with the perceptron algorithm." ></td>
	<td class="line x" title="231:366	In ACL 2004." ></td>
	<td class="line x" title="232:366	M. Collins." ></td>
	<td class="line x" title="233:366	1999." ></td>
	<td class="line x" title="234:366	Head-Driven Statistical Models for Natural Language Parsing." ></td>
	<td class="line x" title="235:366	Ph.D. thesis, University of Pennsylvania." ></td>
	<td class="line x" title="236:366	M. Collins." ></td>
	<td class="line x" title="237:366	2002." ></td>
	<td class="line x" title="238:366	Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms." ></td>
	<td class="line x" title="239:366	In EMNLP 2002." ></td>
	<td class="line x" title="240:366	R. Evans and D. Weir." ></td>
	<td class="line x" title="241:366	1997." ></td>
	<td class="line x" title="242:366	Automaton-based parsing for lexicalized grammars." ></td>
	<td class="line x" title="243:366	In IWPT 1997." ></td>
	<td class="line x" title="244:366	F. Ferreira." ></td>
	<td class="line x" title="245:366	2000." ></td>
	<td class="line x" title="246:366	Syntax in language production: An approach using tree-adjoining grammars." ></td>
	<td class="line x" title="247:366	In L. Wheeldon, editor, Aspects of Language Production." ></td>
	<td class="line x" title="248:366	MIT Press." ></td>
	<td class="line x" title="249:366	R. Frank." ></td>
	<td class="line x" title="250:366	2002." ></td>
	<td class="line x" title="251:366	Phrase Structure Composition and Syntactic Dependencies." ></td>
	<td class="line x" title="252:366	MIT Press." ></td>
	<td class="line x" title="253:366	Y. Freund and R. E. Schapire." ></td>
	<td class="line x" title="254:366	1999." ></td>
	<td class="line x" title="255:366	Large margin classification using the perceptron algorithm." ></td>
	<td class="line x" title="256:366	Machine Learning, 37(3):277296." ></td>
	<td class="line x" title="257:366	A. Fujiyoshi and T. Kasai." ></td>
	<td class="line x" title="258:366	2000." ></td>
	<td class="line x" title="259:366	Spinal-formed context-free tree grammars." ></td>
	<td class="line x" title="260:366	Theory Computing Systems, 33(1)." ></td>
	<td class="line x" title="261:366	J. Henderson." ></td>
	<td class="line x" title="262:366	2000." ></td>
	<td class="line x" title="263:366	A neural network parser that handles sparse data." ></td>
	<td class="line x" title="264:366	In IWPT 2000." ></td>
	<td class="line x" title="265:366	J. Henderson." ></td>
	<td class="line x" title="266:366	2003." ></td>
	<td class="line x" title="267:366	Generative versus discriminative models for statistical left-corner parsing." ></td>
	<td class="line x" title="268:366	In IWPT 2003." ></td>
	<td class="line x" title="269:366	A. K. Joshi and Y. Schabes." ></td>
	<td class="line x" title="270:366	1997." ></td>
	<td class="line x" title="271:366	Tree-adjoining grammars." ></td>
	<td class="line x" title="272:366	In G. Rozenberg and A. Salomaa, editors, Handbook of Formal Languages, volume 3, pages 69  124." ></td>
	<td class="line x" title="273:366	Springer." ></td>
	<td class="line x" title="274:366	A. K. Joshi and B. Srinivas." ></td>
	<td class="line x" title="275:366	1994." ></td>
	<td class="line x" title="276:366	Disambiguation of super parts of speech (or supertags): Almost parsing." ></td>
	<td class="line x" title="277:366	In COLING 1994." ></td>
	<td class="line x" title="278:366	A. Kinyon and C. Prolo." ></td>
	<td class="line x" title="279:366	2002." ></td>
	<td class="line x" title="280:366	A classification of grammar development strategies." ></td>
	<td class="line x" title="281:366	In COLING 2002 Workshop: Grammar Engineering and Evaluation." ></td>
	<td class="line x" title="282:366	J. Lafferty, A. McCallum, and F. Pereira." ></td>
	<td class="line x" title="283:366	2001." ></td>
	<td class="line x" title="284:366	Conditional random fields: Probabilistic models for segmenting and labeling sequence data." ></td>
	<td class="line x" title="285:366	In ICML 2001." ></td>
	<td class="line x" title="286:366	A. Lavelli and G. Satta." ></td>
	<td class="line x" title="287:366	1991." ></td>
	<td class="line x" title="288:366	Bidirectional parsing of lexicalized tree adjoining grammars." ></td>
	<td class="line x" title="289:366	In EACL 1991." ></td>
	<td class="line x" title="290:366	M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz." ></td>
	<td class="line x" title="291:366	1994." ></td>
	<td class="line x" title="292:366	Building a large annotated corpus of English: The Penn Treebank." ></td>
	<td class="line x" title="293:366	Computational Linguistics, 19(2):313330." ></td>
	<td class="line x" title="294:366	R. Mooney." ></td>
	<td class="line x" title="295:366	2004." ></td>
	<td class="line x" title="296:366	Learning semantic parsers: An important but under-studied problem." ></td>
	<td class="line x" title="297:366	In AAAI 2004 Spring Symposium on Language Learning: An Interdisciplinary Perspective." ></td>
	<td class="line x" title="298:366	M. Palmer, D. Gildea, and P. Kingsbury." ></td>
	<td class="line x" title="299:366	2005." ></td>
	<td class="line x" title="300:366	The proposition bank: An annotated corpus of semantic roles." ></td>
	<td class="line x" title="301:366	Computational Linguistics, 31(1)." ></td>
	<td class="line x" title="302:366	C. Prolo." ></td>
	<td class="line x" title="303:366	2003." ></td>
	<td class="line x" title="304:366	LR Parsing for Tree Adjoining Grammars and its Application to Corpus-based Natural Language Parsing." ></td>
	<td class="line x" title="305:366	Ph.D. thesis, University of Pennsylvania." ></td>
	<td class="line x" title="306:366	V. Punyakanok and D. Roth." ></td>
	<td class="line x" title="307:366	2001." ></td>
	<td class="line x" title="308:366	The use of classifiers in sequential inference." ></td>
	<td class="line x" title="309:366	In NIPS 2001." ></td>
	<td class="line x" title="310:366	A. Ratnaparkhi." ></td>
	<td class="line x" title="311:366	1997." ></td>
	<td class="line x" title="312:366	A linear observed time statistical parser based on maximum entropy models." ></td>
	<td class="line x" title="313:366	In EMNLP 1997." ></td>
	<td class="line x" title="314:366	S. Riezler, T. King, R. Kaplan, R. Crouch, J. Maxwell, and M. Johnson." ></td>
	<td class="line x" title="315:366	2002." ></td>
	<td class="line x" title="316:366	Parsing the wall street journal using a lexical-functional grammar and discriminative estimation techniques." ></td>
	<td class="line x" title="317:366	In ACL 2002." ></td>
	<td class="line x" title="318:366	B. Roark." ></td>
	<td class="line x" title="319:366	2001." ></td>
	<td class="line x" title="320:366	Probabilistic top-down parsing and language modeling." ></td>
	<td class="line x" title="321:366	Computational Linguistics, 27(2):249276." ></td>
	<td class="line x" title="322:366	A. Sarkar and A. K. Joshi." ></td>
	<td class="line x" title="323:366	1996." ></td>
	<td class="line x" title="324:366	Coordination in tree adjoining grammars." ></td>
	<td class="line x" title="325:366	In COLING 1996." ></td>
	<td class="line x" title="326:366	Y. Schabes and R. C. Waters." ></td>
	<td class="line x" title="327:366	1995." ></td>
	<td class="line x" title="328:366	A cubic-time, parsable formalism that lexicalizes context-free grammar without changing the trees produced." ></td>
	<td class="line x" title="329:366	Computational Linguistics, 21(4)." ></td>
	<td class="line x" title="330:366	L. Shen and A. K. Joshi." ></td>
	<td class="line x" title="331:366	2005." ></td>
	<td class="line x" title="332:366	Building an LTAG treebank." ></td>
	<td class="line x" title="333:366	Technical Report MS-CIS-05-15, CIS Dept. , UPenn." ></td>
	<td class="line x" title="334:366	L. Shen." ></td>
	<td class="line x" title="335:366	2005." ></td>
	<td class="line x" title="336:366	Statistical Natural Language Processing with Lexicalized Tree Adjoining Grammar." ></td>
	<td class="line x" title="337:366	Ph.D. proposal, University of Pennsylvania." ></td>
	<td class="line x" title="338:366	S. Shieber and M. Johnson." ></td>
	<td class="line x" title="339:366	1993." ></td>
	<td class="line x" title="340:366	Variations on incremental interpretation." ></td>
	<td class="line x" title="341:366	Journal of Psycholinguistic Research, 22(2):287318." ></td>
	<td class="line x" title="342:366	B. Srinivas." ></td>
	<td class="line x" title="343:366	1997." ></td>
	<td class="line x" title="344:366	Performance evaluation of supertagging for partial parsing." ></td>
	<td class="line x" title="345:366	In IWPT 1997." ></td>
	<td class="line x" title="346:366	P. Sturt and V. Lombardo." ></td>
	<td class="line x" title="347:366	2005." ></td>
	<td class="line x" title="348:366	Processing coordinated structures: Incrementality and connectedness." ></td>
	<td class="line x" title="349:366	Cognitive Science, to appear." ></td>
	<td class="line x" title="350:366	B. Taskar, C. Guestrin, and D. Koller." ></td>
	<td class="line x" title="351:366	2003." ></td>
	<td class="line x" title="352:366	Max-margin markov networks." ></td>
	<td class="line x" title="353:366	In NIPS 2003." ></td>
	<td class="line x" title="354:366	G. van Noord." ></td>
	<td class="line x" title="355:366	1994." ></td>
	<td class="line x" title="356:366	Head corner parsing for TAG." ></td>
	<td class="line x" title="357:366	Computational Intelligence, 10(4)." ></td>
	<td class="line x" title="358:366	K. Vijay-Shanker and A. K. Joshi." ></td>
	<td class="line x" title="359:366	1985." ></td>
	<td class="line x" title="360:366	Some computational properties of tree adjoining grammars." ></td>
	<td class="line x" title="361:366	In ACL 1985." ></td>
	<td class="line x" title="362:366	H. Yamada and Y. Matsumoto." ></td>
	<td class="line x" title="363:366	2003." ></td>
	<td class="line x" title="364:366	Statistical dependency analysis with Support Vector Machines." ></td>
	<td class="line x" title="365:366	In IWPT 2003." ></td>
	<td class="line x" title="366:366	818" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P05-1012
Online Large-Margin Training Of Dependency Parsers
McDonald, Ryan;Crammer, Koby;Pereira, Fernando C. N.;"></td>
	<td class="line x" title="1:209	Proceedings of the 43rd Annual Meeting of the ACL, pages 9198, Ann Arbor, June 2005." ></td>
	<td class="line x" title="2:209	c2005 Association for Computational Linguistics Online Large-Margin Training of Dependency Parsers Ryan McDonald Koby Crammer Fernando Pereira Department of Computer and Information Science University of Pennsylvania Philadelphia, PA {ryantm,crammer,pereira}@cis.upenn.edu Abstract We present an effective training algorithm for linearly-scored dependency parsers that implements online largemargin multi-class training (Crammer and Singer, 2003; Crammer et al. , 2003) on top of efficient parsing techniques for dependency trees (Eisner, 1996)." ></td>
	<td class="line x" title="3:209	The trained parsers achieve a competitive dependency accuracy for both English and Czech with no language specific enhancements." ></td>
	<td class="line x" title="4:209	1 Introduction Research on training parsers from annotated data has for the most part focused on models and training algorithms for phrase structure parsing." ></td>
	<td class="line x" title="5:209	The best phrase-structure parsing models represent generatively the joint probability P(x,y) of sentence x having the structure y (Collins, 1999; Charniak, 2000)." ></td>
	<td class="line x" title="6:209	Generative parsing models are very convenient because training consists of computing probability estimates from counts of parsing events in the training set." ></td>
	<td class="line x" title="7:209	However, generative models make complicated and poorly justified independence assumptions and estimations, so we might expect better performance from discriminatively trained models, as has been shown for other tasks like document classification (Joachims, 2002) and shallow parsing (Sha and Pereira, 2003)." ></td>
	<td class="line x" title="8:209	Ratnaparkhis conditional maximum entropy model (Ratnaparkhi, 1999), trained to maximize conditional likelihood P(y|x) of the training data, performed nearly as well as generative models of the same vintage even though it scores parsing decisions in isolation and thus may suffer from the label bias problem (Lafferty et al. , 2001)." ></td>
	<td class="line oc" title="9:209	Discriminatively trained parsers that score entire trees for a given sentence have only recently been investigated (Riezler et al. , 2002; Clark and Curran, 2004; Collins and Roark, 2004; Taskar et al. , 2004)." ></td>
	<td class="line x" title="10:209	The most likely reason for this is that discriminative training requires repeatedly reparsing the training corpus with the current model to determine the parameter updates that will improve the training criterion." ></td>
	<td class="line x" title="11:209	The reparsing cost is already quite high for simple context-free models with O(n3) parsing complexity, but it becomes prohibitive for lexicalized grammars with O(n5) parsing complexity." ></td>
	<td class="line x" title="12:209	Dependency trees are an alternative syntactic representation with a long history (Hudson, 1984)." ></td>
	<td class="line x" title="13:209	Dependency trees capture important aspects of functional relationships between words and have been shown to be useful in many applications including relation extraction (Culotta and Sorensen, 2004), paraphrase acquisition (Shinyama et al. , 2002) and machine translation (Ding and Palmer, 2005)." ></td>
	<td class="line x" title="14:209	Yet, they can be parsed in O(n3) time (Eisner, 1996)." ></td>
	<td class="line x" title="15:209	Therefore, dependency parsing is a potential sweet spot that deserves investigation." ></td>
	<td class="line x" title="16:209	We focus here on projective dependency trees in which a word is the parent of all of its arguments, and dependencies are non-crossing with respect to word order (see Figure 1)." ></td>
	<td class="line x" title="17:209	However, there are cases where crossing dependencies may occur, as is the case for Czech (Hajic, 1998)." ></td>
	<td class="line x" title="18:209	Edges in a dependency tree may be typed (for instance to indicate grammatical function)." ></td>
	<td class="line x" title="19:209	Though we focus on the simpler non-typed 91 root John hit the ball with the bat Figure 1: An example dependency tree." ></td>
	<td class="line x" title="20:209	case, all algorithms are easily extendible to typed structures." ></td>
	<td class="line x" title="21:209	The following work on dependency parsing is most relevant to our research." ></td>
	<td class="line x" title="22:209	Eisner (1996) gave a generative model with a cubic parsing algorithm based on an edge factorization of trees." ></td>
	<td class="line x" title="23:209	Yamada and Matsumoto (2003) trained support vector machines (SVM) to make parsing decisions in a shift-reduce dependency parser." ></td>
	<td class="line x" title="24:209	As in Ratnaparkhis parser, the classifiers are trained on individual decisions rather than on the overall quality of the parse." ></td>
	<td class="line x" title="25:209	Nivre and Scholz (2004) developed a history-based learning model." ></td>
	<td class="line x" title="26:209	Their parser uses a hybrid bottom-up/topdown linear-time heuristic parser and the ability to label edges with semantic types." ></td>
	<td class="line x" title="27:209	The accuracy of their parser is lower than that of Yamada and Matsumoto (2003)." ></td>
	<td class="line x" title="28:209	We present a new approach to training dependency parsers, based on the online large-margin learning algorithms of Crammer and Singer (2003) and Crammer et al.(2003)." ></td>
	<td class="line x" title="30:209	Unlike the SVM parser of Yamada and Matsumoto (2003) and Ratnaparkhis parser, our parsers are trained to maximize the accuracy of the overall tree." ></td>
	<td class="line oc" title="31:209	Our approach is related to those of Collins and Roark (2004) and Taskar et al.(2004) for phrase structure parsing." ></td>
	<td class="line oc" title="33:209	Collins and Roark (2004) presented a linear parsing model trained with an averaged perceptron algorithm." ></td>
	<td class="line o" title="34:209	However, to use parse features with sufficient history, their parsing algorithm must prune heuristically most of the possible parses." ></td>
	<td class="line x" title="35:209	Taskar et al.(2004) formulate the parsing problem in the large-margin structured classification setting (Taskar et al. , 2003), but are limited to parsing sentences of 15 words or less due to computation time." ></td>
	<td class="line n" title="37:209	Though these approaches represent good first steps towards discriminatively-trained parsers, they have not yet been able to display the benefits of discriminative training that have been seen in namedentity extraction and shallow parsing." ></td>
	<td class="line x" title="38:209	Besides simplicity, our method is efficient and accurate, as we demonstrate experimentally on English and Czech treebank data." ></td>
	<td class="line x" title="39:209	2 System Description 2.1 Definitions and Background In what follows, the generic sentence is denoted by x (possibly subscripted); the ith word of x is denoted by xi." ></td>
	<td class="line x" title="40:209	The generic dependency tree is denoted by y. If y is a dependency tree for sentence x, we write (i,j)  y to indicate that there is a directed edge from word xi to word xj in the tree, that is, xi is the parent of xj." ></td>
	<td class="line x" title="41:209	T = {(xt,yt)}Tt=1 denotes the training data." ></td>
	<td class="line x" title="42:209	We follow the edge based factorization method of Eisner (1996) and define the score of a dependency tree as the sum of the score of all edges in the tree, s(x,y) = summationdisplay (i,j)y s(i,j) = summationdisplay (i,j)y w  f(i,j) where f(i,j) is a high-dimensional binary feature representation of the edge from xi to xj." ></td>
	<td class="line x" title="43:209	For example, in the dependency tree of Figure 1, the following feature would have a value of 1: f(i,j) = braceleftbigg 1 if x i=hit and xj=ball 0 otherwise." ></td>
	<td class="line x" title="44:209	In general, any real-valued feature may be used, but we use binary features for simplicity." ></td>
	<td class="line x" title="45:209	The feature weights in the weight vector w are the parameters that will be learned during training." ></td>
	<td class="line x" title="46:209	Our training algorithms are iterative." ></td>
	<td class="line x" title="47:209	We denote by w(i) the weight vector after the ith training iteration." ></td>
	<td class="line x" title="48:209	Finally we define dt(x) as the set of possible dependency trees for the input sentence x and bestk(x;w) as the set of k dependency trees in dt(x) that are given the highest scores by weight vector w, with ties resolved by an arbitrary but fixed rule." ></td>
	<td class="line x" title="49:209	Three basic questions must be answered for models of this form: how to find the dependency tree y with highest score for sentence x; how to learn an appropriate weight vector w from the training data; and finally, what feature representation f(i,j) should be used." ></td>
	<td class="line x" title="50:209	The following sections address each of these questions." ></td>
	<td class="line x" title="51:209	2.2 Parsing Algorithm Given a feature representation for edges and a weight vector w, we seek the dependency tree or 92 h1 h1 h2 h2  s h1 h1 r r+1 h2 h2 t h1 h1 h2 h2  s h1 h1 h2 h2 t h1 h1 s h1 h1 t Figure 2: O(n3) algorithm of Eisner (1996), needs to keep 3 indices at any given stage." ></td>
	<td class="line x" title="52:209	trees that maximize the score function, s(x,y)." ></td>
	<td class="line x" title="53:209	The primary difficulty is that for a given sentence of length n there are exponentially many possible dependency trees." ></td>
	<td class="line x" title="54:209	Using a slightly modified version of a lexicalized CKY chart parsing algorithm, it is possible to generate and represent these sentences in a forest that is O(n5) in size and takes O(n5) time to create." ></td>
	<td class="line x" title="55:209	Eisner (1996) made the observation that if the head of each chart item is on the left or right periphery, then it is possible to parse in O(n3)." ></td>
	<td class="line x" title="56:209	The idea is to parse the left and right dependents of a word independently and combine them at a later stage." ></td>
	<td class="line x" title="57:209	This removes the need for the additional head indices of the O(n5) algorithm and requires only two additional binary variables that specify the direction of the item (either gathering left dependents or gathering right dependents) and whether an item is complete (available to gather more dependents)." ></td>
	<td class="line x" title="58:209	Figure 2 shows the algorithm schematically." ></td>
	<td class="line x" title="59:209	As with normal CKY parsing, larger elements are created bottom-up from pairs of smaller elements." ></td>
	<td class="line x" title="60:209	Eisner showed that his algorithm is sufficient for both searching the space of dependency parses and, with slight modification, finding the highest scoring tree y for a given sentence x under the edge factorization assumption." ></td>
	<td class="line x" title="61:209	Eisner and Satta (1999) give a cubic algorithm for lexicalized phrase structures." ></td>
	<td class="line x" title="62:209	However, it only works for a limited class of languages in which tree spines are regular." ></td>
	<td class="line x" title="63:209	Furthermore, there is a large grammar constant, which is typically in the thousands for treebank parsers." ></td>
	<td class="line x" title="64:209	2.3 Online Learning Figure 3 gives pseudo-code for the generic online learning setting." ></td>
	<td class="line x" title="65:209	A single training instance is considered on each iteration, and parameters updated by applying an algorithm-specific update rule to the instance under consideration." ></td>
	<td class="line x" title="66:209	The algorithm in Figure 3 returns an averaged weight vector: an auxiliary weight vector v is maintained that accumulates Training data: T = {(xt,yt)}Tt=1 1." ></td>
	<td class="line x" title="67:209	w0 = 0; v = 0; i = 0 2." ></td>
	<td class="line x" title="68:209	for n : 1N 3." ></td>
	<td class="line x" title="69:209	for t : 1T 4." ></td>
	<td class="line x" title="70:209	w(i+1) = update w(i) according to instance (xt,yt) 5." ></td>
	<td class="line x" title="71:209	v = v + w(i+1) 6." ></td>
	<td class="line x" title="72:209	i = i + 1 7." ></td>
	<td class="line x" title="73:209	w = v/(N  T) Figure 3: Generic online learning algorithm." ></td>
	<td class="line x" title="74:209	the values of w after each iteration, and the returned weight vector is the average of all the weight vectors throughout training." ></td>
	<td class="line x" title="75:209	Averaging has been shown to help reduce overfitting (Collins, 2002)." ></td>
	<td class="line x" title="76:209	2.3.1 MIRA Crammer and Singer (2001) developed a natural method for large-margin multi-class classification, which was later extended by Taskar et al.(2003) to structured classification: minbardblwbardbl s.t. s(x,y)  s(x,yprime)  L(y,yprime) (x,y)  T, yprime  dt(x) where L(y,yprime) is a real-valued loss for the tree yprime relative to the correct tree y. We define the loss of a dependency tree as the number of words that have the incorrect parent." ></td>
	<td class="line x" title="78:209	Thus, the largest loss a dependency tree can have is the length of the sentence." ></td>
	<td class="line x" title="79:209	Informally, this update looks to create a margin between the correct dependency tree and each incorrect dependency tree at least as large as the loss of the incorrect tree." ></td>
	<td class="line x" title="80:209	The more errors a tree has, the farther away its score will be from the score of the correct tree." ></td>
	<td class="line x" title="81:209	In order to avoid a blow-up in the norm of the weight vector we minimize it subject to constraints that enforce the desired margin between the correct and incorrect trees1." ></td>
	<td class="line x" title="82:209	1The constraints may be unsatisfiable, in which case we can relax them with slack variables as in SVM training." ></td>
	<td class="line x" title="83:209	93 The Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003; Crammer et al. , 2003) employs this optimization directly within the online framework." ></td>
	<td class="line x" title="84:209	On each update, MIRA attempts to keep the norm of the change to the parameter vector as small as possible, subject to correctly classifying the instance under consideration with a margin at least as large as the loss of the incorrect classifications." ></td>
	<td class="line x" title="85:209	This can be formalized by substituting the following update into line 4 of the generic online algorithm, minvextenddoublevextenddoublew(i+1)  w(i)vextenddoublevextenddouble s.t. s(xt,yt)  s(xt,yprime)  L(yt,yprime) yprime  dt(xt) (1) This is a standard quadratic programming problem that can be easily solved using Hildreths algorithm (Censor and Zenios, 1997)." ></td>
	<td class="line x" title="86:209	Crammer and Singer (2003) and Crammer et al.(2003) provide an analysis of both the online generalization error and convergence properties of MIRA." ></td>
	<td class="line x" title="88:209	In equation (1), s(x,y) is calculated with respect to the weight vector after optimization, w(i+1)." ></td>
	<td class="line x" title="89:209	To apply MIRA to dependency parsing, we can simply see parsing as a multi-class classification problem in which each dependency tree is one of many possible classes for a sentence." ></td>
	<td class="line x" title="90:209	However, that interpretation fails computationally because a general sentence has exponentially many possible dependency trees and thus exponentially many margin constraints." ></td>
	<td class="line x" title="91:209	To circumvent this problem we make the assumption that the constraints that matter for large margin optimization are those involving the incorrect trees yprime with the highest scores s(x,yprime)." ></td>
	<td class="line x" title="92:209	The resulting optimization made by MIRA (see Figure 3, line 4) would then be: min vextenddoublevextenddoublew(i+1)  w(i)vextenddoublevextenddouble s.t. s(xt,yt)  s(xt,yprime)  L(yt,yprime) yprime  bestk(xt;w(i)) reducing the number of constraints to the constant k. We tested various values of k on a development data set and found that small values of k are sufficient to achieve close to best performance, justifying our assumption." ></td>
	<td class="line x" title="93:209	In fact, as k grew we began to observe a slight degradation of performance, indicating some overfitting to the training data." ></td>
	<td class="line x" title="94:209	All the experiments presented here use k = 5." ></td>
	<td class="line x" title="95:209	The Eisner (1996) algorithm can be modified to find the k-best trees while only adding an additional O(k logk) factor to the runtime (Huang and Chiang, 2005)." ></td>
	<td class="line x" title="96:209	A more common approach is to factor the structure of the output space to yield a polynomial set of local constraints (Taskar et al. , 2003; Taskar et al. , 2004)." ></td>
	<td class="line x" title="97:209	One such factorization for dependency trees is minvextenddoublevextenddoublew(i+1)  w(i)vextenddoublevextenddouble s.t. s(l,j)  s(k,j)  1 (l,j)  yt,(k,j) / yt It is trivial to show that if these O(n2) constraints are satisfied, then so are those in (1)." ></td>
	<td class="line x" title="98:209	We implemented this model, but found that the required training time was much larger than the k-best formulation and typically did not improve performance." ></td>
	<td class="line x" title="99:209	Furthermore, the k-best formulation is more flexible with respect to the loss function since it does not assume the loss function can be factored into a sum of terms for each dependency." ></td>
	<td class="line x" title="100:209	2.4 Feature Set Finally, we need a suitable feature representation f(i,j) for each dependency." ></td>
	<td class="line x" title="101:209	The basic features in our model are outlined in Table 1a and b. All features are conjoined with the direction of attachment as well as the distance between the two words being attached." ></td>
	<td class="line x" title="102:209	These features represent a system of backoff from very specific features over words and partof-speech tags to less sparse features over just partof-speech tags." ></td>
	<td class="line x" title="103:209	These features are added for both the entire words as well as the 5-gram prefix if the word is longer than 5 characters." ></td>
	<td class="line x" title="104:209	Using just features over the parent-child node pairs in the tree was not enough for high accuracy, because all attachment decisions were made outside of the context in which the words occurred." ></td>
	<td class="line x" title="105:209	To solve this problem, we added two other types of features, which can be seen in Table 1c." ></td>
	<td class="line x" title="106:209	Features of the first type look at words that occur between a child and its parent." ></td>
	<td class="line x" title="107:209	These features take the form of a POS trigram: the POS of the parent, of the child, and of a word in between, for all words linearly between the parent and the child." ></td>
	<td class="line x" title="108:209	This feature was particularly helpful for nouns identifying their parent, since 94 a) Basic Uni-gram Features p-word, p-pos p-word p-pos c-word, c-pos c-word c-pos b) Basic Big-ram Features p-word, p-pos, c-word, c-pos p-pos, c-word, c-pos p-word, c-word, c-pos p-word, p-pos, c-pos p-word, p-pos, c-word p-word, c-word p-pos, c-pos c) In Between POS Features p-pos, b-pos, c-pos Surrounding Word POS Features p-pos, p-pos+1, c-pos-1, c-pos p-pos-1, p-pos, c-pos-1, c-pos p-pos, p-pos+1, c-pos, c-pos+1 p-pos-1, p-pos, c-pos, c-pos+1 Table 1: Features used by system." ></td>
	<td class="line x" title="109:209	p-word: word of parent node in dependency tree." ></td>
	<td class="line x" title="110:209	c-word: word of child node." ></td>
	<td class="line x" title="111:209	p-pos: POS of parent node." ></td>
	<td class="line x" title="112:209	c-pos: POS of child node." ></td>
	<td class="line x" title="113:209	p-pos+1: POS to the right of parent in sentence." ></td>
	<td class="line x" title="114:209	p-pos-1: POS to the left of parent." ></td>
	<td class="line x" title="115:209	c-pos+1: POS to the right of child." ></td>
	<td class="line x" title="116:209	c-pos-1: POS to the left of child." ></td>
	<td class="line x" title="117:209	b-pos: POS of a word in between parent and child nodes." ></td>
	<td class="line x" title="118:209	it would typically rule out situations when a noun attached to another noun with a verb in between, which is a very uncommon phenomenon." ></td>
	<td class="line x" title="119:209	The second type of feature provides the local context of the attachment, that is, the words before and after the parent-child pair." ></td>
	<td class="line x" title="120:209	This feature took the form of a POS 4-gram: The POS of the parent, child, word before/after parent and word before/after child." ></td>
	<td class="line x" title="121:209	The system also used back-off features to various trigrams where one of the local context POS tags was removed." ></td>
	<td class="line x" title="122:209	Adding these two features resulted in a large improvement in performance and brought the system to state-of-the-art accuracy." ></td>
	<td class="line x" title="123:209	2.5 System Summary Besides performance (see Section 3), the approach to dependency parsing we described has several other advantages." ></td>
	<td class="line x" title="124:209	The system is very general and contains no language specific enhancements." ></td>
	<td class="line x" title="125:209	In fact, the results we report for English and Czech use identical features, though are obviously trained on different data." ></td>
	<td class="line x" title="126:209	The online learning algorithms themselves are intuitive and easy to implement." ></td>
	<td class="line x" title="127:209	The efficient O(n3) parsing algorithm of Eisner allows the system to search the entire space of dependency trees while parsing thousands of sentences in a few minutes, which is crucial for discriminative training." ></td>
	<td class="line x" title="128:209	We compare the speed of our model to a standard lexicalized phrase structure parser in Section 3.1 and show a significant improvement in parsing times on the testing data." ></td>
	<td class="line x" title="129:209	The major limiting factor of the system is its restriction to features over single dependency attachments." ></td>
	<td class="line x" title="130:209	Often, when determining the next dependent for a word, it would be useful to know previous attachment decisions and incorporate these into the features." ></td>
	<td class="line x" title="131:209	It is fairly straightforward to modify the parsing algorithm to store previous attachments." ></td>
	<td class="line x" title="132:209	However, any modification would result in an asymptotic increase in parsing complexity." ></td>
	<td class="line x" title="133:209	3 Experiments We tested our methods experimentally on the English Penn Treebank (Marcus et al. , 1993) and on the Czech Prague Dependency Treebank (Hajic, 1998)." ></td>
	<td class="line x" title="134:209	All experiments were run on a dual 64-bit AMD Opteron 2.4GHz processor." ></td>
	<td class="line x" title="135:209	To create dependency structures from the Penn Treebank, we used the extraction rules of Yamada and Matsumoto (2003), which are an approximation to the lexicalization rules of Collins (1999)." ></td>
	<td class="line x" title="136:209	We split the data into three parts: sections 02-21 for training, section 22 for development and section 23 for evaluation." ></td>
	<td class="line x" title="137:209	Currently the system has 6,998,447 features." ></td>
	<td class="line x" title="138:209	Each instance only uses a tiny fraction of these features making sparse vector calculations possible." ></td>
	<td class="line x" title="139:209	Our system assumes POS tags as input and uses the tagger of Ratnaparkhi (1996) to provide tags for the development and evaluation sets." ></td>
	<td class="line x" title="140:209	Table 2 shows the performance of the systems that were compared." ></td>
	<td class="line x" title="141:209	Y&M2003 is the SVM-shiftreduce parsing model of Yamada and Matsumoto (2003), N&S2004 is the memory-based learner of Nivre and Scholz (2004) and MIRA is the the system we have described." ></td>
	<td class="line x" title="142:209	We also implemented an averaged perceptron system (Collins, 2002) (another online learning algorithm) for comparison." ></td>
	<td class="line x" title="143:209	This table compares only pure dependency parsers that do 95 English Czech Accuracy Root Complete Accuracy Root Complete Y&M2003 90.3 91.6 38.4 N&S2004 87.3 84.3 30.4 Avg." ></td>
	<td class="line x" title="144:209	Perceptron 90.6 94.0 36.5 82.9 88.0 30.3 MIRA 90.9 94.2 37.5 83.3 88.6 31.3 Table 2: Dependency parsing results for English and Czech." ></td>
	<td class="line x" title="145:209	Accuracy is the number of words that correctly identified their parent in the tree." ></td>
	<td class="line x" title="146:209	Root is the number of trees in which the root word was correctly identified." ></td>
	<td class="line x" title="147:209	For Czech this is f-measure since a sentence may have multiple roots." ></td>
	<td class="line x" title="148:209	Complete is the number of sentences for which the entire dependency tree was correct." ></td>
	<td class="line x" title="149:209	not exploit phrase structure." ></td>
	<td class="line x" title="150:209	We ensured that the gold standard dependencies of all systems compared were identical." ></td>
	<td class="line x" title="151:209	Table 2 shows that the model described here performs as well or better than previous comparable systems, including that of Yamada and Matsumoto (2003)." ></td>
	<td class="line x" title="152:209	Their method has the potential advantage that SVM batch training takes into account all of the constraints from all training instances in the optimization, whereas online training only considers constraints from one instance at a time." ></td>
	<td class="line x" title="153:209	However, they are fundamentally limited by their approximate search algorithm." ></td>
	<td class="line x" title="154:209	In contrast, our system searches the entire space of dependency trees and most likely benefits greatly from this." ></td>
	<td class="line x" title="155:209	This difference is amplified when looking at the percentage of trees that correctly identify the root word." ></td>
	<td class="line x" title="156:209	The models that search the entire space will not suffer from bad approximations made early in the search and thus are more likely to identify the correct root, whereas the approximate algorithms are prone to error propagation, which culminates with attachment decisions at the top of the tree." ></td>
	<td class="line x" title="157:209	When comparing the two online learning models, it can be seen that MIRA outperforms the averaged perceptron method." ></td>
	<td class="line x" title="158:209	This difference is statistically significant, p < 0.005 (McNemar test on head selection accuracy)." ></td>
	<td class="line x" title="159:209	In our Czech experiments, we used the dependency trees annotated in the Prague Treebank, and the predefined training, development and evaluation sections of this data." ></td>
	<td class="line x" title="160:209	The number of sentences in this data set is nearly twice that of the English treebank, leading to a very large number of features  13,450,672." ></td>
	<td class="line x" title="161:209	But again, each instance uses just a handful of these features." ></td>
	<td class="line x" title="162:209	For POS tags we used the automatically generated tags in the data set." ></td>
	<td class="line x" title="163:209	Though we made no language specific model changes, we did need to make some data specific changes." ></td>
	<td class="line x" title="164:209	In particular, we used the method of Collins et al.(1999) to simplify part-of-speech tags since the rich tags used by Czech would have led to a large but rarely seen set of POS features." ></td>
	<td class="line x" title="166:209	The model based on MIRA also performs well on Czech, again slightly outperforming averaged perceptron." ></td>
	<td class="line x" title="167:209	Unfortunately, we do not know of any other parsing systems tested on the same data set." ></td>
	<td class="line x" title="168:209	The Czech parser of Collins et al.(1999) was run on a different data set and most other dependency parsers are evaluated using English." ></td>
	<td class="line x" title="170:209	Learning a model from the Czech training data is somewhat problematic since it contains some crossing dependencies which cannot be parsed by the Eisner algorithm." ></td>
	<td class="line x" title="171:209	One trick is to rearrange the words in the training set so that all trees are nested." ></td>
	<td class="line x" title="172:209	This at least allows the training algorithm to obtain reasonably low error on the training set." ></td>
	<td class="line x" title="173:209	We found that this did improve performance slightly to 83.6% accuracy." ></td>
	<td class="line x" title="174:209	3.1 Lexicalized Phrase Structure Parsers It is well known that dependency trees extracted from lexicalized phrase structure parsers (Collins, 1999; Charniak, 2000) typically are more accurate than those produced by pure dependency parsers (Yamada and Matsumoto, 2003)." ></td>
	<td class="line xc" title="175:209	We compared our system to the Bikel re-implementation of the Collins parser (Bikel, 2004; Collins, 1999) trained with the same head rules of our system." ></td>
	<td class="line x" title="176:209	There are two ways to extract dependencies from lexicalized phrase structure." ></td>
	<td class="line x" title="177:209	The first is to use the automatically generated dependencies that are explicit in the lexicalization of the trees, we call this system Collinsauto." ></td>
	<td class="line x" title="178:209	The second is to take just the phrase structure output of the parser and run the automatic head rules over it to extract the dependencies, we call this sys96 English Accuracy Root Complete Complexity Time Collins-auto 88.2 92.3 36.1 O(n5) 98m 21s Collins-rules 91.4 95.1 42.6 O(n5) 98m 21s MIRA-Normal 90.9 94.2 37.5 O(n3) 5m 52s MIRA-Collins 92.2 95.8 42.9 O(n5) 105m 08s Table 3: Results comparing our system to those based on the Collins parser." ></td>
	<td class="line x" title="179:209	Complexity represents the computational complexity of each parser and Time the CPU time to parse sec." ></td>
	<td class="line x" title="180:209	23 of the Penn Treebank." ></td>
	<td class="line x" title="181:209	tem Collins-rules." ></td>
	<td class="line x" title="182:209	Table 3 shows the results comparing our system, MIRA-Normal, to the Collins parser for English." ></td>
	<td class="line x" title="183:209	All systems are implemented in Java and run on the same machine." ></td>
	<td class="line x" title="184:209	Interestingly, the dependencies that are automatically produced by the Collins parser are worse than those extracted statically using the head rules." ></td>
	<td class="line x" title="185:209	Arguably, this displays the artificialness of English dependency parsing using dependencies automatically extracted from treebank phrase-structure trees." ></td>
	<td class="line x" title="186:209	Our system falls in-between, better than the automatically generated dependency trees and worse than the head-rule extracted trees." ></td>
	<td class="line x" title="187:209	Since the dependencies returned from our system are better than those actually learnt by the Collins parser, one could argue that our model is actually learning to parse dependencies more accurately." ></td>
	<td class="line x" title="188:209	However, phrase structure parsers are built to maximize the accuracy of the phrase structure and use lexicalization as just an additional source of information." ></td>
	<td class="line x" title="189:209	Thus it is not too surprising that the dependencies output by the Collins parser are not as accurate as our system, which is trained and built to maximize accuracy on dependency trees." ></td>
	<td class="line x" title="190:209	In complexity and run-time, our system is a huge improvement over the Collins parser." ></td>
	<td class="line x" title="191:209	The final system in Table 3 takes the output of Collins-rules and adds a feature to MIRA-Normal that indicates for given edge, whether the Collins parser believed this dependency actually exists, we call this system MIRA-Collins." ></td>
	<td class="line x" title="192:209	This is a well known discriminative training trick  using the suggestions of a generative system to influence decisions." ></td>
	<td class="line x" title="193:209	This system can essentially be considered a corrector of the Collins parser and represents a significant improvement over it." ></td>
	<td class="line x" title="194:209	However, there is an added complexity with such a model as it requires the output of the O(n5) Collins parser." ></td>
	<td class="line x" title="195:209	k=1 k=2 k=5 k=10 k=20 Accuracy 90.73 90.82 90.88 90.92 90.91 Train Time 183m 235m 627m 1372m 2491m Table 4: Evaluation of k-best MIRA approximation." ></td>
	<td class="line x" title="196:209	3.2 k-best MIRA Approximation One question that can be asked is how justifiable is the k-best MIRA approximation." ></td>
	<td class="line x" title="197:209	Table 4 indicates the accuracy on testing and the time it took to train models with k = 1,2,5,10,20 for the English data set." ></td>
	<td class="line x" title="198:209	Even though the parsing algorithm is proportional to O(klogk), empirically, the training times scale linearly with k. Peak performance is achieved very early with a slight degradation around k=20." ></td>
	<td class="line x" title="199:209	The most likely reason for this phenomenon is that the model is overfitting by ensuring that even unlikely trees are separated from the correct tree proportional to their loss." ></td>
	<td class="line x" title="200:209	4 Summary We described a successful new method for training dependency parsers." ></td>
	<td class="line x" title="201:209	We use simple linear parsing models trained with margin-sensitive online training algorithms, achieving state-of-the-art performance with relatively modest training times and no need for pruning heuristics." ></td>
	<td class="line x" title="202:209	We evaluated the system on both English and Czech data to display state-of-theart performance without any language specific enhancements." ></td>
	<td class="line x" title="203:209	Furthermore, the model can be augmented to include features over lexicalized phrase structure parsing decisions to increase dependency accuracy over those parsers." ></td>
	<td class="line x" title="204:209	We plan on extending our parser in two ways." ></td>
	<td class="line x" title="205:209	First, we would add labels to dependencies to represent grammatical roles." ></td>
	<td class="line x" title="206:209	Those labels are very important for using parser output in tasks like information extraction or machine translation." ></td>
	<td class="line x" title="207:209	Second, 97 we are looking at model extensions to allow nonprojective dependencies, which occur in languages such as Czech, German and Dutch." ></td>
	<td class="line x" title="208:209	Acknowledgments: We thank Jan Hajic for answering queries on the Prague treebank, and Joakim Nivre for providing the Yamada and Matsumoto (2003) head rules for English that allowed for a direct comparison with our systems." ></td>
	<td class="line x" title="209:209	This work was supported by NSF ITR grants 0205456, 0205448, and 0428193." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P05-1023
Data-Defined Kernels For Parse Reranking Derived From Probabilistic Models
Henderson, James B.;Titov, Ivan;"></td>
	<td class="line x" title="1:176	Proceedings of the 43rd Annual Meeting of the ACL, pages 181188, Ann Arbor, June 2005." ></td>
	<td class="line x" title="2:176	c2005 Association for Computational Linguistics Data-Defined Kernels for Parse Reranking Derived from Probabilistic Models James Henderson School of Informatics University of Edinburgh 2 Buccleuch Place Edinburgh EH8 9LW, United Kingdom james.henderson@ed.ac.uk Ivan Titov Department of Computer Science University of Geneva 24, rue General Dufour CH-1211 Gen`eve 4, Switzerland ivan.titov@cui.unige.ch Abstract Previous research applying kernel methods to natural language parsing have focussed on proposing kernels over parse trees, which are hand-crafted based on domain knowledge and computational considerations." ></td>
	<td class="line x" title="3:176	In this paper we propose a method for defining kernels in terms of a probabilistic model of parsing." ></td>
	<td class="line x" title="4:176	This model is then trained, so that the parameters of the probabilistic model reflect the generalizations in the training data." ></td>
	<td class="line x" title="5:176	The method we propose then uses these trained parameters to define a kernel for reranking parse trees." ></td>
	<td class="line x" title="6:176	In experiments, we use a neural network based statistical parser as the probabilistic model, and use the resulting kernel with the Voted Perceptron algorithm to rerank the top 20 parses from the probabilistic model." ></td>
	<td class="line x" title="7:176	This method achieves a significant improvement over the accuracy of the probabilistic model." ></td>
	<td class="line x" title="8:176	1 Introduction Kernel methods have been shown to be very effective in many machine learning problems." ></td>
	<td class="line x" title="9:176	They have the advantage that learning can try to optimize measures related directly to expected testing performance (i.e. large margin methods), rather than the probabilistic measures used in statistical models, which are only indirectly related to expected testing performance." ></td>
	<td class="line x" title="10:176	Work on kernel methods in natural language has focussed on the definition of appropriate kernels for natural language tasks." ></td>
	<td class="line oc" title="11:176	In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (Collins and Duffy, 2002; Shen and Joshi, 2003; Shen et al. , 2003; Collins and Roark, 2004)." ></td>
	<td class="line o" title="12:176	These kernels have all been hand-crafted to try reflect properties of parse trees which are relevant to discriminating correct parse trees from incorrect ones, while at the same time maintaining the tractability of learning." ></td>
	<td class="line x" title="13:176	Some work in machine learning has taken an alternative approach to defining kernels, where the kernel is derived from a probabilistic model of the task (Jaakkola and Haussler, 1998; Tsuda et al. , 2002)." ></td>
	<td class="line x" title="14:176	This way of defining kernels has two advantages." ></td>
	<td class="line x" title="15:176	First, linguistic knowledge about parsing is reflected in the design of the probabilistic model, not directly in the kernel." ></td>
	<td class="line x" title="16:176	Designing probabilistic models to reflect linguistic knowledge is a process which is currently well understood, both in terms of reflecting generalizations and controlling computational cost." ></td>
	<td class="line x" title="17:176	Because many NLP problems are unbounded in size and complexity, it is hard to specify all possible relevant kernel features without having so many features that the computations become intractable and/or the data becomes too sparse.1 Second, the kernel is defined using the trained parameters of the probabilistic model." ></td>
	<td class="line x" title="18:176	Thus the kernel is in part determined by the training data, and is automatically tailored to reflect properties of parse trees which are relevant to parsing." ></td>
	<td class="line x" title="19:176	1For example, see (Henderson, 2004) for a discussion of why generative models are better than models parameterized to estimate the a posteriori probability directly." ></td>
	<td class="line x" title="20:176	181 In this paper, we propose a new method for deriving a kernel from a probabilistic model which is specifically tailored to reranking tasks, and we apply this method to natural language parsing." ></td>
	<td class="line x" title="21:176	For the probabilistic model, we use a state-of-the-art neural network based statistical parser (Henderson, 2003)." ></td>
	<td class="line x" title="22:176	The resulting kernel is then used with the Voted Perceptron algorithm (Freund and Schapire, 1998) to reranking the top 20 parses from the probabilistic model." ></td>
	<td class="line x" title="23:176	This method achieves a significant improvement over the accuracy of the probabilistic model alone." ></td>
	<td class="line x" title="24:176	2 Kernels Derived from Probabilistic Models In recent years, several methods have been proposed for constructing kernels from trained probabilistic models." ></td>
	<td class="line x" title="25:176	As usual, these kernels are then used with linear classifiers to learn the desired task." ></td>
	<td class="line x" title="26:176	As well as some empirical successes, these methods are motivated by theoretical results which suggest we should expect some improvement with these classifiers over the classifier which chooses the most probable answer according to the probabilistic model (i.e. the maximum a posteriori (MAP) classifier)." ></td>
	<td class="line x" title="27:176	There is guaranteed to be a linear classifier for the derived kernel which performs at least as well as the MAP classifier for the probabilistic model." ></td>
	<td class="line x" title="28:176	So, assuming a large-margin classifier can optimize a more appropriate criteria than the posterior probability, we should expect the derived kernels classifier to perform better than the probabilistic models classifier, although empirical results on a given task are never guaranteed." ></td>
	<td class="line x" title="29:176	In this section, we first present two previous kernels and then propose a new kernel specifically for reranking tasks." ></td>
	<td class="line x" title="30:176	In each of these discussions we need to characterize the parsing problem as a classification task." ></td>
	<td class="line x" title="31:176	Parsing can be regarded as a mapping from an input space of sentences xX to a structured output space of parse trees yY. On the basis of training sentences, we learn a discriminant function F : X  Y  R. The parse tree y with the largest value for this discriminant function F(x,y) is the output parse tree for the sentence x. We focus on the linear discriminant functions: Fw(x,y) = <w,(x,y)>, where (x,y) is a feature vector for the sentencetree pair, w is a parameter vector for the discriminant function, and <a,b> is the inner product of vectors a and b. In the remainder of this section, we will characterize the kernel methods we consider in terms of the feature extractor (x,y)." ></td>
	<td class="line x" title="32:176	2.1 Fisher Kernels The Fisher kernel (Jaakkola and Haussler, 1998) is one of the best known kernels belonging to the class of probability model based kernels." ></td>
	<td class="line x" title="33:176	Given a generative model of P(z|) with smooth parameterization, the Fisher score of an example z is a vector of partial derivatives of the log-likelihood of the example with respect to the model parameters: (z) = (logP(z|)1,, logP(z|)l )." ></td>
	<td class="line x" title="34:176	This score can be regarded as specifying how the model should be changed in order to maximize the likelihood of the example z. Then we can define the similarity between data points as the inner product of the corresponding Fisher scores." ></td>
	<td class="line x" title="35:176	This kernel is often referred to as the practical Fisher kernel." ></td>
	<td class="line x" title="36:176	The theoretical Fisher kernel depends on the Fisher information matrix, which is not feasible to compute for most practical tasks and is usually omitted." ></td>
	<td class="line x" title="37:176	The Fisher kernel is only directly applicable to binary classification tasks." ></td>
	<td class="line x" title="38:176	We can apply it to our task by considering an example z to be a sentencetree pair (x,y), and classifying the pairs into correct parses versus incorrect parses." ></td>
	<td class="line x" title="39:176	When we use the Fisher score (x,y) in the discriminant function F, we can interpret the value as the confidence that the tree y is correct, and choose the y in which we are the most confident." ></td>
	<td class="line x" title="40:176	2.2 TOP Kernels Tsuda (2002) proposed another kernel constructed from a probabilistic model, called the Tangent vectors Of Posterior log-odds (TOP) kernel." ></td>
	<td class="line x" title="41:176	Their TOP kernel is also only for binary classification tasks, so, as above, we treat the input z as a sentence-tree pair and the output category c  {1,+1} as incorrect/correct." ></td>
	<td class="line x" title="42:176	It is assumed that the true probability distribution is included in the class of probabilistic models and that the true parameter vector star is unique." ></td>
	<td class="line x" title="43:176	The feature extractor of the TOP kernel for 182 the input z is defined by: (z) = (v(z, ), v(z,)1,, v(z,)l ), where v(z, ) = logP(c=+1|z, )  logP(c=1|z, )." ></td>
	<td class="line x" title="44:176	In addition to being at least as good as the MAP classifier, the choice of the TOP kernel feature extractor is motivated by the minimization of the binary classification error of a linear classifier <w,(z)> + b. Tsuda (2002) demonstrates that this error is closely related to the estimation error of the posterior probability P(c=+1|z,star) by the estimator g(<w,(z)> + b), where g is the sigmoid function g(t) = 1/(1 + exp(t))." ></td>
	<td class="line x" title="45:176	The TOP kernel isnt quite appropriate for structured classification tasks because (z) is motivated by binary classificaton error minimization." ></td>
	<td class="line x" title="46:176	In the next subsection, we will adapt it to structured classification." ></td>
	<td class="line x" title="47:176	2.3 A TOP Kernel for Reranking We define the reranking task as selecting a parse tree from the list of candidate trees suggested by a probabilistic model." ></td>
	<td class="line x" title="48:176	Furthermore, we only consider learning to rerank the output of a particular probabilistic model, without requiring the classifier to have good performance when applied to a candidate list provided by a different model." ></td>
	<td class="line x" title="49:176	In this case, it is natural to model the probability that a parse tree is the best candidate given the list of candidate trees: P(yk|x,y1,,ys) = P(x,yk)summationtext t P(x,yt), where y1,,ys is the list of candidate parse trees." ></td>
	<td class="line x" title="50:176	To construct a new TOP kernel for reranking, we apply an approach similar to that used for the TOP kernel (Tsuda et al. , 2002), but we consider the probability P(yk|x,y1,,ys,star) instead of the probability P(c=+1|z,star) considered by Tsuda." ></td>
	<td class="line x" title="51:176	The resulting feature extractor is given by: (x,yk) = (v(x,yk, ), v(x,yk,)1,, v(x,yk,)l ), where v(x,yk, ) = logP(yk|y1,,ys, )  logsummationtexttnegationslash=k P(yt|y1,,ys, )." ></td>
	<td class="line x" title="52:176	We will call this kernel the TOP reranking kernel." ></td>
	<td class="line x" title="53:176	3 The Probabilistic Model To complete the definition of the kernel, we need to choose a probabilistic model of parsing." ></td>
	<td class="line x" title="54:176	For this we use a statistical parser which has previously been shown to achieve state-of-the-art performance, namely that proposed in (Henderson, 2003)." ></td>
	<td class="line x" title="55:176	This parser has two levels of parameterization." ></td>
	<td class="line x" title="56:176	The first level of parameterization is in terms of a historybased generative probability model, but this level is not appropriate for our purposes because it defines an infinite number of parameters (one for every possible partial parse history)." ></td>
	<td class="line x" title="57:176	When parsing a given sentence, the bounded set of parameters which are relevant to a given parse are estimated using a neural network." ></td>
	<td class="line x" title="58:176	The weights of this neural network form the second level of parameterization." ></td>
	<td class="line x" title="59:176	There is a finite number of these parameters." ></td>
	<td class="line x" title="60:176	Neural network training is applied to determine the values of these parameters, which in turn determine the values of the probability models parameters, which in turn determine the probabilistic model of parse trees." ></td>
	<td class="line x" title="61:176	We do not use the complete set of neural network weights to define our kernels, but instead we define a third level of parameterization which only includes the networks output layer weights." ></td>
	<td class="line x" title="62:176	These weights define a normalized exponential model, with the networks hidden layer as the input features." ></td>
	<td class="line x" title="63:176	When we tried using the complete set of weights in some small scale experiments, training the classifier was more computationally expensive, and actually performed slightly worse than just using the output weights." ></td>
	<td class="line x" title="64:176	Using just the output weights also allows us to make some approximations in the TOP reranking kernel which makes the classifier learning algorithm more efficient." ></td>
	<td class="line x" title="65:176	3.1 A History-Based Probability Model As with many other statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000), Henderson (2003) uses a history-based model of parsing." ></td>
	<td class="line x" title="66:176	He defines the mapping from phrase structure trees to parse sequences using a form of left-corner parsing strategy (see (Henderson, 2003) for more details)." ></td>
	<td class="line x" title="67:176	The parser actions include: introducing a new constituent with a specified label, attaching one constituent to another, and predicting the next word of the sentence." ></td>
	<td class="line x" title="68:176	A complete parse consists of a sequence of these actions, d1,,dm, such that performing d1,,dm results in a complete phrase structure tree." ></td>
	<td class="line x" title="69:176	Because this mapping to parse sequences is 183 one-to-one, and the word prediction actions in a complete parse d1,,dm specify the sentence, P(d1,,dm) is equivalent to the joint probability of the output phrase structure tree and the input sentence." ></td>
	<td class="line x" title="70:176	This probability can be then be decomposed into the multiplication of the probabilities of each action decision di conditioned on that decisions prior parse history d1,,di1." ></td>
	<td class="line x" title="71:176	P(d1,,dm) = iP(di|d1,,di1) 3.2 Estimating Decision Probabilities with a Neural Network The parameters of the above probability model are the P(di|d1,,di1)." ></td>
	<td class="line x" title="72:176	There are an infinite number of these parameters, since the parse history d1,,di1 grows with the length of the sentence." ></td>
	<td class="line x" title="73:176	In other work on history-based parsing, independence assumptions are applied so that only a finite amount of information from the parse history can be treated as relevant to each parameter, thereby reducing the number of parameters to a finite set which can be estimated directly." ></td>
	<td class="line x" title="74:176	Instead, Henderson (2003) uses a neural network to induce a finite representation of this unbounded history, which we will denote h(d1,,di1)." ></td>
	<td class="line x" title="75:176	Neural network training tries to find such a history representation which preserves all the information about the history which is relevant to estimating the desired probability." ></td>
	<td class="line x" title="76:176	P(di|d1,,di1)  P(di|h(d1,,di1)) Using a neural network architecture called Simple Synchrony Networks (SSNs), the history representation h(d1,,di1) is incrementally computed from features of the previous decision di1 plus a finite set of previous history representations h(d1,,dj), j < i  1." ></td>
	<td class="line x" title="77:176	Each history representation is a finite vector of real numbers, called the networks hidden layer." ></td>
	<td class="line x" title="78:176	As long as the history representation for position i  1 is always included in the inputs to the history representation for position i, any information about the entire sequence could be passed from history representation to history representation and be used to estimate the desired probability." ></td>
	<td class="line x" title="79:176	However, learning is biased towards paying more attention to information which passes through fewer history representations." ></td>
	<td class="line x" title="80:176	To exploit this learning bias, structural locality is used to determine which history representations are input to which others." ></td>
	<td class="line x" title="81:176	First, each history representation is assigned to the constituent which is on the top of the parsers stack when it is computed." ></td>
	<td class="line x" title="82:176	Then earlier history representations whose constituents are structurally local to the current representations constituent are input to the computation of the correct representation." ></td>
	<td class="line x" title="83:176	In this way, the number of representations which information needs to pass through in order to flow from history representation i to history representation j is determined by the structural distance between is constituent and js constituent, and not just the distance between i and j in the parse sequence." ></td>
	<td class="line x" title="84:176	This provides the neural network with a linguistically appropriate inductive bias when it learns the history representations, as explained in more detail in (Henderson, 2003)." ></td>
	<td class="line x" title="85:176	Once it has computed h(d1,,di1), the SSN uses a normalized exponential to estimate a probability distribution over the set of possible next decisions di given the history: P(di|d1,,di1,)  exp(<di,h(d1,,di1)>)summationtext tN(di1) exp(<t,h(d1,,di1)>), where by t we denote the set of output layer weights, corresponding to the parser action t, N(di1) defines a set of possible next parser actions after the step di1 and  denotes the full set of model parameters." ></td>
	<td class="line x" title="86:176	We trained SSN parsing models, using the on-line version of Backpropagation to perform the gradient descent with a maximum likelihood objective function." ></td>
	<td class="line x" title="87:176	This learning simultaneously tries to optimize the parameters of the output computation and the parameters of the mappings h(d1,,di1)." ></td>
	<td class="line x" title="88:176	With multilayered networks such as SSNs, this training is not guaranteed to converge to a global optimum, but in practice a network whose criteria value is close to the optimum can be found." ></td>
	<td class="line x" title="89:176	4 Large-Margin Optimization Once we have defined a kernel over parse trees, general techniques for linear classifier optimization can be used to learn the given task." ></td>
	<td class="line x" title="90:176	The most sophisticated of these techniques (such as Support Vector Machines) are unfortunately too computationally expensive to be used on large datasets like the Penn Treebank (Marcus et al. , 1993)." ></td>
	<td class="line x" title="91:176	Instead we use a 184 method which has often been shown to be virtually as good, the Voted Perceptron (VP) (Freund and Schapire, 1998) algorithm." ></td>
	<td class="line x" title="92:176	The VP algorithm was originally applied to parse reranking in (Collins and Duffy, 2002) with the Tree kernel." ></td>
	<td class="line x" title="93:176	We modify the perceptron training algorithm to make it more suitable for parsing, where zero-one classification loss is not the evaluation measure usually employed." ></td>
	<td class="line x" title="94:176	We also develop a variant of the kernel defined in section 2.3, which is more efficient when used with the VP algorithm." ></td>
	<td class="line x" title="95:176	Given a list of candidate trees, we train the classifier to select the tree with largest constituent F1 score." ></td>
	<td class="line x" title="96:176	The F1 score is a measure of the similarity between the tree in question and the gold standard parse, and is the standard way to evaluate the accuracy of a parser." ></td>
	<td class="line x" title="97:176	We denote the kth candidate tree for the jth sentence xj by yjk." ></td>
	<td class="line x" title="98:176	Without loss of generality, let us assume that yj1 is the candidate tree with the largest F1 score." ></td>
	<td class="line x" title="99:176	The Voted Perceptron algorithm is an ensemble method for combining the various intermediate models which are produced during training a perceptron." ></td>
	<td class="line x" title="100:176	It demonstrates more stable generalization performance than the normal perceptron algorithm when the problem is not linearly separable (Freund and Schapire, 1998), as is usually the case." ></td>
	<td class="line x" title="101:176	We modify the perceptron algorithm by introducing a new classification loss function." ></td>
	<td class="line x" title="102:176	This modification enables us to treat differently the cases where the perceptron predicts a tree with an F1 score much smaller than that of the top candidate and the cases where the predicted and the top candidates have similar score values." ></td>
	<td class="line x" title="103:176	The natural choice for the loss function would be (yjk,yj1) = F1(yj1)  F1(yjk), where F1(yjk) denotes the F1 score value for the parse tree yjk." ></td>
	<td class="line x" title="104:176	This approach is very similar to slack variable rescaling for Support Vector Machines proposed in (Tsochantaridis et al. , 2004)." ></td>
	<td class="line x" title="105:176	The learning algorithm we employed is presented in figure 1." ></td>
	<td class="line x" title="106:176	When applying kernels with a large training corpus, we face efficiency issues because of the large number of the neural network weights." ></td>
	<td class="line x" title="107:176	Even though we use only the output layer weights, this vector grows with the size of the vocabulary, and thus can be large." ></td>
	<td class="line x" title="108:176	The kernels presented in section 2 all lead to feature vectors without many zero values." ></td>
	<td class="line x" title="109:176	This w = 0 for j = 1 n for k = 2  s if <w,(xj,yjk)> > <w,(xj,yj1)> w = w + (yjk,yj1)((xj,yj1)(xj,yjk)) Figure 1: The modified perceptron algorithm happens because we compute the derivative of the normalization factor used in the networks estimation of P(di|d1,,di1)." ></td>
	<td class="line x" title="110:176	This normalization factor depends on the output layer weights corresponding to all the possible next decisions (see section 3.2)." ></td>
	<td class="line x" title="111:176	This makes an application of the VP algorithm infeasible in the case of a large vocabulary." ></td>
	<td class="line x" title="112:176	We can address this problem by freezing the normalization factor when computing the feature vector." ></td>
	<td class="line x" title="113:176	Note that we can rewrite the model logprobability of the tree as: logP(y|) =summationtext i log( exp(<di,h(d1,,di1)>)summationtext tN(di1) exp(<t,h(d1,,di1)>) ) = summationtext i(<di,h(d1,,di1)>)summationtext i log summationtext tN(di1) exp(<t,h(d1,,di1)>)." ></td>
	<td class="line x" title="114:176	We treat the parameters used to compute the first term as different from the parameters used to compute the second term, and we define our kernel only using the parameters in the first term." ></td>
	<td class="line x" title="115:176	This means that the second term does not effect the derivatives in the formula for the feature vector (x,y)." ></td>
	<td class="line x" title="116:176	Thus the feature vector for the kernel will contain nonzero entries only in the components corresponding to the parser actions which are present in the candidate derivation for the sentence, and thus in the first vector component." ></td>
	<td class="line x" title="117:176	We have applied this technique to the TOP reranking kernel, the result of which we will call the efficient TOP reranking kernel." ></td>
	<td class="line x" title="118:176	5 The Experimental Results We used the Penn Treebank WSJ corpus (Marcus et al. , 1993) to perform empirical experiments on the proposed parsing models." ></td>
	<td class="line x" title="119:176	In each case the input to the network is a sequence of tag-word pairs.2 We report results for two different vocabulary sizes, varying in the frequency with which tag-word pairs must 2We used a publicly available tagger (Ratnaparkhi, 1996) to provide the tags." ></td>
	<td class="line x" title="120:176	185 occur in the training set in order to be included explicitly in the vocabulary." ></td>
	<td class="line x" title="121:176	A frequency threshold of 200 resulted in a vocabulary of 508 tag-word pairs (including tag-unknown word pairs) and a threshold of 20 resulted in 4215 tag-word pairs." ></td>
	<td class="line x" title="122:176	We denote the probabilistic model trained with the vocabulary of 508 by the SSN-Freq200, the model trained with the vocabulary of 4215 by the SSN-Freq20." ></td>
	<td class="line x" title="123:176	Testing the probabilistic parser requires using a beam search through the space of possible parses." ></td>
	<td class="line x" title="124:176	We used a form of beam search which prunes the search after the prediction of each word." ></td>
	<td class="line x" title="125:176	We set the width of this post-word beam to 40 for both testing of the probabilistic model and generating the candidate list for reranking." ></td>
	<td class="line x" title="126:176	For training and testing of the kernel models, we provided a candidate list consisting of the top 20 parses found by the generative probabilistic model." ></td>
	<td class="line x" title="127:176	When using the Fisher kernel, we added the log-probability of the tree given by the probabilistic model as the feature." ></td>
	<td class="line x" title="128:176	This was not necessary for the TOP kernels because they already contain a feature corresponding to the probability estimated by the probabilistic model (see section 2.3)." ></td>
	<td class="line x" title="129:176	We trained the VP model with all three kernels using the 508 word vocabulary (Fisher-Freq200, TOP-Freq200, TOP-Eff-Freq200) but only the efficient TOP reranking kernel model was trained with the vocabulary of 4215 words (TOP-Eff-Freq20)." ></td>
	<td class="line x" title="130:176	The non-sparsity of the feature vectors for other kernels led to the excessive memory requirements and larger testing time." ></td>
	<td class="line x" title="131:176	In each case, the VP model was run for only one epoch." ></td>
	<td class="line x" title="132:176	We would expect some improvement if running it for more epochs, as has been empirically demonstrated in other domains (Freund and Schapire, 1998)." ></td>
	<td class="line x" title="133:176	To avoid repeated testing on the standard testing set, we first compare the different models with their performance on the validation set." ></td>
	<td class="line x" title="134:176	Note that the validation set wasnt used during learning of the kernel models or for adjustment of any parameters." ></td>
	<td class="line x" title="135:176	Standard measures of accuracy are shown in table 1.3 Both the Fisher kernel and the TOP kernels show better accuracy than the baseline probabilistic 3All our results are computed with the evalb program following the standard criteria in (Collins, 1999), and using the standard training (sections 222, 39,832 sentences, 910,196 words), validation (section 24, 1346 sentence, 31507 words), and testing (section 23, 2416 sentences, 54268 words) sets (Collins, 1999)." ></td>
	<td class="line x" title="136:176	LR LP F=1 SSN-Freq200 87.2 88.5 87.8 Fisher-Freq200 87.2 88.8 87.9 TOP-Freq200 87.3 88.9 88.1 TOP-Eff-Freq200 87.3 88.9 88.1 SSN-Freq20 88.1 89.2 88.6 TOP-Eff-Freq20 88.2 89.7 88.9 Table 1: Percentage labeled constituent recall (LR), precision (LP), and a combination of both (F=1) on validation set sentences of length at most 100." ></td>
	<td class="line x" title="137:176	model, but only the improvement of the TOP kernels is statistically significant.4 For the TOP kernel, the improvement over baseline is about the same with both vocabulary sizes." ></td>
	<td class="line x" title="138:176	Also note that the performance of the efficient TOP reranking kernel is the same as that of the original TOP reranking kernel, for the smaller vocabulary." ></td>
	<td class="line oc" title="139:176	For comparison to previous results, table 2 lists the results on the testing set for our best model (TOP-Efficient-Freq20) and several other statistical parsers (Collins, 1999; Collins and Duffy, 2002; Collins and Roark, 2004; Henderson, 2003; Charniak, 2000; Collins, 2000; Shen and Joshi, 2004; Shen et al. , 2003; Henderson, 2004; Bod, 2003)." ></td>
	<td class="line x" title="140:176	First note that the parser based on the TOP efficient kernel has better accuracy than (Henderson, 2003), which used the same parsing method as our baseline model, although the trained network parameters were not the same." ></td>
	<td class="line nc" title="141:176	When compared to other kernel methods, our approach performs better than those based on the Tree kernel (Collins and Duffy, 2002; Collins and Roark, 2004), and is only 0.2% worse than the best results achieved by a kernel method for parsing (Shen et al. , 2003; Shen and Joshi, 2004)." ></td>
	<td class="line x" title="142:176	6 Related Work The first application of kernel methods to parsing was proposed by Collins and Duffy (2002)." ></td>
	<td class="line x" title="143:176	They used the Tree kernel, where the features of a tree are all its connected tree fragments." ></td>
	<td class="line x" title="144:176	The VP algorithm was applied to rerank the output of a probabilistic model and demonstrated an improvement over the baseline." ></td>
	<td class="line x" title="145:176	4We measured significance with the randomized significance test of (Yeh, 2000)." ></td>
	<td class="line x" title="146:176	186 LR LP F=1 Collins99 88.1 88.3 88.2 Collins&Duffy02 88.6 88.9 88.7 Collins&Roark04 88.4 89.1 88.8 Henderson03 88.8 89.5 89.1 Charniak00 89.6 89.5 89.5 TOP-Eff-Freq20 89.1 90.1 89.6 Collins00 89.6 89.9 89.7 Shen&Joshi04 89.5 90.0 89.8 Shen et al.03 89.7 90.0 89.8 Henderson04 89.8 90.4 90.1 Bod03 90.7 90.8 90.7 * F=1 for previous models may have rounding errors." ></td>
	<td class="line x" title="147:176	Table 2: Percentage labeled constituent recall (LR), precision (LP), and a combination of both (F=1) on the entire testing set." ></td>
	<td class="line x" title="148:176	Shen and Joshi (2003) applied an SVM based voting algorithm with the Preference kernel defined over pairs for reranking." ></td>
	<td class="line x" title="149:176	To define the Preference kernel they used the Tree kernel and the Linear kernel as its underlying kernels and achieved state-ofthe-art results with the Linear kernel." ></td>
	<td class="line x" title="150:176	In (Shen et al. , 2003) it was pointed out that most of the arbitrary tree fragments allowed by the Tree kernel are linguistically meaningless." ></td>
	<td class="line x" title="151:176	The authors suggested the use of Lexical Tree Adjoining Grammar (LTAG) based features as a more linguistically appropriate set of features." ></td>
	<td class="line x" title="152:176	They empirically demonstrated that incorporation of these features helps to improve reranking performance." ></td>
	<td class="line x" title="153:176	Shen and Joshi (2004) proposed to improve margin based methods for reranking by defining the margin not only between the top tree and all the other trees in the candidate list but between all the pairs of parses in the ordered candidate list for the given sentence." ></td>
	<td class="line x" title="154:176	They achieved the best results when training with an uneven margin scaled by the heuristic function of the candidates positions in the list." ></td>
	<td class="line x" title="155:176	One potential drawback of this method is that it doesnt take into account the actual F1 score of the candidate and considers only the position in the list ordered by the F1 score." ></td>
	<td class="line x" title="156:176	We expect that an improvement could be achieved by combining our approach of scaling updates by the F1 loss with the all pairs approach of (Shen and Joshi, 2004)." ></td>
	<td class="line x" title="157:176	Use of the F1 loss function during training demonstrated better performance comparing to the 0-1 loss function when applied to a structured classification task (Tsochantaridis et al. , 2004)." ></td>
	<td class="line x" title="158:176	All the described kernel methods are limited to the reranking of candidates from an existing parser due to the complexity of finding the best parse given a kernel (i.e. the decoding problem)." ></td>
	<td class="line x" title="159:176	(Taskar et al. , 2004) suggested a method for maximal margin parsing which employs the dynamic programming approach to decoding and parameter estimation problems." ></td>
	<td class="line x" title="160:176	The efficiency of dynamic programming means that the entire space of parses can be considered, not just a candidate list." ></td>
	<td class="line x" title="161:176	However, not all kernels are suitable for this method." ></td>
	<td class="line x" title="162:176	The dynamic programming approach requires the feature vector of a tree to be decomposable into a sum over parts of the tree." ></td>
	<td class="line x" title="163:176	In particular, this is impossible with the TOP and Fisher kernels derived from the SSN model." ></td>
	<td class="line x" title="164:176	Also, it isnt clear whether the algorithm remains tractable for a large training set with long sentences, since the authors only present results for sentences of length less than or equal to 15." ></td>
	<td class="line x" title="165:176	7 Conclusions This paper proposes a method for deriving a kernel for reranking from a probabilistic model, and demonstrates state-of-the-art accuracy when this method is applied to parse reranking." ></td>
	<td class="line x" title="166:176	Contrary to most of the previous research on kernel methods in parsing, linguistic knowledge does not have to be expressed through a list of features, but instead can be expressed through the design of a probability model." ></td>
	<td class="line x" title="167:176	The parameters of this probability model are then trained, so that they reflect what features of trees are relevant to parsing." ></td>
	<td class="line x" title="168:176	The kernel is then derived from this trained model in such a way as to maximize its usefulness for reranking." ></td>
	<td class="line x" title="169:176	We performed experiments on parse reranking using a neural network based statistical parser as both the probabilistic model and the source of the list of candidate parses." ></td>
	<td class="line x" title="170:176	We used a modification of the Voted Perceptron algorithm to perform reranking with the kernel." ></td>
	<td class="line x" title="171:176	The results were amongst the best current statistical parsers, and only 0.2% worse than the best current parsing methods which use kernels." ></td>
	<td class="line x" title="172:176	We would expect further improvement if we used different models to derive the kernel and to gener187 ate the candidates, thereby exploiting the advantages of combining multiple models, as do the better performing methods using kernels." ></td>
	<td class="line x" title="173:176	In recent years, probabilistic models have become commonplace in natural language processing." ></td>
	<td class="line x" title="174:176	We believe that this approach to defining kernels would simplify the problem of defining kernels for these tasks, and could be very useful for many of them." ></td>
	<td class="line x" title="175:176	In particular, maximum entropy models also use a normalized exponential function to estimate probabilities, so all the methods discussed in this paper would be applicable to maximum entropy models." ></td>
	<td class="line x" title="176:176	This approach would be particularly useful for tasks where there is less data available than in parsing, for which large-margin methods work particularly well." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W05-1505
Corrective Modeling For Non-Projective Dependency Parsing
Hall, Keith B.;Novák, Václav;"></td>
	<td class="line x" title="1:197	Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 4252, Vancouver, October 2005." ></td>
	<td class="line x" title="2:197	c2005 Association for Computational Linguistics Corrective Modeling for Non-Projective Dependency Parsing Keith Hall Center for Language and Speech Processing Johns Hopkins University Baltimore, MD 21218 keith hall@jhu.edu Vaclav Novak Institute of Formal and Applied Linguistics Charles University Prague, Czech Republic novak@ufal.mff.cuni.cz Abstract We present a corrective model for recovering non-projective dependency structures from trees generated by state-of-theart constituency-based parsers." ></td>
	<td class="line x" title="3:197	The continuity constraint of these constituencybased parsers makes it impossible for them to posit non-projective dependency trees." ></td>
	<td class="line x" title="4:197	Analysis of the types of dependency errors made by these parsers on a Czech corpus show that the correct governor is likely to be found within a local neighborhood of the governor proposed by the parser." ></td>
	<td class="line x" title="5:197	Our model, based on a MaxEnt classifier, improves overall dependency accuracy by.7% (a 4.5% reduction in error) with over 50% accuracy for non-projective structures." ></td>
	<td class="line pc" title="6:197	1 Introduction Statistical parsing models have been shown to be successful in recovering labeled constituencies (Collins, 2003; Charniak and Johnson, 2005; Roark and Collins, 2004) and have also been shown to be adequate in recovering dependency relationships (Collins et al. , 1999; Levy and Manning, 2004; Dubey and Keller, 2003)." ></td>
	<td class="line x" title="7:197	The most successful models are based on lexicalized probabilistic context free grammars (PCFGs) induced from constituencybased treebanks." ></td>
	<td class="line x" title="8:197	The linear-precedence constraint of these grammars restricts the types of dependency structures that can be encoded in such trees." ></td>
	<td class="line x" title="9:197	1 A shortcoming of the constituency-based paradigm for parsing is that it is inherently incapable of representing non-projective dependencies trees (we define non-projectivity in the following section)." ></td>
	<td class="line x" title="10:197	This is particularly problematic when parsing free wordorder languages, such as Czech, due to the frequency of sentences with non-projective constructions." ></td>
	<td class="line x" title="11:197	In this work, we explore a corrective model which recovers non-projective dependency structures by training a classifier to select correct dependency pairs from a set of candidates based on parses generated by a constituency-based parser." ></td>
	<td class="line x" title="12:197	We chose to use this model due to the observations that the dependency errors made by the parsers are generally local errors." ></td>
	<td class="line x" title="13:197	For the nodes with incorrect dependency links in the parser output, the correct governor of a node is often found within a local context of the proposed governor." ></td>
	<td class="line x" title="14:197	By considering alternative dependencies based on local deviations of the parser output we constrain the set of candidate governors for each node during the corrective procedure." ></td>
	<td class="line x" title="15:197	We examine two state-of-the-art constituencybased parsers in this work: the Collins Czech parser (1999) and a version of the Charniak parser (2001) that was modified to parse Czech." ></td>
	<td class="line xc" title="16:197	Alternative efforts to recover dependency structure from English are based on reconstructing the movement traces encoded in constituency trees (Collins, 2003; Levy and Manning, 2004; Johnson, 2002; Dubey and Keller, 2003)." ></td>
	<td class="line x" title="17:197	In fact, the fea1 In order to correctly capture the dependency structure, coindexed movement traces are used in a form similar to government and Binding theory, GPSG, etc. 42 w c w a w b bca w c bca w a w b w c w a w b bca Figure 1: Examples of projective and non-projective trees." ></td>
	<td class="line x" title="18:197	The trees on the left and center are both projective." ></td>
	<td class="line x" title="19:197	The tree on the right is non-projective." ></td>
	<td class="line x" title="20:197	tures we use in the current model are similar to those proposed by Levy and Manning (2004)." ></td>
	<td class="line x" title="21:197	However, the approach we propose discards the constituency structure prior to the modeling phase; we model corrective transformations of dependency trees." ></td>
	<td class="line x" title="22:197	The technique proposed in this paper is similar to that of recent parser reranking approaches (Collins, 2000; Charniak and Johnson, 2005); however, while reranking approaches allow a parser to generate a likely candidate set according to a generative model, we consider a set of candidates based on local perturbations of the single most likely tree generated." ></td>
	<td class="line x" title="23:197	The primary reason for such an approach is that we allow dependency structures which would never be hypothesized by the parser." ></td>
	<td class="line x" title="24:197	Specifically, we allow for non-projective dependencies." ></td>
	<td class="line x" title="25:197	The corrective algorithm proposed in this paper shares the motivation of the transformation-based learning work (Brill, 1995)." ></td>
	<td class="line x" title="26:197	We do consider local transformations of the dependency trees; however, the technique presented here is based on a generative model that maximizes the likelihood of good dependents." ></td>
	<td class="line x" title="27:197	We consider a finite set of local perturbations of the tree and use a fixed model to select the best tree by independently choosing optimal dependency links." ></td>
	<td class="line x" title="28:197	In the remainder of the paper we provide a definition of a dependency tree and the motivation for using such trees as well as a description of the particular dataset that we use in our experiments, the Prague Dependency Treebank (PDT)." ></td>
	<td class="line x" title="29:197	In Section 3 we describe the techniques used to adapt constituencybased parsers to train from and generate dependency trees." ></td>
	<td class="line x" title="30:197	Section 4 describes corrective modeling as used in this work and Section 4.2 describes the particular features with which we have experimented." ></td>
	<td class="line x" title="31:197	Section 5 presents the results of a set of experiments we performed on data from the PDT." ></td>
	<td class="line x" title="32:197	2 Syntactic Dependency Trees and the Prague Dependency Treebank A dependency tree is a set of nodes = {w 0,w 1,,w k } where w 0 is the imaginary root node 2 and a set of dependency links G = {g 1,,g k } where g i is an index into  representing the governor of w i . In other words g 3 =1indicates that the governor of w 3 is w 1 . Finally, every node has exactly one governor except for w 0,which has no governor (the tree constraints)." ></td>
	<td class="line x" title="33:197	3 The index of the nodes represents the surface order of the nodes in the sequence (i.e. , w i precedes w j in the sentence if i<j)." ></td>
	<td class="line x" title="34:197	A tree is projective if for every three nodes: w a, w b,andw c where a<b<c;ifw a is governed by w c then w b is transitively governed by w c or if w c is governed by w a then w b is transitively governed by w a . 4 Figure 1 shows examples of projective and non-projective trees." ></td>
	<td class="line x" title="35:197	The rightmost tree, which is non-projective, contains a subtree consisting of w a and w c but not w b ;however,w b occurs between w a and w c in the linear ordering of the nodes." ></td>
	<td class="line x" title="36:197	Projectivity in a dependency tree is akin to the continuity constraint in a constituency tree; such a constraint is 2 The imaginary root node simplifies notation." ></td>
	<td class="line x" title="37:197	3 The dependency structures here are very similar to those described by Melcuk (1988); however the nodes of the dependency trees discussed in this paper are limited to the words of the sentence and are always ordered according to the surface word-order." ></td>
	<td class="line x" title="38:197	4 Node w a is said to transitively govern node w b if w b is a descendant of w a in the dependency tree." ></td>
	<td class="line x" title="39:197	43 implicitly imposed by trees generated from context free grammars (CFGs)." ></td>
	<td class="line x" title="40:197	Strict word-order languages, such as English, exhibit non-projective dependency structures in a relatively constrained set of syntactic configurations (e.g. , right-node raising)." ></td>
	<td class="line x" title="41:197	Traditionally, these movements are encoded in syntactic analyses as traces." ></td>
	<td class="line x" title="42:197	In languages with free word-order, such as Czech, constituency-based representations are overly constrained (Sgall et al. , 1986)." ></td>
	<td class="line x" title="43:197	Syntactic dependency trees encode syntactic subordination relationships allowing the structure to be non-specific about the underlying deep representation." ></td>
	<td class="line x" title="44:197	The relationship between a node and its subordinates expresses a sense of syntactic (functional) entailment." ></td>
	<td class="line x" title="45:197	In this work we explore the dependency structures encoded in the Prague Dependency Treebank (Hajic, 1998; Bohmova et al. , 2002)." ></td>
	<td class="line x" title="46:197	The PDT 1.0 analytical layer is a set of Czech syntactic dependency trees; the nodes of which contain the word forms, morphological features, and syntactic annotations." ></td>
	<td class="line x" title="47:197	These trees were annotated by hand and are intended as an intermediate stage in the annotation of the Tectogrammatical Representation (TR), a deep-syntactic or syntacto-semantic theory of language (Sgall et al. , 1986)." ></td>
	<td class="line x" title="48:197	All current automatic techniques for generating TR structures are based on syntactic dependency parsing." ></td>
	<td class="line x" title="49:197	When evaluating the correctness of dependency trees, we only consider the structural relationships between the words of the sentence (unlabeled dependencies)." ></td>
	<td class="line x" title="50:197	However, the model we propose contains features that are considered part of the dependency rather than the nodes in isolation (e.g. , agreement features)." ></td>
	<td class="line x" title="51:197	We do not propose a model for correctly labeling dependency structures in this work." ></td>
	<td class="line xc" title="52:197	3 Constituency Parsing for Dependency Trees A pragmatic justification for using constituencybased parsers in order to predict dependency structures is that currently the best Czech dependencytree parser is a constituency-based parser (Collins et al. , 1999; Zeman, 2004)." ></td>
	<td class="line x" title="53:197	In fact both Charniaks and Collins generative probabilistic models contain lexical dependency features." ></td>
	<td class="line x" title="54:197	5 From a generative modeling perspective, we use the constraints imposed by constituents (i.e. , projectivity) to enable the encapsulation of syntactic substructures." ></td>
	<td class="line x" title="55:197	This directly leads to efficient parsing algorithms such as the CKY algorithm and related agenda-based parsing algorithms (Manning and Schutze, 1999)." ></td>
	<td class="line x" title="56:197	Additionally, this allows for the efficient computation of the scores for the dynamic-programming state variables (i.e. , the inside and outside probabilities) that are used in efficient statistical parsers." ></td>
	<td class="line x" title="57:197	The computational complexity advantages of dynamic programming techniques along with efficient search techniques (Caraballo and Charniak, 1998; Klein and Manning, 2003) allow for richer predictive models which include local contextual information." ></td>
	<td class="line x" title="58:197	In an attempt to extend a constituency-based parsing model to train on dependency trees, Collins transforms the PDT dependency trees into constituency trees (Collins et al. , 1999)." ></td>
	<td class="line x" title="59:197	In order to accomplish this task, he first normalizes the trees to remove non-projectivities." ></td>
	<td class="line x" title="60:197	Then, he creates artificial constituents based on the parts-of-speech of the words associated with each dependency node." ></td>
	<td class="line x" title="61:197	The mapping from dependency tree to constituency tree is not one-to-one." ></td>
	<td class="line x" title="62:197	Collins describes a heuristic for choosing trees that work well with his parsing model." ></td>
	<td class="line x" title="63:197	3.1 Training a Constituency-based Parser We consider two approaches to creating projective trees from dependency trees exhibiting nonprojectivities." ></td>
	<td class="line x" title="64:197	The first is based on word-reordering and is the model that was used with the Collins parser." ></td>
	<td class="line x" title="65:197	This algorithm identifies non-projective structures and deterministically reorders the words of the sentence to create projective trees." ></td>
	<td class="line x" title="66:197	An alternative method, used by Charniak in the adaptation of his parser for Czech 6 and used by Nivre and Nilsson (2005), alters the dependency links by raising the governor to a higher node in the tree whenever 5 Bilexical dependencies are components of both the Collins and Charniak parsers and effectively model the types of syntactic subordination that we wish to extract in a dependency tree." ></td>
	<td class="line x" title="67:197	(Bilexical models were also proposed by Eisner (Eisner, 1996))." ></td>
	<td class="line x" title="68:197	In the absence of lexicalization, both parsers have dependency features that are encoded as head-constituent to sibling features." ></td>
	<td class="line x" title="69:197	6 This information was provided by Eugene Charniak in a personal communication." ></td>
	<td class="line x" title="70:197	44 Density 0.0 0.2 0.4 0.6 0.8 1.0 0.005 0.006 0.02 0.843 0.084 0.023 0.009 0.005 0.004 less 2 1 1 2 3 4 5 more Density 0.0 0.2 0.4 0.6 0.8 1.0 0.005 0.006 0.022 0.824 0.092 0.029 0.012 0.005 0.005 less 2 1 1 2 3 4 5 more (a)Charniak (b)Collins Figure 2: Statistical distribution of correct governor positions in the Charniak (left) and Collins (right) parser output of parsed PDT development data." ></td>
	<td class="line x" title="71:197	a non-projectivity is observed." ></td>
	<td class="line x" title="72:197	The trees are then transformed into Penn Treebank style constituencies using the technique described in (Collins et al. , 1999)." ></td>
	<td class="line x" title="73:197	Both of these techniques have advantages and disadvantages which we briefly outline here: Reordering The dependency structure is preserved, but the training procedure will learn statistics for structures over word-strings that may not be part of the language." ></td>
	<td class="line x" title="74:197	The parser, however, may be capable of constructing parses for any string of words if a smoothed grammar is being used." ></td>
	<td class="line x" title="75:197	GovernorRaising The dependency structure is corrupted leading the parser to incorporate arbitrary dependency statistics into the model." ></td>
	<td class="line x" title="76:197	However, the parser is trained on true sentences, the words of which are in the correct linear order." ></td>
	<td class="line x" title="77:197	We expect the parser to predict similar incorrect dependencies when sentences similar to the training data are observed." ></td>
	<td class="line x" title="78:197	Although the results presented in (Collins et al. , 1999) used the reordering technique, we have experimented with his parser using the governorraising technique and observe an increase in dependency accuracy." ></td>
	<td class="line x" title="79:197	For the remainder of the paper, we assume the governorraising technique." ></td>
	<td class="line x" title="80:197	The process of generating dependency trees from parsed constituency trees is relatively straightforward." ></td>
	<td class="line x" title="81:197	Both the Collins and Charniak parsers provide head-word annotation on each constituent." ></td>
	<td class="line x" title="82:197	This is precisely the information that we encode in an unlabeled dependency tree, so the dependency structure can simply be extracted from the parsed constituency trees." ></td>
	<td class="line x" title="83:197	Furthermore, the constituency labels can be used to identify the dependency labels; however, we do not attempt to identify correct dependency labels in this work." ></td>
	<td class="line x" title="84:197	3.2 Constituency-based errors We now discuss a quantitative measure for the types of dependency errors made by constituency-based parsing techniques." ></td>
	<td class="line x" title="85:197	For node w i and the correct governor w g  i the distance between the two nodes in the hypothesized dependency tree is: dist(w i,w g  i ) =      d(w i,w g  i ) iff w g  i is ancestor of w i d(w i,w g  i ) iff w g  i is sibling/cousin of w i d(w i,w g  i ) iff w g  i is descendant of w i Ancestor, sibling, cousin, and descendant have the standard interpretation in the context of a tree." ></td>
	<td class="line x" title="86:197	The dependency distance d(w i,w g  i ) is the minimum number of dependency links traversed on the undirected path from w i to w g  i in the hypothesized dependency tree." ></td>
	<td class="line x" title="87:197	The definition of the dist function makes a distinction between paths through the parent of w i (positive values) and paths through chil45 CORRECT(W) 1 Parse sentence W using the constituency-based parser 2 Generate a dependency structure from the constituency tree 3 for w i  W 4 do for w c N(w g h i ) // Local neighborhood of proposed governor 5 do l(c)  P(g  i = c|w i,N(w g h i )) 6 g prime i  arg max c l(c) // Pick the governor in which we are most confident Table 1: Corrective Modeling Procedure dren of w i (negative values)." ></td>
	<td class="line x" title="88:197	We found that a vast majority of the correct governors were actually hypothesized as siblings or grandparents (a dist values of 2)  an extreme local error." ></td>
	<td class="line x" title="89:197	Figure 2 shows a histogram of the fraction of nodes whose correct governor was within a particular dist in the hypothesized tree." ></td>
	<td class="line x" title="90:197	A dist of 1 indicates the correct governor was selected by the parser; in these graphs, the density at dist =1(on the x axis) shows the baseline dependency accuracy of each parser." ></td>
	<td class="line x" title="91:197	Note that if we repaired only the nodes that are within a dist of 2 (grandparents and siblings), we can recover more than 50% of the incorrect dependency links (a raw accuracy improvement of up to 9%)." ></td>
	<td class="line x" title="92:197	We believe this distribution to be indirectly caused by the governor raising projectivization routine." ></td>
	<td class="line x" title="93:197	In the cases where non-projective structures can be repaired by raising the nodes governor to its parent, the correct governor becomes a sibling of the node." ></td>
	<td class="line x" title="94:197	4 Corrective Modeling The error analysis of the previous section suggests that by looking only at a local neighborhood of the proposed governor in the hypothesized trees, we can correct many of the incorrect dependencies." ></td>
	<td class="line x" title="95:197	This fact motivates the corrective modeling procedure employed here." ></td>
	<td class="line x" title="96:197	Table 1 presents the pseudo-code for the corrective procedure." ></td>
	<td class="line x" title="97:197	The set g h contains the indices of governors as predicted by the parser." ></td>
	<td class="line x" title="98:197	The set of governors predicted by the corrective procedure is denoted as g prime . The procedure independently corrects each node of the parsed trees meaning that there is potential for inconsistent governor relationships to exist in the proposed set; specifically, the resulting dependency graph may have cycles." ></td>
	<td class="line x" title="99:197	We employ a greedy search to remove cycles when they are present in the output graph." ></td>
	<td class="line x" title="100:197	The final line of the algorithm picks the governor in which we are most confident." ></td>
	<td class="line x" title="101:197	We use the correctgovernor classification likelihood, P(g  i = j|w i,N(w g h i )), as a measure of the confidence that w c is the correct governor of w i where the parser had proposed w g h i as the governor." ></td>
	<td class="line x" title="102:197	In effect, we create a decision list using the most likely decision if we can (i.e. , there are no cycles)." ></td>
	<td class="line x" title="103:197	If the dependency graph resulting from the most likely decisions does not result in a tree, we use the decision lists to greedily select the tree for which the product of the independent decisions is maximal." ></td>
	<td class="line x" title="104:197	Training the corrective model requires pairs of dependency trees; each pair contains a manuallyannotated tree (i.e. , the gold standard tree) and a tree generated by the parser." ></td>
	<td class="line x" title="105:197	This data is trivially transformed into per-node samples." ></td>
	<td class="line x" title="106:197	For each node w i in the tree, there are |N(w g h i )| samples; one for each governor candidate in the local neighborhood." ></td>
	<td class="line x" title="107:197	One advantage to the type of corrective algorithm presented here is that it is completely disconnected from the parser used to generate the tree hypotheses." ></td>
	<td class="line x" title="108:197	This means that the original parser need not be statistical or even constituency based." ></td>
	<td class="line x" title="109:197	What is critical for this technique to work is that the distribution of dependency errors be relatively local as is the case with the errors made by the Charniak and Collins parsers." ></td>
	<td class="line x" title="110:197	This can be determined via data analysis using the dist metric." ></td>
	<td class="line x" title="111:197	Determining the size of the local neighborhood is data dependent." ></td>
	<td class="line x" title="112:197	If subordinate nodes are considered as candidate governors, then a more robust cycle removal technique is be required." ></td>
	<td class="line x" title="113:197	46 4.1 MaxEnt Estimation We have chosen a MaxEnt model to estimate the governor distributions, P(g  i = j|w i,N(w g h i )).In the next section we outline the feature set with which we have experimented, noting that the features are selected based on linguistic intuition (specifically for Czech)." ></td>
	<td class="line x" title="114:197	We choose not to factor the feature vector as it is not clear what constitutes a reasonable factorization of these features." ></td>
	<td class="line x" title="115:197	For this reason we use the MaxEnt estimator which provides us with the flexibility to incorporate interdependent features independently while still optimizing for likelihood." ></td>
	<td class="line x" title="116:197	The maximum entropy principle states that we wish to find an estimate of p(y|x) Cthat maximizes the entropy over a sample set X for some set of observations Y,wherex  X is an observation and y  Y is a outcome label assigned to that observation, H(p)  summationdisplay xX,yY p(x)p(y|x)logp(y|x) The set C is the candidate set of distributions from which we wish to select p(y|x)." ></td>
	<td class="line x" title="117:197	We define this set as the p(y|x) that meets a feature-based expectation constraint." ></td>
	<td class="line x" title="118:197	Specifically, we want the expected count of a feature, f(x,y), to be equivalent under the distribution p(y|x) and under the observed distribution p(y|x)." ></td>
	<td class="line x" title="119:197	summationdisplay xX,yY p(x)p(y|x)f i (x,y) = summationdisplay xX,yY p(x)p(y|x)f i (x,y) f i (x,y) is a feature of our model with which we capture correlations between observations and outcomes." ></td>
	<td class="line x" title="120:197	In the following section, we describe a set of features with which we have experimented to determine when a word is likely to be the correct governor of another word." ></td>
	<td class="line x" title="121:197	We incorporate the expected feature-count constraints into the maximum entropy objective using Lagrange multipliers (additionally, constraints are added to ensure the distributions p(y|x) are consistent probability distributions): H(p) + summationdisplay i  i summationdisplay xX,yY parenleftbigg p(x)p(y|x)f i (x,y) p(x)p(y|x)f i (x,y) parenrightbigg +  summationdisplay yY p(y|x)  1 Holding the  i s constant, we compute the unconstrained maximum of the above Lagrangian form: p  (y|x)= 1 Z  (x) exp( summationdisplay i  i f i (x,y)) Z  (x)= summationdisplay yY exp( summationdisplay i  i f i (x,y)) giving us the log-linear form of the distributions p(y|x) in C (Z is a normalization constant)." ></td>
	<td class="line x" title="122:197	Finally, we compute the  i s that maximize the objective function:  summationdisplay xX p(x)logZ  (x)+ summationdisplay i  i p(x,y)f i (x,y) A number of algorithms have been proposed to efficiently compute the optimization described in this derivation." ></td>
	<td class="line x" title="123:197	For a more detailed introduction to maximum entropy estimation see (Berger et al. , 1996)." ></td>
	<td class="line x" title="124:197	4.2 Proposed Model Given the above formulation of the MaxEnt estimation procedure, we define features over pairs of observations and outcomes." ></td>
	<td class="line x" title="125:197	In our case, the observations are simply w i, w c,andN(w g h i ) and the outcome is a binary variable indicating whether c = g  i (i.e. , w c is the correct governor)." ></td>
	<td class="line x" title="126:197	In order to limit the dimensionality of the feature space, we consider feature functions over the outcome, the current node w i, the candidate governor node w c and the node proposed as the governor by the parser w g h i . Table 2 describes the general classes of features used." ></td>
	<td class="line x" title="127:197	We write F i to indicate the form of the current child node, F c for the form of the candidate, and F g as the form of the governor proposed by the parser." ></td>
	<td class="line x" title="128:197	A combined feature is denoted as L i T c and indicates we observed a particular lemma for the current node with a particular tag of the candidate." ></td>
	<td class="line x" title="129:197	47 Feature Type Id Description Form F the fully inflected word form as it appears in the data Lemma L the morphologically reduced lemma MTag T a subset of the morphological tag as described in (Collins et al. , 1999) POS P major part-of-speech tag (first field of the morphological tag) ParserGov G true if candidate was proposed as governor by parser ChildCount C the number of children Agreement A(x,y) check for case/number agreement between word x and y Table 2: Description of the classes of features used In all models, we include features containing the form, the lemma, the morphological tag, and the ParserGov feature." ></td>
	<td class="line x" title="130:197	We have experimented with different sets of feature combinations." ></td>
	<td class="line x" title="131:197	Each combination set is intended to capture some intuitive linguistic correlation." ></td>
	<td class="line x" title="132:197	For example, the feature component L i T c will fire if a particular childs lemma L i is observed with a particular candidates morphological tag T c . This feature is intended to capture phenomena surrounding particles; for example, in Czech, the governor of the reflexive particle se will likely be a verb." ></td>
	<td class="line x" title="133:197	4.3 Related Work Recent work by Nivre and Nilsson introduces a technique where the projectivization transformation is encoded in the non-terminals of constituents during parsing (Nivre and Nilsson, 2005)." ></td>
	<td class="line x" title="134:197	This allows for a deterministic procedure that undoes the projectivization in the generated parse trees, creating non-projective structures." ></td>
	<td class="line x" title="135:197	This technique could be incorporated into a statistical parsing framework, however we believe the sparsity of such nonprojective configurations may be problematic when using smoothed backed-off grammars." ></td>
	<td class="line x" title="136:197	We suspect that the deterministic procedure employed by Nivre and Nilsson enables their parser to greedily consider non-projective constructions when possible." ></td>
	<td class="line x" title="137:197	This may also explain the relatively low overall performance of their parser." ></td>
	<td class="line x" title="138:197	A primary difference between the Nivre and Nilsson approach and what we propose in this paper is that of determining the projectivization procedure." ></td>
	<td class="line x" title="139:197	While we exploit particular side-effects of the projectivization procedure, we do not assume any particular algorithm." ></td>
	<td class="line x" title="140:197	Additionally, we consider transformations for all dependency errors where their technique explicitly addresses non-projectivity errors." ></td>
	<td class="line x" title="141:197	We mentioned above that our approach appears to be similar to that of reranking for statistical parsing (Collins, 2000; Charniak and Johnson, 2005)." ></td>
	<td class="line x" title="142:197	While it is true that we are improving upon the output of the automatic parser, we are not considering multiple alternate parses." ></td>
	<td class="line x" title="143:197	Instead, we consider a complete set of alternate trees that are minimal perturbations of the best tree generated by the parser." ></td>
	<td class="line x" title="144:197	In the context of dependency parsing, we do this in order to generate structures that constituency-based parsers are incapable of generating (i.e. , non-projectivities)." ></td>
	<td class="line x" title="145:197	Recent work by Smith and Eisner (2005) on contrastive estimation suggests similar techniques to generate local neighborhoods of a parse; however, the purpose in their work is to define an approximation to the partition function for log-linear estimation (i.e. , the normalization factor in a MaxEnt model)." ></td>
	<td class="line x" title="146:197	5 Empirical Results In this section we report results from experiments on the PDT Czech dataset." ></td>
	<td class="line x" title="147:197	Approximately 1.9% of the words dependencies are non-projective in version 1.0 of this corpus and these occur in 23.2% of the sentences (Hajicova et al. , 2004)." ></td>
	<td class="line x" title="148:197	We used the standard training, development, and evaluation datasets defined in the PDT documentation for all experiments." ></td>
	<td class="line x" title="149:197	7 We use Zhang Lees implementation of the 7 We have used PDT 1.0 (2002) data for the Charniak experiments and PDT 2.0 (2005) data for the Collins experiments." ></td>
	<td class="line x" title="150:197	We use the most recent version of each parser; however we do not have a training program for the Charniak parser and have used the pretrained parser provided by Charniak; this was trained on the training section of the PDT 1.0." ></td>
	<td class="line x" title="151:197	We train our model on the 48 Model Features Description Count ChildCount count of children for the three nodes MTagL T i T c,L i L c,L i T c,T i L c,T i P g conjunctions of MTag and Lemmas MTagF T i T c,F i F c,F i T c,T i F c,T i P g conjunctions of MTag and Forms POSL P i,P c,P g,P i P c P g,P i P g,P c L c conjunctions of POS and Lemma TTT T i T c T g conjunction of tags for each of the three nodes Agr A(T i,T c ),A(T i,T g ) binary feature if case/number agree Trig L i L g T c,T i L g T c,L i L g L c trigrams of Lemma/Tag Table 3: Model feature descriptions." ></td>
	<td class="line x" title="152:197	Model Charniak Parse Trees Collins Parse Trees Devel." ></td>
	<td class="line x" title="153:197	Accuracy NonP Accuracy Devel." ></td>
	<td class="line x" title="154:197	Accuracy NonP Accuracy Baseline 84.3% 15.9% 82.4% 12.0% Simple 84.3% 16.0% 82.5% 12.2% Simple + Count 84.3% 16.7% 82.5% 13.8% Simple + MtagL 84.8% 43.5% 83.2% 44.1% Simple + MtagF 84.8% 42.2% 83.2% 43.2% Simple + POS 84.3% 16.0% 82.4% 12.1% Simple + TTT 84.3% 16.0% 82.5% 12.2% Simple + Agr 84.3% 16.2% 82.5% 12.2% Simple + Trig 84.9% 47.9% 83.1% 47.7% All Features 85.0% 51.9% 83.5% 57.5% Table 4: Comparative results for different versions of our model on the Charniak and Collins parse trees for the PDT development data." ></td>
	<td class="line x" title="155:197	MaxEnt estimator using the L-BFGS optimization algorithms and Gaussian smoothing." ></td>
	<td class="line x" title="156:197	8 Table 4 presents results on development data for the correction model with different feature sets." ></td>
	<td class="line x" title="157:197	The features of the Simple model are the form (F), lemma (L), and morphological tag (M) for the each node, the parser-proposed governor node, and the candidate node; this model also contains the ParserGov feature." ></td>
	<td class="line x" title="158:197	In the tables following rows, we show the results for the simple model augmented with feature sets of the categories described in Table 2." ></td>
	<td class="line x" title="159:197	Table 3 provides a short description of each of the models." ></td>
	<td class="line x" title="160:197	As we believe the Simple model provides the minimum information needed to perform this task, Collins trees via a 20-fold Jackknife training procedure." ></td>
	<td class="line x" title="161:197	8 Using held-out development data, we determined a Gaussian prior parameter setting of 4 worked best." ></td>
	<td class="line x" title="162:197	The optimal number of training iterations was chosen on held-out data for each experiment." ></td>
	<td class="line x" title="163:197	This was generally in the order of a couple hundred iterations of L-BFGS." ></td>
	<td class="line x" title="164:197	The MaxEnt modeling implementation can be found at http://homepages.inf.ed.ac." ></td>
	<td class="line x" title="165:197	uk/s0450736/maxent_toolkit.html." ></td>
	<td class="line x" title="166:197	we experimented with the feature-classes as additions to it." ></td>
	<td class="line x" title="167:197	The final row of Table 4 contains results for the model which includes all features from all other models." ></td>
	<td class="line x" title="168:197	We define NonP Accuracy as the accuracy for the nodes which were non-projective in the original trees." ></td>
	<td class="line x" title="169:197	Although both the Charniak and the Collins parser can never produce non-projective trees, the baseline NonP accuracy is greater than zero." ></td>
	<td class="line x" title="170:197	This is due to the parser making mistakes in the tree such that the originally non-projective nodes dependency is projective." ></td>
	<td class="line x" title="171:197	Alternatively, we report the Non-Projective Precision and Recall for our experiment suite in Table 5." ></td>
	<td class="line x" title="172:197	Here the numerator of the precision is the number of nodes that are non-projective in the correct tree and end up in a non-projective configuration; however, this new configuration may be based on incorrect dependencies." ></td>
	<td class="line x" title="173:197	Recall is the obvious counterpart to precision." ></td>
	<td class="line x" title="174:197	These values correspond to the NonP 49 Model Charniak Parse Trees Collins Parse Trees Precision Recall F-measure Precision Recall F-measure Baseline N/A 0.0% 0.000 N/A 0.0% 0.000 Simple 22.6% 0.3% 0.592 5.0% 0.2% 0.385 Simple + Count 37.3% 1.1% 2.137 16.8% 2.0% 3.574 Simple + MtagL 78.0% 29.7% 43.020 62.4% 35.0% 44.846 Simple + MtagF 78.7% 28.6% 41.953 62.0% 34.3% 44.166 Simple + POS 23.3% 0.3% 0.592 2.5% 0.1% 0.192 Simple + TTT 20.7% 0.3% 0.591 6.1% 0.2% 0.387 Simple + Agr 40.0% 0.5% 0.988 5.7% 0.2% 0.386 Simple + Trig 74.6% 35.0% 47.646 52.3% 40.2% 45.459 All Features 75.7% 39.0% 51.479 48.1% 51.6% 49.789 Table 5: Alternative non-projectivity scores for different versions of our model on the Charniak and Collins parse trees." ></td>
	<td class="line x" title="175:197	accuracy results reported in Table 4." ></td>
	<td class="line x" title="176:197	From these tables, we see that the most effective features (when used in isolation) are the conjunctive MTag/Lemma, MTag/Form, and Trigram MTag/Lemma features." ></td>
	<td class="line x" title="177:197	Model Dependency NonP Accuracy Accuracy Collins 81.6% N/A Collins + Corrective 82.8% 53.1% Charniak 84.4% N/A Charniak + Corrective 85.1% 53.9% Table 6: Final results on PDT evaluation datasets for Collins and Charniaks trees with and without the corrective model Finally, Table 6 shows the results of the full model run on the evaluation data for the Collins and Charniak parse trees." ></td>
	<td class="line x" title="178:197	It appears that the Charniak parser fares better on the evaluation data than does the Collins parser." ></td>
	<td class="line x" title="179:197	However, the corrective model is still successful at recovering non-projective structures." ></td>
	<td class="line x" title="180:197	Overall, we see a significant improvement in the dependency accuracy." ></td>
	<td class="line x" title="181:197	We have performed a review of the errors that the corrective process makes and observed that the model does a poor job dealing with punctuation." ></td>
	<td class="line x" title="182:197	This is shown in Table 7 along with other types of nodes on which we performed well and poorly, respectively." ></td>
	<td class="line x" title="183:197	Collins (1999) explicitly added features to his parser to improve punctuation dependency parsing accuracy." ></td>
	<td class="line x" title="184:197	The PARSEVAL evaluation metTop Five Good/Bad Repairs Well repaired child seisiazjen Well repaired false governor vvsak li na o Well repaired real governor ajestat ba, Poorly repaired child,senaze Poorly repaired false governor a,vsak musli Poorly repaired real governor root sklo, je Table 7: Categorization of corrections and errors made by our model on trees from the Charniak parser." ></td>
	<td class="line x" title="185:197	root is the artificial root node of the PDT tree." ></td>
	<td class="line x" title="186:197	For each node position (child, proposed parent, and correct parent), the top five words are reported (based on absolute count of occurrences)." ></td>
	<td class="line x" title="187:197	The particle se occurs frequently explaining why it occurs in the top five good and top five bad repairs." ></td>
	<td class="line x" title="188:197	Charniak Collins Correct to incorrect 13.0% 20.0% Incorrect to incorrect 21.6% 25.8% Incorrect to correct 65.5% 54.1% Table 8: Categorization of corrections made by our model on Charniak and Collins trees." ></td>
	<td class="line x" title="189:197	ric for constituency-based parsing explicitly ignores punctuation in determining the correct boundaries of constituents (Harrison et al. , 1991) and so should the dependency evaluation." ></td>
	<td class="line x" title="190:197	However, the reported results include punctuation for comparative purposes." ></td>
	<td class="line x" title="191:197	Finally, we show in Table 8 a coarse analysis of the corrective performance of our model." ></td>
	<td class="line x" title="192:197	We are repair50 ing more dependencies than we are corrupting." ></td>
	<td class="line x" title="193:197	6Conclusion We have presented a Maximum Entropy-based corrective model for dependency parsing." ></td>
	<td class="line x" title="194:197	The goal is to recover non-projective dependency structures that are lost when using state-of-the-art constituencybased parsers; we show that our technique recovers over 50% of these dependencies." ></td>
	<td class="line x" title="195:197	Our algorithm provides a simple framework for corrective modeling of dependency trees, making no prior assumptions about the trees." ></td>
	<td class="line x" title="196:197	However, in the current model, we focus on trees with local errors." ></td>
	<td class="line x" title="197:197	Overall, our technique improves dependency parsing and provides the necessary mechanism to recover non-projective structures." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W05-1515
Constituent Parsing By Classification
Turian, Joseph P.;Melamed, I. Dan;"></td>
	<td class="line x" title="1:241	Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 141151, Vancouver, October 2005." ></td>
	<td class="line x" title="2:241	c2005 Association for Computational Linguistics Constituent Parsing by Classification Joseph Turian and I. Dan Melamed {lastname}@cs.nyu.edu Computer Science Department New York University New York, New York 10003 Abstract Ordinary classification techniques can drive a conceptually simple constituent parser that achieves near state-of-the-art accuracy on standard test sets." ></td>
	<td class="line x" title="3:241	Here we present such a parser, which avoids some of the limitations of other discriminative parsers." ></td>
	<td class="line x" title="4:241	In particular, it does not place any restrictions upon which types of features are allowed." ></td>
	<td class="line x" title="5:241	We also present several innovations for faster training of discriminative parsers: we show how training can be parallelized, how examples can be generated prior to training without a working parser, and how independently trained sub-classifiers that have never done any parsing can be effectively combined into a working parser." ></td>
	<td class="line x" title="6:241	Finally, we propose a new figure-of-merit for bestfirst parsing with confidence-rated inferences." ></td>
	<td class="line x" title="7:241	Our implementation is freely available at: http://cs.nyu.edu/~turian/ software/parser/ 1 Introduction Discriminative machine learning methods have improved accuracy on many NLP tasks, such as POStagging (Toutanova et al. , 2003), machine translation (Och & Ney, 2002), and relation extraction (Zhao & Grishman, 2005)." ></td>
	<td class="line x" title="8:241	There are strong reasons to believe the same would be true of parsing." ></td>
	<td class="line x" title="9:241	However, only limited advances have been made thus far, perhaps due to various limitations of extant discriminative parsers." ></td>
	<td class="line x" title="10:241	In this paper, we present some innovations aimed at reducing or eliminating some of these limitations, specifically for the task of constituent parsing:  We show how constituent parsing can be performed using standard classification techniques." ></td>
	<td class="line x" title="11:241	 Classifiers for different non-terminal labels can be induced independently and hence training can be parallelized." ></td>
	<td class="line x" title="12:241	 The parser can use arbitrary information to evaluate candidate constituency inferences." ></td>
	<td class="line x" title="13:241	 Arbitrary confidence scores can be aggregated in a principled manner, which allows beam search." ></td>
	<td class="line x" title="14:241	In Section 2 we describe our approach to parsing." ></td>
	<td class="line x" title="15:241	In Section 3 we present experimental results." ></td>
	<td class="line x" title="16:241	The following terms will help to explain our work." ></td>
	<td class="line x" title="17:241	A span is a range over contiguous words in the input sentence." ></td>
	<td class="line x" title="18:241	Spans cross if they overlap but neither contains the other." ></td>
	<td class="line x" title="19:241	An item (or constituent) is a (span,label) pair." ></td>
	<td class="line x" title="20:241	A state is a set of parse items, none of which may cross." ></td>
	<td class="line x" title="21:241	A parse inference is a pair (S,i), given by the current state S and an item i to be added to it." ></td>
	<td class="line x" title="22:241	A parse path (or history) is a sequence of parse inferences over some input sentence (Klein & Manning, 2001)." ></td>
	<td class="line x" title="23:241	An item ordering (ordering, for short) constrains the order in which items may be inferred." ></td>
	<td class="line x" title="24:241	In particular, if we prescribe a complete item ordering, the parser is deterministic (Marcus, 1980) and each state corresponds to a unique parse path." ></td>
	<td class="line x" title="25:241	For some input sentence and gold-standard parse, a state is correct if the parser can infer zero or more additional items to obtain the gold-standard parse." ></td>
	<td class="line x" title="26:241	A parse path is correct if it leads to a correct state." ></td>
	<td class="line x" title="27:241	An 141 inference is correct if adding its item to its state is correct." ></td>
	<td class="line x" title="28:241	2 Parsing by Classification Recall that with typical probabilistic parsers, our goal is to output the parse P with the highest likelihood for the given input sentence x: P=arg max P2P(x) Pr(P) (1) =arg max P2P(x) productdisplay I2P Pr(I) (2) or, equivalently, =arg max P2P(x) summationdisplay I2P log(Pr(I)) (3) where each I is a constituency inference in the parse path P. In this work, we explore a generalization in which each inference I is assigned a real-valued confidence score Q(I) and individual confidences are aggregated using some function A, which need not be a sum or product: P=arg max P2P(x) A I2P Q(I) (4) In Section 2.1 we describe how we induce scoring function Q(I)." ></td>
	<td class="line x" title="29:241	In Section 2.2 we discuss the aggregation function A. In Section 2.3 we describe the method used to restrict the size of the search space over P(x)." ></td>
	<td class="line x" title="30:241	2.1 Learning the Scoring Function Q(I) During training, our goal is to induce the scoring function Q, which assigns a real-valued confidence score Q(I) to each candidate inference I (Equation 4)." ></td>
	<td class="line x" title="31:241	We treat this as a classification task: If inference I is correct, we would like Q(I) to be a positive value, and if inference I is incorrect, we would like Q(I) to be a negative value." ></td>
	<td class="line x" title="32:241	Training discriminative parsers can be computationally very expensive." ></td>
	<td class="line x" title="33:241	Instead of having a single classifier score every inference, we parallelize training by inducing 26 sub-classifiers, one for each constituent label  in the Penn Treebank (Taylor, Marcus, & Santorini, 2003): Q(I ) = Q (I ), where Q is the -classifier and I is an inference that infers a constituent with label." ></td>
	<td class="line x" title="34:241	For example, the VPclassifier QVP would score the VP-inference in Figure 1, preferably assigning it a positive confidence." ></td>
	<td class="line x" title="35:241	Figure 1 A candidate VP-inference, with headchildren annotated using the rules given in (Collins, 1999)." ></td>
	<td class="line x" title="36:241	VP(was) NP(timing) VBD/was ADJP(perfect) DT/The NN/timing JJ/perfect Each-classifier is independently trained on training set E, where each example e 2E is a tuple (I,y), I is a candidate-inference, and y2f 1g." ></td>
	<td class="line x" title="37:241	y=+1 if I is a correct inference and 1 otherwise." ></td>
	<td class="line x" title="38:241	This approach differs from that of Yamada and Matsumoto (2003) and Sagae and Lavie (2005), who parallelize according to the POS tag of one of the child items." ></td>
	<td class="line x" title="39:241	2.1.1 Generating Training Examples Our method of generating training examples does not require a working parser, and can be run prior to any training." ></td>
	<td class="line x" title="40:241	It is similar to the method used in the literature by deterministic parsers (Yamada & Matsumoto, 2003; Sagae & Lavie, 2005) with one exception: Depending upon the order constituents are inferred, there may be multiple bottom-up paths that lead to the same final parse, so to generate training examples we choose a single random path that leads to the gold-standard parse tree.1 The training examples correspond to all candidate inferences considered in every state along this path, nearly all of which are incorrect inferences (with y = 1)." ></td>
	<td class="line x" title="41:241	For instance, only 4.4% of candidate NP-inferences are correct." ></td>
	<td class="line x" title="42:241	2.1.2 Training Algorithm During training, for each labelwe induce scoring function Q to minimize the loss over training examples E : Q =arg min Q0 summationdisplay (I ;y)2E L(y Q0 (I )) (5) 1 The particular training tree paths used in our experiments are included in the aforementioned implementation so that our results can be replicated under the same experimental conditions." ></td>
	<td class="line x" title="43:241	142 where y Q (I ) is the margin of example (I,y)." ></td>
	<td class="line x" title="44:241	Hence, the learning task is to maximize the margins of the training examples, i.e. induce scoring function Q such that it classifies correct inferences with positive confidence and incorrect inferences with negative confidence." ></td>
	<td class="line x" title="45:241	In our work, we minimized the logistic loss: L(z)=log(1+exp( z)) (6) i.e. the negative log-likelihood of the training sample." ></td>
	<td class="line x" title="46:241	Our classifiers are ensembles of decisions trees, which we boost (Schapire & Singer, 1999) to minimize the above loss using the update equations given in Collins, Schapire, and Singer (2002)." ></td>
	<td class="line x" title="47:241	More specifically, classifier QT is an ensemble comprising decision trees q1,,qT, where: QT (I )= Tsummationdisplay t=1 qt (I ) (7) At iteration t, decision tree qt is grown, its leaves are confidence-rated, and it is added to the ensemble." ></td>
	<td class="line x" title="48:241	The classifier for each constituent label is trained independently, so we henceforth omitsubscripts." ></td>
	<td class="line x" title="49:241	An example (I,y) is assigned weight wt(I,y):2 wt(I,y)= 11+exp(y Qt 1(I)) (8) The total weight of y-value examples that fall in leaf f is Wtf;y: Wtf;y = summationdisplay (I;y0)2E y0=y; I2f wt(I,y) (9) and this leaf has loss Ztf : Ztf =2 radicalBig Wtf;+ Wtf; (10) Growing the decision tree: The loss of the entire decision tree qt is Z(qt)= summationdisplay leaf f2qt Ztf (11) 2 If we were to replace this equation with wt(I;y) = exp(y Qt 1(I)) 1, but leave the remainder of the algorithm unchanged, this algorithm would be confidence-rated AdaBoost (Schapire & Singer, 1999), minimizing the exponential loss L(z) = exp( z)." ></td>
	<td class="line x" title="50:241	In preliminary experiments, however, we found that the logistic loss provided superior generalization accuracy." ></td>
	<td class="line x" title="51:241	We will use Zt as a shorthand for Z(qt)." ></td>
	<td class="line x" title="52:241	When growing the decision tree, we greedily choose node splits to minimize this Z (Kearns & Mansour, 1999)." ></td>
	<td class="line x" title="53:241	In particular, the loss reduction of splitting leaf f using featureinto two children, f ^and f ^:, is Ztf (): Ztf ()=Ztf (Ztf^ +Ztf^: ) (12) To split node f, we choose the  that reduces loss the most: =arg max 2 Ztf () (13) Confidence-rating the leaves: Each leaf f is confidence-rated astf : tf = 12 log Wtf;++epsilon1 Wtf; +epsilon1 (14) Equation 14 is smoothed by the epsilon1 term (Schapire & Singer, 1999) to prevent numerical instability in the case that either Wtf;+ or Wtf; is 0." ></td>
	<td class="line x" title="54:241	In our experiments, we used epsilon1= 10 8." ></td>
	<td class="line x" title="55:241	Although our example weights are unnormalized, so far weve found no benefit from scalingepsilon1 as Collins and Koo (2005) suggest." ></td>
	<td class="line x" title="56:241	All inferences that fall in a particular leaf node are assigned the same confidence: if inference I falls in leaf node f in the tth decision tree, then qt(I)=tf." ></td>
	<td class="line x" title="57:241	2.1.3 Calibrating the Sub-Classifiers An important concern is when to stop growing the decision tree." ></td>
	<td class="line x" title="58:241	We propose the minimum reduction in loss (MRL) stopping criterion: During training, there is a value t at iteration t which serves as a threshold on the minimum reduction in loss for leaf splits." ></td>
	<td class="line x" title="59:241	If there is no splitting feature for leaf f that reduces loss by at least t then f is not split." ></td>
	<td class="line x" title="60:241	Formally, leaf f will not be bisected during iteration t if max 2 Ztf () <t. The MRL stopping criterion is essentiallylscript0 regularization:t corresponds to the lscript0 penalty parameter and each feature with non-zero confidence incurs a penalty oft, so to outweigh the penalty each split must reduce loss by at leastt. t decreases monotonically during training at the slowest rate possible that still allows training to proceed." ></td>
	<td class="line x" title="61:241	We start by initializing 1 to 1, and at the beginning of iteration t we decrease t only if the root node ; of the decision tree cannot be split." ></td>
	<td class="line x" title="62:241	Otherwise,t is set tot 1." ></td>
	<td class="line x" title="63:241	Formally, 143 t =min(t 1,max 2 Zt;())." ></td>
	<td class="line x" title="64:241	In this manner, the decision trees are induced in order of decreasingt. During training, the constituent classifiers Q never do any parsing per se, and they train at different rates: If  nequal 0, then t isnt necessarily equal tot 0." ></td>
	<td class="line x" title="65:241	We calibrate the different classifiers by picking some meta-parameter  and insisting that the sub-classifiers comprised by a particular parser have all reached some fixedin training." ></td>
	<td class="line x" title="66:241	Given , the constituent classifier for label  is Qt, where t  > t+1 . To obtain the final parser, we cross-validate , picking the value whose set of constituent classifiers maximizes accuracy on a development set." ></td>
	<td class="line x" title="67:241	2.1.4 Types of Features used by the Scoring Function Our parser operates bottom-up." ></td>
	<td class="line x" title="68:241	Let the frontier of a state be the top-most items (i.e. the items with no parents)." ></td>
	<td class="line x" title="69:241	The children of a candidate inference are those frontier items below the item to be inferred, the left context items are those frontier items to the left of the children, and the right context items are those frontier items to the right of the children." ></td>
	<td class="line x" title="70:241	For example, in the candidateVP-inference shown in Figure 1, the frontier comprises the NP, VBD, and ADJP items, the VBD and ADJP items are the children of the VPinference (theVBDis its head child), theNPis the left context item, and there are no right context items." ></td>
	<td class="line x" title="71:241	The design of some parsers in the literature restricts the kinds of features that can be usefully and efficiently evaluated." ></td>
	<td class="line x" title="72:241	Our scoring function and parsing algorithm have no such limitations." ></td>
	<td class="line x" title="73:241	Q can, in principle, use arbitrary information from the history to evaluate constituent inferences." ></td>
	<td class="line xc" title="74:241	Although some of our feature types are based on prior work (Collins, 1999; Klein & Manning, 2003; Bikel, 2004), we note that our scoring function uses more history information than typical parsers." ></td>
	<td class="line x" title="75:241	All features check whether an item has some property; specifically, whether the items label/headtag/headword is a certain value." ></td>
	<td class="line x" title="76:241	These features perform binary tests on the state directly, unlike Henderson (2003) which works with an intermediate representation of the history." ></td>
	<td class="line x" title="77:241	In our baseline setup, feature set  contained five different feature types, described in Table 1." ></td>
	<td class="line x" title="78:241	Table 2 Feature item groups." ></td>
	<td class="line x" title="79:241	 all children  all non-head children  all non-leftmost children  all non-rightmost children  all children left of the head  all children right of the head  head-child and all children left of the head  head-child and all children right of the head 2.2 Aggregating Confidences To get the cumulative score of a parse path P, we apply aggregatorAover the confidences Q(I) in Equation 4." ></td>
	<td class="line x" title="80:241	Initially, we definedAin the customary fashion as summing the loss of each inferences confidence: P=arg max P2P(x)    summationdisplay I2P L (Q(I))    (15) with the logistic loss L as defined in Equation 6." ></td>
	<td class="line x" title="81:241	(We negate the final sum because we want to minimize the loss)." ></td>
	<td class="line x" title="82:241	This definition ofAis motivated by viewing L as a negative log-likelihood given by a logistic function (Collins et al. , 2002), and then using Equation 3." ></td>
	<td class="line x" title="83:241	It is also inspired by the multiclass loss-based decoding method of Schapire and Singer (1999)." ></td>
	<td class="line x" title="84:241	With this additive aggregator, loss monotonically increases as inferences are added, as in a PCFG-based parser in which all productions decrease the cumulative probability of the parse tree." ></td>
	<td class="line x" title="85:241	In preliminary experiments, this aggregator gave disappointing results: precision increased slightly, but recall dropped sharply." ></td>
	<td class="line x" title="86:241	Exploratory data analysis revealed that, because each inference incurs some positive loss, the aggregator very cautiously builds the smallest trees possible, thus harming recall." ></td>
	<td class="line x" title="87:241	We had more success by defining A to maximize the minimum confidence." ></td>
	<td class="line x" title="88:241	Essentially, P=arg max P2P(x) minI2P Q(I) (16) Ties are broken according to the second lowest confidence, then the third lowest, and so on." ></td>
	<td class="line x" title="89:241	2.3 Search Given input sentence x, we choose the parse path P in P(x) with the maximum aggregated score (Equation 4)." ></td>
	<td class="line x" title="90:241	Since it is computationally intractable to 144 Table 1 Types of features." ></td>
	<td class="line x" title="91:241	 Child item features test if a particular child item has some property." ></td>
	<td class="line x" title="92:241	E.g. does the item one right of the head have headword perfect?" ></td>
	<td class="line x" title="93:241	(True in Figure 1)  Context item features test if a particular context item has some property." ></td>
	<td class="line x" title="94:241	E.g. does the first item of left context have headtag NN?" ></td>
	<td class="line x" title="95:241	(True)  Grandchild item features test if a particular grandchild item has some property." ></td>
	<td class="line x" title="96:241	E.g. does the leftmost child of the rightmost child item have label JJ?" ></td>
	<td class="line x" title="97:241	(True)  Exists features test if a particular group of items contains an item with some property." ></td>
	<td class="line x" title="98:241	E.g. does some non-head child item have label ADJP?" ></td>
	<td class="line x" title="99:241	(True) Exists features select one of the groups of items specified in Table 2." ></td>
	<td class="line x" title="100:241	Alternately, they can select the terminals dominated by that group." ></td>
	<td class="line x" title="101:241	E.g. is there some terminal item dominated by non-rightmost children items that has headword quux?" ></td>
	<td class="line x" title="102:241	(False) consider every possible sequence of inferences, we use beam search to restrict the size of P(x)." ></td>
	<td class="line x" title="103:241	As an additional guard against excessive computation, search stopped if more than a fixed maximum number of states were popped from the agenda." ></td>
	<td class="line x" title="104:241	As usual, search also ended if the highest-priority state in the agenda could not have a better aggregated score than the best final parse found thus far." ></td>
	<td class="line xc" title="105:241	3 Experiments Following Taskar, Klein, Collins, Koller, and Manning (2004), we trained and tested on 15 word sentences in the English Penn Treebank (Taylor et al. , 2003), 10% of the entire treebank by word count.3 We used sections 0221 (9753 sentences) for training, section 24 (321 sentences) for development, and section 23 (603 sentences) for testing, preprocessed as per Table 3." ></td>
	<td class="line x" title="106:241	We evaluated our parser using the standard PARSEVAL measures (Black et al. , 1991): labelled precision, recall, and F-measure (LPRC, LRCL, and LFMS, respectively), which are computed based on the number of constituents in the parsers output that match those in the gold-standard parse." ></td>
	<td class="line x" title="107:241	We tested whether the observed differences in PARSEVAL measures are significant at p=0.05 using a stratified shuing test (Cohen, 1995, Section 5.3.2) with one million trials.4 As mentioned in Section 1, the parser cannot infer any item that crosses an item already in the state." ></td>
	<td class="line x" title="108:241	3 There was insu cient time before deadline to train on all sentences." ></td>
	<td class="line x" title="109:241	4 The shu ing test we used was originally implemented by Dan Bikel (http://www.cis.upenn.edu/~dbikel/ software.html) and subsequently modified to compute pvalues for LFMS di erences." ></td>
	<td class="line x" title="110:241	We placed three additional candidacy restrictions on inferences: (a) Items must be inferred under the bottom-up item ordering; (b) To ensure the parser does not enter an infinite loop, no two items in a state can have both the same span and the same label; (c) An item can have no more than K = 5 children." ></td>
	<td class="line x" title="111:241	(Only 0.24% of non-terminals in the preprocessed development set have more than five children)." ></td>
	<td class="line x" title="112:241	The number of candidate inferences at each state, as well as the number of training examples generated by the algorithm in Section 2.1.1, is proportional to K. In our experiment, there were roughlyjE j 1.7 million training examples for each classifier." ></td>
	<td class="line x" title="113:241	3.1 Baseline In the baseline setting, context item features (Section 2.1.4) could refer to the two nearest items of context in each direction." ></td>
	<td class="line x" title="114:241	The parser used a beam width of 1000, and was terminated in the rare event that more than 10,000 states were popped from the agenda." ></td>
	<td class="line x" title="115:241	Figure 2 shows the accuracy of the baseline on the development set as training progresses." ></td>
	<td class="line x" title="116:241	Cross-validating the choice of against the LFMS (Section 2.1.3) suggested an optimum of = 1.42." ></td>
	<td class="line x" title="117:241	At this , there were a total of 9297 decision tree splits in the parser (summed over all constituent classifiers), LFMS = 87.16, LRCL = 86.32, and LPRC=88.02." ></td>
	<td class="line x" title="118:241	3.2 Beam Width To determine the effect of the beam width on the accuracy, we evaluated the baseline on the development set using a beam width of 1, i.e. parsing entirely greedily (Wong & Wu, 1999; Kalt, 2004; Sagae & Lavie, 2005)." ></td>
	<td class="line x" title="119:241	Table 4 compares the base145 Table 3 Steps for preprocessing the data." ></td>
	<td class="line x" title="120:241	Starred steps are performed only on input with tree structure." ></td>
	<td class="line x" title="121:241	1." ></td>
	<td class="line x" title="122:241	* Strip functional tags and trace indices, and remove traces." ></td>
	<td class="line x" title="123:241	2." ></td>
	<td class="line x" title="124:241	* Convert PRT to ADVP." ></td>
	<td class="line x" title="125:241	(This convention was established by Magerman (1995).)" ></td>
	<td class="line x" title="126:241	3." ></td>
	<td class="line x" title="127:241	Remove quotation marks (i.e. terminal items tagged  or )." ></td>
	<td class="line x" title="128:241	(Bikel, 2004) 4." ></td>
	<td class="line x" title="129:241	* Raise punctuation." ></td>
	<td class="line x" title="130:241	(Bikel, 2004) 5." ></td>
	<td class="line x" title="131:241	Remove outermost punctuation.a 6." ></td>
	<td class="line x" title="132:241	* Remove unary projections to self (i.e. duplicate items with the same span and label)." ></td>
	<td class="line x" title="133:241	7." ></td>
	<td class="line x" title="134:241	POS tag the text using Ratnaparkhi (1996)." ></td>
	<td class="line x" title="135:241	8." ></td>
	<td class="line x" title="136:241	Lowercase headwords." ></td>
	<td class="line x" title="137:241	9." ></td>
	<td class="line x" title="138:241	Replace any word observed fewer than 5 times in the (lower-cased) training sentences with UNK." ></td>
	<td class="line x" title="139:241	a As pointed out by an anonymous reviewer of Collins (2003), removing outermost punctuation may discard useful information." ></td>
	<td class="line oc" title="140:241	Its also worth noting that Collins and Roark (2004) saw a LFMS improvement of 0.8% over their baseline discriminative parser after adding punctuation features, one of which encoded the sentence-final punctuation." ></td>
	<td class="line x" title="141:241	Figure 2 PARSEVAL scores of the baseline on the 15 words development set of the Penn Treebank." ></td>
	<td class="line x" title="142:241	The top x-axis shows accuracy as the minimum reduction in loss decreases." ></td>
	<td class="line x" title="143:241	The bottom shows the corresponding number of decision tree splits in the parser, summed over all classifiers." ></td>
	<td class="line x" title="144:241	74% 76% 78% 80% 82% 84% 86% 88% 90% 20000 10000 5000 2500 1000 250 74% 76% 78% 80% 82% 84% 86% 88% 90% 0.341.02.75.0102540120 PARSEVAL score Total # of splits Minimum reduction in loss Labelled precision Labelled F-measure Labelled recall line results on the development set with a beam width of 1 and a beam width of 1000.5 The wider beam seems to improve the PARSEVAL scores of the parser, although we were unable to detect a statistically significant improvement in LFMS on our relatively small development set." ></td>
	<td class="line x" title="145:241	5 Using a beam width of 100,000 yielded output identical to using a beam width of 1000." ></td>
	<td class="line x" title="146:241	3.3 Context Size Table 5 compares the baseline to parsers that could not examine as many context items." ></td>
	<td class="line x" title="147:241	A significant portion of the baselines accuracy is due to contextual clues, as evidenced by the poor accuracy of the no context run." ></td>
	<td class="line x" title="148:241	However, we did not detect a significant difference between using one context item or two." ></td>
	<td class="line x" title="149:241	146 Table 4 PARSEVAL results on the 15 words development set of the baseline, varying the beam width." ></td>
	<td class="line x" title="150:241	Also, the MRL that achieved this LFMS and the total number of decision tree splits at this MRL." ></td>
	<td class="line x" title="151:241	Dev Dev Dev MRL #splits LFMS LRCL LPRC  total Beam=1 86.36 86.20 86.53 2.03 7068 Baseline 87.16 86.32 88.02 1.42 9297 Table 5 PARSEVAL results on the 15 words development set, given the amount of context available." ></td>
	<td class="line x" title="152:241	is statistically significant." ></td>
	<td class="line x" title="153:241	The score differences between context 0 and context 1 are significant, whereas the differences between context 1 and the baseline are not." ></td>
	<td class="line x" title="154:241	Dev Dev Dev MRL #splits LFMS LRCL LPRC  total Context 0 75.15 75.28 75.03 3.38 3815 Context 1 86.93 85.78 88.12 2.45 5588 Baseline 87.16 86.32 88.02 1.42 9297 Table 6 PARSEVAL results of decision stumps on the 15 words development set, through 8200 splits." ></td>
	<td class="line x" title="155:241	The differences between the stumps run and the baseline are statistically significant." ></td>
	<td class="line x" title="156:241	Dev Dev Dev MRL #splits LFMS LRCL LPRC  total Stumps 85.72 84.65 86.82 2.39 5217 Baseline 87.07 86.05 88.12 1.92 7283 3.4 Decision Stumps Our features are of relatively fine granularity." ></td>
	<td class="line x" title="157:241	To test if a less powerful machine could provide accuracy comparable to the baseline, we trained a parser in which we boosted decisions stumps, i.e. decision trees of depth 1." ></td>
	<td class="line x" title="158:241	Stumps are equivalent to learning a linear discriminant over the atomic features." ></td>
	<td class="line x" title="159:241	Since the stumps run trained quite slowly, it only reached 8200 splits total." ></td>
	<td class="line x" title="160:241	To ensure a fair comparison, in Table 6 we chose the best baseline parser with at most 8200 splits." ></td>
	<td class="line x" title="161:241	The LFMS of the stumps run on the development set was 85.72%, significantly less accurate than the baseline." ></td>
	<td class="line x" title="162:241	For example, Figure 3 shows a case where NP classification better served by the informative conjunction1^2 found by the decision trees." ></td>
	<td class="line x" title="163:241	Given Figure 3 An example of a decision (a) stump and (b) tree for scoring NP-inferences." ></td>
	<td class="line x" title="164:241	Each leafs value is the confidence assigned to all inferences that fall in this leaf." ></td>
	<td class="line x" title="165:241	1 asks does the first child have a determiner headtag?.2 asks does the last child have a noun label?." ></td>
	<td class="line x" title="166:241	NP classification is better served by the informative conjunction1^2 found by the decision trees." ></td>
	<td class="line x" title="167:241	(a) 1 true f alse +0.5 0 (b) 1 true f alse 2 true f alse 0 +1.0 -0.2 Table 7 PARSEVAL results of deterministic parsers on the 15 words development set through 8700 splits." ></td>
	<td class="line x" title="168:241	A shaded cell means that the difference between this value and that of the baseline is statistically significant." ></td>
	<td class="line x" title="169:241	All differences between l2r and r2l are significant." ></td>
	<td class="line x" title="170:241	Dev Dev Dev MRL #splits LFMS LRCL LPRC  total l2r 83.61 82.71 84.54 3.37 2157 r2l 85.76 85.37 86.15 3.39 1881 Baseline 87.07 86.05 88.12 1.92 7283 the sentence The man left, at the initial state there are six candidate NP-inferences, one for each span, and (NP The man) is the only candidate inference that is correct.1 is true for the correct inference and two of the incorrect inferences ((NP The) and (NP The man left)).1 ^2, on the other hand, is true only for the correct inference, and so it is better at discriminating NPs over this sample." ></td>
	<td class="line x" title="171:241	3.5 Deterministic Parsing Our baseline parser simulates a non-deterministic machine, as at any state there may be several correct decisions." ></td>
	<td class="line x" title="172:241	We trained deterministic variations of the parser, for which we imposed strict left-to-right (l2r) and right-to-left (r2l) item orderings." ></td>
	<td class="line x" title="173:241	For these variations we generated training examples using the corresponding unique path to each gold-standard training tree." ></td>
	<td class="line x" title="174:241	The r2l run reached only 8700 splits total, so in Table 7 we chose the best baseline and l2r 147 Table 8 PARSEVAL results of the full vocabulary parser on the 15 words development set." ></td>
	<td class="line x" title="175:241	The differences between the full vocabulary run and the baseline are not statistically significant." ></td>
	<td class="line x" title="176:241	Dev Dev Dev MRL #splits LFMS LRCL LPRC  total Baseline 87.16 86.32 88.02 1.42 9297 Full vocab 87.50 86.85 88.15 1.27 10711 parser with at most 8700 splits." ></td>
	<td class="line x" title="177:241	r2l parsing is significantly more accurate than l2r." ></td>
	<td class="line x" title="178:241	The reason is that the deterministic runs (l2r and r2l) must avoid prematurely inferring items that come later in the item ordering." ></td>
	<td class="line x" title="179:241	This puts the l2r parser in a tough spot." ></td>
	<td class="line x" title="180:241	If it makes far-right decisions, its more likely to prevent correct subsequent decisions that are earlier in the l2r ordering, i.e. to the left." ></td>
	<td class="line x" title="181:241	But if it makes far-left decisions, then it goes against the right-branching tendency of English sentences." ></td>
	<td class="line x" title="182:241	In contrast, the r2l parser is more likely to be correct when it infers far-right constituents." ></td>
	<td class="line x" title="183:241	We also observed that the accuracy of the deterministic parsers dropped sharply as training progressed (See Figure 4)." ></td>
	<td class="line x" title="184:241	This behavior was unexpected, as the accuracy curve levelled off in every other experiment." ></td>
	<td class="line x" title="185:241	In fact, the accuracy of the deterministic parsers fell even when parsing the training data." ></td>
	<td class="line x" title="186:241	To explain this behavior, we examined the margin distributions of the r2l NP-classifier (Figure 5)." ></td>
	<td class="line x" title="187:241	As training progressed, the NP-classifier was able to reduce loss by driving up the margins of the incorrect training examples, at the expense of incorrectly classifying a slightly increased number of correct training examples." ></td>
	<td class="line x" title="188:241	However, this is detrimental to parsing accuracy." ></td>
	<td class="line x" title="189:241	The more correct inferences with negative confidence, the less likely it is at some state that the highest confidence inference is correct." ></td>
	<td class="line x" title="190:241	This effect is particularly pronounced in the deterministic setting, where there is only one correct inference per state." ></td>
	<td class="line x" title="191:241	3.6 Full Vocabulary As in traditional parsers, the baseline was smoothed by replacing any word that occurs fewer than five times in the training data with the special token UNK (Table 3.9)." ></td>
	<td class="line x" title="192:241	Table 8 compares the baseline to a full vocabulary run, in which the vocabulary contained all words observed in the training data." ></td>
	<td class="line x" title="193:241	As evidenced by the results therein, controlling for lexical sparsity did not significantly improve accuracy in our setting." ></td>
	<td class="line x" title="194:241	In fact, the full vocabulary run is slightly more accurate than the baseline on the development set, although this difference was not statistically significant." ></td>
	<td class="line x" title="195:241	This was a late-breaking result, and we used the full vocabulary condition as our final parser for parsing the test set." ></td>
	<td class="line x" title="196:241	3.7 Test Set Results Table 9 shows the results of our best parser on the 15 words test set, as well as the accuracy reported for a recent discriminative parser (Taskar et al. , 2004) and scores we obtained by training and testing the parsers of Charniak (2000) and Bikel (2004) on the same data." ></td>
	<td class="line x" title="197:241	Bikel (2004) is a clean room reimplementation of the Collins parser (Collins, 1999) with comparable accuracy." ></td>
	<td class="line x" title="198:241	Both Charniak (2000) and Bikel (2004) were trained using the goldstandard tags, as this produced higher accuracy on the development set than using Ratnaparkhi (1996)s tags." ></td>
	<td class="line x" title="199:241	3.8 Exploratory Data Analysis To gain a better understanding of the weaknesses of our parser, we examined a sample of 50 development sentences that the full vocabulary parser did not get entirely correct." ></td>
	<td class="line x" title="200:241	Besides noise and cases of genuine ambiguity, the following list outlines all error types that occurred in more than five sentences, in roughly decreasing order of frequency." ></td>
	<td class="line x" title="201:241	(Note that there is some overlap between these groups.)" ></td>
	<td class="line x" title="202:241	 ADVPs and ADJPs A disproportionate amount of the parsers error was due to ADJPs and ADVPs." ></td>
	<td class="line x" title="203:241	Out of the 12.5% total error of the parser on the development set, an absolute 1.0% was due to ADVPs, and 0.9% due to ADJPs." ></td>
	<td class="line x" title="204:241	The parser had LFMS=78.9%,LPRC=82.5%,LRCL=75.6% on ADVPs, and LFMS = 68.0%,LPRC = 71.2%,LRCL=65.0% on ADJPs." ></td>
	<td class="line x" title="205:241	These constructions can sometimes involve tricky attachment decisions." ></td>
	<td class="line x" title="206:241	For example, in the fragment to get fat in times of crisis, the parsers output was (VP to (VP get (ADJP fat (PP in (NP (NP times) (PP of (NP crisis))))))) instead of the correct construction (VPto (VPget (ADJPfat) (PP in (NP (NP times) (PP of (NP crisis))))))." ></td>
	<td class="line x" title="207:241	148 Figure 4 LFMS of the baseline and the deterministic runs on the 15 words development set of the Penn Treebank." ></td>
	<td class="line x" title="208:241	The x-axis shows the LFMS as training progresses and the number of decision tree splits increases." ></td>
	<td class="line x" title="209:241	74 76 78 80 82 84 86 88 8700 5000 2500 1000 250 74 76 78 80 82 84 86 88 Parseval FMS Total # of splits Baseline Right-to-left Left-to-right Figure 5 The margin distributions of the r2l NP-classifier, early in training and late in training, (a) over the incorrect training examples and (b) over the correct training examples." ></td>
	<td class="line x" title="210:241	(a) -20 0 20 40 60 80 100 120 140 160 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Margin Percentile Late in training Early in training (b) -40 -30 -20 -10 0 10 20 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Margin Percentile Late in training Early in training The amount of noise present in ADJP and ADVP annotations in the PTB is unusually high." ></td>
	<td class="line x" title="211:241	Annotation of ADJP and ADVP unary projections is particularly inconsistent." ></td>
	<td class="line x" title="212:241	For example, the development set contains the sentence The dollar was trading sharply lower in Tokyo ., with sharply lower bracketed as (ADVP (ADVP sharply) lower)." ></td>
	<td class="line x" title="213:241	sharply lower appears 16 times in the complete training section, every time bracketed as (ADVP sharply lower), and sharply higher 10 times, always as (ADVPsharply higher)." ></td>
	<td class="line x" title="214:241	Because of the high number of negative examples, the classifiers 149 Table 9 PARSEVAL results of on the 15 words test set of various parsers in the literature." ></td>
	<td class="line x" title="215:241	The differences between the full vocabulary run and Bikel or Charniak are significant." ></td>
	<td class="line x" title="216:241	Taskar et al.(2004)s output was unavailable for significance testing, but presumably its differences from the full vocab parser are also significant." ></td>
	<td class="line x" title="218:241	Test Test Test Dev Dev Dev LFMS LRCL LPRC LFMS LRCL LPRC Full vocab 87.13 86.47 87.80 87.50 86.85 88.15 Bikel (2004) 88.85 88.31 89.39 86.82 86.43 87.22 Taskar et al.(2004) 89.12 89.10 89.14 89.98 90.22 89.74 Charniak (2000) 90.09 90.01 90.17 89.50 89.69 89.32 bias is to cope with the noise by favoring negative confidences predictions for ambiguous ADJP and ADVP decisions, hence their abysmal labelled recall." ></td>
	<td class="line x" title="220:241	One potential solution is the weight-sharing strategy described in Section 3.5." ></td>
	<td class="line x" title="221:241	 Tagging Errors Many of the parsers errors were due to poor tagging." ></td>
	<td class="line x" title="222:241	Preprocessing sentence Would service be voluntary or compulsory ? gives would/MD service/VB be/VB voluntary/JJ or/CC UNK/JJ and, as a result, the parser brackets service . . ." ></td>
	<td class="line x" title="223:241	compulsory as a VP instead of correctly bracketing service as an NP." ></td>
	<td class="line x" title="224:241	We also found that the tagger we used has difficulties with completely capitalized words, and tends to tag them NNP." ></td>
	<td class="line x" title="225:241	By giving the parser access to the same features used by taggers, especially rich lexical features (Toutanova et al. , 2003), the parser might learn to compensate for tagging errors." ></td>
	<td class="line x" title="226:241	 Attachment decisions The parser does not detect affinities between certain word pairs, so it has difficulties with bilexical dependency decisions." ></td>
	<td class="line x" title="227:241	In principle, bilexical dependencies can be represented as conjunctions of feature given in Section 2.1.4." ></td>
	<td class="line x" title="228:241	Given more training data, the parser might learn these affinities." ></td>
	<td class="line x" title="229:241	4 Conclusions In this work, we presented a near state-of-theart approach to constituency parsing which overcomes some of the limitations of other discriminative parsers." ></td>
	<td class="line x" title="230:241	Like Yamada and Matsumoto (2003) and Sagae and Lavie (2005), our parser is driven by classifiers." ></td>
	<td class="line x" title="231:241	Even though these classifiers themselves never do any parsing during training, they can be combined into an effective parser." ></td>
	<td class="line x" title="232:241	We also presented a beam search method under the objective function of maximizing the minimum confidence." ></td>
	<td class="line x" title="233:241	To ensure efficiency, some discriminative parsers place stringent requirements on which types of features are permitted." ></td>
	<td class="line x" title="234:241	Our approach requires no such restrictions and our scoring function can, in principle, use arbitrary information from the history to evaluate constituent inferences." ></td>
	<td class="line x" title="235:241	Even though our features may be of too fine granularity to discriminate through linear combination, discriminatively trained decisions trees determine useful feature combinations automatically, so adding new features requires minimal human effort." ></td>
	<td class="line nc" title="236:241	Training discriminative parsers is notoriously slow, especially if it requires generating examples by repeatedly parsing the treebank (Collins & Roark, 2004; Taskar et al. , 2004)." ></td>
	<td class="line x" title="237:241	Although training time is still a concern in our setup, the situation is ameliorated by generating training examples in advance and inducing one-vs-all classifiers in parallel, a technique similar in spirit to the POS-tag parallelization in Yamada and Matsumoto (2003) and Sagae and Lavie (2005)." ></td>
	<td class="line x" title="238:241	This parser serves as a proof-of-concept, in that we have not fully exploited the possibilities of engineering intricate features or trying more complex search methods." ></td>
	<td class="line x" title="239:241	Its flexibility offers many opportunities for improvement, which we leave to future work." ></td>
	<td class="line x" title="240:241	Acknowledgments The authors would like to thank Dan Bikel, Mike Collins, Ralph Grishman, Adam Meyers, Mehryar Mohri, Satoshi Sekine, and Wei Wang, as well as the anonymous reviewers, for their helpful comments 150 and constructive criticism." ></td>
	<td class="line x" title="241:241	This research was sponsored by an NSF CAREER award, and by an equipment gift from Sun Microsystems." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E06-1011
Online Learning Of Approximate Dependency Parsing Algorithms
McDonald, Ryan;Pereira, Fernando C. N.;"></td>
	<td class="line x" title="1:205	Online Learning of Approximate Dependency Parsing Algorithms Ryan McDonald Fernando Pereira Department of Computer and Information Science University of Pennsylvania Philadelphia, PA 19104 {ryantm,pereira}@cis.upenn.edu Abstract In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al.(2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word." ></td>
	<td class="line x" title="3:205	We show that those extensions can make the MST framework computationally intractable, but that the intractability can be circumvented with new approximate parsing algorithms." ></td>
	<td class="line x" title="4:205	We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish." ></td>
	<td class="line x" title="5:205	1 Introduction Dependency representations of sentences (Hudson, 1984; Melcuk, 1988) model head-dependent syntactic relations as edges in a directed graph." ></td>
	<td class="line x" title="6:205	Figure 1 displays a dependency representation for the sentence John hit the ball with the bat." ></td>
	<td class="line x" title="7:205	This sentence is an example of a projective (or nested) tree representation, in which all edges can be drawn in the plane with none crossing." ></td>
	<td class="line x" title="8:205	Sometimes a non-projective representations are preferred, as in the sentence in Figure 2.1 In particular, for freer-word order languages, non-projectivity is a common phenomenon since the relative positional constraints on dependents is much less rigid." ></td>
	<td class="line x" title="9:205	The dependency structures in Figures 1 and 2 satisfy the tree constraint: they are weakly connected graphs with a unique root node, and each non-root node has a exactly one parent." ></td>
	<td class="line x" title="10:205	Though trees are 1Examples are drawn from McDonald et al.(2005c)." ></td>
	<td class="line x" title="12:205	more common, some formalisms allow for words to modify multiple parents (Hudson, 1984)." ></td>
	<td class="line x" title="13:205	Recently, McDonald et al.(2005c) have shown that treating dependency parsing as the search for the highest scoring maximum spanning tree (MST) in a graph yields efficient algorithms for both projective and non-projective trees." ></td>
	<td class="line x" title="15:205	When combined with a discriminative online learning algorithm and a rich feature set, these models provide state-of-the-art performance across multiple languages." ></td>
	<td class="line x" title="16:205	However, the parsing algorithms require that the score of a dependency tree factors as a sum of the scores of its edges." ></td>
	<td class="line x" title="17:205	This first-order factorization is very restrictive since it only allows for features to be defined over single attachment decisions." ></td>
	<td class="line x" title="18:205	Previous work has shown that conditioning on neighboring decisions can lead to significant improvements in accuracy (Yamada and Matsumoto, 2003; Charniak, 2000)." ></td>
	<td class="line x" title="19:205	In this paper we extend the MST parsing framework to incorporate higher-order feature representations of bounded-size connected subgraphs." ></td>
	<td class="line x" title="20:205	We also present an algorithm for acyclic dependency graphs, that is, dependency graphs in which a word maydepend on multiple heads." ></td>
	<td class="line x" title="21:205	In both cases parsing is in general intractable and we provide novel approximate algorithms to make these cases tractable." ></td>
	<td class="line x" title="22:205	We evaluate these algorithms within an online learning framework, which has been shown to be robust with respect approximate inference, and describe experiments displaying that these new models lead to state-of-the-art accuracy for English and the best accuracy we know of for Czech and Danish." ></td>
	<td class="line x" title="23:205	2 Maximum Spanning Tree Parsing Dependency-tree parsing as the search for the maximum spanning tree (MST) in a graph was 81 root John saw a dog yesterday which was a Yorkshire Terrier Figure 2: An example non-projective dependency structure." ></td>
	<td class="line x" title="24:205	root hit John ball with the bat the root0 John1 hit2 the3 ball4 with5 the6 bat7 Figure 1: An example dependency structure." ></td>
	<td class="line x" title="25:205	proposed byMcDonald etal.(2005c)." ></td>
	<td class="line x" title="26:205	Thisformulation leads to efficient parsing algorithms for both projective and non-projective dependency trees with the Eisner algorithm (Eisner, 1996) and the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) respectively." ></td>
	<td class="line x" title="27:205	The formulation works by defining the score of a dependency tree to be the sum of edge scores, s(x,y) = summationdisplay (i,j)y s(i,j) where x = x1 xn is an input sentence and y a dependency tree for x. We can view y as a set of tree edges and write (i,j)  y to indicate an edge in y from word xi to word xj." ></td>
	<td class="line x" title="28:205	Consider the example from Figure 1, where the subscripts index the nodes of the tree." ></td>
	<td class="line x" title="29:205	The score of this tree would then be, s(0,2) + s(2,1) + s(2,4) + s(2,5) + s(4,3) + s(5,7) +s(7,6) We call this first-order dependency parsing since scores are restricted to a single edge in the dependency tree." ></td>
	<td class="line x" title="30:205	The score of an edge is in turn computed as the inner product of a high-dimensional feature representation of the edge with a corresponding weight vector, s(i,j) = wf(i,j) This is a standard linear classifier in which the weight vector w are the parameters to be learned during training." ></td>
	<td class="line x" title="31:205	We should note that f(i,j) can be based on arbitrary features of the edge and the input sequence x. Given a directed graph G = (V,E), the maximum spanning tree (MST) problem is to find the highest scoring subgraph of G that satisfies the tree constraint over the vertices V. By defining a graph in which the words in a sentence are the vertices and there is a directed edge between all words with a score as calculated above, McDonald et al.(2005c) showed that dependency parsing is equivalent to finding the MST in this graph." ></td>
	<td class="line x" title="33:205	Furthermore, it was shown that this formulation can lead to state-of-the-art results when combined with discriminative learning algorithms." ></td>
	<td class="line x" title="34:205	Although the MST formulation applies to any directed graph, our feature representations andone oftheparsing algorithms (Eisners) rely onalinear ordering of the vertices, namely the order of the words in the sentence." ></td>
	<td class="line x" title="35:205	2.1 Second-Order MST Parsing Restricting scores to a single edge in a dependency tree gives a very impoverished view of dependency parsing." ></td>
	<td class="line x" title="36:205	Yamadaand Matsumoto (2003) showed that keeping a small amount of parsing history was crucial to improving parsing performance for their locally-trained shift-reduce SVM parser." ></td>
	<td class="line x" title="37:205	It is reasonable to assume that other parsing models might benefit from features over previous decisions." ></td>
	<td class="line x" title="38:205	Here we will focus on methods for parsing second-order spanning trees." ></td>
	<td class="line x" title="39:205	These models factor the score of the tree into the sum of adjacent edge pair scores." ></td>
	<td class="line x" title="40:205	To quantify this, consider again the example from Figure 1." ></td>
	<td class="line x" title="41:205	In the second-order spanning tree model, the score would be, s(0,,2) + s(2,,1) + s(2,,4) + s(2,4,5) + s(4,,3) + s(5,,7) + s(7,,6) Here we use the second-order score function s(i,k,j), which is the score of creating a pair of adjacent edges, from word xi to words xk and xj." ></td>
	<td class="line x" title="42:205	For instance, s(2,4,5) is the score of creating the edges from hit to with and from hit to ball." ></td>
	<td class="line x" title="43:205	The score functions are relative to the left or right of the parent and we never score adjacent edges that are on different sides of the parent (for instance, 82 there is no s(2,1,4) for the adjacent edges from hit to John and ball)." ></td>
	<td class="line x" title="44:205	This independence between left and right descendants allow us to use a O(n3) second-order projective parsing algorithm, as we will see later." ></td>
	<td class="line x" title="45:205	We write s(xi,,xj) when xj is the first left or first right dependent of word xi." ></td>
	<td class="line x" title="46:205	For example, s(2,,4) is the score of creating a dependency from hit to ball, since ball is the first child to the right of hit." ></td>
	<td class="line x" title="47:205	More formally, if the word xi0 has the children shown in this picture, xi0 xi1  xij xij+1  xim the score factors as follows: summationtextj1 k=1 s(i0,ik+1,ik) + s(i0,,ij) + s(i0,,ij+1) +summationtextm1k=j+1s(i0,ik,ik+1) This second-order factorization subsumes the first-order factorization, since the score function could just ignore the middle argument to simulate first-order scoring." ></td>
	<td class="line x" title="48:205	The score of a tree for secondorder parsing is now s(x,y) = summationdisplay (i,k,j)y s(i,k,j) where k and j are adjacent, same-side children of i in the tree y. The second-order model allows us to condition onthe mostrecent parsing decision, thatis, the last dependent picked up by a particular word, which is analogous to the the Markov conditioning of in the Charniak parser (Charniak, 2000)." ></td>
	<td class="line x" title="49:205	2.2 Exact Projective Parsing For projective MST parsing, the first-order algorithm can be extended to the second-order case, as was noted by Eisner (1996)." ></td>
	<td class="line x" title="50:205	The intuition behind the algorithm is shown graphically in Figure 3, which displays both the first-order and secondorder algorithms." ></td>
	<td class="line x" title="51:205	In the first-order algorithm, a word will gather its left and right dependents independently by gathering each half of the subtree rooted by its dependent in separate stages." ></td>
	<td class="line x" title="52:205	By splitting up chart items into left and right components, the Eisner algorithm only requires 3 indices to be maintained at each step, as discussed in detail elsewhere (Eisner, 1996; McDonald et al. , 2005b)." ></td>
	<td class="line x" title="53:205	For the second-order algorithm, the key insight is to delay the scoring of edges until pairs 2-order-non-proj-approx(x,s) Sentence x = x0 xn, x0 = root Weight function s : (i,k,j)  R 1." ></td>
	<td class="line x" title="54:205	Let y = 2-order-proj(x,s) 2." ></td>
	<td class="line x" title="55:205	while true 3." ></td>
	<td class="line x" title="56:205	m = ,c = 1,p = 1 4." ></td>
	<td class="line x" title="57:205	for j : 1n 5." ></td>
	<td class="line x" title="58:205	for i : 0n 6." ></td>
	<td class="line x" title="59:205	yprime = y[i  j] 7." ></td>
	<td class="line x" title="60:205	if tree(yprime) or k : (i,k,j)  y continue 8." ></td>
	<td class="line x" title="61:205	 = s(x,yprime) s(x,y) 9." ></td>
	<td class="line x" title="62:205	if  > m 10." ></td>
	<td class="line x" title="63:205	m = ,c = j,p = i 11." ></td>
	<td class="line x" title="64:205	end for 12." ></td>
	<td class="line x" title="65:205	end for 13." ></td>
	<td class="line x" title="66:205	if m > 0 14." ></td>
	<td class="line x" title="67:205	y = y[p  c] 15." ></td>
	<td class="line x" title="68:205	else return y 16." ></td>
	<td class="line x" title="69:205	end while Figure 4: Approximate second-order nonprojective parsing algorithm." ></td>
	<td class="line x" title="70:205	of dependents have been gathered." ></td>
	<td class="line x" title="71:205	This allows for the collection of pairs of adjacent dependents in a single stage, which allows for the incorporation of second-order scores, while maintaining cubictime parsing." ></td>
	<td class="line x" title="72:205	The Eisner algorithm can be extended to an arbitrary mth-order model with a complexity of O(nm+1), for m > 1." ></td>
	<td class="line x" title="73:205	An mth-order parsing algorithm willworksimilarly tothe second-order algorithm, except that wecollect mpairs of adjacent dependents in succession before attaching them to their parent." ></td>
	<td class="line x" title="74:205	2.3 Approximate Non-projective Parsing Unfortunately, second-order non-projective MST parsing is NP-hard, as shown in appendix A. To circumvent this, we designed an approximate algorithm based on the exact O(n3) second-order projective Eisner algorithm." ></td>
	<td class="line x" title="75:205	The approximation works by first finding the highest scoring projective parse." ></td>
	<td class="line x" title="76:205	It then rearranges edges in the tree, one at a time, as long as such rearrangements increase the overall score and do not violate the tree constraint." ></td>
	<td class="line x" title="77:205	We can easily motivate this approximation by observing that even in non-projective languages like Czech and Danish, most trees are primarily projective with just a few non-projective edges (Nivre and Nilsson, 2005)." ></td>
	<td class="line x" title="78:205	Thus, by starting with the highest scoring projective tree, we are typically only a small number of transformations away from the highest scoring non-projective tree." ></td>
	<td class="line x" title="79:205	The algorithm is shown in Figure 4." ></td>
	<td class="line x" title="80:205	The expression y[i  j] denotes the dependency graph identical to y except that xis parent is xi instead 83 FIRST-ORDER h 1 h3  h1 r r+1 h3 (A) h1 h3 h1 h3 (B) SECOND-ORDER h 1 h2 h2 h3  h1 h2 h2 r r+1 h3 (A) h1 h2 h2 h3  h1 h2 h2 h3 (B) h1 h3 h1 h3 (C) Figure 3: A O(n3) extension of the Eisner algorithm to second-order dependency parsing." ></td>
	<td class="line x" title="81:205	This figure shows how h1 creates a dependency to h3 with the second-order knowledge that the last dependent of h1 was h2." ></td>
	<td class="line x" title="82:205	This is done through the creation of a sibling item in part (B)." ></td>
	<td class="line x" title="83:205	In the first-order model, the dependency to h3 is created after the algorithm has forgotten that h2 was the last dependent." ></td>
	<td class="line x" title="84:205	of what it was in y. The test tree(y) is true iff the dependency graph y satisfies the tree constraint." ></td>
	<td class="line x" title="85:205	In more detail, line 1 of the algorithm sets y to the highest scoring second-order projective tree." ></td>
	<td class="line x" title="86:205	The loop of lines 216 exits only when no further score improvement is possible." ></td>
	<td class="line x" title="87:205	Each iteration seeks the single highest-scoring parent change to y that does not break the tree constraint." ></td>
	<td class="line x" title="88:205	To that effect, the nested loops starting in lines 4 and 5 enumerate all (i,j) pairs." ></td>
	<td class="line x" title="89:205	Line 6 sets yprime to the dependency graph obtained from y by changing xjs parent to xi." ></td>
	<td class="line x" title="90:205	Line 7 checks that the move from y to yprime is valid by testing that xjs parent was not already xi and that yprime is a tree." ></td>
	<td class="line x" title="91:205	Line 8 computes the score change from y to yprime." ></td>
	<td class="line x" title="92:205	If this change is larger than the previous best change, we record how this new tree was created (lines 9-10)." ></td>
	<td class="line x" title="93:205	After considering all possible valid edge changes to the tree, the algorithm checks to see that the best new tree does have a higher score." ></td>
	<td class="line x" title="94:205	If that is the case, we change the tree permanently and re-enter the loop." ></td>
	<td class="line x" title="95:205	Otherwise we exit since there are no single edge switches that can improve the score." ></td>
	<td class="line x" title="96:205	This algorithm allows for the introduction of non-projective edges because we do not restrict any of the edge changes except to maintain the tree property." ></td>
	<td class="line x" title="97:205	In fact, if any edge change is ever made, the resulting tree is guaranteed to be nonprojective, otherwise there would have been a higher scoring projective tree that would have already been found by the exact projective parsing algorithm." ></td>
	<td class="line x" title="98:205	It is not difficult to find examples for which this approximation will terminate without returning the highest-scoring non-projective parse." ></td>
	<td class="line x" title="99:205	It is clear that this approximation will always terminate  there are only a finite number of dependency trees for any given sentence and each iteration of the loop requires an increase in score to continue." ></td>
	<td class="line x" title="100:205	However, the loop could potentially take exponential time, so we will bound the number of edge transformations to a fixed value M. It is easy to argue that this will not hurt performance." ></td>
	<td class="line x" title="101:205	Even in freer-word order languages such as Czech, almost all non-projective dependency trees are primarily projective, modulo a few nonprojective edges." ></td>
	<td class="line x" title="102:205	Thus, if our inference algorithm starts with the highest scoring projective parse, the best non-projective parse only differs by a small number of edge transformations." ></td>
	<td class="line x" title="103:205	Furthermore, it is easy to show that each iteration of the loop takes O(n2) time, resulting in a O(n3 + Mn2) runtime algorithm." ></td>
	<td class="line x" title="104:205	In practice, the approximation terminates after a small number of transformations and we do not need to bound the number of iterations in our experiments." ></td>
	<td class="line x" title="105:205	Weshould note that this is one of many possible approximations wecould have made." ></td>
	<td class="line x" title="106:205	Another reasonable approach would be to first find the highest scoring first-order non-projective parse, and then re-arrange edges based on second order scores in a similar manner to the algorithm we described." ></td>
	<td class="line x" title="107:205	We implemented this method and found that the results were slightly worse." ></td>
	<td class="line x" title="108:205	3 Danish: Parsing Secondary Parents Kromann (2001) argued for a dependency formalism called Discontinuous Grammar and annotated a large set of Danish sentences using this formalism to create the Danish Dependency Treebank (Kromann, 2003)." ></td>
	<td class="line x" title="109:205	The formalism allows for a 84 root Han spejder efter og ser elefanterne He looks for and sees elephants Figure 5: An example dependency tree from the Danish Dependency Treebank (from Kromann (2003))." ></td>
	<td class="line x" title="110:205	word to have multiple parents." ></td>
	<td class="line x" title="111:205	Examples include verb coordination in which the subject or object is an argument of several verbs, and relative clauses in which words must satisfy dependencies both inside and outside the clause." ></td>
	<td class="line x" title="112:205	An example is shown in Figure 5 for the sentence He looks for and sees elephants." ></td>
	<td class="line x" title="113:205	Here, the pronoun He is the subject for both verbs in the sentence, and the noun elephants the corresponding object." ></td>
	<td class="line x" title="114:205	In the Danish Dependency Treebank, roughly 5% of words have more than one parent, which breaks the single parent (or tree) constraint we have previously required on dependency structures." ></td>
	<td class="line x" title="115:205	Kromann also allows for cyclic dependencies, though we deal only with acyclic dependency graphs here." ></td>
	<td class="line x" title="116:205	Though less common than trees, dependency graphs involving multiple parents are well established in the literature (Hudson, 1984)." ></td>
	<td class="line x" title="117:205	Unfortunately, the problem of finding the dependency structure with highest score in this setting is intractable (Chickering et al. , 1994)." ></td>
	<td class="line x" title="118:205	To create an approximate parsing algorithm for dependency structures with multiple parents, we start with our approximate second-order nonprojective algorithm outlined in Figure 4." ></td>
	<td class="line x" title="119:205	We use the non-projective algorithm since the Danish Dependency Treebank contains a small number of non-projective arcs." ></td>
	<td class="line x" title="120:205	We then modify lines 7-10 of this algorithm so that it looks for the change in parent or the addition of a new parent that causes the highest change in overall score and does not create a cycle2." ></td>
	<td class="line x" title="121:205	Like before, we make one change per iteration and that change will depend on the resulting score of the new tree." ></td>
	<td class="line x" title="122:205	Using this simple new approximate parsing algorithm, we train a new parser that can produce multiple parents." ></td>
	<td class="line x" title="123:205	4 Online Learning and Approximate Inference In this section, we review the work of McDonald et al.(2005b) for online large-margin dependency 2We are not concerned with violating the tree constraint." ></td>
	<td class="line x" title="125:205	parsing." ></td>
	<td class="line x" title="126:205	As usual for supervised learning, we assume a training set T = {(xt,yt)}Tt=1, consisting of pairs of a sentence xt and its correct dependency representation yt." ></td>
	<td class="line x" title="127:205	The algorithm is an extension of the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003) to learning with structured outputs, in the present case dependency structures." ></td>
	<td class="line x" title="128:205	Figure 6 gives pseudo-code for the algorithm." ></td>
	<td class="line x" title="129:205	An online learning algorithm considers a single training instance for each update to the weight vector w. We use the common method of setting the final weight vector as the average of the weight vectors after each iteration (Collins, 2002), which has been shown to alleviate overfitting." ></td>
	<td class="line x" title="130:205	On each iteration, the algorithm considers a single training instance." ></td>
	<td class="line x" title="131:205	We parse this instance to obtain a predicted dependency graph, and find the smallest-norm update to the weight vector w that ensures that the training graph outscores the predicted graph by a margin proportional to the loss of the predicted graph relative to the training graph, which is the number of words with incorrect parents in the predicted tree (McDonald et al. , 2005b)." ></td>
	<td class="line x" title="132:205	Note that we only impose margin constraints between the single highest-scoring graph and the correct graph relative to the current weight setting." ></td>
	<td class="line x" title="133:205	Past work on tree-structured outputs has used constraints for the k-best scoring tree (McDonald et al. , 2005b) or even all possible trees by using factored representations (Taskar et al. , 2004; McDonald et al. , 2005c)." ></td>
	<td class="line x" title="134:205	However, we have found that a single margin constraint per example leads to much faster training with a negligible degradation in performance." ></td>
	<td class="line x" title="135:205	Furthermore, this formulation relates learning directly to inference, which is important, since we want the model to set weights relative to the errors made by an approximate inference algorithm." ></td>
	<td class="line x" title="136:205	This algorithm can thus be viewed as a large-margin version of the perceptron algorithm for structured outputs Collins (2002)." ></td>
	<td class="line pc" title="137:205	Online learning algorithms have been shown to be robust even with approximate rather than exact inference in problems such as word alignment (Moore, 2005), sequence analysis (Daume and Marcu, 2005; McDonald et al. , 2005a) and phrase-structure parsing (Collins and Roark, 2004)." ></td>
	<td class="line x" title="138:205	This robustness to approximations comes from the fact that the online framework sets weights with respect to inference." ></td>
	<td class="line x" title="139:205	In other words, the learning method sees common errors due to 85 Training data: T = {(xt,yt)}Tt=1 1." ></td>
	<td class="line x" title="140:205	w(0) = 0; v = 0; i = 0 2." ></td>
	<td class="line x" title="141:205	for n : 1N 3." ></td>
	<td class="line x" title="142:205	for t : 1T 4." ></td>
	<td class="line x" title="143:205	min  w(i+1) w(i)   s.t. s(xt,yt;w(i+1)) s(xt,yprime;w(i+1))  L(yt,yprime) where yprime = arg maxyprime s(xt,yprime;w(i)) 5." ></td>
	<td class="line x" title="144:205	v = v + w(i+1) 6." ></td>
	<td class="line x" title="145:205	i = i + 1 7." ></td>
	<td class="line x" title="146:205	w = v/(N T) Figure 6: MIRA learning algorithm." ></td>
	<td class="line x" title="147:205	We write s(x,y;w(i)) to mean the score of tree y using weight vector w(i)." ></td>
	<td class="line x" title="148:205	approximate inference and adjusts weights to correct for them." ></td>
	<td class="line x" title="149:205	The work of Daume and Marcu (2005) formalizes this intuition by presenting an online learning framework in which parameter updates aremadedirectly withrespect toerrors inthe inference algorithm." ></td>
	<td class="line x" title="150:205	We show in the next section that this robustness extends to approximate dependency parsing." ></td>
	<td class="line x" title="151:205	5 Experiments The score of adjacent edges relies on the definition of a feature representation f(i,k,j)." ></td>
	<td class="line x" title="152:205	As noted earlier, thisrepresentation subsumes thefirst-order representation of McDonald et al.(2005b), so we can incorporate all of their features as well as the new second-order features we now describe." ></td>
	<td class="line x" title="154:205	The old first-order features are built from the parent and child words, their POS tags, and the POS tags of surrounding words and those of words between the child and the parent, as well as the direction and distance from the parent to the child." ></td>
	<td class="line x" title="155:205	The second-order features are built from the following conjunctions of word and POS identity predicates xi-pos, xk-pos, xj-pos xk-pos, xj-pos xk-word, xj-word xk-word, xj-pos xk-pos, xj-word where xi-pos is the part-of-speech of the ith word in the sentence." ></td>
	<td class="line x" title="156:205	We also include conjunctions between these features and the direction and distance from siblingj tosiblingk." ></td>
	<td class="line x" title="157:205	Wedetermined theusefulness of these features on the development set, which also helped us find out that features such as the POS tags of words between the two siblings would not improve accuracy." ></td>
	<td class="line x" title="158:205	We also ignored feaEnglish Accuracy Complete 1st-order-projective 90.7 36.7 2nd-order-projective 91.5 42.1 Table 1: Dependency parsing results for English." ></td>
	<td class="line x" title="159:205	Czech Accuracy Complete 1st-order-projective 83.0 30.6 2nd-order-projective 84.2 33.1 1st-order-non-projective 84.1 32.2 2nd-order-non-projective 85.2 35.9 Table 2: Dependency parsing results for Czech." ></td>
	<td class="line x" title="160:205	tures over triples of words since this would explode the size of the feature space." ></td>
	<td class="line x" title="161:205	We evaluate dependencies on per word accuracy, which is the percentage of words in the sentence with the correct parent in the tree, and on complete dependency analysis." ></td>
	<td class="line x" title="162:205	In our evaluation we exclude punctuation for English and include it for Czech and Danish, which is the standard." ></td>
	<td class="line x" title="163:205	5.1 English Results To create data sets for English, we used the Yamada and Matsumoto (2003) head rules to extract dependency trees from the WSJ, setting sections 2-21 as training, section 22 for development and section 23 for evaluation." ></td>
	<td class="line x" title="164:205	The models rely on part-of-speech tags as input and we used the Ratnaparkhi (1996) tagger to provide these for the development and evaluation set." ></td>
	<td class="line x" title="165:205	These data sets are exclusively projective so we only compare the projective parsers using the exact projective parsing algorithms." ></td>
	<td class="line x" title="166:205	The purpose of these experiments is to gauge the overall benefit from including second-order features with exact parsing algorithms, which can be attained in the projective setting." ></td>
	<td class="line x" title="167:205	Results are shown in Table 1." ></td>
	<td class="line x" title="168:205	We can see that there is clearly an advantage in introducing second-order features." ></td>
	<td class="line x" title="169:205	In particular, the complete tree metric is improved considerably." ></td>
	<td class="line x" title="170:205	5.2 Czech Results For the Czech data, we used the predefined training, development and testing split of the Prague Dependency Treebank (Hajic et al. , 2001), and the automatically generated POS tags supplied with the data, which we reduce to the POS tag set from Collins et al.(1999)." ></td>
	<td class="line x" title="172:205	On average, 23% of the sentences in the training, development and test sets have at least one non-projective dependency, though, less than 2% of total edges are ac86 Danish Precision Recall F-measure 2nd-order-projective 86.4 81.7 83.9 2nd-order-non-projective 86.9 82.2 84.4 2nd-order-non-projective w/ multiple parents 86.2 84.9 85.6 Table 3: Dependency parsing results for Danish." ></td>
	<td class="line x" title="173:205	tually non-projective." ></td>
	<td class="line x" title="174:205	Results are shown in Table 2." ></td>
	<td class="line x" title="175:205	McDonald et al.(2005c) showed a substantial improvement in accuracy by modeling nonprojective edges inCzech, shownbythe difference between two first-order models." ></td>
	<td class="line x" title="177:205	Table 2 shows that a second-order model provides a comparable accuracy boost, even using an approximate non-projective algorithm." ></td>
	<td class="line x" title="178:205	The second-order nonprojective model accuracy of 85.2% is the highest reported accuracy forasingle parserforthese data." ></td>
	<td class="line x" title="179:205	Similar results were obtained by Hall and Novak (2005) (85.1% accuracy) who take the best output of the Charniak parser extended to Czech and rerank slight variations on this output that introduce non-projective edges." ></td>
	<td class="line x" title="180:205	However, this system relies on a much slower phrase-structure parser as its base model as well as an auxiliary reranking module." ></td>
	<td class="line x" title="181:205	Indeed, our second-order projective parser analyzes the test set in 16m32s, and the non-projective approximate parser needs 17m03s toparse theentire evaluation set, showing thatruntime for the approximation is completely dominated by the initial call to the second-order projective algorithm and that the post-process edge transformation loop typically only iterates a few times per sentence." ></td>
	<td class="line x" title="182:205	5.3 Danish Results For our experiments we used the Danish Dependency Treebank v1.0." ></td>
	<td class="line x" title="183:205	The treebank contains a small number of inter-sentence and cyclic dependencies and we removed all sentences that contained such structures." ></td>
	<td class="line x" title="184:205	The resulting data set contained 5384 sentences." ></td>
	<td class="line x" title="185:205	We partitioned the data into contiguous 80/20 training/testing splits." ></td>
	<td class="line x" title="186:205	We held out a subset of the training data for development purposes." ></td>
	<td class="line x" title="187:205	We compared three systems, the standard second-order projective and non-projective parsing models, as well as our modified second-order non-projective model that allows for the introduction of multiple parents (Section 3)." ></td>
	<td class="line x" title="188:205	All systems use gold-standard part-of-speech since no trained tagger is readily available for Danish." ></td>
	<td class="line x" title="189:205	Results are shown in Figure 3." ></td>
	<td class="line x" title="190:205	As might be expected, the nonprojective parser does slightly better than the projective parser because around 1% of the edges are non-projective." ></td>
	<td class="line x" title="191:205	Since each word may have an arbitrary number of parents, we must use precision and recall rather than accuracy to measure performance." ></td>
	<td class="line x" title="192:205	This also means that the correct training loss is no longer the Hamming loss." ></td>
	<td class="line x" title="193:205	Instead, we use false positives plus false negatives over edge decisions, which balances precision and recall as our ultimate performance metric." ></td>
	<td class="line x" title="194:205	As expected, for the basic projective and nonprojective parsers, recall is roughly 5% lower than precision since these models can only pick up at most one parent per word." ></td>
	<td class="line x" title="195:205	For the parser that can introduce multiple parents, we see an increase in recall of nearly 3% absolute with a slight drop in precision." ></td>
	<td class="line x" title="196:205	These results are very promising and further show the robustness of discriminative onlinelearning withapproximate parsing algorithms." ></td>
	<td class="line x" title="197:205	6 Discussion We described approximate dependency parsing algorithms that support higher-order features and multiple parents." ></td>
	<td class="line x" title="198:205	We showed that these approximations can be combined with online learning to achieve fast parsing with competitive parsing accuracy." ></td>
	<td class="line x" title="199:205	These results show that the gain from allowing richer representations outweighs the loss from approximate parsing and further shows the robustness of online learning algorithms with approximate inference." ></td>
	<td class="line x" title="200:205	The approximations we have presented are very simple." ></td>
	<td class="line x" title="201:205	Theystart withareasonably good baseline and make small transformations until the score of the structure converges." ></td>
	<td class="line x" title="202:205	These approximations work because freer-word order languages we studied are still primarily projective, making the approximate starting point close to the goal parse." ></td>
	<td class="line x" title="203:205	However, we would like to investigate the benefits for parsing of more principled approaches to approximate learning and inference techniques such as the learning as search optimization framework of (Daume and Marcu, 2005)." ></td>
	<td class="line x" title="204:205	This framework will possibly allow us to include effectively more global features over the dependency structure than 87 those in our current second-order model." ></td>
	<td class="line x" title="205:205	Acknowledgments This work was supported by NSF ITR grants 0205448." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N06-1045
A Better N-Best List: Practical Determinization Of Weighted Finite Tree Automata
May, Jonathan;Knight, Kevin;"></td>
	<td class="line x" title="1:166	Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 351358, New York, June 2006." ></td>
	<td class="line x" title="2:166	c2006 Association for Computational Linguistics A Better -BestList: Practical Determinization ofWeighted Finite Tree Automata Jonathan May InformationSciences Institute UniversityofSouthernCalifornia MarinadelRey,CA90292 jonmay@isi.edu KevinKnight InformationSciences Institute UniversityofSouthernCalifornia MarinadelRey,CA90292 knight@isi.edu Abstract Ranked lists of output trees from syntactic statistical NLP applications frequently contain multiple repeated entries." ></td>
	<td class="line x" title="3:166	This redundancy leads to misrepresentation of tree weight and reduced information for debugging and tuning purposes." ></td>
	<td class="line x" title="4:166	It is chiefly due to nondeterminism in the weighted automata that produce the results." ></td>
	<td class="line x" title="5:166	We introduce an algorithm that determinizes such automata while preserving proper weights, returning the sum of the weight of all multiply derived trees." ></td>
	<td class="line x" title="6:166	We also demonstrate our algorithms effectiveness ontwo large-scale tasks." ></td>
	<td class="line x" title="7:166	1 Introduction A useful tool in natural language processing tasks suchastranslation,speechrecognition, parsing,etc. , is the ranked list of results." ></td>
	<td class="line x" title="8:166	Modern systems typically produce competing partial results internally and return only the top-scoring complete result to the user." ></td>
	<td class="line x" title="9:166	They are, however, also capable of producing lists of runners-up, and such lists have many practical uses: The lists may be inspected to determine the quality of runners-up and motivate model changes." ></td>
	<td class="line x" title="10:166	The lists may be re-ranked with extra knowledge sources that are difficult to apply during the mainsearch." ></td>
	<td class="line oc" title="11:166	Thelistsmaybeused withannotation and a tuning process, such as in (Collins and Roark, 2004), to iteratively alter feature weights and improve results." ></td>
	<td class="line x" title="12:166	Figure 1 shows the best 10 English translation parse trees obtained froma syntax-based translation system based on (Galley, et." ></td>
	<td class="line x" title="13:166	al., 2004)." ></td>
	<td class="line x" title="14:166	Notice that the same tree occurs multiple times in this list." ></td>
	<td class="line x" title="15:166	Thisrepetition isquitecharacteristic oftheoutputof ranked lists." ></td>
	<td class="line x" title="16:166	It occurs because many systems, such astheonesproposedby(Bod,1992),(Galley,et." ></td>
	<td class="line x" title="17:166	al., 2004), and (Langkilde and Knight, 1998) represent theirresultspaceintermsofweightedpartial results of various sizes that may be assembled in multiple ways." ></td>
	<td class="line x" title="18:166	There is in general more than one way to assemble the partial results to derive the same complete result." ></td>
	<td class="line x" title="19:166	Thus, the -best list of results is really an -best listof derivations." ></td>
	<td class="line x" title="20:166	Whenlist-basedtasks,suchastheonesmentioned above, take as input the top results for some constant,theeffectofrepetition onthesetasksisdeleterious." ></td>
	<td class="line x" title="21:166	A list with many repetitions suffers from a lack of useful information, hampering diagnostics." ></td>
	<td class="line x" title="22:166	Repeated results prevent alternatives that would be highly ranked inasecondary reranking systemfrom even being considered." ></td>
	<td class="line x" title="23:166	And a list of fewer unique trees than expected can cause overfitting when this listisusedtotune." ></td>
	<td class="line x" title="24:166	Furthermore,theactualweightof obtaininganyparticulartreeissplitamongitsrepetitions, distorting the actual relative weights between trees." ></td>
	<td class="line x" title="25:166	(Mohri,1997)encounteredthisprobleminspeech recognition, and presented a solution to the problem of repetition in -best lists of strings that are derived from finite-state automata." ></td>
	<td class="line x" title="26:166	That work described a way to use a powerset construction along 351 34.73: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(caused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))).(.)) 34.74: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(aroused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.)) 34.83: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(caused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.)) 34.83: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(aroused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.)) 34.84: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(caused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.)) 34.85: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(caused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.)) 34.85: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(aroused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.)) 34.85: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(aroused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.)) 34.87: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VB(arouse) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.)) 34.92: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(aroused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.)) Figure 1: Ranked list of machine translation results with repeated trees." ></td>
	<td class="line x" title="27:166	Scores shown are negative logs of calculated weights, thus a lower score indicates a higher weight." ></td>
	<td class="line x" title="28:166	The bulleted sentences indicate identical trees." ></td>
	<td class="line x" title="29:166	with an innovative bookkeeping system to determinizetheautomaton,resultinginanautomatonthat preserves the language but provides a single, properlyweightedderivation foreachstringinit." ></td>
	<td class="line x" title="30:166	Putanother way, if the input automaton has the ability to generate the same string with different weights, the output automaton generates that string with weight equal to the sum of all of the generations of that string in the input automaton." ></td>
	<td class="line x" title="31:166	In (Mohri and Riley, 2002)thistechniquewascombinedwithaprocedure forefficiently obtaining -bestranked lists,yielding alist ofstring results with norepetition." ></td>
	<td class="line x" title="32:166	In this paper we extend that work to deal with grammars that produce trees." ></td>
	<td class="line x" title="33:166	Regular tree grammars (Brainerd, 1969), which subsume the tree substitution grammars developed in the NLP community (Schabes, 1990), are of particular interest to those wishing to work with additional levels of structure that string grammars cannot provide." ></td>
	<td class="line x" title="34:166	The application to parsing is natural, and in machine translation tree grammars can be used to model syntactic transfer, control of function words, reordering, and target-language well-formedness." ></td>
	<td class="line x" title="35:166	In theworldofautomatathesegrammarshaveasanatural dual the finite tree recognizer (Doner, 1970)." ></td>
	<td class="line x" title="36:166	Like tree grammars and packed forests, they are compact ways of representing very large sets of trees." ></td>
	<td class="line x" title="37:166	We will present an algorithm for determinizing weighted finite tree recognizers, and use a variant of the procedure found in (Huang and Chiang, 2005)toobtain -bestlistsoftreesthatareweighted correctly and contain no repetition." ></td>
	<td class="line x" title="38:166	Section2describesrelatedwork." ></td>
	<td class="line x" title="39:166	InSection3,we introduce the formalisms of tree automata, specifically the tree-to-weight transducer." ></td>
	<td class="line x" title="40:166	In Section 4,we present thealgorithm." ></td>
	<td class="line x" title="41:166	Finally,inSection5weshow the results of applying weighted determinization to recognizers obtained from the packed forest output oftwo natural language tasks." ></td>
	<td class="line x" title="42:166	2 Previous Work The formalisms of tree automata are summarized well in (Gecseg and Steinby, 1984)." ></td>
	<td class="line x" title="43:166	Bottom-up tree recognizers are due to (Thatcher and Wright, 1968), (Doner, 1970), and (Magidor and Moran, 1969)." ></td>
	<td class="line x" title="44:166	Top-downtreerecognizers aredueto(Rabin, 1969) and(Magidor and Moran,1969)." ></td>
	<td class="line x" title="45:166	(Comon, et." ></td>
	<td class="line x" title="46:166	al., 1997) show the determinization of unweighted finite-state tree automata, and prove its correctness." ></td>
	<td class="line x" title="47:166	(Borchardt and Vogler, 2003) present determinizationofweighted finite-statetreeautomatawithadifferent method than the one we present here." ></td>
	<td class="line x" title="48:166	While ourmethodisapplicable tofinitetreesets,theprevious method claims the ability to determinize some classes of infinite tree sets." ></td>
	<td class="line x" title="49:166	However, for the finite case the previous method produces an automaton with size on the order of the number of derivations, so the technique is limited when applied to real world data." ></td>
	<td class="line x" title="50:166	3 Grammars, Recognizers, and Transducers Asdescribedin(GecsegandSteinby,1984),treeautomata may be broken into two classes, recognizers andtransducers." ></td>
	<td class="line x" title="51:166	Recognizersreadtreeinputanddecidewhethertheinputisinthelanguagerepresented bytherecognizer." ></td>
	<td class="line x" title="52:166	Formally,abottom-uptreerecognizer isdefined by : 1 is a finiteset of states, 1 Readers familiar with (Gecseg and Steinby, 1984) will noticethatwehaveintroduced astartstate,modified thenotion of initial assignment, and changed the arity of nullary symbols to unary symbols." ></td>
	<td class="line x" title="53:166	Thisistomake treeautomata morepalatable to those accustomed to string automata and to allow for a useful graphical interpretation." ></td>
	<td class="line x" title="54:166	352 Figure 2: Visualization of a bottom-up tree recognizer is aranked alphabet, isthe initial state, isaset of final states, and isafiniteset of transitions from a vector of states to one state that reads a -ary symbol." ></td>
	<td class="line x" title="55:166	Consider the following tree recognizer: 2 Aswithstring automata, itishelpful tohave avisualization tounderstand whattherecognizer isrecognizing." ></td>
	<td class="line x" title="56:166	Figure 2 provides a visualization of the recognizer above." ></td>
	<td class="line x" title="57:166	Notice that some members of are drawn as arcs with multiple (and ordered) tails." ></td>
	<td class="line x" title="58:166	This is the key difference in visualization between string and tree automata  to capture the arity of the symbol being read we must visualize the automata asan ordered hypergraph." ></td>
	<td class="line x" title="59:166	The function of the members of in the hypergraph visualization leads us to refer to the vector of states as an input vector of states, and the single state as an output state." ></td>
	<td class="line x" title="60:166	We will refer to as the transition set of the recognizer." ></td>
	<td class="line x" title="61:166	In string automata, a path through a recognizer consists ofasequence ofedges thatcan befollowed fromastarttoanendstate." ></td>
	<td class="line x" title="62:166	Theconcatenation oflabelsoftheedgesofapath,typically inaleft-to-right order, forms a string in the recognizers language." ></td>
	<td class="line x" title="63:166	In tree automata, however, a hyperpath through a recognizer consists ofasequenceofhyperedges that can be followed, sometimes in parallel, from a start 2 The number denotes the arity of the symbol." ></td>
	<td class="line x" title="64:166	Figure 3: Bottom-up tree-to-weight transducer to an end state." ></td>
	<td class="line x" title="65:166	We arrange the labels of the hyperedges to form a tree in the recognizers language but must now consider proper order in two dimensions." ></td>
	<td class="line x" title="66:166	The proper vertical order is specified by the order of application of transitions, i.e., the labels of transitions followed earlier are placed lower in the treethanthelabels oftransitions followed later." ></td>
	<td class="line x" title="67:166	The properhorizontalorderwithinonelevelofthetreeis specified bytheorderofstates inatransitions input vector." ></td>
	<td class="line x" title="68:166	Intheexamplerecognizer,thetrees and arevalid." ></td>
	<td class="line x" title="69:166	Noticethat maybe recognized intwodifferent hyperpaths." ></td>
	<td class="line x" title="70:166	Like tree recognizers, tree transducers read tree input and decide whether the input is in the language, but they simultaneously produce some output as well." ></td>
	<td class="line x" title="71:166	Since we wish to associate a weight with every acceptable tree in a language, we will consider transducers that produce weights as their output." ></td>
	<td class="line x" title="72:166	Note that in transitioning from recognizers to transducers we are following the convention established in (Mohri, 1997) where a transducer with weight outputs is used to represent a weighted recognizer." ></td>
	<td class="line x" title="73:166	One may consider the determinization of tree-to-weight transducers as equivalent to the determinization ofweighted tree recognizers." ></td>
	<td class="line x" title="74:166	Formally, a bottom-up tree-to-weight transducer is defined by where,,,and aredefined asfor recognizers, and: is a finite set of transitions from a vector of states toone state,reading a -arysymbol and outputting some weight is the initial weight function mapping to is the final weight function mapping 353 to . We must also specify a convention for propagating the weight calculated in every transition." ></td>
	<td class="line x" title="75:166	This can be explicitly defined for each transition but we will simplify matters by defining the propagation of theweighttoadestinationstateasthemultiplication ofthe weightateach source state withtheweight of the production." ></td>
	<td class="line x" title="76:166	We modify the previous example by adding weightsasfollows: Asanexample,considerthefollowingtree-to-weight transducer (,,,and are asbefore): Figure 3 shows the addition of weights onto the automata, forming the above transducer." ></td>
	<td class="line x" title="77:166	Notice the tree yields the weight 0.036 ( ), and yields the weight 0.012 ( ) or0.054 ( ), depending on the hyperpath followed." ></td>
	<td class="line x" title="78:166	Thistransducer isanexampleofanonsubsequential transducer." ></td>
	<td class="line x" title="79:166	A tree transducer is subsequential if foreachvector vof statesandeach there isatmostonetransitionin withinputvectorvand label . These restrictions ensure a subsequential transducer yields a single output for each possible input, that is, itisdeterministic inits output." ></td>
	<td class="line x" title="80:166	Becausewewillreasonaboutthedestinationstate of a transducer transition and the weight of a transducer transition separately, we make the following definition." ></td>
	<td class="line x" title="81:166	For a given v where v is a vector of states,,, and, let v and v . Equivalent shorthand forms are and . 4 Determinization Thedeterminization algorithmispresented asAlgorithm 1." ></td>
	<td class="line x" title="82:166	Ittakes asinput abottom-up tree-to-weight transducer and returns as output a subsequential bottom-up tree-to-weight transducer suchthatthe tree language recognized by is equivalent to that of andtheoutputweightgiveninputtree on is equaltothesumofallpossibleoutputweightsgiven on . Like the algorithm of (Mohri, 1997), this Figure 4: a) Portion of a transducer before determinization; b) The same portion after determinization algorithmwillterminateforautomatathatrecognize finite tree languages." ></td>
	<td class="line x" title="83:166	It may terminate on some automatathatrecognize infinite treelanguages, butwe do not consider any ofthese cases in this work." ></td>
	<td class="line x" title="84:166	Determinizing a tree-to-weight transducer can be thoughtofasatwo-stageprocess." ></td>
	<td class="line x" title="85:166	First,thestructure of the automata must be determined such that a single hyperpath exists for each recognized input tree." ></td>
	<td class="line x" title="86:166	This is achieved by a classic powerset construction, i.e., a state must be constructed in the output transducerthatrepresentsallthepossiblereachabledestination states given aninput and alabel." ></td>
	<td class="line x" title="87:166	Because we areworkingwithtreeautomata, ourinputisavector of states, not a single state." ></td>
	<td class="line x" title="88:166	A comparable powerset construction on unweighted tree automata and a proofofcorrectness canbefoundin(Comon,et." ></td>
	<td class="line x" title="89:166	al., 1997)." ></td>
	<td class="line x" title="90:166	The second consideration to weighted determinizationisproperpropagationofweights." ></td>
	<td class="line x" title="91:166	Forthis we will use (Mohri, 1997)s concept of the residual weight." ></td>
	<td class="line x" title="92:166	We represent in the construction of states in the output transducer not only a subset of states oftheinputtransducer, butalsoanumberassociated with each of these states, called the residual." ></td>
	<td class="line x" title="93:166	Since we want s hyperpath of a particular input tree to have as its associated weight the sumof the weights of the all of s hyperpaths of the input tree, wereplace a set of hyperedges in that have the same input state vector and label with a single hyperedge in bearing the label and the sum of s hyperedge weights." ></td>
	<td class="line x" title="94:166	The destination state of the hyperedge represents the states reachable by s applicable hyperedges and for each state, the proportion of the weight fromthe relevant transition." ></td>
	<td class="line x" title="95:166	Figure 4 shows the determinization of a portion of the example transducer." ></td>
	<td class="line x" title="96:166	Note that the hyperedge 354 Figure 5: Determinized bottom-up tree-to-weight transducer leading to state in the input transducer contributes of the weight on the output transducer hyperedge and the hyperedge leading to state in the input transducer contributes the remaining . This is reflected in the state construction in the output transducer." ></td>
	<td class="line x" title="97:166	Thecompletedeterminization oftheexample transducer is shown inFigure 5." ></td>
	<td class="line x" title="98:166	To encapsulate the representation of states from theinputtransducerandassociatedresidualweights, we define a state in the output transducer as a set of tuples, where and . Since the algorithm builds new states progressively, we will need to represent a vector of states from the output transducer, typically depicted as v. We may construct the vector pair q w from v, where q is a vector of states of the input transducer and w is a vector of residual weights, by choosing a (state, weight) pair from each output state in v. For example, let . Then two possible output transducer states could be and . Ifwechoose v thena valid vector pair q w is q, w . The sets v, v, and v are defined asfollows: v q w from v q . v q w from v q . v q w from v q . . v is the set of vector pairs q w constructed from v where each q is an input vector in atransition withlabel . v isthesetof unique transitions paired with the appropriate pair foreach q w in v . v isthesetofstates reachable fromthe transitions in v . The consideration of vectors of states on the incident edge of transitions effects two noticeable changes on the algorithm as it is presented in (Mohri, 1997)." ></td>
	<td class="line x" title="99:166	The first, relatively trivial, change is the inclusion of the residual of multiple states in the calculation of weights and residuals on lines 16 and 17." ></td>
	<td class="line x" title="100:166	The second change is the production of vectors for consideration." ></td>
	<td class="line x" title="101:166	Whereas the string-based algorithm considered newly-created states in turn, we must consider newly-available vectors." ></td>
	<td class="line x" title="102:166	Foreach newly created state, newly available vectors can be formed by using that state with the other states of the output transducer." ></td>
	<td class="line x" title="103:166	This operation is performed on lines 7and 22 ofthe algorithm." ></td>
	<td class="line x" title="104:166	5 Empirical Studies Wenowturntosomeempiricalstudies." ></td>
	<td class="line x" title="105:166	Weexamine the practical impact of the presented work by showing: That the multiple derivation problem is pervasive in practice and determinization iseffective atremoving duplicate trees." ></td>
	<td class="line x" title="106:166	That duplication causes misleading weighting of individual trees and the summing achieved from weighted determinization corrects this error, leading to re-ordering of the -best list." ></td>
	<td class="line x" title="107:166	That weighted determinization positively affects end-to-end system performance." ></td>
	<td class="line x" title="108:166	We also compare our results to a commonly used technique for estimation of -best lists, i.e., summing over the top derivations to get weight estimates of thetop unique elements." ></td>
	<td class="line x" title="109:166	5.1 Machine translation We obtain packed-forest English outputs from 116 short Chinese sentences computed by a string-totree machine translation system based on (Galley, et." ></td>
	<td class="line x" title="110:166	al., 2004)." ></td>
	<td class="line x" title="111:166	The system is trained on all ChineseEnglish parallel data available from the Linguistic Data Consortium." ></td>
	<td class="line x" title="112:166	The decoder for this system is a CKY algorithm that negotiates the space described in (DeNeefe, et." ></td>
	<td class="line x" title="113:166	al.,2005)." ></td>
	<td class="line x" title="114:166	Nolanguage model was used in this experiment." ></td>
	<td class="line x" title="115:166	The forests contain a median of English parse trees each." ></td>
	<td class="line x" title="116:166	We remove cycles from each 355 Algorithm 1: Weighted Determinization ofTreeAutomata Input: BOTTOM-UPTREE-TO-WEIGHTTRANSDUCER . Output: SUBSEQUENTIALBOTTOM-UPTREE-TO-WEIGHTTRANSDUCER . begin1 2 3 PRIORITYQUEUE4 5 6 ENQUEUE7 while do8 v head9 v10 for each vsuch that do11 if such that then12 s.t.13 14 for each such that v do15 v v16 v v v v s.t.17 v v v18 /* RANK returns the largest hyperedge size that can leave state . COMBINATIONS returns all possible vectors of length containing members of and at least one member of . */ if v is a new state then19 for each u COMBINATIONS v v RANK do 20 ifuis a new vector then21 ENQUEUE u22 v23 DEQUEUE24 end25 forest, 3 applyourdeterminizationalgorithm,andextract the -best trees using a variant of (Huang and Chiang,2005)." ></td>
	<td class="line x" title="117:166	Theeffectsofweighteddeterminization on an -best list are obvious to casual inspection." ></td>
	<td class="line x" title="118:166	Figure 7 shows the improvement in quality of the top 10 trees from our example translation after the application ofthe determinization algorithm." ></td>
	<td class="line x" title="119:166	The improvement observed circumstantially holds up to quantitative analysis as well." ></td>
	<td class="line x" title="120:166	The forestsobtained bythedeterminized grammarshave between 1.39% and 50% of the number of trees of their undeterminized counterparts." ></td>
	<td class="line x" title="121:166	On average, the determinized forests contain 13.7% of the original 3 As in (Mohri, 1997), determinization may be applicable to some automata that recognize infinite languages." ></td>
	<td class="line x" title="122:166	In practice, cycles in tree automata of MT results are almost never desired, since these represent recursive insertion of words." ></td>
	<td class="line x" title="123:166	number of trees." ></td>
	<td class="line x" title="124:166	Since a determinized forest containsnorepeated treesbutcontains exactly thesame unique trees as its undeterminized counterpart, this indicates that an average of 86.3% of the trees in an undeterminized MToutput forest are duplicates." ></td>
	<td class="line x" title="125:166	Weighted determinization also causes a surprisingly large amount of -best reordering." ></td>
	<td class="line x" title="126:166	In 77.6% of the translations, the tree regarded as best is different after determinization." ></td>
	<td class="line x" title="127:166	This means that in a large majority of cases, the tree with the highest weight is not recognized as such in the undeterminized list because its weight is divided among its multiple derivations." ></td>
	<td class="line x" title="128:166	Determinization allows these instances and their associated weights to combine and puts the highest weighted tree, not the highest weighted derivation, atthe top ofthe list." ></td>
	<td class="line x" title="129:166	356 method Bleu undeterminized 21.87 top-500 crunching 23.33 determinized 24.17 Figure 6: Bleu results from string-to-tree machine translation of 116 short Chinese sentences with no language model." ></td>
	<td class="line x" title="130:166	The use of best derivation (undeterminized), estimateofbesttree(top-500), andtrue best tree (determinized) for selection of translation isshown." ></td>
	<td class="line x" title="131:166	We can compare our method with the more commonly used methods of crunching -best lists, where . The duplicate sentences in the trees are combined, hopefully resulting in at least unique members with an estimation of the true tree weight for each unique tree." ></td>
	<td class="line x" title="132:166	Our results indicate this is a rather crude estimation." ></td>
	<td class="line x" title="133:166	When the top 500 derivations of the translations of our test corpus are summed, only 50.6% of them yield an estimated highest-weighted tree that is the same as the true highest-weighted tree." ></td>
	<td class="line x" title="134:166	As a measure of the effect weighted determinization and its consequential re-ordering has on an actual end-to-end evaluation, we obtain Bleu scores forour1-besttranslations fromdeterminization, and compare them with the 1-best translations from the undeterminized forest and the 1-best translations from the top-500 crunching method." ></td>
	<td class="line x" title="135:166	The results are tabulated in Figure 6." ></td>
	<td class="line x" title="136:166	Note that in 26.7% of cases determinization did not terminate in a reasonable amount of time." ></td>
	<td class="line x" title="137:166	For these sentences we used the best parse from top-500 estimation instead." ></td>
	<td class="line x" title="138:166	It is notsurprisingthatdeterminization mayoccasionally take a long time; even for a language of monadic trees (i.e. strings) the determinization algorithm is NP-complete, as implied by (Casacuberta and de la Higuera, 2000) and, e.g.(Dijkstra, 1959)." ></td>
	<td class="line x" title="140:166	5.2 Data-Oriented Parsing Weighted determinization of tree automata is also useful for parsing." ></td>
	<td class="line x" title="141:166	Data-Oriented Parsing (DOP)s methodology is to calculate weighted derivations, but asnoted in(Bod,2003), itis thehighest ranking parse,notderivation,thatisdesired." ></td>
	<td class="line x" title="142:166	Since(Simaan, 1996) showed that finding the highest ranking parse is an NP-complete problem, it has been common to estimate the highest ranking parse bythe previously method Recall Precision F-measure undeterminized 80.23 80.18 80.20 top-500 crunching 80.48 80.29 80.39 determinized 81.09 79.72 80.40 Figure 8: Recall, precision, and F-measure results onDOP-styleparsingofsection23ofthePennTreebank." ></td>
	<td class="line x" title="143:166	The use of best derivation (undeterminized), estimateofbesttree(top-500),andtruebesttree(determinized) for selection of parse output isshown." ></td>
	<td class="line x" title="144:166	described crunching method." ></td>
	<td class="line x" title="145:166	We create a DOP-like parsing model 4 by extracting and weighting a subset of subtrees from sections 2-21 of the Penn Treebank and use a DOPstyle parser to generate packed forest representations of parses of the 2416 sentences of section 23." ></td>
	<td class="line x" title="146:166	The forests contain a median of parse trees." ></td>
	<td class="line x" title="147:166	We then remove cycles and apply weighted determinization to the forests." ></td>
	<td class="line x" title="148:166	The number of trees in each determinized parse forest is reduced by a factor of between 2.1 and . On average, the number of trees is reduced by a factor of 900,000,demonstratingamuchlargernumberofduplicate parses prior to determinization than in the machine translation experiment." ></td>
	<td class="line x" title="149:166	The top-scoring parse after determinization isdifferent fromthe topscoring parse before determinization for 49.1% of the forests, and when the determinization method is approximated by crunching the top-500 parses from the undeterminized list only 55.9% of the topscoring parses are the same, indicating the crunching method is not a very good approximation of determinization." ></td>
	<td class="line x" title="150:166	We use the standard F-measure combination of recall and precision to score the top-scoring parse in each method against reference parses." ></td>
	<td class="line x" title="151:166	The results are tabulated in Figure 8." ></td>
	<td class="line x" title="152:166	Note that in 16.9% of cases determinization did not terminate." ></td>
	<td class="line x" title="153:166	For those sentences we used the best parse fromtop-500 estimation instead." ></td>
	<td class="line x" title="154:166	6 Conclusion We have shown that weighted determinization is useful for recovering -best unique trees from a weighted forest." ></td>
	<td class="line x" title="155:166	As summarized in Figure 9, the 4 This parser acquires a small subset of subtrees, in contrast with DOP, and the beam search for this problem has not been optimized." ></td>
	<td class="line x" title="156:166	357 31.87: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(aroused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.)) 32.11: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(caused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.)) 32.15: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VB(arouse) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.)) 32.55: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VB(cause) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.)) 32.60: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(attracted) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.)) 33.16: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VB(provoke) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.)) 33.27: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBG(causing) NP-C(NPB(DT(the) JJ(american) NNS(protests)))) .(.)) 33.29: S(NP-C(NPB(DT(this) NN(case))) VP(VBD(had) VP-C(VBN(aroused) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.)) 33.31: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(aroused) NP-C(NPB(DT(the) NN(protest)) PP(IN(of) NP-C(NPB(DT(the) NNS(united states))))))) .(.)) 33.33: S(NP-C(NPB(DT(this) NNS(cases))) VP(VBD(had) VP-C(VBN(incurred) NP-C(NPB(DT(the) JJ(american) NNS(protests))))) .(.)) Figure 7: Ranked list ofmachine translation results withno repeated trees." ></td>
	<td class="line x" title="157:166	experiment undeterminized determinized machine translation parsing Figure 9: Median trees per sentence forest in machinetranslationandparsingexperimentsbeforeand after determinization is applied to the forests, removing duplicate trees." ></td>
	<td class="line x" title="158:166	number of repeated trees prior to determinization wastypically verylarge, andthus determinization is critical to recovering true tree weight." ></td>
	<td class="line x" title="159:166	We have improved evaluation scores by incorporating the presented algorithm into our MT work and we believe that other NLP researchers working with trees can similarly benefit from this algorithm." ></td>
	<td class="line x" title="160:166	Further advances in determinization will provide additional benefit to the community." ></td>
	<td class="line x" title="161:166	The translation system detailed here is a string-to-tree system, andthedeterminization algorithm returns the -best unique trees fromapacked forest." ></td>
	<td class="line x" title="162:166	UsersofMTsystems are generally interested in the string yield of those trees, and not the trees per se." ></td>
	<td class="line x" title="163:166	Thus, an algorithm that can return the -best unique strings from apacked forest would be auseful extension." ></td>
	<td class="line x" title="164:166	We plan for our weighted determinization algorithm to be one component in a generally available treeautomatapackageforintersection, composition, training, recognition, and generation of weighted and unweighted tree automata for research tasks such as the ones described above." ></td>
	<td class="line x" title="165:166	Acknowledgments We thank Liang Huang for fruitful discussions which aided in this work and David Chiang, Daniel Marcu,andSteveDeNeefeforreadinganearlydraft andprovidingusefulcomments." ></td>
	<td class="line x" title="166:166	Thisworkwassupported by NSFgrant IIS-0428020." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-1096
An End-To-End Discriminative Approach To Machine Translation
Liang, Percy;Bouchard-Côté, Alexandre;Klein, Dan;Taskar, Ben;"></td>
	<td class="line x" title="1:222	Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 761768, Sydney, July 2006." ></td>
	<td class="line x" title="2:222	c2006 Association for Computational Linguistics An End-to-End Discriminative Approach to Machine Translation Percy Liang Alexandre Bouchard-Cote Dan Klein Ben Taskar Computer Science Division, EECS Department University of California at Berkeley Berkeley, CA 94720 {pliang, bouchard, klein, taskar}@cs.berkeley.edu Abstract We present a perceptron-style discriminative approach to machine translation in which large feature sets can be exploited." ></td>
	<td class="line x" title="3:222	Unlike discriminative reranking approaches, our system can take advantage of learned features in all stages of decoding." ></td>
	<td class="line x" title="4:222	We first discuss several challenges to error-driven discriminative approaches." ></td>
	<td class="line x" title="5:222	In particular, we explore different ways of updating parameters given a training example." ></td>
	<td class="line x" title="6:222	We find that making frequent but smaller updates is preferable to making fewer but larger updates." ></td>
	<td class="line x" title="7:222	Then, we discuss an array of features and show both how they quantitatively increase BLEU score and how they qualitatively interact on specific examples." ></td>
	<td class="line x" title="8:222	One particular feature we investigate is a novel way to introduce learning into the initial phrase extraction process, which has previously been entirely heuristic." ></td>
	<td class="line x" title="9:222	1 Introduction The generative, noisy-channel paradigm has historically served as the foundation for most of the work in statistical machine translation (Brown et al. , 1994)." ></td>
	<td class="line x" title="10:222	At the same time, discriminative methods have provided substantial improvements over generative models on a wide range of NLP tasks." ></td>
	<td class="line x" title="11:222	They allow one to easily encode domain knowledge in the form of features." ></td>
	<td class="line x" title="12:222	Moreover, parameters are tuned to directly minimize error rather than to maximize joint likelihood, which may not correspond well to the task objective." ></td>
	<td class="line x" title="13:222	In this paper, we present an end-to-end discriminative approach to machine translation." ></td>
	<td class="line x" title="14:222	The proposed system is phrase-based, as in Koehn et al.(2003), but uses an online perceptron training scheme to learn model parameters." ></td>
	<td class="line x" title="16:222	Unlike minimum error rate training (Och, 2003), our system is able to exploit large numbers of specific features in the same manner as static reranking systems (Shen et al. , 2004; Och et al. , 2004)." ></td>
	<td class="line x" title="17:222	However, unlike static rerankers, our system does not rely on a baseline translation system." ></td>
	<td class="line x" title="18:222	Instead, it updates based on its own n-best lists." ></td>
	<td class="line x" title="19:222	As parameter estimates improve, the system produces better nbest lists, which can in turn enable better updates in future training iterations." ></td>
	<td class="line x" title="20:222	In this paper, we focus on two aspects of the problem of discriminative translation: the inherent difficulty of learning from reference translations, and the challenge of engineering effective features for this task." ></td>
	<td class="line x" title="21:222	Discriminative learning from reference translations is inherently problematic because standard discriminative methods need to know which outputs are correct and which are not." ></td>
	<td class="line x" title="22:222	However, a proposed translation that differs from a reference translation need not be incorrect." ></td>
	<td class="line x" title="23:222	It may differ in word choice, literalness, or style, yet be fully acceptable." ></td>
	<td class="line x" title="24:222	Pushing our system to avoid such alternate translations is undesirable." ></td>
	<td class="line x" title="25:222	On the other hand, even if a system produces a reference translation, it may do so by abusing the hidden structure (sentence segmentation and alignment)." ></td>
	<td class="line x" title="26:222	We can therefore never be entirely sure whether or not a proposed output is safe to update towards." ></td>
	<td class="line x" title="27:222	We discuss this issue in detail in Section 5, where we show that conservative updates (which push the system towards a local variant of the current prediction) are more effective than more aggressive updates (which try to directly update towards the reference)." ></td>
	<td class="line x" title="28:222	The second major contribution of this work is an investigation of an array of features for our model." ></td>
	<td class="line x" title="29:222	We show how our features quantitatively increase BLEU score, as well as how they qualitatively interact on specific examples." ></td>
	<td class="line x" title="30:222	We first consider learning weights for individual phrases and part-of-speech patterns, showing gains from each." ></td>
	<td class="line x" title="31:222	We then present a novel way to parameterize and introduce learning into the initial phrase extraction process." ></td>
	<td class="line x" title="32:222	In particular, we introduce alignment constellation features, which allow us to weight phrases based on the word alignment pattern that led to their extraction." ></td>
	<td class="line x" title="33:222	This kind of 761 feature provides a potential way to initially extract phrases more aggressively and then later downweight undesirable patterns, essentially learning a weighted extraction heuristic." ></td>
	<td class="line x" title="34:222	Finally, we use POS features to parameterize a distortion model in a limited distortion decoder (Zens and Ney, 2004; Tillmann and Zhang, 2005)." ></td>
	<td class="line x" title="35:222	We show that overall, BLEU score increases from 28.4 to 29.6 on French-English." ></td>
	<td class="line x" title="36:222	2 Approach 2.1 Translation as structured classification Machine translation can be seen as a structured classification task, in which the goal is to learn a mapping from an input (French) sentence x to an output (English) sentence y. Given this setup, discriminative methods allow us to define a broad class of features  that operate on (x,y)." ></td>
	<td class="line x" title="37:222	For example, some features would measure the fluency of y and others would measure the faithfulness of y as a translation of x. However, the translation task in this framework differs from traditional applications of discriminative structured classification such as POS tagging and parsing in a fundamental way." ></td>
	<td class="line x" title="38:222	Whereas in POS tagging, there is a one-to-one correspondence between the words xand the tags y, the correspondence between x and y in machine translation is not only much more complex, but is in fact unknown." ></td>
	<td class="line x" title="39:222	Therefore, we introduce a hidden correspondence structure h and work with the feature vector (x,y,h)." ></td>
	<td class="line x" title="40:222	The phrase-based model of Koehn et al.(2003) is an instance of this framework." ></td>
	<td class="line x" title="42:222	In their model, the correspondence h consists of (1) the segmentation of the input sentence into phrases, (2) the segmentation of the output sentence into the same number of phrases, and (3) a bijection between the input and output phrases." ></td>
	<td class="line x" title="43:222	The feature vector (x,y,h) contains four components: the log probability of the output sentence y under a language model, the score of translating x into y based on a phrase table, a distortion score, and a length penalty.1 In Section 6, we vastly increase the number of features to take advantage of the full power of discriminative training." ></td>
	<td class="line x" title="44:222	Another example of this framework is the hierarchical model of Chiang (2005)." ></td>
	<td class="line x" title="45:222	In this model the correspondence h is a synchronous parse tree 1More components can be added to the feature vector if additional language models or phrase tables are available." ></td>
	<td class="line x" title="46:222	over input and output sentences, and features include the scores of various productions used in the tree." ></td>
	<td class="line x" title="47:222	Given features  and a corresponding set of parameters w, a standard classification rule f is to return the highest scoring output sentence y, maximizing over correspondences h: f(x;w) = argmax y,h w(x,y,h)." ></td>
	<td class="line x" title="48:222	(1) In the phrase-based model, computing the argmax exactly is intractable, so we approximate f with beam decoding." ></td>
	<td class="line pc" title="49:222	2.2 Perceptron-based training To tune the parameters w of the model, we use the averaged perceptron algorithm (Collins, 2002) because of its efficiency and past success on various NLP tasks (Collins and Roark, 2004; Roark et al. , 2004)." ></td>
	<td class="line x" title="50:222	In principle, w could have been tuned by maximizing conditional probability or maximizing margin." ></td>
	<td class="line x" title="51:222	However, these two options require either marginalization or numerical optimization, neither of which is tractable over the space of output sentences y and correspondences h. In contrast, the perceptron algorithm requires only a decoder that computes f(x;w)." ></td>
	<td class="line x" title="52:222	Recall the traditional perceptron update rule on an example (xi,yi) is ww+ (xi,yt)(xi,yp), (2) where yt = yi is the target output and yp = f(xi;w) = argmaxyw(xi,y) is the prediction using the current parameters w. We adapt this update rule to work with hidden variables as follows: ww+ (xi,yt,ht)(xi,yp,hp), (3) where (yp,hp) is the argmax computation in Equation 1, and (yt,ht) is the target that we update towards." ></td>
	<td class="line x" title="53:222	If (yt,ht) is the same argmax computation with the additional constraint that yt = yi, then Equation 3 can be interpreted as a Viterbi approximation to the stochastic gradient EP(h|xi,yi;w)(xi,yi,h)EP(y,h|xi;w)(xi,y,h) for the following conditional likelihood objective: P(yi |xi) summationdisplay h exp(w(xi,yi,h))." ></td>
	<td class="line x" title="54:222	762 a0 a1 a2 a3 a1 a4 a5 a6 a7 a8 a9 a3 a10 a5 a11 a7 a12 a1 a13 a6 a14 a5 a13 a7 a15 a1 a14 a10 a0 a1 a2 a16 a17 a18 a9 a10 a3 a19 a6 a4 a10 a3 a10 a20 a18 a9 a21 a3 a4 a13 a3 a0 a1 a2 a3 a1 a4 a3 a19 a3 a9 a21 a3 a4 a13 a22 a9 a3 a23 a18 a3 a17 a2 a10 a3 a19 a6 a4 a10 a3 a6 a4 a18 a9 a21 a3 a4 a2 a9 a3 a23 a18 a3 a17 a2 a10 a3 a19 a6 a4 a10 a3 a10 a20 a6 a9 a3 a23 a18 a3 a17 a2 a24 a1 a9 a18 a9 a21 a3 a4 a2 a25 a9 a1 a13 a3 a10 a18 a9 a3 a18 a9 a21 a3 a4 a13 a3 a26 a4 a25 a18 a2 a27 a0 a1 a2 a16 a17 a18 a9 a10 a3 a19 a6 a4 a10 a3 a10 a20 a18 a9 a21 a3 a4 a13 a3 a28 a3 a24 a3 a9 a3 a4 a13 a3 a27 a0 a1 a2 a3 a1 a4 a6 a9 a3 a23 a18 a3 a17 a2 a24 a1 a9 a18 a9 a21 a3 a4 a2 a25 a9 a1 a13 a3 a10 a18 a9 a3 a10 a20 a18 a9 a21 a3 a4 a13 a3a0 a1 a2 a16 a17 a18 a9 a0 a1 a2 a3 a1 a4 a0 a1 a2 a16 a17 a18 a9 Figure 1: Given the current prediction (a), there are two possible updates, local (b) and bold (c)." ></td>
	<td class="line x" title="55:222	Although the bold update (c) reaches the reference translation, a bad correspondence is used." ></td>
	<td class="line x" title="56:222	The local update (b) does not reach the reference, but is more reasonable than (c)." ></td>
	<td class="line x" title="57:222	Discriminative training with hidden variables has been handled in this probabilistic framework (Quattoni et al. , 2004; Koo and Collins, 2005), but we choose Equation 3 for efficiency." ></td>
	<td class="line x" title="58:222	It turns out that using the Viterbi approximation (which we call bold updating) is not always the best strategy." ></td>
	<td class="line x" title="59:222	To appreciate the difficulty, consider the example in Figure 1." ></td>
	<td class="line x" title="60:222	Suppose we make the prediction (a) with the current set of parameters." ></td>
	<td class="line x" title="61:222	There are often several acceptable output translations y, for example, (b) and (c)." ></td>
	<td class="line x" title="62:222	Since (c)s output matches the reference translation, should we update towards (c)?" ></td>
	<td class="line x" title="63:222	In this case, the answer is negative." ></td>
	<td class="line x" title="64:222	The problem with (c) is that the correspondence h contains an incorrect alignment (, a)." ></td>
	<td class="line x" title="65:222	However, since h is unobserved, the training procedure has no way of knowing this." ></td>
	<td class="line x" title="66:222	While the output in (b) is farther from the reference, its correspondence h is much more reasonable." ></td>
	<td class="line x" title="67:222	In short, it does not suffice for yt to be good; both yt and ht need to be good." ></td>
	<td class="line x" title="68:222	A major challenge in using the perceptron algorithm for machine translation is determining the target (yt,ht) in Equation 3." ></td>
	<td class="line x" title="69:222	Section 5 discusses possible targets to update towards." ></td>
	<td class="line x" title="70:222	3 Dataset Our experiments were done on the French-English portion of the Europarl corpus (Koehn, 2002), Dataset TRAIN DEV TEST Years 9901 02 03 # sentences 67K first 1K first 1K # words (unk)." ></td>
	<td class="line x" title="71:222	715K 10.4K (35) 10.8K (48) Table 1: The Europarl dataset split we used and various statistics on length 515 sentences." ></td>
	<td class="line x" title="72:222	The number of French word tokens is given, along with the number that were not seen among the 414K total sentences in TRAIN (which includes all lengths)." ></td>
	<td class="line x" title="73:222	which consists of European parliamentary proceedings from 1996 to 2003." ></td>
	<td class="line x" title="74:222	We split the data into three sets according to Table 1." ></td>
	<td class="line x" title="75:222	TRAIN served two purposes: it was used to construct the features, and the 515 length sentences were used for tuning the parameters of those features." ></td>
	<td class="line x" title="76:222	DEV, which consisted of the first 1K length 515 sentences in 2002, was used to evaluate the performance of the system as we developed it." ></td>
	<td class="line x" title="77:222	Note that the DEV set was not used to tune any parameters; tuning was done exclusively on TRAIN." ></td>
	<td class="line x" title="78:222	At the end we ran our models once on TEST to get final numbers.2 4 Models Our experiments used phrase-based models (Koehn et al. , 2003), which require a translation table and language model for decoding and feature computation." ></td>
	<td class="line x" title="79:222	To facilitate comparison with previous work, we created the translation tables using the same techniques as Koehn et al.(2003).3 The language model was a Kneser-Ney interpolated trigram model generated using the SRILM toolkit (Stolcke, 2002)." ></td>
	<td class="line x" title="81:222	We built our own phrase-based beam decoder that can handle arbitrary features.4 The contributions of features are incrementally added into the score as decoding 2We also experimented with several combinations of jackknifing to prevent overfitting, in which we selected features on TRAIN-OLD (19961998 Europarl corpus) and tuned the parameters on TRAIN, or vice-versa." ></td>
	<td class="line x" title="82:222	However, it turned out that using TRAIN-OLD was suboptimal since that data is less relevant to DEV." ></td>
	<td class="line x" title="83:222	Another alternative is to combine TRAINOLD and TRAIN into one dual-purpose dataset." ></td>
	<td class="line x" title="84:222	The differences between this and our current approach were inconclusive." ></td>
	<td class="line x" title="85:222	3In other words, we used GIZA++ to construct a word alignment in each direction and a growth heuristic to combine them." ></td>
	<td class="line x" title="86:222	We extracted all the substrings that are closed under this high-quality word alignment and computed surface statistics from cooccurrences counts." ></td>
	<td class="line x" title="87:222	4In our experiments, we used a beam size of 10, which we found to be only slightly worse than using a beam of 100." ></td>
	<td class="line x" title="88:222	763 proceeds." ></td>
	<td class="line x" title="89:222	We experimented with two levels of distortion: monotonic, where the phrasal alignment is monotonic (but word reordering is still possible within a phrase) and limited distortion, where only adjacent phrases are allowed to exchange positions (Zens and Ney, 2004)." ></td>
	<td class="line x" title="90:222	In the future, we plan to explore our discriminative framework on a full distortion model (Koehn et al. , 2003) or even a hierarchical model (Chiang, 2005)." ></td>
	<td class="line x" title="91:222	Throughout the following experiments, we trained the perceptron algorithm for 10 iterations." ></td>
	<td class="line x" title="92:222	The weights were initialized to 1 on the translation table, 1 on the language model (the blanket features in Section 6), and 0 elsewhere." ></td>
	<td class="line x" title="93:222	The next two sections give experiments on the two key components of a discriminative machine translation system: choosing the proper update strategy (Section 5) and including powerful features (Section 6)." ></td>
	<td class="line x" title="94:222	5 Update strategies This section describes the importance of choosing a good update strategythe difference in BLEU score can be as large as 1.2 between different strategies." ></td>
	<td class="line x" title="95:222	An update strategy specifies the target (yt,ht) that we update towards (Equation 3) given the current set of parameters and a provided reference translation (xi,yi)." ></td>
	<td class="line x" title="96:222	As mentioned in Section 2.2, faithful output (i.e. yt = yi) does not imply that updating towards (yt,ht) is desirable." ></td>
	<td class="line x" title="97:222	In fact, such a constrained target might not even be reachable by the decoder, for example, if the reference is very non-literal." ></td>
	<td class="line x" title="98:222	We explored the following three ways to choose the target (yt,ht):  Bold updating: Update towards the highest scoring option (y,h), where y is constrained to be the reference yi but h is unconstrained." ></td>
	<td class="line x" title="99:222	Examples not reachable by the decoder are skipped." ></td>
	<td class="line x" title="100:222	 Local updating: Generate an n-best list using the current parameters." ></td>
	<td class="line x" title="101:222	Update towards the option with the highest BLEU score.5 5Since BLEU score (k-BLEU with k = 4) involves computing a geometric mean over i-grams, i = 1,, k, it is zero if the translation does not have at least one k-gram in common with the reference translation." ></td>
	<td class="line x" title="102:222	Since a BLEU score of zero is both unhelpful for choosing from the n-best and common when computed on just a single example, we instead used a smoothed version for choosing the target: P4i=1 i-BLEU(x,y)24i+1." ></td>
	<td class="line x" title="103:222	We still report NISTs usual 4-gram BLEU." ></td>
	<td class="line x" title="104:222	a0 a1 a2 a3 a4 a5 a6 a7 a4 a8 a9 a10 a8 a3 a11 a4 a12 a13 a13 a14 a6 a7 a8 a1 a5 a11 a8 a6 a15 a13 a1 a5 a16 a17 a18 a19 a16 a20 a21 a22 a23 a24 a25 a26 a27 a22 a28 a29 a0 a1 a2 a3 a4 a5 a6 a7 a4 a8 a9 a10 a8 a3 a11 a4 a12 a13 a13 a14 a6 a7 a8 a1 a5 a11 a8 a6 a15 a13 a1 a5 a16 a17 a18 a19 a16 a24 a25 a26 a27 a22 a28 a29 a7 a4 a30 a4 a7 a4 a1 a9 a4 a7 a4 a30 a4 a7 a4 a1 a9 a4 a20 a21 a22 a23 a31 a29 a17 a32 a17 a33 a25 a28 a32 a34 a35 a8 a36 a37 a38 a14 a8 a6 a4 a5 a39 a10 a4 a1 a6 a10 a4 a7 a4 a30 a4 a7 a4 a1 a9 a4 a15 a5 a7 a4 a8 a9 a10 a8 a3 a11 a4 a35 a3 a36 a37 a38 a14 a8 a6 a4 a5 a39 a10 a4 a1 a6 a10 a4 a7 a4 a30 a4 a7 a4 a1 a9 a4 a15 a5 a40 a1 a7 a4 a8 a9 a10 a8 a3 a11 a4 Figure 2: The three update strategies under two scenarios." ></td>
	<td class="line x" title="105:222	 Hybrid updating: Do a bold update if the reference is reachable." ></td>
	<td class="line x" title="106:222	Otherwise, do a local update." ></td>
	<td class="line x" title="107:222	Figure 2 shows the space of translations schematically." ></td>
	<td class="line x" title="108:222	On each training example, our decoder produces an n-best list." ></td>
	<td class="line x" title="109:222	The reference translation may or may not be reachable." ></td>
	<td class="line x" title="110:222	Bold updating most resembles the traditional perceptron update rule (Equation 2)." ></td>
	<td class="line x" title="111:222	We are ensured that the target output y will be correct, although the correspondence h might be bad." ></td>
	<td class="line x" title="112:222	Another weakness of bold updating is that we might not make full use of the training data." ></td>
	<td class="line x" title="113:222	Local updating uses every example, but its steps are more cautious." ></td>
	<td class="line x" title="114:222	It can be viewed as dynamic reranking, where parameters are updated using the best option on the n-best list, similar to standard static reranking." ></td>
	<td class="line x" title="115:222	The key difference is that, unlike static reranking, the parameter updates propagate back to the baseline classifier, so that the n-best list improves over time." ></td>
	<td class="line x" title="116:222	In this regard, dynamic reranking remedies one of the main weaknesses of static reranking, which is that the performance of the system is directly limited by the quality of the baseline classifier." ></td>
	<td class="line x" title="117:222	Hybrid updating combines the two strategies: it makes full use of the training data as in local updating, but still tries to make swift progress towards the reference translation as in bold updating." ></td>
	<td class="line x" title="118:222	We conducted experiments to see which of the updating strategies worked best." ></td>
	<td class="line x" title="119:222	We trained on 764 Decoder Bold Local Hybrid Monotonic 34.3 34.6 34.5 Limited distortion 33.5 34.7 33.6 Table 2: Comparison of BLEU scores between different updating strategies for the monotonic and limited distortion decoders on DEV." ></td>
	<td class="line x" title="120:222	5000 of the 67K available examples, using the BLANKET+LEX+POS feature set (Section 6)." ></td>
	<td class="line x" title="121:222	Table 2 shows that local updating is the most effective, especially when using the limited distortion decoder." ></td>
	<td class="line x" title="122:222	In bold updating, only a small fraction of the 5000 examples (1296 for the monotonic decoder and 1601 for the limited distortion decoder) had reachable reference translations, and, therefore, contributed to parameter updates." ></td>
	<td class="line x" title="123:222	One might therefore hypothesize that local updating performs better simply because it is able to leverage more data." ></td>
	<td class="line x" title="124:222	This is not the full story, however, since the hybrid approach (which makes the same number of updates) performs significantly worse than local updating when using the limited distortion decoder." ></td>
	<td class="line x" title="125:222	To see the problem with bold updating, recall the example in Figure 1." ></td>
	<td class="line x" title="126:222	Bold updating tries to reach the reference at all costs, even if it means abusing the hidden correspondence in the process." ></td>
	<td class="line x" title="127:222	In the example, the alignment (, a) is unreasonable, but the algorithm has no way to recognize this." ></td>
	<td class="line x" title="128:222	Local updating is much more stable since it only updates towards sentences in the n-best list." ></td>
	<td class="line x" title="129:222	When using the limited distortion decoder, bold updating is even more problematic because the added flexibility of phrase swaps allows more preposterous alignments to be produced." ></td>
	<td class="line x" title="130:222	Limited distortion decoding actually performs worse than monotonic decoding with bold updating, but better with local updating." ></td>
	<td class="line x" title="131:222	Another difference between bold updating and local updating is that the BLEU score on the training data is dramatically higher for bold updating than for local (or hybrid) updating: 80 for the former versus 40 for the latter." ></td>
	<td class="line x" title="132:222	This is not surprising given that bold updating aggressively tries to obtain the references." ></td>
	<td class="line x" title="133:222	However, what is surprising is that although bold updating appears to be overfitting severely, its BLEU score on the DEV does not suffer much in the monotonic case." ></td>
	<td class="line x" title="134:222	Model DEV BLEU TEST BLEU Monotonic BLANKET (untuned) 33.0 28.3 BLANKET 33.4 28.4 BLANKET+LEX 35.0 29.2 BLANKET+LEX+POS 35.3 29.6 Pharaoh (MERT) 34.5 28.8 Full-distortion Pharaoh (MERT) 34.9 29.5 Table 3: Main results on our system with different feature sets compared to minimum error-rate trained Pharaoh." ></td>
	<td class="line x" title="135:222	6 Features This section shows that by adding an array of expressive features and discriminatively learning their weights, we can obtain a 2.3 increase in BLEU score on DEV." ></td>
	<td class="line x" title="136:222	We add these features incrementally, first tuning blanket features (Section 6.1), then adding lexical features (Section 6.2), and finally adding part-of-speech (POS) features (Section 6.3)." ></td>
	<td class="line x" title="137:222	Table 3 summarizes the performance gains." ></td>
	<td class="line x" title="138:222	For the experiments in this section, we used the local updating strategy and the monotonic decoder for efficiency." ></td>
	<td class="line x" title="139:222	We train on all 67K of the length 5 15 sentences in TRAIN.6 6.1 Blanket features The blanket features (BLANKET) consist of the translation log-probability and the language model log-probability, which are two of the components of the Pharaoh model (Section 2.1)." ></td>
	<td class="line x" title="140:222	After discriminative training, the relative weight of these two features is roughly 2:1, resulting in a BLEU score increase from 33.0 (setting both weights to 1) to 33.4." ></td>
	<td class="line x" title="141:222	The following simple example gives a flavor of the discriminative approach." ></td>
	<td class="line x" title="142:222	The untuned system translated the French phrase trente-cinq langues into five languages in a DEV example." ></td>
	<td class="line x" title="143:222	Although the probability P(five | trente-cinq) = 0.065 is rightly much smaller than P(thirty-five | trente-cinq) = 0.279, the language model favors five languages over thirty-five languages." ></td>
	<td class="line x" title="144:222	The trained system downweights the language model and recovers the correct translation." ></td>
	<td class="line x" title="145:222	6We used sentences of length 515 to facilitate comparisons with Koehn et al.(2003) and to enable rapid experimentation with various feature sets." ></td>
	<td class="line x" title="147:222	Experiments on sentences of length 550 showed similar gains in performance." ></td>
	<td class="line x" title="148:222	765 6.2 Lexical features The blanket features provide a rough guide for translation, but they are far too coarse to fix specific mistakes." ></td>
	<td class="line x" title="149:222	We therefore add lexical features (LEX) to allow for more fine-grained control." ></td>
	<td class="line x" title="150:222	These features come in two varieties." ></td>
	<td class="line x" title="151:222	Lexical phrase features indicate the presence of a specific translation phrase, such as (y a-t-il, are there), and lexical language model features indicate the presence of a specific output n-gram, such as of the." ></td>
	<td class="line x" title="152:222	Lexical language model features have been exploited successfully in discriminative language modeling to improve speech recognition performance (Roark et al. , 2004)." ></td>
	<td class="line x" title="153:222	We confirm the utility of the two kinds of lexical features: BLANKET+LEX achieves a BLEU score of 35.0, an improvement of 1.6 over BLANKET." ></td>
	<td class="line x" title="154:222	To understand the effect of adding lexical features, consider the ten with highest and lowest weights after training: 64 any comments ? -55 (des, of) 63 (y a-t-il, are there) -52 (y a-t-il, are there any) 62 there any comments -42 there any of 57 any comments -39 of comments 46 (des, any) -38 of comments ? These features can in fact be traced back to the following example: Input y a-t-il des observations ? B are there any of comments ? B+L are there any comments ? The second and third rows are the outputs of BLANKET (wrong) and BLANKET+LEX (correct), respectively." ></td>
	<td class="line x" title="155:222	The correction can be accredited to two changes in feature weights." ></td>
	<td class="line x" title="156:222	First, the lexical feature (y a-t-il, are there any) has been assigned a negative weight and (y a-t-il, are there) a positive weight to counter the fact that the former phrase incorrectly had a higher score in the original translation table." ></td>
	<td class="line x" title="157:222	Second, (des, of) is preferred over (des, any), even though the former is a better translation in isolation." ></td>
	<td class="line x" title="158:222	This apparent degradation causes no problems, because when des should actually be translated to of, these words are usually embedded in larger phrases, in which case the isolated translation probability plays no role." ></td>
	<td class="line x" title="159:222	Another example of a related phenomenon is the following: Input  pour cela que j  ai vote favorablement . B  for that i have voted in favour . B+L  for this reason i voted in favour . Counterintuitively, the phrase pair (j  ai, I have) ends up with a very negative weight." ></td>
	<td class="line x" title="160:222	The reason behind this is that in French, j  ai is often used in a paraphrastic construction which should be translated into the simple past in English." ></td>
	<td class="line x" title="161:222	For that to happen, j  ai needs to be aligned with I. Since (j  ai, I) has a small score compare to (j  ai, I have) in the original translation table, downweighting the latter pair allows this sentence to be translated correctly." ></td>
	<td class="line x" title="162:222	A general trend is that literal phrase translations are downweighted." ></td>
	<td class="line x" title="163:222	Lessening the pressure to literally translate certain phrases allows the language model to fill in the gaps appropriately with suitable non-literal translations." ></td>
	<td class="line x" title="164:222	This point highlights the strength of discriminative training: weights are jointly tuned to account for the intricate interactions between overlapping phrases, which is something not achievable by estimating the weights directly from surface statistics." ></td>
	<td class="line x" title="165:222	6.3 Part-of-speech features While lexical features are useful for eliminating specific errors, they have limited ability to generalize to related phrases." ></td>
	<td class="line x" title="166:222	This suggests the use of similar features which are abstracted to the POS level.7 In our experiments, we used the TreeTagger POS tagger (Schmid, 1994), which ships pretrained on several languages, to map each word to its majority POS tag." ></td>
	<td class="line x" title="167:222	We could also relatively easily base our features on context-dependent POS tags: the entire input sentence is available before decoding begins, and the output sentence is decoded left-to-right and could be tagged incrementally." ></td>
	<td class="line x" title="168:222	Where we had lexical phrase features, such as (la realisation du droit, the right), we now also have their POS abstractions, for instance (DT NN IN NN, DT NN)." ></td>
	<td class="line x" title="169:222	This phrase pair is undesirable, not because of particular lexical facts about la realisation, but because dropping a nominal head is generally to be avoided." ></td>
	<td class="line x" title="170:222	The lexical language model features have similar POS counterparts." ></td>
	<td class="line x" title="171:222	With these two kinds of POS features, we obtained an 0.3 increase in BLEU score from BLANKET+LEX to BLANKET+LEX+POS." ></td>
	<td class="line x" title="172:222	Finally, when we use the limited distortion decoder, it is important to learn when to swap adjacent phrases." ></td>
	<td class="line x" title="173:222	Unlike Pharaoh, which simply has a uniform penalty for swaps, we would like to use contextin particular, POS information." ></td>
	<td class="line x" title="174:222	For example, we would like to know that if a (JJ, JJ) 7We also tried using word clusters (Brown et al. , 1992) instead of POS but found that POS was more helpful." ></td>
	<td class="line x" title="175:222	766 se cu re re fu ge abri sur ze ro gr ow th ra te croissance zero, th at sa me, ce meme (a) (b) (c) Figure 3: Three constellation features with example phrase pairs." ></td>
	<td class="line x" title="176:222	Constellations (a) and (b) have large positive weights and (c) has a large negative weight." ></td>
	<td class="line x" title="177:222	phrase is constructed after a (NN, NN) phrase, they are reasonable candidates for swapping because of regular word-order differences between French and English." ></td>
	<td class="line x" title="178:222	While the bulk of our results are presented for the monotonic case, the limited distortion results of Table 2 use these lexical swap features; without parameterized swap features, accuracy was below the untuned monotonic baseline." ></td>
	<td class="line x" title="179:222	An interesting statistic is the number of nonzero feature weights that were learned using each feature set." ></td>
	<td class="line x" title="180:222	BLANKET has only 4 features, while BLANKET+LEX has 1.55 million features.8 Remarkably, BLANKET+LEX+POS has fewer featuresonly 1.24 million." ></td>
	<td class="line x" title="181:222	This is an effect of generalization abilityPOS information somewhat reduces the need for specific lexical features." ></td>
	<td class="line x" title="182:222	6.4 Alignment constellation features Koehn et al.(2003) demonstrated that choosing the appropriate heuristic for extracting phrases is very important." ></td>
	<td class="line x" title="184:222	They showed that the difference in BLEU score between various heuristics was as large as 2.0." ></td>
	<td class="line x" title="185:222	The process of phrase extraction is difficult to optimize in a non-discriminative setting: many heuristics have been proposed (Koehn et al. , 2003), but it is not obvious which one should be chosen for a given language pair." ></td>
	<td class="line x" title="186:222	We propose a natural way to handle this part of the translation pipeline." ></td>
	<td class="line x" title="187:222	The idea is to push the learning process all the way down to the phrase extraction by parameterizing the phrase extraction heuristic itself." ></td>
	<td class="line x" title="188:222	The heuristics in Koehn et al.(2003) decide whether to extract a given phrase pair based on the underlying word alignments (see Figure 3 for three examples), which we call constellations." ></td>
	<td class="line x" title="190:222	Since we do not know which constellations correspond to 8Both the language model and translation table components have two features, one for known words and one for unknown words." ></td>
	<td class="line x" title="191:222	Features -CONST +CONST BLANKET 31.8 32.2 BLANKET+LEX 32.2 32.5 BLANKET+LEX+POS 32.3 32.5 Table 4: DEV BLEU score increase resulting from adding constellation features." ></td>
	<td class="line x" title="192:222	good phrase pairs, we introduce an alignment constellation feature to indicate the presence of a particular alignment constellation.9 Table 4 details the effect of adding constellation features on top of our previous feature sets.10 We get a minor increase in BLEU score from each feature set, although there is no gain by adding POS features in addition to constellation features, probably because POS and constellation features provide redundant information for French-English translations." ></td>
	<td class="line x" title="193:222	It is interesting to look at the constellations with highest and lowest weights, which are perhaps surprising at first glance." ></td>
	<td class="line x" title="194:222	At the top of the list are word inversions (Figure 3 (a) and (b)), while long monotonic constellations fall at the bottom of the list (c)." ></td>
	<td class="line x" title="195:222	Although monotonic translations are much more frequent than word inversions in our dataset, when translations are monotonic, shorter segmentations are preferred." ></td>
	<td class="line x" title="196:222	This phenomenon is another manifestation of the complex interaction of phrase segmentations." ></td>
	<td class="line x" title="197:222	7 Final results The last column of Table 3 shows the performance of our methods on the final TEST set." ></td>
	<td class="line x" title="198:222	Our best test BLEU score is 29.6 using BLANKET+LEX+POS, an increase of 1.3 BLEU over our untuned feature set BLANKET." ></td>
	<td class="line x" title="199:222	The discrepancy between DEV performance and TEST performance is due to temporal distance from TRAIN and high variance in BLEU score.11 We also compared our model with Pharaoh (Koehn et al. , 2003)." ></td>
	<td class="line x" title="200:222	We tuned Pharaohs four parameters using minimum error rate training (Och, 2003) on DEV.12 We obtained an increase of 0.8 9As in the POS features, we map each phrase pair to its majority constellation." ></td>
	<td class="line x" title="201:222	10Due to time constraints, we ran these experiments on 5000 training examples using bold updating." ></td>
	<td class="line x" title="202:222	11For example, the DEV BLEU score for BLANKET+LEX ranges from 28.6 to 33.2, depending on which block of 1000 sentences we chose." ></td>
	<td class="line x" title="203:222	12We used the training scripts from the 2006 MT Shared Task." ></td>
	<td class="line x" title="204:222	We still tuned our model parameters on TRAIN and 767 BLEU over the Pharaoh, run with the monotone flag.13 Even though we are using a monotonic decoder, our best results are still slightly better than the version of Pharaoh that permits arbitrary distortion." ></td>
	<td class="line x" title="205:222	8 Related work In machine translation, most discriminative approaches currently fall into two general categories." ></td>
	<td class="line x" title="206:222	The first approach is to reuse the components of a generative model, but tune their relative weights in a discriminative fashion (Och and Ney, 2002; Och, 2003; Chiang, 2005)." ></td>
	<td class="line x" title="207:222	This approach only works in practice with a small handful of parameters." ></td>
	<td class="line x" title="208:222	The second approach is to use reranking, in which a baseline classifier generates an n-best list of candidate translations, and a separate discriminative classifier chooses amongst them (Shen et al. , 2004; Och et al. , 2004)." ></td>
	<td class="line x" title="209:222	The major limitation of a reranking system is its dependence on the underlying baseline system, which bounds the potential improvement from discriminative training." ></td>
	<td class="line x" title="210:222	In machine translation, this limitation is a real concern; it is common for all translations on moderately-sized n-best lists to be of poor quality." ></td>
	<td class="line x" title="211:222	For instance, Och et al.(2004) reported that a 1000-best list was required to achieve performance gains from reranking." ></td>
	<td class="line x" title="213:222	In contrast, the decoder in our system can use the feature weights learned in the previous iteration." ></td>
	<td class="line x" title="214:222	Tillmann and Zhang (2005) present a discriminative approach based on local models." ></td>
	<td class="line x" title="215:222	Their formulation explicitly decomposed the score of a translation into a sequence of local decisions, while our formulation allows global estimation." ></td>
	<td class="line x" title="216:222	9 Conclusion We have presented a novel end-to-end discriminative system for machine translation." ></td>
	<td class="line x" title="217:222	We studied update strategies, an important issue in online discriminative training for MT, and conclude that making many smaller (conservative) updates is better than making few large (aggressive) updates." ></td>
	<td class="line x" title="218:222	We also investigated the effect of adding many expressive features, which yielded a 0.8 increase in BLEU score over monotonic Pharaoh." ></td>
	<td class="line x" title="219:222	Acknowledgments We would like to thank our reviewers for their comments." ></td>
	<td class="line x" title="220:222	This work was suponly used DEV to optimize the number of training iterations." ></td>
	<td class="line x" title="221:222	13This result is significant with p-value 0.0585 based on approximate randomization (Riezler and Maxwell, 2005)." ></td>
	<td class="line x" title="222:222	ported by a FQRNT fellowship to second author and a Microsoft Research New Faculty Fellowship to the third author." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-1110
Advances In Discriminative Parsing
Turian, Joseph P.;Melamed, I. Dan;"></td>
	<td class="line x" title="1:239	Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 873880, Sydney, July 2006." ></td>
	<td class="line x" title="2:239	c2006 Association for Computational Linguistics Advances in Discriminative Parsing Joseph Turian and I. Dan Melamed {lastname}@cs.nyu.edu Computer Science Department New York University New York, New York 10003 Abstract The present work advances the accuracy and training speed of discriminative parsing." ></td>
	<td class="line x" title="3:239	Our discriminative parsing method has no generative component, yet surpasses a generative baseline on constituent parsing, and does so with minimal linguistic cleverness." ></td>
	<td class="line x" title="4:239	Our model can incorporate arbitrary features of the input and parse state, and performs feature selection incrementally over an exponential feature space during training." ></td>
	<td class="line x" title="5:239	We demonstrate the flexibility of our approach by testing it with several parsing strategies and various feature sets." ></td>
	<td class="line x" title="6:239	Our implementation is freely available at: http://nlp.cs.nyu.edu/parser/." ></td>
	<td class="line x" title="7:239	1 Introduction Discriminative machine learning methods have improved accuracy on many NLP tasks, including POS-tagging, shallow parsing, relation extraction, and machine translation." ></td>
	<td class="line x" title="8:239	Some advances have also been made on full syntactic constituent parsing." ></td>
	<td class="line pc" title="9:239	Successful discriminative parsers have relied on generative models to reduce training time and raise accuracy above generative baselines (Collins & Roark, 2004; Henderson, 2004; Taskar et al. , 2004)." ></td>
	<td class="line n" title="10:239	However, relying on information from a generative model might prevent these approaches from realizing the accuracy gains achieved by discriminative methods on other NLP tasks." ></td>
	<td class="line n" title="11:239	Another problem is training speed: Discriminative parsers are notoriously slow to train." ></td>
	<td class="line x" title="12:239	In the present work, we make progress towards overcoming these obstacles." ></td>
	<td class="line x" title="13:239	We propose a flexible, end-to-end discriminative method for training parsers, demonstrating techniques that might also be useful for other structured prediction problems." ></td>
	<td class="line x" title="14:239	The proposed method does model selection without ad-hoc smoothing or frequency-based feature cutoffs." ></td>
	<td class="line x" title="15:239	It requires no heuristics or human effort to optimize the single important hyper-parameter." ></td>
	<td class="line x" title="16:239	The training regime can use all available information from the entire parse history." ></td>
	<td class="line x" title="17:239	The learning algorithm projects the hand-provided features into a compound feature space and performs incremental feature selection over this large feature space." ></td>
	<td class="line x" title="18:239	The resulting parser achieves higher accuracy than a generative baseline, despite not using a generative model as a feature." ></td>
	<td class="line x" title="19:239	Section 2 describes the parsing algorithm." ></td>
	<td class="line x" title="20:239	Section 3 presents the learning method." ></td>
	<td class="line x" title="21:239	Section 4 presents experiments with discriminative parsers built using these methods." ></td>
	<td class="line x" title="22:239	Section 5 compares our approach to related work." ></td>
	<td class="line x" title="23:239	2 Parsing Algorithm The following terms will help to explain our work." ></td>
	<td class="line x" title="24:239	A span is a range over contiguous words in the input." ></td>
	<td class="line x" title="25:239	Spans cross if they overlap but neither contains the other." ></td>
	<td class="line x" title="26:239	An item is a (span, label) pair." ></td>
	<td class="line x" title="27:239	A state is a partial parse, i.e. a set of items, none of whose spans may cross." ></td>
	<td class="line x" title="28:239	A parse inference is a (state, item) pair, i.e. a state and an item to be added to it." ></td>
	<td class="line x" title="29:239	The frontier of a state consists of the items with no parents yet." ></td>
	<td class="line x" title="30:239	The children of a candidate inference are the frontier items below the item to be inferred, and the head of a candidate inference is the child item chosen by English head rules (Collins, 1999, pp." ></td>
	<td class="line x" title="31:239	238240)." ></td>
	<td class="line x" title="32:239	A parse path is a sequence of parse inferences." ></td>
	<td class="line x" title="33:239	For some input sentence and training parse tree, a state is correct if the parser can infer zero or more additional items to obtain the training parse tree, and an inference 873 is correct if it leads to a correct state." ></td>
	<td class="line x" title="34:239	Given input sentence s, the parser searches for parse p out of the possible parses P(s): p = arg min pP(s) C(p) (1) where C(p) is the cost of parse p under model : C(p) = summationdisplay ip c(i) (2) Section 3.1 describes how to compute c(i)." ></td>
	<td class="line x" title="35:239	Because c(i)R+, the cost of a partial parse monotonically increases as we add items to it." ></td>
	<td class="line x" title="36:239	The parsing algorithm considers a succession of states." ></td>
	<td class="line x" title="37:239	The initial state contains terminal items, whose labels are the POS tags given by the tagger of Ratnaparkhi (1996)." ></td>
	<td class="line x" title="38:239	Each time we pop a state from the agenda, c computes the costs for the candidate bottom-up inferences generated from that state." ></td>
	<td class="line x" title="39:239	Each candidate inference results in a successor state to be placed on the agenda." ></td>
	<td class="line x" title="40:239	The cost function c can consider arbitrary properties of the input and parse state." ></td>
	<td class="line x" title="41:239	We are not aware of any tractable solution to Equation 1, such as dynamic programming." ></td>
	<td class="line x" title="42:239	Therefore, the parser finds p using a variant of uniform-cost search." ></td>
	<td class="line x" title="43:239	The parser implements the search using an agenda that stores entire states instead of single items." ></td>
	<td class="line x" title="44:239	Each time a state is popped from the agenda, the parser uses depth-first search starting from the state that was popped until it (greedily) finds a complete parse." ></td>
	<td class="line x" title="45:239	In preliminary experiments, this search strategy was faster than standard uniformcost search (Russell & Norvig, 1995)." ></td>
	<td class="line x" title="46:239	3 Training Method 3.1 General Setting Our training set I consists of candidate inferences from the parse trees in the training data." ></td>
	<td class="line x" title="47:239	From each training inference i I we generate the tuple X(i), y(i), b(i)." ></td>
	<td class="line x" title="48:239	X(i) is a feature vector describing i, with each element in0, 1}." ></td>
	<td class="line x" title="49:239	We will use X f (i) to refer to the element of X(i) that pertains to feature f. y(i) = +1 if i is correct, and y(i) = 1 if not." ></td>
	<td class="line x" title="50:239	Some training examples might be more important than others, so each is given a bias b(i)  R+, as detailed in Section 3.3." ></td>
	<td class="line x" title="51:239	The goal during training is to induce a hypothesis h(i), which is a real-valued inference scoring function." ></td>
	<td class="line x" title="52:239	In the present work, h is a linear model parameterized by a real vector , which has one entry for each feature f : h(i) = X(i) = summationdisplay f f X f (i) (3) The sign of h(i) predicts the y-value of i and the magnitude gives the confidence in this prediction." ></td>
	<td class="line x" title="53:239	The training procedure optimizes  to minimize the expected risk R over training set I. R is the objective function, a combination of loss function L and regularization term : R(I) = L(I) + (4) The loss of the inference set decomposes into the loss of individual inferences: L(I) = summationdisplay iI l(i) (5) In principle, l can be any loss function, but in the present work we use the log-loss (Collins et al. , 2002): l(i) = b(i)ln(1 + exp((i))) (6) and (i) is the margin of inference i: (i) = y(i)h(i) (7) Inference cost c(i) in Equation 2 is l(i) computed using y(i) = +1 and b(i) = 1, i.e.: c(i) = ln(1 + exp(h(i))) (8)  in Equation 4 is a regularizer, which penalizes complex models to reduce overfitting and generalization error." ></td>
	<td class="line x" title="54:239	We use the lscript1 penalty:  = summationdisplay f |f| (9) where  is a parameter that controls the strength of the regularizer." ></td>
	<td class="line x" title="55:239	This choice of objective R is motivated by Ng (2004), who suggests that, given a learning setting where the number of irrelevant features is exponential in the number of training examples, we can nonetheless learn effectively by building decision trees to minimize the lscript1regularized log-loss." ></td>
	<td class="line x" title="56:239	On the other hand, Ng (2004) suggests that most of the learning algorithms commonly used by discriminative parsers will overfit when exponentially many irrelevant features are present.1 Learning over an exponential feature space is the very setting we have in mind." ></td>
	<td class="line x" title="57:239	A priori, we define only a set A of simple atomic features (given 1including the following learning algorithms: unregularized logistic regression logistic regression with an lscript2 penalty (i.e. a Gaussian prior) SVMs using most kernels multilayer neural nets trained by backpropagation the perceptron algorithm 874 in Section 4)." ></td>
	<td class="line x" title="58:239	The learner then induces compound features, each of which is a conjunction of possibly negated atomic features." ></td>
	<td class="line x" title="59:239	Each atomic feature can have one of three values (yes/no/dont care), so the size of the compound feature space is 3|A|, exponential in the number of atomic features." ></td>
	<td class="line x" title="60:239	It was also exponential in the number of training examples in our experiments (|A||I|)." ></td>
	<td class="line x" title="61:239	3.2 Boosting lscript1-Regularized Decision Trees We use an ensemble of confidence-rated decision trees (Schapire & Singer, 1999) to represent h.2 The path from the root to each node n in a decision tree corresponds to some compound feature f, and we write (n) = f. To score an inference i using a decision tree, we percolate the inferences features X(i) down to a leaf n and return confidence (n)." ></td>
	<td class="line x" title="62:239	An inference i percolates down to node n iff X(n) = 1." ></td>
	<td class="line x" title="63:239	Each leaf node n keeps track of the parameter value (n).3 The score h(i) given to an inference i by the whole ensemble is the sum of the confidences returned by the trees in the ensemble." ></td>
	<td class="line x" title="64:239	Listing 1 Outline of training algorithm." ></td>
	<td class="line x" title="65:239	1: procedure T(I) 2: ensemble 3:  4: while dev set accuracy is increasing do 5: ttree with one (root) node 6: while the root node cannot be split do 7: decay lscript1 parameter  8: while some leaf in t can be split do 9: split the leaf to maximize gain 10: percolate every iI to a leaf node 11: for each leaf n in t do 12: update (n) to minimize R 13: append t to ensemble Listing 1 presents our training algorithm." ></td>
	<td class="line x" title="66:239	At the beginning of training, the ensemble is empty,  = 0, and the lscript1 parameter  is set to(Steps 1.2 and 1.3)." ></td>
	<td class="line x" title="67:239	We train until the objective cannot be further reduced for the current choice of ." ></td>
	<td class="line x" title="68:239	We then determine the accuracy of the parser on a held-out development set using the previous  value (before it was decreased), and stop training when this 2Turian and Melamed (2005) reported that decision trees applied to parsing have higher accuracy and training speed than decision stumps, so we build full decision trees rather than stumps." ></td>
	<td class="line x" title="69:239	3Any given compound feature can appear in more than one tree, but each leaf node has a distinct confidence value." ></td>
	<td class="line x" title="70:239	For simplicity, we ignore this possibility in our discussion." ></td>
	<td class="line x" title="71:239	accuracy reaches a plateau (Step 1.4)." ></td>
	<td class="line x" title="72:239	Otherwise, we relax the regularization penalty by decreasing  (Steps 1.6 and 1.7) and continue training." ></td>
	<td class="line x" title="73:239	In this way, instead of choosing the best  heuristically, we can optimize it during a single training run (Turian & Melamed, 2005)." ></td>
	<td class="line x" title="74:239	Each training iteration (Steps 1.51.13) has several steps." ></td>
	<td class="line x" title="75:239	First, we choose some compound features that have high magnitude gradient with respect to the objective function." ></td>
	<td class="line x" title="76:239	We do this by building a new decision tree, whose leaves represent the chosen compound features (Steps 1.5 1.9)." ></td>
	<td class="line x" title="77:239	Second, we confidence-rate each leaf to minimize the objective over the examples that percolate down to that leaf (Steps 1.101.12)." ></td>
	<td class="line x" title="78:239	Finally, we append the decision tree to the ensemble and update parameter vector  accordingly (Step 1.13)." ></td>
	<td class="line x" title="79:239	In this manner, compound feature selection is performed incrementally during training, as opposed to a priori." ></td>
	<td class="line x" title="80:239	Our strategy minimizing the objective R(I) (Equation 4) is a variant of steepest descent (Perkins et al. , 2003)." ></td>
	<td class="line x" title="81:239	To compute the gradient of the unpenalized loss L with respect to the parameter f of feature f, we have: L(I) f = summationdisplay iI l(i) (i)  (i) f (10) where:  (i) f = y(i)X f (i) (11) Using Equation 6, we define the weight of an example i under the current model as the rate at which loss decreases as the margin of i increases: w(i) =l(i) (i) = b(i) 11 + exp( (i)) (12) Recall that X f (i) is either 0 or 1." ></td>
	<td class="line x" title="82:239	Combining Equations 1012 gives: L(I) f = summationdisplay iIX f (i)=1 y(i)w(i) (13) We define the gain of feature f as: G(I; f ) = max parenleftBigg 0, vextendsinglevextendsinglevextendsingle vextendsinglevextendsinglevextendsingleL(I) f vextendsinglevextendsinglevextendsingle vextendsinglevextendsinglevextendsingle parenrightBigg (14) Equation 14 has this form because the gradient of the penalty term is undefined at f = 0." ></td>
	<td class="line x" title="83:239	This discontinuity is why lscript1 regularization tends to produce sparse models." ></td>
	<td class="line x" title="84:239	If G(I; f ) = 0, then the objective R(I) is at its minimum with respect to parameter f. Otherwise, G(I; f ) is the magnitude 875 of the gradient of the objective as we adjust f in the appropriate direction." ></td>
	<td class="line x" title="85:239	To build each decision tree, we begin with a root node." ></td>
	<td class="line x" title="86:239	The root node corresponds to a dummy always true feature." ></td>
	<td class="line x" title="87:239	We recursively split nodes by choosing a splitting feature that will allow us to increase the gain." ></td>
	<td class="line x" title="88:239	Node n with corresponding compound feature (n) = f can be split by atomic feature a if: G(I; f a) + G(I; f a) > G(I; f ) (15) If no atomic feature satisfies the splitting criterion in Equation 15, then n becomes a leaf node of the decision tree and (n) becomes one of the values to be optimized during the parameter update step." ></td>
	<td class="line x" title="89:239	Otherwise, we choose atomic feature a to split node n: a = arg max aA (G(I; f a) + G(I; f a)) (16) This split creates child nodes n1 and n2, with (n1) = f  a and (n2) = f a. Parameter update is done sequentially on only the most recently added compound features, which correspond to the leaves of the new decision tree." ></td>
	<td class="line x" title="90:239	After the entire tree is built, we percolate examples down to their appropriate leaf nodes." ></td>
	<td class="line x" title="91:239	We then choose for each leaf node n the parameter (n) that minimizes the objective over the examples in that leaf." ></td>
	<td class="line x" title="92:239	A convenient property of decision trees is that the leaves compound features are mutually exclusive." ></td>
	<td class="line x" title="93:239	Their parameters can be directly optimized independently of each other using a line search over the objective." ></td>
	<td class="line x" title="94:239	3.3 The Training Set We choose a single correct path from each training parse tree, and the training examples correspond to all candidate inferences considered in every state along this path.4 In the deterministic setting there is only one correct path, so example generation is identical to that of Sagae and Lavie (2005)." ></td>
	<td class="line x" title="95:239	If parsing proceeds non-deterministically then there might be multiple paths that lead to the same final parse, so we choose one randomly." ></td>
	<td class="line x" title="96:239	This method of generating training examples does not require a working parser and can be run prior to any training." ></td>
	<td class="line x" title="97:239	The disadvantage of this approach is that it minimizes the error of the parser at correct states only." ></td>
	<td class="line x" title="98:239	It does not account for compounded error or 4Nearly all of the examples generated are negative (y =1)." ></td>
	<td class="line x" title="99:239	teach the parser to recover from mistakes gracefully." ></td>
	<td class="line x" title="100:239	Turian and Melamed (2005) observed that uniform example biases b(i) produced lower accuracy as training progressed, because the induced classifiers minimized the error per example." ></td>
	<td class="line x" title="101:239	To minimize the error per state, we assign every training state equal value and share half the value uniformly among the negative examples for the examples generated from that state and the other half uniformly among the positive examples." ></td>
	<td class="line x" title="102:239	We parallelize training by inducing 26 label classifiers (one for each non-terminal label in the Penn Treebank)." ></td>
	<td class="line x" title="103:239	Parallelization might not uniformly reduce training time because different label classifiers train at different rates." ></td>
	<td class="line x" title="104:239	However, parallelization uniformly reduces memory usage because each label classifier trains only on inferences whose consequent item has that label." ></td>
	<td class="line x" title="105:239	4 Experiments Discriminative parsers are notoriously slow to train." ></td>
	<td class="line x" title="106:239	For example, Taskar et al.(2004) took several months to train on the  15 word sentences in the English Penn Treebank (Dan Klein, p.c.)." ></td>
	<td class="line x" title="108:239	The present work makes progress towards faster discriminative parser training: our slowest classifier took fewer than 5 days to train." ></td>
	<td class="line x" title="109:239	Even so, it would have taken much longer to train on the entire treebank." ></td>
	<td class="line x" title="110:239	We follow Taskar et al.(2004) in training and testing on  15 word sentences in the English Penn Treebank (Taylor et al. , 2003)." ></td>
	<td class="line x" title="112:239	We used sections 0221 for training, section 22 for development, and section 23 for testing, preprocessed as per Table 1." ></td>
	<td class="line x" title="113:239	We evaluated our parser using the standard PARSEVAL measures (Black et al. , 1991): labelled precision, labelled recall, and labelled F-measure (Prec., Rec., and F1, respectively), which are based on the number of nonterminal items in the parsers output that match those in the gold-standard parse.5 As mentioned in Section 2, items are inferred bottom-up and the parser cannot infer any item that crosses an item already in the state." ></td>
	<td class="line x" title="116:239	Although there are O(n2) possible (span, label) pairs over a frontier containing n items, we reduce this to the 5n inferences that have at most five children.6 5The correctness of a stratified shuing test has been called into question (Michael Collins, p.c.), so we are not aware of any valid significance tests for observed differences in PARSEVAL scores." ></td>
	<td class="line x" title="117:239	6Only 0.57% of non-terminals in the preprocessed develop876 Table 1 Steps for preprocessing the data." ></td>
	<td class="line x" title="118:239	Starred steps are performed only when parse trees are available in the data (e.g. not on test data)." ></td>
	<td class="line x" title="119:239	1." ></td>
	<td class="line x" title="120:239	* Strip functional tags and trace indices, and remove traces." ></td>
	<td class="line x" title="121:239	2." ></td>
	<td class="line x" title="122:239	* Convert PRT to ADVP." ></td>
	<td class="line x" title="123:239	(This convention was established by Magerman (1995).)" ></td>
	<td class="line x" title="124:239	3." ></td>
	<td class="line x" title="125:239	Remove quotation marks (i.e. terminal items tagged  or )." ></td>
	<td class="line x" title="126:239	(Bikel, 2004) 4." ></td>
	<td class="line x" title="127:239	* Raise punctuation." ></td>
	<td class="line x" title="128:239	(Bikel, 2004) 5." ></td>
	<td class="line x" title="129:239	Remove outermost punctuation.a 6." ></td>
	<td class="line x" title="130:239	* Remove unary projections to self (i.e. duplicate items with the same span and label)." ></td>
	<td class="line x" title="131:239	7." ></td>
	<td class="line x" title="132:239	POS tag the text using the tagger of Ratnaparkhi (1996)." ></td>
	<td class="line x" title="133:239	8." ></td>
	<td class="line x" title="134:239	Lowercase headwords." ></td>
	<td class="line x" title="135:239	aAs pointed out by an anonymous reviewer of Collins (2003), removing outermost punctuation might discard useful information." ></td>
	<td class="line oc" title="136:239	Collins and Roark (2004) saw a LFMS improvement of 0.8% over their baseline discriminative parser after adding punctuation features, one of which encoded the sentence-final punctuation." ></td>
	<td class="line x" title="137:239	To ensure the parser does not enter an infinite loop, no two items in a state can have both the same span and the same label." ></td>
	<td class="line x" title="138:239	Given these restrictions on candidate inferences, there were roughly 40 million training examples generated in the training set." ></td>
	<td class="line x" title="139:239	These were partitioned among the 26 constituent label classifiers." ></td>
	<td class="line x" title="140:239	Building a decision tree (Steps 1.51.9 in Listing 1) using the entire example set I can be very expensive." ></td>
	<td class="line x" title="141:239	We estimate loss gradients (Equation 13) using a sample of the inference set, which gives a 100-fold increase in training speed (Turian & Melamed, 2006)." ></td>
	<td class="line x" title="142:239	Our atomic feature set A contains 300K features, each of the form is there an item in group J whose label/headword/headtag/headtagclass is X?.7 Possible values of X for each predicate are collected from the training data." ></td>
	<td class="line x" title="143:239	For 1n3, possible values for J are: the first/last n child items the first n left/right context items the n children items left/right of the head the head item." ></td>
	<td class="line x" title="144:239	The left and right context items are the frontier items to the left and right of the children of the candidate inference, respectively." ></td>
	<td class="line x" title="145:239	4.1 Different Parsing Strategies To demonstrate the flexibility of our learning procedure, we trained three different parsers: left-to-right (l2r), right-to-left (r2l), ment set have more than five children." ></td>
	<td class="line x" title="146:239	7The predicate headtagclass is a supertype of the headtag." ></td>
	<td class="line x" title="147:239	Given our compound features, these are not strictly necessary, but they accelerate training." ></td>
	<td class="line x" title="148:239	An example is proper noun, which contains the POS tags given to singular and plural proper nouns." ></td>
	<td class="line x" title="149:239	Space constraints prevent enumeration of the headtagclasses, which are instead provided at the URL given in the abstract." ></td>
	<td class="line x" title="150:239	Table 2 Results on the development set, training and testing using only15 word sentences." ></td>
	<td class="line x" title="151:239	active  features % Rec." ></td>
	<td class="line x" title="152:239	% Prec." ></td>
	<td class="line x" title="153:239	F1 l2r 0.040 11.9K 89.86 89.63 89.74 b.u. 0.020 13.7K 89.92 89.84 89.88 r2l 0.014 14.0K 90.66 89.81 90.23 and non-deterministic bottom-up (b.u.)." ></td>
	<td class="line x" title="154:239	The non-deterministic parser was allowed to choose any bottom-up inference." ></td>
	<td class="line x" title="155:239	The other two parsers were deterministic: bottom-up inferences had to be performed strictly left-to-right or rightto-left, respectively." ></td>
	<td class="line x" title="156:239	We stopped training when each parser had 15K active features." ></td>
	<td class="line x" title="157:239	Figure 1 shows the accuracy of the different runs over the development set as training progressed." ></td>
	<td class="line x" title="158:239	Table 2 gives the PARSEVAL scores of these parsers at their optimal lscript1 penalty setting." ></td>
	<td class="line x" title="159:239	We found that the perplexity of the r2l model was low so that, in 85% of the sentences, its greedy parse was the optimal one." ></td>
	<td class="line x" title="160:239	The l2r parser does poorly because its decisions were more difficult than those of the other parsers." ></td>
	<td class="line x" title="161:239	If it inferred far-right items, it was more likely to prevent correct subsequent inferences that were to the left." ></td>
	<td class="line x" title="162:239	But if it inferred far-left items, then it went against the right-branching tendency of English sentences." ></td>
	<td class="line oc" title="163:239	The left-to-right parser would likely improve if we were to use a left-corner transform (Collins & Roark, 2004)." ></td>
	<td class="line x" title="164:239	Parsers in the literature typically choose some local threshold on the amount of search, such as a maximum beam width." ></td>
	<td class="line x" title="165:239	With an accurate scoring function, restricting the search space using a fixed beam width might be unnecessary." ></td>
	<td class="line x" title="166:239	Instead, we imposed a global threshold on exploration of the search space." ></td>
	<td class="line x" title="167:239	Specifically, if the 877 Figure 1 F1 scores on the development set of the Penn Treebank, using only  15 word sentences." ></td>
	<td class="line x" title="168:239	The x-axis shows the number of non-zero parameters in each parser, summed over all classifiers." ></td>
	<td class="line x" title="169:239	85% 86% 87% 88% 89% 90% 15K10K5K2.5K1.5K Devel." ></td>
	<td class="line x" title="170:239	F-measure total number of non-zero parameters right-to-leftleft-to-right bottom up parser has found some complete parse and has explored at least 100K states (i.e. scored at least 100K inferences), search stopped prematurely and the parser would return the (possibly sub-optimal) current best complete parse." ></td>
	<td class="line x" title="171:239	The l2r and r2l parsers never exceeded this threshold, and always found the optimal complete parse." ></td>
	<td class="line x" title="172:239	However, the non-deterministic bottom-up parsers search was cut-short in 28% of the sentences." ></td>
	<td class="line x" title="173:239	The nondeterministic parser can reach each parse state through many different paths, so it searches a larger space than a deterministic parser, with more redundancy." ></td>
	<td class="line x" title="174:239	To gain a better understanding of the weaknesses of our parser, we examined a sample of 50 development sentences that the r2l parser did not get entirely correct." ></td>
	<td class="line x" title="175:239	Roughly half the errors were due to noise and genuine ambiguity." ></td>
	<td class="line x" title="176:239	The remaining errors fell into three types, occurring with roughly the same frequency:  ADVPs and ADJPs The r2l parser had F1 = 81.1% on ADVPs, and F1 = 71.3% on ADJPs." ></td>
	<td class="line x" title="177:239	Annotation of ADJP and ADVP in the PTB is inconsistent, particularly for unary projections." ></td>
	<td class="line x" title="178:239	POS Tagging Errors Many of the parsers errors were due to incorrect POS tags." ></td>
	<td class="line x" title="179:239	In future work we will integrate POS-tagging as inferences of the parser, allowing it to entertain competing hypotheses about the correct tagging." ></td>
	<td class="line x" title="180:239	Bilexical dependencies Although compound features exist to detect affinities between words, the parser had difficulties with bilexical dependency decisions that were unobserved in the training data." ></td>
	<td class="line x" title="181:239	The classifier would need more training data to learn these affinities." ></td>
	<td class="line x" title="182:239	Figure 2 F1 scores of right-to-left parsers with different atomic feature sets on the development set of the Penn Treebank, using only 15 word sentences." ></td>
	<td class="line x" title="183:239	85% 86% 87% 88% 89% 90% 91% 30K20K10K5K2.5K1.5K Devel." ></td>
	<td class="line x" title="184:239	F-measure total number of non-zero parameters kitchen sinkbaseline 4.2 More Atomic Features We compared our right-to-left parser with the baseline set of atomic features to one with a far richer atomic feature set, including unbounded context features, length features, and features of the terminal items." ></td>
	<td class="line x" title="185:239	This kitchen sink parser merely has access to many more item groups J, described in Table 3." ></td>
	<td class="line x" title="186:239	All features are all of the form given earlier, except for length features (Eisner & Smith, 2005)." ></td>
	<td class="line x" title="187:239	Length features compute the size of one of the groups of items in the indented list in Table 3." ></td>
	<td class="line x" title="188:239	The feature determines if this length is equal to/greater than to n, 0n15." ></td>
	<td class="line x" title="189:239	The kitchen sink parser had 1.1 million atomic features, 3.7 times the number available in the baseline." ></td>
	<td class="line x" title="190:239	In future work, we plan to try linguistically more sophisticated features (Charniak & Johnson, 2005) as well as sub-tree features (Bod, 2003; Kudo et al. , 2005)." ></td>
	<td class="line x" title="191:239	Figure 2 shows the accuracy of the right-toleft parsers with different atomic feature sets over the development set as training progressed." ></td>
	<td class="line x" title="192:239	Even though the baseline training made progress more quickly than the kitchen sink, the kitchen sinks F1 surpassed the baselines F1 early in training, and at 6.3K active parameters it achieved a development set F1 of 90.55%." ></td>
	<td class="line x" title="193:239	4.3 Test Set Results To situate our results in the literature, we compare our results to those reported by Taskar et al.(2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on  15 word sentences." ></td>
	<td class="line x" title="195:239	We also compare our parser to a representative non-discriminative 878 Table 3 Item groups available in the kitchen sink run." ></td>
	<td class="line x" title="196:239	 the first/last n child items, 1n4  the first n left/right context items, 1n4  the n children items left/right of the head, 1n4  the nth frontier item left/right of the leftmost/head/rightmost child item, 1n3  the nth terminal item left/right of the leftmost/head/rightmost terminal item dominated by the item being inferred, 1n3  the leftmost/head/rightmost child item of the leftmost/head/rightmost child item  the following groups of frontier items:  all items  left/right context items  non-leftmost/non-head/non-rightmost child items  child items left/right of the head item, inclusive/exclusive  the terminal items dominated by one of the item groups in the indented list above Table 4 Results of parsers on the test set, training and testing using only15 word sentences." ></td>
	<td class="line x" title="197:239	% Rec." ></td>
	<td class="line x" title="198:239	% Prec." ></td>
	<td class="line x" title="199:239	F1 Turian and Melamed (2005) 86.47 87.80 87.13 Bikel (2004) 87.85 88.75 88.30 Taskar et al.(2004) 89.10 89.14 89.12 kitchen sink 89.26 89.55 89.40 parser (Bikel, 2004)8, the only one that we were able to train and test under exactly the same experimental conditions (including the use of POS tags from the tagger of Ratnaparkhi (1996))." ></td>
	<td class="line x" title="201:239	Table 4 shows the PARSEVAL results of these four parsers on the test set." ></td>
	<td class="line x" title="202:239	5 Comparison with Related Work Our parsing approach is based upon a single endto-end discriminative learning machine." ></td>
	<td class="line nc" title="203:239	Collins and Roark (2004) and Taskar et al.(2004) beat the generative baseline only after using the standard trick of using the output from a generative model as a feature." ></td>
	<td class="line x" title="205:239	Henderson (2004) finds that discriminative training was too slow, and reports accuracy higher than generative models by discriminatively reranking the output of his generative model." ></td>
	<td class="line x" title="206:239	Unlike these state-of-the-art discriminative parsers, our method does not (yet) use any information from a generative model to improve training speed or accuracy." ></td>
	<td class="line x" title="207:239	As far as we know, we present the first discriminative parser that does not use information from a generative model to beat a 8Bikel (2004) is a clean room reimplementation of the Collins (1999) model with comparable accuracy." ></td>
	<td class="line x" title="208:239	generative baseline (the Collins model)." ></td>
	<td class="line x" title="209:239	The main limitation of our work is that we can do training reasonably quickly only on short sentences because a sentence with n words generates O(n2) training inferences in total." ></td>
	<td class="line nc" title="210:239	Although generating training examples in advance without a working parser (Turian & Melamed, 2005) is much faster than using inference (Collins & Roark, 2004; Henderson, 2004; Taskar et al. , 2004), our training time can probably be decreased further by choosing a parsing strategy with a lower branching factor." ></td>
	<td class="line x" title="211:239	Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005) generate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O(n) training examples." ></td>
	<td class="line x" title="212:239	An advantage of our approach is its flexibility." ></td>
	<td class="line x" title="213:239	As our experiments showed, it is quite simple to substitute in different parsing strategies." ></td>
	<td class="line x" title="214:239	Although we used very little linguistic information (the head rules and the POS tag classes), our model could also start with more sophisticated task-specific features in its atomic feature set." ></td>
	<td class="line x" title="215:239	Atomic features that access arbitrary information are represented directly without the need for an induced intermediate representation (cf.Henderson, 2004)." ></td>
	<td class="line x" title="217:239	Other papers (Clark & Curran, 2004; Kaplan et al. , 2004, e.g)." ></td>
	<td class="line x" title="218:239	have applied log-linear models to parsing." ></td>
	<td class="line x" title="219:239	These works are based upon conditional models, which include a normalization term." ></td>
	<td class="line x" title="220:239	However, our loss function forgoes normalization, which means that it is easily decomposed into the loss of individual inferences (Equation 5)." ></td>
	<td class="line x" title="221:239	879 Decomposition of the loss allows the objective to be optimized in parallel." ></td>
	<td class="line x" title="222:239	This might be an advantage for larger structured prediction problems where there are more opportunities for parallelization, for example machine translation." ></td>
	<td class="line x" title="223:239	The only important hyper-parameter in our method is the lscript1 penalty factor." ></td>
	<td class="line x" title="224:239	We optimize it as part of the training process, choosing the value that maximizes accuracy on a held-out development set." ></td>
	<td class="line x" title="225:239	This technique stands in contrast to more ad-hoc methods for choosing hyper-parameters, which may require prior knowledge or additional experimentation." ></td>
	<td class="line x" title="226:239	6 Conclusion Our work has made advances in both accuracy and training speed of discriminative parsing." ></td>
	<td class="line x" title="227:239	As far as we know, we present the first discriminative parser that surpasses a generative baseline on constituent parsing without using a generative component, and it does so with minimal linguistic cleverness." ></td>
	<td class="line x" title="228:239	Our approach performs feature selection incrementally over an exponential feature space during training." ></td>
	<td class="line x" title="229:239	Our experiments suggest that the learning algorithm is overfitting-resistant, as hypothesized by Ng (2004)." ></td>
	<td class="line x" title="230:239	If this is the case, it would reduce the effort required for feature engineering." ></td>
	<td class="line x" title="231:239	An engineer can merely design a set of atomic features whose powerset contains the requisite information." ></td>
	<td class="line x" title="232:239	Then, the learning algorithm can perform feature selection over the compound feature space, avoiding irrelevant compound features." ></td>
	<td class="line x" title="233:239	In future work, we shall make some standard improvements." ></td>
	<td class="line x" title="234:239	Our parser should infer its own POS tags to improve accuracy." ></td>
	<td class="line x" title="235:239	A shift-reduce parsing strategy will generate fewer training inferences, and might lead to shorter training times." ></td>
	<td class="line x" title="236:239	Lastly, we plan to give the model linguistically more sophisticated features." ></td>
	<td class="line x" title="237:239	We also hope to apply the model to other structured prediction tasks, such as syntax-driven machine translation." ></td>
	<td class="line x" title="238:239	Acknowledgments The authors would like to thank Chris Pike, Cynthia Rudin, and Ben Wellington, as well as the anonymous reviewers, for their helpful comments and constructive criticism." ></td>
	<td class="line x" title="239:239	This research was sponsored by NSF grants #0238406 and #0415933." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-1628
A Discriminative Model For Tree-To-Tree Translation
Cowan, Brooke;Kucerova, Ivona;Collins, Michael John;"></td>
	<td class="line x" title="1:291	Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 232241, Sydney, July 2006." ></td>
	<td class="line x" title="2:291	c2006 Association for Computational Linguistics A Discriminative Model for Tree-to-Tree Translation Brooke Cowan MIT CSAIL brooke@csail.mit.edu Ivona Kucerova MIT Linguistics Department kucerova@mit.edu Michael Collins MIT CSAIL mcollins@csail.mit.edu Abstract This paper proposes a statistical, treeto-tree model for producing translations." ></td>
	<td class="line x" title="3:291	Two main contributions are as follows: (1) a method for the extraction of syntactic structures with alignment information from a parallel corpus of translations, and (2) use of a discriminative, featurebased model for prediction of these targetlanguage syntactic structureswhich we call aligned extended projections, or AEPs." ></td>
	<td class="line x" title="4:291	An evaluation of the method on translation from German to English shows similar performance to the phrase-based model of Koehn et al.(2003)." ></td>
	<td class="line x" title="6:291	1 Introduction Phrase-based approaches (Och and Ney, 2004) to statistical machine translation (SMT) have recently achieved impressive results, leading to significant improvements in accuracy over the original IBM models (Brown et al. , 1993)." ></td>
	<td class="line x" title="7:291	However, phrase-based models lack a direct representation of syntactic information in the source or target languages; this has prompted several researchers to consider various approaches that make use of syntactic information." ></td>
	<td class="line x" title="8:291	This paper describes a framework for tree-totree based statistical translation." ></td>
	<td class="line x" title="9:291	Our goal is to learn a model that maps parse trees in the source language to parse trees in the target language." ></td>
	<td class="line x" title="10:291	The model is learned from a corpus of translation pairs, where each sentence in the source or target language has an associated parse tree." ></td>
	<td class="line x" title="11:291	We see two major benefits of tree-to-tree based translation." ></td>
	<td class="line x" title="12:291	First, it is possible to explicitly model the syntax of the target language, thereby improving grammaticality." ></td>
	<td class="line x" title="13:291	Second, we can build a detailed model of the correspondence between the source and target parse trees, with the aim of constructing translations that preserve the meaning of source language sentences." ></td>
	<td class="line x" title="14:291	Our translation framework involves a process where the target-language parse tree is broken down into a sequence of clauses, and each clause is then translated separately." ></td>
	<td class="line x" title="15:291	A central concept we introduce in the translation of clauses is that of an aligned extended projection (AEP)." ></td>
	<td class="line x" title="16:291	AEPs are derived from the concept of an extended projection in lexicalized tree adjoining grammars (LTAG) (Frank, 2002), with the addition of alignment information that is based on work in synchronous LTAG (Shieber and Schabes, 1990)." ></td>
	<td class="line x" title="17:291	A key contribution of this paper is a method for learning to map German clauses to AEPs using a featurebased model with a perceptron learning algorithm." ></td>
	<td class="line x" title="18:291	We performed experiments on translation from German to English on the Europarl data set." ></td>
	<td class="line x" title="19:291	Evaluation in terms of both BLEU scores and human judgments shows that our system performs similarly to the phrase-based model of Koehn et al.(2003)." ></td>
	<td class="line x" title="21:291	1.1 A Sketch of the Approach This section provides an overview of the translation process." ></td>
	<td class="line x" title="22:291	We will use the German sentence wir wissen da das haupthemmnis der vorhersehbare widerstand der hersteller war as a running example." ></td>
	<td class="line x" title="23:291	For this example we take the desired translation to be we know that the main obstacle has been the predictable resistance of manufacturers." ></td>
	<td class="line x" title="24:291	Translation of a German sentence proceeds in the following four steps: Step 1: The German sentence is parsed and then broken down into separate parse structures for a sequence of clauses." ></td>
	<td class="line x" title="25:291	For example, the German example above is broken into a parse structure for the clause wir wissen followed by a parse structure for the subordinate clause dawar." ></td>
	<td class="line x" title="26:291	Each of these clauses is then translated separately, using steps 23 below." ></td>
	<td class="line x" title="27:291	Step 2: An aligned extended projection (AEP) is predicted for each German clause." ></td>
	<td class="line x" title="28:291	To illustrate this step, consider translation of the second German clause, which has the following parse structure: 232 s-oc kous-cp da np-sb 1 art das nn haupthemmnis np-pd 2 art der adja vorhersehbare nn widerstand np-ag art der nn hersteller vafin-hd war Note that we use the symbols 1 and 2 to identify the two modifiers (arguments or adjuncts) in the clause, in this case a subject and an object." ></td>
	<td class="line x" title="29:291	A major part of the AEP is a parse-tree fragment, that is similar to a TAG elementary tree (see also Figure 2): SBAR that S NP VP V has VP V been NP Following the work of Frank (2002), we will refer to a structure like this as an extended projection (EP)." ></td>
	<td class="line x" title="30:291	The EP encapsulates the core syntactic structure in the English clause." ></td>
	<td class="line x" title="31:291	It contains the main verb been, as well as the function words that and has." ></td>
	<td class="line x" title="32:291	It also contains a parse tree spine which has the main verb been as one of its leaves, and has the clause label SBAR as its root." ></td>
	<td class="line x" title="33:291	In addition, it specifies positions for arguments in the clausein this case NPs corresponding to the subject and object." ></td>
	<td class="line x" title="34:291	An AEP contains an EP, as well as alignment information about where the German modifiers should be placed in the extended projection." ></td>
	<td class="line x" title="35:291	For example, the AEP in this case would contain the tree fragment shown above, together with an alignment specifying that the modifiers 1 and 2 from the German parse will appear in the EP as subject and object, respectively." ></td>
	<td class="line x" title="36:291	Step 3: The German modifiers are translated and placed in the appropriate positions within the AEP." ></td>
	<td class="line x" title="37:291	For example, the modifiers das haupthemmnis and der vorhersehbare widerstand der hersteller would be translated as the main obstacle, and the predictable resistance of manufacturers, respectively, and then placed into the subject and object positions in the AEP." ></td>
	<td class="line x" title="38:291	Step 4: The individual clause translations are combined to give a final translation." ></td>
	<td class="line x" title="39:291	For example, the translations we know and that the main obstacle has been would be concatenated to give we know that the main obstacle has been  The main focus of this paper will be Step 2: the prediction of AEPs from German clauses." ></td>
	<td class="line x" title="40:291	AEPs are detailed structural objects, and their relationship to the source-language clause can be quite complex." ></td>
	<td class="line x" title="41:291	We use a discriminative feature-based model, trained with the perceptron algorithm, to incrementally predict the AEP in a sequence of steps." ></td>
	<td class="line x" title="42:291	At each step we define features that allow the model to capture a wide variety of dependencies within the AEP itself, or between the AEP and the source-language clause." ></td>
	<td class="line x" title="43:291	1.2 Motivation for the Approach Our approach to tree-to-tree translation is motivated by several observations." ></td>
	<td class="line x" title="44:291	Breaking the source-language tree into clauses (Step 1) considerably simplifies the difficult problem of defining an alignment between source and target trees." ></td>
	<td class="line x" title="45:291	Our impression is that high-quality translations can be produced in a clause-by-clause fashion.1 The use of a feature-based model for AEP prediction (Step 2) allows us to capture complex syntactic correspondences between English and German, as well as grammaticality constraints on the English side." ></td>
	<td class="line x" title="46:291	In this paper, we implement the translation of modifiers (Step 3) with the phrase-based system of Koehn et al.(2003)." ></td>
	<td class="line x" title="48:291	The modifiers in our data set are generally small chunks of text such as NPs, PPs, and ADJPs, which by definition do not include clauses or verbs." ></td>
	<td class="line x" title="49:291	In our approach, we use the phrase-based system to generate n-best lists of candidate translations and then rerank the translations based on grammaticality, i.e., using criteria that judge how well they fit the position in the AEP." ></td>
	<td class="line x" title="50:291	In future work, we might use finite state machines in place of a reranking approach, or recursively apply the AEP approach to the modifiers." ></td>
	<td class="line x" title="51:291	Stitching translated clauses back together (Step 4) is a relatively simple task: in a substantial majority of cases, the German clauses are not embedded, but instead form a linear sequence that accounts for the entire sentence." ></td>
	<td class="line x" title="52:291	In these cases we can simply concatenate the English clause translations to form the full translation." ></td>
	<td class="line x" title="53:291	Embedded clauses in German are slightly more complicated, but it is not difficult to form embedded structures in the English translations." ></td>
	<td class="line x" title="54:291	Section 5.2 of this paper describes the features 1Note that we do not assume that all of the translations in the training data have been produced in a clause-by-clause fashion." ></td>
	<td class="line x" title="55:291	Rather, we assume that good translations for test examples can be produced in this way." ></td>
	<td class="line x" title="56:291	233 we use for AEP prediction in translation from German to English." ></td>
	<td class="line x" title="57:291	Many of the features of the AEP prediction model are specifically tuned to the choice of German and English as the source and target languages." ></td>
	<td class="line x" title="58:291	However, it should be easy to develop new feature sets to deal with other languages or treebanking styles." ></td>
	<td class="line x" title="59:291	We see this as one of the strengths of the feature-based approach." ></td>
	<td class="line x" title="60:291	In the work presented in this paper, we focus on the prediction of clausal AEPs, i.e., AEPs associated with main verbs." ></td>
	<td class="line x" title="61:291	One reason for this is that clause structures are particularly rich and complex from a syntactic perspective." ></td>
	<td class="line x" title="62:291	This means that there should be considerable potential in improving translation quality if we can accurately predict these structures." ></td>
	<td class="line x" title="63:291	It also means that clause-level AEPs are a good test-bed for the discriminative approach to AEP prediction; future work may consider applying these methods to other structures such as NPs, PPs, ADJPs, and so on." ></td>
	<td class="line x" title="64:291	2 Related Work There has been a substantial amount of previous work on approaches that make use of syntactic information in statistical machine translation." ></td>
	<td class="line x" title="65:291	Wu (1997) and Alshawi (1996) describe early work on formalisms that make use of transductive grammars; Graehl and Knight (2004) describe methods for training tree transducers." ></td>
	<td class="line x" title="66:291	Melamed (2004) establishes a theoretical framework for generalized synchronous parsing and translation." ></td>
	<td class="line x" title="67:291	Eisner (2003) discusses methods for learning synchronized elementary tree pairs from a parallel corpus of parsed sentences." ></td>
	<td class="line x" title="68:291	Chiang (2005) has recently shown significant improvements in translation accuracy, using synchronous grammars." ></td>
	<td class="line x" title="69:291	Riezler and Maxwell (2006) describe a method for learning a probabilistic model that maps LFG parse structures in German into LFG parse structures in English." ></td>
	<td class="line x" title="70:291	Yamada and Knight (2001) and Galley et al.(2004) describe methods that make use of syntactic information in the target language alone; Quirk et al.(2005) describe similar methods that make use of dependency representations." ></td>
	<td class="line x" title="73:291	Syntactic parsers in the target language have been used as language models in translation, giving some improvement in accuracy (Charniak et al. , 2001)." ></td>
	<td class="line x" title="74:291	The work of Gildea (2003) involves methods that make use of syntactic information in both the source and target languages." ></td>
	<td class="line x" title="75:291	Other work has attempted to incorporate syntacS NP-A VP V know SBAR-A SBAR-A IN that S NP-A VP V has VP V been NP-A NP D the N obstacle Figure 1: Extended projections for the verbs know and been, and for the noun obstacle." ></td>
	<td class="line x" title="76:291	The EPs were taken from the parse tree for the sentence We know that the main obstacle has been the predictable resistance of manufacturers." ></td>
	<td class="line x" title="77:291	tic information through reranking approaches applied to n-best output from phrase-based systems (Och et al. , 2004)." ></td>
	<td class="line xc" title="78:291	Another class of approaches has shown improvements in translation through reordering, where source language strings are parsed and then reordered, in an attempt to recover a word order that is closer to the target language (Collins et al. , 2005; Xia and McCord, 2004)." ></td>
	<td class="line x" title="79:291	Our approach is closely related to previous work on synchronous tree adjoining grammars (Shieber and Schabes, 1990; Shieber, 2004), and the work on TAG approaches to syntax described by Frank (2002)." ></td>
	<td class="line x" title="80:291	A major departure from previous work on synchronous TAGs is in our use of a discriminative model that incrementally predicts the information in the AEP." ></td>
	<td class="line x" title="81:291	Note also that our model may include features that take into account any part of the German clause." ></td>
	<td class="line x" title="82:291	3 A Translation Architecture Based on Aligned Extended Projections 3.1 Background: Extended Projections (EPs) Extended projections (EPs) play a crucial role in the lexicalized tree adjoining grammar (LTAG) (Joshi, 1985) approach to syntax described by Frank (2002)." ></td>
	<td class="line x" title="83:291	In this paper we focus almost exclusively on extended projections associated with main verbs; note, however, that EPs are typically associated with all content words (nouns, adjectives, etc.)." ></td>
	<td class="line x" title="84:291	As an example, a parse tree for the sentence we know that the main obstacle has been the predictable resistance of manufacturers would make use of EPs for the words we, know, main, obstacle, been, predictable, resistance, and manufacturers." ></td>
	<td class="line x" title="85:291	Function words (in this sentence that, the, has, and of) do not have EPs; instead, as we describe shortly, each function word is incorporated in an EP of some content word." ></td>
	<td class="line x" title="86:291	Figure 1 has examples of EPs." ></td>
	<td class="line x" title="87:291	Each one is an LTAG elementary tree which contains a sin234 gle content word as one of its leaves." ></td>
	<td class="line x" title="88:291	Substitution nodes (such as NP-A or SBAR-A) in the elementary trees specify the positions of arguments of the content words." ></td>
	<td class="line x" title="89:291	Each EP may contain one or more function words that are associated with the content word." ></td>
	<td class="line x" title="90:291	For verbs, these function words include items such as modal verbs and auxiliaries (e.g. , should and has); complementizers (e.g. , that); and wh-words (e.g. , which)." ></td>
	<td class="line x" title="91:291	For nouns, function words include determiners and prepositions." ></td>
	<td class="line x" title="92:291	Elementary trees corresponding to EPs form the basic units in the LTAG approach described by Frank (2002)." ></td>
	<td class="line x" title="93:291	They are combined to form a full parse tree for a sentence using the TAG operations of substitution and adjunction." ></td>
	<td class="line x" title="94:291	For example, the EP for been in Figure 1 can be substituted into the SBAR-A position in the EP for know; the EP for obstacle can be substituted into the subject position of the EP for been." ></td>
	<td class="line x" title="95:291	3.2 Aligned Extended Projections (AEPs) We now build on the idea of extended projections to give a detailed description of AEPs." ></td>
	<td class="line x" title="96:291	Figure 2 shows examples of German clauses paired with the AEPs found in training data.2 The German clause is assumed to have n (where n  0) modifiers." ></td>
	<td class="line x" title="97:291	For example, the first German parse in Figure 2 has two arguments, indexed as 1 and 2." ></td>
	<td class="line x" title="98:291	Each of these modifiers must either have a translation in the corresponding English clause, or must be deleted." ></td>
	<td class="line x" title="99:291	An AEP consists of the following parts: STEM: A string specifying the stemmed form of the main verb in the clause." ></td>
	<td class="line x" title="100:291	SPINE: A syntactic structure associated with the main verb." ></td>
	<td class="line x" title="101:291	The structure has the symbol V as one of its leaf nodes; this is the position of the main verb." ></td>
	<td class="line x" title="102:291	It includes higher projections of the verb such as VPs, Ss, and SBARs." ></td>
	<td class="line x" title="103:291	It also includes leaf nodes NP-A in positions corresponding to noun-phrase arguments (e.g. , the subject or object) of the main verb." ></td>
	<td class="line x" title="104:291	In addition, it may contain leaf nodes labeled with categories such as WHNP or WHADVP where a wh-phrase may be placed." ></td>
	<td class="line x" title="105:291	It may include leaf nodes corresponding to one or more complementizers (common examples being that, if, so that, and so on)." ></td>
	<td class="line x" title="106:291	VOICE: One of two alternatives, active or passive, specifying the voice of the main verb." ></td>
	<td class="line x" title="107:291	2Note that in this paper we consider translation from German to English; in the remainder of the paper we take English to be synonymous with the target language in translation and German to be synonymous with the source language." ></td>
	<td class="line x" title="108:291	SUBJECT: This variable can be one of three types." ></td>
	<td class="line x" title="109:291	If there is no subject position in the SPINE variable, then the value for SUBJECT is NULL." ></td>
	<td class="line x" title="110:291	Otherwise, SUBJECT can either be a string, for example there,3 or an index of one of the n modifiers in the German clause." ></td>
	<td class="line x" title="111:291	OBJECT: This variable is similar to SUBJECT, and can also take three types: NULL, a specific string, or an index of one of the n German modifiers." ></td>
	<td class="line x" title="112:291	It is always NULL if there is no object position in theSPINE; it can never be a modifier index that has already been assigned to SUBJECT." ></td>
	<td class="line x" title="113:291	WH: This variable is always NULL if there is no wh-phrase position within the SPINE; it is always a non-empty string (such as which, or in which) if a wh-phrase position does exist." ></td>
	<td class="line x" title="114:291	MODALS: This is a string of verbs that constitute the modals that appear within the clause." ></td>
	<td class="line x" title="115:291	We use NULL to signify an absence of modals." ></td>
	<td class="line x" title="116:291	INFL: The inflected form of the verb." ></td>
	<td class="line x" title="117:291	MOD(i): There are n modifier variables MOD(1), MOD(2),  , MOD(n) that specify the positions for German arguments that have not already been assigned to the SUBJECT or OBJECT positions in the spine." ></td>
	<td class="line x" title="118:291	Each variable MOD(i) can take one of five possible values:  null: This value is chosen if and only if the modifier has already been assigned to the subject or object position." ></td>
	<td class="line x" title="119:291	 deleted: This means that a translation of the ith German modifier is not present in the English clause." ></td>
	<td class="line x" title="120:291	 pre-sub: The modifier appears after any complementizers or wh-phrases, but before the subject of the English clause." ></td>
	<td class="line x" title="121:291	 post-sub: The modifier appears after the subject of the English clause, but before the modals." ></td>
	<td class="line x" title="122:291	 in-modals: The modifier appears after the first modal in the sequence of modals, but before the second modal or the main verb." ></td>
	<td class="line x" title="123:291	 post-verb: The modifier appears somewhere after the main verb." ></td>
	<td class="line x" title="124:291	3This happens in the case where there exists a subject in the English clause which is not aligned to a modifier in the German clause." ></td>
	<td class="line x" title="125:291	See, for instance, the second example in Figure 2." ></td>
	<td class="line x" title="126:291	235 German Clause English AEP s-oc kous-cp da np-sb 1 art das nn haupthemmnis np-pd 2 art der adja vorhersehbare nn widerstand np-ag art der nn hersteller vafin-hd war Paraphrase: that [np-sb the main obstacle] [np-pd the predictable resistance of manufacturers] was STEM: be SPINE: SBAR-A IN that S NP-A VP V NP-A VOICE: active SUBJECT: 1 OBJECT: 2 WH: NULL MODALS: has INFL: been MOD1: null MOD2: null s pp-mo 1 appr zwischen piat beiden nn gesetzen vvfin-hd bestehen adv-mo 2 also np-sb 3 adja erhebliche adja rechtliche $,, adja praktische kon und adja wirtschaftliche nn unterschiede Paraphrase: [pp-mo between the two pieces of legislation] exist so [np-sb significant legal, practical and economic differences] STEM: be SPINE: S NP-A VP V NP-A VOICE: active SUBJECT: there OBJECT: 3 WH: NULL MODALS: NULL INFL: are MOD1: post-verb MOD2: pre-sub MOD3: null s-rc prels-sb die vp pp-mo 1 appr an pdat jenem nn tag pp-mo 2 appr in ne tschernobyl vvpp-hd gezundet vafin-hd wurde Paraphrase: which [pp-mo on that day] [pp-mo in chernobyl] released were STEM: release SPINE: SBAR WHNP SG-A VP V VOICE: passive SUBJECT: NULL OBJECT: NULL WH: which MODALS: was INFL: released MOD1: post-verb MOD2: post-verb Figure 2: Three examples of German parse trees, together with their aligned extended projections (AEPs) in the training data." ></td>
	<td class="line x" title="127:291	Note that in the second example the correspondence between the German clause and its English translation is not entirely direct." ></td>
	<td class="line x" title="128:291	The subject in the English is the expletive there; the subject in the German clause becomes the object in English." ></td>
	<td class="line x" title="129:291	This is a typical pattern for the German verb bestehen." ></td>
	<td class="line x" title="130:291	The German PP zwischen  appears at the start of the clause in German, but is post-verbal in the English." ></td>
	<td class="line x" title="131:291	The modifier alsowhose English translation is sois in an intermediate position in the German clause, but appears in the pre-subject position in the English clause." ></td>
	<td class="line x" title="132:291	4 Extracting AEPs from a Corpus A crucial step in our approach is the extraction of training examples from a translation corpus." ></td>
	<td class="line x" title="133:291	Each training example consists of a German clause paired with an English AEP (see Figure 2)." ></td>
	<td class="line x" title="134:291	In our experiments, we used the Europarl corpus (Koehn, 2005)." ></td>
	<td class="line x" title="135:291	For each sentence pair from this data, we used a version of the German parser described by Dubey (2005) to parse the German component, and a version of the English parser described by Collins (1999) to parse the English component." ></td>
	<td class="line x" title="136:291	To extract AEPs, we perform the following steps: NP and PP Alignment To align NPs and PPs, first all German and English nouns, personal and possessive pronouns, numbers, and adjectives are identified in each sentence and aligned using GIZA++ (Och and Ney, 2003)." ></td>
	<td class="line x" title="137:291	Next, each NP in an English tree is aligned to an NP or PP in the corresponding German tree in a way that is consistent with the word-alignment information." ></td>
	<td class="line x" title="138:291	That is, the words dominated by the English node must be aligned only to words dominated by the German node, and vice versa." ></td>
	<td class="line x" title="139:291	Note that if there is more than one German node that is consistent, then the one rooted at the minimal subtree is selected." ></td>
	<td class="line x" title="140:291	Clause alignment, and AEP Extraction The next step in the training process is to identify German/English clause pairs which are translations of each other." ></td>
	<td class="line x" title="141:291	We first break each English or German parse tree into a set of clauses; see Appendix A for a description of how we identify clauses." ></td>
	<td class="line x" title="142:291	We retain only those training examples where the English and German sentences have the same number of clauses." ></td>
	<td class="line x" title="143:291	For these retained examples, define the English sentence to contain the clause sequence e1,e2,,en, and the German sentence to contain the clause sequence g1,g2,,gn." ></td>
	<td class="line x" title="144:291	The clauses are ordered according to the position of their main verbs in the original sentence." ></td>
	<td class="line x" title="145:291	We create n candidate pairs (e1,g1),(e2,g2),,(en,gn) (i.e. , force a oneto-one correspondence between the two clause sequences)." ></td>
	<td class="line x" title="146:291	We then discard any clause pairs (e,g) which are inconsistent with the NP/PP alignments for that sentence.4 4A clause pair is inconsistent with the NP/PP alignments if it contains an NP/PP on either the German or English side which is aligned to another NP/PP which is not within the clause pair." ></td>
	<td class="line x" title="147:291	236 Note that this method is deliberately conservative (i.e. , high precision, but lower recall), in that it discards sentence pairs where the English/German sentences have different numbers of clauses." ></td>
	<td class="line x" title="148:291	In practice, we have found that the method yields a large number of training examples, and that these training examples are of relatively high quality." ></td>
	<td class="line x" title="149:291	Future work may consider improved methods for identifying clause pairs, for example methods that make use of labeled training examples." ></td>
	<td class="line x" title="150:291	An AEP can then be extracted from each clause pair." ></td>
	<td class="line x" title="151:291	The EP for the English clause is first extracted, giving values for all variables except for SUBJECT, OBJECT, and MOD(1), . . ., MOD(n)." ></td>
	<td class="line x" title="152:291	The values for the SUBJECT, OBJECT, and MOD(i) variables are derived from the alignments between NPs/PPs, and an alignment of other clauses (ADVPs, ADJPs, etc)." ></td>
	<td class="line x" title="153:291	derived from GIZA++ alignments." ></td>
	<td class="line x" title="154:291	If the English clause has a subject or object which is not aligned to a German modifier, then the value for SUBJECT or OBJECT is taken to be the full English string." ></td>
	<td class="line x" title="155:291	5 The Model 5.1 Beam search and the perceptron In this section we describe linear history-based models with beam search, and the perceptron algorithm for learning in these models." ></td>
	<td class="line x" title="156:291	These methods will form the basis for our model that maps German clauses to AEPs." ></td>
	<td class="line x" title="157:291	We have a training set of n examples, (xi,yi) for i = 1n, where each xi is a German parse tree, and each yi is an AEP." ></td>
	<td class="line x" title="158:291	We follow previous work on history-based models, by representing each yi as a series of N decisions d1,d2,dN." ></td>
	<td class="line x" title="159:291	In our approach, N will be a fixed number for any input x: we take the N decisions to correspond to the sequence of variables STEM, SPINE,  , MOD(1), MOD(2),  , MOD(n) described in section 3." ></td>
	<td class="line x" title="160:291	Each di is a member of a set Di which specifies the set of allowable decisions at the ith point (for example, D2 would be the set of all possible values for SPINE)." ></td>
	<td class="line x" title="161:291	We assume a function ADVANCE(x,d1,d2,,di1) which maps an input x together with a prefix of decisions d1 di1 to a subset ofDi." ></td>
	<td class="line x" title="162:291	ADVANCE is a function that specifies which decisions are allowable for a past history d1,,di1 and an input x. In our case the ADVANCE function implements hard constraints on AEPs (for example, the constraint that the SUBJECT variable must be NULL if no subject position exists in the SPINE)." ></td>
	<td class="line x" title="163:291	For any input x, a well-formed decision sequence for x is a sequence d1,,dN such that for i = 1n, di  ADVANCE(x,d1,,di1)." ></td>
	<td class="line x" title="164:291	We define GEN(x) to be the set of all decision sequences (or AEPs) which are well-formed for x. The model that we will use is a discriminatively-trained, feature-based model." ></td>
	<td class="line x" title="165:291	A significant advantage to feature-based models is their flexibility: it is very easy to sensitize the model to dependencies in the data by encoding new features." ></td>
	<td class="line x" title="166:291	To define a feature-based model, we assume a function (x,d1,,di1,di)  Rd which maps a decision di in context (x,d1,,di1) to a feature vector." ></td>
	<td class="line x" title="167:291	We also assume a vector  Rd of parameter values." ></td>
	<td class="line x" title="168:291	We define the score for any partial or complete decision sequence y = d1,d2,,dm paired with x as: SCORE(x,y) = (x,y)  (1) where (x,y) = summationtextmi=1 (x,d1,,di1,di)." ></td>
	<td class="line x" title="169:291	In particular, given the definitions above, the output structure F(x) for an input x is the highest scoring wellformed structure for x: F(x) = arg max yGEN(x) SCORE(x,y) (2) To decode with the model we use a beam-search method." ></td>
	<td class="line x" title="170:291	The method incrementally builds an AEP in the decision order d1,d2,,dN." ></td>
	<td class="line x" title="171:291	At each point, a beam contains the top M highestscoring partial paths for the first m decisions, where M is taken to be a fixed number." ></td>
	<td class="line x" title="172:291	The score for any partial path is defined in Eq." ></td>
	<td class="line x" title="173:291	1." ></td>
	<td class="line x" title="174:291	The ADVANCE function is used to specify the set of possible decisions that can extend any given path in the beam." ></td>
	<td class="line x" title="175:291	To train the model, we use the averaged perceptron algorithm described by Collins (2002)." ></td>
	<td class="line pc" title="176:291	This combination of the perceptron algorithm with beam-search is similar to that described by Collins and Roark (2004).5 The perceptron algorithm is a convenient choice because it converges quickly  usually taking only a few iterations over the training set (Collins, 2002; Collins and Roark, 2004)." ></td>
	<td class="line x" title="177:291	5.2 The Features of the Model The models features allow it to capture dependencies between the AEP and the German clause, as well as dependencies between different parts of the AEP itself." ></td>
	<td class="line x" title="178:291	The features included in  5Future work may consider alternative algorithms, such as those described by Daume and Marcu (2005)." ></td>
	<td class="line x" title="179:291	237 1 main verb 2 any verb in the clause 3 all verbs, in sequence 4 spine 5 tree 6 preterminal label of left-most child of subject 7 terminal label of left-most child of subject 8 suffix of terminal label of right-most child of subject 9 preterminal label of left-most child of object 10 terminal label of left-most child of object 11 suffix of terminal label of right-most child of object 12 preterminal label of the negation word nicht (not) 13 is either of the strings es gibt (there is/are) or es gab (there was/were) present?" ></td>
	<td class="line x" title="180:291	14 complementizers and wh-words 15 labels of all wh-nonterminals 16 terminal labels of all wh-words 17 preterminal label of a verb in first position 18 terminal label of a verb in first position 19 terminal labels of all words in any relative pronoun under a PP 20 are all of the verbs at the end?" ></td>
	<td class="line x" title="181:291	21 nonterminal label of the root of the tree 22 terminal labels of all words constituting the subject 23 terminal labels of all words constituting the object 24 the leaves dominated by each node in the tree 25 each node in the context of a CFG rule 26 each node in the context of the RHS of a CFG rule 27 each node with its left and right sibling 28 the number of leaves dominated by each node in the tree Table 1: Functions of the German clause used for making features in the AEP prediction model." ></td>
	<td class="line x" title="182:291	can consist of any function of the decision history d1,,di1, the current decision di, or the German clause." ></td>
	<td class="line x" title="183:291	In defining features over AEP/clause pairs, we make use of some basic functions which look at the German clause and the AEP (see Tables 1 and 2)." ></td>
	<td class="line x" title="184:291	We use various combinations of these basic functions in the prediction of each decision di, as described below." ></td>
	<td class="line x" title="185:291	STEM: Features for the prediction of STEM conjoin the value of this variable with each of the functions in lines 113 of Table 1." ></td>
	<td class="line x" title="186:291	For example, one feature is the value of STEM conjoined with the main verb of the German clause." ></td>
	<td class="line x" title="187:291	In addition,  includes features sensitive to the rank of a candidate stem in an externally-compiled lexicon.6 SPINE: Spine prediction features make use of the values of the variables SPINE and STEM from the AEP, as well as functions of the spine in lines 17 of Table 2, conjoined in various ways with the functions in lines 4, 12, and 1421 of Table 1." ></td>
	<td class="line x" title="188:291	Note that the functions in Table 2 allow us to look 6The lexicon is derived from GIZA++ and provides, for a large number of German main verbs, a ranked list of possible English translations." ></td>
	<td class="line x" title="189:291	1 does the SPINE have a subject?" ></td>
	<td class="line x" title="190:291	2 does the SPINE have an object?" ></td>
	<td class="line x" title="191:291	3 does the SPINE have any wh-words?" ></td>
	<td class="line x" title="192:291	4 the labels of any complementizer nonterminals in the SPINE 5 the labels of any wh-nonterminals in the SPINE 6 the nonterminal labels SQ or SBARQ in the SPINE 7 the nonterminal label of the root of the SPINE 8 the grammatical category of the finite verbal form INFL (i.e. , infinitive, 1st-, 2nd-, or 3rd-person pres, pres participle, sing past, plur past, past participle) Table 2: Functions of the English AEP used for making features in the AEP prediction model." ></td>
	<td class="line x" title="193:291	at substructure in the spine." ></td>
	<td class="line x" title="194:291	For instance, one of the features for SPINE is the label SBARQ or SQ, if it exists in the candidate spine, conjoined with a verbal preterminal label if there is a verb in the first position of the German clause." ></td>
	<td class="line x" title="195:291	This feature captures the fact that German yes/no questions begin with a verb in the first position." ></td>
	<td class="line x" title="196:291	VOICE: Voice features in general combine values of VOICE, SPINE, and STEM, with the functions in lines 15, 22, and 23 of Table 1." ></td>
	<td class="line x" title="197:291	SUBJECT: Features used for subject prediction make use of the AEP variables VOICE and STEM." ></td>
	<td class="line x" title="198:291	In addition, if the value of SUBJECT is an index i (see section 3), then  looks at the nonterminal label of the German node indexed by i as well as the surrounding context in the German clausal tree." ></td>
	<td class="line x" title="199:291	Otherwise,  looks at the value ofSUBJECT." ></td>
	<td class="line x" title="200:291	These basic features are combined with the functions in lines 1, 3, and 2427 of Table 1." ></td>
	<td class="line x" title="201:291	OBJECT: We make similar features to those for the prediction of SUBJECT." ></td>
	<td class="line x" title="202:291	In addition,  can look at the value predicted for SUBJECT." ></td>
	<td class="line x" title="203:291	WH: Features for WH look at the values of WH and SPINE, conjoined with the functions in lines 1, 15, and 19 of Table 1." ></td>
	<td class="line x" title="204:291	MODALS: For the prediction of MODALS,  looks at MODALS, SPINE, and STEM, conjoined with the functions in lines 25 and 12 of Table 1." ></td>
	<td class="line x" title="205:291	INFL: The features for INFL include the values of INFL, MODALS, and SUBJECT, and VOICE, and the function in line 8 of Table 2." ></td>
	<td class="line x" title="206:291	MOD(i): For the MOD(i) variables,  looks at the value of MODALS, SPINE and the current MOD(i), as well as the nonterminal label of the root node of the German modifier being placed, and the functions in lines 24 and 28 of Table 1." ></td>
	<td class="line x" title="207:291	238 6 Deriving Full Translations As we described in section 1.1, the translation of a full German sentence proceeds in a series of steps: a German parse tree is broken into a sequence of clauses; each clause is individually translated; and finally, the clause-level translations are combined to form the translation for a full sentence." ></td>
	<td class="line x" title="208:291	The first and last steps are relatively straightforward." ></td>
	<td class="line x" title="209:291	We now show how the second step is achievedi.e. , how AEPs can be used to derive English clause translations from German clauses." ></td>
	<td class="line x" title="210:291	We will again use the following translation pair as an example: da das haupthemmnis der vorhersehbare widerstand der hersteller war./that the main obstacle has been the predictable resistance of manufacturers." ></td>
	<td class="line x" title="211:291	First, an AEP like the one at the top of Figure 2 is predicted." ></td>
	<td class="line x" title="212:291	Then, for each German modifier which does not have the value deleted, an English translation is predicted." ></td>
	<td class="line x" title="213:291	In the example, the modifiers das haupthemmnis and der vorhersehbare widerstand der hersteller would be translated to the main obstacle, and the predictable resistance of manufacturers, respectively." ></td>
	<td class="line x" title="214:291	A number of methods could be used for translation of the modifiers." ></td>
	<td class="line x" title="215:291	In this paper, we use the phrase-based system of Koehn et al.(2003) to generate n-best translations for each of the modifiers, and we then use a discriminative reranking algorithm (Bartlett et al. , 2004) to choose between these modifiers." ></td>
	<td class="line x" title="217:291	The features in the reranking model can be sensitive to various properties of the candidate English translation, for example the words, the part-of-speech sequence or the parse tree for the string." ></td>
	<td class="line x" title="218:291	The reranker can also take into account the original German string." ></td>
	<td class="line x" title="219:291	Finally, the features can be sensitive to properties of the AEP, such as the main verb or the position in which the modifier appears (e.g. , subject, object, pre-sub, post-verb, etc)." ></td>
	<td class="line x" title="220:291	in the English clause." ></td>
	<td class="line x" title="221:291	See Appendix B for a full description of the features used in the modifier translation model." ></td>
	<td class="line x" title="222:291	Note that the reranking stage allows us to filter translation candidates which do not fit syntactically with the position in the English tree." ></td>
	<td class="line x" title="223:291	For example, we can parse the members of the n-best list, and then learn a feature which strongly disprefers prepositional phrases if the modifier appears in subject position." ></td>
	<td class="line x" title="224:291	Finally, the full string is predicted." ></td>
	<td class="line x" title="225:291	In our example, the AEP variables SPINE, MODALS, and INFL in Figure 2 give the ordering <that SUBJECT has been OBJECT>." ></td>
	<td class="line x" title="226:291	The AEP and modifier translations would be combined to give the final English string." ></td>
	<td class="line x" title="227:291	In general, any modifiers assigned to pre-sub, post-sub, in-modals or post-verb are placed in the corresponding position within the spine." ></td>
	<td class="line x" title="228:291	For example, the second AEP in Figure 2 has a spine with ordering <SUBJECT are OBJECT>; modifiers 1 and 2 would be placed in positions pre-sub and post-verb, giving the ordering <MOD2 SUBJECT are OBJECT MOD1>." ></td>
	<td class="line x" title="229:291	Note that modifiers assigned post-verb are placed after the object." ></td>
	<td class="line x" title="230:291	If multiple modifiers appear in the same position (e.g. , post-verb), then they are placed in the order seen in the original German clause." ></td>
	<td class="line x" title="231:291	7 Experiments We applied the approach to translation from German to English, using the Europarl corpus (Koehn, 2005) for our training data." ></td>
	<td class="line x" title="232:291	This corpus contains over 750,000 training sentences; we extracted over 441,000 training examples for the AEP model from this corpus, using the method described in section 4." ></td>
	<td class="line x" title="233:291	We reserved 35,000 of these training examples as development data for the model." ></td>
	<td class="line x" title="234:291	We used a set of features derived from the those described in section 5.2." ></td>
	<td class="line x" title="235:291	This set was optimized using the development data through experimentation with several different feature subsets." ></td>
	<td class="line x" title="236:291	Modifiers within German clauses were translated using the phrase-based model of Koehn et al.(2003)." ></td>
	<td class="line x" title="238:291	We first generated n-best lists for each modifier." ></td>
	<td class="line x" title="239:291	We then built a reranking modelsee section 6to choose between the elements in the n-best lists." ></td>
	<td class="line x" title="240:291	The reranker was trained using around 800 labeled examples from a development set." ></td>
	<td class="line x" title="241:291	The test data for the experiments consisted of 2,000 sentences, and was the same test set as that used by Collins et al.(2005)." ></td>
	<td class="line x" title="243:291	We use the model of Koehn et al.(2003) as a baseline for our experiments." ></td>
	<td class="line x" title="245:291	The AEP-driven model was used to translate all test set sentences where all clauses within the German parse tree contained at least one verb and there was no embedding of clauses there were 1,335 sentences which met these criteria." ></td>
	<td class="line x" title="246:291	The remaining 665 sentences were translated with the baseline system." ></td>
	<td class="line x" title="247:291	This set of 2,000 translations had a BLEU score of 23.96." ></td>
	<td class="line x" title="248:291	The baseline system alone achieved a BLEU score of 25.26 on the same set of 2,000 test sentences." ></td>
	<td class="line x" title="249:291	We also obtained judgments from two human annotators on 239 100 randomly-drawn sentences on which the baseline and AEP-based outputs differed." ></td>
	<td class="line x" title="250:291	For each example the annotator viewed the reference translation, together with the two systems translations presented in a random order." ></td>
	<td class="line x" title="251:291	Annotator 1 judged 62 translations to be equal in quality, 16 translations to be better under the AEP system, and 22 to be better for the baseline system." ></td>
	<td class="line x" title="252:291	Annotator 2 judged 37 translations to be equal in quality, 32 to be better under the baseline, and 31 to be better under the AEP-based system." ></td>
	<td class="line x" title="253:291	8 Conclusions and Future Work We have presented an approach to tree-totree based translation which models a new representationaligned extended projections within a discriminative, feature-based framework." ></td>
	<td class="line x" title="254:291	Our model makes use of an explicit representation of syntax in the target language, together with constraints on the alignments between source and target parse trees." ></td>
	<td class="line x" title="255:291	The current system presents many opportunities for future work." ></td>
	<td class="line x" title="256:291	For example, improvement in accuracy may come from a tighter integration of modifier translation into the overall translation process." ></td>
	<td class="line x" title="257:291	The current method using an n-best reranking model to select the best candidatechooses each modifier independently and then places it into the translation." ></td>
	<td class="line x" title="258:291	We intend to explore an alternative method that combines finite-state machines representing the n-best output from the phrase-based system with finitestate machines representing the complementizers, verbs, modals, and other substrings of the translation derived from the AEP." ></td>
	<td class="line x" title="259:291	Selecting modifiers using this representation would correspond to searching the finite-state network for the most likely path." ></td>
	<td class="line x" title="260:291	A finite-state representation has many advantages, including the ability to easily incorporate an n-gram language model." ></td>
	<td class="line x" title="261:291	Future work may also consider expanded definitions of AEPs." ></td>
	<td class="line x" title="262:291	For example, we might consider AEPs that include larger chunks of phrase structure, or we might consider AEPs that contain more detailed information about the relative ordering of modifiers." ></td>
	<td class="line x" title="263:291	There is certainly room for improvement in the accuracy with which AEPs are predicted in our data; the feature-driven approach allows a wide range of features to be tested." ></td>
	<td class="line x" title="264:291	For example, it would be relatively easy to incorporate a syntactic language model (i.e. , a prior distribution over AEP structures) induced from a large amount of English monolingual data." ></td>
	<td class="line x" title="265:291	Appendix A: Identification of Clauses In the English parse trees, we identify clauses as follows." ></td>
	<td class="line x" title="266:291	Any non-terminal labeled by the parser of (Collins, 1999) as SBAR or SBAR-A is labeled as a clause root." ></td>
	<td class="line x" title="267:291	Any node labeled by the parser as S or S-A is also labeled as the root of a clause, unless it is directly dominated by a non-terminal labeled SBAR or SBAR-A." ></td>
	<td class="line x" title="268:291	Any node labeled SG or SG-A by the parser is labeled as a clause root, unless (1) the node is directly dominated by SBAR or SBAR-A; or (2) the node is directly dominated by a VP, and the node is directly preceded by a verb (POS tag beginning withV) or modal (POS tag beginning with M)." ></td>
	<td class="line x" title="269:291	Any node labeled VP is marked as a clause root if (1) the node is not directly dominated by a VP, S, S-A, SBAR, SBAR-A, SG, or SG-A; or (2) the node is directly preceded by a coordinating conjunction (i.e. , a POS tag labeled as CC)." ></td>
	<td class="line x" title="270:291	In German parse trees, we identify any nodes labeled as S or CS as clause roots." ></td>
	<td class="line x" title="271:291	In addition, we mark any node labeled as VP as a clause root, provided that (1) it is preceded by a coordinating conjunction, i.e., a POS tag labeled as KON; or (2) it has one of the functional tags -mo, -re or -sb." ></td>
	<td class="line x" title="272:291	Appendix B: Reranking Modifier Translations The n-best reranking model for the translation of modifiers considers a list of candidate translations." ></td>
	<td class="line x" title="273:291	We hand-labeled 800 examples, marking the element in each list that would lead to the best translation." ></td>
	<td class="line x" title="274:291	The features of the n-best reranking algorithm are combinations of the basic features in Tables 3 and 4." ></td>
	<td class="line x" title="275:291	Each list contained the n-best translations produced by the phrase-based system of Koehn et al.(2003)." ></td>
	<td class="line x" title="277:291	The lists also contained a supplementary candidate DELETED, signifying that the modifier should be deleted from the English translation." ></td>
	<td class="line x" title="278:291	In addition, each candidate derived from the phrase-based system contributed one new candidate to the list signifying that the first word of the candidate should be deleted." ></td>
	<td class="line x" title="279:291	These additional candidates were motivated by our observation that the optimal candidate in the n-best list produced by the phrase-based system often included an unwanted preposition at the beginning of the string." ></td>
	<td class="line x" title="280:291	240 1 candidate string 2 should the first word of the candidate be deleted?" ></td>
	<td class="line x" title="281:291	3 POS tag of first word of candidate 4 POS tag of last word of candidate 5 top nonterminal of parse of candidate 6 modifier deleted from English translation?" ></td>
	<td class="line x" title="282:291	7 first candidate on n-best list 8 first word of candidate 9 last word of candidate 10 rank of candidate in n-best list 11 is there punctuation at the beginning, middle, or end of the string?" ></td>
	<td class="line x" title="283:291	12 if the first word of the candidate should be deleted, what is the string that is deleted?" ></td>
	<td class="line x" title="284:291	13 if the first word of the candidate should be deleted, what is the POS tag of the word that is deleted?" ></td>
	<td class="line x" title="285:291	Table 3: Functions of the candidate modifier translations used for making features in the n-best reranking model." ></td>
	<td class="line x" title="286:291	1 the position of the modifier (04) in AEP 2 main verb 3 voice 4 subject prediction 5 German input string Table 4: Functions of the German input string and predicted AEP output used for making features in the n-best reranking model." ></td>
	<td class="line x" title="287:291	Acknowledgements We would like to thank Luke Zettlemoyer, Regina Barzilay, Ed Filisko, and Ben Snyder for their valuable comments and help during the writing of this paper." ></td>
	<td class="line x" title="288:291	Thanks also to Jason Rennie and John Barnett for providing human judgments of the translation output." ></td>
	<td class="line x" title="289:291	This work was funded by NSF grants IIS-0347631, IIS-0415030, and DMS-0434222, as well as a grant from NTT, Agmt." ></td>
	<td class="line x" title="290:291	Dtd." ></td>
	<td class="line x" title="291:291	6/21/1998." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-2936
Maximum Spanning Tree Algorithm For Non-Projective Labeled Dependency Parsing
Shimizu, Nobuyuki;"></td>
	<td class="line x" title="1:98	Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), pages 236240, New York City, June 2006." ></td>
	<td class="line x" title="2:98	c2006 Association for Computational Linguistics Maximum Spanning Tree Algorithm for Non-projective Labeled Dependency Parsing Nobuyuki Shimizu Dept. of Computer Science State University of New York at Albany Albany, NY, 12222, USA shimizu@cs.albany.edu Abstract Following (McDonald et al. , 2005), we present an application of a maximum spanning tree algorithm for a directed graph to non-projective labeled dependency parsing." ></td>
	<td class="line oc" title="3:98	Using a variant of the voted perceptron (Collins, 2002; Collins and Roark, 2004; Crammer and Singer, 2003), we discriminatively trained our parser in an on-line fashion." ></td>
	<td class="line x" title="4:98	After just one epoch of training, we were generally able to attain average results in the CoNLL 2006 Shared Task." ></td>
	<td class="line x" title="5:98	1 Introduction Recently, we have seen dependency parsing grow more popular." ></td>
	<td class="line x" title="6:98	It is not rare to see dependency relations used as features, in tasks such as relation extraction (Bunescu and Mooney, 2005) and machine translation (Ding and Palmer, 2005)." ></td>
	<td class="line x" title="7:98	Although English dependency relations are mostly projective, in other languages with more flexible word order, such as Czech, non-projective dependencies are more frequent." ></td>
	<td class="line x" title="8:98	There are generally two methods for learning non-projective dependencies." ></td>
	<td class="line x" title="9:98	You could map a non-projective dependency tree to a projective one, learn and predict the tree, then bring it back to the non-projective dependency tree (Nivre and Nilsson, 2005)." ></td>
	<td class="line x" title="10:98	Non-projective dependency parsing can also be represented as search for a maximum spanning tree in a directed graph, and this technique has been shown to perform well in Czech (McDonald et al. , 2005)." ></td>
	<td class="line x" title="11:98	In this paper, we investigate the effectiveness of (McDonald et al. , 2005) in the various languages given by the CoNLL 2006 shared task for non-projective labeled dependency parsing." ></td>
	<td class="line x" title="12:98	The paper is structured as follows: in section 2 and 3, we review the decoding and learning aspects of (McDonald et al. , 2005), and in section 4, we describe the extension of the algorithm and the features needed for the CoNLL 2006 shared task." ></td>
	<td class="line x" title="13:98	2 Non-Projective Dependency Parsing 2.1 Dependency Structure Let us define x to be a generic sequence of input tokens together with their POS tags and other morphological features, and y to be a generic dependency structure, that is, a set of edges for x. We use the terminology in (Taskar et al. , 2004) for a generic structured output prediction, and define a part." ></td>
	<td class="line x" title="14:98	A part represents an edge together with its label." ></td>
	<td class="line x" title="15:98	A part is a tuple DEPREL,i,j where i is the start point of the edge, j is the end point, and DEPREL is the label of the edge." ></td>
	<td class="line x" title="16:98	The token at i is the head of the token at j. Table 1 shows our formulation of building a nonprojective dependency tree as a prediction problem." ></td>
	<td class="line x" title="17:98	The task is to predict y, the set of parts (column 3, Table 1), given x, the input tokens and their features (column 1 and 2, Table 1)." ></td>
	<td class="line x" title="18:98	In this paper we use the common method of factoring the score of the dependency structure as the sum of the scores of all the parts." ></td>
	<td class="line x" title="19:98	A dependency structure is characterized by its features, and for each feature, we have a correspond236 Token POS Edge Part John NN SUBJ,2,1 saw VBD PRED,0,2 a DT DET,4,3 dog NN OBJ,2,4 yesterday RB ADJU,2,5 which WDT MODWH,7,6 was VBD MODPRED,4,7 a DT DET,10,8 Yorkshire NN MODN,10,9 Terrier NN OBJ,7,10." ></td>
	<td class="line x" title="20:98	.,10,11 Table 1: Example Parts ing weight." ></td>
	<td class="line x" title="22:98	The score of a dependency structure is the sum of these weights." ></td>
	<td class="line x" title="23:98	Now, the dependency structures are factored by the parts, so that each feature is some type of a specialization of a part." ></td>
	<td class="line x" title="24:98	Each part in a dependency structure maps to several features." ></td>
	<td class="line x" title="25:98	If we sum up the weights for these features, we have the score for the part, and if we sum up the scores of the parts, we have the score for the dependency structure." ></td>
	<td class="line x" title="26:98	For example, let us say we would like to find the score of the part OBJ,2,4." ></td>
	<td class="line x" title="27:98	This is the edge going to the 4th token dog in Table 1." ></td>
	<td class="line x" title="28:98	Suppose there are two features for this part." ></td>
	<td class="line x" title="29:98	 There is an edge labeled with OBJ that points to the right." ></td>
	<td class="line x" title="30:98	( = DEPREL, dir(i,j) )  There is an edge labeled with OBJ starting at the token saw which points to the right." ></td>
	<td class="line x" title="31:98	( = DEPREL, dir(i,j), wordi ) If a statement is never true during the training, the weight for it will be 0." ></td>
	<td class="line x" title="32:98	Otherwise there will be a positive weight value." ></td>
	<td class="line x" title="33:98	The score will be the sum of all the weights of the features given by the part." ></td>
	<td class="line x" title="34:98	In the upcoming section, we explain a decoding algorithm for the dependency structures, and later we give a method for learning the weight vector used in the decoding." ></td>
	<td class="line x" title="35:98	2.2 Maximum Spanning Tree Algorithm As in (McDonald et al. , 2005), the decoding algorithm we used is the Chu-Liu-Edmonds (CLE) algorithm (Chu and Liu, 1965; Edmonds, 1967) for finding the Maximum Spanning Tree in a directed graph." ></td>
	<td class="line x" title="36:98	The following is a nice summary by (McDonald et al. , 2005)." ></td>
	<td class="line x" title="37:98	Informally, the algorithm has each vertex in the graph greedily select the incoming edge with highest weight." ></td>
	<td class="line x" title="38:98	Note that the edge is coming from the parent to the child." ></td>
	<td class="line x" title="39:98	This means that given a child node wordj, we are finding the parent, or the head wordi such that the edge (i,j) has the highest weight among all i, i negationslash= j. If a tree results, then this must be the maximum spanning tree." ></td>
	<td class="line x" title="40:98	If not, there must be a cycle." ></td>
	<td class="line x" title="41:98	The procedure identifies a cycle and contracts it into a single vertex and recalculates edge weights going into and out of the cycle." ></td>
	<td class="line x" title="42:98	It can be shown that a maximum spanning tree on the contracted graph is equivalent to a maximum spanning tree in the original graph (Leonidas, 2003)." ></td>
	<td class="line x" title="43:98	Hence the algorithm can recursively call itself on the new graph." ></td>
	<td class="line oc" title="44:98	3 Online Learning Again following (McDonald et al. , 2005), we have used the single best MIRA (Crammer and Singer, 2003), which is a variant of the voted perceptron (Collins, 2002; Collins and Roark, 2004) for structured prediction." ></td>
	<td class="line x" title="45:98	In short, the update is executed when the decoder fails to predict the correct parse, and we compare the correct parse yt and the incorrect parse y suggested by the decoding algorithm." ></td>
	<td class="line x" title="46:98	The weights of the features in y will be lowered, and the weights of the features in yt will be increased accordingly." ></td>
	<td class="line x" title="47:98	4 Experiments Our experiments were conducted on CoNLL-X shared task, with various datasets (Hajic et al. , 2004; Simov et al. , 2005; Simov and Osenova, 2003; Chen et al. , 2003; Bohmova et al. , 2003; Kromann, 2003; van der Beek et al. , 2002; Brants et al. , 2002; Kawata and Bartels, 2000; Afonso et al. , 2002; Dzeroski et al. , 2006; Civit Torruella and Mart Antonn, 2002; Nilsson et al. , 2005; Oflazer et al. , 2003; Atalay et al. , 2003) . 4.1 Dependency Relation The CLE algorithm works on a directed graph with unlabeled edges." ></td>
	<td class="line x" title="48:98	Since the CoNLL-X shared task 237 Given a part DEPREL,i,j DEPREL, dir(i,j) DEPREL, dir(i,j), wordi DEPREL, dir(i,j), posi DEPREL, dir(i,j), wordj DEPREL, dir(i,j), posj DEPREL, dir(i,j), wordi, posi DEPREL, dir(i,j), wordj, posj DEPREL, dir(i,j), wordi1 DEPREL, dir(i,j), posi1 DEPREL, dir(i,j), wordi1, posi1 DEPREL, dir(i,j), wordj1 DEPREL, dir(i,j), posj1 DEPREL, dir(i,j), wordj1, posj1 DEPREL, dir(i,j), wordi+1 DEPREL, dir(i,j), posi+1 DEPREL, dir(i,j), wordi+1, posi+1 DEPREL, dir(i,j), wordj+1 DEPREL, dir(i,j), posj+1 DEPREL, dir(i,j), wordj+1, posj+1 DEPREL, dir(i,j), posi2 DEPREL, dir(i,j), posi+2 DEPREL, dir(i,j), distance = |j  i| additional features DEPREL, dir(i,j), wordi, wordj DEPREL, dir(i,j), posi+1, posi, posi+1 DEPREL, dir(i,j), posi+1, wordi, posi+1 DEPREL, dir(i,j), wordi, posi, posj DEPREL, dir(i,j), posi, wordj, posj Table 2: Binary Features for Each Part requires the labeling of edges, as a preprocessing stage, we created a directed complete graph without multi-edges, that is, given two distinct nodes i and j, exactly two edges exist between them, one from i to j, and the other from j to i. There is no self-pointing edge." ></td>
	<td class="line x" title="49:98	Then we labeled each edge with the highest scoring dependency relation." ></td>
	<td class="line x" title="50:98	This complete graph was given to the CLE algorithm and the edge labels were never altered in the course of finding the maximum spanning tree." ></td>
	<td class="line x" title="51:98	The result is the non-projective dependency tree with labeled edges." ></td>
	<td class="line x" title="52:98	4.2 Features The features we used to score each part (edge) DEPREL,i,j are shown in Table 2." ></td>
	<td class="line x" title="53:98	The index i is the position of the parent and j is that of the child." ></td>
	<td class="line x" title="54:98	wordj = the word token at the position j. posj = the coarse part-of-speech at j. dir(i,j) = R if i < j, and L otherwise." ></td>
	<td class="line x" title="55:98	No other features were used beyond the combinations of the CPOS tag and the word token in Table 2." ></td>
	<td class="line x" title="56:98	We have evaluated our parser on Arabic, Danish, Slovene, Spanish, Turkish and Swedish, and used the additional features listed in Table 2 for all languages except for Danish and Swedish." ></td>
	<td class="line x" title="57:98	The reason for this is simply that the model with the additional features did not fit in the 4 GB of memory used in the training." ></td>
	<td class="line x" title="58:98	Although we could do batch learning by running the online algorithm multiple times, we run the online algorithm just once." ></td>
	<td class="line x" title="59:98	The hardware used is an Intel Pentinum D at 3.0 Ghz with 4 GB of memory, and the software was written in C++." ></td>
	<td class="line x" title="60:98	The training time required was Arabic 204 min, Slovene 87 min, Spanish 413 min, Swedish 1192 min, Turkish 410 min, Danish 381 min." ></td>
	<td class="line x" title="61:98	5 Results The results are shown in Table 3." ></td>
	<td class="line x" title="62:98	Although our feature set is very simple, the results were around the averages." ></td>
	<td class="line x" title="63:98	We will do error analysis of three notable languages: Arabic, Swedish and Turkish." ></td>
	<td class="line x" title="64:98	5.1 Arabic Of 4990 words in the test set, 800 are prepositions." ></td>
	<td class="line x" title="65:98	The prepositions are the most frequently found tokens after nouns in this set." ></td>
	<td class="line x" title="66:98	On the other hand, our head attachment error was 44% for prepositions." ></td>
	<td class="line x" title="67:98	Given the relatively large number of prepositions found in the test set, it is important to get the preposition attachment right to achieve a higher mark in this language." ></td>
	<td class="line x" title="68:98	The obvious solution is to have a feature that connects the head of a preposition to the child of the preposition." ></td>
	<td class="line x" title="69:98	However, such a feature effects the edge based factoring and the decoding algorithm, and we will be forced to modify the MST algorithm in some ways." ></td>
	<td class="line x" title="70:98	5.2 Swedish Due to the memory constraint on the computer, we did not use the additional features for Swedish and our feature heavily relied on the CPOS tag." ></td>
	<td class="line x" title="71:98	At the same time, we have noticed that relatively higher performance of our parser compared to the average coincides with the bigger tag set for CPOS for this corpus." ></td>
	<td class="line x" title="72:98	This suggests that we should be using more fine grained POS in other languages." ></td>
	<td class="line x" title="73:98	5.3 Turkish The difficulty with parsing Turkish stems from the large unlabeled attachment error rate on the nouns 238 Language LAS AV SD Arabic 62.83% 59.92% 6.53 Danish 75.81% 78.31% 5.45 Slovene 64.57% 65.61% 6.78 Spanish 73.17% 73.52% 8.41 Swedish 79.49% 76.44% 6.46 Turkish 54.23% 55.95% 7.71 Language UAS AV SD Arabic 74.27% 73.48% 4.94 Danish 81.72% 84.52% 4.29 Slovene 74.88% 76.53% 4.67 Spanish 77.58% 77.76% 7.81 Swedish 86.62% 84.21% 5.45 Turkish 68.77% 69.35% 5.51 Table 3: Labeled and Unlabeled Attachment Score (39%)." ></td>
	<td class="line x" title="74:98	Since the nouns are the most frequently occurring words in the test set (2209 out of 5021 total), this seems to make Turkish the most challenging language for any system in the shared task." ></td>
	<td class="line x" title="75:98	On the average, there are 1.8 or so verbs per sentence, and nouns have a difficult time attaching to the correct verb or postposition." ></td>
	<td class="line x" title="76:98	This, we think, indicates that there are morphological features or word ordering features that we really need in order to disambiguate them." ></td>
	<td class="line x" title="77:98	6 Future Work As well as making use of fine-grained POS tags and other morphological features, given the error analysis on Arabic, we would like to add features that are dependent on two or more edges." ></td>
	<td class="line x" title="78:98	6.1 Bottom-Up Non-Projective Parsing In order to incorporate features which depend on other edges, we propose Bottom-Up Non-Projective Parsing." ></td>
	<td class="line x" title="79:98	It is often the case that dependency relations can be ordered by how close one relation is to the root of dependency tree." ></td>
	<td class="line x" title="80:98	For example, the dependency relation between a determiner and a noun should be decided before that between a preposition and a noun, and that of a verb and a preposition, and so on." ></td>
	<td class="line x" title="81:98	We can use this information to do bottom-up parsing." ></td>
	<td class="line x" title="82:98	Suppose all words have a POS tag assigned to them, and every edge labeled with a dependency relation is attached to a specific POS tag at the end point." ></td>
	<td class="line x" title="83:98	Also assume that there is an ordering of POS tags such that the edge going to the POS tag needs be decided before other edges." ></td>
	<td class="line x" title="84:98	For example, (1) determiner, (2) noun, (3) preposition, (4) verb would be one such ordering." ></td>
	<td class="line x" title="85:98	We propose the following algorithm:  Assume we have tokens as nodes in a graph and no edges are present at first." ></td>
	<td class="line x" title="86:98	For example, we have tokens I, ate, with, a, spoon, and no edges between them." ></td>
	<td class="line x" title="87:98	 Take the POS tag that needs to be decided next." ></td>
	<td class="line x" title="88:98	Find all edges that go to each token labeled with this POS tag, and put them in the graph." ></td>
	<td class="line x" title="89:98	For example, if the POS is noun, put edges from ate to I, from ate to spoon, from with to I, from with to spoon, from I to spoon, and from spoon to I." ></td>
	<td class="line x" title="90:98	 Run the CLE algorithm on this graph." ></td>
	<td class="line x" title="91:98	This selects the highest incoming edge to each token with the POS tag we are looking at, and remove cycles if any are present." ></td>
	<td class="line x" title="92:98	 Take the resulting forests and for each edge, bring the information on the child node to the parent node." ></td>
	<td class="line x" title="93:98	For example, if this time POS was noun, and there is an edge to a preposition with from a noun spoon, then spoon is absorbed by with." ></td>
	<td class="line x" title="94:98	Note that since no remaining dependency relation will attach to spoon, we can safely ignore spoon from now on." ></td>
	<td class="line x" title="95:98	 Go back and repeat until no POS is remaining and we have a dependency tree." ></td>
	<td class="line x" title="96:98	Now in the next round, when deciding the score of the edge from ate to with, we can use the all information at the token with, including spoon." ></td>
	<td class="line x" title="97:98	7 Conclusion We have extended non-projective unlabeled dependency parsing (McDonald et al. , 2005) to a very simple non-projective labeled dependency and showed that the parser performs reasonably well with small number of features and just one iteration of training." ></td>
	<td class="line x" title="98:98	Based on the analysis of the Arabic parsing results, we have proposed a bottomup non-projective labeled dependency parsing algorithm that allows us to use features dependent on more than one edge, with very little disadvantage compared to the original algorithm." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-3603
Computational Challenges In Parsing By Classification
Turian, Joseph P.;Melamed, I. Dan;"></td>
	<td class="line x" title="1:223	Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 1724, New York City, New York, June 2006." ></td>
	<td class="line x" title="2:223	c2006 Association for Computational Linguistics Computational Challenges in Parsing by Classification Joseph Turian and I. Dan Melamed {lastname}@cs.nyu.edu Computer Science Department New York University New York, New York 10003 Abstract This paper presents a discriminative parser that does not use a generative model in any way, yet whose accuracy still surpasses a generative baseline." ></td>
	<td class="line x" title="3:223	The parser performs feature selection incrementally during training, as opposed to a priori, which enables it to work well with minimal linguistic cleverness." ></td>
	<td class="line x" title="4:223	The main challenge in building this parser was fitting the training data into memory." ></td>
	<td class="line x" title="5:223	We introduce gradient sampling, which increased training speed 100-fold." ></td>
	<td class="line x" title="6:223	Our implementation is freely available at http://nlp.cs.nyu.edu/parser/." ></td>
	<td class="line x" title="7:223	1 Introduction Discriminative machine learning methods have improved accuracy on many NLP tasks, including POS-tagging, shallow parsing, relation extraction, and machine translation." ></td>
	<td class="line x" title="8:223	However, only limited advances have been made on full syntactic constituent parsing." ></td>
	<td class="line pc" title="9:223	Successful discriminative parsers have used generative models to reduce training time and raise accuracy above generative baselines (Collins & Roark, 2004; Henderson, 2004; Taskar et al. , 2004)." ></td>
	<td class="line n" title="10:223	However, relying upon information from a generative model might limit the potential of these approaches to realize the accuracy gains achieved by discriminative methods on other NLP tasks." ></td>
	<td class="line n" title="11:223	Another difficulty is that discriminative parsing approaches can be very task-specific and require quite a bit of trial and error with different hyper-parameter values and types of features." ></td>
	<td class="line x" title="12:223	In the present work, we make progress towards overcoming these obstacles." ></td>
	<td class="line x" title="13:223	We propose a flexible, well-integrated method for training discriminative parsers, demonstrating techniques that might also be useful for other structured learning problems." ></td>
	<td class="line x" title="14:223	The learning algorithm projects the hand-provided atomic features into a compound feature space and performs incremental feature selection from this large feature space." ></td>
	<td class="line x" title="15:223	We achieve higher accuracy than a generative baseline, despite not using the standard trick of including an underlying generative model." ></td>
	<td class="line x" title="16:223	Our training regime does model selection without ad-hoc smoothing or frequency-based feature cutoffs, and requires no heuristics to optimize the single hyper-parameter." ></td>
	<td class="line x" title="17:223	We discuss the computational challenges we overcame to build this parser." ></td>
	<td class="line x" title="18:223	The main difficulty is that the training data fit in memory only using an indirect representation,1 so the most costly operation during training is accessing the features of a particular example." ></td>
	<td class="line x" title="19:223	We show how to train a parser effectively under these conditions." ></td>
	<td class="line x" title="20:223	We also show how to speed up training by using a principled sampling method to estimate the loss gradients used in feature selection." ></td>
	<td class="line x" title="21:223	2 describes the parsing algorithm." ></td>
	<td class="line x" title="22:223	3 presents the learning method and techniques used to reduce training time.4 presents experiments with discriminative parsers built using these methods." ></td>
	<td class="line x" title="23:223	5 dis1Similar memory limitations exist in other large-scale NLP tasks." ></td>
	<td class="line x" title="24:223	Syntax-driven SMT systems are typically trained on an order of magnitude more sentences than English parsers, and unsupervised estimation methods can generate an arbitrary number of negative examples (Smith & Eisner, 2005)." ></td>
	<td class="line x" title="25:223	17 cusses possible issues in scaling to larger example sets." ></td>
	<td class="line x" title="26:223	2 Parsing Algorithm The following terms will help to explain our work." ></td>
	<td class="line x" title="27:223	A span is a range over contiguous words in the input." ></td>
	<td class="line x" title="28:223	Spans cross if they overlap but neither contains the other." ></td>
	<td class="line x" title="29:223	An item is a (span, label) pair." ></td>
	<td class="line x" title="30:223	A state is a partial parse, i.e. a set of items, none of whose spans cross." ></td>
	<td class="line x" title="31:223	A parse inference is a (state, item) pair, i.e. a state and a (consequent) item to be added to it." ></td>
	<td class="line x" title="32:223	The frontier of a state consists of the items with no parents yet." ></td>
	<td class="line x" title="33:223	The children of an inference are the frontier items below the item to be inferred, and the head of an inference is the child item chosen by head rules (Collins, 1999, pp." ></td>
	<td class="line x" title="34:223	238240)." ></td>
	<td class="line x" title="35:223	A parse path is a sequence of parse inferences." ></td>
	<td class="line x" title="36:223	For some input sentence and training parse tree, a state is correct if the parser can infer zero or more additional items to obtain the training parse tree and an inference is correct if it leads to a correct state." ></td>
	<td class="line x" title="37:223	Now, given input sentence s we compute: p = arg min pP(s)    summationdisplay ip l(i)    (1) where P(s) are possible parses of the sentence, and the loss (or cost) l of parse p is summed over the inferences i that lead to the parse." ></td>
	<td class="line x" title="38:223	To find p, the parsing algorithm considers a sequence of states." ></td>
	<td class="line x" title="39:223	The initial state contains terminal items, whose labels are the POS tags given by Ratnaparkhi (1996)." ></td>
	<td class="line x" title="40:223	The parser considers a set of (bottom-up) inferences at each state." ></td>
	<td class="line x" title="41:223	Each inference results in a successor state to be placed on the agenda." ></td>
	<td class="line x" title="42:223	The loss function l can consider arbitrary properties of the input and parse state,2 which precludes a tractable dynamic programming solution to Equation 1." ></td>
	<td class="line x" title="43:223	Therefore, we do standard agenda-based parsing, but instead of items our agenda stores entire states, as per more general best-first search over parsing hypergraphs (Klein & Manning, 2001)." ></td>
	<td class="line x" title="44:223	Each time we pop a state from the agenda, l computes a loss for the bottomup inferences generated from that state." ></td>
	<td class="line x" title="45:223	If the loss of the popped state exceeds that of the current best complete parse, search is done and we have found the optimal parse." ></td>
	<td class="line x" title="46:223	2I.e. we make no context-free assumptions." ></td>
	<td class="line x" title="47:223	3 Training Method 3.1 General Setting From each training inference i  I we generate the tuple X(i), y(i), b(i)." ></td>
	<td class="line x" title="48:223	X(i) is a feature vector describing i, with each element in0, 1}." ></td>
	<td class="line x" title="49:223	The observed y-value y(i) {1,+1} is determined by whether i is a correct inference or not." ></td>
	<td class="line x" title="50:223	Some training examples might be more important than others, so each is given an initial bias b(i)R+." ></td>
	<td class="line x" title="51:223	Our goal during training is to induce a real-valued inference scoring function (hypothesis) h(i;), which is a linear model parameterized by a vector  of reals: h(i;) = X(i) = summationdisplay f f X f (i) (2) Each f is a feature." ></td>
	<td class="line x" title="52:223	The sign of h(i;) predicts the y-value of i and the magnitude gives the confidence in this prediction." ></td>
	<td class="line x" title="53:223	The training procedure optimizes  to minimize the expected risk R: R(I;) = L(I;) +() (3) In principle, L can be any loss function, but in the present work we use the log-loss (Collins et al. , 2002): L(I;) = summationdisplay iI l(i;) = summationdisplay iI b(i)((i;)) (4) where: () = ln(1 + exp()) (5) and the margin of inference i under the current model  is: (i;) = y(i)h(i;) (6) For a particular choice of , l(i) in Equation 1 is computed according to Equation 4 using y(i) = +1 and b(i) = 1." ></td>
	<td class="line x" title="54:223	() in Equation 3 is a regularizer, which penalizes overly complex models to reduce overfitting and generalization error." ></td>
	<td class="line x" title="55:223	We use the lscript1 penalty: () = summationdisplay f |f| (7) where  is the lscript1 parameter that controls the strength of the regularizer." ></td>
	<td class="line x" title="56:223	This choice of objective R is motivated by Ng (2004), who suggests that, given a 18 learning setting where the number of irrelevant features is exponential in the number of training examples, we can nonetheless learn effectively by building decision trees to minimize the lscript1-regularized log-loss." ></td>
	<td class="line x" title="57:223	Conversely, Ng (2004) suggests that most of the learning algorithms commonly used by discriminative parsers will overfit when exponentially many irrelevant features are present.3 Learning over an exponential feature space is the very setting we have in mind." ></td>
	<td class="line x" title="58:223	A priori, we define only a set A of simple atomic features (see 4)." ></td>
	<td class="line x" title="59:223	However, the learner induces compound features, each of which is a conjunction of possibly negated atomic features." ></td>
	<td class="line x" title="60:223	Each atomic feature can have three values (yes/no/dont care), so the size of the compound feature space is 3|A|, exponential in the number of atomic features." ></td>
	<td class="line x" title="61:223	It was also exponential in the number of training examples in our experiments (|A||I|)." ></td>
	<td class="line x" title="62:223	We use an ensemble of confidence-rated decision trees (Schapire & Singer, 1999) to represent h.4 Each node in a decision tree corresponds to a compound feature, and the leaves of the decision trees keep track of the parameter values of the compound features they represent." ></td>
	<td class="line x" title="63:223	To score an inference using a decision tree, we percolate the inference down to a leaf and return that leafs confidence." ></td>
	<td class="line x" title="64:223	The overall score given to an inference by the whole ensemble is the sum of the confidences returned by the trees in the ensemble." ></td>
	<td class="line x" title="65:223	3.2 Boosting lscript1-Regularized Decision Trees Listing 1 presents our training algorithm." ></td>
	<td class="line x" title="66:223	(Sampling will be explained in 3.3." ></td>
	<td class="line x" title="67:223	Until then, assume that the sample S is the entire training set I)." ></td>
	<td class="line x" title="68:223	At the beginning of training, the ensemble is empty,  = 0, and the lscript1 parameter  is set to." ></td>
	<td class="line x" title="69:223	We train until the objective cannot be further reduced for the current choice of ." ></td>
	<td class="line x" title="70:223	We then relax the regularization penalty by decreasing  and continuing training." ></td>
	<td class="line x" title="71:223	We also de3including the following learning algorithms: unregularized logistic regression logistic regression with an lscript2 penalty (i.e. a Gaussian prior) SVMs using most kernels multilayer neural nets trained by backpropagation the perceptron algorithm 4Turian and Melamed (2005) show that that decision trees applied to parsing have higher accuracy and training speed than decision stumps." ></td>
	<td class="line x" title="72:223	Listing 1 Training algorithm." ></td>
	<td class="line x" title="73:223	1: procedure T(I) 2: ensemble 3: h(i)0 for all iI 4: for T = 1do 5: S priority sample I 6: extract X(i) for all iS 7: build decision tree t using S 8: percolate every iI to a leaf node in t 9: for each leaf f in t do 10: choose f to minimize R 11: add f to h(i) for all i in this leaf termine the accuracy of the parser on a held-out development set using the previous  value (before it was decreased), and can stop training when this accuracy plateaus." ></td>
	<td class="line x" title="74:223	In this way, instead of choosing the best  heuristically, we can optimize it during a single training run (Turian & Melamed, 2005)." ></td>
	<td class="line x" title="75:223	Our strategy for optimizing  to minimize the objective R (Equation 3) is a variant of steepest descent (Perkins et al. , 2003)." ></td>
	<td class="line x" title="76:223	Each training iteration has several steps." ></td>
	<td class="line x" title="77:223	First, we choose some new compound features that have high magnitude gradient with respect to the objective function." ></td>
	<td class="line x" title="78:223	We do this by building a new decision tree, whose leaves represent the new compound features.5 Second, we confidencerate each leaf to minimize the objective over the examples that percolate down to that leaf." ></td>
	<td class="line x" title="79:223	Finally, we append the decision tree to the ensemble and update parameter vector  accordingly." ></td>
	<td class="line x" title="80:223	In this manner, compound feature selection is performed incrementally during training, as opposed to a priori." ></td>
	<td class="line x" title="81:223	To build each decision tree, we begin with a root node, and we recursively split nodes by choosing a splitting feature that will allow us to decrease the objective." ></td>
	<td class="line x" title="82:223	We have: L(I;) f = summationdisplay iI l(i;) (i;)  (i;) f (8) where: (i;) f = y(i)X f (i) (9) We define the weight of an example under the current model as: w(i;) =l(i;)(i;) = b(i) 11 + exp((i;))." ></td>
	<td class="line x" title="83:223	(10) 5Any given compound feature can appear in more than one tree." ></td>
	<td class="line x" title="84:223	19 and: W yf (I;) = summationdisplay iIX f (i)=1,y(i)=y w(i;) (11) Combining Equations 811 gives:6 L f = W 1 f W +1 f (12) We define the gain G f of feature f as: G f = max parenleftBigg 0, vextendsinglevextendsinglevextendsingle vextendsinglevextendsinglevextendsingle L f vextendsinglevextendsinglevextendsingle vextendsinglevextendsinglevextendsingle parenrightBigg (13) Equation 13 has this form because the gradient of the penalty term is undefined at f = 0." ></td>
	<td class="line x" title="85:223	This discontinuity is why lscript1 regularization tends to produce sparse models." ></td>
	<td class="line x" title="86:223	If G f = 0, then the objective R is at its minimum with respect to parameter f . Otherwise, G f is the magnitude of the gradient of the objective as we adjust f in the appropriate direction." ></td>
	<td class="line x" title="87:223	The gain of splitting node f using some atomic feature a is defined as G f (a) = G fa + G fa (14) We allow node f to be split only by atomic features a that increase the gain, i.e. G f (a) > G f . If no such feature exists, then f becomes a leaf node of the decision tree and f becomes one of the values to be optimized during the parameter update step." ></td>
	<td class="line x" title="88:223	Otherwise, we choose atomic feature a to split node f : a = arg max aA G f (a) (15) This split creates child nodes f a and f a. If no root node split has positive gain, then training has converged for the current choice of lscript1 parameter ." ></td>
	<td class="line x" title="89:223	Parameter update is done sequentially on only the most recently added compound features, which correspond to the leaves of the new decision tree." ></td>
	<td class="line x" title="90:223	After the entire tree is built, we percolate examples down to their appropriate leaf nodes." ></td>
	<td class="line x" title="91:223	We then choose for each leaf node f the parameter f that minimizes the objective R over the examples in that leaf." ></td>
	<td class="line x" title="92:223	Decision trees ensure that these compound features are mutually exclusive, so they can be directly optimized independently of each other using a line search over the objective R. 6Since  is fixed during a particular training iteration and I is fixed throughout training, we omit parameters (I;) henceforth." ></td>
	<td class="line x" title="93:223	3.3 Sampling for Faster Feature Selection Building a decision tree using the entire example set I can be very expensive, which we will demonstrate in 4.2." ></td>
	<td class="line x" title="94:223	However, feature selection can be effective even if we dont examine every example." ></td>
	<td class="line x" title="95:223	Since the weight of high-margin examples can be several orders of magnitude lower than that of low-margin examples (Equation 10), the contribution of the highmargin examples to feature weights (Equation 11) will be insignificant." ></td>
	<td class="line x" title="96:223	Therefore, we can ignore most examples during feature selection as long as we have good estimates of feature weights, which in turn give good estimates of the loss gradients (Equation 12)." ></td>
	<td class="line x" title="97:223	As shown in Step 1.5 of Listing 1, before building each decision tree we use priority sampling (Duffield et al. , 2005) to choose a small subset of the examples according to the example weights given by the current classifier, and the tree is built using only this subset." ></td>
	<td class="line x" title="98:223	We make the sample small enough that its entire atomic feature matrix will fit in memory." ></td>
	<td class="line x" title="99:223	To optimize decision tree building, we compute and cache the samples atomic feature matrix in advance (Step 1.6)." ></td>
	<td class="line x" title="100:223	Even if the sample is missing important information in one iteration, the training procedure is capable of recovering it from samples used in subsequent iterations." ></td>
	<td class="line x" title="101:223	Moreover, even if a samples gain estimates are inaccurate and the feature selection step chooses irrelevant compound features, confidence updates are based upon the entire training set and the regularization penalty will prevent irrelevant features from having their parameters move away from zero." ></td>
	<td class="line x" title="102:223	3.4 The Training Set Our training set I contains all inferences considered in every state along the correct path for each goldstandard parse tree (Sagae & Lavie, 2005).7 This method of generating training examples does not require a working parser and can be run prior to any training." ></td>
	<td class="line x" title="103:223	The downside of this approach is that it minimizes the error of the parser at correct states only." ></td>
	<td class="line x" title="104:223	It does not account for compounded error or teach the parser to recover from mistakes gracefully." ></td>
	<td class="line x" title="105:223	7Since parsing is done deterministically right-to-left, there can be no more than one correct inference at each state." ></td>
	<td class="line x" title="106:223	20 Turian and Melamed (2005) observed that uniform example biases b(i) produced lower accuracy as training progressed, because the induced classifiers minimized the example-wise error." ></td>
	<td class="line x" title="107:223	Since we aim to minimize the state-wise error, we express this bias by assigning every training state equal value, andfor the examples generated from that state sharing half the value uniformly among the negative examples and the other half uniformly among the positive examples." ></td>
	<td class="line x" title="108:223	Although there are O(n2) possible spans over a frontier containing n items, we reduce this to the O(n) inferences that cannot have more than 5 children." ></td>
	<td class="line x" title="109:223	With no restriction on the number of children, there would be O(n2) bottom-up inferences at each state." ></td>
	<td class="line x" title="110:223	However, only 0.57% of non-terminals in the preprocessed development set have more than five children." ></td>
	<td class="line x" title="111:223	Like Turian and Melamed (2005), we parallelize training by inducing 26 label classifiers (one for each non-terminal label in the Penn Treebank)." ></td>
	<td class="line x" title="112:223	Parallelization might not uniformly reduce training time because different label classifiers train at different rates." ></td>
	<td class="line x" title="113:223	However, parallelization uniformly reduces memory usage because each label classifier trains only on inferences whose consequent item has that label." ></td>
	<td class="line x" title="114:223	Even after parallelization, the atomic feature matrix cannot be cached in memory." ></td>
	<td class="line x" title="115:223	We can store the training inferences in memory using only an indirect representation." ></td>
	<td class="line x" title="116:223	More specifically, for each inference i in the training set, we cache in memory several values: a pointer i to a tree cut, its y-value y(i), its bias b(i), and its confidence h(i) under the current model." ></td>
	<td class="line x" title="117:223	We cache h(i) throughout training because it is needed both in computing the gradient of the objective during decision tree building (Step 1.7) as well as subsequent minimization of the objective over the decision tree leaves (Step 1.10)." ></td>
	<td class="line x" title="118:223	We update the confidences at the end of each training iteration using the newly added tree (Step 1.11)." ></td>
	<td class="line x" title="119:223	The most costly operation during training is to access the feature values in X(i)." ></td>
	<td class="line x" title="120:223	An atomic feature test determines the value Xa(i) for a single atomic feature a by examining the tree cut pointed to by inference i. Alternately, we can perform atomic feature extraction, i.e. determine all non-zero atomic features over i.8 Extraction is 1001000 times more expensive than a single test, but is necessary during decision tree building (Step 1.7) because we need the entire vector X(i) to accumulate inferences in children nodes." ></td>
	<td class="line x" title="121:223	Essentially, for each inference i that falls in some node f, we accumulate w(i) in Wy(i)fa for all a with Xa(i) = 1." ></td>
	<td class="line x" title="122:223	After all the inferences in a node have been accumulated, we try to split the node (Equation 15)." ></td>
	<td class="line x" title="123:223	The negative child weights are each determined as Wyfa = Wyf Wyfa. 4 Experiments We follow Taskar et al.(2004) and Turian and Melamed (2005) in training and testing on  15 word sentences in the English Penn Treebank (Taylor et al. , 2003)." ></td>
	<td class="line x" title="125:223	We used sections 0221 for training, section 22 for development, and section 23, for testing." ></td>
	<td class="line x" title="126:223	We use the same preprocessing steps as Turian and Melamed (2005): during both training and testing, the parser is given text POS-tagged by the tagger of Ratnaparkhi (1996), with capitalization stripped and outermost punctuation removed." ></td>
	<td class="line x" title="127:223	For reasons given in Turian and Melamed (2006), items are inferred bottom-up right-to-left." ></td>
	<td class="line x" title="128:223	As mentioned in 2, the parser cannot infer any item that crosses an item already in the state." ></td>
	<td class="line x" title="129:223	To ensure the parser does not enter an infinite loop, no two items in a state can have both the same span and the same label." ></td>
	<td class="line x" title="130:223	Given these restrictions, there were roughly 40 million training examples." ></td>
	<td class="line x" title="131:223	These were partitioned among the constituent label classifiers." ></td>
	<td class="line x" title="132:223	Our atomic feature set A contains features of the form is there an item in group J whose label/headword/headtag/headtagclass9 is X?." ></td>
	<td class="line x" title="133:223	Possible values of X for each predicate are collected from the training data." ></td>
	<td class="line x" title="134:223	Some examples of possible values for J include the last n child items, the first n left context items, all right context items, and the terminal items dominated by the non-head child items." ></td>
	<td class="line x" title="135:223	Space constraints prevent enumeration of the headtagclasses and atomic feature templates, which are 8Extraction need not take the nave approach of performing|A| different tests, and can be optimized by using knowledge about the nature of the atomic feature templates." ></td>
	<td class="line x" title="136:223	9The predicate headtagclass is a supertype of the headtag." ></td>
	<td class="line x" title="137:223	Given our compound features, these are not strictly necessary, but they accelerate training." ></td>
	<td class="line x" title="138:223	An example is proper noun, which contains the POS tags given to singular and plural proper nouns." ></td>
	<td class="line x" title="139:223	21 Figure 1 F1 score of our parser on the development set of the Penn Treebank, using only15 word sentences." ></td>
	<td class="line x" title="140:223	The dashed line indicates the percent of NP example weight lost due to sampling." ></td>
	<td class="line x" title="141:223	The bottom x-axis shows the number of non-zero parameters in each parser, summed over all label classifiers." ></td>
	<td class="line x" title="142:223	7.5K5K2.5K1.5K1K 84% 85% 86% 87% 88% 89% 90% 91%5.42.51.00.5 Devel." ></td>
	<td class="line x" title="143:223	F-measure total number of non-zero parameters training time (days) 0% 5% 10% 15% 20% 25% 30% 35% weight lost due to sampling instead provided at the URL given in the abstract." ></td>
	<td class="line x" title="144:223	These templates gave 1.1 million different atomic features." ></td>
	<td class="line x" title="145:223	We experimented with smaller feature sets, but found that accuracy was lower." ></td>
	<td class="line x" title="146:223	Charniak and Johnson (2005) use linguistically more sophisticated features, and Bod (2003) and Kudo et al.(2005) use sub-tree features, all of which we plan to try in future work." ></td>
	<td class="line x" title="148:223	We evaluated our parser using the standard PARSEVAL measures (Black et al. , 1991): labelled precision, labelled recall, and labelled F-measure (Prec., Rec., and F1, respectively), which are based on the number of non-terminal items in the parsers output that match those in the gold-standard parse." ></td>
	<td class="line x" title="151:223	The solid curve Figure 1 shows the accuracy of the parser over the development set as training progressed." ></td>
	<td class="line x" title="152:223	The parser exceeded 89% F-measure after 2.5 days of training." ></td>
	<td class="line x" title="153:223	The peak F-measure was 90.55%, achieved at 5.4 days using 6.3K active parameters." ></td>
	<td class="line x" title="154:223	We omit details given by Turian and Melamed (2006) in favor of a longer discussion in 4.2." ></td>
	<td class="line x" title="155:223	4.1 Test Set Results To situate our results in the literature, we compare our results to those reported by Taskar et al.(2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on  15 word sentences." ></td>
	<td class="line x" title="157:223	We also compare our parser to a representative non-discriminative parser (Bikel, Table 1 PARSEVAL results of parsers on the test set, using only15 word sentences." ></td>
	<td class="line x" title="158:223	F1 % Rec." ></td>
	<td class="line x" title="159:223	% Prec." ></td>
	<td class="line x" title="160:223	% Turian and Melamed (2005) 87.13 86.47 87.80 Bikel (2004) 88.30 87.85 88.75 Taskar et al.(2004) 89.12 89.10 89.14 our parser 89.40 89.26 89.55 Table 2 Profile of an NP training iteration, given in seconds, using an AMD Opteron 242 (64-bit, 1.6Ghz)." ></td>
	<td class="line x" title="162:223	Steps refer to Listing 1." ></td>
	<td class="line x" title="163:223	Step Description mean stddev % 1.5 Sample 1.5s 0.07s 0.7% 1.6 Extraction 38.2s 0.13s 18.6% 1.7 Build tree 127.6s 27.60s 62.3% 1.8 Percolation 31.4s 4.91s 15.3% 1.911 Leaf updates 6.2s 1.75s 3.0% 1.511 Total 204.9s 32.6s 100.0% 2004),10 the only one that we were able to train and test under exactly the same experimental conditions (including the use of POS tags from Ratnaparkhi (1996))." ></td>
	<td class="line x" title="164:223	Table 1 shows the PARSEVAL results of these four parsers on the test set." ></td>
	<td class="line x" title="165:223	4.2 Efficiency 40% of non-terminals in the Penn Treebank are NPs." ></td>
	<td class="line x" title="166:223	Consequently, the bottleneck in training is induction of the NP classifier." ></td>
	<td class="line x" title="167:223	It was trained on 1.65 million examples." ></td>
	<td class="line x" title="168:223	Each example had an average of 440 non-zero atomic features (stddev 123), so the direct representation of each example requires a minimum 440  sizeof(int) = 1760 bytes, and the entire atomic feature matrix would require 1760 bytes  1.65 million = 2.8 GB." ></td>
	<td class="line x" title="169:223	Conversely, an indirectly represent inference requires no more 32 bytes: two floats (the cached confidence h(i) and the bias term b(i)), a pointer to a tree cut (i), and a bool (the y-value y(i))." ></td>
	<td class="line x" title="170:223	Indirectly storing the entire example set requires only 32 bytes1.65 million = 53 MB plus the treebank and tree cuts, a total of 400 MB in our implementation." ></td>
	<td class="line x" title="171:223	We used a sample size of|S|= 100, 000 examples to build each decision tree, 16.5 times fewer than the entire example set." ></td>
	<td class="line x" title="172:223	The dashed curve in Figure 1 10Bikel (2004) is a clean room reimplementation of the Collins (1999) model with comparable accuracy." ></td>
	<td class="line x" title="173:223	22 shows the percent of NP example weight lost due to sampling." ></td>
	<td class="line x" title="174:223	As training progresses, fewer examples are informative to the model." ></td>
	<td class="line x" title="175:223	Even though we ignore 94% of examples during feature selection, sampling loses less than 1% of the example weight after a day of training." ></td>
	<td class="line x" title="176:223	The NP classifier used in our final parser was an ensemble containing 2316 trees, which took five days to build." ></td>
	<td class="line x" title="177:223	Overall, there were 96871 decision tree leaves, only 2339 of which were nonzero." ></td>
	<td class="line x" title="178:223	There were an average of 40.4 (7.4 stddev) decision tree splits between the root of a tree and a non-zero leaf, and nearly all nonzero leaves were conjunctions of atomic feature negations (e.g. (some child item is a verb)  (some child item is a preposition))." ></td>
	<td class="line x" title="179:223	The non-zero leaf confidences were quite small in magnitude (0.107 mean, 0.069 stddev) but the training example margins over the entire ensemble were nonetheless quite high: 11.7 mean (2.92 stddev) for correct inferences, 30.6 mean (11.2 stddev) for incorrect inferences." ></td>
	<td class="line x" title="180:223	Table 2 profiles an NP training iteration, in which one decision tree is created and added to the NP ensemble." ></td>
	<td class="line x" title="181:223	Feature selection in our algorithm (Steps 1.51.7) takes 1.5+38.2+127.6 = 167.3s, far faster than in nave approaches." ></td>
	<td class="line x" title="182:223	If we didnt do sampling but had 2.8GB to spare, we could eliminate the extraction step (Step 1.6) and instead cache the entire atomic feature matrix before the loop." ></td>
	<td class="line x" title="183:223	However, tree building (Step 1.7) scales linearly in the number of examples, and would take 16.5127.6s = 2105.4s using the entire example set." ></td>
	<td class="line x" title="184:223	If we didnt do sampling and couldnt cache the atomic feature matrix, tree building would also require repeatedly performing extraction." ></td>
	<td class="line x" title="185:223	The number of individual feature extractions needed to build a single decision tree is the sum over the internal nodes of the number of examples that percolate down to that node." ></td>
	<td class="line x" title="186:223	There are an average of 40.8 (7.8 stddev) internal nodes in each tree and most of the examples fall in nearly all of them." ></td>
	<td class="line x" title="187:223	This property is caused by the lopsided trees induced under lscript1 regularization." ></td>
	<td class="line x" title="188:223	A conservative estimate is that each decision tree requires 25 extractions times the number of examples." ></td>
	<td class="line x" title="189:223	So extraction would add at least 2516.538.2s = 15757.5s on top of 2105.40s, and hence building each decision tree would take at least (15757.5+2105.40)/167.3 100 times as long as it does currently." ></td>
	<td class="line x" title="190:223	Our decision tree ensembles contain over two orders of magnitude more compound features than those in Turian and Melamed (2005)." ></td>
	<td class="line x" title="191:223	Our overall training time was roughly equivalent to theirs." ></td>
	<td class="line x" title="192:223	This ratio corroborates the above estimate." ></td>
	<td class="line x" title="193:223	5 Discussion The NP classifier was trained only on the 1.65 million NP examples in the 9753 training sentences with 15 words (168.8 examples/sentence)." ></td>
	<td class="line x" title="194:223	The number of examples generated is quadratic in the sentence length, so there are 41.7 million NP examples in all 39832 training sentences of the whole Penn Treebank (1050 examples/sentence), 25 times as many as we are currently using." ></td>
	<td class="line x" title="195:223	The time complexity of each step in the training loop (Steps 1.511) is linear over the number of examples used by that step." ></td>
	<td class="line x" title="196:223	When we scale up to the full treebank, feature selection will not require a sample 25 times larger, so it will no longer be the bottleneck in training." ></td>
	<td class="line x" title="197:223	Instead, each iteration will be dominated by choosing leaf confidences and then updating the cached example confidences, which would require 25(31.4s + 6.2s) = 940s per iteration." ></td>
	<td class="line x" title="198:223	These steps are crucial to the current training algorithm, because it is important to have example confidences that are current with respect to the model." ></td>
	<td class="line x" title="199:223	Otherwise, we cannot determine the examples most poorly classified by the current model, and will have no basis for choosing an informative sample." ></td>
	<td class="line x" title="200:223	We might try to save training time by building many decision trees over a single sample and then updating the confidences of the entire example set using all the new trees." ></td>
	<td class="line x" title="201:223	But, if this confidence update is done using feature tests, then we have merely deferred the cost of the confidence update over the entire example set." ></td>
	<td class="line x" title="202:223	The amount of training done on a particular sample is proportional to the time subsequently spent updating confidences over the entire example set." ></td>
	<td class="line x" title="203:223	To spend less time doing confidence updates, we must use a training regime that is sublinear with respect to the training time." ></td>
	<td class="line x" title="204:223	For example, Riezler (2004) reports that the lscript1 regularization term drives many of the models parameters to zero during conjugate gradient optimization, which are 23 then pruned before subsequent optimization steps to avoid numerical instability." ></td>
	<td class="line x" title="205:223	Instead of building decision tree(s) at each iteration, we could perform nbest feature selection followed by parallel optimization of the objective over the sample." ></td>
	<td class="line x" title="206:223	The main limitation of our work so far is that we can do training reasonably quickly only on short sentences, because a sentence with n words generates O(n2) training inferences in total." ></td>
	<td class="line nc" title="207:223	Although generating training examples in advance without a working parser (Sagae & Lavie, 2005) is much faster than using inference (Collins & Roark, 2004; Henderson, 2004; Taskar et al. , 2004), our training time can probably be decreased further by choosing a parsing strategy with a lower branching factor." ></td>
	<td class="line x" title="208:223	Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005) generate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O(n) training examples." ></td>
	<td class="line x" title="209:223	6 Conclusion Our work has made advances in both accuracy and training speed of discriminative parsing." ></td>
	<td class="line x" title="210:223	As far as we know, we present the first discriminative parser that surpasses a generative baseline on constituent parsing without using a generative component, and it does so with minimal linguistic cleverness." ></td>
	<td class="line x" title="211:223	The main bottleneck in our setting was memory." ></td>
	<td class="line x" title="212:223	We could store the examples in memory only using an indirect representation." ></td>
	<td class="line x" title="213:223	The most costly operation during training was accessing the features of a particular example from this indirect representation." ></td>
	<td class="line x" title="214:223	We showed how to train a parser effectively under these conditions." ></td>
	<td class="line x" title="215:223	In particular, we used principled sampling to estimate loss gradients and reduce the number of feature extractions." ></td>
	<td class="line x" title="216:223	This approximation increased the speed of feature selection 100-fold." ></td>
	<td class="line x" title="217:223	We are exploring methods for scaling training up to larger example sets." ></td>
	<td class="line x" title="218:223	We are also investigating the relationship between sample size, training time, classifier complexity, and accuracy." ></td>
	<td class="line x" title="219:223	In addition, we shall make some standard improvements to our parser." ></td>
	<td class="line x" title="220:223	Our parser should infer its own POS tags." ></td>
	<td class="line x" title="221:223	A shift-reduce parsing strategy will generate fewer examples, and might lead to shorter training time." ></td>
	<td class="line x" title="222:223	Lastly, we plan to give the model linguistically more sophisticated features." ></td>
	<td class="line x" title="223:223	We also hope to apply the model to other structured learning tasks, such as syntax-driven SMT." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D07-1009
Incremental Text Structuring with Online Hierarchical Ranking
Chen, Erdong;Snyder, Benjamin;Barzilay, Regina;"></td>
	<td class="line x" title="1:246	Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp." ></td>
	<td class="line x" title="2:246	8391, Prague, June 2007." ></td>
	<td class="line x" title="3:246	c2007 Association for Computational Linguistics Incremental Text Structuring with Online Hierarchical Ranking Erdong Chen, Benjamin Snyder and Regina Barzilay Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology {edc,bsnyder,regina}@csail.mit.edu Abstract Many emerging applications require documents to be repeatedly updated." ></td>
	<td class="line x" title="4:246	Such documents include newsfeeds, webpages, and shared community resources such as Wikipedia." ></td>
	<td class="line x" title="5:246	In this paper we address the task of inserting new information into existing texts." ></td>
	<td class="line x" title="6:246	In particular, we wish to determine the best location in a text for a given piece of new information." ></td>
	<td class="line x" title="7:246	For this process to succeed, the insertion algorithm should be informed by the existing document structure." ></td>
	<td class="line x" title="8:246	Lengthy real-world texts are often hierarchically organized into chapters, sections, and paragraphs." ></td>
	<td class="line x" title="9:246	We present an online ranking model which exploits this hierarchical structure  representationally in its features and algorithmically in its learning procedure." ></td>
	<td class="line x" title="10:246	When tested on a corpus of Wikipedia articles, our hierarchically informed model predicts the correct insertion paragraph more accurately than baseline methods." ></td>
	<td class="line x" title="11:246	1 Introduction Many emerging applications require documents to be repeatedly updated." ></td>
	<td class="line x" title="12:246	For instance, newsfeed articles are continuously revised by editors as new information emerges, and personal webpages are modified as the status of the individual changes." ></td>
	<td class="line x" title="13:246	This revision strategy has become even more prevalent with the advent of community edited web resources, the most notable example being Wikipedia." ></td>
	<td class="line x" title="14:246	At present this process involves massive human effort." ></td>
	<td class="line x" title="15:246	For instance, the English language version of Wikipedia averaged over 3 million edits1 per month in 2006." ></td>
	<td class="line x" title="16:246	Even so, many articles quickly become outdated." ></td>
	<td class="line x" title="17:246	A system that performs such updates automatically could drastically decrease maintenance efforts and potentially improve document quality." ></td>
	<td class="line x" title="18:246	Currently there is no effective way to automatically update documents as new information becomes available." ></td>
	<td class="line x" title="19:246	The closest relevant text structuring technique is the work on sentence ordering, in which a complete reordering of the text is undertaken." ></td>
	<td class="line x" title="20:246	Predictably these methods are suboptimal for this new task because they cannot take advantage of existing text structure." ></td>
	<td class="line x" title="21:246	We introduce an alternative vision of text structuring as a process unfolding over time." ></td>
	<td class="line x" title="22:246	Instead of ordering sentences all at once, we start with a wellformed draft and add new information at each stage, while preserving document coherence." ></td>
	<td class="line x" title="23:246	The basic operation of incremental text structuring is the insertion of new information." ></td>
	<td class="line x" title="24:246	To automate this process, we develop a method for determining the best location in a text for a given piece of new information." ></td>
	<td class="line x" title="25:246	The main challenge is to maintain the continuity and coherence of the original text." ></td>
	<td class="line x" title="26:246	These properties may be maintained by examining sentences adjacent to each potential insertion point." ></td>
	<td class="line x" title="27:246	However, a local sentence comparison method such as this may fail to account for global document coherence (e.g. by allowing the mention of some fact in an inappropriate section)." ></td>
	<td class="line x" title="28:246	This problem is especially acute in the case of lengthy, real-world texts such as books, technical reports, and web pages." ></td>
	<td class="line x" title="29:246	These documents 1http://stats.wikimedia.org/EN/ TablesWikipediaEN.htm 83 are commonly organized hierarchically into sections and paragraphs to aid reader comprehension." ></td>
	<td class="line x" title="30:246	For documents where hierarchical information is not explicitly provided, such as automatic speech transcripts, we can use automatic segmentation methods to induce such a structure (Hearst, 1994)." ></td>
	<td class="line x" title="31:246	Rather than ignoring the inherent hierarchical structure of these texts, we desire to directly model such hierarchies and use them to our advantage  both representationally in our features and algorithmically in our learning procedure." ></td>
	<td class="line x" title="32:246	To achieve this goal, we introduce a novel method for sentence insertion that operates over a hierarchical structure." ></td>
	<td class="line x" title="33:246	Our document representation includes features for each layer of the hierarchy." ></td>
	<td class="line x" title="34:246	For example, the word overlap between the inserted sentence and a section header would be included as an upper-level section feature, whereas a comparison of the sentence with all the words in a paragraph would be a lower-level paragraph feature." ></td>
	<td class="line x" title="35:246	We propose a linear model which simultaneously considers the features of every layer when making insertion decisions." ></td>
	<td class="line x" title="36:246	We develop a novel update mechanism in the online learning framework which exploits the hierarchical decomposition of features." ></td>
	<td class="line x" title="37:246	This mechanism limits model updates to those features found at the highest incorrectly predicted layer, without unnecessarily disturbing the parameter values for the lower reaches of the tree." ></td>
	<td class="line x" title="38:246	This conservative update approach maintains as much knowledge as possible from previously encountered training examples." ></td>
	<td class="line x" title="39:246	We evaluate our method using real-world data where multiple authors have revised preexisting documents over time." ></td>
	<td class="line x" title="40:246	We obtain such a corpus from Wikipedia articles,2 which are continuously updated by multiple authors." ></td>
	<td class="line x" title="41:246	Logs of these updates are publicly available, and are used for training and testing of our algorithm." ></td>
	<td class="line x" title="42:246	Figure 1 shows an example of a Wikipedia insertion." ></td>
	<td class="line x" title="43:246	We believe this data will more closely mirror potential applications than synthetic collections used in previous work on text structuring." ></td>
	<td class="line x" title="44:246	Our hierarchical training method yields significant improvement when compared to a similar nonhierarchical model which instead uses the standard 2Data and code used in this paper are available at http://people.csail.mit.edu/edc/emnlp07/ perceptron update of Collins (2002)." ></td>
	<td class="line x" title="45:246	We also report human performance on the insertion task in order to provide a reasonable upper-bound on machine performance." ></td>
	<td class="line x" title="46:246	An analysis of these results shows that our method closes the gap between machine and human performance substantially." ></td>
	<td class="line x" title="47:246	In the following section, we provide an overview of existing work on text structuring and hierarchical learning." ></td>
	<td class="line x" title="48:246	Then, we define the insertion task and introduce our hierarchical ranking approach to sentence insertion." ></td>
	<td class="line x" title="49:246	Next, we present our experimental framework and data." ></td>
	<td class="line x" title="50:246	We conclude the paper by presenting and discussing our results." ></td>
	<td class="line x" title="51:246	2 Related Work Text Structuring The insertion task is closely related to the extensively studied problem of sentence ordering.3 Most of the existing algorithms represent text structure as a linear sequence and are driven by local coherence constraints (Lapata, 2003; Karamanis et al. , 2004; Okazaki et al. , 2004; Barzilay and Lapata, 2005; Bollegala et al. , 2006; Elsner and Charniak, 2007)." ></td>
	<td class="line x" title="52:246	These methods induce a total ordering based on pairwise relations between sentences." ></td>
	<td class="line x" title="53:246	Researchers have shown that identifying precedence relations does not require deep semantic interpretation of input sentences: shallow distributional features are sufficient for accurate prediction." ></td>
	<td class="line x" title="54:246	Our approach employs similar features to represent nodes at the lowest level of the hierarchy." ></td>
	<td class="line x" title="55:246	The key departure of our work from previous research is the incorporation of hierarchical structure into a corpus-based approach to ordering." ></td>
	<td class="line x" title="56:246	While in symbolic generation and discourse analysis a text is typically analyzed as a tree-like structure (Reiter and Dale, 1990), a linear view is prevalent in data-driven methods to text structuring.4 Moving beyond a linear representation enables us to handle longer texts where a local view of coherence does not suffice." ></td>
	<td class="line x" title="57:246	At the same time, our approach does not require any manual rules for handling tree insertions, in contrast to symbolic text planners." ></td>
	<td class="line x" title="58:246	3Independently and simultaneously with our work, Elsner and Charniak (2007) have studied the sentence insertion task in a different setting." ></td>
	<td class="line x" title="59:246	4Though statistical methods have been used to induce such trees (Soricut and Marcu, 2003), they are not used for ordering and other text-structuring tasks." ></td>
	<td class="line x" title="60:246	84 Shaukat Aziz (born March 6, 1949, Karachi, Pakistan) has been the Finance Minister of Pakistan since November 1999." ></td>
	<td class="line x" title="61:246	He was nominated for the position of Prime Minister after the resignation of Zafarullah Khan Jamali on June 6, 2004." ></td>
	<td class="line x" title="62:246	Education Aziz attended Saint Patricks school, Karachi and Abbottabad Public School." ></td>
	<td class="line x" title="63:246	He graduated with a Bachelor of Science degree from Gordon College, Rawalpindi, in 1967." ></td>
	<td class="line x" title="64:246	He obtained an MBA Degree in 1969 from the Institute of Business Administration, Karachi." ></td>
	<td class="line x" title="65:246	Career In November, 1999, Mr. Aziz became Pakistans Minister of Finance." ></td>
	<td class="line x" title="66:246	As Minister of finance, Mr. Aziz also heads the Economic Coordination Committee of the Cabinet, and the Cabinet Committee on Privatization." ></td>
	<td class="line x" title="67:246	Mr. Aziz was named as Prime Minister by interim Prime Minister Chaudhry Shujaat Hussain after the resignation of Zafarullah Khan Jamali on June 6, 2004." ></td>
	<td class="line x" title="68:246	He is expected to retain his position as Minister of Finance." ></td>
	<td class="line x" title="69:246	In 2001, Mr Aziz was declared Finance Minister of the Year byEuromoney and Bankers Magazine." ></td>
	<td class="line x" title="70:246	Figure 1: An example of Wikipedia insertion." ></td>
	<td class="line x" title="71:246	Hierarchical Learning There has been much recent research on multiclass hierarchical classification." ></td>
	<td class="line x" title="72:246	In this line of work, the set of possible labels is organized hierarchically, and each input must be assigned a node in the resulting tree." ></td>
	<td class="line x" title="73:246	A prototype weight vector is learned for each node, and classification decisions are based on all the weights along the path from node to root." ></td>
	<td class="line x" title="74:246	The essence of this scheme is that the more ancestors two nodes have in common, the more parameters they are forced to share." ></td>
	<td class="line x" title="75:246	Many learning methods have been proposed, including SVM-style optimization (Cai and Hofmann, 2004), incremental least squares estimation (Cesa-Bianchi et al. , 2006b), and perceptron (Dekel et al. , 2004)." ></td>
	<td class="line x" title="76:246	This previous work rests on the assumption that a predetermined set of atomic labels with a fixed hierarchy is given." ></td>
	<td class="line x" title="77:246	In our task, however, the set of possible insertion points  along with their hierarchical organization  is unique to each input document." ></td>
	<td class="line x" title="78:246	Furthermore, nodes exhibit rich internal feature structure and cannot be identified across documents, except insofar as their features overlap." ></td>
	<td class="line x" title="79:246	As is commonly done in NLP tasks, we make use of a feature function which produces one feature vector for each possible insertion point." ></td>
	<td class="line x" title="80:246	We then choose among these feature vectors using a single weight vector (casting the task as a structured ranking problem rather than a classification problem)." ></td>
	<td class="line x" title="81:246	In this framework, an explicit hierarchical view is no longer necessary to achieve parameter tying." ></td>
	<td class="line x" title="82:246	In fact, each parameter will be shared by exactly those insertion points which exhibit the corresponding feature, both across documents and within a single document." ></td>
	<td class="line x" title="83:246	Higher level parameters will thus naturally be shared by all paragraphs within a single section." ></td>
	<td class="line oc" title="84:246	In fact, when the perceptron update rule of (Dekel et al. , 2004)  which modifies the weights of every divergent node along the predicted and true paths  is used in the ranking framework, it becomes virtually identical with the standard, flat, ranking perceptron of Collins (2002).5 In contrast, our approach shares the idea of (Cesa-Bianchi et al. , 2006a) that if a parent class has been predicted wrongly, then errors in the children should not be taken into account. We also view this as one of the key ideas of the incremental perceptron algorithm of (Collins and Roark, 2004), which searches through a complex decision space step-by-step and is immediately updated at the first wrong move." ></td>
	<td class="line p" title="85:246	Our work fuses this idea of selective hierarchical updates with the simplicity of the perceptron algorithm and the flexibility of arbitrary feature sharing inherent in the ranking framework." ></td>
	<td class="line x" title="86:246	3 The Algorithm In this section, we present our sentence insertion model and a method for parameter estimation." ></td>
	<td class="line x" title="87:246	Given a hierarchically structured text composed of sections and paragraphs, the sentence insertion model determines the best paragraph within 5The main remaining difference is that Dekel et al.(2004) use a passive-aggressive update rule (Crammer et al. , 2006) and in doing so enforce a margin based on tree distance." ></td>
	<td class="line x" title="89:246	85 which to place the new sentence." ></td>
	<td class="line x" title="90:246	To identify the exact location of the sentence within the chosen paragraph, local ordering methods such as (Lapata, 2003) could be used." ></td>
	<td class="line x" title="91:246	We formalize the insertion task as a structured ranking problem, and our model is trained using an online algorithm." ></td>
	<td class="line x" title="92:246	The distinguishing feature of the algorithm is a selective correction mechanism that focuses the model update on the relevant layer of the documents feature hierarchy." ></td>
	<td class="line x" title="93:246	The algorithm described below can be applied to any hierarchical ranking problem." ></td>
	<td class="line x" title="94:246	For concreteness, we use the terminology of the sentence insertion task, where a hierarchy corresponds to a document with sections and paragraphs." ></td>
	<td class="line x" title="95:246	3.1 Problem Formulation In a sentence insertion problem, we are given a training sequence of instances (s1,T 1,lscript1),,(sm,T m,lscriptm)." ></td>
	<td class="line x" title="96:246	Each instance contains a sentence s, a hierarchically structured document T, and a node lscript representing the correct insertion point of s into T. Although lscript can generally be any node in the tree, in our problem we need only consider leaf nodes." ></td>
	<td class="line x" title="97:246	We cast this problem in the ranking framework, where a feature vector is associated with each sentence-node pair." ></td>
	<td class="line x" title="98:246	For example, the feature vector of an internal, section-level node may consider the word overlap between the inserted sentence and the section title." ></td>
	<td class="line x" title="99:246	At the leaf level, features may include an analysis of the overlap between the corresponding text and sentence." ></td>
	<td class="line x" title="100:246	In practice, we use disjoint feature sets for different layers of the hierarchy, though in theory they could be shared." ></td>
	<td class="line x" title="101:246	Our goal then is to choose a leaf node by taking into account its feature vector as well as feature vectors of all its ancestors in the tree." ></td>
	<td class="line x" title="102:246	More formally, for each sentence s and hierarchically structured document T, we are given a set of feature vectors, with one for each node: {(s,n) : n  T}." ></td>
	<td class="line x" title="103:246	We denote the set of leaf nodes by L(T) and the path from the root of the tree to a node n by P(n)." ></td>
	<td class="line x" title="104:246	Our model must choose one leaf node among the set L(T) by examining its feature vector (s,lscript) as well as all the feature vectors along its path: {(s,n) : n P(lscript)}." ></td>
	<td class="line x" title="105:246	Input : (s1,T 1,lscript1),,(sm,T m,lscriptm)." ></td>
	<td class="line x" title="106:246	Initialize : Set w1 = 0 Loop : For t = 1,2,,N : 1." ></td>
	<td class="line x" title="107:246	Get a new instance st,T t. 2." ></td>
	<td class="line x" title="108:246	Predict lscriptt = argmaxlscriptL(T)wt (st,lscript)." ></td>
	<td class="line x" title="109:246	3." ></td>
	<td class="line x" title="110:246	Get the new label lscriptt." ></td>
	<td class="line x" title="111:246	4." ></td>
	<td class="line x" title="112:246	If lscriptt = lscriptt: wt+1  wt Else: i  maxi : P(lscriptt)i = P(lscriptt)i} a P(lscriptt)i+1 b P(lscriptt)i+1 wt+1  wt +(s,a)(s,b) Output : wN+1." ></td>
	<td class="line x" title="113:246	Figure 2: Training algorithm for the hierarchical ranking model." ></td>
	<td class="line x" title="114:246	3.2 The Model Our model consists of a weight vector w, each weight corresponding to a single feature." ></td>
	<td class="line x" title="115:246	The features of a leaf are aggregated with the features of all its ancestors in the tree." ></td>
	<td class="line x" title="116:246	The leaf score is then computed by taking the inner product of this aggregate feature vector with the weights w. The leaf with the highest score is then selected." ></td>
	<td class="line x" title="117:246	More specifically, we define the aggregate feature vector of a leaf lscript to be the sum of all features found along the path to the root: (s,lscript) = summationdisplay nP(lscript) (s,n) (1) This has the effect of stacking together features found in a single layer, and adding the values of features found at more than one layer." ></td>
	<td class="line x" title="118:246	Our model then outputs the leaf with the highest scoring aggregate feature vector: arg max lscriptL(T) w(s,lscript) (2) Note that by using this criterion, our decoding method is equivalent to that of the standard linear ranking model." ></td>
	<td class="line x" title="119:246	The novelty of our approach lies in our training algorithm which uses the hierarchical feature decomposition of Equation 1 to pinpoint its updates along the path in the tree." ></td>
	<td class="line x" title="120:246	86 n1 n2 n3 lscript1 lscript2 lscript3 lscript4 (s,T) 42 3 121 lscriptlscript Figure 3: An example of a tree with the corresponding model scores." ></td>
	<td class="line x" title="121:246	The path surrounded by solid lines leads to the correct node lscript1." ></td>
	<td class="line x" title="122:246	The path surrounded by dotted lines leads to lscript3, the predicted output based on the current model." ></td>
	<td class="line x" title="123:246	3.3 Training Our training procedure is implemented in the online learning framework." ></td>
	<td class="line x" title="124:246	The model receives each training instance, and predicts a leaf node according to its current parameters." ></td>
	<td class="line x" title="125:246	If an incorrect leaf node is predicted, the weights are updated based on the divergence between the predicted path and the true path." ></td>
	<td class="line x" title="126:246	We trace the paths down the tree, and only update the weights of the features found at the split point." ></td>
	<td class="line x" title="127:246	Updates for shared nodes along the paths would of course cancel out." ></td>
	<td class="line x" title="128:246	In contrast to the standard ranking perceptron as well as the hierarchical perceptron of (Dekel et al. , 2004), no features further down the divergent paths are incorporated in the update." ></td>
	<td class="line x" title="129:246	For example, if the model incorrectly predicts the section, then only the weights of the section features are updated whereas the paragraph feature weights remain untouched." ></td>
	<td class="line x" title="130:246	More formally, let lscript be the predicted leaf node and let lscript negationslash= lscript be the true leaf node." ></td>
	<td class="line x" title="131:246	Denote by P(lscript)i the ith node on the path from the root to lscript." ></td>
	<td class="line x" title="132:246	Let i be the depth of the lowest common ancestor of lscript and lscript (i.e. , i = maxi : P(lscript)i = P(lscript)i})." ></td>
	<td class="line x" title="133:246	Then the update rule for this round is: w  w+ parenleftBig s,P(lscript)i+1 parenrightBig  parenleftBig s,P(lscript)i+1 parenrightBig (3) Full pseudo-code for our hierarchical online training algorithm is shown in Figure 2." ></td>
	<td class="line x" title="134:246	We illustrate the selective update mechanism on the simple example shown on Figure 3." ></td>
	<td class="line x" title="135:246	The correct prediction is the node lscript1 with an aggregate path score of 5, but lscript3 with the higher score of 6 is predicted." ></td>
	<td class="line x" title="136:246	In this case, both the section and the paragraph are incorrectly predicted." ></td>
	<td class="line x" title="137:246	In response to this mistake, the features associated with the correct section, n2, are added to the weights, and the features of the incorrectly predicted section, n3, are subtracted from the weights." ></td>
	<td class="line x" title="138:246	An alternative update strategy would be to continue to update the feature weights of the leaf nodes, lscript1 and lscript3." ></td>
	<td class="line x" title="139:246	However, by identifying the exact source of path divergence we preserve the previously learned balance between leaf node features." ></td>
	<td class="line x" title="140:246	4 Features Features used in our experiments are inspired by previous work on corpus-based approaches for discourse analysis (Marcu and Echihabi, 2002; Lapata, 2003; Elsner et al. , 2007)." ></td>
	<td class="line x" title="141:246	We consider three types of features: lexical, positional, and temporal." ></td>
	<td class="line x" title="142:246	This section gives a general overview of these features (see code for further details.)" ></td>
	<td class="line x" title="143:246	Lexical Features Lexical features have been shown to provide strong cues for sentence positioning." ></td>
	<td class="line x" title="144:246	To preserve text cohesion, an inserted sentence has to be topically close to its surrounding sentences." ></td>
	<td class="line x" title="145:246	At the paragraph level, we measure topical overlap using the TF*IDF weighted cosine similarity between an inserted sentence and a paragraph." ></td>
	<td class="line x" title="146:246	We also use a more linguistically refined similarity measure that computes overlap considering only subjects and objects." ></td>
	<td class="line x" title="147:246	Syntactic analysis is performed using the MINIPAR parser (Lin, 1998)." ></td>
	<td class="line x" title="148:246	The overlap features are computed at the section level in a similar way." ></td>
	<td class="line x" title="149:246	We also introduce an additional section-level overlap feature that computes the cosine similarity between an inserted sentence and the first sentence in a section." ></td>
	<td class="line x" title="150:246	In our corpus, the opening sentence of a section is typically strongly 87 indicative of its topic, thus providing valuable cues for section level insertions." ></td>
	<td class="line x" title="151:246	In addition to overlap, we use lexical features that capture word co-occurrence patterns in coherent texts." ></td>
	<td class="line x" title="152:246	This measure was first introduced in the context of sentence ordering by Lapata (2003)." ></td>
	<td class="line x" title="153:246	Given a collection of documents in a specific domain, we compute the likelihood that a pair of words co-occur in adjacent sentences." ></td>
	<td class="line x" title="154:246	From these counts, we induce the likelihood that two sentences are adjacent to each other." ></td>
	<td class="line x" title="155:246	For a given paragraph and an inserted sentence, the highest adjacency probability between the inserted sentence and paragraph sentences is recorded." ></td>
	<td class="line x" title="156:246	This feature is also computed at the section level." ></td>
	<td class="line x" title="157:246	Positional Features These features aim to capture user preferences when positioning new information into the body of a document." ></td>
	<td class="line x" title="158:246	For instance, in the Wikipedia data, insertions are more likely to appear at the end of a document than at its beginning." ></td>
	<td class="line x" title="159:246	We track positional information at the section and paragraph level." ></td>
	<td class="line x" title="160:246	At the section level, we record whether a section is the first or last of the document." ></td>
	<td class="line x" title="161:246	At the paragraph level, there are four positional features which indicate the paragraphs position (i.e. , start or end) within its individual section and within the document as a whole." ></td>
	<td class="line x" title="162:246	Temporal Features The text organization may be influenced by temporal relations between underlying events." ></td>
	<td class="line x" title="163:246	In temporally coherent text, events that happen in the same time frame are likely to be described in the same segment." ></td>
	<td class="line x" title="164:246	Our computation of temporal features does not require full fledged temporal interpretation." ></td>
	<td class="line x" title="165:246	Instead, we extract these features based on two categories of temporal cues: verb tense and date information." ></td>
	<td class="line x" title="166:246	The verb tense feature captures whether a paragraph contains at least one sentence using the same tense as the inserted sentence." ></td>
	<td class="line x" title="167:246	For instance, this feature would occur for the inserted sentence in Figure 1 since both the sentence and chosen paragraph employ the past tense." ></td>
	<td class="line x" title="168:246	Another set of features takes into account the relation between the dates in a paragraph and those in an inserted sentence." ></td>
	<td class="line x" title="169:246	We extract temporal expressions using the TIMEX2 tagger (Mani and Wilson, 2000), and compute the time interval for a paragraph bounded by its earliest and latest dates." ></td>
	<td class="line x" title="170:246	We record the degree of overlap between the paragraph time inSection Paragraph Tree Dist T1 J1 0.575 0.5 1.85 J2 0.7 0.525 1.55 T2 J3 0.675 0.55 1.55 J4 0.725 0.55 1.45 Table 1: Accuracy of human insertions compared against gold standard from Wikipedias update log." ></td>
	<td class="line x" title="171:246	T1 is a subset of the data annotated by judges J1 and J2, while T2 is annotated by J3 and J4." ></td>
	<td class="line x" title="172:246	terval and insertion sentence time interval." ></td>
	<td class="line x" title="173:246	5 Experimental Set-Up Corpus Our corpus consists of Wikipedia articles that belong to the category Living People. We focus on this category because these articles are commonly updated: when new facts about a person are featured in the media, a corresponding entry in Wikipedia is likely to be modified." ></td>
	<td class="line x" title="174:246	Unlike entries in a professionally edited encyclopedia, these articles are collaboratively written by multiple users, resulting in significant stylistic and content variations across texts in our corpus." ></td>
	<td class="line x" title="175:246	This property distinguishes our corpus from more stylistically homogeneous collections of biographies used in text generation research (Duboue and McKeown, 2003)." ></td>
	<td class="line x" title="176:246	We obtain data on insertions6 from the update log that accompanies every Wikipedia entry." ></td>
	<td class="line x" title="177:246	For each change in the articles history, the log records an article before and after the change." ></td>
	<td class="line x" title="178:246	From this information, we can identify the location of every inserted sentence." ></td>
	<td class="line x" title="179:246	In cases where multiple insertions occur over time to the same article, they are treated independently of each other." ></td>
	<td class="line x" title="180:246	To eliminate spam, we place constraints on inserted sentences: (1) a sentence has at least 8 tokens and at most 120 tokens; (2) the MINIPAR parser (Lin, 1998) can identify a subject or an object in a sentence." ></td>
	<td class="line x" title="181:246	This process yields 4051 insertion/article pairs, from which 3240 pairs are used for training and 811 pairs for testing." ></td>
	<td class="line x" title="182:246	These insertions are derived from 1503 Wikipedia articles." ></td>
	<td class="line x" title="183:246	Relative to other corpora used in text structuring research (Barzilay and Lee, 2004; Lapata, 2003; Karamanis et al. , 2004), texts in 6Insertion is only one type of recorded update, others include deletions and sentence rewriting." ></td>
	<td class="line x" title="184:246	88 our collection are long: an average article has 32.9 sentences, organized in 3.61 sections and 10.9 paragraphs." ></td>
	<td class="line x" title="185:246	Our corpus only includes articles that have more than one section." ></td>
	<td class="line x" title="186:246	When sentences are inserted between paragraphs, by convention we treat them as part of the previous paragraph." ></td>
	<td class="line x" title="187:246	Evaluation Measures We evaluate our model using insertion accuracy at the section and paragraph level." ></td>
	<td class="line x" title="188:246	This measure computes the percentage of matches between the predicted location of the insertion and the true placement." ></td>
	<td class="line x" title="189:246	We also report the tree distance between the predicted position and the true location of an inserted sentence." ></td>
	<td class="line x" title="190:246	Tree distance is defined as the length of the path through the tree which connects the predicted and the true paragraph positions." ></td>
	<td class="line x" title="191:246	This measure captures section level errors (which raise the connecting path higher up the tree) as well as paragraph level errors (which widen the path across the tree)." ></td>
	<td class="line x" title="192:246	Baselines Our first three baselines correspond to naive insertion strategies." ></td>
	<td class="line x" title="193:246	The RANDOMINS method randomly selects a paragraph for a new sentence, while FIRSTINS and LASTINS insert a sentence into the first and the last paragraph, respectively." ></td>
	<td class="line x" title="194:246	We also compare our HIERARCHICAL method against two competitive baselines, PIPELINE and FLAT." ></td>
	<td class="line x" title="195:246	The PIPELINE method separately trains two rankers, one for section selection and one for paragraph selection." ></td>
	<td class="line x" title="196:246	During decoding, the PIPELINE method first chooses the best section according to the section-layer ranker, and then selects the best paragraph within the chosen section according to the paragraph-layer ranker." ></td>
	<td class="line x" title="197:246	The FLAT method uses the same decoding criterion as our model (Equation 2), thus making use of all the same features." ></td>
	<td class="line x" title="198:246	However, FLAT is trained with the standard ranking perceptron update, without making use of the hierarchical decomposition of features in Equation 1." ></td>
	<td class="line x" title="199:246	Human Performance To estimate the difficulty of sentence insertion, we conducted experiments that evaluate human performance on the task." ></td>
	<td class="line x" title="200:246	Four judges collectively processed 80 sentence/article pairs which were randomly extracted from the test set." ></td>
	<td class="line x" title="201:246	Each insertion was processed by two annotators." ></td>
	<td class="line x" title="202:246	Table 1 shows the insertion accuracy for each judge when compared against the Wikipedia gold standard." ></td>
	<td class="line x" title="203:246	On average, the annotators achieve 66% accuracy in section placement and 53% accuracy Section Paragraph Tree Dist RANDOMINS 0.318* 0.134* 3.10* FIRSTINS 0.250* 0.136* 3.23* LASTINS 0.305* 0.215* 2.96* PIPELINE 0.579 0.314* 2.21* FLAT 0.593 0.313* 2.19* HIERARCHY 0.598 0.383 2.04 Table 2: Accuracy of automatic insertion methods compared against the gold standard from Wikipedias update log." ></td>
	<td class="line x" title="204:246	The third column gives tree distance, where a lower score corresponds to better performance." ></td>
	<td class="line x" title="205:246	Diacritic * (p < 0.01) indicates whether differences in accuracy between the given model and the Hierarchical model is significant (using a Fisher Sign Test)." ></td>
	<td class="line x" title="206:246	in paragraph placement." ></td>
	<td class="line x" title="207:246	We obtain similar results when we compare the agreement of the judges against each other: 65% of section inserts and 48% of paragraph inserts are identical between two annotators." ></td>
	<td class="line x" title="208:246	The degree of variability observed in this experiment is consistent with human performance on other text structuring tasks such as sentence ordering (Barzilay et al. , 2002; Lapata, 2003)." ></td>
	<td class="line x" title="209:246	6 Results Table 2 shows the insertion performance of our model and the baselines in terms of accuracy and tree distance error." ></td>
	<td class="line x" title="210:246	The two evaluation measures are consistent in that they yield roughly identical rankings of the systems." ></td>
	<td class="line x" title="211:246	Assessment of statistical significance is performed using a Fisher Sign Test." ></td>
	<td class="line x" title="212:246	We apply this test to compare the accuracy of the HIERARCHICAL model against each of the baselines." ></td>
	<td class="line x" title="213:246	The results in Table 2 indicate that the naive insertion baselines (RANDOMINS, FIRSTINS, LASTINS) fall substantially behind the more sophisticated, trainable strategies (PIPELINE, FLAT, HIERARCHICAL)." ></td>
	<td class="line x" title="214:246	Within the latter group, our HIERARCHICAL model slightly outperforms the others based on the coarse measure of accuracy at the section level." ></td>
	<td class="line x" title="215:246	However, in the final paragraph-level analysis, the performance gain of our model over its counterparts is quite significant." ></td>
	<td class="line x" title="216:246	Moreover, according to tree distance error, which incorporates error at both the section and the paragraph level, the performance of the 89 HIERARCHICAL method is clearly superior." ></td>
	<td class="line x" title="217:246	This result confirms the benefit of our selective update mechanism as well as the overall importance of joint learning." ></td>
	<td class="line x" title="218:246	Viewing human performance as an upper bound for machine performance highlights the gains of our algorithm." ></td>
	<td class="line x" title="219:246	We observe that the gap between our method and human performance at the paragraph level is 32% smaller than that between the PIPELINE model and human performance, as well as the FLAT model and human performance." ></td>
	<td class="line x" title="220:246	Sentence-level Evaluation Until this point, we have evaluated the accuracy of insertions at the paragraph level, remaining agnostic as to the specific placement within the predicted paragraph." ></td>
	<td class="line x" title="221:246	We perform one final evaluation to test whether the global hierarchical view of our algorithm helps in determining the exact insertion point." ></td>
	<td class="line x" title="222:246	To make sentencelevel insertion decisions, we use a local model in line with previous sentence-ordering work (Lapata, 2003; Bollegala et al. , 2006)." ></td>
	<td class="line x" title="223:246	This model examines the two surrounding sentences of each possible insertion point and extracts a feature vector that includes lexical, positional, and temporal properties." ></td>
	<td class="line x" title="224:246	The model weights are trained using the standard ranking perceptron (Collins, 2002)." ></td>
	<td class="line x" title="225:246	We apply this local insertion model in two different scenarios." ></td>
	<td class="line x" title="226:246	In the first, we ignore the global hierarchical structure of the document and apply the local insertion model to every possible sentence pair." ></td>
	<td class="line x" title="227:246	Using this strategy, we recover 24% of correct insertion points." ></td>
	<td class="line x" title="228:246	The second strategy takes advantage of global document structure by first applying our hierarchical paragraph selection method and only then applying the local insertion to pairs of sentences within the selected paragraph." ></td>
	<td class="line x" title="229:246	This approach yields 35% of the correct insertion points." ></td>
	<td class="line x" title="230:246	This statistically significant difference in performance indicates that purely local methods are insufficient when applied to complete real-world documents." ></td>
	<td class="line x" title="231:246	7 Conclusion and Future Work We have introduced the problem of sentence insertion and presented a novel corpus-based method for this task." ></td>
	<td class="line x" title="232:246	The main contribution of our work is the incorporation of a rich hierarchical text representation into a flexible learning approach for text structuring." ></td>
	<td class="line x" title="233:246	Our learning approach makes key use of the hierarchy by selecting to update only the layer found responsible for the incorrect prediction." ></td>
	<td class="line x" title="234:246	Empirical tests on a large collection of real-world insertion data confirm the advantage of this approach." ></td>
	<td class="line x" title="235:246	Sentence ordering algorithms too are likely to benefit from a hierarchical representation of text." ></td>
	<td class="line x" title="236:246	However, accounting for long-range discourse dependencies in the unconstrained ordering framework is challenging since these dependencies only appear when a particular ordering (or partial ordering) is considered." ></td>
	<td class="line x" title="237:246	An appealing future direction lies in simultaneously inducing hierarchical and linear structure on the input sentences." ></td>
	<td class="line x" title="238:246	In such a model, tree structure could be a hidden variable that is influenced by the observed linear order." ></td>
	<td class="line x" title="239:246	We are also interested in further developing our system for automatic update of Wikipedia pages." ></td>
	<td class="line x" title="240:246	Currently, our system is trained on insertions in which the sentences of the original text are not modified." ></td>
	<td class="line x" title="241:246	However, in some cases additional text revisions are required to guarantee coherence of the generated text." ></td>
	<td class="line x" title="242:246	Further research is required to automatically identify and handle such complex insertions." ></td>
	<td class="line x" title="243:246	Acknowledgments The authors acknowledge the support of the National Science Foundation (CAREER grant IIS0448168 and grant IIS-0415865) and the Microsoft Research Faculty Fellowship." ></td>
	<td class="line x" title="244:246	Any opinions, findings, and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the NSF." ></td>
	<td class="line x" title="245:246	Thanks to S.R.K. Branavan, Eugene Charniak, Michael Collins, Micha Elsner, Jacob Eisenstein, Dina Katabi, Igor Malioutov, Christina Sauper, Luke Zettlemoyer, and the anonymous reviewers for helpful comments and suggestions." ></td>
	<td class="line x" title="246:246	Data used in this work was collected and processed by Christina Sauper." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D07-1033
A New Perceptron Algorithm for Sequence Labeling with Non-Local Features
Kazama, Jun'ichi;Torisawa, Kentaro;"></td>
	<td class="line x" title="1:389	Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp." ></td>
	<td class="line x" title="2:389	315324, Prague, June 2007." ></td>
	<td class="line x" title="3:389	c2007 Association for Computational Linguistics A New Perceptron Algorithm for Sequence Labeling with Non-local Features Junichi Kazama and Kentaro Torisawa Japan Advanced Institute of Science and Technology (JAIST) Asahidai 1-1, Nomi, Ishikawa, 923-1292 Japan {kazama, torisawa}@jaist.ac.jp Abstract We cannot use non-local features with current major methods of sequence labeling such as CRFs due to concerns about complexity." ></td>
	<td class="line x" title="4:389	We propose a new perceptron algorithm that can use non-local features." ></td>
	<td class="line x" title="5:389	Our algorithm allows the use of all types of non-local features whose values are determinedfromthesequenceandthelabels." ></td>
	<td class="line x" title="6:389	The weights of local and non-local features are learned together in the training process with guaranteed convergence." ></td>
	<td class="line x" title="7:389	We present experimental results from the CoNLL 2003 named entity recognition (NER) task to demonstrate the performance of the proposed algorithm." ></td>
	<td class="line x" title="8:389	1 Introduction Many NLP tasks such as POS tagging and named entity recognition have recently been solved as sequence labeling." ></td>
	<td class="line x" title="9:389	Discriminative methods such as Conditional Random Fields (CRFs) (Lafferty et al. , 2001), Semi-Markov Random Fields (Sarawagi and Cohen, 2004), and perceptrons (Collins, 2002a) have been popular approaches for sequence labeling because of their excellent performance, which is mainly due to their ability to incorporate many kinds of overlapping and non-independent features." ></td>
	<td class="line x" title="10:389	However, the common limitation of these methods is that the features are limited to local features, which only depend on a very small number of labels (usually two: the previous and the current)." ></td>
	<td class="line x" title="11:389	Although this limitation makes training and inference tractable, it also excludes the use of possibly useful non-local features that are accessible after all labels are determined." ></td>
	<td class="line x" title="12:389	For example, non-local features such as same phrases in a document do not have different entity classes were shown to be useful in named entity recognition (Sutton and McCallum, 2004; Bunescu and Mooney, 2004; Finkel et al. , 2005; Krishnan and Manning, 2006)." ></td>
	<td class="line x" title="13:389	Weproposeanewperceptronalgorithminthispaper that can use non-local features along with local features." ></td>
	<td class="line x" title="14:389	Although several methods have already been proposed to incorporate non-local features (Sutton and McCallum, 2004; Bunescu and Mooney, 2004; Finkel et al. , 2005; Roth and Yih, 2005; Krishnan and Manning, 2006; Nakagawa and Matsumoto, 2006), these present a problem that the types of non-local features are somewhat constrained." ></td>
	<td class="line x" title="15:389	For example, Finkel et al.(2005) enabled the use of non-local features by using Gibbs sampling." ></td>
	<td class="line x" title="17:389	However, it is unclear how to apply their methodofdeterminingtheparametersofanon-local model to other types of non-local features, which they did not used." ></td>
	<td class="line x" title="18:389	Roth and Yih (2005) enabled the use of hard constraints on labels by using integer linear programming." ></td>
	<td class="line x" title="19:389	However, this is equivalent to only allowing non-local features whose weights are xed to negative innity." ></td>
	<td class="line x" title="20:389	Krishnan and Manning (2006) divided the model into two CRFs, where the second model uses the output of the rst as a kind of non-local information." ></td>
	<td class="line x" title="21:389	However, it is not possible to use non-local features that depend on the labels of the very candidate to be scored." ></td>
	<td class="line x" title="22:389	Nakagawa and Matsumoto (2006) used a Bolzmann distribution to model the correlation of the POS of words having the same lexical form in a document." ></td>
	<td class="line x" title="23:389	However, their method can only be applied when there are convenient links such as the same lexical form." ></td>
	<td class="line x" title="24:389	Since non-local features have not yet been extensively investigated, it is possible for us to nd new useful non-local features." ></td>
	<td class="line x" title="25:389	Therefore, our objective in this study was to establish a framework, where all 315 types of non-local features are allowed." ></td>
	<td class="line x" title="26:389	With non-local features, we cannot use efcient procedures such as forward-backward procedures and the Viterbi algorithm that are required in training CRFs (Lafferty et al. , 2001) and perceptrons (Collins, 2002a)." ></td>
	<td class="line oc" title="27:389	Recently, severalmethods(Collins and Roark, 2004; Daume III and Marcu, 2005; McDonald and Pereira, 2006) have been proposed with similar motivation to ours." ></td>
	<td class="line p" title="28:389	These methods alleviate this problem by using some approximation in perceptron-type learning." ></td>
	<td class="line x" title="29:389	In this paper, we follow this line of research and try to solve the problem by extending Collins perceptron algorithm (Collins, 2002a)." ></td>
	<td class="line x" title="30:389	We exploited the not-so-familiar fact that we can design a perceptron algorithm with guaranteed convergence if we can nd at least one wrong labeling candidate even if we cannot perform exact inference." ></td>
	<td class="line x" title="31:389	We rst ran the A* search only using local features to generate n-best candidates (this can be efciently performed), and then we only calculated the true score with non-local features for these candidates to nd a wrong labeling candidate." ></td>
	<td class="line x" title="32:389	The second key idea was to update the weights of local features during training if this was necessary to generate sufciently good candidates." ></td>
	<td class="line x" title="33:389	The proposed algorithm combined these ideas to achieve guaranteed convergence and effective learning with non-local features." ></td>
	<td class="line x" title="34:389	The remainder of the paper is organized as follows." ></td>
	<td class="line x" title="35:389	Section 2 introduces the Collins perceptron algorithm." ></td>
	<td class="line x" title="36:389	Although this algorithm is the starting point for our algorithm, its baseline performance is not outstanding." ></td>
	<td class="line x" title="37:389	Therefore, we present a margin extension to the Collins perceptron in Section 3." ></td>
	<td class="line x" title="38:389	This margin perceptron became the direct basis of our algorithm." ></td>
	<td class="line x" title="39:389	We then explain our algorithm for nonlocal features in Section 4." ></td>
	<td class="line x" title="40:389	We report the experimental results using the CoNLL 2003 shared task dataset in Section 6." ></td>
	<td class="line x" title="41:389	2 Perceptron Algorithm for Sequence Labeling Collins (2002a) proposed an extension of the perceptron algorithm (Rosenblatt, 1958) to sequence labeling." ></td>
	<td class="line x" title="42:389	Our aim in sequence labeling is to assign label yi  Y to each word xi  X in a sequence." ></td>
	<td class="line x" title="43:389	We denote sequence x1,,xT as x and the corresponding labels as y. We assume weight vector   Rd and feature mapping  that maps each (x,y) to feature vector (x,y) = (1(x,y),,d(x,y))  Rd. The model determines the labels by: y = argmaxyY|x|(x,y), where  denotes the inner product." ></td>
	<td class="line x" title="44:389	The aim of the learning algorithm is to obtain an appropriate weight vector, , given training set {(x1,y1),,(xL,yL)}." ></td>
	<td class="line x" title="45:389	The learning algorithm, which is illustrated in Collins (2002a), proceeds as follows." ></td>
	<td class="line x" title="46:389	The weight vector is initialized to zero." ></td>
	<td class="line x" title="47:389	The algorithm passes over the training examples, and each sequence is decoded using the current weights." ></td>
	<td class="line x" title="48:389	If y is not the correct answery, the weights are updated according to the following rule." ></td>
	<td class="line x" title="49:389	new = + (x,y)(x,y)." ></td>
	<td class="line x" title="50:389	This algorithm is proved to converge (i.e. , there are no more updates) in the separable case (Collins, 2002a).1 Thatis,ifthereexistweightvectorU (with ||U|| = 1),  (> 0), and R (> 0) that satisfy: i,y  Y|xi| (xi,yi)U (xi,y)U  , i,y  Y|xi| ||(xi,yi)(xi,y)||  R, the number of updates is at most R2/2." ></td>
	<td class="line x" title="51:389	The perceptron algorithm only requires one candidatey foreachsequencexi, unlikethetrainingof CRFs where all possible candidates need to be considered." ></td>
	<td class="line x" title="52:389	This inherent property is the key to training with non-local features." ></td>
	<td class="line x" title="53:389	However, note that the tractability of learning and inference relies on how efciently y can be found." ></td>
	<td class="line x" title="54:389	In practice, we can nd y efciently using a Viterbi-type algorithm only when the features are all local, i.e., s(x,y) can be written as the sum of (two label) local features s as s(x,y) = Ti s(x,yi1,yi)." ></td>
	<td class="line x" title="55:389	This locality constraint is also required to make the training of CRFs tractable (Lafferty et al. , 2001)." ></td>
	<td class="line x" title="56:389	One problem with the perceptron algorithm described so far is that it offers no treatment for overtting." ></td>
	<td class="line x" title="57:389	Thus, Collins (2002a) also proposed an averaged perceptron, where the nal weight vector is 1Collins(2002a)alsoprovidedproofthatguaranteedgood learning for the non-separable case." ></td>
	<td class="line x" title="58:389	However, we have only considered the separable case throughout the paper." ></td>
	<td class="line x" title="59:389	316 Algorithm 3.1: Perceptron with margin for sequence labeling (parameters: C)   0 until no more updates do for i  1 to L do8 >> >< >> >: y = argmaxy(xi,y) y = 2nd-besty(xi,y) if y = yi then  = + (xi,yi)(xi,y) else if (xi,yi)(xi,y)  C then  = + (xi,yi)(xi,y) the average of all weight vectors during training." ></td>
	<td class="line x" title="60:389	Howerver, we found in our experiments that the averaged perceptron performed poorly in our setting." ></td>
	<td class="line x" title="61:389	We therefore tried to make the perceptron algorithm more robust to overtting." ></td>
	<td class="line x" title="62:389	We will describe our extension to the perceptron algorithm in the next section." ></td>
	<td class="line x" title="63:389	3 Margin Perceptron Algorithm for Sequence Labeling Weextendedaperceptronwithamargin(Krauthand Mezard, 1987) to sequence labeling in this study, as Collins (2002a) extended the perceptron algorithm to sequence labeling." ></td>
	<td class="line x" title="64:389	In the case of sequence labeling, the margin is dened as: () = minx i miny =yi (xi,yi)(xi,y) |||| Assuming that the best candidate,y, equals the correct answer, y, the margin can be re-written as: = minx i (xi,yi)(xi,y) ||||, wherey = 2nd-besty(xi,y)." ></td>
	<td class="line x" title="65:389	Using this relation, theresultingalgorithmbecomesAlgorithm3.1." ></td>
	<td class="line x" title="66:389	The algorithm tries to enlarge the margin as much as possible, as well as make the best scoring candidate equal the correct answer." ></td>
	<td class="line x" title="67:389	Constant C in Algorithm 3.1 is a tunable parameter, which controls the trade-off between the margin and convergence time." ></td>
	<td class="line x" title="68:389	Based on the proofs in Collins (2002a) and Li et al.(2002), we can prove that the algorithm converges within (2C + R2)/2 updates and that ()  C/(2C + R2) = (/2)(1  (R2/(2C + R2))) after training." ></td>
	<td class="line x" title="70:389	As can be seen, the margin approaches at least half of true margin  (at the cost of innite training time), as C  ." ></td>
	<td class="line x" title="71:389	Note that if the features are all local, the secondbestcandidate(generallyn-bestcandidates)canalso be found efciently by using an A* search that uses the best scores calculated during a Viterbi search as the heuristic estimation (Soong and Huang, 1991)." ></td>
	<td class="line x" title="72:389	There are other methods for improving robustness by making margin larger for the structural output problem." ></td>
	<td class="line x" title="73:389	Such methods include ALMA (Gentile, 2001) used in (Daume III and Marcu, 2005)2, MIRA (Crammer et al. , 2006) used in (McDonald et al. , 2005), and Max-Margin Markov Networks (Taskar et al. , 2003)." ></td>
	<td class="line x" title="74:389	However, to the best of our knowledge, there has been no prior work that has applied a perceptron with a margin (Krauth and Mezard, 1987) to structured output.3 Our method described in this section is one of the easiest to implement, while guaranteeingalargemargin." ></td>
	<td class="line x" title="75:389	WefoundintheexperimentsthatourmethodoutperformedtheCollinsaveraged perceptron by a large margin." ></td>
	<td class="line x" title="76:389	4 Algorithm 4.1 Denition and Basic Idea Having described the basic perceptron algorithms, we will know explain our algorithm that learns the weights of local and non-local features in a unied way." ></td>
	<td class="line x" title="77:389	Assume that we have local features and nonlocal features." ></td>
	<td class="line x" title="78:389	We use the superscript, l, for local features as li(x,y) and g for non-local features as gi(x,y)." ></td>
	<td class="line x" title="79:389	Then, feature mapping is written as a(x,y) = l(x,y) + g(x,y) = (l1(x,y),,ln(x,y),gn+1(x,y),,gd(x,y))." ></td>
	<td class="line x" title="80:389	Here, we dene: l(x,y) = (l1(x,y),,ln(x,y),0,,0) g(x,y) = (0,,0,gn+1(x,y),,gd(x,y)) Ideally, we want to determine the labels using the whole feature set as: y = argmaxyY|x|a(x,y)." ></td>
	<td class="line xc" title="81:389	2(Daume III and Marcu, 2005) also presents the method using the averaged perceptron (Collins, 2002a) 3For re-ranking problems, Shen and Joshi (2004) proposed a perceptron algorithm that also uses margins." ></td>
	<td class="line x" title="82:389	The difference is that our algorithm trains the sequence labeler itself and is much simpler because it only aims at labeling." ></td>
	<td class="line x" title="83:389	317 Algorithm 4.1: Candidate algorithm (parameters: n, C)   0 until no more updates do for i  1 to L do8 >> >> >< >> >> >: {yn} = n-bestyl(xi,y) y = argmaxy{yn}a(xi,y) y = 2nd-besty{yn}a(xi,y) if y = yi & a(xi,yi)a(xi,y)  C then  = + a(xi,yi)a(xi,y) else if a(xi,yi)a(xi,y)  C then  = + a(xi,yi)a(xi,y) However, if there are non-local features, it is impossible to nd the highest scoring candidate efciently, since we cannot use the Viterbi algorithm." ></td>
	<td class="line x" title="84:389	Thus, we cannot use the perceptron algorithms described in the previous sections." ></td>
	<td class="line x" title="85:389	The training of CRFs is also intractable for the same reason." ></td>
	<td class="line x" title="86:389	To deal with this problem, we rst relaxed our objective." ></td>
	<td class="line x" title="87:389	The modied objective was to nd a good model from those with the form: {yn} = n-bestyl(x,y) y = argmaxy{yn}a(x,y), (1) That is, we rst generate n-best candidates {yn} under the local model, l(x,y)  ." ></td>
	<td class="line x" title="88:389	This can be done efciently using the A* algorithm." ></td>
	<td class="line x" title="89:389	We then ndthebestscoringcandidateunderthetotalmodel, a(x,y), only from these n-best candidates." ></td>
	<td class="line x" title="90:389	If n is moderately small, this can also be done in a practical amount of time." ></td>
	<td class="line x" title="91:389	This resembles the re-ranking approach (Collins and Duffy, 2002; Collins, 2002b)." ></td>
	<td class="line x" title="92:389	However, unlike the re-ranking approach, the local model, l(x,y) , and the total model, a(x,y), correlate since they share a part of the vector and are trained at the same time in our algorithm." ></td>
	<td class="line x" title="93:389	The re-ranking approach has the disadvantage that it is necessary to use different training corpora for the rst model and for the second, or to use cross validation type training, to make the training for the second meaningful." ></td>
	<td class="line x" title="94:389	This reduces the effective size of training data or increases training time substantially." ></td>
	<td class="line x" title="95:389	On the other hand, our algorithm has no such disadvantage." ></td>
	<td class="line x" title="96:389	However, we are no longer able to nd the highest scoring candidate under a(x,y)   exactly with this approach." ></td>
	<td class="line x" title="97:389	We cannot thus use the perceptron algorithms directly." ></td>
	<td class="line x" title="98:389	However, by examining the Algorithm 4.2: Perceptron with local and non-local features (parameters: n, Ca, Cl)   0 until no more updates do for i  1 to L do8 >> >> >> >> >> < >> >> >> >> >> : {yn} = n-bestyl(xi,y) y = argmaxy{yn}a(xi,y) y = 2nd-besty{yn}a(xi,y) if y = yi & a(xi,yi)a(xi,y)  Ca then  = + a(xi,yi)a(xi,y) (A) else if a(xi,yi)a(xi,y)  Ca then  = + a(xi,yi)a(xi,y) (A) else (B) 8> < >: if y1 = yi then (y1 represents the best in {yn})  = + l(xi,yi)l(xi,y1) else if l(xi,yi)l(xi,y2)  Cl then  = + l(xi,yi)l(xi,y2) proofs in Collins (2002a), we can see that the essential condition for convergence is that the weights are always updated using some y (= y) that satises: (xi,yi)(xi,y)  0 ( C in the case of a perceptron with a margin)." ></td>
	<td class="line x" title="99:389	(2) That is, y does not necessarily need to be the exact best candidate or the exact second-best candidate." ></td>
	<td class="line x" title="100:389	The algorithm also converges in a nite number of iterations even with Eq." ></td>
	<td class="line x" title="101:389	(1) as long as Eq." ></td>
	<td class="line x" title="102:389	(2) is satised." ></td>
	<td class="line x" title="103:389	4.2 Candidate Algorithm The algorithm we came up with rst based on the above idea, is Algorithm 4.1." ></td>
	<td class="line x" title="104:389	We rst nd the nbest candidates using the local model, l(x,y)." ></td>
	<td class="line x" title="105:389	At this point, we can determine the value of the nonlocal features, g(x,y), to form the whole feature vector, a(x,y), for the n-best candidates." ></td>
	<td class="line x" title="106:389	Next, we re-score and sort them using the total model, a(x,y)  , to nd a candidate that violates the margin condition." ></td>
	<td class="line x" title="107:389	We call this algorithm the candidate algorithm." ></td>
	<td class="line x" title="108:389	After the training has nished, a(xi,yi)    a(xi,y)   > C is guaranteed for all (xi,y) where y  {yn},y = y." ></td>
	<td class="line x" title="109:389	At rst glance, this seems sufcient condition for good models." ></td>
	<td class="line x" title="110:389	However, this is not true because if y  {yn}, the inference dened by Eq." ></td>
	<td class="line x" title="111:389	(1) is not guaranteed to nd the correct answer, y." ></td>
	<td class="line x" title="112:389	In fact, this algorithm does not work well with non-local features as we found in the experiments." ></td>
	<td class="line x" title="113:389	318 4.3 Final Algorithm Our idea for improving the above algorithm is that thelocalmodel,l(x,y),mustatleastbesogood that y  {yn}." ></td>
	<td class="line x" title="114:389	To achieve this, we added a modication term that was intended to improve the local model when the local model was not good enough even when the total model was good enough." ></td>
	<td class="line x" title="115:389	The nal algorithm resulted in Algorithm 4.2." ></td>
	<td class="line x" title="116:389	As canbeseen, thepartmarked(B)hasbeenadded." ></td>
	<td class="line x" title="117:389	We call this algorithm the proposed algorithm." ></td>
	<td class="line x" title="118:389	Note that the algorithm prioritizes the update of the total model, (A), over that of the local model, (B), although the opposite is also possible." ></td>
	<td class="line x" title="119:389	Also note that the update of the local model in (B) is aggressive since it updates the weights until the best candidate output by the local model becomes the correct answer and satises the margin condition." ></td>
	<td class="line x" title="120:389	A conservative updating, where we cease the update when the n-best candidates contain the correct answer, is also possible from our idea above." ></td>
	<td class="line x" title="121:389	We made these choices since they worked better than the other alternatives." ></td>
	<td class="line x" title="122:389	The tunable parameters are the local margin parameter, Cl, the total margin parameter, Ca, and n for the n-best search." ></td>
	<td class="line x" title="123:389	We used C = Cl = Ca in this study to reduce the search space." ></td>
	<td class="line x" title="124:389	We can prove that the algorithm in Algorithm 4.2 also converges in a nite number of iterations." ></td>
	<td class="line x" title="125:389	It converges within (2C + R2)/2 updates, assuming that there exist weight vector Ul (with ||Ul|| = 1 and Uli = 0 (n+1  i  d)),  (> 0), and R (> 0) that satisfy: i,y  Y|xi| l(xi,yi)Ull(xi,y)Ul  , i,y  Y|xi| ||a(xi,yi)a(xi,y)||  R. In addition, we can prove that ()  C/(2C + R2) for the margin after convergence, where () is dened as: minx i miny {yn},=yi a(xi,yi)a(xi,y) |||| See Appendix A for the proofs." ></td>
	<td class="line x" title="126:389	We also incorporated the idea behind Bayes point machines (BPMs) (Herbrich and Graepel, 2000) to improvetherobustnessofourmethodfurther." ></td>
	<td class="line x" title="127:389	BPMs try to cancel out overtting caused by the order of examples, by training several models by shufing the training examples.4 However, it is very time consuming to run the complete training process several times." ></td>
	<td class="line x" title="128:389	We thus ran the training in only one pass over the shufed examples several times, and used the averaged output weight vectors as a new initial weight vector, because we thought that the early part of training would be more seriously affected by the order of examples." ></td>
	<td class="line x" title="129:389	We call this BPM initialization." ></td>
	<td class="line x" title="130:389	5 5 Named Entity Recognition and Non-Local Features We evaluated the performance of the proposed algorithm using the named entity recognition task." ></td>
	<td class="line x" title="131:389	We adopted IOB (IOB2) labeling (Ramshaw and Marcus, 1995), where the rst word of an entity of class C is labeled B-C, the words in the entity are labeled I-C, and other words are labeled O." ></td>
	<td class="line x" title="132:389	We used non-local features based on Finkel et al.(2005)." ></td>
	<td class="line x" title="134:389	These features are based on observations such as same phrases in a document tend to have the same entity class (phrase consistency) and a sub-phrase of a phrase tends to have the same entity class as the phrase (sub-phrase consistency)." ></td>
	<td class="line x" title="135:389	We also implemented the majority version of these features as used in Krishnan and Manning (2006)." ></td>
	<td class="line x" title="136:389	In addition, we used non-local features, which are based on the observation that entities tend to have the same entity class if they are in the same conjunctiveordisjunctiveexpressionasin inU.S., EU,andJapan(conjunctionconsistency)." ></td>
	<td class="line x" title="137:389	Thistype of non-local feature was not used by Finkel et al.(2005) or Krishnan and Manning (2006)." ></td>
	<td class="line x" title="139:389	6 Experiments 6.1 Data and Setting We used the English dataset of the CoNLL 2003 named entity shared task (Tjong et al. , 2003) for the experiments." ></td>
	<td class="line x" title="140:389	It is a corpus of English newspaper articles, where four entity classes, PER, LOC, ORG, and MISC are annotated." ></td>
	<td class="line x" title="141:389	It consists of training, development, and testing sets (14,987, 3,466, 4The results for the perceptron algorithms generally depend on the order of the training examples." ></td>
	<td class="line x" title="142:389	5Note that we can prove that the perceptron algorithms converge even though the weight vector is not initialized as = 0." ></td>
	<td class="line x" title="143:389	319 and 3,684 sentences, respectively)." ></td>
	<td class="line x" title="144:389	Automatically assigned POS tags and chunk tags are also provided." ></td>
	<td class="line x" title="145:389	TheCoNLL2003datasetcontainsdocumentboundary markers." ></td>
	<td class="line x" title="146:389	We concatenated the sentences in the same document according to these markers.6 This generated 964 documents for the training set, 216 documents for the development set, and 231 documents for the testing set." ></td>
	<td class="line x" title="147:389	The documents generated as above become the sequence, x, in the learning algorithms." ></td>
	<td class="line x" title="148:389	We rst evaluated the baseline performance of a CRF model, the Collins perceptron, and the Collins averaged perceptron, as well as the margin perceptron, with only local features." ></td>
	<td class="line x" title="149:389	We next evaluated the performance of our perceptron algorithm proposed for non-local features." ></td>
	<td class="line x" title="150:389	We used the local features summarized in Table 1, which are similar to those used in other studies on named entity recognition." ></td>
	<td class="line x" title="151:389	We omitted features whose surface part listed in Table 1 occurred less than twice in the training corpus." ></td>
	<td class="line x" title="152:389	We used CRF++ (ver." ></td>
	<td class="line x" title="153:389	0.44)7 as the basis of our implementation." ></td>
	<td class="line x" title="154:389	We implemented scaling, which is similar to that for HMMs (see such as (Rabiner, 1989)),intheforward-backwardphaseofCRFtraining to deal with very long sequences due to sentence concatenation.8 We used Gaussian regularization (Chen and Rosenfeld, 2000) for CRF training to avoid overtting." ></td>
	<td class="line x" title="155:389	The parameter of the Gaussian, 2, was tuned usingthedevelopmentset." ></td>
	<td class="line x" title="156:389	Wealsotunedthemargin parameter, C, for the margin perceptron algorithm.9 TheconvergenceofCRFtrainingwasdeterminedby checking the log-likelihood of the model." ></td>
	<td class="line x" title="157:389	The convergence of perceptron algorithms was determined by checking the per-word labeling error, since the 6We used sentence concatenation even when only using local features, since we found it does not degrade accuracy (rather we observed a slight increase)." ></td>
	<td class="line x" title="158:389	7http://chasen.org/taku/software/CRF++ 8We also replaced the optimization module in the original package with that used in the Amis maximum entropy estimator (http://www-tsujii.is.s.u-tokyo.ac.jp/amis) since we encountered problems with the provided module in some cases." ></td>
	<td class="line x" title="159:389	9For the Gaussian parameter, we tested {13, 25, 50, 100, 200, 400, 800} (the accuracy did not change drastically among these values and it seems that there is no accuracy hump even if we use smaller values)." ></td>
	<td class="line x" title="160:389	We tested {500, 1000, 1414, 2000, 2828, 4000, 5657, 8000, 11313, 16000, 32000} for the margin parameters." ></td>
	<td class="line x" title="161:389	Table 1: Local features used." ></td>
	<td class="line x" title="162:389	The value of a node feature is determined from the current label, y0, and a surface feature determined only fromx." ></td>
	<td class="line x" title="163:389	The value of an edge feature is determined by the previous label, y1, the current label, y0, and a surface feature." ></td>
	<td class="line x" title="164:389	Used surface features are the word (w), the downcased word (wl), the POS tag (pos), the chunk tag (chk), the prex of the word of length n (pn), the sufx(sn), thewordformfeatures: 2d-cp(theseare based on (Bikel et al. , 1999)), and the gazetteer features: go for ORG, gp for PER, and gm for MISC." ></td>
	<td class="line x" title="165:389	These represent the (longest) match with an entry in the gazetteer by using IOB2 tags." ></td>
	<td class="line x" title="166:389	Node features: {,x2,x1,x0,x+1,x+2}y0 x =, w, wl, pos, chk, p1, p2, p3, p4, s1, s2, s3, s4, 2d, 4d, d&a, d&-, d&/, d&,, d&., n, ic, ac, l, cp, go, gp, gm Edge features: {,x2,x1,x0,x+1,x+2}y1 y0 x =, w, wl, pos, chk, p1, p2, p3, p4, s1, s2, s3, s4, 2d, 4d, d&a, d&-, d&/, d&,, d&., n, ic, ac, l, cp, go, gp, gm Bigram node features: {x2x1,x1x0,x0x+1}y0 x = wl, pos, chk, go, gp, gm Bigram edge features: {x2x1,x1x0,x0x+1}y1 y0 x = wl, pos, chk, go, gp, gm number of updates was not zero even after a large number of iterations in practice." ></td>
	<td class="line x" title="169:389	We stopped training when the relative change in these values became less than a pre-dened threshold (0.0001) for at least three iterations." ></td>
	<td class="line x" title="170:389	We used n = 20 (n of the n-best) for training sincewecouldnotusetooalargenbecauseitwould have slowed down training." ></td>
	<td class="line x" title="171:389	However, we could examinealargernduringtesting,sincethetestingtime did not dominate the time for the experiment." ></td>
	<td class="line x" title="172:389	We found an interesting property for n in our preliminary experiment." ></td>
	<td class="line x" title="173:389	We found that an even larger n in testing (written as n) achieved higher accuracy, although it is natural to assume that the same n that was used in training would also be appropriate for testing." ></td>
	<td class="line x" title="174:389	We thus used n = 100 to evaluate performance during parameter tuning." ></td>
	<td class="line x" title="175:389	After nding the best C with n = 100, we varied n to investigate its 320 Table 2: Summary of performance (F1)." ></td>
	<td class="line x" title="176:389	Method dev test C (or 2) local features CRF 91.10 86.26 100 Perceptron 89.01 84.03 Averaged perceptron 89.32 84.08 Margin perceptron 90.98 85.64 11313 + non-local features Candidate (n = 100) 90.71 84.90 4000 Proposed (n = 100) 91.95 86.30 5657 Table 3: Effect of n." ></td>
	<td class="line x" title="177:389	Method dev test C Proposed (n = 20) 91.76 86.19 5657 Proposed (n = 100) 91.95 86.30 5657 Proposed (n = 400) 92.13 86.39 5657 Proposed (n = 800) 92.09 86.39 5657 Proposed (n = 1600) 92.13 86.46 5657 Proposed (n = 6400) 92.19 86.38 5657 effects further." ></td>
	<td class="line x" title="178:389	6.2 Results Table 2 compares the results." ></td>
	<td class="line x" title="179:389	CRF outperformed the perceptron by a large margin." ></td>
	<td class="line x" title="180:389	Although the averaged perceptron outperformed the perceptron, the improvement was slight." ></td>
	<td class="line x" title="181:389	However, the margin perceptron greatly outperformed compared to the averaged perceptron." ></td>
	<td class="line x" title="182:389	Yet, CRF still had the best baseline performance with only local features." ></td>
	<td class="line x" title="183:389	The proposed algorithm with non-local features improved the performance on the test set by 0.66 points over that of the margin perceptron without non-local features." ></td>
	<td class="line x" title="184:389	The row Candidate refers to the candidate algorithm (Algorithm 4.1)." ></td>
	<td class="line x" title="185:389	From the results for the candidate algorithm, we can see that the modication part, (B), in Algorithm 4.2 was essential to make learning with non-local features effective." ></td>
	<td class="line x" title="186:389	We next examined the effect of n." ></td>
	<td class="line x" title="187:389	As can be seen from Table 3, an n larger than that for training yields higher performance." ></td>
	<td class="line x" title="188:389	The highest performance with the proposed algorithm was achieved when n = 6400, where the improvement due to non-local features became 0.74 points." ></td>
	<td class="line x" title="189:389	The performance of the related work (Finkel et al. , 2005; Krishnan and Manning, 2006) is listed in Table4." ></td>
	<td class="line x" title="190:389	Wecanseethatthenalperformanceofour algorithm was worse than that of the related work." ></td>
	<td class="line x" title="191:389	We changed the experimental setting slightly to investigate our algorithm further." ></td>
	<td class="line x" title="192:389	Instead of Table 4: The performance of the related work." ></td>
	<td class="line x" title="193:389	Method dev test Finkel et al. , 2005 (Finkel et al. , 2005) baseline CRF 85.51 + non-local features 86.86 Krishnan and Manning, 2006 (Krishnan and Manning, 2006) baseline CRF 85.29 + non-local features 87.24 Table 5: Summary of performance with POS/chunk tags by TagChunk." ></td>
	<td class="line x" title="194:389	Method dev test C (or 2) local features CRF 91.39 86.30 200 Perceptron 89.36 84.35 Averaged perceptron 89.76 84.50 Margin perceptron 91.06 86.24 32000 + non-local features Proposed (n = 100) 92.23 87.04 5657 Proposed (n = 6400) 92.54 87.17 5657 the POS/chunk tags provided in the CoNLL 2003 dataset, we used the tags assigned by TagChunk (Daume III and Marcu, 2005)10 with the intention of using more accurate tags." ></td>
	<td class="line x" title="195:389	The results with this setting are summarized in Table 5." ></td>
	<td class="line x" title="196:389	Performance was better than that in the previous experiment for all algorithms." ></td>
	<td class="line x" title="197:389	We think this was due to the quality of the POS/chunk tags." ></td>
	<td class="line x" title="198:389	It is interesting that the effect of non-local features rose to 0.93 points with n = 6400, even though the baseline performance was also improved." ></td>
	<td class="line x" title="199:389	The resulting performance of the proposed algorithm with non-local features is higher than that of Finkel et al.(2005) and comparable with that of Krishnan and Manning (2006)." ></td>
	<td class="line x" title="201:389	This comparison, of course, is not fair because the setting was different." ></td>
	<td class="line x" title="202:389	However, we think the results demonstrate a potential of our new algorithm." ></td>
	<td class="line x" title="203:389	The effect of BPM initialization was also examined." ></td>
	<td class="line x" title="204:389	The number of BPM runs was 10 in this experiment." ></td>
	<td class="line x" title="205:389	The performance of the proposed algorithm dropped from 91.95/86.30 to 91.89/86.03 without BPM initialization as expected in the setting of the experiment of Table 2." ></td>
	<td class="line x" title="206:389	The performance of the margin perceptron, on the other hand, changed from 90.98/85.64 to 90.98/85.90 without BPM initialization." ></td>
	<td class="line x" title="207:389	This result was unexpected from the result of our preliminary experiment." ></td>
	<td class="line x" title="208:389	However, the performance was changed from 91.06/86.24 to 10http://www.cs.utah.edu/hal/TagChunk/ 321 Table 6: Comparison with re-ranking approach." ></td>
	<td class="line x" title="209:389	Method dev test C local features Margin Perceptron 91.06 86.24 32000 + non-local features Re-ranking 1 (n = 100) 91.62 86.57 4000 Re-ranking 1 (n = 80) 91.71 86.58 4000 Re-ranking 2 (n = 100) 92.08 86.86 16000 Re-ranking 2 (n = 800) 92.26 86.95 16000 Proposed (n = 100) 92.23 87.04 5657 Proposed (n = 6400) 92.54 87.17 5657 Table 7: Comparison of training time (C = 5657)." ></td>
	<td class="line x" title="210:389	Method dev test time (sec.)" ></td>
	<td class="line x" title="211:389	local features Margin Perceptron 91.04 86.28 15,977 + non-local features Re-ranking 1 (n = 100) 91.48 86.53 86,742 Re-ranking 2 (n = 100) 92.02 86.85 112,138 Proposed (n = 100) 92.23 87.04 28,880 91.17/86.08 (i.e. , dropped for the evaluation set as expected), in the setting of the experiment of Table 5." ></td>
	<td class="line x" title="212:389	Since the effect of BPM initialization is not conclusive only from these results, we need more experiments on this." ></td>
	<td class="line x" title="213:389	6.3 Comparison with re-ranking approach Finally, we compared our algorithm with the reranking approach (Collins and Duffy, 2002; Collins, 2002b), where we rst generate the n-best candidates using a model with only local features (the rst model) and then re-rank the candidates using a model with non-local features (the second model)." ></td>
	<td class="line x" title="214:389	We implemented two re-ranking models, reranking 1 and re-ranking 2." ></td>
	<td class="line x" title="215:389	These models differ in how to incorporate the local information in the second model." ></td>
	<td class="line x" title="216:389	re-ranking 1 uses the score of the rst model as a feature in addition to the non-local features as in Collins (2002b)." ></td>
	<td class="line x" title="217:389	re-ranking 2 uses the same local features as the rst model11 in addition to the non-local features." ></td>
	<td class="line x" title="218:389	The rst models were trained using the margin perceptron algorithm in Algorithm 3.1." ></td>
	<td class="line x" title="219:389	The second models were trained using the algorithm, which is obtained by replacing {yn} with the n-best candidates by the rst model." ></td>
	<td class="line x" title="220:389	The rstmodelusedtogeneraten-bestcandidatesforthe development set and the test set was trained using the whole training data." ></td>
	<td class="line x" title="221:389	However, CRFs or perceptrons generally have nearly zero error on the training data, although the rst model should mis-label 11The weights were re-trained for the second model." ></td>
	<td class="line x" title="222:389	to some extent to make the training of the second model meaningful." ></td>
	<td class="line x" title="223:389	To avoid this problem, we adopt cross-validation training as used in Collins (2002b)." ></td>
	<td class="line x" title="224:389	Wesplitthetrainingdatainto5sets." ></td>
	<td class="line x" title="225:389	Wethentrained ve rst models using 4/5 of the data, each of which was used to generate n-best candidates for the remaining 1/5 of the data." ></td>
	<td class="line x" title="226:389	As in the previous experiments, we tuned C using the development set with n = 100 and then tested othervaluesforn." ></td>
	<td class="line x" title="227:389	Table6showstheresults." ></td>
	<td class="line x" title="228:389	Ascan be seen, re-ranking models were outperformed by our proposed algorithm, although they also outperformed the margin perceptron with only local features (re-ranking 2 seems better than re-ranking 1)." ></td>
	<td class="line x" title="229:389	Table 7 shows the training time of each algorithm.12 Our algorithm is much faster than the reranking approach that uses cross-validation training, while achieving the same or higher level of performance." ></td>
	<td class="line oc" title="230:389	7 Discussion As we mentioned, there are some algorithms similar to ours (Collins and Roark, 2004; Daume III and Marcu, 2005; McDonald and Pereira, 2006; Liang et al. , 2006)." ></td>
	<td class="line o" title="231:389	The differences of our algorithm from these algorithms are as follows." ></td>
	<td class="line x" title="232:389	Daume III and Marcu (2005) presented the method called LaSO (Learning as Search Optimization), in which intractable exact inference is approximated by optimizing the behavior of the search process." ></td>
	<td class="line x" title="233:389	The method can access non-local features at each search point, if their values can be determinedfromthesearchdecisionsalreadymade." ></td>
	<td class="line x" title="234:389	They provided robust training algorithms with guaranteed convergence for this framework." ></td>
	<td class="line x" title="235:389	However, a difference is that our method can use non-local features whose value depends on all labels throughout training, and it is unclear whether the features whose values can only be determined at the end of the search (e.g. , majority features) can be learned effectively with such an incremental manner of LaSO." ></td>
	<td class="line x" title="236:389	The algorithm proposed by McDonald and Pereira (2006) is also similar to ours." ></td>
	<td class="line x" title="237:389	Their target was non-projective dependency parsing, where exact inference is intractable." ></td>
	<td class="line x" title="238:389	Instead of using 12Training time was measured on a machine with 2.33 GHz QuadCore Intel Xeons and 8 GB of memory." ></td>
	<td class="line x" title="239:389	C was xed to 5657." ></td>
	<td class="line x" title="240:389	322 n-best/re-scoring approach as ours, their method modies the single best projective parse, which can be found efciently, to nd a candidate with higher score under non-local features." ></td>
	<td class="line x" title="241:389	Liang et al.(2006) used n candidates of a beam search in the Collins perceptron algorithm for machine translation." ></td>
	<td class="line oc" title="243:389	CollinsandRoark(2004)proposedanapproximate incremental method for parsing." ></td>
	<td class="line o" title="244:389	Their method can be used for sequence labeling as well." ></td>
	<td class="line n" title="245:389	These studies, however, did not explain the validity of their updating methods in terms of convergence." ></td>
	<td class="line x" title="246:389	To achieve robust training, Daume III and Marcu (2005) employed the averaged perceptron (Collins, 2002a) and ALMA (Gentile, 2001)." ></td>
	<td class="line oc" title="247:389	Collins and Roark (2004) used the averaged perceptron (Collins, 2002a)." ></td>
	<td class="line x" title="248:389	McDonald and Pereira (2006) used MIRA (Crammer et al. , 2006)." ></td>
	<td class="line x" title="249:389	On the other hand, we employed the margin perceptron (Krauth and Mezard, 1987),extendingittosequencelabeling." ></td>
	<td class="line x" title="250:389	Wedemonstrated that this greatly improved robustness." ></td>
	<td class="line oc" title="251:389	With regard to the local update, (B), in Algorithm 4.2, early updates (Collins and Roark, 2004) and y-good requirement in (Daume III and Marcu, 2005) resemble our local update in that they tried to avoid the situation where the correct answer cannot be output." ></td>
	<td class="line x" title="252:389	Considering such commonality, the way of combining the local update and the non-local updatemightbeoneimportantkeyforfurtherimprovement." ></td>
	<td class="line x" title="253:389	It is still open whether these differences are advantages or disadvantages." ></td>
	<td class="line x" title="254:389	However, we think our algorithm can be a contribution to the study for incorporating non-local features." ></td>
	<td class="line x" title="255:389	The convergence guarantee is important for the condence in the training results, although it does not mean high performance directly." ></td>
	<td class="line x" title="256:389	Our algorithm could at least improve the accuracy of NER with non-local features and it was indicated that our algorithm was superior to the re-ranking approach in terms of accuracy and training cost." ></td>
	<td class="line x" title="257:389	However, the achieved accuracy was not better than that of related work (Finkel et al. , 2005; Krishnan and Manning, 2006) based on CRFs." ></td>
	<td class="line x" title="258:389	Although this might indicate the limitation of perceptron-based methods, it has also been shown that there is still room for improvement in perceptron-based algorithms as our margin perceptron algorithm demonstrated." ></td>
	<td class="line x" title="259:389	8 Conclusion In this paper, we presented a new perceptron algorithm for learning with non-local features." ></td>
	<td class="line x" title="260:389	We think the proposed algorithm is an important step towards achieving our nal objective." ></td>
	<td class="line x" title="261:389	We would like to investigate various types of new non-local features using the proposed algorithm in future work." ></td>
	<td class="line x" title="262:389	Appendix A: Convergence of Algorithm 4.2 Letk be a weight vector before the kth update and epsilon1k be a variable that takes 1 when the kth update is done in (A) and 0 when done in (B)." ></td>
	<td class="line x" title="263:389	The update rule can then be written ask+1 = k +epsilon1k(aa + (1epsilon1k)(l l).13 First, we obtain k+1 Ul = k Ul + epsilon1k(a Ul a Ul) +(1epsilon1k)(l Ul l Ul)  k Ul + epsilon1k + (1epsilon1k) = k Ul +   1 Ul + k = k Therefore, (k)2  (k+1  Ul)2  (||k+1||||Ul||)2 = ||k+1||2  (1)." ></td>
	<td class="line x" title="264:389	On the other hand, we also obtain ||k+1||2  ||k||2 + 2epsilon1kk(a a) +2(1epsilon1k)k(l l) +{epsilon1k(a a) + (1epsilon1k)(l l)}2  ||k||2 + 2C + R2  ||1||2 + k(R2 + 2C) = k(R2 + 2C) (2) We used k(a  a)  Ca, k(l  l)  Cl and Cl = Ca = C to derive 2C in the second inequality." ></td>
	<td class="line x" title="265:389	We used||ll||  ||aa||  R to derive R2." ></td>
	<td class="line x" title="266:389	Combining (1) and (2), we obtain k  (R2 + 2C)/2." ></td>
	<td class="line x" title="267:389	Substituting this into (2) gives ||k||  (R2+2C)/." ></td>
	<td class="line x" title="268:389	Sincey = y andaa > C after convergence, we obtain () = minx i a a  ||||  C/(2C + R 2)." ></td>
	<td class="line x" title="269:389	13We use the shorthand a = a(xi,y i),  a = a(xi,y), l = l(xi,yi), and l = l(xi,y) where y represents the candidate used to update (y, y, y1, or y2)." ></td>
	<td class="line x" title="270:389	323 References D. M. Bikel, R. L. Schwartz, and R. M. Weischedel." ></td>
	<td class="line x" title="271:389	1999." ></td>
	<td class="line x" title="272:389	An algorithm that learns whats in a name." ></td>
	<td class="line x" title="273:389	Machine Learning, 34(1-3):211231." ></td>
	<td class="line x" title="274:389	R. Bunescu and R. J. Mooney." ></td>
	<td class="line x" title="275:389	2004." ></td>
	<td class="line x" title="276:389	Collective information extraction with relational markov networks." ></td>
	<td class="line x" title="277:389	In ACL 2004." ></td>
	<td class="line x" title="278:389	S. F. Chen and R. Rosenfeld." ></td>
	<td class="line x" title="279:389	2000." ></td>
	<td class="line x" title="280:389	A survey of smoothing techniques for ME models." ></td>
	<td class="line x" title="281:389	IEEE Transactions on Speech and Audio Processing, 8(1):3750." ></td>
	<td class="line x" title="282:389	M. Collins and N. Duffy." ></td>
	<td class="line x" title="283:389	2002." ></td>
	<td class="line x" title="284:389	New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron." ></td>
	<td class="line x" title="285:389	In ACL 2002, pages 263270." ></td>
	<td class="line x" title="286:389	M.Collins and B.Roark." ></td>
	<td class="line x" title="287:389	2004." ></td>
	<td class="line x" title="288:389	Incremental parsing with the perceptron algorithm." ></td>
	<td class="line x" title="289:389	In ACL 2004." ></td>
	<td class="line x" title="290:389	M. Collins." ></td>
	<td class="line x" title="291:389	2002a." ></td>
	<td class="line x" title="292:389	Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms." ></td>
	<td class="line x" title="293:389	In EMNLP 2002." ></td>
	<td class="line x" title="294:389	M. Collins." ></td>
	<td class="line x" title="295:389	2002b." ></td>
	<td class="line x" title="296:389	Ranking algorithms for named-entity extraction: Boosting and the voted perceptron." ></td>
	<td class="line x" title="297:389	In ACL 2002." ></td>
	<td class="line x" title="298:389	K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer." ></td>
	<td class="line x" title="299:389	2006." ></td>
	<td class="line x" title="300:389	Online passive-aggressive algorithms." ></td>
	<td class="line x" title="301:389	Journal of Machine Learning Research." ></td>
	<td class="line x" title="302:389	H. Daume III and D. Marcu." ></td>
	<td class="line x" title="303:389	2005." ></td>
	<td class="line x" title="304:389	Learning as search optimization: Approximate large margin methods for structured prediction." ></td>
	<td class="line x" title="305:389	In ICML 2005." ></td>
	<td class="line x" title="306:389	J. R. Finkel, T. Grenager, and C. Manning." ></td>
	<td class="line x" title="307:389	2005." ></td>
	<td class="line x" title="308:389	Incorporating non-local informationin to information extraction systems by Gibbs sampling." ></td>
	<td class="line x" title="309:389	In ACL 2005." ></td>
	<td class="line x" title="310:389	C. Gentile." ></td>
	<td class="line x" title="311:389	2001." ></td>
	<td class="line x" title="312:389	A new approximate maximal margin classication algorithm." ></td>
	<td class="line x" title="313:389	JMLR, 3." ></td>
	<td class="line x" title="314:389	R. Herbrich and T. Graepel." ></td>
	<td class="line x" title="315:389	2000." ></td>
	<td class="line x" title="316:389	Large scale Bayes point machines." ></td>
	<td class="line x" title="317:389	In NIPS 2000." ></td>
	<td class="line x" title="318:389	W. Krauth and M. Mezard." ></td>
	<td class="line x" title="319:389	1987." ></td>
	<td class="line x" title="320:389	Learning algorithms with optimal stability in neural networks." ></td>
	<td class="line x" title="321:389	Journal of Physics A 20, pages 745752." ></td>
	<td class="line x" title="322:389	V. Krishnan and C. D. Manning." ></td>
	<td class="line x" title="323:389	2006." ></td>
	<td class="line x" title="324:389	An effective two-stage model for exploiting non-local dependencies in named entity recognitioin." ></td>
	<td class="line x" title="325:389	In ACL-COLING 2006." ></td>
	<td class="line x" title="326:389	J. Lafferty, A. McCallum, and F. Pereira." ></td>
	<td class="line x" title="327:389	2001." ></td>
	<td class="line x" title="328:389	Conditional random elds: Probabilistic models for segmenting and labeling sequence data." ></td>
	<td class="line x" title="329:389	In ICML 2001, pages 282289." ></td>
	<td class="line x" title="330:389	Y. Li, H. Zaragoza, R. Herbrich, J. Shawe-Taylor, and J. Kandola." ></td>
	<td class="line x" title="331:389	2002." ></td>
	<td class="line x" title="332:389	The perceptron algorithm with uneven margins." ></td>
	<td class="line x" title="333:389	In ICML 2002." ></td>
	<td class="line x" title="334:389	P. Liang, A. Bouchard-Cote, D. Klein, and B. Taskar." ></td>
	<td class="line x" title="335:389	2006." ></td>
	<td class="line x" title="336:389	An end-to-end discriminative approach to machine translation." ></td>
	<td class="line x" title="337:389	In ACL-COLING 2006." ></td>
	<td class="line x" title="338:389	R. McDonald and F. Pereira." ></td>
	<td class="line x" title="339:389	2006." ></td>
	<td class="line x" title="340:389	Online learning of approximate dependency parsing algorithms." ></td>
	<td class="line x" title="341:389	In EACL 2006." ></td>
	<td class="line x" title="342:389	R. McDonald, K. Crammer, and F. Pereira." ></td>
	<td class="line x" title="343:389	2005." ></td>
	<td class="line x" title="344:389	Online large-margin training of dependency parsers." ></td>
	<td class="line x" title="345:389	In ACL 2005." ></td>
	<td class="line x" title="346:389	T. Nakagawa and Y. Matsumoto." ></td>
	<td class="line x" title="347:389	2006." ></td>
	<td class="line x" title="348:389	Guessing parts-of-speech of unknown words using global information." ></td>
	<td class="line x" title="349:389	In ACL-COLING 2006." ></td>
	<td class="line x" title="350:389	L. R. Rabiner." ></td>
	<td class="line x" title="351:389	1989." ></td>
	<td class="line x" title="352:389	A tutorial on hidden Markov models and selected applications in speech recognition." ></td>
	<td class="line x" title="353:389	Proceedings of the IEEE, 77(2):257286." ></td>
	<td class="line x" title="354:389	L. A. Ramshaw and M. P. Marcus." ></td>
	<td class="line x" title="355:389	1995." ></td>
	<td class="line x" title="356:389	Text chunking using transformation-based learning." ></td>
	<td class="line x" title="357:389	In third ACL Workshop on very large corpora." ></td>
	<td class="line x" title="358:389	F. Rosenblatt." ></td>
	<td class="line x" title="359:389	1958." ></td>
	<td class="line x" title="360:389	The perceptron: A probabilistic model for information storage and organization in the brain." ></td>
	<td class="line x" title="361:389	Psycological Review, pages 386407." ></td>
	<td class="line x" title="362:389	D. Roth and W. Yih." ></td>
	<td class="line x" title="363:389	2005." ></td>
	<td class="line x" title="364:389	Integer linear programming inference for conditional random elds." ></td>
	<td class="line x" title="365:389	InICML 2005." ></td>
	<td class="line x" title="366:389	S. Sarawagi and W. W. Cohen." ></td>
	<td class="line x" title="367:389	2004." ></td>
	<td class="line x" title="368:389	Semi-Markov random elds for information extraction." ></td>
	<td class="line x" title="369:389	In NIPS 2004." ></td>
	<td class="line x" title="370:389	L. Shen and A. K. Joshi." ></td>
	<td class="line x" title="371:389	2004." ></td>
	<td class="line x" title="372:389	Flexible margin selection for reranking with full pairwise samples." ></td>
	<td class="line x" title="373:389	In IJCNLP 2004." ></td>
	<td class="line x" title="374:389	F. K. Soong and E. Huang." ></td>
	<td class="line x" title="375:389	1991." ></td>
	<td class="line x" title="376:389	A tree-trellis based fast search for nding the n best sentence hypotheses in continuous speech recognition." ></td>
	<td class="line x" title="377:389	In ICASSP-91." ></td>
	<td class="line x" title="378:389	C. Sutton and A. McCallum." ></td>
	<td class="line x" title="379:389	2004." ></td>
	<td class="line x" title="380:389	Collective segmenation and labeling of distant entitites in information extraction." ></td>
	<td class="line x" title="381:389	University of Massachusetts Technical Report TR 04-49." ></td>
	<td class="line x" title="382:389	B. Taskar, C. Guestrin, and D. Koller." ></td>
	<td class="line x" title="383:389	2003." ></td>
	<td class="line x" title="384:389	Max-margin Markov networks." ></td>
	<td class="line x" title="385:389	In NIPS 2003." ></td>
	<td class="line x" title="386:389	E. F. Tjong, K. Sang, and F. De Meulder." ></td>
	<td class="line x" title="387:389	2003." ></td>
	<td class="line x" title="388:389	Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition." ></td>
	<td class="line x" title="389:389	In CoNLL 2003." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D07-1129
Structural Correspondence Learning for Dependency Parsing
Shimizu, Nobuyuki;Nakagawa, Hiroshi;"></td>
	<td class="line x" title="1:93	Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp." ></td>
	<td class="line x" title="2:93	11661169, Prague, June 2007." ></td>
	<td class="line x" title="3:93	c2007 Association for Computational Linguistics Structural Correspondence Learning for Dependency Parsing Nobuyuki Shimizu Information Technology Center University of Tokyo Tokyo, Japan shimizu@r.dl.itc.u-tokyo.ac.jp Hiroshi Nakagawa Information Technology Center University of Tokyo Tokyo, Japan nakagawa@dl.itc.u-tokyo.ac.jp Abstract Following (Blitzer et al. , 2006), we present an application of structural correspondence learning to non-projective dependency parsing (McDonald et al. , 2005)." ></td>
	<td class="line x" title="4:93	To induce the correspondences among dependency edges from different domains, we looked at every two tokens in a sentence and examined whether or not there is a preposition, a determiner or a helping verb between them." ></td>
	<td class="line x" title="5:93	Three binary linear classifiers were trained to predict the existence of a preposition, etc, on unlabeled data and we used singular value decomposition to induce new features." ></td>
	<td class="line x" title="6:93	During the training, the parser was trained with these additional features in addition to these described in (McDonald et al. , 2005)." ></td>
	<td class="line oc" title="7:93	We discriminatively trained our parser in an on-line fashion using a variant of the voted perceptron (Collins, 2002; Collins and Roark, 2004; Crammer and Singer, 2003)." ></td>
	<td class="line x" title="8:93	1 Introduction We have recently seen growing popularity of dependency parsing." ></td>
	<td class="line x" title="9:93	It is no longer rare to see dependency relations used as features, in tasks such as machine translation (Ding and Palmer, 2005) and relation extraction (Bunescu and Mooney, 2005)." ></td>
	<td class="line x" title="10:93	However, there is one factor that prevents the use of dependency parsing: sparseness of annotated corpora outside Wall Street Journal." ></td>
	<td class="line x" title="11:93	In many situations we need to parse sentences from a target domain with no labeled data, which is a different distribution from a source domain where plentiful labeled training data is available." ></td>
	<td class="line x" title="12:93	In this paper, we investigate the effectiveness of structural correspondence learning (SCL) (Blitzer et al. , 2006) in the domain adaptation task given by the CoNLL 2007." ></td>
	<td class="line x" title="13:93	They hypothesize that a model trained in the source domain using this common feature representation will generalize better to the target domain, and focus on using unlabeled data from both the source and target domains to learn a common feature representation that is meaningful across both domains." ></td>
	<td class="line x" title="14:93	The paper is structured as follows: in section 2, we review the decoding and learning aspects of (McDonald et al. , 2005), in section 3, structural correspondence learning applied to dependency parsing, and in section 4, we describe the experiments and the features needed for the CoNLL 2006 shared task." ></td>
	<td class="line x" title="15:93	2 Non-Projective Dependency Parsing 2.1 Dependency Structure Let us define x to be a generic sequence of input tokens together with their POS tags and other morphological features, and y to be a generic dependency structure, that is, a set of edges for x. A labeled edge is a tuple DEPREL,i  j where i is the start point of the edge, j is the end point, and DEPREL is the label of the edge." ></td>
	<td class="line x" title="16:93	The token at i is the head of the token at j. Table 1 shows our formulation of a structured prediction problem." ></td>
	<td class="line x" title="17:93	Given x, the input tokens and their features (column 2 and 3, Table 1), the task is to pre1166 Index Token POS Labeled Edge 1 John NN SUBJ,2  1 2 saw VBD PRED,0  2 3 a DT DET,4  3 4 dog NN OBJ,2  4 5 yesterday RB ADJU,2  5 6 which WDT MODWH,7  6 7 was VBD MODPRED,4  7 8 a DT DET,10  8 9 Yorkshire NN MODN,10  9 10 Terrier NN OBJ,7  10 11." ></td>
	<td class="line x" title="18:93	.,10  11 Table 1: Example Edges dict y, the set of labeled edges (column 4, Table 1)." ></td>
	<td class="line x" title="20:93	In this paper we use the common method of factoring the score of the dependency structure as the sum of the scores of all the labeled edges." ></td>
	<td class="line x" title="21:93	A dependency structure is characterized by its labeled edges, and for each labeled edge, we have features and corresponding weights." ></td>
	<td class="line x" title="22:93	The score of a dependency structure is the sum of these weights." ></td>
	<td class="line x" title="23:93	For example, let us say we would like to find the score of the labeled edge OBJ,2  4." ></td>
	<td class="line x" title="24:93	This is the edge going to the 4th token dog in Table 1." ></td>
	<td class="line x" title="25:93	The features for this edge could be:  There is an edge starting at saw, with the POS tag VBD, and the distance between the head and the child is 2." ></td>
	<td class="line x" title="26:93	( head = wordj, headPOS = posj, dist(i,j) = |ij| )  There is an edge ending at dog, with the POS tag NN, and the distance between the head and the child is 2." ></td>
	<td class="line x" title="27:93	( child = wordi, childPOS = posi, dist(i,j) = |ij| ) In the upcoming section, we explain a decoding algorithm for the dependency structures, and later we give a method for learning the weight vector used in the decoding." ></td>
	<td class="line x" title="28:93	2.2 Maximum Spanning Tree Algorithm As in (McDonald et al. , 2005), we use Chu-LiuEdmonds (CLE) algorithm (Chu and Liu, 1965; Edmonds, 1967) for decoding." ></td>
	<td class="line x" title="29:93	CLE finds the Maximum Spanning Tree in a directed graph." ></td>
	<td class="line x" title="30:93	The following is a summary given in (McDonald et al. , 2005)." ></td>
	<td class="line x" title="31:93	Informally, the algorithm has each vertex in the graph greedily select the incoming edge with highest weight." ></td>
	<td class="line x" title="32:93	Note that the edge is coming from the parent to the child." ></td>
	<td class="line x" title="33:93	That is, given a child node wordj, we are finding the parent, or the head wordi such that the edge (i,j) has the highest weight among all i, i negationslash= j. If a tree results, then this must be the maximum spanning tree." ></td>
	<td class="line x" title="34:93	If not, there must be a cycle." ></td>
	<td class="line x" title="35:93	The procedure identifies a cycle and contracts it into a single vertex and recalculates edge weights going into and out of the cycle." ></td>
	<td class="line x" title="36:93	It can be shown that a maximum spanning tree on the contracted graph is equivalent to a maximum spanning tree in the original graph (Leonidas, 2003)." ></td>
	<td class="line x" title="37:93	Hence the algorithm can recursively call itself on the new graph." ></td>
	<td class="line oc" title="38:93	2.3 Online Learning Again following (McDonald et al. , 2005), we have used the single best MIRA (Crammer and Singer, 2003), which is a margin aware variant of perceptron (Collins, 2002; Collins and Roark, 2004) for structured prediction." ></td>
	<td class="line x" title="39:93	In short, the update is executed when the decoder fails to predict the correct parse, and we compare the correct parse yt and the incorrect parse y suggested by the decoding algorithm." ></td>
	<td class="line x" title="40:93	The weights of the features in y will be lowered, and the weights of the features in yt will be increased accordingly." ></td>
	<td class="line x" title="41:93	3 Domain Adaptation Following (Blitzer et al. , 2006), we present an application of structural correspondence learning (SCL) to non-projective dependency parsing (McDonald et al. , 2005)." ></td>
	<td class="line x" title="42:93	SCL is a method for adapting a classifier learned in a source domain to a target domain." ></td>
	<td class="line x" title="43:93	We assume that both domains have unlabeled data, but only the source domain has labeled training data." ></td>
	<td class="line x" title="44:93	SCL works as follows: 1." ></td>
	<td class="line x" title="45:93	Define a set of pivot features on the unlabeled data from both domains." ></td>
	<td class="line x" title="46:93	2." ></td>
	<td class="line x" title="47:93	Use these pivot features to learn a mapping from the original feature spaces of both domains to a shared, low-dimensional real-valued feature space." ></td>
	<td class="line x" title="48:93	A high inner product in this new space indicates a high degree of correspondence." ></td>
	<td class="line x" title="49:93	3." ></td>
	<td class="line x" title="50:93	Use both the transformed and original features from the source domain." ></td>
	<td class="line x" title="51:93	4." ></td>
	<td class="line x" title="52:93	Again using both the transformed and original features, test the samples from the target domain." ></td>
	<td class="line x" title="53:93	If we learned a good mapping, then the effectiveness of the classifier in the source domain should transfer to the target domain." ></td>
	<td class="line x" title="54:93	To induce the correspondences among dependency edges in the source domain and the target domain, we looked at every two tokens in a sentence and examined whether or not there is a preposition, a determiner or a helping verb between them." ></td>
	<td class="line x" title="55:93	Although no edge is present in unlabeled data, the 1167 presence of a preposition indicates that this edge between the tokens, if existed, will not be a noun modifier (in English corpus, this label is NMOD)." ></td>
	<td class="line x" title="56:93	Thus, this induced feature should correlate with the label of an edge candidate." ></td>
	<td class="line x" title="57:93	We postulate that the label of an edge candidate, if known, may allow the supervised learner to choose the correct edge among the edge candidates in the target domain." ></td>
	<td class="line x" title="58:93	In the first step, we chose the presence of a preposition, a determiner or a helping verb between tokens as pivot features." ></td>
	<td class="line x" title="59:93	Then three binary linear classifiers were trained to predict the existence of a preposition (prep), determiner (det) and helping verb (hv) on unlabeled data and obtained a weight vector for each classifier." ></td>
	<td class="line x" title="60:93	classifierprep(e) = sign(wprep(e)) classifierdet(e) = sign(wdet(e)) classifierhv(e) = sign(whv(e)) The input to the above classifiers is an edge e instead of a whole sentence x.  is a mapping from an edge to a feature vector." ></td>
	<td class="line x" title="61:93	Since POS tags were not available in unlabeled data, for pivot predictors, we took the subset of the features given by an edge." ></td>
	<td class="line x" title="62:93	The features for pivot predictors are listed in Table 2." ></td>
	<td class="line x" title="63:93	The reminder of the features are the same as ones used in (McDonald et al. , 2005)." ></td>
	<td class="line x" title="64:93	Using each weight vector as a column, we created a weight matrix." ></td>
	<td class="line x" title="65:93	W = [wprep|wdet|whv]." ></td>
	<td class="line x" title="66:93	And run a singular value decomposition to induce a lower dimensional feature space." ></td>
	<td class="line x" title="67:93	W = UV . We then took the transpose of the resulting unitary matrix, U which maps the original data to the space spanned by the principal components, and applied it to the feature vector of every potential edge." ></td>
	<td class="line x" title="68:93	The original feature vector is parenleftbigg fsubset freminder parenrightbigg . We argument the feature vector with the additional feature induced by U." ></td>
	<td class="line x" title="69:93	The augmented feature vectors parenleftBigg f subset freminder Ufsubset parenrightBigg were used throughout the training and testing of the dependency parser." ></td>
	<td class="line x" title="70:93	4 Experiments Our experiments were conducted on CoNLL-2007 shared task domain adaptation track (Nivre et al. , 2007) using treebanks (Marcus et al. , 1993; Johansson and Nugues, 2007; Kulick et al. , 2004)." ></td>
	<td class="line x" title="71:93	Given an edge DEPREL,i,j head1 = wordi1 head = wordi head+1 = wordi+1 child1 = wordj1 child = wordj child+1 = wordj+1 Table 2: Binary Features for Pivot Predictors 4.1 Dependency Relation The CLE algorithm works on a directed graph with unlabeled edges." ></td>
	<td class="line x" title="72:93	Since the CoNLL shared task requires the labeling of edges, as a preprocessing stage, we created a directed complete graph." ></td>
	<td class="line x" title="73:93	Then we labeled each edge with the highest scoring dependency relation." ></td>
	<td class="line x" title="74:93	This complete graph was given to the CLE algorithm and the edge labels were never altered in the course of finding the maximum spanning tree." ></td>
	<td class="line x" title="75:93	4.2 Features The features we used for pivot predictors to classify each edge DEPREL,i,j are shown in Table 2." ></td>
	<td class="line x" title="76:93	The index i is the position of the parent and j is that of the child." ></td>
	<td class="line x" title="77:93	wordj = the word token at the position j. posj = the coarse part-of-speech at j. No other features were used beyond the combinations of the word token in Table 2." ></td>
	<td class="line x" title="78:93	The hardware used was an Intel CPU at 3.0 Ghz with 32 GB of memory, and the software was written in C++." ></td>
	<td class="line x" title="79:93	While more iterations should help, due to the time constraints, we were unable to complete more training." ></td>
	<td class="line x" title="80:93	The parser required a few days to train." ></td>
	<td class="line x" title="81:93	5 Results Unfortunately, we have discovered a bug in our codes after submitting our results for the blind tests, and the reported results in (Nivre et al. , 2007) were not representative of our approach." ></td>
	<td class="line x" title="82:93	The current results (closed class) are shown in Table 3." ></td>
	<td class="line x" title="83:93	For the explanations of Labeled Attachment Score, Unlabeled Attachment Score and Label Accuracy, the readers are suggested to refer to the shared task introductory paper (Nivre et al. , 2007)." ></td>
	<td class="line x" title="84:93	WSJ represents the application of the parser without SCL to the source domain test set, and WSJ-SCL the parser with SCL to the same test set." ></td>
	<td class="line x" title="85:93	Similarily 1168 Domain LAS UAS Label Accuracy WSJ 83.01%  83.43% 86.43%  86.81% 88.77%  88.99% WSJ-SCL 83.43%  83.59% 86.87%  86.93% 88.75%  89.01% Chem 74.75%  75.18% 80.74%  81.24% 82.34%  82.70% Chem-SCL 75.04%  74.91% 81.02%  80.82% 82.18%  82.18% Table 3: Labeled Attachment Score, Unlabeled Attachment Score and Label Accuracy Chem and Chem-SCL represents the application of the parser without SCL and with SCL to the source domain test set respectively." ></td>
	<td class="line x" title="86:93	We did batch learning by running the online algorithm 4 times." ></td>
	<td class="line x" title="87:93	An arrow  indicates how the results after 2nd iteration changed at the end of 4th iteration." ></td>
	<td class="line x" title="88:93	Contrary to our expectations, we seem to see SCL overfitting to the source domain WSJ in this experiment." ></td>
	<td class="line x" title="89:93	Due to the lack of POS tags in unlabeled data, our feature set for pivot predictors uses tokens extensively unlike that for the dependency parser." ></td>
	<td class="line x" title="90:93	Since tokens are not as abstract as POS tags, we suspect induced features may have caused overfitting." ></td>
	<td class="line x" title="91:93	6 Conclusion We presented an application of structural correspondence learning to non-projective dependency parsing." ></td>
	<td class="line x" title="92:93	Effectiveness of SCL for domain adaptation is mixed in this experiment perhaps due to the mismatch between feature sets." ></td>
	<td class="line x" title="93:93	Future work includes use of more sophisticated features such as POS and other morphological features, possibly a joint domain adaptation of POS tagging and dependency parsing for unlabeled data as well as re-examination of pivot features." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N07-1011
First-Order Probabilistic Models for Coreference Resolution
Culotta, Aron;Wick, Michael;McCallum, Andrew;"></td>
	<td class="line x" title="1:193	Proceedings of NAACL HLT 2007, pages 8188, Rochester, NY, April 2007." ></td>
	<td class="line x" title="2:193	c2007 Association for Computational Linguistics First-Order Probabilistic Models for Coreference Resolution Aron Culotta and Michael Wick and Andrew McCallum Department of Computer Science University of Massachusetts Amherst, MA 01003 {culotta,mwick,mccallum}@cs.umass.edu Abstract Traditional noun phrase coreference resolution systems represent features only of pairs of noun phrases." ></td>
	<td class="line x" title="3:193	In this paper, we propose a machine learning method that enables features over sets of noun phrases, resulting in a first-order probabilistic model for coreference." ></td>
	<td class="line x" title="4:193	We outline a set of approximations that make this approach practical, and apply our method to the ACE coreference dataset, achieving a 45% error reduction over a comparable method that only considers features of pairs of noun phrases." ></td>
	<td class="line x" title="5:193	This result demonstrates an example of how a firstorder logic representation can be incorporated into a probabilistic model and scaled efficiently." ></td>
	<td class="line x" title="6:193	1 Introduction Noun phrase coreference resolution is the problem of clustering noun phrases into anaphoric sets." ></td>
	<td class="line x" title="7:193	A standard machine learning approach is to perform a set of independent binary classifications of the form Is mention a coreferent with mention b? This approach of decomposing the problem into pairwise decisions presents at least two related difficulties." ></td>
	<td class="line x" title="8:193	First, it is not clear how best to convert the set of pairwise classifications into a disjoint clustering of noun phrases." ></td>
	<td class="line x" title="9:193	The problem stems from the transitivity constraints of coreference: If a and b are coreferent, and b and c are coreferent, then a and c must be coreferent." ></td>
	<td class="line x" title="10:193	This problem has recently been addressed by a number of researchers." ></td>
	<td class="line x" title="11:193	A simple approach is to perform the transitive closure of the pairwise decisions." ></td>
	<td class="line x" title="12:193	However, as shown in recent work (McCallum and Wellner, 2003; Singla and Domingos, 2005), better performance can be obtained by performing relational inference to directly consider the dependence among a set of predictions." ></td>
	<td class="line x" title="13:193	For example, McCallum and Wellner (2005) apply a graph partitioning algorithm on a weighted, undirected graph in which vertices are noun phrases and edges are weighted by the pairwise score between noun phrases." ></td>
	<td class="line x" title="14:193	A second and less studied difficulty is that the pairwise decomposition restricts the feature set to evidence about pairs of noun phrases only." ></td>
	<td class="line x" title="15:193	This restriction can be detrimental if there exist features of sets of noun phrases that cannot be captured by a combinationofpairwisefeatures." ></td>
	<td class="line x" title="16:193	Asasimpleexample, consider prohibiting coreferent sets that consist only of pronouns." ></td>
	<td class="line x" title="17:193	That is, we would like to require that there be at least one antecedent for a set of pronouns." ></td>
	<td class="line x" title="18:193	The pairwise decomposition does not make it possible to capture this constraint." ></td>
	<td class="line x" title="19:193	In general, we would like to construct arbitrary features over a cluster of noun phrases using the full expressivity of first-order logic." ></td>
	<td class="line x" title="20:193	Enabling this sort of flexible representation within a statistical model has been the subject of a long line of research on first-order probabilistic models (Gaifman, 1964; Halpern, 1990; Paskin, 2002; Poole, 2003; Richardson and Domingos, 2006)." ></td>
	<td class="line x" title="21:193	Conceptually, a first-order probabilistic model can be described quite compactly." ></td>
	<td class="line x" title="22:193	A configuration of the world is represented by a set of predi81 He President Bush Laura Bush She 0.2 0.9 0.7 0.4 0.001 0.6 Figure 1: An example noun coreference graph in which vertices are noun phrases and edge weights areproportionaltotheprobabilitythatthetwonouns arecoreferent." ></td>
	<td class="line x" title="23:193	Partitioningsuchagraphintodisjoint clusters corresponds to performing coreference resolution on the noun phrases." ></td>
	<td class="line x" title="24:193	cates, each of which has an associated real-valued parameter." ></td>
	<td class="line x" title="25:193	The likelihood of each configuration of the world is proportional to a combination of these weighted predicates." ></td>
	<td class="line x" title="26:193	In practice, however, enumerating all possible configurations, or even all the predicates of one configuration, can result in intractable combinatorial growth (de Salvo Braz et al. , 2005; Culotta and McCallum, 2006)." ></td>
	<td class="line x" title="27:193	Inthispaper, wepresentapracticalmethodtoperform training and inference in first-order models of coreference." ></td>
	<td class="line x" title="28:193	We empirically validate our approach on the ACE coreference dataset, showing that the first-order features can lead to an 45% error reduction." ></td>
	<td class="line x" title="29:193	2 Pairwise Model In this section we briefly review the standard pairwise coreference model." ></td>
	<td class="line x" title="30:193	Given a pair of noun phrases xij = {xi,xj}, let the binary random variable yij be 1 if xi and xj are coreferent." ></td>
	<td class="line x" title="31:193	Let F = {fk(xij,y)} be a set of features over xij." ></td>
	<td class="line x" title="32:193	For example, fk(xij,y) may indicate whether xi and xj have the same gender or number." ></td>
	<td class="line x" title="33:193	Each feature fk has an associated real-valued parameter k. The pairwise model is p(yij|xij) = 1Z xij exp summationdisplay k kfk(xij,yij) where Zxij is a normalizer that sums over the two settings of yij." ></td>
	<td class="line x" title="34:193	This is a maximum-entropy classifier (i.e. logistic regression) in which p(yij|xij) is the probability that xi and xj are coreferent." ></td>
	<td class="line x" title="35:193	To estimate  = {k} from labeled training data, we perform gradient ascent to maximize the log-likelihood of the labeled data." ></td>
	<td class="line x" title="36:193	Two critical decisions for this method are (1) how to sample the training data, and (2) how to combine the pairwise predictions at test time." ></td>
	<td class="line x" title="37:193	Systems oftenperformbetterwhenthesedecisionscomplement each other." ></td>
	<td class="line x" title="38:193	Given a data set in which noun phrases have been manually clustered, the training data can be created by simply enumerating over each pair of noun phrases xij, where yij is true if xi and xj are in the same cluster." ></td>
	<td class="line x" title="39:193	However, this approach generates a highly unbalanced training set, with negative examples outnumbering positive examples." ></td>
	<td class="line x" title="40:193	Instead, Soon et al.(2001) propose the following sampling method: Scan the document from left to right." ></td>
	<td class="line x" title="42:193	Compare each noun phrase xi to each preceding noun phrase xj, scanning from right to left." ></td>
	<td class="line x" title="43:193	For each pair xi,xj, create a training instance xij,yij, where yij is 1 if xi and xj are coreferent." ></td>
	<td class="line x" title="44:193	The scan for xj terminates when a positive example is constructed, or the beginning of the document is reached." ></td>
	<td class="line x" title="45:193	This results in a training set that has been pruned of distant noun phrase pairs." ></td>
	<td class="line x" title="46:193	At testing time, we can construct an undirected, weighted graph in which vertices correspond to noun phrases and edge weights are proportional to p(yij|xij)." ></td>
	<td class="line x" title="47:193	Theproblemisthentopartitionthegraph intoclusterswithhighintra-clusteredgeweightsand low inter-cluster edge weights." ></td>
	<td class="line x" title="48:193	An example of such a graph is shown in Figure 1." ></td>
	<td class="line x" title="49:193	Any partitioning method is applicable here; however, perhaps most common for coreference is to perform greedy clustering guided by the word order of the document to complement the sampling method described above (Soon et al. , 2001)." ></td>
	<td class="line x" title="50:193	More precisely, scan the document from left-to-right, assigning each noun phrase xi to the same cluster as the closest preceding noun phrase xj for which p(yij|xij) > , where  is some classification threshold (typically 0.5)." ></td>
	<td class="line x" title="51:193	Note that this method contrasts with standard greedy agglomerative clustering, in which each noun phrase would be assigned to the most probable cluster according to p(yij|xij)." ></td>
	<td class="line x" title="52:193	82 Choosing the closest preceding phrase is common because nearby phrases are a priori more likely to be coreferent." ></td>
	<td class="line x" title="53:193	We refer tothe training and inferencemethods described in this section as the Pairwise Model." ></td>
	<td class="line x" title="54:193	3 First-Order Logic Model We propose augmenting the Pairwise Model to enable classification decisions over sets of noun phrases." ></td>
	<td class="line x" title="55:193	Given a set of noun phrases xj = {xi}, let the binary random variable yj be 1 if all the noun phrases xi  xj are coreferent." ></td>
	<td class="line x" title="56:193	The features fk and weights k are defined as before, but now the features can represent arbitrary attributes over the entire set xj." ></td>
	<td class="line x" title="57:193	This allows us to use the full flexibility of first-order logic to construct features about sets of nouns." ></td>
	<td class="line x" title="58:193	The First-Order Logic Model is p(yj|xj) = 1Z xj exp summationdisplay k kfk(xj,yj) where Zxj is a normalizer that sums over the two settings of yj." ></td>
	<td class="line x" title="59:193	Note that this model gives us the representational power of recently proposed Markov logic networks (Richardson and Domingos, 2006); that is, we can construct arbitrary formulae in first-order logic to characterizethenouncoreferencetask, andcanlearn weights for instantiations of these formulae." ></td>
	<td class="line x" title="60:193	However, naively grounding the corresponding Markov logicnetworkresultsinacombinatorialexplosionof variables." ></td>
	<td class="line x" title="61:193	Below we outline methods to scale training and prediction with this representation." ></td>
	<td class="line x" title="62:193	As in the Pairwise Model, we must decide how to sample training examples and how to combine independent classifications at testing time." ></td>
	<td class="line x" title="63:193	It is important to note that by moving to the First-Order Logic Model, the number of possible predictions has increased exponentially." ></td>
	<td class="line x" title="64:193	In the Pairwise Model, the number of possible y variables is O(|x|2), where x is the set of noun phrases." ></td>
	<td class="line x" title="65:193	In the First-Order Logic Model, the number of possible y variables is O(2|x|): There is a y variable for each possible element of the powerset of x. Of course, we do not enumerate this set; rather, we incrementally instantiate y variables as needed during prediction." ></td>
	<td class="line x" title="66:193	A simple method to generate training examples is to sample positive and negative cluster examples uniformly at random from the training data." ></td>
	<td class="line x" title="67:193	Positive examples are generated by first sampling a true cluster, then sampling a subset of that cluster." ></td>
	<td class="line x" title="68:193	Negative examplesaregeneratedbysamplingtwopositiveexamples and merging them into the same cluster." ></td>
	<td class="line x" title="69:193	At testing time, we perform standard greedy agglomerative clustering, where the score for each merger is proportional to the probability of the newly formed clustering according to the model." ></td>
	<td class="line x" title="70:193	Clustering terminates when there exists no additional merge that improves the probability of the clustering." ></td>
	<td class="line x" title="71:193	We refer to the system described in this section as First-Order Uniform." ></td>
	<td class="line x" title="72:193	4 Error-driven and Rank-based training of the First-Order Model In this section we propose two enhancements to the training procedure for the First-Order Uniform model." ></td>
	<td class="line x" title="73:193	First, because each training example consists of a subset of noun phrases, the number of possible training examples we can generate is exponential in the number of noun phrases." ></td>
	<td class="line x" title="74:193	We propose an errordriven sampling method that generates training examples from errors the model makes on the training data." ></td>
	<td class="line x" title="75:193	The algorithm is as follows: Given initial parameters , perform greedy agglomerative clustering on training document i until an incorrect cluster is formed." ></td>
	<td class="line x" title="76:193	Update the parameter vector according to this mistake, then repeat for the next training document." ></td>
	<td class="line x" title="77:193	This process is repeated for a fixed number of iterations." ></td>
	<td class="line x" title="78:193	Exactly how to update the parameter vector is addressed by the second enhancement." ></td>
	<td class="line x" title="79:193	We propose modifying the optimization criterion of training to perform ranking rather than classification of clusters." ></td>
	<td class="line x" title="80:193	Consider a training example cluster with a negativelabel, indicatingthatnotallofthenounphrases it contains are coreferent." ></td>
	<td class="line x" title="81:193	A classification training algorithm will penalize all the features associated with this cluster, since they correspond to a negative example." ></td>
	<td class="line x" title="82:193	However, becausetheremayexistssubsets of the cluster that are coreferent, features representing these positive subsets may be unjustly penalized." ></td>
	<td class="line x" title="83:193	To address this problem, we propose constructing training examples consisting of one negative exam83 f c y 12 x 2 x 1 y 23 x 3 y 13 f c f c f t Figure 2: An example noun coreference factor graph forthePairwiseModelinwhichfactorsfc modelthe coreference between two nouns, and ft enforce the transitivity among related decisions." ></td>
	<td class="line x" title="84:193	The number of y variables increases quadratically in the number of x variables." ></td>
	<td class="line x" title="85:193	pleandonenearbypositiveexample." ></td>
	<td class="line x" title="86:193	Inparticular, when agglomerative clustering incorrectly merges two clusters, we select the resulting cluster as the negative example, and select as the positive example a cluster that can be created by merging other existing clusters.1 We then update the weight vector so that the positive example is assigned a higher score than the negative example." ></td>
	<td class="line x" title="87:193	This approach allows the update to only penalize the difference between the two features of examples, thereby not penalizing features representing any overlapping coreferent clusters." ></td>
	<td class="line x" title="88:193	To implement this update, we use MIRA (Margin Infused Relaxed Algorithm), a relaxed, online maximum margin training algorithm (Crammer and Singer, 2003)." ></td>
	<td class="line x" title="89:193	It updates the parameter vector with two constraints: (1) the positive example must have a higher score by a given margin, and (2) the change to  should be minimal." ></td>
	<td class="line x" title="90:193	This second constraint is to reduce fluctuations in ." ></td>
	<td class="line x" title="91:193	Let s+(,xj) be the unnormalized score for the positive example and s(,xk) be the unnormalized score of the negative example." ></td>
	<td class="line x" title="92:193	Each update solves the following 1Of the possible positive examples, we choose the one with the highest probability under the current model to guard against large fluctuations in parameter updates f c y 12 x 2 x 1 y 23 x 3 y 13 f c f c f t y 123 f c Figure 3: An example noun coreference factor graph for the First-Order Model in which factors fc model the coreference between sets of nouns, and ft enforce the transitivity among related decisions." ></td>
	<td class="line x" title="93:193	Here, the additional node y123 indicates whether nouns {x1,x2,x3} are all coreferent." ></td>
	<td class="line x" title="94:193	The number of y variables increases exponentially in the number of x variables." ></td>
	<td class="line x" title="95:193	quadratic program: t+1 = argmin  ||t  ||2 s.t. s+(,xj) s(,xk)  1 In this case, MIRA with a single constraint can be efficiently solved in one iteration of the Hildreth and DEsopo method (Censor and Zenios, 1997)." ></td>
	<td class="line x" title="96:193	Additionally, we average the parameters calculated at each iteration to improve convergence." ></td>
	<td class="line x" title="97:193	We refer to the system described in this section as First-Order MIRA." ></td>
	<td class="line x" title="98:193	5 Probabilistic Interpretation In this section, we describe the Pairwise and FirstOrder models in terms of the factor graphs they approximate." ></td>
	<td class="line x" title="99:193	For the Pairwise Model, a corresponding undirected graphical model can be defined as P(y|x) = 1Z x productdisplay yijy fc(yij,xij) productdisplay yij,yjky ft(yij, j,k,yik,xij,xjk,xik) 84 whereZx istheinput-dependentnormalizerandfactor fc parameterizes the pairwise noun phrase compatibility as fc(yij,xij) = exp(summationtextk kfk(yij,xij))." ></td>
	<td class="line x" title="100:193	Factor ft enforces the transitivity constraints by ft() =  if transitivity is not satisfied, 1 otherwise." ></td>
	<td class="line x" title="101:193	This is similar to the model presented in McCallum and Wellner (2005)." ></td>
	<td class="line x" title="102:193	A factor graph for the Pairwise Model is presented in Figure 2 for three noun phrases." ></td>
	<td class="line x" title="103:193	For the First-Order model, an undirected graphical model can be defined as P(y|x) = 1Z x productdisplay yjy fc(yj,xj) productdisplay yjy ft(yj,xj) where Zx is the input-dependent normalizer and factor fc parameterizes the cluster-wise noun phrase compatibility as fc(yj,xj) = exp(summationtextk kfk(yj,xj))." ></td>
	<td class="line x" title="104:193	Again, factor ft enforces the transitivity constraints by ft() =  if transitivity is not satisfied, 1 otherwise." ></td>
	<td class="line x" title="105:193	Here, transitivity is a bit more complicated, since it also requires that if yj = 1, then for any subset xk  xj, yk = 1." ></td>
	<td class="line x" title="106:193	A factor graph for the First-Order Model is presented in Figure 3 for three noun phrases." ></td>
	<td class="line x" title="107:193	The methods described in Sections 2, 3 and 4 can be viewed as estimating the parameters of each factor fc independently." ></td>
	<td class="line x" title="108:193	This approach can therefore be viewed as a type of piecewise approximation of exact parameter estimation in these models (Sutton and McCallum, 2005)." ></td>
	<td class="line x" title="109:193	Here, each fc is a piece of the model trained independently." ></td>
	<td class="line x" title="110:193	These pieces are combined at prediction time using clustering algorithms to enforce transitivity." ></td>
	<td class="line x" title="111:193	Sutton and McCallum (2005) show that such a piecewise approximation can be theoretically justified as minimizing an upper bound of the exact loss function." ></td>
	<td class="line x" title="112:193	6 Experiments 6.1 Data WeapplyourapproachtothenouncoreferenceACE 2004 data, containing 443 news documents with 28,135 noun phrases to be coreferenced." ></td>
	<td class="line x" title="113:193	336 documents are used for training, and the remainder for testing." ></td>
	<td class="line x" title="114:193	All entity types are candidates for coreference (pronouns, named entities, and nominal entities)." ></td>
	<td class="line x" title="115:193	We use the true entity segmentation, and parse each sentence in the corpus using a phrase-structure grammar, as is common for this task." ></td>
	<td class="line x" title="116:193	6.2 Features We follow Soon et al.(2001) and Ng and Cardie (2002) to generate most of our features for the Pairwise Model." ></td>
	<td class="line x" title="118:193	These include:  Match features Check whether gender, number, head text, or entire phrase matches  Mention type (pronoun, name, nominal)  Aliases Heuristically decide if one noun is the acronym of the other  Apposition Heuristically decide if one noun is in apposition to the other  Relative Pronoun Heuristically decide if one nounisarelativepronounreferringtotheother." ></td>
	<td class="line x" title="119:193	 Wordnet features Use Wordnet to decide if one noun is a hypernym, synonym, or antonym of another, or if they share a hypernym." ></td>
	<td class="line x" title="120:193	 Both speak True if both contain an adjacent context word that is a synonym of said. This is a domain-specific feature that helps for many newswire articles." ></td>
	<td class="line x" title="121:193	 Modifiers Match for example, in the phrase President Clinton, President is a modifier of Clinton." ></td>
	<td class="line x" title="122:193	This feature indicates if one noun is a modifier of the other, or they share a modifier." ></td>
	<td class="line x" title="123:193	 Substring True if one noun is a substring of the other (e.g. Egypt and Egyptian)." ></td>
	<td class="line x" title="124:193	TheFirst-OrderModelincludesthefollowingfeatures:  Enumerate each pair of noun phrases and compute the features listed above." ></td>
	<td class="line x" title="125:193	All-X is true if allpairsshareafeatureX,Most-True-Xistrue if the majority of pairs share a feature X, and Most-False-X is true if most of the pairs do not share feature X. 85  Use the output of the Pairwise Model for each pair of nouns." ></td>
	<td class="line x" title="126:193	All-True is true if all pairs are predicted to be coreferent, Most-True is true if most pairs are predicted to be coreferent, and Most-False is true if most pairs are predicted to not be coreferent." ></td>
	<td class="line x" title="127:193	Additionally, Max-True is true if the maximum pairwise score is above threshold, and Min-True if the minimum pairwise score is above threshold." ></td>
	<td class="line x" title="128:193	 Cluster Size indicates the size of the cluster." ></td>
	<td class="line x" title="129:193	 Count how many phrases in the cluster are of each mention type (name, pronoun, nominal), number (singular/plural) and gender (male/female)." ></td>
	<td class="line x" title="130:193	The features All-X and MostTrue-X indicate how frequent each feature is in the cluster." ></td>
	<td class="line x" title="131:193	This feature can capture the soft constraint such that no cluster consists only of pronouns." ></td>
	<td class="line x" title="132:193	In addition to the listed features, we also include conjunctions of size 2, for example Genders match AND numbers match." ></td>
	<td class="line x" title="133:193	6.3 Evaluation We use the B3 algorithm to evaluate the predicted coreferent clusters (Amit and Baldwin, 1998)." ></td>
	<td class="line x" title="134:193	B3 is common in coreference evaluation and is similar to the precision and recall of coreferent links, except that systems are rewarded for singleton clusters." ></td>
	<td class="line x" title="135:193	For each noun phrase xi, let ci be the number of mentions in xis predicted cluster that are in fact coreferent with xi (including xi itself)." ></td>
	<td class="line x" title="136:193	Precision for xi is defined as ci divided by the number of noun phrases in xis cluster." ></td>
	<td class="line x" title="137:193	Recall for xi is defined as the ci divided by the number of mentions in the gold standard cluster for xi." ></td>
	<td class="line x" title="138:193	F1 is the harmonic mean of recall and precision." ></td>
	<td class="line x" title="139:193	6.4 Results In addition to Pairwise, First-Order Uniform, and First-Order MIRA, we also compare against Pairwise MIRA, which differs from First-Order MIRA only by the fact that it is restricted to pairwise features." ></td>
	<td class="line x" title="140:193	Table 1 suggests both that first-order features and error-driven training can greatly improve performance." ></td>
	<td class="line x" title="141:193	TheFirst-OrderModeloutperformsthePairF1 Prec Rec First-Order MIRA 79.3 86.7 73.2 Pairwise MIRA 72.5 92.0 59.8 First-Order Uniform 69.2 79.0 61.5 Pairwise 62.4 62.5 62.3 Table 1: B3 results for ACE noun phrase coreference." ></td>
	<td class="line x" title="142:193	FIRST-ORDER MIRA is our proposed model that takes advantage of first-order features of the data and is trained with error-driven and rank-based methods." ></td>
	<td class="line x" title="143:193	We see that both the first-order features andthetrainingenhancementsimproveperformance consistently." ></td>
	<td class="line x" title="144:193	wise Model in F1 measure for both standard training and error-driven training." ></td>
	<td class="line x" title="145:193	We attribute some of this improvement to the capability of the First-Order model to capture features of entire clusters that may indicate some phrases are not coreferent." ></td>
	<td class="line x" title="146:193	Also, we attribute the gains from error-driven training to the fact that training examples are generated based on errors made on the training data." ></td>
	<td class="line x" title="147:193	(However, we should note that there are also small differences in the feature sets used for error-driven and standard training results.)" ></td>
	<td class="line x" title="148:193	Error analysis indicates that often noun xi is correctly not merged with a cluster xj when xj has a strong internal coherence." ></td>
	<td class="line x" title="149:193	For example, if all 5 mentions of France in a document are string identical, then the system will be extremely cautious of merging a noun that is not equivalent to France into xj, since this will turn off the All-String-Match feature for cluster xj." ></td>
	<td class="line x" title="150:193	To our knowledge, the best results on this dataset were obtained by the meta-classification scheme of Ng (2005)." ></td>
	<td class="line x" title="151:193	Although our train-test splits may differ slightly, the best B-Cubed F1 score reported in Ng (2005) is 69.3%, which is considerably lower than the 79.3% obtained with our method." ></td>
	<td class="line x" title="152:193	Also note that the Pairwise baseline obtains results similar to those in Ng and Cardie (2002)." ></td>
	<td class="line x" title="153:193	7 Related Work There has been a recent interest in training methods that enable the use of first-order features (Paskin, 2002; Daume III and Marcu, 2005b; Richardson and Domingos, 2006)." ></td>
	<td class="line x" title="154:193	Perhaps the most related is 86 learning as search optimization (LASO) (Daume III and Marcu, 2005b; Daume III and Marcu, 2005a)." ></td>
	<td class="line x" title="155:193	Like the current paper, LASO is also an error-driven training method that integrates prediction and training." ></td>
	<td class="line x" title="156:193	However, whereas we explicitly use a ranking-based loss function, LASO uses a binary classification loss function that labels each candidate structure as correct or incorrect." ></td>
	<td class="line x" title="157:193	Thus, each LASO training example contains all candidate predictions, whereas our training examples contain only the highest scoring incorrect prediction and the highest scoring correct prediction." ></td>
	<td class="line x" title="158:193	Our experiments show the advantages of this ranking-based loss function." ></td>
	<td class="line x" title="159:193	Additionally, we provide an empirical study to quantify the effects of different example generation and loss function decisions." ></td>
	<td class="line oc" title="160:193	Collins and Roark (2004) present an incremental perceptron algorithm for parsing that uses early update to update the parameters when an error is encountered." ></td>
	<td class="line x" title="161:193	Our method uses a similar early update in that training examples are only generated for the first mistake made during prediction." ></td>
	<td class="line x" title="162:193	However, they do not investigate rank-based loss functions." ></td>
	<td class="line x" title="163:193	Others have attempted to train global scoring functionsusingGibbssampling(Finkeletal., 2005), message propagation, (Bunescu and Mooney, 2004; Sutton and McCallum, 2004), and integer linear programming (Roth and Yih, 2004)." ></td>
	<td class="line x" title="165:193	The main distinctions of our approach are that it is simple to implement, not computationally intensive, and adaptable to arbitrary loss functions." ></td>
	<td class="line x" title="166:193	There have been a number of machine learning approaches to coreference resolution, traditionally factored into classification decisions over pairs of nouns (Soon et al. , 2001; Ng and Cardie, 2002)." ></td>
	<td class="line x" title="167:193	Nicolae and Nicolae (2006) combine pairwise classification with graph-cut algorithms." ></td>
	<td class="line x" title="168:193	Luo et al.(2004) do enable features between mention-cluster pairs, but do not perform the error-driven and ranking enhancements proposed in our work." ></td>
	<td class="line x" title="170:193	Denis and Baldridge(2007)usearankinglossfunctionforpronoun coreference; however the examples are still pairs of pronouns, and the example generation is not error driven." ></td>
	<td class="line x" title="171:193	Ng (2005) learns a meta-classifier to choose the best prediction from the output of several coreference systems." ></td>
	<td class="line x" title="172:193	While in theory a metaclassifier can flexibly represent features, they do not explore features using the full flexibility of firstorder logic." ></td>
	<td class="line x" title="173:193	Also, their method is neither errordriven nor rank-based." ></td>
	<td class="line x" title="174:193	McCallum and Wellner (2003) use a conditional random field that factors into a product of pairwise decisions about pairs of nouns." ></td>
	<td class="line x" title="175:193	These pairwise decisions are made collectively using relational inference; however, as pointed out in Milch et al.(2004), this model has limited representational power since it does not capture features of entities, only of pairs of mention." ></td>
	<td class="line x" title="177:193	Milch et al.(2005) address these issues by constructing a generative probabilistic model, where noun clusters are sampled from a generative process." ></td>
	<td class="line x" title="179:193	Our current work has similar representationalflexibilityasMilchetal.(2005)butisdiscriminatively trained." ></td>
	<td class="line x" title="180:193	8 Conclusions and Future Work We have presented learning and inference procedures for coreference models using first-order features." ></td>
	<td class="line x" title="181:193	By relying on sampling methods at training time and approximate inference methods at testing time, this approach can be made scalable." ></td>
	<td class="line x" title="182:193	This resultsinacoreferencemodelthatcancapturefeatures over sets of noun phrases, rather than simply pairs of noun phrases." ></td>
	<td class="line x" title="183:193	This is an example of a model with extremely flexible representational power, but for which exact inference is intractable." ></td>
	<td class="line x" title="184:193	The simple approximations we have described here have enabled this more flexible model to outperform a model that is simplified for tractability." ></td>
	<td class="line x" title="185:193	A short-term extension would be to consider features over entire clusterings, such as the number of clusters." ></td>
	<td class="line x" title="186:193	This could be incorporated in a ranking scheme, as in Ng (2005)." ></td>
	<td class="line x" title="187:193	Future work will extend our approach to a wider variety of tasks." ></td>
	<td class="line x" title="188:193	The model we have described here is specific to clustering tasks; however a similar formulation could be used to approach a number of language processing tasks, such as parsing and relation extraction." ></td>
	<td class="line x" title="189:193	Thesetaskscouldbenefitfromfirst-order features, and the present work can guide the approximations required in those domains." ></td>
	<td class="line x" title="190:193	Additionally, we are investigating more sophisticated inference algorithms that will reduce the greediness of the search procedures described here." ></td>
	<td class="line x" title="191:193	87 Acknowledgments We thank Robert Hall for helpful contributions." ></td>
	<td class="line x" title="192:193	This work was supported in part by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under contract #NBCHD030010, in part by U.S. Government contract #NBCH040171 through a subcontract with BBNT Solutions LLC, in part by The Central Intelligence Agency, the National Security Agency and National Science Foundation under NSF grant #IIS-0326249, in part by Microsoft Live Labs, and in part by the Defense Advanced Research Projects Agency (DARPA) under contract #HR0011-06-C-0023." ></td>
	<td class="line x" title="193:193	Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-1069
Generating a Table-of-Contents
Branavan, S.R.K.;Deshpande, Pawan;Barzilay, Regina;"></td>
	<td class="line x" title="1:214	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 544551, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:214	c2007 Association for Computational Linguistics Generating a Table-of-Contents S.R.K. Branavan, Pawan Deshpande and Regina Barzilay Massachusetts Institute of Technology {branavan, pawand, regina}@csail.mit.edu Abstract This paper presents a method for the automatic generation of a table-of-contents." ></td>
	<td class="line x" title="3:214	This type of summary could serve as an effective navigation tool for accessing information in long texts, such as books." ></td>
	<td class="line x" title="4:214	To generate a coherent table-of-contents, we need to capture both global dependencies across different titles in the table and local constraints within sections." ></td>
	<td class="line x" title="5:214	Our algorithm effectively handles these complex dependencies by factoring the model into local and global components, and incrementally constructing the models output." ></td>
	<td class="line x" title="6:214	The results of automatic evaluation and manual assessment confirm the benefits of this design: our system is consistently ranked higher than nonhierarchical baselines." ></td>
	<td class="line x" title="7:214	1 Introduction Current research in summarization focuses on processing short articles, primarily in the news domain." ></td>
	<td class="line x" title="8:214	While in practice the existing summarization methods are not limited to this material, they are not universal: texts in many domains and genres cannot be summarized using these techniques." ></td>
	<td class="line x" title="9:214	A particularly significant challenge is the summarization of longer texts, such as books." ></td>
	<td class="line x" title="10:214	The requirement for high compression rates and the increased need for the preservation of contextual dependencies between summary sentences places summarization of such texts beyond the scope of current methods." ></td>
	<td class="line x" title="11:214	In this paper, we investigate the automatic generation of tables-of-contents, a type of indicative summary particularly suited for accessing information in long texts." ></td>
	<td class="line x" title="12:214	A typical table-of-contents lists topics described in the source text and provides information about their location in the text." ></td>
	<td class="line x" title="13:214	The hierarchical organization of information in the table further refines information access by specifying the relations between different topics and providing rich contextual information during browsing." ></td>
	<td class="line x" title="14:214	Commonly found in books, tables-of-contents can also facilitate access to other types of texts." ></td>
	<td class="line x" title="15:214	For instance, this type of summary could serve as an effective navigation tool for understanding a long, unstructured transcript for an academic lecture or a meeting." ></td>
	<td class="line x" title="16:214	Given a text, our goal is to generate a tree wherein a node represents a segment of text and a title that summarizes its content." ></td>
	<td class="line x" title="17:214	This process involves two tasks: the hierarchical segmentation of the text, and the generation of informative titles for each segment." ></td>
	<td class="line x" title="18:214	The first task can be addressed by using the hierarchical structure readily available in the text (e.g. , chapters, sections and subsections) or by employing existing topic segmentation algorithms (Hearst, 1994)." ></td>
	<td class="line x" title="19:214	In this paper, we take the former approach." ></td>
	<td class="line x" title="20:214	As for the second task, a naive approach would be to employ existing methods of title generation to each segment, and combine the results into a tree structure." ></td>
	<td class="line x" title="21:214	However, the latter approach cannot guarantee that the generated table-of-contents forms a coherent representation of the entire text." ></td>
	<td class="line x" title="22:214	Since titles of different segments are generated in isolation, some of the generated titles may be repetitive." ></td>
	<td class="line x" title="23:214	Even nonrepetitive titles may not provide sufficient information to discriminate between the content of one seg544 Scientific computing Remarkable recursive algorithm for multiplying matrices Divide and conquer algorithm design Making a recursive algorithm Solving systems of linear equations Computing an LUP decomposition Forward and back substitution Symmetric positive definite matrices and least squares approximation Figure 1: A fragment of a table-of-contents generated by our method." ></td>
	<td class="line x" title="24:214	ment and another." ></td>
	<td class="line x" title="25:214	Therefore, it is essential to generate an entire table-of-contents tree in a concerted fashion." ></td>
	<td class="line x" title="26:214	This paper presents a hierarchical discriminative approach for table-of-contents generation." ></td>
	<td class="line x" title="27:214	Figure 1 shows a fragment of a table-of-contents automatically generated by this algorithm." ></td>
	<td class="line x" title="28:214	Our method has two important points of departure from existing techniques." ></td>
	<td class="line x" title="29:214	First, we introduce a structured discriminative model for table-of-contents generation that accounts for a wide range of phrase-based and collocational features." ></td>
	<td class="line x" title="30:214	The flexibility of this model results in improved summary quality." ></td>
	<td class="line x" title="31:214	Second, our model captures both global dependencies across different titles in the tree and local dependencies within sections." ></td>
	<td class="line x" title="32:214	We decompose the model into local and global components that handle different classes of dependencies." ></td>
	<td class="line x" title="33:214	We further reduce the search space through incremental construction of the models output by considering only the promising parts of the decision space." ></td>
	<td class="line x" title="34:214	We apply our method to process a 1,180 page algorithms textbook." ></td>
	<td class="line x" title="35:214	To assess the contribution of our hierarchical model, we compare our method with state-of-the-art methods that generate each segment title independently.1 The results of automatic evaluation and manual assessment of title quality show that the output of our system is consistently ranked higher than that of non-hierarchical baselines." ></td>
	<td class="line x" title="36:214	2 Related Work Although most current research in summarization focuses on newspaper articles, a number of approaches have been developed for processing longer texts." ></td>
	<td class="line x" title="37:214	Most of these approaches are tailored to a par1The code and feature vector data for our model and the baselines are available at http://people.csail.mit.edu/branavan/code/toc." ></td>
	<td class="line x" title="38:214	ticular domain, such as medical literature or scientific articles." ></td>
	<td class="line x" title="39:214	By making strong assumptions about the input structure and the desired format of the output, these methods achieve a high compression rate while preserving summary coherence." ></td>
	<td class="line x" title="40:214	For instance, Teufel and Moens (2002) summarize scientific articles by selecting rhetorical elements that are commonly present in scientific abstracts." ></td>
	<td class="line x" title="41:214	Elhadad and McKeown (2001) generate summaries of medical articles by following a certain structural template in content selection and realization." ></td>
	<td class="line x" title="42:214	Our work, however, is closer to domainindependent methods for summarizing long texts." ></td>
	<td class="line x" title="43:214	Typically, these approaches employ topic segmentation to identify a list of topics described in a document, and then produce a summary for each part (Boguraev and Neff, 2000; Angheluta et al. , 2002)." ></td>
	<td class="line x" title="44:214	In contrast to our method, these approaches perform either sentence or phrase extraction, rather than summary generation." ></td>
	<td class="line x" title="45:214	Moreover, extraction for each segment is performed in isolation, and global constraints on the summary are not enforced." ></td>
	<td class="line x" title="46:214	Finally, our work is also related to research on title generation (Banko et al. , 2000; Jin and Hauptmann, 2001; Dorr et al. , 2003)." ></td>
	<td class="line x" title="47:214	Since work in this area focuses on generating titles for one article at a time (e.g. , newspaper reports), the issue of hierarchical generation, which is unique to our task, does not arise." ></td>
	<td class="line x" title="48:214	However, this is not the only novel aspect of the proposed approach." ></td>
	<td class="line x" title="49:214	Our model learns title generation in a fully discriminative framework, in contrast to the commonly used noisy-channel model." ></td>
	<td class="line x" title="50:214	Thus, instead of independently modeling the selection and grammaticality constraints, we learn both types of features in a single framework." ></td>
	<td class="line x" title="51:214	This joint training regime supports greater flexibility in modeling feature interaction." ></td>
	<td class="line x" title="52:214	545 3 Problem Formulation We formalize the problem of table-of-contents generation as a supervised learning task where the goal is to map a tree of text segments S to a tree of titles T. A segment may correspond to a chapter, section or subsection." ></td>
	<td class="line x" title="53:214	Since the focus of our work is on the generation aspect of table-of-contents construction, we assume that the hierarchical segmentation of a text is provided in the input." ></td>
	<td class="line x" title="54:214	This division can either be automatically computed using one of the many available text segmentation algorithms (Hearst, 1994), or it can be based on demarcations already present in the input (e.g. , paragraph markers)." ></td>
	<td class="line x" title="55:214	During training, the algorithm is provided with a set of pairs (Si,Ti) for i = 1,,p, where Si is the ith tree of text segments, and Ti is the table-ofcontents for that tree." ></td>
	<td class="line x" title="56:214	During testing, the algorithm generates tables-of-contents for unseen trees of text segments." ></td>
	<td class="line x" title="57:214	We also assume that during testing the desired title length is provided as a parameter to the algorithm." ></td>
	<td class="line x" title="58:214	4 Algorithm To generate a coherent table-of-contents, we need to take into account multiple constraints: the titles should be grammatical, they should adequately represent the content of their segments, and the tableof-contents as a whole should clearly convey the relations between the segments." ></td>
	<td class="line x" title="59:214	Taking a discriminative approach for modeling this task would allow us to achieve this goal: we can easily integrate a range of constraints in a flexible manner." ></td>
	<td class="line x" title="60:214	Since the number of possible labels (i.e. , tables-of-contents) is prohibitively large and the labels themselves exhibit a rich internal structure, we employ a structured discriminative model that can easily handle complex dependencies." ></td>
	<td class="line x" title="61:214	Our solution relies on two orthogonal strategies to balance the tractability and the richness of the model." ></td>
	<td class="line x" title="62:214	First, we factor the model into local and global components." ></td>
	<td class="line x" title="63:214	Second, we incrementally construct the output of each component using a search-based discriminative algorithm." ></td>
	<td class="line x" title="64:214	Both of these strategies have the effect of intelligently pruning the decision space." ></td>
	<td class="line x" title="65:214	Our model factorization is driven by the different types of dependencies which are captured by the two components." ></td>
	<td class="line x" title="66:214	The first model is local: for each segment, it generates a list of candidate titles ranked by their individual likelihoods." ></td>
	<td class="line x" title="67:214	This model focuses on grammaticality and word selection constraints, but it does not consider relations among different titles in the table-of-contents." ></td>
	<td class="line x" title="68:214	These latter dependencies are captured in the global model that constructs a tableof-contents by selecting titles for each segment from the available candidates." ></td>
	<td class="line x" title="69:214	Even after this factorization, the decision space for each model is large: for the local model, it is exponential in the length of the segment title, and for the global model it is exponential in the size of the tree." ></td>
	<td class="line x" title="70:214	Therefore, we construct the output for each of these models incrementally using beam search." ></td>
	<td class="line x" title="71:214	The algorithm maintains the most promising partial output structures, which are extended at every iteration." ></td>
	<td class="line x" title="72:214	The model incorporates this decoding procedure into the training process, thereby learning model parameters best suited for the specific decoding algorithm." ></td>
	<td class="line pc" title="73:214	Similar models have been successfully applied in the past to other tasks including parsing (Collins and Roark, 2004), chunking (Daume and Marcu, 2005), and machine translation (Cowan et al. , 2006)." ></td>
	<td class="line x" title="74:214	4.1 Model Structure The model takes as input a tree of text segments S. Each segment sS and its title z are represented as a local feature vector loc(s,z)." ></td>
	<td class="line x" title="75:214	Each component of this vector stores a numerical value." ></td>
	<td class="line x" title="76:214	This feature vector can track any feature of the segment s together with its title z. For instance, the ith component of this vector may indicate whether the bigram (z[j]z[j+ 1]) occurs in s, where z[j] is the jth word in z: (loc(s,z))i = braceleftbigg 1 if (z[j]z[j + 1])s 0 otherwise In addition, our model captures dependencies among multiple titles that appear in the same tableof-contents." ></td>
	<td class="line x" title="77:214	We represent a tree of segments S paired with titles T with the global feature vector glob(S,T)." ></td>
	<td class="line x" title="78:214	The components here are also numerical features." ></td>
	<td class="line x" title="79:214	For example, the ith component of the vector may indicate whether a title is repeated in the table-of-contents T: 546 (glob(S,T))i = braceleftbigg 1 repeated title 0 otherwise Our model constructs a table-of-contents in two basic steps: Step One The goal of this step is to generate a list of k candidate titles for each segment s  S. To do so, for each possible title z, the model maps the feature vector loc(s,z) to a real number." ></td>
	<td class="line x" title="80:214	This mapping can take the form of a linear model, loc(s,z)loc where loc is the local parameter vector." ></td>
	<td class="line x" title="81:214	Since the number of possible titles is exponential, we cannot consider all of them." ></td>
	<td class="line x" title="82:214	Instead, we prune the decision space by incrementally constructing promising titles." ></td>
	<td class="line x" title="83:214	At each iteration j, the algorithm maintains a beam Q of the top k partially generated titles of length j. During iteration j + 1, a new set of candidates is grown by appending a word from s to the right of each member of the beam Q. We then sort the entries in Q: z1,z2, such that loc(s,zi)loc loc(s,zi+1)loc,i. Only the top k candidates are retained, forming the beam for the next iteration." ></td>
	<td class="line x" title="84:214	This process continues until a title of the desired length is generated." ></td>
	<td class="line x" title="85:214	Finally, the list of k candidates is returned." ></td>
	<td class="line x" title="86:214	Step Two Given a set of candidate titles z1,z2,,zk for each segment s  S, our goal is to construct a table-of-contents T by selecting the most appropriate title from each segments candidate list." ></td>
	<td class="line x" title="87:214	To do so, our model computes a score for the pair (S,T) based on the global feature vector glob(S,T): glob(S,T)glob where glob is the global parameter vector." ></td>
	<td class="line x" title="88:214	As with the local model (step one), the number of possible tables-of-contents is too large to be considered exhaustively." ></td>
	<td class="line x" title="89:214	Therefore, we incrementally construct a table-of-contents by traversing the tree of segments in a pre-order walk (i.e. , the order in which segments appear in the text)." ></td>
	<td class="line x" title="90:214	In this case, the beam contains partially generated tablesof-contents, which are expanded by one segment title at a time." ></td>
	<td class="line x" title="91:214	To further reduce the search space, during decoding only the top five candidate titles for a segment are given to the global model." ></td>
	<td class="line x" title="92:214	4.2 Training the Model Training for Step One We now describe how the local parameter vector loc is estimated from training data." ></td>
	<td class="line x" title="93:214	We are given a set of training examples (si,yi) for i = 1,,l, where si is the ith text segment, and yi is the title of this segment." ></td>
	<td class="line oc" title="94:214	This linear model is learned using a variant of the incremental perceptron algorithm (Collins and Roark, 2004; Daume and Marcu, 2005)." ></td>
	<td class="line x" title="95:214	This online algorithm traverses the training set multiple times, updating the parameter vector loc after each training example in case of mis-predictions." ></td>
	<td class="line x" title="96:214	The algorithm encourages a setting of the parameter vector loc that assigns the highest score to the feature vector associated with the correct title." ></td>
	<td class="line x" title="97:214	The pseudo-code of the algorithm is shown in Figure 2." ></td>
	<td class="line x" title="98:214	Given a text segment s and the corresponding title y, the training algorithm maintains a beam Q containing the top k partial titles of length j. The beam is updated on each iteration using the functions GROW and PRUNE." ></td>
	<td class="line x" title="99:214	For every word in segment s and for every partial title in Q, GROW creates a new title by appending this word to the title." ></td>
	<td class="line x" title="100:214	PRUNE retains only the top ranked candidates based on the scoring function loc(s,z)loc." ></td>
	<td class="line x" title="101:214	If y[1j] (i.e. , the prefix of y of length j) is not in the modified beam Q, then loc is updated2 as shown in line 4 of the pseudo-code in Figure 2." ></td>
	<td class="line x" title="102:214	In addition, Q is replaced with a beam containing only y[1j] (line 5)." ></td>
	<td class="line x" title="103:214	This process is performed|y|times." ></td>
	<td class="line x" title="104:214	We repeat this process for all training examples over 50 training iterations." ></td>
	<td class="line x" title="105:214	3 Training for Step Two To train the global parameter vector glob, we are given training examples (Si,Ti) for i = 1,,p, where Si is the ith tree of text segments, andTi is the table-of-contents for that tree." ></td>
	<td class="line x" title="106:214	However, we cannot directly use these tablesof-contents for training our global model: since this model selects one of the candidate titles zi1,,zik returned by the local model, the true title of the segment may not be among these candidates." ></td>
	<td class="line x" title="107:214	Therefore, to determine a new target title for the segment, we need to identify the title in the set of candidates 2If the word in thejth position ofy does not occur ins, then the parameter update is not performed." ></td>
	<td class="line oc" title="108:214	3For decoding, loc is averaged over the training iterations as in Collins and Roark (2004)." ></td>
	<td class="line x" title="109:214	547 s  segment text." ></td>
	<td class="line x" title="110:214	y  segment title." ></td>
	<td class="line x" title="111:214	y[1j]  prefix of y of length j. Q  beam containing partial titles." ></td>
	<td class="line x" title="112:214	1." ></td>
	<td class="line x" title="113:214	for j = 1|y| 2." ></td>
	<td class="line x" title="114:214	Q = PRUNE(GROW(s,Q)) 3." ></td>
	<td class="line x" title="115:214	if y[1j] /Q 4." ></td>
	<td class="line x" title="116:214	loc = loc + loc(s,y[1j]) summationtextzQ loc(s,z)|Q| 5." ></td>
	<td class="line x" title="117:214	Q ={y[1j]} Figure 2: The training algorithm for the local model." ></td>
	<td class="line x" title="118:214	that is closest to the true title." ></td>
	<td class="line x" title="119:214	We employ the L1 distance measure to compare the content word overlap between two titles.4 For each input (S,T), and each segmentsS, we identify the segment title closest in theL1 measure to the true title y5: z = arg mini L1(zi,y) Once all the training targets in the corpus have been identified through this procedure, the global linear model glob(S,T)glob is learned using the same perceptron algorithm as in step one." ></td>
	<td class="line x" title="120:214	Rather than maintaining the beam of partially generated titles, the beam Q holds partially generated tables-ofcontents." ></td>
	<td class="line x" title="121:214	Also, the loop in line 1 of Figure 2 iterates over segment titles rather than words." ></td>
	<td class="line x" title="122:214	The global model is trained over 200 iterations." ></td>
	<td class="line x" title="123:214	5 Features Local Features Our local model aims to generate titles which adequately represent the meaning of the segment and are grammatical." ></td>
	<td class="line x" title="124:214	Selection and contextual preferences are encoded in the local features." ></td>
	<td class="line x" title="125:214	The features that capture selection constraints are specified at the word level, and contextual features are expressed at the word sequence level." ></td>
	<td class="line x" title="126:214	The selection features capture the position of the word, its TF*IDF, and part-of-speech information." ></td>
	<td class="line x" title="127:214	In addition, they also record whether the word occurs in the body of neighboring segments." ></td>
	<td class="line x" title="128:214	We also 4This measure is close to ROUGE-1 which in addition considers the overlap in auxiliary words." ></td>
	<td class="line x" title="129:214	5In the case of ties, one of the titles is picked arbitrarily." ></td>
	<td class="line x" title="130:214	Segment has the same title as its sibling Segment has the same title as its parent Two adjacent sibling titles have the same head Two adjacent sibling titles start with the same word Rank given to the title by the local model Table 1: Examples of global features." ></td>
	<td class="line x" title="131:214	generate conjunctive features by combining features of different types." ></td>
	<td class="line x" title="132:214	The contextual features record the bigram and trigram language model scores, both for words and for part-of-speech tags." ></td>
	<td class="line x" title="133:214	The trigram scores are averaged over the title." ></td>
	<td class="line x" title="134:214	The language models are trained using the SRILM toolkit." ></td>
	<td class="line x" title="135:214	Another type of contextual feature models the collocational properties of noun phrases in the title." ></td>
	<td class="line x" title="136:214	This feature aims to eliminate generic phrases, such as the following section from the generated titles.6 To achieve this effect, for each noun phrase in the title, we measure the ratio of their frequency in the segment to their frequency in the corpus." ></td>
	<td class="line x" title="137:214	Global Features Our global model describes the interaction between different titles in the tree (See Table 1)." ></td>
	<td class="line x" title="138:214	These interactions are encoded in three types of global features." ></td>
	<td class="line x" title="139:214	The first type of global feature indicates whether titles in the tree are redundant at various levels of the tree structure." ></td>
	<td class="line x" title="140:214	The second type of feature encourages parallel constructions within the same tree." ></td>
	<td class="line x" title="141:214	For instance, titles of adjoining segments may be verbalized as noun phrases with the same head (e.g. , Bubble sort algorithm, Merge sort algorithm)." ></td>
	<td class="line x" title="142:214	We capture this property by comparing words that appear in certain positions in adjacent sibling titles." ></td>
	<td class="line x" title="143:214	Finally, our global model also uses the rank of the title provided by the local model." ></td>
	<td class="line x" title="144:214	This feature enables the global model to account for the preferences of the local model in the title selection process." ></td>
	<td class="line x" title="145:214	6 Evaluation Set-Up Data We apply our method to an undergraduate algorithms textbook." ></td>
	<td class="line x" title="146:214	For detailed statistics on the data see Table 2." ></td>
	<td class="line x" title="147:214	We split its table-of-contents into a set 6Unfortunately, we could not use more sophisticated syntactic features due to the low accuracy of statistical parsers on our corpus." ></td>
	<td class="line x" title="148:214	548 Number of Titles 540 Number of Trees 39 Tree Depth 4 Number of Words 269,650 Avg." ></td>
	<td class="line x" title="149:214	Title Length 3.64 Avg." ></td>
	<td class="line x" title="150:214	Branching 3.29 Avg." ></td>
	<td class="line x" title="151:214	Title Duplicates 21 Table 2: Statistics on the corpus used in the experiments." ></td>
	<td class="line x" title="152:214	of independent subtrees." ></td>
	<td class="line x" title="153:214	Given a table-of-contents of depthnwith a root branching factor ofr, we generate r subtrees, with a depth of at most n1." ></td>
	<td class="line x" title="154:214	We randomly select 80% of these trees for training, and the rest are used for testing." ></td>
	<td class="line x" title="155:214	In our experiments, we use ten different randomizations to compensate for the small number of available trees." ></td>
	<td class="line x" title="156:214	Admittedly, this method of generating training and testing data omits some dependencies at the level of the table-of-contents as a whole." ></td>
	<td class="line x" title="157:214	However, the subtrees used in our experiments still exhibit a sufficiently deep hierarchical structure, rich with contextual dependencies." ></td>
	<td class="line x" title="158:214	Baselines As an alternative to our hierarchical discriminative method, we consider three baselines that build a table-of-contents by generating a title for each segment individually, without taking into account the tree structure, and one hierarchical generative baseline." ></td>
	<td class="line x" title="159:214	The first method generates a title for a segment by selecting the noun phrase from that segment with the highest TF*IDF." ></td>
	<td class="line x" title="160:214	This simple method is commonly used to generate keywords for browsing applications in information retrieval, and has been shown to be effective for summarizing technical content (Wacholder et al. , 2001)." ></td>
	<td class="line x" title="161:214	The second baseline is based on the noisy-channel generative (flat generative, FG) model proposed by Banko et al. , (2000)." ></td>
	<td class="line x" title="162:214	Similar to our local model, this method captures both selection and grammatical constraints." ></td>
	<td class="line x" title="163:214	However, these constraints are modeled separately, and then combined in a generative framework." ></td>
	<td class="line x" title="164:214	We use our local model (Flat Discriminative model, FD) as the third baseline." ></td>
	<td class="line x" title="165:214	Like the second baseline, this model omits global dependencies, and only focuses on features that capture relations within individual segments." ></td>
	<td class="line x" title="166:214	In the hierarchical generative (HG) baseline we run our global model on the ranked list of titles produced for each section by the noisy-channel generative model." ></td>
	<td class="line x" title="167:214	The last three baselines and our algorithm are provided with the title length as a parameter." ></td>
	<td class="line x" title="168:214	In our experiments, the algorithms use the reference title length." ></td>
	<td class="line x" title="169:214	Experimental Design: Comparison with reference tables-of-contents Reference based evaluation is commonly used to assess the quality of machine-generated headlines (Wang et al. , 2005)." ></td>
	<td class="line x" title="170:214	We compare our systems output with the table-ofcontents from the textbook using ROUGE metrics." ></td>
	<td class="line x" title="171:214	We employ a publicly available software package,7 with all the parameters set to default values." ></td>
	<td class="line x" title="172:214	Experimental Design: Human assessment The judges were each given 30 segments randomly selected from a set of 359 test segments." ></td>
	<td class="line x" title="173:214	For each test segment, the judges were presented with its text, and 3 alternative titles consisting of the reference and the titles produced by the hierarchical discriminative model, and the best performing baseline." ></td>
	<td class="line x" title="174:214	In addition, the judges had access to all of the segments in the book." ></td>
	<td class="line x" title="175:214	A total of 498 titles for 166 unique segments were ranked." ></td>
	<td class="line x" title="176:214	The system identities were hidden from the judges, and the titles were presented in random order." ></td>
	<td class="line x" title="177:214	The judges ranked the titles based on how well they represent the content of the segment." ></td>
	<td class="line x" title="178:214	Titles were ranked equal if they were judged to be equally representative of the segment." ></td>
	<td class="line x" title="179:214	Six people participated in this experiment." ></td>
	<td class="line x" title="180:214	All the participants were graduate students in computer science who had taken the algorithms class in the past and were reasonably familiar with the material." ></td>
	<td class="line x" title="181:214	7 Results Figure 3 shows fragments of the tables-of-contents generated by our method and the four baselines along with the reference counterpart." ></td>
	<td class="line x" title="182:214	These extracts illustrate three general phenomena that we observed in the test corpus." ></td>
	<td class="line x" title="183:214	First, the titles produced by keyword extraction exhibit a high degree of redundancy." ></td>
	<td class="line x" title="184:214	In fact, 40% of the titles produced by this method are repeated more than once in the table-of-contents." ></td>
	<td class="line x" title="185:214	In 7http://www.isi.edu/licensed-sw/see/rouge/ 549 Reference: hash tables direct address tables hash tables collision resolution by chaining analysis of hashing with chaining open addressing linear probing quadratic probing double hashing Flat Generative: linked list worst case time wasted space worst case running time to show that there are dynamic set occupied slot quadratic function double hashing Flat Discriminative: dictionary operations universe of keys computer memory element in the list hash table with load factor hash table hash function hash function double hashing Keyword Extraction: hash table dynamic set hash function worst case expected number hash table hash function hash table double hashing Hierarchical Generative: dictionary operations worst case time wasted space worst case running time to show that there are collision resolution linear time quadratic function double hashing Hierarchical Discriminative: dictionary operations direct address table computer memory worst case running time hash table with load factor address table hash function quadratic probing double hashing Figure 3: Fragments of tables-of-contents generated by our method and the four baselines along with the corresponding reference." ></td>
	<td class="line x" title="186:214	Rouge-1 Rouge-L Rouge-W Full Match HD 0.256 0.249 0.216 13.5 FD 0.241 0.234 0.203 13.1 HG 0.139 0.133 0.117 5.8 FG 0.094 0.090 0.079 4.1 Keyword 0.168 0.168 0.157 6.3 Table 3: Title quality as compared to the reference for the hierarchical discriminative (HD), flat discriminative (FD), hierarchical generative (HG), flat generative (FG) and Keyword models." ></td>
	<td class="line x" title="187:214	The improvement given by HD over FD in all three Rouge measures is significant at p  0.03 based on the Sign test." ></td>
	<td class="line x" title="188:214	better worse equal HD vs. FD 68 32 49 Reference vs. HD 115 13 22 Reference vs. FD 123 7 20 Table 4: Overall pairwise comparisons of the rankings given by the judges." ></td>
	<td class="line x" title="189:214	The improvement in title quality given by HD over FD is significant at p0.0002 based on the Sign test." ></td>
	<td class="line x" title="190:214	contrast, our method yields 5.5% of the titles as duplicates, as compared to 9% in the reference tableof-contents.8 Second, the fragments show that the two discriminative models  Flat and Hierarchical  have a number of common titles." ></td>
	<td class="line x" title="191:214	However, adding global dependencies to rerank titles generated by the local model changes 30% of the titles in the test set." ></td>
	<td class="line x" title="192:214	Comparison with reference tables-of-contents Table 3 shows the average ROUGE scores over the ten randomizations for the five automatic methods." ></td>
	<td class="line x" title="193:214	The hierarchical discriminative method consistently outperforms the four baselines according to all ROUGE metrics." ></td>
	<td class="line x" title="194:214	At the same time, these results also show that only a small ratio of the automatically generated titles are identical to the reference ones." ></td>
	<td class="line x" title="195:214	In some cases, the machine-generated titles are very close in meaning to the reference, but are verbalized differently." ></td>
	<td class="line x" title="196:214	Examples include pairs such as (Minimum Spanning Trees, Spanning Tree Problem) and (Wallace Tree, Multiplication Circuit).9 While measures like ROUGE can capture the similarity in the first pair, they cannot identify semantic proximity 8Titles such as Analysis and Chapter Outline are repeated multiple times in the text." ></td>
	<td class="line x" title="197:214	9A Wallace Tree is a circuit that multiplies two integers." ></td>
	<td class="line x" title="198:214	550 between the titles in the second pair." ></td>
	<td class="line x" title="199:214	Therefore, we supplement the results of this experiment with a manual assessment of title quality as described below." ></td>
	<td class="line x" title="200:214	Human assessment We analyze the human ratings by considering pairwise comparisons between the models." ></td>
	<td class="line x" title="201:214	Given two models, A and B, three outcomes are possible: A is better than B, B is better than A, or they are of equal quality." ></td>
	<td class="line x" title="202:214	The results of the comparison are summarized in Table 4." ></td>
	<td class="line x" title="203:214	These results indicate that using hierarchical information yields statistically significant improvement (at p  0.0002 based on the Sign test) over a flat counterpart." ></td>
	<td class="line x" title="204:214	8 Conclusion and Future Work This paper presents a method for the automatic generation of a table-of-contents." ></td>
	<td class="line x" title="205:214	The key strength of our method lies in its ability to track dependencies between generation decisions across different levels of the tree structure." ></td>
	<td class="line x" title="206:214	The results of automatic evaluation and manual assessment confirm the benefits of joint tree learning: our system is consistently ranked higher than non-hierarchical baselines." ></td>
	<td class="line x" title="207:214	We also plan to expand our method for the task of slide generation." ></td>
	<td class="line x" title="208:214	Like tables-of-contents, slide bullets are organized in a hierarchical fashion and are written in relatively short phrases." ></td>
	<td class="line x" title="209:214	From the language viewpoint, however, slides exhibit more variability and complexity than a typical table-ofcontents." ></td>
	<td class="line x" title="210:214	To address this challenge, we will explore more powerful generation methods that take into account syntactic information." ></td>
	<td class="line x" title="211:214	Acknowledgments The authors acknowledge the support of the National Science Foundation (CAREER grant IIS0448168 and grant IIS-0415865)." ></td>
	<td class="line x" title="212:214	We would also like to acknowledge the many people who took part in human evaluations." ></td>
	<td class="line x" title="213:214	Thanks to Michael Collins, Benjamin Snyder, Igor Malioutov, Jacob Eisenstein, Luke Zettlemoyer, Terry Koo, Erdong Chen, Zoran Dzunic and the anonymous reviewers for helpful comments and suggestions." ></td>
	<td class="line x" title="214:214	Any opinions, findings, conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the NSF." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-1096
Guided Learning for Bidirectional Sequence Classification
Shen, Libin;Satta, Giorgio;Joshi, Aravind K.;"></td>
	<td class="line x" title="1:214	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 760767, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:214	c2007 Association for Computational Linguistics Guided Learning for Bidirectional Sequence Classification Libin Shen BBN Technologies Cambridge, MA 02138, USA lshen@bbn.com Giorgio Satta Dept. of Inf." ></td>
	<td class="line x" title="3:214	Engg. University of Padua I-35131 Padova, Italy satta@dei.unipd.it Aravind K. Joshi Department of CIS University of Pennsylvania Philadelphia, PA 19104, USA joshi@seas.upenn.edu Abstract In this paper, we propose guided learning, a new learning framework for bidirectional sequence classification." ></td>
	<td class="line x" title="4:214	The tasks of learning the order of inference and training the local classifier are dynamically incorporated into a single Perceptron like learning algorithm." ></td>
	<td class="line x" title="5:214	We apply this novel learning algorithm to POS tagging." ></td>
	<td class="line x" title="6:214	It obtains an error rate of 2.67% on the standard PTB test set, which represents 3.3% relative error reduction over the previous best result on the same data set, while using fewer features." ></td>
	<td class="line x" title="7:214	1 Introduction Many NLP tasks can be modeled as a sequence classification problem, such as POS tagging, chunking, and incremental parsing." ></td>
	<td class="line x" title="8:214	A traditional method to solve this problem is to decompose the whole task into a set of individual tasks for each token in the input sequence, and solve these small tasks in a fixed order, usually from left to right." ></td>
	<td class="line x" title="9:214	In this way, the output of the previous small tasks can be used as the input of the later tasks." ></td>
	<td class="line x" title="10:214	HMM and MaxEnt Markov Model are examples of this method." ></td>
	<td class="line x" title="11:214	Lafferty et al.(2001) showed that this approach suffered from the so called label bias problem (Bottou, 1991)." ></td>
	<td class="line x" title="13:214	They proposed Conditional Random Fields (CRF) as a general solution for sequence classification." ></td>
	<td class="line x" title="14:214	CRF models a sequence as an undirected graph, which means that all the individual tasks are solved simultaneously." ></td>
	<td class="line x" title="15:214	Taskar et al.(2003) improved the CRF method by employing the large margin method to separate the gold standard sequence labeling from incorrect labellings." ></td>
	<td class="line x" title="17:214	However, the complexity of quadratic programming for the large margin approach prevented it from being used in large scale NLP tasks." ></td>
	<td class="line x" title="18:214	Collins (2002) proposed a Perceptron like learning algorithm to solve sequence classification in the traditional left-to-right order." ></td>
	<td class="line x" title="19:214	This solution does not suffer from the label bias problem." ></td>
	<td class="line x" title="20:214	Compared to the undirected methods, the Perceptron like algorithm is faster in training." ></td>
	<td class="line x" title="21:214	In this paper, we will improve upon Collins algorithm by introducing a bidirectional searching strategy, so as to effectively utilize more context information at little extra cost." ></td>
	<td class="line x" title="22:214	When a bidirectional strategy is used, the main problem is how to select the order of inference." ></td>
	<td class="line x" title="23:214	Tsuruoka and Tsujii (2005) proposed the easiest-first approach which greatly reduced the computation complexity of inference while maintaining the accuracy on labeling." ></td>
	<td class="line x" title="24:214	However, the easiest-first approach only serves as a heuristic rule." ></td>
	<td class="line x" title="25:214	The order of inference is not incorporated into the training of the MaxEnt classifier for individual labeling." ></td>
	<td class="line x" title="26:214	Here, we will propose a novel learning framework, namely guided learning, to integrate classification of individual tokens and inference order selection into a single learning task." ></td>
	<td class="line oc" title="27:214	We proposed a Perceptron like learning algorithm (Collins and Roark, 2004; Daume III and Marcu, 2005) for guided learning." ></td>
	<td class="line x" title="28:214	We apply this algorithm to POS tagging, a classic sequence learning problem." ></td>
	<td class="line x" title="29:214	Our system reports an error rate of 2.67% on the standard PTB test set, a relative 3.3% error reduction of the previous best system (Toutanova et al. , 2003) by using fewer features." ></td>
	<td class="line x" title="30:214	By using deterministic search, it obtains an error rate of 2.73%, a 5.9% relative error reduction 760 over the previous best deterministic algorithm (Tsuruoka and Tsujii, 2005)." ></td>
	<td class="line x" title="31:214	The new POS tagger is similar to (Toutanova et al. , 2003; Tsuruoka and Tsujii, 2005) in the way that we employ context features." ></td>
	<td class="line x" title="32:214	We use a bidirectional search strategy (Woods, 1976; Satta and Stock, 1994), and our algorithm is based on Perceptron learning (Collins, 2002)." ></td>
	<td class="line x" title="33:214	A unique contribution of our work is on the integration of individual classification and inference order selection, which are learned simultaneously." ></td>
	<td class="line x" title="34:214	2 Guided Learning for Bidirectional Labeling We first present an example of POS tagging to show the idea of bidirectional labeling." ></td>
	<td class="line x" title="35:214	Then we present the inference algorithm and the learning algorithm." ></td>
	<td class="line x" title="36:214	2.1 An Example of POS tagging Suppose that we have an input sentence Agatha found that book interesting w1 w2 w3 w4 w5 (Step 0) If we scan from left to right, we may find it difficult to resolve the ambiguity of the label for that, which could be either DT (determiner), or IN (preposition or subordinating conjunction) in the Penn Treebank." ></td>
	<td class="line x" title="37:214	However, if we resolve the labels for book and interesting, it would be relatively easy to figure out the correct label for that." ></td>
	<td class="line x" title="38:214	Now, we show how bidirectional inference works on this sample." ></td>
	<td class="line x" title="39:214	Suppose we use beam search with width of 2, and we use a window of (-2, 2) for context features." ></td>
	<td class="line x" title="40:214	For the first step, we enumerate hypotheses for each word." ></td>
	<td class="line x" title="41:214	For example, found could have a label VBN or VBD." ></td>
	<td class="line x" title="42:214	Suppose that at this point the most favorable action, out of the candidate hypotheses, is the assignment of NN to book, according to the context features defined on words." ></td>
	<td class="line x" title="43:214	Then, we resolve the label for book first." ></td>
	<td class="line x" title="44:214	We maintain the top two hypotheses as shown below." ></td>
	<td class="line x" title="45:214	Here, the second most favorable label for book is VB." ></td>
	<td class="line x" title="46:214	NN VB Agatha found that book interesting w1 w2 w3 w4 w5 (Step 1) At the second step, assume the most favorable action is the assignment of label JJ to interesting in the context of NN for book." ></td>
	<td class="line x" title="47:214	Then we maintain the top two hypotheses for span book interesting as shown below." ></td>
	<td class="line x" title="48:214	The second most favorable label for interesting is still JJ, but in the context of VB for book." ></td>
	<td class="line x" title="49:214	NN------JJ VB------JJ Agatha found that book interesting w1 w2 w3 w4 w5 (Step 2) Then, suppose we are most confident for assigning labels VBD and VBN to found, in that order." ></td>
	<td class="line x" title="50:214	We get two separated tagged spans as shown below." ></td>
	<td class="line x" title="51:214	VBD NN------JJ VBN VB------JJ Agatha found that book interesting w1 w2 w3 w4 w5 (Step 3) In the next step, suppose we are most confident for assigning label DT to that under the context of VBD on the left and NN-JJ on the right side, as shown below (second most favorable action, not discussed here, is also displayed)." ></td>
	<td class="line x" title="52:214	After tagging w3, two separated spans merge into one, starting from found to interesting." ></td>
	<td class="line x" title="53:214	VBD---DT---NN------JJ VBD---IN---NN------JJ Agatha found that book interesting w1 w2 w3 w4 w5 (Step 4) For the last step, we assign label NNP to Agatha, which could be an out-of-vocabulary word, under the context of VBD-DT on the right." ></td>
	<td class="line x" title="54:214	NNP---VBD---DT---NN------JJ NNP---VBD---IN---NN------JJ Agatha found that book interesting w1 w2 w3 w4 w5 (Step 5) This simple example has shown the advantage of adopting a flexible search strategy." ></td>
	<td class="line x" title="55:214	However, it is still unclear how we maintain the hypotheses, how we keep candidates and accepted labels and spans, and how we employ dynamic programming." ></td>
	<td class="line x" title="56:214	We will answer these questions in the formal definition of the inference algorithm in the next section." ></td>
	<td class="line x" title="57:214	761 2.2 Inference Algorithm Terminology: Let the input sequence be w1w2wn." ></td>
	<td class="line x" title="58:214	For each token wi, we are expected to assign a label tiT, with T the label set." ></td>
	<td class="line x" title="59:214	A subsequence wiwj is called a span, and is denoted [i,j]." ></td>
	<td class="line x" title="60:214	Each span p considered by the algorithm is associated with one or more hypotheses, that is, sequences over T having the same length as p. Part of the label sequence of each hypothesis is used as a context for labeling tokens outside the span p. For example, if a tri-gram model is adopted, we use the two labels on the left boundary and the two labels on the right boundary of the hypothesis for labeling outside tokens." ></td>
	<td class="line x" title="61:214	The left two labels are called the left interface, and the right two labels are called the right interface." ></td>
	<td class="line x" title="62:214	Left and right interfaces have only one label in case of spans of length one." ></td>
	<td class="line x" title="63:214	A pair s = (Ileft, Iright) with a left and a right interface is called a state." ></td>
	<td class="line x" title="64:214	We partition the hypotheses associated with span p into sets compatible with the same state." ></td>
	<td class="line x" title="65:214	In practice, for span p, we use a matrix Mp indexed by states, so that Mp(s), s = (Ileft, Iright), is the set of all hypotheses associated with p that are compatible with Ileft and Iright." ></td>
	<td class="line x" title="66:214	For a span p and a state s, we denote the associated top hypothesis as s.T = argmax hMp(s) V (h), where V is the score of a hypothesis (defined in (1) below)." ></td>
	<td class="line x" title="67:214	Similarly, we denote the top state for p as p.S = argmax s: Mp(s)negationslash= V (s.T)." ></td>
	<td class="line x" title="68:214	Therefore, for each span p, we have a top hypothesis p.S.T, whose score is the highest among all the hypotheses for span p. Hypotheses are started and grown by means of labeling actions." ></td>
	<td class="line x" title="69:214	For each hypothesis h associated with a span p we maintain its most recent labeling action h.A, involving some token within p, as well as the states h.SL and h.SR that have been used as context by such an action, if any." ></td>
	<td class="line x" title="70:214	Note that h.SL and h.SR refer to spans that are subsequences of p. We recursively compute the score of h as V (h) = V (h.SL.T) + V (h.SR.T) + U(h.A), (1) Algorithm 1 Inference Algorithm Require: token sequence w1wn; Require: beam width B; Require: weight vector w; 1: Initialize P, the set of accepted spans; 2: Initialize Q, the queue of candidate spans; 3: repeat 4: span pprimeargmaxpQ U(p.S.T.A); 5: Update P with pprime; 6: Update Q with pprime and P; 7: until (Q =) where U is the score of an action." ></td>
	<td class="line x" title="71:214	In other words, the score of an hypothesis is the sum of the score of the most recent action h.A and the scores of the top hypotheses of the context states." ></td>
	<td class="line x" title="72:214	The score of an action h.A is computed through a linear function whose weight vector is w, as U(h.A) = wf(h.A), (2) where f(h.A) is the feature vector of action h.A, which depends on h.SL and h.SR. Algorithm: Algorithm 1 is the inference algorithm." ></td>
	<td class="line x" title="73:214	We are given the input sequence and two parameters, beam width B to determine the number of states maintained for each span, and weight vector w used to compute the score of an action." ></td>
	<td class="line x" title="74:214	We first initialize the set P of accepted spans with the empty set." ></td>
	<td class="line x" title="75:214	Then we initialize the queue Q of candidate spans with span [i,i] for each token wi, and for each tT assigned to wi we set M[i,i]((t,t)) ={it}, where it represents the hypothesis consisting of a single action which assigns label t to wi." ></td>
	<td class="line x" title="76:214	This provides the set of starting hypotheses." ></td>
	<td class="line x" title="77:214	As for the example Agatha found that book interesting in the previous subsection, we have  P =  Q ={[1,1],[2,2],[3,3],[4,4],[5,5]} Suppose NN and VB are the two possible POS tags for w4 book." ></td>
	<td class="line x" title="78:214	We have  M[4,4](NN, NN) ={h441 = 4NN}  M[4,4](VB, VB) ={h442 = 4VB} The most recent action of hypothesis h441 is to assign NN to w4." ></td>
	<td class="line x" title="79:214	According to Equation (2), the score 762 of this action U(h441.A) depends on the features defined on the local context of action." ></td>
	<td class="line x" title="80:214	For example, f1001(h441.A) = braceleftbigg 1 if t = NNw1 = that 0 otherwise, where w1 represents the left word." ></td>
	<td class="line x" title="81:214	It should be noted that, for all the features depending on the neighboring tags, the value is always 0, since those tags are still unknown in the step of initialization." ></td>
	<td class="line x" title="82:214	Since this operation does not depend on solved tags, we have V (h441) = U(h411.A), according to Equation (1)." ></td>
	<td class="line x" title="83:214	The core of the algorithm repeatedly selects a candidate span from Q, and uses it to update P and Q, until a span covering the whole sequence is added to P and Q becomes empty." ></td>
	<td class="line x" title="84:214	This is explained in detail below." ></td>
	<td class="line x" title="85:214	At each step, we remove from Q the span pprime such that the action (not hypothesis) score of its top hypothesis, pprime.S.T, is the highest." ></td>
	<td class="line x" title="86:214	This represents the labeling action for the next move that we are most confident about." ></td>
	<td class="line x" title="87:214	Now we need to update P and Q with the selected span pprime." ></td>
	<td class="line x" title="88:214	We add pprime to P, and remove from P the spans included in pprime, if any." ></td>
	<td class="line x" title="89:214	Let S be the set of removed spans." ></td>
	<td class="line x" title="90:214	We remove from Q each span which takes one of the spans inS as context, and replace it with a new candidate span taking pprime (and another accepted span) as context." ></td>
	<td class="line x" title="91:214	We always maintain B different states for each span." ></td>
	<td class="line x" title="92:214	Back to the previous example, after Step 3 is completed, w2 found, w4 book and w5 interesting have been tagged and we have  P ={[2,2],[4,5]}  Q ={[1,2],[2,5]} There are two candidate spans in Q, each with its associated hypotheses and most recent actions." ></td>
	<td class="line x" title="93:214	More specifically, we can either solve w1 based on the context hypotheses for [2,2], resulting in span [1,2], or else solve w3 based on the context hypotheses in [2,2] and [4,5], resulting in span [2,5]." ></td>
	<td class="line x" title="94:214	The top two states for span [2,2] are  M[2,2](VBD, VBD) ={h221 = 2VBD}  M[2,2](VBN, VBN) ={h222 = 2VBN} and the top two states for span [4,5] are  M[4,5](NN-JJ, NN-JJ) ={h451 = (NN,NN)5JJ}  M[4,5](VB-JJ, VB-JJ) ={h452 = (VB,VB)5JJ} Here (NN,NN)5  JJ represents the hypothesis coming from the action of assigning JJ to w5 under the left context state of (NN,NN)." ></td>
	<td class="line x" title="95:214	(VB,VB)5  JJ has a similar meaning.1 We first compute the hypotheses resulting from all possible POS tag assignments to w3, under all possible state combinations of the neighboring spans [2,2] and [4,5]." ></td>
	<td class="line x" title="96:214	Suppose the highest score action consists in the assignment of DT under the left context state (VBD, VBD) and the right context state (NN-JJ, NNJJ)." ></td>
	<td class="line x" title="97:214	We obtain hypothesis h251 = (VBD,VBD)3 DT(NN-JJ, NN-JJ) with V (h251) = V ((VBD,VBD).T) + V ((NN-JJ,NN-JJ).T) + U(h251.A) = V (h221) + V (h451) +wf(h251.A) Here, features for action h251.A may depend on the left tag VBD and right tags NN-JJ, which have been solved before." ></td>
	<td class="line x" title="98:214	More details of the feature functions are given in Section 4.2." ></td>
	<td class="line x" title="99:214	For example, we can have features like f2002(h251.A) = braceleftbigg 1 if t = DTt+2 = JJ 0 otherwise, We maintain the top two states with the highest hypothesis scores, if the beam width is set to two." ></td>
	<td class="line x" title="100:214	We have  M[2,5](VBD-DT, NN-JJ) = {h251 = (VBD,VBD)3DT(NN-JJ,NN-JJ)}  M[2,5](VBD-IN, NN-JJ) = {h252 = (VBD,VBD)3IN(NN-JJ,NN-JJ)} Similarly, we compute the top hypotheses and states for span [1,2]." ></td>
	<td class="line x" title="101:214	Suppose now the hypothesis with the highest action score is h251." ></td>
	<td class="line x" title="102:214	Then we update P by adding [2,5] and removing [2,2] and [4,5], which are covered by [2,5]." ></td>
	<td class="line x" title="103:214	We also update Q by removing [2,5] and [1,2],2 and add new candidate span [1,5] resulting in  P ={[2,5]}  Q ={[1,5]} 1It should be noted that, in these cases, each state contains only one hypothesis." ></td>
	<td class="line x" title="104:214	However, if the span is longer than 4 words, there may exist multiple hypotheses for the same state." ></td>
	<td class="line x" title="105:214	For example, hypotheses DT-NN-VBD-DT-JJ and DTNN-VBN-DT-JJ have the same left interface DT-NN and right interface DT-JJ." ></td>
	<td class="line x" title="106:214	2Span [1,2] depends on [2,2] and [2,2] has been removed from P. So it is no longer a valid candidate given the accepted spans in P. 763 The algorithm is especially designed in such a way that, at each step, some new span is added to P or else some spans already present in P are extended by some token(s)." ></td>
	<td class="line x" title="107:214	Furthermore, no pair of overlapping spans is ever found in P, and the number of pairs of overlapping spans that may be found in Q is always bounded by a constant." ></td>
	<td class="line x" title="108:214	This means that the algorithm performs at most n iterations, and its running time is thereforeO(B2n), that is, linear in the length of the input sequence." ></td>
	<td class="line x" title="109:214	2.3 Learning Algorithm In this section, we propose guided learning, a Perceptron like algorithm, to learn the weight vector w, as shown in Algorithm 2." ></td>
	<td class="line x" title="110:214	We use pprime.G to represent the gold standard hypothesis on span pprime." ></td>
	<td class="line x" title="111:214	For each input sequence Xr and the gold standard sequence of labeling Yr, we first initialize P and Q as in the inference algorithm." ></td>
	<td class="line x" title="112:214	Then we select the span for the next move as in Algorithm 1." ></td>
	<td class="line x" title="113:214	If pprime.S.T, the top hypothesis of the selected span pprime, is compatible with the gold standard, we update P and Q as in Algorithm 1." ></td>
	<td class="line x" title="114:214	Otherwise, we update the weight vector in the Perceptron style, by promoting the features of the gold standard action, and demoting the features of the action of the top hypothesis." ></td>
	<td class="line x" title="115:214	Then we re-generate the queue Q with P and the updated weight vector w. Specifically, we first remove all the elements in Q, and then generate hypotheses for all the possible spans based on the context spans in P. Hypothesis scores and action scores are calculated with the updated weight vector w. A special aspect of Algorithm 2 is that we maintain two scores: the score of the action represents the confidence for the next move, and the score of the hypothesis represents the overall quality of a partial result." ></td>
	<td class="line x" title="116:214	The selection for the next action directly depends on the score of the action, but not on the score of the hypothesis." ></td>
	<td class="line x" title="117:214	On the other hand, the score of the hypothesis is used to maintain top partial results for each span." ></td>
	<td class="line x" title="118:214	We briefly describe the soundness of the Guided Learning Algorithm in terms of two aspects." ></td>
	<td class="line x" title="119:214	First, in Algorithm 2 weight update is activated whenever there exists an incorrect state s, the action score of whose top hypothesis s.T is higher than that of any state in each span." ></td>
	<td class="line x" title="120:214	We demote this action and promote the gold standard action on the same span." ></td>
	<td class="line x" title="121:214	Algorithm 2 Guided Learning Algorithm Require: training sequence pairs(Xr,Yr)}1rR; Require: beam width B and iterations I; 1: w0; 2: for (i1; iI; i++) do 3: for (r1; rR; r++) do 4: Load sequence Xr and gold labeling Yr." ></td>
	<td class="line x" title="122:214	5: Initialize P, the set of accepted spans 6: Initialize Q, the queue of candidate spans; 7: repeat 8: pprimeargmaxpQ U(p.S.T.A); 9: if (pprime.S.T = pprime.G) then 10: Update P with pprime; 11: Update Q with pprime and P; 12: else 13: promote(w,f(pprime.G.A)); 14: demote(w,f(pprime.S.T.A)); 15: Re-generate Q with w and P; 16: end if 17: until (Q =) 18: end for 19: end for However, we do not automatically adopt the gold standard action on this span." ></td>
	<td class="line x" title="123:214	Instead, in the next step, the top hypothesis of another span might be selected based on the score of action, which means that it becomes the most favorable action according to the updated weights." ></td>
	<td class="line x" title="124:214	As a second aspect, if the action score of a gold standard hypothesis is higher than that of any others, this hypothesis and the corresponding span are guaranteed to be selected at line 8 of Algorithm 2." ></td>
	<td class="line x" title="125:214	The reason for this is that the scores of the context hypotheses of a gold standard hypothesis must be no less than those of other hypotheses of the same span." ></td>
	<td class="line x" title="126:214	This could be shown recursively with respect to Equation 1, because the context hypotheses of a gold standard hypothesis are also compatible with the gold standard." ></td>
	<td class="line x" title="127:214	Furthermore, if we take (xi = f(pprime.G.A)f(pprime.S.T.A),yi = +1) as a positive sample, and (xj = f(pprime.S.T.A)f(pprime.G.A),yj =1) as a negative sample, the weight updates at lines 13 764 and 14 are a stochastic approximation of gradient descent that minimizes the squared errors of the misclassified samples (Widrow and Hoff, 1960)." ></td>
	<td class="line x" title="128:214	What is special with our learning algorithm is the strategy used to select samples for training." ></td>
	<td class="line x" title="129:214	In general, this novel learning framework lies between supervised learning and reinforcement learning." ></td>
	<td class="line x" title="130:214	Guided learning is more difficult than supervised learning, because we do not know the order of inference." ></td>
	<td class="line x" title="131:214	The order is learned automatically, and partial output is in turn used to train the local classifier." ></td>
	<td class="line x" title="132:214	Therefore, the order of inference and the local classification are dynamically incorporated in the learning phase." ></td>
	<td class="line x" title="133:214	Guided learning is not as hard as reinforcement learning." ></td>
	<td class="line x" title="134:214	At each local step in learning, we always know the undesirable labeling actions according to the gold standard, although we do not know which is the most desirable." ></td>
	<td class="line x" title="135:214	In this approach, we can easily collect the automatically generated negative samples, and use them in learning." ></td>
	<td class="line x" title="136:214	These negative samples are exactly those we will face during inference with the current weight vector." ></td>
	<td class="line x" title="137:214	In our experiments, we have used Averaged Perceptron (Collins, 2002; Freund and Schapire, 1999) and Perceptron with margin (Krauth and Mezard, 1987) to improve performance." ></td>
	<td class="line x" title="138:214	3 Related Works Tsuruoka and Tsujii (2005) proposed a bidirectional POS tagger, in which the order of inference is handled with the easiest-first heuristic." ></td>
	<td class="line x" title="139:214	Gimenez and M`arquez (2004) combined the results of a left-toright scan and a right-to-left scan." ></td>
	<td class="line x" title="140:214	In our model, the order of inference is dynamically incorporated into the training of the local classifier." ></td>
	<td class="line x" title="141:214	Toutanova et al.(2003) reported a POS tagger based on cyclic dependency network." ></td>
	<td class="line x" title="143:214	In their work, the order of inference is fixed as from left to right." ></td>
	<td class="line x" title="144:214	In this approach, large beam width is required to maintain the ambiguous hypotheses." ></td>
	<td class="line x" title="145:214	In our approach, we can handle tokens that we are most confident about first, so that our system does not need a large beam." ></td>
	<td class="line x" title="146:214	As shown in Section 4.2, even deterministic inference shows rather good results." ></td>
	<td class="line x" title="147:214	Our guided learning can be modeled as a search algorithm with Perceptron like learning (Daume III and Marcu, 2005)." ></td>
	<td class="line x" title="148:214	However, as far as we know, Data Set Sections Sentences Tokens Training 0-18 38,219 912,344 Develop 19-21 5,527 131,768 Test 22-24 5,462 129,654 Table 1: Data set splits the mechanism of bidirectional search with an online learning algorithm has not been investigated before." ></td>
	<td class="line oc" title="149:214	In (Daume III and Marcu, 2005), as well as other similar works (Collins, 2002; Collins and Roark, 2004; Shen and Joshi, 2005), only left-toright search was employed." ></td>
	<td class="line x" title="150:214	Our guided learning algorithm provides more flexibility in search with an automatically learned order." ></td>
	<td class="line x" title="151:214	In addition, our treatment of the score of action and the score of hypothesis is unique (see discussion in Section 2.3)." ></td>
	<td class="line n" title="152:214	Furthermore, compared to the above works, our guided learning algorithm is more aggressive on learning." ></td>
	<td class="line oc" title="153:214	In (Collins and Roark, 2004; Shen and Joshi, 2005), a search stops if there is no hypothesis compatible with the gold standard in the queue of candidates." ></td>
	<td class="line x" title="154:214	In (Daume III and Marcu, 2005), the search is resumed after some gold standard compatible hypotheses are inserted into a queue for future expansion, and the weights are updated correspondingly." ></td>
	<td class="line x" title="155:214	However, there is no guarantee that the updated weights assign a higher score to those inserted gold standard compatible hypotheses." ></td>
	<td class="line x" title="156:214	In our algorithm, the gold standard compatible hypotheses are used for weight update only." ></td>
	<td class="line x" title="157:214	As a result, after each sentence is processed, the weight vector can usually successfully predict the gold standard parse." ></td>
	<td class="line x" title="158:214	Therefore our learning algorithm is aggressive on weight update." ></td>
	<td class="line x" title="159:214	As far as this aspect is concerned, our algorithm is similar to the MIRA algorithm in (Crammer and Singer, 2003)." ></td>
	<td class="line x" title="160:214	In MIRA, one always knows the correct hypothesis." ></td>
	<td class="line x" title="161:214	In our case, we do not know the correct order of operations." ></td>
	<td class="line x" title="162:214	So we use our form of weight update to implement aggressive learning." ></td>
	<td class="line x" title="163:214	4 Experiments on POS Tagging 4.1 Settings We apply our guided learning algorithm to POS tagging." ></td>
	<td class="line x" title="164:214	We carry out experiments on the standard data set of the Penn Treebank (PTB) (Marcus et al. , 1994)." ></td>
	<td class="line x" title="165:214	Following (Ratnaparkhi, 1996; Collins, 2002; Toutanova et al. , 2003; Tsuruoka and Tsujii, 2005), 765 Feature Sets Templates Error% A Ratnaparkhis 3.05 B A + [t0,t1],[t0,t1,t1],[t0,t1,t2] 2.92 C B + [t0,t2],[t0,t2],[t0,t2,w0],[t0,t1,w0],[t0,t1,w0], [t0,t2,w0], [t0,t2,t1,w0],[t0,t1,t1,w0],[t0,t1,t2,w0] 2.84 D C + [t0,w1,w0],[t0,w1,w0] 2.78 E D + [t0,X = prefix or suffix of w0],4 < |X|  9 2.72 Table 2: Experiments on the development data with beam width of 3 we cut the PTB into the training, development and test sets as shown in Table 1." ></td>
	<td class="line x" title="166:214	We use tools provided by CoNLL-2005 3 to extract POS tags from the mrg files of PTB." ></td>
	<td class="line x" title="167:214	So the data set is the same as previous work." ></td>
	<td class="line x" title="168:214	We use the development set to select features and estimate the number of iterations in training." ></td>
	<td class="line x" title="169:214	In our experiments, we enumerate all the POS tags for each word instead of using a dictionary as in (Ratnaparkhi, 1996), since the size of the tag set is tractable and our learning algorithm is efficient enough." ></td>
	<td class="line x" title="170:214	4.2 Results Effect of Features: We first run the experiments to evaluate the effect of features." ></td>
	<td class="line x" title="171:214	We use templates to define features." ></td>
	<td class="line x" title="172:214	For this set of experiments, we set the beam width B = 3 as a balance between speed and accuracy." ></td>
	<td class="line x" title="173:214	The guided learning algorithm usually converges on the development data set in 4-8 iterations over the training data." ></td>
	<td class="line x" title="174:214	Table 2 shows the error rate on the development set with different features." ></td>
	<td class="line x" title="175:214	We first use the same feature set used in (Ratnaparkhi, 1996), which includes a set of prefix, suffix and lexical features, as well as some bi-gram and tri-gram context features." ></td>
	<td class="line x" title="176:214	Following (Collins, 2002), we do not distinguish rare words." ></td>
	<td class="line x" title="177:214	On set A, Ratnaparkhis feature set, our system reports an error rate of 3.05% on the development data set." ></td>
	<td class="line x" title="178:214	With set B, we include a few feature templates which are symmetric to those in Ratnaparkhis set, but are only available with bidirectional search." ></td>
	<td class="line x" title="179:214	With set C, we add more bi-gram and tri-gram features." ></td>
	<td class="line x" title="180:214	With set D, we include bi-lexical features." ></td>
	<td class="line x" title="181:214	With set E, we use prefixes and suffixes of length up to 9, as in (Toutanova et al. , 2003; Tsuruoka and Tsujii, 2005)." ></td>
	<td class="line x" title="182:214	We obtain 2.72% of error rate." ></td>
	<td class="line x" title="183:214	We will use this feature set on our final experiments on the test data." ></td>
	<td class="line x" title="184:214	Effect of Search and Learning Strategies: For the second set of experiments, we evaluate the effect of 3http://www.lsi.upc.es/srlconll/soft.html, package srlconll1.1.tgz." ></td>
	<td class="line x" title="185:214	Search Aggressive?" ></td>
	<td class="line x" title="186:214	Beam=1 Beam=3 L-to-R Yes 2.94 2.82 L-to-R No 3.24 2.75 Bi-Dir Yes 2.84 2.72 Bi-Dir No does not converge Table 3: Experiments on the development data search methods, learning strategies, and beam width." ></td>
	<td class="line x" title="187:214	We use feature set E for this set of experiments." ></td>
	<td class="line x" title="188:214	Table 3 shows the error rates on the development data set with both left-to-right (L-to-R) and bidirectional (Bi-Dir) search methods." ></td>
	<td class="line x" title="189:214	We also tested both aggressive learning and non-aggressive learning strategies with beam width of 1 and 3." ></td>
	<td class="line x" title="190:214	First, with non-aggressive learning on bidirectional search, the error rate does not converge to a comparable number." ></td>
	<td class="line x" title="191:214	This is due to the fact that the search space is too large in bidirectional search, if we do not use aggressive learning to constrain the samples for learning." ></td>
	<td class="line x" title="192:214	With aggressive learning, the bidirectional approach always shows advantages over left-to-right search." ></td>
	<td class="line x" title="193:214	However, the gap is not large." ></td>
	<td class="line x" title="194:214	This is due to the fact that the accuracy of POS tagging is very high." ></td>
	<td class="line x" title="195:214	As a result, we can always keep the gold-standard tags in the beam even with left-to-right search in training." ></td>
	<td class="line x" title="196:214	This can also explain why the performance of leftto-right search with non-aggressive learning is close to bidirectional search if the beam is large enough." ></td>
	<td class="line x" title="197:214	However, with beam width = 1, non-aggressive learning over left-to-right search performs much worse, because in this case it is more likely that the gold-standard tag is not in the beam." ></td>
	<td class="line x" title="198:214	This set of experiments show that guided learning is more preferable for tasks with higher ambiguities." ></td>
	<td class="line x" title="199:214	In our recent work (Shen and Joshi, 2007), we have applied a variant of this algorithm to dependency parsing, and showed significant improvement over left-to-right non-aggressive learning strategy." ></td>
	<td class="line x" title="200:214	Comparison: Table 4 shows the comparison with the previous works on the PTB test sections." ></td>
	<td class="line xc" title="201:214	766 System Beam Error% (Ratnaparkhi, 1996) 5 3.37 (Tsuruoka and Tsujii, 2005) 1 2.90 (Collins, 2002) 2.89 Guided Learning, feature B 3 2.85 (Tsuruoka and Tsujii, 2005) all 2.85 (Gimenez and M`arquez, 2004) 2.84 (Toutanova et al. , 2003) 2.76 Guided Learning, feature E 1 2.73 Guided Learning, feature E 3 2.67 Table 4: Comparison with the previous works According to the experiments shown above, we build our best system by using feature set E with beam width B = 3." ></td>
	<td class="line x" title="202:214	The number of iterations on the training data is estimated with respect to the development data." ></td>
	<td class="line x" title="203:214	We obtain an error rate of 2.67% on the test data." ></td>
	<td class="line x" title="204:214	With deterministic search, or beam with B = 1, we obtain an error rate of 2.73%." ></td>
	<td class="line x" title="205:214	Compared to previous best result on the same data set, 2.76% by (Toutanova et al. , 2003), our best result shows a relative error reduction of 3.3%." ></td>
	<td class="line x" title="206:214	This result is very promising, since we have not used any specially designed features in our experiments." ></td>
	<td class="line x" title="207:214	It is reported in (Toutanova et al. , 2003) that a crude company name detector was used to generate features, and it gave rise to significant improvement in performance." ></td>
	<td class="line x" title="208:214	However, it is difficult for us to duplicate exactly the same feature for the purpose of comparison, although it is convenient to use features like that in our framework." ></td>
	<td class="line x" title="209:214	5 Conclusions In this paper, we propose guided learning, a new learning framework for bidirectional sequence classification." ></td>
	<td class="line x" title="210:214	The tasks of learning the order of inference and training the local classifier are dynamically incorporated into a single Perceptron like algorithm." ></td>
	<td class="line x" title="211:214	We apply this novel algorithm to POS tagging." ></td>
	<td class="line x" title="212:214	It obtains an error rate of 2.67% on the standard PTB test set, which represents 3.3% relative error reduction over the previous best result (Toutanova et al. , 2003) on the same data set, while using fewer features." ></td>
	<td class="line x" title="213:214	By using deterministic search, it obtains an error rate of 2.73%, a 5.9% relative error reduction over the previous best deterministic algorithm (Tsuruoka and Tsujii, 2005)." ></td>
	<td class="line x" title="214:214	It should be noted that the error rate is close to the inter-annotator discrepancy on PTB, the standard test set for POS tagging, therefore it is very difficult to achieve improvement." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-1106
Chinese Segmentation with a Word-Based Perceptron Algorithm
Zhang, Yue;Clark, Stephen;"></td>
	<td class="line x" title="1:195	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 840847, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:195	c2007 Association for Computational Linguistics Chinese Segmentation with a Word-Based Perceptron Algorithm Yue Zhang and Stephen Clark Oxford University Computing Laboratory Wolfson Building, Parks Road Oxford OX1 3QD, UK {yue.zhang,stephen.clark}@comlab.ox.ac.uk Abstract Standard approaches to Chinese word segmentation treat the problem as a tagging task, assigning labels to the characters in the sequence indicating whether the character marks a word boundary." ></td>
	<td class="line x" title="3:195	Discriminatively trained models based on local character features are used to make the tagging decisions, with Viterbi decoding finding the highest scoring segmentation." ></td>
	<td class="line x" title="4:195	In this paper we propose an alternative, word-based segmentor, which uses features based on complete words and word sequences." ></td>
	<td class="line x" title="5:195	The generalized perceptron algorithm is used for discriminative training, and we use a beamsearch decoder." ></td>
	<td class="line x" title="6:195	Closed tests on the first and second SIGHAN bakeoffs show that our system is competitive with the best in the literature, achieving the highest reported F-scores for a number of corpora." ></td>
	<td class="line x" title="7:195	1 Introduction Words are the basic units to process for most NLP tasks." ></td>
	<td class="line x" title="8:195	The problem of Chinese word segmentation (CWS) is to find these basic units for a given sentence, which is written as a continuous sequence of characters." ></td>
	<td class="line x" title="9:195	It is the initial step for most Chinese processing applications." ></td>
	<td class="line x" title="10:195	Chinese character sequences are ambiguous, often requiring knowledge from a variety of sources for disambiguation." ></td>
	<td class="line x" title="11:195	Out-of-vocabulary (OOV) words are a major source of ambiguity." ></td>
	<td class="line x" title="12:195	For example, a difficult case occurs when an OOV word consists of characters which have themselves been seen as words; here an automatic segmentor may split the OOV word into individual single-character words." ></td>
	<td class="line x" title="13:195	Typical examples of unseen words include Chinese names, translated foreign names and idioms." ></td>
	<td class="line x" title="14:195	The segmentation of known words can also be ambiguous." ></td>
	<td class="line x" title="15:195	For example, G1FOCQ should be G1 FO(here)CQ(flour) in the sentence G1FOCQDWD7DS BH (flour and rice are expensive here) or G1(here) FOCQ (inside) in the sentence G1FOCQDSF3 (its cold inside here)." ></td>
	<td class="line x" title="16:195	The ambiguity can be resolved with information about the neighboring words." ></td>
	<td class="line x" title="17:195	In comparison, for the sentences BPA8AQDSAGEF, possible segmentations include BPA8 (the discussion) AQ (will) DS (very) AGEF (be successful) and BPA8AQ(the discussion meeting)DS(very)AGEF(be successful)." ></td>
	<td class="line x" title="18:195	The ambiguity can only be resolved with contextual information outside the sentence." ></td>
	<td class="line x" title="19:195	Human readers often use semantics, contextual information about the document and world knowledge to resolve segmentation ambiguities." ></td>
	<td class="line x" title="20:195	There is no fixed standard for Chinese word segmentation." ></td>
	<td class="line x" title="21:195	Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. , 1996)." ></td>
	<td class="line x" title="22:195	Also, specific NLP tasks may require different segmentation criteria." ></td>
	<td class="line x" title="23:195	For example, ANESGU C4 could be treated as a single word (Bank of Beijing) for machine translation, while it is more naturally segmented into ANES (Beijing) GUC4 (bank) for tasks such as text-to-speech synthesis." ></td>
	<td class="line x" title="24:195	Therefore, supervised learning with specifically defined training data has become the dominant approach." ></td>
	<td class="line x" title="25:195	Following Xue (2003), the standard approach to 840 supervised learning for CWS is to treat it as a tagging task." ></td>
	<td class="line x" title="26:195	Tags are assigned to each character in the sentence, indicating whether the character is a singlecharacter word or the start, middle or end of a multicharacter word." ></td>
	<td class="line x" title="27:195	The features are usually confined to a five-character window with the current character in the middle." ></td>
	<td class="line x" title="28:195	In this way, dynamic programming algorithms such as the Viterbi algorithm can be used for decoding." ></td>
	<td class="line x" title="29:195	Several discriminatively trained models have recently been applied to the CWS problem." ></td>
	<td class="line x" title="30:195	Examples include Xue (2003), Peng et al.(2004) and Shi and Wang (2007); these use maximum entropy (ME) and conditional random field (CRF) models (Ratnaparkhi, 1998; Lafferty et al. , 2001)." ></td>
	<td class="line x" title="32:195	An advantage of these models is their flexibility in allowing knowledge from various sources to be encoded as features." ></td>
	<td class="line x" title="33:195	Contextual information plays an important role in word segmentation decisions; especially useful is information about surrounding words." ></td>
	<td class="line x" title="34:195	Consider the sentence B9H1AMA1AQ, which can be from DAB9 (among which) H1AM (foreign) A1AQ (companies), or B9H1 (in China) AMA1 (foreign companies) AQ EH (business)." ></td>
	<td class="line x" title="35:195	Note that the five-character window surrounding AM is the same in both cases, making the tagging decision for that character difficult given the local window." ></td>
	<td class="line x" title="36:195	However, the correct decision can be made by comparison of the two three-word windows containing this character." ></td>
	<td class="line x" title="37:195	In order to explore the potential of word-based models, we adapt the perceptron discriminative learning algorithm to the CWS problem." ></td>
	<td class="line x" title="38:195	Collins (2002) proposed the perceptron as an alternative to the CRF method for HMM-style taggers." ></td>
	<td class="line x" title="39:195	However, our model does not map the segmentation problem to a tag sequence learning problem, but defines features on segmented sentences directly." ></td>
	<td class="line oc" title="40:195	Hence we use a beam-search decoder during training and testing; our idea is similar to that of Collins and Roark (2004) who used a beam-search decoder as part of a perceptron parsing model." ></td>
	<td class="line x" title="41:195	Our work can also be seen as part of the recent move towards search-based learning methods which do not rely on dynamic programming and are thus able to exploit larger parts of the context for making decisions (Daume III, 2006)." ></td>
	<td class="line x" title="42:195	We study several factors that influence the performance of the perceptron word segmentor, including the averaged perceptron method, the size of the beam and the importance of word-based features." ></td>
	<td class="line x" title="43:195	We compare the accuracy of our final system to the state-of-the-art CWS systems in the literature using the first and second SIGHAN bakeoff data." ></td>
	<td class="line x" title="44:195	Our system is competitive with the best systems, obtaining the highest reported F-scores on a number of the bakeoff corpora." ></td>
	<td class="line x" title="45:195	These results demonstrate the importance of word-based features for CWS." ></td>
	<td class="line x" title="46:195	Furthermore, our approach provides an example of the potential of search-based discriminative training methods for NLP tasks." ></td>
	<td class="line x" title="47:195	2 The Perceptron Training Algorithm We formulate the CWS problem as finding a mapping from an input sentence x  X to an output sentence y  Y, where X is the set of possible raw sentences and Y is the set of possible segmented sentences." ></td>
	<td class="line x" title="48:195	Given an input sentence x, the correct output segmentation F(x) satisfies: F(x) = argmax yGEN(x) Score(y) where GEN(x) denotes the set of possible segmentations for an input sentence x, consistent with notation from Collins (2002)." ></td>
	<td class="line x" title="49:195	The score for a segmented sentence is computed by first mapping it into a set of features." ></td>
	<td class="line x" title="50:195	A feature is an indicator of the occurrence of a certain pattern in a segmented sentence." ></td>
	<td class="line x" title="51:195	For example, it can be the occurrence of FOCQ as a single word, or the occurrence of FO separated from CQ in two adjacent words." ></td>
	<td class="line x" title="52:195	By defining features, a segmented sentence is mapped into a global feature vector, in which each dimension represents the count of a particular feature in the sentence." ></td>
	<td class="line x" title="53:195	The term global feature vector is used by Collins (2002) to distinguish between feature count vectors for whole sequences and the local feature vectors in ME tagging models, which are Boolean valued vectors containing the indicator features for one element in the sequence." ></td>
	<td class="line x" title="54:195	Denote the global feature vector for segmented sentence y with (y)  Rd, where d is the total number of features in the model; then Score(y) is computed by the dot product of vector (y) and a parameter vector   Rd, where i is the weight for the ith feature: Score(y) = (y) 841 Inputs: training examples (xi,yi) Initialization: set  = 0 Algorithm: for t = 1T, i = 1N calculate zi = argmaxyGEN(xi) (y) if zi negationslash= yi  =  + (yi)(zi) Outputs:  Figure 1: the perceptron learning algorithm, adapted from Collins (2002) The perceptron training algorithm is used to determine the weight values ." ></td>
	<td class="line x" title="55:195	The training algorithm initializes the parameter vector as all zeros, and updates the vector by decoding the training examples." ></td>
	<td class="line x" title="56:195	Each training sentence is turned into the raw input form, and then decoded with the current parameter vector." ></td>
	<td class="line x" title="57:195	The output segmented sentence is compared with the original training example." ></td>
	<td class="line x" title="58:195	If the output is incorrect, the parameter vector is updated by adding the global feature vector of the training example and subtracting the global feature vector of the decoder output." ></td>
	<td class="line x" title="59:195	The algorithm can perform multiple passes over the same training sentences." ></td>
	<td class="line x" title="60:195	Figure 1 gives the algorithm, where N is the number of training sentences and T is the number of passes over the data." ></td>
	<td class="line x" title="61:195	Note that the algorithm from Collins (2002) was designed for discriminatively training an HMM-style tagger." ></td>
	<td class="line x" title="62:195	Features are extracted from an input sequence x and its corresponding tag sequence y: Score(x,y) = (x,y) Our algorithm is not based on an HMM." ></td>
	<td class="line x" title="63:195	For a given input sequence x, even the length of different candidates y (the number of words) is not fixed." ></td>
	<td class="line x" title="64:195	Because the output sequence y (the segmented sentence) contains all the information from the input sequence x (the raw sentence), the global feature vector (x,y) is replaced with (y), which is extracted from the candidate segmented sentences directly." ></td>
	<td class="line x" title="65:195	Despite the above differences, since the theorems of convergence and their proof (Collins, 2002) are only dependent on the feature vectors, and not on the source of the feature definitions, the perceptron algorithm is applicable to the training of our CWS model." ></td>
	<td class="line x" title="66:195	2.1 The averaged perceptron The averaged perceptron algorithm (Collins, 2002) was proposed as a way of reducing overfitting on the training data." ></td>
	<td class="line x" title="67:195	It was motivated by the votedperceptron algorithm (Freund and Schapire, 1999) and has been shown to give improved accuracy over the non-averaged perceptron on a number of tasks." ></td>
	<td class="line x" title="68:195	Let N be the number of training sentences, T the number of training iterations, and n,t the parameter vector immediately after the nth sentence in the tth iteration." ></td>
	<td class="line x" title="69:195	The averaged parameter vector   Rd is defined as:  = 1NT summationdisplay n=1N,t=1T n,t To compute the averaged parameters , the training algorithm in Figure 1 can be modified by keeping a total parameter vector n,t =summationtextn,t, which is updated using  after each training example." ></td>
	<td class="line x" title="70:195	After the final iteration,  is computed as n,t/NT." ></td>
	<td class="line x" title="71:195	In the averaged perceptron algorithm,  is used instead of  as the final parameter vector." ></td>
	<td class="line x" title="72:195	With a large number of features, calculating the total parameter vector n,t after each training example is expensive." ></td>
	<td class="line x" title="73:195	Since the number of changed dimensions in the parameter vector  after each training example is a small proportion of the total vector, we use a lazy update optimization for the training process.1 Define an update vector  to record the number of the training sentence n and iteration t when each dimension of the averaged parameter vector was last updated." ></td>
	<td class="line x" title="74:195	Then after each training sentence is processed, only update the dimensions of the total parameter vector corresponding to the features in the sentence." ></td>
	<td class="line x" title="75:195	(Except for the last example in the last iteration, when each dimension of  is updated, no matter whether the decoder output is correct or not)." ></td>
	<td class="line x" title="76:195	Denote the sth dimension in each vector before processing the nth example in the tth iteration as n1,ts, n1,ts and n1,ts = (n,s,t,s)." ></td>
	<td class="line x" title="77:195	Suppose that the decoder output zn,t is different from the training example yn." ></td>
	<td class="line x" title="78:195	Now n,ts, n,ts and n,ts can 1Daume III (2006) describes a similar algorithm." ></td>
	<td class="line x" title="79:195	842 be updated in the following way: n,ts = n1,ts + n1,ts (tN+nt,sNn,s) n,ts = n1,ts + (yn)(zn,t) n,ts = n,ts + (yn)(zn,t) n,ts = (n,t) We found that this lazy update method was significantly faster than the naive method." ></td>
	<td class="line x" title="80:195	3 The Beam-Search Decoder The decoder reads characters from the input sentence one at a time, and generates candidate segmentations incrementally." ></td>
	<td class="line x" title="81:195	At each stage, the next incoming character is combined with an existing candidate in two different ways to generate new candidates: it is either appended to the last word in the candidate, or taken as the start of a new word." ></td>
	<td class="line x" title="82:195	This method guarantees exhaustive generation of possible segmentations for any input sentence." ></td>
	<td class="line x" title="83:195	Two agendas are used: the source agenda and the target agenda." ></td>
	<td class="line x" title="84:195	Initially the source agenda contains an empty sentence and the target agenda is empty." ></td>
	<td class="line x" title="85:195	At each processing stage, the decoder reads in a character from the input sentence, combines it with each candidate in the source agenda and puts the generated candidates onto the target agenda." ></td>
	<td class="line x" title="86:195	After each character is processed, the items in the target agenda are copied to the source agenda, and then the target agenda is cleaned, so that the newly generated candidates can be combined with the next incoming character to generate new candidates." ></td>
	<td class="line x" title="87:195	After the last character is processed, the decoder returns the candidate with the best score in the source agenda." ></td>
	<td class="line x" title="88:195	Figure 2 gives the decoding algorithm." ></td>
	<td class="line x" title="89:195	For a sentence with length l, there are 2l1 different possible segmentations." ></td>
	<td class="line x" title="90:195	To guarantee reasonable running speed, the size of the target agenda is limited, keeping only the B best candidates." ></td>
	<td class="line x" title="91:195	4 Feature templates The feature templates are shown in Table 1." ></td>
	<td class="line x" title="92:195	Features 1 and 2 contain only word information, 3 to 5 contain character and length information, 6 and 7 contain only character information, 8 to 12 contain word and character information, while 13 and 14 contain Input: raw sentence sent  a list of characters Initialization: set agendas src = [[]], tgt = [] Variables: candidate sentence item  a list of words Algorithm: for index = 0sent.length1: var char = sent[index] foreach item in src: // append as a new word to the candidate var item1 = item item1.append(char.toWord()) tgt.insert(item1) // append the character to the last word if item.length > 1: var item2 = item item2[item2.length1].append(char) tgt.insert(item2) src = tgt tgt = [] Outputs: src.best item Figure 2: The decoding algorithm word and length information." ></td>
	<td class="line x" title="93:195	Any segmented sentence is mapped to a global feature vector according to these templates." ></td>
	<td class="line x" title="94:195	There are 356,337 features with non-zero values after 6 training iterations using the development data." ></td>
	<td class="line x" title="95:195	For this particular feature set, the longest range features are word bigrams." ></td>
	<td class="line x" title="96:195	Therefore, among partial candidates ending with the same bigram, the best one will also be in the best final candidate." ></td>
	<td class="line x" title="97:195	The decoder can be optimized accordingly: when an incoming character is combined with candidate items as a new word, only the best candidate is kept among those having the same last word." ></td>
	<td class="line x" title="98:195	5 Comparison with Previous Work Among the character-tagging CWS models, Li et al.(2005) uses an uneven margin alteration of the traditional perceptron classifier (Li et al. , 2002)." ></td>
	<td class="line x" title="100:195	Each character is classified independently, using information in the neighboring five-character window." ></td>
	<td class="line x" title="101:195	Liang (2005) uses the discriminative perceptron algorithm (Collins, 2002) to score whole character tag sequences, finding the best candidate by the global score." ></td>
	<td class="line x" title="102:195	It can be seen as an alternative to the ME and CRF models (Xue, 2003; Peng et al. , 2004), which 843 1 word w 2 word bigram w1w2 3 single-character word w 4 a word starting with character c and having length l 5 a word ending with character c and having length l 6 space-separated characters c1 and c2 7 character bigram c1c2 in any word 8 the first and last characters c1 and c2 of any word 9 word w immediately before character c 10 character c immediately before word w 11 the starting characters c1 and c2 of two consecutive words 12 the ending characters c1 and c2 of two consecutive words 13 a word of length l and the previous word w 14 a word of length l and the next word w Table 1: feature templates do not involve word information." ></td>
	<td class="line x" title="103:195	Wang et al.(2006) incorporates an N-gram language model in ME tagging, making use of word information to improve the character tagging model." ></td>
	<td class="line x" title="105:195	The key difference between our model and the above models is the wordbased nature of our system." ></td>
	<td class="line x" title="106:195	One existing method that is based on sub-word information, Zhang et al.(2006), combines a CRF and a rule-based model." ></td>
	<td class="line x" title="108:195	Unlike the character-tagging models, the CRF submodel assigns tags to subwords, which include single-character words and the most frequent multiple-character words from the training corpus." ></td>
	<td class="line x" title="109:195	Thus it can be seen as a step towards a word-based model." ></td>
	<td class="line x" title="110:195	However, sub-words do not necessarily contain full word information." ></td>
	<td class="line x" title="111:195	Moreover, sub-word extraction is performed separately from feature extraction." ></td>
	<td class="line x" title="112:195	Another difference from our model is the rule-based submodel, which uses a dictionary-based forward maximum match method described by Sproat et al.(1996)." ></td>
	<td class="line x" title="114:195	6 Experiments Two sets of experiments were conducted." ></td>
	<td class="line x" title="115:195	The first, used for development, was based on the part of Chinese Treebank 4 that is not in Chinese Treebank 3 (since CTB3 was used as part of the first bakeoff)." ></td>
	<td class="line x" title="116:195	This corpus contains 240K characters (150K words and 4798 sentences)." ></td>
	<td class="line x" title="117:195	80% of the sentences (3813) were randomly chosen for training and the rest (985 sentences) were used as development testing data." ></td>
	<td class="line x" title="118:195	The accuracies and learning curves for the non-averaged and averaged perceptron were compared." ></td>
	<td class="line x" title="119:195	The influence of particular features and the agenda size were also studied." ></td>
	<td class="line x" title="120:195	The second set of experiments used training and testing sets from the first and second international Chinese word segmentation bakeoffs (Sproat and Emerson, 2003; Emerson, 2005)." ></td>
	<td class="line x" title="121:195	The accuracies are compared to other models in the literature." ></td>
	<td class="line x" title="122:195	F-measure is used as the accuracy measure." ></td>
	<td class="line x" title="123:195	Define precision p as the percentage of words in the decoder output that are segmented correctly, and recall r as the percentage of gold standard output words that are correctly segmented by the decoder." ></td>
	<td class="line x" title="124:195	The (balanced) F-measure is 2pr/(p + r)." ></td>
	<td class="line x" title="125:195	CWS systems are evaluated by two types of tests." ></td>
	<td class="line x" title="126:195	The closed tests require that the system is trained only with a designated training corpus." ></td>
	<td class="line x" title="127:195	Any extra knowledge is not allowed, including common surnames, Chinese and Arabic numbers, European letters, lexicons, part-of-speech, semantics and so on." ></td>
	<td class="line x" title="128:195	The open tests do not impose such restrictions." ></td>
	<td class="line x" title="129:195	Open tests measure a models capability to utilize extra information and domain knowledge, which can lead to improved performance, but since this extra information is not standardized, direct comparison between open test results is less informative." ></td>
	<td class="line x" title="130:195	In this paper, we focus only on the closed test." ></td>
	<td class="line x" title="131:195	However, the perceptron model allows a wide range of features, and so future work will consider how to integrate open resources into our system." ></td>
	<td class="line x" title="132:195	6.1 Learning curve In this experiment, the agenda size was set to 16, for both training and testing." ></td>
	<td class="line x" title="133:195	Table 2 shows the precision, recall and F-measure for the development set after 1 to 10 training iterations, as well as the number of mistakes made in each iteration." ></td>
	<td class="line x" title="134:195	The corresponding learning curves for both the non-averaged and averaged perceptron are given in Figure 3." ></td>
	<td class="line x" title="135:195	The table shows that the number of mistakes made in each iteration decreases, reflecting the convergence of the learning algorithm." ></td>
	<td class="line x" title="136:195	The averaged per844 Iteration 1 2 3 4 5 6 7 8 9 10 P (non-avg) 89.0 91.6 92.0 92.3 92.5 92.5 92.5 92.7 92.6 92.6 R (non-avg) 88.3 91.4 92.2 92.6 92.7 92.8 93.0 93.0 93.1 93.2 F (non-avg) 88.6 91.5 92.1 92.5 92.6 92.6 92.7 92.8 92.8 92.9 P (avg) 91.7 92.8 93.1 92.2 93.1 93.2 93.2 93.2 93.2 93.2 R (avg) 91.6 92.9 93.3 93.4 93.4 93.5 93.5 93.5 93.6 93.6 F (avg) 91.6 92.9 93.2 93.3 93.3 93.4 93.3 93.3 93.4 93.4 #Wrong sentences 3401 1652 945 621 463 288 217 176 151 139 Table 2: accuracy using non-averaged and averaged perceptron." ></td>
	<td class="line x" title="137:195	P precision (%), R recall (%), F F-measure." ></td>
	<td class="line x" title="138:195	B 2 4 8 16 32 64 128 256 512 1024 Tr 660 610 683 830 1111 1645 2545 4922 9104 15598 Seg 18.65 18.18 28.85 26.52 36.58 56.45 95.45 173.38 325.99 559.87 F 86.90 92.95 93.33 93.38 93.25 93.29 93.19 93.07 93.24 93.34 Table 3: the influence of agenda size." ></td>
	<td class="line x" title="139:195	B agenda size, Tr training time (seconds), Seg testing time (seconds), F F-measure." ></td>
	<td class="line x" title="140:195	0.86 0.87 0.88 0.89 0.9 0.91 0.92 0.93 0.94 1 2 3 4 5 6 7 8 9 10 number of training iterations Fme as ur e non-averaged averaged Figure 3: learning curves of the averaged and nonaveraged perceptron algorithms ceptron algorithm improves the segmentation accuracy at each iteration, compared with the nonaveraged perceptron." ></td>
	<td class="line x" title="141:195	The learning curve was used to fix the number of training iterations at 6 for the remaining experiments." ></td>
	<td class="line x" title="142:195	6.2 The influence of agenda size Reducing the agenda size increases the decoding speed, but it could cause loss of accuracy by eliminating potentially good candidates." ></td>
	<td class="line x" title="143:195	The agenda size also affects the training time, and resulting model, since the perceptron training algorithm uses the decoder output to adjust the model parameters." ></td>
	<td class="line x" title="144:195	Table 3 shows the accuracies with ten different agenda sizes, each used for both training and testing." ></td>
	<td class="line x" title="145:195	Accuracy does not increase beyond B = 16." ></td>
	<td class="line x" title="146:195	Moreover, the accuracy is quite competitive even with B as low as 4." ></td>
	<td class="line x" title="147:195	This reflects the fact that the best segmentation is often within the current top few candidates in the agenda.2 Since the training and testing time generally increases as N increases, the agenda size is fixed to 16 for the remaining experiments." ></td>
	<td class="line x" title="148:195	6.3 The influence of particular features Our CWS model is highly dependent upon word information." ></td>
	<td class="line x" title="149:195	Most of the features in Table 1 are related to words." ></td>
	<td class="line x" title="150:195	Table 4 shows the accuracy with various features from the model removed." ></td>
	<td class="line x" title="151:195	Among the features, vocabulary words (feature 1) and length prediction by characters (features 3 to 5) showed strong influence on the accuracy, while word bigrams (feature 2) and special characters in them (features 11 and 12) showed comparatively weak influence." ></td>
	<td class="line x" title="152:195	2The optimization in Section 4, which has a pruning effect, was applied to this experiment." ></td>
	<td class="line x" title="153:195	Similar observations were made in separate experiments without such optimization." ></td>
	<td class="line x" title="154:195	845 Features F Features F All 93.38 w/o 1 92.88 w/o 2 93.36 w/o 3, 4, 5 92.72 w/o 6 93.13 w/o 7 93.13 w/o 8 93.14 w/o 9, 10 93.31 w/o 11, 12 93.38 w/o 13, 14 93.23 Table 4: the influence of features." ></td>
	<td class="line x" title="155:195	(F: F-measure." ></td>
	<td class="line x" title="156:195	Feature numbers are from Table 1) 6.4 Closed test on the SIGHAN bakeoffs Four training and testing corpora were used in the first bakeoff (Sproat and Emerson, 2003), including the Academia Sinica Corpus (AS), the Penn Chinese Treebank Corpus (CTB), the Hong Kong City University Corpus (CU) and the Peking University Corpus (PU)." ></td>
	<td class="line x" title="157:195	However, because the testing data from the Penn Chinese Treebank Corpus is currently unavailable, we excluded this corpus." ></td>
	<td class="line x" title="158:195	The corpora are encoded in GB (PU, CTB) and BIG5 (AS, CU)." ></td>
	<td class="line x" title="159:195	In order to test them consistently in our system, they are all converted to UTF8 without loss of information." ></td>
	<td class="line x" title="160:195	The results are shown in Table 5." ></td>
	<td class="line x" title="161:195	We follow the format from Peng et al.(2004)." ></td>
	<td class="line x" title="163:195	Each row represents a CWS model." ></td>
	<td class="line x" title="164:195	The first eight rows represent models from Sproat and Emerson (2003) that participated in at least one closed test from the table, row Peng represents the CRF model from Peng et al.(2004), and the last row represents our model." ></td>
	<td class="line x" title="166:195	The first three columns represent tests with the AS, CU and PU corpora, respectively." ></td>
	<td class="line x" title="167:195	The best score in each column is shown in bold." ></td>
	<td class="line x" title="168:195	The last two columns represent the average accuracy of each model over the tests it participated in (SAV), and our average over the same tests (OAV), respectively." ></td>
	<td class="line x" title="169:195	For each row the best average is shown in bold." ></td>
	<td class="line x" title="170:195	We achieved the best accuracy in two of the three corpora, and better overall accuracy than the majority of the other models." ></td>
	<td class="line x" title="171:195	The average score of S10 is 0.7% higher than our model, but S10 only participated in the HK test." ></td>
	<td class="line x" title="172:195	Four training and testing corpora were used in the second bakeoff (Emerson, 2005), including the Academia Sinica corpus (AS), the Hong Kong City University Corpus (CU), the Peking University Corpus (PK) and the Microsoft Research Corpus (MR)." ></td>
	<td class="line x" title="173:195	AS CU PU SAV OAV S01 93.8 90.1 95.1 93.0 95.0 S04 93.9 93.9 94.0 S05 94.2 89.4 91.8 95.3 S06 94.5 92.4 92.4 93.1 95.0 S08 90.4 93.6 92.0 94.3 S09 96.1 94.6 95.4 95.3 S10 94.7 94.7 94.0 S12 95.9 91.6 93.8 95.6 Peng 95.6 92.8 94.1 94.2 95.0 96.5 94.6 94.0 Table 5: the accuracies over the first SIGHAN bakeoff data." ></td>
	<td class="line x" title="174:195	AS CU PK MR SAV OAV S14 94.7 94.3 95.0 96.4 95.1 95.4 S15b 95.2 94.1 94.1 95.8 94.8 95.4 S27 94.5 94.0 95.0 96.0 94.9 95.4 Zh-a 94.7 94.6 94.5 96.4 95.1 95.4 Zh-b 95.1 95.1 95.1 97.1 95.6 95.4 94.6 95.1 94.5 97.2 Table 6: the accuracies over the second SIGHAN bakeoff data." ></td>
	<td class="line x" title="175:195	Different encodings were provided, and the UTF8 data for all four corpora were used in this experiment." ></td>
	<td class="line x" title="176:195	Following the format of Table 5, the results for this bakeoff are shown in Table 6." ></td>
	<td class="line x" title="177:195	We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.(2006) for comparison." ></td>
	<td class="line x" title="179:195	Row Zh-a and Zh-b represent the pure sub-word CRF model and the confidence-based combination of the CRF and rule-based models, respectively." ></td>
	<td class="line x" title="180:195	Again, our model achieved better overall accuracy than the majority of the other models." ></td>
	<td class="line x" title="181:195	One system to achieve comparable accuracy with our system is Zh-b, which improves upon the sub-word CRF model (Zh-a) by combining it with an independent dictionary-based submodel and improving the accuracy of known words." ></td>
	<td class="line x" title="182:195	In comparison, our system is based on a single perceptron model." ></td>
	<td class="line x" title="183:195	In summary, closed tests for both the first and the second bakeoff showed competitive results for our 846 system compared with the best results in the literature." ></td>
	<td class="line x" title="184:195	Our word-based system achieved the best Fmeasures over the AS (96.5%) and CU (94.6%) corpora in the first bakeoff, and the CU (95.1%) and MR (97.2%) corpora in the second bakeoff." ></td>
	<td class="line x" title="185:195	7 Conclusions and Future Work We proposed a word-based CWS model using the discriminative perceptron learning algorithm." ></td>
	<td class="line x" title="186:195	This model is an alternative to the existing characterbased tagging models, and allows word information to be used as features." ></td>
	<td class="line x" title="187:195	One attractive feature of the perceptron training algorithm is its simplicity, consisting of only a decoder and a trivial update process." ></td>
	<td class="line x" title="188:195	We use a beam-search decoder, which places our work in the context of recent proposals for searchbased discriminative learning algorithms." ></td>
	<td class="line x" title="189:195	Closed tests using the first and second SIGHAN CWS bakeoff data demonstrated our system to be competitive with the best in the literature." ></td>
	<td class="line x" title="190:195	Open features, such as knowledge of numbers and European letters, and relationships from semantic networks (Shi and Wang, 2007), have been reported to improve accuracy." ></td>
	<td class="line x" title="191:195	Therefore, given the flexibility of the feature-based perceptron model, an obvious next step is the study of open features in the segmentor." ></td>
	<td class="line x" title="192:195	Also, we wish to explore the possibility of incorporating POS tagging and parsing features into the discriminative model, leading to joint decoding." ></td>
	<td class="line x" title="193:195	The advantage is two-fold: higher level syntactic information can be used in word segmentation, while joint decoding helps to prevent bottomup error propagation among the different processing steps." ></td>
	<td class="line x" title="194:195	Acknowledgements This work is supported by the ORS and Clarendon Fund." ></td>
	<td class="line x" title="195:195	We thank the anonymous reviewers for their insightful comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-1202
Perceptron Training for a Wide-Coverage Lexicalized-Grammar Parser
Clark, Stephen;Curran, James R.;"></td>
	<td class="line x" title="1:197	Proceedings of the 5th Workshop on Important Unresolved Matters, pages 9??6, Ann Arbor, June 2005." ></td>
	<td class="line x" title="2:197	c2005 Association for Computational Linguistics Perceptron Training for a Wide-Coverage Lexicalized-Grammar Parser Stephen Clark Oxford University Computing Laboratory Wolfson Building, Parks Road Oxford, OX1 3QD, UK stephen.clark@comlab.ox.ac.uk James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Abstract This paper investigates perceptron training for a wide-coverage CCG parser and compares the perceptron with a log-linear model." ></td>
	<td class="line x" title="3:197	The CCG parser uses a phrase-structure parsing model and dynamic programming in the form of the Viterbi algorithm to find the highest scoring derivation." ></td>
	<td class="line x" title="4:197	The difficulty in using the perceptron for a phrase-structure parsing model is the need for an efficient decoder." ></td>
	<td class="line x" title="5:197	We exploit the lexicalized nature of CCG by using a finite-state supertagger to do much of the parsing work, resulting in a highly efficient decoder." ></td>
	<td class="line x" title="6:197	The perceptron performs as well as the log-linear model; it trains in a few hours on a single machine; and it requires only a few hundred MB of RAM for practical training compared to 20 GB for the log-linear model." ></td>
	<td class="line x" title="7:197	We also investigate the order in which the training examples are presented to the online perceptron learner, and find that order does not significantly affect the results." ></td>
	<td class="line oc" title="8:197	1 Introduction A recent development in data-driven parsing is the use of discriminative training methods (Riezler et al. , 2002; Taskar et al. , 2004; Collins and Roark, 2004; Turian and Melamed, 2006)." ></td>
	<td class="line x" title="9:197	One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function (Johnson et al. , 1999; Riezler et al. , 2002; Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005)." ></td>
	<td class="line x" title="10:197	Maximising the likelihood involves calculating feature expectations, which is computationally expensive." ></td>
	<td class="line x" title="11:197	Dynamic programming (DP) in the form of the inside-outside algorithm can be used to calculate the expectations, if the features are sufficiently local (Miyao and Tsujii, 2002); however, the memory requirements can be prohibitive, especially for automatically extracted, wide-coverage grammars." ></td>
	<td class="line x" title="12:197	In Clark and Curran (2004b) we use cluster computing resources to solve this problem." ></td>
	<td class="line oc" title="13:197	Parsing research has also begun to adopt discriminative methods from the Machine Learning literature, such as the perceptron (Freund and Schapire, 1999; Collins and Roark, 2004) and the largemargin methods underlying Support Vector Machines (Taskar et al. , 2004; McDonald, 2006)." ></td>
	<td class="line x" title="14:197	Parser training involves decoding in an iterative process, updating the model parameters so that the decoder performs better on the training data, according to some training criterion." ></td>
	<td class="line x" title="15:197	Hence, for efficient training, these methods require an efficient decoder; in fact, for methods like the perceptron, the update procedure is so trivial that the training algorithm essentially is decoding." ></td>
	<td class="line x" title="16:197	This paper describes a decoder for a lexicalizedgrammar parser which is efficient enough for practical discriminative training." ></td>
	<td class="line x" title="17:197	We use a lexicalized phrase-structure parser, the CCG parser of Clark and Curran (2004b), together with a DP-based decoder." ></td>
	<td class="line x" title="18:197	The key idea is to exploit the properties of lexicalized grammars by using a finite-state supertagger prior to parsing (Bangalore and Joshi, 1999; Clark and Curran, 2004a)." ></td>
	<td class="line x" title="19:197	The decoder still uses the CKY algorithm, so the worst case complexity of 9 the parsing is unchanged; however, by allowing the supertagger to do much of the parsing work, the efficiency of the decoder is greatly increased in practice." ></td>
	<td class="line x" title="20:197	We chose the perceptron for the training algorithm because it has shown good performance on other NLP tasks; in particular, Collins (2002) reported good performance for a perceptron tagger compared to a Maximum Entropy tagger." ></td>
	<td class="line x" title="21:197	Like Collins (2002), the decoder is the same for both the perceptron and the log-linear parsing models; the only change is the method for setting the weights." ></td>
	<td class="line x" title="22:197	The perceptron model performs as well as the loglinear model, but is considerably easier to train." ></td>
	<td class="line x" title="23:197	Another contribution of this paper is to advance wide-coverage CCG parsing." ></td>
	<td class="line x" title="24:197	Previous discriminative models for CCG (Clark and Curran, 2004b) required cluster computing resources to train." ></td>
	<td class="line x" title="25:197	In this paper we reduce the memory requirements from 20 GB of RAM to only a few hundred MB, but without greatly increasing the training time or reducing parsing accuracy." ></td>
	<td class="line x" title="26:197	This provides state-of-the-art CCG parsing with a practical development environment." ></td>
	<td class="line x" title="27:197	More generally, this work provides a practical environment for experimenting with discriminative models for phrase-structure parsing; because the training time for the CCG parser is relatively short (a few hours), experiments such as comparing alternative feature sets can be performed." ></td>
	<td class="line x" title="28:197	As an example, we investigate the order in which the training examples are presented to the perceptron learner." ></td>
	<td class="line x" title="29:197	Since the perceptron training is an online algorithm ??updating the weights one training sentence at a time ??the order in which the data is processed affects the resulting model." ></td>
	<td class="line x" title="30:197	We consider random ordering; presenting the shortest sentences first; and presenting the longest sentences first; and find that the order does not significantly affect the final results." ></td>
	<td class="line x" title="31:197	We also use the random orderings to investigate model averaging." ></td>
	<td class="line x" title="32:197	We produced 10 different models, by randomly permuting the data, and averaged the weights." ></td>
	<td class="line x" title="33:197	Again the averaging was found to have no impact on the results, showing that the perceptron learner ??at least for this parsing task ??is robust to the order of the training examples." ></td>
	<td class="line x" title="34:197	The contributions of this paper are as follows." ></td>
	<td class="line x" title="35:197	First, we compare perceptron and log-linear parsing models for a wide-coverage phrase-structure parser, the first work we are aware of to do so." ></td>
	<td class="line x" title="36:197	Second, we provide a practical framework for developing discriminative models for CCG, reducing the memory requirements from over 20 GB to a few hundred MB." ></td>
	<td class="line x" title="37:197	And third, given the significantly shorter training time compared to other discriminative parsing models (Taskar et al. , 2004), we provide a practical framework for investigating discriminative training methods more generally." ></td>
	<td class="line x" title="38:197	2 The CCG Parser Clark and Curran (2004b) describes the CCG parser." ></td>
	<td class="line x" title="39:197	The grammar used by the parser is extracted from CCGbank, a CCG version of the Penn Treebank (Hockenmaier, 2003)." ></td>
	<td class="line x" title="40:197	The grammar consists of 425 lexical categories, expressing subcategorisation information, plus a small number of combinatory rules which combine the categories (Steedman, 2000)." ></td>
	<td class="line x" title="41:197	A Maximum Entropy supertagger first assigns lexical categories to the words in a sentence, which are then combined by the parser using the combinatory rules and the CKY algorithm." ></td>
	<td class="line x" title="42:197	A log-linear model scores the alternative parses." ></td>
	<td class="line x" title="43:197	We use the normalform model, which assigns probabilities to single derivations based on the normal-form derivations in CCGbank." ></td>
	<td class="line x" title="44:197	The features in the model are defined over local parts of the derivation and include wordword dependencies." ></td>
	<td class="line x" title="45:197	A packed chart representation allows efficient decoding, with the Viterbi algorithm finding the most probable derivation." ></td>
	<td class="line x" title="46:197	The supertagger is a key part of the system." ></td>
	<td class="line x" title="47:197	It uses a log-linear model to define a distribution over the lexical category set for each word and the previous two categories (Ratnaparkhi, 1996) and the forward backward algorithm efficiently sums over all histories to give a distibution for each word." ></td>
	<td class="line x" title="48:197	These distributions are then used to assign a set of lexical categories to each word (Curran et al. , 2006)." ></td>
	<td class="line x" title="49:197	Supertagging was first defined for LTAG (Bangalore and Joshi, 1999), and was designed to increase parsing speed for lexicalized grammars by allowing a finite-state tagger to do some of the parsing work." ></td>
	<td class="line x" title="50:197	Since the elementary syntactic units in a lexicalized grammar ??in LTAG?s case elementary trees and in CCG?s case lexical categories ??contain a significant amount of grammatical information, combining them together is easier than the parsing typically performed by phrase-structure parsers." ></td>
	<td class="line x" title="51:197	Hence 10 Bangalore and Joshi (1999) refer to supertagging as almost parsing." ></td>
	<td class="line x" title="52:197	Supertagging has been especially successful for CCG: Clark and Curran (2004a) demonstrates the considerable increases in speed that can be obtained through use of a supertagger." ></td>
	<td class="line x" title="53:197	The supertagger interacts with the parser in an adaptive fashion." ></td>
	<td class="line x" title="54:197	Initially the supertagger assigns a small number of categories, on average, to each word in the sentence, and the parser attempts to create a spanning analysis." ></td>
	<td class="line x" title="55:197	If this is not possible, the supertagger assigns more categories, and this process continues until a spanning analysis is found." ></td>
	<td class="line x" title="56:197	The number of categories assigned to each word is determined by a parameter  in the supertagger: all categories are assigned whose forward-backward probabilities are within  of the highest probability category (Curran et al. , 2006)." ></td>
	<td class="line x" title="57:197	Clark and Curran (2004a) also shows how the supertagger can reduce the size of the packed charts to allow discriminative log-linear training." ></td>
	<td class="line x" title="58:197	However, even with the use of a supertagger, the packed charts for the complete CCGbank require over 20 GB of RAM." ></td>
	<td class="line x" title="59:197	Reading the training instances into memory one at a time and keeping a record of the relevant feature counts would be too slow for practical development, since the log-linear model requires hundreds of iterations to converge." ></td>
	<td class="line x" title="60:197	Hence the packed charts need to be stored in memory." ></td>
	<td class="line x" title="61:197	In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of the BFGS training algorithm, to solve this problem." ></td>
	<td class="line x" title="62:197	The need for cluster computing resources presents a barrier to the development of further CCG parsing models." ></td>
	<td class="line x" title="63:197	Hockenmaier and Steedman (2002) describe a generative model for CCG, which only requires a non-iterative counting process for training, but it is generally acknowledged that discriminative models provide greater flexibility and typically higher performance." ></td>
	<td class="line x" title="64:197	In this paper we propose the perceptron algorithm as a solution." ></td>
	<td class="line x" title="65:197	The perceptron is an online learning algorithm, and so the parameters are updated one training instance at a time." ></td>
	<td class="line x" title="66:197	However, the key difference compared with the loglinear training is that the perceptron converges in many fewer iterations, and so it is practical to read the training instances into memory one at a time." ></td>
	<td class="line x" title="67:197	The difficulty in using the perceptron for training phrase-structure parsing models is the need for an efficient decoder (since perceptron training essentially is decoding)." ></td>
	<td class="line x" title="68:197	Here we exploit the lexicalized nature of CCG by using the supertagger to restrict the size of the charts over which Viterbi decoding is performed, resulting in an extremely effcient decoder." ></td>
	<td class="line x" title="69:197	In fact, the decoding is so fast that we can estimate a state-of-the-art discriminative parsing model in only a few hours on a single machine." ></td>
	<td class="line x" title="70:197	3 Perceptron Training The parsing problem is to find a mapping from a set of sentences x ??X to a set of parses y ??Y. We assume that the mapping F is represented through a feature vector (x,y) ??Rd and a parameter vector  ??Rd in the following way (Collins, 2002): F(x) = argmax y?GEN(x) (x,y) (1) where GEN(x) denotes the set of possible parses for sentence x and (x,y)   = summationtexti ii(x,y) is the inner product." ></td>
	<td class="line x" title="71:197	The learning task is to set the parameter values (the feature weights) using the training set as evidence, where the training set consists of examples (xi,yi) for 1 ??i ??N. The decoder is an algorithm which finds the argmax in (1)." ></td>
	<td class="line x" title="72:197	In this paper, Y is the set of possible CCG derivations and GEN(x) enumerates the set of derivations for sentence x. We use the same feature representation (x,y) as in Clark and Curran (2004b), to allow comparison with the log-linear model." ></td>
	<td class="line x" title="73:197	The features are defined in terms of local subtrees in the derivation, consisting of a parent category plus one or two children." ></td>
	<td class="line x" title="74:197	Some features are lexicalized, encoding word-word dependencies." ></td>
	<td class="line x" title="75:197	Features are integervalued, counting the number of times some configuration occurs in a derivation." ></td>
	<td class="line x" title="76:197	GEN(x) is defined by the CCG grammar, plus the supertagger, since the supertagger determines how many lexical categories are assigned to each word in x (through the  parameter)." ></td>
	<td class="line x" title="77:197	Rather than try to recreate the adaptive supertagging described in Section 2 for training, we simply fix the the value of  so that GEN(x) is the set of derivations licenced by the grammar for sentence x, given that value." ></td>
	<td class="line x" title="78:197	 is now a parameter of the training process which we determine experimentally using development data." ></td>
	<td class="line x" title="79:197	The  parameter can be thought of as determining the set of incorrect derivations which the training algorithm 11 uses to ?discriminate against??" ></td>
	<td class="line x" title="80:197	with a smaller value of  resulting in more derivations." ></td>
	<td class="line x" title="81:197	3.1 Feature Forests The same decoder is used for both training and testing: the Viterbi algorithm." ></td>
	<td class="line x" title="82:197	However, the packed representation of GEN(x) in each case is different." ></td>
	<td class="line x" title="83:197	When running the parser, a lot of grammatical information is stored in order to produce linguistically meaningful output." ></td>
	<td class="line x" title="84:197	For training, all that is required is a packed representation of the features on each derivation in GEN(x) for each sentence in the training data." ></td>
	<td class="line x" title="85:197	The feature forests described in Miyao and Tsujii (2002) provide such a representation." ></td>
	<td class="line x" title="86:197	Clark and Curran (2004b) describe how a set of CCG derivations can be represented as a feature forest." ></td>
	<td class="line x" title="87:197	The feature forests are created by first building packed charts for the training sentences, and then extracting the feature information." ></td>
	<td class="line x" title="88:197	Packed charts group together equivalent chart entries." ></td>
	<td class="line x" title="89:197	Entries are equivalent when they interact in the same manner with both the generation of subsequent parse structure and the numerical parse selection." ></td>
	<td class="line x" title="90:197	In practice, this means that equivalent entries have the same span, and form the same structures and generate the same features in any further parsing of the sentence." ></td>
	<td class="line x" title="91:197	Back pointers to the daughters indicate how an individual entry was created, so that any derivation can be recovered from the chart." ></td>
	<td class="line x" title="92:197	A feature forest is essentially a packed chart with only the feature information retained (see Miyao and Tsujii (2002) and Clark and Curran (2004b) for the details)." ></td>
	<td class="line x" title="93:197	Dynamic programming algorithms can be used with the feature forests for efficient estimation." ></td>
	<td class="line x" title="94:197	For the log-linear parsing model in Clark and Curran (2004b), the inside-outside algorithm is used to calculate feature expectations, which are then used by the BFGS algorithm to optimise the likelihood function." ></td>
	<td class="line x" title="95:197	For the perceptron, the Viterbi algorithm finds the features corresponding to the highest scoring derivation, which are then used in a simple additive update process." ></td>
	<td class="line x" title="96:197	3.2 The Perceptron Algorithm The training algorithm initializes the parameter vector as all zeros, and updates the vector by decoding the examples." ></td>
	<td class="line x" title="97:197	Each feature forest is decoded with the current parameter vector." ></td>
	<td class="line x" title="98:197	If the output is incorInputs: training examples (xi,yi) Initialisation: set  = 0 Algorithm: for t = 1T, i = 1N calculate zi = argmaxy?GEN(xi) (xi,y) if zi negationslash= yi  =  +(xi,yi)??xi,zi) Outputs:  Figure 1: The perceptron training algorithm rect, the parameter vector is updated by adding the feature vector of the correct derivation and subtracting the feature vector of the decoder output." ></td>
	<td class="line x" title="99:197	Training typically involves multiple passes over the data." ></td>
	<td class="line x" title="100:197	Figure 1 gives the algorithm, where N is the number of training sentences and T is the number of iterations over the data." ></td>
	<td class="line x" title="101:197	For all the experiments in this paper, we used the averaged version of the perceptron." ></td>
	<td class="line x" title="102:197	Collins (2002) introduced the averaged perceptron, as a way of reducing overfitting, and it has been shown to perform better than the non-averaged version on a number of tasks." ></td>
	<td class="line x" title="103:197	The averaged parameters are defined as follows: s = summationtextt=1T,i=1N t,is /NT where t,is is the value of the sth feature weight after the tth sentence has been processed in the ith iteration." ></td>
	<td class="line x" title="104:197	A naive implementation of the averaged perceptron updates the accumulated weight for each feature after each example." ></td>
	<td class="line x" title="105:197	However, the number of features whose values change for each example is a small proportion of the total." ></td>
	<td class="line x" title="106:197	Hence we use the algorithm described in Daume III (2006) which avoids unnecessary calculations by only updating the accumulated weight for a feature fs when s changes." ></td>
	<td class="line x" title="107:197	4 Experiments The feature forests were created as follows." ></td>
	<td class="line x" title="108:197	First, the value of the  parameter for the supertagger was fixed (for the first set of experiments at 0.004)." ></td>
	<td class="line x" title="109:197	The supertagger was then run over the sentences in Sections 2-21 of CCGbank." ></td>
	<td class="line x" title="110:197	We made sure that every word was assigned the correct lexical category among its set (we did not do this for testing)." ></td>
	<td class="line x" title="111:197	Then the parser was run on the supertagged sentences, using the CKY algorithm and the CCG combinatory rules." ></td>
	<td class="line x" title="112:197	We applied the same normal-form restrictions used in Clark and Curran (2004b): categories can 12 only combine if they have been seen to combine in Sections 2-21 of CCGbank, and only if they do not violate the Eisner (1996a) normal-form constraints." ></td>
	<td class="line x" title="113:197	This part of the process requires a few hundred MB of RAM to run the parser, and takes a few hours for Sections 2-21 of CCGbank." ></td>
	<td class="line x" title="114:197	Any further training times or memory requirements reported do not include the resources needed to create the forests." ></td>
	<td class="line x" title="115:197	The feature forests are extracted from the packed chart representation used in the parser." ></td>
	<td class="line x" title="116:197	We only use a feature forest for training if it contains the correct derivation (according to CCGbank)." ></td>
	<td class="line x" title="117:197	Some forests do not have the correct derivation, even though we ensure the correct lexical categories are present, because the grammar used by the parser is missing some low-frequency rules in CCGbank." ></td>
	<td class="line x" title="118:197	The total number of forests used for the experiments was 35,370 (89% of Sections 2-21) . Only features which occur at least twice in the training data were used, of which there are 477,848." ></td>
	<td class="line x" title="119:197	The complete set of forests used to obtain the final perceptron results in Section 4.1 require 21 GB of disk space." ></td>
	<td class="line x" title="120:197	The perceptron is an online algorithm, updating the weights after each forest is processed." ></td>
	<td class="line x" title="121:197	Each forest is read into memory one at a time, decoding is performed, and the weight values are updated." ></td>
	<td class="line x" title="122:197	Each forest is discarded from memory after it has been used." ></td>
	<td class="line x" title="123:197	Constantly reading forests off disk is expensive, but since the perceptron converges in so few iterations the training times are reasonable." ></td>
	<td class="line x" title="124:197	In contrast, log-linear training takes hundreds of iterations to converge, and so it would be impractical to keep reading the forests off disk." ></td>
	<td class="line x" title="125:197	Also, since loglinear training uses a batch algorithm, it is more convenient to keep the forests in memory at all times." ></td>
	<td class="line x" title="126:197	In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of BFGS, to solve this problem, but need up to 20 GB of RAM." ></td>
	<td class="line x" title="127:197	The feature forest representation, and our implementation of it, is so compact that the perceptron training requires only 20 MB of RAM." ></td>
	<td class="line x" title="128:197	Since the supertagger has already removed much of the practical parsing complexity, decoding one of the forests is extremely quick, and much of the training time is taken with continually reading the forests off disk." ></td>
	<td class="line x" title="129:197	However, the training time for the perceptron is still only around 5 hours for 10 iterations." ></td>
	<td class="line x" title="130:197	model RAM iterations time (mins) perceptron 20 MB 10 312 log-linear 19 GB 475 91 Table 1: Training requirements for the perceptron and log-linear models Table 1 compares the training for the perceptron and log-linear models." ></td>
	<td class="line x" title="131:197	The perceptron was run for 10 iterations and the log-linear training was run to convergence." ></td>
	<td class="line x" title="132:197	The training time for 10 iterations of the perceptron is longer than the log-linear training, although the results in Section 4.1 show that the perceptron typically converges in around 4 iterations." ></td>
	<td class="line x" title="133:197	The striking result in the table is the significantly smaller memory requirement for the perceptron." ></td>
	<td class="line x" title="134:197	4.1 Results Table 2 gives the first set of results for the averaged perceptron model." ></td>
	<td class="line x" title="135:197	These were obtained using Section 00 of CCGbank as development data." ></td>
	<td class="line x" title="136:197	Goldstandard POS tags from CCGbank were used for all the experiments." ></td>
	<td class="line x" title="137:197	The parser provides an analysis for 99.37% of the sentences in Section 00." ></td>
	<td class="line x" title="138:197	The F-scores are based only on the sentences for which there is an analysis." ></td>
	<td class="line x" title="139:197	Following Clark and Curran (2004b), accuracy is measured using F-score over the goldstandard predicate-argument dependencies in CCGbank." ></td>
	<td class="line x" title="140:197	The table shows that the accuracy increases initially with the number of iterations, but converges quickly after only 4 iterations." ></td>
	<td class="line x" title="141:197	The accuracy after only one iteration is also surprisingly high." ></td>
	<td class="line x" title="142:197	Table 3 compares the accuracy of the perceptron and log-linear models on the development data." ></td>
	<td class="line x" title="143:197	LP is labelled precision, LR is labelled recall, and CAT is the lexical category accuracy." ></td>
	<td class="line x" title="144:197	The same feature forests were used for training the perceptron and log-linear models, and the same parser and decoding algorithm were used for testing, so the results for the two models are directly comparable." ></td>
	<td class="line x" title="145:197	The only difference in each case was the weights file used.1 The table also gives the accuracy for the perceptron model (after 6 iterations) when a smaller value of the supertagger  parameter is used during the 1Both of these models have parameters which have been optimised on the development data, in the log-linear case the Gaussian smoothing parameter and in the perceptron case the number of training iterations." ></td>
	<td class="line x" title="146:197	13 iteration 1 2 3 4 5 6 7 8 9 10 F-score 85.87 86.28 86.33 86.49 86.46 86.51 86.47 86.52 86.53 86.54 Table 2: Accuracy on the development data for the averaged perceptron ( = 0.004) model LP LR F CAT log-linear=0.004 87.02 86.07 86.54 93.99 perceptron=0.004 87.11 85.98 86.54 94.03 perceptron=0.002 87.25 86.20 86.72 94.08 Table 3: Comparison of the perceptron and loglinear models on the development data forest creation (with the number of training iterations again optimised on the development data)." ></td>
	<td class="line x" title="147:197	A smaller  value results in larger forests, giving more incorrect derivations for the training algorithm to ?discriminate against??" ></td>
	<td class="line x" title="148:197	Increasing the size of the forests is no problem for the perceptron, since the memory requirements are so modest, but this would cause problems for the log-linear training which is already highly memory intensive." ></td>
	<td class="line x" title="149:197	The table shows that increasing the number of incorrect derivations gives a small improvement in performance for the perceptron." ></td>
	<td class="line x" title="150:197	Table 4 gives the accuracies for the two models on the test data, Section 23 of CCGbank." ></td>
	<td class="line x" title="151:197	Here the coverage of the parser is 99.63%, and again the accuracies are computed only for the sentences with an analysis." ></td>
	<td class="line x" title="152:197	The figures for the averaged perceptron were obtained using 6 iterations, with  = 0.002." ></td>
	<td class="line x" title="153:197	The perceptron slightly outperforms the log-linear model (although we have not carried out significance tests)." ></td>
	<td class="line x" title="154:197	We justify the use of different  values for the two models by arguing that the perceptron is much more flexible in terms of the size of the training forests it can handle." ></td>
	<td class="line x" title="155:197	Note that the important result here is that the perceptron model performs at least as well as the loglinear model." ></td>
	<td class="line x" title="156:197	Since the perceptron is considerably easier to train, this is a useful finding." ></td>
	<td class="line x" title="157:197	Also, since the log-linear parsing model is a Conditional Random Field (CRF), the results suggest that the perceptron should be compared with a CRF for other tasks for which the CRF is considered to give state-of-theart results." ></td>
	<td class="line x" title="158:197	model LP LR F CAT log-linear=0.004 87.39 86.51 86.95 94.07 perceptron=0.002 87.50 86.62 87.06 94.08 Table 4: Comparison of the perceptron and loglinear models on the test data 5 Order of Training Examples As an example of the flexibility of our discriminative training framework, we investigated the order in which the training examples are presented to the online perceptron learner." ></td>
	<td class="line x" title="159:197	These experiments were particularly easy to carry out in our framework, since the 21 GB file containing the complete set of training forests can be sampled from directly." ></td>
	<td class="line x" title="160:197	We stored the position on disk of each of the forests, and selected the forests one by one, according to some order." ></td>
	<td class="line x" title="161:197	The first set of experiments investigated ordering the training examples by sentence length." ></td>
	<td class="line x" title="162:197	Buttery (2006) found that a psychologically motivated Categorial Grammar learning system learned faster when the simplest linguistic examples were presented first." ></td>
	<td class="line x" title="163:197	Table 5 shows the results both when the shortest sentences are presented first and when the longest sentences are presented first." ></td>
	<td class="line x" title="164:197	Training on the longest sentences first provides the best performance, but is no better than the standard ordering." ></td>
	<td class="line x" title="165:197	For the random ordering experiments, forests were randomly sampled from the complete 21 GB training file on disk, without replacement." ></td>
	<td class="line x" title="166:197	The new forests file was then used for the averagedperceptron training, and this process was repeated 9 times." ></td>
	<td class="line x" title="167:197	The number of iterations for each training run was optimised in terms of the accuracy of the resulting model on the development data." ></td>
	<td class="line x" title="168:197	There was little variation among the models, with the best model scoring 86.84% F-score on the development data and the worst scoring 86.63%." ></td>
	<td class="line x" title="169:197	Table 6 shows that the performance of this best model on the test data is only slightly better than the model trained using the CCGbank ordering." ></td>
	<td class="line x" title="170:197	14 iteration 1 2 3 4 5 6 Standard order 86.14 86.30 86.53 86.61 86.69 86.72 Shortest first 85.98 86.41 86.57 86.56 86.54 86.53 Longest first 86.25 86.48 86.66 86.72 86.74 86.75 Table 5: F-score of the averaged perceptron on the development data for different data orderings ( = 0.002) perceptron model LP LR F CAT standard order 87.50 86.62 87.06 94.08 best random order 87.52 86.72 87.12 94.12 averaged 87.53 86.67 87.10 94.09 Table 6: Comparison of various perceptron models on the test data Finally, we used the 10 models (including the model from the original training set) to investigate model averaging." ></td>
	<td class="line x" title="171:197	Corston-Oliver et al.(2006) motivate model averaging for the perceptron in terms of Bayes Point Machines." ></td>
	<td class="line x" title="173:197	The averaged perceptron weights resulting from each permutation of the training data were simply averaged to produce a new model." ></td>
	<td class="line x" title="174:197	Table 6 shows that the averaged model again performs only marginally better than the original model, and not as well as the best-performing ?random??model, which is perhaps not surprising given the small variation among the performances of the component models." ></td>
	<td class="line x" title="175:197	In summary, the perceptron learner appears highly robust to the order of the training examples, at least for this parsing task." ></td>
	<td class="line x" title="176:197	6 Comparison with Other Work Taskar et al.(2004) investigate discriminative training methods for a phrase-structure parser, and also use dynamic programming for the decoder." ></td>
	<td class="line x" title="178:197	The key difference between our work and theirs is that they are only able to train on sentences of 15 words or less, because of the expense of the decoding." ></td>
	<td class="line x" title="179:197	There is work on discriminative models for dependency parsing (McDonald, 2006); since there are efficient decoding algorithms available (Eisner, 1996b), complete resources such as the Penn Treebank can used for estimation, leading to accurate parsers." ></td>
	<td class="line x" title="180:197	There is also work on discriminative models for parse reranking (Collins and Koo, 2005)." ></td>
	<td class="line x" title="181:197	The main drawback with this approach is that the correct parse may get lost in the first phase." ></td>
	<td class="line oc" title="182:197	The existing work most similar to ours is Collins and Roark (2004)." ></td>
	<td class="line o" title="183:197	They use a beam-search decoder as part of a phrase-structure parser to allow practical estimation." ></td>
	<td class="line x" title="184:197	The main difference is that we are able to store the complete forests for training, and can guarantee that the forest contains the correct derivation (assuming the grammar is able to generate it given the correct lexical categories)." ></td>
	<td class="line x" title="185:197	The downside of our approach is the restriction on the locality of the features, to allow dynamic programming." ></td>
	<td class="line o" title="186:197	One possible direction for future work is to compare the search-based approach of Collins and Roark with our DP-based approach." ></td>
	<td class="line x" title="187:197	In the tagging domain, Collins (2002) compared log-linear and perceptron training for HMM-style tagging based on dynamic programming." ></td>
	<td class="line x" title="188:197	Our work could be seen as extending that of Collins since we compare log-linear and perceptron training for a DPbased wide-coverage parser." ></td>
	<td class="line x" title="189:197	7 Conclusion Investigation of discriminative training methods is one of the most promising avenues for breaking the current bottleneck in parsing performance." ></td>
	<td class="line x" title="190:197	The drawback of these methods is the need for an efficient decoder." ></td>
	<td class="line x" title="191:197	In this paper we have demonstrated how the lexicalized nature of CCG can be used to develop a very efficient decoder, which leads to a practical development environment for discriminative training." ></td>
	<td class="line x" title="192:197	We have also provided the first comparison of a perceptron and log-linear model for a wide-coverage phrase-structure parser." ></td>
	<td class="line x" title="193:197	An advantage of the perceptron over the log-linear model is that it is considerably easier to train, requiring 1/1000th of the memory requirements and converging in only 4 iterations." ></td>
	<td class="line x" title="194:197	Given that the global log-linear model used here (CRF) is thought to provide state-of-the-art performance for many NLP tasks, it is perhaps surprising 15 that the perceptron performs as well." ></td>
	<td class="line x" title="195:197	The evaluation in this paper was based solely on CCGbank, but we have shown in Clark and Curran (2007) that the CCG parser gives state-of-the-art performance, outperforming the RASP parser (Briscoe et al. , 2006) by over 5% on DepBank." ></td>
	<td class="line x" title="196:197	This suggests the need for more comparisons of CRFs and discriminative methods such as the perceptron for other NLP tasks." ></td>
	<td class="line x" title="197:197	Acknowledgements James Curran was funded under ARC Discovery grants DP0453131 and DP0665973." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-2211
Symbolic Preference Using Simple Scoring
Newman, P. S.;"></td>
	<td class="line x" title="1:238	Proceedings of the 10th Conference on Parsing Technologies, pages 8392, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:238	c2007 Association for Computational Linguistics Symbolic Preference Using Simple Scoring Paula S. Newman newmanp@acm.org Abstract Despite the popularity of stochastic parsers, symbolic parsing still has some advantages, but is not practical without an effective mechanism for selecting among alternative analyses." ></td>
	<td class="line x" title="3:238	This paper describes the symbolic preference system of a hybrid parser that combines a shallow parser with an overlay parser that builds on the chunks." ></td>
	<td class="line x" title="4:238	The hybrid currently equals or exceeds most stochastic parsers in speed and is approaching them in accuracy." ></td>
	<td class="line x" title="5:238	The preference system is novel in using a simple, three-valued scoring method (-1, 0, or +1) for assigning preferences to constituents viewed in the context of their containing constituents." ></td>
	<td class="line x" title="6:238	The approach addresses problems associated with earlier preference systems, and has considerably facilitated development." ></td>
	<td class="line x" title="7:238	It is ultimately based on viewing preference scoring as an engineering mechanism, and only indirectly related to cognitive principles or corpus-based frequencies." ></td>
	<td class="line x" title="8:238	1 Introduction Despite the popularity of stochastic parsers, symbolic parsing still has some advantages, but is not practical without an effective mechanism for selecting among alternative analyses." ></td>
	<td class="line x" title="9:238	Without it, accept/fail grammar rules must either be overly strong or admit very large numbers of parses." ></td>
	<td class="line x" title="10:238	Symbolic parsers have recently been augmented by stochastic post-processors for output disambiguation, which reduces their independence from corpora." ></td>
	<td class="line x" title="11:238	Both the LFG XLE parser (Kaplan et.al. 2004), and the HPSG LinGO ERG parser (Toutanova et al. 2005) have such additions." ></td>
	<td class="line x" title="12:238	This paper examines significant aspects of a purely symbolic alternative: the preference and pruning system of the RH (Retro-Hybrid) parser (Newman, 2007)." ></td>
	<td class="line x" title="13:238	The parser combines a preexisting, efficient shallow parser with an overlay parser that builds on the emitted chunks." ></td>
	<td class="line x" title="14:238	The overlay parser is 'retro' in that the grammar is related to ATNs (Augmented Transition Networks) originated by Woods (1970)." ></td>
	<td class="line x" title="15:238	RH delivers single 'best' parses providing syntactic categories, syntactic functions, head features, and other information (Figure 1)." ></td>
	<td class="line x" title="16:238	The parenthesized numbers following the category labels in the figure are preference scores, and are explained further on." ></td>
	<td class="line x" title="17:238	While the parses are not quite as detailed as those obtained using 'deep' grammars, the missing information, mostly relating to long distance dependencies, can be added at far less cost in a post-parse phase that operates only on a single best parse." ></td>
	<td class="line x" title="18:238	Methods for doing so, for stochastic parser output, are described by Johnson (2002) and Cahill et al (2004)." ></td>
	<td class="line x" title="19:238	The hybrid parser exceeds most stochastic parsers in speed, and approaches them in accuracy, even based on limited manual 'training' on a particular idiom, so the preference system is a successful one (see Section 6), and continues to improve." ></td>
	<td class="line x" title="20:238	The RH preference system builds on earlier methods." ></td>
	<td class="line x" title="21:238	The major difference is a far simpler scoring system, which has considerably facilitated overlay parser development." ></td>
	<td class="line x" title="22:238	Also, the architecture allows the use of large numbers of preference tests without impacting parser speed." ></td>
	<td class="line x" title="23:238	Finally, the treatment of coordination exploits the lookaheads afforded by the shallow parser to license or bar alternative appositive readings." ></td>
	<td class="line x" title="24:238	Section 2 below discusses symbolic preference systems in general, and section 3 provides an overview of RH parser structure." ></td>
	<td class="line x" title="25:238	Section 4 describes the organization of the RH preference system and the simplified scoring mechanism." ></td>
	<td class="line x" title="26:238	Section 5 discusses the training approach and Section 6 provides some experimental results." ></td>
	<td class="line x" title="27:238	Section 7 summarizes, and indicates directions for further work." ></td>
	<td class="line x" title="28:238	83 Figure 1." ></td>
	<td class="line x" title="29:238	Output Parse Tree for 'Rumsfeld micromanaged daily briefings and rode roughshod over people'." ></td>
	<td class="line x" title="30:238	* indicates head." ></td>
	<td class="line x" title="31:238	Mouseover shows head features for 'micromanaged'." ></td>
	<td class="line x" title="32:238	2 Background: Symbolic Preference 2.1 Principles Preference-based parsing balances necessarily permissive syntactic rules by preference rules that promote more likely interpretations." ></td>
	<td class="line x" title="33:238	One of the earliest works in the area is by Wilks (1975), which presented a view of preference as based on semantic templates." ></td>
	<td class="line x" title="34:238	Throughout the 1980's there was a considerable amount of work devoted to finding general principles, often cognitively oriented, for preference rules, and then to devise mechanisms for using them in practical systems." ></td>
	<td class="line x" title="35:238	Hobbs and Bear (1990) provide a useful summary of the evolved principles." ></td>
	<td class="line x" title="36:238	Slightly restated, these principles are: 1." ></td>
	<td class="line x" title="37:238	Prefer attachments in the 'most restrictive context'." ></td>
	<td class="line x" title="38:238	2." ></td>
	<td class="line x" title="39:238	If that doesn't uniquely determine the result, attach low and parallel, and finally 3." ></td>
	<td class="line x" title="40:238	Adjust the above based on considerations of punctuation Principle 1 suggests that the preference for a constituent in a construction should depend on the extent to which the constituent meets a narrow set of expectations." ></td>
	<td class="line x" title="41:238	Most of the examples given by Hobbs and Bear use either (a) sub-categorization information, e.g., preferring the attachment of a prepositional phrase to a head that expects that particular preposition, or (b) limited semantic information, for example, preferring the attachment of a time expression to an event noun." ></td>
	<td class="line x" title="42:238	Principle 2 implies that in the absence of coordination, attachment should be low, and in the presence of coordination, parallel constituents should be preferred." ></td>
	<td class="line x" title="43:238	Principle 3 relates primarily to the effect of commas in modifying attachment preferences." ></td>
	<td class="line x" title="44:238	2.2 Implementations Abstractly, symbolic preference systems can be thought of as regarding a set of possible parses as a collection of spanning trees over a network of potential relationships, with each edge having a numeric value, and attempting to find the highest scoring tree.1 However, for syntactic parsers, in contrast with dependency parsers, it is convenient to associate scores with constituents as they are built, for consistency with the parser structure, and to permit within-parse pruning." ></td>
	<td class="line x" title="45:238	A basic model for a preference system assigns preference scores to rules." ></td>
	<td class="line x" title="46:238	For a rule C  c1, c2, , cn the preference score PS(CC) of a resultant constituent CC is the sum: PS(cc1) + PS(cc2) + +PS(ccn) + TRS (C, cc1, cc2, , ccn) where PS(cci) is the non-contexted score of constituent cci, and the total relationship score TRS is a value that assesses the relationships among the sibling constituents of CC." ></td>
	<td class="line x" title="47:238	The computation of TRS depends on the parser approach." ></td>
	<td class="line x" title="48:238	For a top-down parser, TRS may be the sum of contexted relationship scores CRS, for example: TRS = CRS (cc1|C) +CRS (cc2|C, cc1), + CRS (cc3|C, cc1, cc2) +  + CRS (cn |C, cc1,.ccn-1) where each CRS (cci|_ ) evaluates cci in the context of the prior content of the constituent CC and the category C Few publications specify details of how preference scores are assigned and combined." ></td>
	<td class="line x" title="49:238	For example, Hobbs and Bear (1990) say only that 'When a 1 The idea has also been used directly in stochastic parsers that consider all possible attachments, for example, by McDonald et al.(2005)." ></td>
	<td class="line x" title="51:238	84 non-terminal node of a parse tree is constructed, it is given an initial score which is the sum of the scores of its child nodes." ></td>
	<td class="line x" title="52:238	Various conditions are checked during the construction of the node and, as a result, a score of 20, 10, 3, -3, -10, or -20 may be added to the initial score'." ></td>
	<td class="line x" title="53:238	McCord (1993), however, carefully describes how the elements of TRS are computed in his slot grammar system." ></td>
	<td class="line x" title="54:238	Each element value is the sum of the results of up to 8 optional, typed tests, relating to structural, syntactic, and semantic conditions." ></td>
	<td class="line x" title="55:238	One of these tests, relating to coordination, is a complex test involving 7 factors assessing parallelism." ></td>
	<td class="line x" title="56:238	2.3 Multi-Level Contexted Scoring The scores assigned by symbolic preference systems to particular relationships or combinations usually indicate not just whether they are preferred or dispreferred, but to what degree." ></td>
	<td class="line x" title="57:238	For example, a score of 1 might indicate that a relationship is good, and 2 that it is better." ></td>
	<td class="line x" title="58:238	Such multi-level scores create problems in tuning parsers to remove undesirable interactions, both in the grammar and the preference system." ></td>
	<td class="line x" title="59:238	Even for interactions foreseen in advance, one must remember or find out the sizes of the preferences involved, to decide how to compensate." ></td>
	<td class="line x" title="60:238	Yamabana et al.(1993) give as an example a bottom-up parser, where an S constituent with a transitive verb head but lacking an object is initially given a strong negative preference, but when it is discovered that the constituent actually functions as a relative clause, the appropriate compensation must be found." ></td>
	<td class="line x" title="62:238	(Their solution uses a vector of preference scores, with the vector positions corresponding to specific types of preference features, together with an accumulator." ></td>
	<td class="line x" title="63:238	It allows the content of vector elements to be erased based on subsequently discovered compensating features)." ></td>
	<td class="line x" title="64:238	For unforeseen interactions, for example when a review of parser outputs finds that the best parse is not given the highest preference score, multi-level contexted scoring requires complex tracing of the contribution of each score to the total, remembering at each point what the score should be, to determine the necessary adjustments." ></td>
	<td class="line x" title="65:238	A different sort of problem of multi-level scoring stems from the unavoidable incompleteness of information." ></td>
	<td class="line x" title="66:238	For example, in Figure 1, the attachment of an object to the 'guessed' verb 'micromanaged' is dispreferred because the verb is not identified as transitive." ></td>
	<td class="line x" title="67:238	Here, the correct reading survives because there are no higher scoring ones." ></td>
	<td class="line x" title="68:238	But in some situations, if such a dispreference were given a large negative score, the parser could be forced into very odd readings not compensated for by other factors." ></td>
	<td class="line x" title="69:238	2.4 Corpus-Based Preference In the early 1990's, the increasing availability and use of corpora, together with a sense that multilevel symbolic preference scores were based on adhoc judgments, led to experiments and systems that used empirical methods to obtain preference weights." ></td>
	<td class="line x" title="70:238	Examples of this work include a system by Liu et al (1990), and experiments by Hindle and Rooth (1993), and Resnik and Hearst (1993).2 These efforts had mixed success, suggesting that while multi-level preference scores are problematic, integrating some corpus data does not solve the problems." ></td>
	<td class="line x" title="71:238	In light of later developments, this might be expected." ></td>
	<td class="line x" title="72:238	Full-scale contemporary stochastic parsers use a broad range of interacting features to obtain their fine-grained results; frequencies of particular relationships are just one aspect." ></td>
	<td class="line x" title="73:238	2.5 OT-based Preference A more recent approach to symbolic preference adapts optimality theory to parser and generator preference." ></td>
	<td class="line x" title="74:238	Optimality Theory (OT) was originally developed to explain phonological rules (Prince and Smolensky, 1993)." ></td>
	<td class="line x" title="75:238	In that use, potential rules are given one 'optimality mark' for each constraint they violate." ></td>
	<td class="line x" title="76:238	The marks, all implicitly negative, are ranked by level of severity." ></td>
	<td class="line x" title="77:238	A best rule R is one for which (a) the most severe level of constraint violation L is  the level violated by any other rule, and (b) if other rules also violate level L constraints, the number of such violations is  the number of violations by R. As adapted for use in the XLE processor for LFG (Frank et al. 1998) optimality marks are associated with parser and generator outputs." ></td>
	<td class="line x" title="78:238	Positive marks are added, and also labeled inter-mark positions within the optimality mark ranking." ></td>
	<td class="line x" title="79:238	The labeled positions influence processor behavior." ></td>
	<td class="line x" title="80:238	For generation, they are used to disprefer infelicitous strings accepted in a parse direction." ></td>
	<td class="line x" title="81:238	And for pars2 McCord (1993) also includes some corpus-based information, but to a very limited extent." ></td>
	<td class="line x" title="82:238	85 ing they can be used to disprefer (actually ignore) rarely-applicable rules, in order to reduce parse time (Kaplan et al, 2004)." ></td>
	<td class="line x" title="83:238	However, because the optimality marks are global, a single dispreference can rule out an entire parse." ></td>
	<td class="line x" title="84:238	To partially overcome this limitation, a further extension (see XLE Online Documentation) allows direct comparisons of alternative readings for the same input extent." ></td>
	<td class="line x" title="85:238	A different optimality mark can be set for each reading, and the use of one such mark in the ranking can be conditioned on the presence of another particular mark for the same extent." ></td>
	<td class="line x" title="86:238	For example, a conditional dispreference can be set for an adjunct reading if an argument reading also exists." ></td>
	<td class="line x" title="87:238	The extension does not address more global interactions, and is said (Forst et al. 2005) to be used mostly as a pre-filter to limit the readings disambiguated by a follow-on stochastic process." ></td>
	<td class="line x" title="88:238	2.6 A Slightly Different View A slightly different view of preferencebased parsing is that the business of a preference system is to work in tandem with a permissive syntactic grammar, to manipulate outcomes." ></td>
	<td class="line x" title="89:238	The difference focuses on the pragmatic role of preference in coercing the parser." ></td>
	<td class="line x" title="90:238	In this light, the principles of section 2.1 are guidelines for desired outcomes, not bases for judging the goodness of a relationship or setting preference values." ></td>
	<td class="line x" title="91:238	Instead, preference values should be set based on their effectiveness in isolating best parses." ></td>
	<td class="line x" title="92:238	Also, in this light, the utility of a preference system lies not only in its contribution to accuracy, but also in its software-engineering convenience." ></td>
	<td class="line x" title="93:238	These considerations led to the simpler, more practical scoring system of the RH overlay parser, described in section 4 below, in which contexted preference scores CRS can have one of only 3 values, -1, 0, or +1." ></td>
	<td class="line x" title="94:238	3 Background: The RH Parser The RH parser consists of three major components, outlined below: the shallow parser, a mediating 'locator' phase, and the overlay parser." ></td>
	<td class="line x" title="95:238	3.1 Shallow Parser The shallow parser used, XIP, was developed by XRCE (Xerox Research Center Europe)." ></td>
	<td class="line x" title="96:238	It is actually a full parser, whose per-sentence output consists of a single tree of basic chunks, together with identifications of (sometimes alternative) typed dependences among the chunk heads (AitMokhtar et al. 2002, Gala 2004)." ></td>
	<td class="line x" title="97:238	But because the XIP dependency analysis for English was not mature at the time that work on RH began, and because a classic parse tree annotated by syntactic functions is more convenient for some applications, we focused on the output chunks." ></td>
	<td class="line x" title="98:238	XIP is astonishingly fast, contributing very little to parse times (about 20%)." ></td>
	<td class="line x" title="99:238	It consists of the XIP processor, plus grammars for a number of languages." ></td>
	<td class="line x" title="100:238	The grammar for a particular language consists of: (a) a finite-state lexicon producing alternative part-of-speech and morphological analyses for each token, together with bit-expressed subcategorization and control features, and (some) semantic features, (b) a substitutable tagger identifying the most probable part of speech for each token, and (c) sequentially applied rule sets that extend and modify lexical information, disambiguate tags, identify named entities and other multiwords, and produce output chunks and inter-chunk head dependences (the latter not used in the hybrid)." ></td>
	<td class="line x" title="101:238	Work on the hybrid parser has included large scale extensions to the XIP English rule sets." ></td>
	<td class="line x" title="102:238	3.2 Locator phase The locator phase accumulates and analyses some of the shallow parser results to expedite the grammar and preference tests of the overlay parser." ></td>
	<td class="line x" title="103:238	For preference tests, for any input position, the positions of important leftward and rightward tokens are identified." ></td>
	<td class="line x" title="104:238	These 'important' tokens include commas, and leftward phrase heads that might serve as alternative attachment points." ></td>
	<td class="line x" title="105:238	Special attention is given to coordination, a constant source of inefficiency and inaccuracy for all parsers." ></td>
	<td class="line x" title="106:238	To limit this problem, an input string is divided into spans ending at coordinating conjunctions, and the chunks following a span are examined to determine what kinds of coordination might be present in the span." ></td>
	<td class="line x" title="107:238	For example, if a chunk following a span Sp is a noun phrase, and there are no verbs in the input following that noun phrase, only noun phrase coordination is considered within Sp." ></td>
	<td class="line x" title="108:238	Also, with heuristic exceptions, the locator phase disallows searching for appositives within 86 long sequences of noun and prepositional phrases ending with a coordinating conjunction." ></td>
	<td class="line x" title="109:238	3.3 Overlay Parser The overlay parser uses a top-down grammar, expressed as a collection of ATN-like grammar networks." ></td>
	<td class="line x" title="110:238	A recursive control mechanism traverses the grammar networks depth-first to build constituents." ></td>
	<td class="line x" title="111:238	The labels on the grammar network arcs represent specialized categories, and are associated with tests that, if successful, either return a chunk or reinvoke the control to attempt to build a constituents for the category." ></td>
	<td class="line x" title="112:238	The labelspecific tests include both context-free tests, and tests taking into account the current context." ></td>
	<td class="line x" title="113:238	For details see (Newman, 2007)." ></td>
	<td class="line x" title="114:238	If an invocation of the control is successful, it returns an output network containing one or more paths, with each path representing an alternative sequence of immediate children of the constituent." ></td>
	<td class="line x" title="115:238	An example output network is shown in figure 2." ></td>
	<td class="line x" title="116:238	Each arc of the network references either a basic chunk, or a final state of a subordinate output network." ></td>
	<td class="line x" title="117:238	Unlike the source grammar networks, the output networks do not contain cycles or converging arcs, so states represent unique paths." ></td>
	<td class="line x" title="118:238	The states contain both (a) information about material already encountered along the path, including syntactic functions and head features, and (b) a preference score for the path to that point." ></td>
	<td class="line x" title="119:238	Thus the figure 2 network represents two alternative noun phrases, one represented by the path containing OS0 and OS1, and one containing OS0, OS1, and OS2." ></td>
	<td class="line x" title="120:238	State OS2 contains the preference score (+1), because attaching a locative pp to a feature of the landscape is preferred." ></td>
	<td class="line x" title="121:238	From To Cat Synfun Reference OSo OS1 NP HEAD NPChunk (The park) OS1 OS2 PP NMOD Final state of PP net for (in Paris) States Score Final?" ></td>
	<td class="line x" title="122:238	OS0 0 No OS1 0 Yes OS2 +1 Yes Figure 2." ></td>
	<td class="line x" title="123:238	Output network for 'The park in Paris' Before an output network is returned from an invocation of the control mechanism, it is pruned to remove lower-scoring paths, and cached." ></td>
	<td class="line x" title="124:238	Output from the overlay parser is a single tree (Figure 1) derived from a highest scoring full path (i.e. final state) of a topmost output network." ></td>
	<td class="line x" title="125:238	If there are several highest scoring paths, low attach considerations select a 'best' one." ></td>
	<td class="line x" title="126:238	The preference scores shown in Figure 1 in parentheses after the category labels are the scores at the succeeding states of the underlying output networks." ></td>
	<td class="line x" title="127:238	4 Preference System Any path in an output network has the form: S0, Ref1, S1, Ref2, , Sn-1, Refn, Sn where Si is a state, and Refi labels an arc, and references either a basic chunk, or a final state of another output network." ></td>
	<td class="line x" title="128:238	A state Si has total preference score TPS(i) where:  TPS(0) = 0  TPS(i), i>0 = TPS( i-1) + PS(Refi) +CRS(Refi)  PS(Refi) is the non-contexted score of the constituent referenced by Refi, that is, the score at the referenced final state." ></td>
	<td class="line x" title="129:238	 CRS(Refi) is the contexted score for Refi, in the context of the network category and the path ending at the previous state i-1." ></td>
	<td class="line x" title="130:238	For example, if Refi refers to a noun phrase considered a second object within a path, and the syntactic head along the path does not expect a second object, CRS(Refi) might be (-1)." ></td>
	<td class="line x" title="131:238	Each value CRS is limited to values in {-1, 0, +1}." ></td>
	<td class="line x" title="132:238	Therefore, no judgment is needed to decide the degree to which a contexted reference is to be dispreferred or preferred." ></td>
	<td class="line x" title="133:238	Also, if the desired parse result does not receive the highest overall score, it is relatively easy to trace the reason." ></td>
	<td class="line x" title="134:238	Pruning (see below) can be disabled and all parses can be displayed, as in Figure 1, which shows the scores TPS(i) in parentheses after the category labels for each Refi (with zero scores not shown)." ></td>
	<td class="line x" title="135:238	Then, if TPS(i) > ( TPS(i-1) + PS(Refi)) it is clear that the contexted reference is preferred." ></td>
	<td class="line x" title="136:238	If multi-level contexted scoring were used instead, it would be necessary to determine whether the reference was preferred to exactly the right degree." ></td>
	<td class="line x" title="137:238	87 Test Block Type Length Independent?" ></td>
	<td class="line x" title="138:238	Indexed By Coordinate Y Parent syncat Subcat Y No index FN1 Y synfun TAG1 Y syncat FN2 N synfun TAG2 N syncat Table 1." ></td>
	<td class="line x" title="139:238	Preference Test Block Types 4.1 Preference test organization To compute the contexted score CRS for a reference, relevant tests are applied until either (a) a score of -1 is obtained, which is used as CRS for the reference, or (b) the tests are exhausted." ></td>
	<td class="line x" title="140:238	In the latter case, CRS is the higher of the values {0, +1} returned by any test." ></td>
	<td class="line x" title="141:238	For purposes of efficiency, the preference tests are divided into typed blocks, as shown in Table 1." ></td>
	<td class="line x" title="142:238	At most one block of each type can be applied to a reference." ></td>
	<td class="line x" title="143:238	Four of the blocks contain tests that are independent of referenced constituent length." ></td>
	<td class="line x" title="144:238	They are applied at most once for a returned output network and the results are assumed for all paths." ></td>
	<td class="line x" title="145:238	The other two blocks are length dependent." ></td>
	<td class="line x" title="146:238	Referring to Table 1, the length-independent coordinate tests are applied only to non-first siblings of coordinated constituents." ></td>
	<td class="line x" title="147:238	The parent category indicates the type of constituents being coordinated and selects the appropriate test block." ></td>
	<td class="line x" title="148:238	Tests in these blocks focus on the semantic consistency of a coordinated sibling with the first one." ></td>
	<td class="line x" title="149:238	Subcategorization tests are applied to prepositional, particle, and clausal dependents of the current head." ></td>
	<td class="line x" title="150:238	These tests consist to a large extent of bit-vector implemented operations, comparing the expected dependent types of the head with lexical features of the prospective dependent." ></td>
	<td class="line x" title="151:238	The tests are made somewhat more complex because of various exceptions, such as (a) temporal and locative phrases, and (b) the presence of a nearer potential head also expecting the dependent type." ></td>
	<td class="line x" title="152:238	The other test block types are selected and accessed either by the syntactic category or the syntactic function of the reference, depending on the focus of the test." ></td>
	<td class="line x" title="153:238	The length-dependent tests include tests of noun-phrases within coordinations to determine whether post modifiers should be applied to the individual phrase or to the coordination as a whole." ></td>
	<td class="line x" title="154:238	The test blocks are expressed in procedural code." ></td>
	<td class="line x" title="155:238	This has allowed the parser to be developed without advance prediction of the types of information needed for the tests, and also has contributed some efficiency." ></td>
	<td class="line x" title="156:238	The blocks, usually short but occasionally long, generally consist of ordered (if-then-else) subtests." ></td>
	<td class="line x" title="157:238	4.2 Preference test scope A contexted preference test can refer to material on three levels of the developing parse tree: (a) the syntactic category of the parent (available because of the top-down parser direction) (b) information about the current output network path, including head features, already-encountered syntactic functions, and a small collection of special-purpose information, and (c) information about the referenced constituent, specifically its head and a list of the immediately contained syntactic functions." ></td>
	<td class="line x" title="158:238	The tests can also reference lookahead information furnished by the locator phase." ></td>
	<td class="line x" title="159:238	This material is sufficient for most purposes." ></td>
	<td class="line x" title="160:238	Limiting the kind of referenced information, particularly not permitting access to sibling constituents or deep elements of the referenced constituent, contributes to performance." ></td>
	<td class="line x" title="161:238	4.3 Pruning Before an output network is completed, it is pruned to remove lower-scoring output network paths." ></td>
	<td class="line x" title="162:238	Any path with the same length as another but with a lower score is pruned." ></td>
	<td class="line x" title="163:238	Also, paths having other lengths but considerably lower preference scores than the best-scoring path are often pruned as well." ></td>
	<td class="line x" title="164:238	4.4 Usage Example To illustrate how the simple scores and modular tests are used to detect and repair problems in the preference system, Figure 1 shows, as noted before, that the attachment of an object to the guessed verb 'micromanaged' is dispreferred." ></td>
	<td class="line x" title="165:238	In this case the probable reason is the lack of a transitive feature for the verb." ></td>
	<td class="line x" title="166:238	To check this, we would look at the FN1 test block for OBJ and find that in fact the test assigns (-1) in this case." ></td>
	<td class="line x" title="167:238	The required modification is best made by adding a transitive feature to guessed verbs." ></td>
	<td class="line x" title="168:238	But there is another problem here: the attachment of the pp 'over people' is not given a positive preference." ></td>
	<td class="line x" title="169:238	Checking the FN1 test block for 88 VMOD and the TAG1 test block for PP finds that there is in fact no subtest that prefers combinations of motion verbs and 'over'." ></td>
	<td class="line x" title="170:238	While this doesn't cause trouble in the example, it could if there were a prior object in the verb phrase." ></td>
	<td class="line x" title="171:238	A subtest or subcategorization feature could be added." ></td>
	<td class="line x" title="172:238	5 Training the Preference System To obtain the preference system, an initial set of tests is identified, based primarily on subcategorization considerations, and then refined and extended based on manual 'training' on large numbers of documents." ></td>
	<td class="line x" title="173:238	Several problem situations result in changes to the system, besides random inspection of scores: (a) the best parse identified is not the correct one, either because the correct parse is not the highest scoring one, or because another parse with the same score was considered 'best' because of low-attach considerations." ></td>
	<td class="line x" title="174:238	(b) The best parse obtained is the correct one, but there are many other parses with the same score, suggesting a need for refinement, both to improve performance and to avoid errors in related circumstances when the correct parse does not 'float' to the top." ></td>
	<td class="line x" title="175:238	(c) No parse is returned for an input, because of imposed space constraints, which indirectly control the amount of time that can be spent to obtain a parse." ></td>
	<td class="line x" title="176:238	In some cases the above problems can be solved by adjusting the base grammar, or by extending lexical information to obtain the appropriate preferences." ></td>
	<td class="line x" title="177:238	For example, the preference scoring problems of Figure 1 can be corrected by adding subcategorization information, as described above." ></td>
	<td class="line x" title="178:238	In other cases, one or more modifications to the preference system are made, adding positive tests to better distinguish best parses, adding negative tests to disprefer incorrect parses, and/or refining existing tests to narrow or expand applicability." ></td>
	<td class="line x" title="179:238	Positive tests often just give credit to expected structures not previously considered to require recognition beyond acceptance by the grammar." ></td>
	<td class="line x" title="180:238	Negative tests fall into many classes, such as: (a) Tests for 'ungrammatical' phenomena that should not be ruled out entirely by the grammar. These include lack of agreement, lack of expected punctuation, and presence of unexpected punctuation (such as a comma between a subject and a verb when there is no comma within the subject)." ></td>
	<td class="line x" title="181:238	(b) Tests for probably incomplete constituents, based on the chunk types that follow them." ></td>
	<td class="line x" title="182:238	(c) Tests for unexpected arguments, except in some circumstances." ></td>
	<td class="line x" title="183:238	For example, 'benefactive' indirect objects ('John baked Mary a cake') are dispreferred if they are not in appropriate semantic classes." ></td>
	<td class="line x" title="184:238	Also, a large, complex collection of positive and negative tests, based on syntactic and semantic factors, are used to distinguish among coordinated and appositive readings, and among alternative attachments of appositives." ></td>
	<td class="line x" title="185:238	If the addition or modification of preference tests does not solve a particular problem, then some more basic changes can be made, such as the introduction of new semantic classes." ></td>
	<td class="line x" title="186:238	And, in rare cases, new features are added to output network states in order to make properties of non-head constituents encountered along a path available for testing both further along the path and in the development of higher-level constituents." ></td>
	<td class="line x" title="187:238	An example is the person and number of syntactic subjects, allowing contexted preference tests for finite verb phrases to check for subject consistency." ></td>
	<td class="line oc" title="188:238	5.1 Relationship to 'supervised' training To illustrate the relationship between the above symbolic training method for preference scoring and corpus-based methods, perhaps the easiest way is to compare it to an adaptation (Collins and Roark, 2004) of the perceptron training method to the problem of obtaining a best parse (either directly, or for parse reranking), because the two methods are analogous in a number of ways." ></td>
	<td class="line x" title="189:238	The basic adapted perceptron training assumes a generator function producing parses for inputs." ></td>
	<td class="line x" title="190:238	Each such parse is associated with a vector of feature values that express the number of times the feature appears in the input or parse." ></td>
	<td class="line x" title="191:238	The features used are those identified by Roark (2001) for a topdown stochastic parser." ></td>
	<td class="line x" title="192:238	The training method obtains a weight vector W (initially 0) for the feature values, by iterating multiple times over pairs <xi, yi> where xi is a training input, and yi is the correct parse for xi." ></td>
	<td class="line x" title="193:238	For each pair, the best current parse zi for xi produced by the generator, with feature value vector V(zi), is selected based on the current value of (W  V(zi))." ></td>
	<td class="line x" title="194:238	Then if zi  yi, W is incremented by V(yi), and dec89 remented by V(zi)." ></td>
	<td class="line x" title="195:238	After training, the weights in W are divided by the number of training steps (# inputs * # iterations)." ></td>
	<td class="line x" title="196:238	The method is analogous to the RH manual training process for preference in a number of ways." ></td>
	<td class="line x" title="197:238	First, the features used were developed for suitability to a top-down parser, for example taking into account superordinate categories at several levels, some lexical information associated with non-head, left-side siblings of a node, and some right-hand lookahead." ></td>
	<td class="line x" title="198:238	Although only one superordinate category is routinely used in RH preference tests, in order to allow caching of output networks for a category, the preference system allows for and occasionally uses the promotion of non-head features of nested constituents to provide similar capability." ></td>
	<td class="line x" title="199:238	Also, the feature weights obtained by the perceptron training method can be seen to focus on patterns that actually matter in distinguishing correct from incorrect parses, as does RH preference training." ></td>
	<td class="line x" title="200:238	Intuitively, the difference is that while symbolic training for RH explicitly pinpoints patterns that distinguish among parses, the perceptron training method accomplishes something similar by postulating some more general features as negative or positive based on particular examples, but allowing the iterations over a large training set to filter out potentially indicative patterns that do not actually serve as such." ></td>
	<td class="line x" title="201:238	These analogies highlight the fact that preference system training, whether symbolic or corpusbased, is ultimately an empirical engineering exercise." ></td>
	<td class="line x" title="202:238	6 Some Experimental Results Tables 2, 3, and 4 summarize some recent results as obtained by testing on Wall Street Journal section 23 of the Penn Treebank (Marcus et al. 1994)." ></td>
	<td class="line x" title="203:238	The RH results were obtained by about 8 weeks of manual training on the genre." ></td>
	<td class="line x" title="204:238	Table 2 compares speed and coverage for RH and Collins Model3 (Collins, 1999) run on the same CPU." ></td>
	<td class="line x" title="205:238	The table also extrapolates the results to two other parsers, based on reported comparisons with Collins." ></td>
	<td class="line x" title="206:238	One extrapolation is to a very fast stochastic parser by Sagae and Lavie (2005)." ></td>
	<td class="line x" title="207:238	The comparison indicates that the RH parser speed is close to that of the best contemporary parsers." ></td>
	<td class="line x" title="208:238	The second extrapolation is to the LFG XLE parser (Kaplan et al. 2004) for English, consisting of a highly developed symbolic parser and grammar, an OT-based preference component, and a stochastic back end to select among remaining alternative parser outputs." ></td>
	<td class="line x" title="209:238	Two sets of values are given for XLE, one obtained using the full English grammar, and one obtained using a reduced grammar ignoring less-frequently applicable rules." ></td>
	<td class="line x" title="210:238	The extrapolation indicates that the coverage of RH is quite good for a symbolic parser with limited training on an idiom." ></td>
	<td class="line x" title="211:238	While the most important factor in RH parser speed is the enormous speed of the shallow parser, the preference and pruning approach of the overlay parser make contributions to both speed and coverage." ></td>
	<td class="line x" title="212:238	This can be seen in Table 2 by the difference between RH parser results with and without pruning." ></td>
	<td class="line x" title="213:238	Pruning increases coverage because without it more parses exceed imposed resource limits." ></td>
	<td class="line x" title="214:238	Table 3 compares accuracy." ></td>
	<td class="line x" title="215:238	The values for Collins and Sagae/Lavie are based on comparison with treebank data for the entire section 23." ></td>
	<td class="line x" title="216:238	However, because RH does not produce treebank-style tags, the RH values are based only on a random Time No full parse Sagae/Lavie ~ 4 min 1.1% RH Prune 5 min 14 sec 10.8% RH NoPrune 7 min 5 sec 13.9 % Collins m3 16 min.6% XLE reduced ~24 minutes unknown XLE full ~80 minutes ~21% Table 2." ></td>
	<td class="line x" title="217:238	Speeds and Extrapolated speeds Fully accurate F-score Avg cross bkts Sagae/Lavie unknwn 86% unknwn Collins Lbl 33.6% 88.2% 1.05 CollinsNoLbl 35.4% 89.4 % 1.05 RH NoLbl 46% 86 % .59 Table 3." ></td>
	<td class="line x" title="218:238	Accuracy Comparison Average Median RH Base 137.10 11 RH Pref 5.04 2 Table 4." ></td>
	<td class="line x" title="219:238	Highest Scoring Parses per Input 90 100-sentence sample from section 23, and compared using a different unlabeled bracketing standard." ></td>
	<td class="line x" title="220:238	For details see Newman (2007)." ></td>
	<td class="line x" title="221:238	For nonparsed sentences the chunks are bracketed." ></td>
	<td class="line x" title="222:238	Accuracy is not extrapolated to XLE because available measurements give f-scores (all  80%) for dependency relations rather than for bracketed constituents." ></td>
	<td class="line x" title="223:238	As a partial indication of the role and effectiveness of the RH preference system, if non-parsed sentences are ignored, the percentage of fully accurate bracketings shown in Table 3 rises to approximately 46/89 = 51.6% (it is actually larger because coverage is higher on the 100-sentence sample)." ></td>
	<td class="line x" title="224:238	As further indication, Table 4 compares, for section 23, the average and median number of parses per sentence obtained by the base grammar alone (RH Base), and the base grammar plus the preference system (RH Pref).3 The table demonstrates that the preference system is a crucial parser component." ></td>
	<td class="line x" title="225:238	Also, the median of 2 parses per sentence obtained using the preference system explains why the fallback low-attach strategy is successful in many cases." ></td>
	<td class="line x" title="226:238	7 Summary and Directions The primary contribution of this work is in demonstrating the feasibility of a vastly simplified symbolic preference scoring method." ></td>
	<td class="line x" title="227:238	The preference scores assigned are neither 'principle-based', nor 'ad-hoc', but explicitly engineered to facilitate the management of undesirable interactions in the grammar and in the preference system itself." ></td>
	<td class="line x" title="228:238	Restricting individual contexted scores to {-1, 0, +1} addresses the problems of multi-level contexted scoring discussed in Section 2, as follows:  No abstract judgment is required to assign a value to a preference or dispreference." ></td>
	<td class="line x" title="229:238	 Information deficiencies contribute only small dispreferences, so they can often be overcome by preferences." ></td>
	<td class="line x" title="230:238	 Compensating for interactions that are foreseen does not require searching the rules to find necessary compensating values." ></td>
	<td class="line x" title="231:238	 For unforeseen interactions discovered when reviewing parser results, the simplified pref3 The values are somewhat inflated because they include duplicate parses, which have not yet been entirely eliminated." ></td>
	<td class="line x" title="232:238	erence scores facilitate finding the sources of the problems and potential methods of solving them." ></td>
	<td class="line x" title="233:238	This approach to symbolic preference has facilitated development and maintenance of the RH parser, and has enabled the production of results with a speed and accuracy comparable to the best stochastic parsers, even with limited training on an idiom." ></td>
	<td class="line x" title="234:238	An interesting question is why this very simple approach does not seem to have been used previously." ></td>
	<td class="line x" title="235:238	Part of the answer may lie in the lack of explicit recognition that symbolic preference scoring is ultimately an engineering problem, and is only indirectly based on cognitive principles or approximations to frequencies of particular relationships." ></td>
	<td class="line x" title="236:238	Ongoing development of the RH preference system includes continuing refinement based on 'manual' training, and continuing expansion of the set of semantic features used as the parser is applied to new domains." ></td>
	<td class="line x" title="237:238	Additional development will also include more encoding of, and attention to, the expected semantic features of arguments." ></td>
	<td class="line x" title="238:238	Experiments are also planned to examine the accuracy/performance tradeoffs of using additional context information in the preference tests." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-2012
ILP-based Conceptual Analysis for Chinese NPs
Ji, Paul D.;Pulman, Stephen G.;"></td>
	<td class="line x" title="1:116	Coling 2008: Companion volume  Posters and Demonstrations, pages 4750 Manchester, August 2008 ILP-based Conceptual Analysis for Chinese NPs Paul D. Ji Center for Language and Philology Oxford University Paul_dji@yahoo.com.uk Stephen Pulman Computing Laboratory Oxford University sgp@clg.ox.ac.uk  ABSTRACT In this paper, we explore a conceptual resource for Chinese nominal phrases, which allows multi-dependency and distinction between dependency and the corresponding exact relation." ></td>
	<td class="line x" title="2:116	We also provide an ILP-based method to learn mapping rules from training data, and use the rules to analyze new nominal phrases." ></td>
	<td class="line x" title="3:116	1 Introduction Nominal phrases have long been a concern in linguistic research and language processing (e.g., Copestake and Briscoe, 2005; Giegerich, 2004)." ></td>
	<td class="line x" title="4:116	Generally, nominal phrases can be classified into two categories according to whether they contain attributive clauses or not." ></td>
	<td class="line x" title="5:116	We focus on nominal phrases without attributive clauses." ></td>
	<td class="line x" title="6:116	Closely related with nominal phrases, nominal compounds or base NPs have also attracted a great attention in language processing." ></td>
	<td class="line x" title="7:116	Generally, nominal compounds refer to nominal phrases consisting of a series of nouns, while base NPs refer to non-recursive nominal phrases." ></td>
	<td class="line x" title="8:116	However, such compounds or base NPs usually co-occur with other non-nominal words in running texts, and it is impossible to separate them during analysis." ></td>
	<td class="line x" title="9:116	Furthermore, there exist syntactic makers for attributive clauses, e.g., which or who in English and   (of) in Chinese, nominal phrases without attributive clauses tend to be a better linguistic category for theoretical and practical investigation." ></td>
	<td class="line x" title="10:116	To analyze NPs, we need first to determine what kinds of information are to be recognized." ></td>
	<td class="line x" title="11:116	In this work, we focus on conceptual relatedness between words." ></td>
	<td class="line x" title="12:116	For example, in linguistics and   2008." ></td>
	<td class="line x" title="13:116	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/)." ></td>
	<td class="line x" title="14:116	Some rights reserved." ></td>
	<td class="line x" title="15:116	law books, linguistic and law are both conceptually related with books, although linguistics doesnt have a superficial syntactic relation with books." ></td>
	<td class="line x" title="16:116	Then, we need to fulfill two sub-tasks." ></td>
	<td class="line x" title="17:116	One is about representation, i.e., what schemes are to be used." ></td>
	<td class="line x" title="18:116	The other is about analysis, i.e., how to derive the formal representation." ></td>
	<td class="line x" title="19:116	Regarding representation scheme, one possible strategy would be using syntactic structures, as are usually used in analysis for sentences." ></td>
	<td class="line x" title="20:116	However, syntactic components for NPs, unlike those for sentences (e.g., V, VP, A, AP, and S, etc.), are difficult to differentiate, and rules governing nominal phrases are especially difficult to determine." ></td>
	<td class="line x" title="21:116	As an example, consider       (bank loan interest), which is a nominal compound consisting of three serial nouns." ></td>
	<td class="line x" title="22:116	For such a NP, if a rule with binary combination is used, it would produce two structures for the unambiguous NP." ></td>
	<td class="line x" title="23:116	If a rule with triple combination is used, as in Chinese Treebank (Xue et al., 2005), it would be difficult to disclose the lexical relation between    (bank) and   (loan)." ></td>
	<td class="line x" title="24:116	Another possible representation strategy would be using dependency structures (Melcuk, 1988)." ></td>
	<td class="line x" title="25:116	Under this strategy, a NP could be represented as a dependency tree, which captures various lexical control or dependency in the phrase." ></td>
	<td class="line x" title="26:116	However, traditional framework only focuses on syntactic dependency, while conceptual relatedness may exist without syntactic relations." ></td>
	<td class="line x" title="27:116	For example, for             (economic development and law construction in Shanghai), in traditional dependency analysis, (Shanghai) would depend on the conjunction word  (and), since conjunction words are usually regarded as heads in coordinate structures." ></td>
	<td class="line x" title="28:116	Although the relatedness may go downward from the head, it would be difficult to derive the relatedness between   (Shanghai) and (economic) or   (law), since the two words are even not heads of the conjuncts      (eco47 nomic development) and     (law development)." ></td>
	<td class="line oc" title="29:116	As to analysis of NPs, there have been a lot of work on statistical techniques for lexical dependency parsing of sentences (Collins and Roark, 2004; McDonald et al., 2005), and these techniques potentially can be used for analysis of NPs if appropriate resources for NPs are available." ></td>
	<td class="line x" title="30:116	However, these techniques are all meant to building a dependency tree, while the conceptual relatedness in NPs may form a graph, with multidependency allowed." ></td>
	<td class="line x" title="31:116	Additionally, these methods generally suffer from the difficulty of local estimation from limited contexts and the structural information is difficult to be exploited (Califf and Mooney, 2003)." ></td>
	<td class="line x" title="32:116	Recently, relational learning methods in general and inductive logic programming (ILP) in particular have attracted a great of attention due to their capability of going beyond finite feature vectors and exploiting unbounded structural information from data (Califf and Mooney, 2003; Page et al., 2003; Srinivasan et al., 2003)." ></td>
	<td class="line x" title="33:116	In this work, we try to extend syntactic dependency to conceptual dependency to capture the embedded lexical relatedness, and use ILP to analyze nominal phrases, making use of the structural information provided by the resources based on conceptual dependency." ></td>
	<td class="line x" title="34:116	2 Conceptual dependency In comparison with syntactic dependency, conceptual dependency may allow a word to be dependent on multiple words at the same time." ></td>
	<td class="line x" title="35:116	For example, in             (Economic development and law construction in Shanghai), (Shanghai) conceptually relates with both (economic) and   (law), while in   (activity of blood donation for university student volunteers),    (university student) relates on both   (volunteer) and   (blood donation)." ></td>
	<td class="line x" title="36:116	In addition, syntactic dependency doesnt exactly specify what kind of relatedness held between words, although the words denoting the relatedness may occur within NPs." ></td>
	<td class="line x" title="37:116	For example, 1) is an ambiguous compound with two possible interpretations listed in 2)." ></td>
	<td class="line x" title="38:116	1)      (student discussion)      2) i)  discussion by students          ii) discussion about students However, the dependency trees corresponding with the two interpretations remain the same: (student) depends on   (discussion) in both cases." ></td>
	<td class="line x" title="39:116	In fact, their difference lies in the exact semantic relations held between the words: (student) is agent and patient of   (discussion) in 2i) and 2ii) respectively." ></td>
	<td class="line x" title="40:116	This suggests that only syntactic dependency is not enough to reflect conceptual difference." ></td>
	<td class="line x" title="41:116	Notice that in (2), the relations between the two words   (student) and   (discussion) are denoted by two proper nouns, agent and patient, which may never co-occur with them in running texts." ></td>
	<td class="line x" title="42:116	However, in some cases, some word cooccurring with two conceptually related words do denote the relatedness exactly." ></td>
	<td class="line x" title="43:116	Consider    (car in read color), where  (color) relates with both   (car) and  (red)." ></td>
	<td class="line x" title="44:116	In the conceptual view,  (color) can be seen as a feature of   (car), and  (red) can be seen as a kind of value for the feature, as was also adopted in dealing with adjectives in WordNet (Fellbaum, 1988)." ></td>
	<td class="line x" title="45:116	In this setting,  (red) directly depends on   (car), and (color) represents the relation between them." ></td>
	<td class="line x" title="46:116	In building the resource for Chinese NPs, the conceptual relatedness is based on semantic reference, while the dependency is based on syntactic or potential syntactic relations." ></td>
	<td class="line x" title="47:116	The feature words we adopt are mostly listed in a medium class, coded as Dn, in a Chinese thesaurus, Tongyici Cilin (henceafter Cilin, Mei et al., 1982)." ></td>
	<td class="line x" title="48:116	Function words (e.g.,  (from)), Part words (e.g.,  (leg)) and Number words (e.g., (count)) are also regarded as feature words." ></td>
	<td class="line x" title="49:116	3 ILP-based Analysis Fig." ></td>
	<td class="line x" title="50:116	1 gives the overall structure of the analysis procedure." ></td>
	<td class="line x" title="51:116	END    Fig." ></td>
	<td class="line x" title="52:116	1 Overall structure of analysis procedure  The analysis consists of two phases, training and parsing." ></td>
	<td class="line x" title="53:116	During the training phase, rules are learned for mapping from conceptual dependency graphs to word strings based on training examples." ></td>
	<td class="line x" title="54:116	During the parsing phase, there are three steps." ></td>
	<td class="line x" title="55:116	Search is to find candidate dependency graphs, Generation is to generate word strings from candidate dependency graphs using Rule Learning Search Generation Evaluation Parsing Training 48 the learned rules, and Evaluation is to compare the generated word strings with the original NPs." ></td>
	<td class="line x" title="56:116	3.1 Training: learning rules For each training sample, we have a nominal phrase and its corresponding conceptual dependency graph." ></td>
	<td class="line x" title="57:116	To learn the rules mapping from dependency graphs to word strings, we need to tag the words with their sense labels, which denote the synsets in the thesaurus (Mei et al., 1982)." ></td>
	<td class="line x" title="58:116	For the sense tagging, we used the same method as in (Yarowsky, 1992) and used the minor categories in the thesaurus as the synsets." ></td>
	<td class="line x" title="59:116	Generally, a rule consists of two parts, Gr and Sr. Gr is a dependency sub-graph and Sr is a sense label string." ></td>
	<td class="line x" title="60:116	Intuitively, conceptual configuration in Gr is represented by the label string of Sr. To capture more structural information, we need to find the maximal sub-graph in the training data, whose corresponding labels form a continuous substring in the training data." ></td>
	<td class="line x" title="61:116	But the problem is NP hard, and we thus use heuristics to find am optimally maximal sub-graphs." ></td>
	<td class="line x" title="62:116	However, the search has a bias to larger sub-graphs, and to avoid the bias, we set the coverage of a sub-graph as the penalty." ></td>
	<td class="line x" title="63:116	Here, the coverage of the sub-graph refers to the percentage of the nodes in the sub-graphs among all the nodes in the training data." ></td>
	<td class="line x" title="64:116	The overall algorithm is: i) to find the most common edge in the training data, whose corresponding label strings are continuous; ii) to add another edge to the sub-graph, if the label strings corresponding with the new sub-graph are still continuous until the coverage of the sub-graph doesnt increase." ></td>
	<td class="line x" title="65:116	After finding such a sub-graph, we merge all the nodes into one, and merge the sense label strings into one, and repeat the process until all the nodes in the training data are covered The result of the learning is a set of rules, and each rule specifies a sub-graph and a label string." ></td>
	<td class="line x" title="66:116	For example, w got a rule which includes the sub-graph in Fig." ></td>
	<td class="line x" title="67:116	2 and sense label string in 3)." ></td>
	<td class="line x" title="68:116	Fig." ></td>
	<td class="line x" title="69:116	2." ></td>
	<td class="line x" title="70:116	Sub-graph in a rule." ></td>
	<td class="line x" title="71:116	3) SL(  )SL(  )SL( )   For rule generalization, we dont try to compress the rule set, and simply use the sense hierarchy in the thesaurus, including the minor, medium and major classes." ></td>
	<td class="line x" title="72:116	3.2 Parsing After training phase, we get a set of learned rules." ></td>
	<td class="line x" title="73:116	During parsing, the task is to find a conceptual dependency graph for a new input data, which would generate the NP using the learned rules." ></td>
	<td class="line x" title="74:116	The optimal parsing can be implemented in a greedy manner." ></td>
	<td class="line x" title="75:116	First, one dependency with two words is selected." ></td>
	<td class="line x" title="76:116	Then, another word is added if the resulted conceptual dependency graph generates a word string which best matches the input nominal phrase." ></td>
	<td class="line x" title="77:116	This process can be repeated until the graph includes all the words in the data." ></td>
	<td class="line x" title="78:116	To compare the generated word string with the original input, we use edit distance between them, which is based on the times of operations (including adjacent move, deletion, insertion) needed to convert one word string to another." ></td>
	<td class="line x" title="79:116	4 Experiments and Evaluation There are 10,000 nominal phrases annotated in the resource, and they were selected from 1,221 articles form the corpora of China daily, 1992." ></td>
	<td class="line x" title="80:116	Table 1 gives the statistics of the resource." ></td>
	<td class="line x" title="81:116	num de structure Nominal compound Dependency with feature Multidependency 10K 4,234 5,766 1,235 976  Table 1." ></td>
	<td class="line x" title="82:116	Statistics of NP resource  Here, de structure refer to the phrase with word   (of)." ></td>
	<td class="line x" title="83:116	Nominal compounds refer to the nominal phrases with no occurrence of  (of)." ></td>
	<td class="line x" title="84:116	Dependency with features refers to those tagged with features, which also occur in the same NPs." ></td>
	<td class="line x" title="85:116	Multi-dependency refers to the number of monodependencies occurring in the multi-dependency." ></td>
	<td class="line x" title="86:116	We randomly selected 10% of the training data as closed test data, and the other 90% or less as training data." ></td>
	<td class="line x" title="87:116	To evaluate the performance of the dependency analysis, we used F-scores as evaluation measure as usual." ></td>
	<td class="line x" title="88:116	Fig." ></td>
	<td class="line x" title="89:116	4 shows the results for overall dependency, multi-dependency and dependency with features." ></td>
	<td class="line x" title="90:116	The results are averaged over 10 random runs." ></td>
	<td class="line x" title="91:116	(and)   (boys)   (some) 49 0 0." ></td>
	<td class="line x" title="92:116	2 0." ></td>
	<td class="line x" title="93:116	4 0." ></td>
	<td class="line x" title="94:116	6 0." ></td>
	<td class="line x" title="95:116	8 1 10% 30% 50% 70% 90% Tr ai ni ng dat a FS c o r e over al  dependency mul t i dependency dependency wi t h f eat ur es  Fig." ></td>
	<td class="line x" title="96:116	3 Performance with varying training data  Fig." ></td>
	<td class="line x" title="97:116	3 demonstrates that with more training data, the performance generally improved." ></td>
	<td class="line x" title="98:116	The performance for dependency with features seemed better than that for overall dependency or multi-dependency." ></td>
	<td class="line x" title="99:116	To check the reason, we found that we treated the Amount words in Number-Amount structures as features, and these words are generally easier to be identified, since they tend to be unambiguous." ></td>
	<td class="line x" title="100:116	Once they were recognized as Amount words, the relevant dependency would be correctly identified." ></td>
	<td class="line x" title="101:116	For an open test, we selected another 1,000 nominal phrases from the same corpus, but from different time period (1994)." ></td>
	<td class="line x" title="102:116	Such phrases were annotated with the same standard as those training data." ></td>
	<td class="line x" title="103:116	Fig." ></td>
	<td class="line x" title="104:116	4 shows the results with varying training data." ></td>
	<td class="line x" title="105:116	0 0." ></td>
	<td class="line x" title="106:116	2 0." ></td>
	<td class="line x" title="107:116	4 0." ></td>
	<td class="line x" title="108:116	6 0." ></td>
	<td class="line x" title="109:116	8 1 10% 30% 50% 70% 90% Tr ai ni ng dat a FS c o r e cl osed t est ( 1000) open t est ( 1000)  Fig." ></td>
	<td class="line x" title="110:116	4 Comparison: Closed test and open test  Fig." ></td>
	<td class="line x" title="111:116	4 shows that the open test performance is generally worse than that of the closed test." ></td>
	<td class="line x" title="112:116	Notice that although the test data was selected from the same resource, but with a different period, which may account for the different performance." ></td>
	<td class="line x" title="113:116	5 Conclusion In this paper, we described a resource for lexical conceptual dependency of Chinese nominal phrases." ></td>
	<td class="line x" title="114:116	Compared with other ones, it allows multi-dependency and distinguishes dependency and relation, which exactly denotes what kinds of dependency held." ></td>
	<td class="line x" title="115:116	We also provided an ILPbased analysis method, in which some rules mapping from conceptual dependency to word strings are learned from the training data, and then the rules are used to find the conceptual dependency graph for a new data." ></td>
	<td class="line x" title="116:116	Compared with other search strategies, this method makes use of the structural information and allows construction of a dependency graph, not just a dependency tree." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D08-1052
LTAG Dependency Parsing with Bidirectional Incremental Construction
Shen, Libin;Joshi, Aravind K.;"></td>
	<td class="line x" title="1:235	Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 495504, Honolulu, October 2008." ></td>
	<td class="line x" title="2:235	c2008 Association for Computational Linguistics LTAG Dependency Parsing with Bidirectional Incremental Construction Libin Shen BBN Technologies lshen@bbn.com Aravind K. Joshi University of Pennsylvania joshi@cis.upenn.edu Abstract In this paper, we first introduce a new architecture for parsing, bidirectional incremental parsing." ></td>
	<td class="line x" title="3:235	We propose a novel algorithm for incremental construction, which can be applied to many structure learning problems in NLP." ></td>
	<td class="line x" title="4:235	We apply this algorithm to LTAG dependency parsing, and achieve significant improvement on accuracy over the previous best result on the same data set." ></td>
	<td class="line x" title="5:235	1 Introduction The phrase Bidirectional Incremental may appear self-contradictory at first sight, since incremental parsing usually means left-to-right parsing in the context of conventional parsing." ></td>
	<td class="line x" title="6:235	In this paper, we will extend the meaning of incremental parsing." ></td>
	<td class="line x" title="7:235	The idea of bidirectional parsing is related to the bidirectional sequential classification method described in (Shen et al., 2007)." ></td>
	<td class="line x" title="8:235	In that paper, a tagger assigns labels to words of highest confidence first, and then these labels in turn serve as the context of later labelling operations." ></td>
	<td class="line x" title="9:235	The bidirectional tagger obtained the best results in literature on POS tagging on the standard PTB dataset." ></td>
	<td class="line x" title="10:235	We extend this method from labelling to structure learning, The search space of structure learning is much larger, so that it is appropriate to exploit confidence scores in search." ></td>
	<td class="line x" title="11:235	In this paper, we are interested in LTAG dependency parsing because TAG parsing is a well known problem of high computational complexity in regular parsing." ></td>
	<td class="line x" title="12:235	In order to get a focus for the learning algorithm, we work on a variant of LTAG based parsing in which we learn the word dependency relations encoded in LTAG derivations instead of the full-fledged trees." ></td>
	<td class="line x" title="13:235	1.1 Parsing Two types of parsing strategies are popular in natural language parsing, which are chart parsing and incremental parsing." ></td>
	<td class="line x" title="14:235	Suppose the input sentence is w1w2wn." ></td>
	<td class="line x" title="15:235	Let cell [i,j] represent wiwi+1wj, a substring of the sentence." ></td>
	<td class="line x" title="16:235	As far as CFG parsing is concerned, a chart parser computes the possible structures over all possible cells [i,j], where 1  i  j  n. The order of computing on these n(n + 1)/2 cells is based on some partial orderprecedesequal, such that [p1,p2]precedesequal[q1,q2] if q1  p1  p2  q2." ></td>
	<td class="line x" title="17:235	In order to employ dynamic programming, one can only use a fragment of a hypothesis to represent the whole hypothesis, which is assumed to satisfy conditional independence assumption." ></td>
	<td class="line x" title="18:235	It is well known that richer context representation gives rise to better parsing performance (Johnson, 1998)." ></td>
	<td class="line x" title="19:235	However, the need for tractability does not allow much internal information to be used to represent a hypothesis." ></td>
	<td class="line x" title="20:235	The designs of hypotheses in (Collins, 1999; Charniak, 2000) show a delicate balance between expressiveness and tractability, which play an important role in natural language parsing." ></td>
	<td class="line oc" title="21:235	Some recent work on incremental parsing (Collins and Roark, 2004; Shen and Joshi, 2005) showed another way to handle this problem." ></td>
	<td class="line o" title="22:235	In these incremental parsers, tree structures are used to represent the left context." ></td>
	<td class="line x" title="23:235	In this way, one can access the whole tree to collect rich context information at the expense of being limited to beam search, which only maintains k-best results at each 495 step." ></td>
	<td class="line x" title="24:235	Compared to chart parsing, incremental parsing searches for the analyses for only 2n1 cells, [1,1],[2,2],[1,2],,[i,i],[1,i],,[1,n], incrementally, while complex structures are used for the analyses for each cell, which satisfy conditional independence under a much weaker assumption." ></td>
	<td class="line x" title="25:235	In this paper, we call this particular approach left-to-right incremental parsing, since one can also search from right to left incrementally in a similar way." ></td>
	<td class="line x" title="26:235	A major problem of the left-to-right approach is that one can only utilize the structural information on the left side but not the right side." ></td>
	<td class="line x" title="27:235	1.2 Parsing as Bidirectional Construction A natural way to handle this problem is to employ bidirectional search, which means we can dynamically search the space in two directions." ></td>
	<td class="line x" title="28:235	So we expand the idea of incremental parsing by introducing greedy search." ></td>
	<td class="line x" title="29:235	Specifically, we look for the hypotheses over the cell [1,n] by building analyses over 2n1 cells [ai,1,ai,2],i = 1,,2n1 step by step, where [a2n1,1,a2n1,2] = [1,n]." ></td>
	<td class="line x" title="30:235	Furthermore, for any [ai,1,ai,2]  ai,1 = ai,2, or j,k, such that [ai,1,ai,2] = [aj,1,ak,2], where j < i, k < i and aj,2 + 1 = ak,1." ></td>
	<td class="line x" title="31:235	It is easy to show that the set {[ai,1,ai,2] | 1  i2n1}forms a tree relation, which means that each cell except the last one will be used to build another cell just once." ></td>
	<td class="line x" title="32:235	In this framework, we can begin with several starting points in a sentence and search in any direction." ></td>
	<td class="line x" title="33:235	So left-to-right parsing is only a special case of incremental parsing defined in this way." ></td>
	<td class="line oc" title="34:235	We still use complex structures to represent the partial analyses, so as to employ both top-down and bottom-up information as in (Collins and Roark, 2004; Shen and Joshi, 2005)." ></td>
	<td class="line x" title="35:235	Furthermore, we can utilize the rich context on both sides of the partial results." ></td>
	<td class="line x" title="36:235	Similar to bidirectional labelling in (Shen et al., 2007), there are two learning tasking in this model." ></td>
	<td class="line x" title="37:235	First, we need to learn which cell we should choose." ></td>
	<td class="line x" title="38:235	At each step, we can select only one path." ></td>
	<td class="line x" title="39:235	Secondly, we need to learn which operation we should take for a given cell." ></td>
	<td class="line x" title="40:235	We maintain k-best candidates for each cell instead of only one, which differentiates this model from normal greedy search." ></td>
	<td class="line x" title="41:235	So our model is more robust." ></td>
	<td class="line x" title="42:235	Furthermore, we need to find an effective way to iterate between these two tasks." ></td>
	<td class="line x" title="43:235	Instead of giving an algorithm specially designed for parsing, we generalize the problem for graphs." ></td>
	<td class="line x" title="44:235	A sentence can be viewed as a graph in which words are viewed as vertices and neighboring words are connected with an arc. In Sections 2 and 3, we will propose decoding and training algorithms respectively for graph-based incremental construction, which can be applied to many structure learning problems in NLP." ></td>
	<td class="line x" title="45:235	We will apply this algorithm to dependency parsing of Lexicalized Tree Adjoining Grammar (Joshi and Schabes, 1997)." ></td>
	<td class="line x" title="46:235	Specifically, we will train and evaluate an LTAG dependency parser over the LTAG treebank described in Shen et al.(2008)." ></td>
	<td class="line x" title="48:235	We report the experimental results on PTB section 23 of the LTAG treebank." ></td>
	<td class="line x" title="49:235	The accuracy on LTAG dependency is 90.5%, which is 1.2 points over 89.3%, the previous best result (Shen and Joshi, 2005) on the same data set." ></td>
	<td class="line x" title="50:235	It should be noted that PTB-based bracketed labelling is not an appropriate evaluation metric here, since the experiments are on an LTAG treebank." ></td>
	<td class="line x" title="51:235	The derived trees in the LTAG treebank are different from the CFG trees in PTB." ></td>
	<td class="line x" title="52:235	Hence, we do not use metrics such as labeled precision and labeled recall for evaluation." ></td>
	<td class="line x" title="53:235	2 Graph-based Incremental Construction 2.1 Idea and Data Structures Now we define the problem formally." ></td>
	<td class="line x" title="54:235	We will use dependency parsing as an example to illustrate the idea." ></td>
	<td class="line x" title="55:235	We are given a connected graph G(V,E) whose hidden structure is U, where V = {vi}, E  V V is a symmetric relation, and U = {uk} is composed of a set of elements that vary with applications." ></td>
	<td class="line x" title="56:235	As far as dependency parsing is concerned, the input graph is simply a chain of vertices, where E(vi1,vi), and its hidden structure is {uk = (vsk,vek,bk)}, where vertex vek depends on vertex vsk with label bk." ></td>
	<td class="line x" title="57:235	A graph-based incremental construction algorithm looks for the hidden structure in a bottom-up 496 style." ></td>
	<td class="line x" title="58:235	Let xi and xj be two sets of connected vertexes in V , where xixj =  and they are directly connected via an edge in E. Let yxi be a hypothesized hidden structure of xi, and yxj a hypothesized hidden structure of xj." ></td>
	<td class="line x" title="59:235	Suppose we choose to combine yxi and yxj with an operation r to build a hypothesized hidden structure for xk = xixj." ></td>
	<td class="line x" title="60:235	We say the process of construction is incremental if the output of the operation, yxk = r(xi,xj,yxi,yxj)  yxi yxj for all the possible xi,xj,yxi,yxj and operation r. As far as dependency parsing is concerned, incrementality means that we cannot remove any links coming from the substructures." ></td>
	<td class="line x" title="61:235	Once yxk is built, we can no longer use yxi or yxj as a building block." ></td>
	<td class="line x" title="62:235	It is easy to see that left to right incremental construction is a special case of our approach." ></td>
	<td class="line x" title="63:235	So the question is how we decide the order of construction as well as the type of operation r. For example, in the very first step of dependency parsing, we need to decide which two words are to be combined as well as the dependency label to be used." ></td>
	<td class="line x" title="64:235	This problem is solved statistically, based on the features defined on the substructures involved in the operation and their context." ></td>
	<td class="line x" title="65:235	Suppose we are given the weights of these features, we will show in the next section how these parameters guide us to build a set of hypothesized hidden structures with beam search." ></td>
	<td class="line x" title="66:235	In Section 3, we will present a Perceptron like algorithm (Collins, 2002; Daume III and Marcu, 2005) to obtain the parameters." ></td>
	<td class="line x" title="67:235	Now we introduce the data structure to be used in our algorithms." ></td>
	<td class="line x" title="68:235	A fragment is a connected sub-graph of G(V,E)." ></td>
	<td class="line x" title="69:235	Each fragment x is associated with a set of hypothesized hidden structures, or fragment hypotheses for short: Y x ={yx1,,yxk}." ></td>
	<td class="line x" title="70:235	Each yx is a possible fragment hypothesis of x. It is easy to see that an operation to combine two fragments may depend on the fragments in the context, i.e. fragments directly connected to one of the operands." ></td>
	<td class="line x" title="71:235	So we introduce the dependency relation over fragments." ></td>
	<td class="line x" title="72:235	Suppose there is a dependency relation D F F, where F  2V is the set of all fragments in graph G. D(xi,xj) means that any operation on a fragment hypothesis of xi depends on the features in the fragment hypothesis of xj, and vice versa." ></td>
	<td class="line x" title="73:235	We are especially interested in the following two dependency relations." ></td>
	<td class="line x" title="74:235	 level-0 dependency: D0(xi,xj)  i = j.  level-1 dependency: D1(xi,xj)  xi and xj are directly connected in G. Level-0 dependency means that the features of a hypothesis for a vertex xi do not depend on the hypotheses for other vertices." ></td>
	<td class="line x" title="75:235	Level-1 dependency means that the features depend on the hypotheses of nearby vertices only." ></td>
	<td class="line x" title="76:235	The learning algorithm for level-0 dependency is similar to the guided learning algorithm for labelling as described in (Shen et al., 2007)." ></td>
	<td class="line x" title="77:235	Level-1 dependency requires more data structures to maintain the hypotheses with dependency relations among them." ></td>
	<td class="line x" title="78:235	However, we do not get into the details of level-1 formalism in this papers for two reasons." ></td>
	<td class="line x" title="79:235	One is the limit of page space and depth of a conference paper." ></td>
	<td class="line x" title="80:235	On the other hand, our experiments show that the parsing performance with level-1 dependency is close to what level-0 dependency could provides." ></td>
	<td class="line x" title="81:235	Interested readers could refer to (Shen, 2006) for detailed description of the learning algorithms for level-1 dependency." ></td>
	<td class="line x" title="82:235	2.2 Algorithms Algorithm 1 shows the procedure of building hypotheses incrementally on a given graph G(V,E)." ></td>
	<td class="line x" title="83:235	Parameter k is used to set the beam width of search." ></td>
	<td class="line x" title="84:235	Weight vector w is used to compute score of an operation." ></td>
	<td class="line x" title="85:235	We have two sets, H and Q, to maintain hypotheses." ></td>
	<td class="line x" title="86:235	Hypotheses in H are selected in beam search, and hypotheses in Q are candidate hypotheses for the next step of search in various directions." ></td>
	<td class="line x" title="87:235	We first initiate the hypotheses for each vertex, and put them into set H. For example, in dependency parsing, the initial value is a set of possible POS tags for each single word." ></td>
	<td class="line x" title="88:235	Then we use a queue Q to collect all the possible hypotheses over the initial hypotheses H. Whenever Q is not empty, we search for the hypothesis with the highest score according to a given weight vector w. Suppose we find (x,y)." ></td>
	<td class="line x" title="89:235	We select 497 Algorithm 1 Incremental Construction Require: graph G(V,E); Require: beam width k; Require: weight vector w; 1: HinitH(); 2: QinitQ(H); 3: repeat 4: (x,y)argmax(x,y)Q score(y); 5: H updateH(H,x); 6: QupdateQ(Q,H,x); 7: until (Q = ) DT MD NNVB CDNN NN NN student will take four coursesthe Figure 1: After initialization top k-best hypotheses for segment x from Q and use them to update H. Then we remove from Q all the hypotheses for segments that have overlap with segment x. In the end, we build new candidate hypotheses with the updated selected hypothesis set H, and add them to Q. 2.3 An Example We use an example of dependency parsing to illustrate the incremental construction algorithm first." ></td>
	<td class="line x" title="90:235	Suppose the input sentence is the student will take four courses." ></td>
	<td class="line x" title="91:235	We are also given the candidate POS tags for each word." ></td>
	<td class="line x" title="92:235	So the graph is just a linear structure in this case." ></td>
	<td class="line x" title="93:235	We use level-0 dependency and set beam width to two." ></td>
	<td class="line x" title="94:235	We use boxes to represent fragments." ></td>
	<td class="line x" title="95:235	The dependency links are from the parent to the child." ></td>
	<td class="line x" title="96:235	Figure 1 shows the result after initialization." ></td>
	<td class="line x" title="97:235	Figure 2 shows the result after the first step, combining the fragments of four and courses." ></td>
	<td class="line x" title="98:235	Figure 3 shows the result after the second step, combining the and student, and figure 4 shows the result after the third step, combining take and four courses." ></td>
	<td class="line x" title="99:235	Due to limited space, we skip the rest operations." ></td>
	<td class="line x" title="100:235	2.4 Description Now we will explain the functions in Algorithm 1 one by one." ></td>
	<td class="line x" title="101:235	DT NN VBMD CD NN NN NN CD NN student will take four coursesthe Figure 2: Step 1 DT NN VBMD CD NN DT NN NN NN CD NN student will take four coursesthe Figure 3: Step 2  initH() initiates hypotheses for each vertex." ></td>
	<td class="line x" title="102:235	Here we set the initial fragment hypotheses, Y xi = {yxi1 ,,yxik }, where xi = {vi} contains only one vertex." ></td>
	<td class="line x" title="103:235	 initQ(H) initiates the queue of candidate operations over the current hypotheses H. Supposed there exist segments xi and xj which are directly connected in G. We apply all possible operations to all fragment hypotheses for xj and xj, and add the result hypotheses in Q. For example, we generate (x,y) with some operation r, where segment x is xixj." ></td>
	<td class="line x" title="104:235	All the candidate operations are organized with respect to the segments." ></td>
	<td class="line x" title="105:235	For each segment, we maintain top k candidates according to their scores." ></td>
	<td class="line x" title="106:235	 updateH(H,x) is used to update hypotheses in H. First, we remove from H all the hypotheses whose corresponding segment is a sub-set of x. Then, we add into H the top k hypotheses for segment x.  updateQ(Q,H,x) is also designed to complete two tasks." ></td>
	<td class="line x" title="107:235	First, we remove from Q all the hypotheses whose corresponding segment has overlap with segment x. Then, we add new candidate hypotheses depending on x in a way 498 DT NN VBMD CD NN DT NN NN CD NN student will take four courses MD the Figure 4: Step 3 Algorithm 2 Parameter Optimization 1: w0; 2: for (round r = 0; r < R; r++) do 3: load graph Gr(V,E), gold standard Hr; 4: initiate H and Q; 5: repeat 6: (x,y)argmax(x,y)Q score(y); 7: if (y is compatible with Hr) then 8: update H and Q; 9: else 10: ypositive(Q,x); 11: promote(w, y); 12: demote(w,y); 13: update Q with w; 14: end if 15: until (Q = ) 16: end for similar to the initQ(H) function." ></td>
	<td class="line x" title="108:235	For each segment, we maintain the top k candidates for each segment." ></td>
	<td class="line x" title="109:235	3 Parameter Optimization In the previous section, we described an algorithm for graph-based incremental construction for a given weight vector w. In Algorithm 2, we present a Perceptron like algorithm to obtain the weight vector for the training data." ></td>
	<td class="line x" title="110:235	For each given training sample (Gr,Hr), where Hr is the gold standard hidden structure of graph Gr, we first initiate cut T, hypotheses HT and candidate queue Q by calling initH and initQ as in Algorithm 1." ></td>
	<td class="line x" title="111:235	Then we use the gold standard Hr to guide the search." ></td>
	<td class="line x" title="112:235	We select candidate (x,y) which has the highest operation score in Q. If y is compatible with Hr, we update H and Q by calling updateH and updateQ as in Algorithm 1." ></td>
	<td class="line x" title="113:235	If y is incompatible with Hr, we treat y as a negative sample, and search for a positive sample y in Q with positive(Q,x)." ></td>
	<td class="line x" title="114:235	If there exists a hypothesis yx for fragment x which is compatible with Hr, then positive(Q,x) returns yx." ></td>
	<td class="line x" title="115:235	Otherwise positive(Q,x) returns the candidate hypothesis which is compatible with Hr and has the highest operation score in Q. Then we update the weight vector w with y and y." ></td>
	<td class="line x" title="116:235	At the end, we update the candidate Q by using the new weights w. In order to improve the performance, we use Perceptron with margin in the training (Krauth and Mezard, 1987)." ></td>
	<td class="line x" title="117:235	The margin is proportional to the loss of the hypothesis." ></td>
	<td class="line x" title="118:235	Furthermore, we use averaged weights (Collins, 2002; Freund and Schapire, 1999) in Algorithm 1." ></td>
	<td class="line x" title="119:235	4 LTAG Dependency Parsing We apply the new algorithm to LTAG dependency parsing on an LTAG Treebank (Shen et al., 2008) extracted from Penn Treebank (Marcus et al., 1994) and Proposition Bank (Palmer et al., 2005)." ></td>
	<td class="line x" title="120:235	Penn Treebank was previously used to train and evaluate various dependency parsers (Yamada and Matsumoto, 2003; McDonald et al., 2005)." ></td>
	<td class="line x" title="121:235	In these works, Magermans rules are used to pick the head at each level according to the syntactic labels in a local context." ></td>
	<td class="line x" title="122:235	The dependency relation encoded in the LTAG Treebank reveals deeper information for the following two reasons." ></td>
	<td class="line x" title="123:235	First, the LTAG architecture itself reveals deeper dependency." ></td>
	<td class="line x" title="124:235	Furthermore, the PTB was reconciled with the Propbank in the LTAG Treebank extraction (Shen et al., 2008)." ></td>
	<td class="line x" title="125:235	We are especially interested in the two types of structures in the LTAG Treebank, predicate adjunction and predicate coordination." ></td>
	<td class="line x" title="126:235	They are used to encode dependency relations which are unavailable in other approaches." ></td>
	<td class="line x" title="127:235	On the other hand, these structures turn out to be a big problem for the general representation of dependency relations, including adjunction and coordination." ></td>
	<td class="line x" title="128:235	We will show that the algorithm proposed here provides a nice solution for this problem." ></td>
	<td class="line x" title="129:235	499 has says now he attach attach packagesunion adjoin attach attach Figure 5: Predicate Adjunction 4.1 Representation of the LTAG Treebank In the LTAG Treebank (Shen et al., 2008), each word is associated with a spinal template, which represents the projection from the lexical item to the root." ></td>
	<td class="line x" title="130:235	Templates are linked together to form a derivation tree." ></td>
	<td class="line x" title="131:235	The topology of the derivation tree shows a type of dependency relation, which we call LTAG dependency here." ></td>
	<td class="line x" title="132:235	There are three types of operations in the LTAG Treebank, which are attachment, adjunction, and coordination." ></td>
	<td class="line x" title="133:235	Attachment is used to represent both substitution and sister adjunction in the traditional LTAG." ></td>
	<td class="line x" title="134:235	So it is similar to the dependency relation in other approaches." ></td>
	<td class="line x" title="135:235	The LTAG dependency can be a non-projective relation thanks to the operation of adjunction." ></td>
	<td class="line x" title="136:235	In the LTAG Treebank, raising verbs and passive ECM verbs are represented as auxiliary trees to be adjoined." ></td>
	<td class="line x" title="137:235	In addition, adjunction is used to handle many cases of discontinuous arguments in Propbank." ></td>
	<td class="line x" title="138:235	For example, in the following sentence, ARG1 of says in Propbank is discontinuous, which is First Union now has packages for seven customer groups." ></td>
	<td class="line x" title="139:235	 First Union, he says, now has packages for seven customer groups." ></td>
	<td class="line x" title="140:235	In the LTAG Treebank, the subtree for he says adjoins onto the node of has, which is the root of the derivation tree, as shown in Figure 5." ></td>
	<td class="line x" title="141:235	Another special aspect of the LTAG Treebank is the representation of predicate coordination." ></td>
	<td class="line x" title="142:235	Figure 6 is the representation of the following sentence." ></td>
	<td class="line x" title="143:235	 I couldnt resist rearing up on my soggy loafers and saluting." ></td>
	<td class="line x" title="144:235	The coordination between rearing and saluting is represented explicitly with a coord-structure, and resist rearing saluting and I attach attach attach coordination Figure 6: Predicate Coordination continuedstock pounded amid attach adjoin attach Figure 7: Non-projective Adjunction this coord-structure attaches to resist." ></td>
	<td class="line x" title="145:235	It is shown in (Shen et al., 2008) that coord-structures could encode the ambiguity of argument sharing, which can be non-projective also." ></td>
	<td class="line x" title="146:235	4.2 Incremental Construction We build LTAG derivation trees incrementally." ></td>
	<td class="line x" title="147:235	A hypothesis of a fragment is represented with a partial derivation tree." ></td>
	<td class="line x" title="148:235	When the fragment hypotheses of two nearby fragments combine, the partial derivation trees are combined into one." ></td>
	<td class="line x" title="149:235	It is trivial to combine two partial derivation trees with attachment." ></td>
	<td class="line x" title="150:235	We simply attach the root of one tree to some node on the other tree which is visible to this root node." ></td>
	<td class="line x" title="151:235	Adjunction is similar to attachment, except that an adjoined subtree may be visible from the other side of the derivation tree." ></td>
	<td class="line x" title="152:235	For example, in sentence  The stock of UAL Corp. continued to be pounded amid signs that British Airways  continued adjoins onto pounded, and amid attaches to continued from the other side of the derivation tree (pounded is between continued and amid), as shown in Figure 7." ></td>
	<td class="line x" title="153:235	The predicate coordination is decomposed into a set of operations to meet the need for incremental processing." ></td>
	<td class="line x" title="154:235	Suppose a coordinated structure attaches to the parent node on the left side." ></td>
	<td class="line x" title="155:235	We build this structure incrementally by attaching the first 500 resist rearing saluting and I attach attach attach conjoin Figure 8: Conjunction a63 a36a7a63 a17a17a17a43 a0a0a9 a64a64a82   a29 a81a81a81a115 a64a64a82 a64a64a82 m1.1 m1.1.1 s1 s2 m1 m1.2 m mr m2 s attach Figure 9: Representation of nodes conjunct to the parent and conjoining other conjuncts to first one." ></td>
	<td class="line x" title="156:235	In this way, we do not need to force the coordination to be built before the attachment." ></td>
	<td class="line x" title="157:235	Either can be executed first." ></td>
	<td class="line x" title="158:235	A sample is shown in Figure 8." ></td>
	<td class="line x" title="159:235	4.3 Features In this section, we will describe the features used in LTAG dependency parsing." ></td>
	<td class="line x" title="160:235	An operation is represented by a 4-tuple  op = (type,dir,posleft,posright), where type {attach,adjoin,conjoin} and dir is used to represent the direction of the operation." ></td>
	<td class="line x" title="161:235	posleft and posright are the POS tags of the two operands." ></td>
	<td class="line x" title="162:235	Features are defined on POS tags and lexical items of the nodes in the context." ></td>
	<td class="line x" title="163:235	In order to represent the features, we use m for the main-node of the operation, s for the sub-node, mr for the parent of the main-node, m1mi for the children of m, and s1sj for the children of s, as shown in Figure 9." ></td>
	<td class="line x" title="164:235	The index always starts from the side where the operation takes place." ></td>
	<td class="line x" title="165:235	We use the Gorn addresses to represent the nodes in the subtrees rooted on m and s. Furthermore, we use lk and rk to represent the nodes in the left and right context of the flat sentence." ></td>
	<td class="line x" title="166:235	We use hl and hr to represent the head of the hypothesis trees on the left and right context respectively." ></td>
	<td class="line x" title="167:235	Let x be a node." ></td>
	<td class="line x" title="168:235	We use x.p to represent the POS tag of node x, and x.w to represent the lexical item of node x. Table 1 show the features used in LTAG dependency parsing." ></td>
	<td class="line x" title="169:235	There are seven classes of features." ></td>
	<td class="line x" title="170:235	The first three classes of features are those defined on only one operand, on both operands, and on the siblings respectively." ></td>
	<td class="line x" title="171:235	If gold standard POS tags are used as input, we define features on the POS tags in the context." ></td>
	<td class="line x" title="172:235	If level-1 dependency is used, we define features on the root node of the hypothesis partial derivation trees in the neighborhood." ></td>
	<td class="line x" title="173:235	Half check and full check features are designed for grammatical check." ></td>
	<td class="line x" title="174:235	For example, in Figure 9, node s attaches onto node m from left." ></td>
	<td class="line x" title="175:235	Then nothing can attach onto s from the right side." ></td>
	<td class="line x" title="176:235	The children of the right side of s are fixed, so we use the half check features to check the completeness of the children of the right half for s. Furthermore, we notice that all the rightmost descendants of s and the leftmost descendants of m at each level become unavailable for any further operation." ></td>
	<td class="line x" title="177:235	So their children are fixed after this operation." ></td>
	<td class="line x" title="178:235	All these nodes are in the form of m1.11 or s1.11." ></td>
	<td class="line x" title="179:235	We use full check features to check the children from both sides for these nodes." ></td>
	<td class="line x" title="180:235	In the discussion above, we ignored adjunction and conjunction." ></td>
	<td class="line x" title="181:235	We need to slightly refine the conditions of checking." ></td>
	<td class="line x" title="182:235	Due to the limit of space, we skip these cases." ></td>
	<td class="line x" title="183:235	5 Experiments We use the same data set as in (Shen and Joshi, 2005)." ></td>
	<td class="line x" title="184:235	We use Sec." ></td>
	<td class="line x" title="185:235	2-21 of the LTAG Treebank for training, Sec." ></td>
	<td class="line x" title="186:235	22 for feature selection, and Sec." ></td>
	<td class="line x" title="187:235	23 for test." ></td>
	<td class="line x" title="188:235	Table 2 shows the comparison of different models." ></td>
	<td class="line x" title="189:235	Beam size is set to five in our experiments." ></td>
	<td class="line x" title="190:235	With level-0 dependency, our system achieves an accuracy of 90.3% at the speed of 4.25 sentences a second on a Xeon 3G Hz processor with JDK 1.5." ></td>
	<td class="line x" title="191:235	With level-1 dependency, the parser achieves 90.5% at 3.59 sentences a second." ></td>
	<td class="line x" title="192:235	Level-1 dependency does not provide much improvement due to the fact that level-0 features provide most of the useful information for this specific application." ></td>
	<td class="line x" title="193:235	It is interesting to compare our system with other dependency parsers." ></td>
	<td class="line x" title="194:235	The accuracy on LTAG depen501 category description templates one operand Features defined on only one operand." ></td>
	<td class="line x" title="195:235	For each template tp, [type,dir,tp] is used as a feature." ></td>
	<td class="line x" title="196:235	(m.p), (m.w), (m.p,m.w), (s.p), (s.w), (s.p,s.w) two operands Features defined on both operands." ></td>
	<td class="line x" title="197:235	For each template tp, [op,tp] is used as a feature." ></td>
	<td class="line x" title="198:235	In addition, [op] is also used as a feature." ></td>
	<td class="line x" title="199:235	(m.w), (s.w), (m.w,s.w) siblings Features defined on the children of the main nodes." ></td>
	<td class="line x" title="200:235	For each template tp, [op,tp], [op,m.w,tp], [op,mr.p,tp] and [op,mr.p,m.w,tp] are used as features." ></td>
	<td class="line x" title="201:235	(m1.p), (m1.p,m2.p), , (m1.p,m2.p,,mi.p) POS context In the case that gold standard POS tags are used as input, features are defined on the POS tags of the context." ></td>
	<td class="line x" title="202:235	For each template tp, [op,tp] is used as a feature." ></td>
	<td class="line x" title="203:235	(l2.p), (l1.p), (r1.p), (r2.p), (l2.p,l1.p), (l1.p,r1.p), (r1.p,r2.p) tree context In the case that level-1 dependency is employed, features are defined on the trees in the context." ></td>
	<td class="line x" title="204:235	For each template tp, [op,tp] is used as a feature." ></td>
	<td class="line x" title="205:235	(hl.p), (hr.p) half check Suppose s1,,sk are all the children of s which are between s and m in the flat sentence." ></td>
	<td class="line x" title="206:235	For each template tp, [tp] is used as a feature." ></td>
	<td class="line x" title="207:235	(s.p,s1.p,s2.p,,sk.p), (m.p,s.p,s1.p,s2.p,,sk.p) and (s.w,s.p,s1.p,s2.p,,sk.p), (s.w,m.p,s.p,s1.p,s2.p,,sk.p) if s.w is a verb full check Let x1, x2, , xk be the children of x, and xr the parent of x. For any x = m1.11 or s1.11, template tp, [tp(x)] is used as a feature." ></td>
	<td class="line x" title="208:235	(x.p,x1.p,x2.p,,xk.p), (xr.p,x.p,x1.p,x2.p,,xk.p) and (x.w,x.p,x1.p,x2.p,,xk.p), (x.w,xr.p,x.p,x1.p,x2.p,,xk.p) if x.w is a verb Table 1: Features defined on the context of operation model accuracy% Shen and Joshi, 2005 89.3 level-0 dependency 90.3 level-1 dependency 90.5 Table 2: Experiments on Sec." ></td>
	<td class="line x" title="209:235	23 of the LTAG Treebank dency is comparable to the numbers of the previous best systems on dependency extracted from PTB with Magermans rules, for example, 90.3% in (Yamada and Matsumoto, 2003) and 90.9% in (McDonald et al., 2005)." ></td>
	<td class="line x" title="210:235	However, their experiments are on the PTB, while ours is on the LTAG corpus." ></td>
	<td class="line x" title="211:235	It should be noted that it is more difficult to learn LTAG dependencies." ></td>
	<td class="line x" title="212:235	Theoretically, the LTAG dependencies reveal deeper relations." ></td>
	<td class="line x" title="213:235	Adjunction can lead to non-projective dependencies, and the dependencies defined on predicate adjunction are linguistically more motivated, as shown in the examples in Figure 5 and 7." ></td>
	<td class="line x" title="214:235	The explicit representation of predicate coordination also provides deeper relations." ></td>
	<td class="line x" title="215:235	For example, in Figure 6, the LTAG dependency contains resist  rearing and resist  saluting, while the Magermans dependency only contains resist  rearing." ></td>
	<td class="line x" title="216:235	The explicit representation of predicate coordination will help to solve for the dependencies for shared arguments." ></td>
	<td class="line x" title="217:235	6 Discussion In our approach, each fragment in the graph is associated with a hidden structure, which means that we cannot reduce it to a labelling task." ></td>
	<td class="line x" title="218:235	Therefore, the problem of interest to us is different from previous 502 work on graphical models, such as CRF (Lafferty et al., 2001) and MMMN (Taskar et al., 2003)." ></td>
	<td class="line x" title="219:235	McAllester et al.(2004) introduced Case-Factor Diagram (CFD) to transform a graph based construction problem to a labeling problem." ></td>
	<td class="line x" title="221:235	However, adjunction, prediction coordination, and long distance dependencies in LTAG dependency parsing make it difficult to implement." ></td>
	<td class="line x" title="222:235	Our approach provides a novel alternative to CFD." ></td>
	<td class="line x" title="223:235	Our learning algorithm stems from Perceptron training in (Collins, 2002)." ></td>
	<td class="line pc" title="224:235	Variants of this method have been successfully used in many NLP tasks, like shallow processing (Daume III and Marcu, 2005), parsing (Collins and Roark, 2004; Shen and Joshi, 2005) and word alignment (Moore, 2005)." ></td>
	<td class="line x" title="225:235	Theoretical justification for those algorithms can be applied to our training algorithm in a similar way." ></td>
	<td class="line x" title="226:235	In our algorithm, dependency is defined on complicated hidden structures instead of on a graph." ></td>
	<td class="line x" title="227:235	Thus long distance dependency in a graph becomes local in hidden structures, which is desirable from linguistic considerations." ></td>
	<td class="line x" title="228:235	The search strategy of our bidirectional dependency parser is similar to that of the bidirectional CFG parser in (Satta and Stock, 1994; Ageno and Rodrguez, 2001; Kay, 1989)." ></td>
	<td class="line x" title="229:235	A unique contribution of this paper is that selection of path and decisions about action are trained simultaneously with discriminative learning." ></td>
	<td class="line x" title="230:235	In this way, we can employ context information more effectively." ></td>
	<td class="line x" title="231:235	7 Conclusion In this paper, we introduced bidirectional incremental parsing, a new architecture of parsing." ></td>
	<td class="line x" title="232:235	We proposed a novel algorithm for graph-based incremental construction, and applied this algorithm to LTAG dependency parsing, revealing deep relations, which are unavailable in other approaches and difficult to learn." ></td>
	<td class="line x" title="233:235	We evaluated the parser on an LTAG Treebank." ></td>
	<td class="line x" title="234:235	Experimental results showed significant improvement over the previous best system." ></td>
	<td class="line x" title="235:235	Incremental construction can be applied to other structure learning problems of high computational complexity, for example, such as machine translation and semantic parsing." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D08-1059
A Tale of Two Parsers: Investigating and Combining Graph-based and Transition-based Dependency Parsing
Zhang, Yue;Clark, Stephen;"></td>
	<td class="line x" title="1:188	Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 562571, Honolulu, October 2008." ></td>
	<td class="line x" title="2:188	c2008 Association for Computational Linguistics A Tale of Two Parsers: investigating and combining graph-based and transition-based dependency parsing using beam-search Yue Zhang and Stephen Clark Oxford University Computing Laboratory Wolfson Building, Parks Road Oxford OX1 3QD, UK {yue.zhang,stephen.clark}@comlab.ox.ac.uk Abstract Graph-based and transition-based approaches to dependency parsing adopt very different views of the problem, each view having its own strengths and limitations." ></td>
	<td class="line x" title="3:188	We study both approaches under the framework of beamsearch." ></td>
	<td class="line x" title="4:188	By developing a graph-based and a transition-based dependency parser, we show that a beam-search decoder is a competitive choice for both methods." ></td>
	<td class="line x" title="5:188	More importantly, we propose a beam-search-based parser that combines both graph-based and transitionbased parsing into a single system for training and decoding, showing that it outperforms both the pure graph-based and the pure transition-based parsers." ></td>
	<td class="line x" title="6:188	Testing on the English and Chinese Penn Treebank data, the combined system gave state-of-the-art accuracies of 92.1% and 86.2%, respectively." ></td>
	<td class="line x" title="7:188	1 Introduction Graph-based (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras et al., 2006) and transition-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) parsing algorithms offer two different approaches to data-driven dependency parsing." ></td>
	<td class="line x" title="8:188	Given an input sentence, a graph-based algorithm finds the highest scoring parse tree from all possible outputs, scoring each complete tree, while a transition-based algorithm builds a parse by a sequence of actions, scoring each action individually." ></td>
	<td class="line x" title="9:188	The terms graph-based and transition-based were used by McDonald and Nivre (2007) to describe the difference between MSTParser (McDonald and Pereira, 2006), which is a graph-based parser with an exhaustive search decoder, and MaltParser (Nivre et al., 2006), which is a transition-based parser with a greedy search decoder." ></td>
	<td class="line x" title="10:188	In this paper, we do not differentiate graph-based and transitionbased parsers by their search algorithms: a graphbased parser can use an approximate decoder while a transition-based parser is not necessarily deterministic." ></td>
	<td class="line x" title="11:188	To make the concepts clear, we classify the two types of parser by the following two criteria: 1." ></td>
	<td class="line x" title="12:188	whether or not the outputs are built by explicit transition-actions, such as Shift and Reduce; 2." ></td>
	<td class="line x" title="13:188	whether it is dependency graphs or transitionactions that the parsing model assigns scores to." ></td>
	<td class="line x" title="14:188	By this classification, beam-search can be applied to both graph-based and transition-based parsers." ></td>
	<td class="line x" title="15:188	Representative of each method, MSTParser and MaltParser gave comparable accuracies in the CoNLL-X shared task (Buchholz and Marsi, 2006)." ></td>
	<td class="line x" title="16:188	However, they make different types of errors, which can be seen as a reflection of their theoretical differences (McDonald and Nivre, 2007)." ></td>
	<td class="line x" title="17:188	MSTParser has the strength of exact inference, but its choice of features is constrained by the requirement of efficient dynamic programming." ></td>
	<td class="line x" title="18:188	MaltParser is deterministic, yet its comparatively larger feature range is an advantage." ></td>
	<td class="line x" title="19:188	By comparing the two, three interesting research questions arise: (1) how to increase the flexibility in defining features for graph-based parsing; (2) how to add search to transition-based parsing; and (3) how to combine the two parsing approaches so that the strengths of each are utilized." ></td>
	<td class="line x" title="20:188	In this paper, we study these questions under one framework: beam-search." ></td>
	<td class="line pc" title="21:188	Beam-search has been successful in many NLP tasks (Koehn et al., 2003; 562 Inputs: training examples (xi,yi) Initialization: set vectorw = 0 Algorithm: // R training iterations; N examples for t = 1R, i = 1N: zi = argmaxyGEN(xi) (y) vectorw if zi negationslash= yi: vectorw = vectorw + (yi)(zi) Outputs: vectorw Figure 1: The perceptron learning algorithm Collins and Roark, 2004), and can achieve accuracy that is close to exact inference." ></td>
	<td class="line x" title="22:188	Moreover, a beamsearch decoder does not impose restrictions on the search problem in the way that an exact inference decoder typically does, such as requiring the optimal subproblem property for dynamic programming, and therefore enables a comparatively wider range of features for a statistical system." ></td>
	<td class="line x" title="23:188	We develop three parsers." ></td>
	<td class="line x" title="24:188	Firstly, using the same features as MSTParser, we develop a graph-based parser to examine the accuracy loss from beamsearch compared to exact-search, and the accuracy gain from extra features that are hard to encode for exact inference." ></td>
	<td class="line x" title="25:188	Our conclusion is that beamsearch is a competitive choice for graph-based parsing." ></td>
	<td class="line x" title="26:188	Secondly, using the transition actions from MaltParser, we build a transition-based parser and show that search has a positive effect on its accuracy compared to deterministic parsing." ></td>
	<td class="line x" title="27:188	Finally, we show that by using a beam-search decoder, we are able to combine graph-based and transition-based parsing into a single system, with the combined system significantly outperforming each individual system." ></td>
	<td class="line x" title="28:188	In experiments with the English and Chinese Penn Treebank data, the combined parser gave 92.1% and 86.2% accuracy, respectively, which are comparable to the best parsing results for these data sets, while the Chinese accuracy outperforms the previous best reported by 1.8%." ></td>
	<td class="line x" title="29:188	In line with previous work on dependency parsing using the Penn Treebank, we focus on projective dependency parsing." ></td>
	<td class="line x" title="30:188	2 The graph-based parser Following MSTParser (McDonald et al., 2005; McDonald and Pereira, 2006), we define the graphVariables: agenda  the beam for state items item  partial parse tree output  a set of output items index,prev  word indexes Input: x  POS-tagged input sentence." ></td>
	<td class="line x" title="31:188	Initialization: agenda = [] Algorithm: for index in 1x.length(): clear output for item in agenda: // for all prev words that can be linked with // the current word at index prev = index1 while prev negationslash= 0: // while prev is valid // add link making prev parent of index newitem = item // duplicate item newitem.link(prev, index) // modify output.append(newitem) // record // if prev does not have a parent word, // add link making index parent of prev if item.parent(prev) == 0: item.link(index, prev) // modify output.append(item) // record prev = the index of the first word before prev whose parent does not exist or is on its left; 0 if no match clear agenda put the best items from output to agenda Output: the best item in agenda Figure 2: A beam-search decoder for graph-based parsing, developed from the deterministic Covington algorithm for projective parsing (Covington, 2001)." ></td>
	<td class="line x" title="32:188	based parsing problem as finding the highest scoring tree y from all possible outputs given an input x: F(x) = argmax yGEN(x) Score(y) where GEN(x) denotes the set of possible parses for the input x. To repeat our earlier comments, in this paper we do not consider the method of finding the argmax to be part of the definition of graph-based parsing, only the fact that the dependency graph itself is being scored, and factored into scores attached to the dependency links." ></td>
	<td class="line x" title="33:188	The score of an output parse y is given by a linear model: Score(y) = (y) vectorw 563 where (y) is the global feature vector from y and vectorw is the weight vector of the model." ></td>
	<td class="line x" title="34:188	We use the discriminative perceptron learning algorithm (Collins, 2002; McDonald et al., 2005) to train the values of vectorw." ></td>
	<td class="line x" title="35:188	The algorithm is shown in Figure 1." ></td>
	<td class="line x" title="36:188	Averaging parameters is a way to reduce overfitting for perceptron training (Collins, 2002), and is applied to all our experiments." ></td>
	<td class="line x" title="37:188	While the MSTParser uses exact-inference (Eisner, 1996), we apply beam-search to decoding." ></td>
	<td class="line x" title="38:188	This is done by extending the deterministic Covington algorithm for projective dependency parsing (Covington, 2001)." ></td>
	<td class="line x" title="39:188	As shown in Figure 2, the decoder works incrementally, building a state item (i.e. partial parse tree) word by word." ></td>
	<td class="line x" title="40:188	When each word is processed, links are added between the current word and its predecessors." ></td>
	<td class="line x" title="41:188	Beam-search is applied by keeping the B best items in the agenda at each processing stage, while partial candidates are compared by scores from the graph-based model, according to partial graph up to the current word." ></td>
	<td class="line x" title="42:188	Before decoding starts, the agenda contains an empty sentence." ></td>
	<td class="line x" title="43:188	At each processing stage, existing partial candidates from the agenda are extended in all possible ways according to the Covington algorithm." ></td>
	<td class="line x" title="44:188	The top B newly generated candidates are then put to the agenda." ></td>
	<td class="line x" title="45:188	After all input words are processed, the best candidate output from the agenda is taken as the final output." ></td>
	<td class="line x" title="46:188	The projectivity of the output dependency trees is guaranteed by the incremental Covington process." ></td>
	<td class="line x" title="47:188	The time complexity of this algorithm is O(n2), where n is the length of the input sentence." ></td>
	<td class="line oc" title="48:188	During training, the early update strategy of Collins and Roark (2004) is used: when the correct state item falls out of the beam at any stage, parsing is stopped immediately, and the model is updated using the current best partial item." ></td>
	<td class="line x" title="49:188	The intuition is to improve learning by avoiding irrelevant information: when all the items in the current agenda are incorrect, further parsing steps will be irrelevant because the correct partial output no longer exists in the candidate ranking." ></td>
	<td class="line x" title="50:188	Table 1 shows the feature templates from the MSTParser (McDonald and Pereira, 2006), which are defined in terms of the context of a word, its parent and its sibling." ></td>
	<td class="line x" title="51:188	To give more templates, features from templates 1  5 are also conjoined with 1 Parent word (P) Pw; Pt; Pwt 2 Child word (C) Cw; Ct; Cwt 3 P and C PwtCwt; PwtCw; PwCwt; PwtCt; PtCwt; PwCw; PtCt 4 A tag Bt PtBtCt between P, C 5 Neighbour words PtPLtCtCLt; of P, C, PtPLtCtCRt; left (PL/CL) PtPRtCtCLt; and right (PR/CR) PtPRtCtCRt; PtPLtCLt; PtPLtCRt; PtPRtCLt; PtPRtCRt; PLtCtCLt; PLtCtCRt; PRtCtCLt; PRtCtCRt; PtCtCLt; PtCtCRt; PtPLtCt; PtPRtCt 6 sibling (S) of C CwSw; CtSt; CwSt; CtSw; PtCtSt; Table 1: Feature templates from MSTParser w  word; t  POS-tag." ></td>
	<td class="line x" title="52:188	1 leftmost (CLC) and PtCtCLCt; rightmost (CRC) PtCtCRCt children of C 2 left (la) and right (ra) Ptla; Ptra; arity of P Pwtla; Pwtra Table 2: Additional feature templates for the graph-based parser the link direction and distance, while features from template 6 are also conjoined with the direction and distance between the child and its sibling." ></td>
	<td class="line x" title="53:188	Here distance refers to the difference between word indexes." ></td>
	<td class="line x" title="54:188	We apply all these feature templates to the graph-based parser." ></td>
	<td class="line x" title="55:188	In addition, we define two extra feature templates (Table 2) that capture information about grandchildren and arity (i.e. the number of children to the left or right)." ></td>
	<td class="line x" title="56:188	These features are not conjoined with information about direction and distance." ></td>
	<td class="line x" title="57:188	They are difficult to include in an efficient dynamic programming decoder, but easy to include in a beam-search decoder." ></td>
	<td class="line x" title="58:188	564 Figure 3: Feature context for the transition-based algorithm 3 The transition-based parser We develop our transition-based parser using the transition model of the MaltParser (Nivre et al., 2006), which is characterized by the use of a stack and four transition actions: Shift, ArcRight, ArcLeft and Reduce." ></td>
	<td class="line x" title="59:188	An input sentence is processed from left to right, with an index maintained for the current word." ></td>
	<td class="line x" title="60:188	Initially empty, the stack is used throughout the parsing process to store unfinished words, which are the words before the current word that may still be linked with the current or a future word." ></td>
	<td class="line x" title="61:188	The Shift action pushes the current word to the stack and moves the current index to the next word." ></td>
	<td class="line x" title="62:188	The ArcRight action adds a dependency link from the stack top to the current word (i.e. the stack top becomes the parent of the current word), pushes the current word on to the stack, and moves the current index to the next word." ></td>
	<td class="line x" title="63:188	The ArcLeft action adds a dependency link from the current word to the stack top, and pops the stack." ></td>
	<td class="line x" title="64:188	The Reduce action pops the stack." ></td>
	<td class="line x" title="65:188	Among the four transition actions, Shift and ArcRight push a word on to the stack while ArcLeft and Reduce pop the stack; Shift and ArcRight read the next input word while ArcLeft and ArcRight add a link to the output." ></td>
	<td class="line x" title="66:188	By repeated application of these actions, the parser reads through the input and builds a parse tree." ></td>
	<td class="line x" title="67:188	The MaltParser works deterministically." ></td>
	<td class="line x" title="68:188	At each step, it makes a single decision and chooses one of the four transition actions according to the current context, including the next input words, the stack and the existing links." ></td>
	<td class="line x" title="69:188	As illustrated in Figure 3, the contextual information consists of the top of stack (ST), the parent (STP) of ST, the leftmost (STLC) and rightmost child (STRC) of ST, the current word (N0), the next three words from the input (N1, N2, N3) and the leftmost child of N0 (N0LC)." ></td>
	<td class="line x" title="70:188	Given the context s, the next action T is decided as follows: T(s) = argmax TACTION Score(T,s) where ACTION = {Shift, ArcRight, ArcLeft, Reduce}." ></td>
	<td class="line x" title="71:188	One drawback of deterministic parsing is error propagation, since once an incorrect action is made, the output parse will be incorrect regardless of the subsequent actions." ></td>
	<td class="line x" title="72:188	To reduce such error propagation, a parser can keep track of multiple candidate outputs and avoid making decisions too early." ></td>
	<td class="line x" title="73:188	Suppose that the parser builds a set of candidates GEN(x) for the input x, the best output F(x) can be decided by considering all actions: F(x) = argmax yGEN(x) summationtext Tprimeact(y) Score(T prime,sTprime) Here Tprime represents one action in the sequence (act(y)) by which y is built, and sTprime represents the corresponding context when Tprime is taken." ></td>
	<td class="line x" title="74:188	Our transition-based algorithm keeps B different sequences of actions in the agenda, and chooses the one having the overall best score as the final parse." ></td>
	<td class="line x" title="75:188	Pseudo code for the decoding algorithm is shown in Figure 4." ></td>
	<td class="line x" title="76:188	Here each state item contains a partial parse tree as well as a stack configuration, and state items are built incrementally by transition actions." ></td>
	<td class="line x" title="77:188	Initially the stack is empty, and the agenda contains an empty sentence." ></td>
	<td class="line x" title="78:188	At each processing stage, one transition action is applied to existing state items as a step to build the final parse." ></td>
	<td class="line x" title="79:188	Unlike the MaltParser, which makes a decision at each stage, our transitionbased parser applies all possible actions to each existing state item in the agenda to generate new items; then from all the newly generated items, it takes the B with the highest overall score and puts them onto the agenda." ></td>
	<td class="line x" title="80:188	In this way, some ambiguity is retained for future resolution." ></td>
	<td class="line x" title="81:188	Note that the number of transition actions needed to build different parse trees can vary." ></td>
	<td class="line x" title="82:188	For example, the three-word sentence A B C can be parsed by the sequence of three actions Shift ArcRight ArcRight (B modifies A; C modifies B) or the sequence of four actions Shift ArcLeft Shift ArcRight (both A and C modifies B)." ></td>
	<td class="line x" title="83:188	To ensure that all final state items are built by the same number of transition actions, we require that the final state 565 Variables: agenda  the beam for state items item  (partial tree, stack config) output  a set of output items index  iteration index Input: x  POS-tagged input sentence." ></td>
	<td class="line x" title="84:188	Initialization: agenda = [(, [])] Algorithm: for index in 1  2x.length() 1: clear output for item in agenda: // when all input words have been read, the // parse tree has been built; only pop." ></td>
	<td class="line x" title="85:188	if item.length() == x.length(): if item.stacksize() > 1: item.Reduce() output.append(item) // when some input words have not been read else: if item.lastaction() negationslash= Reduce: newitem = item newitem.Shift() output.append(newitem) if item.stacksize() > 0: newitem = item newitem.ArcRight() output.append(newitem) if (item.parent(item.stacktop())==0): newitem = item newitem.ArcLeft() output.append(newitem) else: newitem = item newitem.Reduce() output.append(newitem) clear agenda transfer the best items from output to agenda Output: the best item in agenda Figure 4: A beam-search decoding algorithm for transition-based parsing items must 1) have fully-built parse trees; and 2) have only one root word left on the stack." ></td>
	<td class="line x" title="86:188	In this way, popping actions should be made even after a complete parse tree is built, if the stack still contains more than one word." ></td>
	<td class="line x" title="87:188	Now because each word excluding the root must be pushed to the stack once and popped off once during the parsing process, the number of actions Inputs: training examples (xi,yi) Initialization: set vectorw = 0 Algorithm: // R training iterations; N examples for t = 1R, i = 1N: zi = argmaxyGEN(xi)summationtextTprimeact(yi) (Tprime,cprime) vectorw if zi negationslash= yi: vectorw = vectorw +summationtextTprimeact(yi) (Tprime,cTprime) summationtextTprimeact(zi) (Tprime,cTprime) Outputs: vectorw Figure 5: the perceptron learning algorithm for the transition-based parser 1 stack top STwt; STw; STt 2 current word N0wt; N0w; N0t 3 next word N1wt; N1w; N1t 4 ST and N0 STwtN0wt; STwtN0w; STwN0wt; STwtN0t; STtN0wt; STwN0w; STtN0t 5 POS bigram N0tN1t 6 POS trigrams N0tN1tN2t; STtN0tN1t; STPtSTtN0t; STtSTLCtN0t; STtSTRCtN0t; STtN0tN0LCt 7 N0 word N0wN1tN2t; STtN0wN1t; STPtSTtN0w; STtSTLCtN0w; STtSTRCtN0w; STtN0wN0LCt Table 3: Feature templates for the transition-based parser w  word; t  POS-tag." ></td>
	<td class="line x" title="88:188	needed to parse a sentence is always 2n  1, where n is the length of the sentence." ></td>
	<td class="line x" title="89:188	Therefore, the decoder has linear time complexity, given a fixed beam size." ></td>
	<td class="line x" title="90:188	Because the same transition actions as the MaltParser are used to build each item, the projectivity of the output dependency tree is ensured." ></td>
	<td class="line x" title="91:188	We use a linear model to score each transition action, given a context: Score(T,s) = (T,s) vectorw (T,s) is the feature vector extracted from the action T and the context s, and vectorw is the weight vector." ></td>
	<td class="line x" title="92:188	Features are extracted according to the templates shown in Table 3, which are based on the context in Figure 3." ></td>
	<td class="line x" title="93:188	Note that our feature definitions are similar to those used by MaltParser, but rather than using a kernel function with simple features (e.g. STw, 566 N0t, but not STwt or STwN0w), we combine features manually." ></td>
	<td class="line x" title="94:188	As with the graph-based parser, we use the discriminative perceptron (Collins, 2002) to train the transition-based model (see Figure 5)." ></td>
	<td class="line x" title="95:188	It is worth noticing that, in contrast to MaltParser, which trains each action decision individually, our training algorithm globally optimizes all action decisions for a parse." ></td>
	<td class="line x" title="96:188	Again, early update and averaging parameters are applied to the training process." ></td>
	<td class="line x" title="97:188	4 The combined parser The graph-based and transition-based approaches adopt very different views of dependency parsing." ></td>
	<td class="line x" title="98:188	McDonald and Nivre (2007) showed that the MSTParser and MaltParser produce different errors." ></td>
	<td class="line x" title="99:188	This observation suggests a combined approach: by using both graph-based information and transition-based information, parsing accuracy can be improved." ></td>
	<td class="line x" title="100:188	The beam-search framework we have developed facilitates such a combination." ></td>
	<td class="line x" title="101:188	Our graph-based and transition-based parsers share many similarities." ></td>
	<td class="line x" title="102:188	Both build a parse tree incrementally, keeping an agenda of comparable state items." ></td>
	<td class="line x" title="103:188	Both rank state items by their current scores, and use the averaged perceptron with early update for training." ></td>
	<td class="line x" title="104:188	The key differences are the scoring models and incremental parsing processes they use, which must be addressed when combining the parsers." ></td>
	<td class="line x" title="105:188	Firstly, we combine the graph-based and the transition-based score models simply by summation." ></td>
	<td class="line x" title="106:188	This is possible because both models are global and linear." ></td>
	<td class="line x" title="107:188	In particular, the transition-based model can be written as: ScoreT(y) =summationtextTprimeact(y) Score(Tprime,sTprime) =summationtextTprimeact(y) (Tprime,sTprime) vectorwT = vectorwT summationtextTprimeact(y) (Tprime,sTprime) If we takesummationtextTprimeact(y) (Tprime,sTprime) as the global feature vector T(y), we have: ScoreT(y) = T(y) vectorwT which has the same form as the graph-based model: ScoreG(y) = G(y) vectorwG Sections Sentences Words Training 221 39,832 950,028 Dev 22 1,700 40,117 Test 23 2,416 56,684 Table 4: The training, development and test data from PTB We therefore combine the two models to give: ScoreC(y) = ScoreG(y) + ScoreT(y) = G(y) vectorwG + T(y) vectorwT Concatenating the feature vectors G(y) and T(y) to give a global feature vector C(y), and the weight vectors vectorwG and vectorwT to give a weight vector vectorwC, the combined model can be written as: ScoreC(y) = C(y) vectorwC which is a linear model with exactly the same form as both sub-models, and can be trained with the perceptron algorithm in Figure 1." ></td>
	<td class="line x" title="108:188	Because the global feature vectors from the sub models are concatenated, the feature set for the combined model is the union of the sub model feature sets." ></td>
	<td class="line x" title="109:188	Second, the transition-based decoder can be used for the combined system." ></td>
	<td class="line x" title="110:188	Both the graph-based decoder in Figure 2 and the transition-based decoder in Figure 4 construct a parse tree incrementally." ></td>
	<td class="line x" title="111:188	However, the graph-based decoder works on a per-word basis, adding links without using transition actions, and so is not appropriate for the combined model." ></td>
	<td class="line x" title="112:188	The transition-based algorithm, on the other hand, uses state items which contain partial parse trees, and so provides all the information needed by the graph-based parser (i.e. dependency graphs), and hence the combined system." ></td>
	<td class="line x" title="113:188	In summary, we build the combined parser by using a global linear model, the union of feature templates and the decoder from the transition-based parser." ></td>
	<td class="line x" title="114:188	5 Experiments We evaluate the parsers using the English and Chinese Penn Treebank corpora." ></td>
	<td class="line x" title="115:188	The English data is prepared by following McDonald et al.(2005)." ></td>
	<td class="line x" title="117:188	Bracketed sentences from the Penn Treebank (PTB) 3 are split into training, development and test sets 567 Figure 6: The influence of beam size on the transitionbased parser, using the development data X-axis: number of training iterations Y-axis: word precision as shown in Table 4, and then translated into dependency structures using the head-finding rules from Yamada and Matsumoto (2003)." ></td>
	<td class="line x" title="118:188	Before parsing, POS tags are assigned to the input sentence using our reimplementation of the POStagger from Collins (2002)." ></td>
	<td class="line x" title="119:188	Like McDonald et al.(2005), we evaluate the parsing accuracy by the precision of lexical heads (the percentage of input words, excluding punctuation, that have been assigned the correct parent) and by the percentage of complete matches, in which all words excluding punctuation have been assigned the correct parent." ></td>
	<td class="line x" title="121:188	5.1 Development experiments Since the beam size affects all three parsers, we study its influence first; here we show the effect on the transition-based parser." ></td>
	<td class="line x" title="122:188	Figure 6 shows different accuracy curves using the development data, each with a different beam size B. The X-axis represents the number of training iterations, and the Y-axis the precision of lexical heads." ></td>
	<td class="line x" title="123:188	The parsing accuracy generally increases as the beam size increases, while the quantity of increase becomes very small when B becomes large enough." ></td>
	<td class="line x" title="124:188	The decoding times after the first training iteration are 10.2s, 27.3s, 45.5s, 79.0s, 145.4s, 261.3s and 469.5s, respectively, when B = 1,2,4,8,16,32,64." ></td>
	<td class="line x" title="125:188	Word Complete MSTParser 1 90.7 36.7 Graph [M] 91.2 40.8 Transition 91.4 41.8 Graph [MA] 91.4 42.5 MSTParser 2 91.5 42.1 Combined [TM] 92.0 45.0 Combined [TMA] 92.1 45.4 Table 5: Accuracy comparisons using PTB 3 In the rest of the experiments, we set B = 64 in order to obtain the highest possible accuracy." ></td>
	<td class="line x" title="126:188	When B = 1, the transition-based parser becomes a deterministic parser." ></td>
	<td class="line x" title="127:188	By comparing the curves when B = 1 and B = 2, we can see that, while the use of search reduces the parsing speed, it improves the quality of the output parses." ></td>
	<td class="line x" title="128:188	Therefore, beam-search is a reasonable choice for transitionbased parsing." ></td>
	<td class="line x" title="129:188	5.2 Accuracy comparisons The test accuracies are shown in Table 5, where each row represents a parsing model." ></td>
	<td class="line x" title="130:188	Rows MSTParser 1/2 show the first-order (using feature templates 1  5 from Table 1) (McDonald et al., 2005) and secondorder (using all feature templates from Table 1) (McDonald and Pereira, 2006) MSTParsers, as reported by the corresponding papers." ></td>
	<td class="line x" title="131:188	Rows Graph [M] and Graph [MA] represent our graph-based parser using features from Table 1 and Table 1 + Table 2, respectively; row Transition represents our transition-based parser; and rows Combined [TM] and Combined [TMA] represent our combined parser using features from Table 3 + Table 1 and Table 3 + Table 1 + Table 2, respectively." ></td>
	<td class="line x" title="132:188	Columns Word and Complete show the precision of lexical heads and complete matches, respectively." ></td>
	<td class="line x" title="133:188	As can be seen from the table, beam-search reduced the head word accuracy from 91.5%/42.1% (MSTParser 2) to 91.2%/40.8% (Graph [M]) with the same features as exact-inference." ></td>
	<td class="line x" title="134:188	However, with only two extra feature templates from Table 2, which are not conjoined with direction or distance information, the accuracy is improved to 91.4%/42.5% (Graph [MA])." ></td>
	<td class="line x" title="135:188	This improvement can be seen as a benefit of beam-search, which allows the definition of more global features." ></td>
	<td class="line x" title="136:188	568 Sections Sentences Words Training 001815; 16,118 437,859 10011136 Dev 886931; 804 20,453 11481151 Test 816885; 1,915 50,319 11371147 Table 6: Training, development and test data from CTB Non-root Root Comp." ></td>
	<td class="line x" title="137:188	Graph [MA] 83.86 71.38 29.82 Duan 2007 84.36 73.70 32.70 Transition 84.69 76.73 32.79 Combined [TM] 86.13 77.04 35.25 Combined [TMA] 86.21 76.26 34.41 Table 7: Test accuracies with CTB 5 data The combined parser is tested with various sets of features." ></td>
	<td class="line x" title="138:188	Using only graph-based features in Table 1, it gave 88.6% accuracy, which is much lower than 91.2% from the graph-based parser using the same features (Graph [M])." ></td>
	<td class="line x" title="139:188	This can be explained by the difference between the decoders." ></td>
	<td class="line x" title="140:188	In particular, the graph-based model is unable to score the actions Reduce and Shift, since they do not modify the parse tree." ></td>
	<td class="line x" title="141:188	Nevertheless, the score serves as a reference for the effect of additional features in the combined parser." ></td>
	<td class="line x" title="142:188	Using both transition-based features and graphbased features from the MSTParser (Combined [TM]), the combined parser achieved 92.0% perword accuracy, which is significantly higher than the pure graph-based and transition-based parsers." ></td>
	<td class="line x" title="143:188	Additional graph-based features further improved the accuracy to 92.1%/45.5%, which is the best among all the parsers compared.1 5.3 Parsing Chinese We use the Penn Chinese Treebank (CTB) 5 for experimental data." ></td>
	<td class="line x" title="144:188	Following Duan et al.(2007), we 1A recent paper, Koo et al.(2008) reported parent-prediction accuracy of 92.0% using a graph-based parser with a different (larger) set of features (Carreras, 2007)." ></td>
	<td class="line x" title="147:188	By applying separate word cluster information, Koo et al.(2008) improved the accuracy to 93.2%, which is the best known accuracy on the PTB data." ></td>
	<td class="line x" title="149:188	We excluded these from Table 5 because our work is not concerned with the use of such additional knowledge." ></td>
	<td class="line x" title="150:188	split the corpus into training, development and test data as shown in Table 6, and use the head-finding rules in Table 8 in the Appendix to turn the bracketed sentences into dependency structures." ></td>
	<td class="line x" title="151:188	Most of the head-finding rules are from Sun and Jurafsky (2004), while we added rules to handle NN and FRAG, and a default rule to use the rightmost node as the head for the constituent that are not listed." ></td>
	<td class="line x" title="152:188	Like Duan et al.(2007), we use gold-standard POS-tags for the input." ></td>
	<td class="line x" title="154:188	The parsing accuracy is evaluated by the percentage of non-root words that have been assigned the correct head, the percentage of correctly identified root words, and the percentage of complete matches, all excluding punctuation." ></td>
	<td class="line x" title="155:188	The accuracies are shown in Table 7." ></td>
	<td class="line x" title="156:188	Rows Graph [MA], Transition, Combined [TM] and Combined [TMA] show our models in the same way as for the English experiments from Section 5.2." ></td>
	<td class="line x" title="157:188	Row Duan 2007 represents the transition-based model from Duan et al.(2007), which applies beamsearch to the deterministic model from Yamada and Matsumoto (2003), and achieved the previous best accuracy on the data." ></td>
	<td class="line x" title="159:188	Our observations on parsing Chinese are essentially the same as for English." ></td>
	<td class="line x" title="160:188	Our combined parser outperforms both the pure graph-based and the pure transition-based parsers." ></td>
	<td class="line x" title="161:188	It gave the best accuracy we are aware of for dependency parsing using CTB." ></td>
	<td class="line x" title="162:188	6 Related work Our graph-based parser is derived from the work of McDonald and Pereira (2006)." ></td>
	<td class="line x" title="163:188	Instead of performing exact inference by dynamic programming, we incorporated the linear model and feature templates from McDonald and Pereira (2006) into our beam-search framework, while adding new global features." ></td>
	<td class="line x" title="164:188	Nakagawa (2007) and Hall (2007) also showed the effectiveness of global features in improving the accuracy of graph-based parsing, using the approximate Gibbs sampling method and a reranking approach, respectively." ></td>
	<td class="line x" title="165:188	Our transition-based parser is derived from the deterministic parser of Nivre et al.(2006)." ></td>
	<td class="line x" title="167:188	We incorporated the transition process into our beamsearch framework, in order to study the influence of search on this algorithm." ></td>
	<td class="line x" title="168:188	Existing efforts to add search to deterministic parsing include Sagae 569 and Lavie (2006b), which applied best-first search to constituent parsing, and Johansson and Nugues (2006) and Duan et al.(2007), which applied beamsearch to dependency parsing." ></td>
	<td class="line x" title="170:188	All three methods estimate the probability of each transition action, and score a state item by the product of the probabilities of all its corresponding actions." ></td>
	<td class="line x" title="171:188	But different from our transition-based parser, which trains all transitions for a parse globally, these models train the probability of each action separately." ></td>
	<td class="line x" title="172:188	Based on the work of Johansson and Nugues (2006), Johansson and Nugues (2007) studied global training with an approximated large-margin algorithm." ></td>
	<td class="line x" title="173:188	This model is the most similar to our transition-based model, while the differences include the choice of learning and decoding algorithms, the definition of feature templates and our application of the early update strategy." ></td>
	<td class="line x" title="174:188	Our combined parser makes the biggest contribution of this paper." ></td>
	<td class="line x" title="175:188	In contrast to the models above, it includes both graph-based and transition-based components." ></td>
	<td class="line x" title="176:188	An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a), which was reported to be useful in improving dependency parsing (Hall et al., 2007)." ></td>
	<td class="line x" title="177:188	A more recent approach (Nivre and McDonald, 2008) combined MSTParser and MaltParser by using the output of one parser for features in the other." ></td>
	<td class="line x" title="178:188	Both Hall et al.(2007) and Nivre and McDonald (2008) can be seen as methods to combine separately defined models." ></td>
	<td class="line x" title="180:188	In contrast, our parser combines two components in a single model, in which all parameters are trained consistently." ></td>
	<td class="line x" title="181:188	7 Conclusion and future work We developed a graph-based and a transition-based projective dependency parser using beam-search, demonstrating that beam-search is a competitive choice for both parsing approaches." ></td>
	<td class="line x" title="182:188	We then combined the two parsers into a single system, using discriminative perceptron training and beam-search decoding." ></td>
	<td class="line x" title="183:188	The appealing aspect of the combined parser is the incorporation of two largely different views of the parsing problem, thus increasing the information available to a single statistical parser, and thereby significantly increasing the accuracy." ></td>
	<td class="line x" title="184:188	When tested using both English and Chinese dependency data, the combined parser was highly competitive compared to the best systems in the literature." ></td>
	<td class="line x" title="185:188	The idea of combining different approaches to the same problem using beam-search and a global model could be applied to other parsing tasks, such as constituent parsing, and possibly other NLP tasks." ></td>
	<td class="line x" title="186:188	Acknowledgements This work is supported by the ORS and Clarendon Fund." ></td>
	<td class="line x" title="187:188	We thank the anonymous reviewers for their detailed comments." ></td>
	<td class="line x" title="188:188	Appendix Constituent Rules ADJP r ADJP JJ AD; r ADVP r ADVP AD CS JJ NP PP P VA VV; r CLP r CLP M NN NP; r CP r CP IP VP; r DNP r DEG DNP DEC QP; r DP r M; l DP DT OD; l DVP r DEV AD VP; r FRAG r VV NR NN NT; r IP r VP IP NP; r LCP r LCP LC; r LST r CD NP QP; r NP r NP NN IP NR NT; r NN r NP NN IP NR NT; r PP l P PP; l PRN l PU; l QP r QP CLP CD; r UCP l IP NP VP; l VCD l VV VA VE; l VP l VE VC VV VNV VPT VRD VSB VCD VP; l VPT l VA VV; l VRD l VVI VA; l VSB r VV VE; r default r Table 8: Head-finding rules to extract dependency data from CTB 570" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="Data not found"></td>
	<td class="line x" title="1:148	CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 213217 Manchester, August 2008 A Puristic Approach for Joint Dependency Parsing and Semantic Role Labeling Alexander Volokh LT-lab, DFKI 66123 Saarbrcken, Germany Alexander.Volokh@dfki.de Gnter Neumann LT-lab, DFKI 66123 Saarbrcken, Germany neumann@dfki.de  Abstract We present a puristic approach for combining dependency parsing and semantic role labeling." ></td>
	<td class="line x" title="2:148	In a first step, a data-driven strict incremental deterministic parser is used to compute a single syntactic dependency structure using a MEM trained on the syntactic part of the CoNLL 2008 training corpus." ></td>
	<td class="line x" title="3:148	In a second step, a cascade of MEMs is used to identify predicates, and, for each found predicate, to identify its arguments and their types." ></td>
	<td class="line x" title="4:148	All the MEMs used here are trained only with labeled data from the CoNLL 2008 corpus." ></td>
	<td class="line x" title="5:148	We participated in the closed challenge, and obtained a labeled macro F1 for WSJ+Brown of 19.93 (20.13 on WSJ only, 18.14 on Brown)." ></td>
	<td class="line x" title="6:148	For the syntactic dependencies we got similar bad results (WSJ+Brown=16.25, WSJ= 16.22, Brown=16.47), as well as for the semantic dependencies (WSJ+Brown=22.36, WSJ=22.86, Brown=17.94)." ></td>
	<td class="line x" title="7:148	The current results of the experiments suggest that our risky puristic approach of following a strict incremental parsing approach together with the closed data-driven perspective of a joined syntactic and semantic labeling was actually too optimistic and eventually too puristic." ></td>
	<td class="line x" title="8:148	The CoNLL 2008 shared task on joint parsing of syntactic and semantic dependencies (cf.Surdeanu, 2008) offered to us an opportunity to initiate, implement and test new ideas on largescale data-driven incremental dependency parsing." ></td>
	<td class="line x" title="10:148	The topic and papers of the ACL-2004 workshop Incremental Parsing: Bringing Engi  2008." ></td>
	<td class="line x" title="11:148	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/)." ></td>
	<td class="line x" title="12:148	Some rights reserved." ></td>
	<td class="line x" title="13:148	neering and Cognition Together (accessible at http://aclweb.org/anthology-new/W/W04/#0300) present a good recent overview into the field of incremental processing from both an engineering and cognitive point of view." ></td>
	<td class="line x" title="14:148	Our particular interest is the exploration and development of strict incremental deterministic strategies as a means for fast data-driven dependency parsing of large-scale online natural language processing." ></td>
	<td class="line x" title="15:148	By strict incremental processing we mean, that the parser receives a stream of words w1 to wn word by word in left to right order, and that the parser only has information about the current word wi, and the previous words w1 to wi-1.1 By deterministic processing we mean that the parser has to decide immediately and uniquely whether and how to integrate the newly observed word wi with the already constructed (partial) dependency structure without the possibility of revising its decision at later stages." ></td>
	<td class="line x" title="16:148	The strategy is data-driven in the sense that the parsing decisions are made on basis of a statistical language model, which is trained on the syntactic part of the CoNLL 2008 training corpus." ></td>
	<td class="line x" title="17:148	The whole parsing strategy is based on Nivre (2007), but modifies it in several ways, see sec." ></td>
	<td class="line x" title="18:148	2 for details." ></td>
	<td class="line x" title="19:148	Note that there are other approaches of incremental deterministic dependency parsing that assume that the complete input string of a sentence is already given before parsing starts and that this additional right contextual information is also used as a feature source for language modeling, e.g., Nivre (2007)." ></td>
	<td class="line x" title="20:148	In light of the CoNLL 2008 shared task, this actually means that, e.g., part-of-speech tagging and lemmatization has already been performed  1 Note that in a truly strict incremental processing regime the input to the NLP system is actually a stream of signals where even the sentence segmentation is not known in advance." ></td>
	<td class="line x" title="21:148	Since in our current system, the parser receives a sentence as given input, we are less strict as we could be." ></td>
	<td class="line x" title="22:148	213 for the complete sentence before incremental parsing starts, so that this richer source of information is available for defining the feature space." ></td>
	<td class="line x" title="23:148	Since, important word-based information especially for a dependency analysis is already known for the whole sentence before parsing starts, and actually heavily used during parsing, one might wonder, what the benefit of such a weak incremental parsing approach is compared to a non-incremental approach." ></td>
	<td class="line x" title="24:148	Since, we thought that such an incremental processing perspective is a bit too wide (especially when considering the rich input of the CoNLL 2008 shared task), we wanted to explore a strict incremental strategy." ></td>
	<td class="line x" title="25:148	Semantic role labeling is considered as a postprocess that is applied on the output of the syntactic parser." ></td>
	<td class="line x" title="26:148	Following Hacioglu (2004), we consider the labeling of semantic roles as a classification problem of dependency relations into one of several semantic roles." ></td>
	<td class="line x" title="27:148	However, instead of post-processing a dependency tree firstly into a sequence of relations, as done by Hacioglu (2004), we apply a cascade of statistical models on the unmodified dependency tree in order to identify predicates, and, for each found predicate, to identify its arguments and their types." ></td>
	<td class="line x" title="28:148	All the language models used here are trained only with labeled data from the CoNLL 2008 corpus; cf.sec." ></td>
	<td class="line x" title="30:148	3 for more details." ></td>
	<td class="line x" title="31:148	Both, the syntactic parser and the semantic classifier are language independent in the sense that only information contained in the given training corpus is used (e.g., PoS tags, dependency labels, information about direction etc.), but no language specific features, e.g., no PropBank frames nor any other external language and knowledge specific sources." ></td>
	<td class="line x" title="32:148	The complete system has been designed and implemented from scratch after the announcement of the CoNLL 2008 shared task." ></td>
	<td class="line x" title="33:148	The main goal of our participation was therefore actually on being able to create some initial software implementation and baseline experimentations as a starting point for further research in the area of data-driven incremental deterministic parsing." ></td>
	<td class="line x" title="34:148	In the rest of this brief report, we will describe some more details of the syntactic and semantic component in the next two sections, followed by a description and discussion of the achieved results." ></td>
	<td class="line x" title="35:148	1 Syntactic Parsing Our syntactic dependency parser is a variant of the incremental non-projective dependency parser described in Nivre (2007)." ></td>
	<td class="line x" title="36:148	Nivres parser is incremental in the sense, that although the complete list of words of a sentence is known, construction of the dependency tree is performed strictly from left to right." ></td>
	<td class="line x" title="37:148	It uses Treebankinduced classifiers to deterministically predict the actions of the parser." ></td>
	<td class="line x" title="38:148	The classifiers are trained using support vector machines (SVM)." ></td>
	<td class="line x" title="39:148	A further interesting property of the parser is its capability to derive (a subset of) non-projective structures directly." ></td>
	<td class="line x" title="40:148	The core idea here is to exploit a function permissible(i, j, d) that returns true if and only if the dependency links i  j and j  i have a degree less than or equal to d given the dependency graph built so far." ></td>
	<td class="line x" title="41:148	A degree d=0 gives strictly projective parsing, while setting d= gives unrestricted non-projective parsing; cf.Nivre (2007) for more details." ></td>
	<td class="line x" title="43:148	The goal of this function is to restrict the call of a function link(i, j) which is a nondeterministic operation that adds the arc i  j, the arc j  i, or does nothing at all." ></td>
	<td class="line x" title="44:148	Thus the smaller the value of d is the fewer links can be drawn." ></td>
	<td class="line x" title="45:148	The function link(i, j) is directed by a trained SVM classifier that takes as input the feature representation of the dependency tree built so far and the (complete) input x = w1, , wn and outputs a decision for choosing exactly one of the three possible operations." ></td>
	<td class="line x" title="46:148	We have modified Nivres algorithm as follows: 1." ></td>
	<td class="line x" title="47:148	Instead of using classifiers learned by SVM, we are using classifiers based on Maximum Entropy Models (MEMs), cf.(Manning and Schtze, 1999).2 2." ></td>
	<td class="line x" title="49:148	Instead of using the complete input x, we only use the prefix from w1 up to the current word wi." ></td>
	<td class="line x" title="50:148	In this way, we are able to model a stricter incremental processing regime." ></td>
	<td class="line x" title="51:148	3." ></td>
	<td class="line x" title="52:148	We are using a subset of feature set described in Nivre (2007).3  In particular, we had to discard all features from Nivres set that refer to a word right to the current word in order to retain our  2 We are using the opennlp.maxent package available via http://maxent.sourceforge.net/." ></td>
	<td class="line x" title="53:148	3 We mean here all features that are explicitly described in Nivre (2007)." ></td>
	<td class="line x" title="54:148	He also mentions the use of some additional language specific features, but they are not further described, and, hence not known to us." ></td>
	<td class="line x" title="55:148	214 strict incremental behavior." ></td>
	<td class="line x" title="56:148	Additionally, we added the following features: a. Has j more children in the current dependency graph compared with the average number of children of element of same POS." ></td>
	<td class="line x" title="57:148	b. Analogously for node i c. Distance between i and j Although some results  for example Wang et al.(2006)  suggest that SVMs are actually more suitable for deterministic parsing strategies than MEMs, we used MEMs instead of SVM basically for practical reasons: 1) we already had hands-on experience with MEMs, 2) training time was much faster than SVM, and 3) the theoretical basis of MEMs should give us enough flexibility for testing with different sets of features." ></td>
	<td class="line x" title="59:148	Initial experiments applied on the same corpora as used by Nivre (2007), soon showed that our initial prototype is certainly not competitive in its current form." ></td>
	<td class="line x" title="60:148	For example, our best result on the TIGER Treebank of German (Brants et al., 2002) is 53.6% (labeled accuracy), where Nivre reports 85.90%; cf.Volokh (2008) and sec." ></td>
	<td class="line x" title="62:148	4 for more details Anyway, we decided to use it as a basis for the CoNLL 2008 shared task and to combine it with a component for semantic role labeling at least to get some indication of what went wrong." ></td>
	<td class="line x" title="63:148	2 Semantic Role Labeling On the one hand, it is clear that we should expect that our current version of the strict incremental deterministic parsing regime still returns too erroneous dependency analysis." ></td>
	<td class="line x" title="64:148	On the other hand, we decided to apply semantic role labeling on the parsers output." ></td>
	<td class="line x" title="65:148	Hence, the focus was set towards a robust strictly data-driven approach." ></td>
	<td class="line x" title="66:148	Semantic role labeling is modeled as a sequence of classifiers that follow the structure of predicates, i.e., firstly candidate predicates are identified and then the arguments are looked up." ></td>
	<td class="line x" title="67:148	Predicate and argument identification both proceed in two steps: first determine whether a word can be a predicate or argument (or not), and then, each found predicate (argument) is typed." ></td>
	<td class="line x" title="68:148	More precisely, semantic role labeling receives the output of the syntactic parser and performs the following steps in that order: 1." ></td>
	<td class="line x" title="69:148	Classify each word as being a predicate or not using a MEM-based classifier." ></td>
	<td class="line x" title="70:148	2." ></td>
	<td class="line x" title="71:148	Assign to each predicate its reading." ></td>
	<td class="line x" title="72:148	Currently, this is done on basis of the frequency readings as determined from the corpus (for unknown words, we simply assign the reading .01 to the lemma if the whole word was classified as a predicate)." ></td>
	<td class="line x" title="73:148	3." ></td>
	<td class="line x" title="74:148	For each predicate identified in a sentence, classify each word as argument for this predicate or not using a MEM-based classifier." ></td>
	<td class="line x" title="75:148	4." ></td>
	<td class="line x" title="76:148	For each argument identified for each predicate, assign its semantic role using a MEM-based classifier." ></td>
	<td class="line x" title="77:148	For step 1 the following features are used for word wi: 1) word form, 2) word lemma, 3) POS, 4) dependency type, 5) number of dependent elements in subtree of wi, 6) POS of parent, 7) dependency type of parent, 8) children or parent of word belong to prepositions, and 9) parent is predicate." ></td>
	<td class="line x" title="78:148	For step 3 the same features are used as in step 1, but 5) (for arguments the number of children is not important) and two additional features are used: 10) left/right of predicate (arguments are often to the right of its predicate), and 11) distance to predicate (arguments are not far from the predicate)." ></td>
	<td class="line x" title="79:148	Finally, for step 4 the same features are used as in step 1, but 5) and 9)." ></td>
	<td class="line x" title="80:148	3 Experiments As mentioned above, we started the development of the system from scratch with a very small team (actually only one programmer)." ></td>
	<td class="line x" title="81:148	Therefore we wanted to focus on certain aspects, totally abandoning our claims for achieving decent results for the others." ></td>
	<td class="line x" title="82:148	One of our major goals was the construction of correct syntactic trees and the recognition of the predicateargument structure a subtask which mainly corresponds to the unlabeled accuracy." ></td>
	<td class="line x" title="83:148	For that reason we reduced the scale of our experiments concerning such steps as dependency relation labeling, determining the correct reading for the predicates or the proper type of the arguments." ></td>
	<td class="line x" title="84:148	Unfortunately only the labeled accuracy was evaluated at this years task, which was very frustrating in the end." ></td>
	<td class="line x" title="85:148	3.1 Syntactic Dependencies For testing the strict incremental dependency parser we used the CoNLL 2008 shared task training and development set." ></td>
	<td class="line x" title="86:148	Our best syntactic score that we could achieve on the development data was merely unlabeled attachment score (UAL) of 45.31%." ></td>
	<td class="line x" title="87:148	However, as mentioned in sec." ></td>
	<td class="line x" title="88:148	2, we used a set of features proposed by Nivre, 215 which contains 5 features relying on the dependency types." ></td>
	<td class="line x" title="89:148	Since we couldnt develop a good working module for this part of the task due to the lack of time, we couldnt exploit these features." ></td>
	<td class="line x" title="90:148	Note that for this experiment and all others reported below, we used the default settings of the opennlp MEM trainer." ></td>
	<td class="line x" title="91:148	In particular this means that 100 iterations were used in all training runs and that for all experiments no tuning of parameters and smoothing was done, basically because we had no time left to exploit it in a sensible way." ></td>
	<td class="line x" title="92:148	These parts will surely be revised and improved in the future." ></td>
	<td class="line x" title="93:148	3.2 Semantic Dependencies As we describe in the sec." ></td>
	<td class="line x" title="94:148	3 our semantic module consists of 4 steps." ></td>
	<td class="line x" title="95:148	For the first step we achieve the F-score of 76.9%." ></td>
	<td class="line x" title="96:148	Whereas the verb predicates are recognized very well (average score for every verb category is almost 90%), we do badly with the noun categories." ></td>
	<td class="line x" title="97:148	Since our semantic module depends on the input produced by the syntactic parser, and is influenced by its errors, we also did a test assuming a 100% correct parse." ></td>
	<td class="line x" title="98:148	In this scenario we could achieve the F-score of 79.4%." ></td>
	<td class="line x" title="99:148	We have completely neglected the second step of the semantic task." ></td>
	<td class="line x" title="100:148	We didnt even try to do the feature engineering and to train a model for this assignment, basically because of time constraints." ></td>
	<td class="line x" title="101:148	Neither did we try to include some information about the predicate-argument structure in order to do better on this part of the task." ></td>
	<td class="line x" title="102:148	The simple assignment of the statistically most frequent reading for each predicate reduced the accuracy from 76.9% down to 69.3%." ></td>
	<td class="line x" title="103:148	In case of perfect syntactic parse the result went down from 79.4% to 71.5%." ></td>
	<td class="line x" title="104:148	Unfortunately the evaluation software doesnt provide the differentiation between the unlabeled and labeled argument recognition, which corresponds to our third and fourth steps respectively." ></td>
	<td class="line x" title="105:148	Whereas we put some effort on identifying the arguments, we didnt focus on their classification." ></td>
	<td class="line x" title="106:148	Therefore the overall best labeled attachment score for our system is 29.38%, whereas the unlabeled score is 50.74%." ></td>
	<td class="line x" title="107:148	Assuming the perfect parse the labeled score is 32.67% and the unlabeled score is 66.73%." ></td>
	<td class="line x" title="108:148	In our further work we will try to reduce this great deviation between both results." ></td>
	<td class="line x" title="109:148	3.3 Runtime performance One of the main strong sides of the strict incremental approach is its runtime performance." ></td>
	<td class="line x" title="110:148	Since we are restricted in our feature selection to the already seen space to the left of the current word, both the training and the application of our strategy are done fast." ></td>
	<td class="line x" title="111:148	The training of our MEMs for the syntactic part requires 62 minutes." ></td>
	<td class="line x" title="112:148	The training of the models for our semantic components needs 31 minutes." ></td>
	<td class="line x" title="113:148	The test run of our system for the test data from the Brown corpus (425 sentences with 7207 tokens) lasted 1 minute and 18 seconds." ></td>
	<td class="line x" title="114:148	The application on the WSJ test data (2399 sentences with 57676 tokens) took 20 minutes and 42 seconds." ></td>
	<td class="line x" title="115:148	The experiments have been performed on a computer with one Intel Pentium 1,86 Ghz processor and 1GB memory." ></td>
	<td class="line x" title="116:148	4 Results and Discussion The results of running our current version on the CoNLL 2008 shared task test data were actually a knockdown blow." ></td>
	<td class="line x" title="117:148	We participated in the closed challenge, and obtained for the complete problem a labeled macro F1 for WSJ+Brown of 19.93 (20.13 on WSJ only, 18.14 on Brown)." ></td>
	<td class="line x" title="118:148	For the syntactic dependencies we got similar bad results (WSJ+Brown = 16.25, WSJ = 16.22, Brown = 16.47), as well as for the semantic dependencies (WSJ+Brown = 22.36, WSJ = 22.86, Brown = 17.94)." ></td>
	<td class="line x" title="119:148	We see at least the following two reasons for this disastrous result: On the one hand we focused on the construction of correct syntactic trees and the recognition of the predicateargument structure which were only parts of the task." ></td>
	<td class="line x" title="120:148	On the other hand we stuck to our strict incremental approach, which greatly restricted the scope of development of our system." ></td>
	<td class="line x" title="121:148	Whereas the labeling part, which was so far considerably neglected, will surely be improved in the future, the strict incremental strategy in its current form will probably have to be revised." ></td>
	<td class="line x" title="122:148	4.1 Post-evaluation experiments We have already started beginning the improvement of our parsing system, and we briefly discuss our current findings." ></td>
	<td class="line x" title="123:148	On the technical level we already found a software bug that at least partially might explain the unexpected high difference in performance between the results obtained for the development set and the test set." ></td>
	<td class="line x" title="124:148	Correcting this error now yields an UAL of 53.45% and an LAL of 26.95% on the syntactic 216 part of the Brown test data which is a LALimprovement of about 10%." ></td>
	<td class="line x" title="125:148	On the methodological level we are studying the effects of relaxing some of the assumptions of our strict incremental parsing strategy." ></td>
	<td class="line x" title="126:148	In order to do so, we developed a separate model for predicting the unlabeled edges and a separate model for labeling them." ></td>
	<td class="line x" title="127:148	In both cases we used the same features as described in sec." ></td>
	<td class="line x" title="128:148	2, but added features that used a right-context in order to take into account the PoS-tag of the N-next words viz." ></td>
	<td class="line x" title="129:148	N=5 for the syntactic parser and N=3 for the labeling case." ></td>
	<td class="line x" title="130:148	Using both models during parsing interleaved, we obtained UAL=65.17% and LAL=28.47% on the development set." ></td>
	<td class="line x" title="131:148	We assumed that the low LAL might have been caused by a too narrow syntactic context." ></td>
	<td class="line x" title="132:148	In order to test this assumption, we decoupled the prediction of the unlabeled edges and their labeling, such that the determination of the edge labels is performed after the complete unlabeled dependency tree is computed." ></td>
	<td class="line x" title="133:148	Labeling of the dependency edges is then simply performed by running through the constructed parse trees assigning each edge the most probable dependency type." ></td>
	<td class="line x" title="134:148	This two-phase strategy achieved an LAL of 60.44% on the development set, which means an improvement of about 43%." ></td>
	<td class="line x" title="135:148	Applying the two-phase parser on the WSJ test data resulted in UAL=65.22% and LAL=62.83%; applying it on the Brown test data resulted in UAL=66.50% and LAL=61.11%, respectively." ></td>
	<td class="line x" title="136:148	Of course, these results are far from being optimal." ></td>
	<td class="line x" title="137:148	Thus, beside testing and improving our parser on the technical level, we will run further experiments for different context sizes, exploiting different settings of parameters of the classifier and feature values, and eventually testing other ML approaches." ></td>
	<td class="line x" title="138:148	The focus here will be on the development of unlabeled edge models, because it seems that an improvement here is substantial for an overall improvement." ></td>
	<td class="line x" title="139:148	For example, applying the decoupled edge labeling model directly on the given unlabeled dependency trees of the development set (i.e. we assume an UAL of 100%) gave as an LAL of 92.88%." ></td>
	<td class="line x" title="140:148	Beside this, we will also re-investigate interleaved strategies of unlabeled edge and edge labeling prediction as a basis for (mildly-) strict incremental parsing." ></td>
	<td class="line oc" title="141:148	Here, it might be useful to relax the strict linear control regime by exploring beam search strategies, e.g. along the lines of Collins and Roark (2004)." ></td>
	<td class="line x" title="142:148	5 Conclusion We have presented a puristic approach for joint dependency parsing and semantic role labeling." ></td>
	<td class="line x" title="143:148	Since, the development of our approach has been started from scratch, we didnt manage to deal with all problems." ></td>
	<td class="line x" title="144:148	Our focus was on setting up a workable backbone, and then on trying to do as much feature engineering as possible." ></td>
	<td class="line x" title="145:148	Our bad results on the CoNLL 2008 suggest that our current strategy was a bit too optimistic and risky, and that the strict incremental deterministic parsing regime seemed to have failed in its current form." ></td>
	<td class="line x" title="146:148	We are now in the process of analysis of what went wrong, and have already indicated some issues in the paper." ></td>
	<td class="line x" title="147:148	Acknowledgement The work presented here was partially supported by a research grant from the German Federal Ministry of Education, Science, Research and Technology (BMBF) to the DFKI project HyLaP, (FKZ: 01 IW F02)." ></td>
	<td class="line x" title="148:148	We thank the developers of the Opennlp.maxent software package." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1034
Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing
Roark, Brian;Bachrach, Asaf;Cardenas, Carlos;Pallier, Christophe;"></td>
	<td class="line x" title="1:221	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 324333, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:221	c 2009 ACL and AFNLP Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing Brian Roark Asaf Bachrach Carlos Cardenas and Christophe Pallier Center for Spoken Language Understanding, Oregon Health & Science University  INSERM-CEA Cognitive Neuroimaging Unit, Gif sur Yvette, France MIT roark@cslu.ogi.edu asafbac@gmail.com cardenas@mit.edu christophe@pallier.org Abstract A number of recent publications have made use of the incremental output of stochastic parsers to derive measures of high utility for psycholinguistic modeling, following the work of Hale (2001; 2003; 2006)." ></td>
	<td class="line x" title="3:221	In this paper, we present novel methods for calculating separate lexical and syntactic surprisal measures from a single incremental parser using a lexicalized PCFG." ></td>
	<td class="line x" title="4:221	We also present an approximation to entropy measures that would otherwise be intractable to calculate for a grammar of that size." ></td>
	<td class="line x" title="5:221	Empirical results demonstrate the utility of our methods in predicting human reading times." ></td>
	<td class="line x" title="6:221	1 Introduction Assessment of linguistic complexity has played an important role in psycholinguistics and neurolinguistics for a long time, from the use of mean length of utterance and related scores in child language development (Klee and Fitzgerald, 1985), to complexity scores related to reading difficulty in human sentence processing studies (Yngve, 1960; Frazier, 1985; Gibson, 1998)." ></td>
	<td class="line x" title="7:221	Operationally, such linguistic complexity scores are derivedviadeterministicmanual(human)annotation and scoring algorithms of language samples." ></td>
	<td class="line x" title="8:221	Natural language processing has been employed to automate the extraction of such measures (Sagae et al., 2005; Roark et al., 2007), which can have high utility in terms of reduction of time required to annotate and score samples." ></td>
	<td class="line x" title="9:221	More interestingly, however, novel data driven methods are being increasingly employed in this sphere, yielding language sample characterizations that require NLP in their derivation." ></td>
	<td class="line x" title="10:221	For example, scores derived from variously estimated language models have been used to evaluate and classify language samples associated with neurodevelopmental or neurodegenerative disorders (Roark et al., 2007; Solorio and Liu, 2008; Gabani et al., 2009), as well as within general studies of human sentence processing (Hale, 2001; 2003; 2006)." ></td>
	<td class="line x" title="11:221	These scores cannot feasibly be derived by hand, but rather rely on large-scale statistical models and structuredinferencealgorithmstobederived." ></td>
	<td class="line x" title="12:221	This is quickly becoming an important application of NLP, making possible new methods in the study of human language processing in both typical and impaired populations." ></td>
	<td class="line x" title="13:221	The use of broad-coverage parsing for psycholinguistic modeling has become very popular recently." ></td>
	<td class="line x" title="14:221	Hale (2001) suggested a measure (surprisal) derived from an Earley (1970) parser using a probabilistic context-free grammar (PCFG) for psycholinguistic modeling; and in later work (Hale, 2003; 2006) he suggested an alternate parser-derived measure (entropy reduction) that may also account for some human sentence processing performance." ></td>
	<td class="line x" title="15:221	Recent work continues to advocate surprisal in particular as a very useful measure for predicting processing difficulty (Boston et al., 2008a; Boston et al., 2008b; Demberg and Keller, 2008; Levy, 2008), and the measure has been derived using a variety of incremental (left-to-right) parsing strategies, including an Earley parser (Boston et al., 2008a), the Roark (2001) incremental top-down parser (Demberg and Keller, 2008), and an n-best version of the Nivre et al.(2007) incremental dependency parser (Boston et al., 2008a; 2008b)." ></td>
	<td class="line x" title="17:221	Deriving such measures by hand, even for a relatively limited set of stimuli, is not feasible, hence parsing plays a critical role in this developing psycholinguistic enterprise." ></td>
	<td class="line x" title="18:221	There is no single measure that can account for all of the factors influencing human sentence processing performance, and some of the most recent work on using parser-derived measures for psycholinguistic modeling has looked to try to derive multiple, complementary measures." ></td>
	<td class="line x" title="19:221	One of 324 the key distinctions being looked at is syntactic versus lexical expectations (Gibson, 2006)." ></td>
	<td class="line x" title="20:221	For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lexicalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)." ></td>
	<td class="line x" title="21:221	Boston et al.(2008a) capture a similar distinction by making use of an unlexicalized PCFG within an Earley parser and a fully lexicalized unlabeled dependency parser (Nivre et al., 2007)." ></td>
	<td class="line x" title="23:221	As Demberg and Keller (2008) point out, fully unlexicalized grammars ignore important lexico-syntactic information when deriving the syntactic expectations, suchassubcategorizationpreferencesofparticular verbs, which are generally accepted to impact syntactic expectations in human sentence processing (Garnsey et al., 1997)." ></td>
	<td class="line x" title="24:221	Demberg and Keller argue, based on their results, for unlexicalized surprisal instead of lexicalized surprisal." ></td>
	<td class="line x" title="25:221	Here we present a novel method for deriving separate syntactic and lexical surprisal measures from a fully lexicalized incremental parser, to allow for rich probabilistic grammars to be used to derive either measure, and demonstrate the utility of this method versus that of Demberg and Keller in empirical trials." ></td>
	<td class="line x" title="26:221	The use of large-scale lexicalized grammars presents a problem for using an Earley parser to derive surprisal or for the calculation of entropy as Hale (2003; 2006) defines it, because both methods require matrix inversion of a matrix with dimensionalitythesizeofthenon-terminalset." ></td>
	<td class="line x" title="27:221	With very large lexicalized PCFGs, the size of the nonterminal set is too large for tractable matrix inversion." ></td>
	<td class="line x" title="28:221	The use of an incremental, beam-search parser provides a tractable approximation to both measures." ></td>
	<td class="line pc" title="29:221	Incremental top-down and left-corner parsers have been shown to effectively (and efficiently) make use of non-local features from the left-context to yield very high accuracy syntactic parses (Roark, 2001; Henderson, 2003; Collins and Roark, 2004), and we will use such rich models to derive our scores." ></td>
	<td class="line x" title="30:221	In addition to teasing apart syntactic and lexical surprisal (defined explicitly in 3), we present an approximation to the full entropy that Hale (2003; 2006) used to define the entropy reduction hypothesis." ></td>
	<td class="line x" title="31:221	Such an entropy measure is derived via a predictive step, advancing the parses independently of the input, as described in 3.3." ></td>
	<td class="line x" title="32:221	We also present syntactic and lexical alternatives for this measure, and demonstrate the utility of making such a distinction for entropy as well as surprisal." ></td>
	<td class="line x" title="33:221	The purpose of this paper is threefold." ></td>
	<td class="line x" title="34:221	First, to present a careful and well-motivated decomposition of lexical and syntactic expectation-based measures from a given lexicalized PCFG." ></td>
	<td class="line x" title="35:221	Second, to explicitly document methods for calculating these and other measures from a specific incremental parser." ></td>
	<td class="line x" title="36:221	And finally, to present some empirical validation of the novel measures from real reading time trials." ></td>
	<td class="line x" title="37:221	We modified the Roark (2001) parser to calculate the discussed measures1, and the empirical results in 4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time." ></td>
	<td class="line x" title="38:221	2 Notation and preliminaries A probabilistic context-free grammar (PCFG) G = (V,T,S,P,) consists of a set of nonterminal variables V; a set of terminal items (words) T; a special start non-terminal S  V; a set of rule productions P of the form A   for A  V,   (V  T); and a function  that assigns probabilities to each rule in P such that for any given non-terminal symbol X  V,summationtext (X  ) = 1." ></td>
	<td class="line x" title="39:221	For a given rule A    P, let the functionRHSreturntheright-handsideoftherule,i.e., RHS(A  ) = ." ></td>
	<td class="line x" title="40:221	Without loss of generality, we will assume that for every rule A    P, one of two cases holds: either RHS(A  )  T or RHS(A  )  V ." ></td>
	<td class="line x" title="41:221	That is, the right-hand side sequences consist of either (1) exactly one terminal item, or (2) zero or more non-terminals." ></td>
	<td class="line x" title="42:221	Let W  Tn be a terminal string of length n, i.e., W = W1 Wn and |W| = n. Let W[i,j] denote the substring beginning at word Wi and ending at word Wj of the string." ></td>
	<td class="line x" title="43:221	Then W|W| is the last word in the string, and W[1,|W|] is the string as a whole." ></td>
	<td class="line x" title="44:221	Adjacent strings represent concatenation, i.e., W[1,i]W[i+1,j] = W[1,j]." ></td>
	<td class="line x" title="45:221	Thus W[1,i]w represents the string where Wi+1 = w. We can define a derives relation (denotedG for a given PCFG G) as follows: A G  if and only if A    P. A string W  T is in the language of a grammar G if and only if S +G W, i.e., a sequence of one or more derivation steps yields the string from the start 1The parser version will be made publicly available." ></td>
	<td class="line x" title="46:221	325 non-terminal." ></td>
	<td class="line x" title="47:221	A leftmost derivation begins with S and each derivation step replaces the leftmost non-terminal A in the yield with some  such that A    P. For a leftmost derivation S G , where   (V  T), the sequence of derivation steps that yield  can be represented as a tree, with the start symbol S at the root, and the yield sequence  at the leaves of the tree." ></td>
	<td class="line x" title="48:221	A complete tree has only terminal items in the yield, i.e.,   T; a partial tree has some non-terminal items in the yield." ></td>
	<td class="line x" title="49:221	With a leftmost derivation, the yield  =  partitions into an initial sequence of terminals   T followed by a sequence of non-terminals   V ." ></td>
	<td class="line x" title="50:221	For a complete derivation,  = epsilon1; for a partial derivation   V +, i.e., one or more non-terminals." ></td>
	<td class="line x" title="51:221	Let T (G,W[1,i]) be the set of complete trees with W[1,i] as the yield of the tree, given PCFG G. A leftmost derivation D consists of a sequence of |D| steps." ></td>
	<td class="line x" title="52:221	Let Di represent the ith step in the derivation D, and D[i,j] represent the subsequence of steps in D beginning with Di and ending with Dj." ></td>
	<td class="line x" title="53:221	Note that D|D| is the last step in the derivation, and D[1,|D|] is the derivation as a whole." ></td>
	<td class="line x" title="54:221	Each step Di in the derivation is a rule in G, i.e., Di  P for all i. The probability of the derivation and the corresponding tree is: (D) = mproductdisplay i=1 (Di) (1) Let D(G,W[1,i]) be the set of all possible leftmost derivations D (with respect to G) such that RHS(D|D|) = Wi." ></td>
	<td class="line x" title="55:221	Thesearethesetofpartialleftmost derivations whose last step used a production with terminal Wi on the right-hand side." ></td>
	<td class="line x" title="56:221	The prefix probability of W[1,i] with respect to G is PrefixProbG(W[1,i]) = summationdisplay DD(G,W[1,i]) (D) (2) From this prefix probability, we can calculate the conditional probability of each word w  T in the terminal vocabulary, given the preceding sequence W[1,i] as follows: PG(w | W[1,i]) = PrefixProbG(W[1,i]w)P wprimeT PrefixProbG(W[1,i]wprime) = PrefixProbG(W[1,i]w)PrefixProb G(W[1,i]) (3) This, in fact, is precisely the conditional probability that is used for language modeling for such applications as speech recognition and machine translation, which was the motivation for various syntactic language modeling approaches (Jelinek and Lafferty, 1991; Stolcke, 1995; Chelba and Jelinek, 1998; Roark, 2001)." ></td>
	<td class="line x" title="57:221	As with language modeling, it is important to model the end of the string as well, usually with an explicit end symbol, e.g., </s>." ></td>
	<td class="line x" title="58:221	For a string W[1,i], we can calculate its prefix probability as shown above." ></td>
	<td class="line x" title="59:221	To calculate its complete probability, we must sum the probabilities over the set of complete trees T (G,W[1,i])." ></td>
	<td class="line x" title="60:221	In such a way, we can calculate the conditional probability of ending the string with </s> given W[1,i] as follows: PG(</s> | W[1,i]) = summationtext DT (G,W[1,i]) (D) PrefixProbG(W[1,i]) (4) 2.1 Incremental top-down parsing In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here." ></td>
	<td class="line x" title="61:221	As presented in Roark (2004), the probabilities in the PCFG are smoothed so that the parser is guaranteed not to fail due to garden pathing, despite following a beamsearchstrategy." ></td>
	<td class="line x" title="62:221	Hencethereisalwaysanonzero prefix probability as defined in Eq." ></td>
	<td class="line x" title="63:221	2." ></td>
	<td class="line x" title="64:221	The parser follows a top-down leftmost derivation strategy." ></td>
	<td class="line x" title="65:221	The grammar is factored so that every production has either a single terminal item on the right-hand side or is of the form A  B A-B, where A,B  V and the factored A-B category can expand to any sequence of children categories of A that can follow B. This factorization of nary productions continues to nullary factored productions, i.e., the end of the original production A  B1Bn is signaled with an empty production A-B1--Bn epsilon1." ></td>
	<td class="line x" title="66:221	Theparsermaintainsasetofpossibleconnected derivations,weightedviathePCFG.Itusesabeam search, whereby the highest scoring derivations are worked on first, and derivations that fall outside of the beam are discarded." ></td>
	<td class="line x" title="67:221	The reader is referred to Roark (2001; 2004) for specifics about the beam search." ></td>
	<td class="line x" title="68:221	The model conditions the probability of each production on features extracted from the partial tree, including non-local node labels such as parents, grandparents and siblings from the leftcontext, as well as c-commanding lexical items." ></td>
	<td class="line x" title="69:221	Hence this is a lexicalized grammar, though the incremental nature precludes a general head-first strategy, rather one that looks to the left-context for c-commanding lexical items." ></td>
	<td class="line x" title="70:221	To avoid some of the early prediction of structure, the version of the Roark parser that we used 326 performs an additional grammar transformation beyond the simple factorization already described  a selective left-corner transform of left-recursive productions (Johnson and Roark, 2000)." ></td>
	<td class="line x" title="71:221	In the transformed structure, slash categories are used to avoidpredictingleft-recursivestructureuntilsome explicit indication of modification is present, e.g., a preposition." ></td>
	<td class="line x" title="72:221	Thefinalstepinparsing,followingthelastword in the string, is to complete all non-terminals in the yield of the tree." ></td>
	<td class="line x" title="73:221	All of these open nonterminals are composite factored categories, such as S-NP-VP, which are completed by rewriting toepsilon1." ></td>
	<td class="line x" title="74:221	The probability of theseepsilon1productions is what allows for the calculation of the conditional probability of ending the string, shown in Eq." ></td>
	<td class="line x" title="75:221	4." ></td>
	<td class="line x" title="76:221	Onefinalnoteaboutthesizeofthenon-terminal set and the intractability of exact inference for such a scenario." ></td>
	<td class="line x" title="77:221	The non-terminal set not only includes the original atomic non-terminals of the grammar, but also any categories created by grammar factorization (S-NP) or the left-corner transform (NP/NP)." ></td>
	<td class="line x" title="78:221	Additionally, however, to remain context-free, the non-terminal set must include categories that incorporate non-local features used by the statistical model into their label, including parents, grandparents and sibling categories in the left-context, as well as c-commanding lexical heads." ></td>
	<td class="line x" title="79:221	These non-local features must be made local by encoding them in the non-terminal labels, leading to a very large non-terminal set and intractable exact inference." ></td>
	<td class="line x" title="80:221	Heavy smoothing is required when estimating the resulting PCFG." ></td>
	<td class="line x" title="81:221	The benefit of such a non-terminal set is a rich model, which enables a more peaked statistical distribution around high quality syntactic structures and thus more effective pruning of the search space." ></td>
	<td class="line x" title="82:221	The fully connected left-context produced by topdown derivation strategies provides very rich features for the stochastic parsing models." ></td>
	<td class="line x" title="83:221	See Roark (2001; 2004) for discussion of these issues." ></td>
	<td class="line x" title="84:221	We now turn to measures that can be derived fromtheparserwhichmaybeofuseforpsycholinguistic modeling." ></td>
	<td class="line x" title="85:221	3 Parser and grammar derived measures 3.1 Surprisal The surprisal at word Wi is the negative log probability of Wi given the preceding words." ></td>
	<td class="line x" title="86:221	Using prefix probabilities, this can be calculated as: SG(Wi) = log PrefixProbG(W[1,i])PrefixProb G(W[1,i1]) (5) Substituting equation 2 into this, we get SG(Wi) = log summationtext DD(G,W[1,i]) (D)summationtext DD(G,W[1,i1]) (D) (6) If we are using a beam-search parser, some of the derivations are pruned away." ></td>
	<td class="line x" title="87:221	Let B(G,W[1,i])  D(G,W[1,i]) be the set of derivations in the beam." ></td>
	<td class="line x" title="88:221	Then the surprisal can be approximated as SG(Wi)  log summationtext DB(G,W[1,i]) (D)summationtext DB(G,W[1,i1]) (D) (7) Any pruning in the beam search will result in a deficient probability distribution, i.e., a distribution that sums to less than 1." ></td>
	<td class="line x" title="89:221	Roarks thesis (2001) showed that the amount of probability mass lost for this particular approach is very low, hence this provides a very tight bound on the actual surprisal given the model." ></td>
	<td class="line x" title="90:221	3.2 Lexical and Syntactic surprisal High surprisal scores result when the prefix probability at word Wi is low relative to the prefix probability at word Wi1." ></td>
	<td class="line x" title="91:221	Sometimes this is due to the identity of Wi, i.e., it is a surprising word given the context." ></td>
	<td class="line x" title="92:221	Other times, it may not be the lexical identityofthewordsomuchasthesyntacticstructure that must be created to integrate the word into the derivations." ></td>
	<td class="line x" title="93:221	One would like to tease surprisal apart into syntactic surprisal versus lexical surprisal, which would capture this intuition of the lexical versus syntactic dimensions to the score." ></td>
	<td class="line x" title="94:221	Our solution to this has the beneficial property of producing two scores whose sum equals the original surprisal score." ></td>
	<td class="line x" title="95:221	The original surprisal score is calculated via sets of partial derivations at the point when each word Wi is integrated into the syntactic structure, D(G,W[1,i])." ></td>
	<td class="line x" title="96:221	We then calculate the ratio from point to point in sequence." ></td>
	<td class="line x" title="97:221	To tease apart the lexical and syntactic surprisal, we will consider sets of partial derivations immediately before each word Wi is integrated into the syntactic structure, i.e., D[1,|D|1] for D  D(G,W[1,i])." ></td>
	<td class="line x" title="98:221	Recall that the last derivation move for every derivation in the set is from the POS-tag to the lexical item." ></td>
	<td class="line x" title="99:221	Hence the sequence of derivation moves that excludes the last one includes all structure except the word Wi." ></td>
	<td class="line x" title="100:221	Then the syntactic surprisal is calculated as: SynSG(Wi) = log P DD(G,W[1,i]) (D[1,|D|1])P DD(G,W[1,i1]) (D) (8) 327 and the lexical surprisal is calculated as: LexSG(Wi) = log P DD(G,W[1,i]) (D)P DD(G,W[1,i]) (D[1,|D|1]) (9) Note that the numerator of SynSG(Wi) is the denominator of LexSG(Wi), hence they sum to form total surprisal SG(Wi)." ></td>
	<td class="line x" title="101:221	As with total surprisal, these measures can be defined either for the full set D(G,W[1,i]) or for a pruned beam of derivations B(G,W[1,i])  D(G,W[1,i])." ></td>
	<td class="line x" title="102:221	Finally, we replicated the Demberg and Keller (2008) unlexicalized surprisal by replacing every lexical item in the training corpus with its POS-tag, andthenparsingthePOS-tagsofthelanguage samples rather than the words." ></td>
	<td class="line x" title="103:221	This differs from our syntactic surprisal by having no lexical conditioning events for rule probabilities, and by having no ambiguity about the POS-tag of the lexical items in the string." ></td>
	<td class="line x" title="104:221	We will refer to the resulting surprisal measure as POS surprisal to distinguish it from our syntactic surprisal measure." ></td>
	<td class="line x" title="105:221	3.3 Entropy Entropy scores of the sort advocated by Hale (2003; 2006) involve calculation over the set of completederivationsconsistentwiththesetofpartial derivations." ></td>
	<td class="line x" title="106:221	Hale performs this calculation efficiently via matrix inversion, which explains the use of relatively small-scale grammars with tractably sized non-terminal sets." ></td>
	<td class="line x" title="107:221	Such methods are not tractable for the kinds of richly conditioned, large-scale PCFGs that we advocate using here." ></td>
	<td class="line x" title="108:221	At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is required to calculate such measures." ></td>
	<td class="line x" title="109:221	Let H(D) be the entropy over a set of derivations D, calculated as follows: H(D) =  X DD (D)P DprimeD (Dprime) log (D)P DprimeD (Dprime) (10) If the set of derivations D = D(G,W[1,i]) is a set of partial derivations for string W[1,i], then H(D) is a measure of uncertainty over the partial derivations, i.e., the uncertainty regarding the correct analysis of what has already been processed." ></td>
	<td class="line x" title="110:221	This can be calculated directly from the existing parser operations." ></td>
	<td class="line x" title="111:221	If the set of derivations arethecompletederivationsconsistent withtheset of partial derivations  complete derivations that could occur over the set of possible continuations of the string  then this is a measure of the uncertainty about what is yet to come." ></td>
	<td class="line x" title="112:221	We would like measures that can capture this distinction between (a) uncertainty of what has already been processed (current ambiguity) versus (b) uncertainty of what is yet to be processed (predictive entropy)." ></td>
	<td class="line x" title="113:221	In addition, as with surprisal, we would like to tease apart the syntactic uncertainty versus lexical uncertainty." ></td>
	<td class="line x" title="114:221	To calculate the predictive entropy after word sequence W[1,i], we modify the parser as follows: the parser extends the set of partial derivations to include all possible next words (the entire vocabulary plus </s>), and calculates the entropy over that set." ></td>
	<td class="line x" title="115:221	This measure is calculated from just one additional word beyond the current word, and hence is an approximation to Hales conditional entropy of grammatical continuations, which is over complete derivations." ></td>
	<td class="line x" title="116:221	We will denote this as H1G(W[1,i]) and calculate it as follows: H1G(W[1,i]) = H( uniondisplay wT{</s>} D(G,W[1,i]w)) (11) This is performing a predictive step that the baseline parser does not perform, extending the parses to all possible next words." ></td>
	<td class="line x" title="117:221	Unlike surprisal, entropy does not decompose straightforwardly into syntactic and lexical components that sum to the original composite measure." ></td>
	<td class="line x" title="118:221	To tease apart entropy due to syntactic uncertainty versus that due to lexical uncertainty, we can define the set of derivations up to the preterminal (POS-tag) non-terminals as follows." ></td>
	<td class="line x" title="119:221	Let S(D) = {D[1,|D|1] : D  D}, i.e., the set of derivations achieved by removing the last step of all derivations inD. Then we can calculate a syntactic H1G as follows: SynH1G(W[1,i]) = H( [ wT{</s>} S(D(G,W[1,i]w))) (12) Finally, lexical H1G is defined in terms of the conditional probabilities derived from prefix probabilities as defined in Eq." ></td>
	<td class="line x" title="120:221	3." ></td>
	<td class="line x" title="121:221	LexH1G(W[1,i]) =  X wT{</s>} PG(w | W[1,i])logPG(w | W[1,i]) (13) Asapracticalmatter,thesevaluesarecalculated within the Roark parser as follows." ></td>
	<td class="line x" title="122:221	A dummy word is created that can be assigned every POStag,andtheparserextendsfromthecurrentstateto this dummy word." ></td>
	<td class="line x" title="123:221	(The beam threshold is greatly 328 expanded to allow for many possible extensions.)" ></td>
	<td class="line x" title="124:221	Then every word in the vocabulary is substituted for the word, and the appropriate probabilities calculated over the beam." ></td>
	<td class="line x" title="125:221	Finally, the actual next word is substituted, the beam threshold is reduced to the actual working threshold, and the requisite number of analyses are advanced to continue parsingthestring." ></td>
	<td class="line x" title="126:221	Thisrepresentsasignificantamount of additional work for the parser  particularly for vocabulary sizes that we currently use, on the order of tens of thousands of words." ></td>
	<td class="line x" title="127:221	As with surprisal, we can calculate an unlexicalized version of the measure by training and parsing just to POS-tags." ></td>
	<td class="line x" title="128:221	We will refer to this sort of entropy as POS entropy." ></td>
	<td class="line x" title="129:221	4 Empirical validation 4.1 Subjects and stimuli In order to test the psycholinguistic relevance of the different measures produced by the parser, we conducted a word by word reading experiment." ></td>
	<td class="line x" title="130:221	23 native speakers of English read 4 short texts (mean length: 883.5 words, 49.25 sentences)." ></td>
	<td class="line x" title="131:221	The texts were the written versions of narratives used in a parallel fMRI experiment making use of the same parser derived measures and whose results will be published in a different paper (Bachrach et al., 2009)." ></td>
	<td class="line x" title="132:221	The narratives contained a high density of syntactically complex structures (in the form of sentential embeddings, relative clauses and other non-local dependencies) but were constructed so as to appear highly natural." ></td>
	<td class="line x" title="133:221	The modified version of the Roark parser, trained on the Brown Corpus section of the Penn Treebank (Marcus et al., 1993), was used to parse the different narratives and produce the word by word measures." ></td>
	<td class="line x" title="134:221	4.2 Procedure Each narrative was presented line by line (certain sentences required more than one line) on a computer screen (Dell Optiplex 755 running Windows XP Professional) using Linger 2.882." ></td>
	<td class="line x" title="135:221	Each line contained 11.5 words on average." ></td>
	<td class="line x" title="136:221	Each word would appear in its relative position on the screen." ></td>
	<td class="line x" title="137:221	The subject would then be required to push a keyboard button to advance to the next word." ></td>
	<td class="line x" title="138:221	The original word would then disappear and the following word appear in the subsequent position on the screen." ></td>
	<td class="line x" title="139:221	After certain sentences a comprehension question would appear on the screen (10 per narrative)." ></td>
	<td class="line x" title="140:221	This was done in order to encourage 2http://tedlab.mit.edu/dr/Linger/readme.html subjects to pay attention and to provide data for a post-hoc evaluation of comprehension." ></td>
	<td class="line x" title="141:221	After each narrative, subjects were instructed to take a short break (2 minutes on average)." ></td>
	<td class="line x" title="142:221	4.3 Data analysis The log (base 10) of the reaction times were analyzed using a linear mixed effects regression analysis implemented in the language R (Bates et al., 2008)." ></td>
	<td class="line x" title="143:221	Reaction times longer than 1500 ms and shorter than 150 ms (raw) were excluded from the analysis(4.8%oftotaldata)." ></td>
	<td class="line x" title="144:221	Sincebuttonpresslatencies inferior to 150 ms must have been planned prior to the presentation of the word, we considered that they could not reflect stimulus driven effects." ></td>
	<td class="line x" title="145:221	Data from the first and last words on each line were discarded." ></td>
	<td class="line x" title="146:221	The combined data from the 4 narratives was first modeled using a model which included order of word in the narrative3, word length, parserderived lexical surprisal, unigram frequency, bigram probability, syntactic surprisal, lexical entropy, syntactic entropy and mean number of parser derivation steps as numeric regressors." ></td>
	<td class="line x" title="147:221	We also included the unlexicalized POS variants of syntactic surprisal and entropy, along the lines of Demberg and Keller (2008), as detailed in  3." ></td>
	<td class="line x" title="148:221	Table 1 presents the correlations between these mean-centered measures." ></td>
	<td class="line x" title="149:221	In addition, we modeled word class (open/closed) as a categorical factor in order to assess interaction between class and the variables of interest, since such an interaction has been observed in the case of frequency (Bradley, 1983)." ></td>
	<td class="line x" title="150:221	Finally, the random effect part of the model included intercepts for subjects, words and sentences." ></td>
	<td class="line x" title="151:221	We report significant effects at the threshold p < .05." ></td>
	<td class="line x" title="152:221	Given the presence of significant interactions between lexical class (open/closed) and a number of the variables of interests, we decided to split the data set into open and closed class words and model these separately (linear mixed effects with the same numeric variables as in the full model)." ></td>
	<td class="line x" title="153:221	In order to evaluate the usefulness of splitting total surprisal into lexical and syntactic components we compared, using a likelihood ratio test, a model where lexical and syntactic surprisal are modeled as distinct regressors to a model where a single regressor equal to their sum (total surprisal) 3This is a regressor to control for the trend of subjects to read faster later in the narrative." ></td>
	<td class="line x" title="154:221	329 Predictor SynH LexH SynS LexS Freq Bgrm PosS PosH Step WLen Syntactic Entropy (SynH) 1.00 -0.26 0.00 0.24 -0.24 0.20 0.02 0.55 -0.05 0.18 Lexical Entropy (LexH) -0.26 1.00 0.01 -0.40 0.43 -0.38 -0.03 0.02 0.11 -0.29 Syntactic Surprisal (SynS) 0.00 0.01 1.00 -0.12 0.08 0.18 0.77 0.21 0.38 -0.03 Lexical Surprisal (LexS) 0.24 -0.40 -0.12 1.00 -0.81 0.87 -0.10 -0.20 -0.35 0.64 Unigram Frequency (Freq) -0.24 0.43 0.08 -0.81 1.00 -0.69 0.02 0.18 0.31 -0.72 Bigram Probability (Bgrm) 0.20 -0.38 0.18 0.87 -0.69 1.00 0.11 -0.11 -0.16 0.56 POS Surprisal (PosS) 0.02 -0.03 0.77 -0.10 0.02 0.11 1.00 0.22 0.32 0.02 POS Entropy (PosH) 0.55 0.02 0.21 -0.20 0.18 -0.11 0.22 1.00 0.16 -0.11 Derivation steps (Step) -0.05 0.11 0.38 -0.35 0.31 -0.16 0.32 0.16 1.00 -0.24 Word Length (WLen) 0.18 -0.29 -0.03 0.64 -0.72 0.56 0.02 -0.11 -0.24 1.00 Table 1: Correlations between (mean-centered) predictors." ></td>
	<td class="line x" title="155:221	Note that unigram frequencies were represented as logs, other scores as negative logs, hence the sign of the correlations." ></td>
	<td class="line x" title="156:221	was included." ></td>
	<td class="line x" title="157:221	If the larger model provides a significantly better fit than the smaller model, this providesevidencethatdistinguishingbetweenlexical and syntactic contributions to surprisal is relevant." ></td>
	<td class="line x" title="158:221	Since total entropy is not a sum of syntactic and lexical entropy, an analogous test would not be valid in that case." ></td>
	<td class="line x" title="159:221	4.4 Results All subjects successfully answered the comprehension questions (92.8% correct responses, S.D.=5.1)." ></td>
	<td class="line x" title="160:221	In the full model, we observed significantmaineffectsofwordclassaswellasoflexical surprisal, bigram probability, unigram frequency, syntactic entropy, POS entropy and of order in the narrative." ></td>
	<td class="line x" title="161:221	Syntactic surprisal, lexical entropy and number of steps had no significant effect." ></td>
	<td class="line x" title="162:221	Word lengthalsohadnosignificantmaineffectbutinteracted significantly with word class (open/closed)." ></td>
	<td class="line x" title="163:221	Word class also interacted significantly with lexical surprisal, unigram frequency and syntactic surprisal." ></td>
	<td class="line x" title="164:221	The presence of these interactions led us to construct models restricted to open and closed class items respectively." ></td>
	<td class="line x" title="165:221	The estimated parameters are reported in Table 2." ></td>
	<td class="line x" title="166:221	Reading time for open class words showed significant effects of unigram frequency, syntactic surprisal, syntactic entropy, POS entropy and order within the narrative." ></td>
	<td class="line x" title="167:221	The positive effect of length approached significance." ></td>
	<td class="line x" title="168:221	Reading time for closed class words exhibited significant effects of lexical surprisal, bigram probability, syntactic entropy and order in the narrative." ></td>
	<td class="line x" title="169:221	Length had a non-significant negative effect, thus explaining the interaction observed in the full model." ></td>
	<td class="line x" title="170:221	The models with separate lexical and syntactic surprisal performed better than models including combined surprisal." ></td>
	<td class="line x" title="171:221	For open class words, the Akaikes information criterion (AIC) was -54810 for the combined model and -54819 for the independentmodel(likelihoodratiotestcomparingthe Estimate Std." ></td>
	<td class="line x" title="172:221	Error t-value Open-class (Intercept) 2.4010+00 2.391002 100.4* Lexical Surprisal -1.991004 7.281004 -0.3 Word Length 8.971004 4.621004 1.9 Bigram 4.181004 5.271004 0.8 Unigram Freq -2.431003 1.201003 -2.0* Derivation Steps -1.171003 9.021004 -1.3 Syntactic Entropy 2.551003 6.191004 4.1* Lexical Entropy 3.961004 6.681004 0.6 Syntactic Surprisal 3.281003 9.711004 3.4* Order in narrative -1.431005 4.341006 -3.3* POS Surprisal -6.841004 8.111004 -0.8 POS Entropy 1.471003 6.051004 2.4* Closed-class (Intercept) 2.4210+00 2.321002 104.3* Lexical Surprisal 2.021003 7.841004 2.6* Word Length -1.871003 1.131003 -1.7 Bigram 1.191003 4.941004 2.4* Unigram Freq 1.691003 2.671003 0.6 Derivation Steps 3.011004 5.091004 0.6 Syntactic Entropy 3.151003 5.051004 6.2* Lexical Entropy 1.831004 8.631004 0.2 Syntactic Surprisal 3.001004 8.351004 0.4 Order in narrative -1.331005 3.991006 -3.3* POS Surprisal -6.461004 6.811004 -0.9 POS Entropy 6.631004 5.041004 1.3 Table 2: Estimated effects from mixed effects models on open and closed items (stars denote significance at p<.05) two, nested, models: 2(1)=10.7,p<.001)." ></td>
	<td class="line x" title="173:221	For closed class items, combined models AIC was 61467 and full models AIC was -61469 (likelihood ratio test: 2(1)=3.54,p=0.06)." ></td>
	<td class="line x" title="174:221	4.5 Discussion Our results demonstrate the relevance of modeling psycholinguistic processes using an incremental probabilistic parser, and the utility of the novel measures presented here." ></td>
	<td class="line x" title="175:221	Of particular interest are: the significant effects of our syntactic entropy measure; the independent contributions of lexical surprisal, bigram probability and unigram frequency; and the differences between the predictions of the lexicalized parsing model and the unlexicalized (POS) parsing model." ></td>
	<td class="line x" title="176:221	The effect of entropy, or uncertainty regarding 330 the upcoming input independent of the surprise of that input, has been observed in non-linguistic tasks (Hyman, 1953; Bestmann et al., 2008) but to our knowledge has not been quantified before in the context of sentence processing." ></td>
	<td class="line x" title="177:221	The usefulness of computational modeling is particularly evident in the case of entropy given the absence of any subjective procedure for its evaluation4." ></td>
	<td class="line x" title="178:221	The results argue in favor of a predictive parsing architecture (Van Berkum et al., 2005)." ></td>
	<td class="line x" title="179:221	The approach to entropy here differs from the one described in Hale (2006) in a couple of ways." ></td>
	<td class="line x" title="180:221	First, as discussed above, the calculation procedure is different  we focus on extending the derivations with just one word, rather than to all possible complete derivations." ></td>
	<td class="line x" title="181:221	Second, and most importantly, Hale emphasizes entropy reduction (or the gain in information, given an input, regarding the rest of the sentence) as the correlate of cognitive cost while here we are interested in the amount of entropy itself (and not the size of change)." ></td>
	<td class="line x" title="182:221	Interestingly, we observed only an effect of syntactic entropy, not lexical entropy." ></td>
	<td class="line x" title="183:221	Recent ERP work has demonstrated that subjects do form specific lexical predictions in the context of sentence processing (Van Berkum et al., 2005; DeLong et al., 2005) and so we suspect that the absence of lexical entropy effect might be partly due to sparse data." ></td>
	<td class="line x" title="184:221	Lexical surprisal and entropy were calculated using the internal state of a parser trained on the relatively small Brown corpus." ></td>
	<td class="line x" title="185:221	Lexical entropy showed no significant effect while lexical surprisal affected only closed class words." ></td>
	<td class="line x" title="186:221	This pattern of results might be due to the sparseness of the relevant information in such a small corpus (e.g., verb/object preferences) and the relevance of extra-textual dimensions (world knowledge, contextual information) to lexical-specific prediction." ></td>
	<td class="line x" title="187:221	Closed class words are both more frequent (and hence better sampled) and are less sensitive to world knowledge, yet are often determined by the grammatical context." ></td>
	<td class="line x" title="188:221	Demberg and Keller (2008) made use of the same parsing architecture used here to compute a syntactic surprisal measure, but used an unlexicalized parser (down to POS-tags rather than words) for this score." ></td>
	<td class="line x" title="189:221	Their lexicalized surprisal is equivalent to our total surprisal (lexical surprisal + syntactic surprisal), while their POS surprisal is 4The Cloze procedure (Taylor, 1953) is one way to derive probabilities that could be used to calculate entropy, though this procedure is usually conducted with lexical elicitation, which would make syntactic entropy calculations difficult." ></td>
	<td class="line x" title="190:221	derivedfromacompletelydifferentmodel." ></td>
	<td class="line x" title="191:221	Incontrast, our approach achieves lexical and syntactic measures from the same model." ></td>
	<td class="line x" title="192:221	In order to evaluate the difference between the two approaches we added unlexicalized POS surprisal calculated along the lines of that paper to our model, along with an unlexicalized POS entropy from the same model." ></td>
	<td class="line x" title="193:221	We found no effect of unlexicalized POS surprisal5 and a significant (but relatively small) effect of unlexicalized POS entropy." ></td>
	<td class="line x" title="194:221	While syntactic surprisal was correlated with POS surprisal (see Table 1) and syntactic entropy correlated with POS entropy, the fact that our syntactic measures still had a significant effect suggests that lexical information contributes towards the formation of syntactic expectations." ></td>
	<td class="line x" title="195:221	While the effect of surprisal calculated by an incremental top down parser has been already demonstrated (Demberg and Keller, 2008), our results argue for a distinction between the effect of lexical surprisal and that of syntactic surprisal without requiring unlexicalized parsing of the sort that Demberg and Keller advocate." ></td>
	<td class="line x" title="196:221	It is important to keep in mind that this distinction between types of prediction (and as a consequence, prediction error) is not equivalent to the one drawn in the traditional cognitive science modularity debate, which has focused on the source of these predictions." ></td>
	<td class="line x" title="197:221	We found a positive effect of syntactic surprisal in the case of open class words." ></td>
	<td class="line x" title="198:221	The absence of an effect for closed class words remains to be explained." ></td>
	<td class="line x" title="199:221	We quantified word specific surprisal using 3 sources: the parsers internal state (lexical surprisal); probability given the preceding word (negative log bigram probability); and the unigram frequency of the word in a large corpus6." ></td>
	<td class="line x" title="200:221	As can be observed in Table 1, these three measures are highly correlated7." ></td>
	<td class="line x" title="201:221	This is the consequence of the smoothing in the estimation procedure but also relates to a more general fact about language use: overall, more frequent words are also words more expected to appear in a specific context (Anderson and Schooler, 1991)." ></td>
	<td class="line x" title="202:221	Despite these strong correlations, the three measures produced independent 5We also ran the model including unlexicalized POS surprisal without our syntactic surprisal or syntactic entropy, and in this condition the unlexicalized POS surprisal measure had anearlysignificanteffect(t = 1.85), whichisconsistentwith the results in Boston et al.(2008a) and Demberg and Keller (2008)." ></td>
	<td class="line x" title="204:221	6The unigram frequencies came from the HAL corpus (Lund and Burgess, 1996)." ></td>
	<td class="line x" title="205:221	All other statistical models were estimated from the Brown Corpus." ></td>
	<td class="line x" title="206:221	7Unigramfrequencieswererepresentedaslogs, theothers as negative logs, hence the sign of the correlations." ></td>
	<td class="line x" title="207:221	331 effects." ></td>
	<td class="line x" title="208:221	Unigramfrequencyhadasignificanteffect for open class words while bigram probability and lexicalsurprisaleachhadan effectonreadingtime of closed class items." ></td>
	<td class="line x" title="209:221	Bigram probability has been often found to affect reading time using eye movement measures." ></td>
	<td class="line x" title="210:221	This is the first study to demonstrate an additional effect of contextual surprisal given the preceding sentential context (lexical surprisal)." ></td>
	<td class="line x" title="211:221	Demberg and Keller found no effect for surprisal once bigram and unigram probabilities were included in the model but, importantly, they did not distinguish lexical and syntactic surprisal, rather lexicalized and unlexicalized surprisal." ></td>
	<td class="line x" title="212:221	5 Summary Wehavepresentednovelmethodsforteasingapart syntactic and lexical surprisal from a fully lexicalized parser, as well as for extending the operation of a predictive parser to capture novel entropy measures that are also shown to be relevant to psycholinguistic modeling." ></td>
	<td class="line x" title="213:221	Such automatic methods provide psycholinguistically relevant measures that are intractable to calculate by hand." ></td>
	<td class="line x" title="214:221	The empirical validation presented here demonstrated that the new measures  particularly syntactic entropy and syntactic surprisal  have high utility for modeling human reading time data." ></td>
	<td class="line x" title="215:221	Our approach to calculating syntactic surprisal, based on fully lexicalized parsing, provided significant effects, while the POS-tag based (unlexicalized) surprisal  of the sort used in Boston et al.(2008a) and Demberg and Keller (2008)  did not provide a significant effect in our trials." ></td>
	<td class="line x" title="217:221	Further, we showed an effect of lexical surprisal for closed class words even when combined with unigram and bigram probabilities in the same model." ></td>
	<td class="line x" title="218:221	This work contributes to the important, developing enterprise of leveraging data-driven NLP approaches to derive new measures of high utility for psycholinguistic and neuropsychological studies." ></td>
	<td class="line x" title="219:221	Acknowledgments ThankstoMichaelCollins, JohnHaleandShravan Vasishth for valuable discussions about this work." ></td>
	<td class="line x" title="220:221	This research was supported in part by NSF Grant #BCS-0826654." ></td>
	<td class="line x" title="221:221	Any opinions, findings, conclusions or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of the NSF." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1043
Perceptron Reranking for CCG Realization
White, Michael;Rajkumar, Rajakrishnan;"></td>
	<td class="line x" title="1:172	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 410419, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:172	c 2009 ACL and AFNLP Perceptron Reranking for CCG Realization Michael White and Rajakrishnan Rajkumar Department of Linguistics The Ohio State University Columbus, OH, USA {mwhite,raja}@ling.osu.edu Abstract This paper shows that discriminative reranking with an averaged perceptron model yields substantial improvements in realization quality with CCG." ></td>
	<td class="line x" title="3:172	The paper confirms the utility of including language model log probabilities as features in the model, which prior work on discriminative training with log linear models for HPSG realization had called into question." ></td>
	<td class="line x" title="4:172	Theperceptronmodelallowsthecombination of multiple n-gram models to be optimized and then augmented with both syntactic features and discriminative n-gram features." ></td>
	<td class="line x" title="5:172	The full model yields a stateof-the-art BLEU score of 0.8506 on Section 23 of the CCGbank, to our knowledge the best score reported to date using a reversible, corpus-engineered grammar." ></td>
	<td class="line x" title="6:172	1 Introduction In this paper, we show how discriminative training with averaged perceptron models (Collins, 2002) can be used to substantially improve surface realization with Combinatory Categorial Grammar (Steedman, 2000, CCG)." ></td>
	<td class="line x" title="7:172	Velldal and Oepen (2005)andNakanishietal.(2005)haveshownthat discriminative training with log-linear (maximum entropy) models is effective in realization ranking with Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994, HPSG)." ></td>
	<td class="line x" title="8:172	Here we show thataveragedperceptronmodelsalsoperformwell for realization ranking with CCG." ></td>
	<td class="line x" title="9:172	Averaged perceptron models are very simple, just requiring a decoder and a simple update function, yet despite their simplicity they have been shown to achieve state-of-the-art results in Treebank and CCG parsing (Huang, 2008; Clark and Curran, 2007a) as well as on other NLP tasks." ></td>
	<td class="line x" title="10:172	Along the way, we address the question of whether it is beneficial to incorporate n-gram log probabilities as baseline features in a discriminatively trained realization ranking model." ></td>
	<td class="line x" title="11:172	On a limited domain corpus, Velldal & Oepen found that including the n-gram log probability of each candidate realization as a feature in their log-linear model yielded a substantial boost in ranking performance; on the Penn Treebank (PTB), however, Nakanishietal.foundthatincludingann-gramlog prob feature in their model was of no benefit (with the useof bigrams instead of 4-gramssuggested as a possible explanation)." ></td>
	<td class="line x" title="12:172	With these mixed results, the utility of n-gram baseline features for PTBscale discriminative realization ranking has been unclear." ></td>
	<td class="line x" title="13:172	In our particular setting, the question is: Do n-gram log prob features improve performance in broad coverage realization ranking with CCG, where factored language models over words, partof-speech tags and supertags have previously been employed (White et al., 2007; Espinosa et al., 2008)?" ></td>
	<td class="line x" title="14:172	We answer this question in the affirmative, confirming the results of Velldal & Oepen, despite the differences in corpus size and kind of language model." ></td>
	<td class="line x" title="15:172	We show that including n-gram log prob features in the perceptron model is highly beneficial, as the discriminative models we tested without these features performed worse than the generative baseline." ></td>
	<td class="line oc" title="16:172	These findings are in line with Collins & Roarks (2004) results with incremental parsing with perceptrons, where it is suggested that a generative baseline feature provides the perceptron algorithm with a much better starting point for learning." ></td>
	<td class="line x" title="17:172	We also show that discriminative training allows the combination of multiple n-gram models to be optimized, and that the best model augments the n-gram log prob features with both syntactic features and discriminative n-gram features." ></td>
	<td class="line x" title="18:172	The full model yields a stateof-the-art BLEU (Papineni et al., 2002) score of 0.8506 on Section 23 of the CCGbank, which is to our knowledge the best score reported to date 410 using a reversible, corpus-engineered grammar." ></td>
	<td class="line x" title="19:172	The paper is organized as follows." ></td>
	<td class="line x" title="20:172	Section 2 reviewspreviousworkonbroadcoveragerealization with OpenCCG." ></td>
	<td class="line x" title="21:172	Section 3 describes our approach to realization reranking with averaged perceptron models." ></td>
	<td class="line x" title="22:172	Section 4 presents our evaluation of the perceptron models, comparing the results of different feature sets." ></td>
	<td class="line x" title="23:172	Section 5 compares our results to those obtained by related systems and discusses the difficulties of cross-system comparisons." ></td>
	<td class="line x" title="24:172	Finally, Section 6 concludes with a summary and discussion of future directions for research." ></td>
	<td class="line x" title="25:172	2 Background 2.1 Surface Realization with CCG CCG (Steedman, 2000) is a unification-based categorial grammar formalism which is defined almostentirelyintermsoflexicalentriesthatencode sub-categorizationinformationaswellassyntactic feature information (e.g. number and agreement)." ></td>
	<td class="line x" title="26:172	Complementing function application as the standard means of combining a head with its argument, type-raising and composition support transparent analyses for a wide range of phenomena, including right-node raising and long distance dependencies." ></td>
	<td class="line x" title="27:172	An example syntactic derivation appears in Figure 1, with a long-distance dependency between point and make." ></td>
	<td class="line x" title="28:172	Semantic composition happens in parallel with syntactic composition, which makes it attractive for generation." ></td>
	<td class="line x" title="29:172	OpenCCG is a parsing/generation library which works by combining lexical categories for words using CCG rules and multi-modal extensions on rules (Baldridge, 2002) to produce derivations." ></td>
	<td class="line x" title="30:172	Surface realization is the process by which logical forms are transduced to strings." ></td>
	<td class="line x" title="31:172	OpenCCG uses a hybrid symbolic-statistical chart realizer (White, 2006) which takes logical forms as input and produces sentences by using CCG combinators to combine signs." ></td>
	<td class="line x" title="32:172	Edges are grouped into equivalence classes when they have the same syntactic category and cover the same parts of the input logical form." ></td>
	<td class="line x" title="33:172	Alternative realizations are ranked using integrated n-gram or perceptron scoring, and pruning takes place within equivalence classes of edges." ></td>
	<td class="line x" title="34:172	To more robustly support broad coverage surface realization, OpenCCG greedily assembles fragments in the event that the realizer fails to find a complete realization." ></td>
	<td class="line x" title="35:172	To illustrate the input to OpenCCG, consider the semantic dependency graph in Figure 2." ></td>
	<td class="line x" title="36:172	In aa1 heh3 he h2 <Det> <Arg0><Arg1> <TENSE>pres <NUM>sg <Arg0> w1want.01 m1 <Arg1> <GenRel> <Arg1> <TENSE>pres p1point h1have.03 make.03 <Arg0> s[b]\np/np np/n np n s[dcl]\np/np s[dcl]\np/(s[to]\np) np Figure 2: Semantic dependency graph from the CCGbank for He has a point he wants to make [], along with gold-standard supertags (category labels) the graph, each node has a lexical predication (e.g. make.03) and a set of semantic features (e.g. NUMsg); nodes are connected via dependency relations (e.g. ARG0)." ></td>
	<td class="line x" title="37:172	(Gold-standard supertags, or category labels, are also shown; see Section 2.4 for their role in hypertagging.)" ></td>
	<td class="line x" title="38:172	Internally, such graphs are represented using Hybrid Logic Dependency Semantics (HLDS), a dependency-based approach to representing linguistic meaning (Baldridge and Kruijff, 2002)." ></td>
	<td class="line x" title="39:172	In HLDS, each semantic head (corresponding to a node in the graph) is associated with a nominal that identifies its discourse referent, and relations between heads and their dependents are modeled as modal relations." ></td>
	<td class="line x" title="40:172	2.2 Realization from an Enhanced CCGbank Our starting point is an enhanced version of the CCGbank (Hockenmaier and Steedman, 2007)a corpus of CCG derivations derived from the Penn Treebankwith Propbank (Palmer et al., 2005) roles projected onto it (Boxwell and White, 2008)." ></td>
	<td class="line x" title="41:172	To engineer a grammar from this corpus suitable for realization with OpenCCG, the derivations are first revised to reflect the lexicalized treatment of coordination and punctuation assumed by the multi-modal version of CCG that is implemented in OpenCCG (White and Rajkumar, 2008)." ></td>
	<td class="line x" title="42:172	Further changes are necessary to support semantic dependencies rather than surface syntactic ones; in 411 He has a point he wants to make np sdcl\np/np np/n n np sdcl\np/(sto\np) sto\np/(sb\np) sb\np/np > >T >Bnp s/(s\np) s to\np/np >Bs dcl\np/np >Bs dcl/np np\np <np >s dcl\np <s dcl Figure 1: Syntactic derivation from the CCGbank for He has a point he wants to make [] particular, the features and unification constraints in the categories related to semantically empty function words such complementizers, infinitivalto, expletive subjects, and case-marking prepositions are adjusted to reflect their purely syntactic status." ></td>
	<td class="line x" title="43:172	In the second step, a grammar is extracted from the converted CCGbank and augmented with logical forms." ></td>
	<td class="line x" title="44:172	Categories and unary type changing rules (corresponding to zero morphemes) are sorted by frequency and extracted if they meet the specified frequency thresholds." ></td>
	<td class="line x" title="45:172	A separate transformation then uses a few dozen generalized templates to add logical forms to the categories, in a fashion reminiscent of (Bos, 2005)." ></td>
	<td class="line x" title="46:172	As shown in Figure 2, numbered semantic roles are taken from PropBank when available, and more specific relations are introduced in the categories for closedclass items such as determiners." ></td>
	<td class="line x" title="47:172	After logical form insertion, the extracted and augmented grammar is loaded and used to parse the sentences in the CCGbank according to the gold-standard derivation." ></td>
	<td class="line x" title="48:172	If the derivation can be successfully followed, the parse yields a logical form which is saved along with the corpus sentence in order to later test the realizer." ></td>
	<td class="line x" title="49:172	Currently, the algorithm succeeds in creating logical forms for 98.85% of the sentences in the developmentsection(Sect.00)oftheconvertedCCGbank, and 97.06% of the sentences in the test section (Sect." ></td>
	<td class="line x" title="50:172	23)." ></td>
	<td class="line x" title="51:172	Of these, 95.99% of the development LFs are semantic dependency graphs with a single root, while 95.81% of the test LFs have a single root." ></td>
	<td class="line x" title="52:172	The remaining cases, with multiple roots, are missing one or more dependencies required to form a fully connected graph." ></td>
	<td class="line x" title="53:172	Such missing dependencies usually reflect remaining inadequacies in the logical form templates." ></td>
	<td class="line x" title="54:172	An error analysis of OpenCCG output by Rajkumar et al.(2009) recently revealed that out of 2331 named entities (NEs) annotated by the BBN corpus (Weischedel and Brunstein, 2005), 238 were not realized correctly." ></td>
	<td class="line x" title="56:172	For example, multiword NPs like Texas Instruments Japan Ltd. were realized as Japan Texas Instruments Ltd. Accordingly, inspired by Hogan et al.s (2007)s Experiment 1, Rajkumar et al. used the BBN corpus NE annotation to collapse certain classes of NEs." ></td>
	<td class="line x" title="57:172	But unlike Hogan et al.s experiment where all the NEs annotated by the BBN corpus were collapsed, Rajkumar et al. chose to collapse into single tokens only NEs whose exact form can be reasonably expected to be specified in the input to the realizer." ></td>
	<td class="line x" title="58:172	For example, while some quantificational or comparatives phrases like more than $ 10,000 are annotated as MONEY in the BBN corpus, Rajkumar et al. only collapse $ 10,000 into an atomic unit, with more than handled compositionally according to the semantics assigned to it by the grammar. Thus, after transferring the BBN annotations totheCCGbankcorpus, Rajkumaretal.(partially) collapsed NEs which are CCGbank constituents according to the following rules: (1) completely collapse the PERSON, ORGANIZATION, GPE, WORK OF ART major class type entitites; (2) ignore phrases like three decades later, which are annotated as DATE entities; and (3) collapse all phrases with POS tags CD or NNP(S) or lexical items % or $, ensuring that all prototypical named entities are collapsed." ></td>
	<td class="line x" title="59:172	It is worth noting that improvements in our corpus-based grammar engineering process including a more precise treatment of punctuation, better named entity handling and the addition of catch-all logical form templateshave resulted in a 13.5 BLEU point improvement in our baseline realization scores on Section 00 of the CCGbank, from a score of 0.6567 in (Espinosa et al., 2008) to 0.7917 in (Rajkumar et al., 2009), contributing greatly to the state-of-the-art results reported 412 in Section 4." ></td>
	<td class="line x" title="60:172	A further 4.5 point improvement is obtained from the use of named entity classes in language modeling and hypertagging (Rajkumar et al., 2009), as described next, and from our perceptron reranking model, described in Section 3." ></td>
	<td class="line x" title="61:172	2.3 Factored Language Models As in (White et al., 2007; Rajkumar et al., 2009), we use factored language models (Bilmes and Kirchhoff, 2003) over words, part-of-speech tags and supertags1 to score partial and complete realizations." ></td>
	<td class="line x" title="62:172	The trigram models were created using the SRILM toolkit (Stolcke, 2002) on the standard training sections (0221) of the CCGbank, with sentence-initial words (other than proper names) uncapitalized." ></td>
	<td class="line x" title="63:172	While these models are considerably smaller than the ones used in (LangkildeGeary, 2002; Velldal and Oepen, 2005), the training data does have the advantage of being in the same domain and genre." ></td>
	<td class="line x" title="64:172	The models employ interpolated Kneser-Ney smoothing with the default frequency cutoffs." ></td>
	<td class="line x" title="65:172	The best performing model interpolates three component models using rankorder centroid weights: (1) a word trigram model; (2) a word model with semantic classes replacing named entities; and (3) a trigram model that chains a POS model with a supertag model, where the POS model (P) conditions on the previous two POS tags, and the supertag model (S) conditions ontheprevioustwoPOStagsaswellasthecurrent one, as shown below: pPS(vectorFi | vectorFi1i2) = p(Pi | Pi1i2)p(Si | Pii2) (1) Training data for the semantic classreplaced model was created by replacing (collapsed) words withtheirNEclasses,inordertoaddressdatasparsity issues caused by rare words in the same semantic class." ></td>
	<td class="line x" title="66:172	For example, the Section 00 sentence Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 . becomes PERSON , DATE:AGE DATE:AGE old , will join the ORG DESC:OTHER as a nonexecutive PER DESC DATE:DATE DATE:DATE . During realization, word forms are generated, but are then replaced by their semantic classes for scoring using the semantic classreplaced model, similar to Oh and Rudnicky (2002)." ></td>
	<td class="line x" title="67:172	Note that the use of supertags in the factored language model to score possible realizations is 1With CCG, supertags (Bangalore and Joshi, 1999) are lexical categories considered as fine-grained syntactic labels." ></td>
	<td class="line x" title="68:172	distinctfromthepredictionofsupertagsforlexical category assignment: the former takes the words in the local context into account (as in supertagging for parsing), while the latter takes features of the logical form into account." ></td>
	<td class="line x" title="69:172	This latter process we call hypertagging, to which we now turn." ></td>
	<td class="line x" title="70:172	2.4 Hypertagging A crucial component of the OpenCCG realizer is the hypertagger (Espinosa et al., 2008), or supertagger for surface realization, which uses a maximum entropy model to assign the most likely lexicalcategoriestothepredicatesintheinputlogical form, thereby greatly constraining the realizerssearchspace.2 Figure2showsgold-standard supertags for the lexical predicates in the graph; such category labels are predicted by the hypertagger at run-time." ></td>
	<td class="line x" title="71:172	As in recent work on using supertagging in parsing, the hypertagger operates in a multitagging paradigm (Curran et al., 2006), where a variable number of predictions are made per input predicate." ></td>
	<td class="line x" title="72:172	Instead of basing category assignment on linear word and POS context, however, the hypertagger predicts lexical categories basedoncontextswithinadirectedgraphstructure representing the logical form (LF) of the sentence to be realized." ></td>
	<td class="line x" title="73:172	The hypertagger generalizes Bangalore and Rambows (2000) method of using supertags in generation by using maximum entropy models with a larger local context." ></td>
	<td class="line x" title="74:172	During realization, the hypertagger returns a best list of supertags in order of decreasing probability." ></td>
	<td class="line x" title="75:172	Increasing the number of categories returned clearly increases the likelihood that the most-correct supertag is among them, but at a corresponding cost in chart size." ></td>
	<td class="line x" title="76:172	Accordingly, the hypertagger begins with a highly restrictive value for , and backs off to progressively less-restrictive values if no complete realization can be found usingthe setofsupertags returned." ></td>
	<td class="line x" title="77:172	Clark andCurran (2007b) have shown this iterative relaxation strategy to be highly effective in CCG parsing." ></td>
	<td class="line x" title="78:172	3 Perceptron Reranking As Collins (2002) observes, perceptron training involves a simple, on-line algorithm, with few iterations typically required to achieve good performance." ></td>
	<td class="line x" title="79:172	Moreover, averaged perceptronswhich 2The approach has been dubbed hypertagging since it operates at a level above the syntax, moving from semantic representations to syntactic categories." ></td>
	<td class="line x" title="80:172	413 Input: training examples (xi,yi) Initialization: set  = 0, or use optional input model Algorithm: for t = 1T, i = 1N zi = argmaxyGEN(xi)(xi,y)   if zi negationslash= yi  =  + (xi,yi)  (xi,zi) Output:  =summationtextTt=1summationtextNi=1 ti/TN Figure 3: Averaged perceptron training algorithm approximate voted perceptrons, a maximummargin method with attractive theoretical propertiesseem to work remarkably well in practice, while adding little further complexity." ></td>
	<td class="line x" title="81:172	Additionally, since features only take on nonzero values when they appear in training items requiring updates, perceptrons integrate feature selection with, and often produce quite small models, especially when starting with a good baseline." ></td>
	<td class="line x" title="82:172	The generic averaged perceptron training algorithm appears in Figure 3." ></td>
	<td class="line x" title="83:172	In our case, the algorithm trains a model for reranking the n-best realizations generated using our existing factored language model for scoring, with the oracle-best realization considered the correct answer." ></td>
	<td class="line x" title="84:172	Accordingly, the input to the algorithm is a list of pairs (xi,yi), where xi is a logical form, GEN(xi) are the n-best realizations for xi, and yi is the oraclebest member of GEN(xi)." ></td>
	<td class="line x" title="85:172	The oracle-best realization is determined using a 4-gram precision metric (approximating BLEU) against the reference sentence." ></td>
	<td class="line x" title="86:172	We have followed Huang (2008) in using oracle-best targets for training, rather than gold standard ones, in order to better approximate test conditions during training." ></td>
	<td class="line x" title="87:172	However, following Clark & Curran (2007a), during training we seed the realizer with the gold-standard supertags, augmenting the hypertaggers -best list, in order to ensure that the n-best realizations are generally of high quality; consequently, the gold standard realization (i.e., the corpus sentence) usually appears in the n-best list.3 In addition, we use a hypertagger trained on all the training data, to improve hypertaggerperformance,whileexcludingthecur3AsinClark&Curransapproach,weuseasingle value during training, rather than iteratively loosening the  value; the chosen  value determines the size of the discrimation space." ></td>
	<td class="line x" title="88:172	rent training section (in jack-knifed fashion) from the word-based parts of the language model, in order to make the language model scores more realistic." ></td>
	<td class="line x" title="89:172	It remains for future work to determine whetherusingadifferentcompromisebetweenensuring high-quality training data and remaining faithful to the test conditions would yield better results." ></td>
	<td class="line x" title="90:172	Since realization of the n-best lists for training is the most time-consuming part of the process, in our current implementation we perform this step once, generating event files along the way containing feature vectors for each candidate realization." ></td>
	<td class="line x" title="91:172	The event files are used to calculate the frequency distribution for the features, and minimum cutoffs are chosen to trim the feature alphabet to a reasonable size." ></td>
	<td class="line x" title="92:172	Training then takes place by iterating over the event files, ignoring features that do not appear in the alphabet." ></td>
	<td class="line x" title="93:172	As Figure 3 indicates, training consists of calculating the topranked realization according to the current model , and performing an update when the top-ranked realization does not match the oracle-best realization." ></td>
	<td class="line x" title="94:172	Updates to the model add the feature vector (xi,yi) for the missed oracle-best realization, and subtract the feature vector (xi,zi) for the mistakenly top-ranked realization." ></td>
	<td class="line x" title="95:172	The final model averages the models across the T iterations overthetrainingdata,andN testcaseswithineach iteration." ></td>
	<td class="line x" title="96:172	Note that while training the perceptron model involves n-best reranking, realization with the resulting model can be viewed as forest rescoring, sincescoringofallpartialrealizationsisintegrated into the realizers beam search." ></td>
	<td class="line x" title="97:172	In future work, we intend to investigate saving the realizers packed charts, rather than event files, and integrating the unpacking of the charts with the perceptron training algorithm." ></td>
	<td class="line x" title="98:172	The features we employ in our perceptron models are of three kinds." ></td>
	<td class="line x" title="99:172	First, as in the log-linear models of Velldal & Oepen and Nakanishi et al., we incorporate the log probability of the candidate realizations word sequence according to our factoredlanguagemodelasasinglefeatureintheperceptron model." ></td>
	<td class="line x" title="100:172	Since our language model linearly interpolates three component models, we also include the log prob from each component language model as a feature, so that the combination of these components can be optimized." ></td>
	<td class="line x" title="101:172	Second, we include syntactic features in our 414 Feature Type Example LexCat + Word s/s/np + before LexCat + POS s/s/np + IN Rule sdcl  np sdcl\np Rule + Word sdcl  np sdcl\np + bought Rule + POS sdcl  np sdcl\np + VBD Word-Word company, sdcl  np sdcl\np, bought Word-POS company, sdcl  np sdcl\np, VBD POS-Word NN, sdcl  np sdcl\np, bought Word + w bought, sdcl  np sdcl\np + dw POS + w VBD, sdcl  np sdcl\np + dw Word + p bought, sdcl  np sdcl\np + dp POS + p VBD, sdcl  np sdcl\np + dp Word + v bought, sdcl  np sdcl\np + dv POS + v VBD, sdcl  np sdcl\np + dv Table 1: Basic and dependency features from Clark & Currans (2007b) normal form model; distances are in intervening words, punctuation marks and verbs, and are capped at 3, 3 and 2, respectively model by implementing Clark & Currans (2007b) normal form model in OpenCCG.4 The features of this model are listed in Table 1; they are integervalued, representing counts of occurrences in a derivation." ></td>
	<td class="line x" title="102:172	These syntactic features are quite comparable to the dominance-oriented features in the union of the Velldal & Oepen and Nakanishi et al. models, except that our feature set does not include grandparenting, which has been found to have limited utility in CCG parsing." ></td>
	<td class="line x" title="103:172	Our syntactic features also include ones that measure the distance between headwords in terms of intervening words, punctuation marks or verbs; these features generalize the ones in Nakanishi et al.s model." ></td>
	<td class="line x" title="104:172	Note that in contrast to parsing, in realization distance features are non-local, since different partial realizationsinthesameequivalenceclasstypically differinwordorder; asweareworkinginareranking paradigm though, the non-local nature of these features is unproblematic." ></td>
	<td class="line x" title="105:172	Third, we include discriminative n-gram features in our model, following Roark et al.s (2004) approach to discriminative n-gram modeling for speech recognition." ></td>
	<td class="line x" title="106:172	By discriminative n-gram features, we mean features counting the occurrences of each n-gram that is scored by our factored language model, rather than a feature whose value is the log prob determined by the language model." ></td>
	<td class="line x" title="107:172	As Roark et al. note, discriminative training with n-gram features has the potential to learn to nega4We have omitted Clark & Currans root features, since the category we use for the full stop ensures that it must appear at the root of any complete derivation." ></td>
	<td class="line x" title="108:172	Model #Alph-feats #Feats Acc Time full-model 2402173 576176 96.40% 08:53 lp-ngram 1127437 342025 94.52% 05:19 lp-syn 1274740 291728 85.03% 05:57 Table 2: Perceptron Training Detailsnumber of features in the alphabet, number of features in the model, training accuracy and training time (hours) for 10 iterations on a single commodity server tively weight n-grams that appear in some of the GEN(xi) candidates, but which never appear in the naturally occurring corpus used to train a standard, generative language model." ></td>
	<td class="line x" title="109:172	Since our factored language model considers words, semantic classes, part-of-speech tags and supertags, our ngram features represent a considerable generalization of the sequence-oriented features in Velldal & Oepens model, which never contain more than one word and do not include semantic classes." ></td>
	<td class="line x" title="110:172	4 Evaluation 4.1 Experimental Conditions For the experiments reported below, we used a lexico-grammar extracted from Sections 0221 of our enhanced CCGbank, a hypertagging model incorporating named entity class features, and a trigram factored language model over words, named entity classes, part-of-speech tags and supertags, as described in the preceding section." ></td>
	<td class="line x" title="111:172	BLEU scores were calculated after removing the underscores between collapsed NEs." ></td>
	<td class="line x" title="112:172	Events were generated for each training section separately." ></td>
	<td class="line x" title="113:172	As already noted, the hypertagger and POS/supertag language model was trained on all the training sections, while separate word-based models were trained excluding each of the training sections in turn." ></td>
	<td class="line x" title="114:172	Event files for 26530 training sentences with complete realizations were generated in 7 hours and 16 minutes on a cluster using one commodity server per section, with an average n-best list size of 18.2." ></td>
	<td class="line x" title="115:172	Perceptron models were trained on single machines; details for three of the models appear in Table 2." ></td>
	<td class="line x" title="116:172	The complete set of models is listed in Table 3." ></td>
	<td class="line x" title="117:172	4.2 Results Realization results on the development section are given in Table 4." ></td>
	<td class="line x" title="118:172	As the first block of rows afterthebaselineshows,ofthemodelsincorporating a single kind of feature, only the one with the ngram log prob features beats the baseline BLEU 415 Model Description baseline-w3 No perceptron (3g wd only) baseline No perceptron syn-only-nodist All syntactic features except distance ngram-only Just ngram features syn-only Just syntactic features lp-only Just log prob features lp-ngram Log prob + Ngram features lp-syn Log prob + Syntactic features full-model Log prob + Ngram +Syntactic features Table 3: Legend for Experimental Conditions score, with the other models falling well below the baseline (though faring better than the trigramword LM baseline)." ></td>
	<td class="line x" title="119:172	This result confirms the importance of including n-gram log prob features in discriminative realization ranking models, in line with Velldal & Oepens findings, and contra those of Nakanishi et al., even though it was Nakanishi et al. who experimented with the Penn Treebank corpus,whileVelldal&Oepensexperimentswere on a much smaller, limited domain corpus." ></td>
	<td class="line x" title="120:172	The second block of rows shows that both the discriminative n-gram features and the syntactic features provide a substantial boost when used with the ngram log prob features, with the syntactic features yielding a more than 3 BLEU point gain." ></td>
	<td class="line x" title="121:172	The final row shows that the full model works best, though the boosts provided by the syntactic and discriminative n-gram features are clearly not independent." ></td>
	<td class="line x" title="122:172	TheBLEUpointtrendsaremirroredin the percentage of exact match realizations, which goes up by more than 10% from the baseline." ></td>
	<td class="line x" title="123:172	The percentage of complete (i.e., non-fragmentary) realizations, however, goes down; we expect that this is due to the time taken up by our current naive method of feature extraction, which does not cache the features calculated for partial realizations." ></td>
	<td class="line x" title="124:172	Realization results on the standard test section appear in Table 5, confirming the gains made by the full model over the baseline.5 We calculated statistical significance for the main results on the development section using bootstrap random sampling.6 After re-sampling 1000 times, significance was calculated using a paired t-test (999 d.f.)." ></td>
	<td class="line x" title="125:172	The results indicated that lp-only exceeded the baseline, lp-ngram and lp5Note that the baseline for Section 23 uses 4-grams and a filter for balanced punctuation (White and Rajkumar, 2008), unliketheotherreportedconfigurations, whichwouldexplain the somewhat smaller increase seen with this section." ></td>
	<td class="line x" title="126:172	6Scripts for running these tests are available at http://projectile.sv.cmu.edu/research/ public/tools/bootStrap/tutorial.htm Model %Exact %Compl." ></td>
	<td class="line x" title="127:172	BLEU Time baseline-w3 26.00 83.15 0.7646 1.8 baseline 29.00 83.28 0.7963 2.0 syn-only-nodist 26.02 82.69 0.7754 3.2 ngram-only 27.67 82.95 0.7777 3.0 syn-only 28.34 82.74 0.7838 3.4 lp-only 32.01 83.02 0.8009 2.1 lp-ngram 36.31 80.47 0.8183 3.1 lp-syn 39.47 79.74 0.8323 3.5 full-model 40.11 79.63 0.8373 3.6 Table 4: Section 00 Results (98.9% coverage) percentage of exact match and grammatically complete realizations, BLEU scores and average times, in seconds Model %Exact %Complete BLEU baseline 33.74 85.04 0.8173 full-model 40.45 83.88 0.8506 Table 5: Section 23 Results (97.06% coverage) syn exceeded lp-only, and the full model exceeded lp-syn, with p < 0.0001 in each case." ></td>
	<td class="line x" title="128:172	4.3 Examples Table 6 presents four examples where the full model improves upon the baseline." ></td>
	<td class="line x" title="129:172	Example sentence wsj 0020.10 in Table 6 is a case where the perceptron successfully weights the component ngram models, as the lp-ngram model and those thatbuildonitgetitright." ></td>
	<td class="line x" title="130:172	Notethathere,themodifier ordering in small video-viewing is not specified in the logical form and either ordering is possible syntactically." ></td>
	<td class="line x" title="131:172	In wsj 0024.2, number agreement between the conjoined subject noun phrase and verb is obtained only with the full model." ></td>
	<td class="line x" title="132:172	This suggests that the full model is more robust to cases where the grammar is insufficiently precise (number agreement is enforced by the grammar in only the simplest cases)." ></td>
	<td class="line x" title="133:172	Example wsj 0034.9 corrects a VP ordering mismatch, where the corpus sentence is clearly preferred to the one where into oblivion is shifted to the end." ></td>
	<td class="line x" title="134:172	Finally, wsj 0047.13 corrects an animacy mismatch on the wh-pronoun, in large part due to the high negative weight assigned to the discriminative n-gram feature PERSON , which." ></td>
	<td class="line x" title="135:172	Note that the full model still differs from the original sentence in its placement of the adverb reportedly, choosing the arguably more natural position following the auxiliary." ></td>
	<td class="line x" title="136:172	4.4 Comparison to Other Systems Table 7 lists our results in the context of those reported for other systems on PTB Section 23." ></td>
	<td class="line x" title="137:172	The 416 Ref-wsj 0020.10 that measure could compel Taipei s growing number of small video-viewing parlors to pay  baseline,syn-only,ngram-only that measure could compel Taipei s growing number of video-viewing small parlors to  lp-only, lp-ngram, full-model that measure could compel Taipei s growing number of small video-viewing parlors to  Ref-wsj 0024.2 Esso Australia Ltd. , a unit of new york-based Exxon Corp. , and Broken Hill Pty." ></td>
	<td class="line x" title="138:172	operate the fields  all except full-model Esso Australia Ltd. , a unit of new york-based Exxon Corp. , and Broken Hill Pty." ></td>
	<td class="line x" title="139:172	operates the fields  full-model Esso Australia Ltd. , a unit of new york-based Exxon Corp. , and Broken Hill Pty." ></td>
	<td class="line x" title="140:172	operate the fields  Ref-wsj 0034.9 they fell into oblivion after the 1929 crash . baseline, lp-ngram they fell after the 1929 crash into oblivion . lp-only, ngram-only, syn-only, full-model they fell into oblivion after the 1929 crash . Ref-wsj 0047.13 Antonio Novello , whom Mr. Bush nominated to serve as surgeon general , reportedly has assured  baseline,baseline-w3, lp-syn, lp-only Antonio Novello , which Mr. Bush nominated to serve as surgeon general , has reportedly assured  full-model, lp-ngram, syn-only, ngram-syn Antonio Novello , whom Mr. Bush nominated to serve as surgeon general , has reportedly assured  Table 6: Examples of realized output System Coverage BLEU %Exact Callaway (05) 98.5% 0.9321 57.5 OpenCCG (09) 97.1% 0.8506 40.5 Ringger et al.(04) 100.0% 0.836 35.7 Langkilde-Geary (02) 83% 0.757 28.2 Guo et al.(08) 100.0% 0.7440 19.8 Hogan et al.(07) 100.0% 0.6882 OpenCCG (08) 96.0% 0.6701 16.0 Nakanishi et al.(05) 90.8% 0.7733 Table 7: PTB Section 23 BLEU scores and exact match percentages in the NLG literature (Nakanishi et al.s results are for sentences of length 20 or less) most similar systems to ours are those of Nakanishi et al.(2005) and Hogan et al.(2007), as they both involve chart realizers for reversible grammars engineered from the Penn Treebank." ></td>
	<td class="line x" title="147:172	While direct comparisons across systems cannot really be made when inputs vary in their semantic depth and specificity, we observe that our all-sentences BLEU score of 0.8506 exceeds that of Hogan et al., who report a top score of 0.6882 (though with coverage near 100%), and also surpasses Nakanishietal.sscoreof0.7733,despitetheirresultsbeing limited to sentences of length 20 or less (with 91% coverage)." ></td>
	<td class="line x" title="148:172	Velldal & Oepens (2005) system is also closely related, as noted in the introduction, but as their experiments are on a limited domain corpus, their results cannot be compared at all meaningfully." ></td>
	<td class="line x" title="149:172	5 Related Work and Discussion As alluded to above, realization systems cannot be easily compared, even on the same corpus, when their inputs are not the same." ></td>
	<td class="line x" title="150:172	This point is dramatically illustrated in Langkilde-Gearys (2002) system, where a BLEU score of 0.514 is reported for minimally specified inputs on PTB Section 23, while a score of 0.757 is reported for the Permute, no dir case (which perhaps most closely resembles our inputs), and a score of 0.924 is reported for the most fully specified inputs; note, however, that in the latter case word order is determined by sibling order in the inputs, an assumption not commonly made." ></td>
	<td class="line x" title="151:172	As another example, Guo et al.(2008) report a competitive result of 0.7440(with100%coverage)usingadependencybased approach; however, their inputs, like those of Hogan et al., include more surface syntactic information than ours, as they specify case-marking prepositions, wh-pronouns and complementizers." ></td>
	<td class="line x" title="153:172	In a recent experiment to assess the impact of input specificity, we found that including predicates for all prepositions in our logical forms boosted our baseline results by more than 3 BLEU points, with complete realizations found in more than 90% of the test cases, indicating that generating from a more surfacy input is indeed an easier task than generating from a deeper representation." ></td>
	<td class="line x" title="154:172	Given the current lack of consensus on realizer input specificity, we believe it is important to keep in mind that within-system comparisons (such as those in the preceding section) are the ones that should be given the most credence." ></td>
	<td class="line x" title="155:172	Returning to our cross-system comparison, it is perhaps surprising that Callaway (2005) reports the best PTB BLEU score to date, 0.9321, with 98.5% coverage, using a purely symbolic, handcrafted grammar augmented to handle the most frequent coverage issues for the PTB." ></td>
	<td class="line x" title="156:172	While Callaways inputs are unordered, word order is often determined by positional features (e.g. front) or by the type of modification (e.g. describer vs. qualifier), and parts-of-speech are included for lexical items." ></td>
	<td class="line x" title="157:172	Additionally, in contrast to our approach, Callaway makes use of a generationonly grammar, rather than a reversible one, and his approach is less well-suited to producing n-best 417 outputs." ></td>
	<td class="line x" title="158:172	Nevertheless, his high scores do suggest the potential for precise grammar engineering to improve realization quality." ></td>
	<td class="line x" title="159:172	While we have yet to perform a thorough error analysis, our impression is that although the current set of syntactic features substantially improves clausal constituent ordering, a variety of disfluent cases remain." ></td>
	<td class="line x" title="160:172	More thorough investigations of features for constituent ordering in English have been performed by Ringger et al.(2004), Filippova and Strube (2009) and Zhong and Stent (2009), all of whom develop classifiers for determining linear order." ></td>
	<td class="line x" title="162:172	In future work, we plan to investigate whether features inspired by these approaches can be usefully integrated into our perceptron reranker." ></td>
	<td class="line x" title="163:172	Also related to the present work is discriminative training in syntax-based MT (Turian et al., 2007; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009)." ></td>
	<td class="line x" title="164:172	Not surprisingly, since MT is a harder problem than surface realization, syntaxbased MT systems have made use of less precise grammars and more impoverished (target-side) feature sets than those tackling realization ranking." ></td>
	<td class="line x" title="165:172	With progress on discriminative training with large numbers of features in syntax-based MT, the features found to be useful for high-quality surface realization may become increasingly relevant for MT as well." ></td>
	<td class="line x" title="166:172	6 Conclusions In this paper, we have shown how discriminative reranking with an averaged perceptron model can be used to achieve substantial improvements in realization quality with CCG." ></td>
	<td class="line x" title="167:172	Using a comprehensive feature set, we have also confirmed the utility of including language model log probabilities as features in the model, which prior work on discriminative training with log linear models for HPSG realization had called into question." ></td>
	<td class="line x" title="168:172	The perceptron model allows the combination of multiple n-gram models to be optimized and then augmented with both syntactic features and discriminative n-gram features, inspired by related work in discriminative parsing and language modeling for speech recognition." ></td>
	<td class="line x" title="169:172	The full model yields a state-of-the-art BLEU score of 0.8506 on Section 23 of the CCGbank, to our knowledge the best score reported to date using a reversible, corpusengineered grammar, despite our use of deeper, less specific inputs." ></td>
	<td class="line x" title="170:172	Finally, the perceptron model paves the way for exploring the utility of richer feature spaces in statistical realization, including the use of linguistically-motivated and non-local features, a topic which we plan to investigate in future work." ></td>
	<td class="line x" title="171:172	Acknowledgements This work was supported in part by NSF grant IIS0812297 and by an allocation of computing time from the Ohio Supercomputer Center." ></td>
	<td class="line x" title="172:172	Our thanks also to the OSU Clippers group and the anonymous reviewers for helpful comments and discussion." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1127
Bilingually-Constrained (Monolingual) Shift-Reduce Parsing
Huang, Liang;Jiang, Wenbin;Liu, Qun;"></td>
	<td class="line x" title="1:211	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 12221231, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:211	c 2009 ACL and AFNLP Bilingually-Constrained (Monolingual) Shift-Reduce Parsing Liang Huang Google Research 1350 Charleston Rd. Mountain View, CA 94043, USA lianghuang@google.com liang.huang.sh@gmail.com Wenbin Jiang and Qun Liu Key Lab." ></td>
	<td class="line x" title="3:211	of Intelligent Information Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China jiangwenbin@ict.ac.cn Abstract Jointly parsing two languages has been shown to improve accuracies on either or both sides." ></td>
	<td class="line x" title="4:211	However, its search space is much bigger than the monolingual case, forcing existing approaches to employ complicated modeling and crude approximations." ></td>
	<td class="line x" title="5:211	Here we propose a much simpler alternative, bilingually-constrained monolingual parsing, where a source-language parser learns to exploit reorderings as additional observation, but not bothering to build the target-side tree as well." ></td>
	<td class="line x" title="6:211	We show specifically how to enhance a shift-reduce dependency parser with alignment features to resolve shift-reduce conflicts." ></td>
	<td class="line x" title="7:211	Experiments on the bilingual portion of Chinese Treebank show that, with just 3 bilingual features, we can improve parsing accuracies by 0.6% (absolute) for both English and Chinese over a state-of-the-art baseline, with negligible (6%) efficiency overhead, thus much faster than biparsing." ></td>
	<td class="line x" title="8:211	1 Introduction Ambiguity resolution is a central task in Natural Language Processing." ></td>
	<td class="line x" title="9:211	Interestingly, not all languages are ambiguous in the same way." ></td>
	<td class="line x" title="10:211	For example, prepositional phrase (PP) attachment is (notoriously) ambiguous in English (and related European languages), but is strictly unambiguous in Chinese and largely unambiguous Japanese; see (1a) I [ saw Bill ] [ with a telescope ]." ></td>
	<td class="line x" title="11:211	wo [ yong wangyuanjin] [kandao le Bier]." ></td>
	<td class="line x" title="12:211	I used a telescope to see Bill. (1b) I saw [ Bill [ with a telescope ] ]." ></td>
	<td class="line x" title="13:211	wo kandao le [ [ na wangyuanjin ] de Bier]." ></td>
	<td class="line x" title="14:211	I saw Bill who had a telescope at hand. Figure 1: PP-attachment is unambiguous in Chinese, which can help English parsing." ></td>
	<td class="line x" title="15:211	Figure 1 for an example.1 It is thus intuitive to use two languages for better disambiguation, which has been applied not only to this PP-attachment problem (Fossum and Knight, 2008; Schwartz et al., 2003), but also to the more fundamental problem of syntactic parsing which subsumes the former as a subproblem." ></td>
	<td class="line x" title="16:211	For example, Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints, which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al., 2004; Mi and Huang, 2008)." ></td>
	<td class="line x" title="17:211	However, the search space of joint parsing is inevitably much bigger than the monolingual case, 1Chinese uses word-order to disambiguate the attachment (see below)." ></td>
	<td class="line x" title="18:211	By contrast, Japanese resorts to case-markers and the unambiguity is limited: it works for the V or N attachment ambiguities like in Figure 1 (see (Schwartz et al., 2003)) but not for the N1 or N2 case (Mitch Marcus, p.c.)." ></td>
	<td class="line x" title="19:211	1222 forcing existing approaches to employ complicated modeling and crude approximations." ></td>
	<td class="line x" title="20:211	Joint parsing with a simplest synchronous context-free grammar (Wu, 1997) is O(n6) as opposed to the monolingual O(n3) time." ></td>
	<td class="line x" title="21:211	To make things worse, languages are non-isomorphic, i.e., there is no 1to-1 mapping between tree nodes, thus in practice one has to use more expressive formalisms such as synchronous tree-substitution grammars (Eisner, 2003; Galley et al., 2004)." ></td>
	<td class="line x" title="22:211	In fact, rather than joint parsing per se, Burkett and Klein (2008) resort to separate monolingual parsing and bilingual reranking over k2 tree pairs, which covers a tiny fraction of the whole space (Huang, 2008)." ></td>
	<td class="line x" title="23:211	We instead propose a much simpler alternative, bilingually-constrained monolingual parsing, where a source-language parser is extended to exploit the reorderings between languages as additional observation, but not bothering to build a tree for the target side simultaneously." ></td>
	<td class="line x" title="24:211	To illustrate the idea, suppose we are parsing the sentence (1) I saw Bill [PP with a telescope ]." ></td>
	<td class="line x" title="25:211	which has 2 parses based on the attachment of PP: (1a) I [ saw Bill ] [PP with a telescope ]." ></td>
	<td class="line x" title="26:211	(1b) I saw [ Bill [PP with a telescope ]]." ></td>
	<td class="line x" title="27:211	Both are possible, but with a Chinese translation the choice becomes clear (see Figure 1), because a Chinese PP always immediately precedes the phrase it is modifying, thus making PP-attachment strictly unambiguous.2 We can thus use Chinese to help parse English, i.e., whenever we have a PPattachment ambiguity, we will consult the Chinese translation (from a bitext), and based on the alignment information, decide where to attach the English PP." ></td>
	<td class="line x" title="28:211	On the other hand, English can help Chinese parsing as well, for example in deciding the scope of relative clauses which is unambiguous in English but ambiguous in Chinese." ></td>
	<td class="line x" title="29:211	This method is much simpler than joint parsing because it remains monolingual in the backbone, with alignment information merely as soft evidence, rather than hard constraints since automatic word alignment is far from perfect." ></td>
	<td class="line x" title="30:211	It is thus 2to be precise, in Fig." ></td>
	<td class="line x" title="31:211	1(b), the English PP is translated into a Chinese relative clause, but nevertheless all phrasal modifiers attach to the immediate right in Mandarin Chinese." ></td>
	<td class="line x" title="32:211	straightforward to implement within a monolingual parsing algorithm." ></td>
	<td class="line x" title="33:211	In this work we choose shift-reduce dependency parsing for its simplicity and efficiency." ></td>
	<td class="line x" title="34:211	Specifically, we make the following contributions:  we develop a baseline shift-reduce dependency parser using the less popular, but classical, arc-standard style (Section 2), and achieve similar state-of-the-art performance with the the dominant but complicated arceager style of Nivre and Scholz (2004);  we propose bilingual features based on wordalignment information to prefer target-side contiguity in resolving shift-reduce conflicts (Section 3);  we verify empirically that shift-reduce conflicts are the major source of errors, and correct shift-reduce decisions strongly correlate with the above bilingual contiguity conditions even with automatic alignments (Section 5.3);  finally, with just three bilingual features, we improve dependency parsing accuracy by 0.6% for both English and Chinese over the state-of-the-art baseline with negligible (6%) efficiency overhead (Section 5.4)." ></td>
	<td class="line x" title="35:211	2 Simpler Shift-Reduce Dependency Parsing with Three Actions The basic idea of classical shift-reduce parsing from compiler theory (Aho and Ullman, 1972) is to perform a left-to-right scan of the input sentence, and at each step, choose one of the two actions: either shift the current word onto the stack, or reduce the top two (or more) items on the stack, replacing them with their combination." ></td>
	<td class="line x" title="36:211	This idea has been applied to constituency parsing, for example in Sagae and Lavie (2006), and we describe below a simple variant for dependency parsing similar to Yamada and Matsumoto (2003) and the arc-standard version of Nivre (2004)." ></td>
	<td class="line x" title="37:211	2.1 The Three Actions Basically, we just need to split the reduce action into two symmetric (sub-)actions, reduceL and reduceR, depending on which one of the two 1223 stack queue arcs previous S wi|Q A shift S|wi Q A previous S|st1|st Q A reduceL S|st Q A{(st,st1)} reduceR S|st1 Q A{(st1,st)} Table 1: Formal description of the three actions." ></td>
	<td class="line x" title="38:211	Note that shift requires non-empty queue while reduce requires at least two elements on the stack." ></td>
	<td class="line x" title="39:211	items becomes the head after reduction." ></td>
	<td class="line x" title="40:211	More formally, we describe a parser configuration by a tupleS,Q,Awhere S is the stack, Q is the queue of remaining words of the input, and A is the set of dependency arcs accumulated so far.3 At each step, we can choose one of the three actions: 1." ></td>
	<td class="line x" title="41:211	shift: move the head of (a non-empty) queue Q onto stack S; 2." ></td>
	<td class="line x" title="42:211	reduceL: combine the top two items on the stack, st and st1 (t  2), and replace them with st (as the head), and add a left arc (st,st1) to A; 3." ></td>
	<td class="line x" title="43:211	reduceR: combine the top two items on the stack, st and st1 (t2), and replace them with st1 (as the head), and add a right arc (st1,st) to A. These actions are summarized in Table 1." ></td>
	<td class="line x" title="44:211	The initial configuration is always ,w1 wn, with empty stack and no arcs, and the final configuration iswj,,Awhere wj is recognized as the root of the whole sentence, and A encodes a spanning tree rooted at wj." ></td>
	<td class="line x" title="45:211	For a sentence of n words, there are exactly 2n1 actions: n shifts and n1 reductions, since every word must be pushed onto stack once, and every word except the root will eventually be popped in a reduction." ></td>
	<td class="line x" title="46:211	The time complexity, as other shift-reduce instances, is clearly O(n)." ></td>
	<td class="line x" title="47:211	2.2 Example of Shift-Reduce Conflict Figure 2 shows the trace of this paradigm on the example sentence." ></td>
	<td class="line x" title="48:211	For the first two configurations 3a configuration is sometimes called a state (Zhang and Clark, 2008), but that term is confusing with the states in shift-reduce LR/LL parsing, which are quite different." ></td>
	<td class="line x" title="49:211	0 I saw Bill with a  1 shift I saw Bill with a  2 shift I saw Bill with a  3 reduceL saw Bill with a  I 4 shift saw Bill with a  I 5a reduceR saw with a  I Bill 5b shift saw Bill with a  I Figure 2: A trace of 3-action shift-reduce on the example sentence." ></td>
	<td class="line x" title="50:211	Shaded words are on stack, while gray words have been popped from stack." ></td>
	<td class="line x" title="51:211	After step (4), the process can take either (5a) or (5b), which correspond to the two attachments (1a) and (1b) in Figure 1, respectively." ></td>
	<td class="line x" title="52:211	(0) and (1), only shift is possible since there are not enough items on the stack for reduction." ></td>
	<td class="line x" title="53:211	At step (3), we perform a reduceL, making word I a modifier of saw; after that the stack contains a single word and we have to shift the next word Bill (step 4)." ></td>
	<td class="line x" title="54:211	Now we face a shift-reduce conflict: we can either combine saw and Bill in a reduceR action (5a), or shift Bill (5b)." ></td>
	<td class="line x" title="55:211	We will use features extracted from the configuration to resolve the conflict." ></td>
	<td class="line x" title="56:211	For example, one such feature could be a bigram stst1, capturing how likely these two words are combined; see Table 2 for the complete list of feature templates we use in this baseline parser." ></td>
	<td class="line x" title="57:211	We argue that this kind of shift-reduce conflicts are the major source of parsing errors, since the other type of conflict, reduce-reduce conflict (i.e., whether left or right) is relatively easier to resolve given the part-of-speech information." ></td>
	<td class="line x" title="58:211	For example, between a noun and an adjective, the former is much more likely to be the head (and so is a verb vs. a preposition or an adverb)." ></td>
	<td class="line x" title="59:211	Shift-reduce resolution, however, is more non-local, and often involves a triple, for example, (saw, Bill, with) for a typical PP-attachment." ></td>
	<td class="line x" title="60:211	On the other hand, if we indeed make a wrong decision, a reduce-reduce mistake just flips the head and the modifier, and often has a more local effect on the shape of the tree, whereas a shift-reduce mistake always leads 1224 Type Features Unigram st T(st) stT(st) st1 T(st1) st1T(st1) wi T(wi) wiT(wi) Bigram stst1 T(st)T(st1) T(st)T(wi) T(st)st1T(st1) stst1T(st1) stT(st)T(st1) stT(st)st1 stT(st)st1T(st1) Trigram T(st)T(wi)T(wi+1) T(st1)T(st)T(wi) T(st2)T(st1)T(st) stT(wi)T(wi+1) T(st1)stT(wi) Modifier T(st1)T(lc(st1))T(st) T(st1)T(rc(st1))T(st) T(st1)T(st)T(lc(st)) T(st1)T(st)T(rc(st)) T(st1)T(lc(st1))st T(st1)T(rc(st1))st T(st1)stT(lc(st)) Table 2: Feature templates of the baseline parser." ></td>
	<td class="line x" title="61:211	st, st1 denote the top and next to top words on the stack; wi and wi+1 denote the current and next words on the queue." ></td>
	<td class="line x" title="62:211	T() denotes the POS tag of a given word, and lc() and rc() represent the leftmost and rightmost child." ></td>
	<td class="line x" title="63:211	Symbol  denotes feature conjunction." ></td>
	<td class="line x" title="64:211	Each of these templates is further conjoined with the 3 actions shift, reduceL, and reduceR." ></td>
	<td class="line x" title="65:211	to vastly incompatible tree shapes with crossing brackets (for example, [saw Bill] vs. [Bill with a telescope])." ></td>
	<td class="line x" title="66:211	We will see in Section 5.3 that this is indeed the case in practice, thus suggesting us to focus on shift-reduce resolution, which we will return to with the help of bilingual constraints in Section 3." ></td>
	<td class="line x" title="67:211	2.3 Comparison with Arc-Eager The three action system was originally described by Yamada and Matsumoto (2003) (although their methods require multiple passes over the input), and then appeared as arc-standard in Nivre (2004), but was argued against in comparison to the four-action arc-eager variant." ></td>
	<td class="line x" title="68:211	Most subsequent works on shift-reduce or transition-based dependency parsing followed arc-eager (Nivre and Scholz, 2004; Zhang and Clark, 2008), which now becomes the dominant style." ></td>
	<td class="line x" title="69:211	But we argue that arc-standard is preferable because: 1." ></td>
	<td class="line x" title="70:211	in the three action arc-standard system, the stack always contains a list of unrelated subtrees recognized so far, with no arcs between any of them, e.g.(Isaw) and (Bill) in step 4 of Figure 2), whereas the four action arceager style can have left or right arrows between items on the stack; 2." ></td>
	<td class="line x" title="72:211	the semantics of the three actions are atomic and disjoint, whereas the semantics of 4 actions are not completely disjoint." ></td>
	<td class="line x" title="73:211	For example, their Left action assumes an implicit Reduce of the left item, and their Right action assumes an implicit Shift." ></td>
	<td class="line x" title="74:211	Furthermore, these two actions have non-trivial preconditions which also causes the next problem (see below)." ></td>
	<td class="line x" title="75:211	We argue that this is rather complicated to implement." ></td>
	<td class="line x" title="76:211	3." ></td>
	<td class="line x" title="77:211	the arc-standard scan always succeeds, since at the end we can always reduce with empty queue, whereas the arc-eager style sometimes goes into deadends where no action can perform (prevented by preconditions, otherwise the result will not be a wellformed tree)." ></td>
	<td class="line x" title="78:211	This becomes parsing failures in practice (Nivre and Scholz, 2004), leaving more than one fragments on stack." ></td>
	<td class="line x" title="79:211	As we will see in Section 5.1, this simpler arc-standard system performs equally well with a state-of-the-art arc-eager system (Zhang and Clark, 2008) on standard English Treebank parsing (which is never shown before)." ></td>
	<td class="line x" title="80:211	We argue that all things being equal, this simpler paradigm should be preferred in practice." ></td>
	<td class="line x" title="81:211	4 2.4 Beam Search Extension We also enhance deterministic shift-reduce parsing with beam search, similar to Zhang and Clark (2008), where k configurations develop in parallel." ></td>
	<td class="line x" title="82:211	Pseudocode 1 illustrates the algorithm, where we keep an agenda V of the current active configurations, and at each step try to extend them by applying one of the three actions." ></td>
	<td class="line x" title="83:211	We then dump the best k new configurations from the buffer back 4On the other hand, there are also arguments for arceager, e.g., incrementality; see (Nivre, 2004; Nivre, 2008)." ></td>
	<td class="line x" title="84:211	1225 Pseudocode 1 beam-search shift-reduce parsing." ></td>
	<td class="line x" title="85:211	1: Input: POS-tagged word sequence w1 wn 2: start ,w1 wn,  initial config: empty stack, no arcs 3: V{start}  initial agenda 4: for step12n1 do 5: BUF  buffer for new configs 6: for each config in agenda V do 7: for act {shift, reduceL, reduceR}do 8: if act is applicable to config then 9: next apply act to config 10: insert next into buffer BUF 11: Vtop k configurations of BUF 12: Output: the tree of the best config in V into the agenda for the next step." ></td>
	<td class="line x" title="86:211	The complexity of this algorithm is O(nk), which subsumes the determinstic mode as a special case (k = 1)." ></td>
	<td class="line x" title="87:211	2.5 Online Training To train the parser we need an oracle or goldstandard action sequence for gold-standard dependency trees." ></td>
	<td class="line x" title="88:211	This oracle turns out to be non-unique for the three-action system (also non-unique for the four-action system), because left dependents of a head can be reduced either before or after all right dependents are reduced." ></td>
	<td class="line x" title="89:211	For example, in Figure 2, I is a left dependent of saw, and can in principle wait until Bill and with are reduced, and then finally combine with saw." ></td>
	<td class="line x" title="90:211	We choose to use the heuristic of shortest stack that always prefers reduceL over shift, which has the effect that all left dependents are first recognized inside-out, followed by all right dependents, also inside-out, which coincides with the head-driven constituency parsing model of Collins (1999)." ></td>
	<td class="line x" title="91:211	We use the popular online learning algorithm of structured perceptron with parameter averaging (Collins, 2002)." ></td>
	<td class="line oc" title="92:211	Following Collins and Roark (2004) we also use the early-update strategy, where an update happens whenever the goldstandard action-sequence falls off the beam, with the rest of the sequence neglected." ></td>
	<td class="line x" title="93:211	As a special case, for the deterministic mode, updates always co-occur with the first mistake made." ></td>
	<td class="line x" title="94:211	The intuition behind this strategy is that future mistakes are often caused by previous ones, so with the parser on the wrong track, future actions become irrelevant for learning." ></td>
	<td class="line x" title="95:211	See Section 5.3 for more discussions." ></td>
	<td class="line x" title="96:211	(a) I a58a58a58a58a58a58a58a58a58saw Bill with a telescope . wo yong wangyuanjin kandao le Bier." ></td>
	<td class="line x" title="97:211	c(st1,st) =+; reduce is correct (b) I a58a58a58a58a58a58a58a58a58saw Bill with a telescope . wo kandao le na wangyuanjin de Bier." ></td>
	<td class="line x" title="98:211	c(st1,st) =; reduce is wrong (c) I saw a58a58a58a58a58a58a58a58a58a58a58Bill witha58a58a58aa58a58a58a58a58a58a58a58a58a58telescopea58 . wo kandao le na wangyuanjin de Bier." ></td>
	<td class="line x" title="99:211	cR(st,wi) =+; shift is correct (d) I saw a58a58a58a58a58a58a58a58a58Bill witha58a58a58aa58a58a58a58a58a58a58a58a58a58telescopea58 . wo yong wangyuanjin kandao le Bier." ></td>
	<td class="line x" title="100:211	cR(st,wi) =; shift is wrong Figure 3: Bilingual contiguity features c(st1,st) and cR(st,wi) at step (4) in Fig." ></td>
	<td class="line x" title="101:211	2 (facing a shiftreduce decision)." ></td>
	<td class="line x" title="102:211	Bold words are currently on stack while gray ones have been popped." ></td>
	<td class="line x" title="103:211	Here the stack tops are st = Bill, st1 = saw, and the queue head is wi = with; underlined texts mark the source and target spans being considered, and wavy underlines mark the allowed spans (Tab." ></td>
	<td class="line x" title="104:211	3)." ></td>
	<td class="line x" title="105:211	Red bold alignment links violate contiguity constraints." ></td>
	<td class="line x" title="106:211	3 Soft Bilingual Constraints as Features As suggested in Section 2.2, shift-reduce conflicts are the central problem we need to address here." ></td>
	<td class="line x" title="107:211	Our intuition is, whenever we face a decision whether to combine the stack tops st1 and st or to shift the current word wi, we will consult the other language, where the word-alignment information would hopefully provide a preference, as in the running example of PP-attachment (see Figure 1)." ></td>
	<td class="line x" title="108:211	We now develop this idea into bilingual contiguity features." ></td>
	<td class="line x" title="109:211	1226 3.1 A Pro-Reduce Feature c(st1,st) Informally, if the correct decision is a reduction, then it is likely that the corresponding words of st1 and st on the target-side should also form a contiguous span." ></td>
	<td class="line x" title="110:211	For example, in Figure 3(a), the source span of a reduction is [saw  Bill], which maps onto [kandao . . ." ></td>
	<td class="line x" title="111:211	Bier] on the Chinese side." ></td>
	<td class="line x" title="112:211	This target span is contiguous, because no word within this span is aligned to a source word outside of the source span." ></td>
	<td class="line x" title="113:211	In this case we say feature c(st1,st) =+, which encourages reduce." ></td>
	<td class="line x" title="114:211	However, in Figure 3(b), the source span is still [saw  Bill], but this time maps onto a much longer span on the Chinese side." ></td>
	<td class="line x" title="115:211	This target span is discontiguous, since the Chinese words na and wangyuanjin are alinged to English with and telescope, both of which fall outside of the source span." ></td>
	<td class="line x" title="116:211	In this case we say feature c(st1,st) =, which discourages reduce . 3.2 A Pro-Shift Feature cR(st,wi) Similarly, we can develop another feature cR(st,wi) for the shift action." ></td>
	<td class="line x" title="117:211	In Figure 3(c), when considering shifting with, the source span becomes [Bill  with] which maps to [na  Bier] on the Chinese side." ></td>
	<td class="line x" title="118:211	This target span looks like discontiguous in the above definition with wangyuanjin aligned to telescope, but we tolerate this case for the following reasons." ></td>
	<td class="line x" title="119:211	There is a crucial difference between shift and reduce: in a shift, we do not know yet the subtree spans (unlike in a reduce we are always combining two well-formed subtrees)." ></td>
	<td class="line x" title="120:211	The only thing we are sure of in a shift action is that st and wi will be combined before st1 and st are combined (Aho and Ullman, 1972), so we can tolerate any target word aligned to source word still in the queue, but do not allow any target word aligned to an already recognized source word." ></td>
	<td class="line x" title="121:211	This explains the notational difference between cR(st,wi) and c(st1,st), where subscript R means right contiguity." ></td>
	<td class="line x" title="122:211	As a final example, in Figure 3(d), Chinese word kandao aligns to saw, which is already recognized, and this violates the right contiguity." ></td>
	<td class="line x" title="123:211	So cR(st,wi) =, suggesting that shift is probably wrong." ></td>
	<td class="line x" title="124:211	To be more precise, Table 3 shows the formal definitions of the two features." ></td>
	<td class="line x" title="125:211	We basically source target allowed feature f span sp span tp span ap c(st1,st) [st1st] M(sp) [st1st] cR(st,wi) [stwi] M(sp) [stwn] f = + iff." ></td>
	<td class="line x" title="126:211	M1(M(sp))ap Table 3: Formal definition of bilingual features." ></td>
	<td class="line x" title="127:211	M() is maps a source span to the target language, and M1() is the reverse operation mapping back to the source language." ></td>
	<td class="line x" title="128:211	map a source span sp to its target span M(sp), and check whether its reverse image back onto the source language M1(M(sp)) falls inside the allowed span ap." ></td>
	<td class="line x" title="129:211	For cR(st,wi), the allowed span extends to the right end of the sentence.5 3.3 Variations and Implementation To conclude so far, we have got two alignmentbased features, c(st1,st) correlating with reduce, and cR(st,wi) correlating with shift." ></td>
	<td class="line x" title="130:211	In fact, the conjunction of these two features, c(st1,st)cR(st,wi) is another feature with even stronger discrimination power." ></td>
	<td class="line x" title="131:211	If c(st1,st)cR(st,wi) = + it is strongly recommending reduce, while c(st1,st)cR(st,wi) =+ is a very strong signal for shift." ></td>
	<td class="line x" title="132:211	So in total we got three bilingual feature (templates), which in practice amounts to 24 instances (after cross-product with {,+} and the three actions)." ></td>
	<td class="line x" title="133:211	We show in Section 5.3 that these features do correlate with the correct shift/reduce actions in practice." ></td>
	<td class="line x" title="134:211	The naive implemention of bilingual feature computation would be of O(kn2) complexity in the worse case because when combining the largest spans one has to scan over the whole sentence." ></td>
	<td class="line x" title="135:211	We envision the use of a clever datastructure would reduce the complexity, but leave this to future work, as the experiments (Table 8) show that 5Our definition implies that we only consider faithful spans to be contiguous (Galley et al., 2004)." ></td>
	<td class="line x" title="136:211	Also note that source spans include all dependents of st and st1." ></td>
	<td class="line x" title="137:211	1227 the parser is only marginally (6%) slower with the new bilingual features." ></td>
	<td class="line x" title="138:211	This is because the extra work, with just 3 bilingual features, is not the bottleneck in practice, since the extraction of the vast amount of other features in Table 2 dominates the computation." ></td>
	<td class="line x" title="139:211	4 Related Work in Grammar Induction Besides those cited in Section 1, there are some other related work on using bilingual constraints for grammar induction (rather than parsing)." ></td>
	<td class="line x" title="140:211	For example, Hwa et al.(2005) use simple heuristics to project English trees to Spanish and Chinese, but get discouraging accuracy results learned from those projected trees." ></td>
	<td class="line x" title="142:211	Following this idea, Ganchev et al.(2009) and Smith and Eisner (2009) use constrained EM and parser adaptation techniques, respectively, to perform more principled projection, and both achieve encouraging results." ></td>
	<td class="line x" title="144:211	Our work, by constrast, never uses bilingual tree pairs not tree projections, and only uses word alignment alone to enhance a monolingual grammar, which learns to prefer target-side contiguity." ></td>
	<td class="line x" title="145:211	5 Experiments 5.1 Baseline Parser We implement our baseline monolingual parser (in C++) based on the shift-reduce algorithm in Section 2, with feature templates from Table 2." ></td>
	<td class="line x" title="146:211	We evaluate its performance on the standard Penn English Treebank (PTB) dependency parsing task, i.e., train on sections 02-21 and test on section 23 with automatically assigned POS tags (at 97.2% accuracy) using a tagger similar to Collins (2002), and using the headrules of Yamada and Matsumoto (2003) for conversion into dependency trees." ></td>
	<td class="line x" title="147:211	We use section 22 as dev set to determine the optimal number of iterations in perceptron training." ></td>
	<td class="line x" title="148:211	Table 4 compares our baseline against the state-of-the-art graph-based (McDonald et al., 2005) and transition-based (Zhang and Clark, 2008) approaches, and confirms that our system performs at the same level with those stateof-the-art, and runs extremely fast in the deterministic mode (k=1), and still quite fast in the beamsearch mode (k=16)." ></td>
	<td class="line x" title="149:211	parser accuracy secs/sent McDonald et al.(2005) 90.7 0.150 Zhang and Clark (2008) 91.4 0.195 our baseline at k=1 90.2 0.009 our baseline at k=16 91.3 0.125 Table 4: Baseline parser performance on standard Penn English Treebank dependency parsing task." ></td>
	<td class="line x" title="151:211	The speed numbers are not exactly comparable since they are reported on different machines." ></td>
	<td class="line x" title="152:211	Training Dev Test CTB Articles 1-270 301-325 271-300 Bilingual Paris 2745 273 290 Table 5: Training, dev, and test sets from bilingual Chinese Treebank `a la Burkett and Klein (2008)." ></td>
	<td class="line x" title="153:211	5.2 Bilingual Data The bilingual data we use is the translated portion of the Penn Chinese Treebank (CTB) (Xue et al., 2002), corresponding to articles 1-325 of PTB, which have English translations with goldstandard parse trees (Bies et al., 2007)." ></td>
	<td class="line x" title="154:211	Table 5 shows the split of this data into training, development, and test subsets according to Burkett and Klein (2008)." ></td>
	<td class="line x" title="155:211	Note that not all sentence pairs could be included, since many of them are not oneto-one aligned at the sentence level." ></td>
	<td class="line x" title="156:211	Our wordalignments are generated from the HMM aligner of Liang et al.(2006) trained on approximately 1.7M sentence pairs (provided to us by David Burkett, p.c.)." ></td>
	<td class="line x" title="158:211	This aligner outputs soft alignments, i.e., posterior probabilities for each source-target word pair." ></td>
	<td class="line x" title="159:211	We use a pruning threshold of 0.535 to remove low-confidence alignment links,6 and use the remaining links as hard alignments; we leave the use of alignment probabilities to future work." ></td>
	<td class="line x" title="160:211	For simplicity reasons, in the following experiments we always supply gold-standard POS tags as part of the input to the parser." ></td>
	<td class="line x" title="161:211	5.3 Testing our Hypotheses Before evaluating our bilingual approach, we need to verify empirically the two assumptions we made about the parser in Sections 2 and 3: 6and also removing notoriously bad links in{the, a, an} {de, le}following Fossum and Knight (2008)." ></td>
	<td class="line x" title="162:211	1228 sh  re re  sh sh-re re-re # 92 98 190 7 % 46.7% 49.7% 96.4% 3.6% Table 6: [Hypothesis 1] Error distribution in the baseline model (k = 1) on English dev set." ></td>
	<td class="line x" title="163:211	sh  re means should shift, but reduced." ></td>
	<td class="line x" title="164:211	Shiftreduce conflicts overwhelmingly dominate." ></td>
	<td class="line x" title="165:211	1." ></td>
	<td class="line x" title="166:211	(monolingual) shift-reduce conflict is the major source of errors while reduce-reduce conflict is a minor issue; 2." ></td>
	<td class="line x" title="167:211	(bilingual) the gold-standard decisions of shift or reduce should correlate with contiguities of c(st1,st), and of cR(st,wi)." ></td>
	<td class="line x" title="168:211	Hypothesis 1 is verified in Table 6, where we count all the first mistakes the baseline parser makes (in the deterministic mode) on the English dev set (273 sentences)." ></td>
	<td class="line oc" title="169:211	In shift-reduce parsing, further mistakes are often caused by previous ones, so only the first mistake in each sentence (if there is one) is easily identifiable;7 this is also the argument for early update in applying perceptron learning to these incremental parsing algorithms (Collins and Roark, 2004) (see also Section 2)." ></td>
	<td class="line x" title="170:211	Among the 197 first mistakes (other 76 sentences have perfect output), the vast majority, 190 of them (96.4%), are shift-reduce errors (equally distributed between shift-becomesreduce and reduce-becomes-shift), and only 7 (3.6%) are due to reduce-reduce conflicts.8 These statistics confirm our intuition that shift-reduce decisions are much harder to make during parsing, and contribute to the overwhelming majority of errors, which is studied in the next hypothesis." ></td>
	<td class="line x" title="171:211	Hypothesis 2 is verified in Table 7." ></td>
	<td class="line x" title="172:211	We take the gold-standard shift-reduce sequence on the English dev set, and classify them into the four categories based on bilingual contiguity features: (a) c(st1,st), i.e. whether the top 2 spans on stack is contiguous, and (b) cR(st,wi), i.e. whether the 7to be really precise one can define independent mistakes as those not affected by previous ones, i.e., errors made after the parser recovers from previous mistakes; but this is much more involved and we leave it to future work." ></td>
	<td class="line x" title="173:211	8Note that shift-reduce errors include those due to the non-uniqueness of oracle, i.e., between some reduceL and shift." ></td>
	<td class="line x" title="174:211	Currently we are unable to identify genuine errors that would result in an incorrect parse." ></td>
	<td class="line x" title="175:211	See also Section 2.5." ></td>
	<td class="line x" title="176:211	c(st1,st) cR(st,wi) shift reduce +  172  1,209  + 1,432 > 805 + + 4,430  3,696   525  576 total 6,559 = 6,286 Table 7: [Hyp." ></td>
	<td class="line x" title="177:211	2] Correlation of gold-standard shift/reduce decisions with bilingual contiguity conditions (on English dev set)." ></td>
	<td class="line x" title="178:211	Note there is always one more shift than reduce in each sentence." ></td>
	<td class="line x" title="179:211	stack top is contiguous with the current word wi." ></td>
	<td class="line x" title="180:211	According to discussions in Section 3, when (a) is contiguous and (b) is not, it is a clear signal for reduce (to combine the top two elements on the stack) rather than shift, and is strongly supported by the data (first line: 1209 reduces vs. 172 shifts); and while when (b) is contiguous and (a) is not, it should suggest shift (combining st and wi before st1 and st are combined) rather than reduce, and is mildly supported by the data (second line: 1432 shifts vs. 805 reduces)." ></td>
	<td class="line x" title="181:211	When (a) and (b) are both contiguous or both discontiguous, it should be considered a neutral signal, and is also consistent with the data (next two lines)." ></td>
	<td class="line x" title="182:211	So to conclude, this bilingual hypothesis is empirically justified." ></td>
	<td class="line x" title="183:211	On the other hand, we would like to note that these correlations are done with automatic word alignments (in our case, from the Berkeley aligner) which can be quite noisy." ></td>
	<td class="line x" title="184:211	We suspect (and will finish in the future work) that using manual alignments would result in a better correlation, though for the main parsing results (see below) we can only afford automatic alignments in order for our approach to be widely applicable to any bitext." ></td>
	<td class="line x" title="185:211	5.4 Results We incorporate the three bilingual features (again, with automatic alignments) into the baseline parser, retrain it, and test its performance on the English dev set, with varying beam size." ></td>
	<td class="line x" title="186:211	Table 8 shows that bilingual constraints help more with larger beams, from almost no improvement with the deterministic mode (k=1) to +0.5% better with the largest beam (k=16)." ></td>
	<td class="line x" title="187:211	This could be explained by the fact that beam-search is more robust than the deterministic mode, where in the latter, if our 1229 baseline +bilingual k accuracy time (s) accuracy time (s) 1 84.58 0.011 84.67 0.012 2 85.30 0.025 85.62 0.028 4 85.42 0.040 85.81 0.044 8 85.50 0.081 85.95 0.085 16 85.57 0.158 86.07 0.168 Table 8: Effects of beam size k on efficiency and accuracy (on English dev set)." ></td>
	<td class="line x" title="188:211	Time is average per sentence (in secs)." ></td>
	<td class="line x" title="189:211	Bilingual constraints show more improvement with larger beams, with a fractional efficiency overhead over the baseline." ></td>
	<td class="line x" title="190:211	English Chinese monolingual baseline 86.9 85.7 +bilingual features 87.5 86.3 improvement +0.6 +0.6 signficance level p < 0.05 p < 0.08 Berkeley parser 86.1 87.9 Table 9: Final results of dependency accuracy (%) on the test set (290 sentences, beam size k=16)." ></td>
	<td class="line x" title="191:211	bilingual features misled the parser into a mistake, there is no chance of getting back, while in the former multiple configurations are being pursued in parallel." ></td>
	<td class="line x" title="192:211	In terms of speed, both parsers run proportionally slower with larger beams, as the time complexity is linear to the beam-size." ></td>
	<td class="line x" title="193:211	Computing the bilingual features further slows it down, but only fractionally so (just 1.06 times as slow as the baseline at k=16), which is appealing in practice." ></td>
	<td class="line x" title="194:211	By contrast, Burkett and Klein (2008) reported their approach of monolingual k-best parsing followed by bilingual k2-best reranking to be 3.8 times slower than monolingual parsing." ></td>
	<td class="line x" title="195:211	Our final results on the test set (290 sentences) are summarized in Table 9." ></td>
	<td class="line x" title="196:211	On both English and Chinese, the addition of bilingual features improves dependency arc accuracies by +0.6%, which is mildly significant using the Z-test of Collins et al.(2005)." ></td>
	<td class="line x" title="198:211	We also compare our results against the Berkeley parser (Petrov and Klein, 2007) as a reference system, with the exact same setting (i.e., trained on the bilingual data, and testing using gold-standard POS tags), and the resulting trees are converted into dependency via the same headrules." ></td>
	<td class="line x" title="199:211	We use 5 iterations of split-merge grammar induction as the 6th iteration overfits the small training set." ></td>
	<td class="line x" title="200:211	The result is worse than our baseline on English, but better than our bilingual parser on Chinese." ></td>
	<td class="line x" title="201:211	The discrepancy between English and Chinese is probably due to the fact that our baseline feature templates (Table 2) are engineered on English not Chinese." ></td>
	<td class="line x" title="202:211	6 Conclusion and Future Work We have presented a novel parsing paradigm, bilingually-constrained monolingual parsing, which is much simpler than joint (bi-)parsing, yet still yields mild improvements in parsing accuracy in our preliminary experiments." ></td>
	<td class="line x" title="203:211	Specifically, we showed a simple method of incorporating alignment features as soft evidence on top of a state-of-the-art shift-reduce dependency parser, which helped better resolve shift-reduce conflicts with fractional efficiency overhead." ></td>
	<td class="line x" title="204:211	The fact that we managed to do this with only three alignment features is on one hand encouraging, but on the other hand leaving the bilingual feature space largely unexplored." ></td>
	<td class="line x" title="205:211	So we will engineer more such features, especially with lexicalization and soft alignments (Liang et al., 2006), and study the impact of alignment quality on parsing improvement." ></td>
	<td class="line x" title="206:211	From a linguistics point of view, we would like to see how linguistics distance affects this approach, e.g., we suspect EnglishFrench would not help each other as much as English-Chinese do; and it would be very interesting to see what types of syntactic ambiguities can be resolved across different language pairs." ></td>
	<td class="line x" title="207:211	Furthermore, we believe this bilingual-monolingual approach can easily transfer to shift-reduce constituency parsing (Sagae and Lavie, 2006)." ></td>
	<td class="line x" title="208:211	Acknowledgments We thank the anonymous reviewers for pointing to us references about arc-standard." ></td>
	<td class="line x" title="209:211	We also thank Aravind Joshi and Mitch Marcus for insights on PP attachment, Joakim Nivre for discussions on arc-eager, Yang Liu for suggestion to look at manual alignments, and David A. Smith for sending us his paper." ></td>
	<td class="line x" title="210:211	The second and third authors were supported by National Natural Science Foundation of China, Contracts 60603095 and 60736014, and 863 State Key Project No. 2006AA010108." ></td>
	<td class="line x" title="211:211	1230" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1032
Learning with Annotation Noise
Beigman, Eyal;Beigman Klebanov, Beata;"></td>
	<td class="line x" title="1:149	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 280287, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:149	c2009 ACL and AFNLP Learning with Annotation Noise Eyal Beigman Olin Business School Washington University in St. Louis beigman@wustl.edu Beata Beigman Klebanov Kellogg School of Management Northwestern University beata@northwestern.edu Abstract It is usually assumed that the kind of noise existing in annotated data is random classification noise." ></td>
	<td class="line x" title="3:149	Yet there is evidence thatdifferencesbetweenannotatorsarenot always random attention slips but could result from different biases towards the classification categories, at least for the harder-to-decide cases." ></td>
	<td class="line x" title="4:149	Under an annotation generation model that takes this into account, there is a hazard that some of the training instances are actually hard cases with unreliable annotations." ></td>
	<td class="line x" title="5:149	We show that these are relatively unproblematic for an algorithm operating under the 0-1 loss model, whereas for the commonly used voted perceptron algorithm, hard training cases could result in incorrect prediction on the uncontroversial cases at test time." ></td>
	<td class="line x" title="6:149	1 Introduction It is assumed, often tacitly, that the kind of noise existing in human-annotated datasets used in computational linguistics is random classification noise (Kearns, 1993; Angluin and Laird, 1988), resulting from annotator attention slips randomly distributed across instances." ></td>
	<td class="line x" title="7:149	For example, Osborne (2002) evaluates noise tolerance of shallow parsers, with random classification noise taken to be crudely approximating annotation errors. It has been shown, both theoretically and empirically, that this type of noise is tolerated well by the commonly used machine learning algorithms (Cohen, 1997; Blum et al., 1996; Osborne, 2002; Reidsma and Carletta, 2008)." ></td>
	<td class="line x" title="8:149	Yet this might be overly optimistic." ></td>
	<td class="line x" title="9:149	Reidsma and op den Akker (2008) show that apparent differences between annotators are not random slips of attention but rather result from different biases annotators might have towards the classification categories." ></td>
	<td class="line x" title="10:149	When training data comes from one annotator and test data from another, the first annotators biases are sometimes systematic enough for a machine learner to pick them up, with detrimental results for the algorithms performance on the test data." ></td>
	<td class="line x" title="11:149	A small subset of doubly annotated data (for inter-annotator agreement check) and large chunks of singly annotated data (for training algorithms) is not uncommon in computational linguistics datasets; such a setup is prone to problems if annotators are differently biased.1 Annotator bias is consistent with a number of noise models." ></td>
	<td class="line x" title="12:149	For example, it could be that an annotators bias is exercised on each and every instance, making his preferred category likelier for any instance than in another persons annotations." ></td>
	<td class="line x" title="13:149	Another possibility, recently explored by Beigman Klebanov and Beigman (2009), is that some items are really quite clear-cut for an annotator with any bias, belonging squarely within one particular category." ></td>
	<td class="line x" title="14:149	However, some instances  termed hard cases therein  are harder to decide upon, and this is where various preferences and biases come into play." ></td>
	<td class="line x" title="15:149	In a metaphor annotation study reported by Beigman Klebanov et al.(2008), certain markups received overwhelming annotator support when people were asked to validate annotations after a certain time delay." ></td>
	<td class="line x" title="17:149	Other instances saw opinions split; moreover, Beigman Klebanov et al.(2008) observed cases where people retracted their own earlier annotations." ></td>
	<td class="line x" title="19:149	To start accounting for such annotator behavior, Beigman Klebanov and Beigman (2009) proposed a model where instances are either easy, and then all annotators agree on them, or hard, and then each annotator flips his or her own coin to de1The different biases might not amount to much in the small doubly annotated subset, resulting in acceptable interannotator agreement; yet when enacted throughout a large number of instances they can be detrimental from a machine learners perspective." ></td>
	<td class="line x" title="20:149	280 cide on a label (each annotator can have a different coin reflecting his or her biases)." ></td>
	<td class="line x" title="21:149	For annotations generated under such a model, there is a danger of hard instances posing as easy  an observed agreement between annotators being a result of all coins coming up heads by chance." ></td>
	<td class="line x" title="22:149	They therefore define the expected proportion of hard instances in agreed items as annotation noise." ></td>
	<td class="line x" title="23:149	They provide an example from the literature where an annotation noise rate of about 15% is likely." ></td>
	<td class="line x" title="24:149	The question addressed in this article is: How problematic is learning from training data with annotation noise?" ></td>
	<td class="line x" title="25:149	Specifically, we are interested in estimating the degree to which performance on easy instances at test time can be hurt by the presence of hard instances in training data." ></td>
	<td class="line x" title="26:149	Definition 1 The hard case bias,, is the portion of easy instances in the test data that are misclassified as a result of hard instances in the training data." ></td>
	<td class="line x" title="27:149	This article proceeds as follows." ></td>
	<td class="line x" title="28:149	First, we show that a machine learner operating under a 0-1 loss minimization principle could sustain a hard case bias of ( 1N) in the worst case." ></td>
	<td class="line x" title="29:149	Thus, while annotation noise is hazardous for small datasets, it is better tolerated in larger ones." ></td>
	<td class="line x" title="30:149	However, 0-1 loss minimization is computationally intractable for large datasets (Feldman et al., 2006; Guruswami and Raghavendra, 2006); substitute loss functions are often used in practice." ></td>
	<td class="line x" title="31:149	While their tolerance to random classification noise is as good as for 0-1 loss, their tolerance to annotation noise is worse." ></td>
	<td class="line x" title="32:149	For example, the perceptron family of algorithms handle random classification noise well (Cohen, 1997)." ></td>
	<td class="line x" title="33:149	We show in section 3.4 that the widely used Freund and Schapire (1999) voted perceptron algorithm could face a constant hard case bias when confronted with annotation noise in training data, irrespective of the size of the dataset." ></td>
	<td class="line x" title="34:149	Finally, we discuss the implications of our findings for the practice of annotation studies and for data utilization in machine learning." ></td>
	<td class="line x" title="35:149	2 0-1 Loss Let a sample be a sequencex1,,xN drawn uniformly from thed-dimensional discrete cubeId = {1,1}d with corresponding labels y1,,yN  {1,1}." ></td>
	<td class="line x" title="36:149	Suppose further that the learning algorithm operates by finding a hyperplane (w,), w  Rd,  R, that minimizes the empirical errorL(w,) = summationtextj=1N[yjsgn(summationtexti=1dxijwi )]2." ></td>
	<td class="line x" title="37:149	Let there be H hard cases, such that the annotation noise is = HN.2 Theorem 1 In the worst case configuration of instances a hard case bias of = ( 1N) cannot be ruled out with constant confidence." ></td>
	<td class="line x" title="38:149	Idea of the proof: We prove by explicit construction of an adversarial case." ></td>
	<td class="line x" title="39:149	Suppose there is a plane that perfectly separates the easy instances." ></td>
	<td class="line x" title="40:149	The (N) hard instances will be concentrated in a band parallel to the separating plane, that is near enough to the plane so as to trap only about (N) easy instances between the plane and the band (see figure 1 for an illustration)." ></td>
	<td class="line x" title="41:149	For a random labeling of the hard instances, the central limit theorem shows there is positive probability that there would be an imbalance between +1 and 1 labels in favor of 1s on the scale of N, which, with appropriate constants, would lead to the movement of the empirically minimal separation plane to the right of the hard case band, misclassifying the trapped easy cases." ></td>
	<td class="line x" title="42:149	Proof: Let v = v(x) = summationtexti=1dxi denote the sum of the coordinates of an instance in Id and take e = d  F1(  2d2 + 12) and h = dF1( +   2d2 + 12), where F(t) is the cumulative distribution function of the normal distribution." ></td>
	<td class="line x" title="43:149	Suppose further that instances xj such thate <vj <h are all and only hard instances; their labels are coinflips." ></td>
	<td class="line x" title="44:149	All other instances are easy, and labeledy = y(x) = sgn(v)." ></td>
	<td class="line x" title="45:149	In this case, the hyperplane 1d(11) is the true separation plane for the easy instances, with = 0." ></td>
	<td class="line x" title="46:149	Figure 1 shows this configuration." ></td>
	<td class="line x" title="47:149	According to the central limit theorem, ford,N large, the distribution ofvis well approximated by N(0,d)." ></td>
	<td class="line x" title="48:149	If N = c1  2d, for some 0 < c1 < 4, the second application of the central limit theorem ensures that, with high probability, about N = c12d items would fall between e and h (all hard), and   2d2N = c1 radicalbig 2d would fall between 0 ande (all easy, all labeled +1)." ></td>
	<td class="line x" title="49:149	Let Z be the sum of labels of the hard cases, Z = summationtexti=1Hyi." ></td>
	<td class="line x" title="50:149	Applying the central limit theorem a third time, for large N, Z will, with a high probability, be distributed approximately as 2In Beigman Klebanov and Beigman (2009), annotation noise is defined as percentage of hard instances in the agreed annotations; this implies noise measurement on multiply annotated material." ></td>
	<td class="line x" title="51:149	When there is just one annotator, no distinction between easy vs hard instances can be made; in this sense, all hard instances are posing as easy." ></td>
	<td class="line x" title="52:149	281 0 e  h Figure 1: The adversarial case for 0-1 loss." ></td>
	<td class="line x" title="53:149	Squares correspond to easy instances, circles  to hard ones." ></td>
	<td class="line x" title="54:149	Filled squares and circles are labeled 1, empty ones are labeled +1." ></td>
	<td class="line x" title="55:149	N(0,N)." ></td>
	<td class="line x" title="56:149	This implies that a value as low as 2cannot be ruled out with high (say 95%) confidence." ></td>
	<td class="line x" title="57:149	Thus, an imbalance of up to 2N, or of 2 radicalbig c12d, in favor of 1s is possible." ></td>
	<td class="line x" title="58:149	There are between 0 and h about 2c1 radicalbig 2d more1 hard instances than +1 hard instances, as opposed toc1 radicalbig 2d easy instances that are all +1." ></td>
	<td class="line x" title="59:149	As long asc1 < 2c1, i.e. c1 < 4, the empirically minimal threshold would move to h, resulting in a hard case bias of = c 12d (1)c12d = ( 1 N)." ></td>
	<td class="line x" title="60:149	To see that this is the worst case scenario, we note that 0-1 loss sustained on (N) hard cases is the order of magnitude of the possible imbalance between 1 and +1 random labels, which is(N)." ></td>
	<td class="line x" title="61:149	For hard case loss to outweigh the loss on the misclassified easy instances, there cannot be more than(N) of the latter a50 Note that the proof requires that N = (2d) namely, that asymptotically the sample includes a fixed portion of the instances." ></td>
	<td class="line x" title="62:149	If the sample is asymptotically smaller, thene will have to be adjusted such thate = dF1(( 1N) + 12)." ></td>
	<td class="line x" title="63:149	According to theorem 1, for a 10K dataset with 15% hard case rate, a hard case bias of about 1% cannot be ruled out with 95% confidence." ></td>
	<td class="line x" title="64:149	Theorem 1 suggests that annotation noise as defined here is qualitatively different from more malicious types of noise analyzed in the agnostic learning framework (Kearns and Li, 1988; Haussler, 1992; Kearns et al., 1994), where an adversary can not only choose the placement of the hard cases, but also their labels." ></td>
	<td class="line x" title="65:149	In worst case, the 0-1 loss model would sustain a constant rate of error due to malicious noise, whereas annotation noise is tolerated quite well in large datasets." ></td>
	<td class="line x" title="66:149	3 Voted Perceptron Freund and Schapire (1999) describe the voted perceptron." ></td>
	<td class="line oc" title="67:149	This algorithm and its many variants are widely used in the computational linguistics community (Collins, 2002a; Collins and Duffy, 2002; Collins, 2002b; Collins and Roark, 2004; Henderson and Titov, 2005; Viola and Narasimhan, 2005; Cohen et al., 2004; Carreras et al., 2005; Shen and Joshi, 2005; Ciaramita and Johnson, 2003)." ></td>
	<td class="line x" title="68:149	In this section, we show that the voted perceptron can be vulnerable to annotation noise." ></td>
	<td class="line x" title="69:149	The algorithm is shown below." ></td>
	<td class="line x" title="70:149	Algorithm 1 Voted Perceptron Training Input: a labeled training set (x1,y1),,(xN,yN) Output: a list of perceptronsw1,,wN Initialize: t0;w10;10 fort = 1N do ytsign(wt,xt+t) wt+1wt + ytyt2 xt t+1t + ytyt2 wt,xt end for Forecasting Input: a list of perceptronsw1,,wN an unlabeled instancex Output: A forecasted labely yPNt=1 sign(wt,xt+t) ysign(y) The voted perceptron algorithm is a refinement of the perceptron algorithm (Rosenblatt, 1962; Minsky and Papert, 1969)." ></td>
	<td class="line x" title="71:149	Perceptron is a dynamic algorithm; starting with an initial hyperplane w0, it passes repeatedly through the labeled sample." ></td>
	<td class="line x" title="72:149	Whenever an instance is misclassified by wt, the hyperplane is modified to adapt to the instance." ></td>
	<td class="line x" title="73:149	The algorithm terminates once it has passed through the sample without making any classification mistakes." ></td>
	<td class="line x" title="74:149	The algorithm terminates iff the sample can be separated by a hyperplane, and in this case the algorithm finds a separating hyperplane." ></td>
	<td class="line x" title="75:149	Novikoff (1962) gives a bound on the number of iterations the algorithm goes through before termination, when the sample is separable by a margin." ></td>
	<td class="line x" title="76:149	282 The perceptron algorithm is vulnerable to noise, as even a little noise could make the sample inseparable." ></td>
	<td class="line x" title="77:149	In this case the algorithm would cycle indefinitely never meeting termination conditions, wt would obtain values within a certain dynamic range but would not converge." ></td>
	<td class="line x" title="78:149	In such setting, imposing a stopping time would be equivalent to drawing a random vector from the dynamic range." ></td>
	<td class="line x" title="79:149	Freund and Schapire (1999) extend the perceptron to inseparable samples with their voted perceptron algorithm and give theoretical generalization bounds for its performance." ></td>
	<td class="line x" title="80:149	The basic idea underlying the algorithm is that if the dynamic range of the perceptron is not too large then wt would classify most instances correctly most of the time (for most values oft)." ></td>
	<td class="line x" title="81:149	Thus, for a sample x1,,xN the new algorithm would keep track of w0,,wN, and for an unlabeled instance x it would forecast the classification most prominent amongst these hyperplanes." ></td>
	<td class="line x" title="82:149	The bounds given by Freund and Schapire (1999) depend on the hinge loss of the dataset." ></td>
	<td class="line x" title="83:149	In section 3.2 we construct a difficult setting for this algorithm." ></td>
	<td class="line x" title="84:149	To prove that voted perceptron would suffer from a constant hard case bias in this setting using the exact dynamics of the perceptron is beyond the scope of this article." ></td>
	<td class="line x" title="85:149	Instead, in section 3.3 we provide a lower bound on the hinge loss for a simplified model of the perceptron algorithm dynamics, which we argue would be a good approximation to the true dynamics in the setting we constructed." ></td>
	<td class="line x" title="86:149	For this simplified model, we show that the hinge loss is large, and the bounds in Freund and Schapire (1999) cannot rule out a constant level of error regardless of the size of the dataset." ></td>
	<td class="line x" title="87:149	In section 3.4 we study the dynamics of the model and prove that  = (1) for the adversarial setting." ></td>
	<td class="line x" title="88:149	3.1 Hinge Loss Definition 2 The hinge loss of a labeled instance (x,y) with respect to hyperplane (w,) and margin  > 0 is given by  = (,) = max(0,  y(w,x))." ></td>
	<td class="line x" title="89:149	 measures the distance of an instance from beingclassifiedcorrectlywithamargin." ></td>
	<td class="line x" title="90:149	Figure2 shows examples of hinge loss for various data points." ></td>
	<td class="line x" title="91:149	Theorem 2 (Freund and Schapire (1999)) After one pass on the sample, the probability that the voted perceptron algorithm does not        Figure 2: Hinge loss  for various data points incurred by the separator with margin." ></td>
	<td class="line x" title="92:149	predict correctly the label of a test instance xN+1 is bounded by 2N+1EN+1bracketleftbigd+D bracketrightbig2 where D = D(w,,) = radicalBigsummationtext N i=12i . This result is used to explain the convergence of weighted or voted perceptron algorithms (Collins, 2002a)." ></td>
	<td class="line x" title="93:149	Itisusefulaslongastheexpectedvalueof D is not too large." ></td>
	<td class="line x" title="94:149	We show that in an adversarial setting of the annotation noise D is large, hence these bounds are trivial." ></td>
	<td class="line x" title="95:149	3.2 Adversarial Annotation Noise Let a sample be a sequencex1,,xN drawn uniformly from Id with y1,,yN  {1,1}." ></td>
	<td class="line x" title="96:149	Easy cases are labeled y = y(x) = sgn(v) as before, withv = v(x) = summationtexti=1dxi." ></td>
	<td class="line x" title="97:149	The true separation plane for the easy instances is w = 1d(11),  = 0." ></td>
	<td class="line x" title="98:149	Suppose hard cases are those where v(x) > c1d, where c1 is chosen so that the hard instances account for N of all instances.3 Figure 3 shows this setting." ></td>
	<td class="line x" title="99:149	3.3 Lower Bound on Hinge Loss In the simplified case, we assume that the algorithm starts training with the hyperplane w0 = w = 1d(11), and keeps it throughout the training, only updating." ></td>
	<td class="line x" title="100:149	In reality, each hard instancecanbedecomposedintoacomponentthatis parallel tow, and a component that is orthogonal to it." ></td>
	<td class="line x" title="101:149	The expected contribution of the orthogonal 3See the proof of 0-1 case for a similar construction using the central limit theorem." ></td>
	<td class="line x" title="102:149	283 0c 1  E Figure 3: An adversarial case of annotation noise for the voted perceptron algorithm." ></td>
	<td class="line x" title="103:149	component to the algorithms update will be positive due to the systematic positioning of the hard cases, while the contributions of the parallel components are expected to cancel out due to the symmetry of the hard cases around the main diagonal that is orthogonal to w." ></td>
	<td class="line x" title="104:149	Thus, while wt will not necessarily parallel w, it will be close to parallel for mostt> 0." ></td>
	<td class="line x" title="105:149	The simplified case is thus a good approximation of the real case, and the bound we obtain is expected to hold for the real case as well." ></td>
	<td class="line x" title="106:149	For any initial value0 < 0 all misclassified instances are labeled 1 and classified as +1, hence the update will increase 0, and reach 0 soon enough." ></td>
	<td class="line x" title="107:149	We can therefore assume that t  0 for anyt>t0 wheret0 lessmuchN." ></td>
	<td class="line x" title="108:149	Lemma 3 For any t > t0, there exist  = (,T) > 0 such that E(2) ." ></td>
	<td class="line x" title="109:149	Proof: For   0 there are two main sources of hinge loss: easy +1 instances that are classified as 1, and hard -1 instances classified as +1." ></td>
	<td class="line x" title="110:149	These correspond to the two components of the following sum (the inequality is due to disregardingthelossincurredbyacorrectclassification with too wide a margin): E(2)  []summationdisplay l=0 1 2d parenleftbiggd l parenrightbigg ( d  ld +)2 +12 dsummationdisplay l=c1d 1 2d parenleftbiggd l parenrightbigg ( ld  d +)2 Let 0 < T < c1 be a parameter." ></td>
	<td class="line x" title="111:149	For  > Td, misclassified easy instances dominate the loss: E(2)  []summationdisplay l=0 1 2d parenleftbiggd l parenrightbigg ( d  ld +)2  [Td]summationdisplay l=0 1 2d parenleftbiggd l parenrightbigg (T d d  ld +)2  Tdsummationdisplay l=0 1 2d parenleftbiggd l parenrightbigg (T  ld +)2  12pi integraldisplay T 0 (T +t)2et2/2dt = HT() The last inequality follows from a normal approximation of the binomial distribution (see, for example, Feller (1968))." ></td>
	<td class="line x" title="112:149	For 0    Td, misclassified hard cases dominate: E(2)  12 dsummationdisplay l=c1d 1 2d parenleftbiggd l parenrightbigg ( ld  d +)2  12 dsummationdisplay l=c1d 1 2d parenleftbiggd l parenrightbigg ( ld  T d d +)2  12  12pi integraldisplay  1() (tT +)2et2/2dt = H() where 1() is the inverse of the normal distribution density." ></td>
	<td class="line x" title="113:149	Thus E(2)  min{HT(),H()}, and there exists  = (,T) > 0 such that min{HT(),H()}  a50 Corollary 4 The bound in theorem 2 does not converge to zero for largeN." ></td>
	<td class="line x" title="114:149	We recall that Freund and Schapire (1999) bound is proportional toD2 = summationtextNi=12i . It follows from lemma 3 that D2 = (N), hence the bound is ineffective." ></td>
	<td class="line x" title="115:149	3.4 Lower Bound on for Voted Perceptron Under Simplified Dynamics Corollary 4 does not give an estimate on the hard case bias." ></td>
	<td class="line x" title="116:149	Indeed, it could be that wt = w for almost every t. There would still be significant hinge in this case, but the hard case bias for the voted forecast would be zero." ></td>
	<td class="line x" title="117:149	To assess the hard case bias we need a model of perceptron dynamics that would account for the history of hyperplanesw0,,wN theperceptrongoesthroughon 284 a sample x1,,xN." ></td>
	<td class="line x" title="118:149	The key simplification in our model is assuming that wt parallels w for all t, hence the next hyperplane depends only on the offset t. This is a one dimensional Markov random walk governed by the distribution P(t+1t = r|t) = P(x|yt  yt2 w,x = r) In general d  t  d but as mentioned before lemma 3, we may assumet > 0." ></td>
	<td class="line x" title="119:149	Lemma 5 Thereexistsc> 0suchthatwithahigh probabilityt >cdfor most 0 tN. Proof: Letc0 = F1(2 +12);c1 = F1(1)." ></td>
	<td class="line x" title="120:149	We designate the intervalsI0 = [0,c0 d]; I1 = [c0 d,c1 d] andI2 = [c1 d,d] and define Ai = {x : v(x) Ii} fori = 0,1,2." ></td>
	<td class="line x" title="121:149	Note that the constantsc0 andc1 are chosen so that P(A0) = 2 and P(A2) = ." ></td>
	<td class="line x" title="122:149	It follows from the construction in section 3.2 that A0 and A1 are easy instances and A2 are hard." ></td>
	<td class="line x" title="123:149	Given a sample x1,,xN, a misclassification ofxt A0 byt could only happen when an easy +1 instance is classified as 1." ></td>
	<td class="line x" title="124:149	Thus the algorithm would shift t to the left by no more than |vt t| since vt = w,xt." ></td>
	<td class="line x" title="125:149	This shows that t  I0 implies t+1  I0." ></td>
	<td class="line x" title="126:149	In the same manner, it is easy to verify that if t  Ij and xt  Ak then t+1  Ik, unless j = 0 and k = 1, in which case t+1  I0 because xt  A1 would be classified correctly byt I0." ></td>
	<td class="line x" title="127:149	We construct a Markov chain with three states a0 = 0, a1 = c0 d and a2 = c1 d governed by the following transition distribution:    1 2 0 2  2 1  2  2 1 2  3 2 1 2 +    Let Xt be the state at time t. The principal eigenvector of the transition matrix (13, 13, 13) gives the stationary probability distribution of Xt." ></td>
	<td class="line x" title="128:149	Thus Xt  {a1,a2} with probability 23." ></td>
	<td class="line x" title="129:149	Since the transition distribution of Xt mirrors that of t, and since aj are at the leftmost borders of Ij, respectively, it follows that Xt  t for all t, thus Xt  {a1,a2}impliest I1I2." ></td>
	<td class="line x" title="130:149	It follows that t > c0  d with probability 23, and the lemma follows from the law of large numbers a50 Corollary 6 With high probability = (1)." ></td>
	<td class="line x" title="131:149	Proof: Lemma 5 shows that for a sample x1,,xN with high probability t is most of the time to the right of c  d. Consequently for any x in the band 0  v  c  d we get sign(w,x+t) = 1 for mostthence by definition, the voted perceptron would classify such an instance as 1, although it is in fact a +1 easy instance." ></td>
	<td class="line x" title="132:149	Since there are (N) misclassified easy instances, = (1) a50 4 Discussion In this article we show that training with annotation noise can be detrimental for test-time results on easy, uncontroversial instances; we termed this phenomenon hard case bias." ></td>
	<td class="line x" title="133:149	Although under the 0-1 loss model annotation noise can be tolerated for larger datasets (theorem 1), minimizing such loss becomes intractable for larger datasets." ></td>
	<td class="line x" title="134:149	Freund and Schapire (1999) voted perceptron algorithm and its variants are widely used in computational linguistics practice; our results show that it could suffer a constant rate of hard case bias irrespective of the size of the dataset (section 3.4)." ></td>
	<td class="line x" title="135:149	How can hard case bias be reduced?" ></td>
	<td class="line x" title="136:149	One possibility is removing as many hard cases as one can not only from the test data, as suggested in Beigman Klebanov and Beigman (2009), but from the training data as well." ></td>
	<td class="line x" title="137:149	Adding the second annotator is expected to detect about half the hard cases, as they would surface as disagreements between the annotators." ></td>
	<td class="line x" title="138:149	Subsequently, a machine learner can be told to ignore those cases during training, reducing the risk of hard case bias." ></td>
	<td class="line x" title="139:149	While this is certainly a daunting task, it is possible that for annotation studies that do not require expert annotators and extensive annotator training, the newly available access to a large pool of inexpensive annotators, such as the Amazon Mechanical Turk scheme (Snow et al., 2008),4 or embedding the task in an online game played by volunteers (Poesio et al., 2008; von Ahn, 2006) could provide some solutions." ></td>
	<td class="line x" title="140:149	Reidsma and op den Akker (2008) suggest a different option." ></td>
	<td class="line x" title="141:149	When non-overlapping parts of the dataset are annotated by different annotators, each classifier can be trained to reflect the opinion (albeit biased) of a specific annotator, using different parts of the datasets." ></td>
	<td class="line x" title="142:149	Such subjective machines can be applied to a new set of data; an item that causes disagreement between classifiers is then extrapolated to be a case of potential disagreement between the humans they replicate, i.e. 4http://aws.amazon.com/mturk/ 285 a hard case." ></td>
	<td class="line x" title="143:149	Our results suggest that, regardless of the success of such an extrapolation scheme in detecting hard cases, it could erroneously invalidate easy cases: Each classifier would presumably suffer from a certain hard case bias, i.e. classify incorrectly things that are in fact uncontroversial for any human annotator." ></td>
	<td class="line x" title="144:149	If each such classifier has a different hard case bias, some inter-classifier disagreements would occur on easy cases." ></td>
	<td class="line x" title="145:149	Depending on the distribution of those easy cases in the feature space, this could invalidate valuable cases." ></td>
	<td class="line x" title="146:149	If the situation depicted in figure 1 corresponds to the pattern learned by one of the classifiers, it would lead to marking the easy cases closest to the real separation boundary (those between 0 and e) as hard, and hence unsuitable for learning, eliminating the most informative material from the training data." ></td>
	<td class="line x" title="147:149	Reidsma and Carletta (2008) recently showed by simulation that different types of annotator behavior have different impact on the outcomes of machine learning from the annotated data." ></td>
	<td class="line x" title="148:149	Our results provide a theoretical analysis that points in the same direction: While random classification noise is tolerable, other types of noise  such as annotation noise handled here  are more problematic." ></td>
	<td class="line x" title="149:149	It is therefore important to develop models of annotator behavior and of the resulting imperfections of the annotated datasets, in order to diagnose the potential learning problem and suggest mitigation strategies." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1059
Automatic Adaptation of Annotation Standards: Chinese Word Segmentation and POS Tagging &#8211; A Case Study
Jiang, Wenbin;Huang, Liang;Liu, Qun;"></td>
	<td class="line x" title="1:154	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 522530, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:154	c2009 ACL and AFNLP Automatic Adaptation of Annotation Standards: Chinese Word Segmentation and POS Tagging  A Case Study Wenbin Jiang  Liang Huang  Qun Liu  Key Lab." ></td>
	<td class="line x" title="3:154	of Intelligent Information Processing Google Research Institute of Computing Technology 1350 Charleston Rd. Chinese Academy of Sciences Mountain View, CA 94043, USA P.O. Box 2704, Beijing 100190, China lianghuang@google.com {jiangwenbin, liuqun}@ict.ac.cn liang.huang.sh@gmail.com Abstract Manually annotated corpora are valuable but scarce resources, yet for many annotation tasks such as treebanking and sequence labeling there exist multiple corpora with different and incompatible annotation guidelines or standards." ></td>
	<td class="line x" title="4:154	This seems to be a great waste of human efforts, and it would be nice to automatically adapt one annotation standard to another." ></td>
	<td class="line x" title="5:154	We present a simple yet effective strategy that transfers knowledge from a differently annotated corpus to the corpus with desired annotation." ></td>
	<td class="line x" title="6:154	We test the efficacy of this method in the context of Chinese word segmentation and part-of-speech tagging, where no segmentation and POS tagging standards are widely accepted due to the lack of morphology in Chinese." ></td>
	<td class="line x" title="7:154	Experiments show that adaptation from the much larger Peoples Daily corpus to the smaller but more popular Penn Chinese Treebank results in significant improvements in both segmentation and tagging accuracies (with error reductions of 30.2% and 14%, respectively), which in turn helps improve Chinese parsing accuracy." ></td>
	<td class="line x" title="8:154	1 Introduction Much of statistical NLP research relies on some sort of manually annotated corpora to train their models, but these resources are extremely expensive to build, especially at a large scale, for example in treebanking (Marcus et al., 1993)." ></td>
	<td class="line x" title="9:154	However the linguistic theories underlying these annotation efforts are often heavily debated, and as a result there often exist multiple corpora for the same task with vastly different and incompatible annotation philosophies." ></td>
	<td class="line x" title="10:154	For example just for English treebanking there have been the Chomskian-style cjkC3C01 cjkB8B12 cjkD7DC3 cjkCDB34 cjkB7C35 cjkBBAA6 NR NN VV NR U.S. Vice-President visited China cjkC3C01 cjkB8B12 cjkD7DC3 cjkCDB34 cjkB7C35 cjkBBAA6 ns b n v U.S. Vice President visited-China Figure 1: Incompatible word segmentation and POS tagging standards between CTB (upper) and Peoples Daily (below)." ></td>
	<td class="line x" title="11:154	Penn Treebank (Marcus et al., 1993) the HPSG LinGo Redwoods Treebank (Oepen et al., 2002), and a smaller dependency treebank (Buchholz and Marsi, 2006)." ></td>
	<td class="line x" title="12:154	A second, related problem is that the raw texts are also drawn from different domains, which for the above example range from financial news (PTB/WSJ) to transcribed dialog (LinGo)." ></td>
	<td class="line x" title="13:154	These two problems seem be a great waste in human efforts, and it would be nice if one could automatically adapt from one annotation standard and/or domain to another in order to exploit much larger datasets for better training." ></td>
	<td class="line x" title="14:154	The second problem, domain adaptation, is very well-studied, e.g. by Blitzer et al.(2006) and Daume III (2007) (and see below for discussions), so in this paper we focus on the less studied, but equally important problem of annotationstyle adaptation." ></td>
	<td class="line x" title="16:154	We present a very simple yet effective strategy that enables us to utilize knowledge from a differently annotated corpora for the training of a model on a corpus with desired annotation." ></td>
	<td class="line x" title="17:154	The basic idea is very simple: we first train on a source corpus, resulting in a source classifier, which is used to label the target corpus and results in a sourcestyle annotation of the target corpus." ></td>
	<td class="line x" title="18:154	We then 522 train a second model on the target corpus with the first classifiers prediction as additional features for guided learning." ></td>
	<td class="line x" title="19:154	This method is very similar to some ideas in domain adaptation (Daume III and Marcu, 2006; Daume III, 2007), but we argue that the underlying problems are quite different." ></td>
	<td class="line x" title="20:154	Domain adaptation assumes the labeling guidelines are preserved between the two domains, e.g., an adjective is always labeled as JJ regardless of from Wall Street Journal (WSJ) or Biomedical texts, and only the distributions are different, e.g., the word control is most likely a verb in WSJ but often a noun in Biomedical texts (as in control experiment)." ></td>
	<td class="line x" title="21:154	Annotation-style adaptation, however, tackles the problem where the guideline itself is changed, for example, one treebank might distinguish between transitive and intransitive verbs, while merging the different noun types (NN, NNS, etc.), and for example one treebank (PTB) might be much flatter than the other (LinGo), not to mention the fundamental disparities between their underlying linguistic representations (CFG vs. HPSG)." ></td>
	<td class="line x" title="22:154	In this sense, the problem we study in this paper seems much harder and more motivated from a linguistic (rather than statistical) point of view." ></td>
	<td class="line x" title="23:154	More interestingly, our method, without any assumption on the distributions, can be simultaneously applied to both domain and annotation standards adaptation problems, which is very appealing in practice because the latter problem often implies the former, as in our case study." ></td>
	<td class="line x" title="24:154	To test the efficacy of our method we choose Chinese word segmentation and part-of-speech tagging, where the problem of incompatible annotation standards is one of the most evident: so far no segmentation standard is widely accepted due to the lack of a clear definition of Chinese words, and the (almost complete) lack of morphology results in much bigger ambiguities and heavy debates in tagging philosophies for Chinese parts-of-speech." ></td>
	<td class="line x" title="25:154	The two corpora used in this study are the much larger Peoples Daily (PD) (5.86M words) corpus (Yu et al., 2001) and the smaller but more popular Penn Chinese Treebank (CTB) (0.47M words) (Xue et al., 2005)." ></td>
	<td class="line x" title="26:154	They used very different segmentation standards as well as different POS tagsets and tagging guidelines." ></td>
	<td class="line x" title="27:154	For example, in Figure 1, Peoples Daily breaks Vice-President into two words while combines the phrase visited-China as a compound." ></td>
	<td class="line x" title="28:154	Also CTB has four verbal categories (VV for normal verbs, and VC for copulas, etc.) while PD has only one verbal tag (v) (Xia, 2000)." ></td>
	<td class="line x" title="29:154	It is preferable to transfer knowledge from PD to CTB because the latter also annotates tree structures which is very useful for downstream applications like parsing, summarization, and machine translation, yet it is much smaller in size." ></td>
	<td class="line x" title="30:154	Indeed, many recent efforts on Chinese-English translation and Chinese parsing use the CTB as the de facto segmentation and tagging standards, but suffers from the limited size of training data (Chiang, 2007; Bikel and Chiang, 2000)." ></td>
	<td class="line x" title="31:154	We believe this is also a reason why stateof-the-art accuracy for Chinese parsing is much lower than that of English (CTB is only half the size of PTB)." ></td>
	<td class="line x" title="32:154	Our experiments show that adaptation from PD to CTB results in a significant improvement in segmentation and POS tagging, with error reductions of 30.2% and 14%, respectively." ></td>
	<td class="line x" title="33:154	In addition, the improved accuracies from segmentation and tagging also lead to an improved parsing accuracy on CTB, reducing 38% of the error propagation from word segmentation to parsing." ></td>
	<td class="line x" title="34:154	We envision this technique to be general and widely applicable to many other sequence labeling tasks." ></td>
	<td class="line x" title="35:154	In the rest of the paper we first briefly review the popular classification-based method for word segmentation and tagging (Section 2), and then describe our idea of annotation adaptation (Section 3)." ></td>
	<td class="line x" title="36:154	We then discuss other relevant previous work including co-training and classifier combination (Section 4) before presenting our experimental results (Section 5)." ></td>
	<td class="line x" title="37:154	2 Segmentation and Tagging as Character Classification Before describing the adaptation algorithm, we give a brief introduction of the baseline character classification strategy for segmentation, as well as joint segmenation and tagging (henceforth Joint S&T)." ></td>
	<td class="line x" title="38:154	following our previous work (Jiang et al., 2008)." ></td>
	<td class="line x" title="39:154	Given a Chinese sentence as sequence of n characters: C1 C2  Cn where Ci is a character, word segmentation aims to split the sequence into m( n) words: C1:e1 Ce1+1:e2  Cem1+1:em where each subsequence Ci:j indicates a Chinese word spanning from characters Ci to Cj (both in523 Algorithm 1 Perceptron training algorithm." ></td>
	<td class="line x" title="40:154	1: Input: Training examples (xi,yi) 2: vector0 3: for t1  T do 4: for i1  N do 5: ziargmaxzGEN(xi)(xi,z)vector 6: if zinegationslash= yi then 7: vectorvector +(xi,yi)(xi,zi) 8: Output: Parameters vector clusive)." ></td>
	<td class="line x" title="41:154	While in Joint S&T, each word is further annotated with a POS tag: C1:e1/t1 Ce1+1:e2/t2  Cem1+1:em/tm where tk(k = 1m) denotes the POS tag for the word Cek1+1:ek." ></td>
	<td class="line x" title="42:154	2.1 Character Classification Method Xue and Shen (2003) describe for the first time the character classification approach for Chinese word segmentation, where each character is given a boundary tag denoting its relative position in a word." ></td>
	<td class="line x" title="43:154	In Ng and Low (2004), Joint S&T can also be treated as a character classification problem, where a boundary tag is combined with a POS tag in order to give the POS information of the word containing these characters." ></td>
	<td class="line x" title="44:154	In addition, Ng and Low (2004) find that, compared with POS tagging after word segmentation, Joint S&T can achieve higher accuracy on both segmentation and POS tagging." ></td>
	<td class="line x" title="45:154	This paper adopts the tag representation of Ng and Low (2004)." ></td>
	<td class="line x" title="46:154	For word segmentation only, there are four boundary tags:  b: the begin of the word  m: the middle of the word  e: the end of the word  s: a single-character word while for Joint S&T, a POS tag is attached to the tail of a boundary tag, to incorporate the word boundary information and POS information together." ></td>
	<td class="line x" title="47:154	For example, b-NN indicates that the character is the begin of a noun." ></td>
	<td class="line x" title="48:154	After all characters of a sentence are assigned boundary tags (or with POS postfix) by a classifier, the corresponding word sequence (or with POS) can be directly derived." ></td>
	<td class="line x" title="49:154	Take segmentation for example, a character assigned a tag s or a subsequence of words assigned a tag sequence bme indicates a word." ></td>
	<td class="line x" title="50:154	2.2 Training Algorithm and Features Now we will show the training algorithm of the classifier and the features used." ></td>
	<td class="line x" title="51:154	Several classification models can be adopted here, however, we choose the averaged perceptron algorithm (Collins, 2002) because of its simplicity and high accuracy." ></td>
	<td class="line pc" title="52:154	It is an online training algorithm and has been successfully used in many NLP tasks, such as POS tagging (Collins, 2002), parsing (Collins and Roark, 2004), Chinese word segmentation (Zhang and Clark, 2007; Jiang et al., 2008), and so on." ></td>
	<td class="line x" title="53:154	Similar to the situation in other sequence labeling problems, the training procedure is to learn a discriminative model mapping from inputs x  X to outputs y  Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results." ></td>
	<td class="line x" title="54:154	Following Collins, we use a function GEN(x) enumerating the candidate results of an input x , a representationmapping each training example (x,y)  X  Y to a feature vector(x,y)  Rd, and a parameter vector vector  Rd corresponding to the feature vector." ></td>
	<td class="line x" title="55:154	For an input character sequence x, we aim to find an output F(x) that satisfies: F(x) = argmax yGEN(x) (x,y) vector (1) where(x,y)vector denotes the inner product of feature vector (x,y) and the parameter vector vector." ></td>
	<td class="line x" title="56:154	Algorithm 1 depicts the pseudo code to tune the parameter vector vector." ></td>
	<td class="line x" title="57:154	In addition, the averaged parameters technology (Collins, 2002) is used to alleviate overfitting and achieve stable performance." ></td>
	<td class="line x" title="58:154	Table 1 lists the feature template and corresponding instances." ></td>
	<td class="line x" title="59:154	Following Ng and Low (2004), the current considering character is denoted as C0, while the ith character to the left of C0 as Ci, and to the right as Ci." ></td>
	<td class="line x" title="60:154	There are additional two functions of which each returns some property of a character." ></td>
	<td class="line x" title="61:154	Pu() is a boolean function that checks whether a character is a punctuation symbol (returns 1 for a punctuation, 0 for not)." ></td>
	<td class="line x" title="62:154	T() is a multi-valued function, it classifies a character into four classifications: number, date, English letter and others (returns 1, 2, 3 and 4, respectively)." ></td>
	<td class="line x" title="63:154	3 Automatic Annotation Adaptation From this section, several shortened forms are adopted for representation inconvenience." ></td>
	<td class="line x" title="64:154	We use source corpus to denote the corpus with the annotation standard that we dont require, which is of 524 Feature Template Instances Ci (i =22) C2 =cjkBEC5, C1 =cjkA996, C0 =cjkC4EA, C1 =cjkB4FA, C2 = R CiCi+1 (i =21) C2C1 =cjkBEC5cjkA996, C1C0 =cjkA996cjkC4EA, C0C1 =cjkC4EAcjkB4FA, C1C2 =cjkB4FAR C1C1 C1C1 =cjkA996cjkB4FA Pu(C0) Pu(C0) = 0 T(C2)T(C1)T(C0)T(C1)T(C2) T(C2)T(C1)T(C0)T(C1)T(C2) = 11243 Table 1: Feature templates and instances from Ng and Low (Ng and Low, 2004)." ></td>
	<td class="line x" title="65:154	Suppose we are considering the third character cjkC4EA in cjkBEC5cjkA996cjkC4EAcjkB4FAR." ></td>
	<td class="line x" title="66:154	course the source of the adaptation, while target corpus denoting the corpus with the desired standard." ></td>
	<td class="line x" title="67:154	And correspondingly, the two annotation standards are naturally denoted as source standard and target standard, while the classifiers following the two annotation standards are respectively named as source classifier and target classifier, if needed." ></td>
	<td class="line x" title="68:154	Considering that word segmentation and Joint S&T can be conducted in the same character classification manner, we can design an unified standard adaptation framework for the two tasks, by taking the source classifiers classification result as the guide information for the target classifiers classification decision." ></td>
	<td class="line x" title="69:154	The following section depicts this adaptation strategy in detail." ></td>
	<td class="line x" title="70:154	3.1 General Adaptation Strategy In detail, in order to adapt knowledge from the source corpus, first, a source classifier is trained on it and therefore captures the knowledge it contains; then, the source classifier is used to classify the characters in the target corpus, although the classification result follows a standard that we dont desire; finally, a target classifier is trained on the target corpus, with the source classifiers classification result as additional guide information." ></td>
	<td class="line x" title="71:154	The training procedure of the target classifier automatically learns the regularity to transfer the source classifiers predication result from source standard to target standard." ></td>
	<td class="line x" title="72:154	This regularity is incorporated together with the knowledge learnt from the target corpus itself, so as to obtain enhanced predication accuracy." ></td>
	<td class="line x" title="73:154	For a given un-classified character sequence, the decoding is analogous to the training." ></td>
	<td class="line x" title="74:154	First, the character sequence is input into the source classifier to obtain an source standard annotated classification result, then it is input into the target classifier with this classification result as additional information to get the final result." ></td>
	<td class="line x" title="75:154	This coincides with the stacking method for combining dependency parsers (Martins et al., 2008; Nivre and McDonsource corpus train with normal features source classifier train with additional features target classifier target corpus source annotationclassification result Figure 2: The pipeline for training." ></td>
	<td class="line x" title="76:154	raw sentence source classifier source annotationclassification result target classifier target annotation classification result Figure 3: The pipeline for decoding." ></td>
	<td class="line x" title="77:154	ald, 2008), and is also similar to the Pred baseline for domain adaptation in (Daume III and Marcu, 2006; Daume III, 2007)." ></td>
	<td class="line x" title="78:154	Figures 2 and 3 show the flow charts for training and decoding." ></td>
	<td class="line x" title="79:154	The utilization of the source classifiers classification result as additional guide information resorts to the introduction of new features." ></td>
	<td class="line x" title="80:154	For the current considering character waiting for classification, the most intuitive guide features is the source classifiers classification result itself." ></td>
	<td class="line x" title="81:154	However, our effort isnt limited to this, and more special features are introduced: the source classifiers classification result is attached to every feature listed in Table 1 to get combined guide features." ></td>
	<td class="line x" title="82:154	This is similar to feature design in discriminative dependency parsing (McDonald et al., 2005; Mc525 Donald and Pereira, 2006), where the basic features, composed of words and POSs in the context, are also conjoined with link direction and distance in order to obtain more special features." ></td>
	<td class="line x" title="83:154	Table 2 shows an example of guide features and basic features, where  = b  represents that the source classifier classifies the current character as b, the beginning of a word." ></td>
	<td class="line x" title="84:154	Such combination method derives a series of specific features, which helps the target classifier to make more precise classifications." ></td>
	<td class="line x" title="85:154	The parameter tuning procedure of the target classifier will automatically learn the regularity of using the source classifiers classification result to guide its decision making." ></td>
	<td class="line x" title="86:154	For example, if a current considering character shares some basic features in Table 2 and it is classified as b, then the target classifier will probably classify it as m. In addition, the training procedure of the target classifier also learns the relative weights between the guide features and the basic features, so that the knowledge from both the source corpus and the target corpus are automatically integrated together." ></td>
	<td class="line x" title="87:154	In fact, more complicated features can be adopted as guide information." ></td>
	<td class="line x" title="88:154	For error tolerance, guide features can be extracted from n-best results or compacted lattices of the source classifier; while for the best use of the source classifiers output, guide features can also be the classification results of several successive characters." ></td>
	<td class="line x" title="89:154	We leave them as future research." ></td>
	<td class="line x" title="90:154	4 Related Works Co-training (Sarkar, 2001) and classifier combination (Nivre and McDonald, 2008) are two technologies for training improved dependency parsers." ></td>
	<td class="line x" title="91:154	The co-training technology lets two different parsing models learn from each other during parsing an unlabelled corpus: one model selects some unlabelled sentences it can confidently parse, and provide them to the other model as additional training corpus in order to train more powerful parsers." ></td>
	<td class="line x" title="92:154	The classifier combination lets graph-based and transition-based dependency parsers to utilize the features extracted from each others parsing results, to obtain combined, enhanced parsers." ></td>
	<td class="line x" title="93:154	The two technologies aim to let two models learn from each other on the same corpora with the same distribution and annotation standard, while our strategy aims to integrate the knowledge in multiple corpora with different Baseline Features C2 =cjkC3C0 C1 =cjkB8B1 C0 =cjkD7DC C1 =cjkCDB3 C2 =cjkB7C3 C2C1 =cjkC3C0cjkB8B1 C1C0 =cjkB8B1cjkD7DC C0C1 =cjkD7DCcjkCDB3 C1C2 =cjkCDB3cjkB7C3 C1C1 =cjkB8B1cjkCDB3 Pu(C0) = 0 T(C2)T(C1)T(C0)T(C1)T(C2) = 44444 Guide Features  = b C2 =cjkC3C0   = b C1 =cjkB8B1   = b C0 =cjkD7DC   = b C1 =cjkCDB3   = b C2 =cjkB7C3   = b C2C1 =cjkC3C0cjkB8B1   = b C1C0 =cjkB8B1cjkD7DC   = b C0C1 =cjkD7DCcjkCDB3   = b C1C2 =cjkCDB3cjkB7C3   = b C1C1 =cjkB8B1cjkCDB3   = b Pu(C0) = 0   = b T(C2)T(C1)T(C0)T(C1)T(C2) = 44444   = b Table 2: An example of basic features and guide features of standard-adaptation for word segmentation." ></td>
	<td class="line x" title="94:154	Suppose we are considering the third character cjkD7DC in cjkC3C0cjkB8B1cjkD7DCcjkCDB3cjkB7C3cjkBBAA." ></td>
	<td class="line x" title="95:154	annotation-styles." ></td>
	<td class="line x" title="96:154	Gao et al.(2004) described a transformationbased converter to transfer a certain annotationstyle word segmentation result to another style." ></td>
	<td class="line x" title="98:154	They design some class-type transformation templates and use the transformation-based errordriven learning method of Brill (1995) to learn what word delimiters should be modified." ></td>
	<td class="line x" title="99:154	However, this converter need human designed transformation templates, and is hard to be generalized to POS tagging, not to mention other structure labeling tasks." ></td>
	<td class="line x" title="100:154	Moreover, the processing procedure is divided into two isolated steps, conversion after segmentation, which suffers from error propagation and wastes the knowledge in the corpora." ></td>
	<td class="line x" title="101:154	On the contrary, our strategy is automatic, generalizable and effective." ></td>
	<td class="line x" title="102:154	In addition, many efforts have been devoted to manual treebank adaptation, where they adapt PTB to other grammar formalisms, such as such as CCG and LFG (Hockenmaier and Steedman, 2008; Cahill and Mccarthy, 2007)." ></td>
	<td class="line x" title="103:154	However, they are heuristics-based and involve heavy human engineering." ></td>
	<td class="line x" title="104:154	526 5 Experiments Our adaptation experiments are conducted from Peoples Daily (PD) to Penn Chinese Treebank 5.0 (CTB)." ></td>
	<td class="line x" title="105:154	These two corpora are segmented following different segmentation standards and labeled with different POS sets (see for example Figure 1)." ></td>
	<td class="line x" title="106:154	PD is much bigger in size, with about 100K sentences, while CTB is much smaller, with only about 18K sentences." ></td>
	<td class="line x" title="107:154	Thus a classifier trained on CTB usually falls behind that trained on PD, but CTB is preferable because it also annotates tree structures, which is very useful for downstream applications like parsing and translation." ></td>
	<td class="line x" title="108:154	For example, currently, most Chinese constituency and dependency parsers are trained on some version of CTB, using its segmentation and POS tagging as the de facto standards." ></td>
	<td class="line x" title="109:154	Therefore, we expect the knowledge adapted from PD will lead to more precise CTB-style segmenter and POS tagger, which would in turn reduce the error propagation to parsing (and translation)." ></td>
	<td class="line x" title="110:154	Experiments adapting from PD to CTB are conducted for two tasks: word segmentation alone, and joint segmentation and POS tagging (Joint S&T)." ></td>
	<td class="line x" title="111:154	The performance measurement indicators for word segmentation and Joint S&T are balanced F-measure, F = 2PR/(P +R), a function of Precision P and Recall R. For word segmentation, P indicates the percentage of words in segmentation result that are segmented correctly, and R indicates the percentage of correctly segmented words in gold standard words." ></td>
	<td class="line x" title="112:154	For Joint S&T, P and R mean nearly the same except that a word is correctly segmented only if its POS is also correctly labelled." ></td>
	<td class="line x" title="113:154	5.1 Baseline Perceptron Classifier We first report experimental results of the single perceptron classifier on CTB 5.0." ></td>
	<td class="line x" title="114:154	The original corpus is split according to former works: chapters 271300 for testing, chapters 301325 for development, and others for training." ></td>
	<td class="line x" title="115:154	Figure 4 shows the learning curves for segmentation only and Joint S&T, we find all curves tend to moderate after 7 iterations." ></td>
	<td class="line x" title="116:154	The data splitting convention of other two corpora, Peoples Daily doesnt reserve the development sets, so in the following experiments, we simply choose the model after 7 iterations when training on this corpus." ></td>
	<td class="line x" title="117:154	The first 3 rows in each sub-table of Table 3 show the performance of the single perceptron 0.880 0.890 0.900 0.910 0.920 0.930 0.940 0.950 0.960 0.970 0.980  1  2  3  4  5  6  7  8  9  10 F measure number of iterations segmentation only segmentation in Joint S&T Joint S&T Figure 4: Averaged perceptron learning curves for segmentation and Joint S&T. Train on Test on Seg F1% JST F1% Word Segmentation PD PD 97.45  PD CTB 91.71  CTB CTB 97.35  PD  CTB CTB 98.15  Joint S&T PD PD 97.57 94.54 PD CTB 91.68  CTB CTB 97.58 93.06 PD  CTB CTB 98.23 94.03 Table 3: Experimental results for both baseline models and final systems with annotation adaptation." ></td>
	<td class="line x" title="118:154	PD  CTB means annotation adaptation from PD to CTB." ></td>
	<td class="line x" title="119:154	For the upper sub-table, items of JST F1 are undefined since only segmentation is performs." ></td>
	<td class="line x" title="120:154	While in the sub-table below, JST F1 is also undefined since the model trained on PD gives a POS set different from that of CTB." ></td>
	<td class="line x" title="121:154	models." ></td>
	<td class="line x" title="122:154	Comparing row 1 and 3 in the sub-table below with the corresponding rows in the upper sub-table, we validate that when word segmentation and POS tagging are conducted jointly, the performance for segmentation improves since the POS tags provide additional information to word segmentation (Ng and Low, 2004)." ></td>
	<td class="line x" title="123:154	We also see that for both segmentation and Joint S&T, the performance sharply declines when a model trained on PD is tested on CTB (row 2 in each sub-table)." ></td>
	<td class="line x" title="124:154	In each task, only about 92% F1 is achieved." ></td>
	<td class="line x" title="125:154	This obviously fall behind those of the models trained on CTB itself (row 3 in each sub-table), about 97% F1, which are used as the baselines of the following annotation adaptation experiments." ></td>
	<td class="line x" title="126:154	527 POS #Word #BaseErr #AdaErr ErrDec% AD 305 30 19 36.67 AS 76 0 0 BA 4 1 1 CC 135 8 8 CD 356 21 14 33.33 CS 6 0 0 DEC 137 31 23 25.81 DEG 197 32 37  DEV 10 0 0 DT 94 3 1 66.67 ETC 12 0 0 FW 1 1 1 JJ 127 41 44  LB 2 1 1 LC 106 3 2 33.33 M 349 18 4 77.78 MSP 8 2 1 50.00 NN 1715 151 126 16.56 NR 713 59 50 15.25 NT 178 1 2  OD 84 0 0 P 251 10 6 40.00 PN 81 1 1 PU 997 0 1  SB 2 0 0 SP 2 2 2 VA 98 23 21 08.70 VC 61 0 0 VE 25 1 0 100.00 VV 689 64 40 37.50 SUM 6821 213 169 20.66 Table 4: Error analysis for Joint S&T on the developing set of CTB." ></td>
	<td class="line x" title="127:154	#BaseErr and #AdaErr denote the count of words that cant be recalled by the baseline model and adapted model, respectively." ></td>
	<td class="line x" title="128:154	ErrDec denotes the error reduction of Recall." ></td>
	<td class="line x" title="129:154	5.2 Adaptation for Segmentation and Tagging Table 3 also lists the results of annotation adaptation experiments." ></td>
	<td class="line x" title="130:154	For word segmentation, the model after annotation adaptation (row 4 in upper sub-table) achieves an F-measure increment of 0.8 points over the baseline model, corresponding to an error reduction of 30.2%; while for Joint S&T, the F-measure increment of the adapted model (row 4 in sub-table below) is 1 point, which corresponds to an error reduction of 14%." ></td>
	<td class="line x" title="131:154	In addition, the performance of the adapted model for Joint S&T obviously surpass that of (Jiang et al., 2008), which achieves an F1 of 93.41% for Joint S&T, although with more complicated models and features." ></td>
	<td class="line x" title="132:154	Due to the obvious improvement brought by annotation adaptation to both word segmentation and Joint S&T, we can safely conclude that the knowledge can be effectively transferred from on anInput Type Parsing F1% gold-standard segmentation 82.35 baseline segmentation 80.28 adapted segmentation 81.07 Table 5: Chinese parsing results with different word segmentation results as input." ></td>
	<td class="line x" title="133:154	notation standard to another, although using such a simple strategy." ></td>
	<td class="line x" title="134:154	To obtain further information about what kind of errors be alleviated by annotation adaptation, we conduct an initial error analysis for Joint S&T on the developing set of CTB." ></td>
	<td class="line x" title="135:154	It is reasonable to investigate the error reduction of Recall for each word cluster grouped together according to their POS tags." ></td>
	<td class="line x" title="136:154	From Table 4 we find that out of 30 word clusters appeared in the developing set of CTB, 13 clusters benefit from the annotation adaptation strategy, while 4 clusters suffer from it." ></td>
	<td class="line x" title="137:154	However, the compositive error rate of Recall for all word clusters is reduced by 20.66%, such a fact invalidates the effectivity of annotation adaptation." ></td>
	<td class="line x" title="138:154	5.3 Contribution to Chinese Parsing We adopt the Chinese parser of Xiong et al.(2005), and train it on the training set of CTB 5.0 as described before." ></td>
	<td class="line x" title="140:154	To sketch the error propagation to parsing from word segmentation, we redefine the constituent span as a constituent subtree from a start character to a end character, rather than from a start word to a end word." ></td>
	<td class="line x" title="141:154	Note that if we input the gold-standard segmented test set into the parser, the F-measure under the two definitions are the same." ></td>
	<td class="line x" title="142:154	Table 5 shows the parsing accuracies with different word segmentation results as the parsers input." ></td>
	<td class="line x" title="143:154	The parsing F-measure corresponding to the gold-standard segmentation, 82.35, represents the oracle accuracy (i.e., upperbound) of parsing on top of automatic word segmention." ></td>
	<td class="line x" title="144:154	After integrating the knowledge from PD, the enhanced word segmenter gains an F-measure increment of 0.8 points, which indicates that 38% of the error propagation from word segmentation to parsing is reduced by our annotation adaptation strategy." ></td>
	<td class="line x" title="145:154	6 Conclusion and Future Works This paper presents an automatic annotation adaptation strategy, and conducts experiments on a classic problem: word segmentation and Joint 528 S&T. To adapt knowledge from a corpus with an annotation standard that we dont require, a classifier trained on this corpus is used to pre-process the corpus with the desired annotated standard, on which a second classifier is trained with the first classifiers predication results as additional guide information." ></td>
	<td class="line x" title="146:154	Experiments of annotation adaptation from PD to CTB 5.0 for word segmentation and POS tagging show that, this strategy can make effective use of the knowledge from the corpus with different annotations." ></td>
	<td class="line x" title="147:154	It obtains considerable F-measure increment, about 0.8 point for word segmentation and 1 point for Joint S&T, with corresponding error reductions of 30.2% and 14%." ></td>
	<td class="line x" title="148:154	The final result outperforms the latest work on the same corpus which uses more complicated technologies, and achieves the state-of-the-art." ></td>
	<td class="line x" title="149:154	Moreover, such improvement further brings striking Fmeasure increment for Chinese parsing, about 0.8 points, corresponding to an error propagation reduction of 38%." ></td>
	<td class="line x" title="150:154	In the future, we will continue to research on annotation adaptation for other NLP tasks which have different annotation-style corpora." ></td>
	<td class="line x" title="151:154	Especially, we will pay efforts to the annotation standard adaptation between different treebanks, for example, from HPSG LinGo Redwoods Treebank to PTB, or even from a dependency treebank to PTB, in order to obtain more powerful PTB annotation-style parsers." ></td>
	<td class="line x" title="152:154	Acknowledgement This project was supported by National Natural Science Foundation of China, Contracts 60603095 and 60736014, and 863 State Key Project No. 2006AA010108." ></td>
	<td class="line x" title="153:154	We are especially grateful to Fernando Pereira and the anonymous reviewers for pointing us to relevant domain adaption references." ></td>
	<td class="line x" title="154:154	We also thank Yang Liu and Haitao Mi for helpful discussions." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-2011
Incremental Parsing with Monotonic Adjoining Operation
Kato, Yoshihide;Matsubara, Shigeki;"></td>
	<td class="line x" title="1:120	Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 4144, Suntec, Singapore, 4 August 2009." ></td>
	<td class="line x" title="2:120	c 2009 ACL and AFNLP Incremental Parsing with Monotonic Adjoining Operation Yoshihide Kato and Shigeki Matsubara Information Technology Center, Nagoya University Furo-cho, Chikusa-ku, Nagoya, 464-8601 Japan {yosihide,matubara}@el.itc.nagoya-u.ac.jp Abstract This paper describes an incremental parser based on an adjoining operation." ></td>
	<td class="line x" title="3:120	By using the operation, we can avoid the problem of infinite local ambiguity in incremental parsing." ></td>
	<td class="line x" title="4:120	This paper further proposes a restricted version of the adjoining operation, which preserves lexical dependencies of partial parse trees." ></td>
	<td class="line x" title="5:120	Our experimental results showed that the restriction enhances the accuracy of the incremental parsing." ></td>
	<td class="line x" title="6:120	1 Introduction Incremental parser reads a sentence from left to right, and produces partial parse trees which span all words in each initial fragment of the sentence." ></td>
	<td class="line x" title="7:120	Incremental parsing is useful to realize real-time spoken language processing systems, such as a simultaneous machine interpretation system, an automatic captioning system, or a spoken dialogue system (Allen et al., 2001)." ></td>
	<td class="line oc" title="8:120	Several incremental parsing methods have been proposed so far (Collins and Roark, 2004; Roark, 2001; Roark, 2004)." ></td>
	<td class="line o" title="9:120	In these methods, the parsers can produce the candidates of partial parse trees on a word-by-word basis." ></td>
	<td class="line n" title="10:120	However, they suffer from the problem of infinite local ambiguity, i.e., they may produce an infinite number of candidates of partial parse trees." ></td>
	<td class="line x" title="11:120	This problem is caused by the fact that partial parse trees can have arbitrarily nested left-recursive structures and there is no information to predict the depth of nesting." ></td>
	<td class="line x" title="12:120	To solve the problem, this paper proposes an incremental parsing method based on an adjoining operation." ></td>
	<td class="line x" title="13:120	By using the operation, we can avoid the problem of infinite local ambiguity." ></td>
	<td class="line x" title="14:120	This approach has been adopted by Lombardo and Sturt (1997) and Kato et al.(2004)." ></td>
	<td class="line x" title="16:120	However, this raises another problem that their adjoining operations cannot preserve lexical dependencies of partial parse trees." ></td>
	<td class="line x" title="17:120	This paper proposes a restricted version of the adjoining operation which preserves lexical dependencies." ></td>
	<td class="line x" title="18:120	Our experimental results showed that the restriction enhances the accuracy of the incremental parsing." ></td>
	<td class="line oc" title="19:120	2 Incremental Parsing This section gives a description of Collins and Roarks incremental parser (Collins and Roark, 2004) and discusses its problem." ></td>
	<td class="line o" title="20:120	Collins and Roarks parser uses a grammar defined by a 6-tuple G = (V,T,S,#,C,B)." ></td>
	<td class="line x" title="21:120	V is a set of nonterminal symbols." ></td>
	<td class="line x" title="22:120	T is a set of terminal symbols." ></td>
	<td class="line x" title="23:120	S is called a start symbol and S  V . # is a special symbol to mark the end of a constituent." ></td>
	<td class="line x" title="24:120	The rightmost child of every parent is labeled with this symbol." ></td>
	<td class="line x" title="25:120	This is necessary to build a proper probabilistic parsing model." ></td>
	<td class="line x" title="26:120	C is a set of allowable chains." ></td>
	<td class="line x" title="27:120	An allowable chain is a sequence of nonterminal symbols followed by a terminal symbol." ></td>
	<td class="line x" title="28:120	Each chain corresponds to a label sequence on a path from a node to its leftmost descendant leaf." ></td>
	<td class="line x" title="29:120	B is a set of allowable triples." ></td>
	<td class="line o" title="30:120	An allowable triple is a tuple X,Y,Z where X,Y,Z  V . The triple specifies which nonterminal symbol Z is allowed to follow a nonterminal symbol Y under a parent X. For each initial fragment of a sentence, Collins and Roarks incremental parser produces partial parse trees which span all words in the fragment." ></td>
	<td class="line x" title="31:120	Let us consider the parsing process as shown in Figure 1." ></td>
	<td class="line x" title="32:120	For the first word we, the parser produces the partial parse tree (a), if the allowable chain S  NP  PRP  we exists in C. For otherchainswhichstartwithS andendwithwe, the parser produces partial parse trees by using the chains." ></td>
	<td class="line x" title="33:120	For the next word, the parser attaches the chainVPVBPdescribetothepartialparse tree (a) 1." ></td>
	<td class="line x" title="34:120	The attachment is possible when the allowable triple S, NP, VP exists in B. 1More precisely, the chain is attached after attaching endof-constituent#under the NP node." ></td>
	<td class="line x" title="35:120	41 WePRP NP S(a) WePRP NP S(b) describeVBP VP WePRP NP S(c) describeVBP VPNP DTa WePRP NP S(d) describeVBP VPNP DTa WePRP NP S(e) describeVBP VPNP DTa NP NPNP Figure 1: A process in incremental parsing 2.1 Infinite Local Ambiguity Incremental parsing suffers from the problem of infinite local ambiguity." ></td>
	<td class="line x" title="36:120	The ambiguity is caused by left-recursion." ></td>
	<td class="line x" title="37:120	An infinite number of partial parse trees are produced, because we cannot predict the depth of left-recursive nesting." ></td>
	<td class="line x" title="38:120	Let us consider the fragment We describe a. For this fragment, there exist several candidates of partial parse trees." ></td>
	<td class="line x" title="39:120	Figure 1 shows candidates of partial parse trees." ></td>
	<td class="line x" title="40:120	The partial parse tree (c) represents that the noun phrase which starts with a has no adjunct." ></td>
	<td class="line x" title="41:120	The tree (d) represents that the noun phrase has an adjunct or is a conjunct of a coordinated noun phrase." ></td>
	<td class="line x" title="42:120	The tree (e) represents that the noun phrase has an adjunct and the noun phrase with an adjunct is a conjunct of a coordinated noun phrase." ></td>
	<td class="line x" title="43:120	The partial parse trees (d) and (e) are the instances of partial parse trees which have left-recursive structures." ></td>
	<td class="line x" title="44:120	The major problem is that there is no information to determine the depth of left-recursive nesting at this point." ></td>
	<td class="line oc" title="45:120	3 Incremental Parsing Method Based on Adjoining Operation In order to avoid the problem of infinite local ambiguity, the previous works have adopted the following approaches: (1) a beam search strategy (Collins and Roark, 2004; Roark, 2001; Roark, 2004), (2) limiting the allowable chains to those actually observed in the treebank (Collins and Roark, 2004), and (3) transforming the parse trees with a selective left-corner transformation (Johnson and Roark, 2000) before inducing the allowable chains and allowable triples (Collins and Roark, 2004)." ></td>
	<td class="line x" title="46:120	Thefirstandsecondapproachescan preventtheparserfrominfinitelyproducingpartial parse trees, but the parser has to produce partial parse trees as shown in Figure 1." ></td>
	<td class="line x" title="47:120	The local ambiguity still remains." ></td>
	<td class="line x" title="48:120	In the third approach, no left recursive structure exists in the transformed grammar,buttheparsetreesdefinedbythegrammarare different from those defined by the original grammar. It is not clear if partial parse trees defined by the transformed grammar represent syntactic relations correctly." ></td>
	<td class="line x" title="49:120	As an approach to solve these problems, we introduce an adjoining operation to incremental parsing." ></td>
	<td class="line x" title="50:120	Lombardo and Sturt (1997) and Kato et al.(2004) have already adopted this approach." ></td>
	<td class="line x" title="52:120	However, their methods have another problem that their adjoining operations cannot preserve lexical dependencies of partial parse trees." ></td>
	<td class="line x" title="53:120	To solve this problem, this section proposes a restricted version of the adjoining operation." ></td>
	<td class="line x" title="54:120	3.1 Adjoining Operation An adjoining operation is used in Tree-Adjoining Grammar (Joshi, 1985)." ></td>
	<td class="line x" title="55:120	The operation inserts a tree into another tree." ></td>
	<td class="line x" title="56:120	The inserted tree is called an auxiliary tree." ></td>
	<td class="line x" title="57:120	Each auxiliary tree has a leaf called a foot which has the same nonterminal symbol as its root." ></td>
	<td class="line x" title="58:120	An adjoining operation is defined as follows: adjoining An adjoining operation splits a parse tree  at a nonterminal node  and inserts an auxiliary tree  having the same nonterminal symbol as , i.e., combines the upper tree of  with the root of  and the lower tree of  with the foot of ." ></td>
	<td class="line x" title="59:120	Wewritea,()forthepartialparsetreeobtained by adjoining  to  at ." ></td>
	<td class="line x" title="60:120	We use simplest auxiliary trees, which consist of a root and a foot." ></td>
	<td class="line o" title="61:120	As we have seen in Figure 1, Collins and Roarks parser produces partial parse trees such as (c), (d) and (e)." ></td>
	<td class="line x" title="62:120	On the other hand, by using the adjoining operation, our parser produces only the partial parse tree (c)." ></td>
	<td class="line x" title="63:120	When a left-recursive structure is required to parse the sentence, our parser adjoinsit." ></td>
	<td class="line x" title="64:120	Intheexampleabove, theparseradjoins the auxiliary tree NP  NP to the partial parse tree (c) when the word for is read." ></td>
	<td class="line x" title="65:120	This enables 42 WePRP* NPS describeVBP* VP* NP a method We PRP*NP S describeVBP* VP* NP a method adjoining NP* WePRP* NPS describeVBP* VP*NP a method NP* PP forIN* Figure 2: Adjoining operation WePRP* NPS describeVBP* VP*NP John  's We PRP*NP S describeVBP* VP*NPadjoining NP John  's We  describe John  's We  describe  John  's (a) (b) WePRP* NPS describeVBP* VP*NP NP John  's We  describe  John  's  method (c) NN*method Figure 3: Non-monotonic adjoining operation the parser to attach the allowable chain PP  IN  for." ></td>
	<td class="line x" title="66:120	The parsing process is shown in Figure 2." ></td>
	<td class="line x" title="67:120	3.2 Adjoining Operation and Monotonicity By using the adjoining operation, we avoid the problem of infinite local ambiguity." ></td>
	<td class="line x" title="68:120	However, the adjoiningoperation cannotpreservelexicaldependencies of partial parse trees." ></td>
	<td class="line x" title="69:120	Lexical dependency is a kind of relation between words, which represents head-modifier relation." ></td>
	<td class="line x" title="70:120	We can map parse trees to sets of lexical dependencies by identifying the head-child of each constituent in the parse tree (Collins, 1999)." ></td>
	<td class="line x" title="71:120	Let us consider the parsing process as shown in Figure 3." ></td>
	<td class="line x" title="72:120	The partial parse tree (a) is a candidate for the initial fragment We describe John s." ></td>
	<td class="line x" title="73:120	We mark each head-child with a special symbol ." ></td>
	<td class="line x" title="74:120	We obtain three lexical dependencies We  describe, John    and s  describe from (a)." ></td>
	<td class="line x" title="75:120	When the parser reads the next word method, it produces the partial parse tree (b) by adjoining the auxiliary tree NP  NP." ></td>
	<td class="line x" title="76:120	The partial parse tree (b) does not have s  describe." ></td>
	<td class="line x" title="77:120	The dependency s  describe is removed when the parser adjoins the auxiliary tree NP  NP to (a)." ></td>
	<td class="line x" title="78:120	This example demonstrates that the adjoining operation cannot preserve lexical dependencies of partial parse trees." ></td>
	<td class="line x" title="79:120	Now, we define the monotonicity of the adjoining operation." ></td>
	<td class="line x" title="80:120	We say that adjoining an auxiliary tree  to a partial parse tree  at a node  is monotonic when dep()  dep(a,()) where dep is themappingfromaparsetreetoasetofdependencies." ></td>
	<td class="line x" title="81:120	An auxiliary tree  is monotonic if adjoining  to any partial parse tree is monotonic." ></td>
	<td class="line x" title="82:120	We want to exclude any non-monotonic auxiliary tree from the grammar." ></td>
	<td class="line x" title="83:120	For this purpose, we restrict the form of auxiliary trees." ></td>
	<td class="line x" title="84:120	In our framework, all auxiliary trees satisfy the following constraint:  The foot of each auxiliary tree must be the head-child of its parent." ></td>
	<td class="line x" title="85:120	The auxiliary tree NP  NP satisfies the constraint, while NP  NP does not." ></td>
	<td class="line x" title="86:120	3.3 Our Incremental Parser Our incremental parser is based on a probabilistic parsing model which assigns a probability to each operation." ></td>
	<td class="line x" title="87:120	The probability of a partial parse tree is defined by the product of the probabilities of the operations used in its construction." ></td>
	<td class="line x" title="88:120	The probability of attaching an allowable chain c to a partial parse tree  is approximated as follows: P(c | ) = Proot(R | P,L,H,tH,wH,D) Ptemplate(c | R,P,L,H) Pword(w | c,th,wh) where R is the root label of c, c is the sequence which is obtained by omitting the last element from c and w is the last element of c. The probability is conditioned on a limited context of ." ></td>
	<td class="line x" title="89:120	P is a set of the ancestor labels of R. Lis a set of the left-sibling labels of R. H is the head label in L. wH and tH are the head word and head tag of H, respectively." ></td>
	<td class="line x" title="90:120	D is a set of distance features." ></td>
	<td class="line x" title="91:120	wh and th are the word and POS tag modified by w, respectively." ></td>
	<td class="line x" title="92:120	The adjoining probability is approximated as follows: P( | ) = Padjoining( | P,L,H,D) where  is an auxiliary tree or a special symbol nil, the nil means that no auxiliary tree is adjoined." ></td>
	<td class="line oc" title="93:120	The limited contexts used in this model are similar to the previous methods (Collins and Roark, 2004; Roark, 2001; Roark, 2004)." ></td>
	<td class="line pc" title="94:120	To achieve efficient parsing, we use a beam search strategy like the previous methods (Collins and Roark, 2004; Roark, 2001; Roark, 2004)." ></td>
	<td class="line x" title="95:120	For each word position i, our parser has a priority queue Hi." ></td>
	<td class="line oc" title="96:120	Each queue Hi stores the only N-best 43 Table 1: Parsing results LR(%) LP(%) F(%) Roark (2004) 86.4 86.8 86.6 Collins and Roark (2004) 86.5 86.8 86.7 No adjoining 86.3 86.8 86.6 Non-monotonic adjoining 86.1 87.1 86.6 Monotonic adjoining 87.2 87.7 87.4 partial parse trees." ></td>
	<td class="line x" title="97:120	In addition, the parser discards the partial parse tree  whose probability P() is less than the P where P is the highest probability on the queue Hi and  is a beam factor." ></td>
	<td class="line x" title="98:120	4 Experimental Evaluation To evaluate the performance of our incremental parser, we conducted a parsing experiment." ></td>
	<td class="line x" title="99:120	We implemented the following three types of incremental parsers to assess the influence of the adjoining operation and its monotonicity: (1) without adjoining operation, (2) with non-monotonic adjoining operation, and (3) with monotonic adjoining operation." ></td>
	<td class="line x" title="100:120	The grammars were extracted from the parse trees in sections 02-21 of the Wall Street Journal in Penn Treebank." ></td>
	<td class="line x" title="101:120	We identified the head-child in each constituent by using the head rule of Collins (Collins, 1999)." ></td>
	<td class="line x" title="102:120	The probabilistic models were built by using the maximum entropy method." ></td>
	<td class="line x" title="103:120	We set the beam-width N to 300 and the beam factor  to 1011." ></td>
	<td class="line x" title="104:120	Weevaluatedtheparsingaccuracybyusingsection 23." ></td>
	<td class="line x" title="105:120	We measured labeled recall and labeled precision." ></td>
	<td class="line x" title="106:120	Table 1 shows the results2." ></td>
	<td class="line x" title="107:120	Our incremental parser is competitive with the previous ones." ></td>
	<td class="line x" title="108:120	The incremental parser with the monotonic adjoining operation outperforms the others." ></td>
	<td class="line x" title="109:120	The result means that our proposed constraint of auxiliary trees improves parsing accuracy." ></td>
	<td class="line x" title="110:120	5 Conclusion This paper has proposed an incremental parser based on an adjoining operation to solve the problem of infinite local ambiguity." ></td>
	<td class="line x" title="111:120	The adjoining operation causes another problem that the parser cannot preserve lexical dependencies of partial parse trees." ></td>
	<td class="line oc" title="112:120	To tackle this problem, we defined 2The best results of Collins and Roark (2004) (LR=88.4%, LP=89.1% and F=88.8%) are achieved when the parser utilizes the information about the final punctuation and the look-ahead." ></td>
	<td class="line x" title="113:120	However, the parsing process is not on a word-by-word basis." ></td>
	<td class="line x" title="114:120	The results shown in Table 1 are achieved when the parser does not utilize such informations." ></td>
	<td class="line x" title="115:120	the monotonicity of adjoining operation and restricted the form of auxiliary trees to satisfy the constraint of the monotonicity." ></td>
	<td class="line x" title="116:120	Our experimental result showed that the restriction improved the accuracy of our incremental parser." ></td>
	<td class="line x" title="117:120	In future work, we will investigate the incremental parser for head-final language such as Japanese." ></td>
	<td class="line x" title="118:120	Head-final language includes many indirect left-recursive structures." ></td>
	<td class="line x" title="119:120	In this paper, we dealt with direct left-recursive structures only." ></td>
	<td class="line x" title="120:120	To process indirect left-recursive structures, we need to extend our method." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-0508
An Integrated Approach to Robust Processing of Situated Spoken Dialogue
Lison, Pierre;Kruijff, Geert-Jan M.;"></td>
	<td class="line x" title="1:181	Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language SRSL 2009, pages 5865, Athens, Greece, 30 March 2009." ></td>
	<td class="line x" title="2:181	c2009 Association for Computational Linguistics An Integrated Approach to Robust Processing of Situated Spoken Dialogue Pierre Lison Language Technology Lab, DFKI GmbH, Saarbrucken, Germany pierre.lison@dfki.de Geert-Jan M. Kruijff Language Technology Lab, DFKI GmbH, Saarbrucken, Germany gj@dfki.de Abstract Spoken dialogue is notoriously hard to process with standard NLP technologies." ></td>
	<td class="line x" title="3:181	Natural spoken dialogue is replete with disfluent, partial, elided or ungrammatical utterances, all of which are very hard to accommodate in a dialogue system." ></td>
	<td class="line x" title="4:181	Furthermore, speech recognition is known to be a highly error-prone task, especially for complex, open-ended discourse domains." ></td>
	<td class="line x" title="5:181	The combination of these two problems  ill-formed and/or misrecognised speech inputs  raises a major challenge to the development of robust dialogue systems." ></td>
	<td class="line x" title="6:181	We present an integrated approach for addressing these two issues, based on a incremental parser for Combinatory Categorial Grammar." ></td>
	<td class="line x" title="7:181	The parser takes word lattices as input and is able to handle illformed and misrecognised utterances by selectively relaxing its set of grammatical rules." ></td>
	<td class="line x" title="8:181	The choice of the most relevant interpretation is then realised via a discriminative model augmented with contextual information." ></td>
	<td class="line x" title="9:181	The approach is fully implemented in a dialogue system for autonomous robots." ></td>
	<td class="line x" title="10:181	Evaluation results on a Wizard of Oz test suite demonstrate very significant improvements in accuracy and robustness compared to the baseline." ></td>
	<td class="line x" title="11:181	1 Introduction Spoken dialogue is often considered to be one of the most natural means of interaction between a human and a robot." ></td>
	<td class="line x" title="12:181	It is, however, notoriously hard to process with standard language processing technologies." ></td>
	<td class="line x" title="13:181	Dialogue utterances are often incomplete or ungrammatical, and may contain numerous disfluencies like fillers (err, uh, mm), repetitions, self-corrections, etc. Rather than getting crisp-and-clear commands such as Put the red ball inside the box!, it is more likely the robot will hear such kind of utterance: right, now, could you, uh, put the red ball, yeah, inside the ba/ box!." ></td>
	<td class="line x" title="14:181	This is natural behaviour in human-human interaction (Fernandez and Ginzburg, 2002) and can also be observed in several domain-specific corpora for human-robot interaction (Topp et al., 2006)." ></td>
	<td class="line x" title="15:181	Moreover, even in the (rare) case where the utterance is perfectly well-formed and does not contain any kind of disfluencies, the dialogue system still needs to accomodate the various speech recognition errors thay may arise." ></td>
	<td class="line x" title="16:181	This problem is particularly acute for robots operating in realworld noisy environments and deal with utterances pertaining to complex, open-ended domains." ></td>
	<td class="line x" title="17:181	The paper presents a new approach to address these two difficult issues." ></td>
	<td class="line x" title="18:181	Our starting point is the work done by Zettlemoyer and Collins on parsing using relaxed CCG grammars (Zettlemoyer and Collins, 2007) (ZC07)." ></td>
	<td class="line x" title="19:181	In order to account for natural spoken language phenomena (more flexible word order, missing words, etc.), they augment their grammar framework with a small set of non-standard combinatory rules, leading to a relaxation of the grammatical constraints." ></td>
	<td class="line x" title="20:181	A discriminative model over the parses is coupled with the parser, and is responsible for selecting the most likely interpretation(s) among the possible ones." ></td>
	<td class="line x" title="21:181	In this paper, we extend their approach in two important ways." ></td>
	<td class="line x" title="22:181	First, ZC07 focused on the treatment of ill-formed input, and ignored the speech recognition issues." ></td>
	<td class="line x" title="23:181	Our system, to the contrary, is able to deal with both ill-formed and misrecognized input, in an integrated fashion." ></td>
	<td class="line x" title="24:181	This is done by augmenting the set of non-standard combinators with new rules specifically tailored to deal with speech recognition errors." ></td>
	<td class="line x" title="25:181	Second, the only features used by ZC07 are syntactic features (see 3.4 for details)." ></td>
	<td class="line x" title="26:181	We significantly extend the range of features included in the 58 discriminative model, by incorporating not only syntactic, but also acoustic, semantic and contextual information into the model." ></td>
	<td class="line x" title="27:181	An overview of the paper is as follows." ></td>
	<td class="line x" title="28:181	We first describe in Section 2 the cognitive architecture in which our system has been integrated." ></td>
	<td class="line x" title="29:181	We then discuss the approach in detail in Section 3." ></td>
	<td class="line x" title="30:181	Finally, we present in Section 4 the quantitative evaluations on a WOZ test suite, and conclude." ></td>
	<td class="line x" title="31:181	2 Architecture The approach we present in this paper is fully implemented and integrated into a cognitive architecture for autonomous robots." ></td>
	<td class="line x" title="32:181	A recent version of this system is described in (Hawes et al., 2007)." ></td>
	<td class="line x" title="33:181	It is capable of building up visuo-spatial models of a dynamic local scene, continuously plan and execute manipulation actions on objects within that scene." ></td>
	<td class="line x" title="34:181	The robot can discuss objects and their materialand spatial properties for the purpose of visual learning and manipulation tasks." ></td>
	<td class="line x" title="35:181	Figure 1: Architecture schema of the communication subsystem (only for comprehension)." ></td>
	<td class="line x" title="36:181	Figure 2 illustrates the architecture schema for the communication subsystem incorporated in the cognitive architecture (only the comprehension part is shown)." ></td>
	<td class="line x" title="37:181	Starting with ASR, we process the audio signal to establish a word lattice containing statistically ranked hypotheses about word sequences." ></td>
	<td class="line x" title="38:181	Subsequently, parsing constructs grammatical analyses for the given word lattice." ></td>
	<td class="line x" title="39:181	A grammatical analysis constructs both a syntactic analysis of the utterance, and a representation of its meaning." ></td>
	<td class="line x" title="40:181	The analysis is based on an incremental chart parser1 for Combinatory Categorial Grammar (Steedman and Baldridge, 2009)." ></td>
	<td class="line x" title="41:181	These meaning representations are ontologically richly sorted, relational 1Built on top of the OpenCCG NLP library: http://openccg.sf.net structures, formulated in a (propositional) description logic, more precisely in the HLDS formalism (Baldridge and Kruijff, 2002)." ></td>
	<td class="line x" title="42:181	The parser compacts all meaning representations into a single packed logical form (Carroll and Oepen, 2005; Kruijff et al., 2007)." ></td>
	<td class="line x" title="43:181	A packed LF represents content similar across the different analyses as a single graph, using overand underspecification of how different nodes can be connected to capture lexical and syntactic forms of ambiguity." ></td>
	<td class="line x" title="44:181	At the level of dialogue interpretation, a packed logical form is resolved against a SDRS-like dialogue model (Asher and Lascarides, 2003) to establish contextual co-reference and dialogue moves." ></td>
	<td class="line x" title="45:181	Linguistic interpretations must finally be associated with extra-linguistic knowledge about the environment  dialogue comprehension hence needs to connect with other subarchitectures like vision, spatial reasoning or planning." ></td>
	<td class="line x" title="46:181	We realise this information binding between different modalities via a specific module, called the binder, which is responsible for the ontology-based mediation accross modalities (Jacobsson et al., 2008)." ></td>
	<td class="line x" title="47:181	2.1 Context-sensitivity The combinatorial nature of language provides virtually unlimited ways in which we can communicate meaning." ></td>
	<td class="line x" title="48:181	This, of course, raises the question of how precisely an utterance should then be understood as it is being heard." ></td>
	<td class="line x" title="49:181	Empirical studies have investigated what information humans use when comprehending spoken utterances." ></td>
	<td class="line x" title="50:181	An important observation is that interpretation in context plays a crucial role in the comprehension of utterance as it unfolds (Knoeferle and Crocker, 2006)." ></td>
	<td class="line x" title="51:181	During utterance comprehension, humans combine linguistic information with scene understanding and world knowledge." ></td>
	<td class="line x" title="52:181	Figure 2: Context-sensitivity in processing situated dialogue understanding Several approaches in situated dialogue for human-robot interaction have made similar obser59 vations (Roy, 2005; Roy and Mukherjee, 2005; Brick and Scheutz, 2007; Kruijff et al., 2007): A robots understanding can be improved by relating utterances to the situated context." ></td>
	<td class="line x" title="53:181	As we will see in the next section, by incorporating contextual information into our model, our approach to robust processing of spoken dialogue seeks to exploit this important insight." ></td>
	<td class="line x" title="54:181	3 Approach 3.1 Grammar relaxation Our approach to robust processing of spoken dialogue rests on the idea of grammar relaxation: the grammatical constraints specified in the grammar are relaxed to handle slightly ill-formed or misrecognised utterances." ></td>
	<td class="line x" title="55:181	Practically, the grammar relaxation is done via the introduction of non-standard CCG rules (Zettlemoyer and Collins, 2007)." ></td>
	<td class="line x" title="56:181	In Combinatory Categorial Grammar, the rules are used to assemble categories to form larger pieces of syntactic and semantic structure." ></td>
	<td class="line x" title="57:181	The standard rules are application (<,>), composition (B), and type raising (T) (Steedman and Baldridge, 2009)." ></td>
	<td class="line x" title="58:181	Several types of non-standard rules have been introduced." ></td>
	<td class="line x" title="59:181	We describe here the two most important ones: the discourse-level composition rules, and the ASR correction rules." ></td>
	<td class="line x" title="60:181	We invite the reader to consult (Lison, 2008) for more details on the complete set of grammar relaxation rules." ></td>
	<td class="line x" title="61:181	3.1.1 Discourse-level composition rules In natural spoken dialogue, we may encounter utterances containing several independent chunks without any explicit separation (or only a short pause or a slight change in intonation), such as (1) yes take the ball no the other one on your left right and now put it in the box. Even if retrieving a fully structured parse for this utterance is difficult to achieve, it would be useful to have access to a list of smaller discourse units." ></td>
	<td class="line x" title="62:181	Syntactically speaking, a discourse unit can be any type of saturated atomic categories from a simple discourse marker to a full sentence." ></td>
	<td class="line x" title="63:181	The type raising rule Tdu allows the conversion of atomic categories into discourse units: A : @ifdu : @if (Tdu) where A represents an arbitrary saturated atomic category (s, np, pp, etc.)." ></td>
	<td class="line x" title="64:181	The rule>C is responsible for the integration of two discourse units into a single structure: du : @if, du : @jg du : @{d:d-units}(list (FIRST if) (NEXT jg)) (>C) 3.1.2 ASR error correction rules Speech recognition is a highly error-prone task." ></td>
	<td class="line x" title="65:181	It is however possible to partially alleviate this problem by inserting new error-correction rules (more precisely, new lexical entries) for the most frequently misrecognised words." ></td>
	<td class="line x" title="66:181	If we notice e.g. that the ASR system frequently substitutes the word wrong for the word round during the recognition (because of their phonological proximity), we can introduce a new lexical entry in the lexicon in order to correct this error: roundturnstileleftadj : @attitude(wrong) (2) A set of thirteen new lexical entries of this type have been added to our lexicon to account for the most frequent recognition errors." ></td>
	<td class="line x" title="67:181	3.2 Parse selection Using more powerful grammar rules to relax the grammatical analysis tends to increase the number of parses." ></td>
	<td class="line x" title="68:181	We hence need a a mechanism to discriminate among the possible parses." ></td>
	<td class="line x" title="69:181	The task of selecting the most likely interpretation among a set of possible ones is called parse selection." ></td>
	<td class="line x" title="70:181	Once all the possible parses for a given utterance are computed, they are subsequently filtered or selected in order to retain only the most likely interpretation(s)." ></td>
	<td class="line x" title="71:181	This is done via a (discriminative) statistical model covering a large number of features." ></td>
	<td class="line x" title="72:181	Formally, the task is defined as a function F : XY where the domainX is the set of possible inputs (in our case, X is the set of possible word lattices), andY the set of parses." ></td>
	<td class="line x" title="73:181	We assume: 1." ></td>
	<td class="line x" title="74:181	A function GEN(x) which enumerates all possible parses for an input x. In our case, this function simply represents the set of parses of x which are admissible according to the CCG grammar." ></td>
	<td class="line x" title="75:181	2." ></td>
	<td class="line x" title="76:181	A d-dimensional feature vector f(x,y)  Rfracturd, representing specific features of the pair (x,y)." ></td>
	<td class="line x" title="77:181	It can include various acoustic, syntactic, semantic or contextual features which can be relevant in discriminating the parses." ></td>
	<td class="line x" title="78:181	60 3." ></td>
	<td class="line x" title="79:181	A parameter vector wRfracturd." ></td>
	<td class="line x" title="80:181	The function F, mapping a word lattice to its most likely parse, is then defined as: F(x) = argmax yGEN(x) wT f(x,y) (3) where wT  f(x,y) is the inner productsummationtext d s=1ws fs(x,y), and can be seen as a measure of the quality of the parse." ></td>
	<td class="line x" title="81:181	Given the parameters w, the optimal parse of a given utterance x can be therefore easily determined by enumerating all the parses generated by the grammar, extracting their features, computing the inner product wTf(x,y), and selecting the parse with the highest score." ></td>
	<td class="line x" title="82:181	The task of parse selection is an example of structured classification problem, which is the problem of predicting an output y from an input x, where the output y has a rich internal structure." ></td>
	<td class="line x" title="83:181	In the specific case of parse selection, x is a word lattice, and y a logical form." ></td>
	<td class="line x" title="84:181	3.3 Learning 3.3.1 Training data In order to estimate the parameters w, we need a set of training examples." ></td>
	<td class="line x" title="85:181	Unfortunately, no corpus of situated dialogue adapted to our task domain is available to this day, let alone semantically annotated." ></td>
	<td class="line x" title="86:181	The collection of in-domain data via Wizard of Oz experiments being a very costly and timeconsuming process, we followed the approach advocated in (Weilhammer et al., 2006) and generated a corpus from a hand-written task grammar." ></td>
	<td class="line x" title="87:181	To this end, we first collected a small set of WoZ data, totalling about a thousand utterances." ></td>
	<td class="line x" title="88:181	This set is too small to be directly used as a corpus for statistical training, but sufficient to capture the most frequent linguistic constructions in this particular context." ></td>
	<td class="line x" title="89:181	Based on it, we designed a domain-specific CFG grammar covering most of the utterances." ></td>
	<td class="line x" title="90:181	Each rule is associated to a semantic HLDS representation." ></td>
	<td class="line x" title="91:181	Weights are automatically assigned to each grammar rule by parsing our corpus, hence leading to a small stochastic CFG grammar augmented with semantic information." ></td>
	<td class="line x" title="92:181	Once the grammar is specified, it is randomly traversed a large number of times, resulting in a larger set (about 25.000) of utterances along with their semantic representations." ></td>
	<td class="line x" title="93:181	Since we are interested in handling errors arising from speech recognition, we also need to simulate the most frequent recognition errors." ></td>
	<td class="line x" title="94:181	To this end, we synthesise each string generated by the domain-specific CFG grammar, using a text-to-speech engine2, feed the audio stream to the speech recogniser, and retrieve the recognition result." ></td>
	<td class="line x" title="95:181	Via this technique, we are able to easily collect a large amount of training data3." ></td>
	<td class="line x" title="96:181	3.3.2 Perceptron learning The algorithm we use to estimate the parameters w using the training data is a perceptron." ></td>
	<td class="line x" title="97:181	The algorithm is fully online it visits each example in turn and updates w if necessary." ></td>
	<td class="line pc" title="98:181	Albeit simple, the algorithm has proven to be very efficient and accurate for the task of parse selection (Collins and Roark, 2004; Collins, 2004; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007)." ></td>
	<td class="line x" title="99:181	The pseudo-code for the online learning algorithm is detailed in [Algorithm 1]." ></td>
	<td class="line x" title="100:181	It works as follows: the parameters w are first initialised to some arbitrary values." ></td>
	<td class="line x" title="101:181	Then, for each pair (xi,zi) in the training set, the algorithm searchs for the parse yprime with the highest score according to the current model." ></td>
	<td class="line x" title="102:181	If this parse happens to match the best parse which generates zi (which we shall denote y), we move to the next example." ></td>
	<td class="line x" title="103:181	Else, we perform a simple perceptron update on the parameters: w = w+f(xi,y)f(xi,yprime) (4) The iteration on the training set is repeated T times, or until convergence." ></td>
	<td class="line x" title="104:181	The most expensive step in this algorithm is the calculation of yprime = argmaxyGEN(xi) wT  f(xi,y) this is the decoding problem." ></td>
	<td class="line oc" title="105:181	It is possible to prove that, provided the training set (xi,zi) is separable with margin > 0, the algorithm is assured to converge after a finite number of iterations to a model with zero training errors (Collins and Roark, 2004)." ></td>
	<td class="line oc" title="106:181	See also (Collins, 2004) for convergence theorems and proofs." ></td>
	<td class="line x" title="107:181	3.4 Features As we have seen, the parse selection operates by enumerating the possible parses and selecting the 2We used MARY (http://mary.dfki.de) for the text-to-speech engine." ></td>
	<td class="line x" title="108:181	3Because of its relatively artificial character, the quality of such training data is naturally lower than what could be obtained with a genuine corpus." ></td>
	<td class="line x" title="109:181	But, as the experimental results will show, it remains sufficient to train the perceptron for the parse selection task, and achieve significant improvements in accuracy and robustness." ></td>
	<td class="line x" title="110:181	In a near future, we plan to progressively replace this generated training data by a real spoken dialogue corpus adapted to our task domain." ></td>
	<td class="line x" title="111:181	61 Algorithm 1 Online perceptron learning Require: set of n training examples{(xi,zi) : i = 1n} T: number of iterations over the training set GEN(x): function enumerating possible parses for an input x, according to the CCG grammar." ></td>
	<td class="line x" title="112:181	GEN(x,z): function enumerating possible parses for an input x and which have semantics z, according to the CCG grammar." ></td>
	<td class="line x" title="113:181	L(y) maps a parse tree y to its logical form." ></td>
	<td class="line x" title="114:181	Initial parameter vector w0 % Initialise ww0 % Loop T times on the training examples for t = 1T do for i = 1n do % Compute best parse according to current model Let yprime = argmaxyGEN(xi) wT f(xi,y) % If the decoded parsenegationslash= expected parse, update the parameters if L(yprime)negationslash= zi then % Search the best parse for utterance xi with semantics zi Let y = argmaxyGEN(xi,zi) wT f(xi,y) % Update parameter vector w Set w = w + f(xi,y)f(xi,yprime) end if end for end for return parameter vector w one with the highest score according to the linear model parametrised by w. The accuracy of our method crucially relies on the selection of good features f(x,y) for our model that is, features which help discriminating the parses." ></td>
	<td class="line x" title="115:181	They must also be relatively cheap to compute." ></td>
	<td class="line x" title="116:181	In our model, the features are of four types: semantic features, syntactic features, contextual features, and speech recognition features." ></td>
	<td class="line x" title="117:181	3.4.1 Semantic features What are the substructures of a logical form which may be relevant to discriminate the parses?" ></td>
	<td class="line x" title="118:181	We define features on the following information sources: 1." ></td>
	<td class="line x" title="119:181	Nominals: for each possible pair prop,sort, we include a feature fi in f(x,y) counting the number of nominals with ontological sort sort and proposition prop in the logical form." ></td>
	<td class="line x" title="120:181	2." ></td>
	<td class="line x" title="121:181	Ontological sorts: occurrences of specific ontological sorts in the logical form." ></td>
	<td class="line x" title="122:181	Figure 3: graphical representation of the HLDS logical form for I want you to take the mug." ></td>
	<td class="line x" title="123:181	3." ></td>
	<td class="line x" title="124:181	Dependency relations: following (Clark and Curran, 2003), we also model the dependency structure of the logical form." ></td>
	<td class="line x" title="125:181	Each dependency relation is defined as a triple sorta, sortb, label, where sorta denotes the sort of the incoming nominal, sortb the sort of the outgoing nominal, and label is the relation label." ></td>
	<td class="line x" title="126:181	4." ></td>
	<td class="line x" title="127:181	Sequences of dependency relations: number of occurrences of particular sequences (ie." ></td>
	<td class="line x" title="128:181	bigram counts) of dependency relations." ></td>
	<td class="line x" title="129:181	The features on nominals and ontological sorts aim at modeling (aspects of) lexical semantics e.g. which meanings are the most frequent for a given word -, whereas the features on relations and sequence of relations focus on sentential semantics which dependencies are the most frequent." ></td>
	<td class="line x" title="130:181	These features therefore help us handle lexical and syntactic ambiguities." ></td>
	<td class="line x" title="131:181	3.4.2 Syntactic features By syntactic features, we mean features associated to the derivational history of a specific parse." ></td>
	<td class="line x" title="132:181	The main use of these features is to penalise to a correct extent the application of the non-standard rules introduced into the grammar." ></td>
	<td class="line x" title="133:181	To this end, we include in the feature vector f(x,y) a new feature for each non-standard rule, which counts the number of times the rule was applied in the parse." ></td>
	<td class="line x" title="134:181	62 pick s/particle/np cup up corr particle s/np > the np/n balln np > s > Figure 4: CCG derivation of pick cup the ball." ></td>
	<td class="line x" title="135:181	In the derivation shown in the figure 4, the rule corr (correction of a speech recognition error) is applied once, so the corresponding feature value is set to 1." ></td>
	<td class="line x" title="136:181	The feature values for the remaining rules are set to 0, since they are absent from the parse." ></td>
	<td class="line x" title="137:181	These syntactic features can be seen as a penalty given to the parses using these non-standard rules, thereby giving a preference to the normal parses over them." ></td>
	<td class="line x" title="138:181	This mechanism ensures that the grammar relaxation is only applied as a last resort when the usual grammatical analysis fails to provide a full parse." ></td>
	<td class="line x" title="139:181	Of course, depending on the relative frequency of occurrence of these rules in the training corpus, some of them will be more strongly penalised than others." ></td>
	<td class="line x" title="140:181	3.4.3 Contextual features As we have already outlined in the background section, one striking characteristic of spoken dialogue is the importance of context." ></td>
	<td class="line x" title="141:181	Understanding the visual and discourse contexts is crucial to resolve potential ambiguities and compute the most likely interpretation(s) of a given utterance." ></td>
	<td class="line x" title="142:181	The feature vector f(x,y) therefore includes various features related to the context: 1." ></td>
	<td class="line x" title="143:181	Activated words: our dialogue system maintains in its working memory a list of contextually activated words (cfr." ></td>
	<td class="line x" title="144:181	(Lison and Kruijff, 2008))." ></td>
	<td class="line x" title="145:181	This list is continuously updated as the dialogue and the environment evolves." ></td>
	<td class="line x" title="146:181	For each context-dependent word, we include one feature counting the number of times it appears in the utterance string." ></td>
	<td class="line x" title="147:181	2." ></td>
	<td class="line x" title="148:181	Expected dialogue moves: for each possible dialogue move, we include one feature indicating if the dialogue move is consistent with the current discourse model." ></td>
	<td class="line x" title="149:181	These features ensure for instance that the dialogue move following a QuestionYN is a Accept, Reject or another question (e.g. for clarification requests), but almost never an Opening." ></td>
	<td class="line x" title="150:181	3." ></td>
	<td class="line x" title="151:181	Expected syntactic categories: for each atomic syntactic category in the CCG grammar, we include one feature indicating if the category is consistent with the current discourse model." ></td>
	<td class="line x" title="152:181	These features can be used to handle sentence fragments." ></td>
	<td class="line x" title="153:181	3.4.4 Speech recognition features Finally, the feature vector f(x,y) also includes features related to the speech recognition." ></td>
	<td class="line x" title="154:181	The ASR module outputs a set of (partial) recognition hypotheses, packed in a word lattice." ></td>
	<td class="line x" title="155:181	One example of such a structure is given in Figure 5." ></td>
	<td class="line x" title="156:181	Each recognition hypothesis is provided with an associated confidence score, and we want to favour the hypotheses with high confidence scores, which are, according to the statistical models incorporated in the ASR, more likely to reflect what was uttered." ></td>
	<td class="line x" title="157:181	To this end, we introduce three features: the acoustic confidence score (confidence score provided by the statistical models included in the ASR), the semantic confidence score (based on a concept model also provided by the ASR), and the ASR ranking (hypothesis rank in the word lattice, from best to worst)." ></td>
	<td class="line x" title="158:181	Figure 5: Example of word lattice 4 Experimental evaluation We performed a quantitative evaluation of our approach, using its implementation in a fully integrated system (cf.Section 2)." ></td>
	<td class="line x" title="160:181	To set up the experiments for the evaluation, we have gathered a corpus of human-robot spoken dialogue for our task-domain, which we segmented and annotated manually with their expected semantic interpretation." ></td>
	<td class="line x" title="161:181	The data set contains 195 individual utterances along with their complete logical forms." ></td>
	<td class="line x" title="162:181	4.1 Results Three types of quantitative results are extracted from the evaluation results: exact-match, partialmatch, and word error rate." ></td>
	<td class="line x" title="163:181	Tables 1, 2 and 3 illustrate the results, broken down by use of grammar relaxation, use of parse selection, and number of recognition hypotheses considered." ></td>
	<td class="line x" title="164:181	63 Size of word lattice (number of NBests) Grammar relaxation Parse selection Precision Recall F1-value (Baseline) 1 No No 40.9 45.2 43.0 . 1 No Yes 59.0 54.3 56.6 . 1 Yes Yes 52.7 70.8 60.4 . 3 Yes Yes 55.3 82.9 66.3 . 5 Yes Yes 55.6 84.0 66.9 (Full approach) 10 Yes Yes 55.6 84.9 67.2 Table 1: Exact-match accuracy results (in percents)." ></td>
	<td class="line x" title="165:181	Size of word lattice (number of NBests) Grammar relaxation Parse selection Precision Recall F1-value (Baseline) 1 No No 86.2 56.2 68.0 . 1 No Yes 87.4 56.6 68.7 . 1 Yes Yes 88.1 76.2 81.7 . 3 Yes Yes 87.6 85.2 86.4 . 5 Yes Yes 87.6 86.0 86.8 (Full approach) 10 Yes Yes 87.7 87.0 87.3 Table 2: Partial-match accuracy results (in percents)." ></td>
	<td class="line x" title="166:181	Each line in the tables corresponds to a possible configuration." ></td>
	<td class="line x" title="167:181	Tables 1 and 2 give the precision, recall andF1 value for each configuration (respectively for the exactand partial-match), and Table 3 gives the Word Error Rate [WER]." ></td>
	<td class="line x" title="168:181	The first line corresponds to the baseline: no grammar relaxation, no parse selection, and use of the first NBest recognition hypothesis." ></td>
	<td class="line x" title="169:181	The last line corresponds to the results with the full approach: grammar relaxation, parse selection, and use of 10 recognition hypotheses." ></td>
	<td class="line x" title="170:181	Size of word lattice (NBests) Grammar relaxation Parse selection WER 1 No No 20.5 1 Yes Yes 19.4 3 Yes Yes 16.5 5 Yes Yes 15.7 10 Yes Yes 15.7 Table 3: Word error rate (in percents)." ></td>
	<td class="line x" title="171:181	4.2 Comparison with baseline Here are the comparative results we obtained:  Regarding the exact-match results between the baseline and our approach (grammar relaxation and parse selection with all features activated for NBest 10), theF1-measure climbs from 43.0 % to 67.2 %, which means a relative difference of 56.3 %." ></td>
	<td class="line x" title="172:181	 For the partial-match, the F1-measure goes from 68.0 % for the baseline to 87.3 % for our approach  a relative increase of 28.4 %." ></td>
	<td class="line x" title="173:181	 We obverse a significant decrease in WER: we go from 20.5 % for the baseline to 15.7 % with our approach." ></td>
	<td class="line x" title="174:181	The difference is statistically significant (p-value for t-tests is 0.036), and the relative decrease of 23.4 %." ></td>
	<td class="line x" title="175:181	5 Conclusions We presented an integrated approach to the processing of (situated) spoken dialogue, suited to the specific needs and challenges encountered in human-robot interaction." ></td>
	<td class="line x" title="176:181	In order to handle disfluent, partial, ill-formed or misrecognized utterances, the grammar used by the parser is relaxed via the introduction of a set of non-standard combinators which allow for the insertion/deletion of specific words, the combination of discourse fragments or the correction of speech recognition errors." ></td>
	<td class="line x" title="177:181	The relaxed parser yields a (potentially large) set of parses, which are then packed and retrieved by the parse selection module." ></td>
	<td class="line x" title="178:181	The parse selection is based on a discriminative model exploring a set of relevant semantic, syntactic, contextual and acoustic features extracted for each parse." ></td>
	<td class="line x" title="179:181	The parameters of this model are estimated against an automatically generated corpus ofutterance, logical formpairs." ></td>
	<td class="line x" title="180:181	The learning algorithm is an perceptron, a simple albeit efficient technique for parameter estimation." ></td>
	<td class="line x" title="181:181	As forthcoming work, we shall examine the potential extension of our approach in new directions, such as the exploitation of parse selection for incremental scoring/pruning of the parse chart, 64 the introduction of more refined contextual features, or the use of more sophisticated learning algorithms, such as Support Vector Machines." ></td>
</tr></table>
</div
</body></html>
