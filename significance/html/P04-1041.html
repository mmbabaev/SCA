<html><body><head><link rel="stylesheet" type="text/css" href="style.css" /><script src="map.js"></script><script src="jquery-1.7.1.min.js"></script></head>
<div class="dstPaperData">
P04-1041 <div class="dstPaperTitle">Long-Distance Dependency Resolution In Automatically Acquired Wide-Coverage PCFG-Based LFG Approximations</div><div class="dstPaperAuthors">Cahill, Aoife;Burke, Michael;O'Donovan, Ruth;Van Genabith, Josef;Way, Andy;</div>
</div>
<table cellspacing="0" cellpadding="0"><tr>
	<td class="srcData" >Source Paper</td>
	<td class="pp legend" ><input type="checkbox" id="cbIPositive" checked="true"/><label for="cbIPositive">Informal +<label></td>
	<td class="nn legend" ><input type="checkbox" id="cbINegative" checked="true"/><label for="cbINegative">Informal -<label></td>
	<td class="oo legend" ><input type="checkbox" id="cbIObjective" checked="true"/><label for="cbIObjective">Informal Neutral<label></td>
	<td class="ppc legend" ><input type="checkbox" id="cbEPositive" checked="true"/><label for="cbEPositive">Formal +</label></td>
	<td class="nnc legend" ><input type="checkbox" id="cbENegative" checked="true"/><label for="cbENegative">Formal -</label></td>
	<td class="ooc legend" ><input type="checkbox" id="cbEObjective" checked="true"/><label for="cbEObjective">Formal Neutral</label></td>
	<td class="lb"><input type="checkbox" id="cbSentenceBoundary"/><label for="cbSentenceBoundary">Sentence Boundary</label></td>
</tr></table>
<div class="dstPaper">
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P04-1047
Large-Scale Induction And Evaluation Of Lexical Resources From The Penn-II Treebank
O'Donovan, Ruth;Burke, Michael;Cahill, Aoife;Van Genabith, Josef;Way, Andy;"></td>
	<td class="line x" title="1:199	Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II Treebank Ruth ODonovan, Michael Burke, Aoife Cahill, Josef van Genabith, Andy Way National Centre for Language Technology and School of Computing Dublin City University Glasnevin Dublin 9 Ireland {rodonovan,mburke,acahill,josef,away}@computing.dcu.ie Abstract In this paper we present a methodology for extracting subcategorisation frames based on an automatic LFG f-structure annotation algorithm for the Penn-II Treebank." ></td>
	<td class="line x" title="2:199	We extract abstract syntactic function-based subcategorisation frames (LFG semantic forms), traditional CFG categorybased subcategorisation frames as well as mixed function/category-based frames, with or without preposition information for obliques and particle information for particle verbs." ></td>
	<td class="line x" title="3:199	Our approach does not predefine frames, associates probabilities with frames conditional on the lemma, distinguishes between active and passive frames, and fully reflects the effects of long-distance dependencies in the source data structures." ></td>
	<td class="line x" title="4:199	We extract 3586 verb lemmas, 14348 semantic form types (an average of 4 per lemma) with 577 frame types." ></td>
	<td class="line x" title="5:199	We present a large-scale evaluation of the complete set of forms extracted against the full COMLEX resource." ></td>
	<td class="line x" title="6:199	1 Introduction Lexical resources are crucial in the construction of wide-coverage computational systems based on modern syntactic theories (e.g. LFG, HPSG, CCG, LTAG etc.)." ></td>
	<td class="line x" title="7:199	However, as manual construction of such lexical resources is time-consuming, errorprone, expensive and rarely ever complete, it is often the case that limitations of NLP systems based on lexicalised approaches are due to bottlenecks in the lexicon component." ></td>
	<td class="line x" title="8:199	Given this, research on automating lexical acquisition for lexically-based NLP systems is a particularly important issue." ></td>
	<td class="line x" title="9:199	In this paper we present an approach to automating subcategorisation frame acquisition for LFG (Kaplan and Bresnan, 1982) i.e. grammatical function-based systems." ></td>
	<td class="line x" title="10:199	LFG has two levels of structural representation: c(onstituent)structure, and f(unctional)-structure." ></td>
	<td class="line x" title="11:199	LFG differentiates between governable (argument) and nongovernable (adjunct) grammatical functions." ></td>
	<td class="line x" title="12:199	Subcategorisation requirements are enforced through semantic forms specifying the governable grammatical functions required by a particular predicate (e.g. FOCUS( SUBJ)( OBLon))." ></td>
	<td class="line oc" title="13:199	Our approach is based on earlier work on LFG semantic form extraction (van Genabith et al. , 1999) and recent progress in automatically annotating the Penn-II treebank with LFG f-structures (Cahill et al. , 2004b)." ></td>
	<td class="line x" title="14:199	Depending on the quality of the f-structures, reliable LFG semantic forms can then be generated quite simply by recursively reading off the subcategorisable grammatical functions for each local pred value at each level of embedding in the f-structures." ></td>
	<td class="line x" title="15:199	The work reported in (van Genabith et al. , 1999) was small scale (100 trees), proof of concept and required considerable manual annotation work." ></td>
	<td class="line oc" title="16:199	In this paper we show how the extraction process can be scaled to the complete Wall Street Journal (WSJ) section of the Penn-II treebank, with about 1 million words in 50,000 sentences, based on the automatic LFG f-structure annotation algorithm described in (Cahill et al. , 2004b)." ></td>
	<td class="line x" title="17:199	In addition to extracting grammatical function-based subcategorisation frames, we also include the syntactic categories of the predicate and its subcategorised arguments, as well as additional details such as the prepositions required by obliques, and particles accompanying particle verbs." ></td>
	<td class="line x" title="18:199	Our method does not predefine the frames to be extracted." ></td>
	<td class="line x" title="19:199	In contrast to many other approaches, it discriminates between active and passive frames, properly reflects long distance dependencies and assigns conditional probabilities to the semantic forms associated with each predicate." ></td>
	<td class="line x" title="20:199	Section 2 reviews related work in the area of automatic subcategorisation frame extraction." ></td>
	<td class="line x" title="21:199	Our methodology and its implementation are presented in Section 3." ></td>
	<td class="line x" title="22:199	Section 4 presents the results of our lexical extraction." ></td>
	<td class="line x" title="23:199	In Section 5 we evaluate the complete extracted lexicon against the COMLEX resource (MacLeod et al. , 1994)." ></td>
	<td class="line x" title="24:199	To our knowledge, this is the largest evaluation of subcategorisation frames for English." ></td>
	<td class="line x" title="25:199	In Section 6, we conclude and give suggestions for future work." ></td>
	<td class="line x" title="26:199	2 Related Work Creating a (subcategorisation) lexicon by hand is time-consuming, error-prone, requires considerable linguistic expertise and is rarely, if ever, complete." ></td>
	<td class="line x" title="27:199	In addition, a system incorporating a manually constructed lexicon cannot easily be adapted to specific domains." ></td>
	<td class="line x" title="28:199	Accordingly, many researchers have attempted to construct lexicons automatically, especially for English." ></td>
	<td class="line x" title="29:199	(Brent, 1993) relies on local morphosyntactic cues (such as the -ing suffix, except where such a word follows a determiner or a preposition other than to) in the untagged Brown Corpus as probabilistic indicators of six different predefined subcategorisation frames." ></td>
	<td class="line x" title="30:199	The frames do not include details of specific prepositions." ></td>
	<td class="line x" title="31:199	(Manning, 1993) observes that Brents recognition technique is a rather simplistic and inadequate approach to verb detection, with a very high error rate." ></td>
	<td class="line x" title="32:199	Manning feeds the output from a stochastic tagger into a finite state parser, and applies statistical filtering to the parsing results." ></td>
	<td class="line x" title="33:199	He predefines 19 different subcategorisation frames, including details of prepositions." ></td>
	<td class="line x" title="34:199	Applying this technique to approx." ></td>
	<td class="line x" title="35:199	4 million words of New York Times newswire, Manning acquires 4900 subcategorisation frames for 3104 verbs, an average of 1.6 per verb." ></td>
	<td class="line x" title="36:199	(Ushioda et al. , 1993) run a finite state NP parser on a POS-tagged corpus to calculate the relative frequency of just six subcategorisation verb classes." ></td>
	<td class="line x" title="37:199	In addition, all prepositional phrases are treated as adjuncts." ></td>
	<td class="line x" title="38:199	For 1565 tokens of 33 selected verbs, they report an accuracy rate of 83%." ></td>
	<td class="line x" title="39:199	(Briscoe and Carroll, 1997) observe that in the work of (Brent, 1993), (Manning, 1993) and (Ushioda et al. , 1993), the maximum number of distinct subcategorization classes recognized is sixteen, and only Ushioda et al. attempt to derive relative subcategorization frequency for individual predicates." ></td>
	<td class="line x" title="40:199	In contrast, the system of (Briscoe and Carroll, 1997) distinguishes 163 verbal subcategorisation classes by means of a statistical shallow parser, a classifier of subcategorisation classes, and a priori estimates of the probability that any verb will be a member of those classes." ></td>
	<td class="line x" title="41:199	More recent work by Korhonen (2002) on the filtering phase of this approach has improved results." ></td>
	<td class="line x" title="42:199	Korhonen experiments with the use of linguistic verb classes for obtaining more accurate back-off estimates for use in hypothesis selection." ></td>
	<td class="line x" title="43:199	Using this extended approach, the average results for 45 semantically classified test verbs evaluated against hand judgements are precision 87.1% and recall 71.2%." ></td>
	<td class="line x" title="44:199	By comparison, the average results for 30 verbs not classified semantically are precision 78.2% and recall 58.7%." ></td>
	<td class="line x" title="45:199	Carroll and Rooth (1998) use a hand-written head-lexicalised context-free grammar and a text corpus to compute the probability of particular subcategorisation scenarios." ></td>
	<td class="line x" title="46:199	The extracted frames do not contain details of prepositions." ></td>
	<td class="line x" title="47:199	More recently, a number of researchers have applied similar techniques to derive resources for other languages, especially German." ></td>
	<td class="line x" title="48:199	One of these, (Schulte im Walde, 2002), induces a computational subcategorisation lexicon for over 14,000 German verbs." ></td>
	<td class="line x" title="49:199	Using sentences of limited length, she extracts 38 distinct frame types, which contain maximally three arguments each." ></td>
	<td class="line x" title="50:199	The frames may optionally contain details of particular prepositional use." ></td>
	<td class="line x" title="51:199	Her evaluation on over 3000 frequently occurring verbs against the German dictionary Duden Das Stilworterbuch is similar in scale to ours and is discussed further in Section 5." ></td>
	<td class="line x" title="52:199	There has also been some work on extracting subcategorisation details from the Penn Treebank." ></td>
	<td class="line x" title="53:199	(Kinyon and Prolo, 2002) introduce a tool which uses fine-grained rules to identify the arguments, including optional arguments, of each verb occurrence in the Penn Treebank, along with their syntactic functions." ></td>
	<td class="line x" title="54:199	They manually examined the 150+ possible sequences of tags, both functional and categorial, in Penn-II and determined whether the sequence in question denoted a modifier, argument or optional argument." ></td>
	<td class="line x" title="55:199	Arguments were then mapped to traditional syntactic functions." ></td>
	<td class="line x" title="56:199	As they do not include an evaluation, currently it is impossible to say how effective this technique is." ></td>
	<td class="line x" title="57:199	(Xia et al. , 2000) and (Chen and Vijay-Shanker, 2000) extract lexicalised TAGs from the Penn Treebank." ></td>
	<td class="line x" title="58:199	Both techniques implement variations on the approaches of (Magerman, 1994) and (Collins, 1997) for the purpose of differentiating between complement and adjunct." ></td>
	<td class="line x" title="59:199	In the case of (Xia et al. , 2000), invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics." ></td>
	<td class="line x" title="60:199	(Hockenmaier et al. , 2002) outline a method for the automatic extraction of a large syntactic CCG lexicon from Penn-II." ></td>
	<td class="line x" title="61:199	For each tree, the algorithm annotates the nodes with CCG categories in a topdown recursive manner." ></td>
	<td class="line x" title="62:199	In order to examine the coverage of the extracted lexicon in a manner similar to (Xia et al. , 2000), (Hockenmaier et al. , 2002) compared the reference lexicon acquired from Sections 02-21 with a test lexicon extracted from Section 23 of the WSJ." ></td>
	<td class="line x" title="63:199	It was found that the reference CCG lexicon contained 95.09% of the entries in the test lexicon, while 94.03% of the entries in the test TAG lexicon also occurred in the reference lexicon." ></td>
	<td class="line x" title="64:199	Both approaches involve extensive correction and clean-up of the treebank prior to lexical extraction." ></td>
	<td class="line x" title="65:199	3 Our Methodology The first step in the application of our methodology is the production of a treebank annotated with LFG f-structure information." ></td>
	<td class="line x" title="66:199	F-structures are feature structures which represent abstract syntactic information, approximating to basic predicate-argumentmodifier structures." ></td>
	<td class="line oc" title="67:199	We utilise the automatic annotation algorithm of (Cahill et al. , 2004b) to derive a version of Penn-II where each node in each tree is annotated with an LFG functional annotation (i.e. an attribute value structure equation)." ></td>
	<td class="line x" title="68:199	Trees are traversed top-down, and annotation is driven by categorial, basic configurational, trace and Penn-II functional tag information in local subtrees of mostly depth one (i.e. CFG rules)." ></td>
	<td class="line x" title="69:199	The annotation procedure is dependent on locating the head daughter, for which the scheme of (Magerman, 1994) with some changes and amendments is used." ></td>
	<td class="line x" title="70:199	The head is annotated with the LFG equation =." ></td>
	<td class="line x" title="71:199	Linguistic generalisations are provided over the left (the prefix) and the right (suffix) context of the head for each syntactic category occurring as the mother node of such heads." ></td>
	<td class="line x" title="72:199	To give a simple example, the rightmost NP to the left of a VP head under an S is likely to be its subject ( SUBJ =), while the leftmost NP to the right of the V head of a VP is most probably its object ( OBJ =)." ></td>
	<td class="line oc" title="73:199	(Cahill et al. , 2004b) provide four sets of annotation principles, one for non-coordinate configurations, one for coordinate configurations, one for traces (long distance dependencies) and a final catch all and clean up phase." ></td>
	<td class="line x" title="74:199	Distinguishing between argument and adjunct is an inherent step in the automatic assignment of functional annotations." ></td>
	<td class="line x" title="75:199	The satisfactory treatment of long distance dependencies by the annotation algorithm is imperative for the extraction of accurate semantic forms." ></td>
	<td class="line x" title="76:199	The Penn Treebank employs a rich arsenal of traces and empty productions (nodes which do not realise any lexical material) to co-index displaced material with the position where it should be interpreted semantically." ></td>
	<td class="line oc" title="77:199	The algorithm of (Cahill et al. , 2004b) translates the traces into corresponding re-entrancies in the f-structure representation (Figure 1)." ></td>
	<td class="line x" title="78:199	Passive movement is also captured and expressed at f-structure level using a passive:+annotation." ></td>
	<td class="line x" title="79:199	Once a treebank tree is annotated with feature structure equations by the annotation algorithm, the equations are collected and passed to a constraint solver which produces the f-structures." ></td>
	<td class="line x" title="80:199	In order to ensure the quality of the semanS S-TPC1 NP U.N. VP V signs NP treaty NP Det the N headline VP V said S T1     TOPIC bracketleftBigg SUBJ bracketleftbig PRED U.N. bracketrightbig PRED sign OBJ bracketleftbig PRED treaty bracketrightbig bracketrightBigg 1 SUBJ bracketleftBig SPEC the PRED headline bracketrightBig PRED say COMP 1     Figure 1: Penn-II style tree with long distance dependency trace and corresponding reentrancy in f-structure tic forms extracted by our method, we must first ensure the quality of the f-structure annotations." ></td>
	<td class="line oc" title="81:199	(Cahill et al. , 2004b) measure annotation quality in terms of precision and recall against manually constructed, gold-standard f-structures for 105 randomly selected trees from section 23 of the WSJ section of Penn-II." ></td>
	<td class="line o" title="82:199	The algorithm currently achieves an F-score of 96.3% for complete f-structures and 93.6% for preds-only f-structures.1 Our semantic form extraction methodology is based on the procedure of (van Genabith et al. , 1999): For each f-structure generated, for each level of embedding we determine the local PRED value and collect the subcategorisable grammatical functions present at that level of embedding." ></td>
	<td class="line x" title="83:199	Consider the f-structure in Figure 1." ></td>
	<td class="line x" title="84:199	From this we recursively extract the following nonempty semantic forms: say([subj,comp]), sign([subj,obj])." ></td>
	<td class="line x" title="85:199	In effect, in both (van Genabith et al. , 1999) and our approach semantic forms are reverse engineered from automatically generated f-structures for treebank trees." ></td>
	<td class="line x" title="86:199	We extract the following subcategorisable syntactic functions: SUBJ, OBJ, OBJ2, OBLprep, OBL2prep, COMP, XCOMP and PART." ></td>
	<td class="line x" title="87:199	Adjuncts (e.g. ADJ, APP etc) are not included in the semantic forms." ></td>
	<td class="line x" title="88:199	PART is not a syntactic function in the strict sense but we capture the relevant co-occurrence patterns of verbs and particles in the semantic forms." ></td>
	<td class="line x" title="89:199	Just as OBL includes the prepositional head of the PP, PART includes the actual particle which occurs e.g. add([subj,obj,part:up])." ></td>
	<td class="line x" title="90:199	In the work presented here we substantially extend the approach of (van Genabith et al. , 1999) as 1Preds-only measures only paths ending in PRED:VALUE so features such as number, person etc are not included." ></td>
	<td class="line x" title="91:199	regards coverage, granularity and evaluation: First, we scale the approach of (van Genabith et al. , 1999) which was proof of concept on 100 trees to the full WSJ section of the Penn-II Treebank." ></td>
	<td class="line x" title="92:199	Second, our approach fully reflects long distance dependencies, indicated in terms of traces in the Penn-II Treebank and corresponding re-entrancies at f-structure." ></td>
	<td class="line x" title="93:199	Third, in addition to abstract syntactic functionbased subcategorisation frames we compute frames for syntactic function-CFG category pairs, both for the verbal heads and their arguments and also generate pure CFG-based subcat frames." ></td>
	<td class="line x" title="94:199	Fourth, our method differentiates between frames captured for active or passive constructions." ></td>
	<td class="line x" title="95:199	Fifth, our method associates conditional probabilities with frames." ></td>
	<td class="line x" title="96:199	In contrast to much of the work reviewed in the previous section, our system is able to produce surface syntactic as well as abstract functional subcategorisation details." ></td>
	<td class="line x" title="97:199	To incorporate CFG details into the extracted semantic forms, we add an extra feature to the generated f-structures, the value of which is the syntactic category of the pred at each level of embedding." ></td>
	<td class="line x" title="98:199	Exploiting this information, the extracted semantic form for the verb sign looks as follows: sign(v,[subj(np),obj(np)])." ></td>
	<td class="line x" title="99:199	We have also extended the algorithm to deal with passive voice and its effect on subcategorisation behaviour." ></td>
	<td class="line x" title="100:199	Consider Figure 2: not taking voice into account, the algorithm extracts an intransitive frame outlaw([subj])for the transitive outlaw." ></td>
	<td class="line x" title="101:199	To correct this, the extraction algorithm uses the feature value pair passive:+, which appears in the f-structure at the level of embedding of the verb in question, to mark that predicate as occurring in the passive: outlaw([subj],p)." ></td>
	<td class="line x" title="102:199	In order to estimate the likelihood of the cooccurrence of a predicate with a particular argument list, we compute conditional probabilities for subcategorisation frames based on the number of token occurrences in the corpus." ></td>
	<td class="line x" title="103:199	Given a lemma l and an argument list s, the probability of s given l is estimated as: P(s|l) := count(l,s)summationtextn i=1 count(l,si) We use thresholding to filter possible error judgements by our system." ></td>
	<td class="line x" title="104:199	Table 1 shows the attested semantic forms for the verb accept with their associated conditional probabilities." ></td>
	<td class="line x" title="105:199	Note that were the distinction between active and passive not taken into account, the intransitive occurrence ofaccept would have been assigned an unmerited probability." ></td>
	<td class="line x" title="106:199	subj : spec : quant : pred : all adjunct : 2 : pred : almost adjunct : 3 : pred : remain participle : pres 4 : obj : adjunct : 5 : pred : cancer-causing pers : 3 pred : asbestos num : sg pform : of pers : 3 pred : use num : pl passive : + adjunct : 1 : obj : pred : 1997 pform : by xcomp : subj : spec: quant : pred : all adjunct : 2 : pred : almost  passive : + xcomp : subj : spec: quant : pred : all adjunct : 2 : pred : almost   passive : + pred : outlaw tense : past pred : be pred : will modal : + Figure 2: Automatically generated f-structure for the string wsj 0003 23By 1997, almost all remaining uses of cancer-causing asbestos will be outlawed. Semantic Form Frequency Probability accept([subj,obj]) 122 0.813 accept([subj],p) 9 0.060 accept([subj,comp]) 5 0.033 accept([subj,obl:as],p) 3 0.020 accept([subj,obj,obl:as]) 3 0.020 accept([subj,obj,obl:from]) 3 0.020 accept([subj]) 2 0.013 accept([subj,obj,obl:at]) 1 0.007 accept([subj,obj,obl:for]) 1 0.007 accept([subj,obj,xcomp]) 1 0.007 Table 1: Semantic Forms for the verb accept marked with p for passive use." ></td>
	<td class="line x" title="107:199	4 Results We extract non-empty semantic forms2 for 3586 verb lemmas and 10969 unique verbal semantic form types (lemma followed by non-empty argument list)." ></td>
	<td class="line x" title="108:199	Including prepositions associated with the OBLs and particles, this number rises to 14348, an average of 4.0 per lemma (Table 2)." ></td>
	<td class="line x" title="109:199	The number of unique frame types (without lemma) is 38 without specific prepositions and particles, 577 with (Table 3)." ></td>
	<td class="line x" title="110:199	F-structure annotations allow us to distinguish passive and active frames." ></td>
	<td class="line x" title="111:199	5 COMLEX Evaluation We evaluated our induced (verbal) semantic forms against COMLEX (MacLeod et al. , 1994)." ></td>
	<td class="line x" title="112:199	COM2Frames with at least one subcategorised grammatical function." ></td>
	<td class="line x" title="113:199	Without Prep/Part With Prep/Part Sem." ></td>
	<td class="line x" title="114:199	Form Types 10969 14348 Active 8516 11367 Passive 2453 2981 Table 2: Number of Semantic Form Types Without Prep/Part With Prep/Part # Frame Types 38 577 # Singletons 1 243 # Twice Occurring 1 84 # Occurring max." ></td>
	<td class="line x" title="115:199	5 7 415 # Occurring > 5 31 162 Table 3: Number of Distinct Frames for Verbs (not including syntactic category for grammatical function) LEX defines 138 distinct verb frame types without the inclusion of specific prepositions or particles." ></td>
	<td class="line x" title="116:199	The following is a sample entry for the verb reimburse: (VERB :ORTH reimburse :SUBC ((NP-NP) (NP-PP :PVAL (for)) (NP))) Each verb has a :SUBC feature, specifying its subcategorisation behaviour." ></td>
	<td class="line x" title="117:199	For example, reimburse can occur with two noun phrases (NP-NP), a noun phrase and a prepositional phrase headed by for (NP-PP :PVAL (for)) or a single noun phrase (NP)." ></td>
	<td class="line x" title="118:199	Note that the details of the subject noun phrase are not included in COMLEX frames." ></td>
	<td class="line x" title="119:199	Each of the complement types which make up the value of the :SUBC feature is associated with a formal frame definition which looks as follows: (vp-frame np-np :cs ((np 2)(np 3)) :gs (:subject 1 :obj 2 :obj2 3) :ex she asked him his name) The value of the :cs feature is the constituent structure of the subcategorisation frame, which lists the syntactic CF-PSG constituents in sequence." ></td>
	<td class="line x" title="120:199	The value of the :gs feature is the grammatical structure which indicates the functional role played by each of the CF-PSG constituents." ></td>
	<td class="line x" title="121:199	The elements of the constituent structure are indexed, and referenced in the :gs field." ></td>
	<td class="line x" title="122:199	This mapping between constituent structure and functional structure makes the information contained in COMLEX suitable as an evaluation standard for the LFG semantic forms which we induce." ></td>
	<td class="line x" title="123:199	5.1 COMLEX-LFG Mapping We devised a common format for our induced semantic forms and those contained in COMLEX." ></td>
	<td class="line x" title="124:199	This is summarised in Table 4." ></td>
	<td class="line x" title="125:199	COMLEX does not distinguish between obliques and objects so we converted Obji to OBLi as required." ></td>
	<td class="line x" title="126:199	In addition, COMLEX does not explicitly differentiate between COMPs and XCOMPs, but does encode control information for any Comps which occur, thus allowing us to deduce the distinction automatically." ></td>
	<td class="line x" title="127:199	The manually constructed COMLEX entries provided us with a gold standard against which we evaluated the automatically induced frames for the 2992 (active) verbs that both resources have in common." ></td>
	<td class="line x" title="128:199	LFG COMLEX Merged SUBJ Subject SUBJ OBJ Object OBJ OBJ2 Obj2 OBJ2 OBL Obj3 OBL OBL2 Obj4 OBL2 COMP Comp COMP XCOMP Comp XCOMP PART Part PART Table 4: COMLEX and LFG Syntactic Functions We use the computed conditional probabilities to set a threshold to filter the selection of semantic forms." ></td>
	<td class="line x" title="129:199	As some verbs occur less frequently than others we felt it was important to use a relative rather than absolute threshold." ></td>
	<td class="line x" title="130:199	For a threshold of 1%, we disregard any frames with a conditional probability of less than or equal to 0.01." ></td>
	<td class="line x" title="131:199	We carried out the evaluation in a similar way to (Schulte im Walde, 2002)." ></td>
	<td class="line x" title="132:199	The scale of our evaluation is comparable to hers." ></td>
	<td class="line x" title="133:199	This allows us to make tentative comparisons between our respective results." ></td>
	<td class="line x" title="134:199	The figures shown in Table 5 are the results of three different kinds of evaluation with the threshold set to 1% and 5%." ></td>
	<td class="line x" title="135:199	The effect of the threshold increase is obvious in that Precision goes up for each of the experiments while Recall goes down." ></td>
	<td class="line x" title="136:199	For Exp 1, we excluded prepositional phrases entirely from the comparison, i.e. assumed that PPs were adjunct material (e.g. [subj,obl:for] becomes [subj])." ></td>
	<td class="line x" title="137:199	Our results are better for Precision than for Recall compared to Schulte im Walde (op cit.), who reports Precision of 74.53%, Recall of 69.74% and an F-score of 72.05%." ></td>
	<td class="line x" title="138:199	Exp 2 includes prepositional phrases but not parameterised for particular prepositions (e.g. [subj,obl:for] becomes [subj,obl])." ></td>
	<td class="line x" title="139:199	While our figures for Recall are again lower, our results for Precision are considerably higher than those of Schulte im Walde (op cit)." ></td>
	<td class="line x" title="140:199	who recorded Precision of 60.76%, Recall of 63.91% and an F-score of 62.30%." ></td>
	<td class="line x" title="141:199	For Exp. 3, we used semantic forms which contained details of specific prepositions for any subcategorised prepositional phrase." ></td>
	<td class="line x" title="142:199	Our Precision figures are again high (in comparison to 65.52% as recorded by (Schulte im Walde, 2002))." ></td>
	<td class="line x" title="143:199	However, Threshold 1% Threshold 5% P R F-Score P R F-Score Exp. 1 79.0% 59.6% 68.0% 83.5% 54.7% 66.1% Exp. 2 77.1% 50.4% 61.0% 81.4% 44.8% 57.8% Exp. 2a 76.4% 44.5% 56.3% 80.9% 39.0% 52.6% Exp. 3 73.7% 22.1% 34.0% 78.0% 18.3% 29.6% Exp. 3a 73.3% 19.9% 31.3% 77.6% 16.2% 26.8% Table 5: COMLEX Comparison our Recall is very low (compared to the 50.83% that Schulte im Walde (op cit)." ></td>
	<td class="line x" title="144:199	reports)." ></td>
	<td class="line x" title="145:199	Consequently our F-score is also low (Schulte im Walde (op cit.)" ></td>
	<td class="line x" title="146:199	records an F-score of 57.24%)." ></td>
	<td class="line x" title="147:199	Experiments 2a and 3a are similar to Experiments 2 and 3 respectively except they include the specific particle associated with each PART." ></td>
	<td class="line x" title="148:199	5.1.1 Directional Prepositions There are a number of possible reasons for our low recall scores for Experiment 3 in Table 5." ></td>
	<td class="line x" title="149:199	It is a well-documented fact (Briscoe and Carroll, 1997) that subcategorisation frames (and their frequencies) vary across domains." ></td>
	<td class="line x" title="150:199	We have extracted frames from one domain (the WSJ) whereas COMLEX was built using examples from the San Jose Mercury News, the Brown Corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ." ></td>
	<td class="line x" title="151:199	For this reason, it is likely to contain a greater variety of subcategorisation frames than our induced lexicon." ></td>
	<td class="line x" title="152:199	It is also possible that due to human error COMLEX contains subcategorisation frames, the validity of which may be in doubt." ></td>
	<td class="line x" title="153:199	This is due to the fact that the aim of the COMLEX project was to construct as complete a set of subcategorisation frames as possible, even for infrequent verbs." ></td>
	<td class="line x" title="154:199	Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples." ></td>
	<td class="line x" title="155:199	Our recall figure was particularly low in the case of evaluation using details of prepositions (Experiment 3)." ></td>
	<td class="line x" title="156:199	This can be accounted for by the fact that COMLEX errs on the side of overgeneration when it comes to preposition assignment." ></td>
	<td class="line x" title="157:199	This is particularly true of directional prepositions, a list of 31 of which has been prepared and is assigned in its entirety by default to any verb which can potentially appear with any directional preposition." ></td>
	<td class="line x" title="158:199	In a subsequent experiment, we incorporate this list of directional prepositions by default into our semantic form induction process in the same way as the creators of COMLEX have done." ></td>
	<td class="line x" title="159:199	Table 6 shows the results of this experiment." ></td>
	<td class="line x" title="160:199	As expected there is a significant imPrecision Recall F-Score Experiment 3 81.7% 40.8% 54.4% Experiment 3a 83.1% 35.4% 49.7% Table 6: COMLEX Comparison using p-dir(Threshold of 1%) Passive Precision Recall F-Score Experiment 2 80.2% 54.7% 65.1% Experiment 2a 79.7% 46.2% 58.5% Experiment 3 72.6% 33.4% 45.8% Experiment 3a 72.3% 29.3% 41.7% Table 7: Passive evaluation (Threshold of 1%) provement in the recall figure, being almost double the figures reported in Table 5 for Experiments 3 and 3a." ></td>
	<td class="line x" title="161:199	5.1.2 Passive Evaluation Table 7 presents the results of our evaluation of the passive semantic forms we extract." ></td>
	<td class="line x" title="162:199	It was carried out for 1422 verbs which occur with passive frames and are shared by the induced lexicon and COMLEX." ></td>
	<td class="line x" title="163:199	As COMLEX does not provide explicit passive entries, we applied Lexical Redundancy Rules (Kaplan and Bresnan, 1982) to automatically convert the active COMLEX frames to their passive counterparts." ></td>
	<td class="line x" title="164:199	For example, the COMLEX entry see([subj,obj]) is converted to see([subj])." ></td>
	<td class="line x" title="165:199	The resulting precision is very high, a slight increase on that for the active frames." ></td>
	<td class="line x" title="166:199	The recall score drops for passive frames (from 54.7% to 29.3%) in a similar way to that for active frames when prepositional details are included." ></td>
	<td class="line x" title="167:199	5.2 Lexical Accession Rates As well as evaluating the quality of our extracted semantic forms, we also examine the rate at which they are induced." ></td>
	<td class="line x" title="168:199	(Charniak, 1996) and (Krotov et al. , 1998) observed that treebank grammars (CFGs extracted from treebanks) are very large and grow with the size of the treebank." ></td>
	<td class="line x" title="169:199	We were interested in discovering whether the acquisition of lexical material on the same data displays a similar propensity." ></td>
	<td class="line x" title="170:199	Figure 3 displays the accession rates for the semantic forms induced by our method for sections 024 of the WSJ section of the Penn-II treebank." ></td>
	<td class="line x" title="171:199	When we do not distinguish semantic forms by category, all semantic forms together with those for verbs display smaller accession rates than for the PCFG." ></td>
	<td class="line x" title="172:199	We also examined the coverage of our system in a similar way to (Hockenmaier et al. , 2002)." ></td>
	<td class="line x" title="173:199	We extracted a verb-only reference lexicon from Sections 02-21 of the WSJ and subsequently compared this to a test lexicon constructed in the same way from 0 5000 10000 15000 20000 25000 0 5 10 15 20 25 No." ></td>
	<td class="line x" title="174:199	of SFs/Rules WSJ Section All SF Frames All Verbs All SF Frames, no category All Verbs, no category PCFG Figure 3: Accession Rates for Semantic Forms and CFG Rules Entries also in reference lexicon: 89.89% Entries not in reference lexicon: 10.11% Known words: 7.85% Known words, known frames: 7.85% Known words, unknown frames: Unknown words: 2.32% Unknown words, known frames: 2.32% Unknown words, unknown frames: Table 8: Coverage of induced lexicon on unseen data (Verbs Only) Section 23." ></td>
	<td class="line x" title="175:199	Table 8 shows the results of this experiment." ></td>
	<td class="line x" title="176:199	89.89% of the entries in the test lexicon appeared in the reference lexicon." ></td>
	<td class="line x" title="177:199	6 Conclusions We have presented an algorithm and its implementation for the extraction of semantic forms or subcategorisation frames from the Penn-II Treebank, automatically annotated with LFG f-structures." ></td>
	<td class="line x" title="178:199	We have substantially extended an earlier approach by (van Genabith et al. , 1999)." ></td>
	<td class="line x" title="179:199	The original approach was small-scale and proof of concept." ></td>
	<td class="line x" title="180:199	We have scaled our approach to the entire WSJ Sections of PennII (50,000 trees)." ></td>
	<td class="line x" title="181:199	Our approach does not predefine the subcategorisation frames we extract as many other approaches do." ></td>
	<td class="line x" title="182:199	We extract abstract syntactic function-based subcategorisation frames (LFG semantic forms), traditional CFG category-based frames as well as mixed function-category based frames." ></td>
	<td class="line x" title="183:199	Unlike many other approaches to subcategorisation frame extraction, our system properly reflects the effects of long distance dependencies and distinguishes between active and passive frames." ></td>
	<td class="line x" title="184:199	Finally our system associates conditional probabilities with the frames we extract." ></td>
	<td class="line x" title="185:199	We carried out an extensive evaluation of the complete induced lexicon (not just a sample) against the full COMLEX resource." ></td>
	<td class="line x" title="186:199	To our knowledge, this is the most extensive qualitative evaluation of subcategorisation extraction in English." ></td>
	<td class="line x" title="187:199	The only evaluation of a similar scale is that carried out by (Schulte im Walde, 2002) for German." ></td>
	<td class="line x" title="188:199	Our results compare well with hers." ></td>
	<td class="line x" title="189:199	We believe our semantic forms are fine-grained and by choosing to evaluate against COMLEX we set our sights high: COMLEX is considerably more detailed than the OALD or LDOCE used for other evaluations." ></td>
	<td class="line x" title="190:199	Currently work is under way to extend the coverage of our acquired lexicons by applying our methodology to the Penn-III treebank, a more balanced corpus resource with a number of text genres (in addition to the WSJ sections)." ></td>
	<td class="line x" title="191:199	It is important to realise that the induction of lexical resources is part of a larger project on the acquisition of wide-coverage, robust, probabilistic, deep unification grammar resources from treebanks." ></td>
	<td class="line pc" title="192:199	We are already using the extracted semantic forms in parsing new text with robust, wide-coverage PCFG-based LFG grammar approximations automatically acquired from the f-structure annotated Penn-II treebank (Cahill et al. , 2004a)." ></td>
	<td class="line x" title="193:199	We hope to be able to apply our lexical acquisition methodology beyond existing parse-annotated corpora (Penn-II and PennIII): new text is parsed by our PCFG-based LFG approximations into f-structures from which we can then extract further semantic forms." ></td>
	<td class="line x" title="194:199	The work reported here is part of the core component for bootstrapping this approach." ></td>
	<td class="line x" title="195:199	As the extraction algorithm we presented derives semantic forms at f-structure level, it is easily applied to other, even typologically different, languages." ></td>
	<td class="line x" title="196:199	We have successfully ported our automatic annotation algorithm to the TIGER Treebank, despite German being a less configurational language than English, and extracted wide-coverage, probabilistic LFG grammar approximations and lexical resources for German (Cahill et al. , 2003)." ></td>
	<td class="line x" title="197:199	Currently, we are migrating the technique to Spanish, which has freer word order than English and less morphological marking than German." ></td>
	<td class="line x" title="198:199	Preliminary results have been very encouraging." ></td>
	<td class="line x" title="199:199	7 Acknowledgements The research reported here is supported by Enterprise Ireland Basic Research Grant SC/2001/186 and an IRCSET PhD fellowship award." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W04-2003
A Robust And Hybrid Deep-Linguistic Theory Applied To Large-Scale Parsing
Schneider, Gerold;Dowdall, James;Rinaldi, Fabio;"></td>
	<td class="line x" title="1:197	A Robust and Hybrid Deep-Linguistic Theory Applied to Large-Scale Parsing Gerold Schneider, James Dowdall, Fabio Rinaldi Institute of Computational Linguistics, University of Zurich {gschneid,rinaldi}@ifi.unizh.ch, j.m.dowdall@sussex.ac.uk Abstract Modern statistical parsers are robust and quite fast, but their output is relatively shallow when compared to formal grammar parsers." ></td>
	<td class="line x" title="2:197	We suggest to extend statistical approaches to a more deep-linguistic analysis while at the same time keeping the speed and low complexity of a statistical parser." ></td>
	<td class="line x" title="3:197	The resulting parsing architecture suggested, implemented and evaluated here ishighlyrobustandhybridonanumberof levels, combining statistical and rule-based approaches, constituency and dependency grammar, shallow and deep processing, full and nearfull parsing." ></td>
	<td class="line x" title="4:197	With its parsing speed of about 300,000 words per hour and state-of-the-art performance the parser is reliable for a number of large-scale applications discussed in the article." ></td>
	<td class="line x" title="5:197	1 Introduction Robustness in Computational Linguistics has been recently recognized as a central issue for the design of reliable, large-scale Natural Language Processing (NLP) systems." ></td>
	<td class="line x" title="6:197	While the highest possible linguistic coverage is desirable, speed and robustness are equally important in practical applications." ></td>
	<td class="line x" title="7:197	Formal Grammar Parser have carefully crafted grammars written by professional linguists." ></td>
	<td class="line x" title="8:197	In addition to expressing local relations, i.e. relations between a mother and a direct daughter node, a number of non-local relations, i.e. relations involving more than two generations, are also modeled." ></td>
	<td class="line x" title="9:197	An example of a nonlocal relation is the subject control relation in the sentence John wants to leave,whereJohn is not only the explicit subject of want,butequally the implicit subject of leave." ></td>
	<td class="line x" title="10:197	A parser that fails to recognize control subjects misses important information, quantitatively about 3 % of all subjects." ></td>
	<td class="line x" title="11:197	But unrestricted real-world texts still pose a problem to NLP systems that are based on Formal Grammars." ></td>
	<td class="line x" title="12:197	Few hand-crafted, deep linguistic grammars achieve the coverage and robustness needed to parse large corpora (see (Riezler et al. , 2002) for an exception, and (Burke et al. , 2004; Hockenmaier and Steedman, 2002) for approaches extracting formal grammars from the Treebank), and speed remains a serious challenge." ></td>
	<td class="line x" title="13:197	The typical problems can be grouped as follows." ></td>
	<td class="line x" title="14:197	Grammar complexity Fully comprehensive grammars are dicult to maintain and considerably increase parsing complexity." ></td>
	<td class="line x" title="15:197	Note that statistical parsers can equally suer from this problem, see e.g.(Kaplan et al. , 2004)." ></td>
	<td class="line x" title="17:197	Parsing complexity Typicalformalgrammar parser complexity is much higher than the O(n 3 ) for CFG (Eisner, 1997)." ></td>
	<td class="line x" title="18:197	The complexity of some formal grammars is still unknown." ></td>
	<td class="line x" title="19:197	For Tree-Adjoining Grammars (TAG) it is O(n 7 )orO(n 8 ) depending on the implementation (Eisner, 2000)." ></td>
	<td class="line x" title="20:197	(Sarkar et al. , 2000) state that the theoretical bound of worst time complexity for Head-Driven Phrase Structure Grammar (HPSG) parsing is exponential." ></td>
	<td class="line x" title="21:197	Parsing algorithms able to treat completely unrestricted long-distance dependencies are NPcomplete (Neuhaus and Broker, 1997)." ></td>
	<td class="line x" title="22:197	Ranking Returning all syntactically possible analyses for a sentence is not really what is expected of a syntactic analyzer if it should be of practical use, since for a human there is usually only one correct interpretation." ></td>
	<td class="line x" title="23:197	A clear indication of preference, by means of ranking the analyses in a preference order is needed." ></td>
	<td class="line x" title="24:197	Pruning In order to keep search spaces manageable it is in fact necessary to discard unconvincing alternatives already during the parsing process." ></td>
	<td class="line x" title="25:197	In a statistical parser, the ranking of intermediate structures occurs naturally, while a rule-based system has to rely on ad hoc heuristics." ></td>
	<td class="line x" title="26:197	With a beam search in a parse-time pruning system, which means that the total number of alternatives kept is constant from a certain search complexity onwards, real-world parsing time can be reduced to near-linear." ></td>
	<td class="line x" title="27:197	If one were to assume a constantly full beam, or uses an oracle (Nivre, 2004) it is linear in practice." ></td>
	<td class="line x" title="28:197	A number of robust statistical parsers that oer solutions to these problems have now become available (Charniak, 2000; Collins, 1999; Henderson, 2003), but they typically produce CFG constituency data as output, trees that do not express long-distance dependencies." ></td>
	<td class="line x" title="29:197	Although grammatical function and empty nodes annotation expressing long-distance dependencies are provided in Treebanks such as the Penn Treebank (Marcus et al. , 1993), most statistical Treebank trained parsers fully or largely ignore them 1, which entails two problems: first, the training cannot profit from valuable annotation data." ></td>
	<td class="line x" title="30:197	Second, the extraction of long-distance dependencies (LDD) and the mapping to shallow semantic representations is not always possible from the output of these parsers." ></td>
	<td class="line x" title="31:197	This limitation is aggravated by a lack of co-indexation information and parsing errors across an LDD." ></td>
	<td class="line x" title="32:197	In fact, some syntactic relations cannot be recovered on configurational grounds only." ></td>
	<td class="line x" title="33:197	For these reasons, (Johnson, 2002) provocatively refers to them as halfgrammars." ></td>
	<td class="line x" title="34:197	The paper is organized as follows." ></td>
	<td class="line x" title="35:197	We first explore a deep-linguistic grammar theory for English that is inherently designed to be robust by extending the low processing complexity and the robustness of statistical approaches to a more deep-linguistic level, by making careful use of underspecification, grammar compression techniques and using a grammar that directly delivers simple predicate-argument structures." ></td>
	<td class="line x" title="36:197	This allow us to use a context-free grammar at parse-time while successfully treating longdistance dependencies using low-complexity approaches before and after parsing." ></td>
	<td class="line oc" title="37:197	Our approach is to use finite-state approximations of long-distance dependencies, as they are described in (Schneider, 2003a) for Dependency Grammar (DG) and (Cahill et al. , 2004) for Lexical Functional Grammar (LFG)." ></td>
	<td class="line x" title="38:197	(Dienes and Dubey, 2003) show that finite-state preprocessing modules can successfully deal with LDDs." ></td>
	<td class="line x" title="39:197	Our approach is similar in also amounting to a preprocessing recognition of LDDs." ></td>
	<td class="line x" title="40:197	Then we show that the implementation (Pro3Gres) profits from hybridness and is fast 1 (Collins, 1999) Model 2 uses some of the functional labels, and Model 3 some long-distance dependencies and robust enough to do large-scale parsing of totally unrestricted texts and give an overview of its applications." ></td>
	<td class="line x" title="41:197	To conclude, two evaluations are given." ></td>
	<td class="line x" title="42:197	2 A Robust Deep-Linguistic Theory Generally, a linguistic analysis model aims at complete and correct analysis, which means that the mapping between the text data and its syntactic and semantic analysis is sound (the model extracts correct readings) and complete (the model deals with all language phenomena)." ></td>
	<td class="line x" title="43:197	In practice, however, both goals cannot be totally reached." ></td>
	<td class="line x" title="44:197	The main obstacle for soundness is the all-pervasive characteristic of natural language to be ambiguous, where ambiguities can oftenonlyberesolvedwithworldknowledge." ></td>
	<td class="line x" title="45:197	Statistical disambiguation such as (Collins and Brooks, 1995) for PP-attachment or (Collins, 1997; Charniak, 2000) for generative parsing greatly improve disambiguation, but as they model by imitation instead of by understanding, complete soundness has to remain elusive." ></td>
	<td class="line x" title="46:197	As for completeness, already early nave statistical approaches have shown that the problem of grammar size is not solved but even aggravated by a naive probabilistic parser implementation, in which e.g. all CFG rules permitted in the Penn Treebank are extracted." ></td>
	<td class="line x" title="47:197	From his 300,000 words training part of the Penn Treebank (Charniak, 1996) obtains more than 10,000 CFG rules, of which only about 3,000 occur more than once." ></td>
	<td class="line x" title="48:197	It is therefore necessary to either discard infrequent rules, do manual editing, use a dierent rule format such as individual dependencies (Collins, 1996) or gain full linguistic control and insight by using a handwritten grammar  each of which sacrifices total completeness." ></td>
	<td class="line x" title="49:197	2.1 Near-full Parsing Theapproachwehavechosenistousea manually-developed wide-coverage tag sequence grammar (Abney, 1995; Briscoe and Carroll, 2002), and to exclude or restrict rare, marked and error-prone phenomena." ></td>
	<td class="line x" title="50:197	For example, while it is generally possible for nouns to be modified by more than one PP, only nouns seen in the Treebank with several PPs are allowed to have several PPs." ></td>
	<td class="line x" title="51:197	Or, while it is generally possible for a subject to occur to the immediate right of a verb (said she), this is only allowed for verbs seen with a subject to the right in the training corpus, typically verbs of utterance, and only in a comma-delimited or sentence-final context." ></td>
	<td class="line x" title="52:197	This entails that the parser profits from a lean grammar but finds a complete structure spanning the entire sentence in the majority of real-world sentences and needs to resorts to collecting partial parses in the remaining minority." ></td>
	<td class="line x" title="53:197	Starting from the most probable longest span, recursively the most probable longest span to left and right is searched." ></td>
	<td class="line x" title="54:197	Near-full parsing only leads to a very small loss." ></td>
	<td class="line x" title="55:197	If an analysis consists of two partial parses, on the dependency relation level only the one, usually high-level relation between the heads of the two partial parses remains unexpressed." ></td>
	<td class="line x" title="56:197	The risk of returning garden path, locally correct but globally wrong, analyses diminishes with increasing span length." ></td>
	<td class="line x" title="57:197	2.2 Functional Dependency Grammar We follow the broad architecture suggested by (Abney, 1995) which naturally integrates chunking and dependency parsing and has proven to be practical, fast and robust (Collins, 1996; Basili and Zanzotto, 2002)." ></td>
	<td class="line x" title="58:197	Tagging and chunking are very robust, finite-state approaches, parsing then only occurs between heads of chunks." ></td>
	<td class="line x" title="59:197	2 The perspicuous rules of a handwritten dependency grammar build up the possible syntactic structures, which are ranked and pruned by calculating lexical attachment probabilities for the majortiy of the dependency relations used in the grammar." ></td>
	<td class="line x" title="60:197	The grammar contains around 1000 rules containing the dependents and the heads tag, the direction of the dependency, lexical information for closed class words, and context restrictions 3." ></td>
	<td class="line x" title="61:197	Context restrictions express e.g. that only a verb which has an object in its context is allowed to attach a secondary object." ></td>
	<td class="line x" title="62:197	Our approach can be seen as an extension of (Collins and Brooks, 1995) from PP-attachment to most dependency relations." ></td>
	<td class="line x" title="63:197	Training data is a partial mapping of the Penn Treebank to deep-linguistic dependency structures, similar to (Basili et al. , 1998)." ></td>
	<td class="line x" title="64:197	Robustness also depends on the grammar formalism." ></td>
	<td class="line x" title="65:197	While many formalisms fail to 2 Practical experiments using a toy NP and verbgroup grammar have shown that parsing between heads of chunks only is about four times faster than parsing between every word, i.e. without chunking." ></td>
	<td class="line x" title="66:197	3 the number of rules is high because of tag combinatorics leading to many almost identical rules." ></td>
	<td class="line x" title="67:197	A subject relations is e.g. possible between the 6 verb tags and the 4 noun tags project when subcategorized arguments cannot be found, in a grammar like DG, in which maximal projections and terminal nodes are isomorphic, projection can never fail." ></td>
	<td class="line x" title="68:197	In classical DG, only content words can be heads, and there is no distinction between syntactic and semantic dependency  semantic dependency is used as far as possible." ></td>
	<td class="line x" title="69:197	These assumptions entail that there are no functional and no empty nodes, which means that low complexity O(n 3 ) algorithms such as CYK, which is used here, can be employed." ></td>
	<td class="line x" title="70:197	The classical dependency grammar distinction between ordre lineaire and ordre structural, basically an immediate dominance / linear precedence distinction (ID/LP) also has the advantage that a number of phenomena classically assumed to involve long-distance dependencies, fronted or inversed constituents, can be treated locally." ></td>
	<td class="line x" title="71:197	They only need rules that allow an inversion of the canonical dependency direction under well-defined conditions." ></td>
	<td class="line x" title="72:197	As for fronted elements, since DG does not distinguish between external and internal arguments, front positions are always locally available to the verb." ></td>
	<td class="line x" title="73:197	2.3 Underspecification and Disambiguation The cheapest approach to dealing with the allpervasive NL ambiguity is to underspecifiy everything, which leads to a sound and complete mapping, but one that is content-free and absurd." ></td>
	<td class="line x" title="74:197	But in few, carefully selected areas where distinctions do not matter for the task at hand, where the disambiguation task is particularly unreliable, or where inter-annotator agreement is very low, underspecification can serve as a tool to greatly facilitate linguistic analysis." ></td>
	<td class="line x" title="75:197	For example, intra-base NP ambiguities, such as quantifier scope ambiguities do not matter for a parser like ours aiming at predicate-argument structure, and are thus not attempted to analyze." ></td>
	<td class="line x" title="76:197	There is one part-of-speech distinction where inter-annotator agreement is quite low and the performance of taggers generally very poor: the distinction between verbal particles and prepositions." ></td>
	<td class="line x" title="77:197	We currently leave the distinction underspecified, but a statistical disambiguator is being developed." ></td>
	<td class="line x" title="78:197	Conversely, the Penn Treebank annotation is sometimes not specific enough." ></td>
	<td class="line x" title="79:197	The parser distinguishes between the reading of the tag IN as a complementizer or as a preposition, and disambiguates commas as far as it can, between apposition, subordination and conjunction." ></td>
	<td class="line x" title="80:197	Some typical tagging errors can be robustly corrected by the hand-written grammar." ></td>
	<td class="line x" title="81:197	For example, the distinction between verb past tense VBD and participle VBN is unreliable, but can usually be disambiguated in the parsing process by leaving this tag distinction underspecified for a number of constructions." ></td>
	<td class="line x" title="82:197	2.4 Long-distance Dependencies Long-distance dependencies exponentially increase parsing complexity (Neuhaus and Broker, 1997)." ></td>
	<td class="line x" title="83:197	We therefore use an approach that preprocesses, post-processes and partly underspecifies them, allowing us to use a context-free grammar at parse time." ></td>
	<td class="line x" title="84:197	In detail, (1) before the parsing we model dedicated patterns across several levels of constituency subtrees partly leading to dedicated, compressed and fully local dependency relations, (2) we use statistical lexicalized postprocessing, and (3) we rely on traditional Dependency Grammar assumptions (section 2.2)." ></td>
	<td class="line x" title="85:197	2.4.1 Pre-processing (Johnson, 2002) presents a pattern-matching algorithm for post-processing the output of statistical parsers to add empty nodes to their parse trees." ></td>
	<td class="line x" title="86:197	While encouraging results are reported for perfect parses, performance drops considerably when using trees produced by a statistical parser." ></td>
	<td class="line x" title="87:197	If the parser makes a single parsing error anywhere in the tree fragment matched by the pattern, the pattern will no longer match." ></td>
	<td class="line x" title="88:197	This is not unlikely since the statistical model used by the parser does not model these larger tree fragments." ></td>
	<td class="line x" title="89:197	It suggests that one might improve performance by integrating parsing, empty node recovery and antecedent finding in a single system   (Johnson, 2002)." ></td>
	<td class="line x" title="90:197	We have applied structural patterns to the Penn Treebank, where like in perfect parses precision and recall are high, and where in addition functional labels and empty nodes are available, so that patterns similar to Johnsons but  like (Jijkoun, 2003)  relying on functional labels and empty nodes reach precision close to 100%." ></td>
	<td class="line x" title="91:197	Unlike in Johnson, also patterns for local dependencies are used; non-local patterns simply stretch across more subtree-levels." ></td>
	<td class="line x" title="92:197	We use the extracted lexical counts as lexical frequency training material." ></td>
	<td class="line x" title="93:197	Every dependency relation has a group of structural extraction patterns associated with it." ></td>
	<td class="line x" title="94:197	This amounts to a partial mapping of the Penn Treebank to Functional Relation Label Example verbsubject subj he sleeps verbfirst object obj sees it verbsecond object obj2 gave (her) kisses verbadjunct adj ate yesterday verbsubord." ></td>
	<td class="line x" title="95:197	clause sentobj saw (they) came verbprep." ></td>
	<td class="line x" title="96:197	phrase pobj slept in bed nounprep." ></td>
	<td class="line x" title="97:197	phrase modpp draft of paper nounparticiple modpart report written verbcomplementizer compl to eat apples nounpreposition prep to the house Table 1: The most important dependency types used by the parser ? a104 a104 a104 a104 a104 a40 a40 a40 a40 a40 NP-SBJ-X@ noun VP@ a104 a104 a104 a40 a40 a40 V passive verb NP -NONE*-X ? a104 a104 a104 a104a104 a40 a40 a40 a40a40 NP-SBJ-X@ noun VP@ a104 a104 a104 a40 a40 a40 V control-verb S NP-SBJ -NONE*-X Figure 1: The extaction patterns for passive subjects (top) and subject control (bottom) DG (Hajic, 1998), (Tapanainen and Jarvinen, 1997)." ></td>
	<td class="line x" title="98:197	Table 1 gives an overview of the most important dependencies." ></td>
	<td class="line x" title="99:197	The subj relation, for example, has the head of an arbitrarily nested NP with the functional tag SBJ as dependent, and the head of an arbitrarily nested VP as head for all active verbs." ></td>
	<td class="line x" title="100:197	In passive verbs, however, a movement involving an empty constituent is assumed, which corresponds to the extraction pattern in figure 1, where VP@ is an arbitrarily nested VP, and NPSBJ-X@ the arbitrarily nested surface subject and X the co-indexed, moved element." ></td>
	<td class="line x" title="101:197	Movements are generally supposed to be of arbitrary length, but a closer investigation reveals that this type of movement is fixed." ></td>
	<td class="line x" title="102:197	Thesameargumentcanbemadeforother relations, for example control structures, which have the extraction pattern shown in figure 1." ></td>
	<td class="line x" title="103:197	Grammatical role labels, empty node labels and tree configurations spanning several local subtrees are used as integral part of some of the patterns." ></td>
	<td class="line x" title="104:197	This leads to much flatter trees, as typical for DG, which has the advantages that (1) it helps to alleviate sparse data by mapping nested structures that express the same dependency relation, (2) the costly overhead for dealing with unbounded dependencies can be largely avoided, (3) it is ensured that the lexical information that matters is available in one central place, allowing the parser to take one well-informed decision instead of several brittle decisions plagued by sparseness, which greatly reduces complexity and the risk of errors (Johnson, 2002)." ></td>
	<td class="line x" title="105:197	Collapsing deeply nested structures into a single dependency relation is less complex but has the same eect as carefully selecting what goes in to the parse history in historybased approaches." ></td>
	<td class="line x" title="106:197	Much of the interesting work is determining what goes into [the history] H(c)(Charniak, 2000)." ></td>
	<td class="line x" title="107:197	(Schneider, 2003a) shows that the vast majority of LDDs can be treated in this way, essentially compressing non-local subtrees into dedicated relations even before grammar writing starts." ></td>
	<td class="line x" title="108:197	The compressed trees correspond to a simple LFG f-structure." ></td>
	<td class="line x" title="109:197	The trees obtained from parsing can be decompressed into traditional constituency trees including empty nodes and co-indexation, or into shallow semantic structures such as Minimal Logical Forms (MLF) (Rinaldi et al. , 2004b; Schneider et al. , 2000; Schwitter et al. , 1999)." ></td>
	<td class="line x" title="110:197	This approach leaves LDDs underspecified, but recoverable, and makes no claims as to whether empty nodes at an automonous syntactic level exist or not." ></td>
	<td class="line x" title="111:197	2.4.2 Post-Processing After parsing, shared constituents can be extracted again." ></td>
	<td class="line x" title="112:197	The parser explicitly does this for control, raising and semi-auxiliary relations, because the grammar does not distinguish between subordinating clauses with and without control." ></td>
	<td class="line x" title="113:197	A probability model based on the verb semantics is invoked if a subordinate clause without overt subject is seen, in order to decide whether the matrix clause subject or object is shared." ></td>
	<td class="line x" title="114:197	2.4.3 What do we lose?" ></td>
	<td class="line x" title="115:197	Among the 10 most frequent types of empty nodes, which cover more than 60,000 of the 64,000 empty nodes in the Penn treebank, there are only two problematic LDD types: WH Traces and indexed gerunds." ></td>
	<td class="line x" title="116:197	WH traces Only 113 of the 10,659 WHNP antecedents in the Penn Treebank are actually question pronouns." ></td>
	<td class="line x" title="117:197	The vast majority, over 9,000, are relative pronouns." ></td>
	<td class="line x" title="118:197	For them, an inversion of the direction of the relation they have to the verb is allowed if the relative pronoun Figure 2: Pro3Gres flowchart precedes the subject." ></td>
	<td class="line x" title="119:197	This method succeeds in most cases, but linguistic non-standard assumptions need to be made for stranded prepositions." ></td>
	<td class="line x" title="120:197	Only non-subject WH-question pronouns and support verbs need to be treated as real non-local dependencies." ></td>
	<td class="line x" title="121:197	In question sentences, before the main parsing is started, the support verb is attached to any lonely participle chunk in the sentence, and the WH-pronoun pre-parses with any verb." ></td>
	<td class="line x" title="122:197	Indexed Gerunds Unlike in control, raising and semi-auxiliary constructions, the antecedent of an indexed gerund cannot be established easily." ></td>
	<td class="line x" title="123:197	The fact that almost half of the gerunds are non-indexed in the Penn Treebank indicates that information about the unexpressed participant is rather semantic than syntactic in nature, much like in pronoun resolution." ></td>
	<td class="line x" title="124:197	Currently, the parser does not try to decide whether the target gerund is an indexed or non-indexed gerund nor does it try to find the identity of the lacking participant in the latter case." ></td>
	<td class="line x" title="125:197	This is an important reason why recall values for the subject and object relations are lower than the precision values." ></td>
	<td class="line x" title="126:197	3 Robustness in the small In addition to a robust deep-linguistic design (robustness in the large, section 2), the implemented parser, Pro3Gres, uses a number of practical robust approaches in the small at each processing level, such as relying on finitestate tagging and chunking or collecting partial parses if no complete analysis can be found, or using incrementally more aggressive pruning techniques in very long sentences." ></td>
	<td class="line x" title="127:197	During the parsing process, only a certain number of alternatives for each possible span are kept." ></td>
	<td class="line x" title="128:197	Experiments have shown that using a fixed number or a number dependent on the parsing complexity in terms of global chart entries lead to very similar results." ></td>
	<td class="line x" title="129:197	Using reasonable beam sizes increases parsing speed by an order of magnitude while hardly aecting parser performance." ></td>
	<td class="line x" title="130:197	For the fixed number model, performance starts to collapse only when less than 4 alternatives per span are kept." ></td>
	<td class="line x" title="131:197	When a certain complexity has been reached (currently 1000 chart entries), only reductions above a certain probability threshold are permissible." ></td>
	<td class="line x" title="132:197	The threshold starts very low, but is a function of the total number of chart entries." ></td>
	<td class="line x" title="133:197	This entails that even sentences with hundreds of words can be parsed quickly, but it is not aimed at finding complete parses for them, rather a graceful degradation of performance (Menzel, 1995) is intended." ></td>
	<td class="line x" title="134:197	4 A hybrid approach on many levels Pro3Gres profits from being hybrid on many levels." ></td>
	<td class="line x" title="135:197	Hybridness means that the most robust approach can be chosen for each task and each processing level." ></td>
	<td class="line x" title="136:197	statistical vs. rule-based the most obvious way in which Pro3Gres is a hybrid (Schneider, 2003b)." ></td>
	<td class="line x" title="137:197	Unlike formal grammars to which posthoc statistical disambiguators can be added, Pro3Gres has been designed to be hybrid, carefully distinguishing between tasks that can best be solved by finite-state methods, rule-based methods and statistical methods." ></td>
	<td class="line x" title="138:197	While e.g. grammar writing is easy for a linguist, and a naive Treebank grammar suers from similar complexity problems as a comprehensive formal grammar, the scope of application and the amount of ambiguity a rule creates is often beyond our imagination and best handled by a statistical system." ></td>
	<td class="line x" title="139:197	shallow vs. deep the designing philosophy for Pro3Gres has been to stay as shallow as possible to obtain reliable results at each level." ></td>
	<td class="line x" title="140:197	Treebank constituency vs. DG the observation that a DG that expresses grammatical relations is more informative, but also more intuitive to interpret for a non-expert, and that Functional DG can avoid a number of LDD types has made DG the formalism of our choice." ></td>
	<td class="line x" title="141:197	For lexicalizing the grammar, a partial mapping from the largest manually annotated corpus available, the Penn Treebank, was necessary, exhibiting a number of mapping challenges." ></td>
	<td class="line x" title="142:197	history-based vs. mapping-based Pro3Gres is not a parse-history-based approach." ></td>
	<td class="line x" title="143:197	Instead of manually selecting what goes into the history, as is usually done (see (Henderson, 2003) for an exception), we manually select how to linguistically meaningfully map Treebank structures onto dependency relations by the use of mapping patterns adapted from (Johnson, 2002)." ></td>
	<td class="line x" title="144:197	probabilistic vs. statistical Pro3Gres is not a probabilistic system in the sense of a PCFG." ></td>
	<td class="line x" title="145:197	From a practical viewpoint, knowing the probability of a certain rule expansion per se is of little interest." ></td>
	<td class="line x" title="146:197	Pro3Gres models decision probabilities, the probability of a parse is understood to be the product of all the decision probabilities taken during the derivation." ></td>
	<td class="line x" title="147:197	local subtress vs. DOP psycholinguistic experiments and Data-Oriented Parsing (DOP) (Bod et al. , 2003) suggest that people store subtrees of various sizes, from two-word fragments to entire sentences." ></td>
	<td class="line x" title="148:197	But (Goodman, 2003) suggests that the large number of subtrees can be reduced to a compact grammar that makes DOP parsing computationally tractable." ></td>
	<td class="line x" title="149:197	In Pro3Gres, a subset of non-local fragments which, based on linguistic intuition are especially important, are used." ></td>
	<td class="line x" title="150:197	generative vs. structure-generating DG generally, although generative in the sense that connected complete structures are generated, is not generative in the sense that it is always guaranteed to terminate if used for random generation of language." ></td>
	<td class="line x" title="151:197	Since a complete or partial hierarchical structure that follows CFG assumptions due to the employed grammar is built up for each sentence." ></td>
	<td class="line x" title="152:197	Pro3Gres constraint to allow each complement dependency type only once per verb can be seen as a way of rendering it generative in practice." ></td>
	<td class="line x" title="153:197	syntax vs. semantics insteadofusing a back-o to tags (Collins, 1999), semantic classes, Wordnet for nouns and Levin classes for verbs, are used, in the hope that they better manage better to express selectional restrictions than tags." ></td>
	<td class="line x" title="154:197	Practical experiments have shown, however, that, in accordance to (Gildea, 2001) on head-lexicalisation, there is almost no increase in performance." ></td>
	<td class="line x" title="155:197	5 Applications and Evaluation Pro3Gres is currently being applied in a Question Answering system specifically targeted at Figure 3: Dependency Tree output of the SWI Prolog graphical implementation of the parser technical domains (Rinaldi et al. , 2004b)." ></td>
	<td class="line x" title="156:197	One of the main advantages of a dependency-based parser such as Pro3Gres over other parsing approaches is that a mapping from the syntactic layer to a semantic layer (meaning representation) is partly simplified (Molla et al. , 2000; Rinaldi et al. , 2002)." ></td>
	<td class="line x" title="157:197	The original version of the QA system used the Link Grammar (LG) parser (Sleator and Temperley, 1993), which however had a number of significant shortcomings." ></td>
	<td class="line x" title="158:197	In particular the set of the dependency relations used in LG is very idiosyncratic, which makes any syntacticsemantic mapping created for LG necessarily unportable and dicult to extend and maintain." ></td>
	<td class="line x" title="159:197	A recent line of research concerns applications for the Semantic Web." ></td>
	<td class="line x" title="160:197	The documents available in the World Wide Web are mostly written in natural language." ></td>
	<td class="line x" title="161:197	As such, they are understandable only to humans." ></td>
	<td class="line x" title="162:197	One of the directions of Semantic Web research is about adding a layer to the documents that somehow formalizes their content, making it understandable also to software agents." ></td>
	<td class="line x" title="163:197	Such Semantic Web annotations can be seen as a way to mark explicitly the meaning of certain parts of the documents." ></td>
	<td class="line x" title="164:197	The dependency relations provided by a parser such as Pro3Gres, combined with domain specific axioms, allow the creation of (some of) the semantic annotations, as described in (Rinaldi et al. , 2003; Kaljurand et al. , 2004)." ></td>
	<td class="line x" title="165:197	The modified QA system (using Pro3Gres) is being exploited in the area of Life Sciences, for applications concerning Knowledge Discovery over Medline abstracts (Rinaldi et al. , 2004a; Dowdall et al. , 2004)." ></td>
	<td class="line x" title="166:197	We illustrate some of the dierences between general-purpose parsing and the parsing of highly technical texts like Medline and give two evaluations." ></td>
	<td class="line x" title="167:197	5.1 General unrestricted texts We first report an evaluation on sentences from an open domain, which gives a good impression of the performance of the parser on general, unrestricted text." ></td>
	<td class="line x" title="168:197	In traditional constituency approaches, parser evaluation is done in terms of the correspondence of the bracketting between the gold standard and the parser output." ></td>
	<td class="line x" title="169:197	(Lin, 1995; Carroll et al. , 1999) suggest evaluating on the linguistically more meaningful level of syntactic relations." ></td>
	<td class="line x" title="170:197	Two evaluations on the syntactic relation level are reported in the following." ></td>
	<td class="line x" title="171:197	First, a general-purpose evaluation using a hand-compiled gold standard corpus (Carroll et al. , 1999), which contains the grammatical relation data of 500 random sentences from the Susanne corpus." ></td>
	<td class="line x" title="172:197	The performance, shown in table 2, is, according to (Preiss, 2003), similar to a large selection of statistical parsers and a grammatical relation finder." ></td>
	<td class="line x" title="173:197	Relations involving long-distance dependencies form part of these relations." ></td>
	<td class="line x" title="174:197	In order to measure specifically their performance, a selection of them is also given: WH-Subject (WHS), WH-Object (WHO), passive Subject (PSubj), control Subject (CSubj), and the anaphor of the relative clause pronoun (RclSubjA)." ></td>
	<td class="line x" title="175:197	5.2 Parsing highly technical language While measuring general parsing performance is fundamental in the development of any parsing system there is a danger of fostering domain dependence in concentrating on a single domain." ></td>
	<td class="line x" title="176:197	In order to answer how the parser performs over domains markedly dierent to the training corpus, the parser has been applied to the GENIA corpus (Kim et al. , 2003), 2000 MEDLINE abstracts of more than 400,000 words describing the results of Biomedical research." ></td>
	<td class="line x" title="177:197	Average sentence length is 27 words, the lanPercentage Values for some relations, general, on Carroll corpus only LDD-involving Subject Object noun-PP verb-PP subord." ></td>
	<td class="line x" title="178:197	cl. WHS WHO PSubj CSubj RclSubjA Precision 91 89 73 74 68 92 60 n/a 80 89 Recall 81 83 67 83 n/a 90 86 83 n/a 63 Table 2: Results of evaluating the parser output on Carrolls test suite on subject, object, PPattachment and clause subordination relations, and a selective evaluation of 5 relations involving long-distance dependencies (LDD) Percentage Values for some relations, general, on the GENIA corpus Subject Object noun-PP verb-PP subord." ></td>
	<td class="line x" title="179:197	clause Precision 90 94 83 82 71 Recall 86 95 82 84 75 Table 3: Results of evaluating 100 random sentences from the terminology-annotated GENIA corpus, on subject, object, PP-attachment and clause subordination relations guage is very technical and extremely domainspecific." ></td>
	<td class="line x" title="180:197	But the most striking characteristic of this domain is the frequency of MultiWord Terms (MWT) which are known to cause serious problems for NLP systems (Sag et al. , 2002), (Dowdall et al. , 2003)." ></td>
	<td class="line x" title="181:197	The token to chunk ratio: NPs = 2.3, VPs = 1.3 (number of tokens divided by the number of chunks) is unusually high." ></td>
	<td class="line x" title="182:197	The GENIA corpus does not include any syntactic annotation (making standard evaluation more dicult) but approx." ></td>
	<td class="line x" title="183:197	100, 000 MWTs are annotated and assigned a semantic type from the GENIA ontology." ></td>
	<td class="line x" title="184:197	This novel parsing application is designed to determine how parsing performance interacts with MWT recognition as well as the applicability and possible improvements to the probablistic model over this domain, to test the hypothesis if terminology is the key to a successful parsing system." ></td>
	<td class="line x" title="185:197	We do not discard this information, thus simulating a situation in which a near-perfect terminology-recognition tool is at ones disposal." ></td>
	<td class="line x" title="186:197	MWT are regarded as chunks, the parsing thus takes place between between the heads of MWT, words and chunks." ></td>
	<td class="line x" title="187:197	100 random sentences from the GENIA corpus have been manually annotated for this evaluation and compared to the parser output." ></td>
	<td class="line x" title="188:197	Despite the extreme complexity and technical language, parsing performance under these conditions is considerably better than on the Carroll corpus when using automated chunking, as table 3 reveals." ></td>
	<td class="line x" title="189:197	It is worth noting that 10 of the 17 subject precision errors (out of 171 subjects) are hard cases involving long-distance dependencies (1 control, 4 relative pronouns) and 5 verb group chunking errors." ></td>
	<td class="line x" title="190:197	Equally interesting, 2 of the 4 object recall errors (out of 79 objects) are due to 1 mistagging and 1 mischunking." ></td>
	<td class="line x" title="191:197	In practice, MWT extraction is still not automated to the level of chunking or Name Entity recognition simulated in this experiment (for a comprehensive review of the state-of-theart see (Castellv et al. , 2001))." ></td>
	<td class="line x" title="192:197	This is, in a large part, due to the lack of definitive orthographic, morphological and syntactic characteristics to dierentiante between MWTs and canonical phrases." ></td>
	<td class="line x" title="193:197	So MWT extraction remains a semi-automated task performed in cycles with the result of each cycle requiring manual validation." ></td>
	<td class="line x" title="194:197	The return for this time consuming activity are the characteristics of MWTs which can be use to fine tune the algorithms during the next extraction cycle." ></td>
	<td class="line x" title="195:197	6Conclusion We have suggested a robust, deep-linguistic grammar theory delivering grammatical relation structures as output, which are closer to predicate-argument structures than pure constituency structures, and more informative if non-local dependencies are involved." ></td>
	<td class="line x" title="196:197	We have presented an implementation of the theory that is used for large-scale parsing." ></td>
	<td class="line x" title="197:197	An evaluation at the grammatical relation level shows that its performance is state-of-the-art." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="J05-3003
Large-Scale Induction And Evaluation Of Lexical Resources From The Penn-II And Penn-III Treebanks
O'Donovan, Ruth;Burke, Michael;Cahill, Aoife;Van Genabith, Josef;Way, Andy;"></td>
	<td class="line x" title="1:532	Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks Ruth ODonovan  Dublin City University Michael Burke  Dublin City University Aoife Cahill  Dublin City University Josef van Genabith  Dublin City University Andy Way  Dublin City University We present a methodology for extracting subcategorization frames based on an automatic lexical-functional grammar (LFG) f-structure annotation algorithm for the Penn-II and Penn-III Treebanks." ></td>
	<td class="line x" title="2:532	We extract syntactic-function-based subcategorization frames (LFG semantic forms) and traditional CFG category-based subcategorization frames as well as mixed function/category-based frames, with or without preposition information for obliques and particle information for particle verbs." ></td>
	<td class="line x" title="3:532	Our approach associates probabilities with frames conditional on the lemma, distinguishes between active and passive frames, and fully reflects the effects of long-distance dependencies in the source data structures." ></td>
	<td class="line x" title="4:532	In contrast to many other approaches, ours does not predefine the subcategorization frame types extracted, learning them instead from the source data." ></td>
	<td class="line x" title="5:532	Including particles and prepositions, we extract 21,005 lemma frame types for 4,362 verb lemmas, with a total of 577 frame types and an average of 4.8 frame types per verb." ></td>
	<td class="line x" title="6:532	We present a large-scale evaluation of the complete set of forms extracted against the full COMLEX resource." ></td>
	<td class="line x" title="7:532	To our knowledge, this is the largest and most complete evaluation of subcategorization frames acquired automatically for English." ></td>
	<td class="line x" title="8:532	1." ></td>
	<td class="line x" title="9:532	Introduction In modern syntactic theories (e.g. , lexical-functional grammar [LFG] [Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001], head-driven phrase structure grammar [HPSG] [Pollard and Sag 1994], tree-adjoining grammar [TAG] [Joshi 1988], and combinatory categorial grammar [CCG] [Ades and Steedman 1982]), the lexicon is the central repository for much morphological, syntactic, and semantic information." ></td>
	<td class="line x" title="10:532	 National Centre for Language Technology, School of Computing, Dublin City University, Glasnevin, Dublin 9, Ireland." ></td>
	<td class="line x" title="11:532	E-mail: {rodonovan,mburke,acahill,josef,away}@computing.dcu.ie." ></td>
	<td class="line x" title="12:532	 Centre for Advanced Studies, IBM, Dublin, Ireland." ></td>
	<td class="line x" title="13:532	Submission received: 19 March 2004; revised submission received: 18 December 2004; accepted for publication: 2 March 2005." ></td>
	<td class="line x" title="14:532	 2005 Association for Computational Linguistics Computational Linguistics Volume 31, Number 3 Extensive lexical resources, therefore, are crucial in the construction of wide-coverage computational systems based on such theories." ></td>
	<td class="line x" title="15:532	One important type of lexical information is the subcategorization requirements of an entry (i.e. , the arguments a predicate must take in order to form a grammatical construction)." ></td>
	<td class="line x" title="16:532	Lexicons, including subcategorization details, were traditionally produced by hand." ></td>
	<td class="line x" title="17:532	However, as the manual construction of lexical resources is time consuming, error prone, expensive, and rarely ever complete, it is often the case that the limitations of NLP systems based on lexicalized approaches are due to bottlenecks in the lexicon component." ></td>
	<td class="line x" title="18:532	In addition, subcategorization requirements may vary across linguistic domain or genre (Carroll and Rooth 1998)." ></td>
	<td class="line x" title="19:532	Manning (1993) argues that, aside from missing domain-specific complementation trends, dictionaries produced by hand will tend to lag behind real language use because of their static nature." ></td>
	<td class="line x" title="20:532	Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue." ></td>
	<td class="line x" title="21:532	Aside from the extraction of theory-neutral subcategorization lexicons, there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG, CCG, and HPSG (Chen and Vijay-Shanker 2000; Xia 1999; Hockenmaier, Bierner, and Baldridge 2004; Nakanishi, Miyao, and Tsujii 2004)." ></td>
	<td class="line x" title="22:532	In this article we present an approach to automating the process of lexical acquisition for LFG (i.e. , grammatical-function-based systems)." ></td>
	<td class="line x" title="23:532	However, our approach also generalizes to CFG category-based approaches." ></td>
	<td class="line x" title="24:532	In LFG, subcategorization requirements are enforced through semantic forms specifying which grammatical functions are required by a particular predicate." ></td>
	<td class="line oc" title="25:532	Our approach is based on earlier work on LFG semantic form extraction (van Genabith, Sadler, and Way 1999) and recent progress in automatically annotating the Penn-II and Penn-III Treebanks with LFG f-structures (Cahill et al. 2002; Cahill, McCarthy, et al. 2004)." ></td>
	<td class="line x" title="26:532	Our technique requires a treebank annotated with LFG functional schemata." ></td>
	<td class="line x" title="27:532	In the early approach of van Genabith, Sadler, and Way (1999), this was provided by manually annotating the rules extracted from the publicly available subset of the AP Treebank to automatically produce corresponding f-structures." ></td>
	<td class="line x" title="28:532	If the f-structures are of high quality, reliable LFG semantic forms can be generated quite simply by recursively reading off the subcategorizable grammatical functions for each local PRED value at each level of embedding in the f-structures." ></td>
	<td class="line x" title="29:532	The work reported in van Genabith, Sadler, and Way (1999) was small scale (100 trees) and proof of concept and required considerable manual annotation work." ></td>
	<td class="line x" title="30:532	It did not associate frames with probabilities, discriminate between frames for active and passive constructions, properly reflect the effects of long-distance dependencies (LDDs), or include CFG category information." ></td>
	<td class="line xc" title="31:532	In this article we show how the extraction process can be scaled to the complete Wall Street Journal (WSJ) section of the Penn-II Treebank, with about one million words in 50,000 sentences, based on the automatic LFG f-structure annotation algorithm described in Cahill et al.(2002) and Cahill, McCarthy, et al.(2004)." ></td>
	<td class="line x" title="34:532	More recently we have extended the extraction approach to the larger, domain-diverse Penn-III Treebank." ></td>
	<td class="line x" title="35:532	Aside from the parsed WSJ section, this version of the treebank contains parses for a subsection of the Brown corpus (almost 385,000 words in 24,000 trees) taken from a variety of text genres." ></td>
	<td class="line x" title="36:532	1 In addition to extracting grammatical-function1 For the remainder of this work, when we refer to the Penn-II Treebank, we mean the parse-annotated WSJ, and when we refer to the Penn-III Treebank, we mean the parse-annotated WSJ and Brown corpus combined." ></td>
	<td class="line x" title="37:532	330 ODonovan et al. Large-Scale Induction and Evaluation of Lexical Resources based subcategorization frames, we also include the syntactic categories of the predicate and its subcategorized arguments, as well as additional details such as the prepositions required by obliques and particles accompanying particle verbs." ></td>
	<td class="line x" title="38:532	Our method discriminates between active and passive frames, properly reflects LDDs in the source data structures, assigns conditional probabilities to the semantic forms associated with each predicate, and does not predefine the subcategorization frames extracted." ></td>
	<td class="line x" title="39:532	In Section 2 of this article, we briefly outline LFG, presenting typical lexical entries and the encoding of subcategorization information." ></td>
	<td class="line x" title="40:532	Section 3 reviews related work in the area of automatic subcategorization frame extraction." ></td>
	<td class="line x" title="41:532	Our methodology and its implementation are presented in Section 4." ></td>
	<td class="line x" title="42:532	In Section 5 we present results from the extraction process." ></td>
	<td class="line x" title="43:532	We evaluate the complete induced lexicon against the COMLEX resource (Grishman, MacLeod, and Meyers 1994) and present the results in Section 6." ></td>
	<td class="line x" title="44:532	To our knowledge, this is by far the largest and most complete evaluation of subcategorization frames automatically acquired for English." ></td>
	<td class="line x" title="45:532	In Section 7, we examine the coverage of our lexicon in regard to unseen data and the rate at which new lexical entries are learned." ></td>
	<td class="line x" title="46:532	Finally, in Section 8 we conclude and give suggestions for future work." ></td>
	<td class="line x" title="47:532	2." ></td>
	<td class="line x" title="48:532	Subcategorization in LFG Lexical functional grammar (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) is a member of the family of constraint-based grammars." ></td>
	<td class="line x" title="49:532	It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicateargumentmodifier relations and certain morphosyntactic properties such as tense, aspect, and case." ></td>
	<td class="line x" title="50:532	C-structure takes the form of phrase structure trees and is defined in terms of CFG rules and lexical entries." ></td>
	<td class="line x" title="51:532	F-structure is produced from functional annotations on the nodes of the c-structure and implemented in terms of recursive feature structures (attributevalue matrices)." ></td>
	<td class="line x" title="52:532	This is exemplified by the analysis of the string The inquiry soon focused on the judge (wsj 0267 72) using the grammar in Figure 1, which results in the annotated c-structure and f-structure in Figure 2." ></td>
	<td class="line x" title="53:532	The value of the PRED attribute in an f-structure is a semantic form gf 1, gf 2,, gf n , where  is a lemma and gf a grammatical function." ></td>
	<td class="line x" title="54:532	The semantic form provides an argument list gf 1,gf 2,,gf n  specifying the governable grammatical functions (or arguments) required by the predicate to form a grammatical construction." ></td>
	<td class="line x" title="55:532	In Figure 1 the verb FOCUS requires a subject and an oblique object introduced by the preposition on: FOCUS( SUBJ)( OBL on )." ></td>
	<td class="line x" title="56:532	The argument list can be empty, as in the PRED value for judge in Figure 1." ></td>
	<td class="line x" title="57:532	According to Dalrymple (2001), LFG assumes the following universally available inventory of grammatical functions: SUBJ(ect), OBJ(ect), OBJ , COMP, XCOMP, OBL(ique) , ADJ(unct), XADJ." ></td>
	<td class="line x" title="58:532	OBJ  and OBL  represent families of grammatical functions indexed by their semantic role, represented by the theta subscript." ></td>
	<td class="line x" title="59:532	This list of grammatical functions is divided into governable (subcategorizable) grammatical functions (arguments) and nongovernable (nonsubcategorizable) grammatical functions (modifiers/adjuncts), as summarized in Table 1." ></td>
	<td class="line x" title="60:532	2 LFGs may also involve morphological and semantic levels of representation." ></td>
	<td class="line x" title="61:532	331 Computational Linguistics Volume 31, Number 3 Figure 1 Sample LFG rules and lexical entries." ></td>
	<td class="line x" title="62:532	A number of languages allow the possibility of object functions in addition to the primary OBJ, such as the second or indirect object in English." ></td>
	<td class="line x" title="63:532	Oblique arguments are realized as prepositional phrases in English." ></td>
	<td class="line x" title="64:532	COMP, XCOMP,andXADJ are all clausal functions which differ in the way in which they are controlled." ></td>
	<td class="line x" title="65:532	A COMP is a closed function which contains its own internal SUBJ: The judge thinks [COMP that it will resume]." ></td>
	<td class="line x" title="66:532	XCOMP and XADJ are open functions not requiring an internal SUBJ." ></td>
	<td class="line x" title="67:532	The subject is instead specified externally in the matrix phrase: The judge wants [XCOMP to open an inquiry]." ></td>
	<td class="line x" title="68:532	While many linguistic theories state subcategorization requirements in terms of phrase structure (CFG categories), Dalrymple (2001) questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the language-specific constituent structure level." ></td>
	<td class="line x" title="69:532	LFG argues that subcategorization requirements are best stated at the f-structure level, in functional rather than phrasal terms." ></td>
	<td class="line x" title="70:532	This is because of the assumption that abstract grammatical functions are primitive concepts as opposed to derivatives 332 ODonovan et al. Large-Scale Induction and Evaluation of Lexical Resources Figure 2 Cand f-structures for Penn Treebank sentence wsj 0267 72, The inquiry soon focused on the judge." ></td>
	<td class="line x" title="71:532	of phrase structural position." ></td>
	<td class="line x" title="72:532	In LFG, the subcategorization requirements of a particular predicate are expressed by its semantic form: FOCUS( SUBJ)( OBL on ) in Figure 1." ></td>
	<td class="line x" title="73:532	The subcategorization requirements expressed by semantic forms are enforced at f-structure level through completeness and coherence well-formedness conditions on f-structure (Kaplan and Bresnan 1982): An f-structure is locally complete iff it contains all the governable grammatical functions that its predicate governs." ></td>
	<td class="line x" title="74:532	An f-structure is complete iff it and all its subsidiary f-structures are locally complete." ></td>
	<td class="line x" title="75:532	An f-structure is locally coherent iff all the governable grammatical functions that it contains are governed by a local predicate." ></td>
	<td class="line x" title="76:532	An f-structure is coherent iff it and all its subsidiary f-structures are locally coherent." ></td>
	<td class="line x" title="77:532	(page 211) Consider again the f-structure in Figure 2." ></td>
	<td class="line x" title="78:532	The semantic form associated with the verb focus is FOCUS( SUBJ)( OBL on )." ></td>
	<td class="line x" title="79:532	The f-structure is locally complete, as it contains the SUBJ and an OBL with the preposition on specified by the semantic form." ></td>
	<td class="line x" title="80:532	The f-structure also satisfies the coherence condition, as it does not contain any governable grammatical functions other than the SUBJ and OBL required by the local PRED." ></td>
	<td class="line x" title="81:532	333 Computational Linguistics Volume 31, Number 3 Table 1 Governable and nongovernable grammatical functions in LFG." ></td>
	<td class="line x" title="82:532	Governable GFs Nongovernable GFs SUBJ ADJ OBJ XADJ XCOMP COMP OBJ  OBL  Because of the specific form of the LFG lexicon, our extraction approach differs in interesting ways from that of previous lexical extraction experiments." ></td>
	<td class="line x" title="83:532	This contrast is made evident in Sections 3 and 4." ></td>
	<td class="line x" title="84:532	3. Related Work The encoding of verb subcategorization properties is an essential step in the construction of computational lexicons for tasks such as parsing, generation, and machine translation." ></td>
	<td class="line x" title="85:532	Creating such a resource by hand is time consuming and error prone, requires considerable linguistic expertise, and is rarely if ever complete." ></td>
	<td class="line x" title="86:532	In addition, a hand-crafted lexicon cannot be easily adapted to specific domains or account for linguistic change." ></td>
	<td class="line x" title="87:532	Accordingly, many researchers have attempted to construct lexicons automatically, especially for English." ></td>
	<td class="line x" title="88:532	In this section, we discuss approaches to CFG-based subcategorization frame extraction as well as attempts to induce lexical resources which comply with specific linguistic theories or express information in terms of more abstract predicate-argument relations." ></td>
	<td class="line x" title="89:532	The evaluation of these approaches is discussed in greater detail in Section 6, in which we compare our results with those reported elsewhere in the literature." ></td>
	<td class="line x" title="90:532	We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input." ></td>
	<td class="line x" title="91:532	Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon." ></td>
	<td class="line x" title="92:532	Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames." ></td>
	<td class="line x" title="93:532	The frames do not include details of specific prepositions." ></td>
	<td class="line x" title="94:532	Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames." ></td>
	<td class="line x" title="95:532	Ushioda et al.(1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes." ></td>
	<td class="line x" title="97:532	The experiment is limited by the fact that all prepositional phrases are treated as adjuncts." ></td>
	<td class="line x" title="98:532	Ushioda et al.(1993) employ an additional statistical method based on log-linear models and Bayes theorem to filter the extra noise introduced by the parser and were the first to induce relative frequencies for the extracted frames." ></td>
	<td class="line x" title="100:532	Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw text through a stochastic tagger and a finite-state parser (which includes a set of simple rules for subcategorization frame recognition) in order to extract verbs and the constituents with which they co-occur." ></td>
	<td class="line x" title="101:532	He assumes 19 different subcategorization 334 ODonovan et al. Large-Scale Induction and Evaluation of Lexical Resources frame definitions, and the extracted frames include details of specific prepositions." ></td>
	<td class="line x" title="102:532	The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993)." ></td>
	<td class="line x" title="103:532	Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb." ></td>
	<td class="line x" title="104:532	Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection." ></td>
	<td class="line x" title="105:532	The frames incorporate control information and details of specific prepositions." ></td>
	<td class="line x" title="106:532	Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames." ></td>
	<td class="line x" title="107:532	Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection." ></td>
	<td class="line x" title="108:532	Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar and a text corpus to compute the probability of particular subcategorization patterns." ></td>
	<td class="line x" title="109:532	The approach is iterative with the aim of estimating the distribution of subcategorization frames associated with a particular predicate." ></td>
	<td class="line x" title="110:532	They perform a mapping between their frames and those of the OALD, resulting in 15 frame types." ></td>
	<td class="line x" title="111:532	These do not contain details of specific prepositions." ></td>
	<td class="line x" title="112:532	More recently, a number of researchers have applied similar techniques to automatically derive lexical resources for languages other than English." ></td>
	<td class="line x" title="113:532	Schulte im Walde (2002a, 2002b) uses a head-lexicalized probabilistic context-free grammar similar to that of Caroll and Rooth (1998) to extract subcategorization frames from a large German newspaper corpus from the 1990s." ></td>
	<td class="line x" title="114:532	She predefines 38 distinct frame types, which contain maximally three arguments each and are made up of a combination of the following: nominative, dative, and accusative noun phrases; reflexive pronouns; prepositional phrases; expletive es; subordinated nonfinite clauses; subordinated finite clauses; and copula constructions." ></td>
	<td class="line x" title="115:532	The frames may optionally contain details of particular prepositional use." ></td>
	<td class="line x" title="116:532	Unsupervised training is performed on a large German newspaper corpus, and the resulting probabilistic grammar establishes the relevance of different frame types to a specific lexical head." ></td>
	<td class="line x" title="117:532	Because of computing time constraints, Schulte im Walde limits sentence length for grammar training and parsing." ></td>
	<td class="line x" title="118:532	Sentences of length between 5 and 10 words were used to bootstrap the lexicalized grammar model." ></td>
	<td class="line x" title="119:532	For lexicalized training, sentences of length between 5 and 13 words were used." ></td>
	<td class="line x" title="120:532	The result is a subcategorization lexicon for over 14,000 German verbs." ></td>
	<td class="line x" title="121:532	The extensive evaluation carried out by Schulte im Walde will be discussed in greater detail in Section 6." ></td>
	<td class="line x" title="122:532	Approaches using treebank-based data as a source for subcategorization information, such as ours, do not predefine the frames to be extracted but rather learn them from the data." ></td>
	<td class="line x" title="123:532	Kinyon and Prolo (2002) describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank." ></td>
	<td class="line x" title="124:532	This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank." ></td>
	<td class="line x" title="125:532	Each of these sequences was categorized as a modifier or argument." ></td>
	<td class="line x" title="126:532	Arguments were then mapped to traditional syntactic functions." ></td>
	<td class="line x" title="127:532	For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic function is subject." ></td>
	<td class="line x" title="128:532	In general, argumenthood was preferred over adjuncthoood." ></td>
	<td class="line x" title="129:532	As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is. Sarkar and Zeman (2000) present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank (Hajic 335 Computational Linguistics Volume 31, Number 3 1998)." ></td>
	<td class="line x" title="130:532	Czech is a language with a freer word order than English and so configurational information cannot be relied upon." ></td>
	<td class="line x" title="131:532	In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame." ></td>
	<td class="line x" title="132:532	Finding subcategorization frames involves filtering adjuncts from the observed frame." ></td>
	<td class="line x" title="133:532	This is achieved using three different hypothesis tests: BHT, log-likelihood ratio, and t-score." ></td>
	<td class="line x" title="134:532	The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more)." ></td>
	<td class="line x" title="135:532	Marinov and Hemming (2004) present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank (Simov, Popova, and Osenova 2002)." ></td>
	<td class="line x" title="136:532	In a similar way to that of Sarkar and Zeman (2000), Marinov and Hemmings system collects both arguments and adjuncts." ></td>
	<td class="line x" title="137:532	It then uses the binomial log-likelihood ratio to filter incorrect frames." ></td>
	<td class="line x" title="138:532	The BulTreebank trees are annotated with HPSG-typed feature structure information and thus contain more detail than the dependency trees." ></td>
	<td class="line x" title="139:532	The work done for Bulgarian is small-scale, however, as Marinov and Hemming are working with a preliminary version of the treebank with 580 sentences." ></td>
	<td class="line x" title="140:532	Work has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG." ></td>
	<td class="line x" title="141:532	As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar." ></td>
	<td class="line x" title="142:532	Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing." ></td>
	<td class="line x" title="143:532	The extraction procedure utilizes a head percolation table as introduced by Magerman (1995) in combination with a variation of Collinss (1997) approach to the differentiation between complement and adjunct." ></td>
	<td class="line x" title="144:532	This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question." ></td>
	<td class="line x" title="145:532	The number of frame types extracted (i.e. , an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996." ></td>
	<td class="line x" title="146:532	Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank." ></td>
	<td class="line x" title="147:532	The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997)." ></td>
	<td class="line x" title="148:532	Then the elementary trees are read off in a quite straightforward manner." ></td>
	<td class="line x" title="149:532	Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics." ></td>
	<td class="line x" title="150:532	The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099." ></td>
	<td class="line x" title="151:532	Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank." ></td>
	<td class="line x" title="152:532	For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner." ></td>
	<td class="line x" title="153:532	The first step is to label each node as either a head, complement, or adjunct based on the approaches of Magerman (1994) and Collins (1997)." ></td>
	<td class="line x" title="154:532	Each node is subsequently assigned the relevant category based on its constituent type and surface configuration." ></td>
	<td class="line x" title="155:532	The algorithm handles like coordination and exploits the traces used in the treebank in order to interpret LDDs." ></td>
	<td class="line x" title="156:532	Unlike our approach, those of Xia (1999) and Hockenmaier, Bierner, and Baldridge (2004) include a substantial initial correction and clean-up of the Penn-II trees." ></td>
	<td class="line x" title="157:532	Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii (2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank." ></td>
	<td class="line x" title="158:532	Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997); 336 ODonovan et al. Large-Scale Induction and Evaluation of Lexical Resources the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category." ></td>
	<td class="line x" title="159:532	In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of inverse schemata. 4." ></td>
	<td class="line x" title="160:532	Methodology The first step in the application of our methodology is the production of a treebank annotated with LFG f-structure information." ></td>
	<td class="line x" title="161:532	F-structures are attributevalue structures which represent abstract syntactic information, approximating to basic predicateargumentmodifier structures." ></td>
	<td class="line x" title="162:532	Most of the early work on automatic f-structure annotation (e.g. , van Genabith, Way, and Sadler 1999; Frank 2000; Sadler, van Genabith, and Way 2000) was applied only to small data sets (fewer than 200 sentences) and was largely proof of concept." ></td>
	<td class="line oc" title="163:532	However, more recent work (Cahill et al. 2002; Cahill, McCarthy, et al. 2004) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank (Marcus et al. 1994), containing more than 1,000,000 words and 49,000 sentences." ></td>
	<td class="line oc" title="164:532	We utilize the automatic annotation algorithm of Cahill et al.(2002) and Cahill, McCarthy, et al.(2004) to derive a version of Penn-II in which each node in each tree is annotated with LFG functional annotations in the form of attribute-value structure equations." ></td>
	<td class="line x" title="167:532	The algorithm uses categorial, configurational, local head, and Penn-II functional and trace information." ></td>
	<td class="line x" title="168:532	The annotation procedure is dependent on locating the head daughter, for which an amended version of Magerman (1994) is used." ></td>
	<td class="line x" title="169:532	The head is annotated with the LFG equation =." ></td>
	<td class="line x" title="170:532	Linguistic generalizations are provided over the left (the prefix) and the right (suffix) context of the head for each syntactic category occurring as the mother nodes of such heads." ></td>
	<td class="line x" title="171:532	To give a simple example, the rightmost NP to the left of a VP head under an S is likely to be the subject of the sentence ( SUBJ =), while the leftmost NP to the right of the V head of a VP is most probably the verbs object ( OBJ =)." ></td>
	<td class="line xc" title="172:532	Cahill, McCarthy, et al.(2004) provide four classes of annotation principles: one for noncoordinate configurations, one for coordinate configurations, one for traces (long-distance dependencies), and a final catch all and clean up phase." ></td>
	<td class="line x" title="174:532	The satisfactory treatment of long-distance dependencies by the annotation algorithm is imperative for the extraction of accurate semantic forms." ></td>
	<td class="line x" title="175:532	The Penn Treebank employs a rich arsenal of traces and empty productions (nodes which do not realize any lexical material) to coindex displaced material with the position where it should be interpreted semantically." ></td>
	<td class="line xc" title="176:532	The algorithm of Cahill, McCarthy, et al.(2004) translates the traces into corresponding reentrancies in the f-structure representation by treating null constituents as full nodes and recording the traces in terms of index=i f-structure annotations (Figure 3)." ></td>
	<td class="line x" title="178:532	Passive movement is captured and expressed at f-structure level using a passive:+ annotation." ></td>
	<td class="line x" title="179:532	Once a treebank tree is annotated with feature structure equations by the annotation algorithm, the equations are collected, and a constraint solver produces an f-structure." ></td>
	<td class="line x" title="180:532	In order to ensure the quality of the semantic forms extracted by our method, we must first ensure the quality of the f-structure annotations." ></td>
	<td class="line x" title="181:532	The results of two different evaluations of the automatically generated f-structures are presented in Table 2." ></td>
	<td class="line x" title="182:532	Both use the evaluation software and triple encoding presented in Crouch et al.(2002)." ></td>
	<td class="line x" title="184:532	The first of these is against the DCU 105, a gold-standard set of 105 hand-coded f-structures 337 Computational Linguistics Volume 31, Number 3 Figure 3 Use of reentrancy between TOPIC and COMP to capture long-distance dependency in Penn Treebank sentence wsj 0008 2, Until Congress acts, the government hasnt any authority to issue new debt obligations of any kind, the Treasury said." ></td>
	<td class="line xc" title="185:532	from Section 23 of the Penn Treebank as described in Cahill, McCarthy, et al.(2004)." ></td>
	<td class="line x" title="187:532	For the full set of annotations they achieve precision of over 96.5% and recall of over 96.6%." ></td>
	<td class="line x" title="188:532	There is, however, a risk of overfitting when evaluation is limited to a gold standard of this size." ></td>
	<td class="line oc" title="189:532	More recently, Burke, Cahill, et al.(2004a) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank (King et al. 2003), a set of 700 randomly selected sentences from Section 23 which have been parsed, converted to dependency format, and manually corrected and extended by human validators." ></td>
	<td class="line x" title="191:532	They report precision of over 88.5% and recall of over 86% (Table 2)." ></td>
	<td class="line x" title="192:532	The PARC 700 Dependency Bank differs substantially from both the DCU 105 f-structure bank and the automatically generated f-structures in regard to Table 2 Results of f-structure evaluation." ></td>
	<td class="line x" title="193:532	DCU 105 PARC 700 Precision 96.52% 88.57% Recall 96.62% 86.10% F-score 96.57% 87.32% 338 ODonovan et al. Large-Scale Induction and Evaluation of Lexical Resources the style of linguistic analysis, feature nomenclature, and feature geometry." ></td>
	<td class="line x" title="194:532	Some, but not all, of these differences are captured by automatic conversion software." ></td>
	<td class="line oc" title="195:532	A detailed discussion of the issues inherent in this process and a full analysis of results is presented in Burke, Cahill, et al.(2004a)." ></td>
	<td class="line x" title="197:532	Results broken down by grammatical function for the DCU 105 evaluation are presented in Table 3." ></td>
	<td class="line x" title="198:532	OBL (prepositional phrase) arguments are traditionally difficult to annotate reliably." ></td>
	<td class="line x" title="199:532	The results show, however, that with respect to obliques, the annotation algorithm, while slightly conservative (recall of 82%), is very accurate: 96% of the time it annotates an oblique, the annotation is correct." ></td>
	<td class="line x" title="200:532	A high-quality set of f-structures having been produced, the semantic form extraction methodology is applied." ></td>
	<td class="line x" title="201:532	This is based on and substantially extends both the granularity and coverage of an idea in van Genabith, Sadler, and Way (1999): For each f-structure generated, for each level of embedding we determine the local PRED value and collect the subcategorisable grammatical functions present at that level of embedding." ></td>
	<td class="line x" title="202:532	(page 72) Consider the automatically generated f-structure in Figure 4 for tree wsj 0003 22 in the Penn-II and Penn-III Treebanks." ></td>
	<td class="line x" title="203:532	It is crucial to note that in the automatically generated f-structures the value of the PRED feature is a lemma and not a semantic form." ></td>
	<td class="line x" title="204:532	Exploiting the information contained in the f-structure and applying the method described above, we recursively extract the following nonempty semantic forms: impose([subj, obj, obl:on]), in([obj]), of([obj]),andon([obj]).Ineffect, in both the approach of van Genabith, Sadler, and Way (1999) and our approach, semantic forms are reverse-engineered from automatically generated f-structures for treebank trees." ></td>
	<td class="line x" title="205:532	The automatically induced semantic forms contain the following subcategorizable syntactic functions: SUBJ OBJ OBJ2 OBL prep OBL2 COMP XCOMP PART PART is not a syntactic function in the strict sense, but we decided to capture the relevant co-occurrence patterns of verbs and particles in the semantic forms." ></td>
	<td class="line x" title="206:532	Just as Table 3 Precision and recall on automatically generated f-structures by feature against the DCU 105." ></td>
	<td class="line x" title="207:532	Feature Precision Recall F-score ADJUNCT 892/968 = 92 892/950 = 94 93 COMP 88/92 = 96 88/102 = 86 91 COORD 153/184 = 83 153/167 = 92 87 DET 265/267 = 99 265/269 = 99 99 OBJ 442/459 = 96 442/461 = 96 96 OBL 50/52 = 96 50/61 = 82 88 OBLAG 12/12 = 100 12/12 = 100 100 PASSIVE 76/79 = 96 76/80 = 95 96 RELMOD 46/48 = 96 46/50 = 92 94 SUBJ 396/412 = 96 396/414 = 96 96 TOPIC 13/13 = 100 13/13 = 100 100 TOPICREL 46/49 = 94 46/52 = 88 91 XCOMP 145/153 = 95 145/146 = 99 97 339 Computational Linguistics Volume 31, Number 3 Figure 4 Automatically generated f-structure and extracted semantic forms for the Penn-II Treebank string wsj 0003 22, In July, the Environmental Protection Agency imposed a gradual ban on virtually all uses of asbestos." ></td>
	<td class="line x" title="208:532	OBL prep includes the prepositional head of the PP, PART includes the actual particle which occurs, for example, add([subj, obj, part:up])." ></td>
	<td class="line x" title="209:532	In the work presented here, we substantially extend and scale the approach of van Genabith, Sadler, and Way (1999) in regard to coverage, granularity, and evaluation." ></td>
	<td class="line x" title="210:532	First, we scale the approach to the full WSJ section of the Penn-II Treebank and the parsed Brown corpus section of Penn-III, with a combined total of approximately 75,000 trees." ></td>
	<td class="line x" title="211:532	Van Genabith, Sadler, and Way (1999) was proof of concept on 100 trees." ></td>
	<td class="line x" title="212:532	Second, in contrast to the approach of van Genabith, Sadler, and Way (1999) (and many other approaches), our approach fully reflects long-distance dependencies, indicated in terms of traces in the Penn-II and Penn-III Treebanks and corresponding reentrancies at f-structure." ></td>
	<td class="line x" title="213:532	Third, in addition to abstract syntactic-functionbased subcategorization frames, we also compute frames for syntactic functionCFG category pairs, for both the verbal heads and their arguments, and also generate 340 ODonovan et al. Large-Scale Induction and Evaluation of Lexical Resources Table 4 Conflation of Penn Treebank tags." ></td>
	<td class="line x" title="214:532	Conflated Category Penn Treebank Category JJ JJ JJR JJS NNN NNS NNP NNPS PRP RB RB RBR RBS VVB VBD VBG VBN VBP VBZ MD pure CFG-based subcategorization frames." ></td>
	<td class="line x" title="215:532	Fourth, in contrast to the approach of van Genabith, Sadler, and Way (1999) (and many other approaches), our method differentiates between frames for active and passive constructions." ></td>
	<td class="line x" title="216:532	Fifth, in contrast to that of van Genabith, Sadler, and Way (1999), our method associates conditional probabilities with frames." ></td>
	<td class="line x" title="217:532	Sixth, we evaluate the complete set of semantic forms extracted (not just a selection) against the manually constructed COMLEX (MacLeod, Grishman, and Meyers 1994) resource." ></td>
	<td class="line x" title="218:532	In order to capture CFG-based categorial information, we add a CAT feature to the f-structures automatically generated from the Penn-II and Penn-III Treebanks." ></td>
	<td class="line x" title="219:532	Its value is the syntactic category of the lexical item whose lemma gives rise to the PRED value at that particular level of embedding." ></td>
	<td class="line x" title="220:532	This makes it possible to classify words and their semantic forms based on their syntactic category and reduces the risk of inaccurate assignment of subcategorization frame frequencies due to POS ambiguity, distinguishing, for example, between the nominal and verbal occurrences of the lemma fight." ></td>
	<td class="line x" title="221:532	With this, the output for the verb impose in Figure 4 is impose(v,[subj, obj, obl:on])." ></td>
	<td class="line x" title="222:532	For some of our experiments, we conflate the different verbal (and other) tags used in the Penn Treebanks to a single verbal marker (Table 4)." ></td>
	<td class="line x" title="223:532	As a further extension, the extraction procedure reads off the syntactic category of the head of each of the subcategorized syntactic functions: impose(v,[subj(n),obj(n),obl:on])." ></td>
	<td class="line x" title="224:532	3 In this way, our methodology is able to produce surface syntactic as well as abstract functional subcategorization details." ></td>
	<td class="line x" title="225:532	Dalrymple (2001) argues that there are cases, albeit exceptional ones, in which constraints on syntactic category are an issue in subcategorization." ></td>
	<td class="line x" title="226:532	In contrast to much of the work reviewed in Section 3, which limits itself to the extraction of surface syntactic subcategorization details, our system can provide this information as well as details of grammatical function." ></td>
	<td class="line x" title="227:532	3 We do not associate syntactic categories with OBLsastheyarealwaysPPs." ></td>
	<td class="line x" title="228:532	341 Computational Linguistics Volume 31, Number 3 Another way in which we develop and extend the basic extraction algorithm is to deal with passive voice and its effect on subcategorization behavior." ></td>
	<td class="line x" title="229:532	Consider Figure 5: Not taking into account that the example sentence is a passive construction, the extraction algorithm extracts outlaw([subj])." ></td>
	<td class="line x" title="230:532	This is incorrect, as outlaw is a transitive verb and therefore requires both a subject and an object to form a grammatical sentence in the active voice." ></td>
	<td class="line x" title="231:532	To cope with this problem, the extraction algorithm uses the feature-value pair passive:+, which appears in the f-structure at the level of embedding of the verb in question, to mark that predicate as occurring in the passive: outlaw([subj],p)." ></td>
	<td class="line x" title="232:532	The annotation algorithms accuracy in recognizing passive constructions is reflected by the f-score of 96% reported in Table 3 for the PASSIVE feature." ></td>
	<td class="line x" title="233:532	The syntactic functions COMP and XCOMP refer to clausal complements with different predicate control patterns as described in Section 2." ></td>
	<td class="line x" title="234:532	However, as it stands, neither of these functions betrays anything about the syntactic nature of the constructs in question." ></td>
	<td class="line x" title="235:532	Many lexicons, both automatically acquired and manually created, are more fine grained in their approaches to subcategorized clausal arguments, differentiating, for example, between a that-clause and a to + infinitive clause (Ushioda et al. 1993)." ></td>
	<td class="line x" title="236:532	With only a slight modification, our system, along with the details provided by the automatically generated f-structures, allows us to extract frames with an equivalent level of detail." ></td>
	<td class="line x" title="237:532	For example, to identify a that-clause, we use Figure 5 Automatically generated f-structure for the Penn-II Treebank string wsj 0003 23." ></td>
	<td class="line x" title="238:532	By 1997, almost all remaining uses of cancer-causing asbestos will be outlawed." ></td>
	<td class="line x" title="239:532	342 ODonovan et al. Large-Scale Induction and Evaluation of Lexical Resources Table 5 Semantic forms for the verb accept." ></td>
	<td class="line x" title="240:532	Semantic form Occurrences Conditional probability accept([subj, obj]) 122 0.813 accept ([subj]) 11 0.073 accept([subj, comp]) 5 0.033 accept([subj, obl:as]) 3 0.020 accept([subj, obj, obl:as]) 3 0.020 accept([subj, obj, obl:from]) 3 0.020 accept([subj, obj, obl:at]) 1 0.007 accept([subj, obj, obl:for]) 1 0.007 accept([subj, obj, xcomp]) 1 0.007 the feature-value pair that:+ at f-structure level to read off the following subcategorization frame for the verb add: add([subj,comp(that)])." ></td>
	<td class="line x" title="241:532	Using the feature-value pair to inf:+, we can identify to + infinitive clauses, resulting in the following frame for the verb want: want([subj,xcomp(to inf)])." ></td>
	<td class="line x" title="242:532	We can also derive control information about open complements." ></td>
	<td class="line x" title="243:532	In Figure 5, the reentrant XCOMP subject is identical to the subject of will in the matrix clause, which allows us to induce information about the nature of the external control of the XCOMP (i.e. , whether it is subject or object control)." ></td>
	<td class="line x" title="244:532	In order to estimate the likelihood of the co-occurrence of a predicate with a particular argument list, we compute conditional probabilities for subcategorization frames based on the number of token occurrences in the corpus: P(ArgList|) = count(ArgList) summationtext n i=1 count(ArgList i ) where ArgList 1 ArgList n are the possible argument lists which can occur for .Because of variations in verbal subcategorization across domains, probabilities are also useful for predicting the way in which verbs behave in certain contexts." ></td>
	<td class="line x" title="245:532	In Section 6, we use the conditional probabilities to filter possible error judgments by our system." ></td>
	<td class="line x" title="246:532	Tables 57 show, with varying levels of analysis, the attested semantic forms for the verb accept with their associated conditional probabilities." ></td>
	<td class="line x" title="247:532	The effect of differentiating between the active and passive occurrences of verbs can be seen in the different conditional probabilities associated with the intransitive frame ([subj]) of the verb accept (shown in boldface type) in Tables 5 and 6." ></td>
	<td class="line x" title="248:532	4 Table 7 shows the joint grammaticalfunction/syntactic-category-based subcategorization frames." ></td>
	<td class="line x" title="249:532	5." ></td>
	<td class="line x" title="250:532	Results We extract semantic forms for 4,362 verb lemmas from Penn-III." ></td>
	<td class="line x" title="251:532	Table 8 shows the number of distinct semantic form types (i.e. , lemma and argument list combination) 4 Given these, it is possible to condition frames on both lemma ()andvoice(v: active/passive): P(ArgList|, v) = count(ArgList, v) summationtext n i=1 count(ArgList i, v) 343 Computational Linguistics Volume 31, Number 3 Table 6 Semantic forms for the verb accept marked with p for passive use." ></td>
	<td class="line x" title="252:532	Semantic form Occurrences Conditional probability accept([subj, obj]) 122 0.813 accept ([subj],p) 9 0.060 accept([subj, comp]) 5 0.033 accept([subj, obl:as],p) 3 0.020 accept([subj, obj, obl:as]) 3 0.020 accept([subj, obj, obl:from]) 3 0.020 accept ([subj]) 2 0.013 accept([subj, obj, obl:at]) 1 0.007 accept([subj, obj, obl:for]) 1 0.007 accept([subj, obj, xcomp]) 1 0.007 Table 7 Semantic forms for the verb accept including syntactic category for each grammatical function." ></td>
	<td class="line x" title="253:532	Semantic form Occurrences Conditional probability accept([subj(n), obj(n)]) 116 0.773 accept([subj(n)]) 11 0.073 accept([subj(n), comp(that)]) 4 0.027 accept([subj(n), obj(n), obl:from]) 3 0.020 accept([subj(n), obl:as]) 3 0.020 Other 13 0.087 extracted." ></td>
	<td class="line x" title="254:532	Discriminating obliques by associated preposition and recording particle information, the algorithm finds a total of 21,005 semantic form types, 16,000 occurring in active voice and 5,005 in passive voice." ></td>
	<td class="line x" title="255:532	When the obliques are parameterized for prepositions and particles are included for particle verbs, we find an average of 4.82 semantic form types per verb." ></td>
	<td class="line x" title="256:532	Without the inclusion of details for individual prepositions or particles, there was an average of 3.45 semantic form types per verb." ></td>
	<td class="line x" title="257:532	Unlike many of the researchers whose work is reviewed in Section 3, we do not predefine the frames extracted by our system." ></td>
	<td class="line x" title="258:532	Table 9 shows the numbers of distinct frame types extracted from Penn-II, ignoring PRED values." ></td>
	<td class="line x" title="259:532	5 We provide two columns of statistics, one in which all oblique (PP) arguments are condensed into one OBL function and all particle arguments are condensed into part, and the other in which we differentiate among obl:to (e.g. , give), obl:on (e.g. , rely), obl:for (e.g. , compensate), etc. , and likewise for particles." ></td>
	<td class="line x" title="260:532	Collapsing obliques and particles into simple functions, we extract 38 frame types." ></td>
	<td class="line x" title="261:532	Discriminating particles and obliques by preposition, we extract 577 frame types." ></td>
	<td class="line x" title="262:532	Table 10 shows the same results for Penn-III, with 50 simple frame types and 1,084 types when parameterized for prepositions and particles." ></td>
	<td class="line x" title="263:532	We also show the result of applying absolute thresholding techniques to the semantic forms induced." ></td>
	<td class="line x" title="264:532	Applying an absolute threshold of five occurrences, we still generate 162 frame types 5 To recap, if two verbs have the same subcategorization requirements (e.g. , give([subj, obj, obj2]), send([subj, obj, obj2])), then that frame [subj, obj, obj2] is counted only once." ></td>
	<td class="line x" title="265:532	344 ODonovan et al. Large-Scale Induction and Evaluation of Lexical Resources Table 8 Number of semantic form types for Penn-III." ></td>
	<td class="line x" title="266:532	Without prepositions and particles With prepositions and particles Semantic form types 15,166 21,005 Active 11,038 16,000 Passive 4,128 5,005 Table 9 Number of frame types for verbs for Penn-II." ></td>
	<td class="line x" title="267:532	Without prepositions With prepositions and particles and particles Number of frame types 38 577 Number of singletons 1 243 Number occurring twice 1 84 Number occurring five or fewer times 7 415 Number occurring more than five times 31 162 from Penn-II and 221 from Penn-III." ></td>
	<td class="line x" title="268:532	Briscoe and Carroll (1997), by comparison, employ 163 distinct predefined frames." ></td>
	<td class="line x" title="269:532	6." ></td>
	<td class="line x" title="270:532	Evaluation Most of the previous approaches discussed in Section 3 have been evaluated to different degrees." ></td>
	<td class="line x" title="271:532	In general, a small number of frequently occurring verbs is selected, and the subcategorization frames extracted for these verbs (from some quantity of unseen test data) are compared to a gold standard." ></td>
	<td class="line x" title="272:532	The gold standard is either manually custom-made based on the test data or adapted from an existing external resource such as the OALD (Hornby 1980) or COMLEX (MacLeod, Grishman, and Meyers 1994)." ></td>
	<td class="line x" title="273:532	There are advantages and disadvantages to both types of gold standard." ></td>
	<td class="line x" title="274:532	While it is time-consuming to manually construct a custom-made standard, the resulting standard has the advantage of containing only the subcategorization frames exhibited in the test data." ></td>
	<td class="line x" title="275:532	Using an existing externally produced resource is quicker, but the gold Table 10 Number of frame types for verbs for Penn-III." ></td>
	<td class="line x" title="276:532	Without prepositions With prepositions and particles and particles Number of frame types 50 1,084 Number of singletons 6 544 Number occurring twice 2 147 Number occurring five or fewer times 12 863 Number occurring more than five times 38 221 345 Computational Linguistics Volume 31, Number 3 standard may contain many more frames than those which occur in the data from which the test lexicon is induced or, indeed, may omit relevant correct frames contained in the data." ></td>
	<td class="line x" title="277:532	As a result, systems generally score better against custom-made, manually established gold standards." ></td>
	<td class="line x" title="278:532	Carroll and Rooth (1998) achieve an F-score of 77% against the OALD when they evaluate a selection of 100 verbs with absolute frequency of greater than 500 each." ></td>
	<td class="line x" title="279:532	Their system recognizes 15 frames, and these do not contain details of subcategorizedfor prepositions." ></td>
	<td class="line x" title="280:532	Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3." ></td>
	<td class="line x" title="281:532	Sarkar and Zeman (2000) evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88%." ></td>
	<td class="line x" title="282:532	However, their evaluation does not examine the extracted subcategorization frames but rather the argumentadjunct distinctions posited by their system." ></td>
	<td class="line x" title="283:532	The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German." ></td>
	<td class="line x" title="284:532	She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden (Dudenredaktion 2001)." ></td>
	<td class="line x" title="285:532	We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3." ></td>
	<td class="line x" title="286:532	We carried out a large-scale evaluation of our automatically induced lexicon (2,993 active verb lemmas for Penn-II and 3,529 for Penn-III, as well as 1,422 passive verb lemmas from Penn-II) against the COMLEX resource." ></td>
	<td class="line x" title="287:532	To our knowledge this is the most extensive evaluation ever carried out for English lexical extraction." ></td>
	<td class="line x" title="288:532	We conducted a number of experiments on the subcategorization frames extracted from Penn-II and Penn-III which are described and discussed in Sections 6.2, 6.3, and 6.4." ></td>
	<td class="line x" title="289:532	Finding a common format for the gold standard and induced lexical entries is a nontrivial task." ></td>
	<td class="line x" title="290:532	To ensure that we did not bias the evaluation in favor of either resource, we carried out two different mappings for the frames from Penn-II and Penn-III: COMLEX-LFG Mapping I and COMLEX-LFG Mapping II." ></td>
	<td class="line x" title="291:532	For each mapping we carried out six basic experiments (and two additional ones for COMLEX-LFG Mapping II) for the active subcategorization frames extracted." ></td>
	<td class="line x" title="292:532	Within each experiment, the following factors were varied: level of prepositional phrase detail, level of particle detail, relative threshold (1% or 5%), and incorporation of an expanded set of directional prepositions." ></td>
	<td class="line x" title="293:532	Using the second mapping we also evaluated the automatically extracted passive frames and experimented with absolute thresholds." ></td>
	<td class="line x" title="294:532	Direct comparison of subcategorization frame acquisition systems is difficult because of variations in the number of frames extracted, the number of test verbs, the gold standards used, the size of the test data, and the level of detail in the subcategorization frames (e.g. , whether they are parameterized for specific prepositions)." ></td>
	<td class="line x" title="295:532	Therefore, in order to establish a baseline against which to compare our results, following Schulte in Walde (2002b), we assigned the two most frequent frame types (transitive and intransitive) by default to each verb and compared this artificial lexicon to the gold standard." ></td>
	<td class="line x" title="296:532	The section concludes with a full discussion of the reported results." ></td>
	<td class="line x" title="297:532	6.1 COMLEX We evaluate our induced semantic forms against COMLEX (MacLeod, Grishman, and Meyers 1994), a computational machine-readable lexicon containing syntactic information for approximately 38,000 English headwords." ></td>
	<td class="line x" title="298:532	Its creators paid particular attention to the encoding of more detailed subcategorization information than is available in either the OALD or the LDOCE (Proctor 1978), both for verbs and for nouns 346 ODonovan et al. Large-Scale Induction and Evaluation of Lexical Resources Figure 6 Intersection between active-verb lemma types in COMLEX and the Penn-II-induced lexicon." ></td>
	<td class="line x" title="299:532	and adjectives which take complements (Grishman, MacLeod, and Meyers 1994)." ></td>
	<td class="line x" title="300:532	By choosing to evaluate against COMLEX, we set our sights high: Our extracted semantic forms are fine-grained, and COMLEX is considerably more detailed than the OALD or LDOCE used for earlier evaluations." ></td>
	<td class="line x" title="301:532	While our system can generate semantic forms for any lemma (regardless of part of speech) which induces a PRED value, we have thus far evaluated the automatic generation of subcategorization frames for verbs only." ></td>
	<td class="line x" title="302:532	COMLEX defines 138 distinct verb frame types without the inclusion of specific prepositions or particles." ></td>
	<td class="line x" title="303:532	As COMLEX contains information other than subcategorization details, it was necessary for us to extract the subcategorization frames associated with each verbal lexicon entry." ></td>
	<td class="line x" title="304:532	The following is a sample entry for the verb reimburse: (VERB :ORTH reimburse:SUBC ((NP-NP) (NP-PP :PVAL (for)) (NP))) Each entry is organized as a nested set of typed feature-value lists." ></td>
	<td class="line x" title="305:532	The first symbol (i.e. , VERB) gives the part of speech." ></td>
	<td class="line x" title="306:532	The value of the :ORTH feature is the base form of the verb." ></td>
	<td class="line x" title="307:532	Any entry with irregular morphological behavior will also include the features :PLURAL,:PAST, and so on, with the relevant values." ></td>
	<td class="line x" title="308:532	All verbs have a :SUBC feature, and for our purposes, this is the most interesting feature." ></td>
	<td class="line x" title="309:532	In the case of the example above, the subcategorization values specify that reimburse can occur with two object noun phrases (NP-NP), an object noun phrase followed by a prepositional phrase headed by for (NP-PP :PVAL (for)) or just an object noun phrase (NP)." ></td>
	<td class="line x" title="310:532	(Note that the details of the subject are not included in COMLEX frames)." ></td>
	<td class="line x" title="311:532	What makes the COMLEX resource particularly suitable for our evaluation is that each of the complement types (NP-NP, NP-PP,andNP) which make up the value of the :SUBC feature is associated with a formal frame definition which looks like the following: (vp-frame np-np :cs ((np 2)(np 3)) :gs (:subject 1 :obj 2 :obj2 3) :ex she asked him his name) The value of the :cs feature is the constituent structure of the subcategorization frame, which lists the syntactic CF-PSG constituents in sequence (omitting the subject, again)." ></td>
	<td class="line x" title="312:532	The value of the :gs feature is the grammatical structure which indicates the functional role played by each of the CF-PSG constituents." ></td>
	<td class="line x" title="313:532	The elements of the 347 Computational Linguistics Volume 31, Number 3 Figure 7 Intersection between active-verb lemma types in COMLEX and the Penn-III-induced lexicon." ></td>
	<td class="line x" title="314:532	constituent structure are indexed, and these indices are referenced in the :gs field." ></td>
	<td class="line x" title="315:532	The index 1 always refers to the surface subject of the verb." ></td>
	<td class="line x" title="316:532	This mapping between constituent structure and functional structure makes the information contained in COMLEX particularly suitable as an evaluation standard for the LFG semantic forms which we induce." ></td>
	<td class="line x" title="317:532	We present the evaluation for the verbs which occur in an active context in the treebank." ></td>
	<td class="line x" title="318:532	COMLEX does not provide passive frames." ></td>
	<td class="line x" title="319:532	For Penn-II, there are 2,993 verb lemmas (used actively) that both resources have in common." ></td>
	<td class="line x" title="320:532	2,669 verb lemmas appear in COMLEX but not in the induced lexicon, and 416 verb lemmas (used actively) appear in the induced lexicon but not in COMLEX (Figure 6)." ></td>
	<td class="line x" title="321:532	For Penn-III, COMLEX and the induced lexicon share 3,529 verb lemmas (used actively)." ></td>
	<td class="line x" title="322:532	This is shown in Figure 7." ></td>
	<td class="line x" title="323:532	6 6.2 COMLEX-LFG Mapping I and Penn-II In order to carry out the evaluation, we have to find a common format for the expression of subcategorization information between our induced LFG-style subcategorization frames and those contained in COMLEX." ></td>
	<td class="line x" title="324:532	The following are the common syntactic functions: SUBJ, OBJ, OBJ i, COMP,andPART." ></td>
	<td class="line x" title="325:532	Unlike our system, COMLEX does not distinguish an OBL from an OBJ i, so we converted all the obliques in the induced frames to OBJ i . As in COMLEX, the value of i depends on the number of objects/obliques already present in the semantic form." ></td>
	<td class="line x" title="326:532	COMLEX does not differentiate between COMPs and XCOMPs as our system does (control information is expressed in a different way: see Section 6.3), so we conflate our two LFG categories to that of COMP." ></td>
	<td class="line x" title="327:532	The process is summarized in Table 11." ></td>
	<td class="line x" title="328:532	The manually constructed COMLEX entries provide a gold standard against which we evaluate the automatically induced frames." ></td>
	<td class="line x" title="329:532	We calculate the number of true positives (tps) (where our semantic forms and those from COMLEX are the same), the number of false negatives ( fns) (those frames which appeared in COMLEX but were not produced by our system), and the number of false positives ( fps) (those frames 6 Given these figures, one might begin to wonder about the value of automatic induction." ></td>
	<td class="line x" title="330:532	First, COMLEX does not rank frames by probabilities, which are essential in disambiguation." ></td>
	<td class="line x" title="331:532	Second, the coverage of COMLEX is not complete: 518 lemmas discovered by the induction experiment are not listed in COMLEX; see the error analysis in Section 6.5." ></td>
	<td class="line x" title="332:532	348 ODonovan et al. Large-Scale Induction and Evaluation of Lexical Resources Table 11 Mapping I: Merging of COMLEX and LFG syntactic functions." ></td>
	<td class="line x" title="333:532	Our syntactic functions COMLEX syntactic functions Merged function SUBJ Subject SUBJ OBJ Object OBJ OBJ2 Obj2 OBJ i OBL Obj3 OBL2 Obj4 COMP Comp COMP XCOMP PART Part PART produced by our system which do not appear in COMLEX)." ></td>
	<td class="line x" title="334:532	We calculate precision, recall, and F-score using the following standard equations: recall = tp tp + fn precision = tp tp + fp f-score = 2  recall  precision recall + precision We use the frequencies associated with each of our semantic forms in order to set a relative threshold to filter the selection of semantic forms." ></td>
	<td class="line x" title="335:532	For a threshold of 1% we disregard any semantic forms with a conditional probability (i.e. , given a lemma) of less than or equal to 0.01." ></td>
	<td class="line x" title="336:532	As some verbs occur less frequently than others, we think it is important to use a relative rather than absolute threshold (as in Carroll and Rooth [1998], for instance) in this way." ></td>
	<td class="line x" title="337:532	We carried out the evaluation in a similar way to Schulte im Waldes (2002b) for German, the only experiment comparable in scale to ours." ></td>
	<td class="line x" title="338:532	Despite the obvious differences in approach and language, this allows us to make some tentative comparisons between our respective results." ></td>
	<td class="line x" title="339:532	The statistics shown in Table 12 give the results of three different experiments with the relative threshold set to 1%." ></td>
	<td class="line x" title="340:532	As for all the results tables, the baseline statistics (simply assigning the most frequent frames, in this case transitive and intransitive, to each lemma by default) are in each case shown in the left column, and the results achieved by our induced lexicon are presented in the right column." ></td>
	<td class="line x" title="341:532	Distinguishing between complement and adjunct prepositional phrases is a notoriously difficult aspect of automatic subcategorization frame acquisition." ></td>
	<td class="line x" title="342:532	For this reason, following the evaluation setup in Schulte im Walde (2002b), the three experiments vary with respect to the amount of prepositional information contained in the subcategorization frames." ></td>
	<td class="line x" title="343:532	Experiment 1." ></td>
	<td class="line x" title="344:532	Here we excluded subcategorized prepositional-phrase arguments entirely from the comparison." ></td>
	<td class="line x" title="345:532	In a manner similar to that of Schulte im Walde (2002b), any 349 Computational Linguistics Volume 31, Number 3 Table 12 Results of Penn-II evaluation of active frames against COMLEX (relative threshold of 1%)." ></td>
	<td class="line x" title="346:532	Precision Recall F-score Mapping I Baseline Induced Baseline Induced Baseline Induced Experiment 1 66.1% 75.2% 65.8% 69.1% 66.0% 72.0% Experiment 2 71.5% 65.5% 64.3% 63.1% 67.7% 64.3% Experiment 3 64.7% 71.8% 11.9% 16.8% 20.1% 27.3% frames containing an OBL were mapped to the same frame type minus that argument." ></td>
	<td class="line x" title="347:532	For example, the frame [subj,obl:for] becomes [subj]." ></td>
	<td class="line x" title="348:532	Using a relative threshold of 1% (Table 12), our results (precision of 75.2%, recall of 69.1%, and F-score of 72.0%) are remarkably similar to those of Schulte im Walde (2002b), who reports precision of 74.53%, recall of 69.74%, and an f-score of 72.05%." ></td>
	<td class="line x" title="349:532	Experiment 2." ></td>
	<td class="line x" title="350:532	Here we include subcategorized prepositional phrase arguments but only in their simplest form; that is, they were not parameterized for particular prepositions." ></td>
	<td class="line x" title="351:532	For example, the frame [subj,obl:for] is rewritten as [subj,obl].Usinga relative threshold of 1% (Table 12), our results (precision of 65.5%, recall of 63.1%, and F-score of 64.3%) compare favorably to those of Schulte im Walde (2002b), who recorded precision of 60.76%, recall of 63.91%, and an F-score of 62.30%." ></td>
	<td class="line x" title="352:532	Experiment 3." ></td>
	<td class="line x" title="353:532	Here we used semantic forms which contain details of specific prepositions for any subcategorized prepositional phrase (e.g. , [subj,obl:for])." ></td>
	<td class="line x" title="354:532	Using a relative threshold of 1% (Table 12), our precision figure (71.8%) is quite high (in comparison to 65.52% as recorded by Schulte im Walde [2002b])." ></td>
	<td class="line x" title="355:532	However our recall (16.8%) is very low (compared to the 50.83% that Schulte im Walde [2002b] reports)." ></td>
	<td class="line x" title="356:532	Consequently our F-score (27.3%) is also low (Schulte im Walde [2002b] records an F-score of 57.24%)." ></td>
	<td class="line x" title="357:532	The reason for this is discussed in Section 6.2.1." ></td>
	<td class="line x" title="358:532	The statistics in Table 13 are the result of the second experiment, in which the relative threshold was increased to 5%." ></td>
	<td class="line x" title="359:532	The effect of such an increase is obvious in that precision goes up (by as much as 5%) for each of the three evaluations while recall goes down (by as much as 5.5%)." ></td>
	<td class="line x" title="360:532	This is to be expected, as a greater threshold means that there are fewer semantic forms associated with each verb in the induced lexicon, but they are more likely to be correct because of their greater frequency of occurrence." ></td>
	<td class="line x" title="361:532	The conditional probabilities we associate with each semantic form together with thresholding can be used to customize the induced lexicon to the task for which it is required, that is, whether a very precise lexicon is preferred to one with broader Table 13 Results of Penn-II evaluation of active frames against COMLEX (relative threshold of 5%)." ></td>
	<td class="line x" title="362:532	Precision Recall F-score Mapping I Baseline Induced Baseline Induced Baseline Induced Experiment 1 66.1% 80.2% 65.8% 63.6% 66.0% 70.9% Experiment 2 71.5% 69.6% 64.3% 56.9% 67.7% 62.7% Experiment 3 64.7% 76.7% 11.9% 13.9% 20.1% 23.5% 350 ODonovan et al. Large-Scale Induction and Evaluation of Lexical Resources coverage." ></td>
	<td class="line x" title="363:532	In Tables 12 and 13, the baseline is exceeded in all experiments with the exception of Experiment 2." ></td>
	<td class="line x" title="364:532	This can be attributed to Mapping I, in which OBL i becomes OBJ i (Table 11)." ></td>
	<td class="line x" title="365:532	Experiment 2 includes obliques without the specific preposition, meaning that in this mapping, the frame [subj,obj:with] becomes [subj,obj]." ></td>
	<td class="line x" title="366:532	Therefore, the transitive baseline frame scores better than it should against the gold standard." ></td>
	<td class="line x" title="367:532	A more fine-grained LFG-COMLEX mapping in which this effect disappears is presented in Section 6.3." ></td>
	<td class="line x" title="368:532	6.2.1 Directional Prepositions." ></td>
	<td class="line x" title="369:532	Our recall statistic was particularly low in the case of evaluation using details of prepositions (Experiment 3, Tables 12 and 13)." ></td>
	<td class="line x" title="370:532	This can be accounted for by the fact that the creators of COMLEX have chosen to err on the side of overgeneration in regard to the list of prepositions which may occur with a verb and a subcategorization frame containing a prepositional phrase." ></td>
	<td class="line x" title="371:532	This is particularly true of directional prepositions." ></td>
	<td class="line x" title="372:532	For COMLEX, a list of 31 directional prepositions (Table 14) was prepared and assigned in its entirety by default to any verb which can potentially appear with any directional preposition in order to save time and avoid the risk of missing prepositions." ></td>
	<td class="line x" title="373:532	Grishman, MacLeod, and Meyers (1994) acknowledge that this can lead to a preposition list which is a little rich for a particular verb, but this is the approach they have chosen to take." ></td>
	<td class="line x" title="374:532	In a subsequent experiment, we incorporated this list of directional prepositions by default into our semantic form induction process in the same way as the creators of COMLEX have done." ></td>
	<td class="line x" title="375:532	Table 15 shows that doing so results in a significant improvement in the recall statistic (45.1%), as would be expected, with the new statistic being almost three times as good as the result reported in Table 12 for Experiment 3 (16.8%)." ></td>
	<td class="line x" title="376:532	There is also an improvement in the precision figure (from 71.8% to 86.9%)." ></td>
	<td class="line x" title="377:532	This is due to a substantial increase in the number of true positives (from 5,612 to 14,675) compared with a stationary false positive figure (2,205 in both cases)." ></td>
	<td class="line x" title="378:532	The f-score increases from 27.3% to 59.4%." ></td>
	<td class="line x" title="379:532	6.3 COMLEX-LFG Mapping II and Penn-II The COMLEX-LFG Mapping I presented above establishes a least common denominator for the COMLEX and our LFG-inspired resources." ></td>
	<td class="line x" title="380:532	More-fine-grained mappings are possible: in order to ensure that the mapping from our semantic forms to the COMLEX frames did not oversimplify the information in the automatically extracted subcategorization frames, we conducted a further set of experiments in which we converted the information in the COMLEX entries to the format of our extracted semantic forms." ></td>
	<td class="line x" title="381:532	We explicitly differentiated between OBLsandOBJs by automatically Table 14 COMLEX directional prepositions." ></td>
	<td class="line x" title="382:532	about across along around behind below beneath between beyond by down from in inside into off on onto out out of outside over past through throughout to toward toward up up to via 351 Computational Linguistics Volume 31, Number 3 Table 15 Penn-II evaluation of active frames against COMLEX using p-dir list (relative threshold of 1%)." ></td>
	<td class="line x" title="383:532	Mapping I Precision Recall F-score Experiment 3 86.9% 45.1% 59.4% deducing whether a COMLEX OBJ i was coindexed with an NP or a PP." ></td>
	<td class="line x" title="384:532	Furthermore, as can be seen in the following example, COMLEX frame definitions contain details of the control patterns of sentential complements, encoded using the :features attribute." ></td>
	<td class="line x" title="385:532	This allows for automatic discrimination between COMPsandXCOMPs." ></td>
	<td class="line x" title="386:532	(vp-frame to-inf-sc :cs (vp 2 :mood to-infinitive :subject 1) :features (:control subject) :gs (:subject 1 :comp 2) :ex I wanted to come) The mapping is summarized in Table 16." ></td>
	<td class="line x" title="387:532	The results of the subsequent evaluation are presented in Tables 17 and 18." ></td>
	<td class="line x" title="388:532	We have added Experiments 2a and 3a." ></td>
	<td class="line x" title="389:532	These are the same as Experiments 2 and 3, except that they additionally include the specific particle with each PART function." ></td>
	<td class="line x" title="390:532	While the recall figures in Tables 17 and 18 are slightly lower than those in Tables 12 and 13, changing the mapping in this way results in an increase in precision in each case (by as much as 11.6%)." ></td>
	<td class="line x" title="391:532	The results of the lexical evaluation are consistently better than the baseline, in some cases by almost 16% (Experiment 2, threshold 5%)." ></td>
	<td class="line x" title="392:532	Notice that in contrast to Tables 12 and 13, in the more-fine-grained COMLEX-LFG Mapping II presented here, all experiments exceed the baseline." ></td>
	<td class="line x" title="393:532	6.3.1 Directional Prepositions." ></td>
	<td class="line x" title="394:532	The recall figures for Experiments 3 and 3a in Table 17 (24.0% and 21.5%) and Table 18 (19.7% and 17.4%) drop in a similar fashion to the results seen in Tables 12 and 13." ></td>
	<td class="line x" title="395:532	For this reason, we again incorporated the list of 31 directional prepositions (Table 14) by default and reran Experiments 3 and 3a for a threshold of 1%." ></td>
	<td class="line x" title="396:532	The results are presented in Table 19." ></td>
	<td class="line x" title="397:532	The effect was as expected: The recall scores for the two experiments increased to 40.8% and 35.4% (from 24.0% and 22.5%), and the F-scores increased to 54.4% and 49.7% (from 35.9% and 33.0%)." ></td>
	<td class="line x" title="398:532	6.3.2 Passive Evaluation." ></td>
	<td class="line x" title="399:532	Table 20 presents the results of evaluating the extracted passive semantic forms for 1,422 verb lemmas shared by the induced lexicon and COMLEX." ></td>
	<td class="line x" title="400:532	Table 16 Mapping II: Merging of COMLEX and LFG syntactic functions." ></td>
	<td class="line x" title="401:532	Our syntactic functions COMLEX syntactic functions Merged function SUBJ Subject SUBJ OBJ Object OBJ OBJ2 Obj2 OBJ2 OBL Obj3 OBL OBL2 Obj4 OBL2 COMP Comp COMP XCOMP Comp XCOMP PART Part PART 352 ODonovan et al. Large-Scale Induction and Evaluation of Lexical Resources Table 17 Results of Penn-II evaluation of active frames against COMLEX (relative threshold of 1%)." ></td>
	<td class="line x" title="402:532	Precision Recall F-score Mapping II Baseline Induced Baseline Induced Baseline Induced Experiment 1 72.1% 79.0% 58.5% 59.6% 64.6% 68.0% Experiment 2 65.2% 77.1% 37.4% 50.4% 47.5% 61.0% Experiment 2a 65.2% 76.4% 32.7% 44.5% 43.6% 56.3% Experiment 3 65.2% 75.9% 15.2% 24.0% 24.7% 35.9% Experiment 3a 65.2% 71.0% 13.6% 21.5% 22.5% 33.0% Table 18 Results of Penn-II evaluation of active frames against COMLEX (relative threshold of 5%)." ></td>
	<td class="line x" title="403:532	Precision Recall F-score Mapping II Baseline Induced Baseline Induced Baseline Induced Experiment 1 72.1% 83.5% 58.5% 54.7% 64.6% 66.1% Experiment 2 65.2% 81.4% 37.4% 44.8% 47.5% 57.8% Experiment 2a 65.2% 80.9% 32.7% 39.0% 43.6% 52.6% Experiment 3 65.2% 75.9% 15.2% 19.7% 24.7% 31.3% Experiment 3a 65.2% 75.5% 13.6% 17.4% 22.5% 28.3% We applied lexical-redundancy rules (Kaplan and Bresnan 1982) to automatically convert the active COMLEX frames to their passive counterparts: For example, subjects are demoted to optional by oblique agents, and direct objects become subjects." ></td>
	<td class="line x" title="404:532	The resulting precision was very high (from 72.3% to 80.2%), and there was the expected drop in recall when prepositional details were included (from 54.7% to 29.3%)." ></td>
	<td class="line x" title="405:532	Table 19 Penn-II evaluation of active frames against COMLEX using p-dir list (relative threshold of 1%)." ></td>
	<td class="line x" title="406:532	Mapping II Precision Recall F-score Experiment 3 81.7% 40.8% 54.4% Experiment 3a 83.1% 35.4% 49.7% Table 20 Results of Penn-II evaluation of passive frames (relative threshold of 1%)." ></td>
	<td class="line x" title="407:532	Passive Precision Recall F-score Experiment 2 80.2% 54.7% 65.1% Experiment 2a 79.7% 46.2% 58.5% Experiment 3 72.6% 33.4% 45.8% Experiment 3a 72.3% 29.3% 41.7% 353 Computational Linguistics Volume 31, Number 3 6.3.3 Absolute Thresholds." ></td>
	<td class="line x" title="408:532	Many of the previous approaches discussed in Section 3 use a limited number of verbs for evaluation, based on the verbs absolute frequency in the corpus." ></td>
	<td class="line x" title="409:532	We carried out a similar experiment." ></td>
	<td class="line x" title="410:532	Table 21 shows the results of Experiment 2 for all verbs, for the verb lemmas with an absolute frequency greater than 100, and for verbs with a frequency greater than 200." ></td>
	<td class="line x" title="411:532	The use of an absolute threshold results in an increase in precision (from 77.1% to 82.3% and 81.7%), an increase in recall (from 50.4% to 60.8% to 58.7%), and an overall increase in F-score (from 61.0% to 69.9% and 68.4%)." ></td>
	<td class="line x" title="412:532	6.4 Penn-III (Mapping-II) Recently we have applied our methodology to the Penn-III Treebank, a more balanced corpus resource with a number of text genres." ></td>
	<td class="line x" title="413:532	Penn-III consists of the WSJ section from Penn-II as well as a parse-annotated subset of the Brown corpus." ></td>
	<td class="line x" title="414:532	The Brown corpus comprises 24,242 trees compiled from a variety of text genres including popular lore, general fiction, science fiction, mystery and detective fiction, and humor." ></td>
	<td class="line x" title="415:532	It has been shown (Roland and Jurafsky 1998) that the subcategorization tendencies of verbs vary across linguistic domains." ></td>
	<td class="line x" title="416:532	Our aim, therefore, is to increase the scope of the induced lexicon not only in terms of the verb lemmas for which there are entries, but also in terms of the frames with which they co-occur." ></td>
	<td class="line x" title="417:532	The f-structure annotation algorithm was extended with only minor amendments to cover the parsed Brown corpus." ></td>
	<td class="line x" title="418:532	The most important of these was the way in which we distinguish between oblique and adjunct." ></td>
	<td class="line x" title="419:532	We noted in Section 4 that our method of assigning an oblique annotation in Penn-II was precise, albeit conservative." ></td>
	<td class="line x" title="420:532	Because of a change of annotation policy in Penn-III, the -CLR tag (indicating a close relationship between a PP and the local syntactic head), information which we had previously exploited, is no longer used." ></td>
	<td class="line x" title="421:532	For Penn-III the algorithm annotates all PPs which do not carry a Penn adverbial functional tag (such as -TMP or -LOC) and occur as the sisters of the verbal head of a VP as obliques." ></td>
	<td class="line x" title="422:532	In addition, the algorithm annotates as obliques PPs associated with -PUT (locative complements of the verb put) or -DTV (second object in ditransitives) tags." ></td>
	<td class="line x" title="423:532	When evaluating the application of the lexical extraction system on Penn-III, we carried out two sets of experiments, identical in each case to those described for Penn-II in Section 6.3, including the use of relative (1% and 5%) rather than absolute thresholds." ></td>
	<td class="line x" title="424:532	For the first set of experiments we evaluated the lexicon induced from the parseannotated Brown corpus only." ></td>
	<td class="line x" title="425:532	This evaluation was performed for 2,713 active-verb lemmas using the more fine-grained Mapping-II." ></td>
	<td class="line x" title="426:532	Tables 22 and 23 show that the results generally exceed the baseline, in some cases by almost 10%, similar to those recorded for Penn-II (Tables 17 and 18)." ></td>
	<td class="line x" title="427:532	While the precision is slightly lower than that reported for the experiments in Tables 17 and 18, in particular for Experiments 2, 2a, 3, Table 21 Penn-II evaluation of active frames against COMLEX using absolute thresholds (Experiment 2)." ></td>
	<td class="line x" title="428:532	Threshold Precision Recall F-score All 77.1% 50.4% 61.0% Threshold 100 82.3% 60.8% 69.9% Threshold 200 81.7% 58.7% 68.4% 354 ODonovan et al. Large-Scale Induction and Evaluation of Lexical Resources Table 22 Results of Penn-III active frames (Brown Corpus only) COMLEX comparison (relative threshold of 1%)." ></td>
	<td class="line x" title="429:532	Precision Recall F-Score Mapping II Baseline Induced Baseline Induced Baseline Induced Experiment 1 73.2% 79.2% 60.1% 60.0% 66.0% 68.2% Experiment 2 66.0% 70.5% 37.5% 50.5% 47.8% 58.9% Experiment 2a 66.0% 71.3% 32.7% 44.5% 43.7% 54.8% Experiment 3 66.0% 64.3% 15.2% 23.1% 24.8% 34.0% Experiment 3a 66.0% 64.1% 13.5% 20.7% 22.4% 31.3% and 3a, in which details of obliques are included, the recall in each of these experiments is slightly higher than that recorded for Penn-II." ></td>
	<td class="line x" title="430:532	We conjecture that the main reason for this is that the amended approach to the annotation of obliques is slightly less precise and conservative than the largely -CLR-tag-driven approach taken for Penn-II." ></td>
	<td class="line x" title="431:532	Consequently we record an increase in recall and a drop in precision." ></td>
	<td class="line x" title="432:532	This trend is repeated in the second set of experiments." ></td>
	<td class="line x" title="433:532	In this instance, we combined the lexicon extracted from the WSJ with that extracted from the parse-annotated Brown corpus, and evaluated the resulting resource for 3,529 active-verb lemmas." ></td>
	<td class="line x" title="434:532	The results are shown in Tables 24 and 25." ></td>
	<td class="line x" title="435:532	The results compare very positively against the baseline." ></td>
	<td class="line x" title="436:532	The precision scores are lower (by between 1.5% and 9.7%) than those reported for Penn-II (Tables 17 and 18)." ></td>
	<td class="line x" title="437:532	There has however been a significant increase in recall (up to 8.7%) and an overall increase in F-score (by up to 4.4%)." ></td>
	<td class="line x" title="438:532	6.5 Error Analysis and Discussion The work presented in this section highlights a number of issues associated with the evaluation of automatically induced subcategorization frames against an existing external gold standard, in this case COMLEX." ></td>
	<td class="line x" title="439:532	While this evaluation approach is arguably less labor-intensive than the manual construction of a custom-made gold standard, it does introduce a number of difficulties into the evaluation procedure." ></td>
	<td class="line x" title="440:532	It is a nontrivial task to convert both the gold standard and the induced resource to a common Table 23 Results of Penn-III active frames (Brown corpus only) COMLEX comparison (relative threshold of 5%)." ></td>
	<td class="line x" title="441:532	Precision Recall F-score Mapping II Baseline Induced Baseline Induced Baseline Induced Experiment 1 73.2% 82.7% 60.1% 56.4% 66.0% 67.0% Experiment 2 66.0% 74.6% 37.5% 46.1% 47.8% 57.0% Experiment 2a 66.0% 76.0% 32.7% 40.0% 43.7% 52.4% Experiment 3 66.0% 69.2% 15.2% 18.7% 24.8% 29.5% Experiment 3a 66.0% 69.0% 13.5% 16.6% 22.4% 26.7% 355 Computational Linguistics Volume 31, Number 3 Table 24 Results of Penn-III active frames (Brown and WSJ) COMLEX comparison (relative threshold of 1%)." ></td>
	<td class="line x" title="442:532	Precision Recall F-score Mapping II Baseline Induced Baseline Induced Baseline Induced Experiment 1 71.2% 77.4% 62.9% 66.2% 66.8% 71.4% Experiment 2 64.5% 70.4% 40.0% 58.0% 49.3% 63.6% Experiment 2a 64.5% 71.5% 35.1% 51.9% 45.5% 60.2% Experiment 3 64.5% 66.2% 17.0% 27.4% 26.8% 38.8% Experiment 3a 64.5% 66.0% 15.1% 24.8% 24.5% 36.0% format in order to facilitate evaluation." ></td>
	<td class="line x" title="443:532	In addition, as our results show, the choice of common format and mapping to it can affect the results." ></td>
	<td class="line x" title="444:532	In COMLEX-LFG Mapping I (Section 6.2), we found that mapping from the induced lexicon to COMLEX resulted in higher recall scores than those achieved when we (effectively) reversed the mapping (COMLEX-LFG Mapping II [Section 6.3])." ></td>
	<td class="line x" title="445:532	The first mapping is essentially a conflation of our more fine-grained LFG grammatical functions with the more generic COMLEX functions, while the second mapping tries to maintain as many distinctions as possible." ></td>
	<td class="line x" title="446:532	Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data." ></td>
	<td class="line x" title="447:532	As noted above, it is well documented (Roland and Jurafsky 1998) that subcategorization frames (and their frequencies) vary across domains." ></td>
	<td class="line x" title="448:532	We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ." ></td>
	<td class="line x" title="449:532	For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon." ></td>
	<td class="line x" title="450:532	It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases." ></td>
	<td class="line x" title="451:532	This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs." ></td>
	<td class="line x" title="452:532	Lexicographers were allowed to extrapolate from the citations found, a procedure Table 25 Results of Penn-III active frames (Brown and WSJ) COMLEX comparison (relative threshold of 5%)." ></td>
	<td class="line x" title="453:532	Precision Recall F-score Mapping II Baseline Induced Baseline Induced Baseline Induced Experiment 1 71.2% 82.0% 62.9% 61.0% 66.8% 69.9% Experiment 2 64.5% 74.3% 40.0% 53.5% 49.3% 62.2% Experiment 2a 64.5% 76.4% 35.1% 45.1% 45.5% 56.7% Experiment 3 64.5% 71.1% 17.0% 21.5% 26.8% 33.0% Experiment 3a 64.5% 70.8% 15.1% 19.2% 24.5% 30.2% 356 ODonovan et al. Large-Scale Induction and Evaluation of Lexical Resources which is bound to be less certain than the assignment of frames based entirely on existing examples." ></td>
	<td class="line x" title="454:532	As a generalization, Briscoe (2001) notes that lexicons such as COMLEX tend to demonstrate high precision but low recall." ></td>
	<td class="line x" title="455:532	Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX." ></td>
	<td class="line x" title="456:532	Precision was quite high (95%), but recall was low (84%)." ></td>
	<td class="line x" title="457:532	This has an effect on both the precision and recall scores of our system against COMLEX." ></td>
	<td class="line x" title="458:532	In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26." ></td>
	<td class="line x" title="459:532	We randomly selected 80 false negatives (fn) and 80 false positives (fp) across a range of active frame types containing prepositional and particle detail taken from Penn-III and manually examined them in order to classify them as correct or incorrect. Of the 80 fps, 33 were manually judged to be legitimate subcategorization frames." ></td>
	<td class="line x" title="460:532	For example, as Table 26 shows, there are a number of correct transitive verbs ([subj,obj]) in our automatically induced lexicon which are not included in COMLEX." ></td>
	<td class="line x" title="461:532	This examination was also useful in highlighting to us the frame types on which the lexical extraction procedure was performing poorly, in our case, those containing XCOMPs and those containing OBJ2S." ></td>
	<td class="line x" title="462:532	Out of 80 fns, 14 were judged to be incorrect when manually examined." ></td>
	<td class="line x" title="463:532	These can be broken down as follows: one intransitive frame, three ditransitive frames, three frames containing a COMP, and seven frames containing an oblique were found to be invalid." ></td>
	<td class="line x" title="464:532	7." ></td>
	<td class="line x" title="465:532	Lexical Accession Rates In addition to evaluating the quality of our extracted semantic forms, we also examined the rate at which they are induced." ></td>
	<td class="line x" title="466:532	This can be expressed as a measure of the coverage of the induced lexicon on new data." ></td>
	<td class="line x" title="467:532	Following Hockenmaier, Bierner, and Baldridge (2002), Xia (1999), and Miyao, Ninomiya, and Tsujii (2004), we extract a reference lexicon from Sections 0221 of the WSJ." ></td>
	<td class="line x" title="468:532	We then compare this to a test lexicon from Section 23." ></td>
	<td class="line x" title="469:532	Table 27 shows the results of the evaluation of the coverage of an induced lexicon for verbs only." ></td>
	<td class="line x" title="470:532	There is a corresponding semantic form in the reference lexicon for 89.89% of the verbs in Section 23." ></td>
	<td class="line x" title="471:532	10.11% of the entries in the test lexicon did not appear in the reference lexicon." ></td>
	<td class="line x" title="472:532	Within this group, we can distinguish between known words, which have an entry in the reference lexicon, and unknown words, which do not exist at all in the reference lexicon." ></td>
	<td class="line x" title="473:532	In the same way we make the distinction Table 26 Error analysis." ></td>
	<td class="line x" title="474:532	Frame type COMLEX: False negatives Induced: False positives Correct Incorrect Correct Incorrect [subj] 91 46 [subj, obj] 10 0 9 1 [subj, obj, obj2] 73 19 [ , xcomp, ] 10 0 1 10 [ , comp, ] 73 45 [ , obl, ] 237116 357 Computational Linguistics Volume 31, Number 3 Table 27 Coverage of induced lexicon (WSJ 0221) on unseen data (WSJ 23) (verbs only)." ></td>
	<td class="line x" title="475:532	Entries also in reference lexicon 89.89% Entries not in reference lexicon 10.11% Known words 7.85% Known words, known frames 7.85% Known words, unknown frames 0 Unknown words 2.32% Unknown words, known frames 2.32% Unknown words, unknown frames 0 between known frames and unknown frames." ></td>
	<td class="line x" title="476:532	There are, therefore, four different cases in which an entry may not appear in the reference lexicon." ></td>
	<td class="line x" title="477:532	Table 27 shows that the most common case is that of known verbs occurring with a different, although known, subcategorization frame (7.85%)." ></td>
	<td class="line x" title="478:532	The rate of accession may also be represented graphically." ></td>
	<td class="line x" title="479:532	In Charniak (1996) and Krotov et al.(1998), it was observed that treebank grammars (CFGs extracted from treebanks) are very large and grow with the size of the treebank." ></td>
	<td class="line x" title="481:532	We were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity." ></td>
	<td class="line x" title="482:532	Figure 8 graphs the rate of induction of semantic form and CFG rule types from Penn-III (the WSJ and parse-annotated Brown corpus combined)." ></td>
	<td class="line x" title="483:532	Because of the variation in the size of sections between the Brown and the WSJ, we plotted accession against word count." ></td>
	<td class="line x" title="484:532	The first part of the graph (up to 1,004,414 words) Figure 8 Comparison of accession rates for semantic form and CFG rule types for Penn-III (nonempty frames) (WSJ followed by Brown)." ></td>
	<td class="line x" title="485:532	358 ODonovan et al. Large-Scale Induction and Evaluation of Lexical Resources represents the rate of accession from the WSJ, and the final 384,646 words are those of the Brown corpus." ></td>
	<td class="line x" title="486:532	The seven curves represent the following: The acquisition of semantic form types (nonempty) for all syntactic categories with and without specific preposition and particle information, the acquisition of semantic form types (nonempty) for all verbs with and without specific preposition and particle information, the number of lemmas associated with the extract semantic forms, and the acquisition of CFG rule types." ></td>
	<td class="line x" title="487:532	The curve representing the growth in the overall size of the lexicon is similar in shape to that of the PCFG, while the rate of increase in the number of verbal semantic forms (particularly when obliques and particles are excluded) appears to slow more quickly." ></td>
	<td class="line x" title="488:532	Figure 8 shows the effect of domain diversity from the Brown section in terms of increased growth rates for 1e+06 words upward." ></td>
	<td class="line x" title="489:532	Figure 9 depicts the same information, this time extracted from the Brown section first followed by the WSJ." ></td>
	<td class="line x" title="490:532	The curves are different, but similar trends are represented." ></td>
	<td class="line x" title="491:532	This time the effects of domain diversity for the Brown section are discernible by comparing the absolute accession rate for the 0.4e+06 mark between Figures 8 and 9." ></td>
	<td class="line x" title="492:532	Figure 10 shows the result when we abstract away from semantic forms (verb frame combinations) to subcategorization frames and plot their rate of accession." ></td>
	<td class="line x" title="493:532	The graph represents the growth rate of frame types for Penn-III (WSJ followed by Brown and Brown followed by WSJ)." ></td>
	<td class="line x" title="494:532	The curve rises sharply initially but gradually levels, practically flattening out, despite the increase in the number of words." ></td>
	<td class="line x" title="495:532	This reflects the information about Section 23 in Table 27, where we demonstrate that although new verb frame combinations occur, all of the frame types in Section 23 have been seen by the lexical extraction program in previous sections." ></td>
	<td class="line x" title="496:532	Figure 9 Comparison of accession rates for semantic form and CFG rule types for Penn-III (nonempty frames) (Brown followed by WSJ)." ></td>
	<td class="line x" title="497:532	359 Computational Linguistics Volume 31, Number 3 Figure 10 Accession rates for frame types (without prepositions and particles) for Penn-III." ></td>
	<td class="line x" title="498:532	Figure 11 shows that including information about prepositions and particles in the frames results in an accession rate which continues to grow, albeit ever more slowly, with the increase in size of the extraction data." ></td>
	<td class="line x" title="499:532	This emphasizes the advantage of our approach, which extracts frames containing such information without the limitation of predefinition." ></td>
	<td class="line x" title="500:532	Figure 11 Accession rates for frame types for Penn-III." ></td>
	<td class="line x" title="501:532	360 ODonovan et al. Large-Scale Induction and Evaluation of Lexical Resources 8." ></td>
	<td class="line x" title="502:532	Conclusions and Further Work We have presented an algorithm for the extraction of semantic forms (or subcategorization frames) from the Penn-II and Penn-III Treebanks, automatically annotated with LFG f-structures." ></td>
	<td class="line x" title="503:532	In contrast to many other approaches, ours does not predefine the subcategorization frames we extract." ></td>
	<td class="line x" title="504:532	We have applied the algorithm to the WSJ sections of Penn-II (50,000 trees) (ODonovan et al. 2004) and to the parse-annotated Brown corpus of Penn-III (almost 25,000 additional trees)." ></td>
	<td class="line x" title="505:532	We extract syntactic-function-based subcategorization frames (LFG semantic forms) and traditional CFG category-based frames, as well as mixed-function-category-based frames." ></td>
	<td class="line x" title="506:532	Unlike many other approaches to subcategorization frame extraction, our system properly reflects the effects of long-distance dependencies." ></td>
	<td class="line x" title="507:532	Also unlike many approaches, our method distinguishes between active and passive frames." ></td>
	<td class="line x" title="508:532	Finally, our system associates conditional probabilities with the frames we extract." ></td>
	<td class="line x" title="509:532	Making the distinction between the behavior of verbs in active and passive contexts is particularly important for the accurate assignment of probabilities to semantic forms." ></td>
	<td class="line x" title="510:532	We carried out an extensive evaluation of the complete induced lexicon against the full COMLEX resource." ></td>
	<td class="line x" title="511:532	To our knowledge, this is the most extensive qualitative evaluation of subcategorization extraction in English." ></td>
	<td class="line x" title="512:532	The only evaluation of a similar scale is that carried out by Schulte im Walde (2002b) for German." ></td>
	<td class="line x" title="513:532	The results reported here for Penn-II compare favorably against the baseline and, in fact, are an improvement on those reported in ODonovan et al.(2004)." ></td>
	<td class="line x" title="515:532	The results for the larger, more domain-diverse Penn-III lexicon are very encouraging, in some cases almost 15% above the baseline." ></td>
	<td class="line x" title="516:532	We believe our semantic forms are fine-grained, and by choosing to evaluate against COMLEX, we set our sights high: COMLEX is considerably more detailed than the OALD or LDOCE used for other earlier evaluations." ></td>
	<td class="line x" title="517:532	Our error analysis also revealed some interesting issues associated with using an external standard such as COMLEX." ></td>
	<td class="line x" title="518:532	In the future, we hope to evaluate the automatic annotations and extracted lexicon against Propbank (Kingsbury and Palmer 2002)." ></td>
	<td class="line x" title="519:532	Apart from the related approach of Miyao, Ninomiya, and Tsujii (2004), which does not distinguish between argument and adjunct prepositional phrases, our treebank and automatic f-structure annotation-based architecture for the automatic acquisition of detailed subcategorization frames is quite unlike any of the architectures presented in the literature." ></td>
	<td class="line x" title="520:532	Subcategorization frames are reverse-engineered and almost a byproduct of the automatic f-structure annotation algorithm." ></td>
	<td class="line oc" title="521:532	It is important to realize that the induction of lexical resources is part of a larger project on the acquisition of wide-coverage, robust, probabilistic, deep unification grammar resources from treebanks Burke, Cahill, et al.(2004b)." ></td>
	<td class="line oc" title="523:532	We are already using the extracted semantic forms in parsing new text with robust, wide-coverage probabilistic LFG grammar approximations automatically acquired from the f-structure-annotated Penn-II treebank, specifically in the resolution of LDDs, as described in Cahill, Burke, et al.(2004)." ></td>
	<td class="line x" title="525:532	We hope to be able to apply our lexical acquisition methodology beyond existing parse-annotated corpora (Penn-II and Penn-III): New text is parsed by our probabilistic LFG approximations into f-structures from which we can then extract further semantic forms." ></td>
	<td class="line x" title="526:532	The work reported here is part of the core components for bootstrapping this approach." ></td>
	<td class="line x" title="527:532	In the shorter term, we intend to make the extracted subcategorization lexicons from Penn-II and Penn-III available as a downloadable public-domain research resource." ></td>
	<td class="line oc" title="528:532	We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank (Brants et al. 2002) and Penn Chinese Treebank (Xue, Chiou, and Palmer 2002), extracting wide-coverage, probabilistic LFG grammar 361 Computational Linguistics Volume 31, Number 3 approximations and lexical resources for German (Cahill et al. 2003) and Chinese (Burke, Lam, et al. 2004)." ></td>
	<td class="line x" title="529:532	The lexical resources, however, have not yet been evaluated." ></td>
	<td class="line x" title="530:532	This, and much else, has to await further research." ></td>
	<td class="line x" title="531:532	Acknowledgments The research reported here is partially supported by Enterprise Ireland Basic Research Grant SC/2001/186, an IRCSET PhD fellowship award, and an IBM PhD fellowship award." ></td>
	<td class="line x" title="532:532	We are particularly grateful to our anonymous reviewers, whose insightful comments have helped to improve this article considerably." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P05-1013
Pseudo-Projective Dependency Parsing
Nivre, Joakim;Nilsson, Jens;"></td>
	<td class="line x" title="1:139	Proceedings of the 43rd Annual Meeting of the ACL, pages 99106, Ann Arbor, June 2005." ></td>
	<td class="line x" title="2:139	c2005 Association for Computational Linguistics Pseudo-Projective Dependency Parsing Joakim Nivre and Jens Nilsson School of Mathematics and Systems Engineering Vaxjo University SE-35195 Vaxjo, Sweden {nivre,jni}@msi.vxu.se Abstract In order to realize the full potential of dependency-based syntactic parsing, it is desirable to allow non-projective dependency structures." ></td>
	<td class="line x" title="3:139	We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures." ></td>
	<td class="line x" title="4:139	Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy." ></td>
	<td class="line x" title="5:139	This leads to the best reported performance for robust non-projective parsing of Czech." ></td>
	<td class="line x" title="6:139	1 Introduction It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate treatment of languages with variable word order, where discontinuous syntactic constructions are more common than in languages like English (Melcuk, 1988; Covington, 1990)." ></td>
	<td class="line x" title="7:139	However, this argument is only plausible if the formal framework allows non-projective dependency structures, i.e. structures where a head and its dependents may correspond to a discontinuous constituent." ></td>
	<td class="line x" title="8:139	From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness." ></td>
	<td class="line x" title="9:139	Thus, most broad-coverage parsers based on dependency grammar have been restricted to projective structures." ></td>
	<td class="line x" title="10:139	This is true of the widely used link grammar parser for English (Sleator and Temperley, 1993), which uses a dependency grammar of sorts, the probabilistic dependency parser of Eisner (1996), and more recently proposed deterministic dependency parsers (Yamada and Matsumoto, 2003; Nivre et al. , 2004)." ></td>
	<td class="line x" title="11:139	It is also true of the adaptation of the Collins parser for Czech (Collins et al. , 1999) and the finite-state dependency parser for Turkish by Oflazer (2003)." ></td>
	<td class="line x" title="12:139	This is in contrast to dependency treebanks, e.g. Prague Dependency Treebank (Hajic et al. , 2001b), Danish Dependency Treebank (Kromann, 2003), and the METU Treebank of Turkish (Oflazer et al. , 2003), which generally allow annotations with nonprojective dependency structures." ></td>
	<td class="line x" title="13:139	The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions." ></td>
	<td class="line x" title="14:139	While the proportion of sentences containing non-projective dependencies is often 1525%, the total proportion of non-projective arcs is normally only 12%." ></td>
	<td class="line x" title="15:139	As long as the main evaluation metric is dependency accuracy per word, with state-of-the-art accuracy mostly below 90%, the penalty for not handling non-projective constructions is almost negligible." ></td>
	<td class="line x" title="16:139	Still, from a theoretical point of view, projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal." ></td>
	<td class="line x" title="17:139	99 (Only one of them concerns quality.) R Z (Out-of a7 a4 a63 AuxP P nich them a7 a4 a63 Atr VB je is T jen only a7 a4 a63 AuxZ C jedna one-FEM-SG a7 a4 a63 Sb R na to a7 a4 a63 AuxP N4 kvalitu quality a63 a7 a4Adv Z:." ></td>
	<td class="line x" title="18:139	a7 a4 a63 AuxZ Figure 1: Dependency graph for Czech sentence from the Prague Dependency Treebank1 There exist a few robust broad-coverage parsers that produce non-projective dependency structures, notably Tapanainen and Jarvinen (1997) and Wang and Harper (2004) for English, Foth et al.(2004) for German, and Holan (2004) for Czech." ></td>
	<td class="line x" title="20:139	In addition, there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington, 1990; Kahane et al. , 1998; Duchier and Debusmann, 2001; Holan et al. , 2001; Hellwig, 2003)." ></td>
	<td class="line oc" title="21:139	Finally, since non-projective constructions often involve long-distance dependencies, the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun and de Rijke, 2004; Cahill et al. , 2004; Levy and Manning, 2004; Campbell, 2004)." ></td>
	<td class="line x" title="22:139	In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques." ></td>
	<td class="line x" title="23:139	First, the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al. , 1998) and encoding information about these lifts in arc labels." ></td>
	<td class="line x" title="24:139	When the parser is trained on the transformed data, it will ideally learn not only to construct projective dependency structures but also to assign arc labels that encode information about lifts." ></td>
	<td class="line x" title="25:139	By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence." ></td>
	<td class="line x" title="26:139	to non-projective structures." ></td>
	<td class="line x" title="27:139	We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al. , 1998)." ></td>
	<td class="line x" title="28:139	The rest of the paper is structured as follows." ></td>
	<td class="line x" title="29:139	In section 2 we introduce the graph transformation techniques used to projectivize and deprojectivize dependency graphs, and in section 3 we describe the data-driven dependency parser that is the core of our system." ></td>
	<td class="line x" title="30:139	We then evaluate the approach in two steps." ></td>
	<td class="line x" title="31:139	First, in section 4, we evaluate the graph transformation techniques in themselves, with data from the Prague Dependency Treebank and the Danish Dependency Treebank." ></td>
	<td class="line x" title="32:139	In section 5, we then evaluate the entire parsing system by training and evaluating on data from the Prague Dependency Treebank." ></td>
	<td class="line x" title="33:139	2 Dependency Graph Transformations We assume that the goal in dependency parsing is to construct a labeled dependency graph of the kind depicted in Figure 1." ></td>
	<td class="line x" title="34:139	Formally, we define dependency graphs as follows: 1." ></td>
	<td class="line x" title="35:139	Let R ={r1,,rm}be the set of permissible dependency types (arc labels)." ></td>
	<td class="line x" title="36:139	2." ></td>
	<td class="line x" title="37:139	A dependency graph for a string of words W = w1wn is a labeled directed graph D = (W,A), where (a) W is the set of nodes, i.e. word tokens in the input string, ordered by a linear precedence relation <, (b) A is a set of labeled arcs (wi,r,wj), where wi,wj W, rR, (c) for every wj W, there is at most one arc (wi,r,wj)A. 100 (Only one of them concerns quality.) R Z (Out-of a7 a4 a63 AuxP P nich them a7 a4 a63 Atr VB je is T jen only a7 a4 a63 AuxZ C jedna one-FEM-SG a7 a4 a63 Sb R na to a7 a4 a63 AuxP N4 kvalitu quality a63 a7 a4Adv Z:." ></td>
	<td class="line x" title="38:139	a7 a4 a63 AuxZ Figure 2: Projectivized dependency graph for Czech sentence 3." ></td>
	<td class="line x" title="39:139	A graph D = (W,A) is well-formed iff it is acyclic and connected." ></td>
	<td class="line x" title="40:139	If (wi,r,wj)A, we say that wi is the head of wj and wj a dependent of wi." ></td>
	<td class="line x" title="41:139	In the following, we use the notation wi rwj to mean that (wi,r,wj)A; we also use wiwj to denote an arc with unspecified label and wi wj for the reflexive and transitive closure of the (unlabeled) arc relation." ></td>
	<td class="line x" title="42:139	The dependency graph in Figure 1 satisfies all the defining conditions above, but it fails to satisfy the condition of projectivity (Kahane et al. , 1998): 1." ></td>
	<td class="line x" title="43:139	An arc wiwk is projective iff, for every word wj occurring between wi and wk in the string (wi<wj<wk or wi>wj>wk), wi wj." ></td>
	<td class="line x" title="44:139	2." ></td>
	<td class="line x" title="45:139	A dependency graph D = (W,A) is projective iff every arc in A is projective." ></td>
	<td class="line x" title="46:139	The arc connecting the head jedna (one) to the dependent Z (out-of) spans the token je (is), which is not dominated by jedna." ></td>
	<td class="line x" title="47:139	As observed by Kahane et al.(1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi  wk such that wi  wj holds in the original graph." ></td>
	<td class="line x" title="49:139	Here we use a slightly different notion of lift, applying to individual arcs and moving their head upwards one step at a time: LIFT(wj wk) = braceleftBigg wiwk if wiwj undefined otherwise Intuitively, lifting an arc makes the word wk dependent on the head wi of its original head wj (which is unique in a well-formed dependency graph), unless wj is a root in which case the operation is undefined (but then wj  wk is necessarily projective if the dependency graph is well-formed)." ></td>
	<td class="line x" title="50:139	Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case." ></td>
	<td class="line x" title="51:139	However, since we want to preserve as much of the original structure as possible, we are interested in finding a transformation that involves a minimal number of lifts." ></td>
	<td class="line x" title="52:139	Even this may be nondeterministic, in case the graph contains several non-projective arcs whose lifts interact, but we use the following algorithm to construct a minimal projective transformation Dprime = (W,Aprime) of a (nonprojective) dependency graph D = (W,A): PROJECTIVIZE(W, A) 1 Aprime A 2 while (W,Aprime) is non-projective 3 aSMALLEST-NONP-ARC(Aprime) 4 Aprime (Aprime{a}){LIFT(a)} 5 return (W,Aprime) The function SMALLEST-NONP-ARC returns the non-projective arc with the shortest distance from head to dependent (breaking ties from left to right)." ></td>
	<td class="line x" title="53:139	Applying the function PROJECTIVIZE to the graph in Figure 1 yields the graph in Figure 2, where the problematic arc pointing to Z has been lifted from the original head jedna to the ancestor je." ></td>
	<td class="line x" title="54:139	Using the terminology of Kahane et al.(1998), we say that jedna is the syntactic head of Z, while je is its linear head in the projectivized representation." ></td>
	<td class="line x" title="56:139	Unlike Kahane et al.(1998), we do not regard a projectivized representation as the final target of the parsing process." ></td>
	<td class="line x" title="58:139	Instead, we want to apply an in101 Lifted arc label Path labels Number of labels Baseline d p n Head dh p n(n+ 1) Head+Path dh p 2n(n+ 1) Path d p 4n Table 1: Encoding schemes (d = dependent, h = syntactic head, p = path; n = number of dependency types) verse transformation to recover the underlying (nonprojective) dependency graph." ></td>
	<td class="line x" title="59:139	In order to facilitate this task, we extend the set of arc labels to encode information about lifting operations." ></td>
	<td class="line x" title="60:139	In principle, it would be possible to encode the exact position of the syntactic head in the label of the arc from the linear head, but this would give a potentially infinite set of arc labels and would make the training of the parser very hard." ></td>
	<td class="line x" title="61:139	In practice, we can therefore expect a trade-off such that increasing the amount of information encoded in arc labels will cause an increase in the accuracy of the inverse transformation but a decrease in the accuracy with which the parser can construct the labeled representations." ></td>
	<td class="line x" title="62:139	To explore this tradeoff, we have performed experiments with three different encoding schemes (plus a baseline), which are described schematically in Table 1." ></td>
	<td class="line x" title="63:139	The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label dh for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure." ></td>
	<td class="line x" title="64:139	Using this encoding scheme, the arc from je to Z in Figure 2 would be assigned the label AuxPSb (signifying an AuxP that has been lifted from a Sb)." ></td>
	<td class="line x" title="65:139	In the second scheme, Head+Path, we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p." ></td>
	<td class="line x" title="66:139	Thus, the arc from je to jedna will be labeled Sb(to indicate that there is a syntactic head below it)." ></td>
	<td class="line x" title="67:139	In the third and final scheme, denoted Path, we keep the extra infor2Note that this is a baseline for the parsing experiment only (Experiment 2)." ></td>
	<td class="line x" title="68:139	For Experiment 1 it is meaningless as a baseline, since it would result in 0% accuracy." ></td>
	<td class="line x" title="69:139	mation on path labels but drop the information about the syntactic head of the lifted arc, using the label d instead of dh (AuxPinstead of AuxPSb)." ></td>
	<td class="line x" title="70:139	As can be seen from the last column in Table 1, both Head and Head+Path may theoretically lead to a quadratic increase in the number of distinct arc labels (Head+Path being worse than Head only by a constant factor), while the increase is only linear in the case of Path." ></td>
	<td class="line x" title="71:139	On the other hand, we can expect Head+Path to be the most useful representation for reconstructing the underlying non-projective dependency graph." ></td>
	<td class="line x" title="72:139	In approaching this problem, a variety of different methods are conceivable, including a more or less sophisticated use of machine learning." ></td>
	<td class="line x" title="73:139	In the present study, we limit ourselves to an algorithmic approach, using a deterministic breadthfirst search." ></td>
	<td class="line x" title="74:139	The details of the transformation procedure are slightly different depending on the encoding schemes:  Head: For every arc of the form wi dh wn, we search the graph top-down, left-to-right, breadth-first starting at the head node wi." ></td>
	<td class="line x" title="75:139	If we find an arc wl hwm, called a target arc, we replace wi dhwn by wm dwn; otherwise we replace wi dhwn by wi dwn (i.e. we let the linear head be the syntactic head)." ></td>
	<td class="line x" title="76:139	 Head+Path: Same as Head, but the search only follows arcs of the form wj pwk and a target arc must have the form wl hwm; if no target arc is found, Head is used as backoff." ></td>
	<td class="line x" title="77:139	 Path: Same as Head+Path, but a target arc must have the form wl p wm and no outgoing arcs of the form wm pprimewo; no backoff." ></td>
	<td class="line x" title="78:139	In section 4 we evaluate these transformations with respect to projectivized dependency treebanks, and in section 5 they are applied to parser output." ></td>
	<td class="line x" title="79:139	Before 102 Feature type Top1 Top Next Next+1 Next+2 Next+3 Word form + + + + Part-of-speech + + + + + + Dep type of head + leftmost dep + + rightmost dep + Table 2: Features used in predicting the next parser action we turn to the evaluation, however, we need to introduce the data-driven dependency parser used in the latter experiments." ></td>
	<td class="line x" title="80:139	3 Memory-Based Dependency Parsing In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al. , 2004) and English (Nivre and Scholz, 2004)." ></td>
	<td class="line x" title="81:139	The parser builds dependency graphs by traversing the input from left to right, using a stack to store tokens that are not yet complete with respect to their dependents." ></td>
	<td class="line x" title="82:139	At each point during the derivation, the parser has a choice between pushing the next input token onto the stack  with or without adding an arc from the token on top of the stack to the token pushed  and popping a token from the stack  with or without adding an arc from the next input token to the token popped." ></td>
	<td class="line x" title="83:139	More details on the parsing algorithm can be found in Nivre (2003)." ></td>
	<td class="line x" title="84:139	The choice between different actions is in general nondeterministic, and the parser relies on a memorybased classifier, trained on treebank data, to predict the next action based on features of the current parser configuration." ></td>
	<td class="line x" title="85:139	Table 2 shows the features used in the current version of the parser." ></td>
	<td class="line x" title="86:139	At each point during the derivation, the prediction is based on six word tokens, the two topmost tokens on the stack, and the next four input tokens." ></td>
	<td class="line x" title="87:139	For each token, three types of features may be taken into account: the word form; the part-of-speech assigned by an automatic tagger; and labels on previously assigned dependency arcs involving the token  the arc from its head and the arcs to its leftmost and rightmost dependent, respectively." ></td>
	<td class="line x" title="88:139	Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness." ></td>
	<td class="line x" title="89:139	For robustness reasons, the parser may output a set of dependency trees instead of a single tree." ></td>
	<td class="line x" title="90:139	most dependent of the next input token, dependency type features are limited to tokens on the stack." ></td>
	<td class="line x" title="91:139	The prediction based on these features is a knearest neighbor classification, using the IB1 algorithm and k = 5, the modified value difference metric (MVDM) and class voting with inverse distance weighting, as implemented in the TiMBL software package (Daelemans et al. , 2003)." ></td>
	<td class="line x" title="92:139	More details on the memory-based prediction can be found in Nivre et al.(2004) and Nivre and Scholz (2004)." ></td>
	<td class="line x" title="94:139	4 Experiment 1: Treebank Transformation The first experiment uses data from two dependency treebanks." ></td>
	<td class="line x" title="95:139	The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text, annotated on three levels, the morphological, analytical and tectogrammatical levels (Hajic, 1998)." ></td>
	<td class="line x" title="96:139	Our experiments all concern the analytical annotation, and the first experiment is based only on the training part." ></td>
	<td class="line x" title="97:139	The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus, with annotation of primary and secondary dependencies (Kromann, 2003)." ></td>
	<td class="line x" title="98:139	The entire treebank is used in the experiment, but only primary dependencies are considered.4 In all experiments, punctuation tokens are included in the data but omitted in evaluation scores." ></td>
	<td class="line x" title="99:139	In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2." ></td>
	<td class="line x" title="100:139	As shown in Table 3, the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT." ></td>
	<td class="line x" title="101:139	However, the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT." ></td>
	<td class="line x" title="102:139	The last four 4If secondary dependencies had been included, the dependency graphs would not have satisfied the well-formedness conditions formulated in section 2." ></td>
	<td class="line x" title="103:139	103 # Lifts in projectivization Data set # Sentences % NonP # Tokens % NonP 1 2 3 >3 PDT training 73,088 23.15 1,255,333 1.81 93.79 5.60 0.51 0.11 DDT total 5,512 15.48 100,238 0.94 79.49 13.28 4.36 2.87 Table 3: Non-projective sentences and arcs in PDT and DDT (NonP = non-projective) Data set Head H+P Path PDT training (28 labels) 92.3 (230) 99.3 (314) 97.3 (84) DDT total (54 labels) 92.3 (123) 99.8 (147) 98.3 (99) Table 4: Percentage of non-projective arcs recovered correctly (number of labels in parentheses) columns in Table 3 show the distribution of nonprojective arcs with respect to the number of lifts required." ></td>
	<td class="line x" title="104:139	It is worth noting that, although nonprojective constructions are less frequent in DDT than in PDT, they seem to be more deeply nested, since only about 80% can be projectivized with a single lift, while almost 95% of the non-projective arcs in PDT only require a single lift." ></td>
	<td class="line x" title="105:139	In the second part of the experiment, we applied the inverse transformation based on breadth-first search under the three different encoding schemes." ></td>
	<td class="line x" title="106:139	The results are given in Table 4." ></td>
	<td class="line x" title="107:139	As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets." ></td>
	<td class="line x" title="108:139	However, it can be noted that the results for the least informative encoding, Path, are almost comparable, while the third encoding, Head, gives substantially worse results for both data sets." ></td>
	<td class="line x" title="109:139	We also see that the increase in the size of the label sets for Head and Head+Path is far below the theoretical upper bounds given in Table 1." ></td>
	<td class="line x" title="110:139	The increase is generally higher for PDT than for DDT, which indicates a greater diversity in non-projective constructions." ></td>
	<td class="line x" title="111:139	5 Experiment 2: Memory-Based Parsing The second experiment is limited to data from PDT.5 The training part of the treebank was projectivized under different encoding schemes and used to train memory-based dependency parsers, which were run on the test part of the treebank, consisting of 7,507 5Preliminary experiments using data from DDT indicated that the limited size of the treebank creates a severe sparse data problem with respect to non-projective constructions." ></td>
	<td class="line x" title="112:139	sentences and 125,713 tokens.6 The inverse transformation was applied to the output of the parsers and the result compared to the gold standard test set." ></td>
	<td class="line x" title="113:139	Table 5 shows the overall parsing accuracy attained with the three different encoding schemes, compared to the baseline (no special arc labels) and to training directly on non-projective dependency graphs." ></td>
	<td class="line x" title="114:139	Evaluation metrics used are Attachment Score (AS), i.e. the proportion of tokens that are attached to the correct head, and Exact Match (EM), i.e. the proportion of sentences for which the dependency graph exactly matches the gold standard." ></td>
	<td class="line x" title="115:139	In the labeled version of these metrics (L) both heads and arc labels must be correct, while the unlabeled version (U) only considers heads." ></td>
	<td class="line x" title="116:139	The first thing to note is that projectivizing helps in itself, even if no encoding is used, as seen from the fact that the projective baseline outperforms the non-projective training condition by more than half a percentage point on attachment score, although the gain is much smaller with respect to exact match." ></td>
	<td class="line x" title="117:139	The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score." ></td>
	<td class="line x" title="118:139	With respect to exact match, the improvement is even more noticeable, which shows quite clearly that even if non-projective dependencies are rare on the token level, they are nevertheless important for getting the global syntactic structure correct." ></td>
	<td class="line x" title="119:139	All improvements over the baseline are statistically significant beyond the 0.01 level (McNemars 6The part-of-speech tagging used in both training and testing was the uncorrected output of an HMM tagger distributed with the treebank; cf.Hajic et al.(2001a)." ></td>
	<td class="line x" title="122:139	104 Encoding UAS LAS UEM LEM Non-projective 78.5 71.3 28.9 20.6 Baseline 79.1 72.0 29.2 20.7 Head 80.1 72.8 31.6 22.2 Head+Path 80.0 72.8 31.8 22.4 Path 80.0 72.7 31.6 22.0 Table 5: Parsing accuracy (AS = attachment score, EM = exact match; U = unlabeled, L = labeled) Unlabeled Labeled Encoding P R F P R F Head 61.3 54.1 57.5 55.2 49.8 52.4 Head+Path 63.9 54.9 59.0 57.9 50.6 54.0 Path 58.2 49.5 53.4 51.0 45.7 48.2 Table 6: Precision, recall and F-measure for non-projective arcs test)." ></td>
	<td class="line x" title="123:139	By contrast, when we turn to a comparison of the three encoding schemes it is hard to find any significant differences, and the overall impression is that it makes little or no difference which encoding scheme is used, as long as there is some indication of which words are assigned their linear head instead of their syntactic head by the projective parser." ></td>
	<td class="line x" title="124:139	This may seem surprising, given the experiments reported in section 4, but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift, where the encoding of path information is often redundant." ></td>
	<td class="line x" title="125:139	It is likely that the more complex cases, where path information could make a difference, are beyond the reach of the parser in most cases." ></td>
	<td class="line x" title="126:139	However, if we consider precision, recall and Fmeasure on non-projective dependencies only, as shown in Table 6, some differences begin to emerge." ></td>
	<td class="line x" title="127:139	The most informative scheme, Head+Path, gives the highest scores, although with respect to Head the difference is not statistically significant, while the least informative scheme, Path  with almost the same performance on treebank transformation  is significantly lower (p < 0.01)." ></td>
	<td class="line x" title="128:139	On the other hand, given that all schemes have similar parsing accuracy overall, this means that the Path scheme is the least likely to introduce errors on projective arcs." ></td>
	<td class="line x" title="129:139	The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers." ></td>
	<td class="line x" title="130:139	Although the best published results for the Collins parser is 80% UAS (Collins, 1999), this parser reaches 82% when trained on the entire training data set, and an adapted version of Charniaks parser (Charniak, 2000) performs at 84% (Jan Hajic, pers." ></td>
	<td class="line x" title="131:139	comm.)." ></td>
	<td class="line x" title="132:139	However, the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech, with a best performance of 73% UAS (Holan, 2004)." ></td>
	<td class="line x" title="133:139	Compared to related work on the recovery of long-distance dependencies in constituency-based parsing, our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process, via an extension of the set of syntactic categories, whereas most other approaches rely on postprocessing only." ></td>
	<td class="line x" title="134:139	However, while Dienes and Dubey recognize empty categories in a pre-processing step and only let the parser find their antecedents, we use the parser both to detect dislocated dependents and to predict either the type or the location of their syntactic head (or both) and use post-processing only to transform the graph in accordance with the parsers analysis." ></td>
	<td class="line x" title="135:139	6 Conclusion We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques." ></td>
	<td class="line x" title="136:139	The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy, 105 especially with respect to the exact match criterion, leading to the best reported performance for robust non-projective parsing of Czech." ></td>
	<td class="line x" title="137:139	Acknowledgements This work was supported in part by the Swedish Research Council (621-2002-4207)." ></td>
	<td class="line x" title="138:139	Memory-based classifiers for the experiments were created using TiMBL (Daelemans et al. , 2003)." ></td>
	<td class="line x" title="139:139	Special thanks to Jan Hajic and Matthias Trautner Kromann for assistance with the Czech and Danish data, respectively, and to Jan Hajic, Tomas Holan, Dan Zeman and three anonymous reviewers for valuable comments on a preliminary version of the paper." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E06-1010
Constraints On Non-Projective Dependency Parsing
Nivre, Joakim;"></td>
	<td class="line x" title="1:122	Constraints on Non-Projective Dependency Parsing Joakim Nivre Vaxjo University, School of Mathematics and Systems Engineering Uppsala University, Department of Linguistics and Philology joakim.nivre@msi.vxu.se Abstract We investigate a series of graph-theoretic constraints on non-projective dependency parsing and their effect on expressivity, i.e. whether they allow naturally occurring syntactic constructions to be adequately represented, and efficiency, i.e. whether they reduce the search space for the parser." ></td>
	<td class="line x" title="2:122	In particular, we define a new measure for the degree of non-projectivity in an acyclic dependency graph obeying the single-head constraint." ></td>
	<td class="line x" title="3:122	The constraints are evaluated experimentally using data from the Prague Dependency Treebank and the Danish Dependency Treebank." ></td>
	<td class="line x" title="4:122	Theresults indicate that, whereas complete linguistic coverage in principle requires unrestricted non-projective dependency graphs, limiting the degree of non-projectivity to at most 2 can reduce average running time from quadratic to linear, while excluding less than 0.5% of the dependency graphs found in the two treebanks." ></td>
	<td class="line x" title="5:122	This is a substantial improvement over the commonly used projective approximation (degree 0), which excludes 1525% of the graphs." ></td>
	<td class="line x" title="6:122	1 Introduction Data-driven approaches to syntactic parsing has until quite recently been limited to representations that do not capture non-local dependencies." ></td>
	<td class="line x" title="7:122	This is true regardless of whether representations are based on constituency, where such dependencies are traditionally represented by empty categories and coindexation to avoid explicitly discontinuous constituents, or on dependency, where it is more common to use a direct encoding of so-called nonprojective dependencies." ></td>
	<td class="line x" title="8:122	Whilethissurface dependency approximation (Levy and Manning, 2004) may be acceptable for certain applications of syntactic parsing, it is clearly not adequate as a basis for deep semantic interpretation, which explains the growing body of research devoted to different methods for correcting this approximation." ></td>
	<td class="line oc" title="9:122	Most of this work has so far focused either on post-processing to recover non-local dependencies from context-free parse trees (Johnson, 2002; Jijkoun and De Rijke, 2004; Levy and Manning, 2004; Campbell, 2004), or on incorporating nonlocal dependency information in nonterminal categories in constituency representations (Dienes and Dubey, 2003; Hockenmaier, 2003; Cahill et al. , 2004) or in the categories used to label arcs in dependency representations (Nivre and Nilsson, 2005)." ></td>
	<td class="line x" title="10:122	By contrast, there is very little work on parsing methods that allow discontinuous constructions to be represented directly in the syntactic structure, whether by discontinuous constituent structures or by non-projective dependency structures." ></td>
	<td class="line x" title="11:122	Notable exceptions are Plaehn (2000), where discontinuous phrase structure grammar parsing is explored, and McDonald et al.(2005b), where nonprojective dependency structures are derived using spanning tree algorithms from graph theory." ></td>
	<td class="line x" title="13:122	Onequestion that arises ifwewanttopursue the structure-based approach is how to constrain the class of permissible structures." ></td>
	<td class="line x" title="14:122	On the one hand, we want to capture all the constructions that are found in natural languages, or at least to provide a much better approximation than before." ></td>
	<td class="line x" title="15:122	On the other hand, it must still be possible for the parser not only to search the space of permissible structures in an efficient way but also to learn to select the most appropriate structure for a given sentence with sufficient accuracy." ></td>
	<td class="line x" title="16:122	This is the usual tradeoff 73 between expressivity and complexity, where a less restricted class of permissible structures can capture more complex constructions, but where the enlarged search space makes parsing harder with respect to both accuracy and efficiency." ></td>
	<td class="line x" title="17:122	Whereas extensions to context-free grammar have been studied quite extensively, there are very few corresponding results for dependency-based systems." ></td>
	<td class="line x" title="18:122	Since Gaifman (1965) proved that his projective dependency grammar is weakly equivalent to context-free grammar, Neuhaus and Broker (1997) have shown that the recognition problem for a dependency grammar that can define arbitrary non-projective structures is NP complete, but there are no results for systems of intermediate complexity." ></td>
	<td class="line x" title="19:122	The pseudo-projective grammar proposed by Kahane et al.(1998) can be parsed in polynomial time and captures non-local dependencies through a form of gap-threading, but the structures generated by the grammar are strictly projective." ></td>
	<td class="line x" title="21:122	Moreover, the study of formal grammarsisonly partially relevant for research ondatadriven dependency parsing, where most systems are not grammar-based but rely on inductive inference from treebank data (Yamada and Matsumoto, 2003; Nivre et al. , 2004; McDonald et al. , 2005a)." ></td>
	<td class="line x" title="22:122	For example, despite the results of Neuhaus and Broker (1997), McDonald et al.(2005b) perform parsing with arbitrary non-projective dependency structures in O(n2) time." ></td>
	<td class="line x" title="24:122	In this paper, we will therefore approach the problem from a slightly different angle." ></td>
	<td class="line x" title="25:122	Instead of investigating formal dependency grammars and their complexity, we will impose a series of graphtheoretic constraints on dependency structures and see how these constraints affect expressivity and parsing efficiency." ></td>
	<td class="line x" title="26:122	The approach is mainly experimental and we evaluate constraints using data from two dependency-based treebanks, the Prague Dependency Treebank (Hajic et al. , 2001) and the Danish Dependency Treebank (Kromann, 2003)." ></td>
	<td class="line x" title="27:122	Expressivity is investigated by examining how large a proportion of the structures found in the treebanks are parsable under different constraints, and efficiency is addressed by considering the number of potential dependency arcs that need to be processed when parsing these structures." ></td>
	<td class="line x" title="28:122	This is a relevant metric for data-driven approaches, where parsing timeisoften dominated by thecomputation of model predictions or scores for such arcs." ></td>
	<td class="line x" title="29:122	The parsing experiments are performed with a variant of Covingtons algorithm for dependency parsing (Covington, 2001), using the treebank as an oracle in order to establish an upper bound on accuracy." ></td>
	<td class="line x" title="30:122	However, the results are relevant for a larger class of algorithms that derive nonprojective dependency graphs by treating every possible word pair as a potential dependency arc. The paper is structured as follows." ></td>
	<td class="line x" title="31:122	In section 2 we define dependency graphs, and in section 3 we formulate a number of constraints that can be used to define different classes of dependency graphs, ranging from unrestricted non-projective to strictly projective." ></td>
	<td class="line x" title="32:122	In section 4 we introduce the parsing algorithm used in the experiments, and in section 5 we describe the experimental setup." ></td>
	<td class="line x" title="33:122	In section 6 we present the results of the experiments and discuss their implications for non-projective dependency parsing." ></td>
	<td class="line x" title="34:122	We conclude in section 7." ></td>
	<td class="line x" title="35:122	2 Dependency Graphs A dependency graph is a labeled directed graph, the nodes of which are indices corresponding to the tokens of a sentence." ></td>
	<td class="line x" title="36:122	Formally: Definition 1 Given a set R of dependency types (arc labels), a dependency graph for a sentence x = (w1,,wn) is a labeled directed graph G = (V,E,L), where: 1." ></td>
	<td class="line x" title="37:122	V = Zn+1 2." ></td>
	<td class="line x" title="38:122	EV V 3." ></td>
	<td class="line x" title="39:122	L : ER Definition 2 A dependency graph G is wellformed if and only if: 1." ></td>
	<td class="line x" title="40:122	The node 0 is a root (ROOT)." ></td>
	<td class="line x" title="41:122	2." ></td>
	<td class="line x" title="42:122	G is connected (CONNECTEDNESS).1 The set of V of nodes (or vertices) is the set Zn+1 ={0,1,2,,n}(nZ+), i.e., the set of non-negative integers up to and including n. This means that every token index i of the sentence is a node (1in) and that there is a special node 0, which does not correspond to any token of the sentence and which will always be a root of the dependency graph (normally the only root)." ></td>
	<td class="line x" title="43:122	The set E of arcs (or edges) is a set of ordered pairs (i,j), whereiandj are nodes." ></td>
	<td class="line x" title="44:122	Since arcs are used to represent dependency relations, we will 1To be more exact, we require G to be weakly connected, which entails that the corresponding undirected graph is connected, whereas a strongly connected graph has a directed path between any pair of nodes." ></td>
	<td class="line x" title="45:122	74 (Only one of them concerns quality.) 0 1 R Z (Out-of a7 a4 a63 AuxP 2 P nich them a7 a4 a63 Atr 3 VB je is a7 a4 a63 Pred 4 T jen only a7 a4 a63 AuxZ 5 C jedna one-FEM-SG a7 a4 a63 Sb 6 R na to a7 a4 a63 AuxP 7 N4 kvalitu quality a63 a7 a4Adv 8 Z:." ></td>
	<td class="line x" title="46:122	a7 a4 a63 AuxK Figure 1: Dependency graph for Czech sentence from the Prague Dependency Treebank say that i is the head and j is the dependent of the arc (i,j)." ></td>
	<td class="line x" title="47:122	As usual, we will use the notation i  j to mean that there is an arc connecting i and j (i.e. , (i,j)  E) and we will use the notation i j for the reflexive and transitive closure of the arc relation E (i.e. , i  j if and only if i = j or there is a path of arcs connecting i to j)." ></td>
	<td class="line x" title="48:122	The function L assigns a dependency type (arc label) r R to every arc eE. Figure 1 shows a Czech sentence from the Prague Dependency Treebank with a well-formed dependency graph according to Definition 12." ></td>
	<td class="line x" title="49:122	3 Constraints Theonly conditions sofarimposedondependency graphs is that the special node 0 be a root and that the graph be connected." ></td>
	<td class="line x" title="50:122	Here are three further constraints that are common in the literature: 3." ></td>
	<td class="line x" title="51:122	Every node has at most one head, i.e., if ij then there is no node k such that k negationslash= i and kj (SINGLE-HEAD)." ></td>
	<td class="line x" title="52:122	4." ></td>
	<td class="line x" title="53:122	The graph G is acyclic, i.e., if ij then not j i (ACYCLICITY)." ></td>
	<td class="line x" title="54:122	5." ></td>
	<td class="line x" title="55:122	The graph G is projective, i.e., if ij then i k, for every node k such that i < k < j or j < k < i (PROJECTIVITY)." ></td>
	<td class="line x" title="56:122	Note that these conditions are independent in that none of them is entailed by any (combination) of the others." ></td>
	<td class="line x" title="57:122	However, the conditions SINGLEHEAD and ACYCLICITY together with the basic well-formedness conditions entail that the graph is a tree rooted at the node 0." ></td>
	<td class="line x" title="58:122	These constraints are assumed in almost all versions of dependency grammar, especially in computational systems." ></td>
	<td class="line x" title="59:122	By contrast, the PROJECTIVITY constraint is much more controversial." ></td>
	<td class="line x" title="60:122	Broadly speaking, we can say that whereas most practical systems for dependency parsing do assume projectivity, most dependency-based linguistic theories donot." ></td>
	<td class="line x" title="61:122	More precisely, most theoretical formulations of dependency grammar regard projectivity as the norm but also recognize the need for non-projective representations to capture non-local dependencies (Melcuk, 1988; Hudson, 1990)." ></td>
	<td class="line x" title="62:122	In order to distinguish classes of dependency graphs thatfallinbetweenarbitrary non-projective and projective, we define a notion of degree of non-projectivity, such that projective graphs have degree 0 while arbitrary non-projective graphs have unbounded degree." ></td>
	<td class="line x" title="63:122	Definition 3 LetG = (V,E,L) beawell-formed dependency graph, satisfying SINGLE-HEAD and ACYCLICITY, and let Ge be the subgraph of G that only contains nodes between i and j for the arc e = (i,j) (i.e. , Ve ={i+1,,j1}if i < j and Ve ={j+1,,i1}if i > j)." ></td>
	<td class="line x" title="64:122	1." ></td>
	<td class="line x" title="65:122	The degree of an arc eE is the number of connected components c in Ge such that the root of c is not dominated by the head of e. 2." ></td>
	<td class="line x" title="66:122	The degree of G is the maximum degree of any arc eE. To exemplify the notion of degree, we note that the dependency graph in Figure 1 (which satisfies SINGLE-HEAD and ACYCLICITY) has degree 1." ></td>
	<td class="line x" title="67:122	The only non-projective arc in the graph is (5,1) and G(5,1) contains three connected components, each of which consists of a single root node (2, 3 and 4)." ></td>
	<td class="line x" title="68:122	Since only one of these, 3, is not dominated by 5, the arc (5,1) has degree 1." ></td>
	<td class="line x" title="69:122	4 Parsing Algorithm Covington (2001) describes a parsing strategy for dependency representations that has been known 75 since the 1960s but not presented in the literature." ></td>
	<td class="line x" title="70:122	The left-to-right (or incremental) version of this strategy can be formulated in the following way: PARSE(x = (w1,,wn)) 1 for i = 1 up to n 2 for j = i1 down to 1 3 LINK(i, j) The operation LINK(i, j) nondeterministically chooses between (i) adding the arc i  j (with some label), (ii) adding the arc j i (with some label), and(iii)adding noarcatall." ></td>
	<td class="line x" title="71:122	Inthis way, the algorithm builds a graph by systematically trying to link every pair of nodes (i,j) (i > j)." ></td>
	<td class="line x" title="72:122	This graph will be a well-formed dependency graph, provided that we also add arcs from the root node 0 to every root node in1,,n}." ></td>
	<td class="line x" title="73:122	Assuming that the LINK(i, j)operation can be performed in some constant time c, the running time of the algorithm issummationtextni=1 c(n1) = c(n22  n2), which in terms of asymptotic complexity is O(n2)." ></td>
	<td class="line x" title="74:122	In the experiments reported in the following sections, we modify this algorithm by making the performance of LINK(i, j) conditional on the arcs (i,j) and (j,i) being permissible under the given graph constraints: PARSE(x = (w1,,wn)) 1 for i = 1 up to n 2 for j = i1 down to 1 3 if PERMISSIBLE(i, j, C) 4 LINK(i, j) The function PERMISSIBLE(i, j, C) returns true iff i  j and j  i are permissible arcs relative to the constraint C and the partially built graph G. For example, with the constraint SINGLEHEAD, LINK(i, j) will not be performed if both i and j already have a head in the dependency graph." ></td>
	<td class="line x" title="75:122	We call the pairs (i,j) (i > j) for which LINK(i, j) is performed (for a given sentence and set of constraints) the active pairs, and we use the number of active pairs, as a function of sentence length, as an abstract measure of running time." ></td>
	<td class="line x" title="76:122	This is well motivated if the time required to compute PERMISSIBLE(i,j,C) is insignificant compared to the time needed for LINK(i,j), as is typically the case in data-driven systems, where LINK(i,j) requires a call to a trained classifier, while PERMISSIBLE(i,j,C) only needs access to the partially built graph G. Theresults obtained in this waywill be partially dependent on the particular algorithm used, but they can in principle be generalized to any algorithm that tries to link all possible word pairs and that satisfies the following condition: For any graph G = (V,E,L) derived by the algorithm, if e,eprime E and e covers eprime, then the algorithm adds eprime before e. This condition is satisfied not only by Covingtons incremental algorithm but also by algorithms that add arcs strictly in order of increasing length, such as the algorithm of Eisner (2000) and other algorithms based on dynamic programming." ></td>
	<td class="line x" title="77:122	5 Experimental Setup The experiments are based on data from two treebanks." ></td>
	<td class="line x" title="78:122	The Prague Dependency Treebank (PDT) contains 1.5M words of newspaper text, annotated in three layers (Hajic, 1998; Hajic et al. , 2001) according to the theoretical framework of Functional Generative Description (Sgall et al. , 1986)." ></td>
	<td class="line x" title="79:122	Our experiments concern only the analytical layer and are based on the dedicated training section of the treebank." ></td>
	<td class="line x" title="80:122	The Danish Dependency Treebank (DDT) comprises 100K words of text selected from the Danish PAROLEcorpus, with annotation of primary and secondary dependencies based on Discontinuous Grammar (Kromann, 2003)." ></td>
	<td class="line x" title="81:122	Only primary dependencies are considered in the experiments, which are based on 80% of the data (again the standard training section)." ></td>
	<td class="line x" title="82:122	The experiments are performed by parsing each sentence of the treebanks while using the gold standard dependency graph for that sentence as an oracletoresolve thenondeterministic choice inthe LINK(i, j) operation as follows: LINK(i, j) 1 if (i,j)Eg 2 EE{(i,j)} 3 if (j,i)Eg 4 EE{(j,i)} where Eg is the arc relation of the gold standard dependency graph Gg and E is the arc relation of the graph G built by the parsing algorithm." ></td>
	<td class="line x" title="83:122	Conditions are varied by cumulatively adding constraints in the following order: 1." ></td>
	<td class="line x" title="84:122	SINGLE-HEAD 2." ></td>
	<td class="line x" title="85:122	ACYCLICITY 3." ></td>
	<td class="line x" title="86:122	Degree dk (k1) 4." ></td>
	<td class="line x" title="87:122	PROJECTIVITY 76 Table 1: Proportion of dependency arcs and complete graphs correctly parsed under different constraints in the Prague Dependency Treebank (PDT) and the Danish Dependency Treebank (DDT) PDT DDT Constraint Arcs Graphs Arcs Graphs n = 1255590 n = 73088 n = 80193 n = 4410 PROJECTIVITY 96.1569 76.8498 97.7754 84.6259 d1 99.7854 97.7507 99.8940 98.0272 d2 99.9773 99.5731 99.9751 99.5238 d3 99.9956 99.9179 99.9975 99.9546 d4 99.9983 99.9863 100.0000 100.0000 d5 99.9987 99.9945 100.0000 100.0000 d10 99.9998 99.9986 100.0000 100.0000 ACYCLICITY 100.0000 100.0000 100.0000 100.0000 SINGLE-HEAD 100.0000 100.0000 100.0000 100.0000 None 100.0000 100.0000 100.0000 100.0000 The purpose of the experiments is to study how different constraints influence expressivity and running time." ></td>
	<td class="line x" title="88:122	The first dimension is investigated by comparing the dependency graphs produced by the parser with the gold standard dependency graphs in the treebank." ></td>
	<td class="line x" title="89:122	This gives an indication of the extent to which naturally occurring structures can beparsed correctly under different constraints." ></td>
	<td class="line x" title="90:122	The results are reported both as the proportion of individual dependency arcs (per token) and as the proportion of complete dependency graphs (per sentence) recovered correctly by the parser." ></td>
	<td class="line x" title="91:122	In order to study the effects on running time, we examine how the number of active pairs varies as a function of sentence length." ></td>
	<td class="line x" title="92:122	Whereas the asymptotic worst-case complexity remains O(n2) under all conditions, the average running time will decrease with the number of active pairs if the LINK(i, j) operation is more expensive than the call to PERMISSIBLE(i, j, C)." ></td>
	<td class="line x" title="93:122	For data-driven dependency parsing, this is relevant not only for parsing efficiency, butalsobecause itmayimprove training efficiency by reducing the number ofpairs that need to be included in the training data." ></td>
	<td class="line x" title="94:122	6 Results and Discussion Table 1 displays the proportion of dependencies (single arcs) and sentences (complete graphs) in the two treebanks that can be parsed exactly with Covingtons algorithm under different constraints." ></td>
	<td class="line x" title="95:122	Starting at the bottom of the table, we see that the unrestricted algorithm (None) of course reproduces all the graphs exactly, but we also see that the constraints SINGLE-HEAD and ACYCLICITY do not put any real restrictions on expressivity with regard to the data at hand." ></td>
	<td class="line x" title="96:122	However, this is primarily a reflection of the design of the treebank annotation schemes, which in themselves require dependency graphs to obey these constraints.2 If we go to the other end of the table, we see that PROJECTIVITY, on the other hand, has a very noticeable effect on the parsers ability to capture the structures found in the treebanks." ></td>
	<td class="line x" title="97:122	Almost 25% of the sentences in PDT, and more than 15% in DDT, are beyond its reach." ></td>
	<td class="line x" title="98:122	At the level of individual dependencies, the effect is less conspicuous, but it is still the case in PDT that one dependency in twenty-five cannot be found by the parser even with a perfect oracle (one in fifty in DDT)." ></td>
	<td class="line x" title="99:122	It should be noted that the proportion of lost dependencies is about twice as high as the proportion of dependencies that are non-projective in themselves (Nivre and Nilsson, 2005)." ></td>
	<td class="line x" title="100:122	This is due to error propagation, since some projective arcs are blocked from the parsers view because of missing non-projective arcs." ></td>
	<td class="line x" title="101:122	Considering different bounds on the degree of non-projectivity, finally, wesee that even the tightest possible bound (d  1) gives a much better approximation than PROJECTIVITY, reducing the 2Itshould be remembered thatweareonly concerned with one layer of each annotation scheme, the analytical layer in PDT and the primary dependencies in DDT." ></td>
	<td class="line x" title="102:122	Taking several layers into account simultaneously would have resulted in more complex structures." ></td>
	<td class="line x" title="103:122	77 Table 2: Quadratic curve estimation for y = ax+bx2 (y = number of active pairs, x = number of words) PDT DDT Constraint a b r2 a b r2 PROJECTIVITY 1.9181 0.0093 0.979 1.7591 0.0108 0.985 d1 3.2381 0.0534 0.967 2.2049 0.0391 0.969 d2 3.1467 0.1192 0.967 2.0273 0.0680 0.964 ACYCLICITY 0.3845 0.2587 0.971 1.4285 0.1106 0.967 SINGLE-HEAD 0.7187 0.2628 0.976 1.9003 0.1149 0.967 None 0.5000 0.5000 1.000 0.5000 0.5000 1.000 proportion of non-parsable sentences with about 90% in both treebanks." ></td>
	<td class="line x" title="104:122	At the level of individual arcs, the reduction is even greater, about 95% for both data sets." ></td>
	<td class="line x" title="105:122	And ifweallow amaximum degree of2, wecan capture morethan99.9% ofall dependencies, and more than 99.5% of all sentences, in both PDTand DDT.Atthe same time, there seems to be no principled upper bound on the degree of non-projectivity, since in PDT not even an upper bound of 10 is sufficient to correctly capture all dependency graphs in the treebank.3 Let us now see how different constraints affect running time, as measured by the number of active pairs in relation to sentence length." ></td>
	<td class="line x" title="106:122	A plot of this relationship for a subset of the conditions can be found in Figure 2." ></td>
	<td class="line x" title="107:122	For reasons of space, we only display the data from DDT, but the PDT data exhibit very similar patterns." ></td>
	<td class="line x" title="108:122	Both treebanks are represented in Table 2, where we show the result of fitting the quadratic equation y = ax + bx2 to the data from each condition (where y is the numberofactivewordsandxisthenumberofwordsin thesentence)." ></td>
	<td class="line x" title="109:122	Theamountofvariance explained is given by the r2 value, which shows a very good fit under all conditions, with statistical significance beyond the 0.001 level.4 Both Figure 2 and Table 2 show very clearly that, with no constraints, the relationship between words and active pairs is exactly the one predicted by the worst case complexity (cf.section 4) and that, with each added constraint, this relationship becomes more and more linear in shape." ></td>
	<td class="line x" title="111:122	When we get to PROJECTIVITY, the quadratic coefficient b is so small that the average running time is practically linear for the great majority of sentences." ></td>
	<td class="line x" title="112:122	3The single sentence thatis not parsed correctly atd 10 has a dependency arc of degree 12." ></td>
	<td class="line x" title="113:122	4The curve estimation has been performed using SPSS." ></td>
	<td class="line x" title="114:122	However, the complexity is not much worse for the bounded degrees of non-projectivity (d  1, d  2)." ></td>
	<td class="line x" title="115:122	More precisely, for both data sets, the linear term ax dominates the quadratic term bx2 for sentences up to 50 words at d  1 and up to 30 words at d  2." ></td>
	<td class="line x" title="116:122	Given that sentences of 50 words or less represent 98.9% of all sentences in PDT and 98.3% in DDT (the corresponding percentages for 30 words being 88.9% and 86.0%), it seems that the average case running time can be regarded as linear also for these models." ></td>
	<td class="line x" title="117:122	7 Conclusion We have investigated a series of graph-theoretic constraints on dependency structures, aiming to find a better approximation than PROJECTIVITY for the structures found in naturally occurring data, while maintaining good parsing efficiency." ></td>
	<td class="line x" title="118:122	In particular, we have defined the degree of nonprojectivity in terms of the maximum number of connected components that occur under a dependency arc without being dominated by the head of that arc. Empirical experiments based on data from two treebanks, from different languages and with different annotation schemes, have shown that limiting the degree d of non-projectivity to 1 or 2 gives an average case running time that is linear in practice and allows us to capture about 98% of the dependency graphs actually found in the treebanks with d  1, and about 99.5% with d  2." ></td>
	<td class="line x" title="119:122	This is a substantial improvement over the projective approximation, which only allows 7585% of the dependency graphs to be captured exactly." ></td>
	<td class="line x" title="120:122	This suggests that the integration of such constraints into non-projective parsing algorithms will improve both accuracy and efficiency, but we have to leave the corroboration of this hypothesis as a topic for future research." ></td>
	<td class="line x" title="121:122	78 0.0 20.0 40.0 60.0 80.0 100.0 Words 0.00 1000.00 2000.00 3000.00 4000.00 Pairs None 0.0 20.0 40.0 60.0 80.0 100.0 Words 0.00 200.00 400.00 600.00 800.00 1000.00 1200.00 Pairs Single-Head 0.0 20.0 40.0 60.0 80.0 100.0 Words 0.00 200.00 400.00 600.00 800.00 1000.00 1200.00 Pairs Acyclic 0.0 20.0 40.0 60.0 80.0 100.0 Words 0.00 200.00 400.00 600.00 800.00 Pairs d <= 2 0.0 20.0 40.0 60.0 80.0 100.0 Words 0.00 100.00 200.00 300.00 400.00 500.00 600.00 Pairs d <= 1 0.0 20.0 40.0 60.0 80.0 100.0 Words 0.00 50.00 100.00 150.00 200.00 250.00 Pairs Projectivity Figure 2: Number of active pairs as a function of sentence length under different constraints (DDT) 79 Acknowledgments The research reported in this paper was partially funded by the Swedish Research Council (6212002-4207)." ></td>
	<td class="line x" title="122:122	The insightful comments of three anonymous reviewers helped improve the final version of the paper." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N06-1019
Partial Training For A Lexicalized-Grammar Parser
Clark, Stephen;Curran, James R.;"></td>
	<td class="line x" title="1:159	Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 144151, New York, June 2006." ></td>
	<td class="line x" title="2:159	c2006 Association for Computational Linguistics Partial Training for a Lexicalized-Grammar Parser Stephen Clark Oxford University Computing Laboratory Wolfson Building, Parks Road Oxford, OX1 3QD, UK stephen.clark@comlab.ox.ac.uk James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Abstract We propose a solution to the annotation bottleneck for statistical parsing, by exploiting the lexicalized nature of Combinatory Categorial Grammar (CCG)." ></td>
	<td class="line x" title="3:159	The parsing model uses predicate-argument dependencies for training, which are derived from sequences of CCG lexical categories rather than full derivations." ></td>
	<td class="line x" title="4:159	A simple method is used for extracting dependencies from lexical category sequences, resulting in high precision, yet incomplete and noisy data." ></td>
	<td class="line x" title="5:159	The dependency parsing model of Clark and Curran (2004b) is extended to exploit this partial training data." ></td>
	<td class="line x" title="6:159	Remarkably, the accuracy of the parser trained on data derived from category sequences alone is only 1.3% worse in terms of F-score than the parser trained on complete dependency structures." ></td>
	<td class="line x" title="7:159	1 Introduction State-of-the-art statistical parsers require large amounts of hand-annotated training data, and are typically based on the Penn Treebank, the largest treebank available for English." ></td>
	<td class="line pc" title="8:159	Even robust parsers using linguistically sophisticated formalisms, such as TAG (Chiang, 2000), CCG (Clark and Curran, 2004b; Hockenmaier, 2003), HPSG (Miyao et al. , 2004) and LFG (Riezler et al. , 2002; Cahill et al. , 2004), often use training data derived from the Penn Treebank." ></td>
	<td class="line x" title="9:159	The labour-intensive nature of the treebank development process, which can take many years, creates a significant barrier for the development of parsers for new domains and languages." ></td>
	<td class="line x" title="10:159	Previous work has attempted parser adaptation without relying on treebank data from the new domain (Steedman et al. , 2003; Lease and Charniak, 2005)." ></td>
	<td class="line x" title="11:159	In this paper we propose the use of annotated data in the new domain, but only partially annotated data, which reduces the annotation effort required (Hwa, 1999)." ></td>
	<td class="line x" title="12:159	We develop a parsing model whichcanbetrainedusingpartialdata,byexploiting the properties of lexicalized grammar formalisms." ></td>
	<td class="line x" title="13:159	The formalism we use is Combinatory Categorial Grammar (Steedman, 2000), together with a parsing model described in Clark and Curran (2004b) which we adapt for use with partial data." ></td>
	<td class="line x" title="14:159	Parsing with Combinatory Categorial Grammar (CCG) takes place in two stages: first, CCG lexical categories are assigned to the words in the sentence, and then the categories are combined by the parser (Clark and Curran, 2004a)." ></td>
	<td class="line x" title="15:159	The lexical categories can be thought of as detailed part of speech tags and typically express subcategorization information." ></td>
	<td class="line x" title="16:159	We exploit the fact that CCG lexical categories contain a lot of syntactic information, and can therefore be used for training a full parser, even though attachment information is not explicitly represented in a category sequence." ></td>
	<td class="line x" title="17:159	Our partial training regime only requires sentences to be annotated with lexical categories, rather than full parse trees; therefore the data can be produced much more quickly for a new domain or language (Clark et al. , 2004)." ></td>
	<td class="line x" title="18:159	The partial training method uses the log-linear dependency model described in Clark and Curran (2004b), which uses sets of predicate-argument de144 pendencies, ratherthanderivations, fortraining." ></td>
	<td class="line x" title="19:159	Our novel idea is that, since there is so much information in the lexical category sequence, most of the correct dependencies can be easily inferred from the categories alone." ></td>
	<td class="line x" title="20:159	More specifically, for a given sentence and lexical category sequence, we train on those predicate-argument dependencies which occur in k% of the derivations licenced by the lexical categories." ></td>
	<td class="line x" title="21:159	By setting the k parameter high, we can produce a set of high precision dependencies for training." ></td>
	<td class="line x" title="22:159	A similar idea is proposed by Carroll and Briscoe (2002) for producing high precision data for lexical acquisition." ></td>
	<td class="line x" title="23:159	Using this procedure we are able to produce dependency data with over 99% precision and, remarkably, up to 86% recall, when compared against the complete gold-standard dependency data." ></td>
	<td class="line x" title="24:159	The high recall figure results from the significant amount of syntactic information in the lexical categories, which reduces the ambiguity in the possible dependency structures." ></td>
	<td class="line x" title="25:159	Since the recall is not 100%, we require a log-linear training method which works with partial data." ></td>
	<td class="line x" title="26:159	Riezler et al.(2002) describe a partial training method for a log-linear LFG parsing model in which the correct LFG derivations for a sentence are those consistent with the less detailed gold standard derivation from the Penn Treebank." ></td>
	<td class="line x" title="28:159	We use a similar method here by treating a CCG derivation as correct if it is consistent with the highprecisionpartialdependencystructure." ></td>
	<td class="line x" title="29:159	Section3explains what we mean by consistency in this context." ></td>
	<td class="line x" title="30:159	Surprisingly, the accuracy of the parser trained on partial data approaches that of the parser trained on full data: our best partial-data model is only 1.3% worse in terms of dependency F-score than the fulldata model, despite the fact that the partial data does not contain any explicit attachment information." ></td>
	<td class="line x" title="31:159	2 The CCG Parsing Model Clark and Curran (2004b) describes two log-linear parsing models for CCG: a normal-form derivation model and a dependency model." ></td>
	<td class="line x" title="32:159	In this paper we use the dependency model, which requires sets of predicate-argument dependencies for training.1 1Hockenmaier and Steedman (2002) describe a generative model of normal-form derivations; one possibility for training this model on partial data, which has not been explored, is to use the EM algorithm (Pereira and Schabes, 1992)." ></td>
	<td class="line x" title="33:159	The predicate-argument dependencies are represented as 5-tuples: hf,f,s,ha,l, where hf is the lexical item of the lexical category expressing the dependency relation; f is the lexical category; s is the argument slot; ha is the head word of the argument; and l encodes whether the dependency is non-local." ></td>
	<td class="line x" title="34:159	For example, the dependency encoding company as the object of bought (as in IBM bought the company) is represented as follows: bought2, (S\NP1)/NP2, 2, company4, (1) CCG dependency structures are sets of predicateargument dependencies." ></td>
	<td class="line x" title="35:159	We define the probability ofadependencystructureasthesumoftheprobabilities of all those derivations leading to that structure (Clark and Curran, 2004b)." ></td>
	<td class="line x" title="36:159	Spurious ambiguity in CCG means that there can be more than one derivation leading to any one dependency structure." ></td>
	<td class="line x" title="37:159	Thus, the probability of a dependency structure, pi, given a sentence, S, is defined as follows: P(pi|S) = summationdisplay d(pi) P(d,pi|S) (2) where (pi) is the set of derivations which lead to pi." ></td>
	<td class="line x" title="38:159	The probability of a d,pi pair, , conditional on a sentence S, is defined using a log-linear form: P(|S) = 1Z S e.f() (3) where .f() =summationtexti ifi()." ></td>
	<td class="line x" title="39:159	The function fi is the integer-valued frequency function of the ith feature; i is the weight of the ith feature; and ZS is a normalising constant." ></td>
	<td class="line x" title="40:159	Clark and Curran (2004b) describes the training procedure for the dependency model, which uses a discriminativeestimationmethodbymaximisingthe conditional likelihood of the model given the data (Riezler et al. , 2002)." ></td>
	<td class="line x" title="41:159	The optimisation of the objective function is performed using the limited-memory BFGS numerical optimisation algorithm (Nocedal and Wright, 1999; Malouf, 2002), which requires calculation of the objective function and the gradient of the objective function at each iteration." ></td>
	<td class="line x" title="42:159	The objective function is defined below, where L() is the likelihood and G() is a Gaussian prior term for smoothing." ></td>
	<td class="line x" title="43:159	145 He anticipates growth for the auto maker NP (S[dcl]\NP)/NP NP (NP\NP)/NP NP[nb]/N N/N N Figure 1: Example sentence with CCG lexical categories Lprime() = L() G() (4) = msummationdisplay j=1 log summationdisplay d(pij) e.f(d,pij)  msummationdisplay j=1 log summationdisplay (Sj) e.f()  nsummationdisplay i=1 2i 22 S1,,Sm are the sentences in the training data; pi1,,pim are the corresponding gold-standard dependency structures; (S) is the set of possible derivation, dependency-structure pairs for S;  is a smoothing parameter; and n is the number of features." ></td>
	<td class="line x" title="44:159	The components of the gradient vector are: Lprime() i = msummationdisplay j=1 summationdisplay d(pij) e.f(d,pij)fi(d,pij) summationtext d(pij) e.f(d,pij) (5)  msummationdisplay j=1 summationdisplay (Sj) e.f()fi() summationtext (Sj) e.f()  i2 The first two terms of the gradient are expectations of feature fi: the first expectation is over all derivations leading to each gold-standard dependency structure, and the second is over all derivations for each sentence in the training data." ></td>
	<td class="line x" title="45:159	The estimation process attempts to make the expectations in (5) equal (ignoring the Gaussian prior term)." ></td>
	<td class="line x" title="46:159	Another way to think of the estimation process is that it attempts to put as much mass as possible on the derivations leading to the gold-standard structures (Riezler et al. , 2002)." ></td>
	<td class="line x" title="47:159	Calculation of the feature expectations requires summing over all derivations for a sentence, and summing over all derivations leading to a goldstandard dependency structure." ></td>
	<td class="line x" title="48:159	Clark and Curran (2003) shows how the sum over the complete derivation space can be performed efficiently using a packed chart and the inside-outside algorithm, and Clark and Curran (2004b) extends this method to sum over all derivations leading to a gold-standard dependency structure." ></td>
	<td class="line x" title="49:159	3 Partial Training The partial data we use for training the dependency model is derived from CCG lexical category sequences only." ></td>
	<td class="line x" title="50:159	Figure 1 gives an example sentence adapted from CCGbank (Hockenmaier, 2003) together with its lexical category sequence." ></td>
	<td class="line x" title="51:159	Note that, although the attachment of the prepositional phrase to the noun phrase is not explicitly represented, it can be inferred in this example because the lexical category assigned to the preposition has to combine with a noun phrase to the left, and in this example there is only one possibility." ></td>
	<td class="line x" title="52:159	One of the key insights in this paper is that the significant amount of syntactic information in CCG lexical categories allows us to infer attachment information in many cases." ></td>
	<td class="line x" title="53:159	Theprocedureweuseforextractingdependencies from a sequence of lexical categories is to return all thosedependencieswhichoccurink%ofthederivations licenced by the categories." ></td>
	<td class="line x" title="54:159	By giving the k parameter a high value, we can extract sets of dependencies with very high precision; in fact, assuming that the correct lexical category sequence licences the correct derivation, setting k to 100 must result in 100% precision, sinceanydependency whichoccurs in every derivation must occur in the correct derivation." ></td>
	<td class="line x" title="55:159	Of course the recall is not guaranteed to be high; decreasingk has the effect of increasing recall, but at the cost of decreasing precision." ></td>
	<td class="line x" title="56:159	The training method described in Section 2 can be adapted to use the (potentially incomplete) sets of dependencies returned by our extraction procedure." ></td>
	<td class="line x" title="57:159	In Section 2 a derivation was considered correct if it produced the complete set of gold-standard dependencies." ></td>
	<td class="line x" title="58:159	In our partial-data version a derivation is considered correct if it produces dependencies which are consistent with the dependencies returned by our extraction procedure." ></td>
	<td class="line x" title="59:159	We define consistency as follows: a set of dependencies D is consistent with a set G if G is a subset of D. We also say that a derivation d is consistent with dependency set G if G is a subset of the dependencies produced by d. 146 This definition of correct derivation will introduce some noise into the training data." ></td>
	<td class="line x" title="60:159	Noise arises from sentences where the recall of the extracted dependencies is less than 100%, since some of the derivations which are consistent with the extracted dependencies for such sentences will be incorrect." ></td>
	<td class="line x" title="61:159	Noise also arises from sentences where the precisionoftheextracteddependenciesislessthan100%, since for these sentences every derivation which is consistent with the extracted dependencies will be incorrect." ></td>
	<td class="line x" title="62:159	The hope is that, if an incorrect derivation produces mostly correct dependencies, then it can still be useful for training." ></td>
	<td class="line x" title="63:159	Section 4 shows how the precision and recall of the extracted dependencies varies with k and how this affects parsing accuracy." ></td>
	<td class="line x" title="64:159	The definitions of the objective function (4) and the gradient (5) for training remain the same in the partial-data case; the only differences are that (pi) isnowdefinedtobethosederivationswhichareconsistent with the partial dependency structure pi, and the gold-standard dependency structures pij are the partial structures extracted from the gold-standard lexical category sequences.2 Clark and Curran (2004b) gives an algorithm for finding all derivations in a packed chart which produce a particular set of dependencies." ></td>
	<td class="line x" title="65:159	This algorithm is required for calculating the value of the objective function (4) and the first feature expectation in (5)." ></td>
	<td class="line x" title="66:159	We adapt this algorithm for finding all derivations which are consistent with a partial dependency structure." ></td>
	<td class="line x" title="67:159	The new algorithm is shown in Figure 2." ></td>
	<td class="line x" title="68:159	The algorithm relies on the definition of a packed chart, which is an instance of a feature forest (Miyao and Tsujii, 2002)." ></td>
	<td class="line x" title="69:159	The idea behind a packed chart is that equivalent chart entries of the same type and in the same cell are grouped together, and back pointers to the daughters indicate how an individual entry was created." ></td>
	<td class="line x" title="70:159	Equivalent entries form the same structures in any subsequent parsing." ></td>
	<td class="line x" title="71:159	A feature forest is defined in terms of disjunctive and conjunctive nodes." ></td>
	<td class="line x" title="72:159	For a packed chart, the individual entries in acell are conjunctive nodes, and the equivalence classes of entries are disjunctive nodes." ></td>
	<td class="line x" title="73:159	The definition of a feature forest is as follows: A feature forest  is a tuple C,D,R,, where: 2Note that the procedure does return all the gold-standard dependencies for some sentences." ></td>
	<td class="line x" title="74:159	C,D,R,, is a packed chart / feature forest G is a set of dependencies returned by the extraction procedure Let c be a conjunctive node Let d be a disjunctive node deps(c) is the set of dependencies on node c cdeps(c) = |deps(c)  G| dmax(c) =summationtextd(c) dmax(d)+ cdeps(c) dmax(d) = maxdmax(c) | c  (d)} mark(d): mark d as a correct node foreach c  (d) if dmax(c) == dmax(d) mark c as a correct node foreach dprime  (c) mark(dprime) foreach dr  R such that dmax." ></td>
	<td class="line x" title="75:159	(dr) = |G| mark(dr) Figure 2: Finding nodes in derivations consistent with a partial dependency structure  C is a set of conjunctive nodes;  D is a set of disjunctive nodes;  R  D is a set of root disjunctive nodes;   : D  2C isaconjunctivedaughterfunction;   : C  2D is a disjunctive daughter function." ></td>
	<td class="line x" title="76:159	Dependencies are associated with conjunctive nodes in the feature forest." ></td>
	<td class="line x" title="77:159	For example, if the disjunctive nodes (equivalence classes of individual entries) representing the categories NP and S\NP combine to produce a conjunctive node S, the resulting S node will have a verb-subject dependency associated with it." ></td>
	<td class="line x" title="78:159	In Figure 2, cdeps(c) is the number of dependencies on conjunctive node c which appear in partial structure G; dmax(c) is the maximum number of dependencies in G produced by any sub-derivation headed by c; dmax(d) is the same value for disjunctive node d. Recursive definitions for calculating these values are given; the base case occurs when conjunctive nodes have no disjunctive daughters." ></td>
	<td class="line x" title="79:159	Thealgorithmidentifiesallthoserootnodesheading derivations which are consistent with the partial dependency structure G, and traverses the chart topdown marking the nodes in those derivations." ></td>
	<td class="line x" title="80:159	The insight behind the algorithm is that, for two conjunctive nodes in the same equivalence class, if one node heads a sub-derivation producing more dependencies in G than the other node, then the node with 147 less dependencies inGcannot be part of a derivation consistent with G. The conjunctive and disjunctive nodes appearing in derivations consistent with G form a new goldstandard feature forest." ></td>
	<td class="line x" title="81:159	The gold-standard forest, and the complete forest containing all derivations spanning the sentence, can be used to estimate the likelihood value and feature expectations required by the estimation algorithm." ></td>
	<td class="line x" title="82:159	Let Efi be the expected value of fi over the forest  for model ; then the values in (5) can be obtained by calculating Ej fi for the complete forest j for each sentence Sj in the training data (the second sum in (5)), and also Ej fi for each forest j of derivations consistentwiththepartialgold-standarddependencystructure for sentence Sj (the first sum in (5)): L() i = msummationdisplay j=1 (Ej fi Ej fi) (6) The likelihood in (4) can be calculated as follows: L() = msummationdisplay j=1 (logZj  logZj) (7) where logZ is the normalisation constant for ." ></td>
	<td class="line x" title="83:159	4 Experiments The resource used for the experiments is CCGbank (Hockenmaier, 2003), which consists of normalform CCG derivations derived from the phrasestructure trees in the Penn Treebank." ></td>
	<td class="line x" title="84:159	It also contains predicate-argument dependencies which we use for development and final evaluation." ></td>
	<td class="line x" title="85:159	4.1 Accuracy of Dependency Extraction Sections 2-21 of CCGbank were used to investigate the accuracy of the partial dependency structures returned by the extraction procedure." ></td>
	<td class="line x" title="86:159	Full, correct dependency structures for the sentences in 2-21 were created by running our CCG parser (Clark and Curran, 2004b) over the gold-standard derivation for each sentence, outputting the dependencies." ></td>
	<td class="line x" title="87:159	This resultedinfulldependencystructuresfor37,283ofthe sentences in sections 2-21." ></td>
	<td class="line x" title="88:159	Table 1 gives precision and recall values for the dependencies obtained from the extraction procedure, for the 37,283 sentences for which we have k Precision Recall SentAcc 0.99999 99.76 74.96 13.84 0.9 99.69 79.37 16.52 0.85 99.65 81.30 18.40 0.8 99.57 82.96 19.51 0.7 99.09 85.87 22.46 0.6 98.00 88.67 26.28 Table 1: Accuracy of the Partial Dependency Data complete dependency structures." ></td>
	<td class="line x" title="89:159	The SentAcc column gives the percentage of training sentences for which the partial dependency structures are completely correct." ></td>
	<td class="line x" title="90:159	For a given sentence, the extraction procedure returns all dependencies occurring in at least k% of the derivations licenced by the goldstandard lexical category sequence." ></td>
	<td class="line x" title="91:159	The lexical category sequences for the sentences in 2-21 can easily be read off the CCGbank derivations." ></td>
	<td class="line x" title="92:159	The derivations licenced by a lexical category sequence were created using the CCG parser described inClarkandCurran(2004b)." ></td>
	<td class="line x" title="93:159	Theparserusesasmall number of combinatory rules to combine the categories, along with the CKY chart-parsing algorithm described in Steedman (2000)." ></td>
	<td class="line x" title="94:159	It also uses some unary type-changing rules and punctuation rules obtainedfromthederivationsinCCGbank.3 Theparser builds a packed representation, and counting the number of derivations in which a dependency occurs can be performed using a dynamic programming algorithm similar to the inside-outside algorithm." ></td>
	<td class="line x" title="95:159	Table 1 shows that, by varying the value of k, it is possible to get the recall of the extracted dependencies as high as 85.9%, while still maintaining a precision value of over 99%." ></td>
	<td class="line x" title="96:159	4.2 Accuracy of the Parser The training data for the dependency model was created by first supertagging the sentences in sections 2-21, using the supertagger described in Clark and Curran (2004b).4 The average number of categories 3Since our training method is intended to be applicable in the absence of derivation data, the use of such rules may appear suspect." ></td>
	<td class="line x" title="97:159	However, we argue that the type-changing and punctuation rules could be manually created for a new domain by examining the lexical category data." ></td>
	<td class="line x" title="98:159	4An improved version of the supertagger was used for this paper in which the forward-backward algorithm is used to calculate the lexical category probability distributions." ></td>
	<td class="line x" title="99:159	148 assigned to each word is determined by a parameter, , in the supertagger." ></td>
	<td class="line x" title="100:159	A category is assigned to a word if the categorys probability is within  of the highest probability category for that word." ></td>
	<td class="line x" title="101:159	For these experiments, we used a  value of 0.01, which assigns roughly 1.6 categories to each word, on average; we also ensured that the correct lexical category was in the set assigned to each word." ></td>
	<td class="line x" title="102:159	(We did not do this when parsing the test data)." ></td>
	<td class="line x" title="103:159	For some sentences, the packed charts can become very large." ></td>
	<td class="line x" title="104:159	Thesupertaggingapproachweadoptfortraining differs to that used for testing: if the size of the chart exceeds some threshold, the value of  is increased, reducing ambiguity, and the sentence is supertagged and parsed again." ></td>
	<td class="line x" title="105:159	The threshold which limits the size of the charts was set at 300000 individual entries." ></td>
	<td class="line x" title="106:159	Two further values of  were used: 0.05 and 0.1." ></td>
	<td class="line x" title="107:159	Packed charts were created for each sentence and stored in memory." ></td>
	<td class="line x" title="108:159	It is essential that the packed charts for each sentence contain at least one derivation leading to the gold-standard dependency structure." ></td>
	<td class="line x" title="109:159	Not all rule instantiations in CCGbank can be produced by our parser; hence it is not possible to produce the gold standard for every sentence in Sections 2-21." ></td>
	<td class="line x" title="110:159	For the full-data model we used 34336 sentences (86.7% of the total)." ></td>
	<td class="line x" title="111:159	For the partial-data models we were able to use slightly more, since the partial structures are easier to produce." ></td>
	<td class="line x" title="112:159	Here we used 35,709 sentences (k = 0.85)." ></td>
	<td class="line x" title="113:159	Since some of the packed charts are very large, we used an 18-node Beowulf cluster, together with a parallel version of the BFGS training algorithm." ></td>
	<td class="line x" title="114:159	The training time and number of iterations to convergencewere172minutesand997iterationsforthe full-data model, and 151 minutes and 861 iterations for the partial-data model (k = 0.85)." ></td>
	<td class="line x" title="115:159	Approximate memory usage in each case was 17.6 GB of RAM." ></td>
	<td class="line x" title="116:159	The dependency model uses the same set of features described in Clark and Curran (2004b): dependency features representing predicate-argument dependencies (with and without distance measures); rule instantiation features encoding the combining categories together with the result category (with and without a lexical head); lexical category features, consisting of wordcategory pairs at the leaf nodes; and root category features, consisting of headwordcategory pairs at the root nodes." ></td>
	<td class="line x" title="117:159	Further k LP LR F CatAcc 0.99999 85.80 84.51 85.15 93.77 0.9 85.86 84.51 85.18 93.78 0.85 85.89 84.50 85.19 93.71 0.8 85.89 84.45 85.17 93.70 0.7 85.52 84.07 84.79 93.72 0.6 84.99 83.70 84.34 93.65 FullData 87.16 85.84 86.50 93.79 Random 74.63 72.53 73.57 89.31 Table 2: Accuracy of the Parser on Section 00 generalised features for each feature type are formed by replacing words with their POS tags." ></td>
	<td class="line x" title="118:159	Only features which occur more than once in the training data are included, except that the cutoff for the rule features is 10 or more and the counting is performed across all derivations licenced by the gold-standard lexical category sequences." ></td>
	<td class="line x" title="119:159	The larger cutoff was used since the productivity of the grammarcanleadtolargenumbersofthesefeatures." ></td>
	<td class="line x" title="120:159	The dependency model has 548590 features." ></td>
	<td class="line x" title="121:159	In ordertoprovideafaircomparison, thesamefeatureset was used for the partial-data and full-data models." ></td>
	<td class="line x" title="122:159	The CCG parsing consists of two phases: first the supertagger assigns the most probable categories to each word, and then the small number of combinatory rules, plus the type-changing and punctuation rules, are used with the CKY algorithm to build a packedchart.5 WeusethemethoddescribedinClark and Curran (2004b) for integrating the supertagger with the parser: initially a small number of categories is assigned to each word, and more categories are requested if the parser cannot find a spanning analysis." ></td>
	<td class="line x" title="123:159	The maximum-recall algorithm described in Clark and Curran (2004b) is used to find the highest scoring dependency structure." ></td>
	<td class="line x" title="124:159	Table2givestheaccuracyoftheparseronSection 00 of CCGbank, evaluated against the predicateargument dependencies in CCGbank.6 The table gives labelled precision, labelled recall and F-score, and lexical category accuracy." ></td>
	<td class="line x" title="125:159	Numbers are given for the partial-data model with various values of k, and for the full-data model, which provides an up5Gold-standard POS tags from CCGbank were used for all the experiments in this paper." ></td>
	<td class="line x" title="126:159	6There are some dependency types produced by our parser which are not in CCGbank; these were ignored for evaluation." ></td>
	<td class="line x" title="127:159	149 LP LR F CatAcc k = 0.85 86.21 85.01 85.60 93.90 FullData 87.50 86.37 86.93 94.01 Table 3: Accuracy of the Parser on Section 23 k Precision Recall SentAcc 0.99999 99.71 80.16 17.48 0.9999 99.68 82.09 19.13 0.999 99.49 85.18 22.18 0.99 99.00 88.95 27.69 0.95 98.34 91.69 34.95 0.9 97.82 92.84 39.18 Table 4: Accuracy of the Partial Dependency Data using Inside-Outside Scores per bound for the partial-data model." ></td>
	<td class="line x" title="128:159	We also give a lower bound which we obtain by randomly traversing a packed chart top-down, giving equal probability to each conjunctive node in an equivalence class." ></td>
	<td class="line x" title="129:159	The precision and recall figures are over those sentences for which the parser returned an analysis (99.27% of Section 00)." ></td>
	<td class="line x" title="130:159	The best result is obtained for a k value of 0.85, which produces partial dependency data with a precision of 99.7 and a recall of 81.3." ></td>
	<td class="line x" title="131:159	Interestingly, the results show that decreasing k further, which results in partial data with a higher recall and only a slight loss in precison, harms the accuracy of the parser." ></td>
	<td class="line x" title="132:159	The Random result also dispels any suspicion that the partial-model is performing well simply because of the supertagger; clearly there is still much work to be done after the supertagging phase." ></td>
	<td class="line x" title="133:159	Table 3 gives the accuracy of the parser on Section23, usingthebestperformingpartial-datamodel on Section 00." ></td>
	<td class="line x" title="134:159	The precision and recall figures are over those sentences for which the parser returned an analysis (99.63% of Section 23)." ></td>
	<td class="line x" title="135:159	The results show that the partial-data model is only 1.3% Fscore short of the upper bound." ></td>
	<td class="line x" title="136:159	4.3 Further Experiments with Inside-Outside In a final experiment, we attempted to exploit the high accuracy of the partial-data model by using it to provide new training data." ></td>
	<td class="line x" title="137:159	For each sentence in Section 2-21, we parsed the gold-standard lexical category sequences and used the best performing partial-data model to assign scores to each dependency in the packed chart." ></td>
	<td class="line x" title="138:159	The score for a dependency was the sum of the probabilities of all derivations producing that dependency, which can be calculated using the inside-outside algorithm." ></td>
	<td class="line x" title="139:159	(This is the score used by the maximum-recall parsing algorithm)." ></td>
	<td class="line x" title="140:159	Partial dependency structures were then created by returning all dependencies whose score was above some threshold k, as before." ></td>
	<td class="line x" title="141:159	Table 4 gives the accuracy of the data created by this procedure." ></td>
	<td class="line x" title="142:159	Note how these values differ to those reported in Table 1." ></td>
	<td class="line x" title="143:159	We then trained the dependency model on this partial data using the same method as before." ></td>
	<td class="line x" title="144:159	However, the peformance of the parser on Section 00 using these new models was below that of the previous best performing partial-data model for all values of k. We report this negative result because we had hypothesised that using a probability model to score the dependencies, rather than simply the number of derivations in which they occur, would lead to improved performance." ></td>
	<td class="line x" title="145:159	5 Conclusions Our main result is that it is possible to train a CCG dependency model from lexical category sequences alone and still obtain parsing results which are only 1.3% worse in terms of labelled F-score than a model trained on complete data." ></td>
	<td class="line x" title="146:159	This is a noteworthy result and demonstrates the significant amount of information encoded in CCG lexical categories." ></td>
	<td class="line x" title="147:159	The engineering implication is that, since the dependency model can be trained without annotating recursive structures, and only needs sequence information at the word level, then it can be ported rapidly to a new domain (or language) by annotating new sequence data in that domain." ></td>
	<td class="line x" title="148:159	One possible response to this argument is that, since the lexical category sequence contains so much syntactic information, then the task of annotating category sequences must be almost as labour intensive as annotating full derivations." ></td>
	<td class="line x" title="149:159	To test this hypothesis fully would require suitable annotation tools and subjects skilled in CCG annotation, which we do not currently have access to." ></td>
	<td class="line x" title="150:159	However, there is some evidence that annotating category sequences can be done very efficiently." ></td>
	<td class="line x" title="151:159	Clark et al.(2004) describes a porting experiment 150 in which a CCG parser is adapted for the question domain." ></td>
	<td class="line x" title="153:159	The supertagger component of the parser is trained on questions annotated at the lexical category level only." ></td>
	<td class="line x" title="154:159	The training data consists of over 1,000 annotated questions which took less than a week to create." ></td>
	<td class="line x" title="155:159	This suggests, as a very rough approximation, that 4 annotators could annotate 40,000 sentences with lexical categories (the size of the Penn Treebank) in a few months." ></td>
	<td class="line x" title="156:159	Another advantage of annotating with lexical categories is that a CCG supertagger can be used to perform most of the annotation, with the human annotator only required to correct the mistakes made by the supertagger." ></td>
	<td class="line x" title="157:159	An accurate supertagger can be bootstrapped quicky, leaving only a small number of corrections for the annotator." ></td>
	<td class="line x" title="158:159	A similar procedure is suggestedbyDoranetal.(1997)forportingan LTAG grammar to a new domain." ></td>
	<td class="line x" title="159:159	We have a proposed a novel solution to the annotation bottleneck for statistical parsing which exploits the lexicalized nature of CCG, and may therefore be applicable to other lexicalized grammar formalisms such as LTAG." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-1130
Robust PCFG-Based Generation Using Automatically Acquired LFG Approximations
Cahill, Aoife;Van Genabith, Josef;"></td>
	<td class="line x" title="1:183	Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 10331040, Sydney, July 2006." ></td>
	<td class="line oc" title="2:183	c2006 Association for Computational Linguistics Robust PCFG-Based Generation using Automatically Acquired LFG Approximations Aoife Cahill1 and Josef van Genabith1,2 1 National Centre for Language Technology (NCLT) School of Computing, Dublin City University, Dublin 9, Ireland 2 Center for Advanced Studies, IBM Dublin, Ireland {acahill,josef}@computing.dcu.ie Abstract We present a novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations (Cahill et al. , 2004) automatically extracted from treebanks, maximising the probability of a tree given an f-structure." ></td>
	<td class="line x" title="3:183	We evaluate our approach using stringbased evaluation." ></td>
	<td class="line x" title="4:183	We currently achieve coverage of 95.26%, a BLEU score of 0.7227 and string accuracy of 0.7476 on the Penn-II WSJ Section 23 sentences of length 20." ></td>
	<td class="line x" title="5:183	1 Introduction Wide coverage grammars automatically extracted from treebanks are a corner-stone technology in state-of-the-art probabilistic parsing." ></td>
	<td class="line x" title="6:183	They achieve robustness and coverage at a fraction of the development cost of hand-crafted grammars." ></td>
	<td class="line x" title="7:183	It is surprising to note that to date, such grammars do not usually figure in the complementary operation to parsing  natural language surface realisation." ></td>
	<td class="line x" title="8:183	Research on statistical natural language surface realisation has taken three broad forms, differing in where statistical information is applied in the generation process." ></td>
	<td class="line x" title="9:183	Langkilde (2000), for example, uses n-gram word statistics to rank alternative output strings from symbolic hand-crafted generators to select paths in parse forest representations." ></td>
	<td class="line x" title="10:183	Bangalore and Rambow (2000) use n-gram word sequence statistics in a TAG-based generation model to rank output strings and additional statistical and symbolic resources at intermediate generation stages." ></td>
	<td class="line x" title="11:183	Ratnaparkhi (2000) uses maximum entropy models to drive generation with word bigram or dependency representations taking into account (unrealised) semantic features." ></td>
	<td class="line x" title="12:183	Valldal and Oepen (2005) present a discriminative disambiguation model using a hand-crafted HPSG grammar for generation." ></td>
	<td class="line x" title="13:183	Belz (2005) describes a method for building statistical generation models using an automatically created generation treebank for weather forecasts." ></td>
	<td class="line x" title="14:183	None of these probabilistic approaches to NLG uses a full treebank grammar to drive generation." ></td>
	<td class="line x" title="15:183	Bangalore et al.(2001) investigate the effect of training size on performance while using grammars automatically extracted from the PennII Treebank (Marcus et al. , 1994) for generation." ></td>
	<td class="line x" title="17:183	Using an automatically extracted XTAG grammar, they achieve a string accuracy of 0.749 on their test set." ></td>
	<td class="line x" title="18:183	Nakanishi et al.(2005) present probabilistic models for a chart generator using a HPSG grammar acquired from the Penn-II Treebank (the Enju HPSG)." ></td>
	<td class="line x" title="20:183	They investigate discriminative disambiguation models following Valldal and Oepen (2005) and their best model achieves coverage of 90.56% and a BLEU score of 0.7723 on Penn-II WSJ Section 23 sentences of length 20." ></td>
	<td class="line oc" title="21:183	In this paper we present a novel PCFG-based architecture for probabilistic generation based on wide-coverage, robust Lexical Functional Grammar (LFG) approximations automatically extracted from treebanks (Cahill et al. , 2004)." ></td>
	<td class="line x" title="22:183	In Section 2 we briefly describe LFG (Kaplan and Bresnan, 1982)." ></td>
	<td class="line x" title="23:183	Section 3 presents our generation architecture." ></td>
	<td class="line x" title="24:183	Section 4 presents evaluation results on the Penn-II WSJ Section 23 test set using string-based metrics." ></td>
	<td class="line x" title="25:183	Section 5 compares our approach with alternative approaches in the literature." ></td>
	<td class="line x" title="26:183	Section 6 concludes and outlines further research." ></td>
	<td class="line x" title="27:183	2 Lexical Functional Grammar Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982) is a constraint-based theory of grammar." ></td>
	<td class="line x" title="28:183	It (minimally) posits two levels of representation, c(onstituent)-structure and f(unctional)structure." ></td>
	<td class="line x" title="29:183	C-structure is represented by contextfree phrase-structure trees, and captures surface 1033 S = NP VP ( SUBJ)=  = NNP V SBAR = = ( COMP)=  They believe S ( PRED) = pro ( PRED) = believe = ( NUM) = PL ( TENSE) = present ( PERS) = 3 NP VP ( SUBJ)=  = NNP V = = John resigned ( PRED) = John ( PRED) = resign ( NUM) = SG ( TENSE) = PAST ( PERS) = 3 f1:      PRED BELIEVE(SUBJ)(COMP) SUBJ f2: bracketleftbigg PRED PRO NUM PL PERS 3 bracketrightbigg COMP f3:   SUBJ f4: bracketleftbigg PRED JOHN NUM SG PERS 3 bracketrightbigg PRED RESIGN(SUBJ) TENSE PAST   TENSE PRESENT      Figure 1: Cand f-structures for the sentence They believe John resigned." ></td>
	<td class="line x" title="30:183	grammatical configurations such as word order." ></td>
	<td class="line x" title="31:183	The nodes in the trees are annotated with functional equations (attribute-value structure constraints) which are resolved to produce an fstructure." ></td>
	<td class="line x" title="32:183	F-structures are recursive attributevalue matrices, representing abstract syntactic functions." ></td>
	<td class="line x" title="33:183	F-structures approximate to basic predicate-argument-adjunct structures or dependency relations." ></td>
	<td class="line x" title="34:183	Figure 1 shows the cand fstructures for the sentence They believe John resigned." ></td>
	<td class="line oc" title="35:183	3 PCFG-Based Generation for Treebank-Based LFG Resources Cahill et al.(2004) present a method to automatically acquire wide-coverage robust probabilistic LFG approximations1 from treebanks." ></td>
	<td class="line o" title="37:183	The method is based on an automatic f-structure annotation algorithm that associates nodes in treebank trees with f-structure equations." ></td>
	<td class="line x" title="38:183	For each tree, the equations are collected and passed on to a constraint solver which produces an f-structure for the tree." ></td>
	<td class="line oc" title="39:183	Cahill et al.(2004) present two parsing architectures: the pipeline and the integrated parsing architecture." ></td>
	<td class="line x" title="41:183	In the pipeline architecture, a PCFG (or a history-based lexicalised generative parser) is extracted from the treebank and used to parse unseen text into trees, the resulting trees are annotated with f-structure equations by the f-structure annotation algorithm and a constraint solver produces an f-structure." ></td>
	<td class="line x" title="42:183	In the in1The resources are approximations in that (i) they do not enforce LFG completeness and coherence constraints and (ii) PCFG-based models can only approximate LFG and similar constraint-based formalisms (Abney, 1997)." ></td>
	<td class="line x" title="43:183	tegrated architecture, first the treebank trees are automatically annotated with f-structure information, f-structure annotated PCFGs with rules of the form NP(OBJ=)DT(=) NN(=) are extracted, syntactic categories followed by equations are treated as monadic CFG categories during grammar extraction and parsing, unseen text is parsed into trees with f-structure annotations, the annotations are collected and a constraint solver produces an f-structure." ></td>
	<td class="line oc" title="44:183	The generation architecture presented here builds on the integrated parsing architecture resources of Cahill et al.(2004)." ></td>
	<td class="line oc" title="46:183	The generation process takes an f-structure (such as the f-structure on the right in Figure 1) as input and outputs the most likely f-structure annotated tree (such as the tree on the left in Figure 1) given the input fstructure argmaxTreeP(Tree|F-Str) where the probability of a tree given an fstructure is decomposed as the product of the probabilities of all f-structure annotated productions contributing to the tree but where in addition to conditioning on the LHS of the production (as in the integrated parsing architecture of Cahill et al.(2004)) each production X  Y is now also conditioned on the set of f-structure features Feats -linked2 to the LHS of the rule." ></td>
	<td class="line x" title="48:183	For an f-structure annotated tree Tree and f-structure F-Str with (Tree)=F-Str:3 2 links LFGs c-structure to f-structure in terms of manyto-one functions from tree nodes into f-structure." ></td>
	<td class="line x" title="49:183	3 resolves the equations in Tree into F-Str (if satisfiable) in terms of the piece-wise function ." ></td>
	<td class="line oc" title="50:183	1034 Conditioning F-Structure Features Grammar Rules Probability {PRED, SUBJ, COMP, TENSE} VP(=)  VBD(=) SBAR(COMP=) 0.4998 {PRED, SUBJ, COMP, TENSE} VP(=)  VBP(=) SBAR(COMP=) 0.0366 {PRED, SUBJ, COMP, TENSE} VP(=)  VBD(=), S(COMP=) 6.48e-6 {PRED, SUBJ, COMP, TENSE} VP(=)  VBD(=) S(COMP=) 3.88e-6 {PRED, SUBJ, COMP, TENSE} VP(=)  VBP(=), SBARQ(COMP=) 7.86e-7 {PRED, SUBJ, COMP, TENSE} VP(=)  VBD(=) SBARQ(COMP=) 1.59e-7 Table 1: Example VP Generation rules automatically extracted from Sections 0221 of the Penn-II Treebank P(Tree|F-Str) := productdisplay X  Y in Tree (X) = Feats P(X  Y |X,Feats) (1) P(X  Y |X,Feats) = P(X  Y,X,Feats)P(X,Feats) = (2) P(X  Y,Feats) P(X,Feats)  #(X  Y,Feats) #(X  ,Feats) (3) and where probabilities are estimated using a simple MLE and rule counts (#) from the automatically f-structure annotated treebank resource of Cahill et al.(2004)." ></td>
	<td class="line x" title="52:183	Lexical rules (rules expanding preterminals) are conditioned on the full set of (atomic) feature-value pairs -linked to the RHS." ></td>
	<td class="line x" title="53:183	The intuition for conditioning rules in this way is that local f-structure components of the input f-structure drive the generation process." ></td>
	<td class="line oc" title="54:183	This conditioning effectively turns the f-structure annotated PCFGs of Cahill et al.(2004) into probabilistic generation grammars." ></td>
	<td class="line x" title="56:183	For example, in Figure 1 (where -links are represented as arrows), we automatically extract the rule S(=)  NP(SUBJ=) VP(=) conditioned on the feature set {PRED,SUBJ,COMP,TENSE}." ></td>
	<td class="line x" title="57:183	The probability of the rule is then calculated by counting the number of occurrences of that rule (and the associated set of features), divided by the number of occurrences of rules with the same LHS and set of features." ></td>
	<td class="line x" title="58:183	Table 1 gives example VP rule expansions with their probabilities when we train a grammar from Sections 0221 of the Penn Treebank." ></td>
	<td class="line x" title="59:183	3.1 Chart Generation Algorithm The generation algorithm is based on chart generation as first introduced by Kay (1996) with Viterbi-pruning." ></td>
	<td class="line x" title="60:183	The generation grammar is first converted into Chomsky Normal Form (CNF)." ></td>
	<td class="line x" title="61:183	We recursively build a chart-like data structure in a bottom-up fashion." ></td>
	<td class="line x" title="62:183	In contrast to packing of locally equivalent edges (Carroll and Oepen, 2005), in our approach if two chart items have equivalent rule left-hand sides and lexical coverage, only the most probable one is kept." ></td>
	<td class="line x" title="63:183	Each grammatical function-labelled (sub-)f-structure in the overall fstructure indexes a (sub-)chart." ></td>
	<td class="line x" title="64:183	The chart for each f-structure generates the most probable tree for that f-structure, given the internal set of conditioning f-structure features and its grammatical function label." ></td>
	<td class="line x" title="65:183	At each level, grammatical function indexed charts are initially unordered." ></td>
	<td class="line x" title="66:183	Charts are linearised by generation grammar rules once the charts themselves have produced the most probable tree for the chart." ></td>
	<td class="line x" title="67:183	Our example in Figure 1 generates the following grammatical function indexed, embedded and (at each level of embedding) unordered (sub-)chart configuration: SUBJ f :2 COMP f :3 SUBJ f :4TOP f :1 For each local subchart, the following algorithm is applied: Add lexical rules While subchart is Changing Apply unary productions Apply binary productions Propagate compatible rules 3.2 A Worked Example As an example, we step through the construction of the COMP-indexed chart at level f3 of the f-structure in Figure 1." ></td>
	<td class="line x" title="68:183	For lexical rules, we check the feature set at the sub-f-structure level and the values of the features." ></td>
	<td class="line x" title="69:183	Only features associated with lexical material are considered." ></td>
	<td class="line x" title="70:183	The SUBJ-indexed sub-chart f4 is constructed by first adding the rule NNP(=)  John(PRED=John,NUM=pl,PERS=3)." ></td>
	<td class="line x" title="71:183	If more than one lexical rule corresponds to a particular set of features and values in the f-structure, we add all rules with different LHS categories." ></td>
	<td class="line x" title="72:183	If two or more 1035 rules with equal LHS categories match the feature set, we only add the most probable one." ></td>
	<td class="line x" title="73:183	Unary productions are applied if the RHS of the unary production matches the LHS of an item already in the chart and the feature set of the unary production matches the conditioning feature set of the local sub-f-structure." ></td>
	<td class="line x" title="74:183	In our example, this results in the rule NP(SUBJ=)  NNP(=), conditioned on {NUM, PERS, PRED}, being added to the sub-chart at level f4 (the probability associated with this item is the probability of the rule multiplied by the probability of the previous chart item which combines with the new rule)." ></td>
	<td class="line x" title="75:183	When a rule is added to the chart, it is automatically associated with the yield of the rule, allowing us to propagate chunks of generated material upwards in the chart." ></td>
	<td class="line x" title="76:183	If two items in the chart have the same LHS (and the same yield independent of word order), only the item with the highest probability is kept." ></td>
	<td class="line x" title="77:183	This Viterbi-style pruning ensures that processing is efficient." ></td>
	<td class="line x" title="78:183	At sub-chart f4 there are no binary rules that can be applied." ></td>
	<td class="line x" title="79:183	At this stage, it is not possible to add any more items to the sub-chart, therefore we propagate items in the chart that are compatible with the sub-chart index SUBJ." ></td>
	<td class="line x" title="80:183	In our example, only the rule NP(SUBJ=)  NNP(=) (which yields the string John) is propagated to the next level up in the overall chart for consideration in the next iteration." ></td>
	<td class="line x" title="81:183	If the yield of an item being propagated upwards in the chart is subsumed by an element already at that level, the subsumed item is removed." ></td>
	<td class="line x" title="82:183	This results in efficiently treating the well known problem originally described in Kay (1996), where one unnecessarily retains sub-optimal strings." ></td>
	<td class="line x" title="83:183	For example, generating the string The very tall strong athletic man, one does not want to keep variations such as The very tall man, or The athletic man, if one can generate the entire string." ></td>
	<td class="line x" title="84:183	Our method ensures that only the most probable tree with the longest yield will be propagated upwards." ></td>
	<td class="line x" title="85:183	The COMP-indexed chart at level f3 of the fstructure is constructed in a similar fashion." ></td>
	<td class="line x" title="86:183	First the lexical rule V(=)  resigned is added." ></td>
	<td class="line x" title="87:183	Next, conditioning on {PRED, SUBJ, TENSE}, the unary rule VP(=)  V(=) (with yield resigned) is added." ></td>
	<td class="line x" title="88:183	We combine the new VP(=) rule with the NP(SUBJ=) already present from the previous iteration to enable us to add the rule S(=)  NP(SUBJ=) VP(=), conditioned on {PRED, SUBJ, TENSE}." ></td>
	<td class="line x" title="89:183	The yield of this rule is John resigned." ></td>
	<td class="line x" title="90:183	Next, conditioning on the same feature set, we add the rule SBAR(comp=)  S(=) with yield John resigned to the chart." ></td>
	<td class="line x" title="91:183	It is not possible to add any more new rules, so at this stage, only the SBAR(COMP=) rule with yield John resigned is propagated up to the next level." ></td>
	<td class="line x" title="92:183	The process continues until at the outermost level of the f-structure, there are no more rules to be added to the chart." ></td>
	<td class="line x" title="93:183	At this stage, we search for the most probable rule with TOP as its LHS category and return the yield of this rule as the output of the generation process." ></td>
	<td class="line x" title="94:183	Generation fails if there is no rule with LHS TOP at this level in the chart." ></td>
	<td class="line x" title="95:183	3.3 Lexical Smoothing Currently, the only smoothing in the system applies at the lexical level." ></td>
	<td class="line oc" title="96:183	Our backoff uses the built-in lexical macros4 of the automatic fstructure annotation algorithm of Cahill et al.(2004) to identify potential part-of-speech categories corresponding to a particular set of features." ></td>
	<td class="line x" title="98:183	Following Baayen and Sproat (1996) we assume that unknown words have a probability distribution similar to hapax legomena." ></td>
	<td class="line x" title="99:183	We add a lexical rule for each POS tag that corresponds to the fstructure features at that level to the chart with a probability computed from the original POS tag probability distribution multiplied by a very small constant." ></td>
	<td class="line x" title="100:183	This means that lexical rules seen during training have a much higher probability than lexical rules added during the smoothing phase." ></td>
	<td class="line x" title="101:183	Lexical smoothing has the advantage of boosting coverage (as shown in Tables 3, 4, 5 and 6 below) but slightly degrades the quality of the strings generated." ></td>
	<td class="line x" title="102:183	We believe that the tradeoff in terms of quality is worth the increase in coverage." ></td>
	<td class="line x" title="103:183	Smoothing is not carried out when there is no suitable phrasal grammar rule that applies during the process of generation." ></td>
	<td class="line x" title="104:183	This can lead to the generation of partial strings, since some f-structure components may fail to generate a corresponding string." ></td>
	<td class="line x" title="105:183	In such cases, generation outputs the concatenation of the strings generated by the remaining components." ></td>
	<td class="line x" title="106:183	4 Experiments We train our system on WSJ Sections 0221 of the Penn-II Treebank and evaluate against the raw 4The lexical macros associate POS tags with sets of features, for example the tag NNS (plural noun) is associated with the features PRED=$LEMMA and NUM=pl. 1036 S. length  20  25  30  40 all Training 16667 23597 29647 36765 39832 Test 1034 1464 1812 2245 2416 Table 2: Number of training and test sentences per sentence length strings from Section 23." ></td>
	<td class="line x" title="107:183	We use Section 22 as our development set." ></td>
	<td class="line x" title="108:183	As part of our evaluation, we experiment with sentences of varying length (20, 25, 30, 40, all), both in training and testing." ></td>
	<td class="line x" title="109:183	Table 2 gives the number of training and test sentences for each sentence length." ></td>
	<td class="line oc" title="110:183	In each case, we use the automatically generated f-structures from Cahill et al.(2004) from the original Section 23 treebank trees as f-structure input to our generation experiments." ></td>
	<td class="line x" title="112:183	We automatically mark adjunct and coordination scope in the input f-structure." ></td>
	<td class="line x" title="113:183	Notice that these automatically generated f-structures are not perfect, i.e. they are not guaranteed to be complete and coherent (Kaplan and Bresnan, 1982): a local f-structure may contain material that is not supposed to be there (incoherence) and/or may be missing material that is supposed to be there (incompleteness)." ></td>
	<td class="line x" title="114:183	The results presented below show that our method is robust with respect to the quality of the f-structure input and will always attempt to generate partial output rather than fail." ></td>
	<td class="line x" title="115:183	We consider this an important property as pristine generation input cannot always be guaranteed in realistic application scenarios, such as probabilistic transfer-based machine translation where generation input may contain a certain amount of noise." ></td>
	<td class="line x" title="116:183	4.1 Pre-Training Treebank Transformations During the development of the generation system, we carried out error analysis on our development set WSJ Section 22 of the Penn-II Treebank." ></td>
	<td class="line x" title="117:183	We identified some initial pre-training transformations to the treebank that help generation." ></td>
	<td class="line x" title="118:183	Punctuation: Punctuation is not usually encoded in f-structure representations." ></td>
	<td class="line x" title="119:183	Because our architecture is completely driven by rules conditioned by f-structure information automatically extracted from an f-structure annotated treebank, its placement of punctuation is not principled." ></td>
	<td class="line x" title="120:183	This led to anomalies such as full stops appearing mid sentence and quotation marks appearing in undesired locations." ></td>
	<td class="line x" title="121:183	One partial solution to this was to reduce the amount of punctuation that the system trained on." ></td>
	<td class="line x" title="122:183	We removed all punctuation apart from commas and full stops from the training data." ></td>
	<td class="line x" title="123:183	We did not remove any punctuation from the evaluation test set (Section 23), but our system will ever only produce commas and full stops." ></td>
	<td class="line x" title="124:183	In the evaluation (Tables 3, 4, 5 and 6) we are penalised for the missing punctuation." ></td>
	<td class="line x" title="125:183	To solve the problem of full stops appearing mid sentence, we carry out a punctuation post-processing step on all generated strings." ></td>
	<td class="line x" title="126:183	This removes mid-sentence full stops and adds missing full stops at the end of generated sentences prior to evaluation." ></td>
	<td class="line x" title="127:183	We are working on a more appropriate solution allowing the system to generate all punctuation." ></td>
	<td class="line x" title="128:183	Case: English does not have much case marking, and for parsing no special treatment was encoded." ></td>
	<td class="line x" title="129:183	However, when generating, it is very important that the first person singular pronoun is I in the nominative case and me in the accusative." ></td>
	<td class="line x" title="130:183	Given the original grammar used in parsing, our generation system was not able to distinguish nominative from accusative contexts." ></td>
	<td class="line x" title="131:183	The solution we implemented was to carry out a grammar transformation in a pre-processing step, to automatically annotate personal pronouns with their case information." ></td>
	<td class="line x" title="132:183	This resulted in phrasal and lexical rules such as NP(SUBJ)  PRPnom(=) and PRPnom(=)  I and greatly improved the accuracy of the pronouns generated." ></td>
	<td class="line x" title="133:183	4.2 String-Based Evaluation We evaluate the output of our generation system against the raw strings of Section 23 using the Simple String Accuracy and BLEU (Papineni et al. , 2002) evaluation metrics." ></td>
	<td class="line x" title="134:183	Simple String Accuracy is based on the string edit distance between the output of the generation system and the gold standard sentence." ></td>
	<td class="line x" title="135:183	BLEU is the weighted average of n-gram precision against the gold standard sentences." ></td>
	<td class="line x" title="136:183	We also measure coverage as the percentage of input f-structures that generate a string." ></td>
	<td class="line x" title="137:183	For evaluation, we automatically expand all contracted words." ></td>
	<td class="line x" title="138:183	We only evaluate strings produced by the system (similar to Nakanishi et al.(2005))." ></td>
	<td class="line x" title="140:183	We conduct a total of four experiments." ></td>
	<td class="line x" title="141:183	The parameters we investigate are lexical smoothing (Section 3.3) and partial output." ></td>
	<td class="line x" title="142:183	Partial output is a robustness feature for cases where a sub-fstructure component fails to generate a string and the system outputs a concatenation of the strings generated by the remaining components, rather than fail completely." ></td>
	<td class="line x" title="143:183	1037 Sentence length of Evaluation Section 23 Sentences of length: Training Data Metric  20  25  30  40 all  20 BLEU 0.6812 0.6601 0.6373 0.6013 0.5793 String Accuracy 0.7274 0.7052 0.6875 0.6572 0.6431 Coverage 96.52 95.83 94.59 93.76 93.92  25 BLEU 0.6915 0.6800 0.6696 0.6396 0.6233 String Accuracy 0.7262 0.7095 0.6983 0.6731 0.6618 Coverage 96.52 95.83 94.59 93.76 93.92  30 BLEU 0.6979 0.6881 0.6792 0.6576 0.6445 String Accuracy 0.7317 0.7169 0.7075 0.6853 0.6749 Coverage 97.97 97.95 97.41 97.15 97.31  40 BLEU 0.7045 0.6951 0.6852 0.6715 0.6605 String Accuracy 0.7349 0.7212 0.7074 0.6881 0.6788 Coverage 98.45 98.36 98.01 97.82 97.93 all BLEU 0.7077 0.6974 0.6859 0.6734 0.6651 String Accuracy 0.7373 0.7221 0.7087 0.6894 0.6808 Coverage 98.65 98.5 98.12 97.95 98.05 Table 3: Generation +partial output +lexical smoothing Sentence length of Evaluation Section 23 Sentences of length: Training Data Metric  20  25  30  40 all all BLEU 0.6253 0.6097 0.5887 0.5730 0.5590 String Accuracy 0.6886 0.6688 0.6513 0.6317 0.6207 Coverage 91.20 91.19 90.84 90.33 90.11 Table 4: Generation +partial output -lexical smoothing Varying the length of the sentences included in the training data (Tables 3 and 5) shows that results improve (both in terms of coverage and string quality) as the length of sentence included in the training data increases." ></td>
	<td class="line x" title="144:183	Tables 3 and 5 give the results for the experiments including lexical smoothing and varying partial output." ></td>
	<td class="line x" title="145:183	Table 3 (+partial, +smoothing) shows that training on sentences of all lengths and evaluating all strings (including partial outputs), our system achieves coverage of 98.05%, a BLEU score of 0.6651 and string accuracy of 0.6808." ></td>
	<td class="line x" title="146:183	Table 5 (-partial, +smoothing) shows that coverage drops to 89.49%, BLEU score increases to 0.6979 and string accuracy to 0.7012, when the system is trained on sentences of all lengths." ></td>
	<td class="line x" title="147:183	Similarly, for strings 20, coverage drops from 98.65% to 95.26%, BLEU increases from 0.7077 to 0.7227 and String Accuracy from 0.7373 to 0.7476." ></td>
	<td class="line x" title="148:183	Including partial output increases coverage (by more than 8.5 percentage points for all sentences) and hence robustness while slightly decreasing quality." ></td>
	<td class="line x" title="149:183	Tables 3 (+partial, +smoothing) and 4 (+partial, -smoothing) give results for the experiments including partial output but varying lexical smoothing." ></td>
	<td class="line x" title="150:183	With no lexical smoothing (Table 4), the system (trained on all sentence lengths) produces strings for 90.11% of the input f-structures and achieves a BLEU score of 0.5590 and string accuracy of 0.6207." ></td>
	<td class="line x" title="151:183	Switching off lexical smoothing has a negative effect on all evaluation metrics (coverage and quality), because many more strings produced are now partial (since for PRED values unseen during training, no lexical entries are added to the chart)." ></td>
	<td class="line x" title="152:183	Comparing Tables 5 (-partial, +smoothing) and 6 (-partial, -smoothing), where the system does not produce any partial outputs and lexical smoothing is varied, shows that training on all sentence lengths, BLEU score increases from 0.6979 to 0.7147 and string accuracy increases from 0.7012 to 0.7192." ></td>
	<td class="line x" title="153:183	At the same time, coverage drops dramatically from 89.49% (Table 5) to 47.60% (Table 6)." ></td>
	<td class="line x" title="154:183	Comparing Tables 4 and 6 shows that while partial output almost doubles coverage, this comes at a price of a severe drop in quality (BLEU score drops from 0.7147 to 0.5590)." ></td>
	<td class="line x" title="155:183	On the other hand, comparing Tables 5 and 6 shows that lexical smoothing achieves a similar increase in coverage with only a very slight drop in quality." ></td>
	<td class="line x" title="156:183	5 Discussion Nakanishi et al.(2005) achieve 90.56% coverage and a BLEU score of 0.7723 on Section 23 1038 Sentence length of Evaluation Section 23 Sentences of length: Training Data Metric  20  25  30  40 all  20 BLEU 0.7326 0.7185 0.7165 0.7082 0.7052 String Accuracy 0.76 0.7428 0.7363 0.722 0.7175 Coverage 85.49 81.56 77.26 71.94 69.08  25 BLEU 0.7300 0.7235 0.7218 0.7118 0.7077 String Accuracy 0.7517 0.7382 0.7315 0.7172 0.7116 Coverage 89.65 87.77 84.38 80.31 78.56  30 BLEU 0.7207 0.7125 0.7107 0.6991 0.6946 String Accuracy 0.747 0.7336 0.7275 0.711 0.7045 Coverage 93.23 92.14 89.74 86.59 85.18  40 BLEU 0.7221 0.7140 0.7106 0.7016 0.6976 String Accuracy 0.746 0.7331 0.7236 0.7072 0.7001 Coverage 94.58 93.85 91.89 89.62 88.33 all BLEU 0.7227 0.7145 0.7095 0.7011 0.6979 String Accuracy 0.7476 0.7331 0.7239 0.7077 0.7012 Coverage 95.26 94.40 92.55 90.69 89.49 Table 5: Generation -partial output +lexical smoothing Sentence length of Evaluation Section 23 Sentences of length: Training Data Metric  20  25  30  40 all all BLEU 0.7272 0.7237 0.7201 0.7160 0.7147 String Accuracy 0.7547 0.7436 0.7361 0.7237 0.7192 Coverage 61.99 57.38 53.64 47.60 47.60 Table 6: Generation -partial output -lexical smoothing sentences, restricted to length 20 for efficiency reasons." ></td>
	<td class="line x" title="158:183	Langkilde-Gearys (2002) best system achieves 82.8% coverage, a BLEU score of 0.924 and string accuracy of 0.945 against Section 23 sentences of all lengths." ></td>
	<td class="line x" title="159:183	Callaway (2003) achieves 98.7% coverage and a string accuracy of 0.6607 on sentences of all lengths." ></td>
	<td class="line x" title="160:183	Our best results for sentences of length  20 are coverage of 95.26%, BLEU score of 0.7227 and string accuracy of 0.7476." ></td>
	<td class="line x" title="161:183	For all sentence lengths, our best results are coverage of 89.49%, a BLEU score of 0.6979 and string accuracy of 0.7012." ></td>
	<td class="line x" title="162:183	Using hand-crafted grammar-based generation systems (Langkilde-Geary, 2002; Callaway, 2003), it is possible to achieve very high results." ></td>
	<td class="line x" title="163:183	However, hand-crafted systems are expensive to construct and not easily ported to new domains or other languages." ></td>
	<td class="line x" title="164:183	Our methodology, on the other hand, is based on resources automatically acquired from treebanks and easily ported to new domains and languages, simply by retraining on suitable data." ></td>
	<td class="line o" title="165:183	Recent work on the automatic acquisition of multilingual LFG resources from treebanks for Chinese, German and Spanish (Burke et al. , 2004; Cahill et al. , 2005; ODonovan et al. , 2005) has shown that given a suitable treebank, it is possible to automatically acquire high quality LFG resources in a very short space of time." ></td>
	<td class="line x" title="166:183	The generation architecture presented here is easily ported to those different languages and treebanks." ></td>
	<td class="line oc" title="167:183	6 Conclusion and Further Work We present a new architecture for stochastic LFG surface realisation using the automatically annotated treebanks and extracted PCFG-based LFG approximations of Cahill et al.(2004)." ></td>
	<td class="line x" title="169:183	Our model maximises the probability of a tree given an fstructure, supporting a simple and efficient implementation that scales to wide-coverage treebankbased resources." ></td>
	<td class="line x" title="170:183	An improved model would maximise the probability of a string given an fstructure by summing over trees with the same yield." ></td>
	<td class="line x" title="171:183	More research is required to implement such a model efficiently using packed representations (Carroll and Oepen, 2005)." ></td>
	<td class="line x" title="172:183	Simple PCFGbased models, while effective and computationally efficient, can only provide approximations to LFG and similar constraint-based formalisms (Abney, 1997)." ></td>
	<td class="line x" title="173:183	Research on discriminative disambiguation methods (Valldal and Oepen, 2005; Nakanishi et al. , 2005) is important." ></td>
	<td class="line x" title="174:183	Kaplan and Wedekind (2000) show that for certain linguistically interesting classes of LFG (and PATR etc)." ></td>
	<td class="line x" title="175:183	grammars, generation from f-structures yields a context free language." ></td>
	<td class="line x" title="176:183	Their proof involves the notion of a 1039 refinement grammar where f-structure information is compiled into CFG rules." ></td>
	<td class="line x" title="177:183	Our probabilistic generation grammars bear a conceptual similarity to Kaplan and Wedekinds refinement grammars." ></td>
	<td class="line x" title="178:183	It would be interesting to explore possible connections between the treebank-based empirical work presented here and the theoretical constructs in Kaplan and Wedekinds proofs." ></td>
	<td class="line x" title="179:183	We presented a full set of generation experiments on varying sentence lengths training on Sections 0221 of the Penn Treebank and evaluating on Section 23 strings." ></td>
	<td class="line x" title="180:183	Sentences of length 20 achieve coverage of 95.26%, BLEU score of 0.7227 and string accuracy of 0.7476 against the raw Section 23 text." ></td>
	<td class="line x" title="181:183	Sentences of all lengths achieve coverage of 89.49%, BLEU score of 0.6979 and string accuracy of 0.7012." ></td>
	<td class="line x" title="182:183	Our method is robust and can cope with noise in the f-structure input to generation and will attempt to produce partial output rather than fail." ></td>
	<td class="line x" title="183:183	Acknowledgements We gratefully acknowledge support from Science Foundation Ireland grant 04/BR/CS0370 for the research reported in this paper." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-2018
Using Machine-Learning To Assign Function Labels To Parser Output For Spanish
Chrupala, Grzegorz;Van Genabith, Josef;"></td>
	<td class="line x" title="1:184	Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 136143, Sydney, July 2006." ></td>
	<td class="line x" title="2:184	c2006 Association for Computational Linguistics Using Machine-Learning to Assign Function Labels to Parser Output for Spanish Grzegorz Chrupaa1 and Josef van Genabith1,2 1National Center for Language Technology Dublin City University Glasnevin, Dublin 9, Ireland 2IBM Dublin Center for Advanced Studies grzegorz.chrupala@computing.dcu.ie josef@computing.dcu.ie Abstract Data-driven grammatical function tag assignment has been studied for English using the Penn-II Treebank data." ></td>
	<td class="line x" title="3:184	In this paper we address the question of whether such methods can be applied successfully to other languages and treebank resources." ></td>
	<td class="line x" title="4:184	In addition to tag assignment accuracy and f-scores we also present results of a task-based evaluation." ></td>
	<td class="line x" title="5:184	We use three machine-learning methods to assign Cast3LB function tags to sentences parsed with Bikels parser trained on the Cast3LB treebank." ></td>
	<td class="line x" title="6:184	The best performing method, SVM, achieves an f-score of 86.87% on gold-standard trees and 66.67% on parser output a statistically significant improvement of 6.74% over the baseline." ></td>
	<td class="line x" title="7:184	In a task-based evaluation we generate LFG functional-structures from the functiontag-enriched trees." ></td>
	<td class="line x" title="8:184	On this task we achive an f-score of 75.67%, a statistically significant 3.4% improvement over the baseline." ></td>
	<td class="line oc" title="9:184	1 Introduction The research presented in this paper forms part of an ongoing effort to develop methods to induce wide-coverage multilingual LexicalFunctional Grammar (LFG) (Bresnan, 2001) resources from treebanks by means of automatically associating LFG f-structure information with constituency trees produced by probabilistic parsers (Cahill et al. , 2004)." ></td>
	<td class="line x" title="10:184	Inducing deep syntactic analyses from treebank data avoids the cost and time involved in manually creating wide-coverage resources." ></td>
	<td class="line x" title="11:184	Lexical Functional Grammar f-structures provide a level of syntactic representation based on the notion of grammatical functions (e.g. Subject, Object, Oblique, Adjunct etc.)." ></td>
	<td class="line x" title="12:184	This level is more abstract and cross-linguistically more uniform than constituency trees." ></td>
	<td class="line x" title="13:184	F-structures also include explicit encodings of phenomena such as control and raising, pro-drop or long distance dependencies." ></td>
	<td class="line x" title="14:184	Those characteristics make this level a suitable representation for many NLP applications such as transfer-based Machine Translation or Question Answering." ></td>
	<td class="line oc" title="15:184	The f-structure annotation algorithm used for inducing LFG resources from the Penn-II treebank for English (Cahill et al. , 2004) uses configurational, categorial, function tag and trace information." ></td>
	<td class="line x" title="16:184	In contrast to English, in many other languages configurational information is not a good predictor for LFG grammatical function assignment." ></td>
	<td class="line x" title="17:184	For such languages the function tags included in many treebanks are a much more important source of information for the LFG annotation algorithm than Penn-II tags are for English." ></td>
	<td class="line x" title="18:184	Cast3LB (Civit and Mart, 2004), the Spanish treebank used in the current research, contains comprehensive grammatical function annotation." ></td>
	<td class="line x" title="19:184	In the present paper we use a machine-learning approach in order to add Cast3LB function tags to nodes of basic constituent trees output by a probabilistic parser trained on Cast3LB." ></td>
	<td class="line x" title="20:184	To our knowledge, this paper is the first to describe applying a data-driven approach to function-tag assignment to a language other than English." ></td>
	<td class="line x" title="21:184	Our method statistically significantly outperforms the previously used approach which relied exclusively on the parser to produce trees with Cast3LB tags (ODonovan et al. , 2005)." ></td>
	<td class="line x" title="22:184	Additionally, we perform a task-driven evaluation of our Cast3LB tag assignment method by using the tag-enriched trees as input to the Spanish LFG fstructure annotation algorithm and evaluating the quality of the resulting f-structures." ></td>
	<td class="line x" title="23:184	Section 2 describes the Spanish Cast3LB treebank." ></td>
	<td class="line x" title="24:184	In Section 3 we describe previous research in LFG induction for English and Spanish as well 136 as research on data-driven function tag assignment to parsed text in English." ></td>
	<td class="line x" title="25:184	Section 4 provides the details of our approach to the Cast3LB function tag assignment task." ></td>
	<td class="line x" title="26:184	In Sections 5 and 6 we present evaluation results for our method." ></td>
	<td class="line x" title="27:184	In Section 7 we present the error analysis of the results." ></td>
	<td class="line x" title="28:184	Finally, in Section 8 we conclude and discuss ideas for further research." ></td>
	<td class="line x" title="29:184	2 The Spanish Treebank As input to our LFG annotation algorithm we use the output of Bikels parser (Bikel, 2002) trained on the Cast3LB treebank (Civit and Mart, 2004)." ></td>
	<td class="line x" title="30:184	Cast3LB contains around 3,500 constituency trees (100,000 words) taken from different genres of European and Latin American Spanish." ></td>
	<td class="line x" title="31:184	The POS tags used in Cast3LB encode morphological information in addition to Part-of-Speech information." ></td>
	<td class="line x" title="32:184	Due to the relatively flexible order of main sentence constituents in Spanish, Cast3LB uses a flat, multiply-branching structure for the S node." ></td>
	<td class="line x" title="33:184	There is no VP node, but rather all complements and adjuncts depending on a verb are sisters to the gv (Verb Group) node containing this verb." ></td>
	<td class="line x" title="34:184	An example sentence (with the corresponding f-structure) is shown in Figure 1." ></td>
	<td class="line x" title="35:184	Tree nodes are additionally labelled with grammatical function tags." ></td>
	<td class="line x" title="36:184	Table 1 provides a list of function tags with short explanations." ></td>
	<td class="line x" title="37:184	Civit (2004) provides Cast3LB function tag guidelines." ></td>
	<td class="line x" title="38:184	Functional tags carry some of the information that would be encoded in terms of tree configurations in languages with stricter constituent order constraints than Spanish." ></td>
	<td class="line oc" title="39:184	3 Previous Work 3.1 LFG Annotation A methodology for automatically obtaining LFG f-structures from trees output by probabilistic parsers trained on the Penn-II treebank has been described by Cahill et al.(2004)." ></td>
	<td class="line o" title="41:184	It has been shown that the methods can be ported to other languages and treebanks (Burke et al. , 2004; Cahill et al. , 2003), including Cast3LB (ODonovan et al. , 2005)." ></td>
	<td class="line oc" title="42:184	Some properties of Spanish and the encoding of syntactic information in the Cast3LB treebank make it non-trivial to apply the method of automatically mapping c-structures to f-structures used by Cahill et al.(2004), which assigns grammatical Tag Meaning ATR Attribute of copular verb CAG Agent of passive verb CC Compl." ></td>
	<td class="line x" title="44:184	of circumstance CD Direct object CD.Q Direct object of quantity CI Indirect object CPRED Predicative complement CPRED.CD Predicative of Direct Object CPRED.SUJ Predicative of Subject CREG Prepositional object ET Textual element IMPERS Impersonal marker MOD Verbal modifier NEG Negation PASS Passive marker SUJ Subject VOC Vocative Table 1: List of function tags in Cast3LB." ></td>
	<td class="line x" title="45:184	functions to tree nodes based on their phrasal category, the category of the mother node and their position relative to the local head." ></td>
	<td class="line x" title="46:184	In Spanish, the order of sentence constituents is flexible and their position relative to the head is an imperfect predictor of grammatical function." ></td>
	<td class="line x" title="47:184	Also, much of the information that the Penn-II Treebank encodes in terms of tree configurations is encoded in Cast3LB in the form of function tags." ></td>
	<td class="line x" title="48:184	As Cast3LB trees lack a VP node, the configurational information normally used in English to distinguish Subjects (NP which is left sister to VP) from Direct Objects (NP which is right sister to V) is not available in Cast3LB-style trees." ></td>
	<td class="line x" title="49:184	This means that assigning correct LFG functional annotations to nodes in Cast3LB trees is rather difficult without use of Cast3LB function tags, and those tags are typically absent in output generated by probabilistic parsers." ></td>
	<td class="line x" title="50:184	In order to solve this difficulty, ODonovan et al.(2005) train Bikels parser to output complex category-function labels." ></td>
	<td class="line x" title="52:184	A complex label such as sn-SUJ (an NP node tagged with the Subject grammatical function) is treated as an atomic category in the training data, and is output in the trees produced by the parser." ></td>
	<td class="line x" title="53:184	This baseline process is represented in Figure 2." ></td>
	<td class="line x" title="54:184	This approach can be problematic for two main reasons." ></td>
	<td class="line x" title="55:184	Firstly, by treating complex labels as atomic categories the number of unique labels increases and parse quality can deteriorate due to sparse data problems." ></td>
	<td class="line x" title="56:184	Secondly, this approach, by relying on the parser to assign function tags, offers 137 S neg-NEG no not gv espere expect sn-SUJ el lector the reader sn-CD una definicion a definition         PRED esperarSUBJ,OBJ NEG + TENSE PRES MOOD SUBJUNCTIVE SUBJ bracketleftBigg SPEC bracketleftbig SPEC-FORM EL bracketrightbig PRED lector bracketrightBigg OBJ bracketleftBigg SPEC bracketleftbig SPEC-FORM UNO bracketrightbig PRED definicion bracketrightBigg         Figure 1: On the left flat structure of S. Cast3LB function tags are shown in bold." ></td>
	<td class="line x" title="57:184	On the right the corresponding (simplified) LFG f-structure." ></td>
	<td class="line x" title="58:184	Translation: Let the reader not expect a definition." ></td>
	<td class="line x" title="59:184	Figure 2: Processing architecture for the baseline." ></td>
	<td class="line x" title="60:184	limited control over, or room for improvement in, this task." ></td>
	<td class="line x" title="61:184	3.2 Adding Function Tags to Parser Output The solution we adopt instead is to add Cast3LB functional tags to simple constituent trees output by the parser, as a postprocessing step." ></td>
	<td class="line x" title="62:184	For English, such approaches have been shown to give good results for the output of parsers trained on the Penn-II Treebank." ></td>
	<td class="line x" title="63:184	Blaheta and Charniak (2000) use a probabilistic model with feature dependencies encoded by means of feature trees to add Penn-II Treebank function tags to Charniaks parser output." ></td>
	<td class="line x" title="64:184	They report an f-score 88.472% on original treebank trees and 87.277% on the correctly parsed subset of tree nodes." ></td>
	<td class="line x" title="65:184	Jijkoun and de Rijke (2004) describe a method of enriching output of a parser with information that is included in the original Penn-II trees, such as function tags, empty nodes and coindexations." ></td>
	<td class="line x" title="66:184	They first transform Penn trees to a dependency format and then use memory-based learning to perform various graph transformations." ></td>
	<td class="line x" title="67:184	One of the transformations is node relabelling, which adds function tags to parser output." ></td>
	<td class="line x" title="68:184	They report an fscore of 88.5% for the task of function tagging on correctly parsed constituents." ></td>
	<td class="line x" title="69:184	4 Assigning Cast3LB Function Tags to Parsed Spanish Text The complete processing architecture of our approach is depicted in Figure 3." ></td>
	<td class="line x" title="70:184	We describe it in detail in this and the following sections." ></td>
	<td class="line x" title="71:184	We divided the Spanish treebank into a training set of 80%, a development set of 10%, and a test set of 10% of all trees." ></td>
	<td class="line x" title="72:184	We randomly assigned treebank files to these sets to ensure that different textual genres are about equally represented among the training, development and test trees." ></td>
	<td class="line x" title="73:184	4.1 Constituency Parsing For constituency parsing we use Bikels (2002) parser for which we developed a Spanish language package adapted to the Cast3LB data." ></td>
	<td class="line x" title="74:184	Prior to parsing, we perform one of the tree transformations described by Cowan and Collins (2005), i.e. we add a CP and SBAR nodes to subordinate and relative clauses." ></td>
	<td class="line x" title="75:184	This is undone in parser output." ></td>
	<td class="line x" title="76:184	The category labels in the Spanish treebank are rather fine grained and often contain redundant information.1 We preprocess the treebank and re1For example there are several labels for Nominal Group, 138 Figure 3: Processing architecture for the machinelearning-based method." ></td>
	<td class="line x" title="77:184	duce the number of category labels, only retaining distinctions that we deem useful for our purposes.2 For constituency parsing we also reduce the number of POS tags by including only selected morphological features." ></td>
	<td class="line x" title="78:184	Table 2 provides the list of features included for the different parts of speech." ></td>
	<td class="line x" title="79:184	In our experiments we use gold standard POS tagged development and test-set sentences as input rather than tagging text automatically." ></td>
	<td class="line x" title="80:184	The results of the evaluation of parsing performance on the test set are shown in Table 3." ></td>
	<td class="line x" title="81:184	Labelled bracketing f-score for all sentences is just below 84% for all sentences, and 84.58% for sentences of length  70." ></td>
	<td class="line x" title="82:184	In comparison, Cowan and Collins (2005) report an f-score of 85.1% ( 70) using a version of Collins parser adapted for Cast3LB, and using reranking to boost perforsuch as grup.nom.ms (masculine singular), grup.nom.fs (feminine singular), grup.nom.mp (masculine plural) etc. This number and gender information is already encoded in the POS tags of nouns heading these constituents." ></td>
	<td class="line x" title="83:184	2The labels we retain are the following: INC, S, S.NF, S.NF.R, S.NF, S.R, conj.subord, coord, data, espec, gerundi, grup.nom, gv, infinitiu, interjeccio, morf, neg, numero, prep, relatiu, s.a, sa, sadv, sn, sp, and versions of those suffixed with.co to indicate coordination)." ></td>
	<td class="line x" title="84:184	Part of Speech Features included Determiner type, number Noun type, number Adjective type, number Pronoun type, number, person Verb type, number, mood Adverb type Conjunction type Table 2: Features included in POS tags." ></td>
	<td class="line x" title="85:184	Type refers to subcategories of parts of speech such as e.g. common and proper for nouns, or main, auxiliary and semiauxiliary for verbs." ></td>
	<td class="line x" title="86:184	For details see (Civit, 2000)." ></td>
	<td class="line x" title="87:184	LB Precision LB Recall F-score All 84.18 83.74 83.96  70 84.82 84.35 84.58 Table 3: Parser performance." ></td>
	<td class="line x" title="88:184	mance." ></td>
	<td class="line x" title="89:184	They use a different, more reduced category label set as well as a different training-test split." ></td>
	<td class="line x" title="90:184	Both Cowan and Collins and the present paper report scores which ignore punctuation." ></td>
	<td class="line x" title="91:184	4.2 Cast3LB Function Tagging For the task of Cast3LB function tag assignment we experimented with three generic machine learning algorithms: a memory-based learner (Daelemans and van den Bosch, 2005), a maximum entropy classifier (Berger et al. , 1996) and a Support Vector Machine classifier (Vapnik, 1998)." ></td>
	<td class="line x" title="92:184	For each algorithm we use the same set of features to represent nodes that are to be assigned one of the Cast3LB function tags." ></td>
	<td class="line x" title="93:184	We use a special null tag for nodes where no Cast3LB tag is present." ></td>
	<td class="line x" title="94:184	In Cast3LB only nodes in certain contexts are eligible for function tags." ></td>
	<td class="line x" title="95:184	For this reason we only consider a subset of all nodes as candidates for function tag assignment, namely those which are sisters of nodes with the category labels gv (Verb Group), infinitiu (Infinitive) and gerundi (Gerund)." ></td>
	<td class="line x" title="96:184	For these candidates we extract the following three types of features encoding configurational, morphological and lexical information for the target node and neighboring context nodes:  Node features: position relative to head, head lemma, alternative head lemma (i.e. the head of NP in PP), head POS, category, definiteness, agreement with head verb, yield, human/nonhuman 139  Local features: head verb, verb person, verb number, parent category  Context features: node features (except position) of the two previous and two following sister nodes (if present)." ></td>
	<td class="line x" title="97:184	We used cross-validation for refining the set of features and for tuning the parameters of the machine-learning algorithms." ></td>
	<td class="line x" title="98:184	We did not use any additional automated feature-selection procedure." ></td>
	<td class="line x" title="99:184	We made use of the following implementations: TiMBL (Daelemans et al. , 2004) for MemoryBased Learning, the MaxEnt Toolkit (Le, 2004) for Maximum Entropy and LIBSVM (Chang and Lin, 2001) for Support Vector Machines." ></td>
	<td class="line x" title="100:184	For TiMBL we used k nearest neighbors = 7 and the gain ratio metric for feature weighting." ></td>
	<td class="line x" title="101:184	For MaxEnt, we used the L-BFGS parameter estimation and 110 iterations, and we regularize the model using a Gaussian prior with 2 = 1." ></td>
	<td class="line x" title="102:184	For SVM we used the RBF kernel with  = 27 and the cost parameter C = 32." ></td>
	<td class="line x" title="103:184	5 Cast3LB Tag Assignment Evaluation We present evaluation results on the original goldstandard trees of the test set as well as on the test-set sentences parsed by Bikels parser." ></td>
	<td class="line x" title="104:184	For the evaluation of Cast3LB function tagging performance on gold trees the most straightforward metric is the accuracy, or the proportion of all candidate nodes that were assigned the correct label." ></td>
	<td class="line x" title="105:184	However we cannot use this metric for evaluating results on the parser output." ></td>
	<td class="line x" title="106:184	The trees output by the parser are not identical to gold standard trees due to parsing errors, and the set of candidate nodes extracted from parsed trees will not be the same as for gold trees." ></td>
	<td class="line x" title="107:184	For this reason we use an alternative metric which is independent of tree configuration and uses only the Cast3LB function labels and positional indices of tokens in a sentence." ></td>
	<td class="line x" title="108:184	For each function-tagged tree we first remove the punctuation tokens." ></td>
	<td class="line x" title="109:184	Then we extract a set of tuples of the form GF,i,j, where GF is the Cast3LB function tag and i  j is the range of tokens spanned by the node annotated with this function." ></td>
	<td class="line x" title="110:184	We use the standard measures of precision, recall and f-score to evaluate the results." ></td>
	<td class="line x" title="111:184	Results for the three algorithms are shown in Table 4." ></td>
	<td class="line x" title="112:184	MBL and MaxEnt show a very similar performance, while SVM outperforms both, t t t t t 7.0 7.5 8.0 8.5 9.0 9.5 0.76 0.80 0.84 0.88 log(n) Accuracy s s s s s m m m m m Figure 4: Learning curves for TiMBL (t), MaxEnt (m) and SVM (s)." ></td>
	<td class="line x" title="113:184	Acc." ></td>
	<td class="line x" title="114:184	Prec." ></td>
	<td class="line x" title="115:184	Recall F-score MBL 87.55 87.00 82.98 84.94 MaxEnt 88.06 87.66 86.87 85.52 SVM 89.34 88.93 84.90 86.87 Table 4: Cast3LB function tagging performance for gold-standard trees scoring 89.34% on accuracy and 86.87% on fscore." ></td>
	<td class="line x" title="116:184	The learning curves for the three algorithms, shown in Figure 4, are also informative, with SVM outperforming the other two methods for all training set sizes." ></td>
	<td class="line x" title="117:184	In particular, the last section of the plot shows SVM performing almost as well as MBL with half as much learning material." ></td>
	<td class="line x" title="118:184	Neither of the three curves shows signs of having reached a maximum, which indicates that inPrecision Recall F-score all corr." ></td>
	<td class="line x" title="119:184	all corr." ></td>
	<td class="line x" title="120:184	all corr." ></td>
	<td class="line x" title="121:184	Baseline 59.26 72.63 60.61 75.35 59.93 73.96 MBL 64.74 78.09 64.18 78.75 64.46 78.42 MaxEnt 65.48 78.90 64.55 79.44 65.01 79.17 SVM 66.96 80.58 66.38 81.27 66.67 80.92 Table 5: Cast3LB function tagging performance for parser output, for all constituents, and for correctly parsed constituents only 140 Methods p-value Baseline vs SVM 1.169109 Baseline vs MBL 2.117106 MBL vs MaxEnt 0.0799 MaxEnt vs SVM 0.0005 Table 6: Statistical significance testing results on for the Cast3LB tag assignment on parser output." ></td>
	<td class="line x" title="122:184	Precision Recall F-score Baseline 73.95 70.67 72.27 SVM 76.90 74.48 75.67 Table 7: LFG F-structure evaluation results for parser output creasing the size of the training data should result in further improvements in performance." ></td>
	<td class="line x" title="123:184	Table 5 shows the performance of the three methods on parser output." ></td>
	<td class="line x" title="124:184	The baseline contains the results achieved by treating compound category-function labels as atomic during parser training so that they are included in parser output." ></td>
	<td class="line x" title="125:184	For this task we present two sets of results: (i) for all constituents, and (ii) for correctly parsed constituents only." ></td>
	<td class="line x" title="126:184	Again the best algorithm turns out to be SVM." ></td>
	<td class="line x" title="127:184	It outperforms the baseline by a large margin (6.74% for all constituents)." ></td>
	<td class="line x" title="128:184	The difference in performance for gold standard trees, and the correctly parsed constituents in parser output is rather larger than what Blaheta and Charniak report." ></td>
	<td class="line x" title="129:184	Further analysis is needed to identify the source of this difference but we suspect that one contributing factor is the use of greater number of context features combined with a higher parse error rate in comparison to their experiments on the Penn II Treebank." ></td>
	<td class="line x" title="130:184	Since any misanalysis of constituency structure in the vicinity of target node can have negative impact, greater reliance on context means greater susceptibility to parse errors." ></td>
	<td class="line x" title="131:184	Another factor to consider is the fact that we trained and adjusted parameters on goldstandard trees, and the model learned may rely on features of those trees that the parser is unable to reproduce." ></td>
	<td class="line x" title="132:184	For the experiments on parser output (all constituents) we performed a series of sign tests in order to determine to what extent the differences in performance between the different methods are statistically significant." ></td>
	<td class="line x" title="133:184	For each pair of methods we calculate the f-score for each sentence in the test set." ></td>
	<td class="line x" title="134:184	For those sentences on which the scores differ (i.e. the number of trials) we calculate in how many cases the second method is better than the first (i.e. the number of successes)." ></td>
	<td class="line x" title="135:184	We then perform the test with the null hypothesis that the probability of success is chance (= 0.5) and the alternative hypothesis that the probability of success is greater than chance (> 0.5)." ></td>
	<td class="line x" title="136:184	The results are summarized in Table 6." ></td>
	<td class="line x" title="137:184	Given that we perform 4 pairwise comparisons, we apply the Bonferroni correction and adjust our target  = 4 . For the confidence level 95% ( = 0.0125) all pairs give statistically significant results, except for MBL vs MaxEnt." ></td>
	<td class="line x" title="138:184	6 Task-Based LFG Annotation Evaluation Finally, we also evaluated the actual f-structures obtained by running the LFG-annotation algorithm on trees produced by the parser and enriched with Cast3LB function tags assigned using SVM." ></td>
	<td class="line x" title="139:184	For this task-based evaluation we produced a gold standard consisting of f-structures corresponding to all sentences in the test set." ></td>
	<td class="line x" title="140:184	The LFG-annotation algorithm was run on the test set trees (which contained original Cast3LB treebank function tags), and the resulting f-structures were manually corrected." ></td>
	<td class="line x" title="141:184	Following Crouch et al.(2002), we convert the f-structures to triples of the form GF,Pi,Pj, where Pi is the value of the PRED attribute of the f-structure, GF is an LFG grammatical function attribute, and Pj is the value of the PRED attribute of the f-structure which is the value of the GF attribute." ></td>
	<td class="line x" title="143:184	This is done recursively for each level of embedding in the f-structure." ></td>
	<td class="line x" title="144:184	Attributes with atomic values are ignored for the purposes of this evaluation." ></td>
	<td class="line x" title="145:184	The results obtained are shown in Table 7." ></td>
	<td class="line x" title="146:184	We also performed a statistical significance test for these results, using the same method as for the Cast3LB tag assigment task." ></td>
	<td class="line x" title="147:184	The p-value given by the sign test was 2.118105, comfortably below  = 1%." ></td>
	<td class="line x" title="148:184	The higher scores achieved in the LFG fstructure evaluation in comparison with the preceding Cast3LB tag assignment evaluation (Table 5) can be attributed to two main factors." ></td>
	<td class="line x" title="149:184	Firstly, the mapping from Cast3LB tags to LFG grammatical functions is not one-to-one." ></td>
	<td class="line x" title="150:184	For example three Cast3LB tags (CC, MOD and ET) are all mapped to LFG ADJUNCT." ></td>
	<td class="line x" title="151:184	Thus mistagging a MOD as 141 ATR CC CD CI CREG MOD SUJ ATR 136 2 0 0 0 0 5 CC 6 552 12 4 25 18 6 CD 1 19 418 5 3 0 26 CI 0 6 1 50 1 0 0 CREG 0 6 0 2 43 0 0 MOD 0 0 0 0 0 19 0 SUJ 0 8 24 2 0 0 465 Table 8: Simplified confusion matrix for SVM on test-set gold-standard trees." ></td>
	<td class="line x" title="152:184	The gold-standard Cast3LB function tags are shown in the first row, the predicted tags in the first column." ></td>
	<td class="line x" title="153:184	So e.g. SUJ was mistagged as CD in 26 cases." ></td>
	<td class="line x" title="154:184	Low frequency function tags as well as those rarely mispredicted have been omitted for clarity." ></td>
	<td class="line x" title="155:184	CC does not affect the f-structure score." ></td>
	<td class="line x" title="156:184	On the other hand the Cast3LB CD tag can be mapped to OBJ, COMP, or XCOMP, and it can be easily decided which one is appropriate depending on the category label of the target node." ></td>
	<td class="line x" title="157:184	Additionally many nodes which receive no function tag in Cast3LB, such as noun modifiers, are straightforwardly mapped to LFG ADJUNCT." ></td>
	<td class="line x" title="158:184	Similarly, objects of prepositions receive the LFG OBJ function." ></td>
	<td class="line x" title="159:184	Secondly, the f-structure evaluation metric is less sensitive to small constituency misconfigurations: it is not necessary to correctly identify the token range spanned by a target node as long as the head (which provides the PRED attribute) is correct." ></td>
	<td class="line x" title="160:184	7 Error Analysis In order to understand sources of error and determine how much room for further improvement there is, we examined the most common cases of Cast3LB function mistagging." ></td>
	<td class="line x" title="161:184	A simplified confusion matrix with the most common Cast3LB tags is shown in Table 8." ></td>
	<td class="line x" title="162:184	The most common mistakes occur between SUJ and CD, in both directions, and many also CREGs are erroneously tagged as CC." ></td>
	<td class="line x" title="163:184	7.1 Subject vs Direct Object We noticed that in over 50% of cases when a Direct Object (CD) was misidentified as Subject (SUJ), the target nodes mother was a relative clause." ></td>
	<td class="line x" title="164:184	It turns out that in Spanish relative clauses genuine syntactic ambiguity is not uncommon." ></td>
	<td class="line x" title="165:184	Consider the following Spanish phrase: (1) Sistemas Systems que which usan use el DET 95% 95% de of los DET ordenadores." ></td>
	<td class="line x" title="166:184	computers Its translation into English is either Systems that use 95% of computers or alternatively Systems that 95% of computers use." ></td>
	<td class="line x" title="167:184	In Spanish, unlike in English, preverbal / postverbal position of a constituent is not a good guide to its grammatical function in this and similar contexts." ></td>
	<td class="line x" title="168:184	Human annotators can use their world knowledge to decide on the correct semantic role of a target constituent and use it in assigning a correct grammatical function, but such information is obviously not used in our machine learning methods." ></td>
	<td class="line x" title="169:184	Thus such mistakes seem likely to remain unresolvable in our current approach." ></td>
	<td class="line x" title="170:184	7.2 Prepositional Object vs Adjunct The frequent misidentification of Prepositional Objects (CREG) as Adjuncts (CC) seen in Table 8 can be accounted for by several factors." ></td>
	<td class="line x" title="171:184	Firstly, Prepositional Objects are strongly dependent on specific verbs and the comparatively small size of our training data means that there is limited opportunity for a machine-learning algorithm to learn low-frequency lexical dependencies." ></td>
	<td class="line x" title="172:184	Here the obvious solution is to use a more adequate amount of training material when it becomes available." ></td>
	<td class="line x" title="173:184	A further problem with the Prepositional Object Adjunct distinction is its inherent fuzziness." ></td>
	<td class="line x" title="174:184	Because of this, treebank designers may fail to provide easy-to-follow, clearcut guidelines and human annotators necessarily exercise a certain degree of arbitrariness in assigning one or the other function." ></td>
	<td class="line x" title="175:184	8 Conclusions and Future Research Our research has shown that machine-learningbased Cast3LB tag assignment as a postprocessing step to raw tree parser output statistically significantly outperforms a baseline where the parser itself is trained to learn category / Cast3LB-function pairs." ></td>
	<td class="line x" title="176:184	In contrast to the parser-based method, the machine-learning-based method avoids some sparse data problems and allows for more control over Cast3LB tag assignment." ></td>
	<td class="line x" title="177:184	We have found that the SVM algorithm outperforms the other two machine learning methods used." ></td>
	<td class="line x" title="178:184	142 In addition, we evaluated Cast3LB tag assignment in a task-based setting in the context of automatically acquiring LFG resources for Spanish from Cast3LB." ></td>
	<td class="line x" title="179:184	Machine-learning-based Cast3LB tag assignment yields statistically-significantly improved LFG f-structures compared to parserbased assignment." ></td>
	<td class="line x" title="180:184	One limitation of our method is the fact that it treats the classification task separately for each target node." ></td>
	<td class="line x" title="181:184	It thus fails to observe constraints on the possible sequences of grammatical function tags in the same local context." ></td>
	<td class="line x" title="182:184	Some functions are unique, such as the Subject, whereas others (Direct and Indirect Object) can only be realized by a full NP once, although they can be doubled by a clitic pronoun." ></td>
	<td class="line x" title="183:184	Capturing such global constraints will need further work." ></td>
	<td class="line x" title="184:184	Acknowledgements We gratefully acknowledge support from Science Foundation Ireland grant 04/IN/I527 for the research reported in this paper." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D07-1027
Recovering Non-Local Dependencies for Chinese
Guo, Yuqing;Wang, Haifeng;Van Genabith, Josef;"></td>
	<td class="line x" title="1:285	Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp." ></td>
	<td class="line x" title="2:285	257266, Prague, June 2007." ></td>
	<td class="line x" title="3:285	c2007 Association for Computational Linguistics Recovering Non-Local Dependencies for Chinese Yuqing Guo NCLT, School of Computing Dublin City University Dublin 9, Ireland yguo@computing.dcu.ie Haifeng Wang Toshiba (China) Research and Development Center Beijing, 100738, China wanghaifeng@rdc.toshiba.com.cn Josef van Genabith NCLT, School of Computing Dublin City University IBM CAS, Dublin, Ireland josef@computing.dcu.ie Abstract To date, work on Non-Local Dependencies (NLDs) has focused almost exclusively on English and it is an open research question how well these approaches migrate to other languages." ></td>
	<td class="line x" title="4:285	This paper surveys non-local dependency constructions in Chinese as represented in the Penn Chinese Treebank (CTB) and provides an approach for generating proper predicate-argument-modifier structures including NLDs from surface contextfree phrase structure trees." ></td>
	<td class="line x" title="5:285	Our approach recovers non-local dependencies at the level of Lexical-Functional Grammar f-structures, using automatically acquired subcategorisation frames and f-structure paths linking antecedents and traces in NLDs." ></td>
	<td class="line x" title="6:285	Currently our algorithm achieves 92.2% f-score for trace insertion and 84.3% for antecedent recovery evaluating on gold-standard CTB trees, and 64.7% and 54.7%, respectively, on CTBtrained state-of-the-art parser output trees." ></td>
	<td class="line x" title="7:285	1 Introduction A substantial number of linguistic phenomena such as topicalisation, relativisation, coordination and raising & control constructions, permit a constituent in one position to bear the grammatical role associated with another position." ></td>
	<td class="line x" title="8:285	These relationships are referred to Non-Local Dependencies (NLDs), where the surface location of the constituent is called BBantecedentBC, and the site where the antecedent should be interpreted semantically is called BBtraceBC." ></td>
	<td class="line x" title="9:285	Capturing non-local dependencies is crucial to the accurate and complete determination of semantic interpretation in the form of predicateargument-modifier structures or deep dependencies." ></td>
	<td class="line x" title="10:285	However, with few exceptions (Model 3 of Collins, 1999; Schmid, 2006), output trees produced by state-of-the-art broad coverage statistical parsers (Charniak, 2000; Bikel, 2004) are only surface context-free phrase structure trees (CFG-trees) without empty categories and coindexation to represent displaced constituents." ></td>
	<td class="line x" title="11:285	Because of the importance of non-local dependencies in the proper determination of predicate-argument structures, recent years have witnessed a considerable amount of research on reconstructing such hidden relationships in CFG-trees." ></td>
	<td class="line x" title="12:285	Three strategies have been proposed: (i) post-processing parser output with pattern matchers (Johnson, 2002), linguistic principles (Campbell, 2004) or machine learning methods (Higgins, 2003; Levy and Manning, 2004; Gabbard et al. , 2006) to recover empty nodes and identify their antecedents;1 (ii) integrating non-local dependency recovery into the parser by enriching a simple PCFG model with GPSG-style gap features (Collins, 1999; Schmid, 2006); (iii) pre-processing the input sentence with a finite-state trace tagger which detects empty nodes before parsing, and identify the antecedents on the parser output with the gap information (Dienes and Dubey, 2003a; Dienes and Dubey, 2003b)." ></td>
	<td class="line oc" title="13:285	In addition to CFG-oriented approaches, a number of richer treebank-based grammar acquisition and parsing methods based on HPSG (Miyao et al. , 2003), CCG (Clark and Hockenmaier, 2002), LFG (Riezler et al. , 2002; Cahill et al. , 2004) and Dependency Grammar (Nivre and Nilsson, 2005) incorporate non-local dependencies into their deep syntactic or semantic representations." ></td>
	<td class="line x" title="14:285	A common characteristic of all these approaches 1(Jijkoun, 2003; Jijkoun and Rijke, 2004) also describe postprocessing methods to recover NLDs, which are applied to syntactic dependency structures converted from CFG-trees." ></td>
	<td class="line x" title="15:285	257 is that, to date, the research has focused almost entirely on English,2 despite the disparity in type and frequency of non-local dependencies for various languages." ></td>
	<td class="line x" title="16:285	In this paper, we address recovering non-local dependencies for Chinese, a language drastically different from English and whose special features such as lack of morphological inflection make NLD recovery more challenging." ></td>
	<td class="line pc" title="17:285	Inspired by (Cahill et al. , 2004)s methodology which was originally designed for English and Penn-II treebank, our approach to Chinese non-local dependency recovery is based on Lexical-Functional Grammar (LFG), a formalism that involves both phrase structure trees and predicate-argument structures." ></td>
	<td class="line x" title="18:285	NLDs are recovered in LFG f-structures using automatically acquired subcategorisation frames and finite approximations of functional uncertainty equations describing NLD paths at the level of f-structures." ></td>
	<td class="line x" title="19:285	The paper is structured as follows: in Section 2 we outline the distinguishing features of Chinese nonlocal dependencies compared to English." ></td>
	<td class="line oc" title="20:285	In Section 3 we review (Cahill et al. , 2004)s method for recovering English NLDs in treebank-based LFG approximations." ></td>
	<td class="line x" title="21:285	In Section 4, we describe how we modify and substantially extend the previous method to recover all types of NLDs for Chinese data." ></td>
	<td class="line x" title="22:285	We present experiments and provide a dependencybased evaluation in Section 5." ></td>
	<td class="line x" title="23:285	Finally we conclude and summarise future work." ></td>
	<td class="line x" title="24:285	2 Non-Local Dependencies in Chinese In the Penn Chinese Treebank (CTB) (Xue et al. , 2002) non-local dependencies are represented in terms of empty categories (ECs) and (for some of them) coindexation with antecedents, as exemplified in Figure 1." ></td>
	<td class="line x" title="25:285	Following previous work for English and the CTB annotation scheme, we use BBnonlocal dependenciesBCas a cover term for all missing or dislocated elements represented in the CTB as an empty category (with or without coindexation/antecedent), and our use of the term remains agnostic about fine-grained distinctions between nonlocal dependencies drawn in the theoretical linguistics literature." ></td>
	<td class="line x" title="26:285	In order to give an overview on the character2 (Levy and Manning, 2004) is the only approach we are aware of that has been applied to both English and German." ></td>
	<td class="line x" title="27:285	(1) G0 AHFBD9GV ANDRCZ CSGD AR AZ DUCJ not want look-for train have potential DE new writer (People) dont want to look for and train new writers who have potential. IP NP-SBJ -NONE*pro* VP ADVP AD G0 not VP VV AHFB want IP-OBJ NP-SBJ -NONE*PRO* VP VP VV D9GV look for NP-OBJ -NONE*RNR*-2 PU AX VP VV ANDR train NP-OBJ-2 CP WHNP-1 -NONE*OP* CP IP NP-SBJ -NONE*T*-1 VP VE CZ have NP NN CSGD potential DEC AR DE ADJP JJ AZ new NP NN DUCJ writer Figure 1: Example of non-local annotations in CTB, including dropped subject (*pro*), control subject (*PRO*), relative clause (*T*), and coordination (*RNR*)." ></td>
	<td class="line x" title="28:285	istics of Chinese non-local dependencies, we extracted all empty categories together with coindexed antecedents from the Penn Chinese Treebank version 5.1 (CTB5.1)." ></td>
	<td class="line x" title="29:285	Table 1 gives a breakdown of the most frequent types of empty categories and their antecedents, which account for 43,791 of the total 43,954 (99.6%) ECs in CTB5.1.3 According to their different linguistics properties, we divide the empty nodes listed in Table 1 into three major types: null relative pronouns, locally mediated dependencies, and long-distance dependencies." ></td>
	<td class="line x" title="30:285	Null Relative Pronouns (lines 2, 7) themselves are local dependencies, and thus are not coindexed with an antecedent." ></td>
	<td class="line x" title="31:285	But they mediate non-local dependencies by functioning as antecedents for the dis3An extensive description of the types of empty categories and the use of coindexation in CTB can be found in Section VI of the bracketing guidelines." ></td>
	<td class="line x" title="32:285	258 Antecedent POS Label Count Description 1 WHNP NP *T* 11670 WH trace (e.g. *OP*ELC1/ChinaD9AJ/launch*T*AR/DEELB4/satellite) 2 WHNP *OP* 11621 Empty relative pronouns (e.g. *OP*ELC1/ChinaD9AJ/launchAR/DEELB4/satellite) 3 NP *PRO* 10946 Control constructions (e.g. GXD4/hereG0/notC6/allow*PRO*FGGJ/smoke) 4 NP *pro* 7481 Pro-drop situations (e.g. *pro*G0/notC9/everE1AK/encounterAR/DEEVC3/problem) 5 IP IP *T* 575 Topicalisation (e.g. F3DM/weCD/canC1/winENEM/heCO/say*T*) 6 WHPP PP *T* 337 WH trace (e.g. *OP*BOED/population*T*E7BK/denseBBER/area) 7 WHPP *OP* 337 Empty relative pronouns (e.g. *OP*BOED/populationE7BK/denseBBER/area) 8 NP NP * 291 Raising & passive constructions (e.g. F3DM/weAQ/BEIH0G0/exclude*BFA9/outside) 9 NP NP *RNR* 258 Coordinations (e.g. AJDD/encourage*RNR*G2/andDGEX/supportG5CL/investment) 10 CLP CLP *RNR* 182 Coordinations (e.g. FM/five*RNR*E6/toEB/tenF3/hundred millionA3/Yuan) 11 NP NP *T* 93 Topicalisation (e.g. AUCH/salaryFT/allCM/use*T*BH/forE0CF/pleasure) Table 1: The distribution of the most frequent types of empty categories and their antecedents in CTB5.1." ></td>
	<td class="line x" title="33:285	The types with frequency less than 30 are ignored." ></td>
	<td class="line x" title="34:285	located constituent inside a relative clause.4 Locally Mediated Dependencies are non-local as they are projected through a third lexical item (such as a control or raising verb) which involves a dependency between two adjacent levels and they are therefore bounded." ></td>
	<td class="line x" title="35:285	This type encompasses: (line 8) raising constructions, and short-bei constructions (passivisation); (line 3) control constructions, which includes two different types: a generic *PRO* with an arbitrary reading (approximately equals to unexpressed subjects of to-infinitive and gerund verbs in English); and a *PRO* with definite reference (subject or object control).5 Long-Distance Dependencies (LDDs) differ from locally mediated dependencies, in that the path linking the antecedent and trace might be unbounded (also called unbounded, long-range dependencies)." ></td>
	<td class="line x" title="36:285	LDDs include the following phenomena: Wh-traces in relative clauses, where an argument (line 1) or adjunct (line 6) BBmovesBCand is coindexed with theBBextractionBCsite." ></td>
	<td class="line x" title="37:285	Topicalisation (lines 5, 11) is one of the typical LDDs in English, whereas in Chinese not all topics involve displacement, for instance (2)." ></td>
	<td class="line x" title="38:285	(2) AGEU EICD DL DF Beijing autumn most beautiful Autumn is the most beautiful in Beijing. 4Null relative pronouns used in the CTB annotation are to distinguish relative clauses in which an argument or adjunct of the embedded verb is BBmissingBCfrom complement (appositive) clauses which do not involve non-local dependencies." ></td>
	<td class="line x" title="39:285	5However in this case the CTB annotation doesnt coindex the locus (trace) with its controller (antecedent)." ></td>
	<td class="line x" title="40:285	Coordination is divided into two groups: right node raising of an NP phrase which is an argument shared by the coordinate predicates (line 9); and the coordination of quantifier phrases (line 10) and verbal phrases (3), in which the antecedent and trace are both predicates and possibly take their own arguments or adjuncts." ></td>
	<td class="line x" title="41:285	(3) F3G2 EMEPC7 AM GYCX G2 *RNR*EAAJ I and he respectively go to company and *RNR* hospital I went to the company and he went to the hospital respectively. Pro-drop situations (line 4) are prominent in Chinese because subject and object are only semantically but not syntactically required." ></td>
	<td class="line x" title="42:285	Nevertheless we also treat pro-drop as a long-distance dependency as in principle the dropped subjects can be determined from the general (often inter-sentential) context." ></td>
	<td class="line x" title="43:285	Table 2 gives a quantitative comparison of NLDs between Chinese data in CTB5.1 and English in Penn-II." ></td>
	<td class="line x" title="44:285	The data reveals that: first, NLDs in Chinese are much more frequent than in English (by nearly 1.5 times); and moreover 69% are not explicitly linked to an antecedent, compared to 43% for English, due to the high prevalence of pro-drop in Chinese." ></td>
	<td class="line x" title="45:285	# of # of # of # non% nonsent EC EC/sent coindex coindex Chinese 18,804 43,954 2.34 30,429 69.23 English 49,207 79,245 1.61 34,455 43.48 Table 2: Comparison of NLDs between Chinese data in CTB5.1 and English in Penn-II." ></td>
	<td class="line x" title="46:285	259 (4) CP F3DMCM BHE0CF money we use to please Money, we use for pleasure. IP NP-TPC-1 NN CP money NP-SBJ PN F3DM we VP VP VV CM use NP-OBJ -NONE*T*-1 IP NP-SBJ -NONE*PRO* VP MSP BH to VV E0CF please IP NP-TPC [TOPIC=] [TOPIC=COMP*OBJ] NN [=] CP money NP-SBJ [SUBJ=] PN [=] F3DM we VP [=] VV [=] CM use VP [XCOMP=] [SUBJ=XCOMP:SUBJ] MSP [msp=BH] BH to VV [=] E0CF please f1 :               PRED CMSUBJ, OBJ, XCOMP GLOSS use TOPIC f2 : bracketleftBigg PRED CP GLOSS money bracketrightBigg 1 SUBJ f3 : bracketleftBigg PRED F3DM GLOSS we bracketrightBigg 2 OBJ 1 XCOMP f4 :     PRED E0CFSUBJ GLOSS please SUBJ 2 MSP BH                   (a) (b) (c) Figure 2: (a) the CTB tree; (b) LFG c-structure with functional equations; (c) corresponding f-structure." ></td>
	<td class="line x" title="47:285	() in the functional annotation refers to the f-structure associated with the mother node and () to that of the local node." ></td>
	<td class="line x" title="48:285	3 NLD Recovery in LFG Approximations 3.1 Lexical Functional Grammar Lexical Functional Grammar (Kaplan and Bresnan, 1982) is a constraint-based grammar formalism which minimally involves two levels of syntactic representation: c(onstituent)-structure and f(unctional)-structure." ></td>
	<td class="line x" title="49:285	C-structure takes the form of CFG-trees and captures surface grammatical configurations." ></td>
	<td class="line x" title="50:285	F-structure encodes more abstract grammatical functions (GFs) such as SUBJ(ect), OBJ(ect), COMP(lement), ADJ(unct) and TOPIC etc. , in the form of Attribute Value Matrices which approximate to basic predicate-argument-adjunct structures or dependency relations." ></td>
	<td class="line x" title="51:285	C-structures are related to f-structures by functional annotations (cf.Figure 2 (b) & (c))." ></td>
	<td class="line x" title="53:285	In LFG, non-local dependencies are captured at f-structure level in terms of reentrancies, indicated 1 for the topicalisation and 2 for the control construction in Figure 2(c) obviating the need for traces and coindexation in the c-structure (Figure 2(b)), unlike in CTB trees (Figure 2(a))." ></td>
	<td class="line x" title="54:285	LFG uses functional uncertainty (FU) equations (regular expressions) to specify paths in f-structures between the trace and its antecedent." ></td>
	<td class="line x" title="55:285	To account for the reentrancy 1 in the f-structure, a FU equation of the form TOPIC=COMP*OBJ is required (as the length of the dependency might be unbounded)." ></td>
	<td class="line x" title="56:285	The equation states that the value of the TOPIC attribute is token identical with the value of the final OBJ argument along a path through the immediately enclosing f-structure along zero or more COMP attributes." ></td>
	<td class="line x" title="57:285	In addition to FU equations, subcategorisation information is also a significant ingredient in LFGs account of non-local dependencies." ></td>
	<td class="line x" title="58:285	Subcategorisation frames (subcat frames) specify the governable grammatical functions (i.e. arguments) required by a particular predicate." ></td>
	<td class="line x" title="59:285	In Figure 2(c) each predicate in the f-structure is followed by its subcat frame." ></td>
	<td class="line oc" title="60:285	3.2 F-Structure Based NLD Recovery (Cahill et al. , 2004) presented a NLD recovery algorithm operating at LFG f-structure for treebankbased LFG approximations." ></td>
	<td class="line o" title="61:285	The method automatically converts Penn-II treebank trees with traces and coindexation into proper f-structures where traces and coindexation in treebank trees (Figure 2(a)) are represented as corresponding reentrances in fstructures (Figure 2(c)), and from the f-structures automatically extracts subcat frames by collecting all arguments of the local predicate at each level of the f-structures, and further acquires finite approximations of FU equations by extracting paths linking the reentracies occurring in the f-structures." ></td>
	<td class="line oc" title="62:285	(Cahill et al. , 2004)s approach for English resolves three LDD types in parser output trees without traces and coindexation (Figure 2(b)), i.e. topicalisation (TOPIC), wh-movement in relative clauses (TOPIC REL) and interrogatives (FOCUS)." ></td>
	<td class="line x" title="63:285	Given 260 a set of subcat frames s for lemma w with probabilities P(s|w), a set of paths p linking reentrancies conditioned on the triggering antecedent a (TOPIC, TOPIC REL or FOCUS) with probabilities P(p|a), the core algorithm recursively traverses an f-structure f to: find a TOPIC|TOPIC REL|FOCUS:g pair; traverse f along path p to the sub-f-structure h; retrieve the local PRED:w at h, and insert g to h iff * all GFs specified in the subcat frame s except g are present at h (completeness condition) * no other governable GFs present at h are specified in s (coherence condition) rank resolution candidates according to the product of subcat frame and NLD path probabilities (Eq." ></td>
	<td class="line x" title="64:285	1)." ></td>
	<td class="line x" title="65:285	P(s|w)  P(p|a) (1) 4 NLD Recovery Algorithm for Chinese 4.1 Automatic F-Structure Generation Our NLD recovery is done at the level of LFG fstructures." ></td>
	<td class="line pc" title="66:285	Inspired by (Cahill et al. , 2004; Burke et al. , 2004), we have implemented an f-structure annotation algorithm to automatically obtain f-structures from CFG-trees in the CTB5.1." ></td>
	<td class="line x" title="67:285	The f-structure annotation algorithm, described below, is applied both to the original CTB trees providing functional tags, traces and coindexation to generate the training corpus, and to the parser output trees without traces and coindexation to provide the f-structure input for NLD recovery." ></td>
	<td class="line x" title="68:285	1." ></td>
	<td class="line x" title="69:285	The CFG-trees are head-lexicalised by headfinding rules similar to (Collins, 1999), adapted to CTB." ></td>
	<td class="line x" title="70:285	2." ></td>
	<td class="line x" title="71:285	Each local subtree of depth one is partitioned by the head into left and right context." ></td>
	<td class="line x" title="72:285	Leftright context rules exploiting configurational, categorial and CTB functional tag information are used to assign each left and right constituent with appropriate functional equations." ></td>
	<td class="line x" title="73:285	3." ></td>
	<td class="line x" title="74:285	Empty nodes and coindexation in the CTB trees are automatically captured into corresponding reentrances at f-structure via functional equations." ></td>
	<td class="line x" title="75:285	4." ></td>
	<td class="line x" title="76:285	All the functional equations are collected and then passed to a constraint solver to generate f-structures." ></td>
	<td class="line nc" title="77:285	4.2 Adaptation to Chinese (Cahill et al. , 2004)s algorithm (Section 3.2) only resolves certain NLDs with known types of antecedents (TOPIC, TOPIC REL and FOCUS) at fstructures." ></td>
	<td class="line x" title="78:285	However, as illustrated in Section 2, except for relative clauses, the antecedents in Chinese NLDs do not systematically correspond to types of grammatical function." ></td>
	<td class="line x" title="79:285	Furthermore nearly 70% of all empty categories are not coindexed with an antecedent." ></td>
	<td class="line oc" title="80:285	In order to resolve all Chinese NLDs represented in the CTB, we modify and substantially extend the (Cahill et al. , 2004) (henceforth C04 for short) algorithm as follows: Given the set of subcat frames s for the word w, and a set of paths p for the trace t, the algorithm traverses the f-structure f to: predict a dislocated argument t at a sub-fstructure h by comparing the local PRED:w to ws subcat frames s t can be inserted at h if h together with t is complete and coherent relative to subcat frame s traverse f starting from t along the path p link t to its antecedent a if ps ending GF a exists in a sub-f-structure within f; or leave t without an antecedent if an empty path for t exists In the modified algorithm, we condition the probability of NLD path p (including the empty path without an antecedent) on the GF associated of the trace t rather than the antecedent a as in C04." ></td>
	<td class="line x" title="81:285	The path probability P(p|t) is estimated as: P(p|t) = count(p,t)summationtextn i=1 count(pi,t) (2) In contrast even to English, Chinese has very little morphological information." ></td>
	<td class="line x" title="82:285	As a result, every word in Chinese has a unique form regardless of its syntactic distribution." ></td>
	<td class="line o" title="83:285	For this reason we use more syntactic features w feats in addition to word form to discriminate between appropriate subcat frames s. For a given word w, w feats include: 261 w pos: the part-of-speech of w w gf: the grammatical function of w P(s|w,w feats) replaces C04s P(s|w) as lexical subcat frame probability and is estimated as: P(s|w,w feats) = count(s,w,w feats)summationtextn i=1 count(si,w,w feats) (3) As more conditioning features may cause sever sparse-data problems, in order to increase the coverage of the automatically acquired subcat frames, the subcat frame frequencies count(s,w,w feats) are smoothed by backing off to ws part-of-speech w pos according to Eq." ></td>
	<td class="line x" title="84:285	(4)." ></td>
	<td class="line x" title="85:285	P(s|w pos) is estimated according to Eq." ></td>
	<td class="line x" title="86:285	(5) and weighted by a parameter ." ></td>
	<td class="line x" title="87:285	The lexical subcat frame probabilities are estimated from the smoothed frequencies as shown in Eq." ></td>
	<td class="line x" title="88:285	(6)." ></td>
	<td class="line x" title="89:285	countbk(s,w,w feats) = count(s,w,w feats) (4) +P(s|w pos) P(s|w pos) = count(s,w pos,w gf)summationtextn i=1 count(si,w pos,w gf) (5) Pbk(s|w,w feats) = countbk(s,w,w feats)summationtextn i=1 countbk(si,w,w feats) (6) Finally, NLD resolutions are ranked according to: Pbk(s|w,w feats)  mproductdisplay j=1 P(p|tj) (7) As, apart from the maximum number of arguments in a subcat frame, there is no a priori limit on the number of dislocated arguments in a local fstructure, we rank resolutions with the product of the path probabilities of each (of m) missing argument(s)." ></td>
	<td class="line x" title="90:285	4.3 A Hybrid Fine-Grained Strategy As described in Section 2, there are three types of NLDs in the CTB, and their different linguistic properties may require fine-grained recovery strategies." ></td>
	<td class="line x" title="91:285	Furthermore, as the NLD recovery method described in Section 4.2 is triggered by BBmissingBCsubcategorisable grammatical functions, a few cases of NLDs in which the trace is not an argument in the f-structure, e.g. an ADJUNCT or TOPIC in relative clauses or an null PRED in verbal coordination, can not be recovered by the algorithm." ></td>
	<td class="line o" title="92:285	Table 3 shows the types of NLD that can be recovered by C04 and by the algorithm presented in Section 4.2." ></td>
	<td class="line x" title="93:285	Table 3 shows that a hybrid methodology is required to resolve all types of NLDs in the CTB." ></td>
	<td class="line x" title="94:285	The hybrid method involves four strategies:  Applying a few simple heuristic rules to insert the empty PRED for coordinations and null relative pronouns for relative constructions." ></td>
	<td class="line x" title="95:285	The former is done by comparing the part-of-speech of the local predicates and their arguments in each coordinate; and the latter is triggered by GF ADJUNCT REL in our system." ></td>
	<td class="line x" title="96:285	 Inserting an empty node with GF SUBJ for short-bei construction, control and raising constructions, and relate it to the upper-level SUBJ or OBJ accordingly." ></td>
	<td class="line o" title="97:285	 Exploiting the C04 algorithm to resolve the whtrace in relativisation, including ungovernable GFs TOPIC and ADJUNCT." ></td>
	<td class="line x" title="98:285	 Using our modified algorithm (Section 4.2) to resolve the remaining types, viz." ></td>
	<td class="line x" title="99:285	long-distance dependencies in Chinese." ></td>
	<td class="line o" title="100:285	Antecedent Trace Topic Rel Other Null Argument Adjunct C04    Ours     Table 3: Comparison of the ability of NLD recovery for Chinese between C04 and our algorithm 5 Experiments and Evaluation For all our experiments, we used the first 760 articles (chtb 001.fid to chtb 931.fid, 10,384 sentences) of CTB5.1, from which 75 double-annotated files (chtb 001.fid to chtb 043.fid and chtb 900.fid to chtb 931.fid, 1,046 sentences) were used as test data,6 75 files (chtb 306.fid to chtb 325.fid and chtb 400.fid to chtb 454.fid, 1,082 sentences) were held out as development data, while the other 610 files (8,256 sentences) were used as training data." ></td>
	<td class="line x" title="101:285	Experiments were carried out on two different kinds of input: first on CTB gold standard trees stripped of all empty nodes and coindexation information; and 6The complete list of double-annotated files can be found in the documentation of CTB5.1." ></td>
	<td class="line x" title="102:285	262 second, on the output trees of Bikels parser (Bikel, 2004)." ></td>
	<td class="line x" title="103:285	The evaluation metric adopted by most previous work used the label and string position of the trace and its antecedent (Johnson, 2002)." ></td>
	<td class="line x" title="104:285	As pointed out by (Campbell, 2004), this metric is insensitive to the correct attachment of the EC into the parse tree, and more importantly it is not clear whether it adequately measures performance in predicateargument structure recovery." ></td>
	<td class="line x" title="105:285	Therefore, we use a predicate-argument based evaluation method instead." ></td>
	<td class="line x" title="106:285	The NLD recovery is represented as a triple in the form of REL(PRED : loc, GF : loc), where REL is the relation between the dislocated GF and the PRED." ></td>
	<td class="line x" title="107:285	In the evaluation for insertion of traces, the GF is represented by the empty category, and in the evaluation for antecedent recovery, the GF is realised by the predicate of the antecedent, e.g. OBJ(CM/use:3, CP/money:1) in Figure 2(c)." ></td>
	<td class="line x" title="108:285	The antecedent and PRED are both numbered with their string position in the input sentence." ></td>
	<td class="line x" title="109:285	Precision, recall and f-score are calculated for the evaluation." ></td>
	<td class="line x" title="110:285	5.1 CTB-Based F-Structure and NLD Resources Acquisition 5.1.1 Automatically Acquired F-Structures As described in Section 4.1, we automatically generate LFG f-structures from the CTB trees to obtain the training data and generate f-structures from the parser output trees, on which the NLDs will be recovered." ></td>
	<td class="line x" title="111:285	To evaluate the performance of the automatic f-structure annotation algorithm, we randomly selected 200 sentences from the test set and manually annotated the f-structures to generate a gold standard." ></td>
	<td class="line x" title="112:285	The evaluation metric is the same as for NLD recovery in terms of predicate-argument relations." ></td>
	<td class="line x" title="113:285	Table 4 reports the results against the 200sentence gold standard given the original CTB trees and trees output by Bikels parser." ></td>
	<td class="line x" title="114:285	Dependencies Precision Recall F-Score CTB Trees 95.60 95.82 95.71 Parser Output 74.37 73.15 73.75 Table 4: Evaluation of f-structure annotation 5.1.2 Acquiring Subcat Frames and NLD Paths From the automatically generated f-structure training data, we extract 144,119 different lexical subcat frames and 178 paths linking traces and antecedents for NLD recovery." ></td>
	<td class="line x" title="115:285	Tables 5 & 6 show some examples of the automatically extracted subcat frames and NLD paths respectively." ></td>
	<td class="line x" title="116:285	Word:POS-GF(Subcat Frames) Prob." ></td>
	<td class="line x" title="117:285	C5G9:VV-adj rel([subj,obj]) 0.7655 C5G9:VV-adj rel([subj]) 0.1537 C5G9:VV-adj rel([subj,xcomp]) 0.0337   C5G9:VV-coord([subj,obj]) 0.7915 C5G9:VV-coord([subj]) 0.0975   C5G9:VV-top([subj,obj]) 0.5247 C5G9:VV-top([subj,comp]) 0.2077   Table 5: Examples of subcat frames Trace (Path) Prob." ></td>
	<td class="line x" title="118:285	adjunct(up-adjunct:down-topic rel) 0.9018 adjunct(up-adjunct:up-coord:down-topic rel) 0.0192 adjunct(NULL) 0.0128   obj(up-obj:down-topic rel) 0.7915 obj(up-obj:up-coord:down-coord:down-obj) 0.1108   subj(NULL) 0.3903 subj(up-subj:down-topic rel) 0.2092   Table 6: Examples of NLD paths 5.2 The Basic Model The basic algorithm described in Section 4.2 can be used to indiscriminately resolve almost all NLD types for Chinese including locally mediated dependencies with few exceptions (traces with modifier GFs, which accounts for about 1.5% of all NLDs in CTB5.1)." ></td>
	<td class="line x" title="119:285	Table 7 shows the results of the basic algorithm for trace insertion and antecedent recovery on both stripped CTB trees and parser output trees." ></td>
	<td class="line o" title="120:285	For comparison, we implemented the C04 algorithm on our data and evaluated the result." ></td>
	<td class="line o" title="121:285	Since the basic algorithm focus on argument traces, results for arguments only are given separately." ></td>
	<td class="line n" title="122:285	Table 7 shows that the C04 algorithm achieves a high precision but as expected a low recall due to its limitation to certain types of NLDs." ></td>
	<td class="line o" title="123:285	By contrast, our basic algorithm scored higher recall but lower precision, which is understandable as the C04 algorithm identifies the trace given a known antecedent, whereas our algorithm tries to identify both the trace and antecedent." ></td>
	<td class="line x" title="124:285	Compared to trace 263 Insertion Recovery CTB Trees Parser Output CTB Trees Parser Output Prec." ></td>
	<td class="line x" title="125:285	Rec." ></td>
	<td class="line x" title="126:285	F Prec." ></td>
	<td class="line x" title="127:285	Rec." ></td>
	<td class="line x" title="128:285	F Prec." ></td>
	<td class="line x" title="129:285	Rec." ></td>
	<td class="line x" title="130:285	F Prec." ></td>
	<td class="line x" title="131:285	Rec." ></td>
	<td class="line oc" title="132:285	F (Cahill et al. , 2004) overall 95.98 57.86 72.20 73.00 40.28 51.91 90.16 54.35 67.82 65.54 36.16 46.61 args only 98.64 42.03 58.94 82.69 30.54 44.60 86.36 36.80 51.61 66.08 24.40 35.64 Basic Model overall 92.44 91.28 91.85 63.87 62.15 63.00 63.12 62.33 62.72 42.69 41.54 42.10 args only 89.42 92.95 91.15 60.89 63.45 62.15 47.92 49.81 48.84 31.41 32.73 32.06 Basic Model with Subject Path Constraint overall 92.16 91.36 91.76 63.72 62.20 62.95 75.96 75.30 75.63 50.82 49.61 50.21 args only 89.04 93.08 91.02 60.69 63.52 62.07 66.15 69.15 67.62 42.77 44.76 44.76 Table 7: Evaluation of trace insertion and antecedent recovery for C04 algorithm, our basic algorithm and basic algorithm with the subject path constraint." ></td>
	<td class="line x" title="133:285	Insertion Recovery Basic Model Hybrid Model Basic Model Hybrid Model Prec." ></td>
	<td class="line x" title="134:285	Rec." ></td>
	<td class="line x" title="135:285	F Prec." ></td>
	<td class="line x" title="136:285	Rec." ></td>
	<td class="line x" title="137:285	F Prec." ></td>
	<td class="line x" title="138:285	Rec." ></td>
	<td class="line x" title="139:285	F Prec." ></td>
	<td class="line x" title="140:285	Rec." ></td>
	<td class="line x" title="141:285	F Overall 92.16 91.36 91.76 92.86 91.45 92.15 75.96 75.30 75.63 84.92 83.64 84.28 SUBJ 92.95 97.81 95.32 94.38 97.81 96.06 66.93 70.42 68.63 81.61 84.57 83.06 OBJ 65.28 64.98 65.13 78.95 55.30 65.04 61.57 61.29 61.43 75.66 53.00 62.33 ADJUNCT 0.0 0.0 0.0 38.24 25.49 30.59 0.0 0.0 0.0 38.24 25.49 30.59 TOPIC 0.0 0.0 0.0 33.33 35.14 34.21 0.0 0.0 0.0 33.33 35.14 34.21 TOPIC REL 99.85 99.39 99.62 99.85 99.39 99.62 99.85 99.39 99.62 99.85 99.39 99.62 COORD 90.00 100.00 94.74 90.00 100.00 94.74 90.00 100.00 94.74 90.00 100.00 94.74 Table 8: Breakdown of trace insertion and antecedent recovery results on stripped CTB trees for the hybrid model by major grammatical functions." ></td>
	<td class="line x" title="142:285	insertion, the general results for antecedent identification are rather poor." ></td>
	<td class="line x" title="143:285	Examining the development data, we found that most recovery errors were due to wrongly treating missing SUBJs as a PRO (using empty NLD paths)." ></td>
	<td class="line x" title="144:285	Since the subject in Chinese has a very strong tendency to be omitted if it can be inferred from context, the empty NLD path (without any antecedent) has the greatest probability in all resolution paths conditioned on SUBJ, and prevents the SUBJ from finding a proper antecedent in certain cases." ></td>
	<td class="line x" title="145:285	To test the effect of the empty path on SUBJ, we weighted non-empty paths for SUBJ so as to suppress the empty path." ></td>
	<td class="line x" title="146:285	After testing on the development set, the optimal weight was found to be 1.9." ></td>
	<td class="line x" title="147:285	The subject path constraint model shows a dramatic improvement of 12.9% and 8.1% for the overall result of antecedent recovery on CTB trees and parser output trees." ></td>
	<td class="line x" title="148:285	5.3 The Hybrid Fine-Grained Model As proposed in Section 4.3, we implemented a more fine-grained strategy to capture specific linguistic properties of different NLD types in the CTB." ></td>
	<td class="line oc" title="149:285	We also combine our basic algorithm (Section 4.2) with (Cahill et al. , 2004)s algorithm in order to resolve the modifier-function traces." ></td>
	<td class="line o" title="150:285	The two algorithms may conflict due to (i) inserting the same trace at the same site but related to different antecedents or (ii) resolving the same antecedent to different traces." ></td>
	<td class="line p" title="151:285	We keep the traces inserted by the C04 algorithm and abandon those inserted by our algorithm in case of conflict, as the results in Section 5.2 suggest that C04 has a higher precision than ours." ></td>
	<td class="line x" title="152:285	Table 8 reports the results of trace insertion and antecedent recovery, respectively, on stripped CTB trees, broken down by major GFs." ></td>
	<td class="line x" title="153:285	The fine-grained hybrid model allows us to recover NLDs with traces with modifier functions and, more importantly it is sensitive to particular linguistic properties of different NLD types." ></td>
	<td class="line x" title="154:285	As the hybrid model separates the locally mediated dependencies from other long-distance dependencies, it increases the f-score by 8.7% for antecedent recovery compared with the basic model." ></td>
	<td class="line x" title="155:285	Table 9 reports the results of the hybrid model on parser output trees, which shows an increase of 3.6% for antecedent re264 covery (compared with Table 7)." ></td>
	<td class="line x" title="156:285	Insertion Recovery Prec." ></td>
	<td class="line x" title="157:285	Rec." ></td>
	<td class="line x" title="158:285	F Prec." ></td>
	<td class="line x" title="159:285	Rec." ></td>
	<td class="line x" title="160:285	F overall 64.07 62.37 63.21 54.53 53.08 53.79 Table 9: Evaluation of hybrid model for trace insertion and antecedent recovery on parser output trees." ></td>
	<td class="line x" title="161:285	5.4 Better Training for Parser Output Our experiments show that although our NLD recovery algorithm performs well on stripped CTB trees, it is sensitive to the noise in parser output trees, with a performance drop of about 30%." ></td>
	<td class="line x" title="162:285	This is in contrast to English data, on which (Johnson, 2002) reports a drop of 7-9% moving from treebank trees to parser output trees." ></td>
	<td class="line x" title="163:285	No doubt this is partially due to the poor performance of the parser on Chinese data." ></td>
	<td class="line x" title="164:285	It is widely accepted that parsing Chinese is more difficult than parsing other more configurational or richer morphological languages, such as English.7 Our NLD recovery algorithm runs on automatically generated LFG f-structures." ></td>
	<td class="line x" title="165:285	The f-structure annotation algorithm is highly tailored to the CTB bracketing scheme (using configurational, categorial and functional tag information), and suffers considerably from errors produced by the parser." ></td>
	<td class="line x" title="166:285	Table 4 shows that performance of the f-structure annotation decreases sharply (about 22%) for the parser output trees and this contributes to the eventual trace insertion and antecedent recovery performance drop." ></td>
	<td class="line x" title="167:285	Since the f-structures automatically generated from parser output trees are substantially different from those generated from the original CTB trees, our method to obtain the NLD resolution training data suffers from a serious drawback: the training data come from perfect CTB trees, whereas test data are derived from imperfect parser output trees." ></td>
	<td class="line x" title="168:285	This constitutes a serious drawback for machine learning based approaches, such as ours: ideally, instances seen during training should be similar to unseen test data." ></td>
	<td class="line x" title="169:285	To make training examples more similar to test instances, we reparse the training set to obtain better training data." ></td>
	<td class="line x" title="170:285	To avoid running the parser on the training data, we carried out 10-fold-cross training, dividing the training data into 10 parts and parsing 7(Bikel, 2004) reports 89% f-score for English parsing of Penn-II treebank data and 79% f-score for Chinese parsing on CTB version 3." ></td>
	<td class="line x" title="171:285	each part in turn with the parser trained on the remaining 9 parts." ></td>
	<td class="line x" title="172:285	The reparsed training data are more similar to the test data than the original perfect CTB trees." ></td>
	<td class="line x" title="173:285	We then converted both the reparsed training data and the original CTB trees into f-structures, and by comparing with the f-structures generated from the original CTB trees, we recovered the empty nodes and coindexation on the f-structures generated from the reparsed training data." ></td>
	<td class="line x" title="174:285	We used parser output based f-structures to train our NLD recovery model and recovered NLDs for parser output trees from the test data." ></td>
	<td class="line x" title="175:285	Table 10 presents the results for trace insertion and antecedent recovery on parser output trees using the improved training method, which shows a clear increase in precision and almost the same recall over the normal training (Table 9)." ></td>
	<td class="line x" title="176:285	Insertion Recovery Prec." ></td>
	<td class="line x" title="177:285	Rec." ></td>
	<td class="line x" title="178:285	F Prec." ></td>
	<td class="line x" title="179:285	Rec." ></td>
	<td class="line x" title="180:285	F overall 67.29 62.33 64.71 56.88 52.69 54.71 Table 10: Evaluation of hybrid model for trace insertion and antecedent recovery on parser output trees with better training." ></td>
	<td class="line x" title="181:285	6 Conclusion We have presented an algorithm for recovering nonlocal dependencies for Chinese." ></td>
	<td class="line oc" title="182:285	Our method revises and considerably extends the approach of (Cahill et al. , 2004) originally designed for English, and, to the best of our knowledge, is the first NLD recovery algorithm for Chinese." ></td>
	<td class="line nc" title="183:285	The evaluation shows that our algorithm considerably outperforms (Cahill et al. , 2004)s with respect to Chinese data." ></td>
	<td class="line x" title="184:285	In future work, we will refine and extend the conditioning features in our models to discriminate subcat frames and explore the possibilities to use the Chinese Propbank and Hownet to supplement our automatically acquired subcat frames." ></td>
	<td class="line x" title="185:285	We will investigate ways of closing the gap between the performance of gold-standard and parer output trees, including improving parsing result for Chinese." ></td>
	<td class="line x" title="186:285	We also plan to adapt other NLD recovery methods (Jijkoun and Rijke, 2004; Schmid, 2006) to Chinese and compare them with the current results." ></td>
	<td class="line x" title="187:285	Acknowledgements This research is funded by Science Foundation Ireland grant 04/IN/I527." ></td>
	<td class="line x" title="188:285	265 References Aoife Cahill, Michael Burke, Ruth ODonovan, Josef van Genabith and Andy Way." ></td>
	<td class="line x" title="189:285	2004." ></td>
	<td class="line x" title="190:285	Long-Distance Dependency Resolution in Automatically Acquired Wide-Coverage PCFG-Based LFG Approximations." ></td>
	<td class="line x" title="191:285	In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 320-327." ></td>
	<td class="line x" title="192:285	Barcelona, Spain." ></td>
	<td class="line x" title="193:285	Daniel M. Bikel." ></td>
	<td class="line x" title="194:285	2004." ></td>
	<td class="line x" title="195:285	On the Parameter Space of Generative Lexicalized Statistical Parsing Models." ></td>
	<td class="line x" title="196:285	Ph.D. thesis, Department of Computer & Information Science, University of Pennsylvania." ></td>
	<td class="line x" title="197:285	Philadelphia, PA. Derrick Higgins." ></td>
	<td class="line x" title="198:285	2003." ></td>
	<td class="line x" title="199:285	A machine-learning approach to the identification of WH gaps." ></td>
	<td class="line x" title="200:285	In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics, 99-102." ></td>
	<td class="line x" title="201:285	Budapest, Hungary." ></td>
	<td class="line x" title="202:285	Eugene Charniak." ></td>
	<td class="line x" title="203:285	2000." ></td>
	<td class="line x" title="204:285	A Maximum-Entropy-Inspired Parser." ></td>
	<td class="line x" title="205:285	In Proceedings of the 1st Annual Meeting of the North American Chapter of the Association for Computational Linguistics, pages 132-139." ></td>
	<td class="line x" title="206:285	Seattle, WA." ></td>
	<td class="line x" title="207:285	Helmut Schmid." ></td>
	<td class="line x" title="208:285	2006." ></td>
	<td class="line x" title="209:285	Trace Prediction and Recovery With Unlexicalized PCFGs and Slash Features." ></td>
	<td class="line x" title="210:285	In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 177-184." ></td>
	<td class="line x" title="211:285	Sydney, Australia." ></td>
	<td class="line x" title="212:285	Joakim Nivre and Jens Nilsson." ></td>
	<td class="line x" title="213:285	2005." ></td>
	<td class="line x" title="214:285	Pseudo-Projective Dependency Parsing." ></td>
	<td class="line x" title="215:285	In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 99-106." ></td>
	<td class="line x" title="216:285	Ann Arbor, Michigan." ></td>
	<td class="line x" title="217:285	Mark Johnson." ></td>
	<td class="line x" title="218:285	2002." ></td>
	<td class="line x" title="219:285	A Simple Pattern-Matching Algorithm for Recovering Empty Nodes and Their Antecedents." ></td>
	<td class="line x" title="220:285	In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 136-143." ></td>
	<td class="line x" title="221:285	Philadelphia, PA. Michael Burke, Olivia Lam, Rowena Chan, Aoife Cahill, Ruth ODonovan, Adams Bodomo, Josef van Genabith and Andy Way." ></td>
	<td class="line x" title="222:285	2004." ></td>
	<td class="line x" title="223:285	Treebank-Based Acquisition of a Chinese Lexical-Functional Grammar." ></td>
	<td class="line x" title="224:285	In Proceedings of the 18th Pacific Asia Conference on Language, Information and Computation, pages 161-172, Tokyo, Japan." ></td>
	<td class="line x" title="225:285	Michael Collins." ></td>
	<td class="line x" title="226:285	1999." ></td>
	<td class="line x" title="227:285	Head-Driven Statistical Models for Natural Language Parsing." ></td>
	<td class="line x" title="228:285	Ph.D. thesis, Department of Computer & Information Science, University of Pennsylvania." ></td>
	<td class="line x" title="229:285	Philadelphia, PA. Nianwen Xue, Fu-Dong Chiou, and Martha Palmer." ></td>
	<td class="line x" title="230:285	2002." ></td>
	<td class="line x" title="231:285	Building a Large-Scale Annotated Chinese Corpus." ></td>
	<td class="line x" title="232:285	In Proceedings of the 19th International Conference on Computational Linguistics, pages 1100-1106." ></td>
	<td class="line x" title="233:285	Taipei, Taiwan." ></td>
	<td class="line x" title="234:285	Peter Dienes and Amit Dubey." ></td>
	<td class="line x" title="235:285	2003a." ></td>
	<td class="line x" title="236:285	Deep syntactic processing by combining shallow methods." ></td>
	<td class="line x" title="237:285	In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 431-438." ></td>
	<td class="line x" title="238:285	Sapporo, Japan." ></td>
	<td class="line x" title="239:285	Peter Dienes and Amit Dubey." ></td>
	<td class="line x" title="240:285	2003b." ></td>
	<td class="line x" title="241:285	Antecedent Recovery: Experiments with a Trace Tagger." ></td>
	<td class="line x" title="242:285	In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 33-40." ></td>
	<td class="line x" title="243:285	Sapporo, Japan." ></td>
	<td class="line x" title="244:285	Richard Campbell." ></td>
	<td class="line x" title="245:285	2004." ></td>
	<td class="line x" title="246:285	Using Linguistic Principles to Recover Empty Categories." ></td>
	<td class="line x" title="247:285	In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 645-652." ></td>
	<td class="line x" title="248:285	Barcelona, Spain." ></td>
	<td class="line x" title="249:285	Roger Levy and Christopher D. Manning." ></td>
	<td class="line x" title="250:285	2004." ></td>
	<td class="line x" title="251:285	Deep Dependencies from Context-Free Statistical Parsers: Correcting the Surface Dependency Approximation." ></td>
	<td class="line x" title="252:285	In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 327-334." ></td>
	<td class="line x" title="253:285	Barcelona, Spain." ></td>
	<td class="line x" title="254:285	Ronald M. Kaplan and Joan Bresnan." ></td>
	<td class="line x" title="255:285	1982." ></td>
	<td class="line x" title="256:285	Lexical Functional Grammar: a Formal System for Grammatical Representation." ></td>
	<td class="line x" title="257:285	The Mental Representation of Grammatical Relations, pages 173-282." ></td>
	<td class="line x" title="258:285	MIT Press, Cambridge, MA." ></td>
	<td class="line x" title="259:285	Ryan Gabbard, Seth Kulick, and Mitch Marcus." ></td>
	<td class="line x" title="260:285	2006." ></td>
	<td class="line x" title="261:285	Fully Parsing the Penn Treebank In Proceedings of the Human Language Technology Conference / North American Chapter of the Association of Computational Linguistics, pages 184-191." ></td>
	<td class="line x" title="262:285	New York, USA." ></td>
	<td class="line x" title="263:285	Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard Crouch, John T. Maxwell III and Mark Johnson." ></td>
	<td class="line x" title="264:285	2002." ></td>
	<td class="line x" title="265:285	Parsing the Wall Street Journal using a Lexical-Functional Grammar and Discriminative Estimation Techniques." ></td>
	<td class="line x" title="266:285	In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 271-278." ></td>
	<td class="line x" title="267:285	Philadelphia, PA. Stephen Clark and Julia Hockenmaier." ></td>
	<td class="line x" title="268:285	2002." ></td>
	<td class="line x" title="269:285	Building Deep Dependency Structures with a Wide-CoverageCCG Parser." ></td>
	<td class="line x" title="270:285	In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 327-334." ></td>
	<td class="line x" title="271:285	Philadelphia, PA. Valentin Jijkoun." ></td>
	<td class="line x" title="272:285	2003." ></td>
	<td class="line x" title="273:285	Finding Non-Local Dependencies: Beyond Pattern Matching." ></td>
	<td class="line x" title="274:285	In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 37-43." ></td>
	<td class="line x" title="275:285	Sapporo, Japan." ></td>
	<td class="line x" title="276:285	Valentin Jijkoun and Maarten de Rijke." ></td>
	<td class="line x" title="277:285	2004." ></td>
	<td class="line x" title="278:285	Enriching the Output of a Parser Using Memory-Based Learning." ></td>
	<td class="line x" title="279:285	In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 311-318." ></td>
	<td class="line x" title="280:285	Barcelona, Spain." ></td>
	<td class="line x" title="281:285	Yusuke Miyao, Takashi Ninomiya, and Junichi Tsujii." ></td>
	<td class="line x" title="282:285	2003." ></td>
	<td class="line x" title="283:285	Probabilistic Modeling of Argument Structures Including Non-Local Dependencies." ></td>
	<td class="line x" title="284:285	In Proceedings of the 2003 Conference on Recent Advances in Natural Language Processing, pages 285-291." ></td>
	<td class="line x" title="285:285	Philadelphia, PA ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D07-1028
Exploiting Multi-Word Units in History-Based Probabilistic Generation
Hogan, Deirdre;Cafferkey, Conor;Cahill, Aoife;Van Genabith, Josef;"></td>
	<td class="line x" title="1:188	Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp." ></td>
	<td class="line x" title="2:188	267276, Prague, June 2007." ></td>
	<td class="line x" title="3:188	c2007 Association for Computational Linguistics Exploiting Multi-Word Units in History-Based Probabilistic Generation Deirdre Hogan, Conor Cafferkey, Aoife Cahill and Josef van Genabith National Centre for Language Technology School of Computing, Dublin City University Dublin 9, Ireland dhogan,ccafferkey,josef@computing.dcu.ie Abstract We present a simple history-based model for sentence generation from LFG f-structures, which improves on the accuracy of previous models by breaking down PCFG independence assumptions so that more f-structure conditioning context is used in the prediction of grammar rule expansions." ></td>
	<td class="line x" title="4:188	In addition, we present work on experiments with named entities and other multi-word units, showing a statistically significant improvement of generation accuracy." ></td>
	<td class="line x" title="5:188	Tested on section 23 of the Penn Wall Street Journal Treebank, the techniques described in this paper improve BLEU scores from 66.52 to 68.82, and coverage from 98.18% to 99.96%." ></td>
	<td class="line x" title="6:188	1 Introduction Sentence generation, or surface realisation, is the task of generating meaningful, grammatically correct and fluent text from some abstract semantic or syntactic representation of the sentence." ></td>
	<td class="line x" title="7:188	It is an important and growing field of natural language processing with applications in areas such as transferbased machine translation (Riezler and Maxwell, 2006) and sentence condensation (Riezler et al. , 2003)." ></td>
	<td class="line x" title="8:188	While recent work on generation in restricted domains, such as (Belz, 2007), has shown promising results there remains much room for improvement particularly for broad coverage and robust generators, like those of Nakanishi et al.(2005) and Cahill  Now at the Institut fur Maschinelle Sprachverarbeitung, Universitat Stuttgart, Azenbergstrae 12, D-70174 Stuttgart, Germany." ></td>
	<td class="line x" title="10:188	aoife.cahill@ims.uni-stuttgart.de and van Genabith (2006), which do not rely on handcrafted grammars and thus can easily be ported to new languages." ></td>
	<td class="line x" title="11:188	This paper is concerned with sentence generation from Lexical-Functional Grammar (LFG) fstructures (Kaplan, 1995)." ></td>
	<td class="line x" title="12:188	We present improvements in previous LFG-based generation models firstly by breaking down PCFG independence assumptions so that more f-structure conditioning context is included when predicting grammar rule expansions." ></td>
	<td class="line x" title="13:188	This history-based approach has worked well in parsing (Collins, 1999; Charniak, 2000) and we show that it also improves PCFG-based generation." ></td>
	<td class="line x" title="14:188	We also present work on utilising named entities and other multi-word units to improve generation results for both accuracy and coverage." ></td>
	<td class="line x" title="15:188	There has been a limited amount of exploration into the use of multi-word units in probabilistic parsing, for example in (Kaplan and King, 2003) (LFG parsing) and (Nivre and Nilsson, 2004) (dependency parsing)." ></td>
	<td class="line x" title="16:188	We are not aware of any similar work on generation." ></td>
	<td class="line x" title="17:188	In the LFG-based generation algorithm presented by Cahill and van Genabith (2006) complex named entities (i.e. those consisting of more than one word token) and other multi-word units can be fragmented in the surface realization." ></td>
	<td class="line x" title="18:188	We show that the identification of such units may be used as a simple measure to constrain the generation models output." ></td>
	<td class="line x" title="19:188	We take the generator of (Cahill and van Genabith, 2006) as our baseline generator." ></td>
	<td class="line x" title="20:188	When tested on f-structures for all sentences from Section 23 of the Penn Wall Street Journal (WSJ) treebank (Mar267 cus et al. , 1993), the techniques described in this paper improve BLEU score from 66.52 to 68.82." ></td>
	<td class="line x" title="21:188	In addition, coverage is increased from 98.18% to almost 100% (99.96%)." ></td>
	<td class="line x" title="22:188	The remainder of the paper is structured as follows: in Section 2 we review related work on statistical sentence generation." ></td>
	<td class="line x" title="23:188	Section 3 describes the baseline generation model and in Section 4 we show how the new history-based model improves over the baseline." ></td>
	<td class="line x" title="24:188	In Section 5 we describe the source of the multi-word units (MWU) used in our experiments and the various techniques we employ to make use of these MWUs in the generation process." ></td>
	<td class="line x" title="25:188	Section 6 gives experimental details and results." ></td>
	<td class="line x" title="26:188	2 Related Work on Statistical Generation In (statistical) generators, sentences are generated from an abstract linguistic encoding via the application of grammar rules." ></td>
	<td class="line x" title="27:188	These rules can be handcrafted grammar rules, such as those of (LangkildeGeary, 2002; Carroll and Oepen, 2005), created semi-automatically (Belz, 2007) or, alternatively, extracted fully automatically from treebanks (Bangalore and Rambow, 2000; Nakanishi et al. , 2005; Cahill and van Genabith, 2006)." ></td>
	<td class="line x" title="28:188	Insofar as it is a broad coverage generator, which has been trained and tested on sections of the WSJ corpus, our generator is closer to the generators of (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Nakanishi et al. , 2005) than to those designed for more restricted domains such as weather forecast (Belz, 2007) and air travel domains (Ratnaparkhi, 2000)." ></td>
	<td class="line x" title="29:188	Another feature which characterises statistical generators is the probability model used to select the most probable sentence from among the space of all possible sentences licensed by the grammar." ></td>
	<td class="line x" title="30:188	One generation technique is to first generate all possible sentences, storing them in a word lattice (Langkilde and Knight, 1998) or, alternatively, a generation forest, a packed represention of alternate trees proposed by the generator (Langkilde, 2000), and then select the most probable sequence of words via an n-gram language model." ></td>
	<td class="line x" title="31:188	Increasingly syntax-based information is being incorporated directly into the generation model." ></td>
	<td class="line x" title="32:188	For example, Carroll and Oepen (2005) describe a sentence realisation process which uses a hand-crafted HPSG grammar to generate a generation forest." ></td>
	<td class="line x" title="33:188	A selective unpacking algorithm allows the extraction of an n-best list of realisations where realisation ranking is based on a maximum entropy model." ></td>
	<td class="line x" title="34:188	This unpacking algorithm is used in (Velldal and Oepen, 2005) to rank realisations with features defined over HPSG derivation trees." ></td>
	<td class="line x" title="35:188	They achieved the best results when combining the tree-based model with an n-gram language model." ></td>
	<td class="line x" title="36:188	Nakanishi et al.(2005) describe a treebankextracted HPSG-based chart generator." ></td>
	<td class="line x" title="38:188	Importing techniques developed for HPSG parsing, they apply a log linear model to a packed representation of all alternative derivation trees for a given input." ></td>
	<td class="line x" title="39:188	They found that a model which included syntactic information outperformed a bigram model as well as a combination of bigram and syntax model." ></td>
	<td class="line x" title="40:188	The probability model described in this paper also incorporates syntactic information, however, unlike the discriminative HPSG models just described, it is a generative historyand PCFG-based model." ></td>
	<td class="line x" title="41:188	While Belz (2007) and Humphreys et al.(2001) mention the use of contextual features for the rules in their generation models, they do not provide details nor do they provide a formal probability model." ></td>
	<td class="line x" title="43:188	To the best of our knowledge this is the first paper providing a probabilistic generative, history-based generation model." ></td>
	<td class="line x" title="44:188	3 Surface Realisation from f-Structures Cahill and van Genabith (2006) present a probabilistic surface generation model for LFG (Kaplan, 1995)." ></td>
	<td class="line x" title="45:188	LFG is a constraint-based theory of grammar, which analyses strings in terms of c(onstituency)-structure and f(unctional)-structure (Figure 1)." ></td>
	<td class="line x" title="46:188	C-structure is defined in terms of CFGs, and f-structures are recursive attribute-value matrices which represent abstract syntactic functions (such as SUBJect, OBJect, OBLique, COMPlement (sentential), ADJ(N)unct), agreement, control, longdistance dependencies and some semantic information (e.g. tense, aspect)." ></td>
	<td class="line x" title="47:188	C-structures and f-structures are related in a projection architecture in terms of a piecewise correspondence .1 The correspondence is indicated in 1Our formalisation follows (Kaplan, 1995)." ></td>
	<td class="line x" title="48:188	268 S = NP VP ( SUBJ)=  = NNP V NP = = ( OBJ)=  Susan contacted PRP ( PRED) = Susan ( PRED) = contact = ( NUM) = SG ( TENSE) = past ( PERS) = 3 her ( PRED) = pro ( NUM) = SG ( PERS) = 3 f1:     PRED CONTACT(SUBJ)(OBJ) SUBJ f2: bracketleftbigg PRED SUSAN NUM SG PERS 3 bracketrightbigg OBJ f2: bracketleftbigg PRED PRO NUM SG PERS 3 bracketrightbigg TENSE PAST     Figure 1: Cand f-structures with  links for the sentence Susan contacted her." ></td>
	<td class="line x" title="49:188	terms of the curvy arrows pointing from c-structure nodes to f-structure components in Figure 1." ></td>
	<td class="line x" title="50:188	Given a c-structure node ni, the corresponding f-structure component fj is (ni)." ></td>
	<td class="line x" title="51:188	F-structures and the cstructure/f-structure correspondence are described in terms of functional annotations on c-structure nodes (CFG grammar rules)." ></td>
	<td class="line x" title="52:188	An equation of the form (F) =  states that the f-structure associated with the mother of the current c-structure node () has an attribute (grammatical function) (F), whose value is the f-structure of the current node ()." ></td>
	<td class="line x" title="53:188	The up-arrows and down-arrows are shorthand for (M(ni)) = (ni) where ni is the c-structure node annotated with the equation.2 Treebest := argmaxTreeP(Tree|F-Str) (1) P(Tree|F-Str) := productdisplay X  Y in Tree Feats = {ai|vj((X))ai = vj} P(X  Y |X, Feats) (2) The generation model of (Cahill and van Genabith, 2006) maximises the probability of a tree given an f-structure (Eqn." ></td>
	<td class="line x" title="54:188	1), and the string generated is the yield of the highest probability tree." ></td>
	<td class="line x" title="55:188	The generation process is guided by purely local information in the input f-structure: f-structure annotated CFG rules (LHS  RHS) are conditioned on their LHSs and on the set of features/attributes Feats = {ai|vj(LHS)ai = vj}3 -linked to the LHS (Eqn." ></td>
	<td class="line x" title="56:188	2M is the mother function on CFG tree nodes." ></td>
	<td class="line x" title="57:188	3In words, Feats is the set of top level features/attributes (those attributes ai for which there is a value vi) of the fstructure  linked to the LHS." ></td>
	<td class="line x" title="58:188	2)." ></td>
	<td class="line x" title="59:188	Table 1 shows a generation grammar rule and conditioning features extracted from the example in Figure 1." ></td>
	<td class="line x" title="60:188	The probability of a tree is decomposed into the product of the probabilities of the f-structure annotated rules (conditioned on the LHS and local Feats) contributing to the tree." ></td>
	<td class="line x" title="61:188	Conditional probabilities are estimated using maximum likelihood estimation." ></td>
	<td class="line x" title="62:188	grammar rule local conditioning features S(=) NP(SUBJ=) VP(=) S(=), {SUBJ,OBJ,PRED,TENSE} Table 1: Example grammar rule (from Figure 1)." ></td>
	<td class="line x" title="63:188	Cahill and van Genabith (2006) note that conditioning f-structure annotated generation rules on local features (Eqn." ></td>
	<td class="line x" title="64:188	2) can sometimes cause the model to make inappropriate choices." ></td>
	<td class="line x" title="65:188	Consider the following scenario where in addition to the c-/f-structure in Figure 1, the training set contains the c-/f-structure displayed in Figure 2." ></td>
	<td class="line x" title="66:188	From Figures 1 and 2, the model learns (among others) the generation rules and conditional probabilities displayed in Tables 2 and 3." ></td>
	<td class="line x" title="67:188	F-Struct Feats Grammar Rules Prob {SUBJ, OBJ, PRED} S(=)  NP(SUBJ=) VP(=) 1 {SUBJ, OBJ, PRED} VP(=)  V(=) NP(OBJ=) 1 {NUM, PER, GEN} NP(SUBJ=)  NNP(=) 0.5 {NUM, PER, GEN} NP(SUBJ=)  PRP(=) 0.5 {NUM, PER, GEN} NP(OBJ=)  PRP(=) 1 Table 2: A sample of internal grammar rules extracted from Figures 1 and 2." ></td>
	<td class="line x" title="68:188	Given the input f-structure (for She accepted) in Figure 3, (and assuming suitable generation rules for intransitive VPs and accepted) the model would produce the inappropriate highest probability tree of Figure 4 with an incorrect case for the pronoun in subject position." ></td>
	<td class="line x" title="69:188	269 S = NP VP ( SUBJ)=  = PRP V NP = = ( OBJ)=  She hired PRP ( PRED) = pro ( PRED) = hire = ( NUM) = SG ( TENSE) = past ( PERS) = 3 her ( PRED) = pro ( NUM) = SG ( PERS) = 3 f1:     PRED HIRE(SUBJ)(OBJ) SUBJ f2: bracketleftbigg PRED PRO NUM SG PERS 3 bracketrightbigg OBJ f2: bracketleftbigg PRED PRO NUM SG PERS 3 bracketrightbigg TENSE PAST     Figure 2: Cand f-structures with  links for the sentence She hired her." ></td>
	<td class="line x" title="70:188	F-Struct Feats Grammar Rules Prob {PRED=PRO,NUM=SG PER=3, GEN=FEM} PRP(=)  she 0.33 {PRED=PRO,NUM=SG PER=3, GEN=FEM} PRP(=)  her 0.66 Table 3: A sample of lexical item rules extracted from Figures 1 and 2." ></td>
	<td class="line x" title="71:188	    SUBJ   PRED pro NUM sg PERS 3 GEND fem   PRED accept TENSE past     Figure 3: Input f-structure for She accepted." ></td>
	<td class="line x" title="72:188	To solve the problem, Cahill and van Genabith (2006) apply an automatic generation grammar transformation to their training data: they automatically label CFG nodes with additional case information and the model now learns the new improved generation rules of Tables 4 and 5." ></td>
	<td class="line x" title="73:188	Note how the additional case labelling subverts the problematic independence assumptions of the probability model and communicates the fact that a subject NP has to be realised as nominative case from the S  NP-nom VP production, via the intermediate NP-nom  PRP-nom, down to the lexical production PRP-nom  she." ></td>
	<td class="line x" title="74:188	The labelling guarantees that, given the example f-structure in Figure 3, the model generates the correct string she accepted." ></td>
	<td class="line x" title="75:188	F-Struct Feats Grammar Rules {SUBJ, OBJ, PRED} S(=)  NP-nom(SUBJ=) VP(=) {SUBJ, OBJ, PRED} VP(=)  V(=) NP-acc(OBJ=) {NUM, PER, GEN} NP-nom(SUBJ=)  PRP-nom(=) {NUM, PER, GEN} NP-nom(SUBJ=)  NNP-nom(=) {NUM, PER, GEN} NP-acc(OBJ=)  PRP-acc(=) Table 4: Internal grammar rules with case markings." ></td>
	<td class="line x" title="76:188	S = NP VP ( SUBJ)=  = PRP V = = her accepted ( PRED) = pro ( PRED) = hire ( NUM) = SG ( TENSE) = past ( PERS) = 3 Figure 4: Inappropriate output: her accepted." ></td>
	<td class="line x" title="77:188	F-Struct Feats Grammar Rules {PRED=PRO,NUM=SG PER=3, GEN=FEM} PRP-nom(=)  she {PRED=PRO,NUM=SG PER=3, GEN=FEM} PRP-acc(=)  her Table 5: Lexical item rules with case markings 4 A History-Based Generation Model The automatic generation grammar transform presented in (Cahill and van Genabith, 2006) provides a solution to coarse-grained and (in fact) inappropriate independence assumptions in the basic generation model." ></td>
	<td class="line x" title="78:188	However, there is a sense in which the proposed cure improves on the symptoms, but not the cause of the problem: it weakens independence assumptions by multiplying and hence increasing the specificity of conditioning CFG category labels." ></td>
	<td class="line x" title="79:188	There is another option available to us, and that is the option we will explore in this paper: instead of applying a generation grammar transform, we will improve the f-structure-based conditioning of the generation rule probabilities." ></td>
	<td class="line x" title="80:188	In the original model, rules are conditioned on purely local f-structure context: the set of features/attributes -linked to the LHS of a grammar rule." ></td>
	<td class="line x" title="81:188	As a direct consequence of this, the conditioning (and hence the model) cannot not distinguish between NP, PRP and NNP rules 270 appropriate to e.g. subject (SUBJ) or object contexts (OBJ) in a given input f-structure." ></td>
	<td class="line x" title="82:188	However, the required information can easily be incorporated into the generation model by uniformly conditioning generation rules on their parent (mother) grammatical function, in addition to the local -linked feature set." ></td>
	<td class="line x" title="83:188	This additional conditioning has the effect of making the choice of generation rules sensitive to the history of the generation process, and, we argue, provides a simpler, more uniform, general, intuitive and natural probabilistic generation model obviating the need for CFG-grammar transforms in the original proposal of (Cahill and van Genabith, 2006)." ></td>
	<td class="line x" title="84:188	In the new model, each generation rule is now conditioned on the LHS rule CFG category, the set of features -linked to LHS and the parent grammatical function of the f-structure -linked to LHS." ></td>
	<td class="line x" title="85:188	In a given c-/f-structure pair, for a CFG node n, the parent grammatical function of the f-structure -linked to n is that grammatical function GF, which, if we take the f-structure -linked to the mother M(n), and apply it to GF, returns the f-structure -linked to n: ((M(n))GF) = (n)." ></td>
	<td class="line x" title="86:188	The basic idea is best explained by way of an example." ></td>
	<td class="line x" title="87:188	Consider again Figure 1." ></td>
	<td class="line x" title="88:188	The mother grammatical function of the f-structure f2 associated with node NP(SUBJ=) and its daughter NNP(=) (via the = functional annotation) is SUBJ, as ((M(n2))SUBJ) = (n2), or equivalently (f1SUBJ) = f2." ></td>
	<td class="line x" title="89:188	Given Figures 1 and 2 as training set, the improved model learns the generation rules (the mother grammatical function of the outermost f-structure is assumed to be a dummy TOP grammatical function) of Tables 6 and 7." ></td>
	<td class="line x" title="90:188	F-Struct Feats Grammar Rules {SUBJ, OBJ, PRED, TOP} S(=)  NP(SUBJ=) VP(=) {SUBJ, OBJ, PRED, TOP} VP(=)  V(=) NP(OBJ=) {NUM, PER, GEN, SUBJ} NP(SUBJ=)  PRP(=) {NUM, PER, GEN, OBJ} NP(OBJ=)  PRP(=) {NUM, PER, GEN, SUBJ} NP(SUBJ=)  NNP(=) Table 6: Grammar rules with extra feature extracted from F-Structures." ></td>
	<td class="line x" title="91:188	Note, that for our example the effect of the uniform additional conditioning on mother grammatical function has the same effect as the generation grammar transform of (Cahill and van Genabith, 2006), but without the need for the gramF-Struct Feats Grammar Rules {PRED=PRO,NUM=SG PER=3, GEN=FEM, SUBJ} PRP(=)  she {PRED=PRO,NUM=SG PER=3, GEN=FEM, OBJ} PRP(=)  her Table 7: Lexical item rules." ></td>
	<td class="line x" title="92:188	mar transform." ></td>
	<td class="line x" title="93:188	Given the input f-structure in Figure 3, the model will generate the correct string she accepted." ></td>
	<td class="line x" title="94:188	In addition, uniform conditioning on mother grammatical function is more general than the case-phenomena specific generation grammar transform of (Cahill and van Genabith, 2006), in that it applies to each and every sub-part of a recursive input f-structure driving generation, making available relevant generation history (context) to guide local generation decisions." ></td>
	<td class="line x" title="95:188	The new history-based probabilistic generation model is defined as: P(Tree|F-Str) := productdisplay X  Y in Tree Feats = {ai|vj((X))ai = vj} ((M(X)))GF = (X) P(X  Y |X, Feats,GF) (3) Note that the new conditioning feature, the fstructure mother grammatical function, GF, is available from structure previously generated in the cstructure tree." ></td>
	<td class="line x" title="96:188	As such, it is part of the history of the tree, i.e. it has already been generated in the topdown derivation of the tree." ></td>
	<td class="line x" title="97:188	In this way, the generation model resembles history-based models for parsing (Black et al. , 1992; Collins, 1999; Charniak, 2000)." ></td>
	<td class="line x" title="98:188	Unlike, say, the parent annotation for parsing of (Johnson, 1998) the parent GF feature for a particular node expansion is not merely extracted from the parent node in the c-structure tree, but is sometimes extracted from an ancestor node further up the c-structure tree via intervening = functional annotations." ></td>
	<td class="line x" title="99:188	Section 6 provides evaluation results for the new model on section 23 of the Penn treebank." ></td>
	<td class="line x" title="100:188	5 Multi-Word Units In another effort to improve generator accuracy over the baseline model we explored the use of multiword units in generation." ></td>
	<td class="line x" title="101:188	We expect that the identification of MWUs may be useful in imposing wordorder constraints and reducing the complexity of the generation task." ></td>
	<td class="line x" title="102:188	Take, for example, the following 271      APP     ADJUNCT bracketleftBigg PRED New NUM sg PERS 3 bracketrightBigg PRED York NUM sg PERS 3          bracketleftBigg APP bracketleftBigg PRED New York NUM sg PERS 3 bracketrightBiggbracketrightBigg      APP     ADJUNCT bracketleftBigg PRED New/NE1 1 NUM sg PERS 3 bracketrightBigg PRED York/NE1 2 NUM sg PERS 3          Figure 5: Three different f-structure formats." ></td>
	<td class="line x" title="103:188	From left to right: the original f-structure format; the MWU chunk format; the MWU mark-up format." ></td>
	<td class="line x" title="104:188	two sentences which show the gold version of a sentence followed by the version of the sentence produced by the generator: Gold By this time, it was 4:30 a.m. in New York, and Mr. Smith fielded a call from a New York customer wanting an opinion on the British stock market, which had been having troubles of its own even before Friday s New York market break." ></td>
	<td class="line x" title="105:188	Test By this time, in New York, it was 4:30 aa.m., and Mr. Smith fielded a call from New a customer York, wanting an opinion on the market British stock which had been having troubles of its own even before Friday s New York market break . The gold version of the sentence contains a multiword unit, New York, which appears fragmented in the generator output." ></td>
	<td class="line x" title="106:188	If multi-word units were either treated as one token throughout the generation process, or, alternatively, if a constraint were imposed on the generator such that multi-word units were always generated in the correct order, then this should help improve generation accuracy." ></td>
	<td class="line x" title="107:188	In Section 5.1 we describe the various techniques that were used to incorporate multi-word units into the generation process and in 5.2 we detail the different types and sources of multi-word unit used in the experiments." ></td>
	<td class="line x" title="108:188	Section 6 provides evaluation results on test and development sets from the WSJ treebank." ></td>
	<td class="line x" title="109:188	5.1 Incorporating MWUs into the Generation Process We carried out three types of experiment which, in different ways, enabled the generation process to respect the restrictions on word-order provided by multi-word units." ></td>
	<td class="line x" title="110:188	For the first experiments (type 1), the WSJ treebank training and test data were altered so that multi-word units are concatenated into single words (for example, New York becomes New York)." ></td>
	<td class="line x" title="111:188	As in (Cahill and van Genabith, 2006) fstructures are generated from the (now altered) treebank and from this data, along with the treebank trees, the PCFG-based grammar, which is used for training the generation model, is extracted." ></td>
	<td class="line x" title="112:188	Similarly, the f-structures for the test and development sets are created from Penn Treebank trees which have been modified so that multi-word units form single units." ></td>
	<td class="line x" title="113:188	The leftmost and middle f-structures in Figure 5 show an example of an original f-structure format and a named-entity chunked format, respectively." ></td>
	<td class="line x" title="114:188	Strings output by the generator are then postprocessed so that the concatenated word sequences are converted back into single words." ></td>
	<td class="line x" title="115:188	In the second experiment (type 2) only the test data was altered with no concatenation of MWUs carried out on the training data." ></td>
	<td class="line x" title="116:188	In the final experiments (type 3), instead of concatenating named entities, a constraint is introduced to the generation algorithm which penalises the generation of sequences of words which violate the internal word order of named entities." ></td>
	<td class="line x" title="117:188	The input is marked-up in such a way that, although named entities are no longer chunked together to form single words, the algorithm can read which items are part of named entities." ></td>
	<td class="line x" title="118:188	See the rightmost f-structure in Figure 5 for an example of an f-structure markedup in this way." ></td>
	<td class="line x" title="119:188	The tag NE1 1, for example, indicates that the sub-f-structure is part of a named identity with id number 1 and that the item corresponds to the first word of the named entity." ></td>
	<td class="line x" title="120:188	The baseline generation algorithm, following Kay (1996)s work on chart generation, already contains the hard constraint that when combining two chart edges they must cover disjoint sets of words." ></td>
	<td class="line x" title="121:188	We added an additional constraint which prevents edges from being combined if this would result in the generation of a string which contained a named entity which was 272 either incomplete or where the words in the named entity were generated in the wrong order." ></td>
	<td class="line x" title="122:188	5.2 Types of MWUs used in Experiments We carry out experiments with multi-word units from three different sources." ></td>
	<td class="line x" title="123:188	First, we use the output of the maximum entropy-based named entity recognition system of (Chieu and Ng, 2003)." ></td>
	<td class="line x" title="124:188	This system identifies four types of named entity: person, organisation, location, and miscellaneous." ></td>
	<td class="line x" title="125:188	Additionally we use a dictionary of candidate multi-word expressions based on a list from the Stanford Multiword Expression Project4." ></td>
	<td class="line x" title="126:188	Finally, we also carry out experiments with multi-word units extracted from the BBN Pronoun Coreference and Entity Type Corpus (Weischedel and Brunstein, 2005)." ></td>
	<td class="line x" title="127:188	This supplements the Penn WSJ treebanks one million words of syntax-annotated Wall Street Journal text with additional annotations of 23 named entity types, including nominal-type named entities such as person, organisation, location, etc. as well as numeric types such as date, time, quantity and money." ></td>
	<td class="line x" title="128:188	Since the BBN corpus data is very comprehensive and is handannotated we take this be be a gold standard, representing an upper bound for any gains that might be made by identifying complex named entities in our experiments.5 Table 8 gives examples of the various types of MWUs identified by the three sources." ></td>
	<td class="line x" title="129:188	For our purposes we are not concerned with the distinctions between different types of named entities; we are merely exploiting the fact that they may be treated as atomic units in the generation model." ></td>
	<td class="line x" title="130:188	In all cases we disregard multi-word units that cross the original syntactic bracketing of the WSJ treebank." ></td>
	<td class="line x" title="131:188	An overview of the various types of multi-word units used in our experiments is presented in Table 9." ></td>
	<td class="line x" title="132:188	6 Experimental Evaluation All experiments were carried out on the WSJ treebank with sections 02-21 for training, section 24 for development and section 23 for final test results." ></td>
	<td class="line oc" title="133:188	The LFG annotation algorithm of (Cahill et al. , 2004) was used to produce the f-structures for development, test and training sets." ></td>
	<td class="line x" title="134:188	4mwe.stanford.edu 5Although it is possible there are other types of MWUs that may be more suitable to the task than the named entities identified by BBN, so further gains might be possible." ></td>
	<td class="line x" title="135:188	MWU type Examples Names Martha Matthews Yoshio Hatakeyama Organisations Rolls-Royce Motor Cars Inc. Washington State University Locations New York City New Zealand Time expressions October 19th two years ago the 21st century Quantities $2.7 million to $3 million about 25 % 60 mph Prepositional expressions in fact at the time on average Table 8: Examples of some of the types of MWU from the three different sources." ></td>
	<td class="line x" title="136:188	average number average length (Chieu and Ng, 2003) 0.61 2.40 Stanford MWE Project 0.10 2.48 BBN Corpus 1.15 2.66 Table 9: Average number of MWUs per sentence and average MWU length in the WSJ treebank grouped by MWU source." ></td>
	<td class="line x" title="137:188	Table 10 shows the final results for section 23." ></td>
	<td class="line x" title="138:188	For each test we present BLEU score results as well as String Edit Distance and coverage." ></td>
	<td class="line x" title="139:188	We measure statistical significance using two different tests." ></td>
	<td class="line x" title="140:188	First we use a bootstrap resampling method, popular for machine translation evaluations, to measure the significance of improvements in BLEU scores, with a resampling rate of 1000.6 We also calculated the significance of an increase in String Edit Distance by carrying out a paired t-test on the mean difference of the String Edit Distance scores." ></td>
	<td class="line x" title="141:188	In Table 10, greatermuch means significant at level 0.005." ></td>
	<td class="line x" title="142:188	> means significant at level 0.05." ></td>
	<td class="line x" title="143:188	In Table 10, Baseline gives the results of the generation algorithm of (Cahill and van Genabith, 2006)." ></td>
	<td class="line x" title="144:188	HB Model refers to the improved model with the increased history context, as described in Section 4." ></td>
	<td class="line x" title="145:188	The results, where for example the BLEU score rises from 66.52 to 67.24, show that even increasing the conditioning context by a limited 6Scripts for running the bootstrapping method carried out in our evaluation are available for download at projectile.is.cs.cmu.edu/research/public/tools/bootStrap/tutorial.htm 273 Section 23 (2416 sentences) Model BLEU StringEd Coverage BLEU Bootstrap Signif StringEd Paired T-Test 1." ></td>
	<td class="line x" title="146:188	Baseline 66.52 68.69 98.18 2." ></td>
	<td class="line x" title="147:188	HB Model 67.24 69.89 99.88 greatermuch 1 greatermuch 1 3." ></td>
	<td class="line x" title="148:188	+MWU Best Automatic 67.81 70.36 99.92 greatermuch 2 greatermuch 2 4." ></td>
	<td class="line x" title="149:188	MWU BBN 68.82 70.92 99.96 greatermuch 3 > 3 Table 10: Results on Section 23 for all sentence lengths." ></td>
	<td class="line x" title="150:188	amount increases the accuracy of the system significantly for both BLEU and String Edit Distance." ></td>
	<td class="line x" title="151:188	In addition, coverage goes up from 98.18% to 99.88%." ></td>
	<td class="line x" title="152:188	+MWU Best Automatic displays our best results using automatically identified named entities." ></td>
	<td class="line x" title="153:188	These were achieved using experiment type 2, described in Section 5, with the MWUs produced by (Chieu and Ng, 2003)." ></td>
	<td class="line x" title="154:188	Results displayed in Table 10 up to this point are cumulative." ></td>
	<td class="line x" title="155:188	The final row in Table 10, MWU BBN, shows the best results with BBN MWUs: the history-based model with BBN multiword units incorporated in a type 1 experiment." ></td>
	<td class="line x" title="156:188	We now discuss the various MWU experiments in more detail." ></td>
	<td class="line x" title="157:188	See Table 11 for a breakdown of the MWU experiment results on the development set, WSJ section 24." ></td>
	<td class="line x" title="158:188	Our baseline for these experiments is the history-based generator presented in Section 4." ></td>
	<td class="line x" title="159:188	For each experiment type described in Section 5.1 we ran three experiments, varying the source of MWUs." ></td>
	<td class="line x" title="160:188	First, MWUs came from the automatic NE recogniser of (Chieu and Ng, 2003), then we added the MWUs from the Stanford list and finally we ran tests with MWUs extracted from the BBN corpus." ></td>
	<td class="line x" title="161:188	Our first set of experiments (type 1), where both training data and development set data were MWUchunked, produced the worst results for the automatically chunked MWUs." ></td>
	<td class="line x" title="162:188	BLEU score accuracy actually decreased for the automatically chunked MWU experiments." ></td>
	<td class="line x" title="163:188	In an error analysis of type 1 experiments with (Chieu and Ng, 2003) concatenated MWUs, we inspected those sentences where accuracy had decreased from the baseline." ></td>
	<td class="line x" title="164:188	We found that for over half (51.5%) of these sentences, the input f-structures contained no multi-word units at all." ></td>
	<td class="line x" title="165:188	The problem for these sentences therefore lay with the probabilistic grammar extracted from the MWUchunked training data." ></td>
	<td class="line x" title="166:188	When the source of MWU for the type 1 experiments was the BBN, however, accuracy improved significantly over the baseline and the result is the highest accuracy achieved over all experiment types." ></td>
	<td class="line x" title="167:188	One possible reason for the low accuracy scores in the type 1 experiments with the (Chieu and Ng, 2003) MWU chunked data could be noisy MWUs which negatively affect the grammar. For example, the named entity recogniser of (Chieu and Ng, 2003) achieves an accuracy of 88.3% on section 23 of the Penn Treebank." ></td>
	<td class="line x" title="168:188	In order to avoid changing the grammar through concatenation of MWU components (as in experiment type 1) and thus risking side-effects which cause some heretofore likely constructions become less likely and vice versa, we ran the next set of experiments (type 2) which leave the original grammar intact and alter the input f-structures only." ></td>
	<td class="line x" title="169:188	These experiments were more successful overall and we achieved an improvement over the baseline for both BLEU and String Edit Distance scores with all MWU types." ></td>
	<td class="line x" title="170:188	As can be seen from Table 11 the best score for automatically chunked MWUs are with the (Chieu and Ng, 2003) MWUs." ></td>
	<td class="line x" title="171:188	Accuracy decreases marginally when we added the Stanford MWUs." ></td>
	<td class="line x" title="172:188	In our final set of experiments (type 3) although the accuracy for all three types of MWUs improves over the baseline, accuracy is a little below the type 2 experiments." ></td>
	<td class="line x" title="173:188	It is difficult to compare sentence generators since the information contained in the input varies greatly between systems, systems are evaluated on different test sets and coverage also varies considerably." ></td>
	<td class="line x" title="174:188	In order to compare our system with those of (Nakanishi et al. , 2005) and (Langkilde-Geary, 2002) we report our best results with automatically acquired MWUs for sentences of  20 words in length on section 23: our system gets coverage of 100% and a BLEU score of 71.39." ></td>
	<td class="line x" title="175:188	For the same test set Nakanishi et al.(2005) achieved coverage of 90.75 and a BLEU score of 77.33." ></td>
	<td class="line x" title="177:188	Langkilde-Geary (2002) re274 Section 24 (1346 sentences) Model MWUs BLEU StringEd Coverage HB Model 65.85 69.93 99.93 type 1 (Chieu and Ng, 2003) 65.81 70.34 99.93 (training and test data chunked) +Stanford MWEs 64.81 69.67 99.93 BBN 67.24 71.46 99.93 type 2 (Chieu and Ng, 2003) 66.37 70.26 99.93 (test data chunked) +Stanford MWEs 66.28 70.21 99.93 BBN 66.84 70.74 99.93 type 3 (Chieu and Ng, 2003) 66.30 70.12 100 (internal generation constraint) +Stanford MWEs 66.07 70.02 99.93 BBN 66.45 70.14 99.93 Table 11: Results on Section 24, all sentence lengths." ></td>
	<td class="line x" title="178:188	ports 82.7% coverage and a BLEU score of 75.7% on the same test set with the permute,no dir type input." ></td>
	<td class="line x" title="179:188	Langkilde-Geary (2002) report results for experiments with varying levels of linguistic detail in the input given to the generator." ></td>
	<td class="line x" title="180:188	As with Nakanishi et al.(2005) we find the permute,no dir type of input is most comparable to the level of information contained in our input f-structures." ></td>
	<td class="line x" title="182:188	Finally, the symbolic generator of Callaway (2003) reports a Simple String Accuracy score of 88.84 and coverage of 98.7% on section 23 for all sentence lengths." ></td>
	<td class="line x" title="183:188	7 Conclusion and Future Work We have presented techniques which improve the accuracy of an already state-of-art surface generation model." ></td>
	<td class="line x" title="184:188	We found that a history-based model that increases conditioning context in PCFG style rules by simply including the grammatical function of the f-structure parent, improves generator accuracy." ></td>
	<td class="line x" title="185:188	In the future we will experiment with increasing conditioning context further and using more sophisticated smoothing techniques to avoid sparse data problems when conditioning is increased." ></td>
	<td class="line x" title="186:188	We have also demonstrated that automatically acquired multi-word units can bring about moderate, but significant, improvements in generator accuracy." ></td>
	<td class="line x" title="187:188	For automatically acquired MWUs, we found that this could best be achieved by concatenating input items when generating the f-structure input to the generator, while training the input generation grammar on the original (i.e. non-MWU concatenated) sections of the treebank." ></td>
	<td class="line x" title="188:188	Relying on the BBN corpus as a source of multi-word units, we gave an upper bound to the potential usefulness of multi-word units in generation and showed that automatically acquired multi-word units, encouragingly, give results not far below the upper bound." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N07-2031
RH: A Retro-Hybrid Parser
Newman, P. S.;"></td>
	<td class="line x" title="1:104	Proceedings of NAACL HLT 2007, Companion Volume, pages 121124, Rochester, NY, April 2007." ></td>
	<td class="line x" title="2:104	c2007 Association for Computational Linguistics RH: A Retro Hybrid Parser Paula S. Newman newmanp@acm.org Abstract Contemporary parser research is, to a large extent, focused on statistical parsers and deep-unification-based parsers." ></td>
	<td class="line x" title="3:104	This paper describes an alternative, hybrid architecture in which an ATN-like parser, augmented by many preference tests, builds on the results of a fast chunker." ></td>
	<td class="line x" title="4:104	The combination is as efficient as most stochastic parsers, and accuracy is close and continues to improve." ></td>
	<td class="line x" title="5:104	These results raise questions about the practicality of deep unification for symbolic parsing." ></td>
	<td class="line x" title="6:104	1 Introduction The original goals of the RH parser were to obtain accurate parses where (a) application speed was needed, and (b) large amounts of annotated material for a subject idiom were not available." ></td>
	<td class="line x" title="7:104	Additional goals that evolved were (c) that parses for particular documents could be brought to an almost arbitrary level of correctness for research purposes, by grammar correction, and (d) that information collected during parsing could be modified for an application with a modest amount of effort." ></td>
	<td class="line x" title="8:104	Goal (a) ruled out the use of unification-based symbolic parsers, because deep unification is a relatively slow operation, no matter what amount of computational sophistication is employed." ></td>
	<td class="line x" title="9:104	Until very recently, goal (b) ruled out stochastic parsers, but new results (McClosky et al. 2006) suggest this may no longer be the case." ></td>
	<td class="line x" title="10:104	However, the 'additional' goals still favor symbolic parsing." ></td>
	<td class="line x" title="11:104	To meet these goals, the RH parser combines a very efficient shallow parser with an overlay parser that is 'retro', in that the grammar is related to Augmented Transition Networks (Woods, 1970), operating on the shallow-parser output." ></td>
	<td class="line x" title="12:104	A major 'augmentation' is a preference-scoring component." ></td>
	<td class="line x" title="13:104	Section 2 below reviews the shallow parser used, and Section 3 describes the overlay parser." ></td>
	<td class="line x" title="14:104	Some current results are presented in section 4." ></td>
	<td class="line x" title="15:104	Section 5 examines some closely-related work, and Section 6 discusses some implications." ></td>
	<td class="line x" title="16:104	2 The XIP Parser for English XIP is a robust parser developed by Xerox Research Center Europe." ></td>
	<td class="line x" title="17:104	It is actually a full parser that produces a tree of chunks, plus identification of (sometimes alternative) typed dependencies among the chunk heads (Ait-Mokhtar et al. 2002, Gala 2004)." ></td>
	<td class="line x" title="18:104	But because the XIP dependency analyzer for English was incomplete when RH work began, and because classic parse trees are more convenient for discourse-related applications, we focused on the chunk output." ></td>
	<td class="line x" title="19:104	XIP is astonishingly fast, contributing very little to RH parse time." ></td>
	<td class="line x" title="20:104	It consists of the XIP engine, plus language-specific grammars, each consisting of: (a) a finite state lexicon producing alternative tags and morphological analyses for each token, together with subcategorization, control and (some) semantic class features, (b) a part of speech tagger, and (c) conveniently expressed, layered rule sets that perform the following functions: Lexicon extension, which adds words and adds or overrides feature information, Lexical disambiguation (including use of the tagger to provide default assignments) Multi-word identification for named entities, dates, short constructions, etc. Chunking, obtaining basic chunks such as basic adjective, adverbial, noun and prepositional phrases." ></td>
	<td class="line x" title="21:104	Dependency Analysis (not used in RH) All rule sets have been extended within RH development except for the dependency rule sets 3 Overlay Parser The overlay parser builds on chunker output to produce a single tree (figure 1) providing syntactic categories and functions, heads, and head features." ></td>
	<td class="line x" title="22:104	The output tree requires further processing to obtain long distance dependency information, and make some unambiguous coordination adjustments 121 Figure 1." ></td>
	<td class="line x" title="23:104	Output Parse Tree." ></td>
	<td class="line x" title="24:104	* indicates head." ></td>
	<td class="line x" title="25:104	Mouseover shows head features Some of this has already been done in a post-parse phase." ></td>
	<td class="line oc" title="26:104	The feasibility of such post-parse deepening (for a statistical parser) is demonstrated by Cahill et al (2004)." ></td>
	<td class="line x" title="27:104	The major parser components are a control, the ATN-like grammar networks, and collections of tests." ></td>
	<td class="line x" title="28:104	The control is invoked recursively to build non-chunk constituents by following grammar network paths and creating output networks." ></td>
	<td class="line x" title="29:104	Figure 2 shows the arcs of an excerpt from a grammar network used to build a noun phrase." ></td>
	<td class="line x" title="30:104	The Test labels on the arcs resemble specialized categories." ></td>
	<td class="line x" title="31:104	The MetaOps (limited in the illustration to Prolog-like cuts) expedite processing by permitting or barring exploration of further ordered arcs originating at the same state." ></td>
	<td class="line x" title="32:104	An output network, illustrated in figure 3, mirrors the full paths traversed in a grammar netFrom To Test Syn fun Fin al? Meta Op S1 S1 PREADV PRE No cut S1 S2 PRON HEAD Yes cut S1 S3 PROPER HEAD Yes cut S1 S4 S7 BASENP HEAD Yes cut //After pronoun S2 REFL REFL Yes cut S2 PEOPLE APPS Yes cut Figure 2." ></td>
	<td class="line x" title="33:104	Some arcs of grammar network for GNP From To Cat Synfun Ref OSa OSb NP HEAD NPChunk (The park) OSb OSc PP NMOD Final state of PP net for (in Paris) States Score Final?" ></td>
	<td class="line x" title="34:104	Osa 0 No Osb 0 Yes OSc 1 Yes Figure 3." ></td>
	<td class="line x" title="35:104	Output network for 'The park in Paris' work by one invocation of the control." ></td>
	<td class="line x" title="36:104	The arcs refer either to chunks or to final states of other output networks." ></td>
	<td class="line x" title="37:104	Output networks do not contain cycles or converging arcs, so states represent unique paths." ></td>
	<td class="line x" title="38:104	They carry head and other path information, and a preference score." ></td>
	<td class="line x" title="39:104	The final parser output is a single tree, derived from a highest scoring path of a topmost output network." ></td>
	<td class="line x" title="40:104	Ties are broken by low attach considerations." ></td>
	<td class="line x" title="41:104	Each invocation of the control is given a grammar network entry state and a desired constituent category." ></td>
	<td class="line x" title="42:104	After initializing a new output network, the arcs from the given entry state are followed." ></td>
	<td class="line x" title="43:104	Processing an arc may begin with an optional pretest." ></td>
	<td class="line x" title="44:104	If that succeeds, or there is no pretest, a constructive test follows." ></td>
	<td class="line x" title="45:104	The tests are indexed by grammar network test labels, and are expressed as blocks of procedural code, for initial flexibility in determining the necessary checks." ></td>
	<td class="line x" title="46:104	Pretests include fast feasibility checks, and contexted checks of consistency of the potential new constituent with the current output network path." ></td>
	<td class="line x" title="47:104	Constructive tests can make additional feasibility checks." ></td>
	<td class="line x" title="48:104	If these checks succeed, either a chunk is returned, or the control is reentered to try to build a subordinate output network." ></td>
	<td class="line x" title="49:104	Results are cached, to avoid repeated testing." ></td>
	<td class="line x" title="50:104	After a chunk or subordinate network ON' is returned from a constructive test, one new arc Ai is added to the current output network ON to represent each full path through ON'." ></td>
	<td class="line x" title="51:104	All added arcs have the same origin state in ON, but unique successor states and associated preference scores." ></td>
	<td class="line x" title="52:104	The preference score is the sum of the score at the common origin state, plus the score of the represented path in ON', plus a contexted score for the alternative within ON." ></td>
	<td class="line x" title="53:104	The latter is one of <-1, 0, +1>, and expresses the consistency of Ai with the current path with respect to dependency, coordination and apposition." ></td>
	<td class="line x" title="54:104	Structural and punctuation 122 aspects are also considered." ></td>
	<td class="line x" title="55:104	Preference tests are indexed by syntactic category or syntactic function, and are organized for speed." ></td>
	<td class="line x" title="56:104	Most tests are independent of Ai length, and can be applied once and the results assumed for all Ai." ></td>
	<td class="line x" title="57:104	Before a completed output network is returned, paths ending at those lower scoring final states which cannot ultimately be optimal are pruned." ></td>
	<td class="line x" title="58:104	Such pruning is critical to efficiency." ></td>
	<td class="line x" title="59:104	4 Indicative Current Results To provide a snapshot of current RH parser performance, we compare its current speed and accuracy directly to those of a widely used statistical parser, Collins model 3 (Collins, 1999), and indirectly to two other parsers." ></td>
	<td class="line x" title="60:104	Wall Street Journal section 23 of the Penn Treebank (Marcus et al. 1994) was used in all experiments." ></td>
	<td class="line x" title="61:104	'Training' of the RH parser on the Wall Street Journal area (beyond general RH development) occupied about 8 weeks, and involved testing and (non-exhaustively) correcting the parser using two WSJ texts: (a) section 00, and (b) 700 sentences of section 23 used as a dependency bank by King et al.(2003)." ></td>
	<td class="line x" title="63:104	The latter were used early in RH development, and so were included in the training set." ></td>
	<td class="line x" title="64:104	4.1 Comparative Speed Table 1 compares RH parser speed with Collins model 3, using the same CPU, showing the elapsed times for the entire 2416-line section 23." ></td>
	<td class="line x" title="65:104	The results are then extrapolated to two other parsers, based on published comparisons with Collins." ></td>
	<td class="line x" title="66:104	The extrapolation to XLE, a mature unification-based parser that uses a disambiguating statistical post-processor, is drawn from Kaplan et al.(2004)." ></td>
	<td class="line x" title="68:104	Results are given for both the full grammar and a reduced version that omits less likely rules." ></td>
	<td class="line x" title="69:104	The second comparison is with the fast stochastic parser by Sagae and Lavie (2005)." ></td>
	<td class="line x" title="70:104	Summarizing these results, RH is much faster than Collins model 3 and the reduced version of XLE, but a bit slower than Sagae-Lavie." ></td>
	<td class="line x" title="71:104	The table also compares coverage, as percentages of non-parsed sentences." ></td>
	<td class="line x" title="72:104	For RH this was 10% for the test set discussed below, which did not contain any training sentences, and was 10.4% for the full section 23." ></td>
	<td class="line x" title="73:104	This is reasonable for a symbolic parser with limited training on an idiom, and better than the 21% reported for XLE English." ></td>
	<td class="line x" title="74:104	Time No full parse Sagae/ Lavie ~ 4 min 1.1% RH parser 5 min 10% Collins m3 16 min.6% XLE full ~80 minutes ~21% XLE reduced ~24 minutes unknown Table 1: Speeds and Extrapolated speeds Fully accurate F-score Avg cross brackets Sagae/Lavie unknwn 86% unknwn Collins Lbl 33.6% 88.2% 1.05 CollinsNoLbl 35.4% 89.4 % 1.05 RH NoLbl 46% 86 % .59 Table 2." ></td>
	<td class="line x" title="75:104	Accuracy Comparison 4.2 Comparative Acccuracy Table 2 primarily compares the accuracy of the Collins model 3 and RH parsers." ></td>
	<td class="line x" title="76:104	The entries show the proportion of fully accurate parses, the f-score average of bracket precision and recall, and average crossing brackets, as obtained by EVALB (Sekine and Collins, 1997)." ></td>
	<td class="line x" title="77:104	The RH f-score is currently somewhat lower, but the proportion of fully correct parses is significantly higher." ></td>
	<td class="line x" title="78:104	This data may be biased toward RH, because, of necessity, the test set used is smaller, and a different bracketing method is used." ></td>
	<td class="line x" title="79:104	For Collins model 3, the entries show both labeled and unlabeled results for all of WSJ section 23." ></td>
	<td class="line x" title="80:104	The Collins results were generated from the bracketed output and Penn Treebank gold standard files provided in a recent Collins download." ></td>
	<td class="line x" title="81:104	But because RH does not generate treebank style tags, the RH entries reflect a test only on a random sample of 100 sentences from the 1716 sentences of section 23 not used as 'training' data, using a different, available, gold standard creation and bracketing method." ></td>
	<td class="line x" title="82:104	In that method (Newman, 2005), parser results are produced in a 'TextTree' form, initially developed for fast visual review of parser output, and then edited to obtain gold standard trees." ></td>
	<td class="line x" title="83:104	Both sets of trees are then bracketed by a script to obtain, e.g., {An automatic transformation {of parse trees} {to text trees}} {can expedite {parser output reviews}} 123 For non-parsed sentences in the parser outputs, brackets are applied to the chunks." ></td>
	<td class="line x" title="84:104	EVALB is then used to compare the two sets of bracketed results." ></td>
	<td class="line x" title="85:104	Accuracy for XLE is not given, because the results reported by Kaplan et al.(2004) compare labeled functional dependencies drawn from LFG f-structures with equivalents derived automatically from Collins outputs." ></td>
	<td class="line x" title="87:104	(All f-scores are <= 80%)." ></td>
	<td class="line x" title="88:104	5 Related Work Several efforts combine a chunker with a dependency analyzer operating on the chunks, including XIP itself." ></td>
	<td class="line x" title="89:104	The XIP dependency analyzer is very fast, but we do not have current coverage or accuracy data for XIP English." ></td>
	<td class="line x" title="90:104	Other related hybrids do not build on chunks, but, rather, adjust full parsers to require or prefer results consistent with chunk boundaries." ></td>
	<td class="line x" title="91:104	Daum et al.(2003) use chunks to constrain a WCDG grammar for German, reducing parse times by about 2/3 (but the same results are obtained using a tagger alone)." ></td>
	<td class="line x" title="93:104	They estimate that an ideal chunker would reduce times by about 75%." ></td>
	<td class="line x" title="94:104	No absolute numbers are given." ></td>
	<td class="line x" title="95:104	Also, Frank et al.(2003) use a German topological field identifier to constrain an HPSG parser." ></td>
	<td class="line x" title="97:104	They show speedups of about 2.2 relative to a tagged baseline, on a corpus whose average sentence length is about 9 words." ></td>
	<td class="line x" title="98:104	6 Discussion We have shown that the RH hybrid can compete with stochastic parsers in efficiency and, with only limited 'training' on an idiom, can approach them in accuracy." ></td>
	<td class="line x" title="99:104	Also, the test organization prevents speed from degrading as the parser is improved." ></td>
	<td class="line x" title="100:104	The method is significant in itself, but also leads to questions about the advantages of deepunification-based parsers for practical NLP." ></td>
	<td class="line x" title="101:104	These parsers are relatively slow, and their large numbers of results require disambiguation, e.g., by corpustrained back-ends." ></td>
	<td class="line x" title="102:104	They do provide more information than RH, but there is much evidence that the additional information can be obtained by rapid analysis of a single best parse." ></td>
	<td class="line x" title="103:104	Also, it has never been shown that their elegant notations actually facilitate grammar development and maintenance." ></td>
	<td class="line x" title="104:104	Finally, while unification grammars are reversible for use in generation, good generation methods remain an open research problem." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-1032
Formalism-Independent Parser Evaluation with CCG and DepBank
Clark, Stephen;Curran, James R.;"></td>
	<td class="line x" title="1:189	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 248255, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:189	c2007 Association for Computational Linguistics Formalism-Independent Parser Evaluation with CCG and DepBank Stephen Clark Oxford University Computing Laboratory Wolfson Building, Parks Road Oxford, OX1 3QD, UK stephen.clark@comlab.ox.ac.uk James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Abstract A key question facing the parsing community is how to compare parsers which use different grammar formalisms and produce different output." ></td>
	<td class="line x" title="3:189	Evaluating a parser on the same resource used to create it can lead to non-comparable accuracy scores and an over-optimistic view of parser performance." ></td>
	<td class="line x" title="4:189	In this paper we evaluate a CCG parser on DepBank, and demonstrate the difficulties in converting the parser output into DepBank grammatical relations." ></td>
	<td class="line x" title="5:189	In addition we present a method for measuring the effectiveness of the conversion, which provides an upper bound on parsing accuracy." ></td>
	<td class="line x" title="6:189	The CCG parser obtains an F-score of 81.9% on labelled dependencies, against an upper bound of 84.8%." ></td>
	<td class="line x" title="7:189	We compare the CCG parser against the RASP parser, outperforming RASP by over 5% overall and on the majority of dependency types." ></td>
	<td class="line oc" title="8:189	1 Introduction Parsers have been developed for a variety of grammar formalisms, for example HPSG (Toutanova et al. , 2002; Malouf and van Noord, 2004), LFG (Kaplan et al. , 2004; Cahill et al. , 2004), TAG (Sarkar and Joshi, 2003), CCG (Hockenmaier and Steedman, 2002; Clark and Curran, 2004b), and variants of phrase-structure grammar (Briscoe et al. , 2006), including the phrase-structure grammar implicit in the Penn Treebank (Collins, 2003; Charniak, 2000)." ></td>
	<td class="line x" title="9:189	Different parsers produce different output, for example phrase structure trees (Collins, 2003), dependency trees (Nivre and Scholz, 2004), grammatical relations (Briscoe et al. , 2006), and formalismspecific dependencies (Clark and Curran, 2004b)." ></td>
	<td class="line x" title="10:189	This variety of formalisms and output creates a challenge for parser evaluation." ></td>
	<td class="line x" title="11:189	The majority of parser evaluations have used test sets drawn from the same resource used to develop the parser." ></td>
	<td class="line x" title="12:189	This allows the many parsers based on the Penn Treebank, for example, to be meaningfully compared." ></td>
	<td class="line x" title="13:189	However, there are two drawbacks to this approach." ></td>
	<td class="line x" title="14:189	First, parser evaluations using different resources cannot be compared; for example, the Parseval scores obtained by Penn Treebank parsers cannot be compared with the dependency F-scores obtained by evaluating on the Parc Dependency Bank." ></td>
	<td class="line x" title="15:189	Second, using the same resource for development and testing can lead to an over-optimistic view of parser performance." ></td>
	<td class="line x" title="16:189	In this paper we evaluate a CCG parser (Clark and Curran, 2004b) on the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006)." ></td>
	<td class="line x" title="17:189	The CCG parser produces head-dependency relations, so evaluating the parser should simply be a matter of converting the CCG dependencies into those in DepBank." ></td>
	<td class="line x" title="18:189	Such conversions have been performed for other parsers, including parsers producing phrase structure output (Kaplan et al. , 2004; Preiss, 2003)." ></td>
	<td class="line x" title="19:189	However, we found that performing such a conversion is a time-consuming and non-trivial task." ></td>
	<td class="line x" title="20:189	The contributions of this paper are as follows." ></td>
	<td class="line x" title="21:189	First, we demonstrate the considerable difficulties associated with formalism-independent parser evaluation, highlighting the problems in converting the 248 output of a parser from one representation to another." ></td>
	<td class="line x" title="22:189	Second, we develop a method for measuring how effective the conversion process is, which also provides an upper bound for the performance of the parser, given the conversion process being used; this method can be adapted by other researchers to strengthen their own parser comparisons." ></td>
	<td class="line x" title="23:189	And third, we provide the first evaluation of a widecoverage CCG parser outside of CCGbank, obtaining impressive results on DepBank and outperforming the RASP parser (Briscoe et al. , 2006) by over 5% overall and on the majority of dependency types." ></td>
	<td class="line x" title="24:189	2 Previous Work The most common form of parser evaluation is to apply the Parseval metrics to phrase-structure parsers based on the Penn Treebank, and the highest reported scores are now over 90% (Bod, 2003; Charniak and Johnson, 2005)." ></td>
	<td class="line x" title="25:189	However, it is unclear whether these high scores accurately reflect the performance of parsers in applications." ></td>
	<td class="line x" title="26:189	It has been argued that the Parseval metrics are too forgiving and that phrase structure is not the ideal representation for a gold standard (Carroll et al. , 1998)." ></td>
	<td class="line x" title="27:189	Also, using the same resource for training and testing may result in the parser learning systematic errors which are present in both the training and testing material." ></td>
	<td class="line x" title="28:189	An example of this is from CCGbank (Hockenmaier, 2003), where all modifiers in noun-noun compound constructions modify the final noun (because the Penn Treebank, from which CCGbank is derived, does not contain the necessary information to obtain the correct bracketing)." ></td>
	<td class="line x" title="29:189	Thus there are nonnegligible, systematic errors in both the training and testing material, and the CCG parsers are being rewarded for following particular mistakes." ></td>
	<td class="line x" title="30:189	There are parser evaluation suites which have been designed to be formalism-independent and which have been carefully and manually corrected." ></td>
	<td class="line x" title="31:189	Carroll et al.(1998) describe such a suite, consisting of sentences taken from the Susanne corpus, annotated with Grammatical Relations (GRs) which specify the syntactic relation between a head and dependent." ></td>
	<td class="line x" title="33:189	Thus all that is required to use such a scheme, in theory, is that the parser being evaluated is able to identify heads." ></td>
	<td class="line x" title="34:189	A similar resource  the Parc Dependency Bank (DepBank) (King et al. , 2003)  has been created using sentences from the Penn Treebank." ></td>
	<td class="line x" title="35:189	Briscoe and Carroll (2006) reannotated this resource using their GRs scheme, and used it to evaluate the RASP parser." ></td>
	<td class="line x" title="36:189	Kaplan et al.(2004) compare the Collins (2003) parser with the Parc LFG parser by mapping LFG Fstructures and Penn Treebank parses into DepBank dependencies, claiming that the LFG parser is considerably more accurate with only a slight reduction in speed." ></td>
	<td class="line x" title="38:189	Preiss (2003) compares the parsers of Collins (2003) and Charniak (2000), the GR finder of Buchholz et al.(1999), and the RASP parser, using the Carroll et al.(1998) gold-standard." ></td>
	<td class="line x" title="41:189	The Penn Treebank trees of the Collins and Charniak parsers, and the GRs of the Buchholz parser, are mapped into the required GRs, with the result that the GR finder of Buchholz is the most accurate." ></td>
	<td class="line x" title="42:189	The major weakness of these evaluations is that there is no measure of the difficultly of the conversion process for each of the parsers." ></td>
	<td class="line x" title="43:189	Kaplan et al.(2004) clearly invested considerable time and expertise in mapping the output of the Collins parser into the DepBank dependencies, but they also note that This conversion was relatively straightforward for LFG structures However, a certain amount of skill and intuition was required to provide a fair conversion of the Collins trees." ></td>
	<td class="line x" title="45:189	Without some measure of the difficulty  and effectiveness  of the conversion, there remains a suspicion that the Collins parser is being unfairly penalised." ></td>
	<td class="line x" title="46:189	One way of providing such a measure is to convert the original gold standard on which the parser is based and evaluate that against the new gold standard (assuming the two resources are based on the same corpus)." ></td>
	<td class="line x" title="47:189	In the case of Kaplan et al.(2004), the testing procedure would include running their conversion process on Section 23 of the Penn Treebank and evaluating the output against DepBank." ></td>
	<td class="line x" title="49:189	As well as providing some measure of the effectiveness of the conversion, this method would also provide an upper bound for the Collins parser, giving the score that a perfect Penn Treebank parser would obtain on DepBank (given the conversion process)." ></td>
	<td class="line x" title="50:189	We perform such an evaluation for the CCG parser, with the surprising result that the upper bound on DepBank is only 84.8%, despite the considerable effort invested in developing the conversion process." ></td>
	<td class="line x" title="51:189	249 3 The CCG Parser Clark and Curran (2004b) describes the CCG parser used for the evaluation." ></td>
	<td class="line x" title="52:189	The grammar used by the parser is extracted from CCGbank, a CCG version of the Penn Treebank (Hockenmaier, 2003)." ></td>
	<td class="line x" title="53:189	The grammar consists of 425 lexical categories  expressing subcategorisation information  plus a small number of combinatory rules which combine the categories (Steedman, 2000)." ></td>
	<td class="line x" title="54:189	A supertagger first assigns lexical categories to the words in a sentence, which are then combined by the parser using the combinatory rules and the CKY algorithm." ></td>
	<td class="line x" title="55:189	A log-linear model scores the alternative parses." ></td>
	<td class="line x" title="56:189	We use the normal-form model, which assigns probabilities to single derivations based on the normal-form derivations in CCGbank." ></td>
	<td class="line x" title="57:189	The features in the model are defined over local parts of the derivation and include word-word dependencies." ></td>
	<td class="line x" title="58:189	A packed chart representation allows efficient decoding, with the Viterbi algorithm finding the most probable derivation." ></td>
	<td class="line x" title="59:189	The parser outputs predicate-argument dependencies defined in terms of CCG lexical categories." ></td>
	<td class="line x" title="60:189	More formally, a CCG predicate-argument dependency is a 5-tuple: hf,f,s,ha,l, where hf is the lexical item of the lexical category expressing the dependency relation; f is the lexical category; s is the argument slot; ha is the head word of the argument; and l encodes whether the dependency is long-range." ></td>
	<td class="line x" title="61:189	For example, the dependency encoding company as the object of bought (as in IBM bought the company) is represented as follows: bought, (S\NP1)/NP2, 2, company,  (1) The lexical category (S\NP1)/NP2 is the category of a transitive verb, with the first argument slot corresponding to the subject, and the second argument slot corresponding to the direct object." ></td>
	<td class="line x" title="62:189	The final field indicates the nature of any long-range dependency; in (1) the dependency is local." ></td>
	<td class="line x" title="63:189	The predicate-argument dependencies  including long-range dependencies  are encoded in the lexicon by adding head and dependency annotation to the lexical categories." ></td>
	<td class="line x" title="64:189	For example, the expanded category for the control verb persuade is (((S[dcl]persuade\NP1)/(S[to]2\NPX))/NPX,3)." ></td>
	<td class="line x" title="65:189	Numerical subscripts on the argument categories represent dependency relations; the head of the final declarative sentence is persuade; and the head of the infinitival complements subject is identified with the head of the object, using the variable X, as in standard unification-based accounts of control." ></td>
	<td class="line x" title="66:189	Previous evaluations of CCG parsers have used the predicate-argument dependencies from CCGbank as a test set (Hockenmaier and Steedman, 2002; Clark and Curran, 2004b), with impressive results of over 84% F-score on labelled dependencies." ></td>
	<td class="line x" title="67:189	In this paper we reinforce the earlier results with the first evaluation of a CCG parser outside of CCGbank." ></td>
	<td class="line x" title="68:189	4 Dependency Conversion to DepBank For the gold standard we chose the version of DepBank reannotated by Briscoe and Carroll (2006), consisting of 700 sentences from Section 23 of the Penn Treebank." ></td>
	<td class="line x" title="69:189	The B&C scheme is similar to the original DepBank scheme (King et al. , 2003), but overall contains less grammatical detail; Briscoe and Carroll (2006) describes the differences." ></td>
	<td class="line x" title="70:189	We chose this resource for the following reasons: it is publicly available, allowing other researchers to compare against our results; the GRs making up the annotation share some similarities with the predicateargument dependencies output by the CCG parser; and we can directly compare our parser against a non-CCG parser, namely the RASP parser." ></td>
	<td class="line x" title="71:189	We chose not to use the corpus based on the Susanne corpus (Carroll et al. , 1998) because the GRs are less like the CCG dependencies; the corpus is not based on the Penn Treebank, making comparison more difficult because of tokenisation differences, for example; and the latest results for RASP are on DepBank." ></td>
	<td class="line x" title="72:189	The GRs are described in Briscoe and Carroll (2006) and Briscoe et al.(2006)." ></td>
	<td class="line x" title="74:189	Table 1 lists the GRs used in the evaluation." ></td>
	<td class="line x" title="75:189	As an example, the sentence The parent sold Imperial produces three GRs: (det parent The), (ncsubj sold parent ) and (dobj sold Imperial)." ></td>
	<td class="line x" title="76:189	Note that some GRs  in this example ncsubj  have a subtype slot, giving extra information." ></td>
	<td class="line x" title="77:189	The subtype slot for ncsubj is used to indicate passive subjects, with the null value   for active subjects and obj for passive subjects." ></td>
	<td class="line x" title="78:189	Other subtype slots are discussed in Section 4.2." ></td>
	<td class="line x" title="79:189	The CCG dependencies were transformed into GRs in two stages." ></td>
	<td class="line x" title="80:189	The first stage was to create a mapping between the CCG dependencies and the 250 GR description conj coordinator aux auxiliary det determiner ncmod non-clausal modifier xmod unsaturated predicative modifier cmod saturated clausal modifier pmod PP modifier with a PP complement ncsubj non-clausal subject xsubj unsaturated predicative subject csubj saturated clausal subject dobj direct object obj2 second object iobj indirect object pcomp PP which is a PP complement xcomp unsaturated VP complement ccomp saturated clausal complement ta textual adjunct delimited by punctuation Table 1: GRs in B&Cs annotation of DepBank GRs." ></td>
	<td class="line x" title="81:189	This involved mapping each argument slot in the 425 lexical categories in the CCG lexicon onto a GR." ></td>
	<td class="line x" title="82:189	In the second stage, the GRs created from the parser output were post-processed to correct some of the obvious remaining differences between the CCG and GR representations." ></td>
	<td class="line x" title="83:189	In the process of performing the transformation we encountered a methodological problem: without looking at examples it was difficult to create the mapping and impossible to know whether the two representations were converging." ></td>
	<td class="line x" title="84:189	Briscoe et al.(2006) split the 700 sentences in DepBank into a test and development set, but the latter only consists of 140 sentences which was not enough to reliably create the transformation." ></td>
	<td class="line x" title="86:189	There are some development files in the RASP release which provide examples of the GRs, which were used when possible, but these only cover a subset of the CCG lexical categories." ></td>
	<td class="line x" title="87:189	Our solution to this problem was to convert the gold standard dependencies from CCGbank into GRs and use these to develop the transformation." ></td>
	<td class="line x" title="88:189	So we did inspect the annotation in DepBank, and compared it to the transformed CCG dependencies, but only the gold-standard CCG dependencies." ></td>
	<td class="line x" title="89:189	Thus the parser output was never used during this process." ></td>
	<td class="line x" title="90:189	We also ensured that the dependency mapping and the post processing are general to the GRs scheme and not specific to the test set or parser." ></td>
	<td class="line x" title="91:189	4.1 Mapping the CCG dependencies to GRs Table 2 gives some examples of the mapping; %l indicates the word associated with the lexical category CCG lexical category slot GR (S[dcl]\NP1)/NP2 1 (ncsubj %l %f ) (S[dcl]\NP1)/NP2 2 (dobj %l %f) (S\NP)/(S\NP)1 1 (ncmod %f %l) (NP\NP1)/NP2 1 (ncmod %f %l) (NP\NP1)/NP2 2 (dobj %l %f) NP[nb]/N1 1 (det %f %l) (NP\NP1)/(S[pss]\NP)2 1 (xmod %f %l) (NP\NP1)/(S[pss]\NP)2 2 (xcomp %l %f) ((S\NP)\(S\NP)1)/S[dcl]2 1 (cmod %f %l) ((S\NP)\(S\NP)1)/S[dcl]2 2 (ccomp %l %f) ((S[dcl]\NP1)/NP2)/NP3 2 (obj2 %l %f) (S[dcl]\NP1)/(S[b]\NP)2 2 (aux %f %l) Table 2: Examples of the dependency mapping and %f is the head of the constituent filling the argument slot." ></td>
	<td class="line x" title="92:189	Note that the order of %l and %f varies according to whether the GR represents a complement or modifier, in line with the Briscoe and Carroll annotation." ></td>
	<td class="line x" title="93:189	For many of the CCG dependencies, the mapping into GRs is straightforward." ></td>
	<td class="line x" title="94:189	For example, the first two rows of Table 2 show the mapping for the transitive verb category (S[dcl]\NP1)/NP2 : argument slot 1 is a non-clausal subject and argument slot 2 is a direct object." ></td>
	<td class="line x" title="95:189	Creating the dependency transformation is more difficult than these examples suggest." ></td>
	<td class="line x" title="96:189	The first problem is that the mapping from CCG dependencies to GRs is many-to-many." ></td>
	<td class="line x" title="97:189	For example, the transitive verb category (S[dcl]\NP)/NP applies to the copula in sentences like Imperial Corp. is the parent of Imperial Savings & Loan." ></td>
	<td class="line x" title="98:189	With the default annotation, the relation between is and parent would be dobj, whereas in DepBank the argument of the copula is analysed as an xcomp." ></td>
	<td class="line x" title="99:189	Table 3 gives some examples of how we attempt to deal with this problem." ></td>
	<td class="line x" title="100:189	The constraint in the first example means that, whenever the word associated with the transitive verb category is a form of be, the second argument is xcomp, otherwise the default case applies (in this case dobj)." ></td>
	<td class="line x" title="101:189	There are a number of categories with similar constraints, checking whether the word associated with the category is a form of be." ></td>
	<td class="line x" title="102:189	The second type of constraint, shown in the third line of the table, checks the lexical category of the word filling the argument slot." ></td>
	<td class="line x" title="103:189	In this example, if the lexical category of the preposition is PP/NP, then the second argument of (S[dcl]\NP)/PP maps to iobj; thus in The loss stems from several factors the relation between the verb and preposition is (iobj stems from)." ></td>
	<td class="line x" title="104:189	If the lexical category of 251 CCG lexical category slot GR constraint example (S[dcl]\NP1)/NP2 2 (xcomp %l %f) word=be The parent is Imperial (dobj %l %f) The parent sold Imperial (S[dcl]\NP1)/PP2 2 (iobj %l %f) cat=PP/NP The loss stems from several factors (xcomp %l %f) cat=PP/(S[ng]\NP) The future depends on building ties (S[dcl]\NP1)/(S[to]\NP)2 2 (xcomp %f %l %k) cat=(S[to]\NP)/(S[b]\NP) wants to wean itself away from Table 3: Examples of the many-to-many nature of the CCG dependency to GRs mapping, and a ternary GR the preposition is PP/(S[ng]\NP), then the GR is xcomp; thus in The future depends on building ties the relation between the verb and preposition is (xcomp depends on)." ></td>
	<td class="line x" title="105:189	There are a number of CCG dependencies with similar constraints, many of them covering the iobj/xcomp distinction." ></td>
	<td class="line x" title="106:189	The second difficulty is that not all the GRs are binary relations, whereas the CCG dependencies are all binary." ></td>
	<td class="line x" title="107:189	The primary example of this is to-infinitival constructions." ></td>
	<td class="line x" title="108:189	For example, in the sentence The company wants to wean itself away from expensive gimmicks, the CCG parser produces two dependencies relating wants, to and wean, whereas there is only one GR: (xcomp to wants wean)." ></td>
	<td class="line x" title="109:189	The final row of Table 3 gives an example." ></td>
	<td class="line x" title="110:189	We implement this constraint by introducing a %k variable into the GR template which denotes the argument of the category in the constraint column (which, as before, is the lexical category of the word filling the argument slot)." ></td>
	<td class="line x" title="111:189	In the example, the current category is (S[dcl]\NP1)/(S[to]\NP)2, which is associated with wants; this combines with (S[to]\NP)/(S[b]\NP), associated with to; and the argument of (S[to]\NP)/(S[b]\NP) is wean." ></td>
	<td class="line x" title="112:189	The %k variable allows us to look beyond the arguments of the current category when creating the GRs." ></td>
	<td class="line x" title="113:189	A further difficulty is that the head passing conventions differ between DepBank and CCGbank." ></td>
	<td class="line x" title="114:189	By head passing we mean the mechanism which determines the heads of constituents and the mechanism by which words become arguments of longrange dependencies." ></td>
	<td class="line x" title="115:189	For example, in the sentence The group said it would consider withholding royalty payments, the DepBank and CCGbank annotations create a dependency between said and the following clause." ></td>
	<td class="line x" title="116:189	However, in DepBank the relation is between said and consider, whereas in CCGbank the relation is between said and would." ></td>
	<td class="line x" title="117:189	We fixed this problem by defining the head of would consider to be consider rather than would, by changing the annotation of all the relevant lexical categories in the CCG lexicon (mainly those creating aux relations)." ></td>
	<td class="line x" title="118:189	There are more subject relations in CCGbank than DepBank." ></td>
	<td class="line x" title="119:189	In the previous example, CCGbank has a subject relation between it and consider, and also it and would, whereas DepBank only has the relation between it and consider." ></td>
	<td class="line x" title="120:189	In practice this means ignoring a number of the subject dependencies output by the CCG parser." ></td>
	<td class="line x" title="121:189	Another example where the dependencies differ is the treatment of relative pronouns." ></td>
	<td class="line x" title="122:189	For example, in Sen. Mitchell, who had proposed the streamlining, the subject of proposed is Mitchell in CCGbank but who in DepBank." ></td>
	<td class="line x" title="123:189	Again, we implemented this change by fixing the head annotation in the lexical categories which apply to relative pronouns." ></td>
	<td class="line x" title="124:189	4.2 Post processing of the GR output To obtain some idea of whether the schemes were converging, we performed the following oracle experiment." ></td>
	<td class="line x" title="125:189	We took the CCG derivations from CCGbank corresponding to the sentences in DepBank, and forced the parser to produce goldstandard derivations, outputting the newly created GRs." ></td>
	<td class="line x" title="126:189	Treating the DepBank GRs as a gold-standard, and comparing these with the CCGbank GRs, gave precision and recall scores of only 76.23% and 79.56% respectively (using the RASP evaluation tool)." ></td>
	<td class="line x" title="127:189	Thus given the current mapping, the perfect CCGbank parser would achieve an F-score of only 77.86% when evaluated against DepBank." ></td>
	<td class="line x" title="128:189	On inspecting the output, it was clear that a number of general rules could be applied to bring the schemes closer together, which was implemented as a post-processing script." ></td>
	<td class="line x" title="129:189	The first set of changes deals with coordination." ></td>
	<td class="line x" title="130:189	One significant difference between DepBank and CCGbank is the treatment of coordinations as arguments." ></td>
	<td class="line x" title="131:189	Consider the example The president and chief executive officer said the loss stems from several factors." ></td>
	<td class="line x" title="132:189	For both DepBank and the transformed CCGbank there are two conj GRs arising 252 from the coordination: (conj and president) and (conj and officer)." ></td>
	<td class="line x" title="133:189	The difference arises in the subject of said: in DepBank the subject is and: (ncsubj said and ), whereas in CCGbank there are two subjects: (ncsubj said president ) and (ncsubj said officer )." ></td>
	<td class="line x" title="134:189	We deal with this difference by replacing any pairs of GRs which differ only in their arguments, and where the arguments are coordinated items, with a single GR containing the coordination term as the argument." ></td>
	<td class="line x" title="135:189	Ampersands are a frequently occurring problem in WSJ text." ></td>
	<td class="line x" title="136:189	For example, the CCGbank analysis of Standard & Poors index assigns the lexical category N/N to both Standard and &, treating them as modifiers of Poor, whereas DepBank treats & as a coordinating term." ></td>
	<td class="line x" title="137:189	We fixed this by creating conj GRs between any & and the two words either side; removing the modifier GR between the two words; and replacing any GRs in which the words either side of the & are arguments with a single GR in which & is the argument." ></td>
	<td class="line x" title="138:189	The ta relation, which identifies text adjuncts delimited by punctuation, is difficult to assign correctly to the parser output." ></td>
	<td class="line x" title="139:189	The simple punctuation rules used by the parser do not contain enough information to distinguish between the various cases of ta." ></td>
	<td class="line x" title="140:189	Thus the only rule we have implemented, which is somewhat specific to the newspaper genre, is to replace GRs of the form (cmod say arg) with (ta quote arg say), where say can be any of say, said or says." ></td>
	<td class="line x" title="141:189	This rule applies to only a small subset of the ta cases but has high enough precision to be worthy of inclusion." ></td>
	<td class="line x" title="142:189	A common source of error is the distinction between iobj and ncmod, which is not surprising given the difficulty that human annotators have in distinguishing arguments and adjuncts." ></td>
	<td class="line x" title="143:189	There are many cases where an argument in DepBank is an adjunct in CCGbank, and vice versa." ></td>
	<td class="line x" title="144:189	The only change we have made is to turn all ncmod GRs with of as the modifier into iobj GRs (unless the ncmod is a partitive predeterminer)." ></td>
	<td class="line x" title="145:189	This was found to have high precision and applies to a large number of cases." ></td>
	<td class="line x" title="146:189	There are some dependencies in CCGbank which do not appear in DepBank." ></td>
	<td class="line x" title="147:189	Examples include any dependencies in which a punctuation mark is one of the arguments; these were removed from the output." ></td>
	<td class="line x" title="148:189	We attempt to fill the subtype slot for some GRs." ></td>
	<td class="line x" title="149:189	The subtype slot specifies additional information about the GR; examples include the value obj in a passive ncsubj, indicating that the subject is an underlying object; the value num in ncmod, indicating a numerical quantity; and prt in ncmod to indicate a verb particle." ></td>
	<td class="line x" title="150:189	The passive case is identified as follows: any lexical category which starts S[pss]\NP indicates a passive verb, and we also mark any verbs POS tagged VBN and assigned the lexical category N/N as passive." ></td>
	<td class="line x" title="151:189	Both these rules have high precision, but still leave many of the cases in DepBank unidentified." ></td>
	<td class="line x" title="152:189	The numerical case is identified using two rules: the num subtype is added if any argument in a GR is assigned the lexical category N/N[num], and if any of the arguments in an ncmod is POS tagged CD." ></td>
	<td class="line x" title="153:189	prt is added to an ncmod if the modifiee has any of the verb POS tags and if the modifier has POS tag RP." ></td>
	<td class="line x" title="154:189	The final columns of Table 4 show the accuracy of the transformed gold-standard CCGbank dependencies when compared with DepBank; the simple post-processing rules have increased the F-score from 77.86% to 84.76%." ></td>
	<td class="line x" title="155:189	This F-score is an upper bound on the performance of the CCG parser." ></td>
	<td class="line x" title="156:189	5 Results The results in Table 4 were obtained by parsing the sentences from CCGbank corresponding to those in the 560-sentence test set used by Briscoe et al.(2006)." ></td>
	<td class="line x" title="158:189	We used the CCGbank sentences because these differ in some ways to the original Penn Treebank sentences (there are no quotation marks in CCGbank, for example) and the parser has been trained on CCGbank." ></td>
	<td class="line x" title="159:189	Even here we experienced some unexpected difficulties, since some of the tokenisation is different between DepBank and CCGbank and there are some sentences in DepBank which have been significantly shortened compared to the original Penn Treebank sentences." ></td>
	<td class="line x" title="160:189	We modified the CCGbank sentences  and the CCGbank analyses since these were used for the oracle experiments  to be as close to the DepBank sentences as possible." ></td>
	<td class="line x" title="161:189	All the results were obtained using the RASP evaluation scripts, with the results for the RASP parser taken from Briscoe et al.(2006)." ></td>
	<td class="line x" title="163:189	The results for CCGbank were obtained using the oracle method described above." ></td>
	<td class="line x" title="164:189	253 RASP CCG parser CCGbank Relation Prec Rec F Prec Rec F Prec Rec F # GRs aux 93.33 91.00 92.15 94.20 89.25 91.66 96.47 90.33 93.30 400 conj 72.39 72.27 72.33 79.73 77.98 78.84 83.07 80.27 81.65 595 ta 42.61 51.37 46.58 52.31 11.64 19.05 62.07 12.59 20.93 292 det 87.73 90.48 89.09 95.25 95.42 95.34 97.27 94.09 95.66 1 114 ncmod 75.72 69.94 72.72 75.75 79.27 77.47 78.88 80.64 79.75 3 550 xmod 53.21 46.63 49.70 43.46 52.25 47.45 56.54 60.67 58.54 178 cmod 45.95 30.36 36.56 51.50 61.31 55.98 64.77 69.09 66.86 168 pmod 30.77 33.33 32.00 0.00 0.00 0.00 0.00 0.00 0.00 12 ncsubj 79.16 67.06 72.61 83.92 75.92 79.72 88.86 78.51 83.37 1 354 xsubj 33.33 28.57 30.77 0.00 0.00 0.00 50.00 28.57 36.36 7 csubj 12.50 50.00 20.00 0.00 0.00 0.00 0.00 0.00 0.00 2 dobj 83.63 79.08 81.29 87.03 89.40 88.20 92.11 90.32 91.21 1 764 obj2 23.08 30.00 26.09 65.00 65.00 65.00 66.67 60.00 63.16 20 iobj 70.77 76.10 73.34 77.60 70.04 73.62 83.59 69.81 76.08 544 xcomp 76.88 77.69 77.28 76.68 77.69 77.18 80.00 78.49 79.24 381 ccomp 46.44 69.42 55.55 79.55 72.16 75.68 80.81 76.31 78.49 291 pcomp 72.73 66.67 69.57 0.00 0.00 0.00 0.00 0.00 0.00 24 macroaverage 62.12 63.77 62.94 65.61 63.28 64.43 71.73 65.85 68.67 microaverage 77.66 74.98 76.29 82.44 81.28 81.86 86.86 82.75 84.76 Table 4: Accuracy on DepBank." ></td>
	<td class="line x" title="165:189	F-score is the balanced harmonic mean of precision (P) and recall (R): 2PR/(P + R)." ></td>
	<td class="line x" title="166:189	# GRs is the number of GRs in DepBank." ></td>
	<td class="line x" title="167:189	The CCG parser results are based on automatically assigned POS tags, using the Curran and Clark (2003) tagger." ></td>
	<td class="line x" title="168:189	The coverage of the parser on DepBank is 100%." ></td>
	<td class="line x" title="169:189	For a GR in the parser output to be correct, it has to match the gold-standard GR exactly, including any subtype slots; however, it is possible for a GR to be incorrect at one level but correct at a subsuming level.1 For example, if an ncmod GR is incorrectly labelled with xmod, but is otherwise correct, it will be correct for all levels which subsume both ncmod and xmod, for example mod." ></td>
	<td class="line x" title="170:189	The microaveraged scores are calculated by aggregating the counts for all the relations in the hierarchy, including the subsuming relations; the macro-averaged scores are the mean of the individual scores for each relation (Briscoe et al. , 2006)." ></td>
	<td class="line x" title="171:189	The results show that the performance of the CCG parser is higher than RASP overall, and also higher on the majority of GR types (especially the more frequent types)." ></td>
	<td class="line x" title="172:189	RASP uses an unlexicalised parsing model and has not been tuned to newspaper text." ></td>
	<td class="line x" title="173:189	On the other hand it has had many years of development; thus it provides a strong baseline for this test set." ></td>
	<td class="line x" title="174:189	The overall F-score for the CCG parser, 81.86%, is only 3 points below that for CCGbank, which pro1The GRs are arranged in a hierarchy, with those in Table 1 at the leaves; a small number of more general GRs subsume these (Briscoe and Carroll, 2006)." ></td>
	<td class="line x" title="175:189	vides an upper bound for the CCG parser (given the conversion process being used)." ></td>
	<td class="line x" title="176:189	6 Conclusion A contribution of this paper has been to highlight the difficulties associated with cross-formalism parser comparison." ></td>
	<td class="line x" title="177:189	Note that the difficulties are not unique to CCG, and many would apply to any crossformalism comparison, especially with parsers using automatically extracted grammars." ></td>
	<td class="line x" title="178:189	Parser evaluation has improved on the original Parseval measures (Carroll et al. , 1998), but the challenge remains to develop a representation and evaluation suite which can be easily applied to a wide variety of parsers and formalisms." ></td>
	<td class="line x" title="179:189	Despite the difficulties, we have given the first evaluation of a CCG parser outside of CCGbank, outperforming the RASP parser by over 5% overall and on the majority of dependency types." ></td>
	<td class="line x" title="180:189	Can the CCG parser be compared with parsers other than RASP?" ></td>
	<td class="line x" title="181:189	Briscoe and Carroll (2006) give a rough comparison of RASP with the Parc LFG parser on the different versions of DepBank, obtaining similar results overall, but they acknowledge that the results are not strictly comparable because of the different annotation schemes used." ></td>
	<td class="line x" title="182:189	Comparison with Penn Treebank parsers would be difficult because, for many constructions, the Penn Treebank trees and 254 CCG derivations are different shapes, and reversing the mapping Hockenmaier used to create CCGbank would be very difficult." ></td>
	<td class="line x" title="183:189	Hence we challenge other parser developers to map their own parse output into the version of DepBank used here." ></td>
	<td class="line x" title="184:189	One aspect of parser evaluation not covered in this paper is efficiency." ></td>
	<td class="line x" title="185:189	The CCG parser took only 22.6 seconds to parse the 560 sentences in DepBank, with the accuracy given earlier." ></td>
	<td class="line x" title="186:189	Using a cluster of 18 machines we have also parsed the entire Gigaword corpus in less than five days." ></td>
	<td class="line x" title="187:189	Hence, we conclude that accurate, large-scale, linguistically-motivated NLP is now practical with CCG." ></td>
	<td class="line x" title="188:189	Acknowledgements We would like to thanks the anonymous reviewers for their helpful comments." ></td>
	<td class="line x" title="189:189	James Curran was funded under ARC Discovery grants DP0453131 and DP0665973." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-0411
Dependency-Based Automatic Evaluation for Machine Translation
Owczarzak, Karolina;Van Genabith, Josef;Way, Andy;"></td>
	<td class="line x" title="1:166	Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 8087, Rochester, New York, April 2007." ></td>
	<td class="line x" title="2:166	c2007 Association for Computational Linguistics Dependency-Based Automatic Evaluation for Machine Translation Karolina Owczarzak Josef van Genabith Andy Way National Centre for Language Technology School of Computing, Dublin City University Dublin 9, Ireland {owczarzak,josef,away}@computing.dcu.ie Abstract We present a novel method for evaluating the output of Machine Translation (MT), based on comparing the dependency structures of the translation and reference rather than their surface string forms." ></td>
	<td class="line x" title="3:166	Our method uses a treebank-based, widecoverage, probabilistic Lexical-Functional Grammar (LFG) parser to produce a set of structural dependencies for each translation-reference sentence pair, and then calculates the precision and recall for these dependencies." ></td>
	<td class="line x" title="4:166	Our dependencybased evaluation, in contrast to most popular string-based evaluation metrics, will not unfairly penalize perfectly valid syntactic variations in the translation." ></td>
	<td class="line x" title="5:166	In addition to allowing for legitimate syntactic differences, we use paraphrases in the evaluation process to account for lexical variation." ></td>
	<td class="line x" title="6:166	In comparison with other metrics on 16,800 sentences of Chinese-English newswire text, our method reaches high correlation with human scores." ></td>
	<td class="line x" title="7:166	An experiment with two translations of 4,000 sentences from Spanish-English Europarl shows that, in contrast to most other metrics, our method does not display a high bias towards statistical models of translation." ></td>
	<td class="line x" title="8:166	1 Introduction Since their appearance, string-based evaluation metrics such as BLEU (Papineni et al. , 2002) and NIST (Doddington, 2002) have been the standard tools used for evaluating MT quality." ></td>
	<td class="line x" title="9:166	Both score a candidate translation on the basis of the number of n-grams shared with one or more reference translations." ></td>
	<td class="line x" title="10:166	Automatic measures are indispensable in the development of MT systems, because they allow MT developers to conduct frequent, costeffective, and fast evaluations of their evolving models." ></td>
	<td class="line x" title="11:166	These advantages come at a price, though: an automatic comparison of n-grams measures only the string similarity of the candidate translation to one or more reference strings, and will penalize any divergence from them." ></td>
	<td class="line x" title="12:166	In effect, a candidate translation expressing the source meaning accurately and fluently will be given a low score if the lexical and syntactic choices it contains, even though perfectly legitimate, are not present in at least one of the references." ></td>
	<td class="line x" title="13:166	Necessarily, this score would differ from a much more favourable human judgement that such a translation would receive." ></td>
	<td class="line x" title="14:166	The limitations of string comparison are the reason why it is advisable to provide multiple references for a candidate translation in BLEUor NIST-based evaluations." ></td>
	<td class="line x" title="15:166	While Zhang and Vogel (2004) argue that increasing the size of the test set gives even more reliable system scores than multiple references, this still does not solve the inadequacy of BLEU and NIST for sentence-level or small set evaluation." ></td>
	<td class="line x" title="16:166	In addition, in practice even a number of references do not capture the whole potential variability of the translation." ></td>
	<td class="line x" title="17:166	Moreover, when designing a statistical MT system, the need for large amounts of training data limits the researcher to collections of parallel corpora such as Europarl (Koehn, 2005), which provides only one reference, namely the target text; and the cost of creating additional reference translations of the test set, usually a few thousand sentences long, is often prohibitive." ></td>
	<td class="line x" title="18:166	Therefore, it would be desirable to find an evaluation method that accepts legitimate syntactic and lexical differences 80 between the translation and the reference, thus better mirroring human assessment." ></td>
	<td class="line x" title="19:166	In this paper, we present a novel method that automatically evaluates the quality of translation based on the dependency structure of the sentence, rather than its surface form." ></td>
	<td class="line x" title="20:166	Dependencies abstract away from the particulars of the surface string (and CFG tree) realization and provide a normalized representation of (some) syntactic variants of a given sentence." ></td>
	<td class="line oc" title="21:166	The translation and reference files are analyzed by a treebank-based, probabilistic Lexical-Functional Grammar (LFG) parser (Cahill et al. , 2004), which produces a set of dependency triples for each input." ></td>
	<td class="line x" title="22:166	The translation set is compared to the reference set, and the number of matches is calculated, giving the precision, recall, and f-score for that particular translation." ></td>
	<td class="line x" title="23:166	In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) and Owczarzak et al.(2006) in adding a number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score." ></td>
	<td class="line x" title="25:166	Comparing the LFG-based evaluation method with other popular metrics: BLEU, NIST, General Text Matcher (GTM) (Turian et al. , 2003), Translation Error Rate (TER) (Snover et al. , 2006)1, and METEOR (Banerjee and Lavie, 2005), we show that combining dependency representations with paraphrases leads to a more accurate evaluation that correlates better with human judgment." ></td>
	<td class="line x" title="26:166	The remainder of this paper is organized as follows: Section 2 gives a basic introduction to LFG; Section 3 describes related work; Section 4 describes our method and gives results of two experiments on different sets of data: 4,000 sentences from Spanish-English Europarl and 16,800 sentences of Chinese-English newswire text from the Linguistic Data Consortiums (LDC) Multiple Translation project; Section 5 discusses ongoing work; Section 6 concludes." ></td>
	<td class="line x" title="27:166	1 As we focus on purely automatic metrics, we omit HTER (Human-Targeted Translation Error Rate) here." ></td>
	<td class="line x" title="28:166	2 Lexical-Functional Grammar In Lexical-Functional Grammar (Bresnan, 2001) sentence structure is represented in terms of c(onstituent)-structure and f(unctional)-structure." ></td>
	<td class="line x" title="29:166	C-structure represents the surface string word order and the hierarchical organisation of phrases in terms of CFG trees." ></td>
	<td class="line x" title="30:166	F-structures are recursive feature (or attribute-value) structures, representing abstract grammatical relations, such as subj(ect), obj(ect), obl(ique), adj(unct), approximating to predicate-argument structure or simple logical forms." ></td>
	<td class="line x" title="31:166	C-structure and f-structure are related in terms of functional annotations (attribute-value structure equations) in c-structure trees, describing f-structures." ></td>
	<td class="line x" title="32:166	While c-structure is sensitive to surface word order, f-structure is not." ></td>
	<td class="line x" title="33:166	The sentences John resigned yesterday and Yesterday, John resigned will receive different tree representations, but identical f-structures, shown in (1)." ></td>
	<td class="line x" title="34:166	(1) C-structure: F-structure: S NP VP | John V NP-TMP | | resigned yesterday SUBJ PRED john NUM sg PERS 3 PRED resign TENSE past ADJ {[PRED yesterday]} S NP NP VP | | | Yesterday John V | resigned SUBJ PRED john NUM sg PERS 3 PRED resign TENSE past ADJ {[PRED yesterday]} Notice that if these two sentences were a translation-reference pair, they would receive a less-than-perfect score from string-based metrics." ></td>
	<td class="line x" title="35:166	For example, BLEU with add-one smoothing2 gives this pair a score of barely 0.3781." ></td>
	<td class="line x" title="36:166	The f-structure can also be described as a flat set of triples." ></td>
	<td class="line x" title="37:166	In triples format, the f-structure in (1) could be represented as follows: {subj(resign, john), pers(john, 3), num(john, sg), tense(resign, 2 We use smoothing because the original BLEU gives zero points to sentences with fewer than one four-gram." ></td>
	<td class="line x" title="38:166	81 past), adj(resign, yesterday), pers(yesterday, 3), num(yesterday, sg)}." ></td>
	<td class="line oc" title="39:166	Cahill et al.(2004) presents Penn-II Treebankbased LFG parsing resources." ></td>
	<td class="line o" title="41:166	Her approach distinguishes 32 types of dependencies, including grammatical functions and morphological information." ></td>
	<td class="line x" title="42:166	This set can be divided into two major groups: a group of predicate-only dependencies and non-predicate dependencies." ></td>
	<td class="line x" title="43:166	Predicate-only dependencies are those whose path ends in a predicate-value pair, describing grammatical relations." ></td>
	<td class="line x" title="44:166	For example, for the f-structure in (1), predicate-only dependencies would include: {subj(resign, john), adj(resign, yesterday)}.3 In parser evaluation, the quality of the fstructures produced automatically can be checked against a set of gold standard sentences annotated with f-structures by a linguist." ></td>
	<td class="line x" title="45:166	The evaluation is conducted by calculating the precision and recall between the set of dependencies produced by the parser, and the set of dependencies derived from the human-created f-structure." ></td>
	<td class="line x" title="46:166	Usually, two versions of f-score are calculated: one for all the dependencies for a given input, and a separate one for the subset of predicate-only dependencies." ></td>
	<td class="line oc" title="47:166	In this paper, we use the parser developed by Cahill et al.(2004), which automatically annotates input text with c-structure trees and f-structure dependencies, reaching high precision and recall rates." ></td>
	<td class="line x" title="49:166	4 3 Related work The insensitivity of BLEU and NIST to perfectly legitimate syntactic and lexical variation has been raised, among others, in Callison-Burch et al.(2006), but the criticism is widespread." ></td>
	<td class="line x" title="51:166	Even the creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level (Papineni et al. , 2002)." ></td>
	<td class="line x" title="52:166	A side 3 Other predicate-only dependencies include: apposition, complement, open complement, coordination, determiner, object, second object, oblique, second oblique, oblique agent, possessive, quantifier, relative clause, topic, relative clause pronoun." ></td>
	<td class="line x" title="53:166	The remaining non-predicate dependencies are: adjectival degree, coordination surface form, focus, complementizer forms: if, whether, and that, modal, number, verbal particle, participle, passive, person, pronoun surface form, tense, infinitival clause." ></td>
	<td class="line x" title="54:166	4 http://lfg-demo.computing.dcu.ie/lfgparser.html effect of this phenomenon is that BLEU is less reliable for smaller data sets, so the advantage it provides in the speed of evaluation is to some extent counterbalanced by the time spent by developers on producing a sufficiently large test set in order to obtain a reliable score for their system." ></td>
	<td class="line x" title="55:166	Recently a number of attempts to remedy these shortcomings have led to the development of other automatic MT evaluation metrics." ></td>
	<td class="line x" title="56:166	Some of them concentrate mainly on word order, like General Text Matcher (Turian et al. , 2003), which calculates precision and recall for translationreference pairs, weighting contiguous matches more than non-sequential matches, or Translation Error Rate (Snover et al. , 2005), which computes the number of substitutions, inserts, deletions, and shifts necessary to transform the translation text to match the reference." ></td>
	<td class="line x" title="57:166	Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al. , 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy." ></td>
	<td class="line x" title="58:166	Kauchak and Barzilay (2006) and Owczarzak et al.(2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet5 in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in Owczarzak et al.(2006)." ></td>
	<td class="line x" title="61:166	Another metric making use of synonyms is the linear regression model developed by Russo-Lassner et al.(2005), which makes use of stemming, WordNet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching." ></td>
	<td class="line x" title="63:166	Kulesza and Schieber (2004), on the other hand, train a Support Vector Machine using features like proportion of n-gram matches and word error rate to judge a given translations distance from human-level quality." ></td>
	<td class="line x" title="64:166	Nevertheless, these metrics use only stringbased comparisons, even while taking into consideration reordering." ></td>
	<td class="line x" title="65:166	By contrast, our dependency-based method concentrates on utilizing linguistic structure to establish a comparison between translated sentences and their reference." ></td>
	<td class="line x" title="66:166	5 http://wordnet.princeton.edu/ 82 4 LFG f-structure in MT evaluation The process underlying the evaluation of fstructure quality against a gold standard can be used in automatic MT evaluation as well: we parse the translation and the reference, and then, for each sentence, we check the set of translation dependencies against the set of reference dependencies, counting the number of matches." ></td>
	<td class="line x" title="67:166	As a result, we obtain the precision and recall scores for the translation, and we calculate the f-score for the given pair." ></td>
	<td class="line x" title="68:166	Because we are comparing two outputs that were produced automatically, there is a possibility that the result will not be noise-free." ></td>
	<td class="line x" title="69:166	To assess the amount of noise that the parser may introduce we conducted an experiment where 100 English Europarl sentences were modified by hand in such a way that the position of adjuncts was changed, but the sentence remained grammatical and the meaning was not changed." ></td>
	<td class="line x" title="70:166	This way, an ideal parser should give both the source and the modified sentence the same fstructure, similarly to the case presented in (1)." ></td>
	<td class="line x" title="71:166	The modified sentences were treated like a translation file, and the original sentences played the part of the reference." ></td>
	<td class="line x" title="72:166	Each set was run through the parser." ></td>
	<td class="line x" title="73:166	We evaluated the dependency triples obtained from the translation against the dependency triples for the reference, calculating the f-score, and applied other metrics (TER, METEOR, BLEU, NIST, and GTM) to the set in order to compare scores." ></td>
	<td class="line x" title="74:166	The results, inluding the distinction between f-scores for all dependencies and predicate-only dependencies, appear in Table 1." ></td>
	<td class="line x" title="75:166	baseline modified TER 0.0 6.417 METEOR 1.0 0.9970 BLEU 1.0000 0.8725 NIST 11.5232 11.1704 (96.94%) GTM 100 99.18 dep f-score 100 96.56 dep_preds f-score 100 94.13 Table 1." ></td>
	<td class="line x" title="76:166	Scores for sentences with reordered adjuncts The baseline column shows the upper bound for a given metric: the score which a perfect translation, word-for-word identical to the reference, would obtain.6 In the other column we list the scores that the metrics gave to the translation containing reordered adjunct." ></td>
	<td class="line x" title="77:166	As can be seen, the dependency and predicate-only dependency scores are lower than the perfect 100, reflecting the noise introduced by the parser." ></td>
	<td class="line x" title="78:166	To show the difference between the scoring based on LFG dependencies and other metrics in an ideal situation, we created another set of a hundred sentences with reordered adjuncts, but this time selecting only those reordered sentences that were given the same set of dependencies by the parser (in other words, we simulated having the ideal parser)." ></td>
	<td class="line x" title="79:166	As can be seen in Table 2, other metrics are still unable to tolerate legitimate variation in the position of adjuncts, because the sentence surface form differs from the reference; however, it is not treated as an error by the parser." ></td>
	<td class="line x" title="80:166	baseline modified TER 0.0 7.841 METEOR 1.0 0.9956 BLEU 1.0000 0.8485 NIST 11.1690 10.7422 (96.18%) GTM 100 99.35 dep f-score 100 100 dep_preds f-score 100 100 Table 2." ></td>
	<td class="line x" title="81:166	Scores for sentences with reordered adjuncts in an ideal situation 4.1 Initial experiment  Europarl In the first experiment, we attempted to determine whether the dependency-based measure is biased towards statistical MT output, a problem that has been observed for n-gram-based metrics like BLEU and NIST." ></td>
	<td class="line x" title="82:166	Callison-Burch et al.(2006) report that BLEU and NIST favour n-gram-based MT models such as Pharaoh (Koehn, 2004), so the translations produced by rule-based systems score lower on the automatic evaluation, even though human judges consistently rate their output higher than Pharaohs translation." ></td>
	<td class="line x" title="84:166	Others repeatedly 6 Two things have to be noted here: (1) in case of NIST the perfect score differs from text to text, which is why we provide the percentage points as well, and (2) in case of TER the lower the score, the better the translation, so the perfect translation will receive 0, and there is no upper bound on the score, which makes this particular metric extremely difficult to directly compare with others." ></td>
	<td class="line x" title="85:166	83 observed this tendency in previous research as well; in one experiment, reported in Owczarzak et al.(2006), where the rule-based system Logomedia7 was compared with Pharaoh, BLEU scored Pharaoh 0.0349 points higher, NIST scored Pharaoh 0.6219 points higher, but human judges scored Logomedia output 0.19 points higher (on a 5-point scale)." ></td>
	<td class="line x" title="87:166	4.1.1 Experimental design In order to check for the existence of a bias in the dependency-based metric, we created a set of 4,000 sentences drawn randomly from the SpanishEnglish subset of Europarl (Koehn, 2005), and we produced two translations: one by a rule-based system Logomedia, and the other by the standard phrase-based statistical decoder Pharaoh, using alignments produced by GIZA++8 and the refined word alignment strategy of Och and Ney (2003)." ></td>
	<td class="line x" title="88:166	The translations were scored with a range of metrics: BLEU, NIST, GTM, TER, METEOR, and the dependency-based method." ></td>
	<td class="line x" title="89:166	4.1.2 Adding synonyms Besides the ability to allow syntactic variants as valid translations, a good metric should also be able to accept legitimate lexical variation." ></td>
	<td class="line x" title="90:166	We introduced synonyms and paraphrases into the process of evaluation, creating new best-matching references for the translations using either paraphrases derived from the test set itself (following Owczarzak et al.(2006)) or WordNet synonyms (as in Kauchak and Barzilay (2006))." ></td>
	<td class="line x" title="92:166	Bitext-derived paraphrases Owczarzak et al.(2006) describe a simple way to produce a list of paraphrases, which can be useful in MT evaluation, by running word alignment software on the test set that is being evaluated." ></td>
	<td class="line x" title="94:166	Paraphrases derived in this way are specific to the domain at hand and contain low-level syntactic variants in addition to word-level synonymy." ></td>
	<td class="line x" title="95:166	Using the standard GIZA++ software and the refined word alignment strategy of Och and Ney (2003) on our test set of 4,000 Spanish-English sentences, the method generated paraphrases for just over 1100 items." ></td>
	<td class="line x" title="96:166	These paraphrases served to 7 http://www.lec.com/ 8 http://www.fjoch.com/GIZA++ create new individual best-matching references for the Logomedia and Pharaoh translations." ></td>
	<td class="line x" title="97:166	Due to the small size of the paraphrase set, only about 20% of reference sentences were actually modified to better reflect the translation." ></td>
	<td class="line x" title="98:166	This, in turn, led to little difference in scores." ></td>
	<td class="line x" title="99:166	WordNet synonyms To maximize the number of matches between a translation and a reference, Kauchak and Barzilay (2006) use WordNet synonyms during evaluation." ></td>
	<td class="line x" title="100:166	In addition, METEOR also has an option of including WordNet in the evaluation process." ></td>
	<td class="line x" title="101:166	As in the case of bitext-derived paraphrases, we used WordNet synonyms to create new best-matching references for each of the two translations." ></td>
	<td class="line x" title="102:166	This time, given the extensive database containing synonyms for over 150,000 items, around 70% of reference sentences were modified: 67% for Pharaoh, and 75% for Logomedia." ></td>
	<td class="line x" title="103:166	Note that the number of substitutions is higher for Logomedia; this confirms the intuition that the translation produced by Pharaoh, trained on the domain which is also the source of the reference text, will need fewer lexical replacements than Logomedia, which is based on a general non-domain-specific model." ></td>
	<td class="line x" title="104:166	4.1.3 Results Table 3 shows the difference between the scores which Pharaohs and Logomedias translations obtained from each metric: a positive number shows by how much Pharaohs score was higher than Logomedias, and a negative number reflects Logomedias higher score (the percentages are absolute values)." ></td>
	<td class="line x" title="105:166	As can be seen, all the metrics scored Pharaoh higher, inlcuding METEOR and the dependency-based method that were boosted with WordNet." ></td>
	<td class="line x" title="106:166	The values in the table are sorted in descending order, from the largest to the lowest advantage of Pharaoh over Logomedia." ></td>
	<td class="line x" title="107:166	Interestingly, next to METEOR boosted with WordNet, it is the dependency-based method, and especially the predicates-only version, that shows the least bias towards the phrase-based translation." ></td>
	<td class="line x" title="108:166	In the next step, we selected from this set smaller subsets of sentences that were more and more similar in terms of translation quality (as determined by a sentences BLEU score)." ></td>
	<td class="line x" title="109:166	As the similarity of the translation quality increased, most metrics lowered their bias, as is shown in Table 4." ></td>
	<td class="line x" title="110:166	The first column shows the case where the sentences chosen differed at the most by 0.05 84 points BLEU score; in the second column the difference was lowered to 0.01; and in the third column to 0.005." ></td>
	<td class="line x" title="111:166	The numbers following the hash signs in the header row indicate the number of sentences in a given set." ></td>
	<td class="line x" title="112:166	metric PH score  LM score TER 1.997 BLEU 7.16% NIST 6.58% dep 4.93% dep+paraphr 4.80% GTM 3.89% METEOR 3.80% dep_preds 3.79% dep+paraphr_preds 3.70% dep+WordNet 3.55% dep+WordNet_preds 2.60% METEOR+WordNet 1.56% Table 3." ></td>
	<td class="line x" title="113:166	Difference between scores assigned to Pharaoh and Logomedia." ></td>
	<td class="line x" title="114:166	Positive numbers show by how much Pharaohs score was higher than Logomedias. Legend: dep = dependency f-score, paraph = paraphrases, _preds = predicate-only f-score." ></td>
	<td class="line x" title="115:166	~ 0.05 #1692 ~ 0.01 #567 ~ 0.005 #335 NIST 2.29% NIST 1.76% NIST 1.48% BLEU 0.95% BLEU 0.42% BLEU 0.59% GTM 0.94% GTM 0.29% GTM -0.09% d+p 0.67% d 0.04% d+p -0.15% d 0.61% d+p 0.02% d -0.24% d+WN -0.29% d+WN -0.78% d+WN -0.99% d+p_pr -0.70% M -0.99% d+p_pr -1.30% d_pr -0.75% d_pr -1.37% d_pr -1.43% M -1.03% d+p_pr -1.38% M -1.57% d+WN_pr -1.43% d+WN_pr -1.97% d+WN_pr -1.94% M+WN -2.51% M+WN -2.21% M+WN -2.74% TER -1.579 TER -1.228 TER -1.739 Table 4." ></td>
	<td class="line x" title="116:166	Difference between scores assigned to Pharaoh and Logomedia for sets of increasing similarity." ></td>
	<td class="line x" title="117:166	Positive numbers show Pharaohs advantage, negative numbers show Logomedias advantage." ></td>
	<td class="line x" title="118:166	Legend: d = dependency fscore, p = paraphrases, _pr = predicate-only f-score, M = METEOR, WN = WordNet." ></td>
	<td class="line x" title="119:166	These results confirm earlier suggestions that the predicate-only version of the dependencybased evaluation is less biased in favour of the statistical MT system than the version that includes all dependency types." ></td>
	<td class="line x" title="120:166	Adding a sufficient number of lexical choices reduces the bias even further; although again, paraphrases generated from the test set only are too few to make a significant difference." ></td>
	<td class="line x" title="121:166	Similarly to METEOR, the dependency-based method shows on the whole lower bias than other metrics." ></td>
	<td class="line x" title="122:166	However, we cannot be certain that the underlying scores vary linearly with each other and with human judgements, as we have no framework of reference such as human segment-level assessment of translation quality in this case." ></td>
	<td class="line x" title="123:166	Therefore, the correlation with human judgement is analysed in our next experiment." ></td>
	<td class="line x" title="124:166	4.2 Correlation with human judgement  MultiTrans To calculate how well the dependency-based method correlates with human judgement, and how it compares to the correlation shown by other metrics, we conducted an experiment on ChineseEnglish newswire text." ></td>
	<td class="line x" title="125:166	4.2.1 Experimental design We used the data from the Linguistic Data Consortium Multiple Translation Chinese (MTC) Parts 2 and 4." ></td>
	<td class="line x" title="126:166	The data consists of multiple translations of Chinese newswire text, four humanproduced references, and segment-level human scores for a subset of the translation-reference pairs." ></td>
	<td class="line x" title="127:166	Although a single translated segment was always evaluated by more than one judge, the judges used a different reference every time, which is why we treated each translation-referencehuman score triple as a separate segment." ></td>
	<td class="line x" title="128:166	In effect, the test set created from this data contained 16,800 segments." ></td>
	<td class="line x" title="129:166	As in the previous experiment, the translation was scored using BLEU, NIST, GTM, TER, METEOR, and the dependency-based method." ></td>
	<td class="line x" title="130:166	4.2.2 Results We calculated Pearsons correlation coefficient for segment-level scores that were given by each metric and by human judges." ></td>
	<td class="line x" title="131:166	The results of the correlation are shown in Table 5." ></td>
	<td class="line x" title="132:166	Note that the correlation for TER is negative, because in TER zero is the perfect score, in contrast to other metrics where zero is the worst possible score; however, this time the absolute values can be easily compared to each other." ></td>
	<td class="line x" title="133:166	Rows are ordered 85 by the highest value of the (absolute) correlation with the human score." ></td>
	<td class="line x" title="134:166	First, it seems like none of the metrics is very good at reflecting human fluency judgments; the correlation values in the first column are significantly lower than the correlation with accuracy." ></td>
	<td class="line x" title="135:166	However, the dependency-based method in almost all its versions has decidedly the highest correlation in this area." ></td>
	<td class="line x" title="136:166	This can be explained by the methods sensitivity to the grammatical structure of the sentence: a more grammatical translation is also a translation that is more fluent." ></td>
	<td class="line x" title="137:166	H_FL H_AC H_AVE d+WN 0.168 M+WN 0.294 M+WN 0.255 d 0.162 M 0.278 d+WN 0.244 d+WN_pr 0.162 NIST 0.273 M 0.242 BLEU 0.155 d+WN 0.266 NIST 0.238 d_pr 0.154 GTM 0.260 d 0.236 M+WN 0.153 d 0.257 GTM 0.230 M 0.149 d+WN_pr 0.232 d+WN_pr 0.220 NIST 0.146 d_pr 0.224 d_pr 0.212 GTM 0.146 BLEU 0.199 BLEU 0.197 TER -0.133 TER -0.192 TER -0.182 Table 5." ></td>
	<td class="line x" title="138:166	Pearsons correlation between human scores and evaluation metrics." ></td>
	<td class="line x" title="139:166	Legend: d = dependency f-score, _pr = predicate-only f-score, M = METEOR, WN = WordNet, H_FL = human fluency score, H_AC = human accuracy score, H_AVE = human average score.9 Second, and somewhat surprisingly, in this detailed examination the relative order of the metrics changed." ></td>
	<td class="line x" title="140:166	The predicate-only version of the dependency-based method appears to be less adequate for correlation with human scores than its non-restricted versions." ></td>
	<td class="line x" title="141:166	As to the correlation with human evaluation of translation accuracy, our method currently falls short of METEOR and even NIST." ></td>
	<td class="line x" title="142:166	This is caused by the fact that both METEOR and NIST assign relatively little importance to the position of a specific word in a sentence, therefore rewarding the translation for content rather than linguistic form." ></td>
	<td class="line x" title="143:166	For our dependency-based method, the noise introduced by the parser might be the reason for low correlation: if even one side of the translation-reference pair contains parsing errors, this may lead to a less reliable score." ></td>
	<td class="line x" title="144:166	An obvious solution to this problem, 9 In general terms, an increase of 0.015 between any two scores is significant with a 95% confidence interval." ></td>
	<td class="line x" title="145:166	which we are examining at the moment, is to include a number of best parses for each side of the evaluation." ></td>
	<td class="line x" title="146:166	High correlation with human judgements of fluency and lower correlation with accuracy results in a high second place for our dependency-based method when it comes to the average correlation coefficient." ></td>
	<td class="line x" title="147:166	The WordNet-boosted dependencybased method scores only slightly lower than METEOR with WordNet." ></td>
	<td class="line x" title="148:166	These results are very encouraging, especially as we see a number of ways the dependency-based method could be further developed." ></td>
	<td class="line x" title="149:166	5 Current and future work While the idea of a dependency-based method is a natural step in the direction of a deeper linguistic analysis for MT evaluation, it does require an LFG grammar and parser for the target language." ></td>
	<td class="line x" title="150:166	There are several obvious areas for improvement with respect to the method itself." ></td>
	<td class="line x" title="151:166	First, we would also like to adapt the process of translation-reference dependency comparison to include n-best parsers for the input sentences, as well as some basic transformations which would allow an even deeper logical analysis of input (e.g. passive to active voice transformation)." ></td>
	<td class="line x" title="152:166	Second, we want to repeat both experiments using a paraphrase set derived from a large parallel corpus, rather than the test set, as described in Owczarzak et al.(2006)." ></td>
	<td class="line x" title="154:166	While retaining the advantage of having a similar size to a corresponding set of WordNet synonyms, this set will also capture low-level syntactic variations, which can increase the number of matches and the correlation with human scores." ></td>
	<td class="line x" title="155:166	Finally, we want to take advantage of the fact that the score produced by the dependencybased method is the proportional average of fscores for a group of up to 32 (but usually far fewer) different dependency types." ></td>
	<td class="line x" title="156:166	We plan to implement a set of weights, one for each dependency type, trained in such a way as to maximize the correlation of the final dependency fscore with human evaluation." ></td>
	<td class="line x" title="157:166	6 Conclusions In this paper we present a novel way of evaluating MT output." ></td>
	<td class="line x" title="158:166	So far, all metrics relied on 86 comparing translation and reference on a string level." ></td>
	<td class="line x" title="159:166	Even given reordering, stemming, and synonyms for individual words, current methods are still far from reaching human ability to assess the quality of translation." ></td>
	<td class="line x" title="160:166	Our method compares the sentences on the level of their grammatical structure, as exemplified by their f-structure dependency triples produced by an LFG parser." ></td>
	<td class="line x" title="161:166	The dependency-based method can be further augmented by using paraphrases or WordNet synonyms, and is available in full version and predicate-only version." ></td>
	<td class="line x" title="162:166	In our experiments we showed that the dependency-based method correlates higher than any other metric with human evaluation of translation fluency, and shows high correlation with the average human score." ></td>
	<td class="line x" title="163:166	The use of dependencies in MT evaluation is a rather new idea and requires more research to improve it, but the method shows potential to become an accurate evaluation metric." ></td>
	<td class="line x" title="164:166	Acknowledgements This work was partly funded by Microsoft Ireland PhD studentship 2006-8 for the first author of the paper." ></td>
	<td class="line x" title="165:166	We would also like to thank our reviewers for their insightful comments." ></td>
	<td class="line x" title="166:166	All remaining errors are our own." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-0714
Labelled Dependencies in Machine Translation Evaluation
Owczarzak, Karolina;Van Genabith, Josef;Way, Andy;"></td>
	<td class="line x" title="1:140	Proceedings of the Second Workshop on Statistical Machine Translation, pages 104??11, Prague, June 2007." ></td>
	<td class="line x" title="2:140	c2007 Association for Computational Linguistics Labelled Dependencies in Machine Translation Evaluation Karolina Owczarzak Josef van Genabith Andy Way National Centre for Language Technology School of Computing, Dublin City University Dublin 9, Ireland {owczarzak,josef,away}@computing.dcu.ie Abstract We present a method for evaluating the quality of Machine Translation (MT) output, using labelled dependencies produced by a Lexical-Functional Grammar (LFG) parser." ></td>
	<td class="line x" title="3:140	Our dependencybased method, in contrast to most popular string-based evaluation metrics, does not unfairly penalize perfectly valid syntactic variations in the translation, and the addition of WordNet provides a way to accommodate lexical variation." ></td>
	<td class="line x" title="4:140	In comparison with other metrics on 16,800 sentences of Chinese-English newswire text, our method reaches high correlation with human scores." ></td>
	<td class="line x" title="5:140	1 Introduction Since the creation of BLEU (Papineni et al. , 2002) and NIST (Doddington, 2002), the subject of automatic evaluation metrics for MT has been given quite a lot of attention." ></td>
	<td class="line x" title="6:140	Although widely popular thanks to their speed and efficiency, both BLEU and NIST have been criticized for inadequate accuracy of evaluation at the segment level (Callison-Burch et al. , 2006)." ></td>
	<td class="line x" title="7:140	As string based-metrics, they are limited to superficial comparison of word sequences between a translated sentence and one or more reference sentences, and are unable to accommodate any legitimate grammatical variation when it comes to lexical choices or syntactic structure of the translation, beyond what can be found in the multiple references." ></td>
	<td class="line x" title="8:140	A natural next step in the field of evaluation was to introduce metrics that would better reflect our human judgement by accepting synonyms in the translated sentence or evaluating the translation on the basis of what syntactic features it shares with the reference." ></td>
	<td class="line x" title="9:140	Our method follows and substantially extends the earlier work of Liu and Gildea (2005), who use syntactic features and unlabelled dependencies to evaluate MT quality, outperforming BLEU on segment-level correlation with human judgement." ></td>
	<td class="line x" title="10:140	Dependencies abstract away from the particulars of the surface string (and syntactic tree) realization and provide a ?normalized??representation of (some) syntactic variants of a given sentence." ></td>
	<td class="line x" title="11:140	While Liu and Gildea (2005) calculate n-gram matches on non-labelled head-modifier sequences derived by head-extraction rules from syntactic trees, we automatically evaluate the quality of translation by calculating an f-score on labelled dependency structures produced by a LexicalFunctional Grammar (LFG) parser." ></td>
	<td class="line x" title="12:140	These dependencies differ from those used by Liu and Gildea (2005), in that they are extracted according to the rules of the LFG grammar and they are labelled with a type of grammatical relation that connects the head and the modifier, such as subject, determiner, etc. The presence of grammatical relation labels adds another layer of important linguistic information into the comparison and allows us to account for partial matches, for example when a lexical item finds itself in a correct relation but with an incorrect partner." ></td>
	<td class="line x" title="13:140	Moreover, we use a number of best parses for the translation and the reference, which serves to decrease the amount of noise that can be introduced by the process of parsing and extracting dependency information." ></td>
	<td class="line oc" title="14:140	The translation and reference files are analyzed by a treebank-based, probabilistic LFG parser (Cahill et al. , 2004), which produces a set of dependency triples for each input." ></td>
	<td class="line x" title="15:140	The translation set is compared to the reference set, and the number of matches is calculated, giving the 104 precision, recall, and f-score for each particular translation." ></td>
	<td class="line x" title="16:140	In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score." ></td>
	<td class="line x" title="17:140	In an experiment on 16,800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium?s (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (Turian et al. , 2003), Translation Error Rate (TER) (Snover et al. , 2006)1, and METEOR (Banerjee and Lavie, 2005), and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment." ></td>
	<td class="line x" title="18:140	Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in Liu and Gildea (2005)." ></td>
	<td class="line x" title="19:140	The remainder of this paper is organized as follows: Section 2 gives a basic introduction to LFG; Section 3 describes related work; Section 4 describes our method and gives results of the experiment on the Multiple Translation data; Section 5 discusses ongoing work; Section 6 concludes." ></td>
	<td class="line x" title="20:140	2 Lexical-Functional Grammar In Lexical-Functional Grammar (Kaplan and Bresnan, 1982; Bresnan, 2001) sentence structure is represented in terms of c(onstituent)-structure and f(unctional)-structure." ></td>
	<td class="line x" title="21:140	C-structure represents the word order of the surface string and the hierarchical organisation of phrases in terms of CFG trees." ></td>
	<td class="line x" title="22:140	F-structures are recursive feature (or attribute-value) structures, representing abstract grammatical relations, such as subj(ect), obj(ect), obl(ique), adj(unct), etc. , approximating to predicate-argument structure or simple logical forms." ></td>
	<td class="line x" title="23:140	C-structure and f-structure are related in 1 We omit HTER (Human-Targeted Translation Error Rate), as it is not fully automatic and requires human input." ></td>
	<td class="line x" title="24:140	terms of functional annotations (attribute-value structure equations) in c-structure trees, describing f-structures." ></td>
	<td class="line x" title="25:140	While c-structure is sensitive to surface rearrangement of constituents, f-structure abstracts away from the particulars of the surface realization." ></td>
	<td class="line x" title="26:140	The sentences John resigned yesterday and Yesterday, John resigned will receive different tree representations, but identical f-structures, shown in (1)." ></td>
	<td class="line x" title="27:140	(1) C-structure: F-structure: S NP VP | John V NP-TMP | | resigned yesterday SUBJ PRED john NUM sg PERS 3 PRED resign TENSE past ADJ {[PRED yesterday]} S NP NP VP | | | Yesterday John V | resigned SUBJ PRED john NUM sg PERS 3 PRED resign TENSE past ADJ {[PRED yesterday]} Note that if these sentences were a translationreference pair, they would receive a less-thanperfect score from string-based metrics." ></td>
	<td class="line x" title="28:140	For example, BLEU with add-one smoothing2 gives this pair a score of barely 0.3781." ></td>
	<td class="line x" title="29:140	This is because, although all three unigrams from the ?translation??" ></td>
	<td class="line x" title="30:140	(John; resigned; yesterday) are present in the reference, which contains four items including the comma (Yesterday;,; John; resigned), the ?translation??contains only one bigram (John resigned) that matches the ?reference??(Yesterday,;, John; John resigned), and no matching trigrams." ></td>
	<td class="line x" title="31:140	The f-structure can also be described in terms of a flat set of triples." ></td>
	<td class="line x" title="32:140	In triples format, the fstructure in (1) is represented as follows: {subj(resign, john), pers(john, 3), num(john, sg), tense(resign, past), adj(resign, yesterday), pers(yesterday, 3), num(yesterday, sg)}." ></td>
	<td class="line x" title="33:140	2 We use smoothing because the original BLEU metric gives zero points to sentences with fewer than one fourgram." ></td>
	<td class="line oc" title="34:140	105 Cahill et al.(2004) presents a set of Penn-II Treebank-based LFG parsing resources." ></td>
	<td class="line o" title="36:140	Their approach distinguishes 32 types of dependencies, including grammatical functions and morphological information." ></td>
	<td class="line o" title="37:140	This set can be divided into two major groups: a group of predicate-only dependencies and non-predicate dependencies." ></td>
	<td class="line x" title="38:140	Predicate-only dependencies are those whose path ends in a predicate-value pair, describing grammatical relations." ></td>
	<td class="line x" title="39:140	For example, for the fstructure in (1), predicate-only dependencies would include: {subj(resign, john), adj(resign, yesterday)}." ></td>
	<td class="line x" title="40:140	Other predicate-only dependencies include: apposition, complement, open complement, coordination, determiner, object, second object, oblique, second oblique, oblique agent, possessive, quantifier, relative clause, topic, and relative clause pronoun." ></td>
	<td class="line x" title="41:140	The remaining non-predicate dependencies are: adjectival degree, coordination surface form, focus, complementizer forms: if, whether, and that, modal, number, verbal particle, participle, passive, person, pronoun surface form, tense, and infinitival clause." ></td>
	<td class="line x" title="42:140	In parser evaluation, the quality of the fstructures produced automatically can be checked against a set of gold standard sentences annotated with f-structures by a linguist." ></td>
	<td class="line x" title="43:140	The evaluation is conducted by calculating the precision and recall between the set of dependencies produced by the parser, and the set of dependencies derived from the human-created f-structure." ></td>
	<td class="line x" title="44:140	Usually, two versions of f-score are calculated: one for all the dependencies for a given input, and a separate one for the subset of predicate-only dependencies." ></td>
	<td class="line oc" title="45:140	In this paper, we use the parser developed by Cahill et al.(2004), which automatically annotates input text with c-structure trees and f-structure dependencies, obtaining high precision and recall rates." ></td>
	<td class="line x" title="47:140	3 3 Related work 3.1 String-based metrics The insensitivity of BLEU and NIST to perfectly legitimate syntactic and lexical variation has been raised, among others, in Callison-Burch et al.(2006), but the criticism is widespread." ></td>
	<td class="line x" title="49:140	Even the 3 A demo of the parser can be found at http://lfgdemo.computing.dcu.ie/lfgparser.html creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level (Papineni et al. , 2002)." ></td>
	<td class="line x" title="50:140	Recently a number of attempts to remedy these shortcomings have led to the development of other automatic MT evaluation metrics." ></td>
	<td class="line x" title="51:140	Some of them concentrate mainly on word order, like General Text Matcher (Turian et al. , 2003), which calculates precision and recall for translationreference pairs, weighting contiguous matches more than non-sequential matches, or Translation Error Rate (Snover et al. , 2006), which computes the number of substitutions, insertions, deletions, and shifts necessary to transform the translation text to match the reference." ></td>
	<td class="line x" title="52:140	Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al. , 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy." ></td>
	<td class="line x" title="53:140	Kauchak and Barzilay (2006) and Owczarzak et al.(2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet4 in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in Owczarzak et al.(2006)." ></td>
	<td class="line x" title="56:140	Another metric making use of synonyms is the linear regression model developed by Russo-Lassner et al.(2005), which makes use of stemming, WordNet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching." ></td>
	<td class="line x" title="58:140	Kulesza and Shieber (2004), on the other hand, train a Support Vector Machine using features such as proportion of ngram matches and word error rate to judge a given translation?s distance from human-level quality." ></td>
	<td class="line x" title="59:140	3.2 Dependency-based metric The metrics described above use only string-based comparisons, even while taking into consideration reordering." ></td>
	<td class="line x" title="60:140	By contrast, Liu and Gildea (2005) present three metrics that use syntactic and unlabelled dependency information." ></td>
	<td class="line x" title="61:140	Two of these metrics are based on matching syntactic subtrees between the translation and the reference, and one 4 http://wordnet.princeton.edu/ 106 is based on matching headword chains, i.e. sequences of words that correspond to a path in the unlabelled dependency tree of the sentence." ></td>
	<td class="line x" title="62:140	Dependency trees are created by extracting a headword for each node of the syntactic tree, according to the rules used by the parser of Collins (1999), where every subtree represents the modifier information for its root headword." ></td>
	<td class="line x" title="63:140	The dependency trees for the translation and the reference are converted into flat headword chains, and the number of overlapping n-grams between the translation and the reference chains is calculated." ></td>
	<td class="line x" title="64:140	Our method, extending this line of research with the use of labelled LFG dependencies, partial matching, and n-best parses, allows us to considerably outperform Liu and Gildea?s (2005) highest correlations with human judgement (they report 0.144 for the correlation with human fluency judgement, 0.202 for the correlation with human overall judgement), although it has to be kept in mind that such comparison is only tentative, as their correlation is calculated on a different test set." ></td>
	<td class="line x" title="65:140	4 LFG f-structure in MT evaluation LFG-based automatic MT evaluation reflects the same process that underlies the evaluation of parser-produced f-structure quality against a gold standard: we parse the translation and the reference, and then, for each sentence, we check the set of labelled translation dependencies against the set of labelled reference dependencies, counting the number of matches." ></td>
	<td class="line x" title="66:140	As a result, we obtain the precision and recall scores for the translation, and we calculate the f-score for the given pair." ></td>
	<td class="line x" title="67:140	4.1 Determining parser noise Because we are comparing two outputs that were produced automatically, there is a possibility that the result will not be noise-free, even if the parser fails to provide a parse only in 0.1% of cases." ></td>
	<td class="line x" title="68:140	To assess the amount of noise that the parser introduces, Owczarzak et al.(2006) conducted an experiment where 100 English sentences were hand-modified so that the position of adjuncts was changed, but the sentence remained grammatical and the meaning was not influenced." ></td>
	<td class="line x" title="70:140	This way, an ideal parser should give both the source and the modified sentence the same f-structure, similarly to the example presented in (1)." ></td>
	<td class="line x" title="71:140	The modified sentences were treated like a translation file, and the original sentences played the part of the reference." ></td>
	<td class="line x" title="72:140	Each set was run through the parser, and the dependency triples obtained from the ?translation??were compared against the dependency triples for the ?reference??" ></td>
	<td class="line x" title="73:140	calculating the f-score." ></td>
	<td class="line x" title="74:140	Additionally, the same ?translationreference??set was scored with other metrics (TER, METEOR, BLEU, NIST, and GTM)." ></td>
	<td class="line x" title="75:140	The results, including the distinction between f-scores for all dependencies and predicate-only dependencies, appear in Table 1." ></td>
	<td class="line x" title="76:140	baseline modified TER 0.0 6.417 METEOR 1.0 0.9970 BLEU 1.0000 0.8725 NIST 11.5232 11.1704 (96.94%) GTM 100 99.18 dep f-score 100 96.56 dep_preds f-score 100 94.13 Table 1." ></td>
	<td class="line x" title="77:140	Scores for sentences with reordered adjuncts The baseline column shows the upper bound for a given metric: the score which a perfect translation, word-for-word identical to the reference, would obtain.5 The other column lists the scores that the metrics gave to the ?translation??containing reordered adjunct." ></td>
	<td class="line x" title="78:140	As can be seen, the dependency and predicate-only dependency scores are lower than the perfect 100, reflecting the noise introduced by the parser." ></td>
	<td class="line x" title="79:140	We propose that the problem of parser noise can be alleviated by introducing a number of best parses into the comparison between the translation and the reference." ></td>
	<td class="line x" title="80:140	Table 2 shows how increasing the number of parses available for comparison brings our method closer to an ideal noise-free parser." ></td>
	<td class="line x" title="81:140	5 Two things have to be noted here: (1) in the case of NIST the perfect score differs from text to text, which is why the percentage points are provided along the numerical score, and (2) in the case of TER the lower the score, the better the translation, so the perfect translation will receive 0, and there is no upper bound on the score, which makes this particular metric extremely difficult to directly compare with others." ></td>
	<td class="line x" title="82:140	107 dependency f-score 1 best 96.56 2 best 97.31 5 best 97.90 10 best 98.31 20 best 98.59 30 best 98.74 50 best 98.79 baseline 100 Table 2." ></td>
	<td class="line x" title="83:140	Dependency f-scores for sentences with reordered adjuncts with n-best parses available It has to be noted, however, that increasing the number of parses beyond a certain threshold does little to further improve results, and at the same time it considerably decreases the efficiency of the method, so it is important to find the right balance between these two factors." ></td>
	<td class="line x" title="84:140	In our opinion, the optimal value would be 10-best parses." ></td>
	<td class="line x" title="85:140	4.2 Correlation with human judgement ??" ></td>
	<td class="line x" title="86:140	MultiTrans 4.2.1 Experimental design To evaluate the correlation with human assessment, we used the data from the Linguistic Data Consortium Multiple Translation Chinese (MTC) Parts 2 and 4, which consists of multiple translations of Chinese newswire text, four humanproduced references, and segment-level human scores for a subset of the translation-reference pairs." ></td>
	<td class="line x" title="87:140	Although a single translated segment was always evaluated by more than one judge, the judges used a different reference every time, which is why we treated each translation-referencehuman score triple as a separate segment." ></td>
	<td class="line x" title="88:140	In effect, the test set created from this data contained 16,800 segments." ></td>
	<td class="line x" title="89:140	As in the previous experiment, the translation was scored using BLEU, NIST, GTM, TER, METEOR, and our labelled dependencybased method." ></td>
	<td class="line x" title="90:140	4.2.2 Labelled dependency-based method We examined a number of modifications of the dependency-based method in order to find out which one gives the highest correlation with human scores." ></td>
	<td class="line x" title="91:140	The correlation differences between immediate neighbours in the ranking were often too small to be statistically significant; however, there is a clear overall trend towards improvement." ></td>
	<td class="line x" title="92:140	Besides the plain version of the dependency fscore, we also looked at the f-score calculated on predicate dependencies only (ignoring ?atomic??" ></td>
	<td class="line x" title="93:140	features such as person, number, tense, etc.), which turned out not to correlate well with human judgements." ></td>
	<td class="line x" title="94:140	Another addition was the use of 2-, 10-, or 50best parses of the translation and reference sentences, which partially neutralized parser noise and resulted in increased correlations." ></td>
	<td class="line x" title="95:140	We also created a version where predicate dependencies of the type subj(resign,John) are split into two parts, each time replacing one of the elements participating in the relation with a variable, giving in effect subj(resign,x) and subj(y,John)." ></td>
	<td class="line x" title="96:140	This lets us score partial matches, where one correct lexical object happens to find itself in the correct relation, but with an incorrect ?partner??" ></td>
	<td class="line x" title="97:140	Lastly, we added WordNet synonyms into the matching process to accommodate lexical variation, and to compare our WordNet-enhanced method with the WordNet-enhanced version of METEOR." ></td>
	<td class="line x" title="98:140	4.2.3 Results We calculated Pearson?s correlation coefficient for segment-level scores that were given by each metric and by human judges." ></td>
	<td class="line x" title="99:140	The results of the correlation are shown in Table 3." ></td>
	<td class="line x" title="100:140	Note that the correlation for TER is negative, because in TER zero is the perfect score, in contrast to other metrics where zero is the worst possible score; however, this time the absolute values can be easily compared to each other." ></td>
	<td class="line x" title="101:140	Rows are ordered by the highest value of the (absolute) correlation with the human score." ></td>
	<td class="line x" title="102:140	First, it seems like none of the metrics is very good at reflecting human fluency judgments; the correlation values in the first column are significantly lower than the correlation with accuracy." ></td>
	<td class="line x" title="103:140	This finding has been previously reported, among others, in Liu and Gildea (2005)." ></td>
	<td class="line x" title="104:140	However, the dependency-based method in almost all its versions has decidedly the highest correlation in this area." ></td>
	<td class="line x" title="105:140	This can be explained by the method?s sensitivity to the grammatical structure of the sentence: a more grammatical translation is also a translation that is more fluent." ></td>
	<td class="line x" title="106:140	As to the correlation with human evaluation of translation accuracy, our method currently falls 108 short of METEOR." ></td>
	<td class="line x" title="107:140	This is caused by the fact that METEOR assign relatively little importance to the position of a specific word in a sentence, therefore rewarding the translation for content rather than linguistic form." ></td>
	<td class="line x" title="108:140	Interestingly, while METEOR, with or without WordNet, considerably outperforms all other metrics when it comes to the correlation with human judgements of translation accuracy, it falls well behind most versions of our dependency-based method in correlation with human scores of translation fluency." ></td>
	<td class="line x" title="109:140	Surprisingly, adding partial matching to the dependency-based method resulted in the greatest increase in correlation levels, to the extent that the partial-match versions consistently outperformed versions with a larger number of parses available but without the partial match." ></td>
	<td class="line x" title="110:140	The most interesting effect was that the partial-match versions (even those with just a single parse) offered results comparable to or higher than the addition of WordNet to the matching process when it comes to accuracy and overall judgement." ></td>
	<td class="line x" title="111:140	5 Current and future work Fluency and accuracy are two very different aspects of translation quality, each with its own set of conditions along which the input is evaluated." ></td>
	<td class="line x" title="112:140	Therefore, it seems unfair to expect a single automatic metric to correlate highly with human judgements of both at the same time." ></td>
	<td class="line x" title="113:140	This pattern is very noticeable in Table 3: if a metric is (relatively) good at correlating with fluency, its accuracy correlation suffers (GTM might serve as an example here), and the opposite holds as well (see METEOR?s scores)." ></td>
	<td class="line x" title="114:140	It does not mean that any improvement that increases the method?s correlation with one aspect will result in a decrease in the correlation with the other aspect; but it does suggest that a possible way of development would be to target these correlations separately, if we want our automated metrics to reflect human scores better." ></td>
	<td class="line x" title="115:140	At the same time, string-based metrics might have already exhausted their potential when it comes to increasing their correlation with human evaluation; as has been pointed out before, these metrics can only tell us that two strings differ, but they cannot distinguish legitimate grammatical variance from ungrammatical variance." ></td>
	<td class="line x" title="116:140	As the quality of MT Table 3." ></td>
	<td class="line x" title="117:140	Pearson?s correlation between human scores and evaluation metrics." ></td>
	<td class="line x" title="118:140	Legend: d = dependency f-score, _pr = predicate-only f-score, 2, 10, 50 = n-best parses; var = partial-match version; M = METEOR, WN = WordNet6 improves, the community will need metrics that are more sensitive in this respect." ></td>
	<td class="line x" title="119:140	After all, the true quality of MT depends on producing grammatical output which describes the same concept as the source utterance, and the string identity with a reference is only a very selective approximation of this goal." ></td>
	<td class="line x" title="120:140	6 In general terms, an increase of 0.022 or more between any two scores in the same column is significant with a 95% confidence interval." ></td>
	<td class="line x" title="121:140	The statistical significance of correlation differences was calculated using Fisher?s z??" ></td>
	<td class="line x" title="122:140	transformation and the general formula for confidence interval." ></td>
	<td class="line x" title="123:140	fluency accuracy average d_50+WN 0.177 M+WN 0.294 M+WN 0.255 d+WN 0.175 M 0.278 d_50_var 0.252 d_50_var 0.174 d_50_var 0.273 d_50+WN 0.250 GTM 0.172 NIST 0.273 d_10_var 0.250 d_10_var 0.172 d_10_var 0.273 d_2_var 0.247 d_50 0.171 d_2_var 0.270 d+WN 0.244 d_2_var 0.168 d_50+WN 0.269 d_50 0.243 d_10 0.168 d_var 0.266 d_var 0.243 d_var 0.165 d_50 0.262 M 0.242 d_2 0.164 d_10 0.262 d_10 0.242 d 0.161 d+WN 0.260 NIST 0.238 BLEU 0.155 d_2 0.257 d_2 0.237 M+WN 0.153 d 0.256 d 0.235 M 0.149 d_pr 0.240 d_pr 0.216 NIST 0.146 GTM 0.203 GTM 0.208 d_pr 0.143 BLEU 0.199 BLEU 0.197 TER -0.133 TER -0.192 TER -0.182 109 In order to maximize the correlation with human scores of fluency, we plan to look more closely at the parser output, and implement some basic transformations which would allow an even deeper logical analysis of input (e.g. passive to active voice transformation)." ></td>
	<td class="line x" title="124:140	Additionally, we want to take advantage of the fact that the score produced by the dependencybased method is the proportional average of matches for a group of up to 32 (but usually far fewer) different dependency types." ></td>
	<td class="line x" title="125:140	We plan to implement a set of weights, one for each dependency type, trained in such a way as to maximize the correlation of the final dependency fscore with human evaluation." ></td>
	<td class="line x" title="126:140	In a preliminary experiment, for example, assigning a low weight to the topic dependency increases our correlations slightly (this particular case can also be seen as a transformation into a more basic logical form by removing non-elementary dependency types)." ></td>
	<td class="line x" title="127:140	In a similar direction, we want to experiment more with the f-score calculations." ></td>
	<td class="line x" title="128:140	Initial check shows that assigning a higher weight to recall than to precision improves results." ></td>
	<td class="line x" title="129:140	To improve the correlation with accuracy judgements, we would like to experiment using a paraphrase set derived from a large parallel corpus, as described in Owczarzak et al.(2006)." ></td>
	<td class="line x" title="131:140	While retaining the advantage of having a similar size to a corresponding set of WordNet synonyms, this set will also capture low-level syntactic variations, which can increase the number of matches." ></td>
	<td class="line x" title="132:140	6 Conclusions In this paper we present a linguisticallymotivated method for automatically evaluating the output of Machine Translation." ></td>
	<td class="line x" title="133:140	Most currently used popular metrics rely on comparing translation and reference on a string level." ></td>
	<td class="line x" title="134:140	Even given reordering, stemming, and synonyms for individual words, current methods are still far from reaching human ability to assess the quality of translation, and there exists a need in the community to develop more dependable metrics." ></td>
	<td class="line x" title="135:140	Our method explores one such direction of development, comparing the sentences on the level of their grammatical structure, as exemplified by their fstructure labelled dependency triples produced by an LFG parser." ></td>
	<td class="line x" title="136:140	In our experiments we showed that the dependency-based method correlates higher than any other metric with human evaluation of translation fluency, and shows high correlation with the average human score." ></td>
	<td class="line x" title="137:140	The use of dependencies in MT evaluation has not been extensively researched before (one exception here would be Liu and Gildea (2005)), and requires more research to improve it, but the method shows potential to become an accurate evaluation metric." ></td>
	<td class="line x" title="138:140	Acknowledgements This work was partly funded by Microsoft Ireland PhD studentship 2006-8 for the first author of the paper." ></td>
	<td class="line x" title="139:140	We would also like to thank our reviewers and Dan Melamed for their insightful comments." ></td>
	<td class="line x" title="140:140	All remaining errors are our own." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-2206
Improving the Efficiency of a Wide-Coverage CCG Parser
Djordjevic, Bojan;Curran, James R.;Clark, Stephen;"></td>
	<td class="line x" title="1:208	Proceedings of the 10th Conference on Parsing Technologies, pages 3947, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:208	c2007 Association for Computational Linguistics Improving the Efficiency of a Wide-Coverage CCG Parser Bojan Djordjevic and James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia {bojan,james}@it.usyd.edu.au Stephen Clark Computing Laboratory Oxford University Wolfson Building, Parks Road Oxford, OX1 3QD, UK stephen.clark@comlab.ox.ac.uk Abstract The C&C CCG parser is a highly efficient linguistically motivated parser." ></td>
	<td class="line x" title="3:208	The efficiency is achieved using a tightly-integrated supertagger, which assigns CCG lexical categories to words in a sentence." ></td>
	<td class="line x" title="4:208	The integration allows the parser to request more categories if it cannot find a spanning analysis." ></td>
	<td class="line x" title="5:208	We present several enhancements to the CKY chart parsing algorithm used by the parser." ></td>
	<td class="line x" title="6:208	The first proposal is chart repair, which allows the chart to be efficiently updated by adding lexical categories individually, and we evaluate several strategies for adding these categories." ></td>
	<td class="line x" title="7:208	The second proposal is to add constraints to the chart which require certain spans to be constituents." ></td>
	<td class="line x" title="8:208	Finally, weproposepartialbeamsearchtofurther reduce the search space." ></td>
	<td class="line x" title="9:208	Overall, the parsing speed is improved by over 35% with negligible loss of accuracy or coverage." ></td>
	<td class="line oc" title="10:208	1 Introduction A recent theme in parsing research has been the application of statistical methods to linguistically motivated grammars, for example LFG (Kaplan et al. , 2004; Cahill et al. , 2004), HPSG (Toutanova et al. , 2002; Malouf and van Noord, 2004), TAG (Sarkar and Joshi, 2003) and CCG (Hockenmaier andSteedman,2002; ClarkandCurran,2004b)." ></td>
	<td class="line x" title="11:208	The attraction of linguistically motivated parsers is the potential to produce rich output, in particular the predicate-argumentstructurerepresentingtheunderlying meaning of a sentence." ></td>
	<td class="line x" title="12:208	The disadvantage of such parsers is that they are typically not very efficient, parsing a few sentences per second on commodity hardware (Kaplan et al. , 2004)." ></td>
	<td class="line x" title="13:208	The C&C CCG parser (Clark and Curran, 2004b) is an order of magnitude faster, but is still limited to around 25 sentences per second." ></td>
	<td class="line x" title="14:208	The key to efficient CCG parsing is a finite-state supertagger which performs much of the parsing work (Bangalore and Joshi, 1999)." ></td>
	<td class="line x" title="15:208	CCG is a lexicalised grammar formalism, in which elementary syntactic structures  in CCGs case lexical categories expressing subcategorisation information  are assigned to the words in a sentence." ></td>
	<td class="line x" title="16:208	CCG supertagging can be performed accurately and efficiently by a Maximum Entropy tagger (Clark and Curran, 2004a)." ></td>
	<td class="line x" title="17:208	Since the lexical categories contain so much grammatical information, assigning them with low average ambiguity leaves the parser, which combines them together, with much less work to do at parse time." ></td>
	<td class="line x" title="18:208	Hence Bangalore and Joshi (1999), in the context of LTAG parsing, refer to supertagging as almost parsing." ></td>
	<td class="line x" title="19:208	Clark and Curran (2004a) presents a novel method of integrating the supertagger and parser: initially only a small number of categories, on average, is assigned to each word, and the parser attempts to find a spanning analysis using the CKY chart-parsing algorithm." ></td>
	<td class="line x" title="20:208	If one cannot be found, the parserrequestsmorecategoriesfromthesupertagger andbuildsthechartagainfromscratch." ></td>
	<td class="line x" title="21:208	Thisprocess repeats until the parser is able to build a chart containing a spanning analysis.1 1Tsuruoka and Tsujii (2004) investigate a similar idea in the context of the CKY algorithm for a PCFG." ></td>
	<td class="line x" title="22:208	39 The supertagging accuracy is high enough that the parser fails to find a spanning analysis using the initial category assignment in approximately 4% of Wall Street Journal sentences (?)." ></td>
	<td class="line x" title="23:208	However, parsing this 4%, which largely consists of the longer sentences, is disproportionately expensive." ></td>
	<td class="line x" title="24:208	This paper describes several modifications to the C&C parser which improve parsing efficiency without reducing accuracy or coverage by reducing the impact of the longer sentences." ></td>
	<td class="line x" title="25:208	The first involves chart repair, where the CKY chart is repaired when extra lexical categories are added (according to the scheme described above), instead of being rebuilt from scratch." ></td>
	<td class="line x" title="26:208	This allows an even tighter integration of the supertagger, in that the parser is able to request individual categories." ></td>
	<td class="line x" title="27:208	We explore methods for choosing which individual categories to add, resulting in an 11% speed improvement." ></td>
	<td class="line x" title="28:208	The next modification involves parsing with constraints, so that certain spans are required to be constituents." ></td>
	<td class="line x" title="29:208	This reduces the search space considerably by eliminating a large number of constituents which cross the boundaries of these spans." ></td>
	<td class="line x" title="30:208	The best set of constraints results in a 10% speed improvement over the original parser." ></td>
	<td class="line x" title="31:208	These constraints are general enough that they could be applied to any constituency-based parser." ></td>
	<td class="line x" title="32:208	Finally, we experiment with several beam strategies to reduce the search space, finding that a partial beam which operates on part of the chart is most effective, giving a further 6.1% efficiency improvement." ></td>
	<td class="line x" title="33:208	The chart repair and constraints interact in an interesting, and unexpected, manner when combined, giving a 35.7% speed improvement overall without any loss in accuracy or coverage." ></td>
	<td class="line x" title="34:208	This speed improvement is particularly impressive because it involves techniques which only apply to 4% of Wall Street Journal sentences." ></td>
	<td class="line x" title="35:208	2 The CCG Parser Clark and Curran (2004b) describes the CCG parser." ></td>
	<td class="line x" title="36:208	The grammar used by the parser is extracted from CCGbank, a CCG version of the Penn Treebank (Hockenmaier, 2003)." ></td>
	<td class="line x" title="37:208	The grammar consists of 425 lexical categories plus a small number of combinatory rules which combine the categories (Steedman, 2000)." ></td>
	<td class="line x" title="38:208	A Maximum Entropy supertagger first assigns lexical categories to the words in a sentence, which are then combined by the parser using the combinatory rules." ></td>
	<td class="line x" title="39:208	A log-linear model scores the alternative parses." ></td>
	<td class="line x" title="40:208	We use the normal-form model, which assigns probabilities to single derivations based on the normal-form derivations in CCGbank." ></td>
	<td class="line x" title="41:208	The features in the model are defined over local parts of the derivation and include word-word dependencies." ></td>
	<td class="line x" title="42:208	A packed chart representation allows efficient decoding, with the Viterbi algorithm finding the most probable derivation." ></td>
	<td class="line x" title="43:208	The supertagger uses a log-linear model to define a distribution over the lexical category set for each word and the previous two categories (Ratnaparkhi, 1996) and the forward backward algorithm efficiently sums over all histories to give a distribution for each word." ></td>
	<td class="line x" title="44:208	These distributions are then used to assign a set of lexical categories to each word (?)." ></td>
	<td class="line x" title="45:208	The number of categories in each set is determined by a parameter : all categories are assigned whose forward-backward probabilities are within  of the highest probability category (?)." ></td>
	<td class="line x" title="46:208	If the parser cannot then find a spanning analysis, the value of  is reduced  so that more lexical categories are assigned  and the parser tries again." ></td>
	<td class="line x" title="47:208	This process repeats until an analysis spanning the whole sentence is found." ></td>
	<td class="line x" title="48:208	In our previous work, when the parser was unable to find a spanning analysis, the chart was destroyed and then rebuilt from scratch with more lexical categories assigned to each word." ></td>
	<td class="line x" title="49:208	However, this rebuilding process is wasteful because the new chart is always a superset of the old one and could be created by just updating the previous chart." ></td>
	<td class="line x" title="50:208	We describe the chart repair process in Section 3 which allows additional categories to be assigned to an existing chart and the CKY algorithm run over just those parts of the chart which require modification." ></td>
	<td class="line x" title="51:208	2.1 Chart Parsing The parser uses the CKY chart parsing algorithm (Kasami, 1965; Younger, 1967) described in Steedman (2000)." ></td>
	<td class="line x" title="52:208	The CKY algorithm applies naturally to CCG since the grammar is binary." ></td>
	<td class="line x" title="53:208	It builds the chart bottom-up, starting with the lexical categories spanningsinglewords, incrementallyincreasingthespan until the whole sentence is covered." ></td>
	<td class="line x" title="54:208	Since the constituentsarebuiltinorderofspansize, ateverystage 40 all the sub-constituents which could be used to create a particular new constituent are already present in the chart." ></td>
	<td class="line x" title="55:208	Thechartsarepackedbygroupingtogetherequivalent chart entries, which allows a large number of derivations to be represented efficiently." ></td>
	<td class="line x" title="56:208	Entries are equivalent when they interact in the same manner with both the generation of subsequent parse structure and the statistical parse selection." ></td>
	<td class="line x" title="57:208	In practice, this means that equivalent entries have the same span; form the same structures, i.e. the remaining derivation plus dependencies, in any subsequent parsing; and generate the same features in any subsequent parsing." ></td>
	<td class="line x" title="58:208	The Viterbi algorithm is used to find the most probable derivation from a packed chart." ></td>
	<td class="line x" title="59:208	For each equivalenceclassofindividualentries,werecordthe entry at the root of the subderivation which has the highest score for the class." ></td>
	<td class="line x" title="60:208	The equivalence classes are defined so that any other individual entry cannot be part of the highest scoring derivation for the sentence." ></td>
	<td class="line x" title="61:208	The highest-scoring subderivations can be calculated recursively using the highest-scoring equivalence classes that were combined to create the individual entry." ></td>
	<td class="line x" title="62:208	Given a sentence of n words, we define pos  {0,,n  1} to be the starting position of an entry in the chart (represented by a CCG category) and span  {1,,n} its length." ></td>
	<td class="line x" title="63:208	Let cell(pos,span) bethesetofcategorieswhichspanthesentencefrom pos to pos + span." ></td>
	<td class="line x" title="64:208	These will be combinations of categoriesincell(pos,k)andcell(pos+k,spank) for all k  {1,,span1}." ></td>
	<td class="line x" title="65:208	The chart is a two dimensionalarrayindexedbyposandspan." ></td>
	<td class="line x" title="66:208	Thevalid (pos,span) pairs correspond to pos + span  n, that is, to spans that do not extend beyond the end of the sentence." ></td>
	<td class="line x" title="67:208	The squares represent valid cells in Figure 1." ></td>
	<td class="line x" title="68:208	The span from position 3 with length 4, i.e. cell(3,4), is marked with a diamond in Figure 2." ></td>
	<td class="line x" title="69:208	3 Chart Repair Theparserinteractswiththesupertaggerbydecreasing the value of the  parameter when a spanning analysis cannot be found for a sentence." ></td>
	<td class="line x" title="70:208	This has the effect of adding more lexical categories to the chart." ></td>
	<td class="line x" title="71:208	Instead of rebuilding the chart from scratch when new categories are added, it can be repaired affected cells cell with a newcategory added10 5 0 1 2 3 4 5 6 7 8 9 1 span pos 2 3 4 6 7 8 9 Figure 1: Cells affected by chart repair." ></td>
	<td class="line x" title="72:208	by modifying cells that are affected by the new categories." ></td>
	<td class="line x" title="73:208	Considering the case where a single lexical category is added to the ith word in an n word sentence, the new category can only affect the cells that satisfy pos  i and pos+span > i. These cells are shown in Figure 1 for the word at position 3." ></td>
	<td class="line x" title="74:208	Thenumberofaffectedcellsis(npos)(pos+1), and so the average over the sentence is approximately 1n integraltextn10 (n  p)(p + 1) dp  n26 cells." ></td>
	<td class="line x" title="75:208	The totalnumberofcellsinthechartis n(n+1)2." ></td>
	<td class="line x" title="76:208	Thechart can therefore be repaired bottom up, in CKY order, by updating a third of the cells on average." ></td>
	<td class="line x" title="77:208	Additional lexical categories for a word are inserted into the corresponding cell in the bottom row, with the additional categories being marked as new." ></td>
	<td class="line x" title="78:208	For each cell C in the second row, each pair of cells A and B is considered whose spans combine to create the span of C. In the original CKY, all categories from A are combined with all categories from B. In chart repair, categories are only combined if at least one of them is new, because otherwise the result is already inC. The categories added toC are marked, and the process is repeated for all affected cells in CKY order." ></td>
	<td class="line x" title="79:208	Chart repair speeds up parsing for two reasons." ></td>
	<td class="line x" title="80:208	First, it reuses previous computations and eliminates wasteful rebuilding of the chart." ></td>
	<td class="line x" title="81:208	Second, it allows lexical categories to be added to the chart one at a 41 affected cells invalid cells required cell 6 7 8 9 1 span pos 2 3 4 6 7 8 9 10 5 0 1 2 3 4 5 Figure 2: Cells affected by adding a constraint." ></td>
	<td class="line x" title="82:208	timeuntilaspanningderivationisfound." ></td>
	<td class="line x" title="83:208	Intheoriginal approach extra categories were added in bulk by changing the  level, which significantly increased the average ambiguity." ></td>
	<td class="line x" title="84:208	Chart repair allows the minimum amount of ambiguity to be added for a spanning derivation to be found." ></td>
	<td class="line x" title="85:208	The C&C parserhasapredefinedlimitonthenumber of categories in the chart." ></td>
	<td class="line x" title="86:208	If this is exceeded before a spanning analysis is found then the parser fails on the sentence." ></td>
	<td class="line x" title="87:208	Our new strategy allows a chart containing a spanning analysis to be built with the minimum number of categories possible." ></td>
	<td class="line x" title="88:208	This means that some sentences can now be parsed that would have previously exceeded the limit, slightly increasing coverage." ></td>
	<td class="line x" title="89:208	3.1 Category selection The order in which lexical categories are added to the chart will impact on parsing speed and accuracy, and so we evaluate several alternatives." ></td>
	<td class="line x" title="90:208	The first ordering ( VALUE) is by decreasing  value, where the  value is the ratio between the probability of the most likely category and the probability of the given category for that word.2 The second ordering (PROB) is by decreasing category probability 2We are overloading the use of  for convenience." ></td>
	<td class="line x" title="91:208	Here,  refers to the variable ratio dependent on the particular category, whereas the  value used in supertagging is a cutoff applied to the variable ratio." ></td>
	<td class="line x" title="92:208	as assigned by the supertagger using the forwardbackward algorithm." ></td>
	<td class="line x" title="93:208	We also investigated ordering categories using information from the chart." ></td>
	<td class="line x" title="94:208	Examining the sentences which required chart repair showed that, when a word is missing the correct category, the cells affected (as defined in Section 3) by the cell are often empty." ></td>
	<td class="line x" title="95:208	The CHART orderingusesthisobservationto select the next lexical category to assign." ></td>
	<td class="line x" title="96:208	It selects the word corresponding to the cell with the highest number of empty affected cells, and then adds the highest probability category not in the chart for thatword." ></td>
	<td class="line x" title="97:208	Finally, weincludeda RANDOM ordering baseline for comparison purposes." ></td>
	<td class="line x" title="98:208	4 Constraints The set of possible derivations can be constrained if we know in advance that a particular span is required to be the yield of a single constituent in the correct parse." ></td>
	<td class="line x" title="99:208	A constraint on span p reduces the search space because p must be the yield of a single cell." ></td>
	<td class="line x" title="100:208	This means that cells with yields that cross the boundary of p cannot be part of a correct derivation, and do not need to be considered (the grey cells in Figure 2)." ></td>
	<td class="line x" title="101:208	In addition, if a cell yields p as a prefix or suffix (the hashed cells in Figure 2) then it also has constraints on how it can be created." ></td>
	<td class="line x" title="102:208	Figure 2 shows an example constraint requiring words 36 to be a constituent, which corresponds to p = cell(3,4)." ></td>
	<td class="line x" title="103:208	Consider cell(3,7): it yields words 39 and so contains p as the prefix." ></td>
	<td class="line x" title="104:208	Normally it can be created by combining cell(3,1) with cell(4,6), orcell(3,2)withcell(5,5),andsoonuptocell(3,6) withcell(9,1)." ></td>
	<td class="line x" title="105:208	Howeverthefirstthreecombinations are not allowed because the second child crosses the boundary of p. This gives a lower limit for the span of the left child." ></td>
	<td class="line x" title="106:208	Similarly, if p is the suffix of the span of a cell then there is a lower limit on the span of the right child." ></td>
	<td class="line x" title="107:208	As the example demonstrates, a single constraint can eliminate many combinations, reducing the search space significantly, and thus improving parsing efficiency." ></td>
	<td class="line x" title="108:208	4.1 Creating Constraints Howcanweknowinadvancethatthecorrectderivation must yield specific spans, since this appears to require knowledge of the parse itself?" ></td>
	<td class="line x" title="109:208	We have ex42 plored constraints derived from shallow parsing and from the raw sentence." ></td>
	<td class="line x" title="110:208	Our results demonstrate that simple constraints can reduce parsing time significantly without loss of coverage or accuracy." ></td>
	<td class="line x" title="111:208	Chunk tags were used to create constraints." ></td>
	<td class="line x" title="112:208	We experimented with both gold standard chunks from the Penn Treebank and also chunker output from the C&C chunk tagger." ></td>
	<td class="line x" title="113:208	The tagger is very similar to the Maximum Entropy POS tagger described in Curran and Clark (2003)." ></td>
	<td class="line x" title="114:208	Only NP chunks were used because the accuracy of the tagger for other chunks is lower." ></td>
	<td class="line x" title="115:208	The Penn Treebank chunks required modification because CCGbank analyses some constructions differently." ></td>
	<td class="line x" title="116:208	We also created longer NPs by concatenatingadjacentbase NPs,forexampleinthecase of possessives." ></td>
	<td class="line x" title="117:208	A number of punctuation constraints were used and had a significant impact especially for longer sentences." ></td>
	<td class="line x" title="118:208	There are a number of punctuation rules in CCGbank which absorb a punctuation mark by combining it with a category and returning a category of the same type." ></td>
	<td class="line x" title="119:208	These rules are very productive, combining with many constituent types." ></td>
	<td class="line x" title="120:208	However, in CCGbank the sentence final punctuation is always attached at the root." ></td>
	<td class="line x" title="121:208	A constraint on the first n  1 words was added to force the parser to only attach the sentence final punctuation once the rest of the sentence has been parsed." ></td>
	<td class="line x" title="122:208	Constraints are placed around parenthesised and quoted phrases that usually form constituents beforeattachingelsewhere." ></td>
	<td class="line x" title="123:208	Constraintsarealsoplaced around phrases bound by colons, semicolons, or hyphens." ></td>
	<td class="line x" title="124:208	These constraints are especially effective for long sentences with many clauses separated by semicolons, reducing the sentence to a number of smaller units which significantly improves parsing efficiency." ></td>
	<td class="line x" title="125:208	In some instances, adding constraints can be harmful to parsing efficiency and/or accuracy." ></td>
	<td class="line x" title="126:208	Lack of precision in the constraints can come from noisy output from NLP components, e.g. the chunker, or from rules which are not always applicable, e.g. punctuation constraints." ></td>
	<td class="line x" title="127:208	We find that the punctuation constraints are particularly effective while the gold standard chunks are required to gain any benefit for the NP constraints." ></td>
	<td class="line x" title="128:208	Adding constraints also hasthepotentialtoincreasecoveragebecausethereduced search space means that longer sentences can be parsed without exceeding the pre-defined limits on chart size." ></td>
	<td class="line x" title="129:208	5 Selective Beam Search Beam search involves greedy elimination of low probability partial derivations before they can form complete derivations." ></td>
	<td class="line x" title="130:208	It is used in many parsers to reduce the search space, for example Collins (2003)." ></td>
	<td class="line x" title="131:208	We use a variable width beam where all categories c in a particular cell C that satisfy score(c) < maxscore(x)|x  C}  B, for some beam cutoff B, are removed." ></td>
	<td class="line x" title="132:208	The category scores score(c) are log probabilities." ></td>
	<td class="line x" title="133:208	In the C&C parser, the entire packed chart is constructed first and then the spanning derivations are marked." ></td>
	<td class="line x" title="134:208	Only the partial derivations that form part of spanning derivations are scored to select the best parse, which is a small fraction of the categories in the chart." ></td>
	<td class="line x" title="135:208	Because the categories are scored with a complex statistical model with a large number of features, the time spent calculating scores is significant." ></td>
	<td class="line x" title="136:208	We found that applying a beam to every cell duringtheconstructionofthechartwasmoreexpensive than not using the beam at all." ></td>
	<td class="line x" title="137:208	When the beam was made harsh enough to be worthwhile, it reduced accuracy and coverage significantly." ></td>
	<td class="line x" title="138:208	We propose selective beam search where the beam is only applied to spans of particular lengths." ></td>
	<td class="line x" title="139:208	The shorter spans are most important to cull because therearemanymoreofthemandremovingthemhas the largest impact in terms of reducing the search space." ></td>
	<td class="line x" title="140:208	However, the supertagger already acts like a beam at the lexical category level and the parser model has fewer features at this level, so the beam may be more accurate for longer spans." ></td>
	<td class="line x" title="141:208	We therefore expect the beam to be most effective for spans of intermediate length." ></td>
	<td class="line x" title="142:208	6 Experiments The parser was trained on CCGbank sections 02-21 and section 00 was used for development." ></td>
	<td class="line x" title="143:208	The performance is measured in terms of coverage, F-score and parsing time." ></td>
	<td class="line x" title="144:208	The F-score is for labelled dependencies compared against the predicate-argument dependencies in CCGbank." ></td>
	<td class="line x" title="145:208	The time reported includes loading the grammar and statistical model, which takes around 5 seconds, and parsing the 1913 43 sentences in section 00." ></td>
	<td class="line x" title="146:208	The failure rate (opposite of coverage) is broken down into sentences with length  40 and > 40 because longer sentences are more difficult to parse and the C&C parser already has very high coverage on shorter sentences." ></td>
	<td class="line x" title="147:208	There are 1784 1-40 word sentences and 129 41+ word sentences." ></td>
	<td class="line x" title="148:208	The average length and standard deviation in the 41+ set are 50.8 and 31.5 respectively." ></td>
	<td class="line x" title="149:208	All experiments used gold standard POS tags." ></td>
	<td class="line x" title="150:208	Original and REPAIR do not use constraints." ></td>
	<td class="line x" title="151:208	The NP(GOLD) experiments use Penn Treebank gold standard NP chunks to determine an upper bound on the utility of chunk constraints." ></td>
	<td class="line x" title="152:208	The times reported for NP(C&C) using the C&C chunker include thetimetoloadthechunkermodelandrunthechunker (around 1.3 seconds)." ></td>
	<td class="line x" title="153:208	PUNCT adds all of the punctuation constraints." ></td>
	<td class="line x" title="154:208	Finally the best system was compared against the original parser on section 23, which has 2257 sentences of length 1-40 and 153 of length 41+." ></td>
	<td class="line x" title="155:208	The maximum length is only 65, which explains the high coverage for the 41+ section." ></td>
	<td class="line x" title="156:208	6.1 Chart Repair Results The results in Table 1 show that chart repair gives an immediate 11.1% improvement in speed and a small 0.21% improvement in accuracy." ></td>
	<td class="line x" title="157:208	96.1% of sentences do not require chart repair because they are successfully parsed using the initial set of lexical categories supplied by the supertagger." ></td>
	<td class="line x" title="158:208	Hence, 11% is a significant improvement for less than 4% of the sentences." ></td>
	<td class="line x" title="159:208	We believe the accuracy was improved (on top of the efficiency) because of the way the repair process adds new categories." ></td>
	<td class="line x" title="160:208	Adding categories individually allows the parser to be influenced by the probabilities which the supertagger assigns, which are not directly modelled in the parser." ></td>
	<td class="line x" title="161:208	If we were to add this information from the supertagger into the parser statistical model directly we would expect almost no accuracy difference between the original method and chart repair." ></td>
	<td class="line x" title="162:208	Table 2 shows the impact of different category ordering approaches for chart repair (with PUNCT constraints)." ></td>
	<td class="line x" title="163:208	The most effective approach is to use the information from the chart about the proportion of empty cells, which adds half as many categories METHOD secs % F-SCORE CATS RANDOM 70.2 -16.2 86.57 23.1  VALUE 60.4  86.66 15.7 PROB 60.1 0.5 86.65 14.3 CHART 57.2 5.3 86.61 7.0 Table 2: Category ordering for chart repair." ></td>
	<td class="line x" title="164:208	on average as the  value and probability based approaches." ></td>
	<td class="line x" title="165:208	All of our approaches significantly outperform randomly selecting extra categories." ></td>
	<td class="line x" title="166:208	The CHART category ordering is used for the remaining experiments." ></td>
	<td class="line x" title="167:208	6.2 Constraints Results The results in Table 1 show that, without chart repair, using gold standard noun phrases does not improve efficiency, while using noun phrases identified by the C&C chunker decreases speed by 10.8%." ></td>
	<td class="line x" title="168:208	They both also slightly reduce parsing accuracy." ></td>
	<td class="line x" title="169:208	The number of times the parsing process had to be restarted with the constraints removed, was more costly than the reduction of the search space." ></td>
	<td class="line x" title="170:208	This is unsurprising because the chunk data was not obtained from CCGbank and the chunker is not accurate enough for the constraints to improve parsing efficiency." ></td>
	<td class="line x" title="171:208	The most frequent inconsistencies between CCGbank and chunks extracted from the Penn Treebank were fixed in a preprocessing step as explained in Section 4.1, but the less frequent constructions are still problematic." ></td>
	<td class="line x" title="172:208	Thebestresultsforparsingwithconstraints(without repair) were with both punctuation and gold standard noun phrase constraints, with 20.5% improvement in speed and 0.42% in coverage, but an F-score penalty of 0.3%." ></td>
	<td class="line x" title="173:208	This demonstrates the possible efficiency gain with a perfect chunker  the corresponding results with the C&C chunker are still worse than without constraints." ></td>
	<td class="line x" title="174:208	The best results without a decrease in accuracy use only punctuation constraints, with10.4%increaseinspeedand0.37% in coverage." ></td>
	<td class="line x" title="175:208	The punctuation constraints also have the advantage of being simple to implement." ></td>
	<td class="line x" title="176:208	The best overall efficiency gain was obtained when punctuation and gold standard noun phrases were used with chart repair, with a 45.4% improvement in speed and 0.63% in coverage, and a 0.4% drop in accuracy." ></td>
	<td class="line x" title="177:208	The best results without a drop in 44 METHOD secs % F-SCORE COVER n  40 n > 40 Original 88.3  86.54 98.85 0.392 11.63 REPAIR 78.5 11.1 86.75 99.01 0.336 10.08 NP(GOLD) 88.4 -0.1 86.27 99.06 0.224 10.85 NP(C&C) 97.8 -10.8 86.31 99.16 0.224 9.30 PUNCT 79.1 10.4 86.56 99.22 0.168 9.30 NP(GOLD) + PUNCT 69.8 20.5 86.24 99.27 0.168 8.53 NP(C&C) + PUNCT 97.0 -9.9 86.31 99.16 0.168 10.08 NP(GOLD) + REPAIR 65.0 26.4 86.04 99.37 0.224 6.20 NP(C&C) + REPAIR 77.5 12.2 86.35 99.37 0.224 6.20 PUNCT + REPAIR 57.2 35.2 86.61 99.48 0.168 5.43 NP(GOLD) + PUNCT + REPAIR 48.2 45.4 86.14 99.48 0.168 5.43 NP(C&C) + PUNCT + REPAIR 63.2 28.4 86.43 99.53 0.163 3.88 Table 1: Parsing performance on section 00 with constraints and chart repair METHOD secs % F-SCORE COVER n  40 n > 40 Original 88.3  86.54 98.85 0.392 11.63 PUNCT 79.1 10.4 86.56 99.22 0.168 9.30 REPAIR 78.5 11.1 86.75 99.01 0.336 10.08 PUNCT + REPAIR 57.2 35.2 86.61 99.48 0.168 5.43 PUNCT + REPAIR + BEAM 52.4 40.7 86.56 99.48 0.168 5.43 Table 3: Best performance on Section 00 accuracywerewithonlypunctuationconstraintsand chart repair, with improvements of 35.2% speed and 0.63% coverage." ></td>
	<td class="line x" title="178:208	Coverage on both short and long sentences is improved  the best results show a 43% and67%decreaseinfailurerateforsentencelengths in the ranges 1-40 and 41+ respectively." ></td>
	<td class="line x" title="179:208	6.3 Partial Beam Results We found that using the selective beam on 12 word spans had negligible impact on speed and accuracy." ></td>
	<td class="line x" title="180:208	Using the beam on 34 word spans had the most impact without accuracy penalty, improving efficiency by another 5%." ></td>
	<td class="line x" title="181:208	Experiments with the selective beam on longer spans continued to improve efficiency, but with a much greater penalty in F-score, e.g. a further 5% at a cost of 0.5% F-score for 36 wordspans." ></td>
	<td class="line x" title="182:208	However,weareinterestedinefficiency improvements with negligible cost to accuracy." ></td>
	<td class="line x" title="183:208	6.4 Overall Results Table 3 summarises the results for section 00." ></td>
	<td class="line x" title="184:208	The chart repair and punctuation constraints individually increase parsing efficiency by around 10%." ></td>
	<td class="line x" title="185:208	However, the most interesting result is that in combination they increase efficiency by over 35%." ></td>
	<td class="line x" title="186:208	This is because the cost of rebuilding the chart when the constraints are incorrect has been significantly reduced by chart repair." ></td>
	<td class="line x" title="187:208	Finally, the use of the selective beam gives modest improvement of 5.5%." ></td>
	<td class="line x" title="188:208	The overall efficiency gain on section 00 is 40.7% with an additional 0.5% coverage, halving both the number of short and long sentences that fail to be parsed." ></td>
	<td class="line x" title="189:208	Table 4 shows the performance of the punctuation constraints, chart repair and selective beam system on section 23." ></td>
	<td class="line x" title="190:208	The results are consistent with section 00, showing a 30.9% improvement in speed and 0.29% in coverage, with accuracy staying at roughly the same level." ></td>
	<td class="line x" title="191:208	The results show a consistent 3540% reduction in parsing time and a 40-65% reduction in parse failure rate." ></td>
	<td class="line x" title="192:208	7 Conclusion We have introduced several modifications to CKY parsing for CCG that significantly increase parsing efficiency without an accuracy or coverage penalty." ></td>
	<td class="line x" title="193:208	45 METHOD secs % F-SCORE COVER n  40 n > 40 Original 91.3  86.92 99.29 0.621 1.961 PUNCT + REPAIR + BEAM 58.7 35.7 86.82 99.58 0.399 0.654 Table 4: Best performance on Section 23 Chart repair improves efficiency by reusing the chart from the previous parse attempts." ></td>
	<td class="line x" title="194:208	This allows us to further tighten the parser-supertagger integration by adding one lexical category at a time until a spanning derivation is found." ></td>
	<td class="line x" title="195:208	We have also explored several approaches to selecting which category to add next." ></td>
	<td class="line x" title="196:208	We intend to further explore strategies for determining which category to add next when a parse fails." ></td>
	<td class="line x" title="197:208	This includes combining chart and probability based orderings." ></td>
	<td class="line x" title="198:208	Chart repair alone gives an 11.1% efficiency improvement." ></td>
	<td class="line x" title="199:208	Constraints improve efficiency by avoiding the construction of sub-derivations that will not be used." ></td>
	<td class="line x" title="200:208	They have a significant impact on parsing speed and coverage without reducing the accuracy, provided the constraints are identified with sufficient precision." ></td>
	<td class="line x" title="201:208	Punctuation constraints give a 10.4% improvement, but NP constraints require higher precision NP chunking than is currently available for CCGbank." ></td>
	<td class="line x" title="202:208	Constraints and chart repair both manipulate the chart for more efficient parsing." ></td>
	<td class="line x" title="203:208	Adding categories one at a time using chart repair is almost a form of agenda-based parsing." ></td>
	<td class="line x" title="204:208	We intend to explore other methods for pruning the space and agenda-based parsing, in particular A* parsing (Klein and Manning, 2003), which will allow only the most probablepartsofthecharttobebuilt,improvingefficiency while still ensuring the optimal derivation is found." ></td>
	<td class="line x" title="205:208	When all of our modifications are used parsing speed increases by 35-40% and the failure rate decreasesby40-65%,bothforsentencesoflength1-40 and 41+, with a negligible accuracy penalty." ></td>
	<td class="line x" title="206:208	The result is an even faster state-of-the-art wide-coverage CCG parser." ></td>
	<td class="line x" title="207:208	Acknowledgements We would like to thank the anonymous reviewers for their feedback." ></td>
	<td class="line x" title="208:208	James Curran was funded under ARCDiscoverygrantsDP0453131andDP0665973." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-2211
Symbolic Preference Using Simple Scoring
Newman, P. S.;"></td>
	<td class="line x" title="1:238	Proceedings of the 10th Conference on Parsing Technologies, pages 8392, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:238	c2007 Association for Computational Linguistics Symbolic Preference Using Simple Scoring Paula S. Newman newmanp@acm.org Abstract Despite the popularity of stochastic parsers, symbolic parsing still has some advantages, but is not practical without an effective mechanism for selecting among alternative analyses." ></td>
	<td class="line x" title="3:238	This paper describes the symbolic preference system of a hybrid parser that combines a shallow parser with an overlay parser that builds on the chunks." ></td>
	<td class="line x" title="4:238	The hybrid currently equals or exceeds most stochastic parsers in speed and is approaching them in accuracy." ></td>
	<td class="line x" title="5:238	The preference system is novel in using a simple, three-valued scoring method (-1, 0, or +1) for assigning preferences to constituents viewed in the context of their containing constituents." ></td>
	<td class="line x" title="6:238	The approach addresses problems associated with earlier preference systems, and has considerably facilitated development." ></td>
	<td class="line x" title="7:238	It is ultimately based on viewing preference scoring as an engineering mechanism, and only indirectly related to cognitive principles or corpus-based frequencies." ></td>
	<td class="line x" title="8:238	1 Introduction Despite the popularity of stochastic parsers, symbolic parsing still has some advantages, but is not practical without an effective mechanism for selecting among alternative analyses." ></td>
	<td class="line x" title="9:238	Without it, accept/fail grammar rules must either be overly strong or admit very large numbers of parses." ></td>
	<td class="line x" title="10:238	Symbolic parsers have recently been augmented by stochastic post-processors for output disambiguation, which reduces their independence from corpora." ></td>
	<td class="line x" title="11:238	Both the LFG XLE parser (Kaplan et.al. 2004), and the HPSG LinGO ERG parser (Toutanova et al. 2005) have such additions." ></td>
	<td class="line x" title="12:238	This paper examines significant aspects of a purely symbolic alternative: the preference and pruning system of the RH (Retro-Hybrid) parser (Newman, 2007)." ></td>
	<td class="line x" title="13:238	The parser combines a preexisting, efficient shallow parser with an overlay parser that builds on the emitted chunks." ></td>
	<td class="line x" title="14:238	The overlay parser is 'retro' in that the grammar is related to ATNs (Augmented Transition Networks) originated by Woods (1970)." ></td>
	<td class="line x" title="15:238	RH delivers single 'best' parses providing syntactic categories, syntactic functions, head features, and other information (Figure 1)." ></td>
	<td class="line x" title="16:238	The parenthesized numbers following the category labels in the figure are preference scores, and are explained further on." ></td>
	<td class="line x" title="17:238	While the parses are not quite as detailed as those obtained using 'deep' grammars, the missing information, mostly relating to long distance dependencies, can be added at far less cost in a post-parse phase that operates only on a single best parse." ></td>
	<td class="line oc" title="18:238	Methods for doing so, for stochastic parser output, are described by Johnson (2002) and Cahill et al (2004)." ></td>
	<td class="line x" title="19:238	The hybrid parser exceeds most stochastic parsers in speed, and approaches them in accuracy, even based on limited manual 'training' on a particular idiom, so the preference system is a successful one (see Section 6), and continues to improve." ></td>
	<td class="line x" title="20:238	The RH preference system builds on earlier methods." ></td>
	<td class="line x" title="21:238	The major difference is a far simpler scoring system, which has considerably facilitated overlay parser development." ></td>
	<td class="line x" title="22:238	Also, the architecture allows the use of large numbers of preference tests without impacting parser speed." ></td>
	<td class="line x" title="23:238	Finally, the treatment of coordination exploits the lookaheads afforded by the shallow parser to license or bar alternative appositive readings." ></td>
	<td class="line x" title="24:238	Section 2 below discusses symbolic preference systems in general, and section 3 provides an overview of RH parser structure." ></td>
	<td class="line x" title="25:238	Section 4 describes the organization of the RH preference system and the simplified scoring mechanism." ></td>
	<td class="line x" title="26:238	Section 5 discusses the training approach and Section 6 provides some experimental results." ></td>
	<td class="line x" title="27:238	Section 7 summarizes, and indicates directions for further work." ></td>
	<td class="line x" title="28:238	83 Figure 1." ></td>
	<td class="line x" title="29:238	Output Parse Tree for 'Rumsfeld micromanaged daily briefings and rode roughshod over people'." ></td>
	<td class="line x" title="30:238	* indicates head." ></td>
	<td class="line x" title="31:238	Mouseover shows head features for 'micromanaged'." ></td>
	<td class="line x" title="32:238	2 Background: Symbolic Preference 2.1 Principles Preference-based parsing balances necessarily permissive syntactic rules by preference rules that promote more likely interpretations." ></td>
	<td class="line x" title="33:238	One of the earliest works in the area is by Wilks (1975), which presented a view of preference as based on semantic templates." ></td>
	<td class="line x" title="34:238	Throughout the 1980's there was a considerable amount of work devoted to finding general principles, often cognitively oriented, for preference rules, and then to devise mechanisms for using them in practical systems." ></td>
	<td class="line x" title="35:238	Hobbs and Bear (1990) provide a useful summary of the evolved principles." ></td>
	<td class="line x" title="36:238	Slightly restated, these principles are: 1." ></td>
	<td class="line x" title="37:238	Prefer attachments in the 'most restrictive context'." ></td>
	<td class="line x" title="38:238	2." ></td>
	<td class="line x" title="39:238	If that doesn't uniquely determine the result, attach low and parallel, and finally 3." ></td>
	<td class="line x" title="40:238	Adjust the above based on considerations of punctuation Principle 1 suggests that the preference for a constituent in a construction should depend on the extent to which the constituent meets a narrow set of expectations." ></td>
	<td class="line x" title="41:238	Most of the examples given by Hobbs and Bear use either (a) sub-categorization information, e.g., preferring the attachment of a prepositional phrase to a head that expects that particular preposition, or (b) limited semantic information, for example, preferring the attachment of a time expression to an event noun." ></td>
	<td class="line x" title="42:238	Principle 2 implies that in the absence of coordination, attachment should be low, and in the presence of coordination, parallel constituents should be preferred." ></td>
	<td class="line x" title="43:238	Principle 3 relates primarily to the effect of commas in modifying attachment preferences." ></td>
	<td class="line x" title="44:238	2.2 Implementations Abstractly, symbolic preference systems can be thought of as regarding a set of possible parses as a collection of spanning trees over a network of potential relationships, with each edge having a numeric value, and attempting to find the highest scoring tree.1 However, for syntactic parsers, in contrast with dependency parsers, it is convenient to associate scores with constituents as they are built, for consistency with the parser structure, and to permit within-parse pruning." ></td>
	<td class="line x" title="45:238	A basic model for a preference system assigns preference scores to rules." ></td>
	<td class="line x" title="46:238	For a rule C  c1, c2, , cn the preference score PS(CC) of a resultant constituent CC is the sum: PS(cc1) + PS(cc2) + +PS(ccn) + TRS (C, cc1, cc2, , ccn) where PS(cci) is the non-contexted score of constituent cci, and the total relationship score TRS is a value that assesses the relationships among the sibling constituents of CC." ></td>
	<td class="line x" title="47:238	The computation of TRS depends on the parser approach." ></td>
	<td class="line x" title="48:238	For a top-down parser, TRS may be the sum of contexted relationship scores CRS, for example: TRS = CRS (cc1|C) +CRS (cc2|C, cc1), + CRS (cc3|C, cc1, cc2) +  + CRS (cn |C, cc1,.ccn-1) where each CRS (cci|_ ) evaluates cci in the context of the prior content of the constituent CC and the category C Few publications specify details of how preference scores are assigned and combined." ></td>
	<td class="line x" title="49:238	For example, Hobbs and Bear (1990) say only that 'When a 1 The idea has also been used directly in stochastic parsers that consider all possible attachments, for example, by McDonald et al.(2005)." ></td>
	<td class="line x" title="51:238	84 non-terminal node of a parse tree is constructed, it is given an initial score which is the sum of the scores of its child nodes." ></td>
	<td class="line x" title="52:238	Various conditions are checked during the construction of the node and, as a result, a score of 20, 10, 3, -3, -10, or -20 may be added to the initial score'." ></td>
	<td class="line x" title="53:238	McCord (1993), however, carefully describes how the elements of TRS are computed in his slot grammar system." ></td>
	<td class="line x" title="54:238	Each element value is the sum of the results of up to 8 optional, typed tests, relating to structural, syntactic, and semantic conditions." ></td>
	<td class="line x" title="55:238	One of these tests, relating to coordination, is a complex test involving 7 factors assessing parallelism." ></td>
	<td class="line x" title="56:238	2.3 Multi-Level Contexted Scoring The scores assigned by symbolic preference systems to particular relationships or combinations usually indicate not just whether they are preferred or dispreferred, but to what degree." ></td>
	<td class="line x" title="57:238	For example, a score of 1 might indicate that a relationship is good, and 2 that it is better." ></td>
	<td class="line x" title="58:238	Such multi-level scores create problems in tuning parsers to remove undesirable interactions, both in the grammar and the preference system." ></td>
	<td class="line x" title="59:238	Even for interactions foreseen in advance, one must remember or find out the sizes of the preferences involved, to decide how to compensate." ></td>
	<td class="line x" title="60:238	Yamabana et al.(1993) give as an example a bottom-up parser, where an S constituent with a transitive verb head but lacking an object is initially given a strong negative preference, but when it is discovered that the constituent actually functions as a relative clause, the appropriate compensation must be found." ></td>
	<td class="line x" title="62:238	(Their solution uses a vector of preference scores, with the vector positions corresponding to specific types of preference features, together with an accumulator." ></td>
	<td class="line x" title="63:238	It allows the content of vector elements to be erased based on subsequently discovered compensating features)." ></td>
	<td class="line x" title="64:238	For unforeseen interactions, for example when a review of parser outputs finds that the best parse is not given the highest preference score, multi-level contexted scoring requires complex tracing of the contribution of each score to the total, remembering at each point what the score should be, to determine the necessary adjustments." ></td>
	<td class="line x" title="65:238	A different sort of problem of multi-level scoring stems from the unavoidable incompleteness of information." ></td>
	<td class="line x" title="66:238	For example, in Figure 1, the attachment of an object to the 'guessed' verb 'micromanaged' is dispreferred because the verb is not identified as transitive." ></td>
	<td class="line x" title="67:238	Here, the correct reading survives because there are no higher scoring ones." ></td>
	<td class="line x" title="68:238	But in some situations, if such a dispreference were given a large negative score, the parser could be forced into very odd readings not compensated for by other factors." ></td>
	<td class="line x" title="69:238	2.4 Corpus-Based Preference In the early 1990's, the increasing availability and use of corpora, together with a sense that multilevel symbolic preference scores were based on adhoc judgments, led to experiments and systems that used empirical methods to obtain preference weights." ></td>
	<td class="line x" title="70:238	Examples of this work include a system by Liu et al (1990), and experiments by Hindle and Rooth (1993), and Resnik and Hearst (1993).2 These efforts had mixed success, suggesting that while multi-level preference scores are problematic, integrating some corpus data does not solve the problems." ></td>
	<td class="line x" title="71:238	In light of later developments, this might be expected." ></td>
	<td class="line x" title="72:238	Full-scale contemporary stochastic parsers use a broad range of interacting features to obtain their fine-grained results; frequencies of particular relationships are just one aspect." ></td>
	<td class="line x" title="73:238	2.5 OT-based Preference A more recent approach to symbolic preference adapts optimality theory to parser and generator preference." ></td>
	<td class="line x" title="74:238	Optimality Theory (OT) was originally developed to explain phonological rules (Prince and Smolensky, 1993)." ></td>
	<td class="line x" title="75:238	In that use, potential rules are given one 'optimality mark' for each constraint they violate." ></td>
	<td class="line x" title="76:238	The marks, all implicitly negative, are ranked by level of severity." ></td>
	<td class="line x" title="77:238	A best rule R is one for which (a) the most severe level of constraint violation L is  the level violated by any other rule, and (b) if other rules also violate level L constraints, the number of such violations is  the number of violations by R. As adapted for use in the XLE processor for LFG (Frank et al. 1998) optimality marks are associated with parser and generator outputs." ></td>
	<td class="line x" title="78:238	Positive marks are added, and also labeled inter-mark positions within the optimality mark ranking." ></td>
	<td class="line x" title="79:238	The labeled positions influence processor behavior." ></td>
	<td class="line x" title="80:238	For generation, they are used to disprefer infelicitous strings accepted in a parse direction." ></td>
	<td class="line x" title="81:238	And for pars2 McCord (1993) also includes some corpus-based information, but to a very limited extent." ></td>
	<td class="line x" title="82:238	85 ing they can be used to disprefer (actually ignore) rarely-applicable rules, in order to reduce parse time (Kaplan et al, 2004)." ></td>
	<td class="line x" title="83:238	However, because the optimality marks are global, a single dispreference can rule out an entire parse." ></td>
	<td class="line x" title="84:238	To partially overcome this limitation, a further extension (see XLE Online Documentation) allows direct comparisons of alternative readings for the same input extent." ></td>
	<td class="line x" title="85:238	A different optimality mark can be set for each reading, and the use of one such mark in the ranking can be conditioned on the presence of another particular mark for the same extent." ></td>
	<td class="line x" title="86:238	For example, a conditional dispreference can be set for an adjunct reading if an argument reading also exists." ></td>
	<td class="line x" title="87:238	The extension does not address more global interactions, and is said (Forst et al. 2005) to be used mostly as a pre-filter to limit the readings disambiguated by a follow-on stochastic process." ></td>
	<td class="line x" title="88:238	2.6 A Slightly Different View A slightly different view of preferencebased parsing is that the business of a preference system is to work in tandem with a permissive syntactic grammar, to manipulate outcomes." ></td>
	<td class="line x" title="89:238	The difference focuses on the pragmatic role of preference in coercing the parser." ></td>
	<td class="line x" title="90:238	In this light, the principles of section 2.1 are guidelines for desired outcomes, not bases for judging the goodness of a relationship or setting preference values." ></td>
	<td class="line x" title="91:238	Instead, preference values should be set based on their effectiveness in isolating best parses." ></td>
	<td class="line x" title="92:238	Also, in this light, the utility of a preference system lies not only in its contribution to accuracy, but also in its software-engineering convenience." ></td>
	<td class="line x" title="93:238	These considerations led to the simpler, more practical scoring system of the RH overlay parser, described in section 4 below, in which contexted preference scores CRS can have one of only 3 values, -1, 0, or +1." ></td>
	<td class="line x" title="94:238	3 Background: The RH Parser The RH parser consists of three major components, outlined below: the shallow parser, a mediating 'locator' phase, and the overlay parser." ></td>
	<td class="line x" title="95:238	3.1 Shallow Parser The shallow parser used, XIP, was developed by XRCE (Xerox Research Center Europe)." ></td>
	<td class="line x" title="96:238	It is actually a full parser, whose per-sentence output consists of a single tree of basic chunks, together with identifications of (sometimes alternative) typed dependences among the chunk heads (AitMokhtar et al. 2002, Gala 2004)." ></td>
	<td class="line x" title="97:238	But because the XIP dependency analysis for English was not mature at the time that work on RH began, and because a classic parse tree annotated by syntactic functions is more convenient for some applications, we focused on the output chunks." ></td>
	<td class="line x" title="98:238	XIP is astonishingly fast, contributing very little to parse times (about 20%)." ></td>
	<td class="line x" title="99:238	It consists of the XIP processor, plus grammars for a number of languages." ></td>
	<td class="line x" title="100:238	The grammar for a particular language consists of: (a) a finite-state lexicon producing alternative part-of-speech and morphological analyses for each token, together with bit-expressed subcategorization and control features, and (some) semantic features, (b) a substitutable tagger identifying the most probable part of speech for each token, and (c) sequentially applied rule sets that extend and modify lexical information, disambiguate tags, identify named entities and other multiwords, and produce output chunks and inter-chunk head dependences (the latter not used in the hybrid)." ></td>
	<td class="line x" title="101:238	Work on the hybrid parser has included large scale extensions to the XIP English rule sets." ></td>
	<td class="line x" title="102:238	3.2 Locator phase The locator phase accumulates and analyses some of the shallow parser results to expedite the grammar and preference tests of the overlay parser." ></td>
	<td class="line x" title="103:238	For preference tests, for any input position, the positions of important leftward and rightward tokens are identified." ></td>
	<td class="line x" title="104:238	These 'important' tokens include commas, and leftward phrase heads that might serve as alternative attachment points." ></td>
	<td class="line x" title="105:238	Special attention is given to coordination, a constant source of inefficiency and inaccuracy for all parsers." ></td>
	<td class="line x" title="106:238	To limit this problem, an input string is divided into spans ending at coordinating conjunctions, and the chunks following a span are examined to determine what kinds of coordination might be present in the span." ></td>
	<td class="line x" title="107:238	For example, if a chunk following a span Sp is a noun phrase, and there are no verbs in the input following that noun phrase, only noun phrase coordination is considered within Sp." ></td>
	<td class="line x" title="108:238	Also, with heuristic exceptions, the locator phase disallows searching for appositives within 86 long sequences of noun and prepositional phrases ending with a coordinating conjunction." ></td>
	<td class="line x" title="109:238	3.3 Overlay Parser The overlay parser uses a top-down grammar, expressed as a collection of ATN-like grammar networks." ></td>
	<td class="line x" title="110:238	A recursive control mechanism traverses the grammar networks depth-first to build constituents." ></td>
	<td class="line x" title="111:238	The labels on the grammar network arcs represent specialized categories, and are associated with tests that, if successful, either return a chunk or reinvoke the control to attempt to build a constituents for the category." ></td>
	<td class="line x" title="112:238	The labelspecific tests include both context-free tests, and tests taking into account the current context." ></td>
	<td class="line x" title="113:238	For details see (Newman, 2007)." ></td>
	<td class="line x" title="114:238	If an invocation of the control is successful, it returns an output network containing one or more paths, with each path representing an alternative sequence of immediate children of the constituent." ></td>
	<td class="line x" title="115:238	An example output network is shown in figure 2." ></td>
	<td class="line x" title="116:238	Each arc of the network references either a basic chunk, or a final state of a subordinate output network." ></td>
	<td class="line x" title="117:238	Unlike the source grammar networks, the output networks do not contain cycles or converging arcs, so states represent unique paths." ></td>
	<td class="line x" title="118:238	The states contain both (a) information about material already encountered along the path, including syntactic functions and head features, and (b) a preference score for the path to that point." ></td>
	<td class="line x" title="119:238	Thus the figure 2 network represents two alternative noun phrases, one represented by the path containing OS0 and OS1, and one containing OS0, OS1, and OS2." ></td>
	<td class="line x" title="120:238	State OS2 contains the preference score (+1), because attaching a locative pp to a feature of the landscape is preferred." ></td>
	<td class="line x" title="121:238	From To Cat Synfun Reference OSo OS1 NP HEAD NPChunk (The park) OS1 OS2 PP NMOD Final state of PP net for (in Paris) States Score Final?" ></td>
	<td class="line x" title="122:238	OS0 0 No OS1 0 Yes OS2 +1 Yes Figure 2." ></td>
	<td class="line x" title="123:238	Output network for 'The park in Paris' Before an output network is returned from an invocation of the control mechanism, it is pruned to remove lower-scoring paths, and cached." ></td>
	<td class="line x" title="124:238	Output from the overlay parser is a single tree (Figure 1) derived from a highest scoring full path (i.e. final state) of a topmost output network." ></td>
	<td class="line x" title="125:238	If there are several highest scoring paths, low attach considerations select a 'best' one." ></td>
	<td class="line x" title="126:238	The preference scores shown in Figure 1 in parentheses after the category labels are the scores at the succeeding states of the underlying output networks." ></td>
	<td class="line x" title="127:238	4 Preference System Any path in an output network has the form: S0, Ref1, S1, Ref2, , Sn-1, Refn, Sn where Si is a state, and Refi labels an arc, and references either a basic chunk, or a final state of another output network." ></td>
	<td class="line x" title="128:238	A state Si has total preference score TPS(i) where:  TPS(0) = 0  TPS(i), i>0 = TPS( i-1) + PS(Refi) +CRS(Refi)  PS(Refi) is the non-contexted score of the constituent referenced by Refi, that is, the score at the referenced final state." ></td>
	<td class="line x" title="129:238	 CRS(Refi) is the contexted score for Refi, in the context of the network category and the path ending at the previous state i-1." ></td>
	<td class="line x" title="130:238	For example, if Refi refers to a noun phrase considered a second object within a path, and the syntactic head along the path does not expect a second object, CRS(Refi) might be (-1)." ></td>
	<td class="line x" title="131:238	Each value CRS is limited to values in {-1, 0, +1}." ></td>
	<td class="line x" title="132:238	Therefore, no judgment is needed to decide the degree to which a contexted reference is to be dispreferred or preferred." ></td>
	<td class="line x" title="133:238	Also, if the desired parse result does not receive the highest overall score, it is relatively easy to trace the reason." ></td>
	<td class="line x" title="134:238	Pruning (see below) can be disabled and all parses can be displayed, as in Figure 1, which shows the scores TPS(i) in parentheses after the category labels for each Refi (with zero scores not shown)." ></td>
	<td class="line x" title="135:238	Then, if TPS(i) > ( TPS(i-1) + PS(Refi)) it is clear that the contexted reference is preferred." ></td>
	<td class="line x" title="136:238	If multi-level contexted scoring were used instead, it would be necessary to determine whether the reference was preferred to exactly the right degree." ></td>
	<td class="line x" title="137:238	87 Test Block Type Length Independent?" ></td>
	<td class="line x" title="138:238	Indexed By Coordinate Y Parent syncat Subcat Y No index FN1 Y synfun TAG1 Y syncat FN2 N synfun TAG2 N syncat Table 1." ></td>
	<td class="line x" title="139:238	Preference Test Block Types 4.1 Preference test organization To compute the contexted score CRS for a reference, relevant tests are applied until either (a) a score of -1 is obtained, which is used as CRS for the reference, or (b) the tests are exhausted." ></td>
	<td class="line x" title="140:238	In the latter case, CRS is the higher of the values {0, +1} returned by any test." ></td>
	<td class="line x" title="141:238	For purposes of efficiency, the preference tests are divided into typed blocks, as shown in Table 1." ></td>
	<td class="line x" title="142:238	At most one block of each type can be applied to a reference." ></td>
	<td class="line x" title="143:238	Four of the blocks contain tests that are independent of referenced constituent length." ></td>
	<td class="line x" title="144:238	They are applied at most once for a returned output network and the results are assumed for all paths." ></td>
	<td class="line x" title="145:238	The other two blocks are length dependent." ></td>
	<td class="line x" title="146:238	Referring to Table 1, the length-independent coordinate tests are applied only to non-first siblings of coordinated constituents." ></td>
	<td class="line x" title="147:238	The parent category indicates the type of constituents being coordinated and selects the appropriate test block." ></td>
	<td class="line x" title="148:238	Tests in these blocks focus on the semantic consistency of a coordinated sibling with the first one." ></td>
	<td class="line x" title="149:238	Subcategorization tests are applied to prepositional, particle, and clausal dependents of the current head." ></td>
	<td class="line x" title="150:238	These tests consist to a large extent of bit-vector implemented operations, comparing the expected dependent types of the head with lexical features of the prospective dependent." ></td>
	<td class="line x" title="151:238	The tests are made somewhat more complex because of various exceptions, such as (a) temporal and locative phrases, and (b) the presence of a nearer potential head also expecting the dependent type." ></td>
	<td class="line x" title="152:238	The other test block types are selected and accessed either by the syntactic category or the syntactic function of the reference, depending on the focus of the test." ></td>
	<td class="line x" title="153:238	The length-dependent tests include tests of noun-phrases within coordinations to determine whether post modifiers should be applied to the individual phrase or to the coordination as a whole." ></td>
	<td class="line x" title="154:238	The test blocks are expressed in procedural code." ></td>
	<td class="line x" title="155:238	This has allowed the parser to be developed without advance prediction of the types of information needed for the tests, and also has contributed some efficiency." ></td>
	<td class="line x" title="156:238	The blocks, usually short but occasionally long, generally consist of ordered (if-then-else) subtests." ></td>
	<td class="line x" title="157:238	4.2 Preference test scope A contexted preference test can refer to material on three levels of the developing parse tree: (a) the syntactic category of the parent (available because of the top-down parser direction) (b) information about the current output network path, including head features, already-encountered syntactic functions, and a small collection of special-purpose information, and (c) information about the referenced constituent, specifically its head and a list of the immediately contained syntactic functions." ></td>
	<td class="line x" title="158:238	The tests can also reference lookahead information furnished by the locator phase." ></td>
	<td class="line x" title="159:238	This material is sufficient for most purposes." ></td>
	<td class="line x" title="160:238	Limiting the kind of referenced information, particularly not permitting access to sibling constituents or deep elements of the referenced constituent, contributes to performance." ></td>
	<td class="line x" title="161:238	4.3 Pruning Before an output network is completed, it is pruned to remove lower-scoring output network paths." ></td>
	<td class="line x" title="162:238	Any path with the same length as another but with a lower score is pruned." ></td>
	<td class="line x" title="163:238	Also, paths having other lengths but considerably lower preference scores than the best-scoring path are often pruned as well." ></td>
	<td class="line x" title="164:238	4.4 Usage Example To illustrate how the simple scores and modular tests are used to detect and repair problems in the preference system, Figure 1 shows, as noted before, that the attachment of an object to the guessed verb 'micromanaged' is dispreferred." ></td>
	<td class="line x" title="165:238	In this case the probable reason is the lack of a transitive feature for the verb." ></td>
	<td class="line x" title="166:238	To check this, we would look at the FN1 test block for OBJ and find that in fact the test assigns (-1) in this case." ></td>
	<td class="line x" title="167:238	The required modification is best made by adding a transitive feature to guessed verbs." ></td>
	<td class="line x" title="168:238	But there is another problem here: the attachment of the pp 'over people' is not given a positive preference." ></td>
	<td class="line x" title="169:238	Checking the FN1 test block for 88 VMOD and the TAG1 test block for PP finds that there is in fact no subtest that prefers combinations of motion verbs and 'over'." ></td>
	<td class="line x" title="170:238	While this doesn't cause trouble in the example, it could if there were a prior object in the verb phrase." ></td>
	<td class="line x" title="171:238	A subtest or subcategorization feature could be added." ></td>
	<td class="line x" title="172:238	5 Training the Preference System To obtain the preference system, an initial set of tests is identified, based primarily on subcategorization considerations, and then refined and extended based on manual 'training' on large numbers of documents." ></td>
	<td class="line x" title="173:238	Several problem situations result in changes to the system, besides random inspection of scores: (a) the best parse identified is not the correct one, either because the correct parse is not the highest scoring one, or because another parse with the same score was considered 'best' because of low-attach considerations." ></td>
	<td class="line x" title="174:238	(b) The best parse obtained is the correct one, but there are many other parses with the same score, suggesting a need for refinement, both to improve performance and to avoid errors in related circumstances when the correct parse does not 'float' to the top." ></td>
	<td class="line x" title="175:238	(c) No parse is returned for an input, because of imposed space constraints, which indirectly control the amount of time that can be spent to obtain a parse." ></td>
	<td class="line x" title="176:238	In some cases the above problems can be solved by adjusting the base grammar, or by extending lexical information to obtain the appropriate preferences." ></td>
	<td class="line x" title="177:238	For example, the preference scoring problems of Figure 1 can be corrected by adding subcategorization information, as described above." ></td>
	<td class="line x" title="178:238	In other cases, one or more modifications to the preference system are made, adding positive tests to better distinguish best parses, adding negative tests to disprefer incorrect parses, and/or refining existing tests to narrow or expand applicability." ></td>
	<td class="line x" title="179:238	Positive tests often just give credit to expected structures not previously considered to require recognition beyond acceptance by the grammar." ></td>
	<td class="line x" title="180:238	Negative tests fall into many classes, such as: (a) Tests for 'ungrammatical' phenomena that should not be ruled out entirely by the grammar. These include lack of agreement, lack of expected punctuation, and presence of unexpected punctuation (such as a comma between a subject and a verb when there is no comma within the subject)." ></td>
	<td class="line x" title="181:238	(b) Tests for probably incomplete constituents, based on the chunk types that follow them." ></td>
	<td class="line x" title="182:238	(c) Tests for unexpected arguments, except in some circumstances." ></td>
	<td class="line x" title="183:238	For example, 'benefactive' indirect objects ('John baked Mary a cake') are dispreferred if they are not in appropriate semantic classes." ></td>
	<td class="line x" title="184:238	Also, a large, complex collection of positive and negative tests, based on syntactic and semantic factors, are used to distinguish among coordinated and appositive readings, and among alternative attachments of appositives." ></td>
	<td class="line x" title="185:238	If the addition or modification of preference tests does not solve a particular problem, then some more basic changes can be made, such as the introduction of new semantic classes." ></td>
	<td class="line x" title="186:238	And, in rare cases, new features are added to output network states in order to make properties of non-head constituents encountered along a path available for testing both further along the path and in the development of higher-level constituents." ></td>
	<td class="line x" title="187:238	An example is the person and number of syntactic subjects, allowing contexted preference tests for finite verb phrases to check for subject consistency." ></td>
	<td class="line x" title="188:238	5.1 Relationship to 'supervised' training To illustrate the relationship between the above symbolic training method for preference scoring and corpus-based methods, perhaps the easiest way is to compare it to an adaptation (Collins and Roark, 2004) of the perceptron training method to the problem of obtaining a best parse (either directly, or for parse reranking), because the two methods are analogous in a number of ways." ></td>
	<td class="line x" title="189:238	The basic adapted perceptron training assumes a generator function producing parses for inputs." ></td>
	<td class="line x" title="190:238	Each such parse is associated with a vector of feature values that express the number of times the feature appears in the input or parse." ></td>
	<td class="line x" title="191:238	The features used are those identified by Roark (2001) for a topdown stochastic parser." ></td>
	<td class="line x" title="192:238	The training method obtains a weight vector W (initially 0) for the feature values, by iterating multiple times over pairs <xi, yi> where xi is a training input, and yi is the correct parse for xi." ></td>
	<td class="line x" title="193:238	For each pair, the best current parse zi for xi produced by the generator, with feature value vector V(zi), is selected based on the current value of (W  V(zi))." ></td>
	<td class="line x" title="194:238	Then if zi  yi, W is incremented by V(yi), and dec89 remented by V(zi)." ></td>
	<td class="line x" title="195:238	After training, the weights in W are divided by the number of training steps (# inputs * # iterations)." ></td>
	<td class="line x" title="196:238	The method is analogous to the RH manual training process for preference in a number of ways." ></td>
	<td class="line x" title="197:238	First, the features used were developed for suitability to a top-down parser, for example taking into account superordinate categories at several levels, some lexical information associated with non-head, left-side siblings of a node, and some right-hand lookahead." ></td>
	<td class="line x" title="198:238	Although only one superordinate category is routinely used in RH preference tests, in order to allow caching of output networks for a category, the preference system allows for and occasionally uses the promotion of non-head features of nested constituents to provide similar capability." ></td>
	<td class="line x" title="199:238	Also, the feature weights obtained by the perceptron training method can be seen to focus on patterns that actually matter in distinguishing correct from incorrect parses, as does RH preference training." ></td>
	<td class="line x" title="200:238	Intuitively, the difference is that while symbolic training for RH explicitly pinpoints patterns that distinguish among parses, the perceptron training method accomplishes something similar by postulating some more general features as negative or positive based on particular examples, but allowing the iterations over a large training set to filter out potentially indicative patterns that do not actually serve as such." ></td>
	<td class="line x" title="201:238	These analogies highlight the fact that preference system training, whether symbolic or corpusbased, is ultimately an empirical engineering exercise." ></td>
	<td class="line x" title="202:238	6 Some Experimental Results Tables 2, 3, and 4 summarize some recent results as obtained by testing on Wall Street Journal section 23 of the Penn Treebank (Marcus et al. 1994)." ></td>
	<td class="line x" title="203:238	The RH results were obtained by about 8 weeks of manual training on the genre." ></td>
	<td class="line x" title="204:238	Table 2 compares speed and coverage for RH and Collins Model3 (Collins, 1999) run on the same CPU." ></td>
	<td class="line x" title="205:238	The table also extrapolates the results to two other parsers, based on reported comparisons with Collins." ></td>
	<td class="line x" title="206:238	One extrapolation is to a very fast stochastic parser by Sagae and Lavie (2005)." ></td>
	<td class="line x" title="207:238	The comparison indicates that the RH parser speed is close to that of the best contemporary parsers." ></td>
	<td class="line x" title="208:238	The second extrapolation is to the LFG XLE parser (Kaplan et al. 2004) for English, consisting of a highly developed symbolic parser and grammar, an OT-based preference component, and a stochastic back end to select among remaining alternative parser outputs." ></td>
	<td class="line x" title="209:238	Two sets of values are given for XLE, one obtained using the full English grammar, and one obtained using a reduced grammar ignoring less-frequently applicable rules." ></td>
	<td class="line x" title="210:238	The extrapolation indicates that the coverage of RH is quite good for a symbolic parser with limited training on an idiom." ></td>
	<td class="line x" title="211:238	While the most important factor in RH parser speed is the enormous speed of the shallow parser, the preference and pruning approach of the overlay parser make contributions to both speed and coverage." ></td>
	<td class="line x" title="212:238	This can be seen in Table 2 by the difference between RH parser results with and without pruning." ></td>
	<td class="line x" title="213:238	Pruning increases coverage because without it more parses exceed imposed resource limits." ></td>
	<td class="line x" title="214:238	Table 3 compares accuracy." ></td>
	<td class="line x" title="215:238	The values for Collins and Sagae/Lavie are based on comparison with treebank data for the entire section 23." ></td>
	<td class="line x" title="216:238	However, because RH does not produce treebank-style tags, the RH values are based only on a random Time No full parse Sagae/Lavie ~ 4 min 1.1% RH Prune 5 min 14 sec 10.8% RH NoPrune 7 min 5 sec 13.9 % Collins m3 16 min.6% XLE reduced ~24 minutes unknown XLE full ~80 minutes ~21% Table 2." ></td>
	<td class="line x" title="217:238	Speeds and Extrapolated speeds Fully accurate F-score Avg cross bkts Sagae/Lavie unknwn 86% unknwn Collins Lbl 33.6% 88.2% 1.05 CollinsNoLbl 35.4% 89.4 % 1.05 RH NoLbl 46% 86 % .59 Table 3." ></td>
	<td class="line x" title="218:238	Accuracy Comparison Average Median RH Base 137.10 11 RH Pref 5.04 2 Table 4." ></td>
	<td class="line x" title="219:238	Highest Scoring Parses per Input 90 100-sentence sample from section 23, and compared using a different unlabeled bracketing standard." ></td>
	<td class="line x" title="220:238	For details see Newman (2007)." ></td>
	<td class="line x" title="221:238	For nonparsed sentences the chunks are bracketed." ></td>
	<td class="line x" title="222:238	Accuracy is not extrapolated to XLE because available measurements give f-scores (all  80%) for dependency relations rather than for bracketed constituents." ></td>
	<td class="line x" title="223:238	As a partial indication of the role and effectiveness of the RH preference system, if non-parsed sentences are ignored, the percentage of fully accurate bracketings shown in Table 3 rises to approximately 46/89 = 51.6% (it is actually larger because coverage is higher on the 100-sentence sample)." ></td>
	<td class="line x" title="224:238	As further indication, Table 4 compares, for section 23, the average and median number of parses per sentence obtained by the base grammar alone (RH Base), and the base grammar plus the preference system (RH Pref).3 The table demonstrates that the preference system is a crucial parser component." ></td>
	<td class="line x" title="225:238	Also, the median of 2 parses per sentence obtained using the preference system explains why the fallback low-attach strategy is successful in many cases." ></td>
	<td class="line x" title="226:238	7 Summary and Directions The primary contribution of this work is in demonstrating the feasibility of a vastly simplified symbolic preference scoring method." ></td>
	<td class="line x" title="227:238	The preference scores assigned are neither 'principle-based', nor 'ad-hoc', but explicitly engineered to facilitate the management of undesirable interactions in the grammar and in the preference system itself." ></td>
	<td class="line x" title="228:238	Restricting individual contexted scores to {-1, 0, +1} addresses the problems of multi-level contexted scoring discussed in Section 2, as follows:  No abstract judgment is required to assign a value to a preference or dispreference." ></td>
	<td class="line x" title="229:238	 Information deficiencies contribute only small dispreferences, so they can often be overcome by preferences." ></td>
	<td class="line x" title="230:238	 Compensating for interactions that are foreseen does not require searching the rules to find necessary compensating values." ></td>
	<td class="line x" title="231:238	 For unforeseen interactions discovered when reviewing parser results, the simplified pref3 The values are somewhat inflated because they include duplicate parses, which have not yet been entirely eliminated." ></td>
	<td class="line x" title="232:238	erence scores facilitate finding the sources of the problems and potential methods of solving them." ></td>
	<td class="line x" title="233:238	This approach to symbolic preference has facilitated development and maintenance of the RH parser, and has enabled the production of results with a speed and accuracy comparable to the best stochastic parsers, even with limited training on an idiom." ></td>
	<td class="line x" title="234:238	An interesting question is why this very simple approach does not seem to have been used previously." ></td>
	<td class="line x" title="235:238	Part of the answer may lie in the lack of explicit recognition that symbolic preference scoring is ultimately an engineering problem, and is only indirectly based on cognitive principles or approximations to frequencies of particular relationships." ></td>
	<td class="line x" title="236:238	Ongoing development of the RH preference system includes continuing refinement based on 'manual' training, and continuing expansion of the set of semantic features used as the parser is applied to new domains." ></td>
	<td class="line x" title="237:238	Additional development will also include more encoding of, and attention to, the expected semantic features of arguments." ></td>
	<td class="line x" title="238:238	Experiments are also planned to examine the accuracy/performance tradeoffs of using additional context information in the preference tests." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1038
Dependency-Based N-Gram Models for General Purpose Sentence Realisation
Guo, Yuqing;Van Genabith, Josef;Wang, Haifeng;"></td>
	<td class="line x" title="1:151	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 297304 Manchester, August 2008 Dependency-Based N-Gram Models for General Purpose Sentence Realisation Yuqing Guo NCLT, School of Computing Dublin City University Dublin 9, Ireland yguo@computing.dcu.ie Josef van Genabith NCLT, School of Computing Dublin City University IBM CAS, Dublin, Ireland josef@computing.dcu.ie Haifeng Wang Toshiba (China) Research & Development Center Beijing, 100738, China wanghaifeng@rdc.toshiba.com.cn Abstract We present dependency-based n-gram models for general-purpose, widecoverage, probabilistic sentence realisation." ></td>
	<td class="line x" title="2:151	Our method linearises unordered dependencies in input representations directly rather than via the application of grammar rules, as in traditional chartbased generators." ></td>
	<td class="line x" title="3:151	The method is simple, efficient, and achieves competitive accuracy and complete coverage on standard English (Penn-II, 0.7440 BLEU, 0.05 sec/sent) and Chinese (CTB6, 0.7123 BLEU, 0.14 sec/sent) test data." ></td>
	<td class="line x" title="4:151	1 Introduction Sentence generation,1 or surface realisation can be described as the problem of producing syntactically, morphologically, and orthographically correct sentences from a given semantic or syntactic representation." ></td>
	<td class="line x" title="5:151	Most general-purpose realisation systems developed to date transform the input into surface form via the application of a set of grammar rules based on particular linguistic theories, e.g. Lexical Functional Grammar (LFG), HeadDriven Phrase Structure Grammar (HPSG), Combinatory Categorial Grammar (CCG), Tree Adjoining Grammar (TAG) etc. These grammar rules are either carefully handcrafted, as those used in FUF/SURGE (Elhadad, 1991), LKB (Carroll et al., c2008." ></td>
	<td class="line x" title="6:151	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="7:151	Some rights reserved." ></td>
	<td class="line x" title="8:151	1In this paper, the term generation is used generally for what is more strictly referred to by the term tactical generation or surface realisation." ></td>
	<td class="line o" title="9:151	1999), OpenCCG (White, 2004) and XLE (Crouch et al., 2007), or created semi-automatically (Belz, 2007), or fully automatically extracted from annotated corpora, like the HPSG (Nakanishi et al., 2005), LFG (Cahill and van Genabith, 2006; Hogan et al., 2007) and CCG (White et al., 2007) resources derived from the Penn-II Treebank (PTB) (Marcus et al., 1993)." ></td>
	<td class="line x" title="10:151	Over the last decade, probabilistic models have become widely used in the field of natural language generation (NLG), often in the form of a realisation ranker in a two-stage generation architecture." ></td>
	<td class="line x" title="11:151	The two-stage methodology is characterised by a separation between generation and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realisation from the space." ></td>
	<td class="line x" title="12:151	By and large, two statistical models are used in the rankers to choose output strings:  N-gram language models over different units, such as word-level bigram/trigram models (Bangalore and Rambow, 2000; Langkilde, 2000), or factored language models integrated with syntactic tags (White et al., 2007)." ></td>
	<td class="line x" title="13:151	 Log-linear models with different syntactic and semantic features (Velldal and Oepen, 2005; Nakanishi et al., 2005; Cahill et al., 2007)." ></td>
	<td class="line x" title="14:151	To date, however, probabilistic models learning direct mappings from generation input to surface strings, without the effort to construct a grammar, have rarely been explored." ></td>
	<td class="line x" title="15:151	An exception is Ratnaparkhi (2000), who presents maximum entropy models to learn attribute ordering and lexical choice for sentence generation from a semantic representation of attribute-value pairs, restricted to an air travel domain." ></td>
	<td class="line x" title="16:151	297 S NP PRP We VP VBP believe PP IN in NP NP DT the NN law PP IN of NP NNS averages f1                   PRED believe TENSE pres SUBJ f2  PRED proPERS 1 NUM pl   OBL f3            PFORM in OBJ f4           PRED law PERS 3 NUM sg SPEC f5 bracketleftbigg DET f6 bracketleftBig PRED the bracketrightBigbracketrightbigg ADJ     f7     PFORM of OBJ f8  PRED averagePERS 3 NUM pl                                                  (a.) c-structure (b.) f-structure string We believe in the law of averages position 1 2 3 4 5 6 7 f1 SUBJ PRED OBL f3 PFORM OBJ f4 SPEC PRED ADJ f7 PFORM OBJ (c.) linearised grammatical functions / bilexical dependencies Figure 1: Cand f-structures for the sentence We believe in the law of averages." ></td>
	<td class="line x" title="17:151	In this paper, we develop an efficient, widecoverage generator based on simple n-gram models to directly linearise dependency relations from the input representations." ></td>
	<td class="line x" title="18:151	Our work is aimed at general-purpose sentence generation but couched in the framework of Lexical Functional Grammar." ></td>
	<td class="line x" title="19:151	We give an overview of LFG and the dependency representations we use in Section 2." ></td>
	<td class="line x" title="20:151	We describe the general idea of our dependency-based generation in Section 3 and give details of the n-gram generation models in Section 4." ></td>
	<td class="line x" title="21:151	Section 5 explains the experiments and provides results for both English and Chinese data." ></td>
	<td class="line x" title="22:151	Section 6 compares the results with previous work and between languages." ></td>
	<td class="line x" title="23:151	Finally we conclude with a summary and outline future work." ></td>
	<td class="line x" title="24:151	2 LFG-Based Generation 2.1 Lexical Functional Grammar Lexical Functional Grammar (Kaplan and Bresnan, 1982) is a constraint-based grammar formalism which postulates (minimally) two levels of representation: c(onstituent)-structure and f(unctional)-structure." ></td>
	<td class="line x" title="25:151	As illustrated in Figure 1, a c-structure is a conventional phrase structure tree and captures surface grammatical configurations." ></td>
	<td class="line x" title="26:151	The f-structure encodes more abstract functional relations like SUBJ(ect), OBJ(ect) and ADJ(unct)." ></td>
	<td class="line x" title="27:151	F-structures are hierarchical attributevalue matrix representations of bilexical labelled dependencies, approximating to basic predicateargument/adjunct structures.2 Attributes in fstructure come in two different types:  Grammatical Functions (GFs) indicate the relationship between the predicate and dependents." ></td>
	<td class="line x" title="28:151	GFs can be divided into:  arguments are subcategorised for by the predicate, such as SUBJ(ect), OBJ(ect), and thus can only occur once in each local f-structure." ></td>
	<td class="line x" title="29:151	 modifiers like ADJ(unct), COORD(inate) are not subcategorised for by the predicate, and can occur any number of times in a local f-structure." ></td>
	<td class="line x" title="30:151	 Atomic-valued features describe linguistic properties of the predicate, such as TENSE, ASPECT, MOOD, PERS, NUM, CASE etc. 2.2 Generation from F-Structures Work on generation in LFG generally assumes that the generation task is to determine the set of strings of the language that corresponds to a specified fstructure, given a particular grammar (Kaplan and Wedekind, 2000)." ></td>
	<td class="line x" title="31:151	Previous work on generation 2F-structures can be also interpreted as quasi-logical forms (van Genabith and Crouch, 1996), which more closely resemble inputs used by some other generators." ></td>
	<td class="line x" title="32:151	298 within LFG includes the XLE,3 Cahill and van Genabith (2006), Hogan et al.(2007) and Cahill et al.(2007)." ></td>
	<td class="line x" title="35:151	The XLE generates sentences from fstructures according to parallel handcrafted grammars for English, French, German, Norwegian, Japanese, and Urdu." ></td>
	<td class="line x" title="36:151	Based on the German XLE resources, Cahill et al.(2007) describe a two-stage, log-linear generation model." ></td>
	<td class="line oc" title="38:151	Cahill and van Genabith (2006) and Hogan et al.(2007) present a chart generator using wide-coverage PCFG-based LFG approximations automatically acquired from treebanks (Cahill et al., 2004)." ></td>
	<td class="line x" title="40:151	3 Dependency-Based Generation: the Basic Idea Traditional LFG generation models can be regarded as the reverse process of parsing, and use bi-directional f-structure-annotated CFG rules." ></td>
	<td class="line x" title="41:151	In a sense, the generation process is driven by an input dependency (or f-structure) representation, but proceeds through the detour of using dependency-annotated CFG (or PCFG) grammars and chart-based generators." ></td>
	<td class="line x" title="42:151	In this paper, we develop a simple n-gram and dependencybased, wide-coverage, robust, probabilistic generation model, which cuts out the middle-man from previous approaches: the CFG-component." ></td>
	<td class="line oc" title="43:151	Our approach is data-driven: following the methodology in (Cahill et al., 2004; Guo et al., 2007), we automatically convert the English PennII treebank and the Chinese Penn Treebank (Xue et al., 2005) into f-structure banks." ></td>
	<td class="line x" title="44:151	F-structures such as Figure 1(b.) are unordered, i.e. they do not carry information on to the relative surface order of local GFs." ></td>
	<td class="line x" title="45:151	In order to generate a string from an f-structure, we need to linearise the GFs (at each level of embedding) in the f-structure (and map lemmas and features to surface forms)." ></td>
	<td class="line x" title="46:151	We do this in terms of n-gram models over GFs." ></td>
	<td class="line x" title="47:151	In order to build the n-gram models, we linearise the fstructures automatically produced from treebanks by associating the numerical string position (word offset from start of the sentence) with the predicate in each local f-structure, producing GF sequences as in Figure 1(c.)." ></td>
	<td class="line x" title="48:151	Even though the n-gram models are exemplified using LFG f-structures, they are general-purpose models and thus suitable for any bilexical labelled dependency (Nivre, 2006) or predicate-argument type representations, such as the labelled feature3http://www2.parc.com/isl/groups/nltt/xle/ value structures used in HALogen and the functional descriptions in the FUF/SURGE system." ></td>
	<td class="line x" title="49:151	4 N-Gram Models for Dependency-Based Generation 4.1 Basic N-Gram Model The primary task of a sentence generator is to determine the linear order of constituents and words, represented as lemmas in predicates in f-structures." ></td>
	<td class="line x" title="50:151	At a particular local f-structure, the task of generating a string covered by the local f-structure is equivalent to linearising all the GFs present at that local f-structure." ></td>
	<td class="line x" title="51:151	E.g. in f4 in Figure 1, the unordered set of local GFs {SPEC, PRED, ADJ} generates the surface sequence the law of averages." ></td>
	<td class="line x" title="52:151	We linearise the GFs in the set by computing n-gram models, similar to traditional wordbased language models, except using the names of GFs (including PRED) instead of words." ></td>
	<td class="line x" title="53:151	Given a (sub-) f-structure F containing m GFs, the ngram model searches for the best surface sequence Sm1 =s1sm generated by the GF linearisation GFm1 = GF1GFm, which maximises the probability P(GFm1 )." ></td>
	<td class="line x" title="54:151	Using n-gram models, P(GFm1 ) is calculated according to Eq.(1)." ></td>
	<td class="line x" title="55:151	P(GFm1 ) = P(GF1GFm) = mproductdisplay k=1 P(GFk|GFk1kn+1) (1) 4.2 Factored N-Gram Models In addition to the basic n-gram model over bare GFs, we integrate contextual and fine-grained lexical information into several factored models." ></td>
	<td class="line x" title="56:151	Eq.(2) additionally conditions the probability of the n-gram on the parent GF label of the current local f-structure fi, Eq.(3) on the instantiated PRED of the local f-structure fi, and Eq.(4) lexicalises the model, where each GF is augmented with its own predicate lemma." ></td>
	<td class="line x" title="57:151	Pg(GFm1 ) = mproductdisplay k=1 P(GFk|GFk1kn+1,GFi) (2) Pp(GFm1 ) = mproductdisplay k=1 P(GFk|GFk1kn+1,Predi) (3) Pl(GFm1 ) = mproductdisplay k=1 P(Lexk|Lexk1kn+1) (4) 299 To avoid data sparseness, the factored n-gram models Pf are smoothed by linearly interpolating the basic n-gram model P, as in Eq.(5)." ></td>
	<td class="line x" title="58:151	Pf(GFm1 ) = Pf(GFm1 ) + (1)P(GFm1 ) (5) Additionally, the lexicalised n-gram models Pl are combined with the other two models conditioned on the additional parent GF Pg and PRED Pp, as shown in Eqs." ></td>
	<td class="line x" title="59:151	(6) & (7), respectively." ></td>
	<td class="line x" title="60:151	Plg(GFm1 ) = 1Pl(GFm1 ) +2Pg(GFm1 ) +3P(GFm1 ) (6) Plp(GFm1 ) = 1Pl(GFm1 ) + 2Pp(GFm1 ) +3P(GFm1 ) (7) where summationtexti = 1 Table 1 exemplifies the different n-gram models for the local f-structure f4 in Figure 1." ></td>
	<td class="line x" title="61:151	Model N-grams Cond." ></td>
	<td class="line x" title="62:151	basic (P) SPEC PRED ADJ gf (Pg) SPEC PRED ADJ OBL pred (Pp) SPEC PRED ADJ law lex (Pl) SPEC PRED[law] ADJ[of] Table 1: Examples of n-grams for f4 in Figure 1 Besides grammatical functions, we also make use of atomic-valued features like TENSE, PERS, NUM (etc.) to aid linearisation." ></td>
	<td class="line x" title="63:151	The attributes and values of these features are integrated into the GF n-grams for disambiguation (see Section 5.2)." ></td>
	<td class="line x" title="64:151	4.3 Generation Algorithm Our basic n-gram based generation model implements the simplifying assumption that linearisation at one sub-f-structure is independent of linearisation at any other sub-f-structures." ></td>
	<td class="line x" title="65:151	This assumption is feasible for projective dependencies." ></td>
	<td class="line x" title="66:151	In most cases (at least in English and Chinese), non-projective dependencies are only used to account for Long-Distance Dependencies (LDDs)." ></td>
	<td class="line x" title="67:151	Consider sentence (1) discussed in Carroll et al.(1999) and its corresponding fstructure in Figure 2." ></td>
	<td class="line x" title="69:151	In LFG f-structures, LDDs are represented via reentrancies between dislocated TOPIC, TOPIC REL, FOCUS (etc.) GFs and source GFs subcategorised for by local predicates, but only the dislocated GFs are instantiated in generation." ></td>
	<td class="line x" title="70:151	Therefore traces of the source GFs in input f-structures are removed before generation, and non-projective dependencies are transformed into simple projective dependencies." ></td>
	<td class="line x" title="71:151	(1) How quickly did the newspapers say the athlete ran?" ></td>
	<td class="line x" title="72:151	              FOCUS  PRED quickly ADJ braceleftbiggbracketleftBig PRED how bracketrightBigbracerightbigg   1 PRED say SUBJ   PRED newspaper SPEC bracketleftBig PRED the bracketrightBig   COMP      PRED run SUBJ   PRED athlete SPEC bracketleftBig PRED the bracketrightBig   ADJ 1                    Figure 2: schematic f-structure for How quickly did the newspapers say the athlete ran?" ></td>
	<td class="line x" title="73:151	In summary, given an input f-structure f, the core algorithm of the generator recursively traverses f and at each sub-f-structure fi: 1." ></td>
	<td class="line x" title="74:151	instantiates the local predicate at fi and performs inflections/declensions if necessary 2." ></td>
	<td class="line x" title="75:151	calculates the GF linearisations present at fi by n-gram models 3." ></td>
	<td class="line x" title="76:151	finds the most probable GF sequence among all possibilities by Viterbi search 4." ></td>
	<td class="line x" title="77:151	generates the string covered by fi according to the linearised GFs 5 Experiments and Evaluation To test the performance and coverage of our ngram-based generation models, experiments are carried out for both English and Chinese, two languages with distinct properties." ></td>
	<td class="line x" title="78:151	5.1 Experiment Design Experiments on English data are carried out on the WSJ portion of the PTB, using standard training/test/development splits, viz 39,832 sentences from sections 02-21 are used for training, 2,416 sentences from section 23 for testing, while 1,700 sentences from section 22 are held out for development." ></td>
	<td class="line x" title="79:151	The latest version of the Penn Chinese Treebank 6.0 (CTB6), excluding the portion of ACE broadcast news, is used for experiments on Chinese data.4 We follow the recommended splits (in the list-of-file of CTB6) to divide the data into test set, development set and training set." ></td>
	<td class="line x" title="80:151	The training set includes 756 files with a total of 15,663 sentences." ></td>
	<td class="line x" title="81:151	The test set includes 84 files with 1,708 4Sentences labelled as fragment are not included in our development and test set." ></td>
	<td class="line x" title="82:151	300 sentences." ></td>
	<td class="line x" title="83:151	The development set includes 50 files with 1,116 sentences." ></td>
	<td class="line x" title="84:151	Table 2 shows some of the characteristics of the English and Chinese data obtained from the development sets." ></td>
	<td class="line x" title="85:151	Development Set English Chinese num of sent 1,700 1,116 max length of sent (#words) 110 145 ave length of sent (#words) 23 31 num of local fstr 23,289 15,847 num of local fstr per sent 13.70 14.20 max length of local fstr (#gfs) 12 16 ave length of local fstr (#gfs) 2.56 2.90 Table 2: Comparison English and Chinese data The n-gram models are created using the SRILM toolkit (Stolcke, 2002) with Good-Turning smoothing for both the Chinese and English data." ></td>
	<td class="line x" title="86:151	For morphological realisation of English, a set of lexical macros is automatically extracted from the training data." ></td>
	<td class="line x" title="87:151	This is not required for Chinese surface realisation as Chinese has very little morphology." ></td>
	<td class="line x" title="88:151	Lexical macro examples are listed in Table 3." ></td>
	<td class="line x" title="89:151	lexical macro surface word pred=law, num=sg, pers=3 law pred=average, num=pl, pers=3 averages pred=believe, num=pl, tense=pres believe Table 3: Examples of lexical macros The input to our generator are unordered fstructures automatically derived from the development and test set trees of our treebanks, which do not contain any string position information." ></td>
	<td class="line x" title="90:151	But, due to the particulars of the automatic f-structure annotation algorithm, the order of sub-f-structures in set-valued GFs, such as ADJ, COORD, happens to correspond to their surface order." ></td>
	<td class="line x" title="91:151	To avoid unfairly inflating evaluation results, we lexically reorder the GFs in each sub-f-structure of the development and test input before the generation process." ></td>
	<td class="line x" title="92:151	This resembles the permute, no dir type experiment in (Langkilde, 2002)." ></td>
	<td class="line x" title="93:151	5.2 Experimental Results Following (Langkilde, 2002) and other work on general-purpose generators, BLEU score (Papineni et al., 2002), average NIST simple string accuracy (SSA) and percentage of exactly matched sentences are adopted as evaluation metrics." ></td>
	<td class="line x" title="94:151	As our system guarantees that all input fstructures can generate a complete sentence, special coverage-dependent evaluation (as has been adopted in most grammar-based generation systems) is not necessary in our experiments." ></td>
	<td class="line x" title="95:151	Experiments are carried out on an Intel Pentium 4 server, with a 3.80GHz CPU and 3GB memory." ></td>
	<td class="line x" title="96:151	It takes less than 2 minutes to generate all 2,416 sentences (with average sentence length of 21 words) of WSJ section 23 (average 0.05 sec per sentence), and approximately 4 minutes to generate 1,708 sentences (with average sentence length of 30 words) of CTB test data (average 0.14 sec per sentence), using 4-gram models in all experiments." ></td>
	<td class="line x" title="97:151	Our evaluation results for English and Chinese data are shown in Tables 4 and 5, respectively." ></td>
	<td class="line x" title="98:151	Different n-gram models perform nearly consistently in all the experiments on both English and Chinese data." ></td>
	<td class="line x" title="99:151	The results show that factored ngram models outperform the basic n-gram models, and in turn the combined n-gram models outperform single n-gram models." ></td>
	<td class="line x" title="100:151	The combined model interpolating n-grams over lexicalised GFs with ngrams conditioned on PRED achieves the best results in both experiments on English (with feature names) and Chinese (with feature names & values), with BLEU scores of 0.7440 and 0.7123 respectively, and full coverage." ></td>
	<td class="line x" title="101:151	Lexicalisation plays an important role in both English and Chinese, boosting the BLEU score without features from 0.5074 to 0.6741 for English, and from 0.5752 to 0.6639 for Chinese." ></td>
	<td class="line x" title="102:151	Atomic-valued features play an important role in English, and boost the BLEU score from 0.5074 in the baseline model to 0.6842 when feature names are integrated into the n-gram models." ></td>
	<td class="line x" title="103:151	However, feature names in Chinese only increase the BLEU score from 0.5752 to 0.6160." ></td>
	<td class="line x" title="104:151	This is likely to be the case as English has a richer morphology than Chinese, and important function words such as if, to, that are encoded in atomic-valued features in English f-structures, which helps to determine string order." ></td>
	<td class="line x" title="105:151	However, combined feature names and values work better on Chinese data, but turn out to hurt the n-gram model performance for English data." ></td>
	<td class="line x" title="106:151	This may suggest that the feature names in English already include enough information, while the value of morphological features, such as TENSE, NUM does not provide any new information to help determine word order, but aggravate data sparseness instead." ></td>
	<td class="line x" title="107:151	301 WSJ Sec23 Without Features Feature Names Feature Names & Values Model ExMatch BLEU SSA ExMatch BLEU SSA ExMatch BLEU SSA baseline 5.30% 0.5074 57.29% 15.27% 0.6842 69.48% 15.15% 0.6829 69.15% gf 6.62% 0.5318 60.06% 16.76% 0.6969 71.51% 16.68% 0.6977 71.55% pred 8.03% 0.5697 60.73% 16.72% 0.7035 70.12% 16.76% 0.7042 71.08% lex 12.87% 0.6741 69.43% 19.41% 0.7384 74.76% 18.96% 0.7375 74.12% lex+gf 12.62% 0.6611 69.41% 19.70% 0.7388 74.98% 19.74% 0.7405 75.08% lex+pred 12.25% 0.6569 68.04% 19.83% 0.7440 75.34% 19.58% 0.7422 75.04% Table 4: Results for English Penn-II WSJ section 23 Test Without Features Feature Names Feature Names & Values Model ExMatch BLEU SSA ExMatch BLEU SSA ExMatch BLEU SSA baseline 8.96% 0.5752 51.92% 11.77% 0.6160 54.64% 12.30% 0.6239 55.20% gf 9.54% 0.6009 53.02% 12.53% 0.6391 55.78% 13.47% 0.6486 56.60% pred 10.07% 0.6180 53.80% 13.35% 0.6608 56.72% 14.46% 0.6720 57.67% lex 13.93% 0.6639 59.61% 15.16% 0.6770 60.44% 15.98% 0.6804 60.20% lex+gf 14.81% 0.6773 59.92% 15.52% 0.6911 60.97% 16.80% 0.6957 61.07% lex+pred 16.04% 0.6952 60.82% 16.22% 0.7060 61.45% 17.51% 0.7123 61.54% Table 5: Results for Chinese CTB6 test data WSJ Sec23 Sentence length  20 words All sentences Coverage ExMatch BLEU SSA Coverage ExMatch BLEU SSA Langkilde(2002) 82.7% 28.2% 0.757 69.6% Callaway(2003) 98.7% 49.0% 88.84% Nakanishi(2005) 90.75% 0.7733 83.6% 0.705 Cahill(2006) 98.65% 0.7077 73.73% 98.05% 0.6651 68.08% Hogan(2007) 100% 0.7139 99.96% 0.6882 70.92% White(2007) 94.3% 6.9% 0.5768 this paper 100% 35.40% 0.7625 81.09% 100% 19.83% 0.7440 75.34% Table 6: Cross system comparison of results for English WSJ section 23 6 Discussion 6.1 Comparison to Previous Work It is very difficult to compare sentence generators since the information contained in the input representation varies greatly between systems." ></td>
	<td class="line x" title="108:151	The most direct comparison is between our system and those presented in Cahill and van Genabith (2006) and Hogan et al.(2007), as they also use treebankbased automatically generated f-structures as the generator inputs." ></td>
	<td class="line x" title="110:151	The labelled feature-value structures used in HALogen (Langkilde, 2002) and functional descriptions in FUF/SURGE (Callaway, 2003) also bear some broad similarities to our fstructures." ></td>
	<td class="line x" title="111:151	A number of systems using different input but adopting the same evaluation metrics and testing on the same data are listed in Table 6." ></td>
	<td class="line x" title="112:151	Surprisingly (or not), the best results are achieved by a purely symbolic generation systemFUF/SURGE (Callaway, 2003)." ></td>
	<td class="line x" title="113:151	However the approach uses handcrafted grammars which are very time-consuming to produce and adapt to different languages and domains." ></td>
	<td class="line x" title="114:151	Langkilde (2002) reports results for experiments with varying levels of linguistic detail in the input given to the generator." ></td>
	<td class="line x" title="115:151	The type permute, no dir is most comparable to the level of information contained in our f-structure in that the modifiers (adjuncts, coordinates etc.) in the input are not ordered." ></td>
	<td class="line x" title="116:151	However her labelled feature-value structure is more specific than our f-structure as it also includes syntactic properties such as part-of-speech, which might contribute to the higher BLEU score of HALogen." ></td>
	<td class="line x" title="117:151	And moreover, in HALogen nearly 20% of the sentences are only partially generated (or not at all)." ></td>
	<td class="line x" title="118:151	Nakanishi et al.(2005) carry out experiments on sentences up to 20 words, with BLEU scores slightly higher than ours." ></td>
	<td class="line x" title="120:151	However their results without sentence length limitation (listed in the right column), for 500 sentences randomly selected from WSJ Sec22 are lower than ours, even at a lower coverage." ></td>
	<td class="line x" title="121:151	Overall our system is competitive, with best results for coverage (100%), second best for BLEU and SSA scores, and third best overall on exact match." ></td>
	<td class="line x" title="122:151	However, we admit that automatic metrics such as BLEU are not fully reliable to compare different systems, and results vary widely depending on the coverage of the systems and the specificity of the generation input." ></td>
	<td class="line x" title="123:151	302 6.2 Error Analysis and Differences Between the Languages Though our dependency-based n-gram models perform well in both the English and Chinese experiments, we are surprised that experiments on English data produce better results than those for Chinese." ></td>
	<td class="line x" title="124:151	It is widely accepted that English generation is more difficult than Chinese, due to morphological inflections and the somewhat less predictable word order of English compared to Chinese." ></td>
	<td class="line x" title="125:151	This is reflected by the results of the baseline models." ></td>
	<td class="line x" title="126:151	Chinese has a BLEU score of 0.5752 and 8.96% exact match, both are higher than those of English." ></td>
	<td class="line x" title="127:151	However with feature augmentation and lexicalisation, the results for English data exceed Chinese." ></td>
	<td class="line x" title="128:151	This is probably because of the following reasons: Data size of the English training set is more than twice that of Chinese." ></td>
	<td class="line x" title="129:151	Grammatical functions are more fine-grained in English f-structures than those in Chinese." ></td>
	<td class="line x" title="130:151	There are 32 GFs defined for English compared to 20 for Chinese in our input f-structures." ></td>
	<td class="line x" title="131:151	Properties of the languages and data sets are different." ></td>
	<td class="line x" title="132:151	For example, due to lack of inflection and case markers, many sequences of VPs in Chinese have to be treated as coordinates, whereas their counterparts in English act as different grammatical functions, e.g.(2)." ></td>
	<td class="line x" title="134:151	(2) G5CL DEAP B8GNGXDRGREN invest million build this construction invest million yuan to build the construction This results in a total of 7,377 coordinates (4.32 per sentence) in the Chinese development data, compared to 2,699 (1.12 per sentence) in the English data." ></td>
	<td class="line x" title="135:151	The most extreme case in the Chinese data features 14 coordinates of country names in a local f-structure." ></td>
	<td class="line x" title="136:151	This may account for the low SSA score for the Chinese experiments, as many coordinates are tied in the n-gram scoring method and can not be ordered correctly." ></td>
	<td class="line x" title="137:151	Examining the development data shows different types of coordination errors:  syntactic coordinates, but not semantic coordinates, as in sentence (2)." ></td>
	<td class="line x" title="138:151	 syntactic and semantic coordinates, but usually expressed in a fixed order, e.g.(3)." ></td>
	<td class="line x" title="140:151	(3) CDDK D1E8 reform opening-up reform and opening up  syntactic and semantic coordinates, which can freely swap positions, e.g.(4)." ></td>
	<td class="line x" title="142:151	(4) FBATAR EWGD G2 EVB0ARCVE1 plentiful energy and quick thinking energetic and agile At the current stage, our n-gram generation model only keeps the most likely realisation for each local f-structure." ></td>
	<td class="line x" title="143:151	We believe that packing all equivalent elements, like coordinates in a local fstructure into equivalent classes, and outputing nbest candidate realisations will greatly increase the SSA score and may also further benefit the efficiency of the algorithm." ></td>
	<td class="line x" title="144:151	7 Conclusions and Further Work We have described a number of increasingly sophisticated n-gram models for sentence generation from labelled bilexical dependencies, in the form of LFG f-structures." ></td>
	<td class="line x" title="145:151	The models include additional conditioning on parent GFs and different degrees of lexicalisation." ></td>
	<td class="line x" title="146:151	Our method is simple, highly efficient, broad coverage and accurate in practice." ></td>
	<td class="line x" title="147:151	We present experiments on English and Chinese, showing that the method generalises well to different languages and data sets." ></td>
	<td class="line x" title="148:151	We are currently exploring further combinations of conditioning context and lexicalisation, application to different languages and to dependency representations used to train state-of-the-art dependency parsers (Nivre, 2006)." ></td>
	<td class="line x" title="149:151	Acknowledgments This research is funded by Science Foundation Ireland grant 04/IN/I527." ></td>
	<td class="line x" title="150:151	We thank Aoife Cahill for providing the treebank-based LFG resources for the English data." ></td>
	<td class="line x" title="151:151	We gratefully acknowledge the feedback provided by our anonymous reviewers." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="Data not found"></td>
	<td class="line x" title="1:100	Parser-Based Retraining for Domain Adaptation of Probabilistic Generators Deirdre Hogan, Jennifer Foster, Joachim Wagner and Josef van Genabith National Centre for Language Technology School of Computing Dublin City University Ireland {dhogan, jfoster, jwagner, josef}@computing.dcu.ie Abstract While the effect of domain variation on Penntreebank-trainedprobabilisticparsershasbeen investigated in previous work, we study its effect on a Penn-Treebank-trained probabilistic generator." ></td>
	<td class="line x" title="2:100	We show that applying the generator to data from the British National Corpus results in a performance drop (from a BLEU score of 0.66 on the standard WSJ test set to a BLEU score of 0.54 on our BNC test set)." ></td>
	<td class="line x" title="3:100	We develop a generator retraining method where the domain-specific training data is automatically produced using state-of-the-art parser output." ></td>
	<td class="line x" title="4:100	The retraining method recovers a substantial portion of the performance drop, resulting in a generator which achieves a BLEU score of 0.61 on our BNC test data." ></td>
	<td class="line x" title="5:100	1 Introduction Grammars extracted from the Wall Street Journal (WSJ) section of the Penn Treebank have been successfully applied to natural language parsing, and more recently, to natural language generation." ></td>
	<td class="line x" title="6:100	It is clear that high-quality grammars can be extracted for the WSJ domain but it is not so clear how these grammars scale to other text genres." ></td>
	<td class="line x" title="7:100	Gildea (2001), for example, has shown that WSJ-trained parsers suffer a drop in performance when applied to the more varied sentences of the Brown Corpus." ></td>
	<td class="line x" title="8:100	We investigate the effect of domain variation in treebank-grammar-based generation by applying a WSJ-trained generator to sentences from the British National Corpus (BNC)." ></td>
	<td class="line x" title="9:100	As with probabilistic parsing, probabilistic generation aims to produce the most likely output(s) given the input." ></td>
	<td class="line x" title="10:100	We can distinguish three types of probabilistic generators, based on the type of probability model used to select the most likely sentence." ></td>
	<td class="line x" title="11:100	The first type uses an n-gram language model, e.g.(Langkilde, 2000), the second type uses a probability model defined over trees or feature-structureannotated trees, e.g.(Cahill and van Genabith, 2006), and the third type is a mixture of the first and second type, employing n-gram and grammarbased features, e.g.(Velldal and Oepen, 2005)." ></td>
	<td class="line x" title="15:100	The generator used in our experiments is an instance of the second type, using a probability model defined over Lexical Functional Grammar c-structure and f-structure annotations (Cahill and van Genabith, 2006; Hogan et al., 2007)." ></td>
	<td class="line x" title="16:100	In an initial evaluation, we apply our probabilistic WSJ-trained generator to BNC material, and show that the generator suffers a substantial performance degradation, with a drop in BLEU score from 0.66 to 0.54." ></td>
	<td class="line x" title="17:100	We then turn our attention to the problem of adapting the generator so that it can more accurately generate the 1,000 sentences in our BNC test set." ></td>
	<td class="line x" title="18:100	The problem of adapting any NLP system to a domain different from the domain upon which it has been trained and for which no gold standard training material is available is a very real one, and one which has been the focus of much recent research in parsing." ></td>
	<td class="line x" title="19:100	Somesuccesshasbeenachievedbytraining a parser, not on gold standard hand-corrected trees, but on parser output trees." ></td>
	<td class="line x" title="20:100	These parser output trees can by produced by a second parser in a co-training scenario (Steedman et al., 2003), or by the same parser with a reranking component in a type of selftraining scenario(McCloskyetal., 2006)." ></td>
	<td class="line x" title="21:100	Wetackle 165 the problem of domain adaptation in generation in a similar way, by training the generator on domain specific parser output trees instead of manually corrected gold standard trees." ></td>
	<td class="line x" title="22:100	This experiment achieves promising results, with an increase in BLEU score from 0.54 to 0.61." ></td>
	<td class="line x" title="23:100	The method is generic and can be applied to other probabilistic generators (for which suitable training material can be automatically produced)." ></td>
	<td class="line x" title="24:100	2 Background The natural language generator used in our experiments is the WSJ-trained system described in Cahill and van Genabith (2006) and Hogan et al.(2007)." ></td>
	<td class="line x" title="26:100	Sentences are generated from Lexical Functional Grammar (LFG) f-structures (Kaplan and Bresnan, 1982)." ></td>
	<td class="line oc" title="27:100	The f-structures are created automatically by annotating nodes in the gold standard WSJ trees with LFG functional equations and then passing these equations through a constraint solver (Cahill et al., 2004)." ></td>
	<td class="line x" title="28:100	The generation algorithm is a chartbased one which works by finding the most probable tree associated with the input f-structure." ></td>
	<td class="line x" title="29:100	The yield of the most probable tree is the output sentence." ></td>
	<td class="line x" title="30:100	An annotated PCFG, in which the nonterminal symbols are decorated with functional information, is used to generate the most probable tree from an f-structure." ></td>
	<td class="line x" title="31:100	Cahill and van Genabith (2006) attain 98.2% coverage and a BLEU score of 0.6652 on the standard WSJ test set (Section 23)." ></td>
	<td class="line x" title="32:100	Hogan et al.(2007) describe an extension to the system which replaces the annotated PCFG selection model with a more sophisticated history-based probabilistic model." ></td>
	<td class="line x" title="34:100	Instead of conditioning the righthand side of a rule on the lefthand non-terminal and its associated functional information alone, the new model includes non-local conditioning information in the form of functional information associated with ancestor nodes of the lefthand side category." ></td>
	<td class="line x" title="35:100	This system achieves a BLEU score of 0.6724 and 99.9% coverage." ></td>
	<td class="line x" title="36:100	Other WSJ-trained generation systems include Nakanishi et al.(2005) and White et al.(2007)." ></td>
	<td class="line x" title="39:100	Nakanishi et al.(2005) describe a generator trained on a HPSG grammar derived from the WSJ Section of the Penn Treebank." ></td>
	<td class="line x" title="41:100	On sentences of  20 words in length, their system attains coverage of 90.75% and a BLEU score of 0.7733." ></td>
	<td class="line x" title="42:100	White et al.(2007) describe a CCG-based realisation system which has been trained on logical forms derived from CCGBank (Hockenmaier and Steedman, 2005), achieving 94.3% coverage and a BLEU score of 0.5768 on WSJ23 for all sentence lengths." ></td>
	<td class="line x" title="44:100	The input structures upon which these systems are trained vary in form and specificity, but what the systems have in common is that their various input structures are derived from Penn Treebank trees." ></td>
	<td class="line x" title="45:100	3 The BNC Test Data The new English test set consists of 1,000 sentences taken from the British National Corpus (Burnard, 2000)." ></td>
	<td class="line x" title="46:100	The BNC is a one hundred million word balanced corpus of British English from the late twentieth century." ></td>
	<td class="line x" title="47:100	Ninety per cent of it is written text, and the remaining 10% consists of transcribed spontaneous and scripted spoken language." ></td>
	<td class="line x" title="48:100	The BNC sentences in the test set are not chosen completely at random." ></td>
	<td class="line x" title="49:100	Each sentence in the test set has the property of containing a word which appears as a verb in the BNC but not in the usual training sections of the Wall Street Journal section of the Penn Treebank (WSJ02-21)." ></td>
	<td class="line x" title="50:100	Sentences were chosen in this way so that the resulting test set would be a difficult one for WSJ-trained systems." ></td>
	<td class="line x" title="51:100	In order to produce input f-structures for the generator, the test sentences were manually parsed by one annotator, using as references the Penn Treebank trees themselves and the Penn Treebank bracketing guidelines (Bies et al., 1995)." ></td>
	<td class="line x" title="52:100	When the two references did not agree, the guidelines took precedence over the Penn Treebank trees." ></td>
	<td class="line x" title="53:100	Difficult parsing decisions were documented." ></td>
	<td class="line x" title="54:100	Due to time constraints, the annotator did not mark functional tags or traces." ></td>
	<td class="line oc" title="55:100	The context-free gold standard parse trees were transformed into fstructures using the automatic procedure of Cahill et al.(2004)." ></td>
	<td class="line x" title="57:100	4 Experiments Experimental Setup In our first experiment, we apply the original WSJ-trained generator to our BNC test set." ></td>
	<td class="line x" title="58:100	The gold standard trees for our BNC test set differ from the gold standard Wall Street Journal trees, in that they do not contain Penn-II traces or functional tags." ></td>
	<td class="line x" title="59:100	The process which pro166 duces f-structures from trees makes use of trace and functional tag information, if available." ></td>
	<td class="line x" title="60:100	Thus, to ensure that the training and test input f-structures are created in the same way, we use a version of the generator which is trained using gold standard WSJ trees without functional tag or trace information." ></td>
	<td class="line x" title="61:100	When we test this system on the WSJ23 f-structures (produced in the same way as the WSJ training material), the BLEU score decreases slightly from 0.67 to 0.66." ></td>
	<td class="line x" title="62:100	This is our baseline system." ></td>
	<td class="line x" title="63:100	In a further experiment, we attempt to adapt the generator to BNC data by using BNC trees as training material." ></td>
	<td class="line x" title="64:100	Because we lack gold standard BNC trees (apart from those in our test set), we try instead to use parse trees produced by an accurate parser." ></td>
	<td class="line x" title="65:100	We choose the Charniak and Johnson reranking parser because it is freely available and achievesstate-of-the-artaccuracy(aParsevalf-score of 91.3%) on the WSJ domain (Charniak and Johnson, 2005)." ></td>
	<td class="line x" title="66:100	It is, however, affected by domain variation  Foster et al.(2007) report that its f-score drops by approximately 8 percentage points when applied to the BNC domain." ></td>
	<td class="line x" title="68:100	Our training size is 500,000 sentences." ></td>
	<td class="line x" title="69:100	We conduct two experiments: the first, in which 500,000 sentences are extracted randomly from the BNC (minus the test set sentences), and the second in which only shorter sentences, of length  20 words, are chosen as training material." ></td>
	<td class="line x" title="70:100	The rationale behind the second experiment is that shorter sentences are less likely to contain parser errors." ></td>
	<td class="line x" title="71:100	We use the BLEU evaluation metric for our experiments." ></td>
	<td class="line x" title="72:100	We measure both coverage and full coverage." ></td>
	<td class="line x" title="73:100	Coverage measures the number of cases for which the generator produced some kind of output." ></td>
	<td class="line x" title="74:100	Full coverage measures the number of cases for which the generator produced a tree spanning all of the words in the input." ></td>
	<td class="line x" title="75:100	Results The results of our experiments are shown in Fig." ></td>
	<td class="line x" title="76:100	1." ></td>
	<td class="line x" title="77:100	The first row shows the results we obtain when the baseline system is applied to the fstructures derived from the 1,000 BNC gold standard parse trees." ></td>
	<td class="line x" title="78:100	The second row shows the results on the same test set for a system trained on Charniak and Johnson parser output trees for 500,000 BNC sentences." ></td>
	<td class="line x" title="79:100	The results in the final row are obtained by training the generator on Charniak and Johnson parser output trees for 500,000 BNC sentences of length  20 words in length." ></td>
	<td class="line x" title="80:100	Discussion As expected, the performance of the baseline system degrades when faced with out-ofdomain test data." ></td>
	<td class="line x" title="81:100	The BLEU score drops from a 0.66 score for WSJ test data to a 0.54 score for the BNC test data, and full coverage drops from 85.97% to 68.77%." ></td>
	<td class="line x" title="82:100	There is a substantial improvement, however, when the generator is trained on BNC data." ></td>
	<td class="line x" title="83:100	The BLEU score jumps from 0.5358 to 0.6135." ></td>
	<td class="line x" title="84:100	There are at least two possible reasons why a BLEU score of 0.66 is not obtained: The first is that the quality of the f-structure-annotated trees upon which the generator has been trained has degraded." ></td>
	<td class="line x" title="85:100	For the baseline system, the generator is trained on f-structure-annotated trees derived from gold trees." ></td>
	<td class="line x" title="86:100	The new system is trained on f-structureannotated parser output trees, and the performance of Charniak and Johnsons parser degrades when applied to BNC data (Foster et al., 2007)." ></td>
	<td class="line x" title="87:100	The second reason has been suggested by Gildea (2001): WSJ dataiseasiertolearnthanthemorevarieddatainthe Brown Corpus or BNC." ></td>
	<td class="line x" title="88:100	Perhaps even if gold standard BNC parse trees were available for training, the system would not behave as well as it does for WSJ material." ></td>
	<td class="line x" title="89:100	It is interesting to note that training on 500,000 shorter sentences does not appear to help." ></td>
	<td class="line x" title="90:100	We hypothesized that it would improve results because shorter sentences are less likely to contain parser errors." ></td>
	<td class="line x" title="91:100	The drop in full coverage from 86.69% to 79.58% suggests that the number of short sentences needs to be increased so that the size of the training material stays constant." ></td>
	<td class="line x" title="92:100	5 Conclusion We have investigated the effect of domain variation on a LFG-based WSJ-trained generation system by testing the systems performance on 1,000 sentences from the British National Corpus." ></td>
	<td class="line x" title="93:100	PerformancedropsfromaBLEUscoreof0.66onWSJtest data to 0.54 on the BNC test set." ></td>
	<td class="line x" title="94:100	Encouragingly, we have also shown that domain-specific training material produced by a parser can be used to claw back a significant portion of this performance degradation." ></td>
	<td class="line x" title="95:100	Our method is general and could be applied to other WSJ-trained generators (e.g.(Nakanishi et 167 Train BLEU Coverage Full Coverage WSJ02-21 0.5358 99.1 68.77 BNC(500k) 0.6135 99.1 86.69 BNC(500k)  20 words 0.5834 99.1 79.58 Figure 1: Results for 1,000 BNC Sentences al., 2005; White et al., 2007))." ></td>
	<td class="line x" title="97:100	We intend to continue this research by training our generator on parse trees produced by a BNC-self-trained version of the CharniakandJohnsonrerankingparser(Fosteretal., 2007)." ></td>
	<td class="line x" title="98:100	Wealsohopetoextendtheevaluationbeyond the BLEU metric by carrying out a human judgement evaluation." ></td>
	<td class="line x" title="99:100	Acknowledgments This research has been supported by the Enterprise Ireland Commercialisation Fund (CFTD/2007/229), Science Foundation Ireland (04/IN/I527) and the IRCSET Embark Initative (P/04/232)." ></td>
	<td class="line x" title="100:100	We thank the Irish Centre for High End Computing for providing computing facilities." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="Data not found"></td>
	<td class="line x" title="1:142	Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 3643 Manchester, August 2008 Large Scale Production of Syntactic Annotations to Move Forward Patrick Paroubek, Anne Vilnat, Sylvain Loiseau LIMSI-CNRS BP 133 91403 Orsay Cedex France prenom.nom@limsi.fr Gil Francopoulo Tagmatica 126 rue de Picpus 75012 Paris France gil.francopoulo@tagmatica.com Olivier Hamon ELDA and LIPN-P13 55-57 rue Brillat-Savarin 75013 Paris, France hamon@elda.org Eric Villemonte de la Clergerie Alpage-INRIA Dom." ></td>
	<td class="line x" title="2:142	de Voluceau Rocquencourt, B.P. 105, 78153 Le Chesnay, France Eric.De La Clergerie@inria.fr Abstract This article presents the methodology of the PASSAGE project, aiming at syntactically annotating large corpora by composing annotations." ></td>
	<td class="line x" title="3:142	It introduces the annotation format and the syntactic annotation specifications." ></td>
	<td class="line x" title="4:142	It describes an important component of the methodolgy, namely an WEB-based evaluation service, deployed in the context of the first PASSAGE parser evaluation campaign." ></td>
	<td class="line x" title="5:142	1 Introduction The last decade has seen, at the international level, the emergence of a very strong trend of researches on statistical methods in Natural Language Processing." ></td>
	<td class="line x" title="6:142	In our opinion, one of its origins, in particular for English, is the availability of large annotated corpora, such as the Penn Treebank (1M words extracted from the Wall Street journal, with syntactic annotations; 2nd release in 19951, the British National Corpus (100M words covering various styles annotated with parts of speech2), or the Brown Corpus (1M words with morphosyntactic annotations)." ></td>
	<td class="line x" title="7:142	Such annotated corpora were very valuable to extract stochastic grammars or to parametrize disambiguation algorithms." ></td>
	<td class="line x" title="8:142	For instance (Miyao et al., 2004) report an experiment where an HPSG grammar is semi-automatically aquired from the Penn Treebank, by first annotating the treebank with partially specified derivation c2008." ></td>
	<td class="line x" title="9:142	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="10:142	Some rights reserved." ></td>
	<td class="line x" title="11:142	1http://www.cis.upenn.edu/treebank/ 2http://www.natcorp.ox.ac.uk/ trees using heuristic rules , then by extracting lexical entries with the application of inverse grammar rules." ></td>
	<td class="line oc" title="12:142	(Cahill et al., 2004) managed to extract LFG subcategorisation frames and paths linking long distance dependencies reentrancies from f-structures generated automatically for the PennII treebank trees and used them in an long distance dependency resolution algorithm to parse new text." ></td>
	<td class="line p" title="13:142	They achieved around 80% f-score for fstructures parsing on the WSJ part of the Penn-II treebank, a score comparable to the ones of the state-ofthe-art hand-crafted grammars." ></td>
	<td class="line x" title="14:142	With similar results, (Hockenmaier and Steedman, 2007) translated the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations augmented with local and long-range word to word dependencies and used it to train wide-coverage statistical parsers." ></td>
	<td class="line x" title="15:142	The development of the Penn Treebank have led to many similar proposals of corpus annotations3." ></td>
	<td class="line x" title="16:142	However, the development of such treebanks is very costly from an human point of view and represents a long standing effort, in particular for getting of rid of the annotation errors or inconsistencies, unavoidable for any kind of human annotation." ></td>
	<td class="line x" title="17:142	Despite the growing number of annotated corpora, the volume of data that can be manually annotated remains limited thus restricting the experiments that can be tried on automatic grammar acquisition." ></td>
	<td class="line x" title="18:142	Furthermore, designing an annotated corpus involves choices that may block future experiments from acquiring new kinds of linguistic knowledge because they necessitate annotation incompatible or difficult to produce from the existing ones." ></td>
	<td class="line x" title="19:142	With PASSAGE (de la Clergerie et al., 2008b), we believe that a new option becomes possible." ></td>
	<td class="line x" title="20:142	3http://www.ims.uni-stuttgart.de/ projekte/TIGER/related/links.shtml 36 Funded by the French ANR program on Data Warehouses and Knowledge, PASSAGE is a 3year project (20072009), coordinated by INRIA project-team Alpage." ></td>
	<td class="line x" title="21:142	It builds up on the results of the EASy French parsing evaluation campaign, funded by the French Technolangue program, which has shown that French parsing systems are now available, ranging from shallow to deep parsing." ></td>
	<td class="line x" title="22:142	Some of these systems were neither based on statistics, nor extracted from a treebank." ></td>
	<td class="line x" title="23:142	While needing to be improved in robustness, coverage, and accuracy, these systems has nevertheless proved the feasibility to parse medium amount of data (1M words)." ></td>
	<td class="line x" title="24:142	Preliminary experiments made by some of the participants with deep parsers (Sagot and Boullier, 2006) indicate that processing more than 10 M words is not a problem, especially by relying on clusters of machines." ></td>
	<td class="line x" title="25:142	These figures can even be increased for shallow parsers." ></td>
	<td class="line x" title="26:142	In other words, there now exists several French parsing systems that could parse (and re-parse if needed) large corpora between 10 to 100 M words." ></td>
	<td class="line x" title="27:142	Passage aims at pursuing and extending the line of research initiated by the EASy campaign by using jointly 10 of the parsing systems that have participated to EASy." ></td>
	<td class="line x" title="28:142	They will be used to parse and re-parse a French corpus of more than 100 M words along the following feedback loop between parsing and resource creation as follows (de la Clergerie et al., 2008a): 1." ></td>
	<td class="line x" title="29:142	Parsing creates syntactic annotations; 2." ></td>
	<td class="line x" title="30:142	Syntactic annotations create or enrich linguistic resources such as lexicons, grammars or annotated corpora; 3." ></td>
	<td class="line x" title="31:142	Linguistic resources created or enriched on the basis of the syntactic annotations are then integrated into the existing parsers; 4." ></td>
	<td class="line x" title="32:142	The enriched parsers are used to create richer (e.g., syntactico-semantic) annotations; 5." ></td>
	<td class="line x" title="33:142	etc. going back to step 1 In order to improve the set of parameters of the parse combination algorithm (inspired from the Recognizer Output Voting Error Reduction, i.e. ROVER, experiments), two parsing evaluation campaigns are planned during PASSAGE, the first of these already took place at the end of 2007 (de la Clergerie et al., 2008b)." ></td>
	<td class="line x" title="34:142	In the following, we present the annotation format specification and the syntactic annotation specifications of PASSAGE, then give an account of how the syntactic annotations were compared in the first evaluation campaign, by first describing the evaluation metrics and the web server infrastructure that was deployed to process them." ></td>
	<td class="line x" title="35:142	We conclude by showing how the results so far achieved in PASSAGE will contribute to the second part of the project, extracting and refining enriched linguistic annotations." ></td>
	<td class="line x" title="36:142	2 PASSAGE Annotation Format The aim is to allow an explicit representation of syntactic annotations for French, whether such annotations come from human annotators or parsers." ></td>
	<td class="line x" title="37:142	The representation format is intended to be used both in the evaluation of different parsers, so the parses representations should be easily comparable, and in the construction of a large scale annotation treebank which requires that all French constructions can be represented with enough details." ></td>
	<td class="line x" title="38:142	The format is based on three distinct specifications and requirements: 1." ></td>
	<td class="line x" title="39:142	MAF (ISO 24611)4 and SynAF (ISO 24615)5 which are the ISO TC37 specifications for morpho-syntactic and syntactic annotation (Ide and Romary, 2002) (Declerck, 2006) (Francopoulo, 2008)." ></td>
	<td class="line x" title="40:142	Let us note that these specifications cannot be called standards because they are work in progress and these documents do not yet have the status Published Standard." ></td>
	<td class="line x" title="41:142	Currently, their official status is only Committee Draft." ></td>
	<td class="line x" title="42:142	2." ></td>
	<td class="line x" title="43:142	The format used during the previous TECHNOLANGUE/EASY evaluation campaign in order to minimize porting effort for the existing tools and corpora." ></td>
	<td class="line x" title="44:142	3." ></td>
	<td class="line x" title="45:142	The degree of legibility of the XML tagging." ></td>
	<td class="line x" title="46:142	From a technical point of view, the format is a compromise between standoff and embedded notation." ></td>
	<td class="line x" title="47:142	The fine grain level of tokens and words is standoff (wrt the primary document) but higher levels use embedded annotations." ></td>
	<td class="line x" title="48:142	A standoff notation is usually considered more powerful but less 4http://lirics.loria.fr/doc pub/maf.pdf 5http://lirics.loria.fr/doc pub/ N421 SynAF CD ISO 24615.pdf 37 Figure 1: UML diagram of the structure of an annotated document readable and not needed when the annotations follow a (unambiguous) tree-like structure." ></td>
	<td class="line x" title="49:142	Let us add that, at all levels, great care has been taken to ensure that the format is mappable onto MAF and SynAF, which are basically standoff notations." ></td>
	<td class="line x" title="50:142	The structure of a PASSAGE annotated document may be summarized with the UML diagram in Figure1." ></td>
	<td class="line x" title="51:142	The document begins by the declaration of all the morpho-syntactic tagsets (MSTAG) that will be used within the document." ></td>
	<td class="line x" title="52:142	These declarations respect the ISO Standard Feature Structure Representation (ISO 24610-1)." ></td>
	<td class="line x" title="53:142	Then, tokens are declared." ></td>
	<td class="line x" title="54:142	They are the smallest unit addressable by other annotations." ></td>
	<td class="line x" title="55:142	A token is unsplittable and holds an identifier, a character range, and a content made of the original character string." ></td>
	<td class="line x" title="56:142	A word form is an element referencing one or several tokens." ></td>
	<td class="line x" title="57:142	It has has two mandatory attributes: an identifier and a list of tokens." ></td>
	<td class="line x" title="58:142	Some optional attributes are allowed like a part of speech, a lemma, an inflected form (possibly after spelling correction or case normalization) and morpho-syntactic tags." ></td>
	<td class="line x" title="59:142	The following XML fragment shows how the original fragment Les chaises can be represented with all the optional attributes offered by the PASSAGE annotation format : <T id='t0' start='0' end='3'> Les </T> <W id='w0' tokens='t0' pos='definiteArticle' lemma='le' form='les' mstag='nP'/> <T id='t1' start='4' end='11'> chaises </T> <W id='w1' tokens='t1' pos='commonNoun' lemma='chaise' form='chaises' mstag='nP gF'/> Note that all parts of speech are taken from the ISO registry 6 (Francopoulo et al., 2008)." ></td>
	<td class="line x" title="60:142	As in MAF, a word may refer to several tokens in order to represent multi-word units like pomme de terre." ></td>
	<td class="line x" title="61:142	Conversely, a unique token may be refered by two different words in order to represent results of split based spelling correction like when unetable is smartly separated into the words une and table." ></td>
	<td class="line x" title="62:142	The same configuration is required to represent correctly agglutination in fused prepositions like the token au that may be rewritten into the sequence of two words `a le." ></td>
	<td class="line x" title="63:142	On the contrary of MAF, cross-reference in token-word links for discontiguous spans is not allowed for the sake of simplicity." ></td>
	<td class="line x" title="64:142	Let us add that one of our requirement is to have PASSAGE annotations mappable onto the MAF model and not to map all MAF annotations onto PASSAGE model." ></td>
	<td class="line x" title="65:142	A G element denotes a syntactic group or a constituent (see details in section 3)." ></td>
	<td class="line x" title="66:142	It may be recursive or non-recursive and has an identifier, a type, and a content made of word forms or groups, if recursive." ></td>
	<td class="line x" title="67:142	All group type values are taken from the ISO registry." ></td>
	<td class="line x" title="68:142	Here is an example : <T id='t0' start='0' end='3'> Les </T> <T id='t1' start='4' end='11'> chaises </T> <G id='g0' type='GN'> <W id='w0' tokens='t0'/> <W id='w1' tokens='t1'/> </G> A group may also hold optional attributes like syntactic tagsets of MSTAG type." ></td>
	<td class="line x" title="69:142	The syntactic relations are represented with a standoff annotations which refer to groups and word forms." ></td>
	<td class="line x" title="70:142	A relation is defined by an identifier, a type, a source, and a target (see details in section 3." ></td>
	<td class="line x" title="71:142	All relation types, like subject or direct object are mappable onto the ISO registry." ></td>
	<td class="line x" title="72:142	An unrestricted number of comments may be added to any element by means of the mark element (i.e. M)." ></td>
	<td class="line x" title="73:142	Finally, a Sentence 6Data Category Registry, see http://syntax." ></td>
	<td class="line x" title="74:142	inist.fr 38 element gathers tokens, word forms, groups, relations and marks and all sentences are included inside a Document element." ></td>
	<td class="line x" title="75:142	3 PASSAGE Syntactic Annotation Specification 3.1 Introduction The annotation formalism used in PASSAGE7 is based on the EASY one(Vilnat et al., 2004) which whose first version was crafted in an experimental project PEAS (Gendner et al., 2003), with inspiration taken from the propositions of (Carroll et al., 2002)." ></td>
	<td class="line x" title="76:142	The definition has been completed with the input of all the actors involved in the EASY evaluation campaign (both parsers developers and corpus providers) and refined with the input of PASSAGE participants." ></td>
	<td class="line x" title="77:142	This formalism aims at making possible the comparison of all kinds of syntactic annotation (shallow or deep parsing, complete or partial analysis), without giving any advantage to any particular approach." ></td>
	<td class="line x" title="78:142	It has six kinds of syntactic chunks, we call constituents and 14 kinds of relations The annotation formalism allows the annotation of minimal, continuous and non recursive constituents, as well as the encoding of relations wich represent syntactic functions." ></td>
	<td class="line x" title="79:142	These relations (all of them being binary, except for the ternary coordination) have sources and targets which may be either forms or constituents (grouping several forms)." ></td>
	<td class="line x" title="80:142	Note that the PASSAGE annotation formalism does not postulate any explicit lexical head." ></td>
	<td class="line x" title="81:142	3.2 Constituent annotations For the PASSAGE campaigns, 6 kinds of constituents (syntactic chunks) have been considered and are illustrated in Table 3.2:  the Noun Phrase (GN for Groupe Nominal) may be made of a noun preceded by a determiner and/or by an adjective with its own modifiers, a proper noun or a pronoun;  the prepositional phrase (GP, for groupe prepositionnel ) may be made of a preposition and the GN it introduces, a contracted determiner and preposition, followed by the introduced GN, a preposition followed by an adverb or a relative pronoun replacing a GP; 7Annotation guide: http://www.limsi.fr/ Recherche/CORVAL/PASSAGE/eval 1/2007 10 05PEAS reference annotations v11.12.html  the verb kernel (NV for noyau verbal ) includes a verb, the clitic pronouns and possible particles attached to it." ></td>
	<td class="line x" title="82:142	Verb kernels may have different forms: conjugated tense, present or past participle, or infinitive." ></td>
	<td class="line x" title="83:142	When the conjugation produces compound forms, distinct NVs are identified;  the adjective phrase (GA for groupe adjectival) contains an adjective when it is not placed before the noun, or past or present participles when they are used as adjectives;  the adverb phrase (GR for groupe adverbial ) contains an adverb;  the verb phrase introduced by a preposition (PV) is a verb kernel with a verb not inflected (infinitive, present participle,), introduced by a preposition." ></td>
	<td class="line x" title="84:142	Some modifiers or adverbs may also be included in PVs." ></td>
	<td class="line x" title="85:142	GN la tr`es grande porte8 (the very big door); Rouletabille eux (they), qui (who) GP de la chambre (from the bedroom), du pavillon (from the lodge) de l`a (from there), dont (whose) NV jentendais (I heared) [on ne lentendait]9 plus (we could no more hear her) Jean [viendra] (Jean will come) [desobeissant] `a leurs parents (disobeying their parents), [fermee] `a clef (key closed) Il [ne veut] pas [venir] (He doesnt want to come), [ils netaient] pas [fermes] (they were not closed), GA les barreaux [intacts] (the intact bars) la solution [retenue] fut (the chosen solution has been), les enfants [desobeissants] (the disobeying children) GR aussi (also) vous nauriez [pas] (you would not) PV [pour aller] `a Paris (for going to Paris), de vraiment bouger (to really move) Table 1: Constituent examples 39 3.2.1 Syntactic Relation annotations The dependencies establishall thelinks between the minimal constituents described above." ></td>
	<td class="line x" title="86:142	All participants, corpus providers and campaign organizers agreed on a list of 14 kinds of dependencies listed below: 1." ></td>
	<td class="line x" title="87:142	subject-verb (SUJ V): may be inside the same NV as between elle and etait in elle etait (she was), or between a GN and a NV as between mademoiselle and appelait in Mademoiselle appelait (Miss was calling); 2." ></td>
	<td class="line x" title="88:142	auxiliary-verb (AUX V), between two NVs as between a and construit in: on a construit une maison (we have built a house); 3." ></td>
	<td class="line x" title="89:142	direct object-verb (COD V): the relation is annotated between a main verb (NV) and a noun phrase (GN), as between construit and la premi`ere automobile in: on a construit la premi`ere automobile (we have built the first car); 4." ></td>
	<td class="line x" title="90:142	complement-verb (CPL V): to link to the verb the complements expressed as GP or PV which may be adjuncts or indirect objects, as between en quelle annee and construit in en quelle annee a-t on construit la premi`ere automobile (In which year did we build the first car); 5." ></td>
	<td class="line x" title="91:142	modifier-verb (MOD V): concerns the constituants which certainly modify the verb, and are not mandatory, as adverbs or adjunct clauses, as between profondement or quand la nuit tombe and dort in Jean dort profondement quand la nuit tombe (Jean deeply sleeps when the night falls); 6." ></td>
	<td class="line x" title="92:142	complementor (COMP): to link the introducer and the verb kernel of a subordinate clause, as between qu and viendra in Je pense quil viendra (I think that he will come); it is also used to link a preposition and a noun phrase when they are not contiguous, preventing us to annotate them as GP; 7." ></td>
	<td class="line x" title="93:142	attribute-subject/object (ATB SO): between the attribute and the verb kernel, and precising that the attribute is relative to (a) the subject as between grand and est in il est grand ), or (b) the object as between etrange and trouve in il trouve cette explication etrange; 8." ></td>
	<td class="line x" title="94:142	modifier-noun (MOD N): to link to the noun all the constituents which modify it, as the adjective, the genitive, the relative clause This dependency is annotated between unique and fenetre in lunique fenetre (the unique window) or between de la chambre and la porte in la porte de la chambre (the bedroom door); 9." ></td>
	<td class="line x" title="95:142	modifier-adjective (MOD A): to relate to the adjective the constituents which modify it, as between tr`es et belle in la tr`es belle collection (the very impressive collection) or between de son fils and fi`ere in elle est fi`ere de son fils (she is proud of her son); 10." ></td>
	<td class="line x" title="96:142	modifier-adverb (MOD R): the same kind of dependency than MOD A for the adverbs, as between tr`es and gentiment in elle vient tr`es gentiment (she comes very kindly); 11." ></td>
	<td class="line x" title="97:142	modifier-preposition (MOD P): to relate to a preposition what modifies it, as between peu and avant in elle vient peu avant lui (she comes just before him); 12." ></td>
	<td class="line x" title="98:142	coordination (COORD): to relate the coordinate and the coordinated elements, as between Pierre, Paul and et in Pierre et Paul arrivent (Paul and Pierre are arriving); 13." ></td>
	<td class="line x" title="99:142	apposition (APP): to link the elements which are placed side by side, when they refer to the same object, as between le depute and Yves Tavernier in Le depute Yves Tavernier  (the Deputy Yves Tavernier); 14." ></td>
	<td class="line x" title="100:142	juxtaposition (JUXT): to link constituents which are neither coordinate nor in an apposition relation, as in enumeration." ></td>
	<td class="line x" title="101:142	It also links clauses as on ne lentendait et elle etait in on ne l entendait plus  elle etait peut-etre morte (we did not hear her any more perhaps she was dead)." ></td>
	<td class="line x" title="102:142	Some dependencies are illustrated in the two annotated sentences illutrated in figure . These annotations have been made using EasyRef, a specific Web annotation tool developed by INRIA." ></td>
	<td class="line x" title="103:142	4 PASSAGE First Evaluation Campaign 4.1 Evalution Service The first PASSAGE evaluation campaign was carried out in two steps." ></td>
	<td class="line x" title="104:142	During the initial one-month development phase, a development corpus was used to improve the quality of 40 Figure 2: Example of two sentences annotations parsers." ></td>
	<td class="line x" title="105:142	This development corpus from the TECHNOLANGUE/EASY is composed of 40,000 sentences, out of which 4,000 sentences have been manually annotated for the gold standard." ></td>
	<td class="line x" title="106:142	Based on these annotated sentences, an automatic WEBbased evaluation server provides fast performance feedback to the parsers developers." ></td>
	<td class="line x" title="107:142	At the end of this first phase, each participant indicated what he thought was his best parser run and got evaluated on a new set of 400 sentences selected from another part of the developement corpus which meanwhile had been manually annotated for the purpose and kept undisclosed." ></td>
	<td class="line x" title="108:142	The two phases represent a strong effort for the evaluators." ></td>
	<td class="line x" title="109:142	To avoid adding the cost of managing the distribution and installation of the evaluation packageateachdeveloperssite, thesolutionofthe WEB evaluation service was chosen." ></td>
	<td class="line x" title="110:142	A few infrastructures have been already experimented in NLP, like GATE (Cunningham et al., 2002) infrastructures, but to our knowledge none has been used to provide an WEB-based evaluation service as PASSAGE did." ></td>
	<td class="line x" title="111:142	The server was designed to manage two categories of users: parser developers and organizers." ></td>
	<td class="line x" title="112:142	To the developers, it provides, almost in real time, confidential and secure access to the automatic evaluation of their submitted parses." ></td>
	<td class="line x" title="113:142	To the organizers, it give access to statistics enabling them to follow the progress made by the developers, and easy management of the test phase." ></td>
	<td class="line x" title="114:142	The evaluation server provides, through a simple WEB browser, access to both coarse and fine grain statistics to a developers performance evaluation, globally for the whole corpus, at the level of a particular syntactic annotation or of a particular genre specific subcorpus, and also at the level of a single annotation for a particular word form." ></td>
	<td class="line x" title="115:142	Figure 3: Overall functional relations results 4.2 Performance Results Ten systems participated to the constituents annotation task." ></td>
	<td class="line x" title="116:142	For most of the systems, F-measure is upto90%andonlythreesystemsarebetween80% and 90%." ></td>
	<td class="line x" title="117:142	The trend is quite the same for Recall and Precision." ></td>
	<td class="line x" title="118:142	Around 96.5% of the constituents returned by the best system are correct and it found 95.5% of the constituents present in gold standard." ></td>
	<td class="line x" title="119:142	Figure3showstheresultsofthesevensystemsthat participated to the functional relations annotation task." ></td>
	<td class="line x" title="120:142	Performance is lower than for constituents and differences between systems are larger, an evidence that the task remains more difficult." ></td>
	<td class="line x" title="121:142	No systems gets a performance above 70% in F-measure, three are above 60% and two above 50%." ></td>
	<td class="line x" title="122:142	The last two systems are above 40%." ></td>
	<td class="line x" title="123:142	4.3 Systems Improvements The higher system gets increasing results from the beginning of the development phase to the test phase for both constituents and relations." ></td>
	<td class="line x" title="124:142	However, although the increase for relations is rather continuous, constituents results grow during the first few development evaluations, then reach a threshold from which results do not vary." ></td>
	<td class="line x" title="125:142	This can be explained by the fact that the constituent scores are rather high, while for relations, scores are lower and starting from low scores." ></td>
	<td class="line x" title="126:142	Using the evaluation server, system improves its performance by 50% for the constituents and 600% for the relations, although performance vary according to the type of relation or constituent." ></td>
	<td class="line x" title="127:142	Moreover, in repeating development evaluations, another consequence was the convergence of precision and recall." ></td>
	<td class="line x" title="128:142	41 5 Parsers outputs combination The idea to combine the output of systems participating to an evalauation campaign in order to obtain a combination with better performance than the best one was invented to our knowledge by J. Fiscus (Fiscus, 1997) in a DARPA/NIST speech recognition evaluation (ROVER/Reduced Output Voting Error Reduction)." ></td>
	<td class="line x" title="129:142	By aligning the output of the participating speech transcription systems and by selecting the hypothesis which was proposed by the majority of the systems, he obtained better performances than these of the best system." ></td>
	<td class="line x" title="130:142	The idea gained support in the speech processing community(Loof et al., 2007) and in general better results are obtained with keeping only the output of the two or three best performing systems, in which case the relative improvement can go up to 20% with respect to the best performance (Schwenk and Gauvain, 2000)." ></td>
	<td class="line x" title="131:142	For text processing, the ROVER procedure was applied to POS tagging (Paroubek, 2000) and machine translation (Matusov et al., 2006)." ></td>
	<td class="line x" title="132:142	In our case, we will use the text itself to realign the annotations provided by the various parser before computing their combination, as we did for our first experiments with the EASY evaluation campaign data (Paroubek et al., 2008)." ></td>
	<td class="line x" title="133:142	Since it is very likely taht the different parsers do not use the same word and sentence segmentation, we will realign all the data along a common word and sentence segmentation obtained by majority vote from the different outputs." ></td>
	<td class="line x" title="134:142	But our motivation for using such procedure is not only concerned with performance improvement but also with the obtention of a confidence measure for the annotation since if all systems agree on a particular annotation, then it is very likely to be true." ></td>
	<td class="line x" title="135:142	At this stage many options are open for the way we want to apply the ROVER algorithm, since we have both constituents and relations in our annotations." ></td>
	<td class="line x" title="136:142	We could vary the selection order (between constituents and relations), or use different comparison functions for the sources/targets of constituents/relations(Patrick Paroubek, 2006), or perform incremental/global merging of the annoations, or explore different weightings/thresholding strategies etc. In passage, ROVER experiments are only beginning and we have yet to determine which is the best strategy before applying it to word and sentence free segmentation data." ></td>
	<td class="line x" title="137:142	In the early experiment we did with the EASy classic PASSAGE track which uses a fixed word and sentence segmentation, we measured an improvement in precision for some specific subcorpora and annotations but improvement in recall was harder to get." ></td>
	<td class="line x" title="138:142	6 Conclusion The definition of a common interchange syntactic annotation format is an essential element of any methodology aiming at the creation of large annotated corpora from the cooperation of parsing systems to acquire new linguistic knowledge." ></td>
	<td class="line x" title="139:142	But the formalism aquires all of its value when backedup by the deployment of a WEB-based evaluation service as the PASSAGE examples shows." ></td>
	<td class="line x" title="140:142	167 experiments were carried out during the development phase (around 17 experiments per participant in one month)." ></td>
	<td class="line x" title="141:142	The results of the test phase were available less than one hour after the end of the development phase." ></td>
	<td class="line x" title="142:142	The service proved so successful that the participants asked after the evaluation, that the evaluation service be extended to support evaluation as a perennial service" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1085
Unbounded Dependency Recovery for Parser Evaluation
Rimell, Laura;Clark, Stephen;Steedman, Mark;"></td>
	<td class="line x" title="1:198	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 813821, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:198	c 2009 ACL and AFNLP Unbounded Dependency Recovery for Parser Evaluation Laura Rimell and Stephen Clark University of Cambridge Computer Laboratory laura.rimell@cl.cam.ac.uk stephen.clark@cl.cam.ac.uk Mark Steedman University of Edinburgh School of Informatics steedman@inf.ed.ac.uk Abstract This paper introduces a new parser evaluation corpus containing around 700 sentences annotated with unbounded dependencies, from seven different grammatical constructions." ></td>
	<td class="line x" title="3:198	We run a series of off-theshelfparsersonthecorpustoevaluatehow well state-of-the-art parsing technology is able to recover such dependencies." ></td>
	<td class="line x" title="4:198	The overall results range from 25% accuracy to 59%." ></td>
	<td class="line x" title="5:198	These low scores call into question the validity of using Parseval scores as a general measure of parsing capability." ></td>
	<td class="line x" title="6:198	We discuss the importance of parsers being able to recover unbounded dependencies, given their relatively low frequency in corpora." ></td>
	<td class="line x" title="7:198	We also analyse the various errors made on these constructions by one of the more successful parsers." ></td>
	<td class="line x" title="8:198	1 Introduction Statistical parsers are now obtaining Parseval scores of over 90% on the WSJ section of the Penn Treebank (Bod, 2003; Petrov and Klein, 2007; Huang, 2008; Carreras et al., 2008)." ></td>
	<td class="line x" title="9:198	McClosky et al.(2006) report an F-score of 92.1% using selftraining applied to the reranker of Charniak and Johnson (2005)." ></td>
	<td class="line x" title="11:198	Such scores, in isolation, may suggest that statistical parsing is close to becoming a solved problem, and that further incremental improvements will lead to parsers becoming as accurate as POS taggers." ></td>
	<td class="line x" title="12:198	A single score in isolation can be misleading, however, for a number of reasons." ></td>
	<td class="line x" title="13:198	First, the single score is an aggregate over a highly skewed distribution of all constituent types; evaluations which look at individual constituent or dependency types show that the accuracies on some, semantically important, constructions, such as coordination and PP-attachment, are much lower (Collins, 1999)." ></td>
	<td class="line x" title="14:198	Second, it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and domains (Gildea, 2001)." ></td>
	<td class="line x" title="15:198	Finally, some researchers have argued that the Parseval metrics (Black et al., 1991) are too forgiving with respect to certain errors and that an evaluation based on syntactic dependencies, for which scores are typically lower, is a better test of parser performance (Lin, 1995; Carroll et al., 1998)." ></td>
	<td class="line x" title="16:198	In this paper we focus on the first issue, that the performance of parsers on some constructions is much lower than the overall score." ></td>
	<td class="line x" title="17:198	The constructions that we focus on are various unbounded dependency constructions." ></td>
	<td class="line x" title="18:198	These are interesting for parser evaluation for the following reasons: one, they provide a strong test of the parsers knowledge of the grammar of the language, since many instances of unbounded dependencies are difficult to recover using shallow techniques in which the grammar is only superficially represented; and two, recovering these dependencies is necessary to completely represent the underlying predicateargument structure of a sentence, useful for applications such as Question Answering and Information Extraction." ></td>
	<td class="line x" title="19:198	To give an example of the sorts of constructions we are considering, and the (in)ability of parsers to recover the corresponding unbounded dependencies, none of the parsers that we have tested were able to recover the dependencies shown in bold from the following sentences: We have also developed techniques for recognizing and locating underground nuclear tests through the waves in the ground which they generate." ></td>
	<td class="line x" title="20:198	By Monday , they hope to have a sheaf of documents both sides can trust." ></td>
	<td class="line x" title="21:198	By means of charts showing wave-travel times and depths in the ocean at various locations , it is possible to estimate the rate of approach and probable time of arrival at Hawaii of a tsunami getting under way at any spot in the Pacific . 813 The contributions of this paper are as follows." ></td>
	<td class="line x" title="22:198	First, we present the first set of results for the recovery of a variety of unbounded dependencies, for a range of existing parsers." ></td>
	<td class="line x" title="23:198	Second, we describe the creation of a publicly available unbounded dependency test suite, and give statistics summarising properties of these dependencies in naturally occurring text." ></td>
	<td class="line x" title="24:198	Third, we demonstrate that performing the evaluation is surprisingly difficult, because of different conventions across the parsers as to how the underlying grammar is represented." ></td>
	<td class="line x" title="25:198	Fourth, we show that current parsing technology is very poor at representing some important elements of the argument structure of sentences, and argue for a more focused constructionbased parser evaluation as a complement to existing grammatical relation-based evaluations." ></td>
	<td class="line x" title="26:198	We also perform an error-analysis for one of the more successful parsers." ></td>
	<td class="line x" title="27:198	There has been some prior work on evaluating parsers on long-range dependencies, but no work we are aware of that has the scope and focus of this paper." ></td>
	<td class="line x" title="28:198	Clark et al.(2004) evaluated a CCG parseronasmallcorpusofobjectextractioncases." ></td>
	<td class="line x" title="30:198	Johnson (2002) began the body of work on inserting traces into the output of Penn Treebank (PTB) parsers, followed by Levy and Manning (2004), among others." ></td>
	<td class="line x" title="31:198	This PTB work focused heavily on the representation in the Treebank, evaluating against patterns in the trace annotation." ></td>
	<td class="line x" title="32:198	In this paper we have tried to be more formalismindependent and construction focused." ></td>
	<td class="line x" title="33:198	2 Unbounded Dependency Corpus 2.1 The constructions An unbounded dependency construction contains a word or phrase which appears to have been moved, while being interpreted in the position of the resulting gap." ></td>
	<td class="line x" title="34:198	An unlimited number of clause boundaries may intervene between the moved element and the gap (hence unbounded)." ></td>
	<td class="line x" title="35:198	Thesevenconstructionsinourcorpuswerechosen for being relatively frequent in text, compared to other unbounded dependency types, and relatively easy to identify." ></td>
	<td class="line x" title="36:198	An example of each construction, along with its associated dependencies, is shown in Table 1." ></td>
	<td class="line x" title="37:198	Here we give a brief description of each construction." ></td>
	<td class="line x" title="38:198	Object extraction from a relative clause is characterised by a relative pronoun (a wh-word or that)introducingaclausefromwhichanargument in object position has apparently been extracted: the paper which I wrote." ></td>
	<td class="line x" title="39:198	Our corpus includes cases where the extracted word is (semantically) the object of a preposition in the verb phrase: the agency that I applied to." ></td>
	<td class="line x" title="40:198	Object extraction from a reduced relative clause is essentially the same, except that there is no overt relative pronoun: the paper I wrote; the agency I applied to." ></td>
	<td class="line x" title="41:198	We did not include participial reduced relatives such as the paper written by the professor." ></td>
	<td class="line x" title="42:198	Subject extraction from a relative clause is characterised by the apparent extraction of an argument from subject position: the instrument that measures depth." ></td>
	<td class="line x" title="43:198	A relative pronoun is obligatory in this construction." ></td>
	<td class="line x" title="44:198	Our corpus includes passive subjects: the instrument which was used by the professor." ></td>
	<td class="line x" title="45:198	Free relatives contain relative pronouns without antecedents: I heard what she said, where what does not refer to any other noun in the sentence." ></td>
	<td class="line x" title="46:198	Freerelativescanusuallybeparaphrasedby nounphrasessuchas the thing she said (astandard diagnosticfordistinguishingthemfromembedded interrogatives like I wonder what she said)." ></td>
	<td class="line x" title="47:198	The majority of sentences in our corpus are object free relatives, but we also included some adverbial free relatives: She told us how to do it." ></td>
	<td class="line x" title="48:198	Objectwh-questionsarequestionsinwhichthe wh-word is the semantic object of the verb: What did you eat?." ></td>
	<td class="line x" title="49:198	Objects of prepositions are included: What city does she live in?." ></td>
	<td class="line x" title="50:198	Also included are a few cases where the wh-word is arguably adverbial, but is selected for by the verb: Where is the park located?." ></td>
	<td class="line x" title="51:198	Right node raising (RNR) is characterised by coordinated phrases from which a shared element apparently moves to the right: Mary saw and Susan bought the book." ></td>
	<td class="line x" title="52:198	This construction is unique within our corpus in that the raised element can have a wide variety of grammatical functions." ></td>
	<td class="line x" title="53:198	Examples include: noun phrase object of verb, noun phrase object of preposition (material about or messages from the communicator), a combination of the two (applied for and won approval), prepositional phrase modifier (president and chief executive of the company), infinitival modifier (the will and the capacity to prevent the event), and modified noun (a good or a bad decision)." ></td>
	<td class="line x" title="54:198	Subject extraction from an embedded clause is characterised by a semantic subject which is ap814 Object extraction from a relative clause Each must match Wismans pie with the fragment that he carries with him." ></td>
	<td class="line x" title="55:198	dobj(carries, fragment) Object extraction from a reduced relative clause Put another way, the decline in the yield suggests stocks have gotten pretty rich in price relative to the dividends they pay, some market analysts say." ></td>
	<td class="line x" title="56:198	dobj(pay, dividends) Subject extraction from a relative clause It consists of a series of pipes and a pressure-measuring chamber which record the rise and fall of the water surface." ></td>
	<td class="line x" title="57:198	nsubj(record, series) nsubj(record, chamber) Free relative He tried to ignore what his own common sense told him, but it wasnt possible; her motives were too blatant." ></td>
	<td class="line x" title="58:198	dobj(told, what) Object wh-question What city does the Tour de France end in?" ></td>
	<td class="line x" title="59:198	pobj(in, city) Right node raising For the third year in a row, consumers voted Bill Cosby first and James Garner second in persuasiveness as spokesmen in TV commercials, according to Video Storyboard Tests, New York." ></td>
	<td class="line x" title="60:198	prep(first, in) prep(second, in) Subject extraction from an embedded clause In assigning to God the responsibility which he learned could not rest with his doctors, Eisenhower gave evidence of that weakening of the moral intuition which was to characterize his administration in the years to follow." ></td>
	<td class="line x" title="61:198	nsubj(rest, responsibility) Table 1: Examples of the seven constructions in the unbounded dependency corpus." ></td>
	<td class="line x" title="62:198	parently extracted across two clause boundaries, as shown in the following bracketing (where  marks the origin of the extracted element): the responsibility which [the government said [ lay with the voters]]." ></td>
	<td class="line x" title="63:198	Our corpus includes sentences where the embedded clause is a so-called small clause, i.e. one with a null copula verb: the plan that she considered foolish, where plan is the semantic subject of foolish." ></td>
	<td class="line x" title="64:198	2.2 The data The corpus consists of approximately 100 sentences for each of the seven constructions; 80 of these were reserved for each construction for testing, giving a test set of 560 sentences in total, and the remainder were used for initial experimentation (for example to ensure that default settings for the various parsers were appropriate for this data)." ></td>
	<td class="line x" title="65:198	We did not annotate the full sentences, since we are only interested in the unbounded dependencies and full annotation of such a corpus would be extremely time-consuming." ></td>
	<td class="line x" title="66:198	With the exception of the question construction, all sentences were taken from the PTB, with roughly half from the WSJ sections (excluding 2-21 which provided the training data for many 815 of the parsers in our set) and half from Brown (roughly balanced across the different sections)." ></td>
	<td class="line x" title="67:198	The questions were taken from the question data in Rimell and Clark (2008), which was obtained from various years of the TREC QA track." ></td>
	<td class="line x" title="68:198	We chose to use the PTB as the main source because the use of traces in the PTB annotation provides a starting point for the identification of unbounded dependencies." ></td>
	<td class="line x" title="69:198	Sentences were selected for the corpus by a combination of automatic and manual processes." ></td>
	<td class="line x" title="70:198	A regular expression applied to PTB trees, searching for appropriate traces for a particular construction, was first used to extract a set of candidate sentences." ></td>
	<td class="line x" title="71:198	All candidates were manually reviewed and, if selected, annotated with one or more grammatical relations representing the relevant unbounded dependencies in the sentence." ></td>
	<td class="line x" title="72:198	Some of the annotation in the treebank makes identification of some constructions straightforward; for example right node raising is explicitly represented as RNR." ></td>
	<td class="line x" title="73:198	Indeed it may have been possible to fully automate this process with use of the tgrep search tool." ></td>
	<td class="line x" title="74:198	However, in order to obtain reliable statistics regarding frequency of occurrence, and to ensure a high-quality resource, we used fairly broad regular expressions to identify the original set followed by manual review." ></td>
	<td class="line x" title="75:198	We chose to represent the dependencies as grammatical relations (GRs) since this format seemed best suited to represent the kind of semantic relationship we are interested in." ></td>
	<td class="line x" title="76:198	GRs are headbased dependencies that have been suggested as a more appropriate representation for general parser evaluation than phrase-structure trees (Carroll et al., 1998)." ></td>
	<td class="line x" title="77:198	Table 1 gives examples of how GRs are used to represent the relevant dependencies." ></td>
	<td class="line x" title="78:198	The particular GR scheme we used was based on the Stanford scheme (de Marneffe et al., 2006); however, the specific GR scheme is not too crucial since the whole sentence is not being represented in the corpus, only the unbounded dependencies." ></td>
	<td class="line x" title="79:198	3 Experiments ThefiveparsersdescribedinSection3.2wereused to parse the test sentences in the corpus, and the percentage of dependencies in the test set recovered by each parser for each construction was calculated." ></td>
	<td class="line x" title="80:198	The details of how the parsers were run and how the parser output was matched against the gold standard are given in Section 3.3." ></td>
	<td class="line x" title="81:198	This Construction WSJ Brown Overall Obj rel clause 2.3 1.1 1.4 Obj reduced rel 2.7 2.8 2.8 Sbj rel clause 10.1 5.7 7.4 Free rel 2.6 0.9 1.3 RNR 2.2 0.9 1.2 Sbj embedded 2.0 0.3 0.4 Table 2: Frequency of constructions in the PTB (percentage of sentences)." ></td>
	<td class="line x" title="82:198	is essentially a recall evaluation, and so is open to abuse; for example, a program which returns all thepossiblewordpairsinasentence,togetherwith all possible labels, would score 100%." ></td>
	<td class="line x" title="83:198	However, this is easily guarded against: we simply assume that each parser is being run in a standard mode, and that each parser has already been evaluated on a full corpus of GRs in order to measure precision and recall across all dependency types." ></td>
	<td class="line x" title="84:198	(Calculating precision for the unbounded dependency evaluation would be difficult since that would require ustoknowhowmany incorrect unboundeddependencies were returned by each parser.)" ></td>
	<td class="line x" title="85:198	3.1 Statistics relating to the constructions Table 2 shows the percentage of sentences in the PTB, from those sections that were examined, which contain an example of each type of unbounded dependency." ></td>
	<td class="line x" title="86:198	Perhaps not surprisingly, root subject extractions from relative clauses are by far the most common, with the remaining constructions occurring in roughly between 1 and 2% of sentences." ></td>
	<td class="line x" title="87:198	Note that, although examples of eachindividualconstructionarerelativelyrare,the combined total is over 10% (assuming that each construction occurs independently)." ></td>
	<td class="line x" title="88:198	Section 6 contains a discussion regarding the frequency of occurrence of these events and the consequences of this for parser performance." ></td>
	<td class="line x" title="89:198	Table 3 shows the average and maximum distance between head and dependent for each construction, as measured by the difference between word indices." ></td>
	<td class="line x" title="90:198	This is a fairly crude measure of distance but gives some indication of how longrange the dependencies are for each construction." ></td>
	<td class="line x" title="91:198	The cases of object extraction from a relative clause and subject extraction from an embedded clause provide the longest dependencies, on average." ></td>
	<td class="line x" title="92:198	The following sentence gives an example of how far apart the head and dependent can be in a 816 Construction Avg Dist Max Dist Obj rel clause 6.8 21 Obj reduced rel 3.4 8 Sbj rel clause 4.4 18 Free rel 3.4 16 Obj wh-question 4.8 9 RNR 4.8 23 Sbj embedded 7.0 21 Table 3: Distance between head and dependent." ></td>
	<td class="line x" title="93:198	subject embedded construction: the same stump which had impaled the car of many a guest in the past thirty years and which he refused to have removed." ></td>
	<td class="line oc" title="94:198	3.2 The parsers The parsers that we chose to evaluate are the C&C CCG parser (Clark and Curran, 2007), the Enju HPSG parser (Miyao and Tsujii, 2005), the RASP parser (Briscoe et al., 2006), the Stanford parser (Klein and Manning, 2003), and the DCU postprocessor of PTB parsers (Cahill et al., 2004), based on LFG and applied to the output of the Charniak and Johnson reranking parser." ></td>
	<td class="line x" title="95:198	Of course we were unable to evaluate every publicly available parser, but we believe these are representative of current wide-coverage robust parsing technology.1 The C&C parser is based on CCGbank (Hockenmaier and Steedman, 2007), a CCG version of thePennTreebank." ></td>
	<td class="line x" title="96:198	Itisideallysuitedforthisevaluation because CCG was designed to capture the unbounded dependencies being considered." ></td>
	<td class="line x" title="97:198	The Enju parser was designed with a similar motivation to C&C, and is also based on an automatically extracted grammar derived from the PTB, but the grammar formalism is HPSG rather than CCG." ></td>
	<td class="line x" title="98:198	Both parsers produce head-word dependencies reflecting the underlying predicate-argument structure of a sentence, and so in theory should be straightforward to evaluate." ></td>
	<td class="line x" title="99:198	The RASP parser is based on a manually constructed POS tag-sequence grammar, with a statistical parse selection component and a robust 1One obvious omission is any form of dependency parser (McDonald et al., 2005; Nivre and Scholz, 2004)." ></td>
	<td class="line x" title="100:198	However, the dependencies returned by these parsers are local, and it would be non-trivial to infer from a series of links whether a long-range dependency had been correctly represented." ></td>
	<td class="line x" title="101:198	Also, dependency parsers are not significantly better at recovering head-based dependencies than constituent parsers based on the PTB (McDonald et al., 2005)." ></td>
	<td class="line x" title="102:198	partial-parsing technique which allows it to return output for sentences which do not obtain a full spanning analysis according to the grammar." ></td>
	<td class="line x" title="103:198	RASP hasnotbeendesignedtocapturemanyofthe dependencies in our corpus; for example, the tagsequence grammar has no explicit representation of verb subcategorisation, and so may not know that there is a missing object in the case of extraction from a relative clause (though it does recover some of these dependencies)." ></td>
	<td class="line x" title="104:198	However, RASP is a popular parser used in a number of applications, anditreturnsdependenciesinasuitableformatfor evaluation, and so we considered it to be an appropriate and useful member of our parser set." ></td>
	<td class="line x" title="105:198	The Stanford parser is representative of a large number of PTB parsers, exemplified by Collins (1997) and Charniak (2000)." ></td>
	<td class="line x" title="106:198	The Parseval scores reported for the Stanford parser are not the highest intheliterature,butarecompetitiveenoughforour purposes." ></td>
	<td class="line x" title="107:198	The advantage of the Stanford parser is thatitreturnsdependenciesinasuitableformatfor our evaluation." ></td>
	<td class="line x" title="108:198	The dependencies are obtained by a set of manually defined rules operating over the phrase-structure trees returned by the parser (de Marneffe et al., 2006)." ></td>
	<td class="line x" title="109:198	Like RASP, the Stanford parserhasnotbeendesignedtocaptureunbounded dependencies; in particular it does not make use of any of the trace information in the PTB." ></td>
	<td class="line x" title="110:198	However, we wanted to include a standard PTB parser in our set to see which of the unbounded dependency constructions it is able to deal with." ></td>
	<td class="line x" title="111:198	Finally, there is a body of work on inserting trace information into the output of PTB parsers (Johnson, 2002; Levy and Manning, 2004), which is the annotation used in the PTB for representing unbounded dependencies." ></td>
	<td class="line x" title="112:198	The work which deals with the PTB representation directly, such as Johnson (2002), is difficult for us to evaluate because it does not produce explicit dependencies." ></td>
	<td class="line x" title="113:198	However, the DCU post-processor is ideal because it does produce dependencies in a GR format." ></td>
	<td class="line oc" title="114:198	It has also obtained competitive scores on general GR evaluation corpora (Cahill et al., 2004)." ></td>
	<td class="line x" title="115:198	3.3 Parser evaluation The parsers were run essentially out-of-the-box when parsing the test sentences." ></td>
	<td class="line x" title="116:198	The one exception was C&C, which required some minor adjusting of parameters, as described in the parser documentation, to obtain close to full coverage on the data." ></td>
	<td class="line x" title="117:198	In addition, the C&C parser comes with a 817 Obj RC Obj Red Sbj RC Free Obj Q RNR Sbj Embed Total C&C 59.3 62.6 80.0 72.6 (81.2) 27.5 49.4 22.4 (59.7) 53.6 Enju 47.3 65.9 82.1 76.2 32.5 47.1 32.9 54.4 DCU 23.1 41.8 56.8 46.4 27.5 40.8 5.9 35.7 Rasp 16.5 1.1 53.7 17.9 27.5 34.5 15.3 25.3 Stanford 22.0 1.1 74.7 64.3 41.2 45.4 10.6 38.1 Table 4: Parser accuracy on the unbounded dependency corpus; the highest score for each construction is in bold; the figures in brackets for C&C derive from the use of a separate question model." ></td>
	<td class="line x" title="118:198	specially designed question model, and so we appliedboththisandthestandardmodeltotheobject wh-question cases." ></td>
	<td class="line x" title="119:198	The parser output was evaluated against each dependency in the corpus." ></td>
	<td class="line x" title="120:198	Due to the various GR schemesusedbytheparsers,anexactmatchonthe dependency label could not always be expected." ></td>
	<td class="line x" title="121:198	We considered a correctly recovered dependency tobeonewherethegold-standardheadanddependent were correctly identified, and the label was an acceptable match to the gold-standard label." ></td>
	<td class="line x" title="122:198	To be an acceptable match, the label had to indicate the grammatical function of the extracted element at least to the level of distinguishing active subjects, passive subjects, objects, and adjuncts." ></td>
	<td class="line x" title="123:198	For example, we allowed an obj (object) relation as a close enough match for dobj (direct object) in the corpus, even though obj does not distinguish different kinds of objects, but we did not allow generic relative pronoun relations that are underspecified for the grammatical role of the extracted element." ></td>
	<td class="line x" title="124:198	The differences in GR schemes were such that weendedupperformingatime-consuminglargely manual evaluation." ></td>
	<td class="line x" title="125:198	We list here some of the key differences that made the evaluation difficult." ></td>
	<td class="line x" title="126:198	In some cases, the parsers set of labels was less fine-grained than the gold standard." ></td>
	<td class="line x" title="127:198	For example, RASP represents the direct objects of both verbs and prepositions as dobj (direct object), whereas the gold-standard uses pobj for the preposition case." ></td>
	<td class="line x" title="128:198	We counted the RASP output as correctly matching the gold standard." ></td>
	<td class="line x" title="129:198	In other cases, the label on the dependency containing the gold-standard head and dependent was too underspecified to be acceptable by itself." ></td>
	<td class="line x" title="130:198	For example, where the gold-standard relation was dobj(placed,buckets), DCU produced relmod(buckets,placed) with a generic relative modifier label." ></td>
	<td class="line x" title="131:198	However, the correct label could be recovered from elsewhere in the parser output, specifically a combination of relpro(buckets,which) and obj(placed,which)." ></td>
	<td class="line x" title="132:198	In this case we counted the DCU output as correctly matching the gold standard." ></td>
	<td class="line x" title="133:198	In some constructions the Stanford scheme, upon which the gold-standard was based, makes different choices about heads than other schemes." ></td>
	<td class="line x" title="134:198	For example, in the the phrase Honolulu, which is the center of the warning system, the corpus containsasubjectdependencywithcenterasthehead: nsubj(center,Honolulu)." ></td>
	<td class="line x" title="135:198	Other schemes, however, treat the auxiliary verb is as the head of the dependency, rather than the predicate nominal center." ></td>
	<td class="line x" title="136:198	As long as the difference in head selection was due solely to the idiosyncracies of the GR schemes involved, we counted the relation as correct." ></td>
	<td class="line x" title="137:198	Finally, the different GR schemes treat coordination differently." ></td>
	<td class="line x" title="138:198	In the corpus, coordinated elements are always represented with two dependencies." ></td>
	<td class="line x" title="139:198	Thus the phrase they may half see and half imagine the old splendor has two gold-standard dependencies: dobj(see,splendor) and dobj(imagine,splendor)." ></td>
	<td class="line x" title="140:198	If a parser produced only the former dependency, but appeared to have the coordination correct, then we awarded two marks, even though the second dependency was not explicitly represented." ></td>
	<td class="line x" title="141:198	4 Results AccuraciesforthevariousparsersareshowninTable 4, with the highest score for each construction in bold." ></td>
	<td class="line x" title="142:198	Enju and C&C are the top performers, operating at roughly the same level of accuracy across most of the constructions." ></td>
	<td class="line x" title="143:198	Use of the C&C questionmodelmadeahugedifferenceforthe whobject construction (81.2% vs. 27.5%), showing that adaptation techniques specific to a particular 818 construction can be successful (Rimell and Clark, 2008)." ></td>
	<td class="line x" title="144:198	Inordertolearnmorefromtheseresults,inSection 5 we analyse the various errors made by the C&C parseroneachconstruction." ></td>
	<td class="line x" title="145:198	Theconclusions that we arrive at for the C&C parser we would also expecttoapplytoEnju,onthewhole,sincethedesign of the two parsers is so similar." ></td>
	<td class="line x" title="146:198	In fact, some of the recommendations for improvement on this corpus, such as the need for a better parsing model to make better attachment decisions, are parser independent." ></td>
	<td class="line x" title="147:198	The poor performance of RASP on this corpus is clearly related to a lack of subcategorisation information, since this is crucial for recovering extracted arguments." ></td>
	<td class="line x" title="148:198	For Stanford, incorporating the trace information from the PTB into the statistical model in some way is likely to help." ></td>
	<td class="line x" title="149:198	The C&C and Enju parsers do this through their respective grammar formalisms." ></td>
	<td class="line x" title="150:198	Our informal impression of the DCU post-processor is that it has much of the machinery available to recover the dependencies that the Enju and C&C parsers do, but for some reason which is unclear to us it performs much worse." ></td>
	<td class="line x" title="151:198	5 Analysis of the C&C Parser We categorised the errors made by the C&C parser onthedevelopmentdataforeachconstruction." ></td>
	<td class="line x" title="152:198	We chose the C&C parser for the analysis because it was one of the top performers and we have more knowledge of its workings than those of Enju." ></td>
	<td class="line x" title="153:198	The C&C parser first uses a supertagger to assign a small number of CCG lexical categories (essentiallysubcategorisationframes)toeachwordin the sentence." ></td>
	<td class="line x" title="154:198	These categories are then combined using a set of combinatory rules to build a CCG derivation." ></td>
	<td class="line x" title="155:198	The parser uses a log-linear probability model to select the highest-scoring derivation (Clark and Curran, 2007)." ></td>
	<td class="line x" title="156:198	In general, errors in dependency recovery may occur if the correct lexical category isnot assigned by thesupertagger for one or more of the words in a sentence, or if an incorrect derivation is chosen by the parsing model." ></td>
	<td class="line x" title="157:198	For unbounded dependency recovery, one source of errors (labeled type 1 in Table 5) is the wrong lexical category being assigned to the word (normally a verb or preposition) governing the extraction site." ></td>
	<td class="line x" title="158:198	In these testaments that I would submit here, if submit is assigned a category for an intransitive rather than transitive verb, the verbobject relation will not be recovered." ></td>
	<td class="line x" title="159:198	1a 1b 1c 1d 2 3 Errs Tot ObjRC 6 5 2 13 20 ObjRed 2 1 1 1 3 8 23 SbjRC 8 1 9 43 Free 1 1 2 22 ObjQ 2 2 4 25 RNR 2 1 7 3 13 28 SbjEmb 3 2 1 4 10 13 Subtotal 6 2 12 4 Total 24 21 14 59 174 Table 5: Error analysis for C&C. Errs is the total number of errors for a construction, Tot is the number of dependencies of that type in the development data." ></td>
	<td class="line x" title="160:198	There are a number of reasons why the wrong category may be assigned." ></td>
	<td class="line x" title="161:198	First, the lexicon may not contain enough information about possible categories for the word (1a), or the necessary category may not exist in the parsers grammar at all (1b)." ></td>
	<td class="line x" title="162:198	Evenifthegrammarcontainsthecorrectcategory and the lexicon makes it available, the parsing model may not choose it (1c)." ></td>
	<td class="line x" title="163:198	Finally, a POStagging error on the word may mislead the parser into assigning the wrong category (1d).2 A second source of errors (type 2) is attachment decisions that the parser makes independently of the unbounded dependency." ></td>
	<td class="line x" title="164:198	In Morgan . . ." ></td>
	<td class="line x" title="165:198	carried in several buckets of water from the spring which he poured into the copper boiler, the parser assigns the correct categories for the relative pronoun and verb, but chooses spring rather than buckets as the head of the relativized NP (i.e. the object of pour)." ></td>
	<td class="line x" title="166:198	Most attachment errors involve prepositional phrases (PPs) and coordination, which have long been known to be areas where parsers need improvement." ></td>
	<td class="line x" title="167:198	Finally, errors in unbounded dependency recovery may be due to complex errors in the surrounding parse context (type 3)." ></td>
	<td class="line x" title="168:198	We will not comment more on these cases since they do not tell us much about unbounded dependencies in particular." ></td>
	<td class="line x" title="169:198	Table 5 shows the distribution of error types across constructions for the C&C parser." ></td>
	<td class="line x" title="170:198	Subject relative clauses, for example, did not have any errors of type 1, because a verb with an extracted 2We considered an error to be type 1 only when the category error occurred on the word governing the extraction site, except in the subject embedded sentences, where we also included the embedding verb, since the category of this verb is key to dependency recovery." ></td>
	<td class="line x" title="171:198	819 subject does not require a special lexical category." ></td>
	<td class="line x" title="172:198	Most of the errors here are of type 2." ></td>
	<td class="line x" title="173:198	For example, in a series of pipes and a pressure-measuring chamber which record the rise and fall of the water surface, the parser attaches the relative clause to chamber but not to series." ></td>
	<td class="line x" title="174:198	Subject embedded sentences show a different pattern." ></td>
	<td class="line x" title="175:198	Many of the errors can be attributed to problems with the lexicon and grammar (1a and 1b)." ></td>
	<td class="line x" title="176:198	For example, in shadows that they imagined were Apaches, theword imagined neverappearsin the training data with the correct category, and so the required entry is missing from the lexicon." ></td>
	<td class="line x" title="177:198	Object extraction from a relative clause had a higher number of errors involving the parsing model (1c)." ></td>
	<td class="line x" title="178:198	In the first carefree, dreamless sleep that she had known, the transitive category is availablefor known, butnotselectedbythemodel." ></td>
	<td class="line x" title="179:198	The majority of the errors made by the parser are due to insufficient grammar coverage or weakness in the parsing model due to sparsity of head dependency data, the same fundamental problems that have dogged automatic parsing since its inception." ></td>
	<td class="line x" title="180:198	Hence one view of statistical parsing is that it has allowed us to solve the easy problems, but we are still no closer to a general solution for the recovery of the difficult dependencies." ></td>
	<td class="line x" title="181:198	One possibility is to create more training data targeting these constructions  effectively active learning by construction  in the way that Rimell and Clark (2008) were able to build a question parser." ></td>
	<td class="line x" title="182:198	We leave this idea for future work." ></td>
	<td class="line x" title="183:198	6 Discussion Unbounded dependencies are rare events, out in the Zipfian long tail." ></td>
	<td class="line x" title="184:198	They will always constitute a fraction of a percent of the overall total of head-dependencies in any corpus, a proportion too small to make a significant impact on global measures of parser accuracy, when expressive parsers are compared to those that merely approximate human grammar using finite-state or context-free covers." ></td>
	<td class="line x" title="185:198	This will remain the case even when such measures are based on dependencies, rather than on parse trees." ></td>
	<td class="line x" title="186:198	Nevertheless, unbounded dependencies remain highly significantin amuch moreimportant sense." ></td>
	<td class="line x" title="187:198	They support the constructions that are central to thoseapplicationsofparsingtechnologyforwhich precision is as important as recall, such as opendomain question-answering." ></td>
	<td class="line x" title="188:198	As low-power approximate parsing methods improve (as they must if they are ever to be usable at all for such tasks), we predict that the impact of the constructions we examine here will become evident." ></td>
	<td class="line x" title="189:198	No matter how infrequent object questions like What do frogs eat? are, if they are answered as if they were subject questions (Herons), users will rightly reject any excuse in terms of the overall statistical distribution of related bags of words." ></td>
	<td class="line x" title="190:198	Whether such improvements in parsers come from the availability of more human-labeled data, or from a breakthrough in unsupervised machine learning, we predict an imminent Uncanny Valley in parsing applications, due to the inability of parsers to recover certain semantically important dependencies, of the kind familiar from humanoid robotics and photorealistic animation." ></td>
	<td class="line x" title="191:198	In such applications, the closer the superficial resemblance to human behavior gets, the more disturbing subtle departures become, and the more they induce mistrust and revulsion in the user." ></td>
	<td class="line x" title="192:198	7 Conclusion In this paper we have demonstrated that current parsing technology is poor at recovering some of the unbounded dependencies which are crucial for fully representing the underlying predicateargument structure of a sentence." ></td>
	<td class="line x" title="193:198	We have also argued that correct recovery of such dependencies will become more important as parsing technology improves, despite the relatively low frequency of occurrence of the corresponding grammatical constructions." ></td>
	<td class="line x" title="194:198	We also see this more focused parser evaluation methodology  in this case construction-focused  as a way of improving parsing technology, as an alternative to the exclusive focus on incremental improvements in overall accuracy measures such as Parseval." ></td>
	<td class="line x" title="195:198	Acknowledgments Laura Rimell and Stephen Clark were supported by EPSRC grant EP/E035698/1." ></td>
	<td class="line x" title="196:198	Mark Steedman was supported by EU IST Cognitive Systems grant IP FP6-2004-IST-4-27657 (PACO-PLUS)." ></td>
	<td class="line x" title="197:198	We would like to thank Aoife Cahill for producing the DCU data." ></td>
	<td class="line x" title="198:198	820" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-2605
Construction of a German HPSG grammar from a detailed treebank
Cramer, Bart;Zhang, Yi;"></td>
	<td class="line x" title="1:224	Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 3745, Suntec, Singapore, 6 August 2009." ></td>
	<td class="line x" title="2:224	c2009 ACL and AFNLP Construction of a German HPSG grammar from a detailed treebank Bart Cramer and Yi Zhang Department of Computational Linguistics & Phonetics, Saarland University, Germany LT-Lab, German Research Center for Artificial Intelligence, Germany {bcramer,yzhang}@coli.uni-saarland.de Abstract Grammar extraction in deep formalisms has received remarkable attention in recent years." ></td>
	<td class="line x" title="3:224	We recognise its value, but try to create a more precision-oriented grammar, by hand-crafting a core grammar, and learning lexical types and lexical items from a treebank." ></td>
	<td class="line x" title="4:224	The study we performed focused on German, and we used the Tiger treebank as our resource." ></td>
	<td class="line x" title="5:224	A completely hand-written grammar in the framework of HPSG forms the inspiration for our core grammar, and is also our frame of reference for evaluation." ></td>
	<td class="line x" title="6:224	1 1 Introduction Previous studies have shown that treebanks can be helpful when constructing grammars." ></td>
	<td class="line x" title="7:224	The most well-known example is PCFG-based statistical parsing (Charniak and Johnson, 2005), where a PCFG is induced from, for instance, the Penn Treebank." ></td>
	<td class="line x" title="8:224	The underlying statistical techniques have been refined in the last decade, and previous work indicates that the labelled f-score of this method converges to around 91%." ></td>
	<td class="line x" title="9:224	An alternative to PCFGs, with more linguistic relevance, is formed by deeper formalisms, such as TAG (Joshi and Schabes, 1997), CCG (Steedman, 1996), LFG (Kaplan and Bresnan, 1995) and HPSG (Pollard and Sag, 1994)." ></td>
	<td class="line x" title="10:224	For LFG (Butt et al., 2002) and HPSG (Flickinger, 2000; Muller, 2002), large hand-written grammars have been developed." ></td>
	<td class="line x" title="11:224	In the case of HPSG, the grammar writers found the small number of principles too restrictive, and created more rules (approximately 50 to 300) to accommodate for phenomena 1The research reported in this paper has been carried out with financial support from the Deutsche Forschungsgemeinschaft and the German Excellence Cluster of Multimodal Computing & Interaction." ></td>
	<td class="line x" title="12:224	that vanilla HPSG cannot describe correctly." ></td>
	<td class="line x" title="13:224	The increased linguistic preciseness comes at a cost, though: such grammars have a lower out-of-thebox coverage, i.e. they will not give an analysis on a certain portion of the corpus." ></td>
	<td class="line x" title="14:224	Experiments have been conducted, where a lexicalised grammar is learnt from treebanks, a methodology for which we coin the name deep grammar extraction." ></td>
	<td class="line x" title="15:224	The basic architecture of such an experiment is to convert the treebank to a format that is compatible with the chosen linguistic formalism, and read off the lexicon from that converted treebank." ></td>
	<td class="line x" title="16:224	Because all these formalisms are heavily lexicalised, the core grammars only consist of a small number of principles or operators." ></td>
	<td class="line x" title="17:224	In the case of CCG (Hockenmaier and Steedman, 2002), the core grammar consists of the operators that CCG stipulates: function application, composition and type-raising." ></td>
	<td class="line x" title="18:224	Standard HPSG defines a few schemata, but these are usually adapted for a large-scale grammar." ></td>
	<td class="line x" title="19:224	Miyao et al.(2004) tailor their core grammar for optimal use with the Penn Treebank and the English language, for example by adding a new schema for relative clauses." ></td>
	<td class="line pc" title="21:224	Hockenmaier and Steedman (2002), Miyao et al.(2004) and Cahill et al.(2004) show fairly good results on the Penn Treebank (for CCG, HPSG and LFG, respectively): these parsers achieve accuracies on predicate-argument relations between 80% and 87%, which show the feasibility and scalability of this approach." ></td>
	<td class="line n" title="24:224	However, while this is a simple method for a highly configurational language like English, it is more difficult to extend to languages with more complex morphology or with word orders that display more freedom." ></td>
	<td class="line x" title="25:224	Hockenmaier (2006) is the only study known to the authors that applies this method to German, a language that displays these properties." ></td>
	<td class="line x" title="26:224	This article reports on experiments where the advantages of hand-written and derived grammars 37 are combined." ></td>
	<td class="line x" title="27:224	Compared to previous deep grammar extraction approaches, a more sophisticated core grammar (in the framework of HPSG) is created." ></td>
	<td class="line x" title="28:224	Also, more detailed syntactic features are learnt from the resource treebank, which leads to a more precise lexicon." ></td>
	<td class="line x" title="29:224	Parsing results are compared with GG (German Grammar), a previously hand-written German HPSG grammar (Muller, 2002; Crysmann, 2003; Crysmann, 2005)." ></td>
	<td class="line x" title="30:224	2 Core grammar 2.1 Head-driven phrase structure grammar This study has been entirely embedded in the HPSG framework (Pollard and Sag, 1994)." ></td>
	<td class="line x" title="31:224	This is a heavily lexicalised, constraint-based theory of syntax, and it uses typed feature structures as its representation." ></td>
	<td class="line x" title="32:224	HPSG introduces a small number of principles (most notably, the Head Feature Principle) that guide the construction of a few Immediate Dominance schemata." ></td>
	<td class="line x" title="33:224	These schemata are meant to be the sole basis to combine words and phrases." ></td>
	<td class="line x" title="34:224	Examples of schemata are headcomplement, head-subject, head-specifier, headfiller (for long-distance dependencies) and headmodifier." ></td>
	<td class="line x" title="35:224	In this study, the core grammar is an extension of the off-the-shelf version of HPSG." ></td>
	<td class="line x" title="36:224	The type hierarchy is organised by a typed feature structure hierarchy (Carpenter, 1992), and can be read by the LKB system (Copestake, 2002) and the PET parser (Callmeier, 2000)." ></td>
	<td class="line x" title="37:224	The output is given in Minimal Recursion Semantics (Copestake et al., 2005) format, which can be minimally described as a way to include scope information in dependency output." ></td>
	<td class="line x" title="38:224	2.2 The German language Not unlike English, German uses verb position to distinguish between different clause types." ></td>
	<td class="line x" title="39:224	In declarative sentences, verbs are positioned in the second position, while subordinate classes are verb-final." ></td>
	<td class="line x" title="40:224	Questions and imperatives are verbinitial." ></td>
	<td class="line x" title="41:224	However, German displays some more freedom with respect to the location of subjects, complements and adjuncts: they can be scrambled rather freely." ></td>
	<td class="line x" title="42:224	The following sentences are all grammatical, and have approximately the same meaning: (1) a. Der The.NOM Prasident President.NOM hat has gestern yesterday das the.ACC Buch book.ACC gelesen." ></td>
	<td class="line x" title="43:224	read.PERF." ></td>
	<td class="line x" title="44:224	The president read the book yesterday b. Gestern hat der Prasident das Buch gelesen." ></td>
	<td class="line x" title="45:224	c. Das Buch hat der Prasident gestern gelesen." ></td>
	<td class="line x" title="46:224	As can be seen, the main verb is placed at second position (the so-called left bracket), but all other verbs remain at the end of the sentence, in the right bracket." ></td>
	<td class="line x" title="47:224	Most linguistic theories about German recognise the existence of topological fields: the Vorfeld before the left bracket, the Mittelfeld between both brackets, and the Nachfeld after the right bracket." ></td>
	<td class="line x" title="48:224	The first two are mainly used for adjuncts and arguments, whereas the Nachfeld is typically, but not necessarily, used for extraposed material (e.g. relative clauses or comparative phrases) and some VPs." ></td>
	<td class="line x" title="49:224	Again, the following examples mean roughly the same: (2) a. Er He hat has das the.ACC Buch, Book.ACC, das that sie she empfohlen recommended hat, has, gelesen." ></td>
	<td class="line x" title="50:224	read.PERF." ></td>
	<td class="line x" title="51:224	He has read the book that she recommended." ></td>
	<td class="line x" title="52:224	b. Er hat das Buch gelesen, das sie empfohlen hat." ></td>
	<td class="line x" title="53:224	c. Das Buch hat er gelesen, das sie empfohlen hat." ></td>
	<td class="line x" title="54:224	Another distinctive feature of German is its relatively rich morphology." ></td>
	<td class="line x" title="55:224	Nominals are marked with case, gender and number, and verbs with number, person, tense and mood." ></td>
	<td class="line x" title="56:224	Adjectives and nouns have to agree with respect to gender, number and declension type, the latter being determined by the (non-)existence and type of determiner used in the noun phrase." ></td>
	<td class="line x" title="57:224	Verbs and subjects have to agree with respect to number and person." ></td>
	<td class="line x" title="58:224	German also displays highly productive noun compounding, which amplifies the need for effective unknown word handling." ></td>
	<td class="line x" title="59:224	Verb particles can either be separated from or concatenated to the verb: compare Er schlaft aus (He sleeps in) and Er 38 Amerikaner   no-det VAL bracketleftbigg SPEC  SUBCAT  bracketrightbigg     noun VAL bracketleftbigg SPEC det SUBCAT  bracketrightbigg   mussen      verb VAL 1 SLASH 2 XCOMP   verb VAL 1 SLASH 2        hart bracketleftbigg adverb MOD verb bracketrightbigg arbeiten   verb-inf VAL bracketleftbig SUBJ np-nombracketrightbig SLASH      slash-subj VAL bracketleftbig SUBJ bracketrightbig SLASH np-nom     mod-head VAL bracketleftbig SUBJ bracketrightbig SLASH np-nom     head-cluster VAL bracketleftbig SUBJ bracketrightbig SLASH np-nom     filler-head VAL bracketleftbig SUBJ bracketrightbig SLASH    Figure 1: This figure shows a (simplified) parse tree of the sentence Amerikaner mussen hart arbeiten (Americans have to work hard)." ></td>
	<td class="line x" title="60:224	wird ausschlafen (He will sleep in)." ></td>
	<td class="line x" title="61:224	In such verbs, the word zu (which translates to the English to in to sleep) can be infixed as well: er versucht auszuschlafen (He tries to sleep in)." ></td>
	<td class="line x" title="62:224	These characteristics make German a comparatively complex language to parse with CFGs: more variants of the same lemma have to be memorised, and the expansion of production rules will be more diverse, with a less peaked statistical distribution." ></td>
	<td class="line x" title="63:224	Efforts have been made to adapt existing CFG models to German (Dubey and Keller, 2003), but the results still dont compare to state-of-theart parsing of English." ></td>
	<td class="line x" title="64:224	2.3 Structure of the core grammar The grammar uses the main tenets from Headdriven Phrase Structure Grammar (Pollard and Sag, 1994)." ></td>
	<td class="line x" title="65:224	However, different from earlier deep grammar extraction studies, more sophisticated structures are added." ></td>
	<td class="line x" title="66:224	Muller (2002) proposes a new schema (head-cluster) to account for verb clusters in the right bracket, which includes the possibility to merge subcategorisation frames of e.g. object-control verbs and its dependent verb." ></td>
	<td class="line x" title="67:224	Separate rules for determinerless NPs, genitive modification, coordination of common phrases, relative phrases and direct speech are also created." ></td>
	<td class="line x" title="68:224	The free word order of German is accounted for by scrambling arguments with lexical rules, and by allowing adjuncts to be a modifier of unsaturated verb phrases." ></td>
	<td class="line x" title="69:224	All declarative phrases are considered to be head-initial, with an adjunct or argument fronted using the SLASH feature, which is then discharged using the head-filler schema." ></td>
	<td class="line x" title="70:224	The idea put forward by, among others, (Kiss and Wesche, 1991) that all sentences should be rightbranching is linguistically pleasing, but turns out be computationally very expensive (Crysmann, 2003), and the right-branching reading should be replaced by a left-branching reading when the right bracket is empty (i.e. when there is no auxiliary verb present)." ></td>
	<td class="line x" title="71:224	An example of a sentence is presented in figure 1." ></td>
	<td class="line x" title="72:224	It receives a right-branching analysis, because the infinitive arbeiten resides in the right bracket." ></td>
	<td class="line x" title="73:224	The unary rule slash-subj moves the required subject towards the SLASH value, so that it can be discharged in the Vorfeld by the head-filler schema." ></td>
	<td class="line x" title="74:224	mussen is an example of an argument attraction verb, because it pulls the valence feature (containing SUBJ, SUBCAT etc; not visible in the diagram) to itself." ></td>
	<td class="line x" title="75:224	The head-cluster rule assures that the VAL value then percolates upwards." ></td>
	<td class="line x" title="76:224	Because Amerikaner does not have a specifier, a separate unary rule (no-det) takes care of discharging the SPEC feature, before it can be combined with the filler-head rule." ></td>
	<td class="line x" title="77:224	As opposed to (Hockenmaier, 2006), this study 39 (a) teure Detektive kann sich der Supermarkt nicht leisten NP MO HD NP DET HD VP HDNGOA DA S SBOCHD (b) teure Detektive kann sich der Supermarkt nicht leisten NP MO HD NP DET HD S OA HD REFL SB MO VC Figure 2: (a) shows the original sentence, whereas (b) shows the sentence after preprocessing." ></td>
	<td class="line x" title="78:224	Note that NP is now headed, that the VP node is deleted, and that the verbal cluster is explicitly marked in (b)." ></td>
	<td class="line x" title="79:224	The glossary of this sentence is Expensive.ACC detectives.ACC can REFL the.NOM supermarket.NOM not afford employs a core lexicon for words that have marked semantic behaviour." ></td>
	<td class="line x" title="80:224	These are usually closed word classes, and include items such as raising and auxiliary verbs, possessives, reflexives, articles, complementisers etc. The size of this core lexicon is around 550 words." ></td>
	<td class="line x" title="81:224	Note that, because the core lexicon only contains function words, its coverage is negligible without additional entries." ></td>
	<td class="line x" title="82:224	3 Derivation of the lexicon 3.1 The Tiger treebank The Tiger treebank (Brants et al., 2002) is a treebank that embraces the concept of constituency, but can have crossing branches, i.e. the tree might be non-projective." ></td>
	<td class="line x" title="83:224	This allowed the annotators to capture the German free word order." ></td>
	<td class="line x" title="84:224	Around onethird of the sentences received a non-projective analysis." ></td>
	<td class="line x" title="85:224	An example can be found in figure 2." ></td>
	<td class="line x" title="86:224	Additionally, it annotates each branch with a syntactic function." ></td>
	<td class="line x" title="87:224	The text comes from a German newspaper (Frankfurter Rundschau)." ></td>
	<td class="line x" title="88:224	It was annotated semiautomatically, using a cascaded HMM model." ></td>
	<td class="line x" title="89:224	After each phase of the HMM model, the output was corrected by human annotators." ></td>
	<td class="line x" title="90:224	The corpus consists of over 50,000 sentences, with an average sentence length of 17.6 tokens (including punctuation)." ></td>
	<td class="line x" title="91:224	The treebank employs 26 phrase categories, 56 PoS tags and 48 edge labels." ></td>
	<td class="line x" title="92:224	It also encodes number, case and gender at the noun terminals, and tense, person, number and mood at verbs." ></td>
	<td class="line x" title="93:224	Whether a verb is finite, an infinitive or a participle is encoded in the PoS tag." ></td>
	<td class="line x" title="94:224	A peculiarity in the annotation of noun phrases is the lack of headedness, which was meant to keep the annotation as theory-independent as reasonably possible." ></td>
	<td class="line x" title="95:224	3.2 Preprocessing A number of changes had to be applied to the treebank to facilitate the read-off procedure:  A heuristic head-finding procedure is applied in the spirit of (Magerman, 1995)." ></td>
	<td class="line x" title="96:224	We use priority lists to find the NPs head, determiner, appositions and modifiers." ></td>
	<td class="line x" title="97:224	PPs and CPs are also split into a head and its dependent." ></td>
	<td class="line x" title="98:224	 If a verb has a separated verb particle, this particle is attached to the lemma of the verb." ></td>
	<td class="line x" title="99:224	For instance, if the verb schlafen has a particle aus, the lemma will be turned into ausschlafen (sleep in)." ></td>
	<td class="line x" title="100:224	If this is not done, subcategorisation frames will be attributed to the wrong lemma." ></td>
	<td class="line x" title="101:224	 Sentences with auxiliaries are non-projective, if the adjunct of the embedded VP is in the Vorfeld." ></td>
	<td class="line x" title="102:224	This can be solved by flattening the tree (removing the VP node), and marking the verbal cluster (VC) explicitly." ></td>
	<td class="line x" title="103:224	See figure 2 for an example." ></td>
	<td class="line x" title="104:224	67.6% of the original Tiger treebank is projective, and with this procedure, this is lifted to 80.1%." ></td>
	<td class="line x" title="105:224	 The Tiger treebank annotates reflexive pronouns with the PoS tag PRF, but does not distinguish the syntactic role." ></td>
	<td class="line x" title="106:224	Therefore, if a verb has an object that has PRF as its part-ofspeech, the label of that edge is changed into REFL, so that reflexive verbs can be found." ></td>
	<td class="line x" title="107:224	40 (a) 0 10 20 30 40 50 0 10000 20000 30000 40000 50000 (b) 0 10 20 30 40 50 0 200 400 600 800 1000 (c) 0 10 20 30 40 50 0 0.1 0.2 0.3 0.4 0.5 Figure 3: These graphs show learning curves of the algorithm on the first 45,000 sentences of the Tiger treebank." ></td>
	<td class="line x" title="108:224	Graph (a) indicates the amount of lemmas learnt (from top to bottom: nouns, names, adjectives, verbs and adverbs)." ></td>
	<td class="line x" title="109:224	The graph in (b) shows the number of distinct lexical types for verbs that are learnt." ></td>
	<td class="line x" title="110:224	Graph (c) shows the average proportion of morphological forms that is observed per verb lemma, assuming that each verb has 28 different forms: infinitive, zu (to) + infinitive, participle, imperative and 24 finite forms (3 (person) * 2 (number) * 2 (tense) * 2 (mood))." ></td>
	<td class="line x" title="111:224	The preprocessing stage failed in 1.1% of the instances." ></td>
	<td class="line x" title="112:224	3.3 Previous work The method described in Hockenmaier (2006) first converts the Tiger analysis to a tree, after which the lexical types were derived." ></td>
	<td class="line x" title="113:224	Because it was the authors goal to convert all sentences, some rather crude actions had to be taken to render non-projective trees projective: whenever a certain node introduces non-projectivity, some of its daughters are moved to the parent tree, until that node is projective." ></td>
	<td class="line x" title="114:224	Below, we give two examples where this will lead to incorrect semantic composition, with the consequence of flawed lexicon entries." ></td>
	<td class="line x" title="115:224	We argue that it is questionable whether the impressive conversion scores actually represent a high conversion quality." ></td>
	<td class="line x" title="116:224	It would be interesting to see how this grammar performs in a real parsing task, but no such study has been carried out so far." ></td>
	<td class="line x" title="117:224	The first case deals with extraposed material in the Nachfeld." ></td>
	<td class="line x" title="118:224	Typical examples include relative phrases, comparatives and PH/RE constructions2." ></td>
	<td class="line x" title="119:224	2NPs, AVPs and PPs can, instead of their usual headed structure, be divided in two parts: a placeholder and a repeated element." ></td>
	<td class="line x" title="120:224	These nodes often introduce nonprojectivity, and it is not straightforward to create a valid linguistic analysis for these phenomena." ></td>
	<td class="line x" title="121:224	Example sentences of these categories (NPs, AVPs and PPs, respectively) are: (1) [ PH Es ] ist wirklich schwer zu sagen, [ RE welche Positionen er einnimmt ] (2) Man mu sie also [ PH so ] behandeln , [ RE wie man eine Weltanschauungsbewegung behandelt ] (3) Alles deutet [ PH darauf ] hin [ RE da sie es nicht schaffen wird ] These examples all have the RE in the Nachfeld, but their placement actually has a large variety." ></td>
	<td class="line x" title="122:224	The consequence is that the head of the extraposed material will be connected to the verb, instead of to the genuine head." ></td>
	<td class="line x" title="123:224	Another example where Hockenmaiers algorithm will create incorrect lexical entries is when the edge label is PAR (for parentheses) or in the case of appositions." ></td>
	<td class="line x" title="124:224	Consider the following sentence: (3) mit with 160 160 Planstellen permanent posts (etliche (several sind are allerdings however noch still unbesetzt) unoccupied) The conclusion that will be drawn from this sentence is that sind can modify nouns, which is only true due to the parentheses, and has no relation with the specific characteristics of sind." ></td>
	<td class="line x" title="125:224	Similarly, appositions will act as modifiers of nouns." ></td>
	<td class="line x" title="126:224	Although one might argue that this is the canonical CCG derivation for these phenomena, it is not in the spirit of the HPSG grammars, and we believe that these constructions are better handled in rules than in the lexicon." ></td>
	<td class="line x" title="127:224	3.4 Procedure In our approach, we will be more conservative, and the algorithm will only add facts to its knowledge base if the evidence is convincing." ></td>
	<td class="line x" title="128:224	That means that less Tiger graphs will get projective analyses, but that doesnt have to be a curse: we can derive lexical types from non-projective analyses just as well, and leave the responsibility for solving the more complex grammatical phenomena to the core grammar." ></td>
	<td class="line x" title="129:224	For example, lexical rules will deal with fronting and Mittelfeld scrambling, as we have stated before." ></td>
	<td class="line x" title="130:224	This step of the 41 procedure has indeed strong affinity with deep lexical acquisition, except for the fact that in DLA all lexical types are known, and this is not the case in this study: the hand-written lexical type hierarchy is still extended with new types that are derived from the resource treebank, mostly for verbs." ></td>
	<td class="line x" title="131:224	The basic procedure is as follows:  Traverse the graph top-down." ></td>
	<td class="line x" title="132:224	 For each node:  Identify the nodes head (or the deepest verb in the verb cluster3);  For each complement of this node, add this complement to the heads subcategorisation frame." ></td>
	<td class="line x" title="133:224	 For each modifier, add this head to the possible MOD values of the modifiers head." ></td>
	<td class="line x" title="134:224	 For each lexical item, a mapping of (lemma, morphology)word form is created." ></td>
	<td class="line x" title="135:224	After this procedure, the following information is recorded for the verb lemma leisten from figure 2:  It has a subcategorisation frame npnom-reflnpacc." ></td>
	<td class="line x" title="136:224	 Its infinitive form is leisten." ></td>
	<td class="line x" title="137:224	The core grammar defines that possible subjects are nominative NPs, expletive es and CPs." ></td>
	<td class="line x" title="138:224	Expletives are considered to be entirely syntactic (and not semantic), so they will not receive a dependency relation." ></td>
	<td class="line x" title="139:224	Complements may include predicative APs, predicative NPs, genitive, dative and accusative NPs, prepositional complements, CPs, reflexives, separable particles (also purely syntactic), and any combination of these." ></td>
	<td class="line x" title="140:224	For nonverbs, the complements are ordered (i.e. it is a list, and not a verb)." ></td>
	<td class="line x" title="141:224	Verb complementation patterns are sets, which means that duplicate complements are not allowed." ></td>
	<td class="line x" title="142:224	For verbs, it is also recorded whether the auxiliary verb to mark the perfect tense should be either haben (default) or sein (mostly verbs that have to do with movement)." ></td>
	<td class="line x" title="143:224	Nouns are annotated with whether they can have appositions or not." ></td>
	<td class="line x" title="144:224	3That means that the head of a S/VP-node is assumed to be contained in the lexicon, as it must be some sort of auxiliary." ></td>
	<td class="line x" title="145:224	Results from the derivation procedure are graphed in figure 3." ></td>
	<td class="line x" title="146:224	The number of nouns and names is still growing after 45,000 sentences, which is an expected result, given the infinite nature of names and frequent noun compounding." ></td>
	<td class="line x" title="147:224	However, it appears that verbs, adjectives and adverbs are converging to a stable level." ></td>
	<td class="line x" title="148:224	On the other hand, lexical types are still learnt, and this shows a downside of our approach: the deeper the extraction procedure is, the more data is needed to reach the same level of learning." ></td>
	<td class="line x" title="149:224	The core grammar contains a little less than 100 lexical types, and on top of that, 636 lexical types are learnt, of which 579 are for verbs." ></td>
	<td class="line x" title="150:224	It is interesting to see that the number of lexical types is considerably lower than in (Hockenmaier, 2006), where around 2,500 lexical types are learnt." ></td>
	<td class="line x" title="151:224	This shows that our approach has a higher level of generalisation, and is presumably a consequence of the fact that the German CCG grammar needs distinct lexical types for verb-initial and verb-final constructions, and for different argument scramblings in the Mittelfeld, whereas in our approach, hand-written lexical rules are used to do the scrambling." ></td>
	<td class="line x" title="152:224	The last graph shows that the number of word forms is still insufficient." ></td>
	<td class="line x" title="153:224	We assume that each verb can have 28 different word forms." ></td>
	<td class="line x" title="154:224	As can be seen, it is clear that only a small part of this area is learnt." ></td>
	<td class="line x" title="155:224	One direction for future research might be to find ways to automatically expand the lexicon after the derivation procedure, or to hand-code morphological rules in the core grammar." ></td>
	<td class="line x" title="156:224	4 Parsing 4.1 Methodology All experiments in this article use the first 45,000 sentences as training data, and the consecutive 5,000 sentences as test data." ></td>
	<td class="line x" title="157:224	The remaining 472 sentences are not used." ></td>
	<td class="line x" title="158:224	We used the PET parser (Callmeier, 2000) to do all parsing experiments." ></td>
	<td class="line x" title="159:224	The parser was instructed to yield a parse error after 50,000 passive edges were used." ></td>
	<td class="line x" title="160:224	Ambiguity packing (Oepen and Carroll, 2000) and selective unpacking (Zhang et al., 2007) were used to reduce memory footprint and speed up the selection of the top-1000 analyses." ></td>
	<td class="line x" title="161:224	The maximum entropy model, used for selective unpacking, was based on 200 treebanked sentences of up to 20 words from the training set." ></td>
	<td class="line x" title="162:224	Part-of-speech tags delivered by the stock version of the TnT tagger (Brants, ) were 42 Tiger T.+TnT GG Out of vocabulary 71.9 % 5.2 % 55.6 % Parse error 0.2 % 1.5 % 0.2 % Unparsed 7.9 % 37.7 % 28.2 % Parsed 20.0 % 55.6 % 16.0 % Total 100.0 % 100.0 % 100.0 % Avg." ></td>
	<td class="line x" title="163:224	length 8.6 12.8 8.0 Avg." ></td>
	<td class="line x" title="164:224	nr." ></td>
	<td class="line x" title="165:224	of parses 399.0 573.1 19.2 Avg." ></td>
	<td class="line x" title="166:224	time (s) 9.3 15.8 11.6 Table 1: This table shows coverage results on the held-out test set." ></td>
	<td class="line x" title="167:224	The first column denotes how the extracted grammar performs without unknown word guessing." ></td>
	<td class="line x" title="168:224	The second column uses PoS tags and generic types to guide the grammar when an unknown word is encountered." ></td>
	<td class="line x" title="169:224	The third column is the performance of the fully hand-written HPSG German grammar by (Muller, 2002; Crysmann, 2003)." ></td>
	<td class="line x" title="170:224	OOV stands for out-of-vocabulary." ></td>
	<td class="line x" title="171:224	A parse error is recorded when the passive edge limit (set to 50,000) has been reached." ></td>
	<td class="line x" title="172:224	The bottom three rows only gives information about the sentences where the grammar actually returns at least one parse." ></td>
	<td class="line x" title="173:224	Training set Test set All 100.0 % 100.0 % Avg." ></td>
	<td class="line x" title="174:224	length 14.2 13.5 Coverage 79.0 % 69.0 % Avg." ></td>
	<td class="line x" title="175:224	length 13.2 12.8 Correct (top-1000) 52.0% 33.5 % Avg." ></td>
	<td class="line x" title="176:224	length 10.4 8.5 Table 2: Shown are the treebanking results, giving an impression of the quality of the parses." ></td>
	<td class="line x" title="177:224	The training set and test set are subsets of 200 sentences from the training and test set, respectively." ></td>
	<td class="line x" title="178:224	Coverage means that at least one analysis is found, and correct indicates that the perfect solution was found in the top-1000 parses." ></td>
	<td class="line x" title="179:224	used when unknown word handling was turned on." ></td>
	<td class="line x" title="180:224	These tags were connected to generic lexical types by a hand-written mapping." ></td>
	<td class="line x" title="181:224	The version of GG that was employed (Muller, 2002; Crysmann, 2003) was dated October 20084." ></td>
	<td class="line x" title="182:224	4.2 Results Table 1 shows coverage figures in three different settings." ></td>
	<td class="line x" title="183:224	It is clear that the resulting grammar has a higher coverage than the GG, but this comes at a cost: more ambiguity, and possibly unnecessary ambiguity." ></td>
	<td class="line x" title="184:224	Remarkably, the average processing time is lower, even when the sentence lengths and 4It should be noted that little work has gone in to providing unknown word handling mechanisms, and that is why we didnt include it in our results." ></td>
	<td class="line x" title="185:224	However, in a CoNLL-2009 shared task paper (Zhang et al., 2009), a coverage of 28.6% was reported when rudimentary methods were used." ></td>
	<td class="line x" title="186:224	ambiguity rates are higher." ></td>
	<td class="line x" title="187:224	We attribute this to the smaller feature structure geometry that is introduced by the core grammar (compared to the GG)." ></td>
	<td class="line x" title="188:224	Using unknown word handling immediately improved the coverage, by a large margin." ></td>
	<td class="line x" title="189:224	Larger ambiguity rates were recorded, and the number of parser errors slightly increased." ></td>
	<td class="line x" title="190:224	Because coverage does not imply quality, we wanted to look at the results in a qualitative fashion." ></td>
	<td class="line x" title="191:224	We took a sample of 200 sentences from both the training and the test set, where the ones from the training set did not overlap with the set used to train the MaxEnt model, so that both settings were equally influenced by the rudimentary MaxEnt model." ></td>
	<td class="line x" title="192:224	We evaluated for how many sentences the exactly correct parse tree could be found among the top-1000 parses (see table 2)." ></td>
	<td class="line x" title="193:224	The difference between the performance on the training and test set give an idea of how well the grammar performs on unknown data: if the difference is small, the grammar extends well to unseen data." ></td>
	<td class="line x" title="194:224	Compared to evaluating on lexical coverage, we believe this is a more empirical estimation of how close the acquisition process is to convergence." ></td>
	<td class="line x" title="195:224	Based on the kind of parse trees we observed, the impression was that on both sets, performance was reduced due to the limited predictive power of the disambiguation model." ></td>
	<td class="line x" title="196:224	There were quite a few sentences for which good parses could be expected, because all lexical entries were present." ></td>
	<td class="line x" title="197:224	This experiment also showed that there were systematic ambiguities that were introduced by inconsistent annotation in the Tiger treebank." ></td>
	<td class="line x" title="198:224	For in43 stance, the word ein was learnt as both a number (the English one) and as an article (a), leading to spurious ambiguities for each noun phrase containing the word ein, or one of its morphological variants." ></td>
	<td class="line x" title="199:224	These two factors reinforced each other: if there is spurious ambiguity, it is even harder for a sparsely trained disambiguation model to pull the correct parse inside the top-1000." ></td>
	<td class="line x" title="200:224	The difference between the two correct numbers in table 2 is rather large, meaning that the real coverage might seem disappointingly low." ></td>
	<td class="line x" title="201:224	Not unexpectedly, we found that the generic lexical types for verbs (transitive verb, third person singular) and nouns (any gender, no appositions allowed) was not always correct, harming the results considerably." ></td>
	<td class="line x" title="202:224	A quantitative comparison between deep grammars is always hard." ></td>
	<td class="line x" title="203:224	Between DELPH-IN grammars, coverage has been the main method of evaluation." ></td>
	<td class="line x" title="204:224	However, this score does not reward richness of the semantic output." ></td>
	<td class="line x" title="205:224	Recent evidence from the ERG (Ytrestl et al., 2009) suggests that the ERG reaches a top-500 coverage of around 70% on an unseen domain, a result that this experiment did not approximate." ></td>
	<td class="line x" title="206:224	The goal of GG is not computational, but it serves as a testing ground for linguistic hypotheses." ></td>
	<td class="line x" title="207:224	Therefore, the developers have never aimed at high coverage figures, and crafted the GG to give more detailed analyses and to be suited for both parsing and generation." ></td>
	<td class="line x" title="208:224	We are happy to observe that the coverage figures in this study are higher than GGs (Zhang et al., 2009), but we realise the limited value of this evaluation method." ></td>
	<td class="line x" title="209:224	Future studies will certainly include a more granular evaluation of the grammars performance." ></td>
	<td class="line x" title="210:224	5 Conclusion and discussion We showed how a precise, wide-coverage HPSG grammar for German can be created successfully, by constructing a core grammar by hand, and appending it with linguistic information from the Tiger treebank." ></td>
	<td class="line x" title="211:224	Although this extracted grammar suffers considerably more from overgeneration than the hand-written GG, we argue that our conservative derivation procedure delivers a more detailed, compact and correct compared to previous deep grammar extraction efforts." ></td>
	<td class="line x" title="212:224	The use of the core lexicon allows us to have more linguistically motivated analyses of German than approaches where the core lexicon only comprises the textbook principles/operators." ></td>
	<td class="line x" title="213:224	We compared our lexicon extraction results to those from (Hockenmaier, 2006)." ></td>
	<td class="line x" title="214:224	Also, preliminary parsing experiments are reported, in which we show that this grammar produces reasonable coverage on unseen text." ></td>
	<td class="line x" title="215:224	Although we feel confident about the successful acquisition of the grammar, there still remain some limiting factors in the performance of the grammar when actually parsing." ></td>
	<td class="line x" title="216:224	Compared to coverage figures of around 80%, reported by (Riezler et al., 2001), the proportion of parse forests containing the correct parse in this study is rather low." ></td>
	<td class="line x" title="217:224	The first limit is the constructional coverage, meaning that the core grammar is not able to construct the correct analysis, even though all lexical entries have been derived correctly before." ></td>
	<td class="line x" title="218:224	The most frequent phenomena that are not captured yet are PH/RE constructions and extraposed clauses, and we plan to do an efficient implementation (Crysmann, 2005) of these in a next version of the grammar. Second, as shown in figure 3, data scarcity in the learning of the surface forms of lemmas negatively influences the parsers performance on unseen text." ></td>
	<td class="line x" title="219:224	In this paper, we focused mostly on the correctness of the derivation procedure." ></td>
	<td class="line x" title="220:224	We would like to address the real performance of the grammar/parser combination in future work, which can only be done when parses are evaluated according to a more granular method than we have done in this study." ></td>
	<td class="line x" title="221:224	Furthermore, we ran into the issue that there is no straightforward way to train larger statistical models automatically, which is due to the fact that our approach does not convert the source treebank to the target formalisms format (in our case HPSG), but instead reads off lexical types and lexical entries directly." ></td>
	<td class="line x" title="222:224	We plan to investigate possibilities to have the annotation be guided automatically by the Tiger treebank, so that the disambiguation model can be trained on a much larger amount of training data." ></td>
	<td class="line x" title="223:224	Acknowledgements We would like to thank Rebecca Dridan, Antske Fokkens, Stephan Oepen and the anonymous reviewers for their valuable contributions to this paper." ></td>
	<td class="line x" title="224:224	44" ></td>
</tr></table>
</div
</body></html>
