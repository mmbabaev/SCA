<html><body><head><link rel="stylesheet" type="text/css" href="style.css" /><script src="map.js"></script><script src="jquery-1.7.1.min.js"></script></head>
<div class="dstPaperData">
P04-1035 <div class="dstPaperTitle">A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based On Minimum Cuts</div><div class="dstPaperAuthors">Pang, Bo;Lee, Lillian;</div>
</div>
<table cellspacing="0" cellpadding="0"><tr>
	<td class="srcData" >Source Paper</td>
	<td class="pp legend" ><input type="checkbox" id="cbIPositive" checked="true"/><label for="cbIPositive">Informal +<label></td>
	<td class="nn legend" ><input type="checkbox" id="cbINegative" checked="true"/><label for="cbINegative">Informal -<label></td>
	<td class="oo legend" ><input type="checkbox" id="cbIObjective" checked="true"/><label for="cbIObjective">Informal Neutral<label></td>
	<td class="ppc legend" ><input type="checkbox" id="cbEPositive" checked="true"/><label for="cbEPositive">Formal +</label></td>
	<td class="nnc legend" ><input type="checkbox" id="cbENegative" checked="true"/><label for="cbENegative">Formal -</label></td>
	<td class="ooc legend" ><input type="checkbox" id="cbEObjective" checked="true"/><label for="cbEObjective">Formal Neutral</label></td>
	<td class="lb"><input type="checkbox" id="cbSentenceBoundary"/><label for="cbSentenceBoundary">Sentence Boundary</label></td>
</tr></table>
<div class="dstPaper">
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="H05-1042
Collective Content Selection For Concept-To-Text Generation
Barzilay, Regina;Lapata, Mirella;"></td>
	<td class="line x" title="1:228	Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 331338, Vancouver, October 2005." ></td>
	<td class="line x" title="2:228	c2005 Association for Computational Linguistics Collective Content Selection for Concept-To-Text Generation Regina Barzilay Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology regina@csail.mit.edu Mirella Lapata School of Informatics University of Edinburgh mlap@inf.ed.ac.uk Abstract A content selection component determines which information should be conveyed in the output of a natural language generation system." ></td>
	<td class="line x" title="3:228	We present an efficient method for automatically learning content selection rules from a corpus and its related database." ></td>
	<td class="line x" title="4:228	Our modeling framework treats content selection as a collective classification problem, thus allowing us to capture contextual dependencies between input items." ></td>
	<td class="line x" title="5:228	Experiments in a sports domain demonstrate that this approach achieves a substantial improvement over context-agnostic methods." ></td>
	<td class="line x" title="6:228	1 Introduction Content selection is a fundamental task in conceptto-text generation (Reiter and Dale, 2000)." ></td>
	<td class="line x" title="7:228	A practical generation system typically operates over a large database with multiple entries that could potentially be included in a text." ></td>
	<td class="line x" title="8:228	A content selection component determines what subset of this information to include in the generated document." ></td>
	<td class="line x" title="9:228	For example, consider the task of automatically generating game summaries, given a database containing statistics on Americal football." ></td>
	<td class="line x" title="10:228	Table 1 shows an excerpt from such a database, and its corresponding game summary written by a journalist." ></td>
	<td class="line x" title="11:228	A single football game is typically documented in hundreds of database entries  all actions, player positions, and scores are recorded, along with a wide range of comparative and aggregate statistics." ></td>
	<td class="line x" title="12:228	Only a small fraction of this information is featured in a game summary." ></td>
	<td class="line x" title="13:228	The content selection component aims to identify this subset.1 In existing generation systems the content selection component is manually crafted." ></td>
	<td class="line x" title="14:228	Specifying content selection rules is, however, notoriously difficult, prohibitively so in large domains." ></td>
	<td class="line x" title="15:228	It involves the analysis of a large number of texts from a domain-relevant corpus, familiarity with the associated database, and consultation with domain experts." ></td>
	<td class="line x" title="16:228	Moreover, the task must be repeated for each domain anew." ></td>
	<td class="line x" title="17:228	This paper proposes a data-driven method for learning the content-selection component for a concept-to-text generation system." ></td>
	<td class="line x" title="18:228	We assume that the learning algorithm is provided with a parallel corpus of documents and a corresponding database, in which database entries that should appear in documents are marked." ></td>
	<td class="line x" title="19:228	One possible approach is to formulate content selection as a standard binary classification task: predict whether an item is to be included on the basis of its attributes alone." ></td>
	<td class="line x" title="20:228	In fact, this method is commonly used for content selection in text summarization (e.g. , Kupiec et al. , 1995)." ></td>
	<td class="line x" title="21:228	However, by treating each instance in isolation, we cannot guarantee that the selected database entries are related in a meaningful way, which is essential for the generation of a coherent text." ></td>
	<td class="line x" title="22:228	Rather than selecting each item separately, we propose a method for collective content selection, where all candidates are considered simultaneously for selection." ></td>
	<td class="line x" title="23:228	Collective selection thereby allows us to explicitly optimize coherence in the generated 1The organization of the selected information and its surface realization is typically handled by other components of the generation system, which are outside the scope of this paper." ></td>
	<td class="line x" title="24:228	331 Passing PLAYER CP/AT YDS AVG TD INT Brunell 17/38 192 6.0 0 0 Garcia 14/21 195 9.3 1 0      Rushing PLAYER REC YDS AVG LG TD Suggs 22 82 3.7 25 1       Fumbles PLAYER FUM LOST REC YDS Coles 1 1 0 0 Portis 1 1 0 0 Davis 0 0 1 0 Little 0 0 1 0      Suggs rushed for 82 yards and scored a touchdown in the fourth quarter, leading the Browns to a 17-13 win over the Washington Redskins on Sunday." ></td>
	<td class="line x" title="25:228	Jeff Garcia went 14-of-21 for 195 yards and a TD for the Browns, who didnt secure the win until Coles fumbled with 2:08 left." ></td>
	<td class="line x" title="26:228	The Redskins (1-3) can pin their third straight loss on going just 1-for-11 on third downs, mental mistakes and a costly fumble by Clinton Portis." ></td>
	<td class="line x" title="27:228	Brunell finished 17-of-38 for 192 yards, but was unable to get into any rhythm because Clevelands defense shut down Portis." ></td>
	<td class="line x" title="28:228	The Browns faked a field goal, but holder Derrick Frost was stopped short of a first down." ></td>
	<td class="line x" title="29:228	Brunell then completed a 13-yard pass to Coles, who fumbled as he was being taken down and Browns safety Earl Little recovered." ></td>
	<td class="line x" title="30:228	Table 1: Sample target game description and example of database entries; boldface indicates correspondences between the text and the database (CP/AT: completed out of attempted, YDS: yards, AVG: average, TD: touchdown, INT: interception, REC: received, LG: longest gain, FUM: fumble)." ></td>
	<td class="line x" title="31:228	text: semantically related entries are often selected together." ></td>
	<td class="line x" title="32:228	In essence, the algorithm seeks a subset of candidates that is consistent with the individual preferences of each candidate, and at the same time maximally satisfies contextual constraints." ></td>
	<td class="line x" title="33:228	A graphbased formulation of this optimization problem allows us to find an exact, globally optimal solution, using a min-cut algorithm." ></td>
	<td class="line x" title="34:228	Collective content selection is particularly beneficial to generation systems that operate over relational databases." ></td>
	<td class="line x" title="35:228	Rich structural information available in a database can be readily utilized to determine semantic relatedness between different database entries." ></td>
	<td class="line x" title="36:228	For instance, we can easily find all actions (e.g. , touchdowns and fumbles) associated with a specific player in a game, which could be relevant for generating a summary centered around an individual." ></td>
	<td class="line x" title="37:228	We show how to utilize database relations for discovering meaningful contextual links between database entries." ></td>
	<td class="line x" title="38:228	We evaluate our collective content selection model in a sports domain." ></td>
	<td class="line x" title="39:228	The proposed content selection component operates over a large database containing descriptive statistics about American football games." ></td>
	<td class="line x" title="40:228	Our model yields a 10% increase in F-score, when compared to a standard classification approach, thus demonstrating the benefits of collective content selection on this complex domain." ></td>
	<td class="line x" title="41:228	Furthermore, our results empirically confirm the contribution of discourse constraints for content selection." ></td>
	<td class="line x" title="42:228	In the following section, we provide an overview of existing work on content selection." ></td>
	<td class="line x" title="43:228	Then, we define the learning task and introduce our approach for collective content selection." ></td>
	<td class="line x" title="44:228	Next, we present our experimental framework and data." ></td>
	<td class="line x" title="45:228	We conclude the paper by presenting and discussing our results." ></td>
	<td class="line x" title="46:228	2 Related Work The generation literature provides multiple examples of content selection components developed for various domains (Kukich, 1983; McKeown, 1985; Sripada et al. , 2001; Reiter and Dale, 2000)." ></td>
	<td class="line x" title="47:228	A common theme across different approaches is the emphasis on coherence: related information is selected to produce a text that hangs together (McKeown, 1985)." ></td>
	<td class="line x" title="48:228	Similarly, our method is also guided by coherence constraints." ></td>
	<td class="line x" title="49:228	In our case these constraints are derived automatically, while in symbolic generation systems coherence is enforced by analyzing a large number of texts from a domain-relevant corpus and 332 careful hand-crafting of content selection rules." ></td>
	<td class="line x" title="50:228	Duboue and McKeown (2003) were the first to propose a method for learning content selection rules automatically, thus going beyond mere corpus analysis." ></td>
	<td class="line x" title="51:228	They treat content selection as a classification task." ></td>
	<td class="line x" title="52:228	Given a collection of texts associated with a domain-specific database, their model learns whether a database entry should be selected for presentation or not." ></td>
	<td class="line x" title="53:228	Their modeling approach uses an expressive feature space while considering database entries in isolation." ></td>
	<td class="line x" title="54:228	Similarly to Duboue and McKeown (2003), we view content selection as a classification task and learn selection rules from a database and its corresponding corpus." ></td>
	<td class="line x" title="55:228	In contrast to them, we consider all database entries simultaneously, seeking a globally optimal selection." ></td>
	<td class="line x" title="56:228	Thus, we avoid the need for extensive feature engineering by incorporating discourse constraints into the learning framework." ></td>
	<td class="line x" title="57:228	In addition, we assess whether data-driven methods for content selection scale up to large databases with thousands of interrelated entries, by evaluating our model in a sports domain." ></td>
	<td class="line x" title="58:228	Previous work (Duboue and McKeown, 2003) has tackled the content selection problem for biographical summaries, a simpler domain with fewer entities and interactions among them." ></td>
	<td class="line x" title="59:228	3 The Task We assume that the content selection component takes as input a set of database entries.2 Each entry has a type and a set of attributes associated with its type." ></td>
	<td class="line x" title="60:228	For instance, the database shown in Table 1 contains entries of three types  Passing, Rushing and Fumbles." ></td>
	<td class="line x" title="61:228	Two entries are of type Passing, and each of them has six attributes  PLAYER, CP/AT, YDS, AVG, TD, INT." ></td>
	<td class="line x" title="62:228	In addition, each entry has a label that specifies whether it should be included in a generated text or not." ></td>
	<td class="line x" title="63:228	During the training process, the learning algorithm is provided with n sets of database entries, each associated with a label whose value is known." ></td>
	<td class="line x" title="64:228	In practice, we only require a parallel corpus of game summaries and database entries  label values are derived automatically via alignment (see Section 4 for more details)." ></td>
	<td class="line x" title="65:228	2A terminological note: a database entry is analogous to a row in a relational table; throughout this paper we use the terms entity and database entry interchangeably." ></td>
	<td class="line x" title="66:228	The goal of the content selection component is to select entries from a database, i.e., to determine whether their label values are 0 or 1." ></td>
	<td class="line x" title="67:228	Under this formulation, content selection is restricted to information available in the database; there is no attempt to induce new facts through inference." ></td>
	<td class="line x" title="68:228	In the next section, we describe our learning framework, and explain how it is applied to the content selection task." ></td>
	<td class="line x" title="69:228	3.1 The Collective Classification Approach Generation of a coherent text crucially depends on our ability to select entities that are related in a meaningful way (McKeown, 1985)." ></td>
	<td class="line x" title="70:228	A content selection component that considers every entity in isolation does not have any means to enforce this important discourse constraint." ></td>
	<td class="line x" title="71:228	We therefore formulate content selection as a collective classification task, where all entities that belong to the same database (i.e. , the same football game) are considered simultaneously." ></td>
	<td class="line x" title="72:228	This framework thus enables us to enforce contextual constraints by selecting related entities." ></td>
	<td class="line x" title="73:228	When considered in isolation, some database entries are more likely to be selected than others." ></td>
	<td class="line x" title="74:228	In the American football domain, for example, entries of type Rushing are often extracted if they yield a touchdown.3 Other Rushing entries (e.g. , which do not deliver scoring points) are typically omitted." ></td>
	<td class="line x" title="75:228	In general, the attributes of an entry can provide useful cues for predicting whether it should be selected." ></td>
	<td class="line x" title="76:228	Therefore, we can perform content selection by applying a standard classifier on each entry." ></td>
	<td class="line x" title="77:228	In Section 3.2, we explain in more detail how such a classifier can be trained." ></td>
	<td class="line x" title="78:228	We can also decide about entity selection by analyzing how entities relate to each other in the database." ></td>
	<td class="line x" title="79:228	For instance, in a game where both quarterbacks4 score, it is fairly unorthodox to mention the passing statistics for only one of them." ></td>
	<td class="line x" title="80:228	Label assignments in which either both quarterbacks are selected, or both of them are omitted should be there3A touchdown is the primary method of scoring in American football; a touchdown is worth six points and is accomplished by gaining legal possession of the ball in the opponents end zone." ></td>
	<td class="line x" title="81:228	4A quarterback in American football is the leader of a teams offense." ></td>
	<td class="line x" title="82:228	In most offenses his primary duty is passing the ball." ></td>
	<td class="line x" title="83:228	Quarterbacks are typically evaluated on their passing statistics, including total yardage, completion ratio, touchdowns, and the ability to avoid interceptions." ></td>
	<td class="line x" title="84:228	333 fore preferred." ></td>
	<td class="line x" title="85:228	This relation between quarterback passing statistics exemplifies one type of link that can hold between entities." ></td>
	<td class="line x" title="86:228	Other link types may encode contextual constraints, for instance capturing temporal and locational information." ></td>
	<td class="line x" title="87:228	(In Section 3.3, we describe a method for discovering link types which encapsulate meaningful contextual dependencies)." ></td>
	<td class="line x" title="88:228	By taking into account links between related entities, a content selection component can enforce dependencies in the labeling of related entities." ></td>
	<td class="line x" title="89:228	Our goal is to select a subset of database entities that maximally satisfies linking constraints and is as consistent as possible with the individual preferences of each entity." ></td>
	<td class="line x" title="90:228	Thus, content selection can be naturally stated as an optimization problem  we wish to find a label assignment that minimizes the cost of violating the above constraints." ></td>
	<td class="line x" title="91:228	Let C+ and C be a set of selected and omitted entities, respectively; ind+(x) and ind(x) are scores that capture the individual preference of x to be either selected or omitted, and linkL(x,y) reflects the degree of dependence between the labels of x and y based on a link of type L. Thus, the optimal label assignment for database entries x1,,xn will minimize:  xC+ ind(x)+  xC ind+(x)+ L  xi2C+ x j2C linkL(xi,x j) The first two elements in this expression capture the penalty for assigning entities to classes against their individual preferences." ></td>
	<td class="line x" title="92:228	For instance, the penalty for selecting an entry x 2 C+ will equal ind(x), i.e., xs individual preference of being ommitted." ></td>
	<td class="line x" title="93:228	The third term captures a linking penalty for all pairs of entities (xi,x j) that are connected by a link of type L, and are assigned to different classes." ></td>
	<td class="line oc" title="94:228	This formulation is similar to the energy minimization framework, which is commonly used in image analysis (Besag, 1986; Boykov et al. , 1999) and has been recently applied in natural language processing (Pang and Lee, 2004)." ></td>
	<td class="line x" title="95:228	The principal advantages of this formulation lie in its computational properties." ></td>
	<td class="line x" title="96:228	Despite seeming intractable  the number of possible subsets to consider for selection is exponential in the number of database entities  the inference problem has an exact solution." ></td>
	<td class="line x" title="97:228	Provided that the scores ind+(x), ind(x), and linkL(x,y) are positive, we can find a globally optimal label assignment in polynomial time by computing a minimal cut partition in an appropriately constructed graph (Greig et al. , 1989)." ></td>
	<td class="line x" title="98:228	In the following we first discuss how individual preference scores are estimated." ></td>
	<td class="line x" title="99:228	Next, we describe how to induce links and estimate their scores." ></td>
	<td class="line x" title="100:228	3.2 Computing Individual Preference Scores The individual preference scores are estimated by considering the values of entity attributes, recorded in the database." ></td>
	<td class="line x" title="101:228	The type and number of the attributes are determined by the entity type." ></td>
	<td class="line x" title="102:228	Therefore, we separately estimate individual preference scores for each entity type." ></td>
	<td class="line x" title="103:228	For example, individual scores for entities of type Passing are computed based on six attributes : PLAYER, CP/AT, YDS, AVG, TD, INT (see Table 1)." ></td>
	<td class="line x" title="104:228	Considerable latitude is available when selecting a classifier for delivering the individual preference scores." ></td>
	<td class="line x" title="105:228	In our experiments we used the publicly available BoosTexter system (Schapire and Singer, 2000)." ></td>
	<td class="line x" title="106:228	BoosTexter implements a boosting algorithm that combines many simple, moderately accurate categorization rules into a single, highly accurate rule." ></td>
	<td class="line x" title="107:228	For each example, it outputs a prediction along with a weight whose magnitude indicates the classifiers confidence in the prediction." ></td>
	<td class="line x" title="108:228	We thus set the individual preference scores to the weights obtained from BoosTexter." ></td>
	<td class="line x" title="109:228	The weights range from 1 to 1; we obtained non-negative numbers, simply by adding 1." ></td>
	<td class="line x" title="110:228	It is important to note that BoosTexter is a fairly effective classifier." ></td>
	<td class="line x" title="111:228	When applied to text categorization (Schapire and Singer, 2000), it outperformed a number of alternative classification methods, including Naive Bayes, decision trees, and k-nearest neighbor." ></td>
	<td class="line x" title="112:228	3.3 Link Selection and Scoring The success of collective classification depends on finding links between entities with similar label preferences." ></td>
	<td class="line x" title="113:228	In our application  concept-to-text generation, it is natural to define entity links in terms of their database relatedness." ></td>
	<td class="line x" title="114:228	Since the underlying database contains rich structural information, we can explore a wide range of relations between database entities." ></td>
	<td class="line x" title="115:228	The problem here is finding a set of links that 334 capture important contextual dependencies among many possible combinations." ></td>
	<td class="line x" title="116:228	Instead of manually specifying this set, we propose a corpus-driven method for discovering links automatically." ></td>
	<td class="line x" title="117:228	Automatic link induction can greatly reduce human effort." ></td>
	<td class="line x" title="118:228	Another advantage of the method is that it can potentially identify relations that might escape a human expert and yet, when explicitly modeled, aid in content selection." ></td>
	<td class="line x" title="119:228	We induce important links by adopting a generate-and-prune approach." ></td>
	<td class="line x" title="120:228	We first automatically create a large pool of candidate links." ></td>
	<td class="line x" title="121:228	Next, we select only links with aconsistent label distributions." ></td>
	<td class="line x" title="122:228	Construction of Candidate Links An important design decision is the type of links that we allow our algorithm to consider." ></td>
	<td class="line x" title="123:228	Since our ultimate goal is the generation of a coherent text, we wish to focus on links that capture semantic connectivity between database entities." ></td>
	<td class="line x" title="124:228	An obvious manifestation of semantic relatedness is attribute sharing." ></td>
	<td class="line x" title="125:228	Therefore, we consider links across entities with one or more shared attributes." ></td>
	<td class="line x" title="126:228	An additional constraint is implied by computational considerations: our optimization framework, based on minimal cuts in graphs, supports only pairwise links, so we restrict our attention to binary relations." ></td>
	<td class="line x" title="127:228	We generate a range of candidate link types using the following template: For every pair of entity types Ei and E j, and for every attribute k that is associated with both of them, create a link of type Li;j;k. A pair of entities ha,bi is linked by Li;j;k, if a is of type Ei, b is of type E j and they have the same value for the attribute k. For example, a link that associates statistics on Passing and Rushing performed by the same player is an instantiation of the above with Ei = Rushing, E j = Passing, and k = Player." ></td>
	<td class="line x" title="128:228	In a similar fashion, we construct link types that connect together entities with two or three attributes in common." ></td>
	<td class="line x" title="129:228	Multiple pairs of entries can be connected by the same link type." ></td>
	<td class="line x" title="130:228	If the database consists of n entity types, and the number of attribute types is bounded by m, then the number of link types constructed by this process does not exceed O(n2(m + parenleftbigm2parenrightbig + parenleftbigm3parenrightbig)) O(n2m3)." ></td>
	<td class="line x" title="131:228	In practice, this bound is much lower, since only a few attributes are shared among entity types." ></td>
	<td class="line x" title="132:228	Links can be efficiently computed using SQLs SELECT operator." ></td>
	<td class="line x" title="133:228	Link Filtering Only a small fraction of the automatically generated link types will capture meaningful contextual dependencies." ></td>
	<td class="line x" title="134:228	To filter out spurious links, we turn to the labels of the entities participating in each link." ></td>
	<td class="line x" title="135:228	Only link types in which entities have a similar distribution of label values are selected from the pool of candidates." ></td>
	<td class="line x" title="136:228	We measure similarity in label distribution using the 2 test." ></td>
	<td class="line x" title="137:228	This test has been successfully applied to similar tasks, such as feature selection in text classification (Rogati and Yang, 2002), and can be easily extended to our application." ></td>
	<td class="line x" title="138:228	Given a binary link, our null hypothesis H0 is that the labels of entities related by L are independent." ></td>
	<td class="line x" title="139:228	For each link, we compute the 2 score over a 2-by-2 table that stores joint label values of entity pairs, computed across all database entries present in the training set." ></td>
	<td class="line x" title="140:228	For links with 2 > , the null hypothesis is rejected, and the link is considered a valid discourse constraint." ></td>
	<td class="line x" title="141:228	The value of  is set to 3.84, which corresponds to a 5% level of statistical significance." ></td>
	<td class="line x" title="142:228	Link Weights The score of a link type L is defined as follows: linkL(x,y) = braceleftbigg  L i f (x,y) are linked by L 0 otherwise We estimate link weights L using simulated annealing." ></td>
	<td class="line x" title="143:228	The goal is to find weight values that minimize an objective function, defined as the error rate on the development set5 (see Section 4 for details)." ></td>
	<td class="line x" title="144:228	The individual scores and the link structure of the entities in the development set are predicted automatically using the models trained on the training set." ></td>
	<td class="line x" title="145:228	Starting from a random assignment of weight values, we compute the objective function and generate new weight values using Parks (1990) method." ></td>
	<td class="line x" title="146:228	The procedure stops when no sufficient progress is observed in subsequent iterations." ></td>
	<td class="line x" title="147:228	4 Evaluation Framework We apply the collective classification method just presented to the task of automatically learning content selection rules from a database containing football-related information." ></td>
	<td class="line x" title="148:228	In this section, we first present the sport domain we are working with, and 5Our objective function cannot be optimized analytically." ></td>
	<td class="line x" title="149:228	We therefore resort to heuristic search methods such as simulated annealing." ></td>
	<td class="line x" title="150:228	335 Entity Type Attr Inst %Aligned Entity Type Attr Inst %Aligned Defense 8 14,077 0.00 Passing 5 1,185 59.90 Drive 10 11,111 0.00 Team comparison 4 14,539 0.00 Play-by-Play 8 83,704 3.03 Punt-returns 8 940 5.74 Fumbles 8 2,937 17.78 Punting 9 950 0.87 Game 6 469 0.00 Receiving 8 6,337 11.19 Interceptions 6 894 45.05 Rushing 8 3,631 9.17 Kicking 8 943 26.93 Scoring-sum 9 3,639 53.34 Kickoff-returns 8 1,560 5.24 Team 3 4 0.00 Officials 8 464 0.00 Table 2: Entity types and their attributes in the NFL database; percentage of database entries that are aligned to summary sentences." ></td>
	<td class="line x" title="151:228	describe how we collected a corpus for evaluating collective content selection." ></td>
	<td class="line x" title="152:228	Next, we explain how we automatically obtained annotated data for training and testing our model." ></td>
	<td class="line x" title="153:228	Data As mentioned previously our goal is to generate descriptions of football games." ></td>
	<td class="line x" title="154:228	The sports domain has enjoyed popularity among natural language generation practitioners (Robin, 1994; Tanaka-Ishii et al. , 1998)." ></td>
	<td class="line x" title="155:228	The appeal is partly due to the nature of the domain  it exhibits several fixed patterns in content organization and is therefore amenable to current generation approaches." ></td>
	<td class="line x" title="156:228	At the same time, it is complex enough to present challenges at almost all stages of the generation process." ></td>
	<td class="line x" title="157:228	We compiled a corpus of descriptions of football games from the web." ></td>
	<td class="line x" title="158:228	More specifically, we obtained game summaries from the official site of the American National Football League6 (NFL)." ></td>
	<td class="line x" title="159:228	We collected summaries for the 2003 and 2004 seasons." ></td>
	<td class="line x" title="160:228	These are typically written by Associated Press journalists." ></td>
	<td class="line x" title="161:228	The corpus consists of 468 texts in total (436,580 words)." ></td>
	<td class="line x" title="162:228	The average summary length is 46.8 sentences." ></td>
	<td class="line x" title="163:228	The site not only contains a summary for each game, but also a wealth of statistics describing the performance of individual players and their teams." ></td>
	<td class="line x" title="164:228	It includes a scoring summary and a play-by-play summary giving details of the most important events in the game together with temporal (i.e. , time remaining) and positional (i.e. , location in the field) information." ></td>
	<td class="line x" title="165:228	In sum, for each game the site offers a rich repository of tabulated information which we translated into a relational database." ></td>
	<td class="line x" title="166:228	An excerpt of 6See http://www.nfl.com/scores." ></td>
	<td class="line x" title="167:228	the database is shown in Table 1." ></td>
	<td class="line x" title="168:228	Table 2 displays the entity types contained in our NFL database and lists the number of attributes (Attr) and instantiations (Inst) per type." ></td>
	<td class="line x" title="169:228	The database contains 73,400 entries in total." ></td>
	<td class="line x" title="170:228	Alignment Recall that our collective classification method is supervised." ></td>
	<td class="line x" title="171:228	The training instances are database entries and the class labels indicate whether an instance should be selected for presentation or not." ></td>
	<td class="line x" title="172:228	We could obtain this information via manual annotation performed by domain experts." ></td>
	<td class="line x" title="173:228	Instead, we opted for a less costly, automatic solution that yields large quantities of training and testing data." ></td>
	<td class="line x" title="174:228	To infer which database entries correspond to sentences in the verbalized game summaries, we used a simple anchor-based alignment technique." ></td>
	<td class="line x" title="175:228	In our domain, numbers and proper names appear with high frequency, and they constitute reliable anchors for alignment." ></td>
	<td class="line x" title="176:228	Similar to previous work (Duboue and McKeown, 2003; Sripada et al. , 2001), we employ a simple matching procedure that considers anchor overlap between entity attributes and sentence tokens." ></td>
	<td class="line x" title="177:228	Overall, the alignment procedure produced 7,513 pairs." ></td>
	<td class="line x" title="178:228	7.1% of the database entries were verbalized in our corpus and 31.7% of the corpus sentences had a database entry." ></td>
	<td class="line x" title="179:228	Table 2 presents the proportion of database entries which are verbalized in our corpus, broken down by entity type (see %Aligned)." ></td>
	<td class="line x" title="180:228	To evaluate the accuracy of this procedure, we compared our output with a gold-standard alignment produced by a domain expert." ></td>
	<td class="line x" title="181:228	After analyzing the data from five games, the expert produced 52 alignment pairs; 47 of these pairs were identified 336 Majority Baseline Standard Classifier Collective Classifier Prec Rec F-score Prec Rec F-score Prec Rec F-score Mean 29.40 68.19 40.09 44.88 62.23 49.75 52.71 76.50 60.15 Min 3.57 28.57 6.45 12.50 8.33 13.33 12.50 27.27 19.05 Max 57.14 100.00 65.12 76.92 100.00 75.00 100.00 100.00 100.00 Std Dev 10.93 15.75 12.25 15.36 18.33 13.98 21.29 18.93 19.66 Table 3: Results on content selection (precision, recall and F-score are averages over individual game summaries); comparison between the majority baseline, standard and collective classification." ></td>
	<td class="line x" title="182:228	by the automatic alignment." ></td>
	<td class="line x" title="183:228	In addition, three pairs produced by the program did not match the goldstandard alignment." ></td>
	<td class="line x" title="184:228	Thus, the automatic method achieved 94.0% precision and 90.4% recall." ></td>
	<td class="line x" title="185:228	Data Annotation For training and testing purposes, we only considered entity types for which alignments were observed in our corpus (e.g. , Fumbles, Interceptions; see Table 2)." ></td>
	<td class="line x" title="186:228	Types without alignments can be trivially regarded as inappropriate for selection in the generated text." ></td>
	<td class="line x" title="187:228	We considered database entries for which we found verbalizations in the corpus as positive instances (i.e. , they should be selected); accordingly, nonverbalized entries were considered negative instances (i.e. , they should not be selected)." ></td>
	<td class="line x" title="188:228	The overall dataset contained 105,792 instances (corresponding to 468 game summaries)." ></td>
	<td class="line x" title="189:228	Of these, 15% (68 summaries) were reserved for testing." ></td>
	<td class="line x" title="190:228	We held out 1,930 instances (10 summaries) from the training data for development purposes." ></td>
	<td class="line x" title="191:228	5 Results Our results are summarized in Table 3." ></td>
	<td class="line x" title="192:228	We compare the performance of the collective classifier against a standard classifier." ></td>
	<td class="line x" title="193:228	This can be done in our framework, simply by setting the link scores to zero." ></td>
	<td class="line x" title="194:228	We also report the performance of a majority baseline." ></td>
	<td class="line x" title="195:228	The latter was obtained by defaulting to the majority class for each entity type in the training data." ></td>
	<td class="line x" title="196:228	As can be seen from Table 2, only for two relations  Passing and Scoring-sum  the majority class predicts that the corresponding database instances should be selected for presentation." ></td>
	<td class="line x" title="197:228	Our results confirm that a content selection component can be automatically engineered for the football domain." ></td>
	<td class="line x" title="198:228	The collective classifier achieves an F-score of 60.15%." ></td>
	<td class="line x" title="199:228	This result compares favorably with Duboue and McKeown (2003) whose best model has an F-score of 51.00% on a simpler domain." ></td>
	<td class="line x" title="200:228	Our method has high recall (we want to avoid missing out information that should be presented in the output) but tends to overgenerate as demonstrated by the relatively moderate precision in Table 3." ></td>
	<td class="line x" title="201:228	Erroneous content selection decisions could be remedied by other components later in the generation process." ></td>
	<td class="line x" title="202:228	Alternatively, the obtained content selection rules could be further refined or postprocessed by a domain expert." ></td>
	<td class="line x" title="203:228	Finally, better classification performance should be possible with more expressive feature sets." ></td>
	<td class="line x" title="204:228	As we can see from the weak performance of the standard classifier, attribute values of database entries may not be sufficiently strong predictors." ></td>
	<td class="line x" title="205:228	Considering additional features tailored to the NFL domain could further enhance performance." ></td>
	<td class="line x" title="206:228	However, feature selection is not one of the main objectives of this work." ></td>
	<td class="line x" title="207:228	Our results empirically validate the importance of discourse constraints for content selection (Table 4 illustrates examples of constraints that the model discovered)." ></td>
	<td class="line x" title="208:228	We observe that adding contextual information leads to a 10.4% F-score increase over the standard classifier." ></td>
	<td class="line x" title="209:228	We used a paired t test to examine whether the differences are statistically significant." ></td>
	<td class="line x" title="210:228	The collective model significantly outperforms the standard model on both precision (t = 4.824, p < 0.01) and recall (t = 8.445, p < 0.01)." ></td>
	<td class="line x" title="211:228	It is also significantly better than the majority baseline, both in terms of recall (t = 3.181, p < 0.01) and precision (t = 8.604, p < 0.01)." ></td>
	<td class="line x" title="212:228	The standard classifier performs significantly better than the majority baseline on precision (t = 7.043, p < 0.01) but worse on recall (t =-2.274, p < 0.05)." ></td>
	<td class="line x" title="213:228	6 Conclusions and Future Work In this paper we have presented a novel, data-driven method for automating content selection." ></td>
	<td class="line x" title="214:228	Central 337 fha,bi j a 2 Sum^b 2 Sum^a.Quarter = b.Quarterg fha,bi j a 2 Sum^b 2 Play^Sum.Player1 = Play.Player1 ^Sum.Action = Play.Actiong fha,bi j a 2 Fumbles^b 2 Interceptions ^Fumbles.Player = Interceptions.Playerg Table 4: Examples of automatically derived links." ></td>
	<td class="line x" title="215:228	to our approach is the use of a collective classification model that captures contextual dependencies between input items." ></td>
	<td class="line x" title="216:228	We show that incorporation of discourse constraints yields substantial improvement over context-agnostic methods." ></td>
	<td class="line x" title="217:228	Our approach is linguistically grounded, computationally efficient, and viable in practical applications." ></td>
	<td class="line x" title="218:228	In the future, we plan to explore how to integrate more refined discourse models in the content selection process." ></td>
	<td class="line x" title="219:228	Currently, we consider a limited set of contextual dependencies based on attribute similarity." ></td>
	<td class="line x" title="220:228	Ideally, we would like to express more complex relations between items." ></td>
	<td class="line x" title="221:228	For instance, we may want to represent disjunctive constraints, such as at least one of the defense players should be mentioned in the summary. Such dependencies can be efficiently handled in a collective classification framework by using approximate probabilistic inference (Taskar et al. , 2002)." ></td>
	<td class="line x" title="222:228	Another promising approach is the combination of our automatically acquired cross-entity links with domain knowledge." ></td>
	<td class="line x" title="223:228	Needless to say, content selection is one of several components within a working generation system." ></td>
	<td class="line x" title="224:228	An interesting question is how to integrate our component into a generation pipeline, using feedback from other components to guide collective content selection." ></td>
	<td class="line x" title="225:228	Acknowledgments The authors acknowledge the support of the National Science Foundation (Barzilay; CAREER grant IIS-0448168 and grant IIS-0415865) and EPSRC (Lapata; grant GR/T04540/01)." ></td>
	<td class="line x" title="226:228	We are grateful to Eli Barzilay for his help with data collection, and Luke Zettelmoyer who explained the many rules of American football to us." ></td>
	<td class="line x" title="227:228	Thanks to Michael Collins, Amit Dubey, Noemie Elhadad, Dina Katabi, Frank Keller, Igor Malioutov, Smaranda Muresan, Martin Rinard, Kevin Simler and the anonymous reviewers for helpful comments and suggestions." ></td>
	<td class="line x" title="228:228	Any opinions, findings, and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the National Science Foundation or EPSRC." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="H05-1044
Recognizing Contextual Polarity In Phrase-Level Sentiment Analysis
Wilson, Theresa;Wiebe, Janyce M.;Hoffmann, Paul;"></td>
	<td class="line x" title="1:197	Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 347354, Vancouver, October 2005." ></td>
	<td class="line x" title="2:197	c2005 Association for Computational Linguistics Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis Theresa Wilson Intelligent Systems Program University of Pittsburgh Pittsburgh, PA 15260 twilson@cs.pitt.edu Janyce Wiebe Department of Computer Science University of Pittsburgh Pittsburgh, PA 15260 wiebe@cs.pitt.edu Paul Hoffmann Intelligent Systems Program University of Pittsburgh Pittsburgh, PA 15260 hoffmanp@cs.pitt.edu Abstract This paper presents a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions." ></td>
	<td class="line x" title="3:197	With this approach, the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are significantly better than baseline." ></td>
	<td class="line x" title="4:197	1 Introduction Sentiment analysis is the task of identifying positive and negative opinions, emotions, and evaluations." ></td>
	<td class="line x" title="5:197	Most work on sentiment analysis has been done at the document level, for example distinguishing positive from negative reviews." ></td>
	<td class="line x" title="6:197	However, tasks such as multi-perspective question answering and summarization, opinion-oriented information extraction, and mining product reviews require sentence-level or even phrase-level sentiment analysis." ></td>
	<td class="line x" title="7:197	For example, if a question answering system is to successfully answer questions about peoples opinions, it must be able to pinpoint expressions of positive and negative sentiments, such as we find in the sentences below: (1) African observers generally approved+ of his victory while Western governments denounced it." ></td>
	<td class="line x" title="8:197	(2) A succession of officers filled the TV screen to say they supported+ the people and that the killings were not tolerable. (3) We donprimet hate+ the sinner, he says, but we hate the sin. A typical approach to sentiment analysis is to start with a lexicon of positive and negative words and phrases." ></td>
	<td class="line x" title="9:197	In these lexicons, entries are tagged with their a priori prior polarity: out of context, does the word seem to evoke something positive or something negative." ></td>
	<td class="line x" title="10:197	For example, beautiful has a positive prior polarity, and horrid has a negative prior polarity." ></td>
	<td class="line x" title="11:197	However, the contextual polarity of the phrase in which a word appears may be different from the words prior polarity." ></td>
	<td class="line x" title="12:197	Consider the underlined polarity words in the sentence below: (4) Philip Clapp, president of the National Environment Trust, sums up well the general thrust of the reaction of environmental movements: There is no reason at all to believe that the polluters are suddenly going to become reasonable. Of these words, Trust, well, reason, and reasonable have positive prior polarity, but they are not all being used to express positive sentiments." ></td>
	<td class="line x" title="13:197	The word reason is negated, making the contextual polarity negative." ></td>
	<td class="line x" title="14:197	The phrase no reason at all to believe changes the polarity of the proposition that follows; because reasonable falls within this proposition, its contextual polarity becomes negative." ></td>
	<td class="line x" title="15:197	The word Trust is simply part of a referring expression and is not being used to express a sentiment; thus, its contextual polarity is neutral." ></td>
	<td class="line x" title="16:197	Similarly for polluters: in the context of the article, it simply refers to companies that pollute." ></td>
	<td class="line x" title="17:197	Only well has the same prior and contextual polarity." ></td>
	<td class="line x" title="18:197	Many things must be considered in phrase-level sentiment analysis." ></td>
	<td class="line x" title="19:197	Negation may be local (e.g. , not good), or involve longer-distance dependencies such as the negation of the proposition (e.g. , does not look very good) or the negation of the subject (e.g. , 347 no one thinks that its good)." ></td>
	<td class="line x" title="20:197	In addition, certain phrases that contain negation words intensify rather than change polarity (e.g. , not only good but amazing)." ></td>
	<td class="line x" title="21:197	Contextual polarity may also be influenced by modality (e.g. , whether the proposition is asserted to be real (realis) or not real (irrealis)  no reason at all to believe is irrealis, for example); word sense (e.g. , Environmental Trust versus He has won the peoples trust); the syntactic role of a word in the sentence (e.g. , polluters are versus they are polluters); and diminishers such as little (e.g. , little truth, little threat)." ></td>
	<td class="line x" title="22:197	(See (Polanya and Zaenen, 2004) for a more detailed discussion of contextual polarity influencers.)" ></td>
	<td class="line x" title="23:197	This paper presents new experiments in automatically distinguishing prior and contextual polarity." ></td>
	<td class="line x" title="24:197	Beginning with a large stable of clues marked with prior polarity, we identify the contextual polarity of the phrases that contain instances of those clues in the corpus." ></td>
	<td class="line x" title="25:197	We use a two-step process that employs machine learning and a variety of features." ></td>
	<td class="line x" title="26:197	The first step classifies each phrase containing a clue as neutral or polar." ></td>
	<td class="line x" title="27:197	The second step takes all phrases marked in step one as polar and disambiguates their contextual polarity (positive, negative, both, or neutral)." ></td>
	<td class="line x" title="28:197	With this approach, the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are significantly better than baseline." ></td>
	<td class="line x" title="29:197	In addition, we describe new manual annotations of contextual polarity and a successful inter-annotator agreement study." ></td>
	<td class="line x" title="30:197	2 Manual Annotation Scheme To create a corpus for the experiments below, we added contextual polarity judgments to existing annotations in the Multi-perspective Question Answering (MPQA) Opinion Corpus1, namely to the annotations of subjective expressions2." ></td>
	<td class="line x" title="31:197	A subjective expression is any word or phrase used to express an opinion, emotion, evaluation, stance, speculation, 1The MPQA Corpus is described in (Wiebe et al. , 2005) and available at nrrc.mitre.org/NRRC/publications.htm." ></td>
	<td class="line x" title="32:197	2In the MPQA Corpus, subjective expressions are direct subjective expressions with non-neutral expression intensity, plus all the expressive subjective elements." ></td>
	<td class="line x" title="33:197	Please see (Wiebe et al. , 2005) for more details on the existing annotations in the MPQA Corpus." ></td>
	<td class="line x" title="34:197	etc. A general covering term for such states is private state (Quirk et al. , 1985)." ></td>
	<td class="line x" title="35:197	In the MPQA Corpus, subjective expressions of varying lengths are marked, from single words to long phrases." ></td>
	<td class="line x" title="36:197	For this work, our focus is on sentiment expressions  positive and negative expressions of emotions, evaluations, and stances." ></td>
	<td class="line x" title="37:197	As these are types of subjective expressions, to create the corpus, we just needed to manually annotate the existing subjective expressions with their contextual polarity." ></td>
	<td class="line x" title="38:197	In particular, we developed an annotation scheme3 for marking the contextual polarity of subjective expressions." ></td>
	<td class="line x" title="39:197	Annotators were instructed to tag the polarity of subjective expressions as positive, negative, both, or neutral." ></td>
	<td class="line x" title="40:197	The positive tag is for positive emotions (Im happy), evaluations (Great idea!), and stances (She supports the bill)." ></td>
	<td class="line x" title="41:197	The negative tag is for negative emotions (Im sad), evaluations (Bad idea!), and stances (Shes against the bill)." ></td>
	<td class="line x" title="42:197	The both tag is applied to sentiment expressions that have both positive and negative polarity." ></td>
	<td class="line x" title="43:197	The neutral tag is used for all other subjective expressions: those that express a different type of subjectivity such as speculation, and those that do not have positive or negative polarity." ></td>
	<td class="line x" title="44:197	Below are examples of contextual polarity annotations." ></td>
	<td class="line x" title="45:197	The tags are in boldface, and the subjective expressions with the given tags are underlined." ></td>
	<td class="line x" title="46:197	(5) Thousands of coup supporters celebrated (positive) overnight, waving flags, blowing whistles." ></td>
	<td class="line x" title="47:197	(6) The criteria set by Rice are the following: the three countries in question are repressive (negative) and grave human rights violators (negative) . . ." ></td>
	<td class="line x" title="48:197	(7) Besides, politicians refer to good and evil (both) only for purposes of intimidation and exaggeration." ></td>
	<td class="line x" title="49:197	(8) Jerome says the hospital feels (neutral) no different than a hospital in the states." ></td>
	<td class="line x" title="50:197	The annotators were asked to judge the contextual polarity of the sentiment that is ultimately being conveyed by the subjective expression, i.e., once the sentence has been fully interpreted." ></td>
	<td class="line x" title="51:197	Thus, the subjective expression, they have not succeeded, and 3The annotation instructions are available at http://www.cs.pitt.edu/twilson." ></td>
	<td class="line x" title="52:197	348 will never succeed, was marked as positive in the sentence, They have not succeeded, and will never succeed, in breaking the will of this valiant people." ></td>
	<td class="line x" title="53:197	The reasoning is that breaking the will of a valiant people is negative; hence, not succeeding in breaking their will is positive." ></td>
	<td class="line x" title="54:197	3 Agreement Study To measure the reliability of the polarity annotation scheme, we conducted an agreement study with two annotators, using 10 documents from the MPQA Corpus." ></td>
	<td class="line x" title="55:197	The 10 documents contain 447 subjective expressions." ></td>
	<td class="line x" title="56:197	Table 1 shows the contingency table for the two annotators judgments." ></td>
	<td class="line x" title="57:197	Overall agreement is 82%, with a Kappa () value of 0.72." ></td>
	<td class="line x" title="58:197	Neutral Positive Negative Both Total Neutral 123 14 24 0 161 Positive 16 73 5 2 96 Negative 14 2 167 1 184 Both 0 3 0 3 6 Total 153 92 196 6 447 Table 1: Agreement for Subjective Expressions (Agreement: 82%, : 0.72) For 18% of the subjective expressions, at least one annotator used an uncertain tag when marking polarity." ></td>
	<td class="line x" title="59:197	If we consider these cases to be borderline and exclude them from the study, percent agreement increases to 90% and Kappa rises to 0.84." ></td>
	<td class="line x" title="60:197	Thus, the annotator agreement is especially high when both are certain." ></td>
	<td class="line x" title="61:197	(Note that all annotations are included in the experiments described below.)" ></td>
	<td class="line x" title="62:197	4 Corpus In total, 15,991 subjective expressions from 425 documents (8,984 sentences) were annotated with contextual polarity as described above." ></td>
	<td class="line x" title="63:197	Of these sentences, 28% contain no subjective expressions, 25% contain only one, and 47% contain two or more." ></td>
	<td class="line x" title="64:197	Of the 4,247 sentences containing two or more subjective expressions, 17% contain mixtures of positive and negative expressions, and 62% contain mixtures of polar (positive/negative/both) and neutral subjective expressions." ></td>
	<td class="line x" title="65:197	The annotated documents are divided into two sets." ></td>
	<td class="line x" title="66:197	The first (66 documents/1,373 sentences/2,808 subjective expressions) is a development set, used for data exploration and feature development." ></td>
	<td class="line x" title="67:197	We use the second set (359 documents/7,611 sentences/13,183 subjective expressions) in 10-fold cross-validation experiments, described below." ></td>
	<td class="line x" title="68:197	5 Prior-Polarity Subjectivity Lexicon For the experiments in this paper, we use a lexicon of over 8,000 subjectivity clues." ></td>
	<td class="line x" title="69:197	Subjectivity clues are words and phrases that may be used to express private states, i.e., they have subjective usages (though they may have objective usages as well)." ></td>
	<td class="line x" title="70:197	For this work, only single-word clues are used." ></td>
	<td class="line x" title="71:197	To compile the lexicon, we began with a list of subjectivity clues from (Riloff and Wiebe, 2003)." ></td>
	<td class="line x" title="72:197	The words in this list were grouped in previous work according to their reliability as subjectivity clues." ></td>
	<td class="line x" title="73:197	Words that are subjective in most contexts were marked strongly subjective (strongsubj), and those that may only have certain subjective usages were marked weakly subjective (weaksubj)." ></td>
	<td class="line x" title="74:197	We expanded the list using a dictionary and a thesaurus, and also added words from the General Inquirer positive and negative word lists (GeneralInquirer, 2000) which we judged to be potentially subjective." ></td>
	<td class="line x" title="75:197	We also gave the new words reliability tags, either strongsubj or weaksubj." ></td>
	<td class="line x" title="76:197	The next step was to tag the clues in the lexicon with their prior polarity." ></td>
	<td class="line x" title="77:197	For words that came from positive and negative word lists (General-Inquirer, 2000; Hatzivassiloglou and McKeown, 1997), we largely retained their original polarity, either positive or negative." ></td>
	<td class="line x" title="78:197	We assigned the remaining words one of the tags positive, negative, both or neutral." ></td>
	<td class="line x" title="79:197	By far, the majority of clues, 92.8%, are marked as having either positive (33.1%) or negative (59.7%) prior polarity." ></td>
	<td class="line x" title="80:197	Only a small number of clues (0.3%) are marked as having both positive and negative polarity." ></td>
	<td class="line x" title="81:197	6.9% of the clues in the lexicon are marked as neutral." ></td>
	<td class="line x" title="82:197	Examples of these are verbs such as feel, look, and think, and intensifiers such as deeply, entirely, and practically." ></td>
	<td class="line x" title="83:197	These words are included because, although their prior polarity is neutral, they are good clues that a sentiment is being expressed (e.g. , feels slighted, look forward to)." ></td>
	<td class="line x" title="84:197	Including them increases the coverage of the system." ></td>
	<td class="line x" title="85:197	349 6 Experiments The goal of the experiments described below is to classify the contextual polarity of the expressions that contain instances of the subjectivity clues in our lexicon." ></td>
	<td class="line x" title="86:197	What the system specifically does is give each clue instance its own label." ></td>
	<td class="line x" title="87:197	Note that the system does not try to identify expression boundaries." ></td>
	<td class="line x" title="88:197	Doing so might improve performance and is a promising avenue for future research." ></td>
	<td class="line x" title="89:197	6.1 Definition of the Gold Standard We define the gold standard used to train and test the system in terms of the manual annotations described in Section 2." ></td>
	<td class="line x" title="90:197	The gold standard class of a clue instance that is not in a subjective expression is neutral: since the clue is not even in a subjective expression, it is not contained in a sentiment expression." ></td>
	<td class="line x" title="91:197	Otherwise, if a clue instance appears in just one subjective expression (or in multiple subjective expressions with the same contextual polarity), then the class assigned to the clue instance is the class of the subjective expression(s)." ></td>
	<td class="line x" title="92:197	If a clue appears in at least one positive and one negative subjective expression (or in a subjective expression marked as both), then its class is both." ></td>
	<td class="line x" title="93:197	If it is in a mixture of negative and neutral subjective expressions, its class is negative; if it is in a mixture of positive and neutral subjective expressions, its class is positive." ></td>
	<td class="line x" title="94:197	6.2 Performance of a Prior-Polarity Classifier An important question is how useful prior polarity alone is for identifying contextual polarity." ></td>
	<td class="line x" title="95:197	To answer this question, we create a classifier that simply assumes that the contextual polarity of a clue instance is the same as the clues prior polarity, and we explore the classifiers performance on the development set." ></td>
	<td class="line x" title="96:197	This simple classifier has an accuracy of 48%." ></td>
	<td class="line x" title="97:197	From the confusion matrix given in Table 2, we see that 76% of the errors result from words with nonneutral prior polarity appearing in phrases with neutral contextual polarity." ></td>
	<td class="line x" title="98:197	6.3 Contextual Polarity Disambiguation The fact that words with non-neutral prior polarity so frequently appear in neutral contexts led us to Prior-Polarity Classifier Neut Pos Neg Both Total Neut 798 784 698 4 2284 Pos 81 371 40 0 492 Gold Neg 149 181 622 0 952 Both 4 11 13 5 33 Total 1032 1347 1373 9 3761 Table 2: Confusion matrix for the prior-polarity classifier on the development set." ></td>
	<td class="line x" title="99:197	adopt a two-step approach to contextual polarity disambiguation." ></td>
	<td class="line x" title="100:197	For the first step, we concentrate on whether clue instances are neutral or polar in context (where polar in context refers to having a contextual polarity that is positive, negative or both)." ></td>
	<td class="line x" title="101:197	For the second step, we take all clue instances marked as polar in step one, and focus on identifying their contextual polarity." ></td>
	<td class="line x" title="102:197	For both steps, we develop classifiers using the BoosTexter AdaBoost.HM (Schapire and Singer, 2000) machine learning algorithm with 5000 rounds of boosting." ></td>
	<td class="line x" title="103:197	The classifiers are evaluated in 10-fold cross-validation experiments." ></td>
	<td class="line x" title="104:197	6.3.1 Neutral-Polar Classification The neutral-polar classifier uses 28 features, listed in Table 3." ></td>
	<td class="line x" title="105:197	Word Features: Word context is a bag of three word tokens: the previous word, the word itself, and the next word." ></td>
	<td class="line x" title="106:197	The prior polarity and reliability class are indicated in the lexicon." ></td>
	<td class="line x" title="107:197	Modification Features: These are binary relationship features." ></td>
	<td class="line x" title="108:197	The first four involve relationships with the word immediately before or after: if the word is a noun preceded by an adjective, if the preceding word is an adverb other than not, if the preceding word is an intensifier, and if the word itself is an intensifier." ></td>
	<td class="line x" title="109:197	A word is considered an intensifier if it appears in a list of intensifiers and if it precedes a word of the appropriate part-of-speech (e.g. , an intensifier adjective must come before a noun)." ></td>
	<td class="line x" title="110:197	The modify features involve the dependency parse tree for the sentence, obtained by first parsing the sentence (Collins, 1997) and then converting the tree into its dependency representation (Xia and Palmer, 2001)." ></td>
	<td class="line x" title="111:197	In a dependency representation, every node in the tree structure is a surface word (i.e. , there are no abstract nodes such as NP or VP)." ></td>
	<td class="line x" title="112:197	The edge between a parent and a child specifies the grammatical relationship between the two words." ></td>
	<td class="line x" title="113:197	Figure 1 shows 350 Word Features Sentence Features Structure Features word token strongsubj clues in current sentence: count in subject: binary word part-of-speech strongsubj clues in previous sentence: count in copular: binary word context strongsubj clues in next sentence: count in passive: binary prior polarity: positive, negative, both, neutral weaksubj clues in current sentence: count reliability class: strongsubj or weaksubj weaksubj clues in previous sentence: count Modification Features weaksubj clues in next sentence: count Document Feature preceeded by adjective: binary adjectives in sentence: count document topic preceeded by adverb (other than not): binary adverbs in sentence (other than not): count preceeded by intensifier: binary cardinal number in sentence: binary is intensifier: binary pronoun in sentence: binary modifies strongsubj: binary modal in sentence (other than will): binary modifies weaksubj: binary modified by strongsubj: binary modified by weaksubj: binary Table 3: Features for neutral-polar classification T h e h u m a n r i g h t s r e p o r t a p o s e s s u b s t a n t i a l c h a l l e n g e t o U St h e i n t e r p r e t a t i o n o f g o o d a n d e v i l d e t d e t d e t a d j a d j o b j s u b j m o d m o d c o n j c o n j p o b j p o b j p p ( p o s ) ( n e g ) ( p o s ) ( n e g ) ( p o s ) Figure 1: The dependency tree for the sentence The human rights report poses a substantial challenge to the US interpretation of good and evil." ></td>
	<td class="line x" title="114:197	Prior polarity is marked in parentheses for words that match clues from the lexicon." ></td>
	<td class="line x" title="115:197	an example." ></td>
	<td class="line x" title="116:197	The modifies strongsubj/weaksubj features are true if the word and its parent share an adj, mod or vmod relationship, and if its parent is an instance of a clue from the lexicon with strongsubj/weaksubj reliability." ></td>
	<td class="line x" title="117:197	The modified by strongsubj/weaksubj features are similar, but look for relationships and clues in the words children." ></td>
	<td class="line x" title="118:197	Structure Features: These are binary features that are determined by starting with the word instance and climbing up the dependency parse tree toward the root, looking for particular relationships, words, or patterns." ></td>
	<td class="line x" title="119:197	The in subject feature is true if we find a subj relationship." ></td>
	<td class="line x" title="120:197	The in copular feature is true if in subject is false and if a node along the path is both a main verb and a copular verb." ></td>
	<td class="line x" title="121:197	The in passive features is true if a passive verb pattern is found on the climb." ></td>
	<td class="line x" title="122:197	Sentence Features: These are features that were found useful for sentence-level subjectivity classification by Wiebe and Riloff (2005)." ></td>
	<td class="line x" title="123:197	They include counts of strongsubj and weaksubj clues in the current, previous and next sentences, counts of adjectives and adverbs other than not in the current sentence, and binary features to indicate whether the sentence contains a pronoun, a cardinal number, and a modal other than will." ></td>
	<td class="line x" title="124:197	Document Feature: There is one document feature representing the topic of the document." ></td>
	<td class="line x" title="125:197	A document may belong to one of 15 topics ranging from specific (e.g. , the 2002 presidential election in Zimbabwe) to more general (e.g. , economics) topics." ></td>
	<td class="line x" title="126:197	Table 4 gives neutral-polar classification results for the 28-feature classifier and two simpler classifiers that provide our baselines." ></td>
	<td class="line x" title="127:197	The first row in the table lists the results for a classifier that uses just one feature, the word token." ></td>
	<td class="line x" title="128:197	The second row shows the results for a classifier that uses both the word token and the words prior polarity as features." ></td>
	<td class="line x" title="129:197	The results for the 28-feature classifier are listed in the last row." ></td>
	<td class="line x" title="130:197	The 28-feature classifier performs significantly better (1-tailed t-test, p  .05) than the two simpler classifiers, as measured by accuracy, polar F-measure, and neutral F-measure ( = 1)." ></td>
	<td class="line x" title="131:197	It has an accuracy of 75.9%, with a polar F-measure of 63.4 and a neutral F-measure of 82.1." ></td>
	<td class="line x" title="132:197	Focusing on the metrics for polar expressions, its interesting to note that using just the word token as a feature produces a classifier with a precision slightly better than the 28-feature classifier, but with a recall that is 20% lower." ></td>
	<td class="line x" title="133:197	Adding a feature for the prior 351 Word Features word token word prior polarity: positive, negative, both, neutral Polarity Features negated: binary negated subject: binary modifies polarity: positive, negative, neutral, both, notmod modified by polarity: positive, negative, neutral, both, notmod conj polarity: positive, negative, neutral, both, notmod general polarity shifter: binary negative polarity shifter: binary positive polarity shifter: binary Table 6: Features for polarity classification polarity improves recall so that it is only 4.4% lower, but this hurts precision, which drops to 4.2% lower than the 28-feature classifiers precision." ></td>
	<td class="line x" title="134:197	It is only with all the features that we get the best result, good precision with the highest recall." ></td>
	<td class="line x" title="135:197	The clues in the prior-polarity lexicon have 19,506 instances in the test set." ></td>
	<td class="line x" title="136:197	According to the 28-feature neutral-polar classifier, 5,671 of these instances are polar in context." ></td>
	<td class="line x" title="137:197	It is these clue instances that are passed on to the second step in the contextual disambiguation process, polarity classification." ></td>
	<td class="line x" title="138:197	6.3.2 Polarity Classification Ideally, this second step in the disambiguation process would be a three-way classification task, determining whether the contextual polarity is positive, negative or both." ></td>
	<td class="line x" title="139:197	However, although the majority of neutral expressions have been filtered out by the neutral-polar classification in step one, a number still remain." ></td>
	<td class="line x" title="140:197	So, for this step, the polarity classification task remains four-way: positive, negative, both, and neutral." ></td>
	<td class="line x" title="141:197	Table 6 lists the features used by the polarity classifier." ></td>
	<td class="line x" title="142:197	Word token and word prior polarity are unchanged from the neutral-polar classifier." ></td>
	<td class="line x" title="143:197	Negated is a binary feature that captures whether the word is being locally negated: its value is true if a negation word or phrase is found within the four preceeding words or in any of the words children in the dependency tree, and if the negation word is not in a phrase that intensifies rather than negates (e.g. , not only)." ></td>
	<td class="line x" title="144:197	The negated subject feature is true if the subject of the clause containing the word is negated." ></td>
	<td class="line x" title="145:197	The modifies polarity, modified by polarity, and conj polarity features capture specific relationships between the word instance and other polarity words it may be related to." ></td>
	<td class="line x" title="146:197	If the word and its parent in the dependency tree share an obj, adj, mod, or vmod relationship, the modifies polarity feature is set to the prior polarity of the words parent (if the parent is not in our prior-polarity lexicon, its prior polarity is set to neutral)." ></td>
	<td class="line x" title="147:197	The modified by polarity feature is similar, looking for adj, mod, and vmod relationships and polarity clues within the words children." ></td>
	<td class="line x" title="148:197	The conj polarity feature determines if the word is in a conjunction." ></td>
	<td class="line x" title="149:197	If so, the value of this feature is its siblings prior polarity (as above, if the sibling is not in the lexicon, its prior polarity is neutral)." ></td>
	<td class="line x" title="150:197	Figure 1 helps to illustrate these features: modifies polarity is negative for the word substantial, modified by polarity is positive for the word challenge, and conj polarity is negative for the word good. The last three polarity features look in a window of four words before, searching for the presence of particular types of polarity influencers." ></td>
	<td class="line x" title="151:197	General polarity shifters reverse polarity (e.g. , little truth, little threat)." ></td>
	<td class="line x" title="152:197	Negative polarity shifters typically make the polarity of an expression negative (e.g. , lack of understanding)." ></td>
	<td class="line x" title="153:197	Positive polarity shifters typically make the polarity of an expression positive (e.g. , abate the damage)." ></td>
	<td class="line x" title="154:197	The polarity classification results for this second step in the contextual disambiguation process are given in Table 5." ></td>
	<td class="line x" title="155:197	Also listed in the table are results for the two simple classifiers that provide our baselines." ></td>
	<td class="line x" title="156:197	The first line in Table 5 lists the results for the classifier that uses just one feature, the word token. The second line shows the results for the classifier that uses both the word token and the words prior polarity as features." ></td>
	<td class="line x" title="157:197	The last line shows the results for the polarity classifier that uses all 10 features from Table 6." ></td>
	<td class="line x" title="158:197	Mirroring the results from step one, the more complex classifier performs significantly better than the simpler classifiers, as measured by accuracy and all of the F-measures." ></td>
	<td class="line x" title="159:197	The 10-feature classifier achieves an accuracy of 65.7%, which is 4.3% higher than the more challenging baseline provided by the word + prior polarity classifier." ></td>
	<td class="line x" title="160:197	Positive Fmeasure is 65.1 (5.7% higher); negative F-measure is 77.2 (2.3% higher); and neutral F-measure is 46.2 (13.5% higher)." ></td>
	<td class="line x" title="161:197	Focusing on the metrics for positive and negative expressions, we again see that the simpler classifiers 352 Acc Polar Rec Polar Prec Polar F Neut Rec Neut Prec Neut F word token 73.6 45.3 72.2 55.7 89.9 74.0 81.2 word+priorpol 74.2 54.3 68.6 60.6 85.7 76.4 80.7 28 features 75.9 56.8 71.6 63.4 87.0 77.7 82.1 Table 4: Results for Step 1 Neutral-Polar Classification Positive Negative Both Neutral Acc Rec Prec F Rec Prec F Rec Prec F Rec Prec F word token 61.7 59.3 63.4 61.2 83.9 64.7 73.1 9.2 35.2 14.6 30.2 50.1 37.7 word+priorpol 63.0 69.4 55.3 61.6 80.4 71.2 75.5 9.2 35.2 14.6 33.5 51.8 40.7 10 features 65.7 67.1 63.3 65.1 82.1 72.9 77.2 11.2 28.4 16.1 41.4 52.4 46.2 Table 5: Results for Step 2 Polarity Classification." ></td>
	<td class="line x" title="162:197	Experiment Features Removed AB1 negated, negated subject AB2 modifies polarity, modified by polarity AB3 conj polarity AB4 general, negative, and positive polarity shifters Table 7: Features for polarity classification take turns doing better or worse for precision and recall." ></td>
	<td class="line x" title="163:197	Using just the word token, positive precision is slightly higher than for the 10-feature classifier, but positive recall is 11.6% lower." ></td>
	<td class="line x" title="164:197	Add the prior polarity, and positive recall improves, but at the expense of precision, which is 12.6% lower than for the 10-feature classifier." ></td>
	<td class="line x" title="165:197	The results for negative expressions are similar." ></td>
	<td class="line x" title="166:197	The word-token classifier does well on negative recall but poorly on negative precision." ></td>
	<td class="line x" title="167:197	When prior polarity is added, negative recall improves but negative precision drops." ></td>
	<td class="line x" title="168:197	It is only with the addition of the polarity features that we achieve both higher precisions and higher recalls." ></td>
	<td class="line x" title="169:197	To explore how much the various polarity features contribute to the performance of the polarity classifier, we perform four experiments." ></td>
	<td class="line x" title="170:197	In each experiment, a different set of polarity features is excluded, and the polarity classifier is retrained and evaluated." ></td>
	<td class="line x" title="171:197	Table 7 lists the features that are removed for each experiment." ></td>
	<td class="line x" title="172:197	The only significant difference in performance in these experiments is neutral F-measure when the modification features (AB2) are removed." ></td>
	<td class="line x" title="173:197	These ablation experiments show that the combination of features is needed to achieve significant results over baseline for polarity classification." ></td>
	<td class="line oc" title="174:197	7 Related Work Much work on sentiment analysis classifies documents by their overall sentiment, for example determining whether a review is positive or negative (e.g. , (Turney, 2002; Dave et al. , 2003; Pang and Lee, 2004; Beineke et al. , 2004))." ></td>
	<td class="line x" title="175:197	In contrast, our experiments classify individual words and phrases." ></td>
	<td class="line x" title="176:197	A number of researchers have explored learning words and phrases with prior positive or negative polarity (another term is semantic orientation) (e.g. , (Hatzivassiloglou and McKeown, 1997; Kamps and Marx, 2002; Turney, 2002))." ></td>
	<td class="line x" title="177:197	In contrast, we begin with a lexicon of words with established prior polarities, and identify the contextual polarity of phrases in which instances of those words appear in the corpus." ></td>
	<td class="line x" title="178:197	To make the relationship between that task and ours clearer, note that some word lists used to evaluate methods for recognizing prior polarity are included in our prior-polarity lexicon (General Inquirer lists (General-Inquirer, 2000) used for evaluation by Turney, and lists of manually identified positive and negative adjectives, used for evaluation by Hatzivassiloglou and McKeown)." ></td>
	<td class="line x" title="179:197	Some research classifies the sentiments of sentences." ></td>
	<td class="line x" title="180:197	Yu and Hatzivassiloglou (2003), Kim and Hovy (2004), Hu and Liu (2004), and Grefenstette et al.(2001)4 all begin by first creating prior-polarity lexicons." ></td>
	<td class="line x" title="182:197	Yu and Hatzivassiloglou then assign a sentiment to a sentence by averaging the prior semantic orientations of instances of lexicon words in the sentence." ></td>
	<td class="line x" title="183:197	Thus, they do not identify the contextual polarity of individual phrases containing clues, as we 4In (Grefenstette et al. , 2001), the units that are classified are fixed windows around named entities rather than sentences." ></td>
	<td class="line x" title="184:197	353 do in this paper." ></td>
	<td class="line x" title="185:197	Kim and Hovy, Hu and Liu, and Grefenstette et al. multiply or count the prior polarities of clue instances in the sentence." ></td>
	<td class="line x" title="186:197	They also consider local negation to reverse polarity." ></td>
	<td class="line x" title="187:197	However, they do not use the other types of features in our experiments, and they restrict their tags to positive and negative (excluding our both and neutral categories)." ></td>
	<td class="line x" title="188:197	In addition, their systems assign one sentiment per sentence; our system assigns contextual polarity to individual expressions." ></td>
	<td class="line x" title="189:197	As seen above, sentences often contain more than one sentiment expression." ></td>
	<td class="line x" title="190:197	Nasukawa, Yi, and colleagues (Nasukawa and Yi, 2003; Yi et al. , 2003) classify the contextual polarity of sentiment expressions, as we do." ></td>
	<td class="line x" title="191:197	Thus, their work is probably most closely related to ours." ></td>
	<td class="line x" title="192:197	They classify expressions that are about specific items, and use manually developed patterns to classify polarity." ></td>
	<td class="line x" title="193:197	These patterns are high-quality, yielding quite high precision, but very low recall." ></td>
	<td class="line x" title="194:197	Their system classifies a much smaller proportion of the sentiment expressions in a corpus than ours does." ></td>
	<td class="line x" title="195:197	8 Conclusions In this paper, we present a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions." ></td>
	<td class="line x" title="196:197	With this approach, we are able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are significantly better than baseline." ></td>
	<td class="line x" title="197:197	9 Acknowledgments This work was supported in part by the NSF under grant IIS-0208798 and by the Advanced Research and Development Activity (ARDA)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="H05-1045
Identifying Sources Of Opinions With Conditional Random Fields And Extraction Patterns
Choi, Yejin;Cardie, Claire;Riloff, Ellen;Patwardhan, Siddharth;"></td>
	<td class="line x" title="1:194	Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 355362, Vancouver, October 2005." ></td>
	<td class="line x" title="2:194	c2005 Association for Computational Linguistics Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns Yejin Choi and Claire Cardie Department of Computer Science Cornell University Ithaca, NY 14853 {ychoi,cardie}@cs.cornell.edu Ellen Riloff and Siddharth Patwardhan School of Computing University of Utah Salt Lake City, UT 84112 {riloff,sidd}@cs.utah.edu Abstract Recent systems have been developed for sentiment classification, opinion recognition, and opinion analysis (e.g. , detecting polarity and strength)." ></td>
	<td class="line x" title="3:194	We pursue another aspect of opinion analysis: identifying the sources of opinions, emotions, and sentiments." ></td>
	<td class="line x" title="4:194	We view this problem as an information extraction task and adopt a hybrid approach that combines Conditional Random Fields (Lafferty et al. , 2001) and a variation of AutoSlog (Riloff, 1996a)." ></td>
	<td class="line x" title="5:194	While CRFs model source identification as a sequence tagging task, AutoSlog learns extraction patterns." ></td>
	<td class="line x" title="6:194	Our results show that the combination of these two methods performs better than either one alone." ></td>
	<td class="line x" title="7:194	The resulting system identifies opinion sources with 79.3% precision and 59.5% recall using a head noun matching measure, and 81.2% precision and 60.6% recall using an overlap measure." ></td>
	<td class="line x" title="8:194	1 Introduction In recent years, there has been a great deal of interest in methods for automatically identifying opinions, emotions, and sentiments in text." ></td>
	<td class="line oc" title="9:194	Much of this research explores sentiment classification, a text categorization task in which the goal is to classify a document as having positive or negative polarity (e.g. , Das and Chen (2001), Pang et al.(2002), Turney (2002), Dave et al.(2003), Pang and Lee (2004))." ></td>
	<td class="line oc" title="12:194	Other research efforts analyze opinion expressions at the sentence level or below to recognize opinions, their polarity, and their strength (e.g. , Dave et al.(2003), Pang and Lee (2004), Wilson et al.(2004), Yu and Hatzivassiloglou (2003), Wiebe and Riloff (2005))." ></td>
	<td class="line x" title="15:194	Many applications could benefit from these opinion analyzers, including product reputation tracking (e.g. , Morinaga et al.(2002), Yi et al.(2003)), opinion-oriented summarization (e.g. , Cardie et al.(2004)), and question answering (e.g. , Bethard et al.(2004), Yu and Hatzivassiloglou (2003))." ></td>
	<td class="line x" title="20:194	We focus here on another aspect of opinion analysis: automatically identifying the sources of the opinions." ></td>
	<td class="line x" title="21:194	Identifying opinion sources will be especially critical for opinion-oriented questionanswering systems (e.g. , systems that answer questions of the form How does [X] feel about [Y]?) and opinion-oriented summarization systems, both of which need to distinguish the opinions of one source from those of another.1 The goal of our research is to identify direct and indirect sources of opinions, emotions, sentiments, and other private states that are expressed in text." ></td>
	<td class="line x" title="22:194	To illustrate the nature of this problem, consider the examples below: S1: Taiwan-born voters favoring independence 1In related work, we investigate methods to identify the opinion expressions (e.g. , Riloff and Wiebe (2003), Wiebe and Riloff (2005), Wilson et al.(2005)) and the nesting structure of sources (e.g. , Breck and Cardie (2004))." ></td>
	<td class="line x" title="24:194	The target of each opinion, i.e., what the opinion is directed towards, is currently being annotated manually for our corpus." ></td>
	<td class="line x" title="25:194	355 S2: According to the report, the human rights record in China is horrendous." ></td>
	<td class="line x" title="26:194	S3: International officers believe that the EU will prevail." ></td>
	<td class="line x" title="27:194	S4: International officers said US officials want the EU to prevail." ></td>
	<td class="line x" title="28:194	In S1, the phrase Taiwan-born voters is the direct (i.e. , first-hand) source of the favoring sentiment." ></td>
	<td class="line x" title="29:194	In S2, the report is the direct source of the opinion about Chinas human rights record." ></td>
	<td class="line x" title="30:194	In S3, International officers are the direct source of an opinion regarding the EU." ></td>
	<td class="line x" title="31:194	The same phrase in S4, however, denotes an indirect (i.e. , second-hand, third-hand, etc)." ></td>
	<td class="line x" title="32:194	source of an opinion whose direct source is US officials." ></td>
	<td class="line x" title="33:194	In this paper, we view source identification as an information extraction task and tackle the problem using sequence tagging and pattern matching techniques simultaneously." ></td>
	<td class="line x" title="34:194	Using syntactic, semantic, and orthographic lexical features, dependency parse features, and opinion recognition features, we train a linear-chain Conditional Random Field (CRF) (Lafferty et al. , 2001) to identify opinion sources." ></td>
	<td class="line x" title="35:194	In addition, we employ features based on automatically learned extraction patterns and perform feature induction on the CRF model." ></td>
	<td class="line x" title="36:194	We evaluate our hybrid approach using the NRRC corpus (Wiebe et al. , 2005), which is manually annotated with direct and indirect opinion source information." ></td>
	<td class="line x" title="37:194	Experimental results show that the CRF model performs well, and that both the extraction patterns and feature induction produce performance gains." ></td>
	<td class="line x" title="38:194	The resulting system identifies opinion sources with 79.3% precision and 59.5% recall using a head noun matching measure, and 81.2% precision and 60.6% recall using an overlap measure." ></td>
	<td class="line x" title="39:194	2 The Big Picture The goal of information extraction (IE) systems is to extract information about events, including the participants of the events." ></td>
	<td class="line x" title="40:194	This task goes beyond Named Entity recognition (e.g. , Bikel et al.(1997)) because it requires the recognition of role relationships." ></td>
	<td class="line x" title="42:194	For example, an IE system that extracts information about corporate acquisitions must distinguish between the company that is doing the acquiring and the company that is being acquired." ></td>
	<td class="line x" title="43:194	Similarly, an IE system that extracts information about terrorism must distinguish between the person who is the perpetrator and the person who is the victim." ></td>
	<td class="line x" title="44:194	We hypothesized that IE techniques would be wellsuited for source identification because an opinion statement can be viewed as a kind of speech event with the source as the agent." ></td>
	<td class="line x" title="45:194	We investigate two very different learning-based methods from information extraction for the problem of opinion source identification: graphical models and extraction pattern learning." ></td>
	<td class="line x" title="46:194	In particular, we consider Conditional Random Fields (Lafferty et al. , 2001) and a variation of AutoSlog (Riloff, 1996a)." ></td>
	<td class="line x" title="47:194	CRFs have been used successfully for Named Entity recognition (e.g. , McCallum and Li (2003), Sarawagi and Cohen (2004)), and AutoSlog has performed well on information extraction tasks in several domains (Riloff, 1996a)." ></td>
	<td class="line x" title="48:194	While CRFs treat source identification as a sequence tagging task, AutoSlog views the problem as a pattern-matching task, acquiring symbolic patterns that rely on both the syntax and lexical semantics of a sentence." ></td>
	<td class="line x" title="49:194	We hypothesized that a combination of the two techniques would perform better than either one alone." ></td>
	<td class="line x" title="50:194	Section 3 describes the CRF approach to identifying opinion sources and the features that the system uses." ></td>
	<td class="line x" title="51:194	Section 4 then presents a new variation of AutoSlog, AutoSlog-SE, which generates IE patterns to extract sources." ></td>
	<td class="line x" title="52:194	Section 5 describes the hybrid system: we encode the IE patterns as additional features in the CRF model." ></td>
	<td class="line x" title="53:194	Finally, Section 6 presents our experimental results and error analysis." ></td>
	<td class="line x" title="54:194	3 Semantic Tagging via Conditional Random Fields We defined the problem of opinion source identification as a sequence tagging task via CRFs as follows." ></td>
	<td class="line x" title="55:194	Given a sequence of tokens, x = x1x2xn, we need to generate a sequence of tags, or labels, y = y1y2yn." ></td>
	<td class="line x" title="56:194	We define the set of possible label values as S, T, -, where S is the first token (or Start) of a source, T is a non-initial token (i.e. , a conTinuation) of a source, and -is a token that is not part of any source.2 A detailed description of CRFs can be found in 2This is equivalent to the IOB tagging scheme used in syntactic chunkers (Ramshaw and Marcus, 1995)." ></td>
	<td class="line x" title="57:194	356 Lafferty et al.(2001)." ></td>
	<td class="line x" title="59:194	For our sequence tagging problem, we create a linear-chain CRF based on an undirected graph G = (V,E), where V is the set of random variables Y = fYij1 i ng, one for each of n tokens in an input sentence; and E = f(Yi1,Yi)j1 < i ng is the set of n 1 edges forming a linear chain." ></td>
	<td class="line x" title="60:194	For each sentence x, we define a non-negative clique potential exp(summationtextKk=1 kfk(yi1,yi,x)) for each edge, and exp(summationtextKprimek=1 primekfprimek(yi,x)) for each node, where fk() is a binary feature indicator function, k is a weight assigned for each feature function, and K and Kprime are the number of features defined for edges and nodes respectively." ></td>
	<td class="line x" title="61:194	Following Lafferty et al.(2001), the conditional probability of a sequence of labels y given a sequence of tokens x is: P(y|x) = 1Z x exp X i,k k fk(yi1, yi, x)+ X i,k primek fprimek(yi, x)  (1) Zx = X y exp X i,k k fk(yi1, yi, x) + X i,k primek fprimek(yi, x)  (2) where Zx is a normalization constant for each x. Given the training data D, a set of sentences paired with their correct ST- source label sequences, the parameters of the model are trained to maximize the conditional log-likelihoodproducttext (x,y)D P(yjx)." ></td>
	<td class="line x" title="63:194	For inference, given a sentence x in the test data, the tagging sequence y is given by argmaxyprimeP(yprimejx)." ></td>
	<td class="line x" title="64:194	3.1 Features To develop features, we considered three properties of opinion sources." ></td>
	<td class="line x" title="65:194	First, the sources of opinions are mostly noun phrases." ></td>
	<td class="line x" title="66:194	Second, the source phrases should be semantic entities that can bear or express opinions." ></td>
	<td class="line x" title="67:194	Third, the source phrases should be directly related to an opinion expression." ></td>
	<td class="line x" title="68:194	When considering only the first and second criteria, this task reduces to named entity recognition." ></td>
	<td class="line x" title="69:194	Because of the third condition, however, the task requires the recognition of opinion expressions and a more sophisticated encoding of sentence structure to capture relationships between source phrases and opinion expressions." ></td>
	<td class="line x" title="70:194	With these properties in mind, we define the following features for each token/word xi in an input sentence." ></td>
	<td class="line x" title="71:194	For pedagogical reasons, we will describe some of the features as being multi-valued or categorical features." ></td>
	<td class="line x" title="72:194	In practice, however, all features are binarized for the CRF model." ></td>
	<td class="line x" title="73:194	Capitalization features We use two boolean features to represent the capitalization of a word: all-capital, initial-capital." ></td>
	<td class="line x" title="74:194	Part-of-speech features Based on the lexical categories produced by GATE (Cunningham et al. , 2002), each token xi is classified into one of a set of coarse part-of-speech tags: noun, verb, adverb, wh-word, determiner, punctuation, etc. We do the same for neighboring words in a [ 2, +2] window in order to assist noun phrase segmentation." ></td>
	<td class="line x" title="75:194	Opinion lexicon features For each token xi, we include a binary feature that indicates whether or not the word is in our opinion lexicon  a set of words that indicate the presence of an opinion." ></td>
	<td class="line x" title="76:194	We do the same for neighboring words in a [ 1, +1] window." ></td>
	<td class="line x" title="77:194	Additionally, we include for xi a feature that indicates the opinion subclass associated with xi, if available from the lexicon." ></td>
	<td class="line x" title="78:194	(e.g., bless is classified as moderately subjective according to the lexicon, while accuse and berate are classified more specifically as judgments)." ></td>
	<td class="line x" title="79:194	The lexicon is initially populated with approximately 500 opinion words 3 from (Wiebe et al. , 2002), and then augmented with opinion words identified in the training data." ></td>
	<td class="line x" title="80:194	The training data contains manually produced phrase-level annotations for all expressions of opinions, emotions, etc.(Wiebe et al. , 2005)." ></td>
	<td class="line x" title="82:194	We collected all content words that occurred in the training set such that at least 50% of their occurrences were in opinion annotations." ></td>
	<td class="line x" title="83:194	Dependency tree features For each token xi, we create features based on the parse tree produced by the Collins (1999) dependency parser." ></td>
	<td class="line x" title="84:194	The purpose of the features is to (1) encode structural information, and (2) indicate whether xi is involved in any grammatical relations with an opinion word." ></td>
	<td class="line x" title="85:194	Two pre-processing steps are required before features can be constructed: 3Some words are drawn from Levin (1993); others are from Framenet lemmas (Baker et al. 1998) associated with communication verbs." ></td>
	<td class="line x" title="86:194	357 1." ></td>
	<td class="line x" title="87:194	Syntactic chunking." ></td>
	<td class="line x" title="88:194	We traverse the dependency tree using breadth-first search to identify and group syntactically related nodes, producing a flatter, more concise tree." ></td>
	<td class="line x" title="89:194	Each syntactic chunk is also assigned a grammatical role (e.g. , subject, object, verb modifier, time, location, of-pp, by-pp) based on its constituents." ></td>
	<td class="line x" title="90:194	Possessives (e.g. , Clintons idea) and the phrase according to X are handled as special cases in the chunking process." ></td>
	<td class="line x" title="91:194	2." ></td>
	<td class="line x" title="92:194	Opinion word propagation." ></td>
	<td class="line x" title="93:194	Although the opinion lexicon contains only content words and no multi-word phrases, actual opinions often comprise an entire phrase, e.g., is really willing or in my opinion." ></td>
	<td class="line x" title="94:194	As a result, we mark as an opinion the entire chunk that contains an opinion word." ></td>
	<td class="line x" title="95:194	This allows each token in the chunk to act as an opinion word for feature encoding." ></td>
	<td class="line x" title="96:194	After syntactic chunking and opinion word propagation, we create the following dependency tree features for each token xi: the grammatical role of its chunk the grammatical role of xi1s chunk whether the parent chunk includes an opinion word whether xis chunk is in an argument position with respect to the parent chunk whether xi represents a constituent boundary Semantic class features We use 7 binary features to encode the semantic class of each word xi: authority, government, human, media, organizationor company, proper name, and other." ></td>
	<td class="line x" title="97:194	The other class captures 13 semantic classes that cannot be sources, such as vehicle and time." ></td>
	<td class="line x" title="98:194	Semantic class information is derived from named entity and semantic class labels assigned to xi by the Sundance shallow parser (Riloff, 2004)." ></td>
	<td class="line x" title="99:194	Sundance uses named entity recognition rules to label noun phrases as belonging to named entity classes, and assigns semantic tags to individual words based on a semantic dictionary." ></td>
	<td class="line x" title="100:194	Table 1 shows the hierarchy that Sundance uses for semantic classes associated with opinion sources." ></td>
	<td class="line x" title="101:194	Sundance is also used to recognize and instantiate the source extraction patterns PROPER NAMEAUTHORITY LOCATION CITY COUNTRY PLANET PROVINCE PERSON NAME PERSON DESC NATIONALITY TITLE COMPANY GOVERNMENT MEDIA ORGANIZATION HUMAN SOURCE Figure 1: The semantic hierarchy for opinion sources that are learned by AutoSlog-SE, which is described in the next section." ></td>
	<td class="line x" title="102:194	4 Semantic Tagging via Extraction Patterns We also learn patterns to extract opinion sources using a statistical adaptation of the AutoSlog IE learning algorithm." ></td>
	<td class="line x" title="103:194	AutoSlog (Riloff, 1996a) is a supervised extraction pattern learner that takes a training corpus of texts and their associated answer keys as input." ></td>
	<td class="line x" title="104:194	A set of heuristics looks at the context surrounding each answer and proposes a lexicosyntactic pattern to extract that answer from the text." ></td>
	<td class="line x" title="105:194	The heuristics are not perfect, however, so the resulting set of patterns needs to be manually reviewed by a person." ></td>
	<td class="line x" title="106:194	In order to build a fully automatic system that does not depend on manual review, we combined AutoSlogs heuristics with statistics from the annotated training data to create a fully automatic supervised learner." ></td>
	<td class="line x" title="107:194	We will refer to this learner as AutoSlog-SE (Statistically Enhanced variation of AutoSlog)." ></td>
	<td class="line x" title="108:194	AutoSlog-SEs learning process has three steps: Step 1: AutoSlogs heuristics are applied to every noun phrase (NP) in the training corpus." ></td>
	<td class="line x" title="109:194	This generates a set of extraction patterns that, collectively, can extract every NP in the training corpus." ></td>
	<td class="line x" title="110:194	Step 2: The learned patterns are augmented with selectional restrictions that semantically constrain the types of noun phrases that are legitimate extractions for opinion sources." ></td>
	<td class="line x" title="111:194	We used 358 the semantic classes shown in Figure 1 as selectional restrictions." ></td>
	<td class="line x" title="112:194	Step 3: The patterns are applied to the training corpus and statistics are gathered about their extractions." ></td>
	<td class="line x" title="113:194	We count the number of extractions that match annotations in the corpus (correct extractions) and the number of extractions that do not match annotations (incorrect extractions)." ></td>
	<td class="line x" title="114:194	These counts are then used to estimate the probability that the pattern will extract an opinion source in new texts: P(source | patterni) = correct sourcescorrect sources + incorrect sources This learning process generates a set of extraction patterns coupled with probabilities." ></td>
	<td class="line x" title="115:194	In the next section, we explain how these extraction patterns are represented as features in the CRF model." ></td>
	<td class="line x" title="116:194	5 Extraction Pattern Features for the CRF The extraction patterns provide two kinds of information." ></td>
	<td class="line x" title="117:194	SourcePatt indicates whether a word activates any source extraction pattern." ></td>
	<td class="line x" title="118:194	For example, the word complained activates the pattern <subj> complained because it anchors the expression." ></td>
	<td class="line x" title="119:194	SourceExtrindicates whether a word is extracted by any source pattern." ></td>
	<td class="line x" title="120:194	For example, in the sentence President Jacques Chirac frequently complained about Frances economy, the words President, Jacques, and Chirac would all be extracted by the <subj> complained pattern." ></td>
	<td class="line x" title="121:194	Each extraction pattern has frequency and probability values produced by AutoSlog-SE, hence we create four IE pattern-based features for each token xi: SourcePatt-Freq, SourceExtr-Freq, SourcePatt-Prob, and SourceExtr-Prob, where the frequency values are divided into three ranges: f0, 1, 2+g and the probability values are divided into five ranges of equal size." ></td>
	<td class="line x" title="122:194	6 Experiments We used the Multi-Perspective Question Answering (MPQA) corpus4 for our experiments." ></td>
	<td class="line x" title="123:194	This corpus 4The MPQA corpus can be freely obtained at http://nrrc.mitre.org/NRRC/publications.htm." ></td>
	<td class="line x" title="124:194	consists of 535 documents that have been manually annotated with opinion-related information including direct and indirect sources." ></td>
	<td class="line x" title="125:194	We used 135 documents as a tuning set for model development and feature engineering, and used the remaining 400 documents for evaluation, performing 10-fold cross validation." ></td>
	<td class="line x" title="126:194	These texts are English language versions of articles that come from many countries and cover many topics.5 We evaluate performance using 3 measures: overlap match (OL), head match (HM), and exact match (EM)." ></td>
	<td class="line x" title="127:194	OL is a lenient measure that considers an extraction to be correct if it overlaps with any of the annotated words." ></td>
	<td class="line x" title="128:194	HM is a more conservative measure that considers an extraction to be correct if its head matches the head of the annotated source." ></td>
	<td class="line x" title="129:194	We report these somewhat loose measures because the annotators vary in where they place the exact boundaries of a source." ></td>
	<td class="line x" title="130:194	EM is the strictest measure that requires an exact match between the extracted words and the annotated words." ></td>
	<td class="line x" title="131:194	We use three evaluation metrics: recall, precision, and F-measure with recall and precision equally weighted." ></td>
	<td class="line x" title="132:194	6.1 Baselines We developed three baseline systems to assess the difficulty of our task." ></td>
	<td class="line x" title="133:194	Baseline-1 labels as sources all phrases that belong to the semantic categories authority, government, human, media, organizationor company, proper name." ></td>
	<td class="line x" title="134:194	Table 1 shows that the precision is poor, suggesting that the third condition described in Section 3.1 (opinion recognition) does play an important role in source identification." ></td>
	<td class="line x" title="135:194	The recall is much higher but still limited due to sources that fall outside of the semantic categories or are not recognized as belonging to these categories." ></td>
	<td class="line x" title="136:194	Baseline-2 labels a noun phrase as a source if any of the following are true: (1) the NP is the subject of a verb phrase containing an opinion word, (2) the NP follows according to, (3) the NP contains a possessive and is preceded by an opinion word, or (4) the NP follows by and attaches to an opinion word." ></td>
	<td class="line x" title="137:194	Baseline-2s heuristics are designed to address the first and the third conditions in Section 3.1." ></td>
	<td class="line x" title="138:194	Table 1 shows that Baseline-2 is substantially better than Baseline-1." ></td>
	<td class="line x" title="139:194	Baseline-3 5This data was obtained from the Foreign Broadcast Information Service (FBIS), a U.S. government agency." ></td>
	<td class="line x" title="140:194	359 Recall Prec F1 OL 77.3 28.8 42.0 Baseline-1 HM 71.4 28.6 40.8 EM 65.4 20.9 31.7 OL 62.4 60.5 61.4 Baseline-2 HM 59.7 58.2 58.9 EM 50.8 48.9 49.8 OL 49.9 72.6 59.2 Baseline-3 HM 47.4 72.5 57.3 EM 44.3 58.2 50.3 OL 48.5 81.3 60.8 Extraction Patterns HM 46.9 78.5 58.7 EM 41.9 70.2 52.5 CRF: OL 56.1 81.0 66.3 basic features HM 55.1 79.2 65.0 EM 50.0 72.4 59.2 CRF: OL 59.1 82.4 68.9 basic + IE pattern HM 58.1 80.5 67.5 features EM 52.5 73.3 61.2 CRF-FI: OL 57.7 80.7 67.3 basic features HM 56.8 78.8 66.0 EM 51.7 72.4 60.3 CRF-FI: OL 60.6 81.2 69.4 basic + IE pattern HM 59.5 79.3 68.0 features EM 54.1 72.7 62.0 Table 1: Source identification performance table labels a noun phrase as a source if it satisfies both Baseline-1 and Baseline-2s conditions (this should satisfy all three conditions described in Section 3.1)." ></td>
	<td class="line x" title="141:194	As shown in Table 1, the precision of this approach is the best of the three baselines, but the recall is the lowest." ></td>
	<td class="line x" title="142:194	6.2 Extraction Pattern Experiment We evaluated the performance of the learned extraction patterns on the source identification task." ></td>
	<td class="line x" title="143:194	The learned patterns were applied to the test data and the extracted sources were scored against the manual annotations.6 Table 1 shows that the extraction patterns produced lower recall than the baselines, but with considerably higher precision." ></td>
	<td class="line x" title="144:194	These results show that the extraction patterns alone can identify 6These results were obtained using the patterns that had a probability >.50 and frequency > 1." ></td>
	<td class="line x" title="145:194	nearly half of the opinion sources with good accuracy." ></td>
	<td class="line x" title="146:194	6.3 CRF Experiments We developed our CRF model using the MALLET code from McCallum (2002)." ></td>
	<td class="line x" title="147:194	For training, we used a Gaussian prior of 0.25, selected based on the tuning data." ></td>
	<td class="line x" title="148:194	We evaluate the CRF using the basic features from Section 3, both with and without the IE pattern features from Section 5." ></td>
	<td class="line x" title="149:194	Table 1 shows that the CRF with basic features outperforms all of the baselines as well as the extraction patterns, achieving an F-measure of 66.3 using the OL measure, 65.0 using the HM measure, and 59.2 using the EM measure." ></td>
	<td class="line x" title="150:194	Adding the IE pattern features further increases performance, boosting recall by about 3 points for all of the measures and slightly increasing precision as well." ></td>
	<td class="line x" title="151:194	CRF with feature induction." ></td>
	<td class="line x" title="152:194	One limitation of log-linear function models like CRFs is that they cannot form a decision boundary from conjunctions of existing features, unless conjunctions are explicitly given as part of the feature vector." ></td>
	<td class="line x" title="153:194	For the task of identifying opinion sources, we observed that the model could benefit from conjunctive features." ></td>
	<td class="line x" title="154:194	For instance, instead of using two separate features, HUMAN and PARENT-CHUNK-INCLUDESOPINION-EXPRESSION, the conjunction of the two is more informative." ></td>
	<td class="line x" title="155:194	For this reason, we applied the CRF feature induction approach introduced by McCallum (2003)." ></td>
	<td class="line x" title="156:194	As shown in Table 1, where CRF-FI stands for the CRF model with feature induction, we see consistent improvements by automatically generating conjunctive features." ></td>
	<td class="line x" title="157:194	The final system, which combines the basic features, the IE pattern features, and feature induction achieves an F-measure of 69.4 (recall=60.6%, precision=81.2%) for the OL measure, an F-measure of 68.0 (recall=59.5%, precision=79.3%) for the HM measure, and an F-measure of 62.0 (recall=54.1%, precision=72.7%) for the EM measure." ></td>
	<td class="line x" title="158:194	6.4 Error Analysis An analysis of the errors indicated some common mistakes: Some errors resulted from error propagation in 360 our subsystems." ></td>
	<td class="line x" title="159:194	Errors from the sentence boundary detector in GATE (Cunningham et al. , 2002) were especially problematic because they caused the Collins parser to fail, resulting in no dependency tree information." ></td>
	<td class="line x" title="160:194	Some errors were due to complex and unusual sentence structure, which our rather simple feature encoding for CRF could not capture well." ></td>
	<td class="line x" title="161:194	Some errors were due to the limited coverage of the opinion lexicon." ></td>
	<td class="line x" title="162:194	We failed to recognize some cases when idiomatic or vague expressions were used to express opinions." ></td>
	<td class="line x" title="163:194	Below are some examples of errors that we found interesting." ></td>
	<td class="line x" title="164:194	Doubly underlined phrases indicate incorrectly extracted sources (either false positives or false negatives)." ></td>
	<td class="line x" title="165:194	Opinion words are singly underlined." ></td>
	<td class="line x" title="166:194	False positives: (1) Actually, these three countries do have one common denominator, i.e., that their values and policies do not agree with those of the United States and none of them are on good terms with the United States." ></td>
	<td class="line x" title="167:194	(2) Perhaps this is why Fidel Castro has not spoken out against what might go on in Guantanamo." ></td>
	<td class="line x" title="168:194	In (1), their values and policies seems like a reasonable phrase to extract, but the annotation does not mark this as a source, perhaps because it is somewhat abstract." ></td>
	<td class="line x" title="169:194	In (2), spoken out is negated, which means that the verb phrase does not bear an opinion, but our system failed to recognize the negation." ></td>
	<td class="line x" title="170:194	False negatives: (3) And for this reason, too, they have a moral duty to speak out, as Swedish Foreign Minister Anna Lindh, among others, did yesterday." ></td>
	<td class="line x" title="171:194	(4) In particular, Iran and Iraq are at loggerheads with each other to this day." ></td>
	<td class="line x" title="172:194	Example (3) involves a complex sentence structure that our system could not deal with." ></td>
	<td class="line x" title="173:194	(4) involves an uncommon opinion expression that our system did not recognize." ></td>
	<td class="line x" title="174:194	7 Related Work To our knowledge, our research is the first to automatically identify opinion sources using the MPQA opinion annotation scheme." ></td>
	<td class="line x" title="175:194	The most closely related work on opinion analysis is Bethard et al.(2004), who use machine learning techniques to identify propositional opinions and their holders (sources)." ></td>
	<td class="line x" title="177:194	However, their work is more limited in scope than ours in several ways." ></td>
	<td class="line x" title="178:194	Their work only addresses propositional opinions, which are localized in the propositional argument of certain verbs such as believe or realize." ></td>
	<td class="line x" title="179:194	In contrast, our work aims to find sources for all opinions, emotions, and sentiments, including those that are not related to a verb at all." ></td>
	<td class="line x" title="180:194	Furthermore, Berthard et al.s task definition only requires the identification of direct sources, while our task requires the identification of both direct and indirect sources." ></td>
	<td class="line x" title="181:194	Bethard et al. evaluate their system on manually annotated FrameNet (Baker et al. , 1998) and PropBank (Palmer et al. , 2005) sentences and achieve 48% recall with 57% precision." ></td>
	<td class="line x" title="182:194	Our IE pattern learner can be viewed as a cross between AutoSlog (Riloff, 1996a) and AutoSlogTS (Riloff, 1996b)." ></td>
	<td class="line x" title="183:194	AutoSlog is a supervised learner that requires annotated training data but does not compute statistics." ></td>
	<td class="line x" title="184:194	AutoSlog-TS is a weakly supervised learner that does not require annotated data but generates coarse statistics that measure each patterns correlation with relevant and irrelevant documents." ></td>
	<td class="line x" title="185:194	Consequently, the patterns learned by both AutoSlog and AutoSlog-TS need to be manually reviewed by a person to achieve good accuracy." ></td>
	<td class="line x" title="186:194	In contrast, our IE learner, AutoSlog-SE, computes statistics directly from the annotated training data, creating a fully automatic variation of AutoSlog." ></td>
	<td class="line x" title="187:194	8 Conclusion We have described a hybrid approach to the problem of extracting sources of opinions in text." ></td>
	<td class="line x" title="188:194	We cast this problem as an information extraction task, using both CRFs and extraction patterns." ></td>
	<td class="line x" title="189:194	Our research is the first to identify both direct and indirect sources for all types of opinions, emotions, and sentiments." ></td>
	<td class="line x" title="190:194	Directions for future work include trying to increase recall by identifying relationships between opinions and sources that cross sentence boundaries, and relationships between multiple opinion expressions by the same source." ></td>
	<td class="line x" title="191:194	For example, the fact that a coreferring noun phrase was marked as a source in one sentence could be a useful clue for extracting the source from another sentence." ></td>
	<td class="line x" title="192:194	The probability or the strength of an opinion expression may also play a useful role in encouraging or suppressing source extraction." ></td>
	<td class="line x" title="193:194	361 9 Acknowledgments We thank the reviewers for their many helpful comments, and the Cornell NLP group for their advice and suggestions for improvement." ></td>
	<td class="line x" title="194:194	This work was supported by the Advanced Research and Development Activity (ARDA), by NSF Grants IIS-0208028 and IIS-0208985, and by the Xerox Foundation." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="H05-1073
Emotions From Text: Machine Learning For Text-Based Emotion Prediction
Alm, Cecilia Ovesdotter;Roth, Dan;Sproat, Richard W.;"></td>
	<td class="line x" title="1:192	Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 579586, Vancouver, October 2005." ></td>
	<td class="line x" title="2:192	c2005 Association for Computational Linguistics Emotions from text: machine learning for text-based emotion prediction Cecilia Ovesdotter Alm Dept. of Linguistics UIUC Illinois, USA ebbaalm@uiuc.edu Dan Roth Dept. of Computer Science UIUC Illinois, USA danr@uiuc.edu Richard Sproat Dept. of Linguistics Dept. of Electrical Eng." ></td>
	<td class="line x" title="3:192	UIUC Illinois, USA rws@uiuc.edu Abstract In addition to information, text contains attitudinal, and more specifically, emotional content." ></td>
	<td class="line x" title="4:192	This paper explores the text-based emotion prediction problem empirically, using supervised machine learning with the SNoW learning architecture." ></td>
	<td class="line x" title="5:192	The goal is to classify the emotional affinity of sentences in the narrative domain of childrens fairy tales, for subsequent usage in appropriate expressive rendering of text-to-speech synthesis." ></td>
	<td class="line x" title="6:192	Initial experiments on a preliminary data set of 22 fairy tales show encouraging results over a nave baseline and BOW approach for classification of emotional versus non-emotional contents, with some dependency on parameter tuning." ></td>
	<td class="line x" title="7:192	We also discuss results for a tripartite model which covers emotional valence, as well as feature set alternations." ></td>
	<td class="line x" title="8:192	In addition, we present plans for a more cognitively sound sequential model, taking into consideration a larger set of basic emotions." ></td>
	<td class="line x" title="9:192	1 Introduction Text does not only communicate informative contents, but also attitudinal information, including emotional states." ></td>
	<td class="line x" title="10:192	The following reports on an empirical study of text-based emotion prediction." ></td>
	<td class="line x" title="11:192	Section 2 gives a brief overview of the intended application area, whereas section 3 summarizes related work." ></td>
	<td class="line x" title="12:192	Next, section 4 explains the empirical study, including the machine learning model, the corpus, the feature set, parameter tuning, etc. Section 5 presents experimental results from two classification tasks and feature set modifications." ></td>
	<td class="line x" title="13:192	Section 6 describes the agenda for refining the model, before presenting concluding remarks in 7." ></td>
	<td class="line x" title="14:192	2 Application area: Text-to-speech Narrative text is often especially prone to having emotional contents." ></td>
	<td class="line x" title="15:192	In the literary genre of fairy tales, emotions such as HAPPINESS and ANGER and related cognitive states, e.g. LOVE or HATE, become integral parts of the story plot, and thus are of particular importance." ></td>
	<td class="line x" title="16:192	Moreover, the story teller reading the story interprets emotions in order to orally convey the story in a fashion which makes the story come alive and catches the listeners attention." ></td>
	<td class="line x" title="17:192	In speech, speakers effectively express emotions by modifying prosody, including pitch, intensity, and durational cues in the speech signal." ></td>
	<td class="line x" title="18:192	Thus, in order to make text-to-speech synthesis sound as natural and engaging as possible, it is important to convey the emotional stance in the text." ></td>
	<td class="line x" title="19:192	However, this implies first having identified the appropriate emotional meaning of the corresponding text passage." ></td>
	<td class="line x" title="20:192	Thus, an application for emotional text-to-speech synthesis has to solve two basic problems." ></td>
	<td class="line x" title="21:192	First, what emotion or emotions most appropriately describe a certain text passage, and second, given a text passage and a specified emotional mark-up, how to render the prosodic contour in order to convey the emotional content, (Cahn, 1990)." ></td>
	<td class="line x" title="22:192	The text-based emotion prediction task (TEP) addresses the first of these two problems." ></td>
	<td class="line x" title="23:192	579 3 Previous work For a complete general overview of the field of affective computing, see (Picard, 1997)." ></td>
	<td class="line x" title="24:192	(Liu, Lieberman and Selker, 2003) is a rare study in textbased inference of sentence-level emotional affinity." ></td>
	<td class="line x" title="25:192	The authors adopt the notion of basic emotions, cf.(Ekman, 1993), and use six emotion categories: ANGER, DISGUST, FEAR, HAPPINESS, SADNESS, SURPRISE." ></td>
	<td class="line x" title="27:192	They critique statistical NLP for being unsuccessful at the small sentence level, and instead use a database of common-sense knowledge and create affect models which are combined to form a representation of the emotional affinity of a sentence." ></td>
	<td class="line x" title="28:192	At its core, the approach remains dependent on an emotion lexicon and hand-crafted rules for conceptual polarity." ></td>
	<td class="line x" title="29:192	In order to be effective, emotion recognition must go beyond such resources; the authors note themselves that lexical affinity is fragile." ></td>
	<td class="line x" title="30:192	The method was tested on 20 users preferences for an email-client, based on user-composed text emails describing short but colorful events." ></td>
	<td class="line x" title="31:192	While the users preferred the emotional client, this evaluation does not reveal emotion classification accuracy, nor how well the model generalizes on a large data set." ></td>
	<td class="line x" title="32:192	Whereas work on emotion classification from the point of view of natural speech and humancomputer dialogues is fairly extensive, e.g.(Scherer, 2003), (Litman and Forbes-Riley, 2004), this appears not to be the case for text-to-speech synthesis (TTS)." ></td>
	<td class="line x" title="34:192	A short study by (Sugimoto et al. , 2004) addresses sentence-level emotion recognition for Japanese TTS." ></td>
	<td class="line x" title="35:192	Their model uses a composition assumption: the emotion of a sentence is a function of the emotional affinity of the words in the sentence." ></td>
	<td class="line x" title="36:192	They obtain emotional judgements of 73 adjectives and a set of sentences from 15 human subjects and compute words emotional strength based on the ratio of times a word or a sentence was judged to fall into a particular emotion bucket, given the number of human subjects." ></td>
	<td class="line x" title="37:192	Additionally, they conducted an interactive experiment concerning the acoustic rendering of emotion, using manual tuning of prosodic parameters for Japanese sentences." ></td>
	<td class="line x" title="38:192	While the authors actually address the two fundamental problems of emotional TTS, their approach is impractical and most likely cannot scale up for a real corpus." ></td>
	<td class="line x" title="39:192	Again, while lexical items with clear emotional meaning, such as happy or sad, matter, emotion classification probably needs to consider additional inference mechanisms." ></td>
	<td class="line x" title="40:192	Moreover, a nave compositional approach to emotion recognition is risky due to simple linguistic facts, such as context-dependent semantics, domination of words with multiple meanings, and emotional negation." ></td>
	<td class="line o" title="41:192	Many NLP problems address attitudinal meaning distinctions in text, e.g. detecting subjective opinion documents or expressions, e.g.(Wiebe et al, 2004), measuring strength of subjective clauses (Wilson, Wiebe and Hwa, 2004), determining word polarity (Hatzivassiloglou and McKeown, 1997) or texts attitudinal valence, e.g.(Turney, 2002), (Bai, Padman and Airoldi, 2004), (Beineke, Hastie and Vaithyanathan, 2003), (Mullen and Collier, 2003), (Pang and Lee, 2003)." ></td>
	<td class="line x" title="44:192	Here, it suffices to say that the targets, the domain, and the intended application differ; our goal is to classify emotional text passages in childrens stories, and eventually use this information for rendering expressive child-directed storytelling in a text-to-speech application." ></td>
	<td class="line x" title="45:192	This can be useful, e.g. in therapeutic education of children with communication disorders (van Santen et al. , 2003)." ></td>
	<td class="line x" title="46:192	4 Empirical study This part covers the experimental study with a formal problem definition, computational implementation, data, features, and a note on parameter tuning." ></td>
	<td class="line x" title="47:192	4.1 Machine learning model Determining emotion of a linguistic unit can be cast as a multi-class classification problem." ></td>
	<td class="line x" title="48:192	For the flat case, let T denote the text, and s an embedded linguistic unit, such as a sentence, where s  T. Let k be the number of emotion classes E = {em1,em2, ,emk}, where em1 denotes the special case of neutrality, or absence of emotion." ></td>
	<td class="line x" title="49:192	The goal is to determine a mapping function f : s  emi, such that we obtain an ordered labeled pair (s,emi)." ></td>
	<td class="line x" title="50:192	The mapping is based on F = {f1,f2, ,fn}, where F contains the features derived from the text." ></td>
	<td class="line x" title="51:192	Furthermore, if multiple emotion classes can characterize s, then given E  E, the target of the mapping function becomes the ordered pair (s,Eprime)." ></td>
	<td class="line x" title="52:192	Finally, as further discussed in section 6, the hierarchical case of label assignment requires a sequen580 tial model that further defines levels of coarse versus fine-grained classifiers, as done by (Li and Roth, 2002) for the question classification problem." ></td>
	<td class="line x" title="53:192	4.2 Implementation Whereas our goal is to predict finer emotional meaning distinctions according to emotional categories in speech; in this study, we focus on the basic task of recognizing emotional passages and on determining their valence (i.e. positive versus negative) because we currently do not have enough training data to explore finer-grained distinctions." ></td>
	<td class="line x" title="54:192	The goal here is to get a good understanding of the nature of the TEP problem and explore features which may be useful." ></td>
	<td class="line x" title="55:192	We explore two cases of flat classification, using a variation of the Winnow update rule implemented in the SNoW learning architecture (Carlson et al. , 1999),1 which learns a linear classifier in feature space, and has been successful in several NLP applications, e.g. semantic role labeling (Koomen, Punyakanok, Roth and Yih, 2005)." ></td>
	<td class="line x" title="56:192	In the first case, the set of emotion classes E consists of EMOTIONAL versus non-emotional or NEUTRAL, i.e. E = {N,E}." ></td>
	<td class="line x" title="57:192	In the second case, E has been incremented with emotional distinctions according to the valence, i.e. E = {N,PE,NE}." ></td>
	<td class="line x" title="58:192	Experiments used 10-fold cross-validation, with 90% train and 10% test data.2 4.3 Data The goal of our current data annotation project is to annotate a corpus of approximately 185 children stories, including Grimms, H.C. Andersens and B. Potters stories." ></td>
	<td class="line x" title="59:192	So far, the annotation process proceeds as follows: annotators work in pairs on the same stories." ></td>
	<td class="line x" title="60:192	They have been trained separately and work independently in order to avoid any annotation bias and get a true understanding of the task difficulty." ></td>
	<td class="line x" title="61:192	Each annotator marks the sentence level with one of eight primary emotions, see table 1, reflecting an extended set of basic emotions (Ekman, 1993)." ></td>
	<td class="line x" title="62:192	In order to make the annotation process more focused, emotion is annotated from the point of view of the text, i.e. the feeler in the sentence." ></td>
	<td class="line x" title="63:192	While the primary emotions are targets, the sentences are also 1Available from http://l2r.cs.uiuc.edu/cogcomp/ 2Experiments were also run for Perceptron, however the results are not included." ></td>
	<td class="line x" title="64:192	Overall, Perceptron performed worse." ></td>
	<td class="line x" title="65:192	marked for other affective contents, i.e. background mood, secondary emotions via intensity, feeler, and textual cues." ></td>
	<td class="line x" title="66:192	Disagreements in annotations are resolved by a second pass of tie-breaking by the first author, who chooses one of the competing labels." ></td>
	<td class="line x" title="67:192	Eventually, the completed annotations will be made available." ></td>
	<td class="line x" title="68:192	Table 1: Basic emotions used in annotation Abbreviation Emotion class A ANGRY D DISGUSTED F FEARFUL H HAPPY Sa SAD Su+ POSITIVELY SURPRISED SuNEGATIVELY SURPRISED Emotion annotation is hard; interannotator agreement currently range at  =.24.51, with the ratio of observed annotation overlap ranging between 45-64%, depending on annotator pair and stories assigned." ></td>
	<td class="line x" title="69:192	This is expected, given the subjective nature of the annotation task." ></td>
	<td class="line x" title="70:192	The lack of a clear definition for emotion vs. non-emotion is acknowledged across the emotion literature, and contributes to dynamic and shifting annotation targets." ></td>
	<td class="line x" title="71:192	Indeed, a common source of confusion is NEUTRAL, i.e. deciding whether or not a sentence is emotional or non-emotional." ></td>
	<td class="line x" title="72:192	Emotion perception also depends on which characters point-of-view the annotator takes, and on extratextual factors such as annotators personality or mood." ></td>
	<td class="line x" title="73:192	It is possible that by focusing more on the training of annotator pairs, particularly on joint training, agreement might improve." ></td>
	<td class="line x" title="74:192	However, that would also result in a bias, which is probably not preferable to actual perception." ></td>
	<td class="line x" title="75:192	Moreover, what agreement levels are needed for successful expressive TTS remains an empirical question." ></td>
	<td class="line x" title="76:192	The current data set consisted of a preliminary annotated and tie-broken data set of 1580 sentence, or 22 Grimms tales." ></td>
	<td class="line x" title="77:192	The label distribution is in table 2." ></td>
	<td class="line x" title="78:192	NEUTRAL was most frequent with 59.94%." ></td>
	<td class="line x" title="79:192	Table 2: Percent of annotated labels A D F H 12.34% 0.89% 7.03% 6.77% N SA SU+ SU.59.94% 7.34% 2.59% 3.10% 581 Table 3: % EMOTIONAL vs. NEUTRAL examples E N 40.06% 59.94% Table 4: % POSITIVE vs. NEGATIVE vs. NEUTRAL PE NE N 9.87% 30.19% 59.94% Next, for the purpose of this study, all emotional classes, i.e. A, D, F, H, SA, SU+, SU-, were combined into one emotional superclass E for the first experiment, as shown in table 3." ></td>
	<td class="line x" title="80:192	For the second experiment, we used two emotional classes, i.e. positive versus negative emotions; PE={H, SU+} and NE={A, D, F, SA, SU-}, as seen in table 4." ></td>
	<td class="line x" title="81:192	4.4 Feature set The feature extraction was written in python." ></td>
	<td class="line x" title="82:192	SNoW only requires active features as input, which resulted in a typical feature vector size of around 30 features." ></td>
	<td class="line x" title="83:192	The features are listed below." ></td>
	<td class="line x" title="84:192	They were implemented as boolean values, with continuous values represented by ranges." ></td>
	<td class="line x" title="85:192	The ranges generally overlapped, in order to get more generalization coverage." ></td>
	<td class="line x" title="86:192	1." ></td>
	<td class="line x" title="87:192	First sentence in story 2." ></td>
	<td class="line x" title="88:192	Conjunctions of selected features (see below) 3." ></td>
	<td class="line x" title="89:192	Direct speech (i.e. whole quote) in sentence 4." ></td>
	<td class="line x" title="90:192	Thematic story type (3 top and 15 sub-types) 5." ></td>
	<td class="line x" title="91:192	Special punctuation (!" ></td>
	<td class="line x" title="92:192	and ?) 6." ></td>
	<td class="line x" title="93:192	Complete upper-case word 7." ></td>
	<td class="line x" title="94:192	Sentence length in words (0-1, 2-3, 4-8, 9-15, 16-25, 26-35, >35) 8." ></td>
	<td class="line x" title="95:192	Ranges of story progress (5-100%, 15-100%, 80-100%, 90-100%) 9." ></td>
	<td class="line x" title="96:192	Percent of JJ, N, V, RB (0%, 1-100%, 50100%, 80-100%) 10." ></td>
	<td class="line x" title="97:192	V count in sentence, excluding participles (0-1, 0-3, 0-5, 0-7, 0-9, > 9) 11." ></td>
	<td class="line x" title="98:192	Positive and negative word counts (  1,  2,  3,  4,  5,  6) 12." ></td>
	<td class="line x" title="99:192	WordNet emotion words 13." ></td>
	<td class="line x" title="100:192	Interjections and affective words 14." ></td>
	<td class="line x" title="101:192	Content BOW: N, V, JJ, RB words by POS Feature conjunctions covered pairings of counts of positive and negative words with range of story progress or interjections, respectively." ></td>
	<td class="line x" title="102:192	Feature groups 1, 3, 5, 6, 7, 8, 9, 10 and 14 are extracted automatically from the sentences in the stories; with the SNoW POS-tagger used for features 9, 10, and 14." ></td>
	<td class="line x" title="103:192	Group 10 reflects how many verbs are active in a sentence." ></td>
	<td class="line x" title="104:192	Together with the quotation and punctuation, verb domination intends to capture the assumption that emotion is often accompanied by increased action and interaction." ></td>
	<td class="line x" title="105:192	Feature group 4 is based on Finish scholar Antti Aarnes classes of folk-tale types according to their informative thematic contents (Aarne, 1964)." ></td>
	<td class="line x" title="106:192	The current tales have 3 top story types (ANIMAL TALES, ORDINARY FOLK-TALES, and JOKES AND ANECDOTES), and 15 subtypes (e.g. supernatural helpers is a subtype of the ORDINARY FOLK-TALE)." ></td>
	<td class="line x" title="107:192	This feature intends to provide an idea about the storys general affective personality (Picard, 1997), whereas the feature reflecting the story progress is hoped to capture that some emotions may be more prevalent in certain sections of the story (e.g. the happy end)." ></td>
	<td class="line x" title="108:192	For semantic tasks, words are obviously important." ></td>
	<td class="line x" title="109:192	In addition to considering content words, we also explored specific word lists." ></td>
	<td class="line x" title="110:192	Group 11 uses 2 lists of 1636 positive and 2008 negative words, obtained from (Di Cicco et al. , online)." ></td>
	<td class="line x" title="111:192	Group 12 uses lexical lists extracted from WordNet (Fellbaum, 1998), on the basis of the primary emotion words in their adjectival and nominal forms." ></td>
	<td class="line x" title="112:192	For the adjectives, Py-WordNets (Steele et al. , 2004) SIMILAR feature was used to retrieve similar items of the primary emotion adjectives, exploring one additional level in the hierarchy (i.e. similar items of all senses of all words in the synset)." ></td>
	<td class="line x" title="113:192	For the nouns and any identical verbal homonyms, synonyms and hyponyms were extracted manually.3 Feature group 13 used a short list of 22 interjections collected manually by browsing educational ESL sites, whereas the affective word list of 771 words consisted of a combination of the non-neutral words from (JohnsonLaird and Oatley, 1989) and (Siegle, online)." ></td>
	<td class="line x" title="114:192	Only a subset of these lexical lists actually occurred.4 3Multi-words were transformed to hyphenated form." ></td>
	<td class="line x" title="115:192	4At this point, neither stems and bigrams nor a list of onomatopoeic words contribute to accuracy." ></td>
	<td class="line x" title="116:192	Intermediate resource processing inserted some feature noise." ></td>
	<td class="line x" title="117:192	582 The above feature set is henceforth referred to as all features, whereas content BOW is just group 14." ></td>
	<td class="line x" title="118:192	The content BOW is a more interesting baseline than the nave one, P(Neutral), i.e. always assigning the most likely NEUTRAL category." ></td>
	<td class="line x" title="119:192	Lastly, emotions blend and transform (Liu, Lieberman and Selker, 2003)." ></td>
	<td class="line x" title="120:192	Thus, emotion and background mood of immediately adjacent sentences, i.e. the sequencing, seems important." ></td>
	<td class="line x" title="121:192	At this point, it is not implemented automatically." ></td>
	<td class="line x" title="122:192	Instead, it was extracted from the manual emotion and mood annotations." ></td>
	<td class="line x" title="123:192	If sequencing seemed important, an automatic method using sequential target activation could be added next." ></td>
	<td class="line x" title="124:192	4.5 Parameter tuning The Winnow parameters that were tuned included promotional , demotional , activation threshold , initial weights , and the regularization parameter, S, which implements a margin between positive and negative examples." ></td>
	<td class="line x" title="125:192	Given the currently fairly limited data, results from 2 alternative tuning methods, applied to all features, are reported." ></td>
	<td class="line x" title="126:192	 For the condition called sep-tune-eval, 50% of the sentences were randomly selected and set aside to be used for the parameter tuning process only." ></td>
	<td class="line x" title="127:192	Of this subset, 10% were subsequently randomly chosen as test set with the remaining 90% used for training during the automatic tuning process, which covered 4356 different parameter combinations." ></td>
	<td class="line x" title="128:192	Resulting parameters were:  = 1.1,  = 0.5,  = 5,  = 1.0, S = 0.5." ></td>
	<td class="line x" title="129:192	The remaining half of the data was used for training and testing in the 10-fold cross-validation evaluation." ></td>
	<td class="line x" title="130:192	(Also, note the slight change for P(Neutral) in table 5, due to randomly splitting the data.)" ></td>
	<td class="line x" title="131:192	 Given that the data set is currently small, for the condition named same-tune-eval, tuning was performed automatically on all data using a slightly smaller set of combinations, and then manually adjusted against the 10-fold crossvalidation process." ></td>
	<td class="line x" title="132:192	Resulting parameters were:  = 1.2,  = 0.9,  = 4,  = 1, S = 0.5." ></td>
	<td class="line x" title="133:192	All data was used for evaluation." ></td>
	<td class="line x" title="134:192	Emotion classification was sensitive to the selected tuning data." ></td>
	<td class="line x" title="135:192	Generally, a smaller tuning set resulted in pejorative parameter settings." ></td>
	<td class="line x" title="136:192	The random selection could make a difference, but was not explored." ></td>
	<td class="line x" title="137:192	5 Results and discussion This section first presents the results from experiments with the two different confusion sets described above, as well as feature experimentation." ></td>
	<td class="line x" title="138:192	5.1 Classification results Average accuracy from 10-fold cross validation for the first experiment, i.e. classifying sentences as either NEUTRAL or EMOTIONAL, are included in table 5 and figure 1 for the two tuning conditions on the main feature sets and baselines." ></td>
	<td class="line x" title="139:192	As expected, Table 5: Mean classification accuracy: N vs. E, 2 conditions same-tune-eval sep-tune-eval P(Neutral) 59.94 60.05 Content BOW 61.01 58.30 All features except BOW 64.68 63.45 All features 68.99 63.31 All features + sequencing 69.37 62.94 degree of success reflects parameter settings, both for content BOW and all features." ></td>
	<td class="line x" title="140:192	Nevertheless, under these circumstances, performance above a nave baseline and a BOW approach is obtained." ></td>
	<td class="line x" title="141:192	Moreover, sequencing shows potential for contributing in one case." ></td>
	<td class="line x" title="142:192	However, observations also point to three issues: first, the current data set appears to be too small." ></td>
	<td class="line x" title="143:192	Second, the data is not easily separable." ></td>
	<td class="line x" title="144:192	This comes as no surprise, given the subjective nature of the task, and the rather low interannotator agreement, reported above." ></td>
	<td class="line x" title="145:192	Moreover, despite the schematic narrative plots of childrens stories, tales still differ in their overall affective orientation, which increases data complexity." ></td>
	<td class="line x" title="146:192	Third and finally, the EMOTION class is combined by basic emotion labels, rather than an original annotated label." ></td>
	<td class="line x" title="147:192	More detailed averaged results from 10-fold cross-validation are included in table 6 using all features and the separated tuning and evaluation data condition sep-tune-eval." ></td>
	<td class="line x" title="148:192	With these parameters, approximately 3% improvement in accuracy over the nave baseline P(Neutral) was recorded, and 5% over the content BOW, which obviously did poorly with these parameters." ></td>
	<td class="line x" title="149:192	Moreover, precision is 583 0 10 20 30 40 50 60 70 same-tune-eval sep-tune-eval Tuning sets % Accuracy P(Neutral) Content BOWAll features except BOWAll features All features + sequencing Figure 1: Accuracy under different conditions (in %) Table 6: Classifying N vs. E (all features, sep-tune-eval) Measure N E Averaged accuracy 0.63 0.63 Averaged error 0.37 0.37 Averaged precision 0.66 0.56 Averaged recall 0.75 0.42 Averaged F-score 0.70 0.47 higher than recall for the combined EMOTION class." ></td>
	<td class="line x" title="150:192	In comparison, with the same-tune-eval procedure, the accuracy improved by approximately 9% over P(Neutral) and by 8% over content BOW." ></td>
	<td class="line x" title="151:192	In the second experiment, the emotion category was split into two classes: emotions with positive versus negative valence." ></td>
	<td class="line x" title="152:192	The results in terms of precision, recall, and F-score are included in table 7, using all features and the sep-tune-eval condition." ></td>
	<td class="line x" title="153:192	The decrease in performance for the emotion classes mirrors the smaller amounts of data available for each class." ></td>
	<td class="line x" title="154:192	As noted in section 4.3, only 9.87% of the sentences were annotated with a positive emotion, and the results for this class are worse." ></td>
	<td class="line x" title="155:192	Thus, performance seems likely to improve as more annotated story data becomes available; at this point, we are experimenting with merely around 12% of the total texts targeted by the data annotation project." ></td>
	<td class="line x" title="156:192	5.2 Feature experiments Emotions are poorly understood, and it is especially unclear which features may be important for their recognition from text." ></td>
	<td class="line x" title="157:192	Thus, we experimented Table 7: N, PE, and NE (all features, sep-tune-eval) N NE PE Averaged precision 0.64 0.45 0.13 Averaged recall 0.75 0.27 0.19 Averaged F-score 0.69 0.32 0.13 Table 8: Feature group members Word lists interj., WordNet, affective lists, pos/neg Syntactic length ranges, % POS, V-count ranges Story-related % story-progress, 1st sent., story type Orthographic punctuation, upper-case words, quote Conjunctions Conjunctions with pos/neg Content BOW Words (N,V,Adj, Adv) with different feature configurations." ></td>
	<td class="line x" title="160:192	Starting with all features, again using 10-fold cross-validation for the separated tuning-evaluation condition sep-tuneeval, one additional feature group was removed until none remained." ></td>
	<td class="line x" title="161:192	The feature groups are listed in table 8." ></td>
	<td class="line x" title="162:192	Figure 2 on the next page shows the accuracy at each step of the cumulative subtraction process." ></td>
	<td class="line x" title="163:192	While some feature groups, e.g. syntactic, appeared less important, the removal order mattered; e.g. if syntactic features were removed first, accuracy decreased." ></td>
	<td class="line x" title="164:192	This fact also illustrated that features work together; removing any group degraded performance because features interact and there is no true independence." ></td>
	<td class="line x" title="165:192	It was observed that features contributions were sensitive to parameter tuning." ></td>
	<td class="line x" title="166:192	Clearly, further work on developing features which fit the TEP problem is needed." ></td>
	<td class="line x" title="167:192	6 Refining the model This was a first pass of addressing TEP for TTS." ></td>
	<td class="line x" title="168:192	At this point, the annotation project is still on-going, and we only had a fairly small data set to draw on." ></td>
	<td class="line x" title="169:192	Nevertheless, results indicate that our learning approach benefits emotion recognition." ></td>
	<td class="line x" title="170:192	For example, the following instances, also labeled with the same valence by both annotators, were correctly classified both in the binary (N vs. E) and the tripartite polarity task (N, NE, PE), given the separated tuning and evaluation data condition, and using all features: (1a) E/NE: Then he offered the dwarfs money, and prayed and besought them to let him take her away; but they said, We will not part with her for all the gold in the world. 584 Cumulative removal of feature groups 61.81 63.31 62.57 57.95 58.3058.93 59.56 55 60 65 Allfeature s -Wordlis ts -Syntacti c -Story-rel ated -Orthogra phic -Conjunc tions -Content words % Ac curac y All featuresP(Neutral)BOW Figure 2: Averaged effect of feature group removal, using sep-tune-eval (1b) N: And so the little girl really did grow up; her skin was as white as snow, her cheeks as rosy as the blood, and her hair as black as ebony; and she was called Snowdrop." ></td>
	<td class="line x" title="171:192	(2a) E/NE: Ah, she answered, have I not reason to weep?" ></td>
	<td class="line x" title="172:192	(2b) N: Nevertheless, he wished to try him first, and took a stone in his hand and squeezed it together so that water dropped out of it." ></td>
	<td class="line x" title="173:192	Cases (1a) and (1b) are from the well-known FOLK TALE Snowdrop, also called Snow White." ></td>
	<td class="line x" title="174:192	(1a) and (1b) are also correctly classified by the simple content BOW approach, although our approach has higher prediction confidence for E/NE (1a); it also considers, e.g. direct speech, a fairly high verb count, advanced story progress, connotative words and conjunctions thereof with story progress features, all of which the BOW misses." ></td>
	<td class="line x" title="175:192	In addition, the simple content BOW approach makes incorrect predictions at both the bipartite and tripartite levels for examples (2a) and (2b) from the JOKES AND ANECDOTES stories Clever Hans and The Valiant Little Tailor, while our classifier captures the affective differences by considering, e.g. distinctions in verb count, interjection, POS, sentence length, connotations, story subtype, and conjunctions." ></td>
	<td class="line x" title="176:192	Next, we intend to use a larger data set to conduct a more complete study to establish mature findings." ></td>
	<td class="line x" title="177:192	We also plan to explore finer emotional meaning distinctions, by using a hierarchical sequential model which better corresponds to different levels of cognitive difficulty in emotional categorization by humans, and to classify the full set of basic level emotional categories discussed in section 4.3." ></td>
	<td class="line x" title="178:192	Sequential modeling of simple classifiers has been successfully employed to question classification, for example by (Li and Roth, 2002)." ></td>
	<td class="line x" title="179:192	In addition, we are working on refining and improving the feature set, and given more data, tuning can be improved on a sufficiently large development set." ></td>
	<td class="line x" title="180:192	The three subcorpora in the annotation project can reveal how authorship affects emotion perception and classification." ></td>
	<td class="line x" title="181:192	Moreover, arousal appears to be an important dimension for emotional prosody (Scherer, 2003), especially in storytelling (Alm and Sproat, 2005)." ></td>
	<td class="line x" title="182:192	Thus, we are planning on exploring degrees of emotional intensity in a learning scenario, i.e. a problem similar to measuring strength of opinion clauses (Wilson, Wiebe and Hwa, 2004)." ></td>
	<td class="line x" title="183:192	Finally, emotions are not discrete objects; rather they have transitional nature, and blend and overlap along the temporal dimension." ></td>
	<td class="line x" title="184:192	For example, (Liu, Lieberman and Selker, 2003) include parallel estimations of emotional activity, and include smooth585 ing techniques such as interpolation and decay to capture sequential and interactive emotional activity." ></td>
	<td class="line x" title="185:192	Observations from tales indicate that some emotions are more likely to be prolonged than others." ></td>
	<td class="line x" title="186:192	7 Conclusion This paper has discussed an empirical study of the text-based emotion prediction problem in the domain of childrens fairy tales, with child-directed expressive text-to-speech synthesis as goal." ></td>
	<td class="line x" title="187:192	Besides reporting on encouraging results in a first set of computational experiments using supervised machine learning, we have set forth a research agenda for tackling the TEP problem more comprehensively." ></td>
	<td class="line x" title="188:192	8 Acknowledgments We are grateful to the annotators, in particular A. Rasmussen and S. Siddiqui." ></td>
	<td class="line x" title="189:192	We also thank two anonymous reviewers for comments." ></td>
	<td class="line x" title="190:192	This work was funded by NSF under award ITR-#0205731, and NS ITR IIS-0428472." ></td>
	<td class="line x" title="191:192	The annotation is supported by UIUCs Research Board." ></td>
	<td class="line x" title="192:192	The authors take sole responsibility for the work." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="H05-1115
Using Random Walks For Question-Focused Sentence Retrieval
Otterbacher, Jahna C.;Erkan, Gunes;Radev, Dragomir R.;"></td>
	<td class="line x" title="1:207	Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 915922, Vancouver, October 2005." ></td>
	<td class="line x" title="2:207	c2005 Association for Computational Linguistics Using Random Walks for Question-focused Sentence Retrieval Jahna Otterbacher 1,GunesErkan 2, Dragomir R. Radev 1,2 1 School of Information, 2 Department of EECS University of Michigan {jahna,gerkan,radev}@umich.edu Abstract We consider the problem of questionfocused sentence retrieval from complex news articles describing multi-event stories published over time." ></td>
	<td class="line x" title="3:207	Annotators generated a list of questions central to understanding each story in our corpus." ></td>
	<td class="line x" title="4:207	Because of the dynamic nature of the stories, many questions are time-sensitive (e.g. How many victims have been found?) Judges found sentences providing an answer to each question." ></td>
	<td class="line x" title="5:207	To address the sentence retrieval problem, we apply a stochastic, graph-based method for comparing the relative importance of the textual units, which was previously used successfully for generic summarization." ></td>
	<td class="line x" title="6:207	Currently, we present a topic-sensitive version of our method and hypothesize that it can outperform a competitive baseline, which compares the similarity of each sentence to the input question via IDF-weighted word overlap." ></td>
	<td class="line x" title="7:207	In our experiments, the method achieves a TRDRscore that is significantly higher than that of the baseline." ></td>
	<td class="line x" title="8:207	1 Introduction Recent work has motivated the need for systems that support Information Synthesis tasks, in which a user seeks a global understanding of a topic or story (Amigo et al. , 2004)." ></td>
	<td class="line x" title="9:207	In contrast to the classical question answering setting (e.g. TREC-style Q&A (Voorhees and Tice, 2000)), in which the user presents a single question and the system returns a corresponding answer (or a set of likely answers), in this case the user has a more complex information need." ></td>
	<td class="line x" title="10:207	Similarly, when reading about a complex news story, such as an emergency situation, users might seek answers to a set of questions in order to understand it better." ></td>
	<td class="line x" title="11:207	For example, Figure 1 shows the interface to our Web-based news summarization system, which a user has queried for information about Hurricane Isabel." ></td>
	<td class="line x" title="12:207	Understanding such stories is challenging for a number of reasons." ></td>
	<td class="line x" title="13:207	In particular, complex stories contain many sub-events (e.g. the devastation of the hurricane, the relief effort, etc)." ></td>
	<td class="line x" title="14:207	In addition, while some facts surrounding the situation do not change (such as Which area did the hurricane first hit?), others may change with time (How many people have been left homeless?)." ></td>
	<td class="line x" title="15:207	Therefore, we are working towards developing a system for question answering fromclusters ofcomplex stories published over time." ></td>
	<td class="line x" title="16:207	As can be seen at the bottom of Figure 1, we plan to add a component to our current system that allows users to ask questions as they read a story." ></td>
	<td class="line x" title="17:207	They may then choose to receive either a precise answer or a question-focused summary." ></td>
	<td class="line x" title="18:207	Currently, we address the question-focused sentence retrieval task." ></td>
	<td class="line x" title="19:207	While passage retrieval (PR) is clearly not a new problem (e.g.(Robertson et al. , 1992; Salton et al. , 1993)), it remains important and yet often overlooked." ></td>
	<td class="line x" title="21:207	Asnoted by(Gaizauskas etal., 2004), while PR is the crucial first step for question answering, Q&A research has typically not empha915 Hurricane Isabel's outer bands moving onshore produced on 09/18, 6:18 AM 2% Summary The North Carolina coast braced for a weakened but still potent Hurricane Isabel while already rain-soaked areas as far away as Pennsylvania prepared for possibly ruinous flooding." ></td>
	<td class="line x" title="23:207	(2:3) A hurricane warning was in effect from Cape Fear in southern North Carolina to the Virginia-Maryland line, and tropical storm warnings extended from South Carolina to New Jersey." ></td>
	<td class="line x" title="24:207	(2:14) While the outer edge of the hurricane approached the North Carolina coast Wednesday, the center of the storm was still 400 miles south-southeast of Cape Hatteras, N.C., late Wednesday morning." ></td>
	<td class="line x" title="25:207	(3:10) BBC NEWS World Americas Hurricane Isabel prompts US shutdown (4:1) Ask us: What states have been affected by the hurricane so far?" ></td>
	<td class="line x" title="26:207	Around 200,000 people in coastal areas of North Carolina and Virginia were ordered to evacuate or risk getting trapped by flooding from storm surges up to 11 feet." ></td>
	<td class="line x" title="27:207	(5:8) The storm was expected to hit with its full fury today, slamming into the North Carolina coast with 105-mph winds and 45-foot wave crests, before moving through Virginia and bashing the capital with gusts of about 60 mph." ></td>
	<td class="line x" title="28:207	(7:6) Figure 1: Question tracking interface to a summarization system." ></td>
	<td class="line x" title="29:207	sized it." ></td>
	<td class="line x" title="30:207	The specific problem we consider differs from the classic task of PR for a Q&A system in interesting ways, due to the time-sensitive nature of the stories inour corpus." ></td>
	<td class="line x" title="31:207	Forexample, one challenge is that the answer to a users question may be updated and reworded over time by journalists in order to keep a running story fresh, or because the facts themselves change." ></td>
	<td class="line x" title="32:207	Therefore, there is often more than one correct answer to a question." ></td>
	<td class="line x" title="33:207	We aim to develop a method for sentence retrieval that goes beyond finding sentences that are similar to a single query." ></td>
	<td class="line x" title="34:207	To this end, we propose to use a stochastic, graph-based method." ></td>
	<td class="line pc" title="35:207	Recently, graph-based methods have proved useful for a number of NLP and IR tasks such as document re-ranking in ad hoc IR (Kurland and Lee, 2005) and analyzing sentiments in text (Pang and Lee, 2004)." ></td>
	<td class="line x" title="36:207	In (Erkan and Radev, 2004), we introduced the LexRank method and successfully applied it to generic, multi-document summarization." ></td>
	<td class="line x" title="37:207	Presently, we introduce topic-sensitive LexRank in creating a sentence retrieval system." ></td>
	<td class="line x" title="38:207	We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and the question (using IDF-weighed word overlap)." ></td>
	<td class="line x" title="39:207	We demonstrate that LexRank significantly improves question-focused sentence selection over the baseline." ></td>
	<td class="line x" title="40:207	2 Formal description of the problem Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version of the LexRankmethod." ></td>
	<td class="line x" title="41:207	Incontrast to previous PRsystems such as Okapi (Robertson et al. , 1992), which ranks documents for relevancy and then proceeds to find paragraphs related to a question, we address the finer-grained problem of finding sentences containing answers." ></td>
	<td class="line x" title="42:207	In addition, the input to our system is a set of documents relevant to the topic of the query that the user has already identified (e.g. via a search engine)." ></td>
	<td class="line x" title="43:207	Our system does not rank the input documents, nor is it restricted in terms of the number of sentences that may be selected from the same document." ></td>
	<td class="line x" title="44:207	The output of our system, a ranked list of sentences relevant to the users question, can be subsequently used as input to an answer selection system in order to find specific answers from the extracted sentences." ></td>
	<td class="line x" title="45:207	Alternatively, the sentences can be returned to the user as a question-focused summary." ></td>
	<td class="line x" title="46:207	This is similar to snippet retrieval (Wu et al. , 2004)." ></td>
	<td class="line x" title="47:207	However, in our system answers are extracted from a set of multiple documents rather than on a document-by-document basis." ></td>
	<td class="line x" title="48:207	3 Our approach: topic-sensitive LexRank 3.1 The LexRank method In (Erkan and Radev, 2004), the concept of graphbased centrality was used to rank a set of sentences, in producing generic multi-document summaries." ></td>
	<td class="line x" title="49:207	To apply LexRank, a similarity graph is produced for the sentences in an input document set." ></td>
	<td class="line x" title="50:207	In the graph, each node represents a sentence." ></td>
	<td class="line x" title="51:207	There are edges between nodes for which the cosine similarity between the respective pair of sentences exceeds a given threshold." ></td>
	<td class="line x" title="52:207	The degree of a given node is an indication of how much information the respective sentence has in common with other sentences." ></td>
	<td class="line x" title="53:207	Therefore, sentences that contain the most salient information in the document set should be very central within the graph." ></td>
	<td class="line x" title="54:207	Figure 2 shows an example of a similarity graph for a set of five input sentences, using a cosine similarity threshold of 0.15." ></td>
	<td class="line x" title="55:207	Once the similarity graph is constructed, the sentences are then ranked according to their eigenvector centrality." ></td>
	<td class="line x" title="56:207	As previously mentioned, theoriginal LexRankmethod performed well in the context of generic summarization." ></td>
	<td class="line x" title="57:207	Below, we describe a topic-sensitive version of LexRank, which is more appropriate for the question-focused sentence retrieval problem." ></td>
	<td class="line x" title="58:207	In the new approach, the 916 score of asentence isdetermined byamixture model of the relevance of the sentence to the query and the similarity of the sentence to other high-scoring sentences." ></td>
	<td class="line x" title="59:207	3.2 Relevance to the question In topic-sensitive LexRank, we first stem all of the sentences in a set of articles and compute word IDFs by the following formula: idf w =log parenleftBig N +1 0.5+sf w parenrightBig (1) where N is the total number of sentences in the cluster, and sf w is the number of sentences that the word w appears in." ></td>
	<td class="line x" title="60:207	We also stem the question and remove the stop words from it." ></td>
	<td class="line x" title="61:207	Then the relevance of a sentence s to the question q is computed by: rel(s|q)= X wq log(tf w,s +1) log(tf w,q +1) idf w (2) where tf w,s and tf w,q are the number of times w appears in s and q, respectively." ></td>
	<td class="line x" title="62:207	This model has proven to be successful in query-based sentence retrieval (Allan et al. , 2003), and is used as our competitive baseline in this study (e.g. Tables 4, 5 and 7)." ></td>
	<td class="line x" title="63:207	3.3 The mixture model The baseline system explained above does not make use of any inter-sentence information in a cluster." ></td>
	<td class="line x" title="64:207	We hypothesize that a sentence that is similar to the high scoring sentences in the cluster should also have a high score." ></td>
	<td class="line x" title="65:207	For instance, if a sentence that gets a high score in our baseline model is likely to contain an answer to the question, then a related sentence, which may not be similar to the question itself, is also likely to contain an answer." ></td>
	<td class="line x" title="66:207	This idea is captured by the following mixture model, where p(s|q), the score of a sentence s given a question q, is determined as the sum of its relevance to the question (using the same measure as the baseline described above) and the similarity to the other sentences in the document cluster: p(s|q)=d rel(s|q) P zC rel(z|q) +(1d) X vC sim(s,v) P zC sim(z,v) p(v|q) (3) where C is the set of all sentences in the cluster." ></td>
	<td class="line x" title="67:207	The value of d, which we will also refer to as the question bias, is a trade-off between two terms in the Vertices: Sentence Index Salience Sentence 4 0.1973852892722677 Milan fire brigade officials said that 1 0.03614457831325301 At least two people are dead, inclu 0 0.28454242157110576 Officials said the plane was carryin 2 0.1973852892722677 Italian police said the plane was car 3 0.28454242157110576 Rescue officials said that at least th Graph Figure 2: LexRank example: sentence similarity graph with a cosine threshold of 0.15." ></td>
	<td class="line x" title="68:207	equation and is determined empirically." ></td>
	<td class="line x" title="69:207	For higher values of d, we give more importance to the relevance to the question compared to the similarity to the other sentences in the cluster." ></td>
	<td class="line x" title="70:207	The denominators in both terms are for normalization, which are described below." ></td>
	<td class="line x" title="71:207	We use the cosine measure weighted by word IDFs as the similarity between two sentences in a cluster: sim(x,y)= P wx,y tf w,x tf w,y (idf w ) 2 q P x i x (tf x i,x idf x i ) 2  q P y i y (tf y i,y idf y i ) 2 (4) Equation 3 can be written in matrix notation as follows: p =[dA+(1 d)B] T p (5) A is the square matrix such that for a given index i, all the elements in the i th column are proportional to rel(i|q)." ></td>
	<td class="line x" title="72:207	B is also a square matrix such that each entry B(i,j) is proportional to sim(i,j).Bothmatrices are normalized so that row sums add up to 1." ></td>
	<td class="line x" title="73:207	Note that as a result of this normalization, all rows of the resulting square matrixQ =[dA+(1d)B] also add up to 1." ></td>
	<td class="line x" title="74:207	Such a matrix is called stochastic and defines a Markov chain." ></td>
	<td class="line x" title="75:207	If we view each sentence as a state in a Markov chain, thenQ(i,j) specifies the transition probability from state i to state j in the corresponding Markov chain." ></td>
	<td class="line x" title="76:207	The vector p we are looking for in Equation 5 is the stationary distribution of the Markov chain." ></td>
	<td class="line x" title="77:207	An intuitive interpretation of the stationary distribution can be under917 stood by the concept of a random walk on the graph representation of the Markov chain." ></td>
	<td class="line x" title="78:207	With probability d, a transition is made from the current node (sentence) to the nodes that are similar to the query." ></td>
	<td class="line x" title="79:207	With probability (1-d), a transition is made to the nodes that are lexically similar to the current node." ></td>
	<td class="line x" title="80:207	Every transition is weighted according to the similarity distributions." ></td>
	<td class="line x" title="81:207	Each element of the vector p gives the asymptotic probability of ending up at the corresponding state in the long run regardless of the starting state." ></td>
	<td class="line x" title="82:207	The stationary distribution of a Markov chain can be computed by a simple iterative algorithm, called power method." ></td>
	<td class="line x" title="83:207	1 A simpler version of Equation 5, where A is a uniform matrix andBis a normalized binary matrix, is known as PageRank (Brin and Page, 1998; Page et al. , 1998) and used to rank the web pages by the Google search engine." ></td>
	<td class="line x" title="84:207	It was also the model used to rank sentences in (Erkan and Radev, 2004)." ></td>
	<td class="line x" title="85:207	3.4 Experiments with topic-sensitive LexRank We experimented with different values of d on our training data." ></td>
	<td class="line x" title="86:207	We also considered several threshold values for inter-sentence cosine similarities, where we ignored the similarities between the sentences that are below the threshold." ></td>
	<td class="line x" title="87:207	In the training phase of the experiment, we evaluated all combinations of LexRank with d in the range of [0,1] (in increments of 0.10) and with a similarity threshold ranging from [0,0.9] (in increments of 0.05)." ></td>
	<td class="line x" title="88:207	We then found all configurations that outperformed the baseline." ></td>
	<td class="line x" title="89:207	These configurations were then applied to our development/test set." ></td>
	<td class="line x" title="90:207	Finally, our best sentence retrieval system was applied to our test data set and evaluated against the baseline." ></td>
	<td class="line x" title="91:207	The remainder of the paper will explain this process and the results in detail." ></td>
	<td class="line x" title="92:207	4 Experimental setup 4.1 Corpus We built a corpus of 20 multi-document clusters of complex news stories, such as plane crashes, political controversies and natural disasters." ></td>
	<td class="line x" title="93:207	The data 1 The stationary distribution is unique and the power method is guaranteed to converge provided that the Markov chain is ergodic (Seneta, 1981)." ></td>
	<td class="line x" title="94:207	A non-ergodic Markov chain can be made ergodic by reserving a small probability for jumping to any other state from the current state (Page et al. , 1998)." ></td>
	<td class="line x" title="95:207	clusters and their characteristics are shown in Table 1." ></td>
	<td class="line x" title="96:207	The news articles were collected from various sources." ></td>
	<td class="line x" title="97:207	Newstracker clusters were collected automatically by our Web-based news summarization system." ></td>
	<td class="line x" title="98:207	The number of clusters randomly assigned to the training, development/test and test data sets were 11, 3 and 6, respectively." ></td>
	<td class="line x" title="99:207	Next, we assigned each cluster of articles to an annotator, who was asked to read all articles in the cluster." ></td>
	<td class="line x" title="100:207	He or she then generated a list of factual questions key to understanding the story." ></td>
	<td class="line x" title="101:207	Once we collected the questions for each cluster, two judges independently annotated nine of the training clusters." ></td>
	<td class="line x" title="102:207	For each sentence and question pair in a given cluster, the judges were asked to indicate whether or not the sentence contained a complete answer to the question." ></td>
	<td class="line x" title="103:207	Once an acceptable rate of interjudge agreement was verified on the first nine clusters (Kappa (Carletta, 1996) of 0.68), the remaining 11 clusters were annotated by one judge each." ></td>
	<td class="line x" title="104:207	In some cases, the judges did not find any sentences containing the answer for a given question." ></td>
	<td class="line x" title="105:207	Such questions were removed from the corpus." ></td>
	<td class="line x" title="106:207	The final number of questions annotated for answers over the entire corpus was 341, and the distributions of questions per cluster can be found in Table 1." ></td>
	<td class="line x" title="107:207	4.2 Evaluation metrics and methods To evaluate our sentence retrieval mechanism, we produced extract files, which contain a list of sentences deemed to be relevant to the question, for the system and from human judgment." ></td>
	<td class="line x" title="108:207	To compare different configurations of our system to the baseline system, we produced extracts at a fixed length of 20 sentences." ></td>
	<td class="line x" title="109:207	While evaluations of question answering systems are often based on a shorter list of ranked sentences, we chose to generate longer lists for several reasons." ></td>
	<td class="line x" title="110:207	One is that we are developing a PR system, of which the output can then be input to an answer extraction system for further processing." ></td>
	<td class="line x" title="111:207	In such a setting, we would most likely want to generate a relatively longer list of candidate sentences." ></td>
	<td class="line x" title="112:207	As previously mentioned, in our corpus the questions often have more than one relevant answer, so ideally, our PR system would find many of the relevant sentences, sending them on to the answer component to decide which answer(s) should be returned to the user." ></td>
	<td class="line x" title="113:207	Each systems extract file lists the document 918 Cluster Sources Articles Questions Data set Sample question Algerian terror AFP, UPI 2 12 train What is the condition under which threat GIA will take its action?" ></td>
	<td class="line x" title="114:207	Milan plane MSNBC, CNN, ABC, 9 15 train How many people were in the crash Fox, USAToday building at the time of the crash?" ></td>
	<td class="line x" title="115:207	Turkish plane BBC, ABC, 10 12 train To where was the plane headed?" ></td>
	<td class="line x" title="116:207	crash FoxNews, Yahoo Moscow terror UPI, AFP, AP 7 7 train How many people were killed in attack the most recent explosion?" ></td>
	<td class="line x" title="117:207	Rhode Island MSNBC, CNN, ABC, Lycos, 10 8 train Who was to blame for club fire Fox, BBC, Ananova the fire?" ></td>
	<td class="line x" title="118:207	FBI most AFP, UPI 3 14 train How much is the State Department offering wanted for information leading to bin Ladens arrest?" ></td>
	<td class="line x" title="119:207	Russia bombing AP, AFP 2 11 train What was the cause of the blast?" ></td>
	<td class="line x" title="120:207	Bali terror CNN, FoxNews, ABC, 10 30 train What were the motivations attack BBC, Ananova of the attackers?" ></td>
	<td class="line x" title="121:207	Washington DC FoxNews, Haaretz, BBC, 8 28 train What kinds of equipment or weapons sniper BBC, Washington Times, CBS were used in the killings?" ></td>
	<td class="line x" title="122:207	GSPC terror Newstracker 8 29 train What are the charges against group the GSPC suspects?" ></td>
	<td class="line x" title="123:207	China Novelty 43 25 18 train What was the magnitude of the earthquake earthquake in Zhangjiakou?" ></td>
	<td class="line x" title="124:207	Gulfair ABC, BBC, CNN, USAToday, 11 29 dev/test How many people FoxNews, Washington Post were on board?" ></td>
	<td class="line x" title="125:207	David Beckham AFP 20 28 dev/test How long had Beckham been playing for trade MU before he moved to RM?" ></td>
	<td class="line x" title="126:207	Miami airport Newstracker 12 15 dev/test How many concourses does evacuation the airport have?" ></td>
	<td class="line x" title="127:207	US hurricane DUC d04a 14 14 test In which places had the hurricane landed?" ></td>
	<td class="line x" title="128:207	EgyptAir crash Novelty 4 25 29 test How many people were killed?" ></td>
	<td class="line x" title="129:207	Kursk submarine Novelty 33 25 30 test When did the Kursk sink?" ></td>
	<td class="line x" title="130:207	Hebrew University bombing Newstracker 11 27 test How many people were injured?" ></td>
	<td class="line x" title="131:207	Finland mall bombing Newstracker 9 15 test How many people were in the mall at the time of the bombing?" ></td>
	<td class="line x" title="132:207	Putin visits Newstracker 12 20 test What issue concerned British England human rights groups?" ></td>
	<td class="line x" title="133:207	Table 1: Corpus of complex news stories." ></td>
	<td class="line x" title="134:207	and sentence numbers of the top 20 sentences." ></td>
	<td class="line x" title="135:207	The gold standard extracts list the sentences judged as containing answers to a given question by the annotators (and therefore have variable sizes) in no particular order." ></td>
	<td class="line x" title="136:207	2 We evaluated the performance of the systems using two metrics Mean Reciprocal Rank (MRR) (Voorhees and Tice, 2000) and Total Reciprocal Document Rank (TRDR) (Radev et al. , 2005)." ></td>
	<td class="line x" title="137:207	MRR, used in the TREC Q&A evaluations, is the reciprocal rank of the first correct answer (or sentence, in our case) to a given question." ></td>
	<td class="line x" title="138:207	This measure gives us an idea of how far down we must look in the ranked list in order to find a correct answer." ></td>
	<td class="line x" title="139:207	To contrast, TRDR is the total of the reciprocal ranks of all answers found by the system." ></td>
	<td class="line x" title="140:207	In the context of answering questions from complex stories, wherethere is often more than one correct answer to a question, and where answers are typically time-dependent, we should focus on maximizing TRDR, which gives us 2 For clusters annotated by two judges, all sentences chosen by at least one judge were included." ></td>
	<td class="line x" title="141:207	a measure of how many of the relevant sentences were identified by the system." ></td>
	<td class="line x" title="142:207	However, we report both the average MRR and TRDR over all questions in a given data set." ></td>
	<td class="line x" title="143:207	5 LexRank versus the baseline system In the training phase, we searched the parameter space for the values of d (the question bias) and the similarity threshold inorder to optimize the resulting TRDR scores." ></td>
	<td class="line x" title="144:207	For our problem, we expected that a relatively low similarity threshold pair with a high question bias would achieve the best results." ></td>
	<td class="line x" title="145:207	Table 2 shows the effect of varying the similarity threshold." ></td>
	<td class="line x" title="146:207	3 The notation LR[a,d] is used, where a is the similarity threshold and d is the question bias." ></td>
	<td class="line x" title="147:207	The optimal range for the parameter a was between 0.14 and 0.20." ></td>
	<td class="line x" title="148:207	This is intuitive because if the threshold is too high, such that only the most lexically similar sentences are represented in the graph, the method does not find sentences that are related but are more lex3 A threshold of -1 means that no threshold was used such that all sentences were included in the graph." ></td>
	<td class="line x" title="149:207	919 System Ave. MRR Ave. TRDR LR[-1.0,0.65] 0.5270 0.8117 LR[0.02,0.65] 0.5261 0.7950 LR[0.16,0.65] 0.5131 0.8134 LR[0.18,0.65] 0.5062 0.8020 LR[0.20,0.65] 0.5091 0.7944 LR[-1.0,0.80] 0.5288 0.8152 LR[0.02,0.80] 0.5324 0.8043 LR[0.16,0.80] 0.5184 0.8160 LR[0.18,0.80] 0.5199 0.8154 LR[0.20,0.80] 0.5282 0.8152 Table 2: Training phase: effect of similarity threshold (a) on Ave. MRR and TRDR." ></td>
	<td class="line x" title="150:207	System Ave. MRR Ave. TRDR LR[0.02,0.65] 0.5261 0.7950 LR[0.02,0.70] 0.5290 0.7997 LR[0.02,0.75] 0.5299 0.8013 LR[0.02,0.80] 0.5324 0.8043 LR[0.02,0.85] 0.5322 0.8038 LR[0.02,0.90] 0.5323 0.8077 LR[0.20,0.65] 0.5091 0.7944 LR[0.20,0.70] 0.5244 0.8105 LR[0.20,0.75] 0.5285 0.8137 LR[0.20,0.80] 0.5282 0.8152 LR[0.20,0.85] 0.5317 0.8203 LR[0.20,0.90] 0.5368 0.8265 Table 3: Training phase: effect of question bias (d) on Ave. MRR and TRDR." ></td>
	<td class="line x" title="151:207	ically diverse (e.g. paraphrases)." ></td>
	<td class="line x" title="152:207	Table 3 shows the effect of varying the question bias at two different similarity thresholds (0.02 and0.20)." ></td>
	<td class="line x" title="153:207	Itisclear that a high question bias isneeded." ></td>
	<td class="line x" title="154:207	However, asmall probability for jumping to a node that is lexically similar to the given sentence (rather than the question itself) is needed." ></td>
	<td class="line x" title="155:207	Table 4 shows the configurations of LexRank that performed better than the baseline system on the training data, based on mean TRDR scores over the 184 training questions." ></td>
	<td class="line x" title="156:207	We applied all four of these configurations to our unseen development/test data, in order to see if we could further differentiate their performances." ></td>
	<td class="line x" title="157:207	5.1 Development/testing phase The scores for the four LexRank systems and the baseline on the development/test data are shown in System Ave. MRR Ave. TRDR Baseline 0.5518 0.8297 LR[0.14,0.95] 0.5267 0.8305 LR[0.18,0.90] 0.5376 0.8382 LR[0.18,0.95] 0.5421 0.8382 LR[0.20,0.95] 0.5404 0.8311 Table 4: Training phase: systems outperforming the baseline in terms of TRDR score." ></td>
	<td class="line x" title="158:207	System Ave. MRR Ave. TRDR Baseline 0.5709 1.0002 LR[0.14,0.95] 0.5882 1.0469 LR[0.18,0.90] 0.5820 1.0288 LR[0.18,0.95] 0.5956 1.0411 LR[0.20,0.95] 0.6068 1.0601 Table 5: Development testing evaluation." ></td>
	<td class="line x" title="159:207	Cluster B-MRR LR-MRR B-TRDR LR-TRDR Gulfair 0.5446 0.5461 0.9116 0.9797 David Beckham trade 0.5074 0.5919 0.7088 0.7991 Miami airport 0.7401 0.7517 1.7157 1.7028 evacuation Table 6: Average scores by cluster: baseline versus LR[0.20,0.95]." ></td>
	<td class="line x" title="160:207	Table 5." ></td>
	<td class="line x" title="161:207	This time, all four LexRanksystems outperformed the baseline, both in terms of average MRR and TRDR scores." ></td>
	<td class="line x" title="162:207	An analysis of the average scores over the 72 questions within each of the three clusters for the best system, LR[0.20,0.95], is shown in Table 6." ></td>
	<td class="line x" title="163:207	While LexRank outperforms the baseline system on the first two clusters both in terms of MRR and TRDR, their performances are not substantially different on the third cluster." ></td>
	<td class="line x" title="164:207	Therefore, we examined properties of the questions within each cluster in order to see what effect they might have on system performance." ></td>
	<td class="line x" title="165:207	We hypothesized that the baseline system, which compares the similarity of each sentence to the question using IDF-weighted word overlap, should perform well on questions that provide many content words." ></td>
	<td class="line x" title="166:207	To contrast, LexRank might perform better when the question provides fewer content words, since it considers both similarity to the query and inter-sentence similarity." ></td>
	<td class="line x" title="167:207	Out of the 72 questions in the development/test set, the baseline system outperformed LexRank on 22 of the questions." ></td>
	<td class="line x" title="168:207	In fact, the average number of content words among these 22 questions was slightly, but not significantly, higher than the average on the remaining questions (3.63 words per question versus 3.46)." ></td>
	<td class="line x" title="169:207	Given this observation, we experimented with two mixed strategies, in which the number of content words in a question determined whether LexRank or the baseline system was used for sentence retrieval." ></td>
	<td class="line x" title="170:207	We tried threshold values of 4 and 6 content words, however, this did not improve the performance over the pure strategy of system LR[0.20,0.95]." ></td>
	<td class="line x" title="171:207	Therefore, we applied this 920 Ave. MRR Ave. TRDR Baseline 0.5780 0.8673 LR[0.20,0.95] 0.6189 0.9906 p-value na 0.0619 Table 7: Testing phase: baseline vs. LR[0.20,0.95]." ></td>
	<td class="line x" title="172:207	system versus the baseline to our unseen test set of 134 questions." ></td>
	<td class="line x" title="173:207	5.2 Testing phase As shown in Table 7, LR[0.20,0.95] outperformed the baseline system on the test data both in terms of average MRR and TRDR scores." ></td>
	<td class="line x" title="174:207	The improvement in average TRDR score was statistically significant with a p-value of 0.0619." ></td>
	<td class="line x" title="175:207	Since we are interested in a passage retrieval mechanism that finds sentences relevant to a given question, providing input to the question answering component of our system, the improvement in average TRDR score is very promising." ></td>
	<td class="line x" title="176:207	While we saw in Section 5.1 that LR[0.20,0.95] may perform better on some question or cluster types than others, weconclude that it beats the competitive baseline when one is looking to optimize mean TRDR scores over a large set of questions." ></td>
	<td class="line x" title="177:207	However, in future work, we will continue to improve the performance, perhaps by developing mixed strategies using different configurations of LexRank." ></td>
	<td class="line x" title="178:207	6 Discussion The idea behind using LexRank for sentence retrieval is that a system that considers only the similarity between candidate sentences and the input query, and not the similarity between the candidate sentences themselves, is likely to miss some important sentences." ></td>
	<td class="line x" title="179:207	When using any metric to compare sentences and a query, there is always likely to be a tie between multiple sentences (or, similarly, there may be cases where fewer than the number of desired sentences have similarity scores above zero)." ></td>
	<td class="line x" title="180:207	LexRank effectively provides a means to break such ties." ></td>
	<td class="line x" title="181:207	An example of such a scenario is illustrated in Tables 8and 9, whichshowthe top ranked sentences by the baseline and LexRank, respectively for the question What caused the Kursk to sink? from the Kursk submarine cluster." ></td>
	<td class="line x" title="182:207	It can be seen that all top five sentences chosen by the baseline system have Rank Sentence Score Relevant?" ></td>
	<td class="line x" title="183:207	1 The Russian governmental commission on the 4.2282 N accident of the submarine Kursk sinking in the Barents Sea on August 12 has rejected 11 original explanations for the disaster, but still cannot conclude what caused the tragedy indeed, Russian Deputy Premier Ilya Klebanov said here Friday." ></td>
	<td class="line x" title="184:207	2 There has been no final word on what caused 4.2282 N the submarine to sink while participating in a major naval exercise, but Defense Minister Igor Sergeyev said the theory that Kursk may have collided with another object is receiving increasingly concrete confirmation." ></td>
	<td class="line x" title="185:207	3 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Y said Thursday that collision with a big object caused the Kursk nuclear submarine to sink to the bottom of the Barents Sea." ></td>
	<td class="line x" title="186:207	4 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Y said Thursday that collision with a big object caused the Kursk nuclear submarine to sink to the bottom of the Barents Sea." ></td>
	<td class="line x" title="187:207	5 President Clintons national security adviser, 4.2282 N Samuel Berger, has provided his Russian counterpart with a written summary of what U.S. naval and intelligence officials believe caused the nuclear-powered submarine Kursk to sink last month in the Barents Sea, officials said Wednesday." ></td>
	<td class="line x" title="188:207	Table 8: Top ranked sentences using baseline system on the question What caused the Kursk to sink?." ></td>
	<td class="line x" title="189:207	the same sentence score (similarity to the query), yet the top ranking two sentences are not actually relevant according to the judges." ></td>
	<td class="line x" title="190:207	To contrast, LexRank achieved a better ranking of the sentences since it is better able to differentiate between them." ></td>
	<td class="line x" title="191:207	It should be noted that both for the LexRank and baseline systems, chronological ordering of the documents and sentences is preserved, such that in cases where two sentences have the same score, the one published earlier is ranked higher." ></td>
	<td class="line x" title="192:207	7Conclusion We presented topic-sensitive LexRank and applied it to the problem of sentence retrieval." ></td>
	<td class="line x" title="193:207	In a Webbased news summarization setting, users of our system could choose to see the retrieved sentences (as in Table 9) as a question-focused summary." ></td>
	<td class="line x" title="194:207	As indicated in Table 9, each of the top three sentences were judged by our annotators as providing a complete answer to the respective question." ></td>
	<td class="line x" title="195:207	While the first two sentences provide the same answer (a collision caused the Kursk to sink), the third sentence provides a different answer (an explosion caused the disaster)." ></td>
	<td class="line x" title="196:207	While the last two sentences do not provide answers according to our judges, they do provide context information about the situation." ></td>
	<td class="line x" title="197:207	Alternatively, the user might prefer to see the extracted 921 Rank Sentence Score Relevant?" ></td>
	<td class="line x" title="198:207	1 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Y said Thursday that collision with a big object caused the Kursk nuclear submarine to sink to the bottom of the Barents Sea." ></td>
	<td class="line x" title="199:207	2 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Y said Thursday that collision with a big object caused the Kursk nuclear submarine to sink to the bottom of the Barents Sea." ></td>
	<td class="line x" title="200:207	3 The Russian navy refused to confirm this, 0.0125 Y but officers have said an explosion in the torpedo compartment at the front of the submarine apparently caused the Kursk to sink." ></td>
	<td class="line x" title="201:207	4 President Clintons national security adviser, 0.0124 N Samuel Berger, has provided his Russian counterpart with a written summary of what U.S. naval and intelligence officials believe caused the nuclear-powered submarine Kursk to sink last month in the Barents Sea, officials said Wednesday." ></td>
	<td class="line x" title="202:207	5 There has been no final word on what caused 0.0123 N the submarine to sink while participating in a major naval exercise, but Defense Minister Igor Sergeyev said the theory that Kursk may have collided with another object is receiving increasingly concrete confirmation." ></td>
	<td class="line x" title="203:207	Table 9: Top ranked sentences using the LR[0.20,0.95] system on the question What caused the Kursk to sink? answers from the retrieved sentences." ></td>
	<td class="line x" title="204:207	In this case, the sentences selected by our system would be sent to an answer identification component for further processing." ></td>
	<td class="line x" title="205:207	As discussed in Section 2, our goal was to develop a topic-sensitive version of LexRank and to use it to improve a baseline system, which had previously been used successfully for query-based sentence retrieval (Allan et al. , 2003)." ></td>
	<td class="line x" title="206:207	In terms of this task, wehave shown that over a large set of unaltered questions written by our annotators, LexRank can, on average, outperform the baseline system, particularly in terms of TRDR scores." ></td>
	<td class="line x" title="207:207	8 Acknowledgments We would like to thank the members of the CLAIR group at Michigan and in particular Siwei Shen and Yang Ye for their assistance with this project." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="H05-1116
Multi-Perspective Question Answering Using The OpQA Corpus
Stoyanov, Veselin;Cardie, Claire;Wiebe, Janyce M.;"></td>
	<td class="line x" title="1:241	Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 923930, Vancouver, October 2005." ></td>
	<td class="line x" title="2:241	c2005 Association for Computational Linguistics Multi-Perspective Question Answering Using the OpQA Corpus Veselin Stoyanov and Claire Cardie Department of Computer Science Cornell University Ithaca, NY 14850, USA {ves,cardie}@cs.cornell.edu Janyce Wiebe Department of Computer Science University of Pittsburgh Pittsburgh, PA 15260, USA wiebe@cs.pitt.edu Abstract We investigate techniques to support the answering of opinion-based questions." ></td>
	<td class="line x" title="3:241	We first present the OpQA corpus of opinion questions and answers." ></td>
	<td class="line x" title="4:241	Using the corpus, we compare and contrast the properties of fact and opinion questions and answers." ></td>
	<td class="line x" title="5:241	Based on the disparate characteristics of opinion vs. fact answers, we argue that traditional fact-based QA approaches may have difficulty in an MPQA setting without modification." ></td>
	<td class="line x" title="6:241	As an initial step towards the development of MPQA systems, we investigate the use of machine learning and rule-based subjectivity and opinion source filters and show that they can be used to guide MPQA systems." ></td>
	<td class="line x" title="7:241	1 Introduction Much progress has been made in recent years in automatic, open-domain question answering (e.g. , Voorhees (2001), Voorhees (2002), Voorhees and Buckland (2003))." ></td>
	<td class="line x" title="8:241	The bulk of the research in this area, however, addresses fact-based questions like: When did McDonalds open its first restaurant? or What is the Kyoto Protocol?." ></td>
	<td class="line x" title="9:241	To date, however, relatively little research been done in the area of Multi-Perspective Question Answering (MPQA), which targets questions of the following sort:  How is Bushs decision not to ratify the Kyoto Protocol looked upon by Japan and other US allies?" ></td>
	<td class="line x" title="10:241	 How do the Chinese regard the human rights record of the United States?" ></td>
	<td class="line x" title="11:241	In comparison to fact-based question answering (QA), researchers understand far less about the properties of questions and answers in MPQA, and have yet to develop techniques to exploit knowledge of those properties." ></td>
	<td class="line x" title="12:241	As a result, it is unclear whether approaches that have been successful in the domain of fact-based QA will work well for MPQA." ></td>
	<td class="line x" title="13:241	We first present the OpQA corpus of opinion questions and answers." ></td>
	<td class="line x" title="14:241	Using the corpus, we compare and contrast the properties of fact and opinion questions and answers." ></td>
	<td class="line x" title="15:241	We find that text spans identified as answers to opinion questions: (1) are approximately twice as long as those of fact questions, (2) are much more likely (37% vs. 9%) to represent partial answers rather than complete answers, (3) vary much more widely with respect to syntactic category  covering clauses, verb phrases, prepositional phrases, and noun phrases; in contrast, fact answers are overwhelming associated with noun phrases, and (4) are roughly half as likely to correspond to a single syntactic constituent type (16-38% vs. 31-53%)." ></td>
	<td class="line x" title="16:241	Based on the disparate characteristics of opinion vs. fact answers, we argue that traditional fact-based QA approaches may have difficulty in an MPQA setting without modification." ></td>
	<td class="line x" title="17:241	As one such modification, we propose that MPQA systems should rely on natural language processing methods to identify information about opinions." ></td>
	<td class="line x" title="18:241	In experiments in opinion question answering using the OpQA corpus, we find that filtering potential answers using machine learning and rule-based NLP opinion filters substantially improves the performance of an end-to-end MPQA system according to both a mean reciprocal rank (MRR) measure (0.59 vs. a baseline of 0.42) 923 and a metric that determines the mean rank of the first correct answer (MRFA) (26.2 vs. a baseline of 61.3)." ></td>
	<td class="line x" title="19:241	Further, we find that requiring opinion answers to match the requested opinion source (e.g. , does <source> approve of the Kyoto Protocol) dramatically improves the performance of the MPQA system on the hardest questions in the corpus." ></td>
	<td class="line x" title="20:241	The remainder of the paper is organized as follows." ></td>
	<td class="line x" title="21:241	In the next section we summarize related work." ></td>
	<td class="line x" title="22:241	Section 3 describes the OpQA corpus." ></td>
	<td class="line x" title="23:241	Section 4 uses the OpQA corpus to identify potentially problematic issues for handling opinion vs. fact questions." ></td>
	<td class="line x" title="24:241	Section 5 briefly describes an opinion annotation scheme used in the experiments." ></td>
	<td class="line x" title="25:241	Sections 6 and 7 explore the use of opinion information in the design of MPQA systems." ></td>
	<td class="line x" title="26:241	2 Related Work There is a growing interest in methods for the automatic identification and extraction of opinions, emotions, and sentiments in text." ></td>
	<td class="line oc" title="27:241	Much of the relevant research explores sentiment classification, a text categorization task in which the goal is to assign to a document either positive (thumbs up) or negative (thumbs down) polarity (e.g. Das and Chen (2001), Pang et al.(2002), Turney (2002), Dave et al.(2003), Pang and Lee (2004))." ></td>
	<td class="line x" title="30:241	Other research has concentrated on analyzing opinions at, or below, the sentence level." ></td>
	<td class="line oc" title="31:241	Recent work, for example, indicates that systems can be trained to recognize opinions, their polarity, their source, and their strength to a reasonable degree of accuracy (e.g. Dave et al.(2003), Riloff and Wiebe (2003), Bethard et al.(2004), Pang and Lee (2004), Wilson et al.(2004), Yu and Hatzivassiloglou (2003), Wiebe and Riloff (2005))." ></td>
	<td class="line x" title="35:241	Related work in the area of corpus development includes Wiebe et al.s (2005) opinion annotation scheme to identify subjective expressions  expressions used to express opinions, emotions, sentiments and other private states in text." ></td>
	<td class="line x" title="36:241	Wiebe et al. have applied the annotation scheme to create the MPQA corpus consisting of 535 documents manually annotated for phrase-level expressions of opinion." ></td>
	<td class="line x" title="37:241	In addition, the NIST-sponsored TREC evaluation has begun to develop data focusing on opinions  the 2003 Novelty Track features a task that requires systems to identify opinion-oriented documents w.r.t. a specific issue (Voorhees and Buckland, 2003)." ></td>
	<td class="line x" title="38:241	While all of the above work begins to bridge the gap between text categorization and question answering, none of the approaches have been employed or evaluated in the context of MPQA." ></td>
	<td class="line x" title="39:241	3 OpQA Corpus To support our research in MPQA, we created the OpQA corpus of opinion and fact questions and answers." ></td>
	<td class="line x" title="40:241	Additional details on the construction of the corpus as well as results of an interannotator agreement study can be found in Stoyanov et al.(2004)." ></td>
	<td class="line x" title="42:241	3.1 Documents and Questions The OpQA corpus consists of 98 documents that appeared in the world press between June 2001 and May 2002." ></td>
	<td class="line x" title="43:241	All documents were taken from the aforementioned MPQA corpus (Wilson and Wiebe, 2003)1 and are manually annotated with phraselevel opinion information, following the annotation scheme of Wiebe et al.(2005), which is briefly summarized in Section 5." ></td>
	<td class="line x" title="45:241	The documents cover four general (and controversial) topics: President Bushs alternative to the Kyoto protocol (kyoto); the US annual human rights report (humanrights); the 2002 coup detat in Venezuela (venezuela); and the 2002 elections in Zimbabwe and Mugabes reelection (mugabe)." ></td>
	<td class="line x" title="46:241	Each topic is covered by between 19 and 33 documents that were identified automatically via IR methods." ></td>
	<td class="line x" title="47:241	Both fact and opinion questions for each topic were added to the OpQA corpus by a volunteer not associated with the current project." ></td>
	<td class="line x" title="48:241	The volunteer was provided with a set of instructions for creating questions together with two documents on each topic selected at random." ></td>
	<td class="line x" title="49:241	He created between six and eight questions on each topic, evenly split between fact and opinion." ></td>
	<td class="line x" title="50:241	The 30 questions are given in Table 1 sorted by topic." ></td>
	<td class="line x" title="51:241	3.2 Answer annotations Answer annotations were added to the corpus by two annotators according to a set of annotation instruc1The MPQA corpus is available at http://nrrc.mitre.org/NRRC/publications.htm." ></td>
	<td class="line x" title="52:241	The OpQA corpus is available upon request." ></td>
	<td class="line x" title="53:241	924 Kyoto 1f WhatistheKyotoProtocolabout?" ></td>
	<td class="line x" title="54:241	2f WhenwastheKyotoProtocoladopted?" ></td>
	<td class="line x" title="55:241	3f WhoisthepresidentoftheKikoNetwork?" ></td>
	<td class="line x" title="56:241	4f WhatistheKikoNetwork?" ></td>
	<td class="line x" title="57:241	5o DoesthepresidentoftheKikoNetworkapproveoftheUSactionconcerningtheKyotoProtocol?" ></td>
	<td class="line x" title="58:241	6o AretheJapaneseunanimousintheiropinionofBushspositionontheKyotoProtocol?" ></td>
	<td class="line x" title="59:241	7o HowisBushsdecisionnottoratifytheKyotoProtocollookeduponbyJapanandotherUSallies?" ></td>
	<td class="line x" title="60:241	8o HowdoEuropeanUnioncountriesfeelabouttheUSoppositiontotheKyotoprotocol?" ></td>
	<td class="line x" title="61:241	HumanRights 1f WhatisthemurderrateintheUnitedStates?" ></td>
	<td class="line x" title="62:241	2f WhatcountryissuesanannualreportonhumanrightsintheUnitedStates?" ></td>
	<td class="line x" title="63:241	3o HowdotheChineseregardthehumanrightsrecordoftheUnitedStates?" ></td>
	<td class="line x" title="64:241	4f WhoisAndrewWelsdan?" ></td>
	<td class="line x" title="65:241	5o WhatfactorsinfluencethewayinwhichtheUSregardsthehumanrightsrecordsofothernations?" ></td>
	<td class="line x" title="66:241	6o IstheUSAnnualHumanRightsReportreceivedwithuniversalapprovalaroundtheworld?" ></td>
	<td class="line x" title="67:241	Venezuela 1f WhendidHugoChavezbecomePresident?" ></td>
	<td class="line x" title="68:241	2f DidanyprominentAmericansplantovisitVenezuelaimmediatelyfollowingthe2002coup?" ></td>
	<td class="line x" title="69:241	3o DidanythingsurprisinghappenwhenHugoChavezregainedpowerinVenezuelaafterhewasremovedbyacoup?" ></td>
	<td class="line x" title="70:241	4o DidmostVenezuelanssupportthe2002coup?" ></td>
	<td class="line x" title="71:241	5f WhichgovernmentalinstitutionsinVenezuelaweredissolvedbytheleadersofthe2002coup?" ></td>
	<td class="line x" title="72:241	6o HowdidordinaryVenezuelansfeelaboutthe2002coupandsubsequentevents?" ></td>
	<td class="line x" title="73:241	7o DidAmericasupporttheVenezuelanforeignpolicyfollowedbyChavez?" ></td>
	<td class="line x" title="74:241	8f WhoisVice-PresidentofVenezuela?" ></td>
	<td class="line x" title="75:241	Mugabe 1o WhatwastheAmericanandBritishreactiontothereelectionofMugabe?" ></td>
	<td class="line x" title="76:241	2f WheredidMugabevoteinthe2002presidentialelection?" ></td>
	<td class="line x" title="77:241	3f AtwhichprimaryschoolhadMugabebeenexpectedtovoteinthe2002presidentialelection?" ></td>
	<td class="line x" title="78:241	4f HowlonghasMugabeheadedhiscountry?" ></td>
	<td class="line x" title="79:241	5f WhowasexpectingMugabeatMhofuSchoolforthe2002election?" ></td>
	<td class="line x" title="80:241	6o WhatisthebasisfortheEuropeanUnionandUScriticalattitudeandadversarialactiontowardMugabe?" ></td>
	<td class="line x" title="81:241	7o WhatdidSouthAfricawantMugabetodoafterthe2002election?" ></td>
	<td class="line x" title="82:241	8o WhatisMugabesopinionabouttheWests attitudeandactionstowardsthe2002Zimbabweelec-tion?" ></td>
	<td class="line x" title="83:241	Table 1: Questions in the OpQA collection by topic." ></td>
	<td class="line x" title="84:241	f in column 1 indicates a fact question; o, an opinion question." ></td>
	<td class="line x" title="85:241	tions.2 Every text segment that contributes to an answer to any of the 30 questions is annotated as an answer." ></td>
	<td class="line x" title="86:241	In particular, answer annotations include segments that constitute a partial answer." ></td>
	<td class="line x" title="87:241	Partial answers either (1) lack the specificity needed to constitute a full answer (e.g. , before May 2004 partially answers the question When was the Kyoto protocol ratified?" ></td>
	<td class="line x" title="88:241	when a specific date is known) or (2) need to be combined with at least one additional answer segment to fully answer the question (e.g. , the question Are the Japanese unanimous in their opposition of Bushs position on the Kyoto protocol?" ></td>
	<td class="line x" title="89:241	is answered only partially by a segment expressing a single opinion)." ></td>
	<td class="line x" title="90:241	In addition, annotators mark the minimum answer spans (e.g. , a Tokyo organization, vs. a Tokyo organization representing about 150 Japanese groups)." ></td>
	<td class="line x" title="91:241	4 Characteristics of opinion answers Next, we use the OpQA corpus to analyze and compare the characteristics of fact vs. opinion questions." ></td>
	<td class="line x" title="92:241	Based on our findings, we believe that QA systems based solely on traditional QA techniques are likely 2The annotation instructions are available at http://www.cs.cornell.edu/ ves/ Publications/publications.htm." ></td>
	<td class="line x" title="93:241	to be less effective at MPQA than they are at traditional fact-based QA." ></td>
	<td class="line x" title="94:241	4.1 Traditional QA architectures Despite the wide variety of approaches implied by modern QA systems, almost all systems rely on the following two steps (subsystems), which have empirically proven to be effective:  IR module." ></td>
	<td class="line x" title="95:241	The QA system invokes an IR subsystem that employs traditional text similarity measures (e.g. , tf/idf) to retrieve and rank document fragments (sentences or paragraphs) w.r.t. the question (query)." ></td>
	<td class="line x" title="96:241	 Linguistic filters." ></td>
	<td class="line x" title="97:241	QA systems employ a set of filters and text processing components to discard some document fragments." ></td>
	<td class="line x" title="98:241	The following filters have empirically proven to be effective and are used universally: Semantic filters prefer an answer segment that matches the semantic class(es) associated with the question type (e.g. , date or time for when questions; person or organization for who questions)." ></td>
	<td class="line x" title="99:241	Syntactic filters are also configured on the type of question." ></td>
	<td class="line x" title="100:241	The most common and effective syntactic filters select a specific constituent (e.g. , noun phrase) according to the question type (e.g. , who question)." ></td>
	<td class="line x" title="101:241	QA systems typically interleave the above two subsystems with a variety of different processing steps of both the question and the answer." ></td>
	<td class="line x" title="102:241	The goal of the processing is to identify text fragments that contain an answer to the question." ></td>
	<td class="line x" title="103:241	Typical QA systems do not perform any further text processing; they return the text fragment as it occurred in the text." ></td>
	<td class="line x" title="104:241	3 4.2 Corpus-based analysis of opinion answers We hypothesize that QA systems that conform to this traditional architecture will have difficulty handling opinion questions without non-trivial modification." ></td>
	<td class="line x" title="105:241	In support of this hypothesis, we provide statistics from the OpQA corpus to illustrate some of the characteristics that distinguish answers to opinion vs. fact questions, and discuss their implications for a traditional QA system architecture." ></td>
	<td class="line x" title="106:241	Answer length." ></td>
	<td class="line x" title="107:241	We see in Table 2 that the average length of opinion answers in the OpQA corpus 3This architecture is seen mainly in QA systems designed for TRECs factoid and list QA tracks." ></td>
	<td class="line x" title="108:241	Systems competing in the relatively new definition or other tracks have begun to introduce new approaches." ></td>
	<td class="line x" title="109:241	However, most such systems still rely on the IR step and return the text fragment as it occurred in the text." ></td>
	<td class="line x" title="110:241	925 Number of answers Length Number of partials fact 124 5.12 12 (9.68%) opinion 415 9.24 154 (37.11%) Table 2: Number of answers, average answer length (in tokens), and number of partial answers for fact/opinion questions." ></td>
	<td class="line x" title="111:241	is 9.24 tokens, almost double that of fact answers." ></td>
	<td class="line x" title="112:241	Unfortunately, longer answers could present problems for some traditional QA systems." ></td>
	<td class="line x" title="113:241	In particular, some of the more sophisticated algorithms that perform additional processing steps such as logical verifiers (Moldovan et al. , 2002) may be less accurate or computationally infeasible for longer answers." ></td>
	<td class="line x" title="114:241	More importantly, longer answers are likely to span more than a single syntactic constituent, rendering the syntactic filters, and very likely the semantic filters, less effective." ></td>
	<td class="line x" title="115:241	Partial answers." ></td>
	<td class="line x" title="116:241	Table 2 also shows that over 37% of the opinion answers were marked as partial vs. 9.68% of the fact answers." ></td>
	<td class="line x" title="117:241	The implications of partial answers for the traditional QA architecture are substantial: an MPQA system will require an answer generator to (1) distinguish between partial and full answers; (2) recognize redundant partial answers; (3) identify which subset of the partial answers, if any, constitutes a full answer; (4) determine whether additional documents need to be examined to find a complete answer; and (5) asemble the final answer from partial pieces of information." ></td>
	<td class="line x" title="118:241	Syntactic constituent of the answer." ></td>
	<td class="line x" title="119:241	As discussed in Section 4.1, traditional QA systems rely heavily on the predicted syntactic and semantic class of the answer." ></td>
	<td class="line x" title="120:241	Based on answer lengths, we speculated that opinion answers are unlikely to span a single constituent and/or semantic class." ></td>
	<td class="line x" title="121:241	This speculation is confirmed by examining the phrase type associated with OpQA answers using Abneys (1996) CASS partial parser.4 For each question, we count the number of times an answer segment for the question (in the manual annotations) matches each constituent type." ></td>
	<td class="line x" title="122:241	We consider four constituent types  noun phrase (n), verb phrase (v), prepositional phrase (p), and clause (c)  and three matching criteria: 4The parser is available from http://www.vinartus.net/spa/." ></td>
	<td class="line x" title="123:241	Fact Opinion Ques#of MatchingCriteria syn Ques#of MatchingCriteria syn tion answers ex up up/dn type tion answers ex up up/dn type H1 1 0 0 0 H3 15 5 5 5 c H2 4 2 2 2 n H5 24 5 5 10 n H4 1 0 0 0 H6 123 17 23 52 n K1 48 13 14 24 n K5 3 0 0 1 K2 38 13 13 19 n K6 34 6 5 12 c K3 1 1 1 1 cn K7 55 9 8 19 c K4 2 1 1 1 n K8 25 4 4 10 v M2 3 0 0 1 M1 74 10 12 29 v M3 1 0 0 1 M6 12 3 5 7 n M4 10 2 2 5 n M7 1 0 0 0 M5 3 1 1 2 c M8 3 0 0 1 V1 4 3 3 4 n V3 1 1 0 1 c V2 1 1 1 1 n V4 13 2 2 2 c V5 3 0 1 1 V6 9 2 2 5 cn V8 4 2 4 4 n V7 23 3 1 5 Cov124 39 43 66 Cov415 67 70 159 erage 31% 35% 53% erage 16% 17% 38% Table 3: Syntactic Constituent Type for Answers in the OpQA Corpus 1." ></td>
	<td class="line x" title="124:241	The exact match criterion is satisfied only by answer segments whose spans exactly correspond to a constituent in the CASS output." ></td>
	<td class="line x" title="125:241	2." ></td>
	<td class="line x" title="126:241	The up criterion considers an answer to match a CASS constituent if the constituent completely contains the answer and no more than three additional (non-answer) tokens." ></td>
	<td class="line x" title="127:241	3." ></td>
	<td class="line x" title="128:241	The up/dn criterion considers an answer to match a CASS constituent if it matches according to the up criterion or if the answer completely contains the constituent and no more than three additional tokens." ></td>
	<td class="line x" title="129:241	The counts for the analysis of answer segment syntactic type for fact vs. opinion questions are summarized in Table 3." ></td>
	<td class="line x" title="130:241	Results for the 15 fact questions are shown in the left half of the table, and for the 15 opinion questions in the right half." ></td>
	<td class="line x" title="131:241	The leftmost column in each half provides the question topic and number, and the second column indicates the total number of answer segments annotated for the question." ></td>
	<td class="line x" title="132:241	The next three columns show, for each of the ex, up, and up/dn matching criteria, respectively, the number of annotated answer segments that match the majority syntactic type among answer segments for that question/criterion pair." ></td>
	<td class="line x" title="133:241	Using a traditional QA architecture, the MPQA system might filter answers based on this majority type." ></td>
	<td class="line x" title="134:241	The syn type column indicates the majority syntactic type using the exact match criterion; two values in the column indicate a tie for majority syntactic type, and an empty syntactic type indicates that no answer exactly matched any of the four constituent types." ></td>
	<td class="line x" title="135:241	With only a few exceptions, the up and up/dn matching criteria agreed in majority syntactic type." ></td>
	<td class="line x" title="136:241	Results in Table 3 show a significant disparity between fact and opinion questions." ></td>
	<td class="line x" title="137:241	For fact ques926 tions, the syntactic type filter would keep 31%, 35%, or 53% of the correct answers, depending on the matching criterion." ></td>
	<td class="line x" title="138:241	For opinion questions, there is unfortunately a two-fold reduction in the percentage of correct answers that would remain after filtering  only 16%, 17% or 38%, depending on the matching criterion." ></td>
	<td class="line x" title="139:241	More importantly, the majority syntactic type among answers for fact questions is almost always a noun phrase, while no single constituent type emerges as a useful syntactic filter for opinion questions (see the syn phrase columns in Table 3)." ></td>
	<td class="line x" title="140:241	Finally, because semantic class information is generally tied to a particular syntactic category, the effectiveness of traditional semantic filters in the MPQA setting is unclear." ></td>
	<td class="line x" title="141:241	In summary, identifying answers to questions in an MPQA setting within a traditional QA architecture will be difficult." ></td>
	<td class="line x" title="142:241	First, the implicit and explicit assumptions inherent in standard linguistic filters are consistent with the characteristics of factrather than opinion-oriented QA." ></td>
	<td class="line x" title="143:241	In addition, the presence of relatively long answers and partial answers will require a much more complex answer generator than is typically present in current QA systems." ></td>
	<td class="line x" title="144:241	In Sections 6 and 7, we propose initial steps towards modifying the traditional QA architecture for use in MPQA." ></td>
	<td class="line x" title="145:241	In particular, we propose and evaluate two types of opinion filters for MPQA: subjectivity filters and opinion source filters." ></td>
	<td class="line x" title="146:241	Both types of linguistic filters rely on phrase-level and sentencelevel opinion information, which has been manually annotated for our corpus; the next section briefly describes the opinion annotation scheme." ></td>
	<td class="line x" title="147:241	5 Manual Opinion Annotations Documents in our OpQA corpus come from the larger MPQA corpus, which contains manual opinion annotations." ></td>
	<td class="line x" title="148:241	The annotation framework is described in detail in (Wiebe et al. , 2005)." ></td>
	<td class="line x" title="149:241	Here we give a high-level overview." ></td>
	<td class="line x" title="150:241	The annotation framework provides a basis for subjective expressions: expressions used to express opinions, emotions, and sentiments." ></td>
	<td class="line x" title="151:241	The framework allows for the annotation of both directly expressed private states (e.g. , afraid in the sentence John is afraid that Sue might fall,) and opinions expressed by the choice of words and style of language (e.g. , it is about time and oppression in the sentence It is about time that we end Saddams oppression)." ></td>
	<td class="line x" title="152:241	In addition, the annotations include several attributes, including the intensity (with possible values low, medium, high, and extreme) and the source of the private state." ></td>
	<td class="line x" title="153:241	The source of a private state is the person or entity who holds or experiences it." ></td>
	<td class="line x" title="154:241	6 Subjectivity Filters for MPQA Systems This section describes three subjectivity filters based on the above opinion annotation scheme." ></td>
	<td class="line x" title="155:241	Below (in Section 6.3), the filters are used to remove fact sentences from consideration when answering opinion questions, and the OpQA corpus is used to evaluate their effectiveness." ></td>
	<td class="line x" title="156:241	6.1 Manual Subjectivity Filter Much previous research on automatic extraction of opinion information performed classifications at the sentence level." ></td>
	<td class="line x" title="157:241	Therefore, we define sentence-level opinion classifications in terms of the phrase-level annotations." ></td>
	<td class="line x" title="158:241	For our gold standard of manual opinion classifications (dubbed MANUAL for the rest of the paper) we will follow Riloff and Wiebes (2003) convention (also used by Wiebe and Riloff (2005)) and consider a sentence to be opinion if it contains at least one opinion of intensity medium or higher, and to be fact otherwise." ></td>
	<td class="line x" title="159:241	6.2 Two Automatic Subjectivity Filters As discussed in section 2, several research efforts have attempted to perform automatic opinion classification on the clause and sentence level." ></td>
	<td class="line x" title="160:241	We investigate whether such information can be useful for MPQA by using the automatic sentence level opinion classifiers of Riloff and Wiebe (2003) and Wiebe and Riloff (2005)." ></td>
	<td class="line x" title="161:241	Riloff and Wiebe (2003) use a bootstrapping algorithm to perform a sentence-based opinion classification on the MPQA corpus." ></td>
	<td class="line x" title="162:241	They use a set of high precision subjectivity and objectivity clues to identify subjective and objective sentences." ></td>
	<td class="line x" title="163:241	This data is then used in an algorithm similar to AutoSlogTS (Riloff, 1996) to automatically identify a set of extraction patterns." ></td>
	<td class="line x" title="164:241	The acquired patterns are then used iteratively to identify a larger set of subjective and objective sentences." ></td>
	<td class="line x" title="165:241	In our experiments we use 927 precision recall F MPQAcorpus RULEBASED 90.4 34.2 46.6 NAIVE BAYES 79.4 70.6 74.7 Table 4: Precision, recall, and F-measure for the two classifiers." ></td>
	<td class="line x" title="166:241	the classifier that was created by the reimplementation of this bootstrapping process in Wiebe and Riloff (2005)." ></td>
	<td class="line x" title="167:241	We will use RULEBASED to denote the opinion information output by this classifier." ></td>
	<td class="line x" title="168:241	In addition, Wiebe and Riloff used the RULEBASED classifier to produce a labeled data set for training." ></td>
	<td class="line x" title="169:241	They trained a Naive Bayes subjectivity classifier on the labeled set." ></td>
	<td class="line x" title="170:241	We will use NAIVE BAYES to refer to Wiebe and Riloffs naive Bayes classifier.5 Table 4 shows the performance of the two classifiers on the MPQA corpus as reported by Wiebe and Riloff." ></td>
	<td class="line x" title="171:241	6.3 Experiments We performed two types of experiments using the subjectivity filters." ></td>
	<td class="line x" title="172:241	6.3.1 Answer rank experiments Our hypothesis motivating the first type of experiment is that subjectivity filters can improve the answer identification phase of an MPQA system." ></td>
	<td class="line x" title="173:241	We implement the IR subsystem of a traditional QA system, and apply the subjectivity filters to the IR results." ></td>
	<td class="line x" title="174:241	Specifically, for each opinion question in the corpus 6, we do the following: 1." ></td>
	<td class="line x" title="175:241	Split all documents in our corpus into sentences." ></td>
	<td class="line x" title="176:241	2." ></td>
	<td class="line x" title="177:241	Run an information retrieval algorithm7 on the set of all sentences using the question as the query to obtain a ranked list of sentences." ></td>
	<td class="line x" title="178:241	3." ></td>
	<td class="line x" title="179:241	Apply a subjectivity filter to the ranked list to remove all fact sentences from the ranked list." ></td>
	<td class="line x" title="180:241	We test each of the MANUAL, RULEBASED, and NAIVE BAYES subjectivity filters." ></td>
	<td class="line x" title="181:241	We compare the rank of the first answer to each question in the 5Specifically, the one they label Naive Bayes 1." ></td>
	<td class="line x" title="182:241	6We do not evaluate the opinion filters on the 15 fact questions." ></td>
	<td class="line x" title="183:241	Since opinion sentences are defined as containing at least one opinion of intensity medium or higher, opinion sentences can contain factual information and sentence-level opinion filters are not likely to be effective for fact-based QA." ></td>
	<td class="line x" title="184:241	7We use the Lemur toolkits standard tf.idf implementation available from http://www.lemurproject.org/." ></td>
	<td class="line x" title="185:241	Topic Qnum Baseline Manual NaiveBayes Rulebased Kyoto 5 1 1 1 1 6 5 4 4 3 7 1 1 1 1 8 1 1 1 1 Human 3 1 1 1 1 Rights 5 10 6 7 5 6 1 1 1 1 Venezuela 3 106 81 92 35 4 3 2 3 1 6 1 1 1 1 7 3 3 3 2 Mugabe 1 2 2 2 2 6 7 5 5 4 7 447 291 317 153 8 331 205 217 182 MRR: 0.4244 0.5189 0.5078 0.5856 MRFA: 61.3333 40.3333 43.7333 26.2 Table 5: Results for the subjectivity filters." ></td>
	<td class="line x" title="186:241	ranked list before the filter is applied, with the rank of the first answer to the question in the ranked list after the filter is applied." ></td>
	<td class="line x" title="187:241	Results." ></td>
	<td class="line x" title="188:241	Results for the opinion filters are compared to a simple baseline, which performs the information retrieval step with no filtering." ></td>
	<td class="line x" title="189:241	Table 5 gives the results on the 15 opinion questions for the baseline and each of the three subjectivity filters." ></td>
	<td class="line x" title="190:241	The table shows two cumulative measures  the mean reciprocal rank (MRR) 8 and the mean rank of the first answer (MRFA)." ></td>
	<td class="line x" title="191:241	9 Table 5 shows that all three subjectivity filters outperform the baseline: for all three filters, the first answer in the filtered results for all 15 questions is ranked at least as high as in the baseline." ></td>
	<td class="line x" title="192:241	As a result, the three subjectivity filters outperform the baseline in both MRR and MRFA." ></td>
	<td class="line x" title="193:241	Surprisingly, the best performing subjectivity filter is RULEBASED, surpassing the gold standard MANUAL, both in MRR (0.59 vs. 0.52) and MRFA (40.3 vs. 26.2)." ></td>
	<td class="line x" title="194:241	Presumably, the improvement in performance comes from the fact that RULEBASED identifies subjective sentences with the highest precision (and lowest recall)." ></td>
	<td class="line x" title="195:241	Thus, the RULEBASED subjectivity filter discards non-subjective sentences most aggressively." ></td>
	<td class="line x" title="196:241	6.3.2 Answer probability experiments The second experiment, answer probability, begins to explore whether opinion information can be 8The MRR is computed as the average of 1/r, where r is the rank of the first answer." ></td>
	<td class="line x" title="197:241	9MRR has been accepted as the standard performance measure in QA, since MRFA can be strongly affected by outlier questions." ></td>
	<td class="line x" title="198:241	However, the MRR score is dominated by the results in the high end of the ranking." ></td>
	<td class="line x" title="199:241	Thus, MRFA may be more appropriate for our experiments because the filters are an intermediate step in the processing, the results of which other MPQA components may improve." ></td>
	<td class="line x" title="200:241	928 sentence fact opinion Manual fact 56(46.67%) 64(53.33%) opinion 42(10.14%) 372(89.86%) question NaiveBayes fact 49(40.83%) 71(59.17%) opinion 57(13.77%) 357(86.23%) Rulebased fact 96(80.00%) 24(20.00%) opinion 184(44.44%) 230(55.56%) Table 6: Answer probability results." ></td>
	<td class="line x" title="201:241	used in an answer generator." ></td>
	<td class="line x" title="202:241	This experiment considers correspondences between (1) the classes (i.e. , opinion or fact) assigned by the subjectivity filters to the sentences containing answers, and (2) the classes of the questions the answers are responses to (according to the OpQA annotations)." ></td>
	<td class="line x" title="203:241	That is, we compute the probabilities (where ans = answer): P(ans is in a C1 sentence | ans is the answer to a C2 question) for all four combinations of C1=opinion, fact and C2=opinion, fact." ></td>
	<td class="line x" title="204:241	Results." ></td>
	<td class="line x" title="205:241	Results for the answer probability experiment are given in Table 6." ></td>
	<td class="line x" title="206:241	The rows correspond to the classes of the questions the answers respond to, and the columns correspond to the classes assigned by the subjectivity filters to the sentences containing the answers." ></td>
	<td class="line x" title="207:241	The first two rows, for instance, give the results for the MANUAL criterion." ></td>
	<td class="line x" title="208:241	MANUAL placed 56 of the answers to fact questions in fact sentences (46.67% of all answers to fact questions) and 64 (53.33%) of the answers to fact questions in opinion sentences." ></td>
	<td class="line x" title="209:241	Similarly, MANUAL placed 42 (10.14%) of the answers to opinion questions in fact sentences, and 372 (89.86%) of the answers to opinion questions in opinion sentences." ></td>
	<td class="line x" title="210:241	The answer probability experiment sheds some light on the subjectivity filter experiments." ></td>
	<td class="line x" title="211:241	All three subjectivity filters place a larger percentage of answers to opinion questions in opinion sentences than they place in fact sentences." ></td>
	<td class="line x" title="212:241	However, the different filters exhibit different degrees of discrimination." ></td>
	<td class="line x" title="213:241	Answers to opinion questions are almost always placed in opinion sentences by MANUAL (89.86%) and NAIVE BAYES (86.23%)." ></td>
	<td class="line x" title="214:241	While that aspect of their performance is excellent, MANUAL and NAIVE BAYES place more answers to fact questions in opinion rather than fact sentences (though the percentages are in the 50s)." ></td>
	<td class="line x" title="215:241	This is to be expected, because MANUAL and NAIVE BAYES are more conservative and err on the side of classifying sentences as opinions: for MANUAL, the presence of any subjective expression makes the entire sentence opinion, even if parts of the sentence are factual; NAIVE BAYES shows high recall but lower precision in recognizing opinion sentences (see Table 4)." ></td>
	<td class="line x" title="216:241	Conversely, RULEBASED places 80% of the fact answers in fact sentences and only 56% of the opinion answers in opinion sentences." ></td>
	<td class="line x" title="217:241	Again, the lower number of assignments to opinion sentences is to be expected, given the high precision and low recall of the classifier." ></td>
	<td class="line x" title="218:241	But the net result is that, for RULEBASED, the offdiagonals are all less than 50%: it places more answers to fact questions in fact rather than opinion sentences (80%), and more answers to opinion questions in opinion rather than fact sentences (56%)." ></td>
	<td class="line x" title="219:241	This is consistent with its superior performance in the subjectivity filtering experiment." ></td>
	<td class="line x" title="220:241	In addition to explaining the performance of the subjectivity filters, the answer rank experiment shows that the automatic opinion classifiers can be used directly in an answer generator module." ></td>
	<td class="line x" title="221:241	The two automatic classifiers rely on evidence in the sentence to predict the class (the information extraction patterns used by RULEBASED and the features used by NAIVE BAYES)." ></td>
	<td class="line x" title="222:241	In ongoing work we investigate ways to use this evidence to extract and summarize the opinions expressed in text, which is a task similar to that of an answer generator module." ></td>
	<td class="line x" title="223:241	7 Opinion Source Filters for MPQA Systems In addition to subjectivity filters, we also define an opinion source filter based on the manual opinion annotations." ></td>
	<td class="line x" title="224:241	This filter removes all sentences that do not have an opinion annotation with a source that matches the source of the question10." ></td>
	<td class="line x" title="225:241	For this filter we only used the MANUAL source annotations since we did not have access to automatically extracted source information." ></td>
	<td class="line x" title="226:241	We employ the same Answer Rank experiment as in 6.3.1, substituting the source filter for a subjectivity filter." ></td>
	<td class="line x" title="227:241	Results." ></td>
	<td class="line x" title="228:241	Results for the source filter are mixed." ></td>
	<td class="line x" title="229:241	The filter outperforms the baseline on some questions and performs worst on others." ></td>
	<td class="line x" title="230:241	As a result the MRR for the source filter is worse than the base10We manually identified the sources of each of the 15 opinion questions." ></td>
	<td class="line x" title="231:241	929 line (0.4633 vs. 0.4244)." ></td>
	<td class="line x" title="232:241	However, the source filter exhibits by far the best results using the MRFA measure, a value of 11.267." ></td>
	<td class="line x" title="233:241	The performance improvement is due to the filters ability to recognize the answers to the hardest questions, for which the other filters have the most trouble (questions mugabe 7 and 8)." ></td>
	<td class="line x" title="234:241	For these questions, the rank of the first answer improves from 153 to 21, and from 182 to 11, respectively." ></td>
	<td class="line x" title="235:241	With the exception of question venezuela 3, which does not contain a clear source (and is problematic altogether because there is only a single answer in the corpus and the questions qualification as opinion is not clear) the source filter always ranked an answer within the first 25 answers." ></td>
	<td class="line x" title="236:241	Thus, source filters can be especially useful in systems that rely on the presence of an answer within the first few ranked answer segments and then invoke more sophisticated analysis in the additional processing phase." ></td>
	<td class="line x" title="237:241	8 Conclusions We began by giving a high-level overview of the OpQA corpus." ></td>
	<td class="line x" title="238:241	Using the corpus, we compared the characteristics of answers to fact and opinion questions." ></td>
	<td class="line x" title="239:241	Based on the different characteristics, we surmise that traditional QA approaches may not be as effective for MPQA as they have been for fact-based QA." ></td>
	<td class="line x" title="240:241	Finally, we investigated the use of machine learning and rule-based opinion filters and showed that they can be used to guide MPQA systems." ></td>
	<td class="line x" title="241:241	Acknowledgments We would like to thank Diane Litman for her work eliciting the questions for the OpQA corpus, and the anonymous reviewers for their helpful comments.This work was supported by the Advanced Research and Development Activity (ARDA), by NSF Grants IIS-0208028 and IIS0208798, by the Xerox Foundation, and by a NSF Graduate Research Fellowship to the first author." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="I05-2030
Opinion Extraction Using a Learning-Based Anaphora Resolution Technique
Kobayashi, Nozomi;Iida, Ryu;Inui, Kentaro;Matsumoto, Yuji;"></td>
	<td class="line x" title="1:148	Opinion Extraction Using a Learning-Based Anaphora Resolution Technique Nozomi Kobayashi Ryu Iida Kentaro Inui Yuji Matsumoto Nara Institute of Science and Technology Takayama, Ikoma, Nara, 630-0192, Japan {nozomi-k,ryu-i,inui,matsu}@is.naist.jp Abstract This paper addresses the task of extracting opinions from a given document collection." ></td>
	<td class="line x" title="2:148	Assuming that an opinion can be represented as a tuple Subject, Attribute, Value, we propose a computational method to extract such tuples from texts." ></td>
	<td class="line x" title="3:148	In this method, the main task is decomposed into (a) the process of extracting Attribute-Value pairs from a given text and (b) the process of judging whether an extracted pair expresses an opinion of the author." ></td>
	<td class="line x" title="4:148	We apply machine-learning techniques to both subtasks." ></td>
	<td class="line x" title="5:148	We also report on the results of our experiments and discuss future directions." ></td>
	<td class="line x" title="6:148	1 Introduction The explosive spread of communication on the Web has attracted increasing interest in technologies for automatically mining large numbers of message boards and blog pages for opinions and recommendations." ></td>
	<td class="line x" title="7:148	Previous approaches to the task of mining a large-scale document collection for opinions can be classified into two groups: the document classification approach and the information extraction approach." ></td>
	<td class="line oc" title="8:148	In the document classification approach, researchers have been exploring techniques for classifying documents according to semantic/sentiment orientation such as positive vs. negative (e.g.(Dave et al. , 2003; Pang and Lee, 2004; Turney, 2002))." ></td>
	<td class="line x" title="10:148	The information extraction approach, on the other hand, focuses on the task of extracting elements which constitute opinions (e.g.(Kanayama and Nasukawa, 2004; Hu and Liu, 2004; Tateishi et al. , 2001))." ></td>
	<td class="line x" title="12:148	The aim of this paper is to extract opinions that represent an evaluation of a products together with the evidence." ></td>
	<td class="line x" title="13:148	To achieve this, we consider our task from the information extraction viewpoint." ></td>
	<td class="line x" title="14:148	We term the above task opinion extraction in this paper." ></td>
	<td class="line x" title="15:148	While they can be linguistically realized in many ways, opinions on a product are in fact often expressed in the form of an attribute-value pair." ></td>
	<td class="line x" title="16:148	An attribute represents one aspect of a subject and the value is a specific language expression that qualifies or quantifies the aspect." ></td>
	<td class="line x" title="17:148	Given this observation, we approach our goal by reducing the task to a general problem of extracting four-tuples Product, Attribute, Value, Evaluation from a large-scale text collection." ></td>
	<td class="line x" title="18:148	Technology for this opinion extraction task would be useful for collecting and summarizing latent opinions from the Web." ></td>
	<td class="line x" title="19:148	A straightforward application might be generation of radar charts from collected opinions as suggested by Tateishi et al.(2004)." ></td>
	<td class="line x" title="21:148	Consider an example from the automobile domain, I am very satisfied with the powerful engine (of a car)." ></td>
	<td class="line x" title="22:148	We can extract the four-tuple CAR, engine, powerful, satisfied from this sentence." ></td>
	<td class="line x" title="23:148	Note that the distinction between Value and Evaluation is not easy." ></td>
	<td class="line x" title="24:148	Many expressions used to express a Value can also be used to express an Evaluation." ></td>
	<td class="line x" title="25:148	For this reason, we do not distinguish value and evaluation, and therefore consider the task of extracting triplets Product, Attribute, Value." ></td>
	<td class="line x" title="26:148	Another problem with opinion extraction is that we want to get only subjective opinions." ></td>
	<td class="line x" title="27:148	Given this setting, the opinion extraction task can be decomposed into two subtasks: extraction of attributevalue pairs related to a product and determination of its subjectivity." ></td>
	<td class="line x" title="28:148	As we discuss in section 3, an attribute and its value may not appear in a fixed expression and may be separated." ></td>
	<td class="line x" title="29:148	In some cases, the attribute may be missing from a sentence." ></td>
	<td class="line x" title="30:148	In this respect, finding the attribute of a value is similar to finding the missing antecedent of an anaphoric expression." ></td>
	<td class="line x" title="31:148	In this paper, we discuss the similarities and differences between opinion extraction and anaphora resolution." ></td>
	<td class="line x" title="32:148	Then, we apply a machine learning-based method used for anaphora reso173 lution to the opinion extraction problem and report on our experiments conducted on a domainrestricted set of Japanese texts excerpted from review pages on the Web." ></td>
	<td class="line x" title="33:148	2 Related work In this section, we discuss previous approaches to the opinion extraction problem." ></td>
	<td class="line x" title="34:148	In the patternbased approach (Murano and Sato, 2003; Tateishi et al. , 2001), pre-defined extraction patterns and a list of evaluative expressions are used." ></td>
	<td class="line x" title="35:148	These extraction patterns and the list of evaluation expressions need to be manually created." ></td>
	<td class="line x" title="36:148	However, as is the case in information extraction, manual construction of rules may require considerable cost to provide sufficient coverage and accuracy." ></td>
	<td class="line x" title="37:148	Hu and Liu (2004) attempt to extract the attributes of target products on which customers have expressed their opinions using association mining, and to determine whether the opinions are positive or negative." ></td>
	<td class="line x" title="38:148	Their aim is quite similar to our aim, however, our work differs from theirs in that they do not identify the value corresponding to an attribute." ></td>
	<td class="line x" title="39:148	Their aim is to extract the attributes and their semantic orientations." ></td>
	<td class="line x" title="40:148	Taking the semantic parsing-based approach, Kanayama and Nasukawa (2004) apply the idea of transfer-based machine translation to the extraction of attribute-value pairs." ></td>
	<td class="line x" title="41:148	They regard the extraction task as translation from a text to a sentiment unit which consists of a sentiment value, a predicate, and its arguments." ></td>
	<td class="line x" title="42:148	Their idea is to replace the translation patterns and bilingual lexicons with sentiment expression patterns and a lexicon that specifies the polarity of expressions." ></td>
	<td class="line x" title="43:148	Their method first analyzes the predicateargument structure of a given input sentence making use of the sentence analysis component of an existing machine translation engine, and then extracts a sentiment unit from it, if any, using the transfer component." ></td>
	<td class="line x" title="44:148	One important problem the semantic parsing approach encounters is that opinion expressions often appear with anaphoric expressions and ellipses, which need to be resolved to accomplish the opinion extraction task." ></td>
	<td class="line x" title="45:148	Our investigation of an opinion-tagged Japanese corpus (described below) showed that 30% of the attribute-value pairs we found did not have a direct syntactic dependency relation within the sentence, mostly due to ellipsis." ></td>
	<td class="line x" title="46:148	For example 1, dezain-wa a hen-daga watashi-wa -ga suki-da v design a weird I [it] like v (The design is weird, but I like it.)" ></td>
	<td class="line x" title="47:148	This type of case accounted for 46 out of 100 pairs that did not have direct dependency relations." ></td>
	<td class="line x" title="48:148	To analyze predicate argument structure robustly, we have to solve this problem." ></td>
	<td class="line x" title="49:148	In the next section, we discuss the similarity between the anaphora resolution task and the opinion extraction task and propose to apply to opinion extraction a method used for anaphora resolution." ></td>
	<td class="line x" title="50:148	3 Method for opinion extraction 3.1 Analogy with anaphora resolution We consider the task of extracting opinion tuples Product, Attribute, Value from review sites and message boards on the Web dedicated to providing and exchanging information about retail goods." ></td>
	<td class="line x" title="51:148	On these Web pages, products are often specified clearly and so it is frequently a trivial job to extract the information for the Product slot." ></td>
	<td class="line x" title="52:148	We therefore in this paper focus on the problem of extracting Attribute, Value pairs." ></td>
	<td class="line x" title="53:148	In the process of attribute-value pair identification for opinion extraction, we need to deal with the following two cases: (a) both a value and its corresponding attribute appear in the text, and (b) a value appears in the text while its attribute is missing since it is inferable form the value expression and the context." ></td>
	<td class="line x" title="54:148	The upper half of Figure 1 illustrates these two cases in the automobile domain." ></td>
	<td class="line x" title="55:148	In (b), the writer is talking about the size of the car, but the expression size is not explicitly mentioned in the text." ></td>
	<td class="line x" title="56:148	In addition, (b) includes the case where the writer evaluates the product itself." ></td>
	<td class="line x" title="57:148	For example, Im very satisfied with my car!: in this case, a value expression satisfied evaluates the product as a whole, therefore a corresponding attribute does not exists." ></td>
	<td class="line x" title="58:148	For the case (a), we first identify a value expression (like in Figure 1) in a given text and then look for the corresponding attribute in the text." ></td>
	<td class="line x" title="59:148	Since we also see the case (b), on the other hand, we additionally need to consider the problem of whether the corresponding attribute of the identified value expression appears in the text or not." ></td>
	<td class="line x" title="60:148	The structure of these problems is analogous to that of anaphora resolution; namely, there are exactly two cases in anaphora resolution that have a clear correspondence with the above two cases as illustrated in Figure 1: in (a) the noun phrase (NP) is anaphoric; namely, the NPs antecedent appears in the text, and in (b) the noun phrase is non-anaphoric." ></td>
	<td class="line x" title="61:148	A non-anaphoric NP is either ex1  a denotes the word sequence corresponding to the Attribute." ></td>
	<td class="line x" title="62:148	Likewise, we also use  v for the Value." ></td>
	<td class="line x" title="63:148	174 Taro-wa shisetsu-wog17128g15458-gag17129shirabe-te houkokusho-o sakusei-shita (a) (b) Dezain-wa hen-desuga watashi-wa g17128g15458-gag17129 suki-desu g16877g16877g16877g16877g16877 (g15458-ga) Ookii-kedo atsukai-yasui ( it ) large but easy to handle (a) (b) anaphora resolution opinion extraction anaphorantecedent Attribute Value (The design is weird, but I like it.)" ></td>
	<td class="line x" title="64:148	omitted Attribute (It is large, but easy to handle) Tar-NOM attendance-ACC noted report-ACC wrote (Taro noted the attendance and wrote a report.)" ></td>
	<td class="line x" title="65:148	design-NOM weird I-NOM ( it ) like Value Onaka-ga hetta-node kaerouto (g15458-ga) omou hungry go home (I) exophora anaphor (I think Ill go home because Im hungry.)" ></td>
	<td class="line x" title="66:148	Figure 1: Similarity between opinion extraction and anaphora resolution ophoric (i.e. the NP has an implicit referent) or indefinite." ></td>
	<td class="line x" title="67:148	While the figure shows Japanese examples, the similarity between anaphora resolution and opinion extraction is language independent." ></td>
	<td class="line x" title="68:148	This analogy naturally leads us to think of applying existing techniques for anaphora resolution to our opinion extraction task since anaphora resolution has been studied for a considerably longer period in a wider range of disciplines as we briefly review below." ></td>
	<td class="line x" title="69:148	3.2 Existing techniques for anaphora resolution Corpus-based empirical approaches to anaphora resolution have been reasonably successful." ></td>
	<td class="line x" title="70:148	This approach, as exemplified by (Soon et al. , 2001; Iida et al. , 2003; Ng, 2004), is cost effective, while achieving a better performance than the best-performing rule-based systems for the test sets of MUC-6 and MUC-7 2." ></td>
	<td class="line x" title="71:148	As suggested by Figure 1, anaphora resolution can be decomposed into two subtasks: anaphoricity determination and antecedent identification." ></td>
	<td class="line x" title="72:148	Anaphoricity determination is the task of judging whether a given NP is anaphoric or nonanaphoric." ></td>
	<td class="line x" title="73:148	Recent research advances have provided several important findings as follows:  Learning-based methods for antecedent identification can also benefit from the use of linguistic clues inspired by Centering Theory (Grosz et al. , 1995)." ></td>
	<td class="line x" title="74:148	 One useful clue for anaphoricity determination is the availability of a plausible candidate for the antecedent." ></td>
	<td class="line x" title="75:148	If an appropriate candidate for the antecedent is found in the preceding discourse context, the NP is likely to be anaphoric." ></td>
	<td class="line x" title="76:148	For these reasons, an anaphora resolution model performs best if it carries out the following pro2 The 7th Message Understanding Conference (1998): www.itl.nist.gov/iaui/894.02/related projects/muc/ interia seki Dezain-wa hen-desuga watashi-wa suki-desu g16877g16877g16877 interior seat design-NOM weird I-NOM like candidates design like interior like seat like design like candidate attributes real attribute Select the best candidate attribute Decide whether the candidate attribute stands for the real attribute or not design likedesign like real attribute pairedness determination attribute identification opinionhood determination Judge whether the pair expresses an opinion or not opinion Attribute dictionary Value dictionary interior seat design like good ." ></td>
	<td class="line x" title="77:148	target value initialization pair extraction Figure 2: Process of opinion extraction cess in the given order (Iida et al. , 2005): (1) Antecedent identification: Given an NP, identify the best candidate antecedent for it, and (2) Anaphoricity determination: Judge whether the candidate really stands for the true antecedent of the NP." ></td>
	<td class="line x" title="78:148	3.3 An opinion extraction model inspired by analogy with anaphora resolution As illustrated in Figure 2, an opinion extraction model derived from the aforementioned analogy with anaphora resolution as follows: 1." ></td>
	<td class="line x" title="79:148	Initialization: Identify attribute and value candidates by dictionary lookup 2." ></td>
	<td class="line x" title="80:148	Attribute identification: Select a value and identify the best candidate attribute corresponding to the value 3." ></td>
	<td class="line x" title="81:148	Pairedness determination: Decide whether the candidate attribute stands for the real attribute of the value or not (i.e. the value has no explicit corresponding attribute in the text) 4." ></td>
	<td class="line x" title="82:148	Opinionhood determination: Judge whether the obtained attribute-value pair 3 expresses an opinion or not Here, the attribute identification and pairedness determination processes respectively correspond to the antecedent identification and anaphoricity determination processes in anaphora resolution." ></td>
	<td class="line x" title="83:148	Note that our opinion extraction task requires an additional subtask, opinionhood determination  an attribute-value pair appearing in a text does not necessarily constitute an opinion." ></td>
	<td class="line x" title="84:148	We elaborate on the notion of opinionhood in section 4.1." ></td>
	<td class="line x" title="85:148	From the above discussion, we can expect that the findings for anaphora resolution mentioned in 3.2 stated above apply to opinion extraction as well." ></td>
	<td class="line x" title="86:148	In fact, the information about the candidate 3 For simplicity, we call a value both with and without an attribute uniformly by the term attribute-value pair unless the distinction is important." ></td>
	<td class="line x" title="87:148	175 attribute is likely to be useful for pairedness determination." ></td>
	<td class="line x" title="88:148	We therefore expect that carrying out attribute identification before pairedness determination should outperform the counterpart model which executes the two subtasks in the reversed order." ></td>
	<td class="line x" title="89:148	The same analogy also applies to opinionhood determination; namely, we expect that opinion determination is bet performed after attribute determination." ></td>
	<td class="line x" title="90:148	Furthermore, our opinion extraction model also can be implemented in a totally machine learning-based fashion." ></td>
	<td class="line x" title="91:148	4 Evaluation We conducted experiments with Japanese Web documents to empirically evaluate the performance of our opinion extraction model, focusing particularly on the validity of the analogy discussed in the previous section." ></td>
	<td class="line x" title="92:148	4.1 Opinionhood In these experiments, we define an opinion as follows: An opinion is a description that expresses the writers subjective evaluation of a particular subject or a certain aspect of it." ></td>
	<td class="line x" title="93:148	By this definition, we exclude requests, factual or counter-factual descriptions and hearsay evidence from our target opinions." ></td>
	<td class="line x" title="94:148	For example, The engine is powerful is an opinion, while a counterfactual sentence such as If only the engine were more powerful is not regarded as opinion." ></td>
	<td class="line x" title="95:148	4.2 Opinion-tagged corpus We created an opinion-tagged Japanese corpus consisting of 288 review articles in the automobile domain (4,442 sentences)." ></td>
	<td class="line x" title="96:148	While it is not easy to judge whether an expression is a value or an attribute, we asked the annotator to identify attribute and value expressions according to their subjective judgment." ></td>
	<td class="line x" title="97:148	If some attributes are in a hierarchical relation with each other, we asked the annotator to choose the attribute lowest in the hierarchy as the attribute of the value." ></td>
	<td class="line x" title="98:148	For example, in a sound system with poor sound, only sound is annotated as the attribute of the value poor." ></td>
	<td class="line x" title="99:148	The corpus contains 2,191 values with an attribute and 420 values without an attribute." ></td>
	<td class="line x" title="100:148	Most of the attributes appear in the same sentence as their corresponding values or in the immediately preceding sentence (99% of the total number of pairs)." ></td>
	<td class="line x" title="101:148	Therefore, we extract attributes and their corresponding values from the same sentence or from the preceding sentence." ></td>
	<td class="line x" title="102:148	4.3 Experimental method As preprocessing, we analyzed the opiniontagged corpus using the Japanese morphological analyzer ChaSen 4 and the Japanese dependency structure analyzer CaboCha 5 . We used Support Vector Machines to train the models for attribute identification, pairedness determination and opinionhood determination." ></td>
	<td class="line x" title="103:148	We used the 2nd order polynomial kernel as the kernel function for SVMs." ></td>
	<td class="line x" title="104:148	Evaluation was performed by 10-fold cross validation using all the data." ></td>
	<td class="line x" title="105:148	4.3.1 Dictionaries We use dictionaries for identification of attribute and value candidates." ></td>
	<td class="line x" title="106:148	We constructed a attribute dictionary and a value dictionary from review articles about automobiles (230,000 sentences in total) using the semi-automatic method proposed by Kobayashi et al.(2004)." ></td>
	<td class="line x" title="108:148	The data used in this process was different from the opinion-tagged corpus." ></td>
	<td class="line x" title="109:148	Furthermore, we added to the dictionaries expressions which frequently appearing in the opinion-tagged corpus." ></td>
	<td class="line x" title="110:148	The final size of the dictionaries was 3,777 attribute expressions and 3,950 value expressions." ></td>
	<td class="line x" title="111:148	4.3.2 Order of model application To examine the effects of appropriately choosing the order of model application we mentioned in the previous section, we conducted four experiments using different orders (AI indicates attribute identification, PD indicates pairedness determination and OD indicates opinion determination): Proc.1: ODPDAI, Proc.2: ODAIPD Proc.3: AIODPD, Proc.4: AIPDOD Note that Proc.4 is our proposed ordering." ></td>
	<td class="line x" title="112:148	In addition to these models, we adopted a baseline model." ></td>
	<td class="line x" title="113:148	In this model, if the candidate value and a candidate attribute are connected via a dependency relation, the candidate value is judged to have an attribute." ></td>
	<td class="line x" title="114:148	When none of the candidate attributes have a dependency relation, the candidate value is judged not to have an attribute." ></td>
	<td class="line x" title="115:148	We adopted the tournament model for attribute identification (Iida et al. , 2003)." ></td>
	<td class="line x" title="116:148	This model implements a pairwise comparison (i.e. a match) between two candidates in reference to the given value treating it as a binary classification problem, and conducting a tournament which consists of a series of matches, in which the one that prevails through to the final round is declared the 4 http://chasen.naist.jp/ 5 http://chasen.org/taku/software/cabocha/ 176 winner, namely, it is identified as the most likely candidate attribute." ></td>
	<td class="line x" title="117:148	Each of the matches is conducted as a binary classification task in which one or other of the candidate wins." ></td>
	<td class="line x" title="118:148	The pairedness determination task and the opinionhood determination task are also binary classification tasks." ></td>
	<td class="line x" title="119:148	In Proc.1, since pair identification is conducted before finding the best candidate attribute, we used Soon et al.s model (Soon et al. , 2001) for pairedness determination." ></td>
	<td class="line x" title="120:148	This model picks up each possible candidate attribute for a value and determines if it is the attribute for that value." ></td>
	<td class="line x" title="121:148	If all the candidates are determined not to be the attribute, the value is judged not to have an attribute." ></td>
	<td class="line x" title="122:148	In Proc.4, we can use the information about whether the value has a corresponding attribute or not for opinionhood determination." ></td>
	<td class="line x" title="123:148	We therefore create two separate models for when the value does and does not have an attribute." ></td>
	<td class="line x" title="124:148	4.3.3 Features We extracted the following two types of features from the candidate attribute and the candidate value: (a) surface spelling and part-of-speech of the target value expression, as well as those of its dependent phrase and those in its depended phrase(s) (b) relation between the target value and candidate attribute (distance between them, existence of dependency, existence of a cooccurrence relation) We extracted (b) if the model could use both the attribute and the value information." ></td>
	<td class="line x" title="125:148	Existence of a co-occurrence relation is determined by reference to a predefined co-occurrence list that contains attribute-value pair information such as height of vehicle  low." ></td>
	<td class="line x" title="126:148	We created the list from the 230,000 sentences described in section 4.3.1 by applying the attribute and value dictionary and extracting attribute-value pairs if there is a dependency relation between the attribute and the value." ></td>
	<td class="line x" title="127:148	The number of pairs we extracted was about 48,000." ></td>
	<td class="line x" title="128:148	4.4 Results Table 1 shows the results of opinion extraction." ></td>
	<td class="line x" title="129:148	We evaluated the results by recall R and precision P defined as follows (For simplicity, we substitute A-V for attribute-value pair): R = correctly extracted A-V opinions total number of A-V opinions, P = correctly extracted A-V opinions total number of A-V opinions found by the system . In order to demonstrate the effectiveness of the information about the candidate attribute, we evaluated the results of pair extraction and opinionhood determination separately." ></td>
	<td class="line x" title="130:148	Table 2 shows the results." ></td>
	<td class="line x" title="131:148	In the pair extraction, we assume that the value is given, and evaluate how successfully attribute-value pairs are extracted." ></td>
	<td class="line x" title="132:148	4.5 Discussions As Table 1 shows, our proposed ordering is outperformed on the recall in Proc.3, however, the precision is higher than Proc.3 and get the best Fmeasure." ></td>
	<td class="line x" title="133:148	In what follows, we discuss the results of pair extraction and opinionhood determination." ></td>
	<td class="line x" title="134:148	Pair extraction From Table 2, we can see that carrying out attribute identification before pairedness determination outperforms the reverse ordering by 11% better precision and 3% better recall." ></td>
	<td class="line x" title="135:148	This result supports our expectation that knowledge of attribute information assists attributevalue pair extraction." ></td>
	<td class="line x" title="136:148	Focusing on the rows labeled (dependency) and (no dependency) in Table 2, while 80% of the attribute-value pairs in a direct dependency relation are successfully extracted with high precision, the model achieves only 51.7% recall with 61.7% precision for the cases where an attribute and value are not in a direct dependency relation." ></td>
	<td class="line x" title="137:148	According to our error analysis, a major source of errors lies in the attribute identification task." ></td>
	<td class="line x" title="138:148	In this experiment, the precision of attribute identification is 78%." ></td>
	<td class="line x" title="139:148	A major reason for this problem was that the true attributes did not exist in our dictionary." ></td>
	<td class="line x" title="140:148	In addition, a major cause of error in the pair determination stage is cases where an attribute appearing in the preceding sentence causes a false decision." ></td>
	<td class="line x" title="141:148	We need to conduct further investigations in order to resolve these problems." ></td>
	<td class="line x" title="142:148	Opinionhood determination Table 2 also shows that carrying out attribute identification followed by opinionhood determination outperforms the reverse ordering, which supports our expectation that knowing the attribute information aids opinionhood determination." ></td>
	<td class="line x" title="143:148	While it produces better results, our proposed method still has room for improvement in both precision and recall." ></td>
	<td class="line x" title="144:148	Our current error analysis has not identified particular error patterns  the types of errors are very diverse." ></td>
	<td class="line x" title="145:148	However, we need to at least address the issue of modifying the feature set to make the model more sensitive to modality-oriented distinctions such as subjunctive and conditional expressions." ></td>
	<td class="line x" title="146:148	177 Table 1: The precision and the recall for opinion extraction procedure value with attribute value without attribute attribute-value pairs baseline precision 60.5% (1130/1869) 10.6% (249/2340) 32.8% (1379/4209) recall 51.6% (1130/2191) 59.3% (249/420) 52.8% (1379/2611) F-measure 55.7 21.0 40.5 Proc.1 precision 47.3% (864/1828) 21.6% ( 86/399) 42.7% ( 950/2227) recall 39.4% (864/2191) 20.5% ( 86/420) 36.4% ( 950/2611) F-measure 43.0 21.0 39.3 Proc.2 precision 63.0% (1074/1706) 38.0% (198/521) 57.1% (1272/2227) recall 49.0% (1074/2191) 47.1% (198/420) 48.7% (1272/2611) F-measure 55.1 42.0 52.6 Proc.3 precision 74.9% (1277/1632) 29.1% (151/519) 63.8% (1373/2151) recall 55.8% (1222/2191) 36.0% (151/420) 52.6% (1373/2611) F-measure 64.0 32.2 57.7 Proc.4 precision 80.5% (1175/1460) 30.2% (150/497) 67.7% (1325/1957) recall 53.6% (1175/2191) 35.7% (150/420) 50.7% (1325/2611) F-measure 64.4 32.7 58.0 Table 2: The result of pair extraction and opinionhood determination procedure precision recall pair extraction baseline (dependency) 71.1% (1385/1929) 63.2% (1385/2191) PDAI 65.3% (1579/2419) 72.1% (1579/2191) AIPD 76.6% (1645/2148) 75.1% (1645/2191) (dependency) 87.7% (1303/1486) 79.6% (1303/1637) (no dependency) 51.7% ( 342/ 662) 61.7% ( 342/ 554) opinionhood determination OD 74.0% (1554/2101) 60.2% (1554/2581) AIOD 82.2% (1709/2078) 66.2% (1709/2581) 5 Conclusion In this paper, we have proposed a machine learning-based method for the extraction of opinions on consumer products by reducing the problem to that of extracting attribute-value pairs from texts." ></td>
	<td class="line x" title="147:148	We have pointed out the similarity between the tasks of anaphora resolution and opinion extraction, and have applied the machine learningbased method designed for anaphora resolution to opinion extraction." ></td>
	<td class="line x" title="148:148	The experimental results reported in this paper show that identifying the corresponding attribute for a given value expression is effective in both pairedness determination and opinionhood determination." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P05-1015
Seeing Stars: Exploiting Class Relationships For Sentiment Categorization With Respect To Rating Scales
Pang, Bo;Lee, Lillian;"></td>
	<td class="line x" title="1:189	Proceedings of the 43rd Annual Meeting of the ACL, pages 115124, Ann Arbor, June 2005." ></td>
	<td class="line x" title="2:189	c2005 Association for Computational Linguistics Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales Bo Panga0a2a1a3 and Lillian Leea0a2a1a4a5a1a3 (1) Department of Computer Science, Cornell University (2) Language Technologies Institute, Carnegie Mellon University (3) Computer Science Department, Carnegie Mellon University Abstract We address the rating-inference problem, wherein rather than simply decide whether a review is thumbs up or thumbs down, as in previous sentiment analysis work, one must determine an authors evaluation with respect to a multi-point scale (e.g. , one to ve stars )." ></td>
	<td class="line x" title="3:189	This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, three stars is intuitively closer to four stars than to one star." ></td>
	<td class="line x" title="4:189	We rst evaluate human performance at the task." ></td>
	<td class="line x" title="5:189	Then, we apply a metaalgorithm, based on a metric labeling formulation of the problem, that alters a given a6 -ary classi ers output in an explicit attempt to ensure that similar items receive similar labels." ></td>
	<td class="line x" title="6:189	We show that the meta-algorithm can provide signi cant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem." ></td>
	<td class="line x" title="7:189	1 Introduction There has recently been a dramatic surge of interest in sentiment analysis, as more and more people become aware of the scienti c challenges posed and the scope of new applications enabled by the processing of subjective language." ></td>
	<td class="line x" title="8:189	(The papers collected by Qu, Shanahan, and Wiebe (2004) form a representative sample of research in the area)." ></td>
	<td class="line x" title="9:189	Most prior work on the speci c problem of categorizing expressly opinionated text has focused on the binary distinction of positive vs. negative (Turney, 2002; Pang, Lee, and Vaithyanathan, 2002; Dave, Lawrence, and Pennock, 2003; Yu and Hatzivassiloglou, 2003)." ></td>
	<td class="line x" title="10:189	But it is often helpful to have more information than this binary distinction provides, especially if one is ranking items by recommendation or comparing several reviewers opinions: example applications include collaborative ltering and deciding which conference submissions to accept." ></td>
	<td class="line x" title="11:189	Therefore, in this paper we consider generalizing to ner-grained scales: rather than just determine whether a review is thumbs up or not, we attempt to infer the authors implied numerical rating, such as three stars or four stars . Note that this differs from identifying opinion strength (Wilson, Wiebe, and Hwa, 2004): rants and raves have the same strength but represent opposite evaluations, and referee forms often allow one to indicate that one is very con dent (high strength) that a conference submission is mediocre (middling rating)." ></td>
	<td class="line x" title="12:189	Also, our task differs from ranking not only because one can be given a single item to classify (as opposed to a set of items to be ordered relative to one another), but because there are settings in which classi cation is harder than ranking, and vice versa." ></td>
	<td class="line x" title="13:189	One can apply standarda6 -ary classi ers or regression to this rating-inference problem; independent work by Koppel and Schler (2005) considers such 115 methods." ></td>
	<td class="line x" title="14:189	But an alternative approach that explicitly incorporates information about item similarities together with label similarity information (for instance, one star is closer to two stars than to four stars ) is to think of the task as one of metric labeling (Kleinberg and Tardos, 2002), where label relations are encoded via a distance metric." ></td>
	<td class="line x" title="15:189	This observation yields a meta-algorithm, applicable to both semi-supervised (via graph-theoretic techniques) and supervised settings, that alters a given a6 -ary classi ers output so that similar items tend to be assigned similar labels." ></td>
	<td class="line x" title="16:189	In what follows, we rst demonstrate that humans can discern relatively small differences in (hidden) evaluation scores, indicating that rating inference is indeed a meaningful task." ></td>
	<td class="line x" title="17:189	We then present three types of algorithms one-vs-all, regression, and metric labeling that can be distinguished by how explicitly they attempt to leverage similarity between items and between labels." ></td>
	<td class="line x" title="18:189	Next, we consider what item similarity measure to apply, proposing one based on the positive-sentence percentage." ></td>
	<td class="line x" title="19:189	Incorporating this new measure within the metriclabeling framework is shown to often provide signi cant improvements over the other algorithms." ></td>
	<td class="line x" title="20:189	We hope that some of the insights derived here might apply to other scales for text classifcation that have been considered, such as clause-level opinion strength (Wilson, Wiebe, and Hwa, 2004); affect types like disgust (Subasic and Huettner, 2001; Liu, Lieberman, and Selker, 2003); reading level (Collins-Thompson and Callan, 2004); and urgency or criticality (Horvitz, Jacobs, and Hovel, 1999)." ></td>
	<td class="line x" title="21:189	2 Problem validation and formulation We rst ran a small pilot study on human subjects in order to establish a rough idea of what a reasonable classi cation granularity is: if even people cannot accurately infer labels with respect to a ve-star scheme with half stars, say, then we cannot expect a learning algorithm to do so." ></td>
	<td class="line x" title="22:189	Indeed, some potential obstacles to accurate rating inference include lack of calibration (e.g. , what an understated author intends as high praise may seem lukewarm), author inconsistency at assigning ne-grained ratings, and Rating diff." ></td>
	<td class="line x" title="23:189	Pooled Subject 1 Subject 2 a7 or more 100% 100% (35) 100% (15) 2 (e.g. , 1 star) 83% 77% (30) 100% (11) 1 (e.g. , a0a4 star) 69% 65% (57) 90% (10) 0 55% 47% (15) 80% ( 5) Table 1: Human accuracy at determining relative positivity." ></td>
	<td class="line x" title="24:189	Rating differences are given in notches . Parentheses enclose the number of pairs attempted." ></td>
	<td class="line x" title="25:189	ratings not entirely supported by the text1." ></td>
	<td class="line x" title="26:189	For data, we rst collected Internet movie reviews in English from four authors, removing explicit rating indicators from each documents text automatically." ></td>
	<td class="line x" title="27:189	Now, while the obvious experiment would be to ask subjects to guess the rating that a review represents, doing so would force us to specify a xed rating-scale granularity in advance." ></td>
	<td class="line x" title="28:189	Instead, we examined peoples ability to discern relative differences, because by varying the rating differences represented by the test instances, we can evaluate multiple granularities in a single experiment." ></td>
	<td class="line x" title="29:189	Speci cally, at intervals over a number of weeks, we authors (a non-native and a native speaker of English) examined pairs of reviews, attemping to determine whether the rst review in each pair was (1) more positive than, (2) less positive than, or (3) as positive as the second." ></td>
	<td class="line x" title="30:189	The texts in any particular review pair were taken from the same author to factor out the effects of cross-author divergence." ></td>
	<td class="line x" title="31:189	As Table 1 shows, both subjects performed perfectly when the rating separation was at least 3 notches in the original scale (we de ne a notch as a half star in a fouror ve-star scheme and 10 points in a 100-point scheme)." ></td>
	<td class="line x" title="32:189	Interestingly, although human performance drops as rating difference decreases, even at a one-notch separation, both subjects handily outperformed the random-choice baseline of 33%." ></td>
	<td class="line x" title="33:189	However, there was large variation in accuracy between subjects.2 1For example, the critic Dennis Schwartz writes that sometimes the review itself [indicates] the letter grade should have been higher or lower, as the review might fail to take into consideration my overall impression of the lm which I hope to capture in the grade (http://www.sover.net/ ozus/cinema.htm)." ></td>
	<td class="line x" title="34:189	2One contributing factor may be that the subjects viewed disjoint document sets, since we wanted to maximize experimental coverage of the types of document pairs within each difference class." ></td>
	<td class="line x" title="35:189	We thus cannot report inter-annotator agreement, 116 Because of this variation, we de ned two different classi cation regimes." ></td>
	<td class="line x" title="36:189	From the evidence above, a three-class task (categories 0, 1, and 2 essentially negative, middling, and positive, respectively) seems like one that most people would do quite well at (but we should not assume 100% human accuracy: according to our one-notch results, people may misclassify borderline cases like 2.5 stars)." ></td>
	<td class="line x" title="37:189	Our study also suggests that people could do at least fairly well at distinguishing full stars in a zeroto four-star scheme." ></td>
	<td class="line x" title="38:189	However, when we began to construct ve-category datasets for each of our four authors (see below), we found that in each case, either the most negative or the most positive class (but not both) contained only about 5% of the documents." ></td>
	<td class="line x" title="39:189	To make the classes more balanced, we folded these minority classes into the adjacent class, thus arriving at a four-class problem (categories 0-3, increasing in positivity)." ></td>
	<td class="line x" title="40:189	Note that the four-class problem seems to offer more possibilities for leveraging class relationship information than the three-class setting, since it involves more class pairs." ></td>
	<td class="line x" title="41:189	Also, even the two-category version of the rating-inference problem for movie reviews has proven quite challenging for many automated classi cation techniques (Pang, Lee, and Vaithyanathan, 2002; Turney, 2002)." ></td>
	<td class="line x" title="42:189	We applied the above two labeling schemes to a scale dataset3 containing four corpora of movie reviews." ></td>
	<td class="line pc" title="43:189	All reviews were automatically preprocessed to remove both explicit rating indicators and objective sentences; the motivation for the latter step is that it has previously aided positive vs. negative classi cation (Pang and Lee, 2004)." ></td>
	<td class="line x" title="44:189	All of the 1770, 902, 1307, or 1027 documents in a given corpus were written by the same author." ></td>
	<td class="line x" title="45:189	This decision facilitates interpretation of the results, since it factors out the effects of different choices of methods for calibrating authors scales.4 We point out that but since our goal is to recover a reviewers true recommendation, reader-author agreement is more relevant." ></td>
	<td class="line x" title="46:189	While another factor might be degree of English uency, in an informal experiment (six subjects viewing the same three pairs), native English speakers made the only two errors." ></td>
	<td class="line x" title="47:189	3Available at http://www.cs.cornell.edu/People/pabo/moviereview-data as scale dataset v1.0." ></td>
	<td class="line x" title="48:189	4From the Rotten Tomatoes websites FAQ: star systems are not consistent between critics." ></td>
	<td class="line x" title="49:189	For critics like Roger Ebert and James Berardinelli, 2.5 stars or lower out of 4 stars is always negative." ></td>
	<td class="line x" title="50:189	For other critics, 2.5 stars can either be positive it is possible to gather author-speci c information in some practical applications: for instance, systems that use selected authors (e.g. , the Rotten Tomatoes movie-review website where, we note, not all authors provide explicit ratings) could require that someone submit rating-labeled samples of newlyadmitted authors work." ></td>
	<td class="line x" title="51:189	Moreover, our results at least partially generalize to mixed-author situations (see Section 5.2)." ></td>
	<td class="line x" title="52:189	3 Algorithms Recall that the problem we are considering is multicategory classi cation in which the labels can be naturally mapped to a metric space (e.g. , points on a line); for simplicity, we assume the distance metric a8a10a9a12a11a14a13a15a11a17a16a19a18a21a20a23a22a11a25a24a26a11a17a16a27a22 throughout." ></td>
	<td class="line x" title="53:189	In this section, we present three approaches to this problem in order of increasingly explicit use of pairwise similarity information between items and between labels." ></td>
	<td class="line x" title="54:189	In order to make comparisons between these methods meaningful, we base all three of them on Support Vector Machines (SVMs) as implemented in Joachims (1999) a28a30a29a32a31a34a33a36a35a38a37a40a39a42a41 package." ></td>
	<td class="line x" title="55:189	3.1 One-vs-all The standard SVM formulation applies only to binary classi cation." ></td>
	<td class="line x" title="56:189	One-vs-all (OVA) (Rifkin and Klautau, 2004) is a common extension to the a6 -ary case." ></td>
	<td class="line x" title="57:189	Training consists of building, for each label a11, an SVM binary classi er distinguishing label a11 from not-a11 . We consider the nal output to be a label preference function a43a45a44a27a46a2a47 a9a49a48a50a13a15a11a51a18, de ned as the signed distance of (test) item a48 to the a11 side of the a11 vs. not-a11 decision plane." ></td>
	<td class="line x" title="58:189	Clearly, OVA makes no explicit use of pairwise label or item relationships." ></td>
	<td class="line x" title="59:189	However, it can perform well if each class exhibits suf ciently distinct language; see Section 4 for more discussion." ></td>
	<td class="line x" title="60:189	3.2 Regression Alternatively, we can take a regression perspective by assuming that the labels come from a discretization of a continuous function a52 mapping from the or negative." ></td>
	<td class="line x" title="61:189	Even though Eric Lurio uses a 5 star system, his grading is very relaxed." ></td>
	<td class="line x" title="62:189	So, 2 stars can be positive." ></td>
	<td class="line x" title="63:189	Thus, calibration may sometimes require strong familiarity with the authors involved, as anyone who has ever needed to reconcile con icting referee reports probably knows." ></td>
	<td class="line x" title="64:189	117 feature space to a metric space.5 If we choose a52 from a family of suf ciently gradual functions, then similar items necessarily receive similar labels." ></td>
	<td class="line x" title="65:189	In particular, we consider linear, a53 -insensitive SVM regression (Vapnik, 1995; Smola and Schcurrency1olkopf, 1998); the idea is to nd the hyperplane that best ts the training data, but where training points whose labels are within distance a53 of the hyperplane incur no loss." ></td>
	<td class="line x" title="66:189	Then, for (test) instance a48, the label preference function a43a55a54a12a56a58a57 a9a49a48a59a13a15a11a60a18 is the negative of the distance between a11 and the value predicted for a48 by the tted hyperplane function." ></td>
	<td class="line x" title="67:189	Wilson, Wiebe, and Hwa (2004) used SVM regression to classify clause-level strength of opinion, reporting that it provided lower accuracy than other methods." ></td>
	<td class="line x" title="68:189	However, independently of our work, Koppel and Schler (2005) found that applying linear regression to classify documents (in a different corpus than ours) with respect to a three-point rating scale provided greater accuracy than OVA SVMs and other algorithms." ></td>
	<td class="line x" title="69:189	3.3 Metric labeling Regression implicitly encodes the similar items, similar labels heuristic, in that one can restrict consideration to gradual functions." ></td>
	<td class="line x" title="70:189	But we can also think of our task as a metric labeling problem (Kleinberg and Tardos, 2002), a special case of the maximum a posteriori estimation problem for Markov random elds, to explicitly encode our desideratum." ></td>
	<td class="line x" title="71:189	Suppose we have an initial label preference function a43 a9a49a48a50a13a15a11a51a18, perhaps computed via one of the two methods described above." ></td>
	<td class="line x" title="72:189	Also, let a8 be a distance metric on labels, and let a6a55a6a62a61 a9a49a48a63a18 denote the a64 nearest neighbors of item a48 according to some item-similarity function a65a5a66a12a67 . Then, it is quite natural to pose our problem as nding a mapping of instances a48 to labels a11a60a68 (respecting the original labels of the training instances) that minimizes a69 a68a14a70 test a71a72 a24 a43 a9a49a48a50a13a15a11 a68 a18a45a73a26a74 a69 a75 a70a77a76a78a76a80a79a42a81a82a68a51a83a78a84 a9a85a8a86a9a12a11 a68 a13a15a11 a75 a18a2a18 a65a5a66a12a67 a9a49a48a50a13a2a87a88a18a27a89a90a91a13 where a84 is monotonically increasing (we chose a84 a9a85a8a92a18a93a20a94a8 unless otherwise speci ed) and a74 is a trade-off and/or scaling parameter." ></td>
	<td class="line x" title="73:189	(The inner summation is familiar from work in locally-weighted 5We discuss the ordinal regression variant in Section 6." ></td>
	<td class="line x" title="74:189	learning6 (Atkeson, Moore, and Schaal, 1997))." ></td>
	<td class="line x" title="75:189	In a sense, we are using explicit item and label similarity information to increasingly penalize the initial classi er as it assigns more divergent labels to similar items." ></td>
	<td class="line x" title="76:189	In this paper, we only report supervised-learning experiments in which the nearest neighbors for any given test item were drawn from the training set alone." ></td>
	<td class="line x" title="77:189	In such a setting, the labeling decisions for different test items are independent, so that solving the requisite optimization problem is simple." ></td>
	<td class="line x" title="78:189	Aside: transduction The above formulation also allows for transductive semi-supervised learning as well, in that we could allow nearest neighbors to come from both the training and test sets." ></td>
	<td class="line x" title="79:189	We intend to address this case in future work, since there are important settings in which one has a small number of labeled reviews and a large number of unlabeled reviews, in which case considering similarities between unlabeled texts could prove quite helpful." ></td>
	<td class="line x" title="80:189	In full generality, the corresponding multi-label optimization problem is intractable, but for many families of a84 functions (e.g. , convex) there exist practical exact or approximation algorithms based on techniques for nding minimum s-t cuts in graphs (Ishikawa and Geiger, 1998; Boykov, Veksler, and Zabih, 1999; Ishikawa, 2003)." ></td>
	<td class="line pc" title="81:189	Interestingly, previous sentiment analysis research found that a minimum-cut formulation for the binary subjective/objective distinction yielded good results (Pang and Lee, 2004)." ></td>
	<td class="line x" title="82:189	Of course, there are many other related semi-supervised learning algorithms that we would like to try as well; see Zhu (2005) for a survey." ></td>
	<td class="line x" title="83:189	4 Class struggle: nding a label-correlated item-similarity function We need to specify an item similarity function a65a95a66a49a67 to use the metric-labeling formulation described in Section 3.3." ></td>
	<td class="line x" title="84:189	We could, as is commonly done, employ a term-overlap-based measure such as the cosine between term-frequency-based document vectors (henceforth TO(cos) )." ></td>
	<td class="line x" title="85:189	However, Table 2 6If we ignore the a96a98a97a100a99a92a101a49a102a15a103 term, different choices of a104 correspond to different versions of nearest-neighbor learning, e.g., majority-vote, weighted average of labels, or weighted median of labels." ></td>
	<td class="line x" title="86:189	118 Label difference: 1 2 3 Three-class data 37% 33% Four-class data 34% 31% 30% Table 2: Average over authors and class pairs of between-class vocabulary overlap as the class labels of the pair grow farther apart." ></td>
	<td class="line x" title="87:189	shows that in aggregate, the vocabularies of distant classes overlap to a degree surprisingly similar to that of the vocabularies of nearby classes." ></td>
	<td class="line x" title="88:189	Thus, item similarity as measured by TO(cos) may not correlate well with similarity of the items true labels." ></td>
	<td class="line x" title="89:189	We can potentially develop a more useful similarity metric by asking ourselves what, intuitively, accounts for the label relationships that we seek to exploit." ></td>
	<td class="line x" title="90:189	A simple hypothesis is that ratings can be determined by the positive-sentence percentage (PSP) of a text, i.e., the number of positive sentences divided by the number of subjective sentences." ></td>
	<td class="line x" title="91:189	(Termbased versions of this premise have motivated much sentiment-analysis work for over a decade (Das and Chen, 2001; Tong, 2001; Turney, 2002))." ></td>
	<td class="line x" title="92:189	But counterexamples are easy to construct: reviews can contain off-topic opinions, or recount many positive aspects before describing a fatal aw." ></td>
	<td class="line x" title="93:189	We therefore tested the hypothesis as follows." ></td>
	<td class="line x" title="94:189	To avoid the need to hand-label sentences as positive or negative, we rst created a sentence polarity dataset7 consisting of 10,662 movie-review snippets (a striking extract usually one sentence long) downloaded from www.rottentomatoes.com; each snippet was labeled with its source reviews label (positive or negative) as provided by Rotten Tomatoes." ></td>
	<td class="line x" title="95:189	Then, we trained a Naive Bayes classi er on this data set and applied it to our scale dataset to identify the positive sentences (recall that objective sentences were already removed)." ></td>
	<td class="line x" title="96:189	Figure 1 shows that all four authors tend to exhibit a higher PSP when they write a more positive review, and we expect that most typical reviewers would follow suit." ></td>
	<td class="line x" title="97:189	Hence, PSP appears to be a promising basis for computing document similarity for our rating-inference task." ></td>
	<td class="line x" title="98:189	In particular, 7Available at http://www.cs.cornell.edu/People/pabo/moviereview-data as sentence polarity dataset v1.0." ></td>
	<td class="line x" title="99:189	we de ned a24a12a24a88a24a105a24a88a24a38a106 a107 a28 a107 a9a49a48a55a18 to be the two-dimensional vector a9a107 a28 a107 a9a49a48a55a18a95a13a60a108a109a24 a107 a28 a107 a9a49a48a63a18a2a18, and then set the itemsimilarity function required by the metric-labeling optimization function (Section 3.3) to a65a5a66a12a67 a9a49a48a50a13a2a87a88a18a110a20 a111a17a112a114a113a116a115 a24a49a24a105a24a88a24a105a24a82a106 a107 a28 a107 a9a49a48a63a18a17a13 a24a88a24a88a24a105a24a117a106 a107 a28 a107 a9a49a87a105a18a119a118a121a1208 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0 2 4 6 8 10 mean and standard deviation of PSP rating (in notches) Positive-sentence percentage (PSP) statistics Author a Author b Author c Author d Figure 1: Average and standard deviation of PSP for reviews expressing different ratings." ></td>
	<td class="line x" title="100:189	But before proceeding, we note that it is possible that similarity information might yield no extra bene t at all." ></td>
	<td class="line x" title="101:189	For instance, we dont need it if we can reliably identify each class just from some set of distinguishing terms." ></td>
	<td class="line x" title="102:189	If we de ne such terms as frequent ones (a6a23a122 a123a14a124 ) that appear in a single class 50% or more of the time, then we do nd many instances; some examples for one author are: meaningless, disgusting (class 0); pleasant, uneven (class 1); and oscar, gem (class 2) for the three-class case, and, in the four-class case, at, tedious (class 1) versus straightforward, likeable (class 2)." ></td>
	<td class="line x" title="103:189	Some unexpected distinguishing terms for this author are lion for class 2 (threeclass case), and for class 2 in the four-class case, jennifer, for a wide variety of Jennifers." ></td>
	<td class="line x" title="104:189	5 Evaluation This section compares the accuracies of the approaches outlined in Section 3 on the four corpora comprising our scale dataset." ></td>
	<td class="line x" title="105:189	(Results using a125 a0 error were qualitatively similar)." ></td>
	<td class="line x" title="106:189	Throughout, when 8While admittedly we initially chose this function because it was convenient to work with cosines, post hoc analysis revealed that the corresponding metric space stretched certain distances in a useful way." ></td>
	<td class="line x" title="107:189	119 we refer to something as signi cant, we mean statistically so with respect to the paireda126 -test,a127a129a128 a120a124a114a130 . The results that follow are based on a28a92a29a131a31 a33a36a35a38a37a40a39a42a41 s default parameter settings for SVM regression and OVA." ></td>
	<td class="line x" title="108:189	Preliminary analysis of the effect of varying the regression parameter a53 in the four-class case revealed that the default value was often optimal." ></td>
	<td class="line x" title="109:189	The notation Aa73 B denotes metric labeling where method A provides the initial label preference function a43 and B serves as similarity measure." ></td>
	<td class="line x" title="110:189	To train, we rst select the meta-parameters a64 and a74 by running 9-fold cross-validation within the training set." ></td>
	<td class="line x" title="111:189	Fixing a64 and a74 to those values yielding the best performance, we then re-train A (but with SVM parameters xed, as described above) on the whole training set." ></td>
	<td class="line x" title="112:189	At test time, the nearest neighbors of each item are also taken from the full training set." ></td>
	<td class="line x" title="113:189	5.1 Main comparison Figure 2 summarizes our average 10-fold crossvalidation accuracy results." ></td>
	<td class="line x" title="114:189	We rst observe from the plots that all the algorithms described in Section 3 always de nitively outperform the simple baseline of predicting the majority class, although the improvements are smaller in the four-class case." ></td>
	<td class="line x" title="115:189	Incidentally, the data was distributed in such a way that the absolute performance of the baseline itself does not change much between the threeand four-class case (which implies that the three-class datasets were relatively more balanced); and Author cs datasets seem noticeably easier than the others." ></td>
	<td class="line x" title="116:189	We now examine the effect of implicitly using label and item similarity." ></td>
	<td class="line x" title="117:189	In the four-class case, regression performed better than OVA (signi cantly so for two authors, as shown in the righthand table); but for the three-category task, OVA signi cantly outperforms regression for all four authors." ></td>
	<td class="line x" title="118:189	One might initially interprete this ip as showing that in the four-class scenario, item and label similarities provide a richer source of information relative to class-speci c characteristics, especially since for the non-majority classes there is less data available; whereas in the three-class setting the categories are better modeled as quite distinct entities." ></td>
	<td class="line x" title="119:189	However, the three-class results for metric labeling on top of OVA and regression (shown in Figure 2 by black versions of the corresponding icons) show that employing explicit similarities always improves results, often to a signi cant degree, and yields the best overall accuracies." ></td>
	<td class="line x" title="120:189	Thus, we can in fact effectively exploit similarities in the three-class case." ></td>
	<td class="line x" title="121:189	Additionally, in both the threeand fourclass scenarios, metric labeling often brings the performance of the weaker base method up to that of the stronger one (as indicated by the disappearance of upward triangles in corresponding table rows), and never hurts performance signi cantly." ></td>
	<td class="line x" title="122:189	In the four-class case, metric labeling and regression seem roughly equivalent." ></td>
	<td class="line x" title="123:189	One possible interpretation is that the relevant structure of the problem is already captured by linear regression (and perhaps a different kernel for regression would have improved its three-class performance)." ></td>
	<td class="line x" title="124:189	However, according to additional experiments we ran in the four-class situation, the test-set-optimal parameter settings for metric labeling would have produced signi cant improvements, indicating there may be greater potential for our framework." ></td>
	<td class="line x" title="125:189	At any rate, we view the fact that metric labeling performed quite well for both rating scales as a de nitely positive result." ></td>
	<td class="line x" title="126:189	5.2 Further discussion Q: Metric labeling looks like its just combining SVMs with nearest neighbors, and classi er combination often improves performance." ></td>
	<td class="line x" title="127:189	Couldnt we get the same kind of results by combining SVMs with any other reasonable method?" ></td>
	<td class="line x" title="128:189	A: No." ></td>
	<td class="line x" title="129:189	For example, if we take the strongest base SVM method for initial label preferences, but replace PSP with the term-overlap-based cosine (TO(cos)), performance often drops signi cantly." ></td>
	<td class="line x" title="130:189	This result, which is in accordance with Section 4s data, suggests that choosing an item similarity function that correlates well with label similarity is important." ></td>
	<td class="line x" title="131:189	(ovaa73 PSP a132a80a132a80a132a80a132 ovaa73 TO(cos) [3c]; rega73 PSP a132 rega73 TO(cos) [4c]) Q: Could you explain that notation, please?" ></td>
	<td class="line x" title="132:189	A: Triangles point toward the signi cantly better algorithm for some dataset." ></td>
	<td class="line x" title="133:189	For instance, M a132a80a132a80a133 N [3c] means, In the 3-class task, method M is signi cantly better than N for two author datasets and signi cantly worse for one dataset (so the algorithms were statistically indistinguishable on the remaining dataset) . When the algorithms being compared are statistically indistinguishable on 120 Average accuracies, three-class data Average accuracies, four-class data 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 Author a Author b Author c Author d majority ova ova+PSP reg reg+PSP 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 Author a Author b Author c Author d majority ova ova+PSP reg reg+PSP Average ten-fold cross-validation accuracies." ></td>
	<td class="line x" title="134:189	Open icons: SVMs in either one-versus-all (square) or regression (circle) mode; dark versions: metric labeling using the corresponding SVM together with the positive-sentence percentage (PSP)." ></td>
	<td class="line x" title="135:189	The a87 -axes of the two plots are aligned." ></td>
	<td class="line x" title="136:189	Signi cant differences, three-class data Signi cant differences, four-class data ova ova+PSP reg reg+PSP a b c d a b c d a b c d a b c d ova a134a86a134a63a134 . a132a63a132a63a132a86a132 .a132 . . ova+PSP a135a63a135a63a135 . a132a63a132a63a132a86a132 a132a63a132a63a132 . reg a134a63a134a63a134a86a134 a134a86a134a63a134a63a134 .a134 .a134 reg+PSP .a134 . . a134a86a134a63a134 . .a135 .a135 ova ova+PSP reg reg+PSP a b c d a b c d a b c d a b c d ova .a134a63a134a63a134 a134a63a134 . . a134 . .a134 ova+PSP .a135a63a135a63a135 a134 . . ." ></td>
	<td class="line x" title="137:189	a134 . . ." ></td>
	<td class="line x" title="138:189	reg a132a63a132 . . a132 . . ." ></td>
	<td class="line x" title="139:189	reg+PSP a132 . .a132 a132 . . ." ></td>
	<td class="line x" title="140:189	Triangles point towards signi cantly better algorithms for the results plotted above." ></td>
	<td class="line x" title="141:189	Speci cally, if the difference between a row and a column algorithm for a given author dataset (a, b, c, or d) is signi cant, a triangle points to the better one; otherwise, a dot ()." ></td>
	<td class="line x" title="142:189	is shown." ></td>
	<td class="line x" title="143:189	Dark icons highlight the effect of adding PSP information via metric labeling." ></td>
	<td class="line x" title="144:189	Figure 2: Results for main experimental comparisons." ></td>
	<td class="line x" title="145:189	all four datasets (the no triangles case), we indicate this with an equals sign ( = )." ></td>
	<td class="line x" title="146:189	Q: Thanks." ></td>
	<td class="line x" title="147:189	Doesnt Figure 1 show that the positive-sentence percentage would be a good classi er even in isolation, so metric labeling isnt necessary?" ></td>
	<td class="line x" title="148:189	A: No." ></td>
	<td class="line x" title="149:189	Predicting class labels directly from the PSP value via trained thresholds isnt as effective (ovaa73 PSP a132a80a132a80a132a80a132 threshold PSP [3c]; rega73 PSP a132a80a132 threshold PSP [4c])." ></td>
	<td class="line x" title="150:189	Alternatively, we could use only the PSP component of metric labeling by setting the label preference function to the constant function 0, but even with test-set-optimal parameter settings, doing so underperforms the trained metric labeling algorithm with access to an initial SVM classi er (ovaa73 PSP a132a80a132a80a132a80a132 0a73 a107 a28 a107a137a136 [3c]; rega73 PSP a132a80a132 0a73 a107 a28 a107a138a136 [4c])." ></td>
	<td class="line x" title="151:189	Q: What about using PSP as one of the features for input to a standard classi er?" ></td>
	<td class="line x" title="152:189	A: Our focus is on investigating the utility of similarity information." ></td>
	<td class="line x" title="153:189	In our particular rating-inference setting, it so happens that the basis for our pairwise similarity measure can be incorporated as an 121 item-speci c feature, but we view this as a tangential issue." ></td>
	<td class="line x" title="154:189	That being said, preliminary experiments show that metric labeling can be better, barely (for test-set-optimal parameter settings for both algorithms: signi cantly better results for one author, four-class case; statistically indistinguishable otherwise), although one needs to determine an appropriate weight for the PSP feature to get good performance." ></td>
	<td class="line x" title="155:189	Q: You de ned the metric transformation function a84 as the identity function a84 a9a85a8a30a18a139a20a140a8, imposing greater loss as the distance between labels assigned to two similar items increases." ></td>
	<td class="line x" title="156:189	Can you do just as well if you penalize all non-equal label assignments by the same amount, or does the distance between labels really matter?" ></td>
	<td class="line x" title="157:189	A: Youre asking for a comparison to the Potts model, which sets a84 to the function a141 a84 a9a85a8a30a18 a20 a108 if a8 a142 a124, a124 otherwise." ></td>
	<td class="line x" title="158:189	In the one setting in which there is a signi cant difference between the two, the Potts model does worse (ovaa73 PSP a132 ova a141 a73 PSP [3c])." ></td>
	<td class="line x" title="159:189	Also, employing the Potts model generally leads to fewer signi cant improvements over a chosen base method (compare Figure 2s tables with: reg a141 a73 PSP a132 reg [3c]; ova a141 a73 PSP a132a80a132 ova [3c]; ova a141 a73 PSP a20 ova [4c]; but note that reg a141 a73 PSP a132 reg [4c])." ></td>
	<td class="line x" title="160:189	We note that optimizing the Potts model in the multi-label case is NPhard, whereas the optimal metric labeling with the identity metric-transformation function can be ef ciently obtained (see Section 3.3)." ></td>
	<td class="line x" title="161:189	Q: Your datasets had many labeled reviews and only one author each." ></td>
	<td class="line x" title="162:189	Is your work relevant to settings with many authors but very little data for each?" ></td>
	<td class="line x" title="163:189	A: As discussed in Section 2, it can be quite difcult to properly calibrate different authors scales, since the same number of stars even within what is ostensibly the same rating system can mean different things for different authors." ></td>
	<td class="line x" title="164:189	But since you ask: we temporarily turned a blind eye to this serious issue, creating a collection of 5394 reviews by 496 authors with at most 80 reviews per author, where we pretended that our rating conversions mapped correctly into a universal rating scheme." ></td>
	<td class="line x" title="165:189	Preliminary results on this dataset were actually comparable to the results reported above, although since we are not con dent in the class labels themselves, more work is needed to derive a clear analysis of this setting." ></td>
	<td class="line x" title="166:189	(Abusing notation, since were already playing fast and loose: [3c]: baseline 52.4%, reg 61.4%, rega73 PSP 61.5%, ova (65.4%) a133 ovaa73 PSP (66.3%); [4c]: baseline 38.8%, reg (51.9%) a133 rega73 PSP (52.7%), ova (53.8%) a133 ovaa73 PSP (54.6%)) In future work, it would be interesting to determine author-independent characteristics that can be used on (or suitably adapted to) data for speci c authors." ></td>
	<td class="line x" title="167:189	Q: How about trying A: Yes, there are many alternatives." ></td>
	<td class="line x" title="168:189	A few that we tested are described in the Appendix, and we propose some others in the next section." ></td>
	<td class="line x" title="169:189	We should mention that we have not yet experimented with all-vs.-all (AVA), another standard binary-tomulti-category classi er conversion method, because we wished to focus on the effect of omitting pairwise information." ></td>
	<td class="line x" title="170:189	In independent work on 3-category rating inference for a different corpus, Koppel and Schler (2005) found that regression outperformed AVA, and Rifkin and Klautau (2004) argue that in principle OVA should do just as well as AVA." ></td>
	<td class="line x" title="171:189	But we plan to try it out." ></td>
	<td class="line x" title="172:189	6 Related work and future directions In this paper, we addressed the rating-inference problem, showing the utility of employing label similarity and (appropriate choice of) item similarity either implicitly, through regression, or explicitly and often more effectively, through metric labeling." ></td>
	<td class="line x" title="173:189	In the future, we would like to apply our methods to other scale-based classi cation problems, and explore alternative methods." ></td>
	<td class="line x" title="174:189	Clearly, varying the kernel in SVM regression might yield better results." ></td>
	<td class="line x" title="175:189	Another choice is ordinal regression (McCullagh, 1980; Herbrich, Graepel, and Obermayer, 2000), which only considers the ordering on labels, rather than any explicit distances between them; this approach could work well if a good metric on labels is lacking." ></td>
	<td class="line x" title="176:189	Also, one could use mixture models (e.g. , combine positive and negative language models) to capture class relationships (McCallum, 1999; Schapire and Singer, 2000; Takamura, Matsumoto, and Yamada, 2004)." ></td>
	<td class="line x" title="177:189	We are also interested in framing multi-class but non-scale-based categorization problems as metric 122 labeling tasks." ></td>
	<td class="line x" title="178:189	For example, positive vs. negative vs. neutral sentiment distinctions are sometimes considered in which neutral means either objective (Engstrcurrency1om, 2004) or a con ation of objective with a rating of mediocre (Das and Chen, 2001)." ></td>
	<td class="line x" title="179:189	(Koppel and Schler (2005) in independent work also discuss various types of neutrality)." ></td>
	<td class="line x" title="180:189	In either case, we could apply a metric in which positive and negative are closer to objective (or objective+mediocre) than to each other." ></td>
	<td class="line x" title="181:189	As another example, hierarchical label relationships can be easily encoded in a label metric." ></td>
	<td class="line x" title="182:189	Finally, as mentioned in Section 3.3, we would like to address the transductive setting, in which one has a small amount of labeled data and uses relationships between unlabeled items, since it is particularly well-suited to the metric-labeling approach and may be quite important in practice." ></td>
	<td class="line x" title="183:189	Acknowledgments We thank Paul Bennett, Dave Blei, Claire Cardie, Shimon Edelman, Thorsten Joachims, Jon Kleinberg, Oren Kurland, John Lafferty, Guy Lebanon, Pradeep Ravikumar, Jerry Zhu, and the anonymous reviewers for many very useful comments and discussion." ></td>
	<td class="line x" title="184:189	We learned of Moshe Koppel and Jonathan Schlers work while preparing the cameraready version of this paper; we thank them for so quickly answering our request for a pre-print." ></td>
	<td class="line x" title="185:189	Our descriptions of their work are based on that pre-print; we apologize in advance for any inaccuracies in our descriptions that result from changes between their pre-print and their nal version." ></td>
	<td class="line x" title="186:189	We also thank CMU for its hospitality during the year." ></td>
	<td class="line x" title="187:189	This paper is based upon work supported in part by the National Science Foundation (NSF) under grant no." ></td>
	<td class="line x" title="188:189	IIS-0329064 and CCR-0122581; SRI International under subcontract no. 03-000211 on their project funded by the Department of the Interiors National Business Center; and by an Alfred P. Sloan Research Fellowship." ></td>
	<td class="line x" title="189:189	Any opinions, ndings, and conclusions or recommendations expressed are those of the authors and do not necessarily re ect the views or of cial policies, either expressed or implied, of any sponsoring institutions, the U.S. government, or any other entity." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P05-2008
Using Emoticons To Reduce Dependency In Machine Learning Techniques For Sentiment Classification
Read, Jonathon;"></td>
	<td class="line x" title="1:156	Proceedings of the ACL Student Research Workshop, pages 4348, Ann Arbor, Michigan, June 2005." ></td>
	<td class="line x" title="2:156	c2005 Association for Computational Linguistics Using Emoticons to reduce Dependency in Machine Learning Techniques for Sentiment Classification Jonathon Read Department of Informatics University of Sussex United Kingdom j.l.read@sussex.ac.uk Abstract Sentiment Classification seeks to identify a piece of text according to its authors general feeling toward their subject, be it positive or negative." ></td>
	<td class="line x" title="3:156	Traditional machine learning techniques have been applied to this problem with reasonable success, but they have been shown to work well only when there is a good match between the training and test data with respect to topic." ></td>
	<td class="line x" title="4:156	This paper demonstrates that match with respect to domain and time is also important, and presents preliminary experiments with training data labeled with emoticons, which has the potential of being independent of domain, topic and time." ></td>
	<td class="line x" title="5:156	1 Introduction Recent years have seen an increasing amount of research effort expended in the area of understanding sentiment in textual resources." ></td>
	<td class="line x" title="6:156	A sub-topic of this research is that of Sentiment Classification." ></td>
	<td class="line x" title="7:156	That is, given a problem text, can computational methods determine if the text is generally positive or generally negative?" ></td>
	<td class="line x" title="8:156	Several diverse applications exist for this potential technology, ranging from the automatic filtering of abusive messages (Spertus, 1997) to an in-depth analysis of market trends and consumer opinions (Dave et al. , 2003)." ></td>
	<td class="line x" title="9:156	This is a complex and challenging task for a computer to achieve  consider the difficulties involved in instructing a computer to recognise sarcasm, for example." ></td>
	<td class="line x" title="10:156	Previous work has shown that traditional text classification approaches can be quite effective when applied to the sentiment analysis problem." ></td>
	<td class="line x" title="11:156	Models such as Nave Bayes (NB), Maximum Entropy (ME) and Support Vector Machines (SVM) can determine the sentiment of texts." ></td>
	<td class="line x" title="12:156	Pang et al.(2002) used a bagof-features framework (based on unigrams and bigrams) to train these models from a corpus of movie reviews labelled as positive or negative." ></td>
	<td class="line x" title="14:156	The best accuracy achieved was 82.9%, using an SVM trained on unigram features." ></td>
	<td class="line pc" title="15:156	A later study (Pang and Lee, 2004) found that performance increased to 87.2% when considering only those portions of the text deemed to be subjective." ></td>
	<td class="line x" title="16:156	However, Engstrom (2004) showed that the bagof-features approach is topic-dependent." ></td>
	<td class="line x" title="17:156	A classifier trained on movie reviews is unlikely to perform as well on (for example) reviews of automobiles." ></td>
	<td class="line x" title="18:156	Turney (2002) noted that the unigram unpredictable might have a positive sentiment in a movie review (e.g. unpredictable plot), but could be negative in the review of an automobile (e.g. unpredictable steering)." ></td>
	<td class="line x" title="19:156	In this paper, we demonstrate how the models are also domain-dependent  how a classifier trained on product reviews is not effective when evaluating the sentiment of newswire articles, for example." ></td>
	<td class="line x" title="20:156	Furthermore, we show how the models are temporally-dependent  how classifiers are biased by the trends of sentiment apparent during the time-period represented by the training data." ></td>
	<td class="line x" title="21:156	We propose a novel source of training data based on the language used in conjunction with emoticons in Usenet newsgroups." ></td>
	<td class="line x" title="22:156	Training a classifier using this data provides a breadth of features that, while it 43 Testing FIN M&A MIX Training NB FIN 80.3 75.5 74.0 M&A 77.5 75.3 75.8 MIX 70.7 62.9 84.6 SVM FIN 78.8 72.7 68.9 M&A 74.5 75.5 75.5 MIX 72.0 68.9 81.1 Figure 1: Topic dependency in sentiment classification." ></td>
	<td class="line x" title="23:156	Accuracies, in percent." ></td>
	<td class="line x" title="24:156	Best performance on a test set for each model is highlighted in bold." ></td>
	<td class="line x" title="25:156	does not perform to the state-of-the-art, could function independent of domain, topic and time." ></td>
	<td class="line x" title="26:156	2 Dependencies in Sentiment Classification 2.1 Experimental Setup In this section, we describe experiments we have carried out to determine the influence of domain, topic and time on machine learning based sentiment classification." ></td>
	<td class="line x" title="27:156	The experiments use our own implementation of a Nave Bayes classifier and Joachims (1999) SVMlight implementation of a Support Vector Machine classifier." ></td>
	<td class="line x" title="28:156	The models were trained using unigram features, accounting for the presence of feature types in a document, rather than the frequency, as Pang et al.(2002) found that this is the most effective strategy for sentiment classification." ></td>
	<td class="line x" title="30:156	When training and testing on the same set, the mean accuracy is determined using three-fold crossvalidation." ></td>
	<td class="line x" title="31:156	In each case, we use a paired-sample t-test over the set of test documents to determine whether the results produced by one classifier are statistically significantly better than those from another, at a confidence interval of at least 95%." ></td>
	<td class="line x" title="32:156	2.2 Topic Dependency Engstrom (2004) demonstrated how machinelearning techniques for sentiment classification can be topic dependent." ></td>
	<td class="line x" title="33:156	However, that study focused on a three-way classification (positive, negative and neutral)." ></td>
	<td class="line x" title="34:156	In this paper, for uniformity across different data sets, we focus on only positive and negative sentiment." ></td>
	<td class="line x" title="35:156	This experiment also provides an opportunity to evaluate the Nave Bayes classifier as the previous work used SVMs." ></td>
	<td class="line x" title="36:156	We use subsets of a Newswire dataset (kindly proTesting Newswire Polarity 1.0 Training NB Newswire 78.2 57.6 Polarity 1.0 53.2 78.9 SVM Newswire 78.2 63.2 Polarity 1.0 63.6 81.5 Figure 2: Domain dependency in sentiment classification." ></td>
	<td class="line x" title="37:156	Accuracies, in percent." ></td>
	<td class="line x" title="38:156	Best performance on a test set for each model is highlighted in bold." ></td>
	<td class="line x" title="39:156	vided by Roy Lipski of Infonic Ltd)." ></td>
	<td class="line x" title="40:156	that relate to the topics of Finance (FIN), Mergers and Aquisitions (M&A) and a mixture of both topics (MIX)." ></td>
	<td class="line x" title="41:156	Each subset contains further subsets of articles of positive and negative sentiment (selected by independent trained annotators), each containing 100 stories." ></td>
	<td class="line x" title="42:156	We trained a model on a dataset relating to one topic and tested that model using the other topics." ></td>
	<td class="line x" title="43:156	Figure 1 shows the results of this experiment." ></td>
	<td class="line x" title="44:156	The tendency seems to be that performance in a given topic is best if the training data is from the same topic." ></td>
	<td class="line x" title="45:156	For example, the Finance-trained SVM classifier achieved an accuracy of 78.8% against articles from Finance, but only 72.7% when predicting the sentiment of articles from M&A. However, statistical testing showed that the results are not significantly different when training on one topic and testing on another." ></td>
	<td class="line x" title="46:156	It is interesting to note, though, that providing a dataset of mixed topics (the sub-corpus MIX) does not necessarily reduce topic dependency." ></td>
	<td class="line x" title="47:156	Indeed, the performance of the classifiers suffers a great deal when training on mixed data (confidence interval 95%)." ></td>
	<td class="line x" title="48:156	2.3 Domain Dependency We conducted an experiment to compare the accuracy when training a classifier on one domain (newswire articles or movie reviews from the Polarity 1.0 dataset used by Pang et al.(2002)) and testing on the other domain." ></td>
	<td class="line x" title="50:156	In Figure 2, we see a clear indication that models trained on one domain do not perform as well on another domain." ></td>
	<td class="line x" title="51:156	All differences are significant at a confidence interval of 99.9%." ></td>
	<td class="line x" title="52:156	2.4 Temporal Dependency To investigate the effect of time on sentiment classification, we constructed a new set of movie re44 Testing Polarity 1.0 Polarity 2004 Training NB Polarity 1.0 78.9 71.8 Polarity 2004 63.2 76.5 SVM Polarity 1.0 81.5 77.5 Polarity 2004 76.5 80.8 Figure 3: Temporal dependency in sentiment classification." ></td>
	<td class="line x" title="53:156	Accuracies, in percent." ></td>
	<td class="line x" title="54:156	Best performance on a test set for each model is highlighted in bold." ></td>
	<td class="line x" title="55:156	views, following the same approach used by Pang et al.(2002) when they created the Polarity 1.0 dataset." ></td>
	<td class="line x" title="57:156	The data source was the Internet Movie Review Database archive1 of movie reviews." ></td>
	<td class="line x" title="58:156	The reviews were categorised as positive or negative using automatically extracted ratings." ></td>
	<td class="line x" title="59:156	A review was ignored if it was not written in 2003 or 2004 (ensuring that the review was written after any in the Polarity 1.0 dataset)." ></td>
	<td class="line x" title="60:156	This procedure yielded a corpus of 716 negative and 2,669 positive reviews." ></td>
	<td class="line x" title="61:156	To create the Polarity 20042 dataset we randomly selected 700 negative reviews and 700 positive reviews, matching the size and distribution of the Polarity 1.0 dataset." ></td>
	<td class="line x" title="62:156	The next experiment evaluated the performance of the models first against movie reviews from the same time-period as the training set and then against reviews from the other time-period." ></td>
	<td class="line x" title="63:156	Figure 3 shows the resulting accuracies." ></td>
	<td class="line x" title="64:156	These results show that while the models perform well on reviews from the same time-period as the training set, they are not so effective on reviews from other time-periods (confidence interval 95%)." ></td>
	<td class="line x" title="65:156	It is also apparent that the Polarity 2004 dataset performs worse than the Polarity 1.0 dataset (confidence interval 99.9%)." ></td>
	<td class="line x" title="66:156	A possible reason for this is that Polarity 2004 data is from a much smaller time-period than that represented by Polarity 1.0." ></td>
	<td class="line x" title="67:156	3 Sentiment Classification using Emoticons One way of overcoming the domain, topic and time problems we have demonstrated above would be to find a source of much larger and diverse amounts of general text, annotated for sentiment." ></td>
	<td class="line x" title="68:156	Users of 1http://reviews.imdb.com/Reviews/ 2The new datasets described in this paper are available at http://www.sussex.ac.uk/Users/jlr24/data Glyph Meaning Frequency :-) smile 3.8739 ;-) wink 2.4350 :-( frown 0.4961 :-D wide grin 0.1838 :-P tongue sticking out 0.1357 :-O surprise 0.0171 :-| disappointed 0.0146 :( crying 0.0093 :-S confused 0.0075 :-@ angry 0.0038 :-$ embarrassed 0.0007 Figure 4: Examples of emoticons and the frequency of usage observed in Usenet articles, in percent." ></td>
	<td class="line x" title="69:156	For example, 2.435% of downloaded Usenet articles contained a wink emoticon." ></td>
	<td class="line x" title="70:156	electronic methods of communication have developed visual cues that are associated with emotional states in an attempt to state the emotion that their text represents." ></td>
	<td class="line x" title="71:156	These have become known as smileys or emoticons and are glyphs constructed using the characters available on a standard keyboard, representing a facial expression of emotion  see Figure 4 for some examples." ></td>
	<td class="line x" title="72:156	When the author of an electronic communication uses an emoticon, they are effectively marking up their own text with an emotional state." ></td>
	<td class="line x" title="73:156	This marked-up text can be used to train a sentiment classifier if we assume that a smile indicates generally positive text and a frown indicates generally negative text." ></td>
	<td class="line x" title="74:156	3.1 Emoticon Corpus Construction We collected a corpus of text marked-up with emoticons by downloading Usenet newsgroups and saving an article if it contained an emoticon listed in Figure 4." ></td>
	<td class="line x" title="75:156	This process resulted in 766,730 articles being stored, from 10,682,455 messages in 49,759 newsgroups inspected." ></td>
	<td class="line x" title="76:156	Figure 4 also lists the percentage of documents containing each emoticon type, as observed in the Usenet newsgroups." ></td>
	<td class="line x" title="77:156	We automatically extracted the paragraph(s) containing the emoticon of interest (a smile or a frown) from each message and removed any superfluous formatting characters (such as those used to indicate article quotations in message threads)." ></td>
	<td class="line x" title="78:156	In order to prevent quoted text from being considered more than once, any paragraph that began with exactly the same thirty characters as a previously observed paragraph was disregarded." ></td>
	<td class="line x" title="79:156	Finally, we used the classifier developed by Cavnar and Trenkle (1994) to filter 45 Finance M&A Mixed NB 46.0  2.1 55.8  3.8 49.0  1.6 SVM 50.3  1.7 57.8  6.5 55.5  2.7 Figure 5: Performance of Emoticon-trained classifier across topics." ></td>
	<td class="line x" title="80:156	Mean accuracies with standard deviation, in percent." ></td>
	<td class="line x" title="81:156	Newswire Polarity 1.0 NB 50.3  2.2 56.8  1.8 SVM 54.4  2.8 54.0  0.8 Figure 6: Performance of Emoticon-trained classifiers across domains." ></td>
	<td class="line x" title="82:156	Mean accuracies with standard deviation, in percent." ></td>
	<td class="line x" title="83:156	out any paragraphs of non-English text." ></td>
	<td class="line x" title="84:156	This process yielded a corpus of 13,000 article extracts containing frown emoticons." ></td>
	<td class="line x" title="85:156	As investigating skew between positive and negative distributions is outside the scope of this work, we also extracted 13,000 article extracts containing smile emoticons." ></td>
	<td class="line x" title="86:156	The dataset is referred to throughout this paper as Emoticons and contains 748,685 words." ></td>
	<td class="line x" title="87:156	3.2 Emoticon-trained Sentiment Classification This section describes how the Emoticons corpus3 was optimised for use as sentiment classification training data." ></td>
	<td class="line x" title="88:156	2,000 articles containing smiles and 2,000 articles containing frowns were held-out as optimising test data." ></td>
	<td class="line x" title="89:156	We took increasing amounts of articles from the remaining dataset (from 2,000 to 22,000 in increments of 1,000, an equal number being taken from the positive and negative sets) as optimising training data." ></td>
	<td class="line x" title="90:156	For each set of training data we extracted a context of an increasing number of tokens (from 10 to 1,000 in increments of 10) both before and in a window4 around the smile or frown emoticon." ></td>
	<td class="line x" title="91:156	The models were trained using this extracted context and tested on the held-out dataset." ></td>
	<td class="line x" title="92:156	The optimisation process revealed that the bestperforming settings for the Nave Bayes classifier was a window context of 130 tokens taken from the largest training set of 22,000 articles." ></td>
	<td class="line x" title="93:156	Similarly, the best performance for the SVM classifier was found using a window context of 150 tokens taken from 3Note that in these experiments the emoticons are used as anchors from which context is extracted, but are removed from texts before they are used as training or test data." ></td>
	<td class="line x" title="94:156	4Context taken after an emoticon was also investigated, but was found to be inferior." ></td>
	<td class="line x" title="95:156	This is because approximately twothirds of article extracts end in an emoticon so when using aftercontext few features are extracted." ></td>
	<td class="line x" title="96:156	Polarity 1.0 Polarity 2004 NB 56.8  1.8 56.7  2.2 SVM 54.0  0.8 57.8  1.8 Figure 7: Performance of Emoticon-trained classifier across time-periods." ></td>
	<td class="line x" title="97:156	Mean accuracies with standard deviation, in percent." ></td>
	<td class="line x" title="98:156	20,000 articles." ></td>
	<td class="line x" title="99:156	The classifiers performance in predicting the smiles and frowns of article extracts was verified using these optimised parameters and ten-fold crossvalidation." ></td>
	<td class="line x" title="100:156	The mean accuracy of the Nave Bayes classifier was 61.5%, while the SVM classifier was 70.1%." ></td>
	<td class="line x" title="101:156	Using these same classifiers to predict the sentiment of movie reviews in Polarity 1.0 resulted in accuracies of 59.1% (Nave Bayes) and 52.1% (SVM)." ></td>
	<td class="line x" title="102:156	We repeated the optimisation process using a held-out set of 100 positive and 100 negative reviews from the Polarity 1.0 dataset, as it is possible that this test needs different parameter settings." ></td>
	<td class="line x" title="103:156	This revealed an optimum context of a window of 50 tokens taken from a training set of 21,000 articles for the Nave Bayes classifier." ></td>
	<td class="line x" title="104:156	Interestingly, the optimum context for the SVM classifier appeared to be a window of only 20 tokens taken from a mere 2,000 training examples." ></td>
	<td class="line x" title="105:156	This is clearly an anomaly, as these parameters resulted in an accuracy of 48.9% when testing against the reserved reviews of Polarity 1.0." ></td>
	<td class="line x" title="106:156	We attribute this to the presence of noise, both in the training set and in the held-out set, and discuss this below (Section 4.2)." ></td>
	<td class="line x" title="107:156	The second-best parameters according to the optimisation process were a context of 510 tokens taken before an emoticon, from a training set of 20,000 examples." ></td>
	<td class="line x" title="108:156	We used these optimised parameters to evaluate the sentiments of texts in the test sets used to evaluate dependency in Section 2." ></td>
	<td class="line x" title="109:156	Figures 5, 6 and 7 show the final, optimised results across topics, domains and time-periods respectively." ></td>
	<td class="line x" title="110:156	These tables report the average accuracies over three folds, with the standard deviation as a measure of error." ></td>
	<td class="line x" title="111:156	4 Discussion The emoticon-trained classifiers perform well (up to 70% accuracy) when predicting the sentiment of article extracts from the Emoticons dataset, which is encouraging when one considers the high level of 46 Training Testing Coverage Polarity 1.0 Polarity 1.0 69.8 (three-fold cross-validation) Emoticons FIN 54.9 M&A 58.1 MIX 60.2 Newswire 46.1 Polarity 1.0 41.1 Polarity 2004 42.6 Figure 8: Coverage of classifiers, in percent." ></td>
	<td class="line x" title="112:156	noise that is likely to be present in the dataset." ></td>
	<td class="line x" title="113:156	However, they perform only a little better than one would expect by chance when classifying movie reviews, and are not effective in predicting the sentiment of newswire articles." ></td>
	<td class="line x" title="114:156	This is perhaps due to the nature of the datasets  one would expect language to be informal in movie reviews, and even more so in Usenet articles." ></td>
	<td class="line x" title="115:156	In contrast, language in newswire articles is far more formal." ></td>
	<td class="line x" title="116:156	We might therefore infer a further type of dependence in sentiment classification, that of language-style dependency." ></td>
	<td class="line x" title="117:156	Also, note that neither machine-learning model consistently out-performs the other." ></td>
	<td class="line x" title="118:156	We speculate that this, and the generally mediocre performance of the classifiers, is due (at least) to two factors; poor coverage of the features found in the test domains and a high level of noise found in Usenet article extracts." ></td>
	<td class="line x" title="119:156	We investigate these factors below." ></td>
	<td class="line x" title="120:156	4.1 Coverage Figure 8 shows the coverage of the Emoticon-trained classifiers on the various test sets." ></td>
	<td class="line x" title="121:156	In these experiments, we are interested in the coverage in terms of unique token types rather than the frequency of features, as this more closely reflects the training of the models (see Section 2.1)." ></td>
	<td class="line x" title="122:156	The mean coverage of the Polarity 1.0 dataset during three-fold crossvalidation is also listed as an example of the coverage one would expect from a better-performing sentiment classifier." ></td>
	<td class="line x" title="123:156	The Emoticon-trained classifier has much worse coverage in the test sets." ></td>
	<td class="line x" title="124:156	We analysed the change in coverage of the Emoticon-trained classifiers on the Polarity 1.0 dataset." ></td>
	<td class="line x" title="125:156	We found that the coverage continued to improve as more training data was provided; the coverage of unique token types was improving by about 0.6% per 1,000 training examples when the Emoti48 50 52 54 56 58 60 3000 6000 9000 12000 15000 18000Training Size 100 200 300 400 500 600 700 800 900 1000 Context Size 48 50 52 54 56 58 60 Accuracy (%) Figure 9: Change in Performance of the SVM Classifier on held-out reviews from Polarity 1.0, varying training set size and window context size." ></td>
	<td class="line x" title="126:156	The datapoints represent 2,200 experiments in total." ></td>
	<td class="line x" title="127:156	cons dataset was exhausted." ></td>
	<td class="line x" title="128:156	It appears possible that more training data will improve the performance of the Emoticon-trained classifiers by increasing the coverage." ></td>
	<td class="line x" title="129:156	Potential sources for this include online bulletin boards, chat forums, and further newsgroup data from Usenet and Google Groups5." ></td>
	<td class="line x" title="130:156	Future work will utilise these sources to collect more examples of emoticon use and analyse any improvement in coverage and accuracy." ></td>
	<td class="line x" title="131:156	4.2 Noise in Usenet Article Extracts The article extracts collected in the Emoticons dataset may be noisy with respect to sentiment." ></td>
	<td class="line x" title="132:156	The SVM classifier seems particularly affected by this noise." ></td>
	<td class="line x" title="133:156	Figure 9 depicts the change in performance of the SVM classifier when varying the training set size and size of context extracted." ></td>
	<td class="line x" title="134:156	There are significant spikes apparent for the training sizes of 2,000, 3,000 and 6,000 article extracts (as noted in Section 3.2), where the accuracy suddenly increases for the training set size, then quickly decreases for the next set size." ></td>
	<td class="line x" title="135:156	This implies that the classifier is discovering features that are useful in classifying the heldout set, but the addition of more, noisy, texts soon makes the information redundant." ></td>
	<td class="line x" title="136:156	Some examples of noise taken from the Emoticons dataset are: mixed sentiment, e.g. 5http://groups.google.com 47 Sorry about venting my frustration here but I just lost it." ></td>
	<td class="line x" title="137:156	:-( Happy thanks giving everybody :-), sarcasm, e.g. Thank you so much, thats really encouraging :-(, and spelling mistakes, e.g. The movies where for me a major desapointment :-(." ></td>
	<td class="line x" title="138:156	In future work we will investigate ways to remove noisy data from the Emoticons dataset." ></td>
	<td class="line x" title="139:156	5 Conclusions and Future Work This paper has demonstrated that dependency in sentiment classification can take the form of domain, topic, temporal and language style." ></td>
	<td class="line x" title="140:156	One might suppose that dependency is occurring because classifiers are learning the semantic sentiment of texts rather than the general sentiment of language used." ></td>
	<td class="line x" title="141:156	That is, the classifiers could be learning authors sentiment towards named entities (e.g. actors, directors, companies, etc.)." ></td>
	<td class="line x" title="142:156	However, this does not seem to be the case." ></td>
	<td class="line x" title="143:156	In a small experiment, we part-ofspeech tagged the Polarity 2004 dataset and automatically replaced proper nouns with placeholders." ></td>
	<td class="line x" title="144:156	Retraining on this modified text did not significantly affect performance." ></td>
	<td class="line x" title="145:156	But it may be that something more subtle is happening." ></td>
	<td class="line x" title="146:156	Possibly, the classifiers are learning the words associated with the semantic sentiment of entities." ></td>
	<td class="line x" title="147:156	For example, suppose that there has been a well-received movie about mountaineering." ></td>
	<td class="line x" title="148:156	During this movie, there is a particularly stirring scene involving an ice-axe and most of the reviewers mention this scene." ></td>
	<td class="line x" title="149:156	During training, the word ice-axe would become associated with a positive sentiment, whereas one would suppose that this word does not in general express any kind of sentiment." ></td>
	<td class="line x" title="150:156	In future work we will perform further tests to determine the nature of dependency in machine learning techniques for sentiment classification." ></td>
	<td class="line x" title="151:156	One way of evaluating the ice-axe effect could be to build a pseudo-ontology of the movie reviews  a map of the sentiment-bearing relations that would enable the analysis of the dependencies created by the training process." ></td>
	<td class="line x" title="152:156	Other extensions of this work are to collect more text marked-up with emoticons, and to experiment with techniques to automatically remove noisy examples from the training data." ></td>
	<td class="line x" title="153:156	Acknowledgements This research was funded by a UK EPSRC studentship." ></td>
	<td class="line x" title="154:156	I am very grateful to Thorsten Joachims, Roy Lipski, Bo Pang and John Trenkle for kindly making their data or software available, and to the anonymous reviewers for their constructive comments." ></td>
	<td class="line x" title="155:156	Thanks also to Nick Jacobi for his discussion of the ice-axe effect." ></td>
	<td class="line x" title="156:156	Special thanks to my supervisor, John Carroll, for his continued advice and encouragement." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W05-0408
Automatic Identification Of Sentiment Vocabulary: Exploiting Low Association With Known Sentiment Terms
Gamon, Michael;Aue, Anthony;"></td>
	<td class="line x" title="1:174	Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 5764, Ann Arbor, June 2005." ></td>
	<td class="line x" title="2:174	c2005 Association for Computational Linguistics Automatic identification of sentiment vocabulary: exploiting low association with known sentiment terms Michael Gamon Anthony Aue Natural Language Processing Group Natural Language Processing Group Microsoft Research Microsoft Research mgamon@microsoft.com anthaue@microsoft.com Abstract We describe an extension to the technique for the automatic identification and labeling of sentiment terms described in Turney (2002) and Turney and Littman (2002)." ></td>
	<td class="line x" title="3:174	Their basic assumption is that sentiment terms of similar orientation tend to co-occur at the document level." ></td>
	<td class="line x" title="4:174	We add a second assumption, namely that sentiment terms of opposite orientation tend not to co-occur at the sentence level." ></td>
	<td class="line x" title="5:174	This additional assumption allows us to identify sentiment-bearing terms very reliably." ></td>
	<td class="line x" title="6:174	We then use these newly identified terms in various scenarios for the sentiment classification of sentences." ></td>
	<td class="line x" title="7:174	We show that our approach outperforms Turneys original approach." ></td>
	<td class="line x" title="8:174	Combining our approach with a Naive Bayes bootstrapping method yields a further small improvement of classifier performance." ></td>
	<td class="line x" title="9:174	We finally compare our results to precision and recall figures that can be obtained on the same data set with labeled data." ></td>
	<td class="line oc" title="10:174	1 Introduction The field of sentiment classification has received considerable attention from researchers in recent years (Pang and Lee 2002, Pang et al. 2004, Turney 2002, Turney and Littman 2002, Wiebe et al. 2001, Bai et al. 2004, Yu and Hatzivassiloglou 2003 and many others)." ></td>
	<td class="line x" title="11:174	The identification and classification of sentiment constitutes a problem that is orthogonal to the usual task of text classification." ></td>
	<td class="line x" title="12:174	Whereas in traditional text classification the focus is on topic identification, in sentiment classification the focus is on the assessment of the writers sentiment toward the topic." ></td>
	<td class="line oc" title="13:174	Movie and product reviews have been the main focus of many of the recent studies in this area (Pang and Lee 2002, Pang et al. 2004, Turney 2002, Turney and Littman 2002)." ></td>
	<td class="line x" title="14:174	Typically, these reviews are classified at the document level, and the class labels are positive and negative." ></td>
	<td class="line x" title="15:174	In this work, in contrast, we narrow the scope of investigation to the sentence level and expand the set of labels, making a threefold distinction between positive, neutral, and negative." ></td>
	<td class="line x" title="16:174	The narrowing of scope is motivated by the fact that for realistic text mining on customer feedback, the document level is too coarse, as described in Gamon et al.(2005)." ></td>
	<td class="line x" title="18:174	The expansion of the label set is also motivated by real-world concerns; while it is a given that review text expresses positive or negative sentiment, in many cases it is necessary to also identify the cases that dont carry strong expressions of sentiment at all." ></td>
	<td class="line x" title="19:174	Traditional approaches to text classification require large amounts of labeled training data." ></td>
	<td class="line x" title="20:174	Acquisition of such data can be costly and timeconsuming." ></td>
	<td class="line x" title="21:174	Due to the highly domain-specific nature of the sentiment classification task, moving from one domain to another typically requires the acquisition of a new set of training data." ></td>
	<td class="line x" title="22:174	For this reason, unsupervised or very weakly supervised methods for sentiment classification are especially 57 desirable." ></td>
	<td class="line x" title="23:174	1 Our focus, therefore, is on methods that require very little data annotation." ></td>
	<td class="line x" title="24:174	We describe a method to automatically identify the sentiment vocabulary in a domain." ></td>
	<td class="line x" title="25:174	This method rests on three special properties of the sentiment domain: 1." ></td>
	<td class="line x" title="26:174	the presence of certain words can serve as a proxy for the class label 2." ></td>
	<td class="line x" title="27:174	sentiment terms of similar orientation tend to co-occur 3." ></td>
	<td class="line x" title="28:174	sentiment terms of opposite orientation tend to not co-occur at the sentence level." ></td>
	<td class="line x" title="29:174	Turney (2002) and Turney and Littman (2002) exploit the first two generalizations for unsupervised sentiment classification of movie reviews." ></td>
	<td class="line x" title="30:174	They use the two terms excellent and poor as seed terms to determine the semantic orientation of other terms." ></td>
	<td class="line x" title="31:174	These seed terms can be viewed as proxies for the class labels positive and negative, allowing for the exploitation of otherwise unlabeled data: Terms that tend to co-occur with excellent in documents tend to be of positive orientation, and vice versa for poor." ></td>
	<td class="line x" title="32:174	Turney (2002) starts from a small (2 word) set of terms with known orientation (excellent and poor)." ></td>
	<td class="line x" title="33:174	Given a set of terms with unknown sentiment orientation, Turney (2002) then uses the PMI-IR algorithm (Turney 2001) to issue queries to the web and determine, for each of these terms, its pointwise mutual information (PMI) with the two seed words across a large set of documents." ></td>
	<td class="line x" title="34:174	Term candidates are constrained to be adjectives, which tend to be the strongest bearers of sentiment." ></td>
	<td class="line x" title="35:174	The sentiment orientation (SO) of a term is then determined by the difference between its association (PMI) with the positive seed term excellent and its association with the negative seed term poor." ></td>
	<td class="line x" title="36:174	The resulting list of terms and associated sentiment orientations can then be used to implement a classifier: semantic orientation of the terms in a document of unknown sentiment is added up, and if the overall score is positive, the document is classified as being of positive sentiment, otherwise it is classified as negative." ></td>
	<td class="line x" title="37:174	Yu and Hatzivassiloglou (2003) extend this approach by (1) applying it at the sentence level (instead of the document-level), (2) taking into account non-adjectival parts-of-speech, and (3) 1 For domain-specificity of sentiment classification see Engstrm (2004) and Aue and Gamon (2005)." ></td>
	<td class="line x" title="38:174	using larger sets of seed words." ></td>
	<td class="line x" title="39:174	Their classification goal also differs from Turneys: it is to distinguish opinion sentences from factual statements." ></td>
	<td class="line x" title="40:174	Turney et al.s approach is based on the assumption that sentiment terms of similar orientation tend to co-occur in documents." ></td>
	<td class="line x" title="41:174	Our approach takes advantage of a second assumption: At the sentence level, sentiment terms of opposite orientation tend not to co-occur." ></td>
	<td class="line x" title="42:174	This is, of course, an assumption that will only hold in general, with exceptions." ></td>
	<td class="line x" title="43:174	Basically, the assumption is that sentences of the following form: I dislike X. I really like X. are more frequent than mixed sentiment sentences such as I dislike X but I really like Y. It has been our experience that this generalization does hold often enough to be useful." ></td>
	<td class="line x" title="44:174	We propose to utilize this assumption to identify a set of sentiment terms in a domain." ></td>
	<td class="line x" title="45:174	We select the terms that have the lowest PMI scores on the sentence level with respect to a set of manually selected seed words." ></td>
	<td class="line x" title="46:174	If our assumption about low association at the sentence level is correct, this set of low-scoring terms will be particularly rich in sentiment terms." ></td>
	<td class="line x" title="47:174	We can then use this newly identified set to: (1) use Turneys method to find the orientation for the terms and employ the terms and their scores in a classifier, and (2) use Turneys method to find the orientation for the terms and add the new terms as additional seed terms for a second iteration As opposed to Turney (2002), we do not use the web as a resource to find associations, rather we apply the method directly to in-domain data." ></td>
	<td class="line x" title="48:174	This has the disadvantage of not being able to apply the classification to any arbitrary domain." ></td>
	<td class="line x" title="49:174	It is worth noting, however, that even in Turney (2002) the choice of seed words is explicitly motivated by domain properties of movie reviews." ></td>
	<td class="line x" title="50:174	In the remainder of the paper we will describe results from various experiments based on this assumption." ></td>
	<td class="line x" title="51:174	We also show how we can combine this method with a Naive Bayes bootstrapping approach that takes further advantage of the unlabeled data (Nigam et al. 2000)." ></td>
	<td class="line x" title="52:174	58 2 Data For our experiments we used a set of car reviews from the MSN Autos web site." ></td>
	<td class="line x" title="53:174	The data consist of 406,818 customer car reviews written over a fouryear period." ></td>
	<td class="line x" title="54:174	Aside from filtering out examples containing profanity, the data was not edited." ></td>
	<td class="line x" title="55:174	The reviews range in length from a single sentence (56% of all cases) to 50 sentences (a single review)." ></td>
	<td class="line x" title="56:174	Less than 1% of reviews contain ten or more sentences." ></td>
	<td class="line x" title="57:174	There are almost 900,000 sentences in total." ></td>
	<td class="line x" title="58:174	When customers submitted reviews to the website, they were asked for a recommendation on a scale of 1 (negative) to 10 (positive)." ></td>
	<td class="line x" title="59:174	The average score was very high, at 8.3, yielding a strong skew in favor of positive class labels." ></td>
	<td class="line x" title="60:174	We annotated a randomlyselected sample of 3,000 sentences for sentiment." ></td>
	<td class="line x" title="61:174	Each sentence was viewed in isolation and classified as positive, negative or neutral." ></td>
	<td class="line x" title="62:174	The neutral category was applied to sentences with no discernible sentiment, as well as to sentences that expressed both positive and negative sentiment." ></td>
	<td class="line x" title="63:174	Three annotators had pair-wise agreement scores (Cohens Kappa score, Cohen 1960) of 70.10%, 71.78% and 79.93%, suggesting that the task of sentiment classification on the sentence level is feasible but difficult even for people." ></td>
	<td class="line x" title="64:174	This set of data was split into a development test set of 400 sentences and a blind test set of 2600 sentences." ></td>
	<td class="line x" title="65:174	Sentences are represented as vectors of binary unigram features." ></td>
	<td class="line x" title="66:174	The total number of observed unigram features is 72988." ></td>
	<td class="line x" title="67:174	In order to restrict the number of features to a manageable size, we disregard features that occur less than 10 times in the corpus." ></td>
	<td class="line x" title="68:174	With this restriction we obtain a reduced feature set of 13317 features." ></td>
	<td class="line x" title="69:174	3 Experimental Setup Our experiments were performed as follows: We started with a small set of manually-selected and annotated seed terms." ></td>
	<td class="line x" title="70:174	We used 4 positive and 6 negative seed terms." ></td>
	<td class="line x" title="71:174	We decided to use a few more negative seed words because of the inherent positive skew in the data that makes the identification of negative sentences particularly hard." ></td>
	<td class="line x" title="72:174	The terms we used are: positive: negative: good bad excellent lousy love terrible happy hate suck unreliable There was no tuning of the set of initial seed terms; the 10 words were originally chosen intuitively, as words that we observed frequently when manually inspecting the data." ></td>
	<td class="line x" title="73:174	We then used these seed terms in two basic ways: (1) We used them as seeds for a Turneystyle determination of the semantic orientation of words in the corpus (semantic orientation, or SO method)." ></td>
	<td class="line x" title="74:174	As mentioned above, this process is based on the assumption that terms of similar orientation tend to co-occur." ></td>
	<td class="line x" title="75:174	(2) We used them to mine sentiment vocabulary from the unlabeled data using the additional assumption that sentiment terms of opposite orientation tend not to co-occur at the sentence level (sentiment mining, or SM method)." ></td>
	<td class="line x" title="76:174	This method yields a set of sentiment terms, but no orientation for that set of terms." ></td>
	<td class="line x" title="77:174	We continue by using the SO method to find the semantic orientation for this set of sentiment terms, effectively using SM as a feature selection method for sentiment terminology." ></td>
	<td class="line x" title="78:174	Pseudo-code for the SO and SM approaches is provided in Figure 1 and Figure 2." ></td>
	<td class="line x" title="79:174	As a first step for both SO and SM methods (not shown in the pseudocode), PMI needs to be calculated for each pair (f, s) of feature f and seed word s over the collection of feature vectors." ></td>
	<td class="line x" title="80:174	Figure 1: SO method for determining semantic orientation 59 Figure 2: SM method for mining sentiment terms In the first scenario (using straightforward SO), features F range over all observed features in the data (modulo the aforementioned count cutoff of 10)." ></td>
	<td class="line x" title="81:174	In the second scenario (SM + SO), features F range over the n% of features with the lowest PMI scores with respect to any of the seed words that were identified using the sentiment mining technique in Figure 2." ></td>
	<td class="line x" title="82:174	The result of both SO and SM+SO is a list of unigram features which have an associated semantic orientation score, indicating their sentiment orientation: the higher the score, the more positive a term, and vice versa." ></td>
	<td class="line x" title="83:174	This list of features and associated scores can be used to construct a simple classifier: for each sentence with unknown sentiment, we take the sum of the semantic orientation scores for all of the unigrams in that sentence." ></td>
	<td class="line x" title="84:174	This overall score determines the classification of the sentence as positive, neutral or negative as shown in Figure 3." ></td>
	<td class="line x" title="85:174	Scoring and classifying sentence vectors: (1) assigning a sentence score: FOREACH feature f in sentence vector v: Score(v) = Score(v) + SO(f) (2) assigning a class label based on the sentence score: IF Score(v) > threshold1: Class(v) = positive ELSE IF Score(v) < threshold1 AND Score(v) > threshold2: Class(v) = neutral ELSE Class(v) = negative Figure 3: Using SO scores for sentence scoring and classification The two thresholds used in classification need to be determined empirically by taking the distribution of class values in the corpus into account." ></td>
	<td class="line x" title="86:174	For our experiments we simply took the distribution of class labels in the 400 sentence development test set as an approximation of the overall class label distribution: we determined that distribution to be 15.5% for negative sentences, 21.5% for neutral sentences, and 63.0% for positive sentences." ></td>
	<td class="line x" title="87:174	Scores for all sentence vectors in the corpus are then collected using the scoring part of the algorithm in Figure 3." ></td>
	<td class="line x" title="88:174	The scores are sorted and the thresholds are determined as the cutoffs for the top 63% and bottom 15.5% of scores respectively." ></td>
	<td class="line x" title="89:174	4 Results 4.1." ></td>
	<td class="line x" title="90:174	Comparing SO and SM+SO In our first set of experiments we manipulated the following parameters: 1." ></td>
	<td class="line x" title="91:174	the choice of SO or SM+SO method 2." ></td>
	<td class="line x" title="92:174	the choice of n when selecting the n% semantic terms with lowest PMI score in the SM method The tables below show the results of classifying sentence vectors using the unigram features and associated scores produced by SO and SO+SM." ></td>
	<td class="line x" title="93:174	We used the 2,600-sentence manually-annotated test set described previously to establish these numbers." ></td>
	<td class="line x" title="94:174	Since the data exhibit a strong skew in favor of the positive class label, we measure performance not in terms of accuracy but in terms of average precision and recall across the three class labels, as suggested in (Manning and Schtze 2002)." ></td>
	<td class="line x" title="95:174	Avg precision Avg recall SO 0.4481 0.4511 Table 1: Using the SO approach." ></td>
	<td class="line x" title="96:174	Table 1 shows results of using the SO method on the data." ></td>
	<td class="line x" title="97:174	Table 2 presents the results of combining the SM and SO methods for different values of n. The best results are shown in boldface." ></td>
	<td class="line x" title="98:174	As a comparison between Table 1 and Table 2 shows, the highest average precision and recall scores were obtained by combining the SM and SO methods." ></td>
	<td class="line x" title="99:174	Using SM as a feature selection mechanism also reduces the number of features significantly." ></td>
	<td class="line x" title="100:174	While the SO method employed on sentence-level vectors uses 13,000 features, the best-performing SM+SO combination uses only 20% of this feature set, indicating that SM is indeed effective in selecting the most important sentiment-bearing terms." ></td>
	<td class="line x" title="101:174	60 We also determined that the positive impact of SM is not just a matter of reducing the number of features." ></td>
	<td class="line x" title="102:174	If SO without the SM feature selection step is reduced to a comparable number of features by taking the top features according to absolute score, average precision is at 0.4445 and average recall at 0.4464." ></td>
	<td class="line x" title="103:174	N=10 N=20 N=30 N=40 N=50 Avg prec Avg rec Avg prec Avg rec Avg prec Avg rec Avg prec Avg rec Avg prec Avg rec SM+SO SO from document level 0.4351 0.4377 0.4568 0.4605 0.4528 0.4557 0.4457 0.4478 0.4451 0.4475 Table 2: combining SM and SO." ></td>
	<td class="line x" title="104:174	Sentiment terms in top 100 SM terms Sentiment terms in top 100 SO terms excellent, terrible, broke, junk, alright, bargain, grin, highest, exceptional, exceeded, horrible, loved, waste, ok, death, leaking, outstanding, cracked, rebate, warped, hooked, sorry, refuses, excellant, satisfying, died, biggest, competitive, delight, avoid, awful, garbage, loud, okay, competent, upscale, dated, mistake, sucks, superior, high, kill, neither excellent, happy, stylish, sporty, smooth, love, quiet, overall, pleased, plenty, dependable, solid, roomy, safe, good, easy, smaller, luxury, comfortable, style, loaded, space, classy, handling, joy, small, comfort, size, perfect, performance, room, choice, recommended, package, compliments, awesome, unique, fun, holds, comfortably, extremely, value, free, satisfied, little, recommend, limited, great, pleasure Non sentiment terms in top 100 SM terms Non sentiment terms in top 100 SO terms alternative, wont, below, surprisingly, maintained, choosing, comparing, legal, vibration, seemed, claim, demands, assistance, knew, engineering, accelleration, ended, salesperson, performed, started, midsize, site, gonna, lets, plugs, industry, alternator, month, told, vette, 180, powertrain, write, mos, walk, causing, lift, es, segment, $250, 300m, wanna, february, mod, $50, nhtsa, suburbans, manufactured, tiburon, $10, f150, 5000, posted, tt, him, saw, jan, condition, very, handles, milage, definitely, definately, far, drives, shape, color, price, provides, options, driving, rides, sports, heated, ride, sport, forward, expected, fairly, anyone, test, fits, storage, range, family, sedan, trunk, young, weve, black, college, suv, midsize, coupe, 30, shopping, kids, player, saturn, bose, truck, town, am, leather, stereo, car, husband Table 3: the top 100 terms identified by SM and SO Table 3 shows the top 100 terms that were identified by each SM and SO methods." ></td>
	<td class="line x" title="105:174	The terms are categorized into sentiment-bearing and nonsentiment bearing terms by human judgment." ></td>
	<td class="line x" title="106:174	The two sets seem to differ in both strength and orientation of the identified terms." ></td>
	<td class="line x" title="107:174	The SM-identified words have a higher density of negative terms (22 out of 43 versus 2 out of 49 for the SO-identified terms)." ></td>
	<td class="line x" title="108:174	The SM-identified terms also express sentiment more strongly, but this conclusion is more tentative since it may be a consequence of the higher density of negative terms." ></td>
	<td class="line x" title="109:174	4.2." ></td>
	<td class="line x" title="110:174	Multiple iterations: increasing the number of seed features by SM+SO In a second set of experiments, we assessed the question of whether it is possible to use multiple iterations of the SM+SO method to gradually build the list of seed words." ></td>
	<td class="line x" title="111:174	We do this by adding the top n% of features selected by SM, along with their orientation as determined by SO, to the initial set of seed words." ></td>
	<td class="line x" title="112:174	The procedure for this round of experiments is as follows:  take the top n% of features identified by SM (we used n=1 for the reported re61 sults, since preliminary experiments with other values for n did not improve results)  perform SO for these features to determine their orientation  take the top 15.5% negative and top 63% positive (according to class label distribution in the development test set) of the features and add them as negative/positive seed features respectively This iteration increases the number of seed features from the original 10 manually-selected features to a total of 111 seed features." ></td>
	<td class="line x" title="113:174	With this enhanced set of seed features we then re-ran a subset of the experiments in Table 2." ></td>
	<td class="line x" title="114:174	Results are shown in Table 4." ></td>
	<td class="line x" title="115:174	Increasing the number of seed features through the SM feature selection method increases precision and recall by several percentage points." ></td>
	<td class="line x" title="116:174	In particular, precision and recall for negative sentences are boosted." ></td>
	<td class="line x" title="117:174	Avg precision Avg recall SM + SO, n=10, SO from document vectors 0.4826 0.48.76 SM + SO, n=30, SO from document vectors 0.4957 0.4995 SM + SO, n=50, SO from document vectors 0.4914 0.4952 Table 4: Using 2 iterations to increase the seed feature set We also confirmed that these results are truly attributable to the use of the SM method for the first iteration." ></td>
	<td class="line x" title="118:174	If we take an equivalent number of features with strongest semantic orientation according to the SO method and add them to the list of seed features, our results degrade significantly (the resulting classifier performance is significantly different at the 99.9% level as established by the McNemar test)." ></td>
	<td class="line x" title="119:174	This is further evidence that SM is indeed an effective method for selecting sentiment terms." ></td>
	<td class="line x" title="120:174	4.3." ></td>
	<td class="line x" title="121:174	Using the SO classifier to bootstrap a Naive Bayes classifier In a third set of experiments, we tried to improve on the results of the SO classifier by combining it with the bootstrapping approach described in (Nigam et al. 2000)." ></td>
	<td class="line x" title="122:174	The basic idea here is to use the SO classifier to label a subset of the data DL." ></td>
	<td class="line x" title="123:174	This labeled subset of the data is then used to bootstrap a Naive Bayes (NB) classifier on the remaining unlabeled data D U using the Expectation Maximization (EM) algorithm: (1) An initial naive Bayes classifier with parameters  is trained on the documents in DL." ></td>
	<td class="line x" title="124:174	(2) This initial classifier is used to estimate a probability distribution over all classes for each of the documents in DU." ></td>
	<td class="line x" title="125:174	(EStep) (3) The labeled and unlabeled data are then used to estimate parameters for a new classifier." ></td>
	<td class="line x" title="126:174	(M-Step) Steps 2 and 3 are repeated until convergence is achieved when the difference in the joint probability of the data and the parameters falls below the configurable threshold  between iterations." ></td>
	<td class="line x" title="127:174	Another free parameter, , can be used to control how much weight is given to the unlabeled data." ></td>
	<td class="line x" title="128:174	For our experiments we used classifiers from the best SM+SO combination (2 iterations at n=30) from Table 4 above to label 30% of the total data." ></td>
	<td class="line x" title="129:174	Table 5 shows the average precision and recall numbers for the converged NB classifier." ></td>
	<td class="line x" title="130:174	2 In addition to improving average precision and recall, the resulting classifier also has the advantage of producing class probabilities instead of simple scores." ></td>
	<td class="line x" title="131:174	3 Avg precision Avg recall Bootstrapped NB classifier 0.5167 0.52 Table 5: Results obtained by bootstrapping a NB classifier 4.4." ></td>
	<td class="line x" title="132:174	Results from supervised learning: using small sets of labeled data Given infinite resources, we can always annotate enough data to train a classifier using a supervised algorithm that will outperform unsupervised or weakly-supervised methods." ></td>
	<td class="line x" title="133:174	Which approach to take depends entirely on how much time and money are available and on the accuracy requirements for the task at hand." ></td>
	<td class="line x" title="134:174	2 In this experiment,  was set to 0.1 and  was set to 0.05." ></td>
	<td class="line x" title="135:174	3 We also experimented with labeling the whole data set with the best of our SO score classifiers, and then training a linear Support Vector Machine classifier on the data." ></td>
	<td class="line x" title="136:174	The results were considerably worse than any of the reported numbers, so they are not included in this paper." ></td>
	<td class="line x" title="137:174	62 To help situate the precision and recall numbers presented in the tables above, we trained Support Vector Machines (SVMs) using small amounts of labeled data." ></td>
	<td class="line x" title="138:174	SVMs were trained with 500, 1000, 2000, and 2500 labeled sentences." ></td>
	<td class="line x" title="139:174	Annotating 2500 sentences represents approximately eight person-hours of work." ></td>
	<td class="line x" title="140:174	The results can be found in Table 5." ></td>
	<td class="line x" title="141:174	We were pleasantly surprised at how well the unsupervised classifiers described above perform in comparison to state-of-the-art supervised methods (albeit trained on small amounts of data)." ></td>
	<td class="line x" title="142:174	Labeled examples Avg." ></td>
	<td class="line x" title="143:174	Precision Avg." ></td>
	<td class="line x" title="144:174	Recall 500.4878 .4967 1000 .5161 .5105 2000 .5297 .5256 2500 .5017 .5083 Table 6: Average precision and recall for SVMs for small numbers of labeled examples 4.5." ></td>
	<td class="line oc" title="145:174	Results on the movie domain We also performed a small set of experiments on the movie domain using Pang and Lees 2004 data set." ></td>
	<td class="line o" title="146:174	This set consists of 2000 reviews, 1000 each of very positive and very negative reviews." ></td>
	<td class="line o" title="147:174	Since this data set is balanced and the task is only a two-way classification between positive and negative reviews, we only report accuracy numbers here." ></td>
	<td class="line oc" title="148:174	accuracy Training data Turney (2002) 66% unsupervised Pang & Lee (2004) 87.15% supervised Aue & Gamon (2005) 91.4% supervised SO 73.95% unsupervised SM+SO to increase seed words, then SO 74.85% weakly supervised Table 7: Classification accuracy on the movie review domain Turney (2002) achieves 66% accuracy on the movie review domain using the PMI-IR algorithm to gather association scores from the web." ></td>
	<td class="line oc" title="149:174	Pang and Lee (2004) report 87.15% accuracy using a unigram-based SVM classifier combined with subjectivity detection." ></td>
	<td class="line x" title="150:174	Aue and Gamon (2005) use a simple linear SVM classifier based on unigrams, combined with LLR-based feature reduction, to achieve 91.4% accuracy." ></td>
	<td class="line x" title="151:174	Using the Turney SO method on in-domain data instead of web data achieves 73.95% accuracy (using the same two seed words that Turney does)." ></td>
	<td class="line x" title="152:174	Using one iteration of SM+SO to increase the number of seed words, followed by finding SO scores for all words with respect to the enhanced seed word set, yields a slightly higher accuracy of 74.85%." ></td>
	<td class="line x" title="153:174	With additional parameter tuning, this number can be pushed to 76.4%, at which point we achieve statistical significance at the 0.95 level according to the McNemar test, indicating that there is more room here for improvement." ></td>
	<td class="line x" title="154:174	Any reduction of the number of overall features in this domain leads to decreased accuracy, contrary to what we observed in the car review domain." ></td>
	<td class="line x" title="155:174	We attribute this observation to the smaller data set." ></td>
	<td class="line x" title="156:174	5 Discussion 5.1 A note on statistical significance We used the McNemar test to assess whether two classifiers are performing significantly differently." ></td>
	<td class="line x" title="157:174	This test establishes whether the accuracy of two classifiers differs significantly it does not guarantee significance for precision and recall differences." ></td>
	<td class="line x" title="158:174	For the latter, other tests have been proposed (e.g. Chinchor 1995), but time constraints prohibited us from implementing any of those more computationally costly tests." ></td>
	<td class="line x" title="159:174	For the results presented in the previous sections the McNemar test established statistical significance at the 0.99 level over baseline (i.e. the SO results in Table 1) for the multiple iterations results (Table 4) and the bootstrapping approach (Table 5), but not for the SM+SO approach (Table 2)." ></td>
	<td class="line x" title="160:174	5.2 Future work This exploratory set of experiments indicates a number of interesting directions for future work." ></td>
	<td class="line x" title="161:174	A shortcoming of the present work is the manual tuning of cutoff parameters." ></td>
	<td class="line x" title="162:174	This problem could be alleviated in at least two possible ways: First, using a general combination of the ranking of terms according to SM and SO." ></td>
	<td class="line x" title="163:174	In other words, calculate the semantic weight of a term as a combination of SO and its rank in the SM scores." ></td>
	<td class="line x" title="164:174	63 Secondly, following a suggestion by an anonymous reviewer, the Naive Bayes bootstrapping approach could be used in a feedback loop to inform the SO score estimation in the absence of a manually annotated parameter tuning set." ></td>
	<td class="line x" title="165:174	5.3 Summary Our results demonstrate that the SM method can serve as a valid tool to mine sentiment-rich vocabulary in a domain." ></td>
	<td class="line x" title="166:174	SM will yield a list of terms that are likely to have a strong sentiment orientation." ></td>
	<td class="line x" title="167:174	SO can then be used to find the polarity for the selected features by association with the sentiment terms of known polarity in the seed word list." ></td>
	<td class="line x" title="168:174	Performing this process iteratively by first enhancing the set of seed words through SM+SO yields the best results." ></td>
	<td class="line x" title="169:174	While this approach does not compare to the results that can be achieved by supervised learning with large amounts of labeled data, it does improve on results obtained by using SO alone." ></td>
	<td class="line x" title="170:174	We believe that this result is relevant in two respects." ></td>
	<td class="line x" title="171:174	First, by improving average precision and recall on the classification task, we move closer to the goal of unsupervised sentiment classification." ></td>
	<td class="line x" title="172:174	This is a very important goal in itself given the need for out of the box sentiment techniques in business intelligence and the notorious difficulty of rapidly adapting to a new domain (Engstrm 2004, Aue and Gamon 2005)." ></td>
	<td class="line x" title="173:174	Second, the exploratory results reported here may indicate a general source of information for feature selection in natural language tasks: features that have a tendency to be in complementary distribution (especially in smaller linguistic units such as sentences) may often form a class that shares certain properties." ></td>
	<td class="line x" title="174:174	In other words, it is not only the strong association scores that should be exploited but also the particularly weak (negative) associations." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E06-1025
Determining Term Subjectivity And Term Orientation For Opinion Mining
Esuli, Andrea;Sebastiani, Fabrizio;"></td>
	<td class="line x" title="1:169	Determining Term Subjectivity and Term Orientation for Opinion Mining Andrea Esuli1 and Fabrizio Sebastiani2 (1) Istituto di Scienza e Tecnologie dellInformazione  Consiglio Nazionale delle Ricerche Via G Moruzzi, 1  56124 Pisa, Italy andrea.esuli@isti.cnr.it (2) Dipartimento di Matematica Pura e Applicata  Universit`a di Padova Via GB Belzoni, 7  35131 Padova, Italy fabrizio.sebastiani@unipd.it Abstract Opinion mining is a recent subdiscipline of computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses." ></td>
	<td class="line x" title="2:169	To aid the extraction of opinions from text, recent work has tackled the issue of determining the orientation of subjective terms contained in text, i.e. deciding whether a term that carries opinionated content has a positive or a negative connotation." ></td>
	<td class="line x" title="3:169	This is believed to be of key importance for identifying the orientation of documents, i.e. determining whether a document expresses a positive or negative opinion about its subject matter." ></td>
	<td class="line x" title="4:169	We contend that the plain determination of the orientation of terms is not a realistic problem, since it starts from the nonrealistic assumption that we already know whether a term is subjective or not; this would imply that a linguistic resource that marks terms as subjective or objective is available, which is usually not the case." ></td>
	<td class="line x" title="5:169	In this paper we confront the task of deciding whether a given term has a positive connotation, or a negative connotation, or has no subjective connotation at all; this problem thus subsumes the problem of determining subjectivity and the problem of determining orientation." ></td>
	<td class="line x" title="6:169	We tackle this problem by testing three different variants of a semi-supervised method previously proposed for orientation detection." ></td>
	<td class="line x" title="7:169	Our results show that determining subjectivity and orientation is a much harder problem than determining orientation alone." ></td>
	<td class="line x" title="8:169	1 Introduction Opinion mining is a recent subdiscipline of computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses." ></td>
	<td class="line x" title="9:169	Opinion-driven content management has several important applications, such as determining critics opinions about a given product by classifying online product reviews, or tracking the shifting attitudes ofthegeneral public towardapolitical candidate by mining online forums." ></td>
	<td class="line x" title="10:169	Within opinion mining, several subtasks can be identified, all of them having to do with tagging a given document according to expressed opinion: 1." ></td>
	<td class="line x" title="11:169	determining document subjectivity, as in deciding whether a given text has a factual nature (i.e. describes a given situation or event, without expressing a positive or a negative opinion on it) or expresses an opinion on its subject matter." ></td>
	<td class="line oc" title="12:169	This amounts to performing binary text categorization under categories Objective and Subjective (Pang and Lee, 2004; Yu and Hatzivassiloglou, 2003); 2." ></td>
	<td class="line oc" title="13:169	determining document orientation (or polarity), as in deciding if a given Subjective text expresses a Positive or a Negative opinion on its subject matter (Pang and Lee, 2004; Turney, 2002); 3." ></td>
	<td class="line x" title="14:169	determining the strength of document orientation, as in deciding e.g. whether the Positive opinion expressed by a text on its subject matter is Weakly Positive, Mildly Positive, or Strongly Positive (Wilson et al. , 2004)." ></td>
	<td class="line x" title="15:169	To aid these tasks, recent work (Esuli and Sebastiani, 2005; Hatzivassiloglou and McKeown, 1997; Kamps et al. , 2004; Kim and Hovy, 2004; Takamura et al. , 2005; Turney and Littman, 2003) has tackled the issue of identifying the orientation ofsubjective terms contained in text, i.e.determiningwhether atermthat carries opinionated content has a positive or a negative connotation (e.g. deciding that  using Turney and Littmans (2003) examples  honest and intrepid have a positive connotation while disturbing and superfluous have a negative connotation)." ></td>
	<td class="line x" title="16:169	193 This is believed to be of key importance for identifying the orientation of documents, since it is by considering the combined contribution of these terms that one may hope to solve Tasks 1, 2 and 3 above." ></td>
	<td class="line x" title="17:169	The conceptually simplest approach to this latter problem is probably Turneys (2002), who has obtained interesting results on Task 2 by considering the algebraic sum of the orientations of terms as representative of the orientation of the document they belong to; but more sophisticated approaches arealsopossible (Hatzivassiloglou and Wiebe, 2000; Riloff et al. , 2003; Wilson et al. , 2004)." ></td>
	<td class="line x" title="18:169	Implicit in most works dealing with term orientation is the assumption that, for many languages for which one would like to perform opinion mining, there is no available lexical resource where terms are tagged as having either a Positive or a Negative connotation, and that in the absence of such a resource the only available route is to generate such a resource automatically." ></td>
	<td class="line x" title="19:169	However, we think this approach lacks realism, since it is also true that, for the very same languages, there is no available lexical resource where terms are tagged as having either a Subjective or an Objective connotation." ></td>
	<td class="line x" title="20:169	Thus, the availability of an algorithm that tags Subjective terms as being either Positive or Negative is of little help, since determining if a term is Subjective is itself non-trivial." ></td>
	<td class="line x" title="21:169	In this paper we confront the task of determining whether a given term has a Positive connotation (e.g. honest, intrepid), or a Negative connotation (e.g. disturbing, superfluous), or has instead no Subjective connotation at all (e.g. white, triangular); this problem thus subsumes the problem of deciding between Subjective and Objective and the problem of deciding between Positive and Negative." ></td>
	<td class="line x" title="22:169	We tackle this problem by testing three different variants of the semi-supervised method for orientation detection proposed in (Esuli and Sebastiani, 2005)." ></td>
	<td class="line x" title="23:169	Our results show that determining subjectivity and orientation is amuch harder problem than determining orientation alone." ></td>
	<td class="line x" title="24:169	1.1 Outline of the paper The rest of the paper is structured as follows." ></td>
	<td class="line x" title="25:169	Section 2 reviews related work dealing with term orientation and/or subjectivity detection." ></td>
	<td class="line x" title="26:169	Section 3 briefly reviews the semi-supervised method for orientation detection presented in (Esuli and Sebastiani, 2005)." ></td>
	<td class="line x" title="27:169	Section 4 describes in detail three different variants of itwe propose for determining, at the same time, subjectivity and orientation, and describes the general setup of our experiments." ></td>
	<td class="line x" title="28:169	In Section 5 we discuss the results we have obtained." ></td>
	<td class="line x" title="29:169	Section 6 concludes." ></td>
	<td class="line x" title="30:169	2 Related work 2.1 Determining term orientation Most previous works dealing with the properties of terms within an opinion mining perspective have focused on determining term orientation." ></td>
	<td class="line x" title="31:169	Hatzivassiloglou and McKeown (1997) attempt to predict the orientation of subjective adjectives by analysing pairs of adjectives (conjoined by and,or,but,either-or,orneither-nor) extracted from a large unlabelled document set." ></td>
	<td class="line x" title="32:169	The underlying intuition is that the act of conjoining adjectives is subject to linguistic constraints on the orientation of the adjectives involved; e.g. and usually conjoins adjectives of equal orientation, while but conjoins adjectives of opposite orientation." ></td>
	<td class="line x" title="33:169	The authors generate a graph where terms are nodes connected by equal-orientation or opposite-orientation edges, depending on the conjunctions extracted from the document set." ></td>
	<td class="line x" title="34:169	A clustering algorithm then partitions the graph into a Positive cluster and a Negative cluster, based on a relation of similarity induced by the edges." ></td>
	<td class="line x" title="35:169	Turney and Littman (2003) determine term orientation by bootstrapping from two small sets of subjective seed terms (with the seed set for Positive containing terms such as good and nice, and the seed set for Negative containing terms such as bad and nasty)." ></td>
	<td class="line x" title="36:169	Their method is based on computing the pointwise mutual information (PMI) of the target term t with each seed term ti as a measure of their semantic association." ></td>
	<td class="line x" title="37:169	Given a target term t, its orientation value O(t) (where positive value means positive orientation, and higher absolute value means stronger orientation) is given by the sum of the weights of its semantic association with the seed positive terms minus the sum of the weights of its semantic association with the seed negative terms." ></td>
	<td class="line x" title="38:169	For computing PMI, term frequencies and co-occurrence frequencies are measured by querying a document set by means of the AltaVista search engine1 with a t query, a ti query, and a t NEARti query, and using the number of matching documents returned by the search engine as estimates of the probabilities needed for the computation of PMI." ></td>
	<td class="line x" title="39:169	Kamps et al.(2004) consider instead the graph defined on adjectives by the WordNet2 synonymy relation, and determine the orientation of a target 1http://www.altavista.com/ 2http://wordnet.princeton.edu/ 194 adjective t contained in the graph by comparing the lengths of (i) the shortest path between t and the seed term good, and (ii) the shortest path between t and the seed term bad: if the former is shorter than the latter, than t is deemed to be Positive, otherwise it is deemed to be Negative." ></td>
	<td class="line x" title="41:169	Takamura et al.(2005) determine term orientation (for Japanese) according to a spin model, i.e. a physical model of a set of electrons each endowed with one between two possible spin directions, and where electrons propagate their spin direction to neighbouring electrons until the system reaches a stable configuration." ></td>
	<td class="line x" title="43:169	The authors equate terms with electrons and term orientation to spin direction." ></td>
	<td class="line x" title="44:169	They build a neighbourhood matrix connecting each pair of terms if one appears in the gloss ofthe other, and iteratively apply thespin model on the matrix until a minimum energy configuration is reached." ></td>
	<td class="line x" title="45:169	The orientation assigned to a term then corresponds to the spin direction assigned to electrons." ></td>
	<td class="line x" title="46:169	ThesystemofKimandHovy(2004) tackles orientation detection by attributing, to each term, a positivity score and a negativity score; interestingly, terms may thus be deemed to have both a positive and a negative correlation, maybe with different degrees, and some terms may be deemed to carry a stronger positive (or negative) orientation than others." ></td>
	<td class="line x" title="47:169	Their system starts from a set of positive and negative seed terms, and expands the positive (resp." ></td>
	<td class="line x" title="48:169	negative) seed set by adding to it the synonyms of positive (resp." ></td>
	<td class="line x" title="49:169	negative) seed termsandtheantonyms ofnegative(resp." ></td>
	<td class="line x" title="50:169	positive) seed terms." ></td>
	<td class="line x" title="51:169	The system classifies then a target term t into either Positive or Negative by means of two alternative learning-free methods based on the probabilities that synonyms of t also appear in the respective expanded seed sets." ></td>
	<td class="line x" title="52:169	A problem with this method is that it can classify only terms that share somesynonyms withtheexpanded seed sets." ></td>
	<td class="line x" title="53:169	Kim and Hovy also report an evaluation of human inter-coder agreement." ></td>
	<td class="line x" title="54:169	We compare this evaluation with our results in Section 5." ></td>
	<td class="line x" title="55:169	The approach we have proposed for determining term orientation (Esuli and Sebastiani, 2005) is described in more detail in Section 3, since it will be extensively used in this paper." ></td>
	<td class="line x" title="56:169	All these works evaluate the performance of the proposed algorithms by checking them against precompiled sets of Positive and Negative terms, i.e. checking how good the algorithms are at classifying a term known to be subjective into either Positive or Negative." ></td>
	<td class="line x" title="57:169	When tested on the same benchmarks, the methods of (Esuli and Sebastiani, 2005; Turney and Littman, 2003) have performed with comparable accuracies (however, the method of (Esuli and Sebastiani, 2005) is much more efficient than the one of (Turney and Littman, 2003)), and have outperformed the method of (Hatzivassiloglou and McKeown, 1997) by a wide margin and the one by (Kamps et al. , 2004) by a very wide margin." ></td>
	<td class="line x" title="58:169	The methods described in (Hatzivassiloglou and McKeown, 1997) is also limited by the fact that it can only decide the orientation of adjectives, while the method of (Kamps et al. , 2004) is further limited in that it can only work on adjectives that are present in WordNet." ></td>
	<td class="line x" title="59:169	The methods of (Kim and Hovy, 2004; Takamura et al. , 2005) are instead difficult to compare with the other ones since they were not evaluated on publicly available datasets." ></td>
	<td class="line x" title="60:169	2.2 Determining term subjectivity Riloff et al.(2003) develop a method to determine whether a term has a Subjective or an Objective connotation, based on bootstrapping algorithms." ></td>
	<td class="line x" title="62:169	The method identifies patterns for the extraction of subjective nouns from text, bootstrapping from a seed set of 20 terms that the authors judge to be strongly subjective and have found to have high frequency in the text collection from which the subjective nouns must be extracted." ></td>
	<td class="line x" title="63:169	The results of this method are not easy to compare with the ones we present in this paper because of the different evaluation methodologies." ></td>
	<td class="line x" title="64:169	While we adopt the evaluation methodology used in all of the papers reviewed so far (i.e. checking how good our system is at replicating an existing, independently motivated lexical resource), the authors do not test their method on an independently identified set of labelled terms, butonthesetoftermsthatthealgorithm itself extracts." ></td>
	<td class="line x" title="65:169	This evaluation methodology only allows to test precision, and not accuracy tout court, since no quantification can be made of false negatives (i.e. the subjective terms that the algorithm should have spotted but has not spotted)." ></td>
	<td class="line x" title="66:169	In Section 5 this will prevent us from drawing comparisons between this method and our own." ></td>
	<td class="line x" title="67:169	Baroni and Vegnaduzzo (2004) apply the PMI method, first used by Turney and Littman (2003) to determine term orientation, to determine term subjectivity." ></td>
	<td class="line x" title="68:169	Their method uses a small set Ss of 35 adjectives, marked as subjective by human judges, toassign asubjectivity score toeachadjective to be classified." ></td>
	<td class="line x" title="69:169	Therefore, their method, unlike our own, does not classify terms (i.e. take firm classification decisions), but ranks them according to a subjectivity score, on which they evaluate precision at various level of recall." ></td>
	<td class="line x" title="70:169	195 3 Determining term subjectivity and term orientation by semi-supervised learning The method we use in this paper for determining term subjectivity and term orientation is a variant of the method proposed in (Esuli and Sebastiani, 2005) for determining term orientation alone." ></td>
	<td class="line x" title="71:169	This latter method relies on training, in a semisupervised way, a binary classifier that labels terms as either Positive or Negative." ></td>
	<td class="line x" title="72:169	A semisupervised method is a learning process whereby only a small subset L  Tr of the training data Tr are human-labelled." ></td>
	<td class="line x" title="73:169	In origin the training data in U = Tr  L are instead unlabelled; it is the process itself that labels them, automatically, by using L (with the possible addition of other publicly available resources) as input." ></td>
	<td class="line x" title="74:169	The method of (Esuli and Sebastiani, 2005) starts from two small seed (i.e. training) sets Lp and Ln of known Positive and Negativeterms, respectively, and expands them into the two final training sets Trp  Lp andTrn  Ln byadding them new sets of terms Up and Un found by navigating the WordNet graph along the synonymy and antonymy relations3." ></td>
	<td class="line x" title="75:169	This process is based on the hypothesis that synonymy and antonymy, in addition to defining a relation of meaning, also define a relation of orientation, i.e. that two synonyms typically have the same orientation and two antonyms typically have opposite orientation." ></td>
	<td class="line x" title="76:169	The method is iterative, generating two sets Trkp and Trkn at each iteration k, where Trkp  Trk1p   Tr1p = Lp and Trkn  Trk1n    Tr1n = Ln." ></td>
	<td class="line x" title="77:169	At iteration k, Trkp is obtained by adding to Trk1p all synonyms of terms in Trk1p and all antonyms of terms in Trk1n ; similarly, Trkn is obtained by adding to Trk1n all synonyms of terms in Trk1n and allantonyms oftermsinTrk1p . Ifatotal ofK iterations are performed, then Tr = TrKp TrKn . The second main feature of the method presented in (Esuli and Sebastiani, 2005) is that terms are given vectorial representations based on their WordNet glosses (i.e. textual definitions)." ></td>
	<td class="line x" title="78:169	For each term ti in TrTe (Te being the test set, i.e. thesetoftermstobeclassified), atextual representation of ti is generated by collating all the glosses of ti as found in WordNet4." ></td>
	<td class="line x" title="79:169	Each such represen3Several other WordNet lexical relations, and several combinations of them, are tested in (Esuli and Sebastiani, 2005)." ></td>
	<td class="line x" title="80:169	In the present paper we only use the best-performing such combination, as described in detail in Section 4.2." ></td>
	<td class="line x" title="81:169	The version of WordNet used here and in (Esuli and Sebastiani, 2005) is 2.0." ></td>
	<td class="line x" title="82:169	4In general a term ti may have more than one gloss, since tation is converted into vectorial form by standard text indexing techniques (in (Esuli and Sebastiani, 2005) and in the present work, stop words are removed and the remaining words are weighted by cosine-normalized tfidf; no stemming is performed)5." ></td>
	<td class="line x" title="83:169	This representation method is based on the assumption that terms with a similar orientation tend to have similar glosses: for instance, that the glosses of honest and intrepid will both contain appreciative expressions, while the glosses of disturbing and superfluous will both contain derogative expressions." ></td>
	<td class="line x" title="84:169	Note that this method allows to classify any term, independently of its POS, provided there is a gloss for it in the lexical resource." ></td>
	<td class="line x" title="85:169	Once the vectorial representations for all terms inTrTehavebeengenerated, thosefortheterms in Tr are fed to a supervised learner, which thus generates a binary classifier." ></td>
	<td class="line x" title="86:169	This latter, once fed with the vectorial representations of the terms in Te, classifies each of them as either Positive or Negative." ></td>
	<td class="line x" title="87:169	4 Experiments In this paper we extend the method of (Esuli and Sebastiani, 2005) tothedetermination oftermsubjectivity and term orientation altogether." ></td>
	<td class="line x" title="88:169	4.1 Test sets The benchmark (i.e. test set) we use for our experiments is the General Inquirer (GI) lexicon (Stone et al. , 1966)." ></td>
	<td class="line x" title="89:169	This is a lexicon of terms labelled according to a large set of categories6, each one denoting the presence of a specific trait in the term." ></td>
	<td class="line x" title="90:169	The two main categories, and the ones we will be concerned with, are Positive/Negative, which contain 1,915/2,291 terms having a positive/negative orientation (in what follows we will also refer to the category Subjective, which we define as the union of the two categories Positive and Negative)." ></td>
	<td class="line x" title="91:169	In opinion mining research the GI was first used by Turney and Littman (2003), who reduced the list of terms to 1,614/1,982 entries afit may have more than one sense; dictionaries normally associate one gloss to each sense." ></td>
	<td class="line x" title="92:169	5Several combinations of subparts of a WordNet gloss are tested as textual representations of terms in (Esuli and Sebastiani, 2005)." ></td>
	<td class="line x" title="93:169	Of all those combinations, in the present paper we always use the DGS combination, since this is the one that has been shown to perform best in (Esuli and Sebastiani, 2005)." ></td>
	<td class="line x" title="94:169	DGS corresponds to using the entire gloss and performing negation propagation on itstext, i.e. replacing allthe terms that occur after a negation in a sentence with negated versions of the term (see (Esuli and Sebastiani, 2005) for details)." ></td>
	<td class="line x" title="95:169	6The definitions of all such categories are available at http://www.webuse.umd.edu:9090/ 196 terremoving 17termsappearing inboth categories (e.g. deal) and reducing all the multiple entries of the same term in a category, caused by multiple senses, to a single entry." ></td>
	<td class="line x" title="96:169	Likewise, we take all the 7,582 GI terms that are not labelled as either Positive or Negative, as being (implicitly) labelled as Objective, and reduce them to 5,009 terms after combining multiple entries of the same term, caused by multiple senses, to a single entry." ></td>
	<td class="line x" title="97:169	The effectiveness of our classifiers will thus be evaluated in terms of their ability to assign the total 8,605 GI terms to the correct category among Positive, Negative, and Objective7." ></td>
	<td class="line x" title="98:169	4.2 Seed sets and training sets Similarly to (Esuli and Sebastiani, 2005), our training set is obtained by expanding initial seed sets by means of WordNet lexical relations." ></td>
	<td class="line x" title="99:169	The main difference is that our training set is now the union of three sets of training terms Tr = TrKp TrKn TrKo obtained byexpanding, through K iterations, three seed sets Tr1p,Tr1n,Tr1o, one for each of the categories Positive, Negative, and Objective, respectively." ></td>
	<td class="line x" title="100:169	Concerning categories Positive and Negative, we have used the seed sets, expansion policy, and number of iterations, that have performed best in the experiments of (Esuli and Sebastiani, 2005), i.e. the seed sets Tr1p = {good} and Tr1n = {bad} expanded by using the union of synonymy and indirect antonymy, restricting the relations only to terms with the same POS of the original terms (i.e. adjectives), for a total of K = 4 iterations." ></td>
	<td class="line x" title="101:169	The final expanded sets contain 6,053 Positive terms and 6,874 Negative terms." ></td>
	<td class="line x" title="102:169	Concerning the category Objective, the process we have followed is similar, but with a few key differences." ></td>
	<td class="line x" title="103:169	These are motivated by the fact that the Objective category coincides with the complement of the union of Positive and Negative; therefore, Objective terms are more varied and diverse in meaning than the terms in the other two categories." ></td>
	<td class="line x" title="104:169	To obtain a representative expanded set TrKo, we have chosen the seed set Tr1o = {entity} and we have expanded it by using, along with synonymy and antonymy, the WordNet relation of hyponymy (e.g. vehicle / car),andwithout imposing the restriction that the two related terms must have the same POS." ></td>
	<td class="line x" title="105:169	These choices are strictly related to each other: the term entityis the root term of the largest generalization hierarchy in WordNet, with more than 40,000 7We make this labelled term set available for download at http://patty.isti.cnr.it/esuli/software/ SentiGI.tgz." ></td>
	<td class="line x" title="106:169	terms (Devitt and Vogel, 2004), thus allowing to reach a very large number of terms by using the hyponymy relation8." ></td>
	<td class="line x" title="107:169	Moreover, it seems reasonable to assume that terms that refer to entities are likely to have an objective nature, and that hyponyms (and also synonyms and antonyms) of an objective term are also objective." ></td>
	<td class="line x" title="108:169	Note that, at each iteration k, a given term t is added to Trko only if it does not already belong to either Trp or Trn." ></td>
	<td class="line x" title="109:169	We experiment with two different choices for the Tro set, corresponding to the sets generated in K = 3 and K = 4 iterations, respectively; this yields sets Tr3o and Tr4o consisting of 8,353 and 33,870 training terms, respectively." ></td>
	<td class="line x" title="110:169	4.3 Learning approaches and evaluation measures We experiment with three philosophically different learning approaches to the problem of distinguishing between Positive, Negative, and Objective terms." ></td>
	<td class="line x" title="111:169	Approach I is a two-stage method which consists in learning two binary classifiers: the first classifier places terms into either Subjective or Objective, while the second classifier places terms that have been classified as Subjective by thefirstclassifier into either Positive orNegative." ></td>
	<td class="line x" title="112:169	In the training phase, the terms in TrKp TrKn are used as training examples of category Subjective." ></td>
	<td class="line x" title="113:169	Approach II is again based on learning two binary classifiers." ></td>
	<td class="line x" title="114:169	Here, one of them must discriminate between terms that belong to the Positive category and ones that belong to its complement (not Positive), while the other must discriminate between terms that belong to the Negative category and ones that belong to its complement (not Negative)." ></td>
	<td class="line x" title="115:169	Terms that have been classified both into Positive by the former classifier and into (not Negative) by the latter are deemed to be positive, and terms that have been classified both into (not Positive) by the former classifier and into Negative by the latter are deemed to be negative." ></td>
	<td class="line x" title="116:169	The terms that have been classified (i) into both (not Positive) and (not Negative), or (ii) into both Positive and Negative, are taken to be Objective." ></td>
	<td class="line x" title="117:169	In the training phase of Approach II, the terms in TrKn  TrKo are used as training examples of category (not Positive), and the terms in TrKp TrKo are used as training examples of category (not Negative)." ></td>
	<td class="line x" title="118:169	Approach III consists instead in viewing Positive, Negative, and Objective as three categories 8The synonymy relation connects instead only 10,992 terms at most (Kamps et al. , 2004)." ></td>
	<td class="line x" title="119:169	197 with equal status, and in learning a ternary classifier that classifies each term into exactly one among the three categories." ></td>
	<td class="line x" title="120:169	There are several differences among these three approaches." ></td>
	<td class="line x" title="121:169	A first difference, of a conceptual nature, is that only Approaches I and III view Objective as a category, or concept, in its own right, while Approach II views objectivity as a nonexistent entity, i.e. as the absence of subjectivity (in fact, in Approach II the training examples of Objective are only used as training examples of the complements of Positive and Negative)." ></td>
	<td class="line x" title="122:169	Asecond difference isthatApproaches Iand II are based on standard binary classification technology, while Approach III requires multiclass (i.e. 1-of-m) classification." ></td>
	<td class="line x" title="123:169	As a consequence, while for the former we use well-known learners for binary classification (the naive Bayesian learner using the multinomial model (McCallum and Nigam, 1998), support vector machines using linear kernels (Joachims, 1998), the Rocchio learner, and its PrTFIDFprobabilistic version (Joachims, 1997)), for Approach III we use their multiclass versions9." ></td>
	<td class="line x" title="124:169	Before running our learners we make a pass of feature selection, with the intent of retaining only those features that are good at discriminating our categories, while discarding those which are not." ></td>
	<td class="line x" title="125:169	Feature selection is implemented by scoring each feature fk (i.e. each term that occurs in the glosses of at least one training term) by means of the mutual information (MI) function, defined as MI(fk) = summationdisplay c{c1,,cm}, f{fk,fk} Pr(f,c)  log Pr(f,c)Pr(f)Pr(c) (1) and discarding the x% features fk that minimize it." ></td>
	<td class="line x" title="126:169	We will call x% the reduction factor." ></td>
	<td class="line x" title="127:169	Note that thesetc1,,cm}fromEquation 1isinterpreted differently in Approaches I to III, and always consistently with who the categories at stake are." ></td>
	<td class="line x" title="128:169	Since the task we aim to solve is manifold, we will evaluate our classifiers according to two evaluation measures:  SO-accuracy, i.e. the accuracy of a classifier inseparating SubjectivefromObjective,i.e. in deciding term subjectivity alone;  PNO-accuracy, the accuracy of a classifier in discriminating among Positive, Negative, 9The naive Bayesian, Rocchio, and PrTFIDF learners we have used are from Andrew McCallums Bow package (http://www-2.cs.cmu.edu/mccallum/bow/), while the SVMs learner we have used is Thorsten Joachims SV Mlight (http://svmlight.joachims.org/), version 6.01." ></td>
	<td class="line x" title="129:169	Both packages allow the respective learners to be run in multiclass fashion." ></td>
	<td class="line x" title="130:169	Table 1: Average and best accuracy values over the four dimensions analysed in the experiments." ></td>
	<td class="line x" title="131:169	Dimension SO-accuracy PNO-accuracy Avg () Best Avg () Best Approach I .635 (.020) .668 .595 (.029) .635 II .636 (.033) .676 .614 (.037) .660 III .635 (.036) .674 .600 (.039) .648 Learner NB .653 (.014) .674 .619 (.022) .647 SVMs .627 (.033) .671 .601 (.037) .658 Rocchio .624 (.030) .654 .585 (.033) .616 PrTFIDF .637 (.031) .676 .606 (.042) .660 TSR 0% .649 (.025) .676 .619 (.027) .660 50% .650 (.022) .670 .622 (.022) .657 80% .646 (.023) .674 .621 (.021) .647 90% .642 (.024) .667 .616 (.024) .651 95% .635 (.027) .671 .606 (.031) .658 99% .612 (.036) .661 .570 (.049) .647 TrKo set Tr3o .645 (.006) .676 .608 (.007) .658 Tr4o .633 (.013) .674 .610 (.018) .660 and Objective, i.e. in deciding both term orientation and subjectivity." ></td>
	<td class="line x" title="132:169	5 Results We present results obtained from running every combination of (i) the three approaches to classification described in Section 4.3, (ii) the four learners mentioned in the same section, (iii) five different reduction factors for feature selection (0%, 50%, 90%, 95%, 99%), and (iv) the two different training sets (Tr3o and Tr4o) for Objective mentioned in Section 4.2." ></td>
	<td class="line x" title="133:169	We discuss each of these four dimensions of the problem individually, for each one reporting results averaged across all the experiments we have run (see Table 1)." ></td>
	<td class="line x" title="134:169	The first and most important observation is that, with respect to a pure term orientation task, accuracy drops significantly." ></td>
	<td class="line x" title="135:169	In fact, the best SOaccuracy and the best PNO-accuracy results obtained across the 120 different experiments are .676 and .660, respectively (these were obtained by using Approach II with the PrTFIDF learner and no feature selection, with Tro = Tr3o for the .676 SO-accuracy result and Tro = Tr4o for the .660 PNO-accuracy result); this contrasts sharply with the accuracy obtained in (Esuli and Sebastiani, 2005) on discriminating Positive from Negative (where the best run obtained .830 accuracy), on the same benchmarks and essentially the same algorithms." ></td>
	<td class="line x" title="136:169	This suggests that good performance at orientation detection (as e.g. in (Esuli and Sebastiani, 2005; Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003)) may not be a 198 Table 2: Human inter-coder agreement values reported by Kim and Hovy (2004)." ></td>
	<td class="line x" title="137:169	Agreement Adjectives (462) Verbs (502) measure Hum1 vs Hum2 Hum2 vs Hum3 Strict .762 .623 Lenient .890 .851 guarantee of good performance at subjectivity detection, quite evidently a harder (and, as we have suggested, more realistic) task." ></td>
	<td class="line x" title="138:169	This hypothesis is confirmed by an experiment performed by Kim and Hovy (2004) on testing the agreement of two human coders at tagging words with the Positive, Negative, and Objective labels." ></td>
	<td class="line x" title="139:169	The authors define two measures of such agreement: strict agreement, equivalent to our PNO-accuracy, and lenient agreement, which measures the accuracy at telling Negative against the rest." ></td>
	<td class="line x" title="140:169	For any experiment, strict agreement values are then going to be, by definition, lower or equal than the corresponding lenient ones." ></td>
	<td class="line x" title="141:169	Theauthors use two sets of 462 adjectives and 502 verbs, respectively, randomly extracted from the basic English word list of the TOEFL test." ></td>
	<td class="line x" title="142:169	The intercoder agreement results (see Table 2) show a deterioration in agreement (from lenient to strict) of 16.77% for adjectives and 36.42% for verbs." ></td>
	<td class="line x" title="143:169	Following this, we evaluated our best experiment according to these measures, and obtained a strict accuracy value of .660 and a lenient accuracy value of .821, with a relative deterioration of 24.39%, in line with Kim and Hovys observation10." ></td>
	<td class="line x" title="144:169	This confirms that determining subjectivity and orientation is a much harder task than determining orientation alone." ></td>
	<td class="line x" title="145:169	The second important observation is that there is very little variance in the results: across all 120 experiments, average SO-accuracy and PNOaccuracy results were .635 (with standard deviation  = .030) and .603 ( = .036), a mere 6.06% and 8.64% deterioration from the best results reported above." ></td>
	<td class="line x" title="146:169	This seems to indicate that the levels of performance obtained may be hard to improve upon, especially if working in a similar framework." ></td>
	<td class="line x" title="147:169	Let us analyse the individual dimensions of the problem." ></td>
	<td class="line x" title="148:169	Concerning the three approaches to classification described in Section 4.3, Approach II outperforms the other two, but by an extremely narrow margin." ></td>
	<td class="line x" title="149:169	As for the choice of learners, on average the best performer is NB, but again by a very small margin wrt the others." ></td>
	<td class="line x" title="150:169	On average, the 10We observed this trend in all of our experiments." ></td>
	<td class="line x" title="151:169	best reduction factor for feature selection turns out to be 50%, but the performance drop we witness in approaching 99% (a dramatic reduction factor) is extremely graceful." ></td>
	<td class="line x" title="152:169	As for the choice of TrKo, we note that Tr3o and Tr4o elicit comparable levels of performance, with the former performing best at SO-accuracy and the latter performing best at PNO-accuracy." ></td>
	<td class="line x" title="153:169	An interesting observation on the learners we have used is that NB, PrTFIDF and SVMs, unlike Rocchio, generate classifiers that depend on P(ci), the prior probabilities of the classes, which are normally estimated as the proportion of training documents that belong to ci." ></td>
	<td class="line x" title="154:169	In many classification applications this is reasonable, as we may assume that the training data are sampled from the samedistribution fromwhichthetestdataaresampled, and that these proportions are thus indicative of the proportions that we are going to encounter in the test data." ></td>
	<td class="line x" title="155:169	However, in our application this is not the case, since we do not have a natural sample of training terms." ></td>
	<td class="line x" title="156:169	What we have is one human-labelled training term for each category in {Positive,Negative,Objective}, and as many machine-labelled terms as we deem reasonable to include, in possibly different numbers for the different categories; and we have no indication whatsoever as to what the natural proportions among the three might be." ></td>
	<td class="line x" title="157:169	This means that the proportions of Positive, Negative, and Objective terms we decide to include in the training set will strongly bias the classification results if the learner is one of NB, PrTFIDF and SVMs." ></td>
	<td class="line x" title="158:169	We may notice this by looking at Table 3, which shows the average proportion of test terms classified as Objective by each learner, depending on whether we have chosen Tro to coincide with Tr3o or Tr4o; note that the former (resp." ></td>
	<td class="line x" title="159:169	latter) choice means having roughly as many (resp." ></td>
	<td class="line x" title="160:169	roughly five times as many) Objective training terms as there are Positive and Negative ones." ></td>
	<td class="line x" title="161:169	Table 3 shows that, the more Objective training terms there are, the more test terms NB, PrTFIDF and (in particular) SVMs will classify as Objective; this is not true for Rocchio, which is basically unaffected by the variation in size of Tro." ></td>
	<td class="line x" title="162:169	6 Conclusions We have presented a method for determining both term subjectivity and term orientation for opinion mining applications." ></td>
	<td class="line x" title="163:169	This is a valuable advance with respect to the state of the art, since past work in this area had mostly confined to determining term orientation alone, a task that (as we have ar199 Table 3: Average proportion of test terms classified as Objective, for each learner and for each choice of the TrKo set." ></td>
	<td class="line x" title="164:169	Learner Tr3o Tr4o Variation NB .564 ( = .069) .693 (.069) +23.0% SVMs .601 (.108) .814 (.083) +35.4% Rocchio .572 (.043) .544 (.061) -4.8% PrTFIDF .636 (.059) .763 (.085) +20.0% gued) has limited practical significance in itself, given the generalized absence of lexical resources that tag terms as being either Subjective or Objective." ></td>
	<td class="line x" title="165:169	Our algorithms have tagged by orientation and subjectivity the entire General Inquirer lexicon, a complete general-purpose lexicon that is the de facto standard benchmark for researchers in this field." ></td>
	<td class="line x" title="166:169	Our results thus constitute, for this task, the first baseline for other researchers to improve upon." ></td>
	<td class="line x" title="167:169	Unfortunately, our results have shown that an algorithm that had shown excellent, stateof-the-art performance in deciding term orientation (Esuli and Sebastiani, 2005), once modified for the purposes of deciding term subjectivity, performs more poorly." ></td>
	<td class="line x" title="168:169	This has been shown by testing several variants of the basic algorithm, some of them involving radically different supervised learning policies." ></td>
	<td class="line x" title="169:169	The results suggest that deciding term subjectivity is a substantially harder task that deciding term orientation alone." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N06-1027
Learning To Detect Conversation Focus Of Threaded Discussions
Feng, Donghui;Shaw, Erin;Kim, Jihie;Hovy, Eduard H.;"></td>
	<td class="line x" title="1:204	Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 208215, New York, June 2006." ></td>
	<td class="line x" title="2:204	c2006 Association for Computational Linguistics Learning to Detect Conversation Focus of Threaded Discussions Donghui Feng Erin Shaw Jihie Kim Eduard Hovy Information Sciences Institute University of Southern California Marina del Rey, CA, 90292 {donghui, shaw, jihie, hovy}@isi.edu Abstract In this paper we present a novel featureenriched approach that learns to detect the conversation focus of threaded discussions by combining NLP analysis and IR techniques." ></td>
	<td class="line x" title="3:204	Using the graph-based algorithm HITS, we integrate different features such as lexical similarity, poster trustworthiness, and speech act analysis of human conversations with featureoriented link generation functions." ></td>
	<td class="line x" title="4:204	It is the first quantitative study to analyze human conversation focus in the context of online discussions that takes into account heterogeneous sources of evidence." ></td>
	<td class="line x" title="5:204	Experimental results using a threaded discussion corpus from an undergraduate class show that it achieves significant performance improvements compared with the baseline system." ></td>
	<td class="line x" title="6:204	1 Introduction Threaded discussion is popular in virtual cyber communities and has applications in areas such as customer support, community development, interactive reporting (blogging) and education." ></td>
	<td class="line x" title="7:204	Discussion threads can be considered a special case of human conversation, and since we have huge repositories of such discussion, automatic and/or semi-automatic analysis would greatly improve the navigation and processing of the information." ></td>
	<td class="line x" title="8:204	A discussion thread consists of a set of messages arranged in chronological order." ></td>
	<td class="line x" title="9:204	One of the main challenges in the Question Answering domain is how to extract the most informative or important message in the sequence for the purpose of answering the initial question, which we refer to as the conversation focus in this paper." ></td>
	<td class="line x" title="10:204	For example, people may repeatedly discuss similar questions in a discussion forum and so it is highly desirable to detect previous conversation focuses in order to automatically answer queries (Feng et al. , 2006)." ></td>
	<td class="line x" title="11:204	Human conversation focus is a hard NLP (Natural Language Processing) problem in general because people may frequently switch topics in a real conversation." ></td>
	<td class="line x" title="12:204	The threaded discussions make the problem manageable because people typically focus on a limited set of issues within a thread of a discussion." ></td>
	<td class="line x" title="13:204	Current IR (Information Retrieval) techniques are based on keyword similarity measures and do not consider some features that are important for analyzing threaded discussions." ></td>
	<td class="line x" title="14:204	As a result, a typical IR system may return a ranked list of messages based on keyword queries even if, within the context of a discussion, this may not be useful or correct." ></td>
	<td class="line x" title="15:204	Threaded discussion is a special case of human conversation, where people may express their ideas, elaborate arguments, and answer others questions; many of these aspects are unexplored by traditional IR techniques." ></td>
	<td class="line x" title="16:204	First, messages in threaded discussions are not a flat document set, which is a common assumption for most IR systems." ></td>
	<td class="line x" title="17:204	Due to the flexibility and special characteristics involved in human conversations, messages within a thread are not necessarily of equal importance." ></td>
	<td class="line x" title="18:204	The real relationships may differ from the analysis based on keyword similarity measures, e.g., if a 2 nd message corrects a 1 st one, the 2 nd message is probably more important than the 1 st. IR systems may give different results." ></td>
	<td class="line x" title="19:204	Second, messages posted by different users may have different degrees of correctness and trustworthiness, which we refer to as poster trustworthiness in this paper." ></td>
	<td class="line x" title="20:204	For instance, a domain expert is likely to be more reliable than a layman on the domain topic." ></td>
	<td class="line x" title="21:204	208 In this paper we present a novel feature-enriched approach that learns to detect conversation focus of threaded discussions by combining NLP analysis and IR techniques." ></td>
	<td class="line x" title="22:204	Using the graph-based algorithm HITS (Hyperlink Induced Topic Search, Kleinberg, 1999), we conduct discussion analysis taking into account different features, such as lexical similarity, poster trustworthiness, and speech act relations in human conversations." ></td>
	<td class="line x" title="23:204	We generate a weighted threaded discussion graph by applying feature-oriented link generation functions." ></td>
	<td class="line x" title="24:204	All the features are quantified and integrated as part of the weight of graph edges." ></td>
	<td class="line x" title="25:204	In this way, both quantitative features and qualitative features are combined to analyze human conversations, specifically in the format of online discussions." ></td>
	<td class="line x" title="26:204	To date, it is the first quantitative study to analyze human conversation that focuses on threaded discussions by taking into account heterogeneous evidence from different sources." ></td>
	<td class="line x" title="27:204	The study described here addresses the problem of conversation focus, especially for extracting the best answer to a particular question, in the context of an online discussion board used by students in an undergraduate computer science course." ></td>
	<td class="line x" title="28:204	Different features are studied and compared when applying our approach to discussion analysis." ></td>
	<td class="line x" title="29:204	Experimental results show that performance improvements are significant compared with the baseline system." ></td>
	<td class="line x" title="30:204	The remainder of this paper is organized as follows: We discuss related work in Section 2." ></td>
	<td class="line x" title="31:204	Section 3 presents thread representation and the weighted HITS algorithm." ></td>
	<td class="line x" title="32:204	Section 4 details feature-oriented link generation functions." ></td>
	<td class="line x" title="33:204	Comparative experimental results and analysis are given in Section 5." ></td>
	<td class="line x" title="34:204	We discuss future work in Section 6." ></td>
	<td class="line x" title="35:204	2 Related Work Human conversation refers to situations where two or more participants freely alternate in speaking (Levinson, 1983)." ></td>
	<td class="line x" title="36:204	What makes threaded discussions unique is that users participate asynchronously and in writing." ></td>
	<td class="line x" title="37:204	We model human conversation as a set of messages in a threaded discussion using a graph-based algorithm." ></td>
	<td class="line x" title="38:204	Graph-based algorithms are widely applied in link analysis and for web searching in the IR community." ></td>
	<td class="line x" title="39:204	Two of the most prominent algorithms are Page-Rank (Brin and Page, 1998) and the HITS algorithm (Kleinberg, 1999)." ></td>
	<td class="line x" title="40:204	Although they were initially proposed for analyzing web pages, they proved useful for investigating and ranking structured objects." ></td>
	<td class="line oc" title="41:204	Inspired by the idea of graph based algorithms to collectively rank and select the best candidate, research efforts in the natural language community have applied graph-based approaches on keyword selection (Mihalcea and Tarau, 2004), text summarization (Erkan and Radev, 2004; Mihalcea, 2004), word sense disambiguation (Mihalcea et al. , 2004; Mihalcea, 2005), sentiment analysis (Pang and Lee, 2004), and sentence retrieval for question answering (Otterbacher et al. , 2005)." ></td>
	<td class="line x" title="42:204	However, until now there has not been any published work on its application to human conversation analysis specifically in the format of threaded discussions." ></td>
	<td class="line x" title="43:204	In this paper, we focus on using HITS to detect conversation focus of threaded discussions." ></td>
	<td class="line x" title="44:204	Rhetorical Structure Theory (Mann and Thomson, 1988) based discourse processing has attracted much attention with successful applications in sentence compression and summarization." ></td>
	<td class="line x" title="45:204	Most of the current work on discourse processing focuses on sentence-level text organization (Soricut and Marcu, 2003) or the intermediate step (Sporleder and Lapata, 2005)." ></td>
	<td class="line x" title="46:204	Analyzing and utilizing discourse information at a higher level, e.g., at the paragraph level, still remains a challenge to the natural language community." ></td>
	<td class="line x" title="47:204	In our work, we utilize the discourse information at a message level." ></td>
	<td class="line x" title="48:204	Zhou and Hovy (2005) proposed summarizing threaded discussions in a similar fashion to multidocument summarization; but then their work does not take into account the relative importance of different messages in a thread." ></td>
	<td class="line x" title="49:204	Marom and Zukerman (2005) generated help-desk responses using clustering techniques, but their corpus is composed of only two-party, two-turn, conversation pairs, which precludes the need to determine relative importance as in a multi-ply conversation." ></td>
	<td class="line x" title="50:204	In our previous work (Feng et al. , 2006), we implemented a discussion-bot to automatically answer student queries in a threaded discussion but extract potential answers (the most informative message) using a rule-based traverse algorithm that is not optimal for selecting a best answer; thus, the result may contain redundant or incorrect information." ></td>
	<td class="line x" title="51:204	We argue that pragmatic knowledge like speech acts is important in conversation focus analysis." ></td>
	<td class="line x" title="52:204	However, estimated speech act labeling between messages is not sufficient for detecting 209 human conversation focus without considering other features like author information." ></td>
	<td class="line x" title="53:204	Carvalho and Cohen (2005) describe a dependency-network based collective classification method to classify email speech acts." ></td>
	<td class="line x" title="54:204	Our work on conversation focus detection can be viewed as an immediate step following automatic speech act labeling on discussion threads using similar collective classification approaches." ></td>
	<td class="line x" title="55:204	We next discuss our approach to detect conversation focus using the graph-based algorithm HITS by taking into account heterogeneous features." ></td>
	<td class="line x" title="56:204	3 Conversation Focus Detection In threaded discussions, people participate in a conversation by posting messages." ></td>
	<td class="line x" title="57:204	Our goal is to be able to detect which message in a thread contains the most important information, i.e., the focus of the conversation." ></td>
	<td class="line x" title="58:204	Unlike traditional IR systems, which return a ranked list of messages from a flat document set, our task must take into account characteristics of threaded discussions." ></td>
	<td class="line x" title="59:204	First, messages play certain roles and are related to each other by a conversation context." ></td>
	<td class="line x" title="60:204	Second, messages written by different authors may vary in value." ></td>
	<td class="line x" title="61:204	Finally, since postings occur in parallel, by various people, message threads are not necessarily coherent so the lexical similarity among the messages should be analyzed." ></td>
	<td class="line x" title="62:204	To detect the focus of conversation, we integrate a pragmatics study of conversational speech acts, an analysis of message values based on poster trustworthiness and an analysis of lexical similarity." ></td>
	<td class="line x" title="63:204	The subsystems that determine these three sources of evidence comprise the features of our feature-based system." ></td>
	<td class="line x" title="64:204	Because each discussion thread is naturally represented by a directed graph, where each message is represented by a node in the graph, we can apply a graph-based algorithm to integrate these sources and detect the focus of conversation." ></td>
	<td class="line x" title="65:204	3.1 Thread Representation A discussion thread consists of a set of messages posted in chronological order." ></td>
	<td class="line x" title="66:204	Suppose that each message is represented by m i, i =1,2,, n. Then the entire thread is a directed graph that can be represented by G= (V, E), where V is the set of nodes (messages), V= {m i,i=1,,n}, and E is the set of directed edges." ></td>
	<td class="line x" title="67:204	In our approach, the set V is automatically constructed as each message joins in the discussion." ></td>
	<td class="line x" title="68:204	E is a subset of VxV." ></td>
	<td class="line x" title="69:204	We will discuss the feature-oriented link generation functions that construct the set E in Section 4." ></td>
	<td class="line x" title="70:204	We make use of speech act relations in generating the links." ></td>
	<td class="line x" title="71:204	Once a speech act relation is identified between two messages, links will be generated using generation functions described in next section." ></td>
	<td class="line x" title="72:204	When m i is a message node in the thread graph, VmF i )( represents the set of nodes that node m i points to (i.e. , children of m i ), and VmB i )( represents the set of nodes that point to m i (i.e. , parents of m i )." ></td>
	<td class="line x" title="73:204	3.2 Graph-Based Ranking Algorithm: HITS Graph-based algorithms can rank a set of objects in a collective way and the affect between each pair can be propagated into the whole graph iteratively." ></td>
	<td class="line x" title="74:204	Here, we use a weighted HITS (Kleinberg, 1999) algorithm to conduct message ranking." ></td>
	<td class="line x" title="75:204	Kleinberg (1999) initially proposed the graphbased algorithm HITS for ranking a set of web pages." ></td>
	<td class="line x" title="76:204	Here, we adjust the algorithm for the task of ranking a set of messages in a threaded discussion." ></td>
	<td class="line x" title="77:204	In this algorithm, each message in the graph can be represented by two identity scores, hub score and authority score." ></td>
	<td class="line x" title="78:204	The hub score represents the quality of the message as a pointer to valuable or useful messages (or resources, in general)." ></td>
	<td class="line x" title="79:204	The authority score measures the quality of the message as a resource itself." ></td>
	<td class="line x" title="80:204	The weighted iterative updating computations are shown in Equations 1 and 2." ></td>
	<td class="line x" title="81:204	  + = )( 1 )(*)( ij mFm j r iji r mauthoritywmhub (1)   + = )( 1 )(*)( ij mBm j r jii r mhubwmauthority (2) where r and r+1 are the numbers of iterations." ></td>
	<td class="line x" title="82:204	The number of iterations required for HITS to converge depends on the initialization value for each message node and the complexity of the graph." ></td>
	<td class="line x" title="83:204	Graph links can be induced with extra knowledge (e.g. Kurland and Lee, 2005)." ></td>
	<td class="line x" title="84:204	To help integrate our heterogeneous sources of evidence with our graph-based HITS algorithm, we introduce link generation functions for each of the three features, (g i, i=1, 2, 3), to add links between messages." ></td>
	<td class="line x" title="85:204	4 Feature-Oriented Link Generation 210 Conversation structures have received a lot of attention in the linguistic research community (Levinson, 1983)." ></td>
	<td class="line x" title="86:204	In order to integrate conversational features into our computational model, we must convert a qualitative analysis into quantitative scores." ></td>
	<td class="line x" title="87:204	For conversation analysis, we adopted the theory of Speech Acts proposed by (Austin, 1962; Searle, 1969) and defined a set of speech acts (SAs) that relate every pair of messages in the corpus." ></td>
	<td class="line x" title="88:204	Though a pair of messages may only be labeled with one speech act, a message can have multiple SAs with other messages." ></td>
	<td class="line x" title="89:204	We group speech acts by function into three categories, as shown in Figure 1." ></td>
	<td class="line x" title="90:204	Messages may involve a request (REQ), provide information (INF), or fall into the category of interpersonal (INTP) relationship." ></td>
	<td class="line x" title="91:204	Categories can be further divided into several single speech acts." ></td>
	<td class="line x" title="92:204	Figure 1." ></td>
	<td class="line x" title="93:204	Categories of Message Speech Act." ></td>
	<td class="line x" title="94:204	The SA set for our corpus is given in Table 1." ></td>
	<td class="line x" title="95:204	A speech act may a represent a positive, negative or neutral response to a previous message depending on its attitude and recommendation." ></td>
	<td class="line x" title="96:204	We classify each speech act as a direction as POSITIVE (+), NEGATIVE () or NEUTRAL, referred to as SA Direction, as shown in the right column of Table 1." ></td>
	<td class="line x" title="97:204	The features we wish to include in our approach are lexical similarity between messages, poster trustworthiness, and speech act labels between message pairs in our discussion corpus." ></td>
	<td class="line x" title="98:204	The feature-oriented link generation is conducted in two steps." ></td>
	<td class="line x" title="99:204	First, our approach examines in turn all the speech act relations in each thread and generates two types of links based on lexical similarity and SA strength scores." ></td>
	<td class="line x" title="100:204	Second, the system iterates over all the message nodes and assigns each node a self-pointing link associated with its poster trustworthiness score." ></td>
	<td class="line x" title="101:204	The three features are integrated into the thread graph accordingly by the feature-oriented link generation functions." ></td>
	<td class="line x" title="102:204	Multiple links with the same start and end points are combined into one." ></td>
	<td class="line x" title="103:204	Speech Act Name Description Dir." ></td>
	<td class="line x" title="104:204	ACK Acknowledge Confirm or acknowledge + CANS Complex Answer Give answer requiring a full description of procedures, reasons, etc. COMM Command Command or announce COMP Compliment Praise an argument or suggestion + CORR Correct Correct a wrong answer or solution  CRT Criticize Criticize an argument  DESC Describe Describe a fact or situation ELAB Elaborate Elaborate on a previous argument or question OBJ Object Object to an argument or suggestion  QUES Question Ask question about a specific problem SANS Simple Answer Answer with a short phrase or few words (e.g. factoid, yes/no) SUG Suggest Give advice or suggest a solution SUP Support Support an argument or suggestion + Table 1." ></td>
	<td class="line x" title="105:204	Types of message speech acts in corpus." ></td>
	<td class="line x" title="106:204	4.1 Lexical Similarity Discussions are constructed as people express ideas, opinions, and thoughts, so that the text itself contains information about what is being discussed." ></td>
	<td class="line x" title="107:204	Lexical similarity is an important measure for distinguishing relationships between message pairs." ></td>
	<td class="line x" title="108:204	In our approach, we do not compute the lexical similarity of any arbitrary pair of messages, instead, we consider only message pairs that are present in the speech act set." ></td>
	<td class="line x" title="109:204	The cosine similarity between each message pair is computed using the TF*IDF technique (Salton, 1989)." ></td>
	<td class="line x" title="110:204	Messages with similar words are more likely to be semantically-related." ></td>
	<td class="line x" title="111:204	This information is represented by term frequency (TF)." ></td>
	<td class="line x" title="112:204	However, those Inform: INF Interpersonal: INTP COMM QUES Speech Act Request: REQ ACK COMP CRT OBJ SUP CANS CORR DESC ELAB SANS SUG 211 with more general terms may be unintentionally biased when only TF is considered so Inverse Document Frequency (IDF) is introduced to mitigate the bias." ></td>
	<td class="line x" title="113:204	The lexical similarity score can be calculated using their cosine similarity." ></td>
	<td class="line x" title="114:204	),(cos_ ji l mmsimW = (3) For a given a speech act, SA ij (m i m j ), connecting message m i and m j, the link generation function g 1 is defined as follows: )()( 1 l ijij WarcSAg = (4) The new generated link is added to the thread graph connecting message node m i and m j with a weight of W l . 4.2 Poster Trustworthiness Messages posted by different people may have different degrees of trustworthiness." ></td>
	<td class="line x" title="115:204	For example, students who contributed to our corpus did not seem to provide messages of equal value." ></td>
	<td class="line x" title="116:204	To determine the trustworthiness of a person, we studied the responses to their messages throughout the entire corpus." ></td>
	<td class="line x" title="117:204	We used the percentage of POSITIVE responses to a persons messages to measure that persons trustworthiness." ></td>
	<td class="line x" title="118:204	In our case, POSITIVE responses, which are defined above, included SUP, COMP, and ACK." ></td>
	<td class="line x" title="119:204	In addition, if a persons message closed a discussion, we rated it POSITIVE." ></td>
	<td class="line x" title="120:204	Suppose the poster is represented by k person, the poster score, p W, is a weight calculated by ))(( ))(_( )( k k k p personfeedbackcount personfeedbackpositivecount personW = (5) For a given single speech act, SA ij (m i m j ), the poster score indicates the importance of message m i by itself and the generation function is given by )()( 2 p iiij WarcSAg = (6) The generated link is self-pointing, and contains the strength of the poster information." ></td>
	<td class="line x" title="121:204	4.3 Speech Act Analysis We compute the strength of each speech act in a generative way, based on the author and trustworthiness of the author." ></td>
	<td class="line x" title="122:204	The strength of a speech act is a weighted average over all authors." ></td>
	<td class="line x" title="123:204	)( )( )( )()( k P person person s personW SAcount SAcount dirsignSAW k k  = (7) where the sign function of direction is defined with Equation 8." ></td>
	<td class="line x" title="124:204	   = Otherwise 1 NEGATIVE isdir if 1 )(dirsign (8) All SA scores are computed using Equation 7 and projected to [0, 1]." ></td>
	<td class="line x" title="125:204	For a given speech act, SA ij (m i m j ), the generation function will generate a weighted link in the thread graph as expressed in Equation 9." ></td>
	<td class="line x" title="126:204	     = Otherwise )( NEUTRAL is if )( )( 3 s ij ij s ii ij Warc SAWarc SAg (9) The SA scores represent the strength of the relationship between the messages." ></td>
	<td class="line x" title="127:204	Depending on the direction of the SA, the generated link will either go from message m i to m j or from message m i to m i (i.e. , to itself)." ></td>
	<td class="line x" title="128:204	If the SA is NEUTRAL, the link will point to itself and the score is a recommendation to itself." ></td>
	<td class="line x" title="129:204	Otherwise, the link connects two different messages and represents the recommendation degree of the parent to the child message." ></td>
	<td class="line x" title="130:204	5 Experiments 5.1 Experimental Setup We tested our conversation-focus detection approach using a corpus of threaded discussions from three semesters of a USC undergraduate course in computer science." ></td>
	<td class="line x" title="131:204	The corpus includes a total of 640 threads consisting of 2214 messages, where a thread is defined as an exchange containing at least two messages." ></td>
	<td class="line x" title="132:204	Length of thread Number of threads 3 139 4 74 5 47 6 30 7 13 8 11 Table 2." ></td>
	<td class="line x" title="133:204	Thread length distribution." ></td>
	<td class="line x" title="134:204	From the complete corpus, we selected only threads with lengths of greater than two and less than nine (messages)." ></td>
	<td class="line x" title="135:204	Discussion threads with lengths of only two would bias the random guess of our baseline system, while discussion threads with lengths greater than eight make up only 3.7% of the total number of threads (640), and are the least coherent of the threads due to topic-switching and off-topic remarks." ></td>
	<td class="line x" title="136:204	Thus, our evaluation corpus included 314 threads, consisting of 1307 messages, with an average thread length of 4.16 messages per 212 thread." ></td>
	<td class="line x" title="137:204	Table 2 gives the distribution of the lengths of the threads." ></td>
	<td class="line x" title="138:204	The input of our system requires the identification of speech act relations between messages." ></td>
	<td class="line x" title="139:204	Collective classification approaches, similar to the dependency-network based approach that Carvalho and Cohen (2005) used to classify email speech acts, might also be applied to discussion threads." ></td>
	<td class="line x" title="140:204	However, as the paper is about investigating how an SA analysis, along with other features, can benefit conversation focus detection, so as to avoid error propagation from speech act labeling to subsequent processing, we used manually-annotated SA relationships for our analysis." ></td>
	<td class="line x" title="141:204	Code Frequency Percentage (%) ACK 53 3.96 CANS 224 16.73 COMM 8 0.6 COMP 7 0.52 CORR 20 1.49 CRT 23 1.72 DESC 71 5.3 ELAB 105 7.84 OBJ 21 1.57 QUES 450 33.61 SANS 23 1.72 SUG 264 19.72 SUP 70 5.23 Table 3." ></td>
	<td class="line x" title="142:204	Frequency of speech acts." ></td>
	<td class="line x" title="143:204	The corpus contains 1339 speech acts." ></td>
	<td class="line x" title="144:204	Table 3 gives the frequencies and percentages of speech acts found in the data set." ></td>
	<td class="line x" title="145:204	Each SA generates feature-oriented weighted links in the threaded graph accordingly as discussed previously." ></td>
	<td class="line x" title="146:204	Number of best answers Number of threads 1 250 2 56 3 5 4 3 Table 4." ></td>
	<td class="line x" title="147:204	Gold standard length distribution." ></td>
	<td class="line x" title="148:204	We then read each thread and choose the message that contained the best answer to the initial query as the gold standard." ></td>
	<td class="line x" title="149:204	If there are multiple best-answer messages, all of them will be ranked as best, i.e., chosen for the top position." ></td>
	<td class="line x" title="150:204	For example, different authors may have provided suggestions that were each correct for a specified situation." ></td>
	<td class="line x" title="151:204	Table 4 gives the statistics of the numbers of correct messages of our gold standard." ></td>
	<td class="line x" title="152:204	We experimented with further segmenting the messages so as to narrow down the best-answer text, under the assumption that long messages probably include some less-than-useful information." ></td>
	<td class="line x" title="153:204	We applied TextTiling (Hearst, 1994) to segment the messages, which is the technique used by Zhou and Hovy (2005) to summarize discussions." ></td>
	<td class="line x" title="154:204	For our corpus, though, the ratio of segments to messages was only 1.03, which indicates that our messages are relatively short and coherent, and that segmenting them would not provide additional benefits." ></td>
	<td class="line x" title="155:204	5.2 Baseline System To compare the effectiveness of our approach with different features, we designed a baseline system that uses a random guess approach." ></td>
	<td class="line x" title="156:204	Given a discussion thread, the baseline system randomly selects the most important message." ></td>
	<td class="line x" title="157:204	The result was evaluated against the gold standard." ></td>
	<td class="line x" title="158:204	The performance comparisons of the baseline system and other feature-induced approaches are presented next." ></td>
	<td class="line x" title="159:204	5.3 Result Analysis and Discussion We conducted extensive experiments to investigate the performance of our approach with different combinations of features." ></td>
	<td class="line x" title="160:204	As we discussed in Section 4.2, each poster acquires a trustworthiness score based on their behavior via an analysis of the whole corpus." ></td>
	<td class="line x" title="161:204	Table 5 is a sample list of some posters with their poster id, the total number of responses (to their messages), the total number of positive responses, and their poster scores p W . Poster ID Total Response Positive Response p W 193 1 1 1 93 20 18 0.9 38 15 12 0.8 80 8 6 0.75 47 253 182 0.719 22 3 2 0.667 44 9 6 0.667 91 6 4 0.667 147 12 8 0.667 32 10 6 0.6 190 9 5 0.556 97 20 11 0.55 12 2 1 0.5 Table 5." ></td>
	<td class="line x" title="162:204	Sample poster scores." ></td>
	<td class="line x" title="163:204	213 Based on the poster scores, we computed the strength score of each SA with Equation 7 and projected them to [0, 1]." ></td>
	<td class="line x" title="164:204	Table 6 shows the strength scores for all of the SAs." ></td>
	<td class="line x" title="165:204	Each SA has a different strength score and those in the NEGATIVE category have smaller ones (weaker recommendation)." ></td>
	<td class="line x" title="166:204	SA )(SAW s SA )(SAW s CANS 0.8134 COMM 0.6534 DESC 0.7166 ELAB 0.7202 SANS 0.8281 SUG 0.8032 QUES 0.6230 ACK 0.6844 COMP 0.8081 SUP 0.8057 CORR 0.2543 CRT 0.1339 OBJ 0.2405 Table 6." ></td>
	<td class="line x" title="167:204	SA strength scores." ></td>
	<td class="line x" title="168:204	We tested the graph-based HITS algorithm with different feature combinations and set the error rate to be 0.0001 to get the algorithm to converge." ></td>
	<td class="line x" title="169:204	In our experiments, we computed the precision score and the MRR (Mean Reciprocal Rank) score (Voorhees, 2001) of the most informative message chosen (the first, if there was more than one)." ></td>
	<td class="line x" title="170:204	Table 7 shows the performance scores for the system with different feature combinations." ></td>
	<td class="line x" title="171:204	The performance of the baseline system is shown at the top." ></td>
	<td class="line x" title="172:204	The HITS algorithm assigns both a hub score and an authority score to each message node, resulting in two sets of results." ></td>
	<td class="line x" title="173:204	Scores in the HITS_ AUTHORITY rows of Table 7 represent the results using authority scores, while HITS_HUB rows represent the results using hub scores." ></td>
	<td class="line x" title="174:204	Due to the limitation of thread length, the lower bound of the MRR score is 0.263." ></td>
	<td class="line x" title="175:204	As shown in the table, a random guess baseline system can get a precision of 27.71% and a MRR score of 0.539." ></td>
	<td class="line x" title="176:204	When we consider only lexical similarity, the result is not so good, which supports the notion that in human conversation context is often more important than text at a surface level." ></td>
	<td class="line x" title="177:204	When we consider poster and lexical score together, the performance improves." ></td>
	<td class="line x" title="178:204	As expected, the best performances use speech act analysis." ></td>
	<td class="line x" title="179:204	More features do not always improve the performance, for example, the lexical feature will sometimes decrease performance." ></td>
	<td class="line x" title="180:204	Our best performance produced a precision score of 70.38% and an MRR score of 0.825, which is a significant improvement over the baselines precision score of 27.71% and its MRR score of 0.539." ></td>
	<td class="line x" title="181:204	Algorithm & Features Correct (out of 314) Precision (%) MRR Baseline 87 27.71 0.539 Lexical 65 20.70 0.524 Poster 90 28.66 0.569 SA 215 68.47 0.819 Lexical + Poster 91 28.98 0.565 Lexical + SA 194 61.78 0.765 Poster + SA 221 70.38 0.825 HIT S _ A UT H O RIT Y Lexical + Poster + SA 212 67.52 0.793 Lexical 153 48.73 0.682 Poster 79 25.16 0.527 SA 195 62.10 0.771 Lexical + Poster 158 50.32 0.693 Lexical + SA 177 56.37 0.724 Poster + SA 207 65.92 0.793 HIT S _ H UB Lexical + Poster + SA 196 62.42 0.762 Table 7." ></td>
	<td class="line x" title="182:204	System Performance Comparison." ></td>
	<td class="line x" title="183:204	Another widely-used graph algorithm in IR is PageRank (Brin and Page, 1998)." ></td>
	<td class="line x" title="184:204	It is used to investigate the connections between hyperlinks in web page retrieval." ></td>
	<td class="line x" title="185:204	PageRank uses a random walk model of a web surfers behavior." ></td>
	<td class="line x" title="186:204	The surfer begins from a random node m i and at each step either follows a hyperlink with the probability of d, or jumps to a random node with the probability of (1-d)." ></td>
	<td class="line x" title="187:204	A weighted PageRank algorithm is used to model weighted relationships of a set of objects." ></td>
	<td class="line x" title="188:204	The iterative updating expression is    + += )( )( 1 )(*)1()( ij jk mBm j r mFm jk ji i r mPR w w ddmPR (10) where r and r+1 are the numbers of iterations." ></td>
	<td class="line x" title="189:204	We also tested this algorithm in our situation, but the best performance had a precision score of only 47.45% and an MRR score of 0.669." ></td>
	<td class="line x" title="190:204	It may be that PageRanks definition and modeling approach does not fit our situation as well as the HITS approach." ></td>
	<td class="line x" title="191:204	In HITS, the authority and hub214 based approach is better suited to human conversation analysis than PageRank, which only considers the contributions from backward links of each node in the graph." ></td>
	<td class="line x" title="192:204	6 Conclusions and Future Work We have presented a novel feature-enriched approach for detecting conversation focus of threaded discussions for the purpose of answering student queries." ></td>
	<td class="line x" title="193:204	Using feature-oriented link generation and a graph-based algorithm, we derived a unified framework that integrates heterogeneous sources of evidence." ></td>
	<td class="line x" title="194:204	We explored the use of speech act analysis, lexical similarity and poster trustworthiness to analyze discussions." ></td>
	<td class="line x" title="195:204	From the perspective of question answering, this is the first attempt to automatically answer complex and contextual discussion queries beyond factoid or definition questions." ></td>
	<td class="line x" title="196:204	To fully automate discussion analysis, we must integrate automatic SA labeling together with our conversation focus detection approach." ></td>
	<td class="line x" title="197:204	An automatic system will help users navigate threaded archives and researchers analyze human discussion." ></td>
	<td class="line x" title="198:204	Supervised learning is another approach to detecting conversation focus that might be explored." ></td>
	<td class="line x" title="199:204	The tradeoff and balance between system performance and human cost for different learning algorithms is of great interest." ></td>
	<td class="line x" title="200:204	We are also exploring the application of graph-based algorithms to other structured-objects ranking problems in NLP so as to improve system performance while relieving human costs." ></td>
	<td class="line x" title="201:204	Acknowledgements The work was supported in part by DARPA grant DOINBC Contract No." ></td>
	<td class="line x" title="202:204	NBCHC050051, Learning by Reading, and in part by a grant from the Lord Corporation Foundation to the USC Distance Education Network." ></td>
	<td class="line x" title="203:204	The authors want to thank Deepak Ravichandran, Feng Pan, and Rahul Bhagat for their helpful suggestions with the manuscript." ></td>
	<td class="line x" title="204:204	We would also like to thank the HLT-NAACL reviewers for their valuable comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-1133
Are These Documents Written From Different Perspectives? A Test Of Different Perspectives Based On Statistical Distribution Divergence
Lin, Wei-Hao;Hauptmann, Alexander G.;"></td>
	<td class="line x" title="1:268	Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 10571064, Sydney, July 2006." ></td>
	<td class="line x" title="2:268	c2006 Association for Computational Linguistics Are These Documents Written from Different Perspectives?" ></td>
	<td class="line x" title="3:268	A Test of Different Perspectives Based On Statistical Distribution Divergence Wei-Hao Lin Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 U.S.A. whlin@cs.cmu.edu Alexander Hauptmann Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 U.S.A. alex@cs.cmu.edu Abstract In this paper we investigate how to automatically determine if two document collections are written from different perspectives." ></td>
	<td class="line x" title="4:268	By perspectives we mean a point of view, for example, from the perspective of Democrats or Republicans." ></td>
	<td class="line x" title="5:268	We propose a test of different perspectives based on distribution divergence between the statistical models of two collections." ></td>
	<td class="line x" title="6:268	Experimental results show that the test can successfully distinguish document collections of different perspectives from other types of collections." ></td>
	<td class="line x" title="7:268	1 Introduction Conflicts arise when two groups of people take very different perspectives on political, socioeconomical, or cultural issues." ></td>
	<td class="line x" title="8:268	For example, here are the answers that two presidential candidates, John Kerry and George Bush, gave during the third presidential debate in 2004 in response to a question on abortion: (1) Kerry: What is an article of faith for me is not something that I can legislate on somebody who doesnt share that article of faith." ></td>
	<td class="line x" title="9:268	I believe that choice is a womans choice." ></td>
	<td class="line x" title="10:268	Its between a woman, God and her doctor." ></td>
	<td class="line x" title="11:268	And thats why I support that." ></td>
	<td class="line x" title="12:268	(2) Bush: I believe the ideal world is one in which every child is protected in law and welcomed to life." ></td>
	<td class="line x" title="13:268	I understand theres great differences on this issue of abortion, but I believe reasonable people can come together and put good law in place that will help reduce the number of abortions." ></td>
	<td class="line x" title="14:268	After reading the above transcripts some readers may conclude that one takes a pro-choice perspective while the other takes a pro-life perspective, the two dominant perspectives in the abortion controversy." ></td>
	<td class="line x" title="15:268	Perspectives, however, are not always manifested when two pieces of text together are put together." ></td>
	<td class="line x" title="16:268	For example, the following two sentences are from Reuters newswire: (3) Gold output in the northeast China province of Heilongjiang rose 22.7 pct in 1986 from 1985s level, the New China News Agency said." ></td>
	<td class="line x" title="17:268	(4) Exco Chairman Richard Lacy told Reuters the acquisition was being made from Bank of New York Co Inc, which currently holds a 50.1 pct, and from RMJ partners who hold the remainder." ></td>
	<td class="line x" title="18:268	A reader would not from this pair of examples perceive as strongly contrasting perspectives as the Kerry-Bush answers." ></td>
	<td class="line x" title="19:268	Instead, as the Reuters annotators did, one would label Example 3 as gold and Example 4 as acquisition, that is, as two topics instead of two perspectives." ></td>
	<td class="line x" title="20:268	Why does the contrast between Example 1 and Example 2 convey different perspectives, but the contrast between Example 3 and Example 4 result in different topics?" ></td>
	<td class="line x" title="21:268	How can we define the impalpable different perspectives anyway?" ></td>
	<td class="line x" title="22:268	The definition of perspective in the dictionary is subjective evaluation of relative significance,1 but can we have a computable definition to test the existence of different perspectives?" ></td>
	<td class="line x" title="23:268	1The American Heritage Dictionary of the English Language, 4th ed." ></td>
	<td class="line x" title="24:268	We are interested in identifying ideological perspectives (Verdonk, 2002), not first-person or secondperson perspective in narrative." ></td>
	<td class="line x" title="25:268	1057 The research question about the definition of different perspectives is not only scientifically intriguing, it also enables us to develop important natural language processing applications." ></td>
	<td class="line x" title="26:268	Such a computational definition can be used to detect the emergence of contrasting perspectives." ></td>
	<td class="line x" title="27:268	Media and political analysts regularly monitor broadcast news, magazines, newspapers, and blogs to see if there are public opinion splitting." ></td>
	<td class="line x" title="28:268	The huge number of documents, however, make the task extremely daunting." ></td>
	<td class="line x" title="29:268	Therefore an automated test of different perspectives will be very valuable to information analysts." ></td>
	<td class="line x" title="30:268	We first review the relevant work in Section 2." ></td>
	<td class="line x" title="31:268	We take a model-based approach to develop a computational definition of different perspectives." ></td>
	<td class="line x" title="32:268	We first develop statistical models for the two document collections, A and B, and then measure the degree of contrast by calculating the distance between A and B. How document collections are statistically modeled and how distribution difference is estimated are described in Section 3." ></td>
	<td class="line x" title="33:268	The document corpora are described in Section 4." ></td>
	<td class="line x" title="34:268	In Section 5, we evaluate how effective the proposed test of difference perspectives based on statistical distribution." ></td>
	<td class="line x" title="35:268	The experimental results show that the distribution divergence can successfully separate document collections of different perspectives from other kinds of collection pairs." ></td>
	<td class="line x" title="36:268	We also investigate if the pattern of distribution difference is due to personal writing or speaking styles." ></td>
	<td class="line x" title="37:268	2 Related Work There has been interest in understanding how beliefs and ideologies can be represented in computers since mid-sixties of the last century (Abelson and Carroll, 1965; Schank and Abelson, 1977)." ></td>
	<td class="line x" title="38:268	The Ideology Machine (Abelson, 1973) can simulate a right-wing ideologue, and POLITICS (Carbonell, 1978) can interpret a text from conservative or liberal ideologies." ></td>
	<td class="line x" title="39:268	In this paper we take a statistics-based approach, which is very different from previous work that rely very much on manually-constructed knowledge base." ></td>
	<td class="line x" title="40:268	Note that what we are interested in is to determine if two document collections are written from different perspectives, not to model individual perspectives." ></td>
	<td class="line x" title="41:268	We aim to capture the characteristics, specifically the statistical regularities of any pairs of document collections with opposing perspectives." ></td>
	<td class="line x" title="42:268	Given a pair of document collections A and B, our goal is not to construct classifiers that can predict if a document was written from the perspective of A or B (Lin et al. , 2006), but to determine if the document collection pair (A,B) convey opposing perspectives." ></td>
	<td class="line x" title="43:268	There has been growing interest in subjectivity and sentiment analysis." ></td>
	<td class="line x" title="44:268	There are studies on learning subjective language (Wiebe et al. , 2004), identifying opinionated documents (Yu and Hatzivassiloglou, 2003) and sentences (Riloff et al. , 2003; Riloff and Wiebe, 2003), and discriminating between positive and negative language (Turney and Littman, 2003; Pang et al. , 2002; Dave et al. , 2003; Nasukawa and Yi, 2003; Morinaga et al. , 2002)." ></td>
	<td class="line oc" title="45:268	There are also research work on automatically classifying movie or product reviews as positive or negative (Nasukawa and Yi, 2003; Mullen and Collier, 2004; Beineke et al. , 2004; Pang and Lee, 2004; Hu and Liu, 2004)." ></td>
	<td class="line x" title="46:268	Although we expect by its very nature much of the language used when expressing a perspective to be subjective and opinionated, the task of labeling a document or a sentence as subjective is orthogonal to the test of different perspectives." ></td>
	<td class="line x" title="47:268	A subjectivity classifier may successfully identify all subjective sentences in the document collection pair A and B, but knowing the number of subjective sentences in A and B does not necessarily tell us if they convey opposing perspectives." ></td>
	<td class="line x" title="48:268	We utilize the subjectivity patterns automatically extracted from foreign news documents (Riloff and Wiebe, 2003), and find that the percentages of the subjective sentences in the bitterlemons corpus (see Section 4) are similar (65.6% in the Palestinian documents and 66.2% in the Israeli documents)." ></td>
	<td class="line x" title="49:268	The high but almost equivalent number of subjective sentences in two perspectives suggests that perspective is largely expressed in subjective language but subjectivity ratio is not enough to tell if two document collections are written from the same (Palestinian v.s. Palestinian) or different perspectives (Palestinian v.s. Israeli)2." ></td>
	<td class="line x" title="50:268	3 Statistical Distribution Divergence We take a model-based approach to measure to what degree, if any, two document collections are different." ></td>
	<td class="line x" title="51:268	A document is represented as a point 2However, the close subjectivity ratio doesnt mean that subjectivity can never help identify document collections of opposing perspectives." ></td>
	<td class="line x" title="52:268	For example, the accuracy of the test of different perspectives may be improved by focusing on only subjective sentences." ></td>
	<td class="line x" title="53:268	1058 in a V -dimensional space, where V is vocabulary size." ></td>
	<td class="line x" title="54:268	Each coordinate is the frequency of a word in a document, i.e., term frequency." ></td>
	<td class="line x" title="55:268	Although vector representation, commonly known as a bag of words, is oversimplified and ignores rich syntactic and semantic structures, more sophisticated representation requires more data to obtain reliable models." ></td>
	<td class="line x" title="56:268	Practically, bag-of-word representation has been very effective in many tasks, including text categorization (Sebastiani, 2002) and information retrieval (Lewis, 1998)." ></td>
	<td class="line x" title="57:268	We assume that a collection of N documents, y1,y2,,yN are sampled from the following process,   Dirichlet() yi  Multinomial(ni,)." ></td>
	<td class="line x" title="58:268	We first sample a V -dimensional vector  from a Dirichlet prior distribution with a hyperparameter , and then sample a document yi repeatedly from a Multinomial distribution conditioned on the parameter , where ni is the document length of the ith document in the collection and assumed to be known and fixed." ></td>
	<td class="line x" title="59:268	We are interested in comparing the parameter  after observing document collections A and B: p(|A) = p(A|)p()p(A) = Dirichlet(|+ summationdisplay yiA yi)." ></td>
	<td class="line x" title="60:268	The posterior distribution p(|) is a Dirichlet distribution since a Dirichlet distribution is a conjugate prior for a Multinomial distribution." ></td>
	<td class="line x" title="61:268	How should we measure the difference between two posterior distributions p(|A) and p(|B)?" ></td>
	<td class="line x" title="62:268	One common way to measure the difference between two distributions is Kullback-Leibler (KL) divergence (Kullback and Leibler, 1951), defined as follows, D(p(|A)||p(|B)) = integraldisplay p(|A)log p(|A)p(|B)d." ></td>
	<td class="line x" title="63:268	(5) Directly calculating KL divergence according to (5) involves a difficult high-dimensional integral." ></td>
	<td class="line x" title="64:268	As an alternative, we approximate KL divergence using Monte Carlo methods as follows, 1." ></td>
	<td class="line x" title="65:268	Sample 1,2,,M from Dirichlet(|+summationtext yiAyi)." ></td>
	<td class="line x" title="66:268	2." ></td>
	<td class="line x" title="67:268	Return D = 1M summationtextMi=1 log p(i|A)p(i|B) as a Monte Carlo estimate of D(p(|A)||p(|B))." ></td>
	<td class="line x" title="68:268	Algorithms of sampling from Dirichlet distribution can be found in (Ripley, 1987)." ></td>
	<td class="line x" title="69:268	As M  , the Monte Carlo estimate will converge to true KL divergence by the Law of Large Numbers." ></td>
	<td class="line x" title="70:268	4 Corpora To evaluate how well KL divergence between posterior distributions can discern a document collection pair of different perspectives, we collect two corpora of documents that were written or spoken from different perspectives and one newswire corpus that covers various topics, as summarized in Table 1." ></td>
	<td class="line x" title="71:268	No stemming algorithms is performed; no stopwords are removed." ></td>
	<td class="line x" title="72:268	Corpus Subset |D| |d| V bitterlemons Palestinian 290 748.7 10309 Israeli 303 822.4 11668 Pal." ></td>
	<td class="line x" title="73:268	Editor 144 636.2 6294 Pal." ></td>
	<td class="line x" title="74:268	Guest 146 859.6 8661 Isr." ></td>
	<td class="line x" title="75:268	Editor 152 819.4 8512 Isr." ></td>
	<td class="line x" title="76:268	Guest 151 825.5 8812 2004 Presidential Debate Kerry 178 124.7 2554 Bush 176 107.8 2393 1st Kerry 33 216.3 1274 1st Bush 41 155.3 1195 2nd Kerry 73 103.8 1472 2nd Bush 75 89.0 1333 3rd Kerry 72 104.0 1408 3rd Bush 60 98.8 1281 Reuters21578 ACQ 2448 124.7 14293 CRUDE 634 214.7 9009 EARN 3987 81.0 12430 GRAIN 628 183.0 8236 INTEREST 513 176.3 6056 MONEY-FX 801 197.9 8162 TRADE 551 255.3 8175 Table 1: The number of documents |D|, average document length |d|, and vocabulary size V of the three corpora." ></td>
	<td class="line x" title="77:268	The first perspective corpus consists of articles published on the bitterlemons website3 from late 2001 to early 2005." ></td>
	<td class="line x" title="78:268	The website is set up to contribute to mutual understanding [between Palestinians and Israelis] through the open exchange of ideas4." ></td>
	<td class="line x" title="79:268	Every week an issue about the Israeli-Palestinian conflict is selected for discussion (e.g. , Disengagement: unilateral or coordinated?), and a Palestinian editor and an Israeli editor each contribute one article addressing the 3http://www.bitterlemons.org/ 4http://www.bitterlemons.org/about/ about.html 1059 issue." ></td>
	<td class="line x" title="80:268	In addition, the Israeli and Palestinian editors interview a guest to express their views on the issue, resulting in a total of four articles in a weekly edition." ></td>
	<td class="line x" title="81:268	The perspective from which each article is written is labeled as either Palestinian or Israeli by the editors." ></td>
	<td class="line x" title="82:268	The second perspective corpus consists of the transcripts of the three Bush-Kerry presidential debates in 2004." ></td>
	<td class="line x" title="83:268	The transcripts are from the website of the Commission on Presidential Debates5." ></td>
	<td class="line x" title="84:268	Each spoken document is roughly an answer to a question or a rebuttal." ></td>
	<td class="line x" title="85:268	The transcript are segmented by the speaker tags already in the transcripts." ></td>
	<td class="line x" title="86:268	All words from moderators are discarded." ></td>
	<td class="line x" title="87:268	The topical corpus contains newswire from Reuters in 1987." ></td>
	<td class="line x" title="88:268	Reuters-215786 is one of the most common testbeds for text categorization." ></td>
	<td class="line x" title="89:268	Each document belongs to none, one, or more of the 135 categories (e.g. , Mergers and U.S. Dollars)." ></td>
	<td class="line x" title="90:268	The number of documents in each category is not evenly distributed (median 9.0, mean 105.9)." ></td>
	<td class="line x" title="91:268	To estimate statistics reliably, we only consider categories with more than 500 documents, resulting in a total of seven categories (ACQ, CRUDE, EARN, GRAIN, INTEREST, MONEY-FX, and TRADE)." ></td>
	<td class="line x" title="92:268	5 Experiments A test of different perspectives is acute when it can draw distinctions between document collection pairs of different perspectives and document collection pairs of the same perspective and others." ></td>
	<td class="line x" title="93:268	We thus evaluate the proposed test of different perspectives in the following four types of document collection pairs (A,B): Different Perspectives (DP) A and B are written from different perspectives." ></td>
	<td class="line x" title="94:268	For example, A is written from the Palestinian perspective and B is written from the Israeli perspective in the bitterlemons corpus." ></td>
	<td class="line x" title="95:268	Same Perspective (SP) A and B are written from the same perspective." ></td>
	<td class="line x" title="96:268	For example, A and B consist of the words spoken by Kerry." ></td>
	<td class="line x" title="97:268	Different Topics (DT) A and B are written on different topics." ></td>
	<td class="line x" title="98:268	For example, A is about 5http://www.debates.org/pages/ debtrans.html 6http://www.ics.uci.edu/kdd/ databases/reuters21578/reuters21578.html acquisition (ACQ) and B is about crude oil (CRUDE)." ></td>
	<td class="line x" title="99:268	Same Topic (ST) A and B are written on the same topic." ></td>
	<td class="line x" title="100:268	For example, A and B are both about earnings (EARN)." ></td>
	<td class="line x" title="101:268	The effectiveness of the proposed test of different perspectives can thus be measured by how the distribution divergence of DP document collection pairs is separated from the distribution divergence of SP, DT, and ST document collection pairs." ></td>
	<td class="line x" title="102:268	The little the overlap of the range of distribution divergence, the sharper the test of different perspectives." ></td>
	<td class="line x" title="103:268	To account for large variation in the number of words and vocabulary size across corpora, we normalize the total number of words in a document collection to be the same K, and consider only the top C% frequent words in the document collection pair." ></td>
	<td class="line x" title="104:268	We vary the values of K and C, and find that K changes the absolute scale of KL divergence but does not change the rankings of four conditions." ></td>
	<td class="line x" title="105:268	Rankings among four conditions is consistent when C is small." ></td>
	<td class="line x" title="106:268	We only report results of K = 1000,C = 10 in the paper due to space limit." ></td>
	<td class="line x" title="107:268	There are two kinds of variances in the estimation of divergence between two posterior distribution and should be carefully checked." ></td>
	<td class="line x" title="108:268	The first kind of variance is due to Monte Carlo methods." ></td>
	<td class="line x" title="109:268	We assess the Monte Carlo variance by calculating a 100 percent confidence interval as follows, [ D1(2) M, D + 1(1 2) M] where 2 is the sample variance of 1,2,,M, and ()1 is the inverse of the standard normal cumulative density function." ></td>
	<td class="line x" title="110:268	The second kind of variance is due to the intrinsic uncertainties of data generating processes." ></td>
	<td class="line x" title="111:268	We assess the second kind of variance by collecting 1000 bootstrapped samples, that is, sampling with replacement, from each document collection pair." ></td>
	<td class="line x" title="112:268	5.1 Quality of Monte Carlo Estimates The Monte Carlo estimates of the KL divergence from several document collection pair are listed in Table 2." ></td>
	<td class="line x" title="113:268	A complete list of the results is omitted due to the space limit." ></td>
	<td class="line x" title="114:268	We can see that the 95% confidence interval captures well the Monte Carlo estimates of KL divergence." ></td>
	<td class="line x" title="115:268	Note that KL divergence is not symmetric." ></td>
	<td class="line x" title="116:268	The KL divergence 1060 A B D 95% CI ACQ ACQ 2.76 [2.62, 2.89] Palestinian Palestinian 3.00 [3.54, 3.85] Palestinian Israeli 27.11 [26.64, 27.58] Israeli Palestinian 28.44 [27.97, 28.91] Kerry Bush 58.93 [58.22, 59.64] ACQ EARN 615.75 [610.85, 620.65] Table 2: The Monte Carlo estimate D and 95% confidence interval (CI) of the Kullback-Leibler divergence of several document collection pairs (A,B) with the number of Monte Carlo samples M = 1000." ></td>
	<td class="line x" title="117:268	of the pair (Israeli, Palestinian) is not necessarily the same as (Palestinian, Israeli)." ></td>
	<td class="line x" title="118:268	KL divergence is greater than zero (Cover and Thomas, 1991) and equal to zero only when document collections A and B are exactly the same." ></td>
	<td class="line x" title="119:268	Here (ACQ, ACQ) is close to but not exactly zero because they are different samples of documents in the ACQ category." ></td>
	<td class="line x" title="120:268	Since the CIs of Monte Carlo estimates are reasonably tight, we assume them to be exact and ignore the errors from Monte Carlo methods." ></td>
	<td class="line x" title="121:268	5.2 Test of Different Perspectives We now present the main result of the paper." ></td>
	<td class="line x" title="122:268	We calculate the KL divergence between posterior distributions of document collection pairs in four conditions using Monte Carlo methods, and plot the results in Figure 1." ></td>
	<td class="line x" title="123:268	The test of different perspectives based on statistical distribution divergence is shown to be very acute." ></td>
	<td class="line x" title="124:268	The KL divergence of the document collection pairs in the DP condition fall mostly in the middle range, and is well separated from the high KL divergence of the pairs in DT condition and from the low KL divergence of the pairs in SP and ST conditions." ></td>
	<td class="line x" title="125:268	Therefore, by simply calculating the KL divergence of a document collection pair, we can reliably predict that they are written from different perspectives if the value of KL divergence falls in the middle range, from different topics if the value is very large, from the same topic or perspective if the value is very small." ></td>
	<td class="line x" title="126:268	5.3 Personal Writing Styles or Perspectives?" ></td>
	<td class="line x" title="127:268	One may suspect that the mid-range distribution divergence is attributed to personal speaking or writing styles and has nothing to do with different perspectives." ></td>
	<td class="line x" title="128:268	The doubt is expected because half of the bitterlemons corpus are written by one Palestinian editor and one Israeli editor (see Table 1), and the debate transcripts come from only two candidates." ></td>
	<td class="line x" title="129:268	We test the hypothesis by computing the distribution divergence of the document collection pair (Israeli Guest, Palestinian Guest), that is, a Different Perspectives (DP) pair." ></td>
	<td class="line x" title="130:268	There are more than 200 different authors in the Israeli Guest and Palestinian Guest collection." ></td>
	<td class="line x" title="131:268	If the distribution divergence of the pair with diverse authors falls out of the middle range, it will support that mid-range divergence is due to writing styles." ></td>
	<td class="line x" title="132:268	On the other hand, if the distribution divergence still fall in the middle range, we are more confident the effect is attributed to different perspectives." ></td>
	<td class="line x" title="133:268	We compare the distribution divergence of the pair (Israeli Guest, Palestinian Guest) with others in Figure 2." ></td>
	<td class="line x" title="134:268	ST SP DP Guest DT KL Divergence 1 2 5 10 20 50 200 500 Figure 2: The average KL divergence of document collection pairs in the bitterlemons Guest subset (Israeli Guest vs. Palestinian Guest), ST,SP, DP, DT conditions." ></td>
	<td class="line x" title="135:268	The horizontal lines are the same as those in Figure 1." ></td>
	<td class="line x" title="136:268	The results show that the distribution divergence of the (Israeli Guest, Palestinian Guest) pair, as other pairs in the DP condition, still falls in the middle range, and is well separated from SP and ST in the low range and DT in the high range." ></td>
	<td class="line x" title="137:268	The decrease in KL divergence due to writing or speaking styles is noticeable, and the overall effect due to different perspectives is strong enough to make the test robust." ></td>
	<td class="line x" title="138:268	We thus conclude that the test of different perspectives based on distribution divergence indeed captures different perspectives, not personal writing or speaking styles." ></td>
	<td class="line x" title="139:268	5.4 Origins of Differences While the effectiveness of the test of different perspectives is demonstrated in Figure 1, one may 1061 2 5 10 20 50 100 200 500 1000 0.00 0.05 0.10 0.15 KL Divergence Density SP ST DP DT Figure 1: The KL divergence of the document collection pairs in four conditions: Different Perspectives (DP), Same Perspective (SP), Different Topics (DT), and Same Topic (ST)." ></td>
	<td class="line x" title="140:268	Note that the x axis is in log scale." ></td>
	<td class="line x" title="141:268	The Monte Carlo estimates D of the pairs in DP condition are plotted as rugs." ></td>
	<td class="line x" title="142:268	D of the pairs in other conditions are omitted to avoid clutter and summarized in one-dimensional density using Kernel Density Estimation." ></td>
	<td class="line x" title="143:268	The vertical lines are drawn at the points with equivalent densities." ></td>
	<td class="line x" title="144:268	wonder why the distribution divergence of the document collection pair with different perspectives falls in the middle range and what causes the large and small divergence of the document collection pairs with different topics (DT) and the same topic (ST) or perspective (SP), respectively." ></td>
	<td class="line x" title="145:268	In other words where do the differences result from?" ></td>
	<td class="line x" title="146:268	We answer the question by taking a closer look at the causes of the distribution divergence in our model." ></td>
	<td class="line x" title="147:268	We compare the expected marginal difference of  between two posterior distributions p(|A) and p(|B)." ></td>
	<td class="line x" title="148:268	The marginal distribution of the i-th coordinate of , that is, the i-th word in the vocabulary, is a Beta distribution, and thus the expected value can be easily calculated." ></td>
	<td class="line x" title="149:268	We plot the  = E[i|A]E[i|B] against E[i|A] for each condition in Figure 3." ></td>
	<td class="line x" title="150:268	How  is deviated from zero partially explains different patterns of distribution divergence in Figure 1." ></td>
	<td class="line x" title="151:268	In Figure 3d we see that the  increases as  increases, and the deviance from zero is much greater than those in the Same Perspective (Figure 3b) and Same Topic (Figure 3a) conditions." ></td>
	<td class="line x" title="152:268	The large  not only accounts for large distribution divergence of the document pairs in DT conditions, but also shows that words in different topics that is frequent in one topic are less likely to be frequent in the other topic." ></td>
	<td class="line x" title="153:268	At the other extreme, document collection pairs of the Same Perspective (SP) or Same Topic (ST) show very little difference in , which matches our intuition that documents of the same perspective or the same topic use the same vocabulary in a very similar way." ></td>
	<td class="line x" title="154:268	The manner in which  is varied with the value of  in the Different Perspective (DP) condition is very unique." ></td>
	<td class="line x" title="155:268	The  in Figure 3c is not as small as those in the SP and ST conditions, but at the same time not as large as those in DT conditions, resulting in mid-range distribution divergence in Figure 1." ></td>
	<td class="line x" title="156:268	Why do document collections of different perspectives distribute this way?" ></td>
	<td class="line x" title="157:268	Partly because articles from different perspectives focus on the closely related issues (the PalestinianIsraeli conflict in the bitterlemons corpus, or the political and economical issues in the debate corpus), the authors of different perspectives write or speak in a similar vocabulary, but with emphasis on different words." ></td>
	<td class="line x" title="158:268	6 Conclusions In this paper we develop a computational test of different perspectives based on statistical distribution divergence between the statistical models of document collections." ></td>
	<td class="line x" title="159:268	We show that the pro1062 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.04 0.02 0.00 0.02 0.04 (a) Same Topic (ST) 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.04 0.02 0.00 0.02 0.04 (b) Same Topic (SP) 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.04 0.02 0.00 0.02 0.04 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.04 0.02 0.00 0.02 0.04 (c) Two examples of Different Perspective (DP) Figure 3: The  vs.  plots of the typical document collection pairs in four conditions." ></td>
	<td class="line x" title="160:268	The horizontal line is  = 0." ></td>
	<td class="line x" title="161:268	0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.04 0.02 0.00 0.02 0.04 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.04 0.02 0.00 0.02 0.04 (d) Two examples of Different Topics (DT) Figure 3: Contd posed test can successfully separate document collections of different perspectives from other types of document collection pairs." ></td>
	<td class="line x" title="162:268	The distribution divergence falling in the middle range can not simply be attributed to personal writing or speaking styles." ></td>
	<td class="line x" title="163:268	From the plot of multinomial parameter difference we offer insights into where the different patterns of distribution divergence come from." ></td>
	<td class="line x" title="164:268	Although we validate the test of different perspectives by comparing the DP condition with DT, SP, and ST conditions, the comparisons are by no means exhaustive, and the distribution divergence of some document collection pairs may also fall in the middle range." ></td>
	<td class="line x" title="165:268	We plan to investigate more types of document collections pairs, e.g., the document collections from different text genres (Kessler et al. , 1997)." ></td>
	<td class="line x" title="166:268	Acknowledgment We would like thank the anonymous reviewers for useful comments and suggestions." ></td>
	<td class="line x" title="167:268	This material is based on work supported by the Advanced Research and Development Activity (ARDA) under contract number NBCHC040037." ></td>
	<td class="line x" title="168:268	1063 References Robert P. Abelson and J. Douglas Carroll." ></td>
	<td class="line x" title="169:268	1965." ></td>
	<td class="line x" title="170:268	Computer simulation of individual belief systems." ></td>
	<td class="line x" title="171:268	The American Behavioral Scientist, 8:2430, May. Robert P. Abelson, 1973." ></td>
	<td class="line x" title="172:268	Computer Models of Thought and Language, chapter The Structure of Belief Systems, pages 287339." ></td>
	<td class="line x" title="173:268	W. H. Freeman and Company." ></td>
	<td class="line x" title="174:268	Philip Beineke, Trevor Hastie, and Shivakumar Vaithyanathan." ></td>
	<td class="line x" title="175:268	2004." ></td>
	<td class="line x" title="176:268	The sentimental factor: Improving review classification via human-provided information." ></td>
	<td class="line x" title="177:268	In Proceedings of the Association for Computational Linguistics (ACL-2004)." ></td>
	<td class="line x" title="178:268	Jaime G. Carbonell." ></td>
	<td class="line x" title="179:268	1978." ></td>
	<td class="line x" title="180:268	POLITICS: Automated ideological reasoning." ></td>
	<td class="line x" title="181:268	Cognitive Science, 2(1):27 51." ></td>
	<td class="line x" title="182:268	Thomas M. Cover and Joy A. Thomas." ></td>
	<td class="line x" title="183:268	1991." ></td>
	<td class="line x" title="184:268	Elements of Information Theory." ></td>
	<td class="line x" title="185:268	Wiley-Interscience." ></td>
	<td class="line x" title="186:268	Kushal Dave, Steve Lawrence, and David M. Pennock." ></td>
	<td class="line x" title="187:268	2003." ></td>
	<td class="line x" title="188:268	Mining the peanut gallery: Opinion extraction and semantic classification of product reviews." ></td>
	<td class="line x" title="189:268	In Proceedings of the 12th International World Wide Web Conference (WWW2003)." ></td>
	<td class="line x" title="190:268	Minqing Hu and Bing Liu." ></td>
	<td class="line x" title="191:268	2004." ></td>
	<td class="line x" title="192:268	Mining and summarizing customer reviews." ></td>
	<td class="line x" title="193:268	In Proceedings of the 2004 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining." ></td>
	<td class="line x" title="194:268	Brett Kessler, Geoffrey Nunberg, and Hinrich Schutze." ></td>
	<td class="line x" title="195:268	1997." ></td>
	<td class="line x" title="196:268	Automatic detection of text genre." ></td>
	<td class="line x" title="197:268	In Proceedings of the 35th Conference on Association for Computational Linguistics, pages 3238." ></td>
	<td class="line x" title="198:268	S. Kullback and R. A. Leibler." ></td>
	<td class="line x" title="199:268	1951." ></td>
	<td class="line x" title="200:268	On information and sufficiency." ></td>
	<td class="line x" title="201:268	The Annals of Mathematical Statistics, 22(1):7986, March." ></td>
	<td class="line x" title="202:268	David D. Lewis." ></td>
	<td class="line x" title="203:268	1998." ></td>
	<td class="line x" title="204:268	Naive (Bayes) at forty: The independence assumption in information retrieval." ></td>
	<td class="line x" title="205:268	In Proceedings of the 9th European Conference on Machine Learning (ECML)." ></td>
	<td class="line x" title="206:268	Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and Alexander Hauptmann." ></td>
	<td class="line x" title="207:268	2006." ></td>
	<td class="line x" title="208:268	Which side are you on?" ></td>
	<td class="line x" title="209:268	identifying perspectives at the document and sentence levels." ></td>
	<td class="line x" title="210:268	In Proceedings of Tenth Conference on Natural Language Learning (CoNLL)." ></td>
	<td class="line x" title="211:268	S. Morinaga, K. Yamanishi, K. Tateishi, and T. Fukushima." ></td>
	<td class="line x" title="212:268	2002." ></td>
	<td class="line x" title="213:268	Mining product reputations on the web." ></td>
	<td class="line x" title="214:268	In Proceedings of the 2002 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining." ></td>
	<td class="line x" title="215:268	Tony Mullen and Nigel Collier." ></td>
	<td class="line x" title="216:268	2004." ></td>
	<td class="line x" title="217:268	Sentiment analysis using support vector machines with diverse information sources." ></td>
	<td class="line x" title="218:268	In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2004)." ></td>
	<td class="line x" title="219:268	T. Nasukawa and J. Yi." ></td>
	<td class="line x" title="220:268	2003." ></td>
	<td class="line x" title="221:268	Sentiment analysis: Capturing favorability using natural language processing." ></td>
	<td class="line x" title="222:268	In Proceedings of the 2nd International Conference on Knowledge Capture (K-CAP 2003)." ></td>
	<td class="line x" title="223:268	Bo Pang and Lillian Lee." ></td>
	<td class="line x" title="224:268	2004." ></td>
	<td class="line x" title="225:268	A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts." ></td>
	<td class="line x" title="226:268	In Proceedings of the Association for Computational Linguistics (ACL-2004)." ></td>
	<td class="line x" title="227:268	Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan." ></td>
	<td class="line x" title="228:268	2002." ></td>
	<td class="line x" title="229:268	Thumbs up?" ></td>
	<td class="line x" title="230:268	Sentiment classification using machine learning techniques." ></td>
	<td class="line x" title="231:268	In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2002)." ></td>
	<td class="line x" title="232:268	Ellen Riloff and Janyce Wiebe." ></td>
	<td class="line x" title="233:268	2003." ></td>
	<td class="line x" title="234:268	Learning extraction patterns for subjective expressions." ></td>
	<td class="line x" title="235:268	In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2003)." ></td>
	<td class="line x" title="236:268	Ellen Riloff, Janyce Wiebe, and Theresa Wilson." ></td>
	<td class="line x" title="237:268	2003." ></td>
	<td class="line x" title="238:268	Learning subjective nouns using extraction pattern bootstrapping." ></td>
	<td class="line x" title="239:268	In Proceedings of the 7th Conference on Natural Language Learning (CoNLL-2003)." ></td>
	<td class="line x" title="240:268	B. D. Ripley." ></td>
	<td class="line x" title="241:268	1987." ></td>
	<td class="line x" title="242:268	Stochastic Simulation." ></td>
	<td class="line x" title="243:268	Wiley." ></td>
	<td class="line x" title="244:268	Roger C. Schank and Robert P. Abelson." ></td>
	<td class="line x" title="245:268	1977." ></td>
	<td class="line x" title="246:268	Scripts, plans, goals, and understanding: an inquiry into human knowledge structures." ></td>
	<td class="line x" title="247:268	Lawrene Erlbaum Associates." ></td>
	<td class="line x" title="248:268	Fabrizio Sebastiani." ></td>
	<td class="line x" title="249:268	2002." ></td>
	<td class="line x" title="250:268	Machine learning in automated text categorization." ></td>
	<td class="line x" title="251:268	ACM Computing Surveys, 34(1):147, March." ></td>
	<td class="line x" title="252:268	Peter Turney and Michael L. Littman." ></td>
	<td class="line x" title="253:268	2003." ></td>
	<td class="line x" title="254:268	Measuring praise and criticism: Inference of semantic orientation from association." ></td>
	<td class="line x" title="255:268	ACM Transactions on Information Systems (TOIS), 21(4):315346." ></td>
	<td class="line x" title="256:268	Peter Verdonk." ></td>
	<td class="line x" title="257:268	2002." ></td>
	<td class="line x" title="258:268	Stylistics." ></td>
	<td class="line x" title="259:268	Oxford University Press." ></td>
	<td class="line x" title="260:268	Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew Bell, and Melanie Martin." ></td>
	<td class="line x" title="261:268	2004." ></td>
	<td class="line x" title="262:268	Learning subjective language." ></td>
	<td class="line x" title="263:268	Computational Linguistics, 30(3)." ></td>
	<td class="line x" title="264:268	Hong Yu and Vasileios Hatzivassiloglou." ></td>
	<td class="line x" title="265:268	2003." ></td>
	<td class="line x" title="266:268	Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences." ></td>
	<td class="line x" title="267:268	In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2003)." ></td>
	<td class="line x" title="268:268	1064" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-1134
Word Sense And Subjectivity
Wiebe, Janyce M.;Mihalcea, Rada;"></td>
	<td class="line x" title="1:194	Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 10651072, Sydney, July 2006." ></td>
	<td class="line x" title="2:194	c2006 Association for Computational Linguistics Word Sense and Subjectivity Janyce Wiebe Department of Computer Science University of Pittsburgh wiebe@cs.pitt.edu Rada Mihalcea Department of Computer Science University of North Texas rada@cs.unt.edu Abstract Subjectivity and meaning are both important properties of language." ></td>
	<td class="line x" title="3:194	This paper explores their interaction, and brings empirical evidence in support of the hypotheses that (1) subjectivity is a property that can be associated with word senses, and (2) word sense disambiguation can directly benefit from subjectivity annotations." ></td>
	<td class="line x" title="4:194	1 Introduction There is growing interest in the automatic extraction of opinions, emotions, and sentiments in text (subjectivity), to provide tools and support for various NLP applications." ></td>
	<td class="line x" title="5:194	Similarly, there is continuous interest in the task of word sense disambiguation, with sense-annotated resources being developed for many languages, and a growing number of research groups participating in large-scale evaluations such as SENSEVAL." ></td>
	<td class="line x" title="6:194	Though both of these areas are concerned with the semantics of a text, over time there has been little interaction, if any, between them." ></td>
	<td class="line x" title="7:194	In this paper, we address this gap, and explore possible interactions between subjectivity and word sense." ></td>
	<td class="line x" title="8:194	There are several benefits that would motivate such a joint exploration." ></td>
	<td class="line x" title="9:194	First, at the resource level, the augmentation of lexical resources such as WordNet (Miller, 1995) with subjectivity labels could support better subjectivity analysis tools, and principled methods for refining word senses and clustering similar meanings." ></td>
	<td class="line x" title="10:194	Second, at the tool level, an explicit link between subjectivity and word sense could help improve methods for each, by integrating features learned from one into the other in a pipeline approach, or through joint simultaneous learning." ></td>
	<td class="line x" title="11:194	In this paper we address two questions about word sense and subjectivity." ></td>
	<td class="line x" title="12:194	First, can subjectivity labels be assigned to word senses?" ></td>
	<td class="line x" title="13:194	To address this question, we perform two studies." ></td>
	<td class="line x" title="14:194	The first (Section 3) investigates agreement between annotators who manually assign the labels subjective, objective, or both to WordNet senses." ></td>
	<td class="line x" title="15:194	The second study (Section 4) evaluates a method for automatic assignment of subjectivity labels to word senses." ></td>
	<td class="line x" title="16:194	We devise an algorithm relying on distributionally similar words to calculate a subjectivity score, and show how it can be used to automatically assess the subjectivity of a word sense." ></td>
	<td class="line x" title="17:194	Second, can automatic subjectivity analysis be used to improve word sense disambiguation?" ></td>
	<td class="line x" title="18:194	To address this question, the output of a subjectivity sentence classifier is input to a word-sense disambiguation system, which is in turn evaluated on the nouns from the SENSEVAL-3 English lexical sample task (Section 5)." ></td>
	<td class="line x" title="19:194	The results of this experiment show that a subjectivity feature can significantly improve the accuracy of a word sense disambiguation system for those words that have both subjective and objective senses." ></td>
	<td class="line x" title="20:194	A third obvious question is, can word sense disambiguation help automatic subjectivity analysis?" ></td>
	<td class="line x" title="21:194	However, due to space limitations, we do not address this question here, but rather leave it for future work." ></td>
	<td class="line x" title="22:194	2 Background Subjective expressions are words and phrases being used to express opinions, emotions, evaluations, speculations, etc.(Wiebe et al. , 2005)." ></td>
	<td class="line x" title="24:194	A general covering term for such states is private state, a state that is not open to objective obser1065 vation or verification (Quirk et al. , 1985).1 There are three main types of subjective expressions:2 (1) references to private states: His alarm grew." ></td>
	<td class="line x" title="25:194	He absorbed the information quickly." ></td>
	<td class="line x" title="26:194	He was boiling with anger." ></td>
	<td class="line x" title="27:194	(2) references to speech (or writing) events expressing private states: UCC/Disciples leaders roundly condemned the Iranian Presidents verbal assault on Israel." ></td>
	<td class="line x" title="28:194	The editors of the left-leaning paper attacked the new House Speaker." ></td>
	<td class="line x" title="29:194	(3) expressive subjective elements: He would be quite a catch." ></td>
	<td class="line x" title="30:194	Whats the catch?" ></td>
	<td class="line x" title="31:194	That doctor is a quack." ></td>
	<td class="line x" title="32:194	Work on automatic subjectivity analysis falls into three main areas." ></td>
	<td class="line x" title="33:194	The first is identifying words and phrases that are associated with subjectivity, for example, that think is associated with private states and that beautiful is associated with positive sentiments (e.g. , (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Kamps and Marx, 2002; Turney, 2002; Esuli and Sebastiani, 2005))." ></td>
	<td class="line x" title="34:194	Such judgments are made for words." ></td>
	<td class="line x" title="35:194	In contrast, our end task (in Section 4) is to assign subjectivity labels to word senses." ></td>
	<td class="line x" title="36:194	The second is subjectivity classification of sentences, clauses, phrases, or word instances in the context of a particular text or conversation, either subjective/objective classifications or positive/negative sentiment classifications (e.g. ,(Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Dave et al. , 2003; Hu and Liu, 2004))." ></td>
	<td class="line oc" title="37:194	The third exploits automatic subjectivity analysis in applications such as review classification (e.g. , (Turney, 2002; Pang and Lee, 2004)), mining texts for product reviews (e.g. , (Yi et al. , 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005)), summarization (e.g. , (Kim and Hovy, 2004)), information extraction (e.g. , (Riloff et al. , 2005)), 1Note that sentiment, the focus of much recent work in the area, is a type of subjectivity, specifically involving positive or negative opinion, emotion, or evaluation." ></td>
	<td class="line x" title="38:194	2These distinctions are not strictly needed for this paper, but may help the reader appreciate the examples given below." ></td>
	<td class="line x" title="39:194	and question answering (e.g. , (Yu and Hatzivassiloglou, 2003; Stoyanov et al. , 2005))." ></td>
	<td class="line x" title="40:194	Most manual subjectivity annotation research has focused on annotating words, out of context (e.g. , (Heise, 2001)), or sentences and phrases in the context of a text or conversation (e.g. , (Wiebe et al. , 2005))." ></td>
	<td class="line x" title="41:194	The new annotations in this paper are instead targeting the annotation of word senses." ></td>
	<td class="line x" title="42:194	3 Human Judgment of Word Sense Subjectivity To explore our hypothesis that subjectivity may be associated with word senses, we developed a manual annotation scheme for assigning subjectivity labels to WordNet senses,3 and performed an inter-annotator agreement study to assess its reliability." ></td>
	<td class="line x" title="43:194	Senses are classified as S(ubjective), O(bjective), or B(oth)." ></td>
	<td class="line x" title="44:194	Classifying a sense as S means that, when the sense is used in a text or conversation, we expect it to express subjectivity; we also expect the phrase or sentence containing it to be subjective." ></td>
	<td class="line x" title="45:194	We saw a number of subjective expressions in Section 2." ></td>
	<td class="line x" title="46:194	A subset is repeated here, along with relevant WordNet senses." ></td>
	<td class="line x" title="47:194	In the display of each sense, the first part shows the synset, gloss, and any examples." ></td>
	<td class="line x" title="48:194	The second part (marked with =>) shows the immediate hypernym." ></td>
	<td class="line x" title="49:194	His alarm grew." ></td>
	<td class="line x" title="50:194	alarm, dismay, consternation  (fear resulting from the awareness of danger) => fear, fearfulness, fright  (an emotion experienced in anticipation of some specific pain or danger (usually accompanied by a desire to flee or fight)) He was boiling with anger." ></td>
	<td class="line x" title="51:194	seethe, boil  (be in an agitated emotional state; The customer was seething with anger) => be  (have the quality of being; (copula, used with an adjective or a predicate noun); John is rich; This is not a good answer) Whats the catch?" ></td>
	<td class="line x" title="52:194	catch  (a hidden drawback; it sounds good but whats the catch?) => drawback  (the quality of being a hindrance; he pointed out all the drawbacks to my plan) That doctor is a quack." ></td>
	<td class="line x" title="53:194	quack  (an untrained person who pretends to be a physician and who dispenses medical advice) => doctor, doc, physician, MD, Dr. , medico Before specifying what we mean by an objective sense, we give examples." ></td>
	<td class="line x" title="54:194	3All our examples and data used in the experiments are from WordNet 2.0." ></td>
	<td class="line x" title="55:194	1066 The alarm went off." ></td>
	<td class="line x" title="56:194	alarm, warning device, alarm system  (a device that signals the occurrence of some undesirable event) => device  (an instrumentality invented for a particular purpose; the device is small enough to wear on your wrist; a device intended to conserve water) The water boiled." ></td>
	<td class="line x" title="57:194	boil  (come to the boiling point and change from a liquid to vapor; Water boils at 100 degrees Celsius) => change state, turn  (undergo a transformation or a change of position or action; We turned from Socialism to Capitalism; The people turned against the President when he stole the election) He sold his catch at the market." ></td>
	<td class="line x" title="58:194	catch, haul  (the quantity that was caught; the catch was only 10 fish) => indefinite quantity  (an estimated quantity) The ducks quack was loud and brief." ></td>
	<td class="line x" title="59:194	quack  (the harsh sound of a duck) => sound  (the sudden occurrence of an audible event; the sound awakened them) While we expect phrases or sentences containing subjective senses to be subjective, we do not necessarily expect phrases or sentences containing objective senses to be objective." ></td>
	<td class="line x" title="60:194	Consider the following examples: Will someone shut that damn alarm off?" ></td>
	<td class="line x" title="61:194	Cant you even boil water?" ></td>
	<td class="line x" title="62:194	While these sentences contain objective senses of alarm and boil, the sentences are subjective nonetheless." ></td>
	<td class="line x" title="63:194	But they are not subjective due to alarm and boil, but rather to punctuation, sentence forms, and other words in the sentence." ></td>
	<td class="line x" title="64:194	Thus, classifying a sense as O means that, when the sense is used in a text or conversation, we do not expect it to express subjectivity and, if the phrase or sentence containing it is subjective, the subjectivity is due to something else." ></td>
	<td class="line x" title="65:194	Finally, classifying a sense as B means it covers both subjective and objective usages, e.g.: absorb, suck, imbibe, soak up, sop up, suck up, draw, take in, take up  (take in, also metaphorically; The sponge absorbs water well; She drew strength from the ministers words) Manual subjectivity judgments were added to a total of 354 senses (64 words)." ></td>
	<td class="line x" title="66:194	One annotator, Judge 1 (a co-author), tagged all of them." ></td>
	<td class="line x" title="67:194	A second annotator (Judge 2, who is not a co-author) tagged a subset for an agreement study, presented next." ></td>
	<td class="line x" title="68:194	3.1 Agreement Study For the agreement study, Judges 1 and 2 independently annotated 32 words (138 senses)." ></td>
	<td class="line x" title="69:194	16 words have both S and O senses and 16 do not (according to Judge 1)." ></td>
	<td class="line x" title="70:194	Among the 16 that do not have both S and O senses, 8 have only S senses and 8 have only O senses." ></td>
	<td class="line x" title="71:194	All of the subsets are balanced between nouns and verbs." ></td>
	<td class="line x" title="72:194	Table 1 shows the contingency table for the two annotators judgments on this data." ></td>
	<td class="line x" title="73:194	In addition to S, O, and B, the annotation scheme also permits U(ncertain) tags." ></td>
	<td class="line x" title="74:194	S O B U Total S 39 O O 4 43 O 3 73 2 4 82 B 1 O 3 1 5 U 3 2 O 3 8 Total 46 75 5 12 138 Table 1: Agreement on balanced set (Agreement: 85.5%, : 0.74) Overall agreement is 85.5%, with a Kappa () value of 0.74." ></td>
	<td class="line x" title="75:194	For 12.3% of the senses, at least one annotators tag is U. If we consider these cases to be borderline and exclude them from the study, percent agreement increases to 95% and  rises to 0.90." ></td>
	<td class="line x" title="76:194	Thus, annotator agreement is especially high when both are certain." ></td>
	<td class="line x" title="77:194	Considering only the 16-word subset with both S and O senses (according to Judge 1),  is.75, and for the 16-word subset for which Judge 1 gave only S or only O senses,  is .73." ></td>
	<td class="line x" title="78:194	Thus, the two subsets are of comparable difficulty." ></td>
	<td class="line x" title="79:194	The two annotators also independently annotated the 20 ambiguous nouns (117 senses) of the SENSEVAL-3 English lexical sample task used in Section 5." ></td>
	<td class="line x" title="80:194	For this tagging task, U tags were not allowed, to create a definitive gold standard for the experiments." ></td>
	<td class="line x" title="81:194	Even so, the  value for them is 0.71, which is not substantially lower." ></td>
	<td class="line x" title="82:194	The distributions of Judge 1s tags for all 20 words can be found in Table 3 below." ></td>
	<td class="line x" title="83:194	We conclude this section with examples of disagreements that illustrate sources of uncertainty." ></td>
	<td class="line x" title="84:194	First, uncertainty arises when subjective senses are missing from the dictionary." ></td>
	<td class="line x" title="85:194	The labels for the senses of noun assault are (O:O,O:O,O:O,O:UO).4 For verb assault there is a subjective sense: attack, round, assail, lash out, snipe, assault (attack in speech or writing) The editors of the left-leaning paper attacked the new House Speaker However, there is no corresponding sense for 4I.e., the first three were labeled O by both annotators." ></td>
	<td class="line x" title="86:194	For the fourth sense, the second annotator was not sure but was leaning toward O. 1067 noun assault." ></td>
	<td class="line x" title="87:194	A missing sense may lead an annotator to try to see subjectivity in an objective sense." ></td>
	<td class="line x" title="88:194	Second, uncertainty can arise in weighing hypernym against sense." ></td>
	<td class="line x" title="89:194	It is fine for a synset to imply just S or O, while the hypernym implies both (the synset specializes the more general concept)." ></td>
	<td class="line x" title="90:194	However, consider the following, which was tagged (O:UB)." ></td>
	<td class="line x" title="91:194	attack  (a sudden occurrence of an uncontrollable condition; an attack of diarrhea) => affliction  (a cause of great suffering and distress) While the sense is only about the condition, the hypernym highlights subjective reactions to the condition." ></td>
	<td class="line x" title="92:194	One annotator judged only the sense (giving tag O), while the second considered the hypernym as well (giving tag UB)." ></td>
	<td class="line x" title="93:194	4 Automatic Assessment of Word Sense Subjectivity Encouraged by the results of the agreement study, we devised a method targeting the automatic annotation of word senses for subjectivity." ></td>
	<td class="line x" title="94:194	The main idea behind our method is that we can derive information about a word sense based on information drawn from words that are distributionally similar to the given word sense." ></td>
	<td class="line x" title="95:194	This idea relates to the unsupervised word sense ranking algorithm described in (McCarthy et al. , 2004)." ></td>
	<td class="line x" title="96:194	Note, however, that (McCarthy et al. , 2004) used the information about distributionally similar words to approximate corpus frequencies for word senses, whereas we target the estimation of a property of a given word sense (the subjectivity)." ></td>
	<td class="line x" title="97:194	Starting with a given ambiguous word w, we first find the distributionally similar words using the method of (Lin, 1998) applied to the automatically parsed texts of the British National Corpus." ></td>
	<td class="line x" title="98:194	Let DSW = dsw1, dsw2,  , dswn be the list of top-ranked distributionally similar words, sorted in decreasing order of their similarity." ></td>
	<td class="line x" title="99:194	Next, for each sense wsi of the word w, we determine the similarity with each of the words in the list DSW, using a WordNet-based measure of semantic similarity (wnss)." ></td>
	<td class="line x" title="100:194	Although a large number of such word-to-word similarity measures exist, we chose to use the (Jiang and Conrath, 1997) measure, since it was found both to be efficient and to provide the best results in previous experiments involving word sense ranking (McCarthy et al. , 2004)5." ></td>
	<td class="line x" title="101:194	For distributionally similar words 5Note that unlike the above measure of distributional simAlgorithm 1 Word Sense Subjectivity Score Input: Word sense wi Input: Distributionally similar words DSW = {dswj|j = 1n} Output: Subjectivity score subj(wi) 1: subj(wi) = 0 2: totalsim = 0 3: for j = 1 to n do 4: Instsj = all instances of dswj in the MPQA corpus 5: for k in Instsj do 6: if k is in a subj." ></td>
	<td class="line x" title="102:194	expr." ></td>
	<td class="line x" title="103:194	in MPQA corpus then 7: subj(wi) += sim(wi,dswj) 8: else if k is not in a subj." ></td>
	<td class="line x" title="104:194	expr." ></td>
	<td class="line x" title="105:194	in MPQA corpus then 9: subj(wi) -= sim(wi,dswj) 10: end if 11: totalsim += sim(wi,dswj) 12: end for 13: end for 14: subj(wi) = subj(wi) / totalsim that are themselves ambiguous, we use the sense that maximizes the similarity score." ></td>
	<td class="line x" title="106:194	The similarity scores associated with each word dswj are normalized so that they add up to one across all possible senses of w, which results in a score described by the following formula: sim(wsi, dswj) = wnss(wsi,dswj)summationtext iprimesenses(w) wnss(wsiprime,dswj) where wnss(wsi, dswj) = max ksenses(dswj) wnss(wsi, dswkj ) A selection process can also be applied so that a distributionally similar word belongs only to one sense." ></td>
	<td class="line x" title="107:194	In this case, for a given sense wi we use only those distributionally similar words with whom wi has the highest similarity score across all the senses of w. We refer to this case as similarityselected, as opposed to similarity-all, which refers to the use of all distributionally similar words for all senses." ></td>
	<td class="line x" title="108:194	Once we have a list of similar words associated with each sense wsi and the corresponding similarity scores sim(wsi, dswj), we use an annotated corpus to assign subjectivity scores to the senses." ></td>
	<td class="line x" title="109:194	The corpus we use is the MPQA Opinion Corpus, which consists of over 10,000 sentences from the world press annotated for subjective expressions (all three types of subjective expressions described in Section 2).6 ilarity which measures similarity between words, rather than word senses, here we needed a similarity measure that also takes into account word senses as defined in a sense inventory such as WordNet." ></td>
	<td class="line x" title="110:194	6The MPQA corpus is described in (Wiebe et al. , 2005) and available at www.cs.pitt.edu/mpqa/databaserelease/." ></td>
	<td class="line x" title="111:194	1068 Algorithm 1 is our method for calculating sense subjectivity scores." ></td>
	<td class="line x" title="112:194	The subjectivity score is a value in the interval [-1,+1] with +1 corresponding to highly subjective and -1 corresponding to highly objective." ></td>
	<td class="line x" title="113:194	It is a sum of sim scores, where sim(wi,dswj) is added for each instance of dswj that is in a subjective expression, and subtracted for each instance that is not in a subjective expression." ></td>
	<td class="line x" title="114:194	Note that the annotations in the MPQA corpus are for subjective expressions in context." ></td>
	<td class="line x" title="115:194	Thus, the data is somewhat noisy for our task, because, as discussed in Section 3, objective senses may appear in subjective expressions." ></td>
	<td class="line x" title="116:194	Nonetheless, we hypothesized that subjective senses tend to appear more often in subjective expressions than objective senses do, and use the appearance of words in subjective expressions as evidence of sense subjectivity." ></td>
	<td class="line x" title="117:194	(Wiebe, 2000) also makes use of an annotated corpus, but in a different approach: given a word w and a set of distributionally similar words DSW, that method assigns a subjectivity score to w equal to the conditional probability that any member of DSW is in a subjective expression." ></td>
	<td class="line x" title="118:194	Moreover, the end task of that work was to annotate words, while our end task is the more difficult problem of annotating word senses for subjectivity." ></td>
	<td class="line x" title="119:194	4.1 Evaluation The evaluation of the algorithm is performed against the gold standard of 64 words (354 word senses) using Judge 1s annotations, as described in Section 3." ></td>
	<td class="line x" title="120:194	For each sense of each word in the set of 64 ambiguous words, we use Algorithm 1 to determine a subjectivity score." ></td>
	<td class="line x" title="121:194	A subjectivity label is then assigned depending on the value of this score with respect to a pre-selected threshold." ></td>
	<td class="line x" title="122:194	While a threshold of 0 seems like a sensible choice, we perform the evaluation for different thresholds ranging across the [-1,+1] interval, and correspondingly determine the precision of the algorithm at different points of recall7." ></td>
	<td class="line x" title="123:194	Note that the word senses for which none of the distributionally similar words are found in the MPQA corpus are not 7Specifically, in the list of word senses ranked by their subjectivity score, we assign a subjectivity label to the top N word senses." ></td>
	<td class="line x" title="124:194	The precision is then determined as the number of correct subjectivity label assignments out of all N assignments, while the recall is measured as the correct subjective senses out of all the subjective senses in the gold standard data set." ></td>
	<td class="line x" title="125:194	By varying the value of N from 1 to the total number of senses in the corpus, we can derive precision and recall curves." ></td>
	<td class="line x" title="126:194	included in this evaluation (excluding 82 senses), since in this case a subjectivity score cannot be calculated." ></td>
	<td class="line x" title="127:194	The evaluation is therefore performed on a total of 272 word senses." ></td>
	<td class="line x" title="128:194	As a baseline, we use an informed random assignment of subjectivity labels, which randomly assigns S labels to word senses in the data set, such that the maximum number of S assignments equals the number of correct S labels in the gold standard data set." ></td>
	<td class="line x" title="129:194	This baseline guarantees a maximum recall of 1 (which under true random conditions might not be achievable)." ></td>
	<td class="line x" title="130:194	Correspondingly, given the controlled distribution of S labels across the data set in the baseline setting, the precision is equal for all eleven recall points, and is determined as the total number of correct subjective assignments divided by the size of the data set8." ></td>
	<td class="line x" title="131:194	Number Break-even Algorithm of DSW point similarity-all 100 0.41 similarity-selected 100 0.50 similarity-all 160 0.43 similarity-selected 160 0.50 baseline 0.27 Table 2: Break-even point for different algorithm and parameter settings There are two aspects of the sense subjectivity scoring algorithm that can influence the label assignment, and correspondingly their evaluation." ></td>
	<td class="line x" title="132:194	First, as indicated above, after calculating the semantic similarity of the distributionally similar words with each sense, we can either use all the distributionally similar words for the calculation of the subjectivity score of each sense (similarityall), or we can use only those that lead to the highest similarity (similarity-selected)." ></td>
	<td class="line x" title="133:194	Interestingly, this aspect can drastically affect the algorithm accuracy." ></td>
	<td class="line x" title="134:194	The setting where a distributionally similar word can belong only to one sense significantly improves the algorithm performance." ></td>
	<td class="line x" title="135:194	Figure 1 plots the interpolated precision for eleven points of recall, for similarity-all, similarity-selected, and baseline." ></td>
	<td class="line x" title="136:194	As shown in this figure, the precisionrecall curves for our algorithm are clearly above the informed baseline, indicating the ability of our algorithm to automatically identify subjective word senses." ></td>
	<td class="line x" title="137:194	Second, the number of distributionally similar words considered in the first stage of the algorithm can vary, and might therefore influence the 8In other words, this fraction represents the probability of making the correct subjective label assignment by chance." ></td>
	<td class="line x" title="138:194	1069 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Precision recall curves selected all baseline Figure 1: Precision and recall for automatic subjectivity annotations of word senses (DSW=160)." ></td>
	<td class="line x" title="139:194	output of the algorithm." ></td>
	<td class="line x" title="140:194	We experiment with two different values, namely 100 and 160 top-ranked distributionally similar words." ></td>
	<td class="line x" title="141:194	Table 2 shows the break-even points for the four different settings that were evaluated,9 with results that are almost double compared to the informed baseline." ></td>
	<td class="line x" title="142:194	As it turns out, for weaker versions of the algorithm (i.e. , similarity-all), the size of the set of distributionally similar words can significantly impact the performance of the algorithm." ></td>
	<td class="line x" title="143:194	However, for the already improved similarity-selected algorithm version, this parameter does not seem to have influence, as similar results are obtained regardless of the number of distributionally similar words." ></td>
	<td class="line x" title="144:194	This is in agreement with the finding of (McCarthy et al. , 2004) that, in their word sense ranking method, a larger set of neighbors did not influence the algorithm accuracy." ></td>
	<td class="line x" title="145:194	5 Automatic Subjectivity Annotations for Word Sense Disambiguation The final question we address is concerned with the potential impact of subjectivity on the quality of a word sense classifier." ></td>
	<td class="line x" title="146:194	To answer this question, we augment an existing data-driven word sense disambiguation system with a feature reflecting the subjectivity of the examples where the ambiguous word occurs, and evaluate the performance of the new subjectivity-aware classifier as compared to the traditional context-based sense classifier." ></td>
	<td class="line x" title="147:194	We use a word sense disambiguation system that integrates both local and topical features." ></td>
	<td class="line x" title="148:194	9The break-even point (Lewis, 1992) is a standard measure used in conjunction with precision-recall evaluations." ></td>
	<td class="line x" title="149:194	It represents the value where precision and recall become equal." ></td>
	<td class="line x" title="150:194	Specifically, we use the current word and its partof-speech, a local context of three words to the left and right of the ambiguous word, the parts-ofspeech of the surrounding words, and a global context implemented through sense-specific keywords determined as a list of at most five words occurring at least three times in the contexts defining a certain word sense." ></td>
	<td class="line x" title="151:194	This feature set is similar to the one used by (Ng and Lee, 1996), as well as by a number of SENSEVAL systems." ></td>
	<td class="line x" title="152:194	The parameters for sense-specific keyword selection were determined through cross-fold validation on the training set." ></td>
	<td class="line x" title="153:194	The features are integrated in a Naive Bayes classifier, which was selected mainly for its performance in previous work showing that it can lead to a state-of-the-art disambiguation system given the features we consider (Lee and Ng, 2002)." ></td>
	<td class="line x" title="154:194	The experiments are performed on the set of ambiguous nouns from the SENSEVAL-3 English lexical sample evaluation (Mihalcea et al. , 2004)." ></td>
	<td class="line x" title="155:194	We use the rule-based subjective sentence classifier of (Riloff and Wiebe, 2003) to assign an S, O, or B label to all the training and test examples pertaining to these ambiguous words." ></td>
	<td class="line x" title="156:194	This subjectivity annotation tool targets sentences, rather than words or paragraphs, and therefore the tool is fed with sentences." ></td>
	<td class="line x" title="157:194	We also include a surrounding context of two additional sentences, because the classifier considers some contextual information." ></td>
	<td class="line x" title="158:194	Our hypothesis motivating the use of a sentence-level subjectivity classifier is that instances of subjective senses are more likely to be in subjective sentences, and thus that sentence subjectivity is an informative feature for the disambiguation of words having both subjective and objective senses." ></td>
	<td class="line x" title="159:194	For each ambiguous word, we perform two separate runs: one using the basic disambiguation system described earlier, and another using the subjectivity-aware system that includes the additional subjectivity feature." ></td>
	<td class="line x" title="160:194	Table 3 shows the results obtained for these 20 nouns, including word sense disambiguation accuracy for the two different systems, the most frequent sense baseline, and the subjectivity/objectivity split among the word senses (according to Judge 1)." ></td>
	<td class="line x" title="161:194	The words in the top half of the table are the ones that have both S and O senses, and those in the bottom are the ones that do not." ></td>
	<td class="line x" title="162:194	If we were to use Judge 2s tags instead of Judge 1s, only one word would change: source would move from the top to the bottom of the table." ></td>
	<td class="line x" title="163:194	1070 Sense Data Classifier Word Senses subjectivity train test Baseline basic + subj." ></td>
	<td class="line x" title="164:194	Words with subjective senses argument 5 3-S 2-O 221 111 49.4% 51.4% 54.1% atmosphere 6 2-S 4-O 161 81 65.4% 65.4% 66.7% difference 5 2-S 3-O 226 114 40.4% 54.4% 57.0% difficulty 4 2-S 2-O 46 23 17.4% 47.8% 52.2% image 7 2-S 5-O 146 74 36.5% 41.2% 43.2% interest 7 1-S 5-O 1-B 185 93 41.9% 67.7% 68.8% judgment 7 5-S 2-O 62 32 28.1% 40.6% 43.8% plan 3 1-S 2-O 166 84 81.0% 81.0% 81.0% sort 4 1-S 2-O 1-B 190 96 65.6% 66.7% 67.7% source 9 1-S 8-O 64 32 40.6% 40.6% 40.6% Average 46.6% 55.6% 57.5% Words with no subjective senses arm 6 6-O 266 133 82.0% 85.0% 84.2% audience 4 4-O 200 100 67.0% 74.0% 74.0% bank 10 10-O 262 132 62.6% 62.6% 62.6% degree 7 5-O 2-B 256 128 60.9% 71.1% 71.1% disc 4 4-O 200 100 38.0% 65.6% 66.4% organization 7 7-O 112 56 64.3% 64.3% 64.3% paper 7 7-O 232 117 25.6% 49.6% 48.0% party 5 5-O 230 116 62.1% 62.9% 62.9% performance 5 5-O 172 87 26.4% 34.5% 34.5% shelter 5 5-O 196 98 44.9% 65.3% 65.3% Average 53.3% 63.5% 63.3% Average for all words 50.0% 59.5% 60.4% Table 3: Word Sense Disambiguation with and without subjectivity information, for the set of ambiguous nouns in SENSEVAL-3 For the words that have both S and O senses, the addition of the subjectivity feature alone can bring a significant error rate reduction of 4.3% (p < 0.05 paired t-test)." ></td>
	<td class="line x" title="165:194	Interestingly, no improvements are observed for the words with no subjective senses; on the contrary, the addition of the subjectivity feature results in a small degradation." ></td>
	<td class="line x" title="166:194	Overall for the entire set of ambiguous words, the error reduction is measured at 2.2% (significant at p < 0.1 paired t-test)." ></td>
	<td class="line x" title="167:194	In almost all cases, the words with both S and O senses show improvement, while the others show small degradation or no change." ></td>
	<td class="line x" title="168:194	This suggests that if a subjectivity label is available for the words in a lexical resource (e.g. using Algorithm 1 from Section 4), such information can be used to decide on using a subjectivity-aware system, thereby improving disambiguation accuracy." ></td>
	<td class="line x" title="169:194	One of the exceptions is disc, which had a small benefit, despite not having any subjective senses." ></td>
	<td class="line x" title="170:194	As it happens, the first sense of disc is phonograph record." ></td>
	<td class="line x" title="171:194	phonograph record, phonograph recording, record, disk, disc, platter  (sound recording consisting of a disc with continuous grooves; formerly used to reproduce music by rotating while a phonograph needle tracked in the grooves) The improvement can be explained by observing that many of the training and test sentences containing this sense are labeled subjective by the classifier, and indeed this sense frequently occurs in subjective sentences such as This is anyway a stunning disc. Another exception is the noun plan, which did not benefit from the subjectivity feature, although it does have a subjective sense." ></td>
	<td class="line x" title="172:194	This can perhaps be explained by the data set for this word, which seems to be particularly difficult, as the basic classifier itself could not improve over the most frequent sense baseline." ></td>
	<td class="line x" title="173:194	The other word that did not benefit from the subjectivity feature is the noun source, for which its only subjective sense did not appear in the sense-annotated data, leading therefore to an objective only set of examples." ></td>
	<td class="line x" title="174:194	6 Conclusion and Future Work The questions posed in the introduction concerning the possible interaction between subjectivity and word sense found answers throughout the paper." ></td>
	<td class="line x" title="175:194	As it turns out, a correlation can indeed be established between these two semantic properties of language." ></td>
	<td class="line x" title="176:194	Addressing the first question of whether subjectivity is a property that can be assigned to word senses, we showed that good agreement (=0.74) can be achieved between human annotators labeling the subjectivity of senses." ></td>
	<td class="line x" title="177:194	When uncertain cases are removed, the  value is even higher (0.90)." ></td>
	<td class="line x" title="178:194	Moreover, the automatic subjectivity scoring mechanism that we devised was able to successfully assign subjectivity labels to senses, significantly outperforming an informed baseline associated with the task." ></td>
	<td class="line x" title="179:194	While much work remains to be done, this first attempt has proved the feasibility of correctly assigning subjectivity labels to the fine-grained level of word senses." ></td>
	<td class="line x" title="180:194	The second question was also positively answered: the quality of a word sense disambiguation system can be improved with the addition of subjectivity information." ></td>
	<td class="line x" title="181:194	Section 5 provided evidence that automatic subjectivity classification may improve word sense disambiguation performance, but mainly for words with both subjective and objective senses." ></td>
	<td class="line x" title="182:194	As we saw, performance may even degrade for words that do not." ></td>
	<td class="line x" title="183:194	Tying the pieces of this paper together, once the senses in a dictionary have been assigned subjectivity labels, a word sense disambiguation system could consult them to decide whether it should consider or ignore the subjectivity feature." ></td>
	<td class="line x" title="184:194	There are several other ways our results could impact future work." ></td>
	<td class="line x" title="185:194	Subjectivity labels would be a useful source of information when manually augmenting the lexical knowledge in a dictionary, 1071 e.g., when choosing hypernyms for senses or deciding which senses to eliminate when defining a coarse-grained sense inventory (if there is a subjective sense, at least one should be retained)." ></td>
	<td class="line x" title="186:194	Adding subjectivity labels to WordNet could also support automatic subjectivity analysis." ></td>
	<td class="line x" title="187:194	First, the input corpus could be sense tagged and the subjectivity labels of the assigned senses could be exploited by a subjectivity recognition tool." ></td>
	<td class="line x" title="188:194	Second, a number of methods for subjectivity or sentiment analysis start with a set of seed words and then search through WordNet to find other subjective words (Kamps and Marx, 2002; Yu and Hatzivassiloglou, 2003; Hu and Liu, 2004; Kim and Hovy, 2004; Esuli and Sebastiani, 2005)." ></td>
	<td class="line x" title="189:194	However, such searches may veer off course down objective paths." ></td>
	<td class="line x" title="190:194	The subjectivity labels assigned to senses could be consulted to keep the search traveling along subjective paths." ></td>
	<td class="line x" title="191:194	Finally, there could be different strategies for exploiting subjectivity annotations and word sense." ></td>
	<td class="line x" title="192:194	While the current setting considered a pipeline approach, where the output of a subjectivity annotation system was fed to the input of a method for semantic disambiguation, future work could also consider the role of word senses as a possible way of improving subjectivity analysis, or simultaneous annotations of subjectivity and word meanings, as done in the past for other language processing problems." ></td>
	<td class="line x" title="193:194	Acknowledgments We would like to thank Theresa Wilson for annotating senses, and the anonymous reviewers for their helpful comments." ></td>
	<td class="line x" title="194:194	This work was partially supported by ARDA AQUAINT and by the NSF (award IIS-0208798)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-2079
Examining The Role Of Linguistic Knowledge Sources In The Automatic Identification And Classification Of Reviews
Ng, Vincent;Dasgupta, Sajib;Arifin, S. M. Niaz;"></td>
	<td class="line x" title="1:254	Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 611618, Sydney, July 2006." ></td>
	<td class="line x" title="2:254	c2006 Association for Computational Linguistics Examining the Role of Linguistic Knowledge Sources in the Automatic Identification and Classification of Reviews Vincent Ng and Sajib Dasgupta and S. M. Niaz Arifin Human Language Technology Research Institute University of Texas at Dallas Richardson, TX 75083-0688 {vince,sajib,arif}@hlt.utdallas.edu Abstract This paper examines two problems in document-level sentiment analysis: (1) determining whether a given document is a review or not, and (2) classifying the polarity of a review as positive or negative." ></td>
	<td class="line x" title="3:254	We first demonstrate that review identification can be performed with high accuracy using only unigrams as features." ></td>
	<td class="line x" title="4:254	We then examine the role of four types of simple linguistic knowledge sources in a polarity classification system." ></td>
	<td class="line x" title="5:254	1 Introduction Sentiment analysis involves the identification of positive and negative opinions from a text segment." ></td>
	<td class="line x" title="6:254	The task has recently received a lot of attention, with applications ranging from multiperspective question-answering (e.g. , Cardie et al.(2004)) to opinion-oriented information extraction (e.g. , Riloff et al.(2005)) and summarization (e.g. , Hu and Liu (2004))." ></td>
	<td class="line x" title="9:254	Research in sentiment analysis has generally proceeded at three levels, aiming to identify and classify opinions from documents, sentences, and phrases." ></td>
	<td class="line x" title="10:254	This paper examines two problems in document-level sentiment analysis, focusing on analyzing a particular type of opinionated documents: reviews." ></td>
	<td class="line x" title="11:254	The first problem, polarity classification, has the goal of determining a reviews polarity positive ( thumbs up ) or negative ( thumbs down )." ></td>
	<td class="line x" title="12:254	Recent work has expanded the polarity classification task to additionally handle documents expressing a neutral sentiment." ></td>
	<td class="line x" title="13:254	Although studied fairly extensively, polarity classification remains a challenge to natural language processing systems." ></td>
	<td class="line x" title="14:254	We will focus on an important linguistic aspect of polarity classification: examining the role of a variety of simple, yet under-investigated, linguistic knowledge sources in a learning-based polarity classification system." ></td>
	<td class="line x" title="15:254	Specifically, we will show how to build a high-performing polarity classifier by exploiting information provided by (1) high order n-grams, (2) a lexicon composed of adjectives manually annotated with their polarity information (e.g. , happy is annotated as positive and terrible as negative), (3) dependency relations derived from dependency parses, and (4) objective terms and phrases extracted from neutral documents." ></td>
	<td class="line x" title="16:254	As mentioned above, the majority of work on document-level sentiment analysis to date has focused on polarity classification, assuming as input a set of reviews to be classified." ></td>
	<td class="line x" title="17:254	A relevant question is: what if we dont know that an input document is a review in the first place?" ></td>
	<td class="line x" title="18:254	The second task we will examine in this paper review identification attempts to address this question." ></td>
	<td class="line x" title="19:254	Specifically, review identification seeks to determine whether a given document is a review or not." ></td>
	<td class="line x" title="20:254	We view both review identification and polarity classification as a classification task." ></td>
	<td class="line x" title="21:254	For review identification, we train a classifier to distinguish movie reviews and movie-related nonreviews (e.g. , movie ads, plot summaries) using only unigrams as features, obtaining an accuracy of over 99% via 10-fold cross-validation." ></td>
	<td class="line x" title="22:254	Similar experiments using documents from the book domain also yield an accuracy as high as 97%." ></td>
	<td class="line x" title="23:254	An analysis of the results reveals that the high accuracy can be attributed to the difference in the vocabulary employed in reviews and non-reviews: while reviews can be composed of a mixture of subjective and objective language, our non-review documents rarely contain subjective expressions." ></td>
	<td class="line oc" title="24:254	Next, we learn our polarity classifier using positive and negative reviews taken from two movie 611 review datasets, one assembled by Pang and Lee (2004) and the other by ourselves." ></td>
	<td class="line x" title="25:254	The resulting classifier, when trained on a feature set derived from the four types of linguistic knowledge sources mentioned above, achieves a 10-fold cross-validation accuracy of 90.5% and 86.1% on Pang et al.s dataset and ours, respectively." ></td>
	<td class="line x" title="26:254	To our knowledge, our result on Pang et al.s dataset is one of the best reported to date." ></td>
	<td class="line x" title="27:254	Perhaps more importantly, an analysis of these results show that the various types of features interact in an interesting manner, allowing us to draw conclusions that provide new insights into polarity classification." ></td>
	<td class="line x" title="28:254	2 Related Work 2.1 Review Identification As noted in the introduction, while a review can contain both subjective and objective phrases, our non-reviews are essentially factual documents in which subjective expressions can rarely be found." ></td>
	<td class="line x" title="29:254	Hence, review identification can be viewed as an instance of the broader task of classifying whether a document is mostly factual/objective or mostly opinionated/subjective." ></td>
	<td class="line x" title="30:254	There have been attempts on tackling this so-called document-level subjectivity classification task, with very encouraging results (see Yu and Hatzivassiloglou (2003) and Wiebe et al.(2004) for details)." ></td>
	<td class="line xc" title="32:254	2.2 Polarity Classification There is a large body of work on classifying the polarity of a document (e.g. , Pang et al.(2002), Turney (2002)), a sentence (e.g. , Liu et al.(2003), Yu and Hatzivassiloglou (2003), Kim and Hovy (2004), Gamon et al.(2005)), a phrase (e.g. , Wilson et al.(2005)), and a specific object (such as a product) mentioned in a document (e.g. , Morinaga et al.(2002), Yi et al.(2003), Popescu and Etzioni (2005))." ></td>
	<td class="line x" title="39:254	Below we will center our discussion of related work around the four types of features we will explore for polarity classification." ></td>
	<td class="line x" title="40:254	Higher-order n-grams." ></td>
	<td class="line x" title="41:254	While n-grams offer a simple way of capturing context, previous work has rarely explored the use of n-grams as features in a polarity classification system beyond unigrams." ></td>
	<td class="line x" title="42:254	Two notable exceptions are the work of Dave et al.(2003) and Pang et al.(2002)." ></td>
	<td class="line x" title="45:254	Interestingly, while Dave et al. report good performance on classifying reviews using bigrams or trigrams alone, Pang et al. show that bigrams are not useful features for the task, whether they are used in isolation or in conjunction with unigrams." ></td>
	<td class="line x" title="46:254	This motivates us to take a closer look at the utility of higher-order n-grams in polarity classification." ></td>
	<td class="line x" title="47:254	Manually-tagged term polarity." ></td>
	<td class="line x" title="48:254	Much work has been performed on learning to identify and classify polarity terms (i.e. , terms expressing a positive sentiment (e.g. , happy) or a negative sentiment (e.g. , terrible)) and exploiting them to do polarity classification (e.g. , Hatzivassiloglou and McKeown (1997), Turney (2002), Kim and Hovy (2004), Whitelaw et al.(2005), Esuli and Sebastiani (2005))." ></td>
	<td class="line x" title="50:254	Though reasonably successful, these (semi-)automatic techniques often yield lexicons that have either high coverage/low precision or low coverage/high precision." ></td>
	<td class="line x" title="51:254	While manually constructed positive and negative word lists exist (e.g. , General Inquirer1), they too suffer from the problem of having low coverage." ></td>
	<td class="line x" title="52:254	This prompts us to manually construct our own polarity word lists2 and study their use in polarity classification." ></td>
	<td class="line x" title="53:254	Dependency relations." ></td>
	<td class="line x" title="54:254	There have been several attempts at extracting features for polarity classification from dependency parses, but most focus on extracting specific types of information such as adjective-noun relations (e.g. , Dave et al.(2003), Yi et al.(2003)) or nouns that enjoy a dependency relation with a polarity term (e.g. , Popescu and Etzioni (2005))." ></td>
	<td class="line x" title="57:254	Wilson et al.(2005) extract a larger variety of features from dependency parses, but unlike us, their goal is to determine the polarity of a phrase, not a document." ></td>
	<td class="line x" title="59:254	In comparison to previous work, we investigate the use of a larger set of dependency relations for classifying reviews." ></td>
	<td class="line x" title="60:254	Objective information." ></td>
	<td class="line x" title="61:254	The objective portions of a review do not contain the authors opinion; hence features extracted from objective sentences and phrases are irrelevant with respect to the polarity classification task and their presence may complicate the learning task." ></td>
	<td class="line pc" title="62:254	Indeed, recent work has shown that benefits can be made by first separating facts from opinions in a document (e.g, Yu and Hatzivassiloglou (2003)) and classifying the polarity based solely on the subjective portions of the document (e.g. , Pang and Lee (2004))." ></td>
	<td class="line x" title="63:254	Motivated by the work of Koppel and Schler (2005), we identify and extract objective material from nonreviews and show how to exploit such information in polarity classification." ></td>
	<td class="line x" title="64:254	1http://www.wjh.harvard.edu/inquirer/ spreadsheet guid.htm 2Wilson et al.(2005) have also manually tagged a list of terms with their polarity, but this list is not publicly available." ></td>
	<td class="line x" title="66:254	612 Finally, previous work has also investigated features that do not fall into any of the above categories." ></td>
	<td class="line x" title="67:254	For instance, instead of representing the polarity of a term using a binary value, Mullen and Collier (2004) use Turneys (2002) method to assign a real value to represent term polarity and introduce a variety of numerical features that are aggregate measures of the polarity values of terms selected from the document under consideration." ></td>
	<td class="line x" title="68:254	3 Review Identification Recall that the goal of review identification is to determine whether a given document is a review or not." ></td>
	<td class="line x" title="69:254	Given this definition, two immediate questions come to mind." ></td>
	<td class="line x" title="70:254	First, should this problem be addressed in a domain-specific or domainindependent manner?" ></td>
	<td class="line x" title="71:254	In other words, should a review identification system take as input documents coming from the same domain or not?" ></td>
	<td class="line x" title="72:254	Apparently this is a design question with no definite answer, but our decision is to perform domain-specific review identification." ></td>
	<td class="line x" title="73:254	The reason is that the primary motivation of review identification is the need to identify reviews for further analysis by a polarity classification system." ></td>
	<td class="line x" title="74:254	Since polarity classification has almost exclusively been addressed in a domain-specific fashion, it seems natural that its immediate upstream component review identification should also assume domain specificity." ></td>
	<td class="line x" title="75:254	Note, however, that assuming domain specificity is not a self-imposed limitation." ></td>
	<td class="line x" title="76:254	In fact, we envision that the review identification system will have as its upstream component a text classification system, which will classify documents by topic and pass to the review identifier only those documents that fall within its domain." ></td>
	<td class="line x" title="77:254	Given our choice of domain specificity, the next question is: which documents are non-reviews?" ></td>
	<td class="line x" title="78:254	Here, we adopt a simple and natural definition: a non-review is any document that belongs to the given domain but is not a review." ></td>
	<td class="line x" title="79:254	Dataset." ></td>
	<td class="line x" title="80:254	Now, recall from the introduction that we cast review identification as a classification task." ></td>
	<td class="line x" title="81:254	To train and test our review identifier, we use 2000 reviews and 2000 non-reviews from the movie domain." ></td>
	<td class="line x" title="82:254	The 2000 reviews are taken from Pang et al.s polarity dataset (version 2.0)3, which consists of an equal number of positive and negative reviews." ></td>
	<td class="line x" title="83:254	We collect the non-reviews for the 3Available from http://www.cs.cornell.edu/ people/pabo/movie-review-data." ></td>
	<td class="line x" title="84:254	movie domain from the Internet Movie Database website4, randomly selecting any documents from this site that are on the movie topic but are not reviews themselves." ></td>
	<td class="line x" title="85:254	With this criterion in mind, the 2000 non-review documents we end up with are either movie ads or plot summaries." ></td>
	<td class="line x" title="86:254	Training and testing the review identifier." ></td>
	<td class="line x" title="87:254	We perform 10-fold cross-validation (CV) experiments on the above dataset, using Joachims (1999) SVMlight package5 to train an SVM classifier for distinguishing reviews and non-reviews." ></td>
	<td class="line x" title="88:254	All learning parameters are set to their default values.6 Each document is first tokenized and downcased, and then represented as a vector of unigrams with length normalization.7 Following Pang et al.(2002), we use frequency as presence." ></td>
	<td class="line x" title="90:254	In other words, the ith element of the document vector is 1 if the corresponding unigram is present in the document and 0 otherwise." ></td>
	<td class="line x" title="91:254	The resulting classifier achieves an accuracy of 99.8%." ></td>
	<td class="line x" title="92:254	Classifying neutral reviews and non-reviews." ></td>
	<td class="line x" title="93:254	Admittedly, the high accuracy achieved using such a simple set of features is somewhat surprising, although it is consistent with previous results on document-level subjectivity classification in which accuracies of 94-97% were obtained (Yu and Hatzivassiloglou, 2003; Wiebe et al. , 2004)." ></td>
	<td class="line x" title="94:254	Before concluding that review classification is an easy task, we conduct an additional experiment: we train a review identifier on a new dataset where we keep the same 2000 non-reviews but replace the positive/negative reviews with 2000 neutral reviews (i.e. , reviews with a mediocre rating)." ></td>
	<td class="line x" title="95:254	Intuitively, a neutral review contains fewer terms with strong polarity than a positive/negative review." ></td>
	<td class="line x" title="96:254	Hence, this additional experiment would allow us to investigate whether the lack of strong polarized terms in neutral reviews would increase the difficulty of the learning task." ></td>
	<td class="line x" title="97:254	Our neutral reviews are randomly chosen from Pang et al.s pool of 27886 unprocessed movie reviews8 that have either a rating of 2 (on a 4-point scale) or 2.5 (on a 5-point scale)." ></td>
	<td class="line x" title="98:254	Each review then undergoes a semi-automatic preprocessing stage 4See http://www.imdb.com." ></td>
	<td class="line x" title="99:254	5Available from svmlight.joachims.org." ></td>
	<td class="line x" title="100:254	6We tried polynomial and RBF kernels, but none yields better performance than the default linear kernel." ></td>
	<td class="line x" title="101:254	7We observed that not performing length normalization hurts performance slightly." ></td>
	<td class="line x" title="102:254	8Also available from Pangs website." ></td>
	<td class="line x" title="103:254	See Footnote 3." ></td>
	<td class="line x" title="104:254	613 where (1) HTML tags and any header and trailer information (such as date and author identity) are removed; (2) the document is tokenized and downcased; (3) the rating information extracted by regular expressions is removed; and (4) the document is manually checked to ensure that the rating information is successfully removed." ></td>
	<td class="line x" title="105:254	When trained on this new dataset, the review identifier also achieves an accuracy of 99.8%, suggesting that this learning task isnt any harder in comparison to the previous one." ></td>
	<td class="line x" title="106:254	Discussion." ></td>
	<td class="line x" title="107:254	We hypothesized that the high accuracies are attributable to the different vocabulary used in reviews and non-reviews." ></td>
	<td class="line x" title="108:254	As part of our verification of this hypothesis, we plot the learning curve for each of the above experiments.9 We observe that a 99% accuracy was achieved in all cases even when only 200 training instances are used to acquire the review identifier." ></td>
	<td class="line x" title="109:254	The ability to separate the two classes with such a small amount of training data seems to imply that features strongly indicative of one or both classes are present." ></td>
	<td class="line x" title="110:254	To test this hypothesis, we examine the informative features for both classes." ></td>
	<td class="line x" title="111:254	To get these informative features, we rank the features by their weighted log-likelihood ratio (WLLR)10: P (wtjcj) log P (wtjcj)P (w tj:cj), where wt and cj denote the tth word in the vocabulary and the jth class, respectively." ></td>
	<td class="line x" title="112:254	Informally, a feature (in our case a unigram) w will have a high rank with respect to a class c if it appears frequently in c and infrequently in other classes." ></td>
	<td class="line x" title="113:254	This correlates reasonably well with what we think an informative feature should be." ></td>
	<td class="line x" title="114:254	A closer examination of the feature lists sorted by WLLR confirms our hypothesis that each of the two classes has its own set of distinguishing features." ></td>
	<td class="line x" title="115:254	Experiments with the book domain." ></td>
	<td class="line x" title="116:254	To understand whether these good review identification results only hold true for the movie domain, we conduct similar experiments with book reviews and non-reviews." ></td>
	<td class="line x" title="117:254	Specifically, we collect 1000 book reviews (consisting of a mixture of positive, negative, and neutral reviews) from the Barnes 9The curves are not shown due to space limitations." ></td>
	<td class="line x" title="118:254	10Nigam et al.(2000) show that this metric is effective at selecting good features for text classification." ></td>
	<td class="line x" title="120:254	Other commonly-used feature selection metrics are discussed in Yang and Pedersen (1997)." ></td>
	<td class="line x" title="121:254	and Noble website11, and 1000 non-reviews that are on the book topic (mostly book summaries) from Amazon.12 We then perform 10-fold CV experiments using these 2000 documents as before, achieving a high accuracy of 96.8%." ></td>
	<td class="line x" title="122:254	These results seem to suggest that automatic review identification can be achieved with high accuracy." ></td>
	<td class="line x" title="123:254	4 Polarity Classification Compared to review identification, polarity classification appears to be a much harder task." ></td>
	<td class="line x" title="124:254	This section examines the role of various linguistic knowledge sources in our learning-based polarity classification system." ></td>
	<td class="line oc" title="125:254	4.1 Experimental Setup Like several previous work (e.g. , Mullen and Collier (2004), Pang and Lee (2004), Whitelaw et al.(2005)), we view polarity classification as a supervised learning task." ></td>
	<td class="line x" title="127:254	As in review identification, we use SVMlight with default parameter settings to train polarity classifiers13, reporting all results as 10-fold CV accuracy." ></td>
	<td class="line x" title="128:254	We evaluate our polarity classifiers on two movie review datasets, each of which consists of 1000 positive reviews and 1000 negative reviews." ></td>
	<td class="line o" title="129:254	The first one, which we will refer to as Dataset A, is the Pang et al. polarity dataset (version 2.0)." ></td>
	<td class="line x" title="130:254	The second one (Dataset B) was created by us, with the sole purpose of providing additional experimental results." ></td>
	<td class="line o" title="131:254	Reviews in Dataset B were randomly chosen from Pang et al.s pool of 27886 unprocessed movie reviews (see Section 3) that have either a positive or a negative rating." ></td>
	<td class="line o" title="132:254	We followed exactly Pang et al.s guideline when determining whether a review is positive or negative.14 Also, we took care to ensure that reviews included in Dataset B do not appear in Dataset A. We applied to these reviews the same four pre-processing steps that we did to the neutral reviews in the previous section." ></td>
	<td class="line x" title="133:254	4.2 Results The baseline classifier." ></td>
	<td class="line x" title="134:254	We can now train our baseline polarity classifier on each of the two 11www.barnesandnoble.com 12www.amazon.com 13We also experimented with polynomial and RBF kernels when training polarity classifiers, but neither yields better results than linear kernels." ></td>
	<td class="line o" title="135:254	14The guidelines come with their polarity dataset." ></td>
	<td class="line x" title="136:254	Brie y, a positive review has a rating of  3.5 (out of 5) or  3 (out of 4), whereas a negative review has a rating of 2 (out of 5) or  1.5 (out of 4)." ></td>
	<td class="line x" title="137:254	614 System Variation Dataset A Dataset B Baseline 87.1 82.7 Adding bigrams 89.2 84.7 and trigrams Adding dependency 89.0 84.5 relations Adding polarity 90.4 86.2 info of adjectives Discarding objective 90.5 86.1 materials Table 1: Polarity classification accuracies." ></td>
	<td class="line x" title="138:254	datasets." ></td>
	<td class="line x" title="139:254	Our baseline classifier employs as features the k highest-ranking unigrams according to WLLR, with k/2 features selected from each class." ></td>
	<td class="line x" title="140:254	Results with k = 10000 are shown in row 1 of Table 1.15 As we can see, the baseline achieves an accuracy of 87.1% and 82.7% on Datasets A and B, respectively." ></td>
	<td class="line oc" title="141:254	Note that our result on Dataset A is as strong as that obtained by Pang and Lee (2004) via their subjectivity summarization algorithm, which retains only the subjective portions of a document." ></td>
	<td class="line x" title="142:254	As a sanity check, we duplicated Pang et al.s (2002) baseline in which all unigrams that appear four or more times in the training documents are used as features." ></td>
	<td class="line o" title="143:254	The resulting classifier achieves an accuracy of 87.2% and 82.7% for Datasets A and B, respectively." ></td>
	<td class="line x" title="144:254	Neither of these results are significantly different from our baseline results.16 Adding higher-order n-grams." ></td>
	<td class="line x" title="145:254	The negative results that Pang et al.(2002) obtained when using bigrams as features for their polarity classifier seem to suggest that high-order n-grams are not useful for polarity classification." ></td>
	<td class="line x" title="147:254	However, recent research in the related (but arguably simpler) task of text classification shows that a bigrambased text classifier outperforms its unigrambased counterpart (Peng et al. , 2003)." ></td>
	<td class="line x" title="148:254	This prompts us to re-examine the utility of high-order n-grams in polarity classification." ></td>
	<td class="line x" title="149:254	In our experiments we consider adding bigrams and trigrams to our baseline feature set." ></td>
	<td class="line x" title="150:254	However, since these higher-order n-grams significantly outnumber the unigrams, adding all of them to the feature set will dramatically increase the dimen15We experimented with several values of k and obtained the best result with k = 10000." ></td>
	<td class="line x" title="151:254	16We use two-tailed paired t-tests when performing significance testing, with p set to 0.05 unless otherwise stated." ></td>
	<td class="line x" title="152:254	sionality of the feature space and may undermine the impact of the unigrams in the resulting classifier." ></td>
	<td class="line x" title="153:254	To avoid this potential problem, we keep the number of unigrams and higher-order n-grams equal." ></td>
	<td class="line x" title="154:254	Specifically, we augment the baseline feature set (consisting of 10000 unigrams) with 5000 bigrams and 5000 trigrams." ></td>
	<td class="line x" title="155:254	The bigrams and trigrams are selected based on their WLLR computed over the positive reviews and negative reviews in the training set for each CV run." ></td>
	<td class="line x" title="156:254	Results using this augmented feature set are shown in row 2 of Table 1." ></td>
	<td class="line x" title="157:254	We see that accuracy rises significantly from 87.1% to 89.2% for Dataset A and from 82.7% to 84.7% for Dataset B. This provides evidence that polarity classification can indeed benefit from higher-order n-grams." ></td>
	<td class="line x" title="158:254	Adding dependency relations." ></td>
	<td class="line x" title="159:254	While bigrams and trigrams are good at capturing local dependencies, dependency relations can be used to capture non-local dependencies among the constituents of a sentence." ></td>
	<td class="line x" title="160:254	Hence, we hypothesized that our ngram-based polarity classifier would benefit from the addition of dependency-based features." ></td>
	<td class="line x" title="161:254	Unlike most previous work on polarity classification, which has largely focused on exploiting adjective-noun (AN) relations (e.g. , Dave et al.(2003), Popescu and Etzioni (2005)), we hypothesized that subject-verb (SV) and verb-object (VO) relations would also be useful for the task." ></td>
	<td class="line x" title="163:254	The following (one-sentence) review illustrates why." ></td>
	<td class="line x" title="164:254	While I really like the actors, the plot is rather uninteresting." ></td>
	<td class="line x" title="165:254	A unigram-based polarity classifier could be confused by the simultaneous presence of the positive term like and the negative term uninteresting when classifying this review." ></td>
	<td class="line x" title="166:254	However, incorporating the VO relation (like, actors) as a feature may allow the learner to learn that the author likes the actors and not necessarily the movie." ></td>
	<td class="line x" title="167:254	In our experiments, the SV, VO and AN relations are extracted from each document by the MINIPAR dependency parser (Lin, 1998)." ></td>
	<td class="line x" title="168:254	As with n-grams, instead of using all the SV, VO and AN relations as features, we select among them the best 5000 according to their WLLR and retrain the polarity classifier with our n-gram-based feature set augmented by these 5000 dependencybased features." ></td>
	<td class="line x" title="169:254	Results in row 3 of Table 1 are somewhat surprising: the addition of dependencybased features does not offer any improvements over the simple n-gram-based classifier." ></td>
	<td class="line x" title="170:254	615 Incorporating manually tagged term polarity." ></td>
	<td class="line x" title="171:254	Next, we consider incorporating a set of features that are computed based on the polarity of adjectives." ></td>
	<td class="line x" title="172:254	As noted before, we desire a high-precision, high-coverage lexicon." ></td>
	<td class="line x" title="173:254	So, instead of exploiting a learned lexicon, we manually develop one." ></td>
	<td class="line o" title="174:254	To construct the lexicon, we take Pang et al.s pool of unprocessed documents (see Section 3), remove those that appear in either Dataset A or Dataset B17, and compile a list of adjectives from the remaining documents." ></td>
	<td class="line x" title="175:254	Then, based on heuristics proposed in psycholinguistics18, we handannotate each adjective with its prior polarity (i.e. , polarity in the absence of context)." ></td>
	<td class="line x" title="176:254	Out of the 45592 adjectives we collected, 3599 were labeled as positive, 3204 as negative, and 38789 as neutral." ></td>
	<td class="line x" title="177:254	A closer look at these adjectives reveals that they are by no means domain-dependent despite the fact that they were taken from movie reviews." ></td>
	<td class="line x" title="178:254	Now let us consider a simple procedure P for deriving a feature set that incorporates information from our lexicon: (1) collect all the bigrams from the training set; (2) for each bigram that contains at least one adjective labeled as positive or negative according to our lexicon, create a new feature that is identical to the bigram except that each adjective is replaced with its polarity label19; (3) merge the list of newly generated features with the list of bigrams20 and select the top 5000 features from the merged list according to their WLLR." ></td>
	<td class="line x" title="179:254	We then repeat procedure P for the trigrams and also the dependency features, resulting in a total of 15000 features." ></td>
	<td class="line x" title="180:254	Our new feature set comprises these 15000 features as well as the 10000 unigrams we used in the previous experiments." ></td>
	<td class="line x" title="181:254	Results of the polarity classifier that incorporates term polarity information are encouraging (see row 4 of Table 1)." ></td>
	<td class="line o" title="182:254	In comparison to the classifier that uses only n-grams and dependency-based features (row 3), accuracy increases significantly (p =.1) from 89.2% to 90.4% for Dataset A, and from 84.7% to 86.2% for Dataset B. These results suggest that the classifier has benefited from the 17We treat the test documents as unseen data that should not be accessed for any purpose during system development." ></td>
	<td class="line x" title="183:254	18http://www.sci.sdsu.edu/CAL/wordlist 19Neutral adjectives are not replaced." ></td>
	<td class="line x" title="184:254	20A newly generated feature could be misleading for the learner if the contextual polarity (i.e. , polarity in the presence of context) of the adjective involved differs from its prior polarity (see Wilson et al.(2005))." ></td>
	<td class="line x" title="186:254	The motivation behind merging with the bigrams is to create a feature set that is more robust in the face of potentially misleading generalizations." ></td>
	<td class="line x" title="187:254	use of features that are less sparse than n-grams." ></td>
	<td class="line x" title="188:254	Using objective information." ></td>
	<td class="line x" title="189:254	Some of the 25000 features we generated above correspond to n-grams or dependency relations that do not contain subjective information." ></td>
	<td class="line x" title="190:254	We hypothesized that not employing these objective features in the feature set would improve system performance." ></td>
	<td class="line x" title="191:254	More specifically, our goal is to use procedure P again to generate 25000 subjective features by ensuring that the objective ones are not selected for incorporation into our feature set." ></td>
	<td class="line x" title="192:254	To achieve this goal, we first use the following rote-learning procedure to identify objective material: (1) extract all unigrams that appear in objective documents, which in our case are the 2000 non-reviews used in review identification [see Section 3]; (2) from these objective unigrams, we take the best 20000 according to their WLLR computed over the non-reviews and the reviews in the training set for each CV run; (3) repeat steps 1 and 2 separately for bigrams, trigrams and dependency relations; (4) merge these four lists to create our 80000-element list of objective material." ></td>
	<td class="line x" title="193:254	Now, we can employ procedure P to get a list of 25000 subjective features by ensuring that those that appear in our 80000-element list are not selected for incorporation into our feature set." ></td>
	<td class="line x" title="194:254	Results of our classifier trained using these subjective features are shown in row 5 of Table 1." ></td>
	<td class="line x" title="195:254	Somewhat surprisingly, in comparison to row 4, we see that our method for filtering objective features does not help improve performance on the two datasets." ></td>
	<td class="line x" title="196:254	We will examine the reasons in the following subsection." ></td>
	<td class="line x" title="197:254	4.3 Discussion and Further Analysis Using the four types of knowledge sources previously described, our polarity classifier significantly outperforms a unigram-based baseline classifier." ></td>
	<td class="line x" title="198:254	In this subsection, we analyze some of these results and conduct additional experiments in an attempt to gain further insight into the polarity classification task." ></td>
	<td class="line o" title="199:254	Due to space limitations, we will simply present results on Dataset A below, and show results on Dataset B only in cases where a different trend is observed." ></td>
	<td class="line x" title="200:254	The role of feature selection." ></td>
	<td class="line x" title="201:254	In all of our experiments we used the best k features obtained via WLLR." ></td>
	<td class="line x" title="202:254	An interesting question is: how will these results change if we do not perform feature selection?" ></td>
	<td class="line x" title="203:254	To investigate this question, we conduct two 616 experiments." ></td>
	<td class="line x" title="204:254	First, we train a polarity classifier using all unigrams from the training set." ></td>
	<td class="line x" title="205:254	Second, we train another polarity classifier using all unigrams, bigrams, and trigrams." ></td>
	<td class="line x" title="206:254	We obtain an accuracy of 87.2% and 79.5% for the first and second experiments, respectively." ></td>
	<td class="line x" title="207:254	In comparison to our baseline classifier, which achieves an accuracy of 87.1%, we can see that using all unigrams does not hurt performance, but performance drops abruptly with the addition of all bigrams and trigrams." ></td>
	<td class="line x" title="208:254	These results suggest that feature selection is critical when bigrams and trigrams are used in conjunction with unigrams for training a polarity classifier." ></td>
	<td class="line x" title="209:254	The role of bigrams and trigrams." ></td>
	<td class="line x" title="210:254	So far we have seen that training a polarity classifier using only unigrams gives us reasonably good, though not outstanding, results." ></td>
	<td class="line x" title="211:254	Our question, then, is: would bigrams alone do a better job at capturing the sentiment of a document than unigrams?" ></td>
	<td class="line x" title="212:254	To answer this question, we train a classifier using all bigrams (without feature selection) and obtain an accuracy of 83.6%, which is significantly worse than that of a unigram-only classifier." ></td>
	<td class="line x" title="213:254	Similar results were also obtained by Pang et al.(2002)." ></td>
	<td class="line x" title="215:254	It is possible that the worse result is due to the presence of a large number of irrelevant bigrams." ></td>
	<td class="line x" title="216:254	To test this hypothesis, we repeat the above experiment except that we only use the best 10000 bigrams selected according to WLLR." ></td>
	<td class="line x" title="217:254	Interestingly, the resulting classifier gives us a lower accuracy of 82.3%, suggesting that the poor accuracy is not due to the presence of irrelevant bigrams." ></td>
	<td class="line x" title="218:254	To understand why using bigrams alone does not yield a good classification model, we examine a number of test documents and find that the feature vectors corresponding to some of these documents (particularly the short ones) have all zeroes in them." ></td>
	<td class="line x" title="219:254	In other words, none of the bigrams from the training set appears in these reviews." ></td>
	<td class="line x" title="220:254	This suggests that the main problem with the bigram model is likely to be data sparseness." ></td>
	<td class="line x" title="221:254	Additional experiments show that the trigram-only classifier yields even worse results than the bigram-only classifier, probably because of the same reason." ></td>
	<td class="line x" title="222:254	Nevertheless, these higher-order n-grams play a non-trivial role in polarity classification: we have shown that the addition of bigrams and trigrams selected via WLLR to a unigram-based classifier significantly improves its performance." ></td>
	<td class="line x" title="223:254	The role of dependency relations." ></td>
	<td class="line x" title="224:254	In the previous subsection we see that dependency relations do not contribute to overall performance on top of bigrams and trigrams." ></td>
	<td class="line x" title="225:254	There are two plausible reasons." ></td>
	<td class="line x" title="226:254	First, dependency relations are simply not useful for polarity classification." ></td>
	<td class="line x" title="227:254	Second, the higher-order n-grams and the dependency-based features capture essentially the same information and so using either of them would be sufficient." ></td>
	<td class="line x" title="228:254	To test the first hypothesis, we train a classifier using only 10000 unigrams and 10000 dependency-based features (both selected according to WLLR)." ></td>
	<td class="line o" title="229:254	For Dataset A, the classifier achieves an accuracy of 87.1%, which is statistically indistinguishable from our baseline result." ></td>
	<td class="line x" title="230:254	On the other hand, the accuracy for Dataset B is 83.5%, which is significantly better than the corresponding baseline (82.7%) at the p = .1 level." ></td>
	<td class="line x" title="231:254	These results indicate that dependency information is somewhat useful for the task when bigrams and trigrams are not used." ></td>
	<td class="line x" title="232:254	So the first hypothesis is not entirely true." ></td>
	<td class="line x" title="233:254	So, it seems to be the case that the dependency relations do not provide useful knowledge for polarity classification only in the presence of bigrams and trigrams." ></td>
	<td class="line x" title="234:254	This is somewhat surprising, since these n-grams do not capture the non-local dependencies (such as those that may be present in certain SV or VO relations) that should intuitively be useful for polarity classification." ></td>
	<td class="line x" title="235:254	To better understand this issue, we again examine a number of test documents." ></td>
	<td class="line x" title="236:254	Our initial investigation suggests that the problem might have stemmed from the fact that MINIPAR returns dependency relations in which all the verb in ections are removed." ></td>
	<td class="line x" title="237:254	For instance, given the sentence My cousin Paul really likes this long movie, MINIPAR will return the VO relation (like, movie)." ></td>
	<td class="line x" title="238:254	To see why this can be a problem, consider another sentence I like this long movie." ></td>
	<td class="line x" title="239:254	From this sentence, MINIPAR will also extract the VO relation (like, movie)." ></td>
	<td class="line x" title="240:254	Hence, this same VO relation is capturing two different situations, one in which the author himself likes the movie, and in the other, the authors cousin likes the movie." ></td>
	<td class="line x" title="241:254	The overgeneralization resulting from these stemmed relations renders dependency information not useful for polarity classification." ></td>
	<td class="line x" title="242:254	Additional experiments are needed to determine the role of dependency relations when stemming in MINIPAR is disabled." ></td>
	<td class="line x" title="243:254	617 The role of objective information." ></td>
	<td class="line x" title="244:254	Results from the previous subsection suggest that our method for extracting objective materials and removing them from the reviews is not effective in terms of improving performance." ></td>
	<td class="line x" title="245:254	To determine the reason, we examine the n-grams and the dependency relations that are extracted from the nonreviews." ></td>
	<td class="line x" title="246:254	We find that only in a few cases do these extracted objective materials appear in our set of 25000 features obtained in Section 4.2." ></td>
	<td class="line x" title="247:254	This explains why our method is not as effective as we originally thought." ></td>
	<td class="line x" title="248:254	We conjecture that more sophisticated methods would be needed in order to take advantage of objective information in polarity classification (e.g. , Koppel and Schler (2005))." ></td>
	<td class="line x" title="249:254	5 Conclusions We have examined two problems in documentlevel sentiment analysis, namely, review identification and polarity classification." ></td>
	<td class="line x" title="250:254	We first found that review identification can be achieved with very high accuracies (97-99%) simply by training an SVM classifier using unigrams as features." ></td>
	<td class="line x" title="251:254	We then examined the role of several linguistic knowledge sources in polarity classification." ></td>
	<td class="line x" title="252:254	Our results suggested that bigrams and trigrams selected according to the weighted log-likelihood ratio as well as manually tagged term polarity information are very useful features for the task." ></td>
	<td class="line x" title="253:254	On the other hand, no further performance gains are obtained by incorporating dependency-based information or filtering objective materials from the reviews using our proposed method." ></td>
	<td class="line x" title="254:254	Nevertheless, the resulting polarity classifier compares favorably to state-of-the-art sentiment classification systems." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-0302
Toward Opinion Summarization: Linking The Sources
Stoyanov, Veselin;Cardie, Claire;"></td>
	<td class="line x" title="1:190	Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 914, Sydney, July 2006." ></td>
	<td class="line x" title="2:190	c2006 Association for Computational Linguistics Toward Opinion Summarization: Linking the Sources Veselin Stoyanov and Claire Cardie Department of Computer Science Cornell University Ithaca, NY 14850, USA {ves,cardie}@cs.cornell.edu Abstract We target the problem of linking source mentions that belong to the same entity (source coreference resolution), which is needed for creating opinion summaries." ></td>
	<td class="line x" title="3:190	In this paper we describe how source coreference resolution can be transformed into standard noun phrase coreference resolution, apply a state-of-the-art coreference resolution approach to the transformed data, and evaluate on an available corpus of manually annotated opinions." ></td>
	<td class="line x" title="4:190	1 Introduction Sentiment analysis is concerned with the extraction and representation of attitudes, evaluations, opinions, and sentiment from text." ></td>
	<td class="line x" title="5:190	The area of sentiment analysis has been the subject of much recent research interest driven by two primary motivations." ></td>
	<td class="line x" title="6:190	First, there is a desire to provide applications that can extract, represent, and allow the exploration of opinions in the commercial, government, and political domains." ></td>
	<td class="line x" title="7:190	Second, effective sentiment analysis might be used to enhance and improve existing NLP applications such as information extraction, question answering, summarization, and clustering (e.g. Riloff et al.(2005), Stoyanov et al.(2005))." ></td>
	<td class="line x" title="10:190	Several research efforts (e.g. Riloff and Wiebe (2003), Bethard et al.(2004), Wilson et al.(2004), Yu and Hatzivassiloglou (2003), Wiebe and Riloff (2005)) have shown that sentiment information can be extracted at the sentence, clause, or individualopinionexpressionlevel(fine-grainedopinion information)." ></td>
	<td class="line x" title="13:190	However, little has been done to develop methods for combining fine-grained opinion information to form a summary representation in which expressions of opinions from the same source/target1 are grouped together, multiple opinions from a source toward the same target are accumulated into an aggregated opinion, and cumulative statistics are computed for each source/target." ></td>
	<td class="line x" title="14:190	A simple opinion summary2 is shown in Figure 1." ></td>
	<td class="line x" title="15:190	Being able to create opinion summaries is important both for stand-alone applications of sentiment analysis as well as for the potentialusesofsentimentanalysisaspartofother NLP applications." ></td>
	<td class="line x" title="16:190	In this work we address the dearth of approaches for summarizing opinion information." ></td>
	<td class="line x" title="17:190	In particular, we focus on the problem of source coreference resolution, i.e. deciding which source mentions are associated with opinions that belong to the same real-world entity." ></td>
	<td class="line x" title="18:190	In the example from Figure 1 performing source coreference resolution amounts to determining that Stanishev, he, and he refer to the same real-world entities." ></td>
	<td class="line x" title="19:190	Given the associated opinion expressions and their polarity, this source coreference information is the critical knowledgeneededtoproducethesummaryofFigure 1 (although the two target mentions, Bulgaria and our country, would also need to be identified as coreferent)." ></td>
	<td class="line x" title="20:190	Our work is concerned with fine-grained expressions of opinions and assumes that a system can rely on the results of effective opinion and source extractors such as those described in Riloff and Wiebe (2003), Bethard et al.(2004), Wiebe andRiloff(2005)andChoietal.(2005)." ></td>
	<td class="line x" title="22:190	Presented with sources of opinions, we approach the problem of source coreference resolution as the closely 1We use source to denote an opinion holder and target to denote the entity toward which the opinion is directed." ></td>
	<td class="line x" title="23:190	2For simplicity, the example summary does not contain any source/target statistics or combination of multiple opinions from the same source to the same target." ></td>
	<td class="line x" title="24:190	9  [Target Delaying of Bulgarias accession to the EU] would be a serious mistake [Source Bulgarian Prime Minister Sergey Stanishev] said in an interview for the German daily Suddeutsche Zeitung." ></td>
	<td class="line x" title="25:190	[Target Our country] serves as a model and encourages countries from the region to follow despite the difficulties, [Source he] added." ></td>
	<td class="line x" title="26:190	[Target Bulgaria] is criticized by [Source the EU] because of slow reforms in the judiciary branch, the newspaper notes." ></td>
	<td class="line x" title="27:190	Stanishev was elected prime minister in 2005." ></td>
	<td class="line x" title="28:190	Since then, [Source he] has been a prominent supporter of [Target his countrys accession to the EU]." ></td>
	<td class="line x" title="29:190	Stanishev Accession EU Bulgaria Delaying +   + Figure 1: Example of text containing opinions (above) and a summary of the opinions (below)." ></td>
	<td class="line x" title="30:190	In the text, sources and targets of opinions are marked and opinion expressions are shown in italic." ></td>
	<td class="line x" title="31:190	In the summary graph, + stands for positive opinion and for negative." ></td>
	<td class="line x" title="32:190	related task of noun phrase coreference resolution." ></td>
	<td class="line x" title="33:190	However, source coreference resolution differsfromtraditional nounphrase(NP)coreference resolution in two important aspects discussed in Section4." ></td>
	<td class="line x" title="34:190	Nevertheless, asafirstattemptatsource coreference resolution, we employ a state-of-theart machine learning approach to NP coreference resolution developed by Ng and Cardie (2002)." ></td>
	<td class="line x" title="35:190	Using a corpus of manually annotated opinions, we perform an extensive evaluation and obtain strong initial results for the task of source coreference resolution." ></td>
	<td class="line x" title="36:190	2 Related Work Sentiment analysis has been a subject of much recent research." ></td>
	<td class="line x" title="37:190	Several efforts have attempted to automatically extract opinions, emotions, and sentiment from text." ></td>
	<td class="line oc" title="38:190	The problem of sentiment extraction at the document level (sentiment classification) has been tackled as a text categorization task in which the goal is to assign to a document eitherpositive(thumbsup)ornegative(thumbs down) polarity (e.g. Das and Chen (2001), Pang et al.(2002), Turney (2002), Dave et al.(2003), Pang and Lee (2004))." ></td>
	<td class="line x" title="41:190	In contrast, the problem of fine-grained opinion extraction has concentrated on recognizing opinions at the sentence, clause, or individual opinion expression level." ></td>
	<td class="line oc" title="42:190	Recent work has shown that systems can be trained to recognize opinions, their polarity, and their strength at a reasonable degree of accuracy (e.g. Dave et al.(2003), Riloff and Wiebe (2003), Bethard et al.(2004), Pang and Lee (2004), Wilson et al.(2004), Yu and Hatzivassiloglou (2003), Wiebe and Riloff (2005))." ></td>
	<td class="line x" title="46:190	Additionally, researchers have been able to effectively identify sources of opinions automatically (Bethard et al. , 2004; Choi et al. , 2005; Kim and Hovy, 2005)." ></td>
	<td class="line x" title="47:190	Finally, Liu et al.(2005) summarize automatically generated opinions about products and develop interface that allows the summaries to be vizualized." ></td>
	<td class="line x" title="49:190	Our work also draws on previous work in the area of coreference resolution, which is a relatively well studied NLP problem." ></td>
	<td class="line x" title="50:190	Coreference resolution is the problem of deciding what noun phrases in the text (i.e. mentions) refer to the same real-world entities (i.e. are coreferent)." ></td>
	<td class="line x" title="51:190	Generally, successful approaches have relied machine learning methods trained on a corpus of documents annotated with coreference information (such as the MUC and ACE corpora)." ></td>
	<td class="line x" title="52:190	Our approach to source coreference resolution is inspired by the state-of-the-art performance of the method of Ng and Cardie (2002)." ></td>
	<td class="line x" title="53:190	3 Data set We begin our discussion by describing the data set that we use for development and evaluation." ></td>
	<td class="line x" title="54:190	As noted previously, we desire methods that work with automatically identified opinions and sources." ></td>
	<td class="line x" title="55:190	However, for the purpose of developing and evaluating our approaches we rely on a corpus ofmanuallyannotatedopinionsandsources." ></td>
	<td class="line x" title="56:190	More precisely, we rely on the MPQA corpus (Wilson and Wiebe, 2003)3, which contains 535 manually annotated documents." ></td>
	<td class="line x" title="57:190	Full details about the corpus and the process of corpus creation can be found in Wilson and Wiebe (2003); full details of the opinion annotation scheme can be found in Wiebe et al.(2005)." ></td>
	<td class="line x" title="59:190	For the purposes of the discussion in this paper, the following three points suffice." ></td>
	<td class="line x" title="60:190	First, the corpus is suitable for the domains and genres that we target  all documents have occurred in the world press over an 11-month period, between June 2001 and May 2002." ></td>
	<td class="line x" title="61:190	Therefore, the 3The MPQA corpus is available at http://nrrc.mitre.org/NRRC/publications.htm." ></td>
	<td class="line x" title="62:190	10 corpus is suitable for the political and government domains as well as a substantial part of the commercial domain." ></td>
	<td class="line x" title="63:190	However, a fair portion of the commercial domain is concerned with opinion extraction from product reviews." ></td>
	<td class="line x" title="64:190	Work described in this paper does not target the genre of reviews, which appears to differ significantly from newspaper articles." ></td>
	<td class="line x" title="65:190	Second, all documents are manually annotated with phrase-level opinion information." ></td>
	<td class="line x" title="66:190	The annotation scheme of Wiebe et al.(2005) includes phrase level opinions, their sources, as well as other attributes, which are not utilized by our approach." ></td>
	<td class="line x" title="68:190	Additionally, the annotations contain information that allows coreference among source mentions to be recovered." ></td>
	<td class="line x" title="69:190	Finally, the MPQA corpus contains no coreference information for general NPs (which are not sources)." ></td>
	<td class="line x" title="70:190	This might present a problem for traditional coreference resolution approaches, as discussed throughout the paper." ></td>
	<td class="line x" title="71:190	4 Source Coreference Resolution In this Section we define the problem of source coreference resolution, describe its challenges, and provide an overview of our general approach." ></td>
	<td class="line x" title="72:190	We define source coreference resolution as the problem of determining which mentions of opinion sources refer to the same real-world entity." ></td>
	<td class="line x" title="73:190	Source coreference resolution differs from traditional supervised NP coreference resolution in two important aspects." ></td>
	<td class="line x" title="74:190	First, sources of opinions do not exactly correspond to the automatic extractors notion of noun phrases (NPs)." ></td>
	<td class="line x" title="75:190	Second, due mainly to the time-consuming nature of coreference annotation, NP coreference information is incomplete in our data set: NP mentions that are not sources of opinion are not annotated with coreference information (even when they are part of a chain that contains source NPs)4." ></td>
	<td class="line x" title="76:190	In this paper we address the former problem via a heuristic method for mapping sources to NPs and give statistics for the accuracy of the mapping process." ></td>
	<td class="line x" title="77:190	We then apply state-of-the-art coreference resolution methods to the NPs to which sources were 4This problem is illustrated in the example of Figure 1 The underlined Stanishev is coreferent with all of the Stanishev references marked as sources, but, because it is used in an objective sentence rather than as the source of an opinion, thereferencewouldbeomittedfromtheStanishevsource coreference chain." ></td>
	<td class="line x" title="78:190	Unfortunately, this proper noun might be critical in establishing coreference of the final source reference he with the other mentions of the source Stanishev." ></td>
	<td class="line x" title="79:190	Single Match Multiple Matches No Match Total 7811 3461 50 Exact 6242 1303 0 Table 1: Statistics for matching sources to noun phrases." ></td>
	<td class="line x" title="80:190	mapped (source noun phrases)." ></td>
	<td class="line x" title="81:190	The latter problem of developing methods that can work with incomplete supervisory information is addressed in a subsequent effort (Stoyanov and Cardie, 2006)." ></td>
	<td class="line x" title="82:190	Our general approach to source coreference resolution consists of the following steps: 1." ></td>
	<td class="line x" title="83:190	Preprocessing: We preprocess the corpus by running NLP components such as a tokenizer, sentence splitter, POS tagger, parser, and a base NP finder." ></td>
	<td class="line x" title="84:190	Subsequently, we augment the set of the base NPs found by the base NP finder with the help of a named entity finder." ></td>
	<td class="line x" title="85:190	The preprocessing is done following the NP coreference work by Ng and Cardie (2002)." ></td>
	<td class="line x" title="86:190	From the preprocessing step, we obtain an augmented set of NPs in the text." ></td>
	<td class="line x" title="87:190	2." ></td>
	<td class="line x" title="88:190	Source to noun phrase mapping: The problem of mapping (manually or automatically annotated) sources to NPs is not trivial." ></td>
	<td class="line x" title="89:190	We map sources to NPs using a set of heuristics." ></td>
	<td class="line x" title="90:190	3." ></td>
	<td class="line x" title="91:190	Coreference resolution: Finally, we restrict our attention to the source NPs identified in step 2." ></td>
	<td class="line x" title="92:190	We extract a feature vector for every pair of source NPs from the preprocessed corpus and perform NP coreference resolution." ></td>
	<td class="line x" title="93:190	The next two sections give the details of Steps 2 and 3, respectively." ></td>
	<td class="line x" title="94:190	We follow with the results of an evaluation of our approach in Section 7." ></td>
	<td class="line x" title="95:190	5 Mapping sources to noun phrases Thissectiondescribesourmethodforheuristically mapping sources to NPs." ></td>
	<td class="line x" title="96:190	In the context of source coreference resolution we consider a noun phrase to correspond to (or match) a source if the source and the NP cover the exact same span of text." ></td>
	<td class="line x" title="97:190	Unfortunately, the annotated sources did not always match exactly a single automatically extracted NP." ></td>
	<td class="line x" title="98:190	We discovered the following problems: 1." ></td>
	<td class="line x" title="99:190	Inexact span match." ></td>
	<td class="line x" title="100:190	We discovered that often (in 3777 out of the 11322 source mentions) there is no noun phrase whose span matches exactly the source although there are noun phrases that overlap the source." ></td>
	<td class="line x" title="101:190	In most cases this is due to the way spans of sources are marked in the data." ></td>
	<td class="line x" title="102:190	For instance, in some cases determiners are not included in the source span (e.g. Venezuelan people vs. the Venezuelan people)." ></td>
	<td class="line x" title="103:190	In other cases, differences are due to mistakes by the NP extractor (e.g. Muslims rulers was not recognized, while Muslims and rulers were recognized)." ></td>
	<td class="line x" title="104:190	Yet in other cases, manually marked sources do not match the definition of a noun phrase." ></td>
	<td class="line x" title="105:190	This case is described in more detail next." ></td>
	<td class="line x" title="106:190	11 Measure Overall Method and Instance B3 MUC Positive Identification Actual Pos." ></td>
	<td class="line x" title="107:190	Identification rank parameters selection score Prec." ></td>
	<td class="line x" title="108:190	Recall F1 Prec." ></td>
	<td class="line x" title="109:190	Recall F1 B3 1 svm C10 0.01 none 81.8 71.7 80.2 43.7 56.6 57.5 62.9 60.2 400 5 ripper asc L2 soon2 80.7 72.2 74.5 45.2 56.3 55.1 62.1 58.4 Training MUC Score 1 svm C10 0.01 soon1 77.3 74.2 67.4 51.7 58.5 37.8 70.9 49.3 Documents 4 ripper acs L1.5 soon2 78.4 73.6 68.3 49.0 57.0 40.0 69.9 50.9 Positive 1 svm C10 0.05 soon1 72.7 73.9 60.0 57.2 58.6 37.8 71.0 49.3 identification 4 ripper acs L1.5 soon1 78.9 73.6 68.8 48.9 57.2 40.0 69.9 50.9 Actual pos." ></td>
	<td class="line x" title="110:190	1 svm C10 0.01 none 81.8 71.7 80.2 43.7 56.6 57.5 62.9 60.2 identification 2 ripper asc L4 soon2 73.9 69.9 81.1 40.2 53.9 69.8 52.5 60.0 B3 1 ripper acs L4 none 81.8 67.8 91.4 32.7 48.2 72.0 52.5 60.6 9 svm C10 0.01 none 81.4 70.3 81.6 40.8 54.4 58.4 61.6 59.9 200 MUC Score 1 svm C1 0.1 soon1 74.8 73.8 63.2 55.2 58.9 32.1 74.4 44.9 Training 5 ripper acs L1 soon1 77.9 0.732 71.4 46.5 56.3 37.7 69.7 48.9 Documents Positive 1 svm C1 0.1 soon1 74.8 73.8 63.2 55.2 58.9 32.1 74.4 44.9 identification 4 ripper acs L1 soon1 75.3 72.4 69.1 48.0 56.7 33.3 72.3 45.6 Actual pos." ></td>
	<td class="line x" title="111:190	1 ripper acs L4 none 81.8 67.8 91.4 32.7 48.2 72.0 52.5 60.6 identification 10 svm C10 0.01 none 81.4 70.3 81.6 40.8 54.4 58.4 61.6 59.9 Table 2: Performance of the best runs." ></td>
	<td class="line x" title="112:190	For SVMs,  stands for RBF kernel with the shown  parameter." ></td>
	<td class="line x" title="113:190	2." ></td>
	<td class="line x" title="114:190	Multiple NP match." ></td>
	<td class="line x" title="115:190	For 3461 of the 11322 source mentions more than one NP overlaps the source." ></td>
	<td class="line x" title="116:190	In roughly a quarter of these cases the multiple match is due to the presence of nested NPs (introduced by the NP augmentation process introduced in Section 3)." ></td>
	<td class="line x" title="117:190	In other cases the multiple match is caused by source annotations that spanned multiple NPs or included more than only NPs inside its span." ></td>
	<td class="line x" title="118:190	There are three general classes of such sources." ></td>
	<td class="line x" title="119:190	First, some of the marked sourcesareappositivessuchasthecountrysnewpresident, Eduardo Duhalde." ></td>
	<td class="line x" title="120:190	Second, some sources containanNPfollowedbyanattachedprepositionalphrase suchasLatinAmericanleadersatasummitmeetingin Costa Rica." ></td>
	<td class="line x" title="121:190	Third, some sources are conjunctions of NPs such as Britain, Canada and Australia." ></td>
	<td class="line x" title="122:190	Treatment of the latter is still a controversial problem in the context of coreference resolution as it is unclear whether conjunctions represent entities that are distinct fromtheconjuncts." ></td>
	<td class="line x" title="123:190	Forthepurposeofourcurrentwork we do not attempt to address conjunctions." ></td>
	<td class="line x" title="124:190	3." ></td>
	<td class="line x" title="125:190	No matching NP." ></td>
	<td class="line x" title="126:190	Finally, for 50 of the 11322 sources there are no overlapping NPs." ></td>
	<td class="line x" title="127:190	Half of those (25 to be exact) included marking of the word who such as in the sentence Carmona named new ministers, including two military officers who rebelled against Chavez." ></td>
	<td class="line x" title="128:190	From the other 25, 19 included markings of non-NPs including question words, qualifiers, and adjectives such as many, which, and domestically." ></td>
	<td class="line x" title="129:190	The remaining six are rare NPs such as lash and taskforce that are mistakenly not recognized by the NP extractor." ></td>
	<td class="line x" title="130:190	Counts for the different types of matches of sources to NPs are shown in Table 1." ></td>
	<td class="line x" title="131:190	We determine the match in the problematic cases using a set of heuristics: 1." ></td>
	<td class="line x" title="132:190	If a source matches any NP exactly in span, match that source to the NP; do this even if multiple NPs overlap the source  we are dealing with nested NPs. 2." ></td>
	<td class="line x" title="133:190	If no NP matches matches exactly in span then:  If a single NP overlaps the source, then map the sourcetothatNP.Mostlikelywearedealingwith differently marked spans." ></td>
	<td class="line x" title="134:190	 If multiple NPs overlap the source, determine whether the set of overlapping NPs include any non-nested NPs." ></td>
	<td class="line x" title="135:190	If all overlapping NPs are nested with each other, select the NP that is closer in span to the source  we are still dealing with differently marked spans, but now we also have nested NPs." ></td>
	<td class="line x" title="136:190	If there is more than one set of nested NPs, then most likely the source spans more than a single NP." ></td>
	<td class="line x" title="137:190	In this case we select the outermost of the last set of nested NPs before any preposition in the span." ></td>
	<td class="line x" title="138:190	We prefer: the outermost NP because longer NPs contain more information; thelastNPbecauseitislikelytobethehead NP of a phrase (also handles the case of explanation followed by a proper noun); NPs before preposition, because a preposition signals an explanatory prepositional phrase." ></td>
	<td class="line x" title="139:190	3." ></td>
	<td class="line x" title="140:190	If no NP overlaps the source, select the last NP before the source." ></td>
	<td class="line x" title="141:190	In half of the cases we are dealing with the word who, which typically refers to the last preceding NP." ></td>
	<td class="line x" title="142:190	6 Source coreference resolution as coreference resolution Once we isolate the source NPs, we apply coreference resolution using the standard combination of classification and single-link clustering (e.g. Soon et al.(2001) and Ng and Cardie (2002))." ></td>
	<td class="line x" title="144:190	We compute a vector of 57 features for every pair of source noun phrases from the preprocessed corpus." ></td>
	<td class="line x" title="145:190	We use the training set of pairwise instances to train a classifier to predict whether a source NP pair should be classified as positive (the NPs refer to the same entity) or negative (different entities)." ></td>
	<td class="line x" title="146:190	During testing, we use the trained classifier to predict whether a source NP pair is positive and single-link clustering to group together sources that belong to the same entity." ></td>
	<td class="line x" title="147:190	7 Evaluation For evaluation we randomly split the MPQA corpus into a training set consisting of 400 documents 12 and a test set consisting of the remaining 135 documents." ></td>
	<td class="line x" title="148:190	We use the same test set for all evaluations, although not all runs were trained on all 400 training documents as discussed below." ></td>
	<td class="line x" title="149:190	The purpose of our evaluation is to create a strong baseline utilizing the best settings for the NP coreference approach." ></td>
	<td class="line x" title="150:190	As such, we try the two reportedly best machine learning techniques for pairwise classification  RIPPER (for Repeated Incremental Pruning to Produce Error Reduction) (Cohen, 1995) and support vector machines (SVMs) in the SVMlight implementation (Joachims, 1998)." ></td>
	<td class="line x" title="151:190	Additionally, to exclude possible effects of parameter selection, we try many different parameter settings for the two classifiers." ></td>
	<td class="line x" title="152:190	For RIPPER we vary the order of classes and the positive/negative weight ratio." ></td>
	<td class="line x" title="153:190	For SVMs we vary C (themargintradeoff)andthetypeandparameter of the kernel." ></td>
	<td class="line x" title="154:190	In total, we use 24 different settings for RIPPER and 56 for SVMlight." ></td>
	<td class="line x" title="155:190	Additionally, Ng and Cardie reported better results when the training data distribution is balanced through instance selection." ></td>
	<td class="line x" title="156:190	For instance selection they adopt the method of Soon et al.(2001), which selects for each NP the pairs with the n preceding coreferent instances and all intervening non-coreferent pairs." ></td>
	<td class="line x" title="158:190	Following Ng and Cardie (2002), we perform instance selection with n = 1 (soon1 in the results) and n = 2 (soon2)." ></td>
	<td class="line x" title="159:190	With the three different instance selection algorithms (soon1,soon2, and none), the total number of settings is 72 for RIPPER and 168 for SVMa." ></td>
	<td class="line x" title="160:190	However, not all SVM runs completed in the time limit that we set  200 min, so we selected half of the training set (200 documents) at random and trained all classifiers on that set." ></td>
	<td class="line x" title="161:190	We made sure to run to completion on the full training set those SVM settings that produced the best results on the smaller training set." ></td>
	<td class="line x" title="162:190	Table 2 lists the results of the best performing runs." ></td>
	<td class="line x" title="163:190	The upper half of the table gives the results for the runs that were trained on 400 documents and the lower half contains the results for the 200-document training set." ></td>
	<td class="line x" title="164:190	We evaluated using the two widely used performance measures for coreference resolution  MUC score (Vilain et al. , 1995) and B3 (Bagga and Baldwin, 1998)." ></td>
	<td class="line x" title="165:190	In addition, we used performance metrics (precision, recall and F1) on the identification of the positive class." ></td>
	<td class="line x" title="166:190	We compute the latter in two different ways  either by using the pairwise decisions as the classifiers outputs them or by performing the clustering of the source NPs and then considering a pairwise decision to be positive if the two source NPs belong to the same cluster." ></td>
	<td class="line x" title="167:190	The second option (marked actual in Table 2) should be more representative of a good clustering, since coreference decisions are important only in the context of the clusters that they create." ></td>
	<td class="line x" title="168:190	Table 2 shows the performance of the best RIPPER and SVM runs for each of the four evaluation metrics." ></td>
	<td class="line x" title="169:190	The table also lists the rank for each run among the rest of the runs." ></td>
	<td class="line x" title="170:190	7.1 Discussion The absolute B3 and MUC scores for source coreference resolution are comparable to reported state-of-the-art results for NP coreference resolutions." ></td>
	<td class="line x" title="171:190	Results should be interpreted cautiously, however, due to the different characteristics of our data." ></td>
	<td class="line x" title="172:190	Our documents contained 35.34 source NPs per document on average, with coreference chains consisting of only 2.77 NPs on average." ></td>
	<td class="line x" title="173:190	The low average number of NPs per chain may be producing artificially high score for the B3 and MUC scores as the modest results on positive class identification indicate." ></td>
	<td class="line x" title="174:190	From the relative performance of our runs, we observe the following trends." ></td>
	<td class="line x" title="175:190	First, SVMs trained on the full training set outperform RIPPER trained on the same training set as well as the corresponding SVMs trained on the 200-document training set." ></td>
	<td class="line x" title="176:190	The RIPPER runs exhibit the opposite behavior  RIPPER outperforms SVMs on the 200document training set and RIPPER runs trained on the smaller data set exhibit better performance." ></td>
	<td class="line x" title="177:190	Overall, the single best performance is observed by RIPPER using the smaller training set." ></td>
	<td class="line x" title="178:190	Another interesting observation is that the B3 measure correlates well with good actual performance on positive class identification." ></td>
	<td class="line x" title="179:190	In contrast, good MUC performance is associated with runs that exhibit high recall on the positive class." ></td>
	<td class="line x" title="180:190	This confirms some theoretical concerns that MUC score does not reward algorithms that recognize well the absence of links." ></td>
	<td class="line x" title="181:190	In addition, the results confirm our conjecture that actual precision and recall are more indicative of the true performance of coreference algorithms." ></td>
	<td class="line x" title="182:190	13 8 Conclusions As a first step toward opinion summarization we targeted the problem of source coreference resolution." ></td>
	<td class="line x" title="183:190	We showed that the problem can be tackled effectively as noun coreference resolution." ></td>
	<td class="line x" title="184:190	Oneaspectof sourcecoreferenceresolutionthat we do not address is the use of unsupervised information." ></td>
	<td class="line x" title="185:190	The corpus contains many automatically identified non-source NPs, which can be used to benefit source coreference resolution in two ways." ></td>
	<td class="line x" title="186:190	First, a machine learning approach could use the unlabeleddatatoestimatetheoveralldistributions." ></td>
	<td class="line x" title="187:190	Second, some links between sources may be realized through a non-source NPs (see the example of figure 1)." ></td>
	<td class="line x" title="188:190	As a follow-up to the work described in this paper we developed a method that utilizes the unlabeled NPs in the corpus using a structured rule learner (Stoyanov and Cardie, 2006)." ></td>
	<td class="line x" title="189:190	Acknowledgements The authors would like to thank Vincent Ng and Art Munson for providing coreference resolution code, members of the Cornell NLP group (especially Yejin Choi and Art Munson) for many helpful discussions, and the anonymous reviewers for their insightful comments." ></td>
	<td class="line x" title="190:190	This work was supported by the Advanced Research and Development Activity (ARDA), by NSF Grants IIS-0535099 and IIS-0208028, by gifts from Google and the Xerox Foundation, and by an NSF Graduate Research Fellowship to the first author." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-0303
A System For Summarizing And Visualizing Arguments In Subjective Documents: Toward Supporting Decision Making
Fujii, Atsushi;Ishikawa, Tetsuya;"></td>
	<td class="line x" title="1:185	Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 1522, Sydney, July 2006." ></td>
	<td class="line x" title="2:185	c2006 Association for Computational Linguistics A System for Summarizing and Visualizing Arguments in Subjective Documents: Toward Supporting Decision Making Atsushi Fujii Graduate School of Library, Information and Media Studies University of Tsukuba 1-2 Kasuga, Tsukuba, 305-8550, Japan fujii@slis.tsukuba.ac.jp Tetsuya Ishikawa The Historiographical Institute The University of Tokyo 3-1 Hongo 7-chome, Bunkyo-ku Tokyo, 133-0033, Japan ishikawa@hi.u-tokyo.ac.jp Abstract On the World Wide Web, the volume of subjective information, such as opinions and reviews, has been increasing rapidly." ></td>
	<td class="line x" title="3:185	The trends and rules latent in a large set of subjective descriptions can potentially be useful for decision-making purposes." ></td>
	<td class="line x" title="4:185	In this paper, we propose a method for summarizing subjective descriptions, specifically opinions in Japanese." ></td>
	<td class="line x" title="5:185	We visualize the pro and con arguments for a target topic, such as Should Japan introduce the summertime system? Users can summarize the arguments about the topic in order to choose a more reasonable standpoint for decision making." ></td>
	<td class="line x" title="6:185	We evaluate our system, called OpinionReader, experimentally." ></td>
	<td class="line x" title="7:185	1 Introduction On the World Wide Web, users can easily disseminate information irrespective of their own specialty." ></td>
	<td class="line x" title="8:185	Thus, natural language information on the Web is not restricted to objective and authorized information, such as news stories and technical publications." ></td>
	<td class="line x" title="9:185	The volume of subjective information, such as opinions and reviews, has also been increasing rapidly." ></td>
	<td class="line x" title="10:185	Although a single subjective description by an anonymous author is not always reliable, the trends and rules latent in a large set of subjective descriptions can potentially be useful for decisionmaking purposes." ></td>
	<td class="line x" title="11:185	In one scenario, a user may read customer reviews before choosing a product." ></td>
	<td class="line x" title="12:185	In another scenario, a user may assess the pros and cons of a political issue before determining their own attitude on the issue." ></td>
	<td class="line x" title="13:185	The decision making in the above scenarios is performed according to the following processes: (1) collecting documents related to a specific topic from the Web; (2) extracting subjective descriptions from the documents; (3) classifying the subjective descriptions according to their polarity, such as positive/negative or pro/con; (4) organizing (e.g. , summarizing and/or visualizing) the classified descriptions so that users can view important points selectively; (5) making the decision." ></td>
	<td class="line x" title="14:185	Because it is expensive to perform all of the above processes manually, a number of automatic methods have been explored." ></td>
	<td class="line x" title="15:185	Specifically, a large number of methods have been proposed to facilitate processes (2) and (3)." ></td>
	<td class="line x" title="16:185	In this paper, we focus on process (4), and propose a method for summarizing subjective information, specifically opinions in Japanese." ></td>
	<td class="line x" title="17:185	Our method visualizes the pro and con arguments for a target topic, such as Should Japan introduce the summertime system? By process (4), users can summarize the arguments about the topic in order to choose a more reasonable standpoint on it." ></td>
	<td class="line x" title="18:185	Consequently, our system supports decision making by users." ></td>
	<td class="line x" title="19:185	However, process (5) is beyond the scope of this paper, and remains an intellectual activity for human beings." ></td>
	<td class="line x" title="20:185	We describe and demonstrate our prototype system, called OpinionReader." ></td>
	<td class="line x" title="21:185	We also evaluate the components of our system experimentally." ></td>
	<td class="line x" title="22:185	Section 2 surveys previous research on the processing of subjective information." ></td>
	<td class="line x" title="23:185	Section 3 provides an overview of OpinionReader, and Sec15 tion 4 describes the methodologies of its components." ></td>
	<td class="line x" title="24:185	Section 5 describes the experiments and discusses the results obtained." ></td>
	<td class="line x" title="25:185	2 Related Work For process (1) in Section 1, existing search engines can be used to search the Web for documents related to a specific topic." ></td>
	<td class="line x" title="26:185	However, not all retrieved documents include subjective descriptions for the topic." ></td>
	<td class="line x" title="27:185	A solution to this problem is to automatically identify diaries and blogs (Nanno et al. , 2004), which usually include opinionated subjective descriptions." ></td>
	<td class="line oc" title="28:185	For process (2), existing methods aim to distinguish between subjective and objective descriptions in texts (Kim and Hovy, 2004; Pang and Lee, 2004; Riloff and Wiebe, 2003)." ></td>
	<td class="line oc" title="29:185	For process (3), machine-learning methods are usually used to classify subjective descriptions into bipolar categories (Dave et al. , 2003; Beineke et al. , 2004; Hu and Liu, 2004; Pang and Lee, 2004) or multipoint scale categories (Kim and Hovy, 2004; Pang and Lee, 2005)." ></td>
	<td class="line x" title="30:185	For process (4), which is the subject of this paper, Ku et al.(2005) selected documents that include a large number of positive or negative sentences about a target topic, and used their headlines as a summary of the topic." ></td>
	<td class="line x" title="32:185	This is the application of an existing extraction-based summarization method to subjective descriptions." ></td>
	<td class="line x" title="33:185	Hu and Liu (2004) summarized customer reviews of a product such as a digital camera." ></td>
	<td class="line x" title="34:185	Their summarization method extracts nouns and noun phrases as features of the target product, (e.g. , picture for a digital camera), and lists positive and negative reviews on a feature-by-feature basis." ></td>
	<td class="line x" title="35:185	The extracted features are sorted according to the frequency with which each feature appears in the reviews." ></td>
	<td class="line x" title="36:185	This method allows users to browse the reviews in terms of important features of the target product." ></td>
	<td class="line x" title="37:185	Liu et al.(2005) enhanced the above method to allow users to compare different products within a specific category, on a feature-by-feature basis." ></td>
	<td class="line x" title="39:185	3 Overview of OpinionReader Figure 1 depicts the process flow in OpinionReader." ></td>
	<td class="line x" title="40:185	The input is a set of subjective descriptions for a specific topic, classified according to their polarity." ></td>
	<td class="line x" title="41:185	We assume that processes (1)(3) in Section 1 are completed, either manually or automatically, prior to the use of our system." ></td>
	<td class="line x" title="42:185	It is often the case that users post their opinions and state their standpoints, as exemplified by the websites used in our experiments (see Section 5)." ></td>
	<td class="line x" title="43:185	While our primarily target is a set of opinions for a debatable issue classified into pros and cons, a set of customer reviews for a product, classified as positive or negative, can also be submitted.extracting points at issue arranging points at issue ranking opinions opinions about a topic pros cons Figure 1: Process flow in OpinionReader." ></td>
	<td class="line x" title="44:185	Our purpose is to visualize the pro and con arguments about a target topic, so that a user can determine which standpoint is the more reasonable." ></td>
	<td class="line x" title="45:185	We extract points at issue from the opinions and arrange them in a two-dimensional space." ></td>
	<td class="line x" title="46:185	We also rank the opinions that include each point at issue according to their importance, so that a user can selectively read representative opinions on a point-by-point basis." ></td>
	<td class="line x" title="47:185	The output is presented via a graphical interface as shown in Figure 2, which is an example output for the topic privatization of hospitals by joint-stock companies." ></td>
	<td class="line x" title="48:185	The opinions used for this example are extracted from the website for BS debate1." ></td>
	<td class="line x" title="49:185	This interface is accessible via existing Web browsers." ></td>
	<td class="line x" title="50:185	In Figure 2, the x and y axes correspond to the polarity and importance respectively, and each oval denotes an extracted point at issue, such as information disclosure, health insurance, or medical corporation." ></td>
	<td class="line x" title="51:185	Users can easily see which points at issue are most important from each standpoint." ></td>
	<td class="line x" title="52:185	Points at issue that are important and closely related to one particular standpoint are usually the most useful in users decision making." ></td>
	<td class="line x" title="53:185	By clicking on an oval in Figure 2, users can read representative opinions corresponding to that 1http://www.nhk.or.jp/bsdebate/ 16 point at issue." ></td>
	<td class="line x" title="54:185	In Figure 3, two opinions that include information disclosure are presented." ></td>
	<td class="line x" title="55:185	The opinions on the right and left sides are selected from the pros and cons, respectively." ></td>
	<td class="line x" title="56:185	While the pros support information disclosure, the cons insist that they have not recognized its necessity." ></td>
	<td class="line x" title="57:185	As a result, users can browse the pro and con arguments about the topic in detail." ></td>
	<td class="line x" title="58:185	However, for some points at issue, only opinions from a single standpoint are presented, because the other side has no argument about that point." ></td>
	<td class="line x" title="59:185	Given the above functions, users can easily summarize the main points and how they are used in arguing about the topic in support of one standpoint or the other." ></td>
	<td class="line x" title="60:185	If subjective descriptions are classified into more than two categories with a single axis, we can incorporate these descriptions into our system by reclassifying them into just two categories." ></td>
	<td class="line x" title="61:185	Figure 4 is an example of summarizing reviews with a multipoint scale rating." ></td>
	<td class="line x" title="62:185	We used reviews with fivepoint star rating for the movie Star Wars: Episode III2." ></td>
	<td class="line x" title="63:185	We reclassified reviews with 13 stars as cons, and reviews with 45 stars as pros." ></td>
	<td class="line x" title="64:185	In Figure 4, the points at issue are typical words used in the movie reviews (e.g. story), the names of characters (e.g. Anakin, ObiWan, and Palpatine), concepts related to Star Wars (e.g. battle scene and Dark Side), and comparisons with other movies (e.g. , War of the Worlds)." ></td>
	<td class="line x" title="65:185	Existing methods for summarizing opinions (Hu and Liu, 2004; Liu et al. , 2005)." ></td>
	<td class="line x" title="66:185	extract the features of a product, which corresponds to the points at issue in our system, and arrange them along a single dimension representing the importance of features." ></td>
	<td class="line x" title="67:185	The reviews corresponding to each feature are not ranked." ></td>
	<td class="line x" title="68:185	However, in our system, features are arranged to show how the feature relates to each polarity." ></td>
	<td class="line x" title="69:185	The opinions addressing a feature are ranked according to their importance." ></td>
	<td class="line x" title="70:185	We target both opinions and reviews, as shown in Figures 2 and 4, respectively." ></td>
	<td class="line x" title="71:185	4 Methodology 4.1 Extracting Points at Issue In a preliminary investigation of political opinions on the Web, we identified that points at issue can be different language units: words, phrases, 2http://moviessearch.yahoo.co.jp/detail?ty=mv&id=321602 sentences, and combinations of sentences." ></td>
	<td class="line x" title="72:185	We currently target nouns, noun phrases, and verb phrases, whereas existing summarization methods (Hu and Liu, 2004; Liu et al. , 2005) extract only nouns and noun phrases." ></td>
	<td class="line x" title="73:185	Because Japanese sentences lack lexical segmentation, we first use ChaSen3 to perform a morphological analysis of each input sentence." ></td>
	<td class="line x" title="74:185	As a result, we can identify the words in the input and their parts of speech." ></td>
	<td class="line x" title="75:185	To extract nouns and noun phrases, we use handcrafted rules that rely on the word and part-ofspeech information." ></td>
	<td class="line x" title="76:185	We extract words and word sequences that match these rules." ></td>
	<td class="line x" title="77:185	To standardize among the different noun phrases that describe the same content, we paraphrase specific types of noun phrases." ></td>
	<td class="line x" title="78:185	To extract verb phrases, we analyze the syntactic dependency structure of each input sentence, by using CaboCha4." ></td>
	<td class="line x" title="79:185	We then use handcrafted rules to extract verb phrases comprising a noun and a verb from the dependency structure." ></td>
	<td class="line x" title="80:185	It is desirable that the case of a noun (i.e. , postpositional particles) and the modality of a verb (i.e. , auxiliaries) are maintained." ></td>
	<td class="line x" title="81:185	However, if we were to allow variations of case and modality, verb phrases related to almost the same meaning would be regarded as different points at issue and thus the output of our system would contain redundancy." ></td>
	<td class="line x" title="82:185	Therefore, for the sake of conciseness, we currently discard postpositional particles and auxiliaries in verb phrases." ></td>
	<td class="line x" title="83:185	4.2 Arranging Points at Issue In our system, the points at issue extracted as described in Section 4.1 are arranged in a twodimensional space, as shown in Figure 2." ></td>
	<td class="line x" title="84:185	The xaxis corresponds to the polarity of the points at issue, that is the degree to which a point is related to each standpoint." ></td>
	<td class="line x" title="85:185	The y-axis corresponds to the importance of the points at issue." ></td>
	<td class="line x" title="86:185	For a point at issue A, which can be a noun, noun phrase, or verb phrase, the x-coordinate, xA, is calculated by Equation (1): xA = P(pro|A)P(con|A) (1) P(S|A), in which S denotes either the pro or con standpoint, is the probability that an opinion randomly selected from a set of opinions addressing 3http://chasen.naist.jp/hiki/ChaSen/ 4http://cl.aist-nara.ac.jp/taku-ku/software/cabocha/ 17 JGCNVJKPUWTCPEG KPHQTOCVKQPFKUENQUWTG OGFKECNEQTRQTCVKQP RTQHKV EQPRTQ KORTQXGOGPV EQUOGVKEUWTIGT[ EWUVQOGTPGGFU OGFKECNVTGCVOGPVKPHQTOCVKQP Figure 2: Example of visualizing points at issue for privatization of hospitals by joint-stock companies." ></td>
	<td class="line x" title="87:185	1RKPKQPUQH2TQ1RKPKQPQH%QP Figure 3: Example of presenting representative opinions for information disclosure." ></td>
	<td class="line x" title="88:185	1DK9CP #PCMKP 2CNRCVKPGUVQT[ EQPRTQDCVVNGUEGPG &CTM5KFG 9CTQHVJG9QTNFU Figure 4: Example of summarizing reviews with multipoint scale rating for Star Wars: Episode III." ></td>
	<td class="line x" title="89:185	18 A supports S. We calculate P(S|A) as the number of opinions that are classified into S and that include A, divided by the number of opinions that include A. xA ranges from 1 to 1." ></td>
	<td class="line x" title="90:185	A is classified into one of the following three categories depending on the value of xA:  if A appears in the pros more frequently than in the cons, xA is a positive number,  if A appears in the pros and cons equally often, xA is zero,  if A appears in the cons more frequently than in the pros, xA is a negative number." ></td>
	<td class="line x" title="91:185	The calculation of the y-coordinate of A, yA depends on which of the above categories applies to A. If A appears in standpoint S more frequently than in its opposite, we define yA as the probability that a point at issue randomly selected from the opinions classified into S is A. We calculate yA as the frequency of A in the opinions classified into S, divided by the total frequencies of points at issue in the opinions classified into S. Thus, yA ranges from 0 to 1." ></td>
	<td class="line x" title="92:185	However, if A appears in the pros and cons equally often, we use the average of the values of yA for both standpoints." ></td>
	<td class="line x" title="93:185	General words, which are usually high frequency words, tend to have high values for yA." ></td>
	<td class="line x" title="94:185	Therefore, we discard the words whose yA is above a predefined threshold." ></td>
	<td class="line x" title="95:185	We empirically set the threshold at 0.02." ></td>
	<td class="line x" title="96:185	Table 1 shows example points at issue for the topic privatization of hospitals by joint-stock companies and their values of xA and yA." ></td>
	<td class="line x" title="97:185	In Table 1, points at issue, which have been translated into English, are classified into the three categories (i.e. , pro, neutral, and con) according to xA and are sorted according to yA in descending order, for each category." ></td>
	<td class="line x" title="98:185	In Table 1, improvement is the most important in the pro category, and medical corporation is the most important in the con category." ></td>
	<td class="line x" title="99:185	In the pro category, many people expect that the quality of medical treatment will be improved if jointstock companies make inroads into the medical industry." ></td>
	<td class="line x" title="100:185	However, in the con category, many people are concerned about the future of existing medical corporations." ></td>
	<td class="line x" title="101:185	Table 1: Examples of points at issue and their coordinates for privatization of hospitals by jointstock companies." ></td>
	<td class="line x" title="102:185	Point at issue xA yA improvement 0.33 9.2103 information disclosure 0.33 7.9103 health insurance 0.60 5.3103 customer needs 0.50 3.9103 cosmetic surgery 0.00 2.6103 medical corporation 0.69 4.4103 medical institution 0.64 3.6103 medical cost 0.60 3.2103 profit seeking 0.78 3.2103 4.3 Ranking Opinions Given a set of opinions from which a point at issue has been extracted, our purpose now is to rank the opinions in order of importance." ></td>
	<td class="line x" title="103:185	We assume that representative opinions contain many content words that occur frequently in the opinion set." ></td>
	<td class="line x" title="104:185	In our case, content words are nouns, verbs, and adjectives identified by morphological analysis." ></td>
	<td class="line x" title="105:185	We calculate the score of a content word w, s(w), as the frequency of w in the opinion set." ></td>
	<td class="line x" title="106:185	We calculate the importance of an opinion by the sum of s(w) for the words in the opinion." ></td>
	<td class="line x" title="107:185	However, we normalize the importance of the opinion by the number of words in the opinion because long opinions usually include many words." ></td>
	<td class="line x" title="108:185	5 Experiments 5.1 Method The effectiveness of our system should be evaluated from different perspectives." ></td>
	<td class="line x" title="109:185	First, the effectiveness of each component of our system should be evaluated." ></td>
	<td class="line x" title="110:185	Second, the effectiveness of the system as a whole should be evaluated." ></td>
	<td class="line x" title="111:185	In this second evaluation, the evaluation measure is the extent to which the decisions of users can be made correctly and efficiently." ></td>
	<td class="line x" title="112:185	As a first step in our research, in this paper we perform only the first evaluation and evaluate the effectiveness of the methods described in Section 4." ></td>
	<td class="line x" title="113:185	We used the following Japanese websites as the source of opinions, in which pros and cons are posted for specific topics." ></td>
	<td class="line x" title="114:185	(a) BS debate5 (b) ewoman6 5http://www.nhk.or.jp/bsdebate/ 6http://www.ewoman.co.jp/ 19 (c) Official website of the prime minister of Japan and his cabinet7 (d) Yomiuri online8 For evaluation purposes, we collected the pros and cons for five topics." ></td>
	<td class="line x" title="115:185	Table 2 shows the five topics, the number of opinions, and the sources." ></td>
	<td class="line x" title="116:185	For topic #4, we used the opinions collected from two sources to increase the number of opinions." ></td>
	<td class="line x" title="117:185	In Table 2, the background of topic #5 should perhaps be explained." ></td>
	<td class="line x" title="118:185	When using escalators, it is often customary for passengers to stand on one side (either left or right) to allow other passengers to walk past them." ></td>
	<td class="line x" title="119:185	However, some people insist that walking on escalators, which are moving stairs, is dangerous." ></td>
	<td class="line x" title="120:185	Graduate students, none of who was an author of this paper, served as assessors, and produced reference data." ></td>
	<td class="line x" title="121:185	The output of a method under evaluation was compared with the reference data." ></td>
	<td class="line x" title="122:185	For each topic, two assessors were assigned to enhance the degree of objectivity of the results." ></td>
	<td class="line x" title="123:185	Final results were obtained by averaging the results over the assessors and the topics." ></td>
	<td class="line x" title="124:185	5.2 Evaluation of Extracting Points at Issue For each topic used in the experiments, the assessors read the opinions from both standpoints and extracted the points at issue." ></td>
	<td class="line x" title="125:185	We defined the point at issue as the grounds for an argument." ></td>
	<td class="line x" title="126:185	We did not restrict the form of the points at issue." ></td>
	<td class="line x" title="127:185	Thus, the assessors were allowed to extract any continuous language units, such as words, phrases, sentences, and paragraphs, as points at issue." ></td>
	<td class="line x" title="128:185	Because our method is intended to extract points at issue exhaustively and accurately, we used recall and precision as evaluation measures for the extraction." ></td>
	<td class="line x" title="129:185	Recall is the ratio of the number of correct answers extracted automatically to the total number of correct answers." ></td>
	<td class="line x" title="130:185	Precision is the ratio of the number of correct answers extracted automatically to the total number of points at issue extracted automatically." ></td>
	<td class="line x" title="131:185	Table 3 shows the results for each topic, in which System denotes the number of points at issue extracted automatically." ></td>
	<td class="line x" title="132:185	In Table 3, C, R, and P denote the number of correct answers, recall, and precision, respectively, on an assessor-by-assessor basis." ></td>
	<td class="line x" title="133:185	7http://www.kantei.go.jp/ 8http://www.yomiuri.co.jp/komachi/forum/ Looking at Table 3, we see that the results can vary depending on the topic and the assessor." ></td>
	<td class="line x" title="134:185	However, recall and precision were approximately 50% and 4%, respectively, on average." ></td>
	<td class="line x" title="135:185	The ratio of agreement between assessors was low." ></td>
	<td class="line x" title="136:185	When we used the points at issue extracted by one assessor as correct answers and evaluated the effectiveness of the other assessor in the extraction, the recall and precision ranged from 10% to 20% depending on the topic." ></td>
	<td class="line x" title="137:185	To increase the ratio of agreement between assessors, the instruction for assessors needs to be revised for future work." ></td>
	<td class="line x" title="138:185	This was mainly because the viewpoint for a target topic and the language units to be extracted were different, depending on the assessor." ></td>
	<td class="line x" title="139:185	Because our automatic method extracted points at issue exhaustively, the recall was high and the precision was low, irrespective of the assessor." ></td>
	<td class="line x" title="140:185	The ratios of noun phrases (including nouns) and verb phrases to the number of manually extracted points at issue were 78.5% and 2.0%, respectively." ></td>
	<td class="line x" title="141:185	Although the ratio for verb phrases is relatively low, extracting both noun and verb phrases is meaningful." ></td>
	<td class="line x" title="142:185	The recalls of our method for noun phrases and verb phrases were 60.0% and 44.3%, respectively." ></td>
	<td class="line x" title="143:185	Errors were mainly due to noun phrases that were not modeled in our method, such as noun phrases that include a relative clause." ></td>
	<td class="line x" title="144:185	5.3 Evaluation of Arranging Points at Issue As explained in Section 4.2, in our system the points at issue are arranged in a two-dimensional space." ></td>
	<td class="line x" title="145:185	The x and y axes correspond to the polarity and the importance of points at issue, respectively." ></td>
	<td class="line x" title="146:185	Because it is difficult for the assessors to judge the correctness of coordinate values in the twodimensional space, we evaluated the effectiveness of arranging points at issue indirectly." ></td>
	<td class="line x" title="147:185	First, we evaluated the effectiveness of the calculation for the y-axis." ></td>
	<td class="line x" title="148:185	We sorted the points at issue, which were extracted automatically (see Section 5.2), according to their importance." ></td>
	<td class="line x" title="149:185	We evaluated the trade-off between recall and precision by varying the threshold of yA." ></td>
	<td class="line x" title="150:185	We discarded the points at issue whose yA is below the threshold." ></td>
	<td class="line x" title="151:185	Note that while this threshold was used to determine the lower bound of yA, the threshold explained in Section 4.2 (i.e. , 0.02) was used to determine the upper bound of yA and was used consistently irrespective of the lower bound threshold." ></td>
	<td class="line x" title="152:185	20 Table 2: Topics used for experiments." ></td>
	<td class="line x" title="153:185	#Opinions Topic ID Topic Pro Con Source #1 principle of result in private companies 57 29 (a) #2 privatization of hospitals by joint-stock companies 27 44 (a) #3 the summertime system in Japan 14 17 (b) #4 privatization of postal services 28 20 (b), (c) #5 one side walk on an escalator 29 42 (d) Table 3: Recall and precision of extracting points at issue (C: # of correct answers, R: recall (%), P: precision (%))." ></td>
	<td class="line x" title="154:185	Assessor A Assessor B Topic ID System C R P C R P #1 1968 194 58.2 5.7 101 44.6 2.3 #2 1864 66 50.0 1.8 194 60.8 6.3 #3 508 43 48.8 4.1 43 60.5 5.1 #4 949 77 64.9 5.3 96 36.5 3.7 #5 711 91 30.0 3.8 75 18.7 2.0 Table 4 shows the results, in which the precision was improved to 50% by increasing the threshold." ></td>
	<td class="line x" title="155:185	In Figure 2, users can change the threshold of importance by using the panel on the right side to control the number of points at issue presented in the interface." ></td>
	<td class="line x" title="156:185	As a result, users can choose appropriate points at issue precisely." ></td>
	<td class="line x" title="157:185	Second, we evaluated the effectiveness of the calculation for the x-axis." ></td>
	<td class="line x" title="158:185	We evaluated the effectiveness of our method in a binary classification." ></td>
	<td class="line x" title="159:185	For each point at issue extracted by an assessor, the assessor judged which of the two standpoints the point supports." ></td>
	<td class="line x" title="160:185	If a point at issue whose x-coordinate calculated by our method is positive (or negative), it was classified as pro (or con) automatically." ></td>
	<td class="line x" title="161:185	We did not use the points at issue whose x-coordinate was zero for evaluation purposes." ></td>
	<td class="line x" title="162:185	Table 5 shows the results." ></td>
	<td class="line x" title="163:185	While the number of target points at issue was different depending on the topic and the assessor, the difference in classification accuracy was marginal." ></td>
	<td class="line x" title="164:185	For each topic, we averaged the accuracy determined by each assessor and averaged the accuracies over the topic, which gave 95.6%." ></td>
	<td class="line x" title="165:185	Overall, our method performs the binary classification for points at issue with a high accuracy." ></td>
	<td class="line x" title="166:185	Errors were mainly due to opinions that included arguments for both standpoints." ></td>
	<td class="line x" title="167:185	For example, a person supporting a standpoint might suggest that he/she would support the other side under a specific condition." ></td>
	<td class="line x" title="168:185	Points at issue classified incorrectly had usually been extracted from such contradictory opinions." ></td>
	<td class="line x" title="169:185	5.4 Evaluation of Ranking Opinions To evaluate the effectiveness of our method in ranking opinions on a point-by-point basis, we used a method that sorts the opinions randomly as a control." ></td>
	<td class="line x" title="170:185	We compared the accuracy of our method and that of the control." ></td>
	<td class="line x" title="171:185	The accuracy is the ratio of the number of correct answers to the number of opinions presented by the method under evaluation." ></td>
	<td class="line x" title="172:185	For each point at issue extracted by an assessor, the assessor assigned the opinions to one of the following degrees:  A: the opinion argues about the point at issue and is represented,  B: the opinion argues about the point at issue but is not represented,  C: the opinion includes the point at issue but does not argue about it." ></td>
	<td class="line x" title="173:185	We varied the number of top opinions presented by changing the threshold for the rank of opinions." ></td>
	<td class="line x" title="174:185	Table 6 shows the results, in which N denotes the number of top opinions presented." ></td>
	<td class="line x" title="175:185	The column Answer refers to two cases: the case in which only the opinions assigned to A were regarded as correct answers, and the case in which the opinions assigned to A or B were regarded as correct answers." ></td>
	<td class="line x" title="176:185	In either case, our method outperformed the control in ranking accuracy." ></td>
	<td class="line x" title="177:185	Although the accuracy of our method for A opinions was low, the accuracy for A and B 21 Table 4: Trade-off between recall and precision in extracting points at issue." ></td>
	<td class="line x" title="178:185	Threshold 0 0.002 0.004 0.006 0.008 0.010 Recall 0.48 0.17 0.11 0.04 0.03 0.02 Precision 0.04 0.14 0.21 0.31 0.33 0.50 Table 5: Accuracy for classifying points at issue." ></td>
	<td class="line x" title="179:185	Assessor A Assessor B Topic ID #Points Accuracy (%) #Points Accuracy (%) #1 113 98.2 45 97.7 #2 33 91.0 118 94.1 #3 21 95.2 26 100 #4 50 92.0 35 91.4 #5 27 96.3 14 100 Table 6: Accuracy of ranking opinions." ></td>
	<td class="line x" title="180:185	Answer Method N = 1 N = 2 N = 3 A Random 19% 28% 19% Ours 38% 32% 23% A+B Random 81% 83% 75% Ours 87% 87% 83% opinions was high." ></td>
	<td class="line x" title="181:185	This suggests that our method is effective in distinguishing opinions that argue about a specific point and opinions that include the point but do not argue about it." ></td>
	<td class="line x" title="182:185	6 Conclusion In aiming to support users decision making, we have proposed a method for summarizing and visualizing the pro and con arguments about a topic." ></td>
	<td class="line x" title="183:185	Our prototype system, called OpinionReader, extracts points at issue from the opinions for both pro and con standpoints, arranges the points in a two-dimensional space, and allows users to read important opinions on a point-by-point basis." ></td>
	<td class="line x" title="184:185	We have experimentally evaluated the effectiveness of the components of our system." ></td>
	<td class="line x" title="185:185	Future work will include evaluating our system as a whole, and summarizing opinions that change over time." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-0304
User-Directed Sentiment Analysis: Visualizing The Affective Content Of Documents
Gregory, Michelle L.;Chinchor, Nancy A.;Whitney, Paul;Carter, Richard;Hetzler, Elizabeth;Turner, Alan;"></td>
	<td class="line x" title="1:202	Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 2330, Sydney, July 2006." ></td>
	<td class="line x" title="2:202	c2006 Association for Computational Linguistics User-directed Sentiment Analysis: Visualizing the Affective Content of Documents Michelle L. Gregory PNNL 902 Battelle Blvd. Richland Wa." ></td>
	<td class="line x" title="3:202	99354 michelle.gregory@pnl.gov Nancy Chinchor Consultant chinchor@earthlink.net Paul Whitney PNNL 902 Battelle Blvd. Richland Wa." ></td>
	<td class="line x" title="4:202	99354 paul.whitney@pnl.gov Richard Carter PNNL 902 Battelle Blvd. Richland Wa." ></td>
	<td class="line x" title="5:202	99354 richard.carter@pnl.gov Elizabeth Hetzler PNNL 902 Battelle Blvd. Richland Wa." ></td>
	<td class="line x" title="6:202	99354 beth.hetzler@pnl.gov Alan Turner PNNL 902 Battelle Blvd. Richland Wa." ></td>
	<td class="line x" title="7:202	99354 alan.turner@pnl.gov Abstract Recent advances in text analysis have led to finer-grained semantic analysis, including automatic sentiment analysis the task of measuring documents, or chunks of text, based on emotive categories, such as positive or negative." ></td>
	<td class="line x" title="8:202	However, considerably less progress has been made on efficient ways of exploring these measurements." ></td>
	<td class="line x" title="9:202	This paper discusses approaches for visualizing the affective content of documents and describes an interactive capability for exploring emotion in a large document collection." ></td>
	<td class="line x" title="10:202	1 Introduction Recent advances in text analysis have led to finer-grained semantic classification, which enables the automatic exploration of subtle areas of meaning." ></td>
	<td class="line x" title="11:202	One area that has received a lot of attention is automatic sentiment analysisthe task of classifying documents, or chunks of text, into emotive categories, such as positive or negative." ></td>
	<td class="line x" title="12:202	Sentiment analysis is generally used for tracking peoples attitudes about particular individuals or items." ></td>
	<td class="line x" title="13:202	For example, corporations use sentiment analysis to determine employee attitude and customer satisfaction with their products." ></td>
	<td class="line x" title="14:202	Given the plethora of data in digital form, the ability to accurately and efficiently measure the emotional content of documents is paramount." ></td>
	<td class="line oc" title="15:202	The focus of much of the automatic sentiment analysis research is on identifying the affect bearing words (words with emotional content) and on measurement approaches for sentiment (Turney & Littman, 2003; Pang & Lee, 2004; Wilson et al. , 2005)." ></td>
	<td class="line x" title="16:202	While identifying related content is an essential component for automatic sentiment analysis, it only provides half the story." ></td>
	<td class="line x" title="17:202	A useful area of research that has received much less attention is how these measurements might be presented to the users for exploration and added value." ></td>
	<td class="line x" title="18:202	This paper discusses approaches for visualizing affect and describes an interactive capability for exploring emotion in a large document collection." ></td>
	<td class="line x" title="19:202	In Section 2 we review current approaches to identifying the affective content of documents, as well as possible ways of visualizing it." ></td>
	<td class="line x" title="20:202	In Section 3 we describe our approach: The combination of a lexical scoring method to determine the affective content of documents and a visual analytics tool for visualizing it." ></td>
	<td class="line x" title="21:202	We provide a detailed case study in Section 4, followed by a discussion of possible evaluations." ></td>
	<td class="line x" title="22:202	2 Background At the AAAI Symposium on Attitude and Affect held at Stanford in 2004 (Qu et al. , 2005), it was clear that the lexical approach to capturing affect was adequate for broad brush results, but there were no production quality visualizations for presenting those results analytically." ></td>
	<td class="line x" title="23:202	Thus, we began exploring methods and tools for the visualization of lexically-based approaches for measuring affect which could facilitate the exploration of affect within a text collection." ></td>
	<td class="line x" title="24:202	2.1 Affect Extraction Following the general methodology of informational retrieval, there are two pre-dominant methods for identifying sentiment in text: Text classification models and lexical approaches." ></td>
	<td class="line x" title="25:202	Classification models require that a set of documents are hand labeled for affect, and a system is 23 trained on the feature vectors associated with labels." ></td>
	<td class="line o" title="26:202	New text is automatically classified by comparing the feature vectors with the training set." ></td>
	<td class="line oc" title="27:202	(Pang & Lee, 2004; Aue & Gamon, 2005)." ></td>
	<td class="line n" title="28:202	This methodology generally requires a large amount of training data and is domain dependent." ></td>
	<td class="line x" title="29:202	In the lexical approach, documents (Turney & Littman, 2003), phrases (see Wilson et al. , 2005), or sentences (Weibe & Riloff, 2005) are categorized as positive or negative, for example, based on the number of words in them that match a lexicon of sentiment bearing terms." ></td>
	<td class="line x" title="30:202	Major drawbacks of this approach include the contextual variability of sentiment (what is positive in one domain may not be in another) and incomplete coverage of the lexicon." ></td>
	<td class="line x" title="31:202	This latter drawback is often circumvented by employing bootstrapping (Turney & Littman, 2003; Weibe & Riloff, 2005) which allows one to create a larger lexicon from a small number of seed words, and potentially one specific to a particular domain." ></td>
	<td class="line x" title="32:202	2.2 Affect Visualization The uses of automatic sentiment classification are clear (public opinion, customer reviews, product analysis, etc.)." ></td>
	<td class="line x" title="33:202	However, there has not been a great deal of research into ways of visualizing affective content in ways that might aid data exploration and the analytic process." ></td>
	<td class="line x" title="34:202	There are a number of visualizations designed to reveal the emotional content of text, in particular, text that is thought to be highly emotively charged such as conversational transcripts and chat room transcripts (see DiMicco et al. , 2002; Tat & Carpendale, 2002; Lieberman et al. , 2004; Wang et al. , 2004, for example)." ></td>
	<td class="line x" title="35:202	Aside from using color and emoticons to explore individual documents (Liu et al. , 2003) or email inboxes (Mandic & Kerne, 2004), there are very few visualizations suitable for exploring the affect of large collections of text." ></td>
	<td class="line x" title="36:202	One exception is the work of Liu et al.(2005) in which they provide a visualization tool to compare reviews of products,using a bar graph metaphor." ></td>
	<td class="line x" title="38:202	Their system automatically extracts product features (with associated affect) through parsing and pos tagging, having to handle exceptional cases individually." ></td>
	<td class="line x" title="39:202	Their Opinion Observer is a powerful tool designed for a single purpose: comparing customer reviews." ></td>
	<td class="line x" title="40:202	In this paper, we introduce a visual analytic tool designed to explore the emotional content of large collections of open domain documents." ></td>
	<td class="line x" title="41:202	The tools described here work with document collections of all sizes, structures (html, xml,.doc, email, etc), sources (private collections, web, etc.), and types of document collections." ></td>
	<td class="line x" title="42:202	The visualization tool is a mature tool that supports the analytical process by enabling users to explore the thematic content of the collection, use natural language to query the collection, make groups, view documents by time, etc. The ability to explore the emotional content of an entire collection of documents not only enables users to compare the range of affect in documents within the collection, but also allows them to relate affect to other dimensions in the collection, such as major topics and themes, time, and source." ></td>
	<td class="line x" title="43:202	3 The Approach Our methodology combines a traditional lexical approach to scoring documents for affect with a mature visualization tool." ></td>
	<td class="line x" title="44:202	We first automatically identify affect by comparing each document against a lexicon of affect-bearing words and obtain an affect score for each document." ></td>
	<td class="line x" title="45:202	We provide a number of visual metaphors to represent the affect in the collection and a number of tools that can be used to interactively explore the affective content of the data." ></td>
	<td class="line x" title="46:202	3.1 Lexicon and Measurement We use a lexicon of affect-bearing words to identify the distribution of affect in the documents." ></td>
	<td class="line x" title="47:202	Our lexicon authoring system allows affectbearing terms, and their associated strengths, to be bulk loaded, declared manually, or algorithmically suggested." ></td>
	<td class="line x" title="48:202	In this paper, we use a lexicon derived from the General Inquirer (GI) and supplemented with lexical items derived from a semi-supervised bootstrapping task." ></td>
	<td class="line x" title="49:202	The GI tool is a computer-assisted approach for content analyses of textual data (Stone, 1977)." ></td>
	<td class="line x" title="50:202	It includes an extensive lexicon of over 11,000 handcoded word stems and 182 categories." ></td>
	<td class="line x" title="51:202	We used this lexicon, specifically the positive and negative axes, to create a larger lexicon by bootstrapping." ></td>
	<td class="line x" title="52:202	Lexical bootstrapping is a method used to help expand dictionaries of semantic categories (Riloff & Jones, 1999) in the context of a document set of interest." ></td>
	<td class="line x" title="53:202	The approach we have adopted begins with a lexicon of affect bearing words (POS and NEG) and a corpus." ></td>
	<td class="line x" title="54:202	Each document in the corpus receives an affect score by counting the number of words from the seed lexicon that occur in the document; a separate score is given for each affect axis." ></td>
	<td class="line x" title="55:202	Words in the corpus are scored for affect potential by comparing their distribution (using an L1 Distri24 bution metric) of occurrence over the set if documents to the distribution of affect bearing words." ></td>
	<td class="line x" title="56:202	Words that compare favorably with affect are hypothesized as affect bearing words." ></td>
	<td class="line x" title="57:202	Results are then manually culled to determine if in fact they should be included in the lexicon." ></td>
	<td class="line x" title="58:202	Here we report on results using a lexicon built from 8 affect categories, comprising 4 concept pairs:  Positive (n=2236)-Negative (n=2708)  Virtue (n=638)-Vice (n=649)  Pleasure (n=151)-Pain (n=220)  Power Cooperative (n=103)-Power Conflict (n=194) Each document in the collection is compared against all 8 affect categories and receives a score for each." ></td>
	<td class="line x" title="59:202	Scores are based on the summation of each affect axis in the document, normalized by the number of words in the documents." ></td>
	<td class="line x" title="60:202	This provides an overall proportion of positive words, for example, per document." ></td>
	<td class="line x" title="61:202	Scores can also be calculated as the summation of each axis, normalized by the total number of affect words for all axes." ></td>
	<td class="line x" title="62:202	This allows one to quickly estimate the balance of affect in the documents." ></td>
	<td class="line x" title="63:202	For example, using this measurement, one could see that a particular document contains as many positive as negative terms, or if it is heavily skewed towards one or the other." ></td>
	<td class="line x" title="64:202	While the results reported here are based on a predefined lexicon, our system does include a Lexicon Editor in which a user can manually enter their own lexicon or add strengths to lexical items." ></td>
	<td class="line x" title="65:202	Included in the editor is a Lexicon Bootstrapping Utility which the user can use to help create a specialized lexicon of their own." ></td>
	<td class="line x" title="66:202	This utility runs as described above." ></td>
	<td class="line x" title="67:202	Note that while we enable the capability of strength, we have not experimented with that variable here." ></td>
	<td class="line x" title="68:202	All words for all axes have a default strength of .5." ></td>
	<td class="line x" title="69:202	3.2 Visualization To visualize the affective content of a collection of documents, we combined a variety of visual metaphors with a tool designed for visual analytics of documents, IN-SPIRE." ></td>
	<td class="line x" title="70:202	3.2.1 The IN-SPIRE System IN-SPIRE (Hetzler and Turner, 2004) is a visual analytics tool designed to facilitate rapid understanding of large textual corpora." ></td>
	<td class="line x" title="71:202	IN-SPIRE generates a compiled document set from mathematical signatures for each document in a set." ></td>
	<td class="line x" title="72:202	Document signatures are clustered according to common themes to enable information exploration and visualizations." ></td>
	<td class="line x" title="73:202	Information is presented to the user using several visual metaphors to expose different facets of the textual data." ></td>
	<td class="line x" title="74:202	The central visual metaphor is a Galaxy view of the corpus that allows users to intuitively interact with thousands of documents, examining them by theme (see Figure 4, below)." ></td>
	<td class="line x" title="75:202	IN-SPIRE leverages the use of context vectors such as LSA (Deerwester et al. , 1990) for document clustering and projection." ></td>
	<td class="line x" title="76:202	Additional analytic tools allow exploration of temporal trends, thematic distribution by source or other metadata, and query relationships and overlaps." ></td>
	<td class="line x" title="77:202	IN-SPIRE was recently enhanced to support visual analysis of sentiment." ></td>
	<td class="line x" title="78:202	3.2.2 Visual Metaphors In selecting metaphors to represent the affect scores of documents, we started by identifying the kinds of questions that users would want to explore." ></td>
	<td class="line x" title="79:202	Consider, as a guiding example, a set of customer reviews for several commercial products (Hu & Liu, 2004)." ></td>
	<td class="line x" title="80:202	A user reviewing this data might be interested in a number of questions, such as:  What is the range of affect overall?" ></td>
	<td class="line x" title="81:202	 Which products are viewed most positively?" ></td>
	<td class="line x" title="82:202	Most negatively?" ></td>
	<td class="line x" title="83:202	 What is the range of affect for a particular product?" ></td>
	<td class="line x" title="84:202	 How does the affect in the reviews deviate from the norm?" ></td>
	<td class="line x" title="85:202	Which are more negative or positive than would be expected from the averages?" ></td>
	<td class="line x" title="86:202	 How does the feedback of one product compare to that of another?" ></td>
	<td class="line x" title="87:202	 Can we isolate the affect as it pertains to different features of the products?" ></td>
	<td class="line x" title="88:202	In selecting a base metaphor for affect, we wanted to be able to address these kinds of questions." ></td>
	<td class="line x" title="89:202	We wanted a metaphor that would support viewing affect axes individually as well as in pairs." ></td>
	<td class="line x" title="90:202	In addition to representing the most common axes, negative and positive, we wanted to provide more flexibility by incorporating the ability to portray multiple pairs because we suspect that additional axes will help the user explore nuances of emotion in the data." ></td>
	<td class="line x" title="91:202	For our current metaphor, we drew inspiration from the Rose plot used by Florence Nightingale (Wainer, 1997)." ></td>
	<td class="line x" title="92:202	This metaphor is appealing in that it is easily interpreted, that larger scores draw more 25 attention, and that measures are shown in consistent relative location, making it easier to compare measures across document groups." ></td>
	<td class="line x" title="93:202	We use a modified version of this metaphor in which each axis is represented individually but is also paired with its opposite to aid in direct comparisons." ></td>
	<td class="line x" title="94:202	To this end, we vary the spacing between the rose petals to reinforce the pairing." ></td>
	<td class="line x" title="95:202	We also use color; each pair has a common hue, with the more positive of the pair shown in a lighter shade and the more negative one in a darker shade (see Figure 1)." ></td>
	<td class="line x" title="96:202	To address how much the range of affect varies across a set of documents, we adapted the concept of a box plot to the rose petal." ></td>
	<td class="line x" title="97:202	For each axis, we show the median and quartile values as shown in the figure below." ></td>
	<td class="line x" title="98:202	The dark line indicates the median value and the color band portrays the quartiles." ></td>
	<td class="line x" title="99:202	In the plot in Figure 1, for example, the scores vary quite a bit." ></td>
	<td class="line x" title="100:202	Figure 1." ></td>
	<td class="line x" title="101:202	Rose plot adapted to show median and quartile variation." ></td>
	<td class="line x" title="102:202	Another variation we made on the base metaphor was to address a more subtle set of questions." ></td>
	<td class="line x" title="103:202	It may happen that the affect scores within a dataset are largely driven by document membership in particular groups." ></td>
	<td class="line x" title="104:202	For example, in our customer data, it may be that all documents about Product A are relatively positive while those about Product B are relatively negative." ></td>
	<td class="line x" title="105:202	A user wanting to understand customer complaints may have a subtle need." ></td>
	<td class="line x" title="106:202	It is not sufficient to just look at the most negative documents in the dataset, because none of the Product A documents may pass this threshold." ></td>
	<td class="line x" title="107:202	What may also help is to look at all documents that are more negative than one would expect, given the product they discuss." ></td>
	<td class="line x" title="108:202	To carry out this calculation, we use a statistical technique to calculate the Main (or expected) affect value for each group and the Residual (or deviation) affect value for each document with respect to its group (Scheffe, 1999)." ></td>
	<td class="line x" title="109:202	To convey the Residual concept, we needed a representation of deviation from expected value." ></td>
	<td class="line x" title="110:202	We also wanted this portrayal to be similar to the base metaphor." ></td>
	<td class="line x" title="111:202	We use a unit circle to portray the expected value and show deviation by drawing the appropriate rose petals either outside (larger than expected) or inside (smaller than expected) the unit circle, with the color amount showing the amount of deviation from expected." ></td>
	<td class="line x" title="112:202	In the figures below, the dotted circle represents expected value." ></td>
	<td class="line x" title="113:202	The glyph on the left shows a cluster with scores slightly higher than expected for Positive and for Cooperation affect." ></td>
	<td class="line x" title="114:202	The glyph on the right shows a cluster with scores slightly higher than expected for the Negative and Vice affect axes (Figure 2)." ></td>
	<td class="line x" title="115:202	Figure 2." ></td>
	<td class="line x" title="116:202	Rose plot adapted to show deviation from expected values." ></td>
	<td class="line x" title="117:202	3.2.3 Visual Interaction IN-SPIRE includes a variety of analytic tools that allow exploration of temporal trends, thematic distribution by source or other metadata, and query relationships and overlaps." ></td>
	<td class="line x" title="118:202	We have incorporated several interaction capabilities for further exploration of the affect." ></td>
	<td class="line x" title="119:202	Our analysis system allows users to group documents in numerous ways, such as by query results, by metadata (such as the product), by time frame, and by similarity in themes." ></td>
	<td class="line x" title="120:202	A user can select one or more of these groups and see a summary of affect and its variation in those groups." ></td>
	<td class="line x" title="121:202	In addition, the group members are clustered by their affect scores and glyphs of the residual, or variation from expected value, are shown for each of these sub-group clusters." ></td>
	<td class="line x" title="122:202	Below each rose we display a small histogram showing the number of documents represented by that glyph (see Figure 3)." ></td>
	<td class="line x" title="123:202	These allow comparison of affect to cluster or group size." ></td>
	<td class="line x" title="124:202	For example, we find that extreme affect scores are typically found in the smaller clusters, while larger ones often show more mid-range scores." ></td>
	<td class="line x" title="125:202	As the user selects document groups or clusters, we show the proportion of documents selected." ></td>
	<td class="line x" title="126:202	26 Figure 3." ></td>
	<td class="line x" title="127:202	Clusters by affect score, with one rose plot per cluster." ></td>
	<td class="line x" title="128:202	The interaction may also be driven from the affect size." ></td>
	<td class="line x" title="129:202	If a given clustering of affect characteristics is selected, the user can see the themes they represent, how they correlate to metadata, or the time distribution." ></td>
	<td class="line x" title="130:202	We illustrate how the affect visualization and interaction fit into a larger analysis with a brief case study." ></td>
	<td class="line x" title="131:202	4 Case study The IN-SPIRE visualization tool is a non-data specific tool, designed to explore large amounts of textual data for a variety of genres and document types (doc, xml, etc)." ></td>
	<td class="line x" title="132:202	Many users of the system have their own data sets they wish to explore (company internal documents), or data can be harvested directly from the web, either in a single web harvest, or dynamically." ></td>
	<td class="line x" title="133:202	The case study and dataset presented here is intended as an example only, it does not represent the full range of exploration capabilities of the affective content of datasets." ></td>
	<td class="line x" title="134:202	We explore a set of customer reviews, comprising a collection of Amazon reviews for five products (Hu & Liu, 2004)." ></td>
	<td class="line x" title="135:202	While a customer may not want to explore reviews for 5 different product types at once, the dataset is realistic in that a web harvest of one review site will contain reviews of multiple products." ></td>
	<td class="line x" title="136:202	This allows us to demonstrate how the tool enables users to focus on the data and comparisons that they are interested in exploring." ></td>
	<td class="line x" title="137:202	The 5 products in this dataset are:  Canon G3; digital camera  Nikon coolpix 4300; digital camera  Nokia 6610; cell phone  Creative Labs Nomad Jukebox Zen Xtra 40GB; mp3 player  Apex AD2600 Progressive-scan DVD player We begin by clustering the reviews, based on overall thematic content." ></td>
	<td class="line x" title="138:202	The labels are automatically generated and indicate some of the stronger theme combinations in this dataset." ></td>
	<td class="line x" title="139:202	These clusters are driven largely by product vocabulary." ></td>
	<td class="line x" title="140:202	The two cameras cluster in the lower portion; the Zen shows up in the upper right clusters, with the phone in the middle and the Apex DVD player in the upper left and upper middle." ></td>
	<td class="line x" title="141:202	In this image, the pink dots are the Apex DVD reviews." ></td>
	<td class="line x" title="142:202	Figure 4." ></td>
	<td class="line x" title="143:202	Thematic clustering of product review The affect measurements on these documents generate five clusters in our system, each of which is summarized with a rose plot showing affect variation." ></td>
	<td class="line x" title="144:202	This gives us information on the range and distribution of affect overall in this data." ></td>
	<td class="line x" title="145:202	We can select one of these plots, either to review the documents or to interact further." ></td>
	<td class="line x" title="146:202	Selection is indicated with a green border, as shown in the upper middle plot of Figure 5." ></td>
	<td class="line x" title="147:202	Figure 5." ></td>
	<td class="line x" title="148:202	Clusters by affect, with one cluster glyph selected." ></td>
	<td class="line x" title="149:202	The selected documents are relatively positive; they have higher scores in the Positive and Virtue axes and lower scores in the Negative axis." ></td>
	<td class="line x" title="150:202	We may want to see how the documents in this 27 affect cluster distribute over the five products." ></td>
	<td class="line x" title="151:202	This question is answered by the correlation tool, shown in Figure 6; the positive affect cluster contains more reviews on the Zen MP3 player than any of the other products." ></td>
	<td class="line x" title="152:202	Figure 6." ></td>
	<td class="line x" title="153:202	Products represented in one of the positive affect clusters." ></td>
	<td class="line x" title="154:202	Alternatively we could get a summary of affect per product." ></td>
	<td class="line x" title="155:202	Figure 7 shows the affect for the Apex DVD player and the Nokia cell phone." ></td>
	<td class="line x" title="156:202	While both are positive, the Apex has stronger negative ratings than the Nokia." ></td>
	<td class="line x" title="157:202	Figure 7." ></td>
	<td class="line x" title="158:202	Comparison of Affect Scores of Nokia to Apex More detail is apparent by looking at the clusters within one or more groups and examining the deviations." ></td>
	<td class="line x" title="159:202	Figure 8 shows the sub-clusters within the Apex group." ></td>
	<td class="line x" title="160:202	We include the summary for the group as a whole (directly beneath the Apex label), and then show the four sub-clusters by illustrating how they deviate from expected value." ></td>
	<td class="line x" title="161:202	We see that two of these tend to be more positive than expected and two are more negative than expected." ></td>
	<td class="line x" title="162:202	Figure 8." ></td>
	<td class="line x" title="163:202	Summary of Apex products with subclusters showing deviations." ></td>
	<td class="line x" title="164:202	Figure 9." ></td>
	<td class="line x" title="165:202	Thematic distribution of reviews for one product (Apex)." ></td>
	<td class="line x" title="166:202	Looking at the thematic distribution among the Apex documents shows topics that dominate its reviews (Figure 9)." ></td>
	<td class="line x" title="167:202	We can examine the affect across these various clusters." ></td>
	<td class="line x" title="168:202	Figure 10 shows the comparison of the service cluster to the dvd player picture cluster." ></td>
	<td class="line x" title="169:202	This graphic demonstrates that documents with service as a main theme tend to be much more negative, while documents with picture as a main theme are much more positive." ></td>
	<td class="line x" title="170:202	28 Figure 10." ></td>
	<td class="line x" title="171:202	Affect summary and variation for service cluster and picture cluster." ></td>
	<td class="line x" title="172:202	The visualization tool includes a document viewer so that any selection of documents can be reviewed." ></td>
	<td class="line x" title="173:202	For example, a user may be interested in why the service documents tend to be negative, in which case they can review the original reviews." ></td>
	<td class="line x" title="174:202	The doc viewer, shown in Figure 11, can be used at any stage in the process with any number of documents selected." ></td>
	<td class="line x" title="175:202	Individual documents can be viewed by clicking on a document title in the upper portion of the doc viewer." ></td>
	<td class="line x" title="176:202	Figure 11: The Doc Viewer." ></td>
	<td class="line x" title="177:202	In this case study, we have illustrated the usefulness of visualizing the emotional content of a document collection." ></td>
	<td class="line x" title="178:202	Using the tools presented here, we can summarize the dataset by saying that in general, the customer reviews are positive (Figure 5), but reviews for some products are more positive than others (Figures 6 and 7)." ></td>
	<td class="line x" title="179:202	In addition to the general content of the reviews, we can narrow our focus to the features contained in the reviews." ></td>
	<td class="line x" title="180:202	We saw that while reviews for Apex are generally positive (Figure 8), reviews about Apex service tend to be much more negative than reviews about Apex picture (Figure 10)." ></td>
	<td class="line x" title="181:202	5 Evaluation IN-SPIRE is a document visualization tool that is designed to explore the thematic content of a large collection of documents." ></td>
	<td class="line x" title="182:202	In this paper, we have described the added functionality of exploring affect as one of the possible dimensions." ></td>
	<td class="line x" title="183:202	As an exploratory system, it is difficult to define appropriate evaluation metric." ></td>
	<td class="line x" title="184:202	Because the goal of our system is not to discretely bin the documents into affect categories, traditional metrics such as precision are not applicable." ></td>
	<td class="line x" title="185:202	However, to get a sense of the coverage of our lexicon, we did compare our measurements to the hand annotations provided for the customer review dataset." ></td>
	<td class="line x" title="186:202	The dataset had hand scores (-3-3) for each feature contained in each review." ></td>
	<td class="line x" title="187:202	We summed these scores to discretely bin them into positive (>0) or negative (<0)." ></td>
	<td class="line x" title="188:202	We did this both at the feature level and the review level (by looking at the cumulative score for all the features in the review)." ></td>
	<td class="line x" title="189:202	We compared these categorizations to the scores output by our measurement tool." ></td>
	<td class="line x" title="190:202	If a document had a higher proportion of positive words than negative, we classified it as positive, and negative if it had a higher proportion of negative words." ></td>
	<td class="line x" title="191:202	Using a chi-square, we found that the categorizations from our system were related with the hand annotations for both the whole reviews (chi-square=33.02, df=4, p<0.0001) and the individual features (chisquare=150.6, df=4, p<0.0001), with actual agreement around 71% for both datasets." ></td>
	<td class="line x" title="192:202	While this number is not in itself impressive, recall that our lexicon was built independently of the data for which is was applied." ></td>
	<td class="line x" title="193:202	W also expect some agreement to be lost by conflating all scores into discrete bins, we expect that if we compared the numeric values of the hand annotations and our scores, we would have stronger correlations." ></td>
	<td class="line x" title="194:202	These scores only provide an indication that the lexicon we used correlates with the hand annotations for the same data." ></td>
	<td class="line x" title="195:202	As an exploratory system, however, a better evaluation metric would be a user study in which we get feedback on the usefulness of this capability in accomplishing a variety of analytical tasks." ></td>
	<td class="line x" title="196:202	IN-SPIRE is currently deployed in a number of settings, both commercial and government." ></td>
	<td class="line x" title="197:202	The added capabilities for interactively exploring affect have recently been deployed." ></td>
	<td class="line x" title="198:202	We plan to conduct a variety of user evaluations in-situ that focus on its utility in a number of different tasks." ></td>
	<td class="line x" title="199:202	Results of these studies will help steer the further development of this methodology." ></td>
	<td class="line x" title="200:202	29 6 Conclusion We have developed a measurement and visualization approach to affect that we expect to be useful in the context of the IN-SPIRE text analysis toolkit." ></td>
	<td class="line x" title="201:202	Our innovations include the flexibility of the lexicons used, the measurement options, the bootstrapping method and utility for lexicon development, and the visualization of affect using rose plots and interactive exploration in the context of an established text analysis toolkit." ></td>
	<td class="line x" title="202:202	While the case study presented here was conducted in English, all tools described are language independent and we have begun exploring and creating lexicons of affect bearing words in multiple languages." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-1639
Get Out The Vote: Determining Support Or Opposition From Congressional Floor-Debate Transcripts
Thomas, Matt;Pang, Bo;Lee, Lillian;"></td>
	<td class="line x" title="1:176	Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 327335, Sydney, July 2006." ></td>
	<td class="line x" title="2:176	c2006 Association for Computational Linguistics Get out the vote: Determining support or opposition from Congressional floor-debate transcripts Matt Thomas, Bo Pang, and Lillian Lee Department of Computer Science, Cornell University Ithaca, NY 14853-7501 mattthomas84@gmail.com, pabo@cs.cornell.edu, llee@cs.cornell.edu Abstract We investigate whether one can determine from the transcripts of U.S. Congressional floor debates whether the speeches represent support of or opposition to proposed legislation." ></td>
	<td class="line x" title="3:176	To address this problem, we exploit the fact that these speeches occur as part of a discussion; this allows us to use sources of information regarding relationships between discourse segments, such as whether a given utterance indicates agreement with the opinion expressed by another." ></td>
	<td class="line x" title="4:176	We find that the incorporation of such information yields substantial improvements over classifying speeches in isolation." ></td>
	<td class="line x" title="5:176	1 Introduction One ought to recognize that the present political chaos is connected with the decay of language, and that one can probably bring about some improvement by starting at the verbal end." ></td>
	<td class="line x" title="6:176	 Orwell, Politics and the English language We have entered an era where very large amounts of politically oriented text are now available online." ></td>
	<td class="line x" title="7:176	This includes both official documents, such as the full text of laws and the proceedings of legislative bodies, and unofficial documents, such as postings on weblogs (blogs) devoted to politics." ></td>
	<td class="line x" title="8:176	In some sense, the availability of such data is simply a manifestation of a general trend of everybody putting their records on the Internet.1 The 1It is worth pointing out that the United States Library of Congress was an extremely early adopter of Web technology: the THOMAS database (http://thomas.loc.gov) of congresonline accessibility of politically oriented texts in particular, however, is a phenomenon that some have gone so far as to say will have a potentially society-changing effect." ></td>
	<td class="line x" title="9:176	In the United States, for example, governmental bodies are providing and soliciting political documents via the Internet, with lofty goals in mind: electronic rulemaking (eRulemaking) initiatives involving the electronic collection, distribution, synthesis, and analysis of public commentary in the regulatory rulemaking process, may [alter] the citizen-government relationship (Shulman and Schlosberg, 2002)." ></td>
	<td class="line x" title="10:176	Additionally, much media attention has been focused recently on the potential impact that Internet sites may have on politics2, or at least on political journalism3." ></td>
	<td class="line x" title="11:176	Regardless of whether one views such claims as clear-sighted prophecy or mere hype, it is obviously important to help people understand and analyze politically oriented text, given the importance of enabling informed participation in the political process." ></td>
	<td class="line x" title="12:176	Evaluative and persuasive documents, such as a politicians speech regarding a bill or a bloggers commentary on a legislative proposal, form a particularly interesting type of politically oriented text." ></td>
	<td class="line x" title="13:176	People are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion, given the dense nature of legislative language and the fact that (U.S)." ></td>
	<td class="line x" title="14:176	bills often reach several hundred pages in length (Smith et al. , 2005)." ></td>
	<td class="line x" title="15:176	Moreover, political opinions are exsional bills and related data was launched in January 1995, when Mosaic was not quite two years old and Altavista did not yet exist." ></td>
	<td class="line x" title="16:176	2E.g., Internet injects sweeping change into U.S. politics, Adam Nagourney, The New York Times, April 2, 2006." ></td>
	<td class="line x" title="17:176	3E.g., The End of News?, Michael Massing, The New York Review of Books, December 1, 2005." ></td>
	<td class="line x" title="18:176	327 plicitly solicited in the eRulemaking scenario." ></td>
	<td class="line x" title="19:176	In the analysis of evaluative language, it is fundamentally necessary to determine whether the author/speaker supports or disapproves of the topic of discussion." ></td>
	<td class="line x" title="20:176	In this paper, we investigate the following specific instantiation of this problem: we seek to determine from the transcripts of U.S. Congressional floor debates whether each speech (continuous single-speaker segment of text) represents support for or opposition to a proposed piece of legislation." ></td>
	<td class="line x" title="21:176	Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records." ></td>
	<td class="line x" title="22:176	Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography)." ></td>
	<td class="line x" title="23:176	In particular, since we treat each individual speech within a debate as a single document, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al. , 2002; Turney, 2002; Dave et al. , 2003)." ></td>
	<td class="line x" title="24:176	Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently." ></td>
	<td class="line x" title="25:176	A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006)." ></td>
	<td class="line x" title="26:176	Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions." ></td>
	<td class="line x" title="27:176	For example, we may find textual4 evidence of a high likelihood of agreement be4Because we are most interested in techniques applicable across domains, we restrict consideration to NLP aspects of the problem, ignoring external problem-specific information." ></td>
	<td class="line x" title="28:176	For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via the methods we propose), we did not use party-affiliation data." ></td>
	<td class="line x" title="29:176	Indeed, in other settings (e.g. , a movie-discussion listserv) one may not be able to determine the participants political leanings, and such information may not lead to significantly improved results even if it were available." ></td>
	<td class="line x" title="30:176	tween two speakers, such as explicit assertions (I second that!) or quotation of messages in emails or postings (see Mullen and Malouf (2006) but cf.Agrawal et al.(2003))." ></td>
	<td class="line x" title="33:176	Agreement evidence can be a powerful aid in our classification task: for example, we can easily categorize a complicated (or overly terse) document if we find within it indications of agreement with a clearly positive text." ></td>
	<td class="line x" title="34:176	Obviously, incorporating agreement information provides additional benefit only when the input documents are relatively difficult to classify individually." ></td>
	<td class="line x" title="35:176	Intuition suggests that this is true of the data with which we experiment, for several reasons." ></td>
	<td class="line x" title="36:176	First, U.S. congressional debates contain very rich language and cover an extremely wide variety of topics, ranging from flag burning to international policy to the federal budget." ></td>
	<td class="line x" title="37:176	Debates are also subject to digressions, some fairly natural and others less so (e.g. , Why are we discussing this bill when the plight of my constituents regarding this other issue is being ignored?) Second, an important characteristic of persuasive language is that speakers may spend more time presenting evidence in support of their positions (or attacking the evidence presented by others) than directly stating their attitudes." ></td>
	<td class="line x" title="38:176	An extreme example will illustrate the problems involved." ></td>
	<td class="line x" title="39:176	Consider a speech that describes the U.S. flag as deeply inspirational, and thus contains only positive language." ></td>
	<td class="line x" title="40:176	If the bill under discussion is a proposed flag-burning ban, then the speech is supportive; but if the bill under discussion is aimed at rescinding an existing flag-burning ban, the speech may represent opposition to the legislation." ></td>
	<td class="line x" title="41:176	Given the current state of the art in sentiment analysis, it is doubtful that one could determine the (probably topic-specific) relationship between presented evidence and speaker opinion." ></td>
	<td class="line x" title="42:176	Qualitative summary of results The above difficulties underscore the importance of enhancing standard classification techniques with new information sources that promise to improve accuracy, such as inter-document relationships between the documents to be labeled." ></td>
	<td class="line x" title="43:176	In this paper, we demonstrate that the incorporation of agreement modeling can provide substantial improvements over the application of support vector machines (SVMs) in isolation, which represents the state of the art in the individual classification of documents." ></td>
	<td class="line x" title="44:176	The enhanced accuracies are obtained via a fairly primitive automatically-acquired agreement detector 328 total train test development speech segments 3857 2740 860 257 debates 53 38 10 5 average number of speech segments per debate 72.8 72.1 86.0 51.4 average number of speakers per debate 32.1 30.9 41.1 22.6 Table 1: Corpus statistics." ></td>
	<td class="line x" title="45:176	and a conceptually simple method for integrating isolated-document and agreement-based information." ></td>
	<td class="line x" title="46:176	We thus view our results as demonstrating the potentially large benefits of exploiting sentiment-related discourse-segment relationships in sentiment-analysis tasks." ></td>
	<td class="line x" title="47:176	2 Corpus This section outlines the main steps of the process by which we created our corpus (download site: www.cs.cornell.edu/home/llee/data/convote.html)." ></td>
	<td class="line x" title="48:176	GovTrack (http://govtrack.us) is an independent website run by Joshua Tauberer that collects publicly available data on the legislative and fundraising activities of U.S. congresspeople." ></td>
	<td class="line x" title="49:176	Due to its extensive cross-referencing and collating of information, it was nominated for a 2006 Webby award." ></td>
	<td class="line x" title="50:176	A crucial characteristic of GovTrack from our point of view is that the information is provided in a very convenient format; for instance, the floor-debate transcripts are broken into separate HTML files according to the subject of the debate, so we can trivially derive long sequences of speeches guaranteed to cover the same topic." ></td>
	<td class="line x" title="51:176	We extracted from GovTrack all available transcripts of U.S. floor debates in the House of Representatives for the year 2005 (3268 pages of transcripts in total), together with voting records for all roll-call votes during that year." ></td>
	<td class="line x" title="52:176	We concentrated on debates regarding controversial bills (ones in which the losing side generated at least 20% of the speeches) because these debates should presumably exhibit more interesting discourse structure." ></td>
	<td class="line x" title="53:176	Each debate consists of a series of speech segments, where each segment is a sequence of uninterrupted utterances by a single speaker." ></td>
	<td class="line x" title="54:176	Since speech segments represent natural discourse units, we treat them as the basic unit to be classified." ></td>
	<td class="line x" title="55:176	Each speech segment was labeled by the vote (yea or nay) cast for the proposed bill by the person who uttered the speech segment." ></td>
	<td class="line x" title="56:176	We automatically discarded those speech segments belonging to a class of formulaic, generally one-sentence utterances focused on the yielding of time on the house floor (for example, Madam Speaker, I am pleased to yield 5 minutes to the gentleman from Massachusetts), as such speech segments are clearly off-topic." ></td>
	<td class="line x" title="57:176	We also removed speech segments containing the term amendment, since we found during initial inspection that these speeches generally reflect a speakers opinion on an amendment, and this opinion may differ from the speakers opinion on the underlying bill under discussion." ></td>
	<td class="line x" title="58:176	We randomly split the data into training, test, and development (parameter-tuning) sets representing roughly 70%, 20%, and 10% of our data, respectively (see Table 1)." ></td>
	<td class="line x" title="59:176	The speech segments remained grouped by debate, with 38 debates assigned to the training set, 10 to the test set, and 5 to the development set; we require that the speech segments from an individual debate all appear in the same set because our goal is to examine classification of speech segments in the context of the surrounding discussion." ></td>
	<td class="line x" title="60:176	3 Method The support/oppose classification problem can be approached through the use of standard classifiers such as support vector machines (SVMs), which consider each text unit in isolation." ></td>
	<td class="line x" title="61:176	As discussed in Section 1, however, the conversational nature of our data implies the existence of various relationships that can be exploited to improve cumulative classification accuracy for speech segments belonging to the same debate." ></td>
	<td class="line x" title="62:176	Our classification framework, directly inspired by Blum and Chawla (2001), integrates both perspectives, optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label." ></td>
	<td class="line x" title="63:176	In this section, we discuss the specific classification framework that we adopt and the set of mechanisms that we propose for modeling specific types of relationships." ></td>
	<td class="line x" title="64:176	329 3.1 Classification framework Let s1,s2,,sn be the sequence of speech segments within a given debate, and let Y and N stand for the yea and nay class, respectively." ></td>
	<td class="line x" title="65:176	Assume we have a non-negative function ind(s,C) indicating the degree of preference that an individual-document classifier, such as an SVM, has for placing speech-segment s in class C. Also, assume that some pairs of speech segments have weighted links between them, where the non-negative strength (weight) str(lscript) for a link lscript indicates the degree to which it is preferable that the linked speech segments receive the same label." ></td>
	<td class="line x" title="66:176	Then, any class assignment c = c(s1),c(s2),,c(sn) can be assigned a cost summationdisplay s ind(s,c(s))+ summationdisplay s,sprime:c(s)negationslash=c(sprime) summationdisplay lscript betweens,sprime str(lscript), where c(s) is the opposite class from c(s)." ></td>
	<td class="line x" title="67:176	A minimum-cost assignment thus represents an optimum way to classify the speech segments so that each one tends not to be put into the class that the individual-document classifier disprefers, but at the same time, highly associated speech segments tend not to be put in different classes." ></td>
	<td class="line oc" title="68:176	As has been previously observed and exploited in the NLP literature (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Barzilay and Lapata, 2005), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs." ></td>
	<td class="line x" title="69:176	In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision." ></td>
	<td class="line x" title="70:176	3.2 Classifying speech segments in isolation In our experiments, we employed the well-known classifier SVMlight to obtain individual-document classification scores, treating Y as the positive class and using plain unigrams as features.5 Following standard practice in sentiment analysis (Pang et al. , 2002), the input to SVMlight consisted of normalized presence-of-feature (rather than frequency-of-feature) vectors." ></td>
	<td class="line x" title="71:176	The ind value 5SVMlight is available at svmlight.joachims.org." ></td>
	<td class="line x" title="72:176	Default parameters were used, although experimentation with different parameter settings is an important direction for future work (Daelemans and Hoste, 2002; Munson et al. , 2005)." ></td>
	<td class="line x" title="73:176	for each speech segment s was based on the signed distance d(s) from the vector representing s to the trained SVM decision plane: ind(s,Y) def=    1 d(s) > 2s;parenleftBig 1+ d(s)2s parenrightBig /2 |d(s)| 2s; 0 d(s) < 2s where s is the standard deviation of d(s) over all speech segments s in the debate in question, and ind(s,N) def= 1 ind(s,Y)." ></td>
	<td class="line x" title="74:176	We now turn to the more interesting problem of representing the preferences that speech segments may have for being assigned to the same class." ></td>
	<td class="line x" title="75:176	3.3 Relationships between speech segments A wide range of relationships between text segments can be modeled as positive-strength links." ></td>
	<td class="line x" title="76:176	Here we discuss two types of constraints that are considered in this work." ></td>
	<td class="line x" title="77:176	Same-speaker constraints: In Congressional debates and in general social-discourse contexts, a single speaker may make a number of comments regarding a topic." ></td>
	<td class="line x" title="78:176	It is reasonable to expect that in many settings, the participants in a discussion may be convinced to change their opinions midway through a debate." ></td>
	<td class="line x" title="79:176	Hence, in the general case we wish to be able to express soft preferences for all of an authors statements to receive the same label, where the strengths of such constraints could, for instance, vary according to the time elapsed between the statements." ></td>
	<td class="line x" title="80:176	Weighted links are an appropriate means to express such variation." ></td>
	<td class="line x" title="81:176	However, if we assume that most speakers do not change their positions in the course of a discussion, we can conclude that all comments made by the same speaker must receive the same label." ></td>
	<td class="line x" title="82:176	This assumption holds by fiat for the ground-truth labels in our dataset because these labels were derived from the single vote cast by the speaker on the bill being discussed.6 We can implement this assumption via links whose weights are essentially infinite." ></td>
	<td class="line x" title="83:176	Although one can also implement this assumption via concatenation of same-speaker speech segments (see Section 4.3), we view the fact that our graph-based framework incorporates 6We are attempting to determine whether a speech segment represents support or not." ></td>
	<td class="line x" title="84:176	This differs from the problem of determining what the speakers actual opinion is, a problem that, as an anonymous reviewer put it, is complicated by grandstanding, backroom deals, or, more innocently, plain change of mind (I voted for it before I voted against it)." ></td>
	<td class="line x" title="85:176	330 both hard and soft constraints in a principled fashion as an advantage of our approach." ></td>
	<td class="line x" title="86:176	Different-speaker agreements In House discourse, it is common for one speaker to make reference to another in the context of an agreement or disagreement over the topic of discussion." ></td>
	<td class="line x" title="87:176	The systematic identification of instances of agreement can, as we have discussed, be a powerful tool for the development of intelligently selected weights for links between speech segments." ></td>
	<td class="line x" title="88:176	The problem of agreement identification can be decomposed into two sub-problems: identifying references and their targets, and deciding whether each reference represents an instance of agreement." ></td>
	<td class="line x" title="89:176	In our case, the first task is straightforward because we focused solely on by-name references.7 Hence, we will now concentrate on the second, more interesting task." ></td>
	<td class="line x" title="90:176	We approach the problem of classifying references by representing each reference with a wordpresence vector derived from a window of text surrounding the reference.8 In the training set, we classify each reference connecting two speakers with a positive or negative label depending on whether the two voted the same way on the bill under discussion9." ></td>
	<td class="line x" title="91:176	These labels are then used to train an SVM classifier, the output of which is subsequently used to create weights on agreement links in the test set as follows." ></td>
	<td class="line x" title="92:176	Let d(r) denote the distance from the vector representing reference r to the agreement-detector SVMs decision plane, and let r be the standard deviation of d(r) over all references in the debate in question." ></td>
	<td class="line x" title="93:176	We then define the strength agr of the agreement link corresponding to the reference as: agr(r) def=    0 d(r) < agr; d(r)/4r agr  d(r)  4r;  d(r) > 4r. The free parameter  specifies the relative impor7One subtlety is that for the purposes of mining agreement cues (but not for evaluating overall support/oppose classification accuracy), we temporarily re-inserted into our dataset previously filtered speech segments containing the term yield, since the yielding of time on the House floor typically indicates agreement even though the yield statements contain little relevant text on their own." ></td>
	<td class="line x" title="94:176	8We found good development-set performance using the 30 tokens before, 20 tokens after, and the name itself." ></td>
	<td class="line x" title="95:176	9Since we are concerned with references that potentially represent relationships between speech segments, we ignore references for which the target of the reference did not speak in the debate in which the reference was made." ></td>
	<td class="line x" title="96:176	Agreement classifier (referenceagreement?) Devel." ></td>
	<td class="line x" title="97:176	set Test set majority baseline 81.51 80.26 Train: no amdmts; agr = 0 84.25 81.07 Train: with amdmts; agr = 0 86.99 80.10 Table 2: Agreement-classifier accuracy, in percent." ></td>
	<td class="line x" title="98:176	Amdmts=speech segments containing the word amendment." ></td>
	<td class="line x" title="99:176	Recall that boldface indicates results for development-set-optimal settings." ></td>
	<td class="line x" title="100:176	tance of the agr scores." ></td>
	<td class="line x" title="101:176	The threshold agr controls the precision of the agreement links, in that values of agr greater than zero mean that greater confidence is required before an agreement link can be added.10 4 Evaluation This section presents experiments testing the utility of using speech-segment relationships, evaluating against a number of baselines." ></td>
	<td class="line x" title="102:176	All reported results use values for the free parameter  derived via tuning on the development set." ></td>
	<td class="line x" title="103:176	In the tables, boldface indicates the developmentand test-set results for the development-set-optimal parameter settings, as one would make algorithmic choices based on development-set performance." ></td>
	<td class="line x" title="104:176	4.1 Preliminaries: Reference classification Recall that to gather inter-speaker agreement information, the strategy employed in this paper is to classify by-name references to other speakers as to whether they indicate agreement or not." ></td>
	<td class="line x" title="105:176	To train our agreement classifier, we experimented with undoing the deletion of amendmentrelated speech segments in the training set." ></td>
	<td class="line x" title="106:176	Note that such speech segments were never included in the development or test set, since, as discussed in Section 2, their labels are probably noisy; however, including them in the training set allows the classifier to examine more instances even though some of them are labeled incorrectly." ></td>
	<td class="line x" title="107:176	As Table 2 shows, using more, if noisy, data yields better agreement-classification results on the development set, and so we use that policy in all subsequent experiments.11 10Our implementation puts a link between just one arbitrary pair of speech segments among all those uttered by a given pair of apparently agreeing speakers." ></td>
	<td class="line x" title="108:176	The infiniteweight same-speaker links propagate the agreement information to all other such pairs." ></td>
	<td class="line x" title="109:176	11Unfortunately, this policy leads to inferior test-set agree331 Agreement classifier Precision (in percent): Devel." ></td>
	<td class="line x" title="110:176	set Test set agr = 0 86.23 82.55 agr =  89.41 88.47 Table 3: Agreement-classifier precision." ></td>
	<td class="line x" title="111:176	An important observation is that precision may be more important than accuracy in deciding which agreement links to add: false positives with respect to agreement can cause speech segments to be incorrectly assigned the same label, whereas false negatives mean only that agreement-based information about other speech segments is not employed." ></td>
	<td class="line x" title="112:176	As described above, we can raise agreement precision by increasing the threshold agr, which specifies the required confidence for the addition of an agreement link." ></td>
	<td class="line x" title="113:176	Indeed, Table 3 shows that we can improve agreement precision by setting agr to the (positive) mean agreement score  assigned by the SVM agreement-classifier over all references in the given debate12." ></td>
	<td class="line x" title="114:176	However, this comes at the cost of greatly reducing agreement accuracy (development: 64.38%; test: 66.18%) due to lowered recall levels." ></td>
	<td class="line x" title="115:176	Whether or not better speech-segment classification is ultimately achieved is discussed in the next sections." ></td>
	<td class="line x" title="116:176	4.2 Segment-based speech-segment classification Baselines The first two data rows of Table 4 depict baseline performance results." ></td>
	<td class="line x" title="117:176	The #(support)  #(oppos) baseline is meant to explore whether the speech-segment classification task can be reduced to simple lexical checks." ></td>
	<td class="line x" title="118:176	Specifically, this method uses the signed difference between the number of words containing the stem support and the number of words containing the stem oppos (returning the majority class if the difference is 0)." ></td>
	<td class="line x" title="119:176	No better than 62.67% testset accuracy is obtained by either baseline." ></td>
	<td class="line x" title="120:176	Using relationship information Applying an SVM to classify each speech segment in isolation leads to clear improvements over the two baseline methods, as demonstrated in Table 4." ></td>
	<td class="line x" title="121:176	When we impose the constraint that all speech segments uttered by the same speaker receive the same label via same-speaker links, both test-set and ment classification." ></td>
	<td class="line x" title="122:176	Section 4.5 contains further discussion." ></td>
	<td class="line x" title="123:176	12We elected not to explicitly tune the value of agr in order to minimize the number of free parameters to deal with." ></td>
	<td class="line x" title="124:176	Support/oppose classifer (speech segmentyea?) Devel." ></td>
	<td class="line x" title="125:176	set Test set majority baseline 54.09 58.37 #(support)#(oppos) 59.14 62.67 SVM [speech segment] 70.04 66.05 SVM + same-speaker links 79.77 67.21 SVM + same-speaker links + agreement links, agr = 0 89.11 70.81 + agreement links, agr =  87.94 71.16 Table 4: Segment-based speech-segment classification accuracy, in percent." ></td>
	<td class="line x" title="126:176	Support/oppose classifer (speech segmentyea?) Devel." ></td>
	<td class="line x" title="127:176	set Test set SVM [speaker] 71.60 70.00 SVM + agreement links  with agr = 0 88.72 71.28 with agr =  84.44 76.05 Table 5: Speaker-based speech-segment classification accuracy, in percent." ></td>
	<td class="line x" title="128:176	Here, the initial SVM is run on the concatenation of all of a given speakers speech segments, but the results are computed over speech segments (not speakers), so that they can be compared to those in Table 4." ></td>
	<td class="line x" title="129:176	development-set accuracy increase even more, in the latter case quite substantially so." ></td>
	<td class="line x" title="130:176	The last two lines of Table 4 show that the best results are obtained by incorporating agreement information as well." ></td>
	<td class="line x" title="131:176	The highest test-set result, 71.16%, is obtained by using a high-precision threshold to determine which agreement links to add." ></td>
	<td class="line x" title="132:176	While the development-set results would induce us to utilize the standard threshold value of 0, which is sub-optimal on the test set, the agr = 0 agreement-link policy still achieves noticeable improvement over not using agreement links (test set: 70.81% vs. 67.21%)." ></td>
	<td class="line x" title="133:176	4.3 Speaker-based speech-segment classification We use speech segments as the unit of classification because they represent natural discourse units." ></td>
	<td class="line x" title="134:176	As a consequence, we are able to exploit relationships at the speech-segment level." ></td>
	<td class="line x" title="135:176	However, it is interesting to consider whether we really need to consider relationships specifically between speech segments themselves, or whether it suffices to simply consider relationships between the speakers 332 of the speech segments." ></td>
	<td class="line x" title="136:176	In particular, as an alternative to using same-speaker links, we tried a speaker-based approach wherein the way we determine the initial individual-document classification score for each speech segment uttered by a person p in a given debate is to run an SVM on the concatenation of all of ps speech segments within that debate." ></td>
	<td class="line x" title="137:176	(We also ensure that agreement-link information is propagated from speech-segment to speaker pairs.)" ></td>
	<td class="line x" title="138:176	How does the use of same-speaker links compare to the concatenation of each speakers speech segments?" ></td>
	<td class="line x" title="139:176	Tables 4 and 5 show that, not surprisingly, the SVM individual-document classifier works better on the concatenated speech segments than on the speech segments in isolation." ></td>
	<td class="line x" title="140:176	However, the effect on overall classification accuracy is less clear: the development set favors samespeaker links over concatenation, while the test set does not." ></td>
	<td class="line x" title="141:176	But we stress that the most important observation we can make from Table 5 is that once again, the addition of agreement information leads to substantial improvements in accuracy." ></td>
	<td class="line x" title="142:176	4.4 Hard agreement constraints Recall that in in our experiments, we created finite-weight agreement links, so that speech segments appearing in pairs flagged by our (imperfect) agreement detector can potentially receive different labels." ></td>
	<td class="line x" title="143:176	We also experimented with forcing such speech segments to receive the same label, either through infinite-weight agreement links or through a speech-segment concatenation strategy similar to that described in the previous subsection." ></td>
	<td class="line x" title="144:176	Both strategies resulted in clear degradation in performance on both the development and test sets, a finding that validates our encoding of agreement information as soft preferences." ></td>
	<td class="line x" title="145:176	4.5 On the development/test set split We have seen several cases in which the method that performs best on the development set does not yield the best test-set performance." ></td>
	<td class="line x" title="146:176	However, we felt that it would be illegitimate to change the train/development/test sets in a post hoc fashion, that is, after seeing the experimental results." ></td>
	<td class="line x" title="147:176	Moreover, and crucially, it is very clear that using agreement information, encoded as preferences within our graph-based approach rather than as hard constraints, yields substantial improvements on both the development and test set; this, we believe, is our most important finding." ></td>
	<td class="line x" title="148:176	5 Related work Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al. , 2005; Cardie et al. , 2006; Kwon et al. , 2006)." ></td>
	<td class="line x" title="149:176	There has also been work focused upon determining the political leaning (e.g. , liberal vs. conservative) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the unlabeled texts) (Laver et al. , 2003; Efron, 2004; Mullen and Malouf, 2006)." ></td>
	<td class="line x" title="150:176	An exception is Grefenstette et al.(2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site." ></td>
	<td class="line x" title="152:176	Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006)." ></td>
	<td class="line x" title="153:176	Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement." ></td>
	<td class="line x" title="154:176	More sophisticated approaches have been proposed (Hillard et al. , 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al. , 2004)." ></td>
	<td class="line x" title="155:176	Also relevant is work on the general problems of dialog-act tagging (Stolcke et al. , 2000), citation analysis (Lehnert et al. , 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002)." ></td>
	<td class="line x" title="156:176	We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work." ></td>
	<td class="line x" title="157:176	Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g. , between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations." ></td>
	<td class="line x" title="158:176	Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al. , 2003)." ></td>
	<td class="line x" title="159:176	Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al.(2002), Kondor and Lafferty (2002), and Joachims (2003)." ></td>
	<td class="line x" title="161:176	Zhu (2005) maintains a survey of this area." ></td>
	<td class="line x" title="162:176	Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al. , 2001; Getoor et al. , 2002; Taskar et al. , 2002; Taskar et al. , 2003; Taskar et al. , 2004; McCallum and Wellner, 2004)." ></td>
	<td class="line x" title="163:176	It would be interesting to investigate the application of such methods to our problem." ></td>
	<td class="line x" title="164:176	However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve." ></td>
	<td class="line x" title="165:176	6 Conclusion and future work In this study, we focused on very general types of cross-document classification preferences, utilizing constraints based only on speaker identity and on direct textual references between statements." ></td>
	<td class="line x" title="166:176	We showed that the integration of even very limited information regarding inter-document relationships can significantly increase the accuracy of support/opposition classification." ></td>
	<td class="line x" title="167:176	The simple constraints modeled in our study, however, represent just a small portion of the rich network of relationships that connect statements and speakers across the political universe and in the wider realm of opinionated social discourse." ></td>
	<td class="line x" title="168:176	One intriguing possibility is to take advantage of (readily identifiable) information regarding interpersonal relationships, making use of speaker/author affiliations, positions within a social hierarchy, and so on." ></td>
	<td class="line x" title="169:176	Or, we could even attempt to model relationships between topics or concepts, in a kind of extension of collaborative filtering." ></td>
	<td class="line x" title="170:176	For example, perhaps we could infer that two speakers sharing a common opinion on evolutionary biologist Richard Dawkins (a.k.a. Darwins rottweiler) will be likely to agree in a debate centered on Intelligent Design." ></td>
	<td class="line x" title="171:176	While such functionality is well beyond the scope of our current study, we are optimistic that we can develop methods to exploit additional types of relationships in future work." ></td>
	<td class="line x" title="172:176	Acknowledgments We thank Claire Cardie, Jon Kleinberg, Michael Macy, Andrew Myers, and the six anonymous EMNLP referees for valuable discussions and comments." ></td>
	<td class="line x" title="173:176	We also thank Reviewer 1 for generously providing additional post hoc feedback, and the EMNLP chairs Eric Gaussier and Dan Jurafsky for facilitating the process (as well as for allowing authors an extra proceedings page)." ></td>
	<td class="line x" title="174:176	This paper is based upon work supported in part by the National Science Foundation under grant no." ></td>
	<td class="line x" title="175:176	IIS-0329064." ></td>
	<td class="line x" title="176:176	Any opinions, findings, and conclusions or recommendations expressed are those of the authors and do not necessarily reflect the views or official policies, either expressed or implied, of any sponsoring institutions, the U.S. government, or any other entity." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-1640
Partially Supervised Coreference Resolution For Opinion Summarization Through Structured Rule Learning
Stoyanov, Veselin;Cardie, Claire;"></td>
	<td class="line x" title="1:223	Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 336344, Sydney, July 2006." ></td>
	<td class="line x" title="2:223	c2006 Association for Computational Linguistics Partially Supervised Coreference Resolution for Opinion Summarization through Structured Rule Learning Veselin Stoyanov and Claire Cardie Department of Computer Science Cornell University Ithaca, NY 14850, USA {ves,cardie}@cs.cornell.edu Abstract Combining fine-grained opinion information to produce opinion summaries is important for sentiment analysis applications." ></td>
	<td class="line x" title="3:223	Toward that end, we tackle the problem of source coreference resolution  linking together source mentions that refer to the same entity." ></td>
	<td class="line x" title="4:223	The partially supervised nature of the problem leads us to define and approach it as the novel problem of partially supervised clustering." ></td>
	<td class="line x" title="5:223	We propose and evaluate a new algorithm for the task of source coreference resolution that outperforms competitive baselines." ></td>
	<td class="line x" title="6:223	1 Introduction Sentiment analysis is concerned with extracting attitudes, opinions, evaluations, and sentiment from text." ></td>
	<td class="line x" title="7:223	Work in this area has been motivated by the desire to provide information analysis applications in the arenas of government, business, and politics (e.g. Coglianese (2004))." ></td>
	<td class="line x" title="8:223	Additionally, sentiment analysis can augment existing NLP applications such as question answering, information retrieval, summarization, and clustering by providing information about sentiment (e.g. Stoyanov et al.(2005), Riloff et al.(2005))." ></td>
	<td class="line x" title="11:223	To date, research in the area (see Related Work section) has focused on the problem of extracting sentiment both at the document level (coarse-grained sentiment information), and at the level of sentences, clauses, or individual expressions (finegrained sentiment information)." ></td>
	<td class="line x" title="12:223	In contrast, our work concerns the summarization of fine-grained information about opinions." ></td>
	<td class="line x" title="13:223	In particular, while recent research efforts have shown that fine-grained opinions (e.g. Riloff and Wiebe (2003), Bethard et al.(2004), Wiebe and Riloff (2005)) as well as their sources (e.g. Bethard et al.(2004), Choi et al.(2005), Kim and Hovy (2005)) can be extracted automatically, little has been done to create opinion summaries, where opinions from the same source/target are combined, statistics are computed for each source/target and multiple opinions from the same source to the same target are aggregated." ></td>
	<td class="line x" title="17:223	A simple opinion summary is shown in figure 1.1 We expect that this type of opinion summary, based on fine-grained opinion information, will be important for information analysis applications in any domain where the analysis of opinions is critical." ></td>
	<td class="line x" title="18:223	This paper addresses the problem of opinion summarization by considering the creation of simple opinion summaries like those of figure 1." ></td>
	<td class="line x" title="19:223	We propose source coreference resolution  the task of determining which mentions of opinion sources refer to the same entity  as the primary mechanism for identifying the set of opinions attributed to each real-world source." ></td>
	<td class="line x" title="20:223	For this type of summary, source coreference resolution constitutes an integral step in the process of generating full opinion summaries." ></td>
	<td class="line x" title="21:223	For example, given the opinion expressions of figure 1, their polarity, and the associated opinion sources and targets, the bulk of the resulting summary can be produced by recognizing that source mentions Zacarias Moussaoui, he, my, and Mr. Moussaoui all refer to the same person; and that source mentions Mr. Zerkin and Zerkin refer to the same person.2 1For simplicity, the example summary does not contain any source/target statistics." ></td>
	<td class="line x" title="22:223	2In addition, the summary would require the closely related task of target coreference resolution and a means for aggregating the conflicting opinions from Zerkin toward Moussaoui." ></td>
	<td class="line x" title="23:223	336 At first glance, source coreference resolution appears equivalent to the task of noun phrase coreference resolution and therefore amenable to traditional coreference resolution techniques (e.g. Ng and Cardie (2002), Morton (2000))." ></td>
	<td class="line x" title="24:223	We hypothesize in Section 3, however, that the task is likely to succumb to a better solution by treating it in the context of a new machine learning setting that we refer to as partially supervised clustering." ></td>
	<td class="line x" title="25:223	In particular, due to high coreference annotation costs, data sets that are annotated with opinion information (like ours) do not typically include supervisory coreference information for all noun phrases in a document (as would be required for the application of traditional coreference resolution techniques), but only for noun phrases that act as opinion sources (or targets)." ></td>
	<td class="line x" title="26:223	As a result, we define the task of partially supervised clustering, the goal of which is to learn a clustering function from a set of partially specified clustering examples (Section 4)." ></td>
	<td class="line x" title="27:223	We are not aware of prior work on the problem of partially supervised clustering and argue that it differs substantially from that of semi-supervised clustering." ></td>
	<td class="line x" title="28:223	We propose an algorithm for partially supervised clustering that extends a rule learner with structure information and is generally applicable to problems that fit the partially supervised clustering definition (Section 5)." ></td>
	<td class="line x" title="29:223	We apply the algorithm to the source coreference resolution task and evaluate its performance on a standard sentiment analysis data set that includes source coreference chains (Section 6)." ></td>
	<td class="line x" title="30:223	We find that our algorithm outperforms highly competitive baselines by a considerable margin  B3 score of 83.2 vs. 81.8 and 67.1 vs. 60.9 F1 score for the identification of positive source coreference links." ></td>
	<td class="line x" title="31:223	2 Related Work Work relevant to our problem can be split into three main areas  sentiment analysis, traditional noun phrase coreference resolution, and supervised and weakly supervised clustering." ></td>
	<td class="line x" title="32:223	Related work in the former two areas is summarized briefly below." ></td>
	<td class="line x" title="33:223	Supervised and weakly supervised clustering approaches are discussed in Section 4." ></td>
	<td class="line x" title="34:223	Sentiment analysis." ></td>
	<td class="line x" title="35:223	Much of the relevant research in sentiment analysis addresses sentiment classification, a text categorization task of extracting opinion at the coarse-grained document level." ></td>
	<td class="line x" title="36:223	The goal in sentiment classification is to assign to [Source Zacarias Moussaoui] [ complained] at length today about [Target his own lawyer], telling a federal court jury that [Target he] was [ more interested in achieving fame than saving Moussaouis life]." ></td>
	<td class="line x" title="37:223	Mr. Moussaoui said he was appearing on the witness stand to tell the truth." ></td>
	<td class="line x" title="38:223	And one part of the truth, [Source he] said, is that [Target sending him to prison for life] would be [ a greater punishment] than being sentenced to death. [ [Target You] have put your interest ahead of [Source my] life], [Source Mr. Moussaoui] told his court-appointed lawyer Gerald T. Zerkin But, [Source Mr. Zerkin] pressed [Target Mr. Moussaoui], was it [ not true] that he told his lawyers earlier not to involve any Muslims in the defense, not to present any evidence that might persuade the jurors to spare his life?" ></td>
	<td class="line x" title="39:223	[Source Zerkin] seemed to be trying to show the jurors that while [Target the defendant] is generally [+ an honest individual], his conduct shows [Target he] is [ not stable mentally], and thus [ undeserving] of [Target the ultimate punishment]." ></td>
	<td class="line x" title="40:223	Moussaoui Zerkin prison for life ultimate punishment    /+ Figure 1: Example text containing opinions (above) and a summary of the opinions (below)." ></td>
	<td class="line x" title="41:223	Sources and targets of opinions are bracketed; opinion expressions are shown in italics and bracketed with associated polarity, either positive (+) or negative (-)." ></td>
	<td class="line x" title="42:223	The underlined phrase will be explained later in the paper." ></td>
	<td class="line x" title="43:223	a document either positive (thumbs up) or negative (thumbs down) polarity (e.g. Das and Chen (2001), Pang et al.(2002), Turney (2002), Dave et al.(2003))." ></td>
	<td class="line x" title="46:223	Other research has concentrated on analyzing fine-grained opinions at, or below, the sentence level." ></td>
	<td class="line x" title="47:223	Recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to a reasonable degree of accuracy (e.g. Dave et al.(2003), Riloff and Wiebe (2003), Bethard et al.(2004), Wilson et al.(2004), Yu and Hatzivassiloglou (2003), Choi et al.(2005), Kim and Hovy (2005), Wiebe and Riloff (2005))." ></td>
	<td class="line x" title="52:223	Our work extends research on fine-grained opinion extraction by augmenting the opinions with additional information that allows the creation of concise opinion summaries." ></td>
	<td class="line oc" title="53:223	In contrast to the opinion extracts produced by Pang and Lee (2004), our summaries are not text extracts, but rather explicitly identify and 337 characterize the relations between opinions and their sources." ></td>
	<td class="line x" title="54:223	Coreference resolution." ></td>
	<td class="line x" title="55:223	Coreference resolution is a relatively well studied NLP problem (e.g. Morton (2000), Ng and Cardie (2002), Iida et al.(2003), McCallum and Wellner (2003))." ></td>
	<td class="line x" title="57:223	Coreference resolution is defined as the problem of deciding which noun phrases in the text (mentions) refer to the same real world entities (are coreferent)." ></td>
	<td class="line x" title="58:223	Generally, successful approaches to coreference resolution have relied on supervised classification followed by clustering." ></td>
	<td class="line x" title="59:223	For supervised classification these approaches learn a pairwise function to predict whether a pair of noun phrases is coreferent." ></td>
	<td class="line x" title="60:223	Subsequently, when making coreference resolution decisions on unseen documents, the learnt pairwise NP coreference classifier is run, followed by a clustering step to produce the final clusters (coreference chains) of coreferent NPs." ></td>
	<td class="line x" title="61:223	For both training and testing, coreference resolution algorithms rely on feature vectors for pairs of noun phrases that encode linguistic information about the NPs and their local context." ></td>
	<td class="line x" title="62:223	Our general approach to source coreference resolution is inspired by the state-of-the-art performance of one such approach to coreference resolution, which relies on a rule learner and single-link clustering as described in Ng and Cardie (2002)." ></td>
	<td class="line x" title="63:223	3 Source Coreference Resolution In this section we introduce the problem of source coreference resolution in the context of opinion summarization and argue for the need for novel methods for the task." ></td>
	<td class="line x" title="64:223	The task of source coreference resolution is to decide which mentions of opinion sources refer to the same entity." ></td>
	<td class="line x" title="65:223	Much like traditional coreference resolution, we employ a learning approach; however, our approach differs from traditional coreference resolution in its definition of the learning task." ></td>
	<td class="line x" title="66:223	Motivated by the desire to utilize unlabeled examples (discussed later), we define training as an integrated task in which pairwise NP coreference decisions are learned together with the clustering function as opposed to treating each NP pair as a training example." ></td>
	<td class="line x" title="67:223	Thus, our training phase takes as input a set of documents with manually annotated opinion sources together with coreference annotations for the sources; it outputs a classifier that can produce source coreference chains for previously unseen documents containing marked (manually or automatically) opinion sources." ></td>
	<td class="line x" title="68:223	More specifically, the source coreference resolution training phase proceeds through the following steps: 1." ></td>
	<td class="line x" title="69:223	Source-to-NP mapping: We preprocess each document by running a tokenizer, sentence splitter, POS tagger, parser, and an NP finder." ></td>
	<td class="line x" title="70:223	Subsequently, we augment the set of NPs found by the NP finder with the help of a system for named entity detection." ></td>
	<td class="line x" title="71:223	We then map the sources to the NPs." ></td>
	<td class="line x" title="72:223	Since there is no one-to-one correspondence, we use a set of heuristics to create the mapping." ></td>
	<td class="line x" title="73:223	More details about why heuristics are needed and the process used to map sources to NPs can be found in Stoyanov and Cardie (2006)." ></td>
	<td class="line x" title="74:223	2." ></td>
	<td class="line x" title="75:223	Feature vector creation: We extract a feature vector for every pair of NPs from the preprocessed corpus." ></td>
	<td class="line x" title="76:223	We use the features introduced by Ng and Cardie (2002) for the task of coreference resolution." ></td>
	<td class="line x" title="77:223	3." ></td>
	<td class="line x" title="78:223	Classifier construction: Using the feature vectors from step 2, we construct a training set containing one training example per document." ></td>
	<td class="line x" title="79:223	Each training example consists of the feature vectors for all pairs of NPs in the document, including those that do not map to sources, together with the available coreference information for the source noun phrases (i.e. the noun phrases to which sources are mapped)." ></td>
	<td class="line x" title="80:223	The training instances are provided as input to a learning algorithm (see Section 5), which constructs a classifier that can take the instances associated with a new (previously unseen) document and produce a clustering over all NPs in the document." ></td>
	<td class="line x" title="81:223	The testing phase employs steps 1 and 2 as described above, but replaces step 3 by a straightforward application of the learnt classifier." ></td>
	<td class="line x" title="82:223	Since we are interested in coreference information only for the source NPs, we simply discard the non-source NPs from the resulting clustering." ></td>
	<td class="line x" title="83:223	The approach to source coreference resolution described here would be identical to traditional coreference resolution when provided with training examples containing coreference information for all NPs." ></td>
	<td class="line x" title="84:223	However, opinion corpora in general, and our corpus in particular, contain no coreference information about general NPs." ></td>
	<td class="line x" title="85:223	Nevertheless, after manual sources are mapped to NPs in 338 step 1 above, our approach can rely on the available coreference information for the source NPs." ></td>
	<td class="line x" title="86:223	Due to the high cost of coreference annotation, we desire methods that can work in the presence of only this limited amount of coreference information." ></td>
	<td class="line x" title="87:223	A possible workaround the absence of full NP coreference information is to train a traditional coreference system only on the labeled part of the data (indeed that is one of the baselines against which we compare)." ></td>
	<td class="line x" title="88:223	However, we believe that an effective approach to source coreference resolution has to utilize the unlabeled noun phrases because links between sources might be realized through non-source mentions." ></td>
	<td class="line x" title="89:223	This problem is illustrated in figure 1." ></td>
	<td class="line x" title="90:223	The underlined Moussaoui is coreferent with all of the Moussaoui references marked as sources, but, because it is used in an objective sentence rather than as the source of an opinion, the reference would be omitted from the Moussaoui source chain." ></td>
	<td class="line x" title="91:223	Unfortunately, this proper noun phrase might be critical in establishing the coreference of the final source reference he with the other mentions of the source Moussaoui." ></td>
	<td class="line x" title="92:223	As mentioned previously, in order to utilize the unlabeled data, our approach differs from traditional coreference resolution, which uses NP pairs as training instances." ></td>
	<td class="line x" title="93:223	We instead follow the framework of supervised clustering (Finley and Joachims, 2005; Li and Roth, 2005) and consider each document as a training example." ></td>
	<td class="line x" title="94:223	As in supervised clustering, this framework has the additional advantage that the learning algorithm can consider the clustering algorithm when making decisions about pairwise classification, which could lead to improvements in the classifier." ></td>
	<td class="line x" title="95:223	In the next section we describe our approach to classifier construction for step 3 and compare our problem to traditional weakly supervised clustering, characterizing it as an instance of the novel problem of partially supervised clustering." ></td>
	<td class="line x" title="96:223	4 Partially Supervised Clustering In our desire to perform effective source coreference resolution we arrive at the following learning problem  the learning algorithm is presented with a set of partially specified examples of clusterings and acquires a function that can cluster accurately an unseen set of items, while taking advantage of the unlabeled information in the examples." ></td>
	<td class="line x" title="97:223	This setting is to be contrasted with semisupervised clustering (or clustering with constraints), which has received much research attention (e.g. Demiriz et al.(1999), Wagstaff and Cardie (2000), Basu (2005), Davidson and Ravi (2005))." ></td>
	<td class="line x" title="99:223	Semi-supervised clustering can be defined as the problem of clustering a set of items in the presence of limited supervisory information such as pairwise constraints (e.g. two items must/cannot be in the same cluster) or labeled points." ></td>
	<td class="line x" title="100:223	In contrast to our setting, in the semisupervised case there is no training phase  the algorithm receives all examples (labeled and unlabeled) at the same time together with some distance or cost function and attempts to find a clustering that optimizes a given measure (usually based on the distance or cost function)." ></td>
	<td class="line x" title="101:223	Source coreference resolution might alternatively be approached as a supervised clustering problem." ></td>
	<td class="line x" title="102:223	Traditionally, approaches to supervised clustering have treated the pairwise link decisions as a classification problem." ></td>
	<td class="line x" title="103:223	These approaches first learn a distance metric that optimizes the pairwise decisions; and then follow the pairwise classification with a clustering step." ></td>
	<td class="line x" title="104:223	However, these traditional approaches have no obvious way of utilizing the available unlabeled information." ></td>
	<td class="line x" title="105:223	In contrast, we follow recent approaches to supervised clustering that propose ways to learn the distance measure in the context of the clustering decisions (Li and Roth, 2005; Finley and Joachims, 2005; McCallum and Wellner, 2003)." ></td>
	<td class="line x" title="106:223	This provides two advantages for the problem of source coreference resolution." ></td>
	<td class="line x" title="107:223	First, it allows the algorithm to take advantage of the complexity of the rich structural dependencies introduced by the clustering problem." ></td>
	<td class="line x" title="108:223	Viewed traditionally as a hurdle, the structural complexity of clustering may be beneficial in the partially supervised case." ></td>
	<td class="line x" title="109:223	We believe that provided with a few partially specified clustering examples, an algorithm might be able to generalize from the structural dependencies to infer correctly the whole clustering of the items." ></td>
	<td class="line x" title="110:223	In addition, considering pairwise decisions in the context of the clustering can arguably lead to more accurate classifiers." ></td>
	<td class="line x" title="111:223	Unfortunately, none of the supervised clustering approaches is readily applicable to the partially supervised case." ></td>
	<td class="line x" title="112:223	However, by adapting the formal supervised clustering definition, which we do next, we can develop approaches to partially supervised clustering that take advantage of the un339 labeled portions of the data." ></td>
	<td class="line x" title="113:223	Formal definition." ></td>
	<td class="line x" title="114:223	For partially supervised clustering we extend the formal definition of supervised clustering given by Finley and Joachims (2005)." ></td>
	<td class="line x" title="115:223	In the fully supervised setting, an algorithm is given a set S of n training examples (x1,y1),,(xn,yn)  X  Y, where X is the set of all possible sets of items and Y is the set of all possible clusterings of these sets." ></td>
	<td class="line x" title="116:223	For a training example (x,y), x = {x1,x2,,xk} is a set of k items and y = {y1,y2,,yr} is a clustering of the items in x with each yi  x. Additionally, each item can be in no more than one cluster (i,j.yi yj = ) and in the fully supervised case each item is in at least one cluster (x = uniontextyi)." ></td>
	<td class="line x" title="117:223	The goal of the learning algorithm is to acquire a function h : X  Y that can accurately cluster a (previously unseen) set of items." ></td>
	<td class="line x" title="118:223	In the context of source coreference resolution the training set contains one example for each document." ></td>
	<td class="line x" title="119:223	The items in each training example are the NPs and the clustering over the items is the equivalence relation defined by the coreference information." ></td>
	<td class="line x" title="120:223	For source coreference resolution, however, clustering information is unavailable for the non-source NPs." ></td>
	<td class="line x" title="121:223	Thus, to be able to deal with this unlabeled component of the data we arrive to the setting of partially supervised clustering, in which we relax the condition that each item is in at least one cluster (x = uniontextyi) and replace it with the condition x  uniontextyi." ></td>
	<td class="line x" title="122:223	The items with no linking information (items in x\uniontextyi) constitute the unlabeled (unsupervised) component of the partially supervised clustering." ></td>
	<td class="line x" title="123:223	5 Structured Rule Learner We develop a novel method for partially supervised clustering, which is motivated by the success of a rule learner (RIPPER) for coreference resolution (Ng and Cardie, 2002)." ></td>
	<td class="line x" title="124:223	We extend RIPPER so that it can learn rules in the context of singlelink clustering, which both suits our task (i.e. pronouns link to their single antecedent) and has exhibited good performance for coreference resolution (Ng and Cardie, 2002)." ></td>
	<td class="line x" title="125:223	We begin with a brief overview of RIPPER followed by a description of the modifications that we implemented." ></td>
	<td class="line x" title="126:223	For ease of presentation, we assume that we are in the fully supervised case." ></td>
	<td class="line x" title="127:223	We end this section by describing the changes for the partially supervised case." ></td>
	<td class="line x" title="128:223	procedure StRip(TrainData){ GrowData, PruneData = Split(TrainData); //Keep instances from the same document together while(there are positive uncovered instances) { r = growRule(GrowData); r = pruneRule(r, PruneData); DL = relativeDL(Ruleset); if(DL  minDL + d bits) Ruleset.add(r); Mark examples covered by r as +; else exit loop with Ruleset } } procedure growRule(growData){ r = empty rule; for(every unused feature f){ if (f is nominal feature) { for(every possible value v of f) { mark all instances that have values of v for f with +; compute the transitive closure of the positive instances //(including instances marked + from previous rules); compute the infoGain for the future/value combination; } } else //Numeric feature create one bag for each feature value and split the instances into bags; do a forward and a backward pass over the bags keeping a running clustering and compute the information gain for each value; } } add the future/value pair with the best infoGain to r; growData = growData all negative instances; return r; } procedure pruneRule(r, pruneData){ for(all antecedents a in the rule){ apply all antecedents in r up to a to pruneData; compute the transitive closure of the positive instances; compute A(a)  the accuracy of the rule up to antecedent a; } Remove all antecedents after the antecedent for which A(a) is maximum." ></td>
	<td class="line x" title="129:223	} Figure 2: The StRip algorithm." ></td>
	<td class="line x" title="130:223	Additions to RIPPER are shown in bold." ></td>
	<td class="line x" title="131:223	5.1 The RIPPER Algorithm RIPPER (for Repeated Incremental Pruning to Produce Error Reduction) was introduced by Cohen (1995) as an extension of an existing rule induction algorithm." ></td>
	<td class="line x" title="132:223	Cohen (1995) showed that RIPPER produces error rates competitive with C4.5, while exhibiting better running times." ></td>
	<td class="line x" title="133:223	RIPPER consists of two phases  a ruleset is grown and then optimized." ></td>
	<td class="line x" title="134:223	The ruleset creation phase begins by randomly splitting the training data into a rulegrowing set (2/3 of the training data) and a pruning set (the remaining 1/3)." ></td>
	<td class="line x" title="135:223	A rule is then grown on the former set by repeatedly adding the antecedent (the feature value test) with the largest information gain until the accuracy of the rule becomes 1.0 or there are no remaining potential antecedents." ></td>
	<td class="line x" title="136:223	Next the rule is applied to the pruning data and any rulefinal sequence that reduces the accuracy of the rule is removed." ></td>
	<td class="line x" title="137:223	The optimization phase uses the full training 340 set to first grow a replacement rule and a revised rule for each rule in the ruleset." ></td>
	<td class="line x" title="138:223	For each rule, the algorithm then considers the original rule, the replacement rule, and the revised rule, and keeps the rule with the smallest description length in the context of the ruleset." ></td>
	<td class="line x" title="139:223	After all rules are considered, RIPPER attempts to grow residual rules that cover data not already covered by the ruleset." ></td>
	<td class="line x" title="140:223	Finally, RIPPER deletes any rules from the ruleset that reduce the overall minimum description length of the data plus the ruleset." ></td>
	<td class="line x" title="141:223	RIPPER performs two rounds of this optimization phase." ></td>
	<td class="line x" title="142:223	5.2 The StRip Algorithm The property of partially supervised clustering that we want to explore is the structured nature of the decisions." ></td>
	<td class="line x" title="143:223	That is, each decision of whether two items (say a and b) belong to the same cluster has an implication for all items aprime that belong to as cluster and all items bprime that belong to bs cluster." ></td>
	<td class="line x" title="144:223	We target modifications to RIPPER that will allow StRip (for Structured RIPPER) to learn rules that produce good clusterings in the context of single-link clustering." ></td>
	<td class="line x" title="145:223	We extend RIPPER so that every time it makes a decision about a rule, it considers the effect of the rule on the overall clustering of items (as opposed to considering the instances that the rule classifies as positive/negative in isolation)." ></td>
	<td class="line x" title="146:223	More precisely, we precede every computation of rule performance (e.g. information gain or description length) by a transitive closure (i.e. single link clustering) of the data w.r.t. to the pairwise classifications." ></td>
	<td class="line x" title="147:223	Following the transitive closure, all pairs of items that are in the same cluster are considered covered by the rule for performance computation." ></td>
	<td class="line x" title="148:223	The StRip algorithm is given in figure 2, with modifications to the original RIPPER algorithm shown in bold." ></td>
	<td class="line x" title="149:223	Due to space limitations the optimization stage of the algorithm is omitted." ></td>
	<td class="line x" title="150:223	Our modifications to the optimization stage of RIPPER are in the spirit of the rest of the StRip algorithm." ></td>
	<td class="line x" title="151:223	Partially supervised case." ></td>
	<td class="line x" title="152:223	So far we described StRip only for the fully supervised case." ></td>
	<td class="line x" title="153:223	We use a very simple modification to handle the partially supervised setting: we exclude the unlabeled pairs when computing the performance of the rules." ></td>
	<td class="line x" title="154:223	Thus, the unlabeled items do not count as correct or incorrect classifications when acquiring or pruning a rule, although they do participate in the transitive closure." ></td>
	<td class="line x" title="155:223	Links in the unlabeled data are inferred entirely through the indirect links between items in the labeled component that they introduce." ></td>
	<td class="line x" title="156:223	In the example of figure 1, the two problematic unlabeled links are the link between the source mention he and the underlined nonsource NP Mr. Moussaoui and the link between the underlined Mr. Moussaoui to any source mention of Moussaoui." ></td>
	<td class="line x" title="157:223	While StRip will not reward any rule (or rule set) that covers these two links directly, such rules will be rewarded indirectly since they put the source he in the chain for the source Moussaoui." ></td>
	<td class="line x" title="158:223	StRip running time." ></td>
	<td class="line x" title="159:223	StRips running time is generally comparable to that of RIPPER." ></td>
	<td class="line x" title="160:223	We compute transitive closure by using a Union-Find structure, which runs in time O(logn), which for practical purposes can be considered linear (O(n)) 3." ></td>
	<td class="line x" title="161:223	However, when computing the best information gain for a nominal feature, StRip has to make a pass over the data for each value that the feature takes, while RIPPER can split the data into bags and perform the computation in one pass." ></td>
	<td class="line x" title="162:223	6 Evaluation and Results This section describes the source coreference data set, the baselines, our implementation of StRip, and the results of our experiments." ></td>
	<td class="line x" title="163:223	6.1 Data set For evaluation we use the MPQA corpus (Wiebe et al. , 2005).4 The corpus consists of 535 documents from the world press." ></td>
	<td class="line x" title="164:223	All documents in the collection are manually annotated with phraselevel opinion information following the annotation scheme of Wiebe et al.(2005)." ></td>
	<td class="line x" title="166:223	Discussion of the annotation scheme is beyond the scope of this paper; for our purposes it suffices to say that the annotations include the source of each opinion and coreference information for the sources (e.g. source coreference chains)." ></td>
	<td class="line x" title="167:223	The corpus contains no additional noun phrase coreference information." ></td>
	<td class="line x" title="168:223	For our experiments, we randomly split the data set into a training set consisting of 400 documents and a test set consisting of the remaining 135 documents." ></td>
	<td class="line x" title="169:223	We use the same test set for all experi3For the transitive closure, n is the number of items in a document, which is O(k), where k is the number of NP pairs." ></td>
	<td class="line x" title="170:223	Thus, transitive closure is sublinear in the number of training instances." ></td>
	<td class="line x" title="171:223	4The MPQA corpus is available at http://nrrc.mitre.org/NRRC/publications.htm." ></td>
	<td class="line x" title="172:223	341 ments, although some learning runs were trained on 200 training documents (see next Subsection)." ></td>
	<td class="line x" title="173:223	The test set contains a total of 4736 source NPs (average of 35.34 source NPs per document) split into 1710 total source NP chains (average of 12.76 chains per document) for an average of 2.77 source NPs per chain." ></td>
	<td class="line x" title="174:223	6.2 Implementation We implemented the StRip algorithm by modifying JRip  the java implementation of RIPPER included in the WEKA toolkit (Witten and Frank, 2000)." ></td>
	<td class="line x" title="175:223	The WEKA implementation follows the original RIPPER specification." ></td>
	<td class="line x" title="176:223	We changed the implementation to incorporate the modifications suggested by the StRip algorithm; we also modified the underlying data representations and data handling techniques for efficiency." ></td>
	<td class="line x" title="177:223	Also due to efficiency considerations, we train StRip only on the 200-document training set." ></td>
	<td class="line x" title="178:223	6.3 Competitive baselines We compare the results of the new method to three fully supervised baseline systems, each of which employs the same traditional coreference resolution approach." ></td>
	<td class="line x" title="179:223	In particular, we use the aforementioned algorithm proposed by Ng and Cardie (2002), which combines a pairwise NP coreference classifier with single-link clustering." ></td>
	<td class="line x" title="180:223	For one baseline, we train the coreference resolution algorithm on the MPQA src corpus  the labeled portion of the MPQA corpus (i.e. NPs from the source coreference chains) with unlabeled instances removed." ></td>
	<td class="line x" title="181:223	The second and third baselines investigate whether the source coreference resolution task can benefit from NP coreference resolution training data from a different domain." ></td>
	<td class="line x" title="182:223	Thus, we train the traditional coreference resolution algorithm on the MUC6 and MUC7 coreference-annotated corpora5 that contain documents similar in style to those in the MPQA corpus (e.g. newspaper articles), but emanate from different domains." ></td>
	<td class="line x" title="183:223	For all baselines we targeted the best possible systems by trying two pairwise NP classifiers (RIPPER and an SVM in the SV Mlight implementation (Joachims, 1998)), many different parameter settings for the classifiers, two different feature sets, two different training set sizes (the 5We train each baseline using both the development set and the test set from the corresponding MUC corpus." ></td>
	<td class="line x" title="184:223	full training set and a smaller training set consisting of half of the documents selected at random), and three different instance selection algorithms6." ></td>
	<td class="line x" title="185:223	This variety of classifier and training data settings was motivated by reported differences in performance of coreference resolution approaches w.r.t. these variations (Ng and Cardie, 2002)." ></td>
	<td class="line x" title="186:223	More details on the different parameter settings and instance selection algorithms as well as trends in the performance of different settings can be found in Stoyanov and Cardie (2006)." ></td>
	<td class="line x" title="187:223	In the experiments below we report the best performance of each of the two learning algorithms on the MPQA test data." ></td>
	<td class="line x" title="188:223	6.4 Evaluation In addition to the baselines described above, we evaluate StRip both with and without unlabeled data." ></td>
	<td class="line x" title="189:223	That is, we train on the MPQA corpus StRip using either all NPs or just opinion source NPs." ></td>
	<td class="line x" title="190:223	We use the B3 (Bagga and Baldwin, 1998) evaluation measure as well as precision, recall, and F1 measured on the (positive) pairwise decisions." ></td>
	<td class="line x" title="191:223	B3 is a measure widely used for evaluating coreference resolution algorithms." ></td>
	<td class="line x" title="192:223	The measure computes the precision and recall for each NP mention in a document, and then averages them to produce combined results for the entire output." ></td>
	<td class="line x" title="193:223	More precisely, given a mention i that has been assigned to chain ci, the precision for mention i is defined as the number of correctly identified mentions in ci divided by the total number of mentions in ci." ></td>
	<td class="line x" title="194:223	Recall for i is defined as the number of correctly identified mentions in ci divided by the number of mentions in the gold standard chain for i. Results are shown in Table 1." ></td>
	<td class="line x" title="195:223	The first six rows of results correspond to the fully supervised baseline systems trained on different corpora  MUC6, MUC7, and MPQA src." ></td>
	<td class="line x" title="196:223	The seventh row of results shows the performance of StRip using only labeled data." ></td>
	<td class="line x" title="197:223	The final row of the table shows the results for partially supervised learning with unlabeled data." ></td>
	<td class="line x" title="198:223	The table lists results from the best performing run for each algorithm." ></td>
	<td class="line x" title="199:223	Performance among the baselines trained on the MUC data is comparable." ></td>
	<td class="line x" title="200:223	However, the two baseline runs trained on the MPQA src corpus (i.e. results rows five and six) show slightly better performance on the B3 metric than the baselines trained 6The goal of the instance selection algorithms is to balance the data, which contains many more negative than positive instances 342 ML Framework Training set Classifier B3 precision recall F1 Fully supervised MUC6 SVM 81.2 72.6 52.5 60.9 RIPPER 80.7 57.4 63.5 60.3 MUC7 SVM 81.7 65.6 55.9 60.4 RIPPER 79.7 71.6 48.5 57.9 MPQA src SVM 81.8 57.5 62.9 60.2 RIPPER 81.8 72.0 52.5 60.6 StRip 82.3 76.5 56.1 64.6 Partially supervised MPQA all StRip 83.2 77.1 59.4 67.1 Table 1: Results for Source Coreference." ></td>
	<td class="line x" title="201:223	MPQA src stands for the MPQA corpus limited to only source NPs, while MPQA full contains the unlabeled NPs." ></td>
	<td class="line x" title="202:223	on the MUC data, which indicates that for our task the similarity of the documents in the training and test sets appears to be more important than the presence of complete supervisory information." ></td>
	<td class="line x" title="203:223	(Improvements over the RIPPER runs trained on the MUC corpora are statistically significant7, while improvements over the SVM runs are not.)" ></td>
	<td class="line x" title="204:223	Table 1 also shows that StRip outperforms the baselines on both performance metrics." ></td>
	<td class="line x" title="205:223	StRips performance is better than the baselines when trained on MPQA src (improvement not statistically significant, p > 0.20) and even better when trained on the full MPQA corpus, which includes the unlabeled NPs (improvement over the baselines and the former StRip run statistically significant)." ></td>
	<td class="line x" title="206:223	These results confirm our hypothesis that StRip improves due to two factors: first, considering pairwise decisions in the context of the clustering function leads to improvements in the classifier; and, second, StRip can take advantage of the unlabeled portion of the data." ></td>
	<td class="line x" title="207:223	StRips performance is all the more impressive considering the strength of the SVM and RIPPER baselines, which which represent the best runs across the 336 different parameter settings tested for SV Mlight and 144 different settings tested for RIPPER." ></td>
	<td class="line x" title="208:223	In contrast, all four of the StRip runs using the full MPQA corpus (we vary the loss ratio for false positive/false negative cost) outperform those baselines." ></td>
	<td class="line x" title="209:223	7 Future Work Source coreference resolution is only one aspect of opinion summarization." ></td>
	<td class="line x" title="210:223	Additionally, an opinion summarization system will need to handle 7Statistical significance is measured using both a 2-tailed paired t-test and the Wilcoxon matched-pairs signed-ranks test (p < 0.05)." ></td>
	<td class="line x" title="211:223	The two tests agreed on all significance judgements, so we will not report them separately." ></td>
	<td class="line x" title="212:223	the closely related task of target coreference resolution in order to cluster targets of opinions8 and combine multiple conflicting opinions from a source to the same targets." ></td>
	<td class="line x" title="213:223	Furthermore, a fully automatic opinion summarizer requires automatic source and opinion extractors." ></td>
	<td class="line x" title="214:223	While we anticipate that target coreference resolution will be subject to error rates similar to those of source coreference resolution, incorporating these imperfect opinions and sources will further impair the performance of the opinion summarizer." ></td>
	<td class="line x" title="215:223	We are not aware of any measure that can be directly used to assess the goodness of opinion summaries, but plan to develop such in future work in conjunction with the development of methods for creating opinion summaries completely automatically." ></td>
	<td class="line x" title="216:223	The evaluation metrics will likely have to depend on the task for which the summaries are used." ></td>
	<td class="line x" title="217:223	A limitation of our approach to partially supervised clustering is that we do not directly optimize for the performance measure (e.g. B3)." ></td>
	<td class="line x" title="218:223	Other efforts in the area of supervised clustering (Finley and Joachims, 2005; Li and Roth, 2005) have suggested ways to learn distance measures that can optimize directly for a desired performance measure." ></td>
	<td class="line x" title="219:223	We plan to investigate algorithms that can directly optimize for complex measures (such as B3) for the problem of partially supervised clustering." ></td>
	<td class="line x" title="220:223	Unfortunately, a measure as complex as B3 makes extending existing approaches far from trivial due to the difficulty of establishing the connection between individual pairwise decisions (the distance metric) and the score of the clustering algorithm." ></td>
	<td class="line x" title="221:223	Acknowledgements The authors would like to thank Vincent Ng and Art Munson for providing coreference resolution 8We did not tackle the task of target coreference resolution in this paper because the MPQA corpus did not contain target annotations at the time of publication." ></td>
	<td class="line x" title="222:223	343 code, members of the Cornell NLP group (especially Yejin Choi and Art Munson) for many helpful discussions, and the anonymous reviewers for their insightful comments." ></td>
	<td class="line x" title="223:223	This work was supported by the Advanced Research and Development Activity (ARDA), by NSF Grants IIS0535099 and IIS-0208028, by gifts from Google and the Xerox Foundation, and by an NSF Graduate Research Fellowship to the first author." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-1642
Fully Automatic Lexicon Expansion For Domain-Oriented Sentiment Analysis
Kanayama, Hiroshi;Nasukawa, Tetsuya;"></td>
	<td class="line x" title="1:252	Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 355363, Sydney, July 2006." ></td>
	<td class="line x" title="2:252	c2006 Association for Computational Linguistics Fully Automatic Lexicon Expansion for Domain-oriented Sentiment Analysis Hiroshi Kanayama Tetsuya Nasukawa Tokyo Research Laboratory, IBM Japan, Ltd. 1623-14 Shimotsuruma, Yamato-shi, Kanagawa-ken, 242-8502 Japan {hkana,nasukawa}@jp.ibm.com Abstract This paper proposes an unsupervised lexicon building method for the detection of polar clauses, which convey positive or negative aspects in a specific domain." ></td>
	<td class="line x" title="3:252	The lexical entries to be acquired are called polar atoms, the minimum human-understandable syntactic structures that specify the polarity of clauses." ></td>
	<td class="line x" title="4:252	As a clue to obtain candidate polar atoms, we use context coherency, the tendency for same polarities to appear successively in contexts." ></td>
	<td class="line x" title="5:252	Using the overall density and precision of coherency in the corpus, the statistical estimation picks up appropriate polar atoms among candidates, without any manual tuning of the threshold values." ></td>
	<td class="line x" title="6:252	The experimental results show that the precision of polarity assignment with the automatically acquired lexicon was 94% on average, and our method is robust for corpora in diverse domains and for the size of the initial lexicon." ></td>
	<td class="line x" title="7:252	1 Introduction Sentiment Analysis (SA) (Nasukawa and Yi, 2003; Yi et al. , 2003) is a task to recognize writers feelings as expressed in positive or negative comments, by analyzing unreadably large numbers of documents." ></td>
	<td class="line x" title="8:252	Extensive syntactic patterns enable us to detect sentiment expressions and to convert them into semantic structures with high precision, as reported by Kanayama et al.(2004)." ></td>
	<td class="line x" title="10:252	From the example Japanese sentence (1) in the digital camera domain, the SA system extracts a sentiment representation as (2), which consists of a predicate and an argument with positive (+) polarity." ></td>
	<td class="line x" title="11:252	(1) Kono kamera-ha subarashii-to omou." ></td>
	<td class="line x" title="12:252	I think this camera is splendid. (2) [+] splendid(camera) SA in general tends to focus on subjectivesentimentexpressions, whichexplicitlydescribe an authors preference as in the above example (1)." ></td>
	<td class="line x" title="13:252	Objective (or factual) expressions such as in the following examples (3) and (4) may be out of scope even though they describe desirable aspects in a specific domain." ></td>
	<td class="line x" title="14:252	However, when customers or corporate users use SA system for their commercial activities, such domain-specific expressions have a more important role, since they convey strong or weak points of the product more directly, and may influence their choice to purchase a specific product, as an example." ></td>
	<td class="line x" title="15:252	(3) Kontorasuto-ga kukkiri-suru." ></td>
	<td class="line x" title="16:252	The contrast is sharp. (4) Atarashii kishu-ha zuumu-mo tsuite-iru." ></td>
	<td class="line x" title="17:252	The new model has a zoom lens, too. This paper addresses the Japanese version of Domain-oriented Sentiment Analysis, which identifies polar clauses conveying goodness and badness in a specific domain, including rather objective expressions." ></td>
	<td class="line x" title="18:252	Building domain-dependent lexicons for many domains is much harder work than preparing domainindependent lexicons and syntactic patterns, because the possible lexical entries are too numerous, and they may differ in each domain." ></td>
	<td class="line x" title="19:252	To solve this problem, we have devised an unsupervised method to acquire domaindependent lexical knowledge where a user has only to collect unannotated domain corpora." ></td>
	<td class="line x" title="20:252	The knowledge to be acquired is a domaindependent set of polar atoms." ></td>
	<td class="line x" title="21:252	A polar atom is a minimum syntactic structure specifying polarity in a predicative expression." ></td>
	<td class="line x" title="22:252	For example, to detect polar clauses in the sentences (3) 355 and (4)1, the following polar atoms (5) and (6) should appear in the lexicon: (5) [+] kukkiri-suru to be sharp (6) [+] tsuku  zuumu-ga to have  zoom lens-NOM The polar atom (5) specified the positive polarity of the verb kukkiri-suru." ></td>
	<td class="line x" title="23:252	This atom can be generally used for this verb regardless of its arguments." ></td>
	<td class="line x" title="24:252	In the polar atom (6), on the other hand, the nominative case of the verb tsuku (have) is limited to a specific noun zuumu (zoom lens), since the verb tsuku does not hold the polarity in itself." ></td>
	<td class="line x" title="25:252	The automatic decision for the scopes of the atoms is one of the major issues." ></td>
	<td class="line x" title="26:252	For lexical learning from unannotated corpora, our method uses context coherency in terms of polarity, an assumption that polar clauses with the same polarity appear successively unless the context is changed with adversative expressions." ></td>
	<td class="line x" title="27:252	Exploiting this tendency, we can collect candidate polar atoms with their tentative polarities as those adjacent to the polar clauses which have been identified by their domain-independent polar atoms in the initial lexicon." ></td>
	<td class="line x" title="28:252	We use both intrasentential and inter-sentential contexts to obtain more candidate polar atoms." ></td>
	<td class="line x" title="29:252	Our assumption is intuitively reasonable, but there are many non-polar (neutral) clauses adjacent to polar clauses." ></td>
	<td class="line x" title="30:252	Errors in sentence delimitation or syntactic parsing also result in false candidate atoms." ></td>
	<td class="line x" title="31:252	Thus, to adopt a candidate polar atom for the new lexicon, some threshold values for the frequencies or ratios are required, but they depend on the type of the corpus, the size of the initial lexicon, etc. Our algorithm is fully automatic in the sense that the criteria for the adoption of polar atoms are set automatically by statistical estimation based on the distributions of coherency: coherent precision and coherent density." ></td>
	<td class="line x" title="32:252	No manual tuning process is required, so the algorithm only needs unannotated domain corpora and the initial lexicon." ></td>
	<td class="line x" title="33:252	Thus our learning method can be used not only by the developers of the system, but also by endusers." ></td>
	<td class="line x" title="34:252	This feature is very helpful for users to 1The English translations are included only for convenience." ></td>
	<td class="line x" title="35:252	analyze documents in new domains." ></td>
	<td class="line x" title="36:252	In the next section, we review related work, and Section 3 describes our runtime SA system." ></td>
	<td class="line x" title="37:252	In Section 4, our assumption for unsupervised learning, context coherency and its key metrics, coherent precision and coherent density are discussed." ></td>
	<td class="line x" title="38:252	Section 5 describes our unsupervised learning method." ></td>
	<td class="line x" title="39:252	Experimental resultsareshowninSection6, andweconclude in Section 7." ></td>
	<td class="line x" title="40:252	2 Related Work Sentiment analysis has been extensively studied in recent years." ></td>
	<td class="line x" title="41:252	The target of SA in this paper is wider than in previous work." ></td>
	<td class="line x" title="42:252	For example, Yu and Hatzivassiloglou (2003) separated facts from opinions and assigned polarities only to opinions." ></td>
	<td class="line x" title="43:252	In contrast, our system detects factual polar clauses as well as sentiments." ></td>
	<td class="line x" title="44:252	Unsupervised learning for sentiment analysis is also being studied." ></td>
	<td class="line x" title="45:252	For example, Hatzivassiloglou and McKeown (1997) labeled adjectives as positive or negative, relying on semantic orientation." ></td>
	<td class="line x" title="46:252	Turney (2002) used collocation with excellent or poor to obtain positive and negative clues for document classification." ></td>
	<td class="line x" title="47:252	In this paper, we use contextual information which is wider than for the contexts they used, and address the problem of acquiring lexical entries from the noisy clues." ></td>
	<td class="line oc" title="48:252	Inter-sentential contexts as in our approach were used as a clue also for subjectivity analysis (Riloff and Wiebe, 2003; Pang and Lee, 2004), which is two-fold classification into subjective and objective sentences." ></td>
	<td class="line x" title="49:252	Compared to it, this paper solves a more difficult problem: three-fold classification into positive, negative and non-polar expressions using imperfect coherency in terms of sentiment polarity." ></td>
	<td class="line x" title="50:252	Learningmethodsforphrase-levelsentiment analysis closely share an objective of our approach." ></td>
	<td class="line x" title="51:252	Popescu and Etzioni (2005) achieved high-precision opinion phrases extraction by using relaxation labeling." ></td>
	<td class="line x" title="52:252	Their method iteratively assigns a polarity to a phrase, relying on semantic orientation of co-occurring words in specific relations in a sentence, but the scope of semantic orientation is limited to within a sentence." ></td>
	<td class="line x" title="53:252	Wilson et al.(2005) proposed supervised learning, dividing the resources into 356 Document to analyze a45 Sentence Delimitation   Sentences a63Proposition Detection Propositions Clauses a63Polarity Assignment +  Polarities Polar Clauses Modality Patterns Conjunctive Patterns a42 Polar Atoms a45 Figure 1: The flow of the clause-level SA." ></td>
	<td class="line x" title="55:252	prior polarity and context polarity, which are similar to polar atoms and syntactic patterns in this paper, respectively." ></td>
	<td class="line x" title="56:252	Wilson et al. prepared prior polarities from existing resources, and learned the context polarities by using prior polarities and annotated corpora." ></td>
	<td class="line x" title="57:252	Therefore the prerequisite data and learned data are opposite from those in our approach." ></td>
	<td class="line x" title="58:252	We took the approach used in this paper because we want to acquire more domain-dependent knowledge, and context polarity is easier to access in Japanese2." ></td>
	<td class="line x" title="59:252	Our approach and their work can complement each other." ></td>
	<td class="line x" title="60:252	3 Methodology of Clause-level SA As Figure 1 illustrates, the flow of our sentiment analysis system involves three steps." ></td>
	<td class="line x" title="61:252	The first step is sentence delimitation: the input document is divided into sentences." ></td>
	<td class="line x" title="62:252	The second step is proposition detection: propositions which can form polar clauses are identifiedineachsentence." ></td>
	<td class="line x" title="63:252	Thethirdstepis polarity assignment: the polarity of each proposition is examined by considering the polar atoms." ></td>
	<td class="line x" title="64:252	This section describes the last two processes, which are based on a deep sentiment analysis method analogous to machine translation (Kanayama et al. , 2004) (hereafter the MT method)." ></td>
	<td class="line x" title="65:252	3.1 Proposition Detection Our basic tactic for clause-level SA is the highprecision detection of polar clauses based on deep syntactic analysis." ></td>
	<td class="line x" title="66:252	Clause-level means that only predicative verbs and adjectives such 2For example, indirect negation such as caused by a subject nobody or a modifier seldom is rare in Japanese." ></td>
	<td class="line x" title="67:252	as in (7) are detected, and adnominal (attributive) usages of verbs and adjectives as in (8) are ignored, because utsukushii (beautiful) in (8) does not convey a positive polarity." ></td>
	<td class="line x" title="68:252	(7) E-ga utsukushii." ></td>
	<td class="line x" title="69:252	The picture is beautiful. (8) Utsukushii hito-ni aitai." ></td>
	<td class="line x" title="70:252	I want to meet a beautiful person. Here we use the notion of a proposition as a clause without modality, led by a predicative verb or a predicative adjective." ></td>
	<td class="line x" title="71:252	The propositions detected from a sentence are subject to the assignment of polarities." ></td>
	<td class="line x" title="72:252	Basically, we detect a proposition only at the head of a syntactic tree3." ></td>
	<td class="line x" title="73:252	However, this limitation reduces the recall of sentiment analysis to a very low level." ></td>
	<td class="line x" title="74:252	In the example (7) above, utsukushii is the head of the tree, while those initial clauses in (9) to (11) below are not." ></td>
	<td class="line x" title="75:252	In order to achieve higher recall while maintaininghighprecision, weapplytwotypes of syntactic patterns, modality patterns and conjunctive patterns4, to the tree structures from the full-parsing." ></td>
	<td class="line x" title="76:252	(9) Sore-ha utsukushii-to omou." ></td>
	<td class="line x" title="77:252	I think it is beautiful. (10) Sore-ha utsukushiku-nai." ></td>
	<td class="line x" title="78:252	It is not beautiful. (11) Sore-ga utsukushii-to yoi." ></td>
	<td class="line x" title="79:252	I hope it is beautiful. Modality patterns match some auxiliary verbs or corresponding sentence-final expressions, to allow for specific kinds of modality and negation." ></td>
	<td class="line x" title="80:252	One of the typical patterns is [ v to omou] (I think v )5, which allows utsukushii in (9) to be a proposition." ></td>
	<td class="line x" title="81:252	Also negation is handled with a modality pattern, such as [ v nai] (not v )." ></td>
	<td class="line x" title="82:252	In this case a neg feature is attached to the proposition to identify utsukushii in (10) as a negated proposition." ></td>
	<td class="line x" title="83:252	On the other hand, no proposition is identified in (11) due to the deliberate absence of a pattern [ v to yoi] (I hope v )." ></td>
	<td class="line x" title="84:252	We used a total of 103 domain-independent modality patterns, most of which are derived from the 3This is same as the rightmost part of the sentence since all Japanese modification is directed left to right." ></td>
	<td class="line x" title="85:252	4These two types of patterns correspond to auxiliary patterns in the MT method, and can be applied independent of domains." ></td>
	<td class="line x" title="86:252	5 v denotes a verb or an adjective." ></td>
	<td class="line x" title="87:252	357 coordinative (roughly and) -te, -shi, -ueni, -dakedenaku, -nominarazu causal (roughly because) -tame, -kara, -node adversative (roughly but) -ga, -kedo, -keredo, monono, -nodaga Table 1: Japanese conjunctions used for conjunctive patterns." ></td>
	<td class="line x" title="88:252	MT method, and some patterns are manually added for this work to achieve higher recall." ></td>
	<td class="line x" title="89:252	Another type of pattern is conjunctive patterns, which allow multiple propositions in a sentence." ></td>
	<td class="line x" title="90:252	We used a total of 22 conjunctive patterns also derived from the MT method, as exemplified in Table 1." ></td>
	<td class="line x" title="91:252	In such cases of coordinative clauses and causal clauses, both clauses can be polar clauses." ></td>
	<td class="line x" title="92:252	On the other hand, no proposition is identified in a conditional clause due to the absence of corresponding conjunctive patterns." ></td>
	<td class="line x" title="93:252	3.2 Polarity Assignment Using Polar Atoms To assign a polarity to each proposition, polar atoms in the lexicon are compared to the proposition." ></td>
	<td class="line x" title="94:252	A polar atom consists of polarity, verb or adjective, and optionally, its arguments." ></td>
	<td class="line x" title="95:252	Example (12) is a simple polar atom, where no argument is specified." ></td>
	<td class="line x" title="96:252	This atom matches any proposition whose head is utsukushii." ></td>
	<td class="line x" title="97:252	Example (13) is a complex polar atom, which assigns a negative polarity to any proposition whose head is the verb kaku and where the accusative case is miryoku." ></td>
	<td class="line x" title="98:252	(12) [+] utsukushii to be beautiful (13) [] kaku  miryoku-wo to lack  attraction-ACC A polarity is assigned if there exists a polar atom for which verb/adjective and the arguments coincide with the proposition, and otherwise no polarity is assigned." ></td>
	<td class="line x" title="99:252	The opposite polarity of the polar atom is assigned to a proposition which has the neg feature." ></td>
	<td class="line x" title="100:252	We used a total of 3,275 polar atoms, most of which are derived from an English sentiment lexicon (Yi et al. , 2003)." ></td>
	<td class="line x" title="101:252	According to the evaluation of the MT method (Kanayama et al. , 2004), highprecision sentiment analysis had been achieved using the polar atoms and patterns, where the splendid light have-zoom small-LCD  satisfied  high-price a73a27 a9 Inter-sentential Context a54 a54 Intra-sentential Context Figure 2: The concept of the intraand intersentential contexts, where the polarities are perfectly coherent." ></td>
	<td class="line x" title="102:252	The symbol  denotes the existence of an adversative conjunction." ></td>
	<td class="line x" title="103:252	system never took positive sentiment for negative and vice versa, and judged positive or negative to neutral expressions in only about 10% cases." ></td>
	<td class="line x" title="104:252	However, the recall is too low, and most of the lexicon is for domain-independent expressions, and thus we need more lexical entries to grasp the positive and negative aspects in a specific domain." ></td>
	<td class="line x" title="105:252	4 Context Coherency This section introduces the intraand intersententialcontextsinwhichweassume context coherency for polarity, and describes some preliminary analysis of the assumption." ></td>
	<td class="line x" title="106:252	4.1 Intra-sentential and Inter-sentential Context The identification of propositions described in Section 3.1 clarifies our viewpoint of the contexts." ></td>
	<td class="line x" title="107:252	Here we consider two types of contexts: intra-sentential context and intersentential context." ></td>
	<td class="line x" title="108:252	Figure 2 illustrates the context coherency in a sample discourse (14), where the polarities are perfectly coherent." ></td>
	<td class="line x" title="109:252	(14) Kono kamera-ha subarashii-to omou." ></td>
	<td class="line x" title="110:252	I think this camera is splendid. Karui-shi, zuumu-mo tsuite-iru." ></td>
	<td class="line x" title="111:252	Its light and has a zoom lens. Ekishou-ga chiisai-kedo, manzoku-da." ></td>
	<td class="line x" title="112:252	Though the LCD is small, Im satisfied. Tada, nedan-ga chotto takai." ></td>
	<td class="line x" title="113:252	But, the price is a little high. The intra-sentential context is the link between propositions in a sentence, which are detected as coordinative or causal clauses." ></td>
	<td class="line x" title="114:252	If there is an adversative conjunction such as -kedo (but) in the third sentence in (14), a flag is attached to the relation, as denoted with  in Figure 2." ></td>
	<td class="line x" title="115:252	Though there are differences in syntactic phenomena, this is sim358 shikashi (however), demo (but), sorenanoni (even though), tadashi (on condition that), dakedo (but), gyakuni (on the contrary), tohaie (although), keredomo (however), ippou (on the other hand) Table 2: Inter-sentential adversative expressions." ></td>
	<td class="line x" title="116:252	Domain Post." ></td>
	<td class="line x" title="117:252	Sent." ></td>
	<td class="line x" title="118:252	Len." ></td>
	<td class="line x" title="119:252	digital cameras 263,934 1,757,917 28.3 movies 163,993 637,054 31.5 mobile phones 155,130 609,072 25.3 cars 159,135 959,831 30.9 Table 3: The corpora from four domains used in this paper." ></td>
	<td class="line x" title="120:252	The Post. and Sent. columns denote the numbers of postings and sentences, respectively." ></td>
	<td class="line x" title="121:252	Len. is the average length of sentences (in Japanese characters)." ></td>
	<td class="line x" title="122:252	ilar to the semantic orientation proposed by Hatzivassiloglou and McKeown (1997)." ></td>
	<td class="line x" title="123:252	The inter-sentential context is the link between propositions in the main clauses of pairs of adjacent sentences in a discourse." ></td>
	<td class="line x" title="124:252	The polarities are assumed to be the same in the inter-sentential context, unless there is an adversative expression as those listed in Table 2." ></td>
	<td class="line x" title="125:252	If no proposition is detected as in a nominal sentence, the context is split." ></td>
	<td class="line x" title="126:252	That is, there is no link between the proposition of the previous sentence and that of the next sentence." ></td>
	<td class="line x" title="127:252	4.2 Preliminary Study on Context Coherency We claim these two types of context can be used for unsupervised learning as clues to assign a tentative polarity to unknown expressions." ></td>
	<td class="line x" title="128:252	To validate our assumption, we conducted preliminary observations using various corpora." ></td>
	<td class="line x" title="129:252	4.2.1 Corpora Throughout this paper we used Japanese corpora from discussion boards in four different domains, whose features are shown in Table 3." ></td>
	<td class="line x" title="130:252	All of the corpora have clues to the boundaries of postings, so they were suitable to identify the discourses." ></td>
	<td class="line x" title="131:252	4.2.2 Coherent Precision How strong is the coherency in the context proposed in Section 4.1?" ></td>
	<td class="line x" title="132:252	Using the polar clauses detected by the SA system with the initial lexicon, we observed the coherent precision of domain d with lexicon L, defined as: cp(d,L) = #(Coherent)#(Coherent)+#(Conflict) (15) where #(Coherent) and #(Conflict) are occurrence counts of the same and opposite polarities observed between two polar clauses as observed in the discourse." ></td>
	<td class="line x" title="133:252	As the two polar clauses, we consider the following types: Window." ></td>
	<td class="line x" title="134:252	A polar clause and the nearest polar clause which is found in the preceding n sentences in the discourse." ></td>
	<td class="line x" title="135:252	Context." ></td>
	<td class="line x" title="136:252	Two polar clauses in the intrasentential and/or inter-sentential context described in Section 4.1." ></td>
	<td class="line x" title="137:252	This is the viewpoint of context in our method." ></td>
	<td class="line x" title="138:252	Table 4 shows the frequencies of coherent pairs, conflicting pairs, and the coherent precision for half of the digital camera domain corpus." ></td>
	<td class="line x" title="139:252	Baseline is the percentage of positive clauses among the polar clauses6." ></td>
	<td class="line x" title="140:252	For the Window method, we tested for n=0, 1, 2, and ." ></td>
	<td class="line x" title="141:252	0 means two propositions within a sentence." ></td>
	<td class="line x" title="142:252	Apparently, the larger the window size, the smaller the cp value." ></td>
	<td class="line x" title="143:252	When the window size is , implying anywhere within a discourse, the ratio is larger than the baseline by only 2.7%, and thus these types of coherency are not reliable even though the number of clues is relatively large." ></td>
	<td class="line x" title="144:252	Context shows the coherency of the two types of context that we considered." ></td>
	<td class="line x" title="145:252	The cp values are much higher than those in the Window methods, because the relationships between adjacent pairs of clauses are handled more appropriately by considering syntactic trees, adversative conjunctions, etc. The cp values for inter-sentential and intra-sentential contexts are almost the same, and thus both contexts can be used to obtain 2.5 times more clues for the intra-sentential context." ></td>
	<td class="line x" title="146:252	In the rest of this paper we will use both contexts." ></td>
	<td class="line x" title="147:252	We also observed the coherent precision for each domain corpus." ></td>
	<td class="line x" title="148:252	The results in the center column of Table 5 indicate the number is slightly different among corpora, but all of them are far from perfect coherency." ></td>
	<td class="line x" title="149:252	6Ifthereisapolarclausewhosepolarityisunknown, the polarity is correctly predicted with at least 57.0% precision by assuming positive." ></td>
	<td class="line x" title="150:252	359 Model Coherent Conflict cp(d,L) Baseline 57.0% Window n = 0 3,428 1,916 64.1% n = 1 11,448 6,865 62.5% n = 2 16,231 10,126 61.6% n =  26,365 17,831 59.7% Context intra." ></td>
	<td class="line x" title="151:252	2,583 996 72.2% inter." ></td>
	<td class="line x" title="152:252	3,987 1,533 72.2% both 6,570 2,529 72.2% Table 4: Coherent precision with various viewpoints of contexts." ></td>
	<td class="line x" title="153:252	Domain cp(d,L) cd(d,L) digital cameras 72.2% 7.23% movies 76.7% 18.71% mobile phones 72.9% 7.31% cars 73.4% 7.36% Table 5: Coherent precision and coherent density for each domain." ></td>
	<td class="line x" title="154:252	4.2.3 Coherent Density Besides the conflicting cases, there are many more cases where a polar clause does not appear in the polar context." ></td>
	<td class="line x" title="155:252	We also observed the coherent density of the domain d with the lexicon L defined as: cd(d,L) = #(Coherent)#(Polar) (16) This indicates the ratio of polar clauses that appear in the coherent context, among all of the polar clauses detected by the system." ></td>
	<td class="line x" title="156:252	The right column of Table 5 shows the coherent density in each domain." ></td>
	<td class="line x" title="157:252	The movie domain has notably higher coherent density than the others." ></td>
	<td class="line x" title="158:252	This indicates the sentiment expressions are more frequently used in the movie domain." ></td>
	<td class="line x" title="159:252	The next section describes the method of our unsupervised learning using this imperfect context coherency." ></td>
	<td class="line x" title="160:252	5 Unsupervised Learning for Acquisition of Polar Atoms Figure 3 shows the flow of our unsupervised learning method." ></td>
	<td class="line x" title="161:252	First, the runtime SA system identifies the polar clauses, and the candidate polar atoms are collected." ></td>
	<td class="line x" title="162:252	Then, each candidate atom is validated using the two metrics in the previous section, cp and cd, which are calculated from all of the polar clauses found in the domain corpus." ></td>
	<td class="line x" title="163:252	Domain Corpus d a45 Initial Lexicon L a42 SA a54 Polar Clauses contexta45 a18a85 Candidate Polar Atoms f(a),p(a),n(a) cd(d,L) cp(d,L) ?test a54 a82a9 a78 a63 ? testa45 a14 a45 a45 New Lexicon Figure 3: The flow of the learning process." ></td>
	<td class="line x" title="164:252	ID Candidate Polar Atom f(a) p(a) n(a) 1* chiisai to be small 3,014 226 227 2 shikkari-suru to be firm 246 54 10 3 chiisai  bodii-ga 11 4 0to be small  body-NOM 4* todoku  mokuyou-ni 2 0 2to be deliveredon Thursday Table 6: Examples of candidate polar atoms and their frequencies." ></td>
	<td class="line x" title="165:252	* denotes that it should not be added to the lexicon." ></td>
	<td class="line x" title="166:252	f(a), p(a), and n(a) denote the frequency of the atom and in positive and negative contexts, respectively." ></td>
	<td class="line x" title="167:252	5.1 Counts of Candidate Polar Atoms From each proposition which does not have a polarity, candidate polar atoms in the form of simple atoms (just a verb or adjective) or complex atoms (a verb or adjective and its rightmost argument consisting of a pair of a noun and a postpositional) are extracted." ></td>
	<td class="line x" title="168:252	For each candidate polar atom a, the total appearances f(a), and the occurrences in positive contexts p(a) and negative contexts n(a) are counted, based on the context of the adjacent clauses (using the method described in Section 4.1)." ></td>
	<td class="line x" title="169:252	If the proposition has the neg feature, the polarity is inverted." ></td>
	<td class="line x" title="170:252	Table 6 shows examples of candidate polar atoms with their frequencies." ></td>
	<td class="line x" title="171:252	5.2 Determination for Adding to Lexicon Among the located candidate polar atoms, how can we distinguish true polar atoms, which should be added to the lexicon, from fake polar atoms, which should be discarded?" ></td>
	<td class="line x" title="172:252	As shown in Section 4, both the coherent precision (72-77%) and the coherent density (7-19%) are so small that we cannot rely on each single appearance of the atom in the polar context." ></td>
	<td class="line x" title="173:252	One possible approach is to set the threshold values for frequency in a polar context, max(p(a),n(a)) and for the ratio of appearances in polar contexts among the to360 tal appearances, max(p(a),n(a))f(a) . However, the optimum threshold values should depend on the corpus and the initial lexicon." ></td>
	<td class="line x" title="174:252	In order to set general criteria, here we assume that a true positive polar atom a should have higher p(a)f(a) than its average i.e. coherent density, cd(d,L+a), and also have higher p(a) p(a)+n(a) than its average i.e. coherent precision, cp(d,L+a) and these criteria should be met with 90% confidence, where L+a is the initial lexicon with a added." ></td>
	<td class="line x" title="175:252	Assuming the binomial distribution, a candidate polar atom is adopted as a positive polar atom7 if both (17) and (18) are satisfied8." ></td>
	<td class="line x" title="176:252	q > cd(d,L), where p(a)summationdisplay k=0 f(a)Ckqk(1q)f(a)k = 0.9 (17) r > cp(d,L) or n(a) = 0, where p(a)summationdisplay k=0 p(a)+n(a)Ckrk(1r)p(a)+n(a)k= 0.9 (18) We can assume cd(d,L+a) similarequal cd(d,L), and cp(d,L+a) similarequal cp(d,L) when L is large." ></td>
	<td class="line x" title="177:252	We compute the confidence interval using approximation with the F-distribution (Blyth, 1986)." ></td>
	<td class="line x" title="178:252	These criteria solve the problems in minimum frequency and scope of the polar atoms simultaneously." ></td>
	<td class="line x" title="179:252	In the example of Table 6, the simple atom chiisai (ID=1) is discarded because it does not meet (18), while the complex atom chiisai  bodii-ga (ID=3) is adopted as a positive atom." ></td>
	<td class="line x" title="180:252	shikkari-suru (ID=2) is adopted as a positive simple atom, even though 10 cases out of 64 were observed in the negative context." ></td>
	<td class="line x" title="181:252	On the other hand, todoku  mokuyou-ni (ID=4) is discarded because it does not meet (17), even though n(a)f(a) = 1.0, i.e. always observed in negative contexts." ></td>
	<td class="line x" title="182:252	6 Evaluation 6.1 Evaluation by Polar Atoms First we propose a method of evaluation of the lexical learning." ></td>
	<td class="line x" title="183:252	7The criteria for the negative atoms are analogous." ></td>
	<td class="line x" title="184:252	8nCr notation is used here for combination (n choose k)." ></td>
	<td class="line x" title="185:252	Annotator B Positive Neutral Negative AnnoPositive 65 11 3 tator Neutral 3 72 0 A Negative 1 4 41 Table 7: Agreement of two annotators judgments of 200 polar atoms." ></td>
	<td class="line x" title="186:252	=0.83." ></td>
	<td class="line x" title="187:252	It is costly to make consistent and large gold standards in multiple domains, especially in identification tasks such as clauselevel SA (cf.classification tasks)." ></td>
	<td class="line x" title="189:252	Therefore we evaluated the learning results by asking human annotators to classify the acquired polar atoms as positive, negative, and neutral, instead of the instances of polar clauses detected with the new lexicon." ></td>
	<td class="line x" title="190:252	This can be done because the polar atoms themselves are informative enough to imply to humans whether the expressions hold positive or negative meanings in the domain." ></td>
	<td class="line x" title="191:252	To justify the reliability of this evaluation method, two annotators9 evaluated 200 randomly selected candidate polar atoms in the digital camera domain." ></td>
	<td class="line x" title="192:252	The agreement results are shown in Table 7." ></td>
	<td class="line x" title="193:252	The manual classification was agreed upon in 89% of the cases and the Kappa value was 0.83, which is high enough to be considered consistent." ></td>
	<td class="line x" title="194:252	Using manual judgment of the polar atoms, we evaluated the performance with the following three metrics." ></td>
	<td class="line x" title="195:252	Type Precision." ></td>
	<td class="line x" title="196:252	The coincidence rate of the polarity between the acquired polar atom and the human evaluators judgments." ></td>
	<td class="line x" title="197:252	It is always false if the evaluators judged it as neutral. Token Precision." ></td>
	<td class="line x" title="198:252	The coincidence rate of the polarity, weighted by its frequency in the corpus." ></td>
	<td class="line x" title="199:252	This metric emulates the precision of the detection of polar clauses with newly acquired poler atoms, in the runtime SA system." ></td>
	<td class="line x" title="200:252	Relative Recall." ></td>
	<td class="line x" title="201:252	The estimated ratio of the number of detected polar clauses with the expanded lexicon to the number of detected polar clauses with the initial lex9For each domain, we asked different annotators who are familiar with the domain." ></td>
	<td class="line x" title="202:252	They are not the authors of this paper." ></td>
	<td class="line x" title="203:252	361 Domain # Type Token RelativePrec." ></td>
	<td class="line x" title="204:252	Prec." ></td>
	<td class="line x" title="205:252	Recall digital cameras 708 65% 96.5% 1.28 movies 462 75% 94.4% 1.19 mobile phones 228 54% 92.1% 1.13 cars 487 68% 91.5% 1.18 Table 8: Evaluation results with our method." ></td>
	<td class="line x" title="206:252	The column # denotes the number of polar atoms acquired in each domain." ></td>
	<td class="line x" title="207:252	icon." ></td>
	<td class="line x" title="208:252	Relative recall will be 1 when no newpolaratomisacquired." ></td>
	<td class="line x" title="209:252	Sincetheprecision was high enough, this metric can be used for approximation of the recall, which is hard to evaluate in extraction tasks such as clause-/phrase-level SA." ></td>
	<td class="line x" title="210:252	6.2 Robustness for Different Conditions 6.2.1 Diversity of Corpora For each of the four domain corpora, the annotators evaluated 100 randomly selected polar atoms which were newly acquired by our method, to measure the precisions." ></td>
	<td class="line x" title="211:252	Relative recall is estimated by comparing the numbers of detected polar clauses from randomly selected 2,000 sentences, with and without the acquired polar atoms." ></td>
	<td class="line x" title="212:252	Table 8 shows the results." ></td>
	<td class="line x" title="213:252	The token precision is higher than 90% in all of the corpora, including the movie domain, which is considered to be difficult for SA (Turney, 2002)." ></td>
	<td class="line x" title="214:252	This is extremely high precision for this task, because the correctness of both the extraction and polarity assignment was evaluated simultaneously." ></td>
	<td class="line x" title="215:252	The relative recall 1.28 in the digital camera domain means the recall is increased from 43%10 to 55%." ></td>
	<td class="line x" title="216:252	The difference was smaller in other domains, but the domain-dependent polar clauses are much informative than general ones, thus the highprecision detection significantly enhances the system." ></td>
	<td class="line x" title="217:252	To see the effects of our method, we conducted a control experiment which used preset criteria." ></td>
	<td class="line x" title="218:252	To adopt the candidate atom a, the frequency of polarity, max(p(a),n(a)) was required to be 3 or more, and the ratio of polarity, max(p(a),n(a))f(a) was required to be higher than the threshold ." ></td>
	<td class="line x" title="219:252	Varying  from 0.05 to 10The human evaluation result for digital camera domain (Kanayama et al. , 2004)." ></td>
	<td class="line x" title="220:252	a54  a45 Relative recall Token precision 0.5 1 1.0 1.1 1.2 star star  = 0.05 star  = 0.1starstar = 0.3starstarstar star star = 0.8 star stardigital cameras    = 0.05 = 0.1    = 0.3     movies (our method) a14a89 Figure 4: Relative recall vs. token precision with various preset threshold values  for the digital camera and movie domains." ></td>
	<td class="line x" title="221:252	The rightmost star and circle denote the performance of our method." ></td>
	<td class="line x" title="222:252	0.8, we evaluated the token precision and the relative recall in the domains of digital cameras and movies." ></td>
	<td class="line x" title="223:252	Figure 4 shows the results." ></td>
	<td class="line x" title="224:252	The results showed both relative recall and token precision were lower than in our method for every , in both corpora." ></td>
	<td class="line x" title="225:252	The optimum  was 0.3 in the movie domain and 0.1 in the digital camera domain." ></td>
	<td class="line x" title="226:252	Therefore, in this preset approach, a tuning process is necessary for each domain." ></td>
	<td class="line x" title="227:252	Our method does not require this tuning, and thus fully automatic learning was possible." ></td>
	<td class="line x" title="228:252	Unlike the normal precision-recall tradeoff, the token precision in the movie domain got lower when the  is strict." ></td>
	<td class="line x" title="229:252	This is due to the frequent polar atoms which can be acquired at the low ratios of the polarity." ></td>
	<td class="line x" title="230:252	Our method does not discard these important polar atoms." ></td>
	<td class="line x" title="231:252	6.2.2 Size of the Initial Lexicon We also tested the performance while varying the size of the initial lexicon L. We prepared three subsets of the initial lexicon, L0.8, L0.5, and L0.2, removing polar atoms randomly." ></td>
	<td class="line x" title="232:252	These lexicons had 0.8, 0.5, 0.2 times the polar atoms, respectively, compared to L. Table 9 shows the precisions and recalls using these lexicons for the learning process." ></td>
	<td class="line x" title="233:252	Though the cd values vary, the precision was stable, which means that our method was robust even for different sizes of the lexicon." ></td>
	<td class="line x" title="234:252	The smaller the initial lexicon, the higher the relative recall, because the polar atoms which were removed from L were recovered in the learning process." ></td>
	<td class="line x" title="235:252	This result suggests the possibility of 362 lexicon cd Token Prec." ></td>
	<td class="line x" title="236:252	Relative Rec." ></td>
	<td class="line x" title="237:252	L 7.2% 96.5% 1.28 L0.8 6.1% 97.5% 1.41 L0.5 3.9% 94.2% 2.10 L0.2 3.6% 84.8% 3.55 Table 9: Evaluation results for various sizes of the initial lexicon (the digital camera domain)." ></td>
	<td class="line x" title="238:252	the bootstrapping method from a small initial lexicon." ></td>
	<td class="line x" title="239:252	6.3 Qualitative Evaluation As seen in the agreement study, the polar atoms used in our study were intrinsically meaningful to humans." ></td>
	<td class="line x" title="240:252	This is because the atoms are predicate-argument structures derived from predicative clauses, and thus humans could imagine the meaning of a polar atom by generating the corresponding sentence in its predicative form." ></td>
	<td class="line x" title="241:252	In the evaluation process, some interesting results were observed." ></td>
	<td class="line x" title="242:252	For example, a negative atom nai  kerare-ga (to be free from vignetting) was acquired in the digital camera domain." ></td>
	<td class="line x" title="243:252	Even the evaluator who was familiar with digital cameras did not know the term kerare (vignetting), but after looking up the dictionary she labeled it as negative." ></td>
	<td class="line x" title="244:252	Our learning method could pick up such technical terms and labeled them appropriately." ></td>
	<td class="line x" title="245:252	Also, there were discoveries in the error analysis." ></td>
	<td class="line x" title="246:252	An evaluator assigned positive to aru  kamera-ga (to have camera) in the mobile phone domain, but the acquired polar atom had the negative polarity." ></td>
	<td class="line x" title="247:252	This was actually an insight from the recent opinions that many userswantphoneswithoutcamerafunctions11." ></td>
	<td class="line x" title="248:252	7 Conclusion We proposed an unsupervised method to acquire polar atoms for domain-oriented SA, and demonstrated its high performance." ></td>
	<td class="line x" title="249:252	The lexicon can be expanded automatically by using unannotated corpora, and tuning of the threshold values is not required." ></td>
	<td class="line x" title="250:252	Therefore even end-users can use this approach to improve the sentiment analysis." ></td>
	<td class="line x" title="251:252	These features allow them to do on-demand analysis of more narrow domains, such as the domain of digital 11Perhaps because cameras tend to consume battery power and some users dont need them." ></td>
	<td class="line x" title="252:252	cameras of a specific manufacturer, or the domain of mobile phones from the female users point of view." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-1652
Feature Subsumption For Opinion Analysis
Riloff, Ellen;Patwardhan, Siddharth;Wiebe, Janyce M.;"></td>
	<td class="line x" title="1:189	Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 440448, Sydney, July 2006." ></td>
	<td class="line x" title="2:189	c2006 Association for Computational Linguistics Feature Subsumption for Opinion Analysis Ellen Riloff and Siddharth Patwardhan School of Computing University of Utah Salt Lake City, UT 84112 {riloff,sidd}@cs.utah.edu Janyce Wiebe Department of Computer Science University of Pittsburgh Pittsburgh, PA 15260 wiebe@cs.pitt.edu Abstract Lexical features are key to many approaches to sentiment analysis and opinion detection." ></td>
	<td class="line x" title="3:189	A variety of representations have been used, including single words, multi-word Ngrams, phrases, and lexicosyntactic patterns." ></td>
	<td class="line x" title="4:189	In this paper, we use a subsumption hierarchy to formally de ne different types of lexical features and their relationship to one another, both in terms of representational coverage and performance." ></td>
	<td class="line x" title="5:189	We use the subsumption hierarchy in two ways: (1) as an analytic tool to automatically identify complex features that outperform simpler features, and (2) to reduce a feature set by removing unnecessary features." ></td>
	<td class="line x" title="6:189	We show that reducing the feature set improves performance on three opinion classi cation tasks, especially when combined with traditional feature selection." ></td>
	<td class="line x" title="7:189	1 Introduction Sentiment analysis and opinion recognition are active research areas that have many potential applications, including review mining, product reputation analysis, multi-document summarization, and multi-perspective question answering." ></td>
	<td class="line x" title="8:189	Lexical features are key to many approaches, and a variety of representations have been used, including single words, multi-word Ngrams, phrases, and lexico-syntactic patterns." ></td>
	<td class="line x" title="9:189	It is common for different features to overlap representationally." ></td>
	<td class="line x" title="10:189	For example, the unigram happy will match all of the texts that the bigram very happy matches." ></td>
	<td class="line x" title="11:189	Since both features represent a positive sentiment and the bigram matches fewer contexts than the unigram, it is probably suf cient just to have the unigram." ></td>
	<td class="line x" title="12:189	However, there are many cases where a feature captures a subtlety or non-compositional meaning that a simpler feature does not." ></td>
	<td class="line x" title="13:189	For example, basket case is a highly opinionated phrase, but the words basket and case individually are not." ></td>
	<td class="line x" title="14:189	An open question in opinion analysis is how often more complex feature representations are needed, and which types of features are most valuable." ></td>
	<td class="line x" title="15:189	Our rst goal is to devise a method to automatically identify features that are representationally subsumed by a simpler feature but that are better opinion indicators." ></td>
	<td class="line x" title="16:189	These subjective expressions could then be added to a subjectivity lexicon (Esuli and Sebastiani, 2005), and used to gain understanding about which types of complex features capture meaningful expressions that are important for opinion recognition." ></td>
	<td class="line x" title="17:189	Many opinion classi ers are created by adopting a kitchen sink approach that throws together a variety of features." ></td>
	<td class="line x" title="18:189	But in many cases adding new types of features does not improve performance." ></td>
	<td class="line x" title="19:189	For example, Pang et al.(2002) found that unigrams outperformed bigrams, and unigrams outperformed the combination of unigrams plus bigrams." ></td>
	<td class="line x" title="21:189	Our second goal is to automatically identify features that are unnecessary because similar features provide equal or better coverage and discriminatory value." ></td>
	<td class="line x" title="22:189	Our hypothesis is that a reduced feature set, which selectively combines unigrams with only the most valuable complex features, will perform better than a larger feature set that includes the entire kitchen sink of features." ></td>
	<td class="line x" title="23:189	In this paper, we explore the use of a subsumption hierarchy to formally de ne the subsumption relationships between different types of textual features." ></td>
	<td class="line x" title="24:189	We use the subsumption hierarchy in two ways." ></td>
	<td class="line x" title="25:189	First, we use subsumption as an an440 alytic tool to compare features of different complexities and automatically identify complex features that substantially outperform their simpler counterparts." ></td>
	<td class="line x" title="26:189	Second, we use the subsumption hierarchy to reduce a feature set based on representational overlap and on performance." ></td>
	<td class="line x" title="27:189	We conduct experiments with three opinion data sets and show that the reduced feature sets can improve classi cation performance." ></td>
	<td class="line x" title="28:189	2 The Subsumption Hierarchy 2.1 Text Representations We analyze two feature representations that have been used for opinion analysis: Ngrams and Extraction Patterns." ></td>
	<td class="line x" title="29:189	Information extraction (IE) patterns are lexico-syntactic patterns that represent expressions which identify role relationships." ></td>
	<td class="line x" title="30:189	For example, the pattern <subj> ActVP(recommended) extracts the subject of active-voice instances of the verb recommended as the recommender." ></td>
	<td class="line x" title="31:189	The pattern <subj> PassVP(recommended) extracts the subject of passive-voice instances of recommended as the object being recommended." ></td>
	<td class="line x" title="32:189	(Riloff and Wiebe, 2003) explored the idea of using extraction patterns to represent more complex subjective expressions that have noncompositional meanings." ></td>
	<td class="line x" title="33:189	For example, the expression drive (someone) up the wall expresses the feeling of being annoyed, but the meanings of the words drive, up, and wall have no emotional connotations individually." ></td>
	<td class="line x" title="34:189	Furthermore, this expression is not a xed word sequence that can be adequately modeled by Ngrams." ></td>
	<td class="line x" title="35:189	Any noun phrase can appear between the words drive and up, so a exible representation is needed to capture the general pattern drives <NP> up the wall." ></td>
	<td class="line x" title="36:189	This example represents a general phenomenon: many expressions allow intervening noun phrases and/or modifying terms." ></td>
	<td class="line x" title="37:189	For example: stepped on <mods> toes Ex: stepped on the boss toes dealt <np> <mods> blow Ex: dealt the company a decisive blow brought <np> to <mods> knees Ex: brought the man to his knees (Riloff and Wiebe, 2003) also showed that syntactic variations of the same verb phrase can behave very differently." ></td>
	<td class="line x" title="38:189	For example, they found that passive-voice constructions of the verb ask had a 100% correlation with opinion sentences, but active-voice constructions had only a 63% correlation with opinions." ></td>
	<td class="line x" title="39:189	Pattern Type Example Pattern <subj> PassVP <subj> is satis ed <subj> ActVP <subj> complained <subj> ActVP Dobj <subj> dealt blow <subj> ActInfVP <subj> appear to be <subj> PassInfVP <subj> is meant to be <subj> AuxVP Dobj <subj> has position <subj> AuxVP Adj <subj> is happy ActVP <dobj> endorsed <dobj> InfVP <dobj> to condemn <dobj> ActInfVP <dobj> get to know <dobj> PassInfVP <dobj> is meant to be <dobj> Subj AuxVP <dobj> fact is <dobj> NP Prep <np> opinion on <np> ActVP Prep <np> agrees with <np> PassVP Prep <np> is worried about <np> InfVP Prep <np> to resort to <np> <possessive> NP <noun>s speech Figure 1: Extraction Pattern Types Our goal is to use the subsumption hierarchy to identify Ngram and extraction pattern features that are more strongly associated with opinions than simpler features." ></td>
	<td class="line x" title="40:189	We used three types of features in our research: unigrams, bigrams, and IE patterns." ></td>
	<td class="line x" title="41:189	The Ngram features were generated using the Ngram Statistics Package (NSP) (Banerjee and Pedersen, 2003).1 The extraction patterns (EPs) were automatically generated using the Sundance/AutoSlog software package (Riloff and Phillips, 2004)." ></td>
	<td class="line x" title="42:189	AutoSlog relies on the Sundance shallow parser and can be applied exhaustively to a text corpus to generate IE patterns that can extract every noun phrase in the corpus." ></td>
	<td class="line x" title="43:189	AutoSlog has been used to learn IE patterns for the domains of terrorism, joint ventures, and microelectronics (Riloff, 1996), as well as for opinion analysis (Riloff and Wiebe, 2003)." ></td>
	<td class="line x" title="44:189	Figure 1 shows the 17 types of extraction patterns that AutoSlog generates." ></td>
	<td class="line x" title="45:189	PassVP refers to passive-voice verb phrases (VPs), ActVP refers to active-voice VPs, InfVP refers to in nitive VPs, and AuxVP refers 1NSP is freely available for use under the GPL from http://search.cpan.org/dist/Text-NSP." ></td>
	<td class="line x" title="46:189	We discarded Ngrams that consisted entirely of stopwords." ></td>
	<td class="line x" title="47:189	We used a list of 281 stopwords." ></td>
	<td class="line x" title="48:189	441 to VPs where the main verb is a form of to be or to have . Subjects (subj), direct objects (dobj), PP objects (np), and possessives can be extracted by the patterns.2 2.2 The Subsumption Hierarchy We created a subsumption hierarchy that de nes the representational scope of different types of features." ></td>
	<td class="line x" title="49:189	We will say that feature A representationally subsumes feature B if the set of text spans that match feature A is a superset of the set of text spans that match feature B. For example, the unigram happy subsumes the bigram very happy because the set of text spans that match happy includes the text spans that match very happy . First, we de ne a hierarchy of valid subsumption relationships, shown in Figure 2." ></td>
	<td class="line x" title="50:189	The 2Gram node, for example, is a child of the 1Gram node because a 1Gram can subsume a 2Gram." ></td>
	<td class="line x" title="51:189	Ngrams may subsume extraction patterns as well." ></td>
	<td class="line x" title="52:189	Every extraction pattern has at least one corresponding 1Gram that will subsume it.3." ></td>
	<td class="line x" title="53:189	For example, the 1Gram recommended subsumes the pattern <subj> ActVP(recommended) because the pattern only matches active-voice instances of recommended . An extraction pattern may also subsume another extraction pattern." ></td>
	<td class="line x" title="54:189	For example, <subj> ActVP(recommended) subsumes <subj> ActVP(recommended) Dobj(movie) . To compare speci c features we need to formally de ne the representation of each type of feature in the hierarchy." ></td>
	<td class="line x" title="55:189	For example, the hierarchy dictates that a 2Gram can subsume the pattern ActInfVP <dobj>, but this should hold only if the words in the bigram correspond to adjacent words in the pattern." ></td>
	<td class="line x" title="56:189	For example, the 2Gram to sh subsumes the pattern ActInfVP(like to sh) <dobj> . But the 2Gram like sh should not subsume it." ></td>
	<td class="line x" title="57:189	Similarly, consider the pattern InfVP(plan) <dobj>, which represents the in nitive to plan . This pattern subsumes the pattern ActInfVP(want to plan) <dobj>, but it should not subsume the pattern ActInfVP(plan to start) . To ensure that different features truly subsume each other representationally, we formally de ne each type of feature based on words, sequential 2However, the items extracted by the patterns are not actually used by our opinion classi ers; only the patterns themselves are matched against the text." ></td>
	<td class="line x" title="58:189	3Because every type of extraction pattern shown in Figure 1 contains at least one word (not including the extracted phrases, which are not used as part of our feature representation)." ></td>
	<td class="line x" title="59:189	dependencies, and syntactic dependencies." ></td>
	<td class="line x" title="60:189	A sequential dependency between words wi and wi+1 means that wi and wi+1 must be adjacent, and that wi must precede wi+1." ></td>
	<td class="line x" title="61:189	Figure 3 shows the formal de nition of a bigram (2Gram) node." ></td>
	<td class="line x" title="62:189	The bigram is de ned as two words with a sequential dependency indicating that they must be adjacent." ></td>
	<td class="line x" title="63:189	Name = 2Gram Constituent[0] = WORD1 Constituent[1] = WORD2 Dependency = Sequential(0, 1) Figure 3: 2Gram De nition A syntactic dependency between words wi and wi+1 means that wi has a speci c syntactic relationship to wi+1, and wi must precede wi+1." ></td>
	<td class="line x" title="64:189	For example, consider the extraction pattern NP Prep <np>, in which the object of the preposition attaches to the NP." ></td>
	<td class="line x" title="65:189	Figure 4 shows the de nition of this extraction pattern in the hierarchy." ></td>
	<td class="line x" title="66:189	The pattern itself contains three components: the NP, the attaching preposition, and the object of the preposition (which is the NP that the pattern extracts)." ></td>
	<td class="line x" title="67:189	The de nition also includes two syntactic dependencies: the rst dependency is between the NP and the preposition (meaning that the preposition syntactically attaches to the NP), while the second dependency is between the preposition and the extraction (meaning that the extracted NP is the syntactic object of the preposition)." ></td>
	<td class="line x" title="68:189	Name = NP Prep <np> Constituent[0] = NP Constituent[1] = PREP Constituent[2] = NP EXTRACTION Dependency = Syntactic(0, 1) Dependency = Syntactic(1, 2) Figure 4: NP Prep <np> Pattern De nition Consequently, the bigram affair with will not subsume the extraction pattern affair with <np> because the bigram requires the noun and preposition to be adjacent but the pattern does not." ></td>
	<td class="line x" title="69:189	For example, the extraction pattern matches the text an affair in his mind with Countess Olenska but the bigram does not." ></td>
	<td class="line x" title="70:189	Conversely, the extraction pattern does not subsume the bigram either because the pattern requires syntactic attachment but the bigram does not." ></td>
	<td class="line x" title="71:189	For example, the bigram matches 442 <subj> ActVP <subj> ActInfVP <subj> ActVP Dobj <subj> PassVP <subj> PassInfVP InfVP <dobj> ActInfVP <dobj> PassInfVP <dobj> 1Gram 2Gram <possessive> NP <subj> AuxVP AdjP <subj> AuxVP Dobj ActVP <dobj> ActVP Prep <np> NP Prep <np> PassVP Prep <np> Subj AuxVP <dobj> 3Gram ActVP Prep:OF <np> InfVP Prep <np> NP Prep:OF <np> PassVP Prep:OF <np> 4Gram InfVP Prep:OF <np> Figure 2: The Subsumption Hierarchy the sentence He ended the affair with a sense of relief, but the extraction pattern does not." ></td>
	<td class="line x" title="72:189	Figure 5 shows the de nition of another extraction pattern, InfVP <dobj>, which includes both syntactic and sequential dependencies." ></td>
	<td class="line x" title="73:189	This pattern would match the text to protest high taxes . The pattern de nition has three components: the in nitive to, a verb, and the direct object of the verb (which is the NP that the pattern extracts)." ></td>
	<td class="line x" title="74:189	The de nition also shows two syntactic dependencies." ></td>
	<td class="line x" title="75:189	The rst dependency indicates that the verb syntactically attaches to the in nitive to . The second dependency indicates that the extracted NP syntactically attaches to the verb (i.e. , it is the direct object of that particular verb)." ></td>
	<td class="line x" title="76:189	The pattern de nition also includes a sequential dependency, which speci es that to must be adjacent to the verb." ></td>
	<td class="line x" title="77:189	Strictly speaking, our parser does not require them to be adjacent." ></td>
	<td class="line x" title="78:189	For example, the parser allows intervening adverbs to split in nitives (e.g. , to strongly protest high taxes ), and this does happen occasionally." ></td>
	<td class="line x" title="79:189	But split innitives are relatively rare, so in the vast majority of cases the in nitive to will be adjacent to the verb." ></td>
	<td class="line x" title="80:189	Consequently, we decided that a bigram (e.g. , to protest ) should representationally subsume this extraction pattern because the syntactic exibility afforded by the pattern is negligible." ></td>
	<td class="line x" title="81:189	The sequential dependency link represents this judgment call that the in nitive to and the verb are adjacent in most cases." ></td>
	<td class="line x" title="82:189	For all of the node de nitions, we used our best judgment to make decisions of this kind." ></td>
	<td class="line x" title="83:189	We tried to represent major distinctions between features, without getting caught up in minor differences that were likely to be negligible in practice." ></td>
	<td class="line x" title="84:189	Name = InfVP <dobj> Constituent[0] = INFINITIVE TO Constituent[1] = VERB Constituent[2] = DOBJ EXTRACTION Dependency = Syntactic(0, 1) Dependency = Syntactic(1, 2) Dependency = Sequential(0, 1) Figure 5: InfVP <dobj> Pattern De nition To use the subsumption hierarchy, we assign each feature to its appropriate node in the hierarchy based on its type." ></td>
	<td class="line x" title="85:189	Then we perform a topdown breadthrst traversal." ></td>
	<td class="line x" title="86:189	Each feature is compared with the features at its ancestor nodes." ></td>
	<td class="line x" title="87:189	If a features words and dependencies are a superset of an ancestors words and dependencies, then it is subsumed by the (more general) ancestor and discarded.4 When the subsumption process is nished, a feature remains in the hierarchy only if 4The words that they have in common must also be in the same relative order." ></td>
	<td class="line x" title="88:189	443 there are no features above it that subsume it." ></td>
	<td class="line x" title="89:189	2.3 Performance-based Subsumption Representational subsumption is concerned with whether one feature is more general than another." ></td>
	<td class="line x" title="90:189	But the purpose of using the subsumption hierarchy is to identify more complex features that outperform simpler ones." ></td>
	<td class="line x" title="91:189	Applying the subsumption hierarchy to features without regard to performance would simply eliminate all features that have a more general counterpart in the feature set." ></td>
	<td class="line x" title="92:189	For example, all bigrams would be discarded if their component unigrams were also present in the hierarchy." ></td>
	<td class="line x" title="93:189	To estimate the quality of a feature, we use Information Gain (IG) because that has been shown to work well as a metric for feature selection (Forman, 2003)." ></td>
	<td class="line x" title="94:189	We will say that feature A behaviorally subsumes feature B if two criteria are met: (1) A representationally subsumes B, and (2) IG(A) IG(B) , where  is a parameter representing an acceptable margin of performance difference." ></td>
	<td class="line x" title="95:189	For example, if =0 then condition (2) means that feature A is just as valuable as feature B because its information gain is the same or higher." ></td>
	<td class="line x" title="96:189	If >0 then feature A is allowed to be a little worse than feature B, but within an acceptable margin." ></td>
	<td class="line x" title="97:189	For example, =.0001 means that As information gain may be up to .0001 lower than Bs information gain, and that is considered to be an acceptable performance difference (i.e. , A is good enough that we are comfortable discarding B in favor of the more general feature A)." ></td>
	<td class="line x" title="98:189	Note that based on the subsumption hierarchy shown in Figure 2, all 1Grams will always survive the subsumption process because they cannot be subsumed by any other types of features." ></td>
	<td class="line x" title="99:189	Our goal is to identify complex features that are worth adding to a set of unigram features." ></td>
	<td class="line oc" title="100:189	3 Data Sets We used three opinion-related data sets for our analyses and experiments: the OP data set created by (Wiebe et al. , 2004), the Polarity data set5 created by (Pang and Lee, 2004), and the MPQA data set created by (Wiebe et al. , 2005).6 The OP and Polarity data sets involve document-level opinion classi cation, while the MPQA data set involves 5Version v2.0, which is available at: http://www.cs.cornell.edu/people/pabo/movie-review-data/ 6Available at http://www.cs.pitt.edu/mpqa/databaserelease/ sentence-level classi cation." ></td>
	<td class="line x" title="101:189	The OP data consists of 2,452 documents from the Penn Treebank (Marcus et al. , 1993)." ></td>
	<td class="line x" title="102:189	Metadata tags assigned by the Wall Street Journal de ne the opinion/non-opinion classes: the class of any document labeled Editorial, Letter to the Editor, Arts & Leisure Review, or Viewpoint by the Wall Street Journal is opinion, and the class of documents in all other categories (such as Business and News) is non-opinion." ></td>
	<td class="line x" title="103:189	This data set is highly skewed, with only 9% of the documents belonging to the opinion class." ></td>
	<td class="line x" title="104:189	Consequently, a trivial (but useless) opinion classi er that labels all documents as nonopinion articles would achieve 91% accuracy." ></td>
	<td class="line o" title="105:189	The Polarity data consists of 700 positive and 700 negative reviews from the Internet Movie Database (IMDb) archive." ></td>
	<td class="line x" title="106:189	The positive and negative classes were derived from author ratings expressed in stars or numerical values." ></td>
	<td class="line x" title="107:189	The MPQA data consists of English language versions of articles from the world press." ></td>
	<td class="line x" title="108:189	It contains 9,732 sentences that have been manually annotated for subjective expressions." ></td>
	<td class="line x" title="109:189	The opinion/non-opinion classes are derived from the lower-level annotations: a sentence is an opinion if it contains a subjective expression of medium or higher intensity; otherwise, it is a non-opinion sentence." ></td>
	<td class="line x" title="110:189	55% of the sentences belong to the opinion class." ></td>
	<td class="line x" title="111:189	4 Using the Subsumption Hierarchy for Analysis In this section, we illustrate how the subsumption hierarchy can be used as an analytic tool to automatically identify features that substantially outperform simpler counterparts." ></td>
	<td class="line x" title="112:189	These features represent specialized usages and expressions that would be good candidates for addition to a subjectivity lexicon." ></td>
	<td class="line x" title="113:189	Figure 6 shows pairs of features, where the rst is more general and the second is more speci c. These feature pairs were identi ed by the subsumption hierarchy as being representationally similar but behaviorally different (so the more speci c feature was retained)." ></td>
	<td class="line x" title="114:189	The IGain column shows the information gain values produced from the training set of one cross-validation fold." ></td>
	<td class="line x" title="115:189	The Class column shows the class that the more speci c feature is correlated with (the more general feature is usually not strongly correlated with either class)." ></td>
	<td class="line x" title="116:189	The top table in Figure 6 contains examples for the opinion/non-opinion classi cation task from 444 Opinion/Non-Opinion Classi cation ID Feature IGain Class Example A1 line .0016 . . ." ></td>
	<td class="line x" title="117:189	issue consists of notes backed by credit line receivables A2 the line .0075 opin lays it on the line; steps across the line B1 nation .0046 . . ." ></td>
	<td class="line x" title="118:189	has 750,000 cable-tv subscribers around the nation B2 a nation .0080 opin Its not that we are spawning a nation of ascetics . . ." ></td>
	<td class="line x" title="119:189	C1 begin .0006 Campeau buyers will begin writing orders C2 begin with .0036 opin To begin with, we should note that in contrast D1 bene ts .0040 . . ." ></td>
	<td class="line x" title="120:189	earlier period included $235,000 in tax bene ts." ></td>
	<td class="line x" title="121:189	DEP NP Prep(bene ts to) .0090 opin . . ." ></td>
	<td class="line x" title="122:189	boon to the rich with no proven bene ts to the economy E1 due .0001 . . ." ></td>
	<td class="line x" title="123:189	an estimated $ 1.23 billion in debt due next spring EEP ActVP Prep(due to) .0038 opin Its all due to the intense scrutiny Positive/Negative Sentiment Classi cation ID Feature IGain Class Example F1 short .0014 to make a long story short F2 nothing short .0039 pos nothing short of spectacular G1 ugly .0008 an ugly monster on a cruise liner G2 and ugly .0054 neg its a disappointment to see something this dumb and ugly H1 disaster .0010 rated pg-13 for disaster related elements HEP AuxVP Dobj(be disaster) .0048 neg . . ." ></td>
	<td class="line x" title="124:189	this is such a confused disaster of a lm I1 work .0002 the next day during the drive to work IEP ActVP(work) .0062 pos the lm will work just as well J1 manages .0003 he still manages to nd time for his wife JEP ActInfVP(manages to keep) .0054 pos this lm manages to keep up a rapid pace Figure 6: Sample features that behave differently, as revealed by the subsumption hierarchy." ></td>
	<td class="line x" title="125:189	(1 ) unigram; 2 ) bigram; EP ) extraction pattern) the OP data." ></td>
	<td class="line x" title="126:189	The more speci c features are more strongly correlated with opinion articles." ></td>
	<td class="line x" title="127:189	Surprisingly, simply adding a determiner can dramatically change behavior." ></td>
	<td class="line x" title="128:189	Consider A2." ></td>
	<td class="line x" title="129:189	There are many subjective idioms involving the line (two are shown in the table; others include toe the line and draw the line ), while objective language about credit lines, phone lines, etc. uses the determiner less often." ></td>
	<td class="line x" title="130:189	Similarly, consider B2." ></td>
	<td class="line x" title="131:189	Adding a to nation often corresponds to an abstract reference used when making an argument (e.g. , a nation of ascetics ), whereas other instances of nation are used more literally (e.g. , the 6th largest in the nation )." ></td>
	<td class="line x" title="132:189	21% of feature B1s instances appear in opinion articles, while 70% of feature B2s instances are in opinion articles." ></td>
	<td class="line x" title="133:189	Begin with (C2) captures an adverbial phrase used in argumentation ( To begin with ) but does not match objective usages such as will begin an action." ></td>
	<td class="line x" title="134:189	The word bene ts alone (D1) matches phrases like tax bene ts and employee bene ts that are not opinion expressions, while DEP typically matches positive senses of the word bene ts . Interestingly, the bigram bene ts to is not highly correlated with opinions because it matches in nitive phrases such as tax bene ts to provide and health bene ts to cut . In this case, the extraction pattern NP Prep(bene ts to) is more discriminating than the bigram for opinion classi cation." ></td>
	<td class="line x" title="135:189	The extraction pattern EEP is also highly correlated with opinions, while the unigram due and the bigram due to are not." ></td>
	<td class="line o" title="136:189	The bottom table in Figure 6 shows feature pairs identi ed for their behavioral differences on the Polarity data set, where the task is to distinguish positive reviews from negative reviews." ></td>
	<td class="line x" title="137:189	F2 and G2 are bigrams that behave differently from their component unigrams." ></td>
	<td class="line x" title="138:189	The expression nothing short (of) is typically used to express positive sentiments, while nothing and short by themselves are not." ></td>
	<td class="line x" title="139:189	The word ugly is often used as a descriptive modi er that is not expressing a sentiment per se, while and ugly appears in predicate adjective constructions that are expressing a negative sentiment." ></td>
	<td class="line x" title="140:189	The extraction pattern HEP is more discriminatory than H1 because it distinguishes negative sentiments ( the lm is a disaster!" ></td>
	<td class="line x" title="141:189	) from plot descriptions ( the disaster movie )." ></td>
	<td class="line x" title="142:189	IEP shows that active-voice usages of work are strong positive indicators, while the unigram work appears in a variety of both positive and negative contexts." ></td>
	<td class="line x" title="143:189	Finally, JEP shows that the expression manages to keep is a strong positive indicator, while manages by itelf is much less discriminating." ></td>
	<td class="line x" title="144:189	445 These examples illustrate that the subsumption hierarchy can be a powerful tool to better understand the behaviors of different kinds of features, and to identify speci c features that may be desirable for inclusion in specialized lexical resources." ></td>
	<td class="line x" title="145:189	5 Using the Subsumption Hierarchy to Reduce Feature Sets When creating opinion classi ers, people often throw in a variety of features and trust the machine learning algorithm to gure out how to make the best use of them." ></td>
	<td class="line x" title="146:189	However, we hypothesized that classi ers may perform better if we can proactively eliminate features that are not necesary because they are subsumed by other features." ></td>
	<td class="line x" title="147:189	In this section, we present a series of experiments to explore this hypothesis." ></td>
	<td class="line x" title="148:189	First, we present the results for an SVM classi er trained using different sets of unigram, bigram, and extraction pattern features, both before and after subsumption." ></td>
	<td class="line x" title="149:189	Next, we evaluate a standard feature selection approach as an alternative to subsumption and then show that combining subsumption with standard feature selection produces the best results of all." ></td>
	<td class="line x" title="150:189	5.1 Classi cation Experiments To see whether feature subsumption can improve classi cation performance, we trained an SVM classi er for each of the three opinion data sets." ></td>
	<td class="line x" title="151:189	We used the SVMlight (Joachims, 1998) package with a linear kernel." ></td>
	<td class="line x" title="152:189	For the Polarity and OP data we discarded all features that have frequency < 5, and for the MPQA data we discarded features that have frequency < 2 because this data set is substantially smaller." ></td>
	<td class="line x" title="153:189	All of our experimental results are averages over 3-fold cross-validation." ></td>
	<td class="line x" title="154:189	First, we created 4 baseline classi ers: a 1Gram classi er that uses only the unigram features; a 1+2Gram classi er that uses unigram and bigram features; a 1+EP classi er that uses unigram and extraction pattern features, and a 1+2+EP classier that uses all three types of features." ></td>
	<td class="line x" title="155:189	Next, we created analogous 1+2Gram, 1+EP, and 1+2+EP classi ers but applied the subsumption hierarchy rst to eliminate unnecessary features before training the classi er." ></td>
	<td class="line x" title="156:189	We experimented with three delta values for the subsumption process: =.0005, .001, and .002." ></td>
	<td class="line x" title="157:189	Figures 7, 8, and 9 show the results." ></td>
	<td class="line x" title="158:189	The subsumption process produced small but consistent improvements on all 3 data sets." ></td>
	<td class="line x" title="159:189	For example, Figure 8 shows the results on the OP data, where all of the accuracy values produced after subsumption (the rightmost 3 columns) are higher than the accuracy values produced without subsumption (the Base[line] column)." ></td>
	<td class="line x" title="160:189	For all three data sets, the best overall accuracy (shown in boldface) was always achieved after subsumption." ></td>
	<td class="line o" title="161:189	Features Base =.0005 =.001 =.002 1Gram 79.8 1+2Gram 81.2 81.0 81.3 81.0 1+EP 81.7 81.4 81.4 82.0 1+2+EP 81.7 82.3 82.3 82.7 Figure 7: Accuracies on Polarity Data Features Base =.0005 =.001 =.002 1Gram 97.5 1+2Gram 98.0 98.7 98.6 98.7 1+EP 97.2 97.8 97.9 97.9 1+2+EP 97.8 98.6 98.7 98.7 Figure 8: Accuracies on OP Data Features Base =.0005 =.001 =.002 1Gram 74.8 1+2Gram 74.3 74.9 74.6 74.8 1+EP 74.4 74.6 74.6 74.6 1+2+EP 74.4 74.9 74.7 74.6 Figure 9: Accuracies on MPQA Data We also observed that subsumption had a dramatic effect on the F-measure scores on the OP data, which are shown in Figure 10." ></td>
	<td class="line x" title="162:189	The OP data set is fundamentally different from the other data sets because it is so highly skewed, with 91% of the documents belonging to the non-opinion class." ></td>
	<td class="line x" title="163:189	Without subsumption, the classi er was conservative about assigning documents to the opinion class, achieving F-measure scores in the 82-88 range." ></td>
	<td class="line x" title="164:189	After subsumption, the overall accuracy improved but the F-measure scores increased more dramatically." ></td>
	<td class="line x" title="165:189	These numbers show that the subsumption process produced not only a more accurate classi er, but a more useful classi er that identi es more documents as being opinion articles." ></td>
	<td class="line x" title="166:189	For the MPQA data, we get a very small improvement of 0.1% (74.8% ! 74.9%) using subsumption." ></td>
	<td class="line x" title="167:189	But note that without subsumption the performance actually decreased when bigrams and 446 Features Base =.0005 =.001 =.002 1Gram 84.5 1+2Gram 88.0 92.5 92.0 92.3 1+EP 82.4 86.9 87.4 87.4 1+2+EP 86.7 91.8 92.5 92.3 Figure 10: F-measures on OP Data 97.6 97.8 98 98.2 98.4 98.6 98.8 99 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 Accuracy (%) Top N Baseline Subsumption d=0.002 Feature Selection Subsumption d=0.002 + Feature Selection Figure 11: Feature Selection on OP Data extraction patterns were added!" ></td>
	<td class="line x" title="168:189	The subsumption process counteracted the negative effect of adding the more complex features." ></td>
	<td class="line x" title="169:189	5.2 Feature Selection Experiments We conducted a second series of experiments to determine whether a traditional feature selection approach would produce the same, or better, improvements as subsumption." ></td>
	<td class="line x" title="170:189	For each feature, we computed its information gain (IG) and then selected the N features with the highest scores.7 We experimented with values of N ranging from 1,000 to 10,000 in increments of 1,000." ></td>
	<td class="line x" title="171:189	We hypothesized that applying subsumption before traditional feature selection might also help to identify a more diverse set of high-performing features." ></td>
	<td class="line x" title="172:189	In a parallel set of experiments, we explored this hypothesis by rst applying subsumption to reduce the size of the feature set, and then selecting the best N features using information gain." ></td>
	<td class="line x" title="173:189	Figures 11, 12, and 13 show the results of these experiments for the 1+2+EP classi ers." ></td>
	<td class="line x" title="174:189	Each graph shows four lines." ></td>
	<td class="line x" title="175:189	One line corresponds to the baseline classi er with no subsumption, and another line corresponds to the baseline classi er with subsumption using the best  value for that data set." ></td>
	<td class="line x" title="176:189	Each of these two lines corresponds to 7In the case of ties, we included all features with the same score as the Nth-best as well." ></td>
	<td class="line o" title="177:189	78 78.5 79 79.5 80 80.5 81 81.5 82 82.5 83 83.5 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 Accuracy (%) Top N Baseline Subsumption d=0.002 Feature Selection Subsumption d=0.002 + Feature Selection Figure 12: Feature Selection on Polarity Data 72 72.5 73 73.5 74 74.5 75 75.5 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 Accuracy (%) Top N Baseline Subsumption d=0.0005 Feature Selection Subsumption d=0.0005 + Feature Selection Figure 13: Feature Selection on MPQA Data just a single data point (accuracy value), but we drew that value as a line across the graph for the sake of comparison." ></td>
	<td class="line x" title="178:189	The other two lines on the graph correspond to (a) feature selection for different values of N (shown on the x-axis), and (b) subsumption followed by feature selection for different values of N. On all 3 data sets, traditional feature selection performs worse than the baseline in some cases, and it virtually never outperforms the best classier trained after subsumption (but without feature selection)." ></td>
	<td class="line x" title="179:189	Furthermore, the combination of subsumption plus feature selection generally performs best of all, and nearly always outperforms feature selection alone." ></td>
	<td class="line x" title="180:189	For all 3 data sets, our best accuracy results were achieved by performing subsumption prior to feature selection." ></td>
	<td class="line o" title="181:189	The best accuracy results are 99.0% on the OP data, 83.1% on the Polarity data, and 75.4% on the MPQA data." ></td>
	<td class="line x" title="182:189	For the OP data, the improvement over baseline for both accuracy and F-measure are statistically signi cant at the p < 0.05 level (paired t-test)." ></td>
	<td class="line x" title="183:189	For the MPQA data, the improvement over baseline is 447 statistically signi cant at the p < 0.10 level." ></td>
	<td class="line x" title="184:189	6 Related Work Many features and classi cation algorithms have been explored in sentiment analysis and opinion recognition." ></td>
	<td class="line xc" title="185:189	Lexical cues of differing complexities have been used, including single words and Ngrams (e.g. , (Mullen and Collier, 2004; Pang et al. , 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003; Wiebe et al. , 2004)), as well as phrases and lexico-syntactic patterns (e.g, (Kim and Hovy, 2004; Hu and Liu, 2004; Popescu and Etzioni, 2005; Riloff and Wiebe, 2003; Whitelaw et al. , 2005))." ></td>
	<td class="line x" title="186:189	While many of these studies investigate combinations of features and feature selection, this is the rst work that uses the notion of subsumption to compare Ngrams and lexico-syntactic patterns to identify complex features that outperform simpler counterparts and to reduce a combined feature set to improve opinion classi cation." ></td>
	<td class="line x" title="187:189	7 Conclusions This paper uses a subsumption hierarchy of feature representations as (1) an analytic tool to compare features of different complexities, and (2) an automatic tool to remove unnecessary features to improve opinion classi cation performance." ></td>
	<td class="line x" title="188:189	Experiments with three opinion data sets showed that subsumption can improve classi cation accuracy, especially when combined with feature selection." ></td>
	<td class="line x" title="189:189	Acknowledgments This research was supported by NSF Grants IIS0208798 and IIS-0208985, the ARDA AQUAINT Program, and the Institute for Scienti c Computing Research and the Center for Applied Scienti c Computing within Lawrence Livermore National Laboratory." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D07-1035
Low-Quality Product Review Detection in Opinion Summarization
Liu, Jingjing;Cao, Yunbo;Lin, Chin Yew;Huang, Yalou;Zhou, Ming;"></td>
	<td class="line x" title="1:276	Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp." ></td>
	<td class="line x" title="2:276	334342, Prague, June 2007." ></td>
	<td class="line x" title="3:276	c2007 Association for Computational Linguistics Low-Quality Product Review Detection in Opinion Summarization Jingjing Liu Nankai University Tianjin, China v-jingil@microsoft.com Yunbo Cao Microsoft Research Asia Beijing, China yucao@microsoft.com Chin-Yew Lin Microsoft Research Asia Beijing, China cyl@microsoft.com Yalou Huang Nankai University Tianjin, China huangyl@nankai.edu.cn Ming Zhou Microsoft Research Asia Beijing, China mingzhou@microsoft.com Abstract Product reviews posted at online shopping sites vary greatly in quality." ></td>
	<td class="line x" title="4:276	This paper addresses the problem of detecting lowquality product reviews." ></td>
	<td class="line x" title="5:276	Three types of biases in the existing evaluation standard of product reviews are discovered." ></td>
	<td class="line x" title="6:276	To assess the quality of product reviews, a set of specifications for judging the quality of reviews is first defined." ></td>
	<td class="line x" title="7:276	A classificationbased approach is proposed to detect the low-quality reviews." ></td>
	<td class="line x" title="8:276	We apply the proposed approach to enhance opinion summarization in a two-stage framework." ></td>
	<td class="line x" title="9:276	Experimental results show that the proposed approach effectively (1) discriminates lowquality reviews from high-quality ones and (2) enhances the task of opinion summarization by detecting and filtering lowquality reviews." ></td>
	<td class="line xc" title="10:276	1 Introduction In the past few years, there has been an increasing interest in mining opinions from product reviews (Pang, et al, 2002; Liu, et al, 2004; Popescu and Etzioni, 2005)." ></td>
	<td class="line x" title="11:276	However, due to the lack of editorial and quality control, reviews on products vary greatly in quality." ></td>
	<td class="line x" title="12:276	Thus, it is crucial to have a mechanism capable of assessing the quality of reviews and detecting low-quality/noisy reviews." ></td>
	<td class="line x" title="13:276	Some shopping sites already provide a function of assessing the quality of reviews." ></td>
	<td class="line x" title="14:276	For example, Amazon1 allows users to vote for the helpfulness of each review and then ranks the reviews based on the accumulated votes." ></td>
	<td class="line x" title="15:276	However, according to our survey in Section 3, users votes at Amazon have three kinds of biases as follows: (1) imbalance vote bias, (2) winner circle bias, and (3) early bird bias." ></td>
	<td class="line x" title="16:276	Existing studies (Kim et al, 2006; Zhang and Varadarajan, 2006) used these users votes for training ranking models to assess the quality of reviews, which therefore are subject to these biases." ></td>
	<td class="line x" title="17:276	In this paper, we demonstrate the aforementioned biases and define a standard specification to measure the quality of product reviews." ></td>
	<td class="line x" title="18:276	We then manually annotate a set of ground-truth with real world product review data conforming to the specification." ></td>
	<td class="line x" title="19:276	To automatically detect low-quality product reviews, we propose a classification-based approach learned from the annotated ground-truth." ></td>
	<td class="line x" title="20:276	The proposed approach explores three aspects of product reviews, namely informativeness, readability, and subjectiveness." ></td>
	<td class="line x" title="21:276	We apply the proposed approach to opinion summarization, a typical opinion mining task." ></td>
	<td class="line x" title="22:276	The proposed approach enhances the existing work in a two-stage framework, where the low-quality review detection is applied right before the summarization stage." ></td>
	<td class="line x" title="23:276	Experimental results show that the proposed approach can discriminate low-quality reviews from high-quality ones effectively." ></td>
	<td class="line x" title="24:276	In addition, the task of opinion summarization can be enhanced by detecting and filtering low-quality reviews." ></td>
	<td class="line x" title="25:276	1 http://www.amazon.com 334 The rest of the paper is organized as follows: Section 2 introduces the related work." ></td>
	<td class="line x" title="26:276	In Section 3, we define the quality of product reviews." ></td>
	<td class="line x" title="27:276	In Section 4, we present our approach to detecting lowquality reviews." ></td>
	<td class="line x" title="28:276	In Section 5, we empirically verify the effectiveness of the proposed approach and its use for opinion summarization." ></td>
	<td class="line x" title="29:276	Section 6 summarizes our work in this paper and points out the future work." ></td>
	<td class="line x" title="30:276	2 Related Work 2.1 Evaluating Helpfulness of Reviews The problem of evaluating helpfulness of reviews (Kim et al, 2006), also known as learning utility of reviews (Zhang and Varadarajan, 2006), is quite similar to our problem of assessing the quality of reviews." ></td>
	<td class="line x" title="31:276	In practice, researchers in this area considered the problem as a ranking problem and solved it with regression models." ></td>
	<td class="line x" title="32:276	In the process of model training and testing, they used the ground-truth derived from users votes of helpfulness provided by Amazon." ></td>
	<td class="line x" title="33:276	As we will show later in Section 3, these models all suffered from three types of voting bias." ></td>
	<td class="line x" title="34:276	In our work, we avoid using users votes by developing a specification on the quality of reviews and building a ground-truth according to the specification." ></td>
	<td class="line x" title="35:276	2.2 Mining Opinions from Reviews One area of research on opinion mining from product reviews is to judge whether a review expresses a positive or a negative opinion." ></td>
	<td class="line x" title="36:276	For example, Turney (2006) presented a simple unsupervised learning algorithm in judging reviews as thumbs up (recommended) or thumbs down (not recommended)." ></td>
	<td class="line x" title="37:276	Pang et al (2002) considered the same problem and presented a set of supervised machine learning approaches to it." ></td>
	<td class="line oc" title="38:276	For other work see also Dave et al.(2003), Pang and Lee (2004, 2005)." ></td>
	<td class="line x" title="40:276	Another area of research on opinion mining is to extract and summarize users opinions from product reviews (Hu and Liu, 2004; Liu et al. , 2005; Popescu and Etzioni, 2005)." ></td>
	<td class="line x" title="41:276	Typically, a sentence or a text segment in the reviews is treated as the basic unit." ></td>
	<td class="line x" title="42:276	The polarity of users sentiments on a product feature in each unit is extracted." ></td>
	<td class="line x" title="43:276	Then the aggregation of the polarities of individual sentiments is presented to users so that they can have an at-a-glance view on how other experienced users rated on a certain product." ></td>
	<td class="line x" title="44:276	The major weakness in the existing studies is that all the reviews, including low-quality ones, are taken into consideration and treated equally for generating the summary." ></td>
	<td class="line x" title="45:276	In this paper, we enhance the application by detecting and filtering low-quality reviews." ></td>
	<td class="line x" title="46:276	In order to achieve that, we first define what the quality of reviews is. 3 Quality of Product Reviews In this section, we will first show three biases of users votes observed on Amazon, and then present our specification on the quality of product reviews." ></td>
	<td class="line x" title="47:276	3.1 Amazon Ground-truth In our study, we use the product reviews on digital cameras crawled from Amazon as our data set." ></td>
	<td class="line x" title="48:276	The data set consists of 23,141 reviews on 946 digital cameras." ></td>
	<td class="line x" title="49:276	At the Amazon site, users could vote for a review with a helpful or unhelpful label." ></td>
	<td class="line x" title="50:276	Thus, for each review there are two numbers indicating the statistics of these two labels, namely the number of helpful votes and that of unhelpful ones." ></td>
	<td class="line x" title="51:276	Kim et al (2006) used the percentage of helpful votes as the measure of evaluating the quality of reviews in their experiments." ></td>
	<td class="line x" title="52:276	We call the ground-truth based on this measure as Amazon ground-truth." ></td>
	<td class="line x" title="53:276	Certainly, the ground-truth has the advantage of convenience." ></td>
	<td class="line x" title="54:276	However, we identify three types of biases that make the Amazon ground-truth not always suitable for determining the quality of reviews." ></td>
	<td class="line x" title="55:276	We describe these biases in details in the rest of this section." ></td>
	<td class="line x" title="56:276	3.1.1 Imbalance Vote Bias Figure 1." ></td>
	<td class="line x" title="57:276	Reviews percentage scores At the Amazon site, users tend to value others opinions positively rather than negatively." ></td>
	<td class="line x" title="58:276	From Figure 1, we can see that a half of the 23,141 0 2000 4000 6000 8000 10000 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 # R ev iews Percentage of 'helpful' votes 335 reviews (corresponding to the two bars on the right of the figure) have more than 90% helpful votes, including 9,100 reviews with 100% helpful votes." ></td>
	<td class="line x" title="59:276	From an in-depth investigation on these highly-voted reviews, we observed that some did not really have as good quality as the votes hint." ></td>
	<td class="line x" title="60:276	For example, in Figure 2, the review about Canon PowerShot S500 receives 40 helpful votes out of 40 votes although it only gives very brief description on the product features in its second paragraph." ></td>
	<td class="line x" title="61:276	We call this type of bias imbalance vote bias." ></td>
	<td class="line x" title="62:276	This is my second Canon digital elph camera." ></td>
	<td class="line x" title="63:276	Both were great cameras." ></td>
	<td class="line x" title="64:276	Recently upgraded to the S500." ></td>
	<td class="line x" title="65:276	About 6 months later I get the dreaded E18 error." ></td>
	<td class="line x" title="66:276	I searched the Internet and found numerous people having problems." ></td>
	<td class="line x" title="67:276	When I determined the problem to be the lens not fully extending I decided to give it a tug." ></td>
	<td class="line x" title="68:276	It clicked and the camera came on, ready to take pictures." ></td>
	<td class="line x" title="69:276	Turning it off and on produced the E18 again." ></td>
	<td class="line x" title="70:276	While turning it on I gave it a nice little bump on the side (where the USB connector is) and the lens popped out on its own." ></td>
	<td class="line x" title="71:276	No problems since." ></td>
	<td class="line x" title="72:276	Its a nice compact and light camera and takes great photos and videos." ></td>
	<td class="line x" title="73:276	Only complaint (other than E18) is the limit of 30-second videos on 640x480 mode." ></td>
	<td class="line x" title="74:276	I've got a 512MB compact flash card, I should be able to take as much footage as I have memory in one take." ></td>
	<td class="line x" title="75:276	Figure 2." ></td>
	<td class="line x" title="76:276	An example review 3.1.2 Winner Circle Bias Figure 3." ></td>
	<td class="line x" title="77:276	Votes of the top-50 ranked reviews There also exists a bootstrapping effect of hot reviews at the Amazon site." ></td>
	<td class="line x" title="78:276	Figure 3 shows the helpful votes for the top 50 ranked reviews." ></td>
	<td class="line x" title="79:276	The numbers are averaged over 127 digital cameras which have no less than 50 reviews." ></td>
	<td class="line x" title="80:276	As shown in this figure, the top two reviews hold more than 250 and 140 votes respectively on average; while the numbers of votes held by lower-ranked reviews decrease exponentially." ></td>
	<td class="line x" title="81:276	This is so-called the winner circle bias: the more votes a review gains, the more default authority it would appear to the readers, which in turn will influence the objectivity of the readers votes." ></td>
	<td class="line x" title="82:276	Also, the higher ranked reviews would attract more eyeballs and therefore gain more peoples votes." ></td>
	<td class="line x" title="83:276	This mutual influence among labelers should be avoided when the votes are used as the evaluation standard." ></td>
	<td class="line x" title="84:276	3.1.3 Early Bird Bias Figure 4." ></td>
	<td class="line x" title="85:276	Dependency on publication date Publication date can influence the accumulation of users votes." ></td>
	<td class="line x" title="86:276	In Figure 4, the nth publication date represents the nth month after the product is released." ></td>
	<td class="line x" title="87:276	The number in the figure is averaged over all the digital cameras in the data set." ></td>
	<td class="line x" title="88:276	We can observe a clear trend that the earlier a review is posted, the more votes it will get." ></td>
	<td class="line x" title="89:276	This is simply because reviews posted earlier are exposed to users for a longer time." ></td>
	<td class="line x" title="90:276	Therefore, some high quality reviews may get fewer users vote because of later publication." ></td>
	<td class="line x" title="91:276	We call this early bird bias." ></td>
	<td class="line x" title="92:276	3.2 Specification of Quality Besides these aforementioned biases, using the raw rating from readers directly also fails to provide a clear guideline for what a good review consists of." ></td>
	<td class="line x" title="93:276	In this section, we provide such a guideline, which we name as the specification (SPEC)." ></td>
	<td class="line x" title="94:276	In the SPEC, we define four categories of review quality which represent different values of the reviews to users purchase decision: best review, good review, fair review, and bad review." ></td>
	<td class="line x" title="95:276	A generic description of the SPEC is as follows: A best review must be a rather complete and detailed comment on a product." ></td>
	<td class="line x" title="96:276	It presents several aspects of a product and provides convincing opinions with enough evidence." ></td>
	<td class="line x" title="97:276	Usually a best review could be taken as the main reference that users only need to read before making their purchase decision on a certain product." ></td>
	<td class="line x" title="98:276	The first review in Figure 5 is a best review." ></td>
	<td class="line x" title="99:276	It presents several product features and provides convincing opinions with sufficient evidence." ></td>
	<td class="line x" title="100:276	It is also in a good format for readers to easily understand." ></td>
	<td class="line x" title="101:276	Note that we omit some words in the example to save the space." ></td>
	<td class="line x" title="102:276	0 50 100 150 200 250 300 1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 #V ote s he ld by re vie ws Ranking positions of reviews 0 10 20 30 40 50 60 1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49# Vot es he ld by re vie ws Publication Date 336 A good review is a relatively complete comment on a product, but not with as much supporting evidence as necessary." ></td>
	<td class="line x" title="103:276	It could be used as a strong and influential reference, but not as the only recommendation." ></td>
	<td class="line x" title="104:276	The second review in Figure 5 is such an example." ></td>
	<td class="line x" title="105:276	A fair review contains a very brief description on a product." ></td>
	<td class="line x" title="106:276	It does not supply detailed evaluation on the product, but only comments on some aspects of the product." ></td>
	<td class="line x" title="107:276	For example, the third review in Figure 5 mainly talks about the delay between pictures, but less about other aspects of the camera." ></td>
	<td class="line x" title="108:276	A bad review is usually an incorrect description of a product with misleading information." ></td>
	<td class="line x" title="109:276	It talks little about a specific product but much about some general topics (e.g. photography)." ></td>
	<td class="line x" title="110:276	For example, the last review in Figure 5 talks about the topic of generic battery, but does not specify any digital camera." ></td>
	<td class="line x" title="111:276	A bad review is an unhelpful review that can be ignored." ></td>
	<td class="line x" title="112:276	Best Review: I purchased this camera about six months ago after my Kodak Easyshare camera completely died on me. I did a little research and read only good things about this Canon camera so I decided to go with it because it was very reasonably priced (about $200)." ></td>
	<td class="line x" title="113:276	Not only did the camera live up to my expectations, it surpassed them by leaps and bounds!" ></td>
	<td class="line x" title="114:276	Here are the things I have loved about this camera: BATTERY this camera has the best battery of any digital camera I have ever owned or used." ></td>
	<td class="line x" title="115:276	 EASY TO USE I was able to  PICTURE QUALITY all of the pictures I've taken and printed out have been great." ></td>
	<td class="line x" title="116:276	 FEATURES I love the ability to quickly and easily  LCD SCREEN I was hoping  SD MEMORY CARD I was also looking for a camera that used SD memory cards." ></td>
	<td class="line x" title="117:276	Mostly because I cannot stress how highly I recommend this camera." ></td>
	<td class="line x" title="118:276	I will never buy another digital camera besides Canon again." ></td>
	<td class="line x" title="119:276	And the A610 (as well as the A620 the 7.0MP version) is the best digital camera I've ever used." ></td>
	<td class="line x" title="120:276	Good Review: The Sony DSC 'P10' Digital Camera is the top pick for CSC." ></td>
	<td class="line x" title="121:276	Running against cameras like Olympus stylus, Canon Powereshot, Sony V1, Nikon, Fuji, and More." ></td>
	<td class="line x" title="122:276	The new release of 5.0 mega pixels has shot prices for digital cameras up to $1000+." ></td>
	<td class="line x" title="123:276	This camera I purchased through a Private Dealer cost me $400.86." ></td>
	<td class="line x" title="124:276	The Retail Price is Running $499.00 to $599.00." ></td>
	<td class="line x" title="125:276	Purchase this camera from a wholesale dealer for the best price $377.00." ></td>
	<td class="line x" title="126:276	Great Photo Even in dim light w/o a flash." ></td>
	<td class="line x" title="127:276	The p10 is very compact." ></td>
	<td class="line x" title="128:276	Can easily fit into any pocket." ></td>
	<td class="line x" title="129:276	The camera can record 90 minutes of mpeg like a home movie." ></td>
	<td class="line x" title="130:276	There are a lot of great digital cameras on the market that shoot good pictures and video." ></td>
	<td class="line x" title="131:276	What makes the p10 the top pick is it comes with a rechargeable lithium battery." ></td>
	<td class="line x" title="132:276	Many use AA batteries, the digital camera consumes theses AA batteries in about two hours time while the unit is on." ></td>
	<td class="line x" title="133:276	That can add continuous expense to the camera." ></td>
	<td class="line x" title="134:276	It's also the best resolution on the market." ></td>
	<td class="line x" title="135:276	6.0 megapix is out, though only a few." ></td>
	<td class="line x" title="136:276	And the smallest that we found." ></td>
	<td class="line x" title="137:276	Also the best price for a major brand." ></td>
	<td class="line x" title="138:276	Fair Review: There is nothing wrong with the 2100 except for the very noticeable delay between pics." ></td>
	<td class="line x" title="139:276	The camera's digital processor takes about 5 seconds after a photo is snapped to ready itself for the next one." ></td>
	<td class="line x" title="140:276	Otherwise, the optics, the 3X optical zoom and the 2 megapixel resolution are fine for anything from Internet apps to 8' x 10' print enlarging." ></td>
	<td class="line x" title="141:276	It is competent, not spectacular, but it gets the job done at an agreeable price point." ></td>
	<td class="line x" title="142:276	Bad Review: I want to point out that you should never buy a generic battery, like the person from San Diego who reviewed the S410 on May 15, 2004, was recommending." ></td>
	<td class="line x" title="143:276	Yes you'd save money, but there have been many reports of generic batteries exploding when charged for too long." ></td>
	<td class="line x" title="144:276	And don't think if your generic battery explodes you can sue somebody and win millions." ></td>
	<td class="line x" title="145:276	These batteries are made in sweatshops in China, India and Korea, and I doubt you can find anybody to sue." ></td>
	<td class="line x" title="146:276	So play it safe, both for your own sake and the camera's sake." ></td>
	<td class="line x" title="147:276	If you want a spare, get a real Canon one." ></td>
	<td class="line x" title="148:276	Figure 5." ></td>
	<td class="line x" title="149:276	Example reviews 3.3 Annotation of Quality According to the SPEC defined above, we built a ground-truth from the Amazon data set." ></td>
	<td class="line x" title="150:276	We randomly selected 100 digital cameras and 50 reviews for each camera." ></td>
	<td class="line x" title="151:276	Totally we have 4,909 reviews since some digital cameras have fewer than 50 unique reviews." ></td>
	<td class="line x" title="152:276	Then we hired two annotators to label the reviews with the SPEC as their guideline." ></td>
	<td class="line x" title="153:276	As the result, we have two independent copies of annotations on 4,909 reviews, with the labels of best, good, fair, and bad." ></td>
	<td class="line x" title="154:276	Table 1 shows the confusion matrix between the two copies of annotation." ></td>
	<td class="line x" title="155:276	The value of the kappa statistic (Cohen, 1960) calculated from the matrix is 0.8142." ></td>
	<td class="line x" title="156:276	This shows that the two annotators achieved highly consistent results by following the SPEC, although they worked independently." ></td>
	<td class="line x" title="157:276	Annotation 1 Annotation 2 best good fair bad total best 294 44 2 0 340 good 66 639 113 0 818 fair 0 200 1,472 113 1,785 bad 1 2 78 1,885 1,966 total 361 885 1,665 1,998 4,909 Table 1." ></td>
	<td class="line x" title="158:276	Confusion matrix bet." ></td>
	<td class="line x" title="159:276	the annotations In order to examine the difference between our annotations and Amazon ground-truth, we evaluate the Amazon ground-truth against the annotations, 337 with the measure of error rate of preference pairs (Herbrich et al, 1999)." ></td>
	<td class="line x" title="160:276	         = |                        ||                  | (1) where the preference pair is defined as a pair of reviews with a order." ></td>
	<td class="line x" title="161:276	For example, a best review and a good review correspond to a preference pair with the order of best review preferring to good review." ></td>
	<td class="line x" title="162:276	The all preference pairs are collected from one of the annotations (the annotation 1 or the annotation 2) by ignoring the pairs from the same category." ></td>
	<td class="line x" title="163:276	The incorrect preference pairs are the preference pairs collected from the Amazon ground-truth but not with the same order as that in the all preference pairs." ></td>
	<td class="line x" title="164:276	The order of the preference pair collected from the Amazon ground-truth is evaluated on the basis of the percentage score as described in Section 3.1." ></td>
	<td class="line x" title="165:276	The error rate of preference pairs based on the annotation 1 and that based on the annotation 2 are 0.448 and 0.446, respectively, averaged over 100 digital cameras." ></td>
	<td class="line x" title="166:276	The high error rate of preference pairs demonstrates that the Amazon ground-truth diverges from the annotations (our ground-truth) significantly." ></td>
	<td class="line x" title="167:276	To discover which kind of ground-truth is more reasonable, we ask an additional annotator (the third annotator) to compare these two kinds of ground-truth." ></td>
	<td class="line x" title="168:276	More specifically, we randomly selected 100 preference pairs whose orders the two kinds of ground-truth dont agree on (called incorrect preference pairs in the evaluation above)." ></td>
	<td class="line x" title="169:276	As for our ground-truth, we choose the Annotation 1 in the new test." ></td>
	<td class="line x" title="170:276	Then, the third annotator is asked to assign a preference order for each selected pair." ></td>
	<td class="line x" title="171:276	Note that the third annotator is blind to both our specification and the existing preference order." ></td>
	<td class="line x" title="172:276	Last, we evaluate the two kinds of ground-truth with the new annotation." ></td>
	<td class="line x" title="173:276	Among 100 pairs, our ground-truth agrees to the new annotation on 85 pairs while the Amazon ground-truth agrees to the new annotation on 15 pairs." ></td>
	<td class="line x" title="174:276	To confirm the result, yet another annotator (the fourth annotator) is called to repeat the same annotation independently as the third one." ></td>
	<td class="line x" title="175:276	And we obtain the same statistical result (85 vs. 15) although the fourth annotator does not agree with the third annotator on some pairs." ></td>
	<td class="line x" title="176:276	In practice, we treat the reviews in the first three categories (best, good and fair) as highquality reviews and those in the bad category as low-quality reviews, since our goal is to identify low quality reviews that should not be considered when creating product review summaries." ></td>
	<td class="line x" title="177:276	4 Classification of Product Reviews We employ a statistical machine learning approach to address the problem of detecting low-quality products reviews." ></td>
	<td class="line x" title="178:276	Given a training data set  =  ,  1, we construct a model that can minimize the error in prediction of y given x (generalization error)." ></td>
	<td class="line x" title="179:276	Here     and   = {          ,           } represents a product review and a label, respectively." ></td>
	<td class="line x" title="180:276	When applied to a new instance x, the model predicts the corresponding y and outputs the score of the prediction." ></td>
	<td class="line x" title="181:276	4.1 The Learning Model In our study, we focus on differentiating lowquality product reviews from high-quality ones." ></td>
	<td class="line x" title="182:276	Thus, we treat the task as a binary classification problem." ></td>
	<td class="line x" title="183:276	We employ SVM (Support Vector Machines) (Vapnik, 1995) as the model of classification." ></td>
	<td class="line x" title="184:276	Given an instance x (product review), SVM assigns a score to it based on   =    +  (2) where w denotes a vector of weights and b denotes an intercept." ></td>
	<td class="line x" title="185:276	The higher the value of f(x) is, the higher the quality of the instance x is. In classification, the sign of f(x) is used." ></td>
	<td class="line x" title="186:276	If it is positive, then x is classified into the positive category (high-quality reviews), otherwise into the negative category (low-quality reviews)." ></td>
	<td class="line x" title="187:276	The construction of SVM needs labeled training data (in our case, the categories are high-quality reviews and low-quality reviews)." ></td>
	<td class="line x" title="188:276	Briefly, the learning algorithm creates the hyper plane in (2), such that the hyper plane separates the positive and negative instances in the training data with the largest margin." ></td>
	<td class="line x" title="189:276	4.2 Product Feature Resolution Product features (e.g. , image quality for digital camera) in a review are good indicators of review quality." ></td>
	<td class="line x" title="190:276	However, different product features may refer to the same meaning (e.g. , battery life and power), which will bring redundancy in the study." ></td>
	<td class="line x" title="191:276	In this paper, we formulize the problem as the resolution of product features." ></td>
	<td class="line x" title="192:276	Thus, the 338 problem is reduced to how to determine the equivalence of a product feature in different forms." ></td>
	<td class="line x" title="193:276	In (Hu and Liu, 2004), the matching of different product features is mentioned briefly and addressed by fuzzy matching." ></td>
	<td class="line x" title="194:276	However, there exist many cases where the method fails to match the multiple mentions, e.g., battery life and power, because it only considers string similarity." ></td>
	<td class="line x" title="195:276	In this paper we propose to resolve the problem by leveraging two kinds of evidence: one is surface string evidence, the other is contextual evidence." ></td>
	<td class="line x" title="196:276	We use edit distance (Ukkonen, 1985) to compare the similarity between the surface strings of two mentions, and use contextual similarity to reflect the semantic similarity between two mentions." ></td>
	<td class="line x" title="197:276	When using contextual similarity, we split all the reviews into sentences." ></td>
	<td class="line x" title="198:276	For each mention of a product feature, we take it as a query and search for all the relevant sentences." ></td>
	<td class="line x" title="199:276	Then we construct a vector for the mention, by taking each unique term in the relevant sentences as a dimension of the vector." ></td>
	<td class="line x" title="200:276	The cosine similarity between two vectors of mentions is then present to measure the contextual similarity between two mentions." ></td>
	<td class="line x" title="201:276	4.3 Feature Development for Learning To detect low-quality reviews, our proposed approach explores three aspects of product reviews, namely informativeness, subjectiveness, and readability." ></td>
	<td class="line x" title="202:276	We denote the features employed for learning as learning features, discriminative from the product features we discussed above." ></td>
	<td class="line x" title="203:276	4.3.1 Features on Informativeness As for informativeness, the resolution of product features is employed when we generate the learning features as listed below." ></td>
	<td class="line x" title="204:276	Pairs mapping to the same product feature will be treated as the same product feature, when we calculate the frequency and the number of product features." ></td>
	<td class="line x" title="205:276	We apply the approach proposed in (Hu and Liu, 2004) to extract product features." ></td>
	<td class="line x" title="206:276	We also use a list of product names and a list of brand names to generate the learning features." ></td>
	<td class="line x" title="207:276	Both lists can be collected from the Amazon site because they are relatively stable within a time interval." ></td>
	<td class="line x" title="208:276	The learning features on the informativeness of a review are as follows." ></td>
	<td class="line x" title="209:276	 Sentence level (SL)  The number of sentences in the review  The average length of sentences  The number of sentences with product features  Word level (WL)  The number of words in the review  The number of products (e.g. , DMC-FZ50, EX-Z1000) in the review  The number of products in the title of a review  The number of brand names (e.g. , Canon, Sony) in the review  The number of brand names in the title of a review  Product feature level (PFL)  The number of product features in the review  The total frequency of product features in the review  The average frequency of product features in the review  The number of product features in the title of a review  The total frequency of product features in the title of a review 4.3.2 Features on Readability We make use of several features at paragraph level which indicate the underlying structure of the reviews." ></td>
	<td class="line x" title="210:276	These features include,  The number of paragraphs in the review  The average length of paragraphs in the review  The number of paragraph separators in the review Here, we refer to the keywords, such as Pros vs. Cons as paragraph separators." ></td>
	<td class="line x" title="211:276	The keywords usually appear at the beginning of paragraphs for categorizing two contrasting aspects of a product." ></td>
	<td class="line x" title="212:276	We extract the nouns and noun phrases at the beginning of each paragraph from the 4,909 reviews and use the most frequent 30 pairs of keywords as paragraph separators." ></td>
	<td class="line x" title="213:276	Table 2 provides some examples of the extracted separators." ></td>
	<td class="line x" title="214:276	Separators Separators Positive Negative Positive Negative Pros Cons The Good The Bad Strength Weakness Thumb up Bummer PLUSES MINUSES Positive Negative Advantages Drawbacks Likes Dislikes The upsides Downsides GOOD THINGS BAD THINGS Table 2." ></td>
	<td class="line x" title="215:276	Examples of paragraph separators 339 4.3.3 Features on Subjectiveness We also take the subjectiveness of reviews into consideration." ></td>
	<td class="line x" title="216:276	Unlike previous work (Kim et al, 2006; Zhang and Varadarajan, 2006) using shallow syntactic information directly, we use a sentiment analysis tool (Hu and Liu, 2004) which aggregates a set of shallow syntactic information." ></td>
	<td class="line x" title="217:276	The tool is a classifier capable of determining the sentiment polarity of each sentence." ></td>
	<td class="line x" title="218:276	We create three learning features regarding the subjectiveness of reviews." ></td>
	<td class="line x" title="219:276	 The percentage of positive sentences in the review  The percentage of negative sentences in the review  The percentage of subjective sentences (regardless of positive or negative) in the review 5 Experiments In this section, we describe our experiments with the proposed classification-based approach to lowquality review detection, and its effectiveness on the task of opinion summarization." ></td>
	<td class="line x" title="220:276	5.1 Detecting Low-quality Reviews In our proposed approach, the problem of assessing quality of reviews is formalized as a binary classification problem." ></td>
	<td class="line x" title="221:276	We conduct experiments by taking reviews in the categories of best, good, and fair as high-quality reviews and those in the bad category as low-quality reviews." ></td>
	<td class="line x" title="222:276	As for classification model, we utilize the SVMLight toolkit (Joachims, 2004)." ></td>
	<td class="line x" title="223:276	We randomly divide the 100 queries of digital cameras into two sets, namely a training set of 50 queries and a test set of 50 queries." ></td>
	<td class="line x" title="224:276	For the two copies of annotations, we use the same division." ></td>
	<td class="line x" title="225:276	We use the training set from annotation 1 to train the model and apply the model to the test sets from both annotation 1 and annotation 2, respectively." ></td>
	<td class="line x" title="226:276	Table 3 reports the accuracies of our approach to review classification." ></td>
	<td class="line x" title="227:276	The accuracy is defined as the percentage of correctly classified reviews." ></td>
	<td class="line x" title="228:276	We take the approach that utilizes only the category of features on sentence level (SL) as the baseline, and incrementally add other categories of features on informativeness, readability and subjectiveness." ></td>
	<td class="line x" title="229:276	We can see that both the features on word level (WL) and those on product feature level (PFL) can improve the performance of classification much." ></td>
	<td class="line x" title="230:276	The features on readability can still increase the accuracy although the contribution is much less." ></td>
	<td class="line x" title="231:276	The features on subjectiveness, however, make no contribution." ></td>
	<td class="line x" title="232:276	Feature Category Annotation1 Annotation2 Informativeness SL 73.59% 72.81% WL 80.41% 79.15% PFL 83.30% 82.37% Readability 83.93% 82.91% Subjectiveness 83.84% 82.96% Table 3." ></td>
	<td class="line x" title="233:276	Low-quality reviews detection We also conduct a more detailed analysis on each individual feature." ></td>
	<td class="line x" title="234:276	Two categories of features on title and brand name have poor performance, which is due to the lack of information in the title and the low coverage of brand names in a review, respectively." ></td>
	<td class="line x" title="235:276	5.2 Summarizing Sentiments of Reviews One potential application of low-quality review detection is the opinion summarization of reviews." ></td>
	<td class="line x" title="236:276	The process of opinion summarization of reviews with regards to a query of a product consists of the following steps (Liu et al, 2005): 1." ></td>
	<td class="line x" title="237:276	From each of the reviews, identify every text segment with opinion in the review, and determine the polarities of the opinion segments." ></td>
	<td class="line x" title="238:276	2." ></td>
	<td class="line x" title="239:276	For each product feature, generate a positive opinion set and a negative opinion set of opinion segments, denoted as POS( ) and NOS( )." ></td>
	<td class="line x" title="240:276	3." ></td>
	<td class="line x" title="241:276	For each product feature, aggregate the numbers of segments in POS( ) andNOS( ), as opinion summarization on the product feature." ></td>
	<td class="line x" title="242:276	In this process, all the reviews contribute the same." ></td>
	<td class="line x" title="243:276	However, different reviews do hold different authorities." ></td>
	<td class="line x" title="244:276	A positive/negative opinion from a high-quality review should not have the same weight as that from a low-quality review." ></td>
	<td class="line x" title="245:276	We use a two-stage approach to enhance the reliability of summarization." ></td>
	<td class="line x" title="246:276	That is, we add a process of low-quality review detection before the summarization process, so that the summarization result is obtained based on the high-quality reviews only." ></td>
	<td class="line x" title="247:276	We are to demonstrate how much difference the proposed two-stage approach can bring into the opinion summarization." ></td>
	<td class="line x" title="248:276	We use the best classification model trained as described in Section 5.1 to filter low-quality reviews, and do summarization on the high-quality 340 reviews associated to the 50 test queries." ></td>
	<td class="line x" title="249:276	We denote the proposed approach and the old approach as two-stage and one-stage, respectively." ></td>
	<td class="line x" title="250:276	Due to the limited space, we only give a visual comparison of the two approaches on image quality in Figure 6." ></td>
	<td class="line x" title="251:276	The upper figure shows the summarization of positive opinions and the lower figure shows that of negative opinions." ></td>
	<td class="line x" title="252:276	From the figures we can see that the two-stage approach preserves fewer text segments as the result of filtering out many low-quality product reviews." ></td>
	<td class="line x" title="253:276	Figure 6." ></td>
	<td class="line x" title="254:276	Summarization on image quality To show the comparison on more features in a compressed space, we give the statistic ratio of change between two approaches instead." ></td>
	<td class="line x" title="255:276	As for the evaluation measure, we define RatioOfChange (ROC) on a feature f as, ROC  = Rateone  stage   Ratetwo  stage ( )Rate one  stage ( ) (3) where Rate *(f) is defined as, Rate ( ) = |POS( )||POS( )| + |NOS( )| (4) Table 4 shows some statistic results on ROC on five product features, namely image quality(IQ), battery, LCD screen (LCD), flash and movie mode (MM)." ></td>
	<td class="line x" title="256:276	The values in the cells are the percentage of queries whose ROC is larger/smaller than the respective thresholds." ></td>
	<td class="line x" title="257:276	We can see that a large portion of queries have big changes on the values of ROC." ></td>
	<td class="line x" title="258:276	This means that the result achieved by the two-stage approach is substantially different from that achieved by the one-stage approach." ></td>
	<td class="line x" title="259:276	%Query RatioOfChange (+) >0.30 >0.25 >0.20 >0.15 >0.10 >0.05 IQ 2% 4% 4% 10% 14% 22% Battery 10% 14% 18% 30% 38% 50% LCD 12% 18% 20% 22% 24% 28% Flash 6% 10% 16% 20% 26% 42% MM 6% 8% 8% 12% 18% 26% %Query RatioOfChange (-) <-0.30 <-0.25 <-0.20 <-0.15 <-0.10 <-0.05 IQ 4% 6% 10% 14% 18% 44% Battery 2% 4% 4% 10% 14% 22% LCD 4% 4% 8% 12% 22% 28% Flash 4% 6% 8% 16% 18% 28% MM 8% 10% 16% 18% 34% 42% Table 4." ></td>
	<td class="line x" title="260:276	RatioOfChange on five features There is no standard way to evaluate the quality of opinion summarization as it is rather a subjective problem." ></td>
	<td class="line x" title="261:276	In order to demonstrate the impact of the two-stage approach, we turn to external authoritative sources other than Amazon.com as the objective evaluation reference." ></td>
	<td class="line x" title="262:276	We observe that CNET2 provides a professional editors review for many products, which gives a rating in the range of 1~10 on product features." ></td>
	<td class="line x" title="263:276	9 digital cameras out of the 50 test queries are found to have the editors rating on image quality at CNET." ></td>
	<td class="line x" title="264:276	We use this rating to compare with the results of our opinion summarization." ></td>
	<td class="line x" title="265:276	We rescale the Rate scores obtained by both the one-stage approach and the two-stage approach into the range of 1-10 in order to perform the comparison." ></td>
	<td class="line x" title="266:276	Figure 7 provides the visual comparison." ></td>
	<td class="line x" title="267:276	We can see that the result achieved by the two-stage approach has a much better (closer) resemblance to CNET rating than one-stage approach does." ></td>
	<td class="line x" title="268:276	This indicates that our two-stage approach can achieve a more consistent summarization result to the professional evaluations by the editors." ></td>
	<td class="line x" title="269:276	Although the CNET rating is not the absolute standard for product evaluation, it provides a professional yet objective evaluation of the products." ></td>
	<td class="line x" title="270:276	Therefore, the experimental results demonstrate that our proposed approach could achieve more reliable opinion summarization which is closer to the generic evaluation from authoritative sources." ></td>
	<td class="line x" title="271:276	2 http://www.cnet.com 0 30 60 90 120 1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 Num be r of suppor ting se nte nce s (P os itiv e) QueryID One-stage Two-stage 0 20 40 60 80 1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 Num be r of suppor ting se nte nce s (Ne gat ive ) QueryID One-stage Two-stage 341 Figure 7." ></td>
	<td class="line x" title="272:276	Comparison with CNET rating 6 Conclusion In this paper, we studied the problem of detecting low-quality product reviews." ></td>
	<td class="line x" title="273:276	Our contribution can be summarized in two-fold: (1) we discovered three types of biases in the ground-truth used extensively in the existing work, and proposed a specification on the quality of product reviews." ></td>
	<td class="line x" title="274:276	The three biases that we discovered are imbalance vote bias, winner circle bias, and early bird bias." ></td>
	<td class="line x" title="275:276	(2) Rooting on the new ground-truth (conforming to the proposed specification), we proposed a classification-based approach to low-quality product review detection, which yields better performance of opinion summarization." ></td>
	<td class="line x" title="276:276	We hope to explore our future work in several areas, such as further consolidating the new ground-truth from different points of view and verifying the effectiveness of low-quality review detection with other applications." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N07-1013
Improving Diversity in Ranking using Absorbing Random Walks
Zhu, Xiaojin;Goldberg, Andrew;Van Gael, Jurgen;Andrzejewski, David;"></td>
	<td class="line x" title="1:254	Proceedings of NAACL HLT 2007, pages 97104, Rochester, NY, April 2007." ></td>
	<td class="line x" title="2:254	c2007 Association for Computational Linguistics Improving Diversity in Ranking using Absorbing Random Walks Xiaojin Zhu Andrew B. Goldberg Jurgen Van Gael David Andrzejewski Department of Computer Sciences University of Wisconsin, Madison Madison, WI 53705 {jerryzhu, goldberg, jvangael, andrzeje}@cs.wisc.edu Abstract We introduce a novel ranking algorithm called GRASSHOPPER, which ranks items with an emphasis on diversity." ></td>
	<td class="line x" title="3:254	That is, the top items should be different from each other in order to have a broad coverage of the whole item set." ></td>
	<td class="line x" title="4:254	Many natural language processing tasks can benefit from such diversity ranking." ></td>
	<td class="line x" title="5:254	Our algorithm is based on random walks in an absorbing Markov chain." ></td>
	<td class="line x" title="6:254	We turn ranked items into absorbing states, which effectively prevents redundant items from receiving a high rank." ></td>
	<td class="line x" title="7:254	We demonstrate GRASSHOPPERs effectiveness on extractive text summarization: our algorithm ranks between the 1st and 2nd systems on DUC 2004 Task 2; and on a social network analysis task that identifies movie stars of the world." ></td>
	<td class="line x" title="8:254	1 Introduction Many natural language processing tasks involve ranking a set of items." ></td>
	<td class="line x" title="9:254	Sometimes we want the top items to be not only good individually but also diverse collectively." ></td>
	<td class="line x" title="10:254	For example, extractive text summarization generates a summary by selecting a few good sentences from one or more articles on the same topic (Goldstein et al. , 2000)." ></td>
	<td class="line x" title="11:254	This can be formulated as ranking all the sentences, and taking the top ones." ></td>
	<td class="line x" title="12:254	A good sentence is one that is representative, i.e., similar to many other sentences, so that it likely conveys the central meaning of the articles." ></td>
	<td class="line x" title="13:254	On the other hand, we do not want multiple nearidentical sentences." ></td>
	<td class="line x" title="14:254	The top sentences should be diverse." ></td>
	<td class="line x" title="15:254	As another example, in information retrieval on news events, an article is often published by multiple newspapers with only minor changes." ></td>
	<td class="line x" title="16:254	It is undesirable to rank all copies of the same article highly, even though it may be the most relevant." ></td>
	<td class="line x" title="17:254	Instead, the top results should be different and complementary." ></td>
	<td class="line x" title="18:254	In other words, one wants subtopic diversity in retrieval results (Zhai et al. , 2003)." ></td>
	<td class="line x" title="19:254	The need for diversity in ranking is not unique to natural language processing." ></td>
	<td class="line x" title="20:254	In social network analysis, people are connected by their interactions, e.g., phone calls." ></td>
	<td class="line x" title="21:254	Active groups of people have strong interactions among them, but many groups may exist with fewer interactions." ></td>
	<td class="line x" title="22:254	If we want a list of people that represent various groups, it is important to consider both activity and diversity, and not to fill the list with people from the same active groups." ></td>
	<td class="line x" title="23:254	Given the importance of diversity in ranking, there has been significant research in this area." ></td>
	<td class="line x" title="24:254	Perhaps the most well-known method is maximum marginal relevance (MMR) (Carbonell and Goldstein, 1998), as well as cross-sentence informational subsumption (Radev, 2000), mixture models (Zhang et al. , 2002), subtopic diversity (Zhai et al. , 2003), diversity penalty (Zhang et al. , 2005), and others." ></td>
	<td class="line x" title="25:254	The basic idea is to penalize redundancy by lowering an items rank if it is similar to items already ranked." ></td>
	<td class="line x" title="26:254	However, these methods often treat centrality ranking and diversity ranking separately, sometimes with heuristic procedures." ></td>
	<td class="line x" title="27:254	97 We propose GRASSHOPPER (Graph Random-walk with Absorbing StateS that HOPs among PEaks for Ranking), a novel ranking algorithm that encourages diversity." ></td>
	<td class="line x" title="28:254	GRASSHOPPER is an alternative to MMR and variants, with a principled mathematical model and strong empirical performance." ></td>
	<td class="line x" title="29:254	It ranks a set of items such that: 1." ></td>
	<td class="line x" title="30:254	A highly ranked item is representative of a local group in the set, i.e., it is similar to many other items (centrality); 2." ></td>
	<td class="line x" title="31:254	The top items cover as many distinct groups as possible (diversity); 3." ></td>
	<td class="line x" title="32:254	It incorporates an arbitrary pre-specified ranking as prior knowledge (prior)." ></td>
	<td class="line x" title="33:254	Importantly GRASSHOPPER achieves these in a unified framework of absorbing Markov chain random walks." ></td>
	<td class="line x" title="34:254	The key idea is the following: We define a random walk on a graph over the items." ></td>
	<td class="line x" title="35:254	Items which have been ranked so far become absorbing states." ></td>
	<td class="line x" title="36:254	These absorbing states drag down the importance of similar unranked states, thus encouraging diversity." ></td>
	<td class="line x" title="37:254	Our model naturally balances centrality, diversity, and prior." ></td>
	<td class="line x" title="38:254	We discuss the algorithm in Section 2." ></td>
	<td class="line x" title="39:254	We present GRASSHOPPERs empirical results on text summarization and social network analysis in Section 3." ></td>
	<td class="line x" title="40:254	2 The GRASSHOPPER Algorithm 2.1 The Input GRASSHOPPER requires three inputs: a graph W, a probability distribution r that encodes the prior ranking, and a weight   [0,1] that balances the two." ></td>
	<td class="line x" title="41:254	The user needs to supply a graph with n nodes, one for each item." ></td>
	<td class="line x" title="42:254	The graph is represented by an nn weight matrix W, where wij is the weight on the edge from i to j. It can be either directed or undirected." ></td>
	<td class="line x" title="43:254	W is symmetric for undirected graphs." ></td>
	<td class="line x" title="44:254	The weights are non-negative." ></td>
	<td class="line x" title="45:254	The graph does not need to be fully connected: if there is no edge from item i to j, then wij = 0." ></td>
	<td class="line x" title="46:254	Self-edges are allowed." ></td>
	<td class="line x" title="47:254	For example, in text summarization one can create an undirected, fully connected graph on the sentences." ></td>
	<td class="line x" title="48:254	The edge between sentences i,j has weight wij, their cosine similarity." ></td>
	<td class="line x" title="49:254	In social network analysis one can create a directed graph with wij being the number of phone calls i made to j. The graph should be constructed carefully to reflect domain knowledge." ></td>
	<td class="line oc" title="50:254	For examples, see (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Pang and Lee, 2004)." ></td>
	<td class="line x" title="51:254	The user can optionally supply an arbitrary ranking on the items as prior knowledge." ></td>
	<td class="line x" title="52:254	In this case GRASSHOPPER can be viewed as a re-ranking method." ></td>
	<td class="line x" title="53:254	For example, in information retrieval, the prior ranking can be the ranking by relevance scores." ></td>
	<td class="line x" title="54:254	In text summarization, it can be the position of sentences in the original article." ></td>
	<td class="line x" title="55:254	(There is evidence that the first few sentences in an article are likely good summaries)." ></td>
	<td class="line x" title="56:254	Somewhat unconventionally, the prior ranking is represented as a probability distribution r = (r1,,rn) such that ri  0,summationtextni=1 ri = 1." ></td>
	<td class="line x" title="57:254	The highest-ranked item has the largest probability, the next item has smaller probability, and so on." ></td>
	<td class="line x" title="58:254	A distribution gives the user more control." ></td>
	<td class="line x" title="59:254	For example ra = (0.1,0.7,0.2) and rb = (0.3,0.37,0.33) both represent the same ranking of items 2, 3, 1, but with different strengths." ></td>
	<td class="line x" title="60:254	When there is no prior ranking, one can let r = (1/n,,1/n), the uniform distribution." ></td>
	<td class="line x" title="61:254	2.2 Finding the First Item We find the first item in GRASSHOPPER ranking by teleporting random walks." ></td>
	<td class="line x" title="62:254	Imagine a random walker on the graph." ></td>
	<td class="line x" title="63:254	At each step, the walker may do one of two things: with probability , she moves to a neighbor state1 according to the edge weights; otherwise she is teleported to a random state according to the distribution r. Under mild conditions (which are satisfied in our setting, see below), the stationary distribution of the random walk defines the visiting probabilities of the nodes." ></td>
	<td class="line x" title="64:254	The states with large probabilities can be regarded as central items, an idea used in Google PageRank (Page et al. , 1998) and other information retrieval systems (Kurland and Lee, 2005; Zhang et al. , 2005), text summarization (Erkan and Radev, 2004), keyword extraction (Mihalcea and Tarau, 2004) and so on." ></td>
	<td class="line x" title="65:254	Depending on , items high on the user-supplied prior ranking r may also have large stationary probabilities, which is a way to incorporate the prior ranking." ></td>
	<td class="line x" title="66:254	As an example, we created a toy data set with 300 points in Figure 1(a)." ></td>
	<td class="line x" title="67:254	There are roughly three groups with different densities." ></td>
	<td class="line x" title="68:254	We created a fully connected graph on the data, with larger edge weights if points are closer2." ></td>
	<td class="line x" title="69:254	Figure 1(b) shows the stationary distribution of the random walk on the graph." ></td>
	<td class="line x" title="70:254	1We use state, node and item interchangeably." ></td>
	<td class="line x" title="71:254	2We use wij = exp(bardblxi xjbardbl2/0.16),  = 1." ></td>
	<td class="line x" title="72:254	98 0 5 100 2 4 6 8 0 5 10 05 100 0.005 0.01 0.015 g1 0 5 10 05 100 2 4 6 g 2 0 5 10 05 100 0.5 1 1.5 g 3 (a) (b) (c) (d) Figure 1: (a) A toy data set." ></td>
	<td class="line x" title="73:254	(b) The stationary distribution pi reflects centrality." ></td>
	<td class="line x" title="74:254	The item with the largest probability is selected as the first item g1." ></td>
	<td class="line x" title="75:254	(c) The expected number of visits v to each node after g1 becomes an absorbing state." ></td>
	<td class="line x" title="76:254	(d) After both g1 and g2 become absorbing states." ></td>
	<td class="line x" title="77:254	Note the diversity in g1,g2,g3 as they come from different groups." ></td>
	<td class="line x" title="78:254	Items at group centers have higher probabilities, and tighter groups have overall higher probabilities." ></td>
	<td class="line x" title="79:254	However, the stationary distribution does not address diversity at all." ></td>
	<td class="line x" title="80:254	If we were to rank the items by their stationary distribution, the top list would be dominated by items from the center group in Figure 1(b)." ></td>
	<td class="line x" title="81:254	Therefore we only use the stationary distribution to find the first item, and use a method described in the next section to rank the remaining items." ></td>
	<td class="line x" title="82:254	Formally we first define an n  n raw transition matrix P by normalizing the rows of W: Pij = wij/summationtextnk=1 wik, so that Pij is the probability that the walker moves to j from i. We then make the walk a teleporting random walk P by interpolating each row with the user-supplied initial distribution r: P =  P + (1 )1r, (1) where 1 is an all-1 vector, and 1r is the outer product." ></td>
	<td class="line x" title="83:254	If  < 1 and r does not have zero elements, our teleporting random walk P is irreducible (possible to go to any state from any state by teleporting), aperiodic (the walk can return to a state after any number of steps), all states are positive recurrent (the expected return time to any state is finite) and thus ergodic (Grimmett and Stirzaker, 2001)." ></td>
	<td class="line x" title="84:254	Therefore P has a unique stationary distribution pi = Ppi." ></td>
	<td class="line x" title="85:254	We take the state with the largest stationary probability to be the first item g1 in GRASSHOPPER ranking: g1 = argmaxni=1 pii." ></td>
	<td class="line x" title="86:254	2.3 Ranking the Remaining Items As mentioned early, the key idea of GRASSHOPPER is to turn ranked items into absorbing states." ></td>
	<td class="line x" title="87:254	We first turn g1 into an absorbing state." ></td>
	<td class="line x" title="88:254	Once the random walk reaches an absorbing state, the walk is absorbed and stays there." ></td>
	<td class="line x" title="89:254	It is no longer informative to compute the stationary distribution of an absorbing Markov chain, because the walk will eventually be absorbed." ></td>
	<td class="line x" title="90:254	Nonetheless, it is useful to compute the expected number of visits to each node before absorption." ></td>
	<td class="line x" title="91:254	Intuitively, those nodes strongly connected to g1 will have many fewer visits by the random walk, because the walk tends to be absorbed soon after visiting them." ></td>
	<td class="line x" title="92:254	In contrast, groups of nodes far away from g1 still allow the random walk to linger among them, and thus have more visits." ></td>
	<td class="line x" title="93:254	In Figure 1(c), once g1 becomes an absorbing node (represented by a circle on the floor), the center group is no longer the most prominent: nodes in this group have fewer visits than the left group." ></td>
	<td class="line x" title="94:254	Note now the y-axis is the number of visits instead of probability." ></td>
	<td class="line x" title="95:254	GRASSHOPPER selects the second item g2 with the largest expected number of visits in this absorbing Markov chain." ></td>
	<td class="line x" title="96:254	This naturally inhibits items similar to g1 and encourages diversity." ></td>
	<td class="line x" title="97:254	In Figure 1(c), the item near the center of the left group is selected as g2." ></td>
	<td class="line x" title="98:254	Once g2 is selected, it is converted into an absorbing state, too." ></td>
	<td class="line x" title="99:254	This is shown in Figure 1(d)." ></td>
	<td class="line x" title="100:254	The right group now becomes the most prominent, since both the left and center groups contain an absorbing state." ></td>
	<td class="line x" title="101:254	The next item g3 in ranking will come from the right group." ></td>
	<td class="line x" title="102:254	Also note the range of y-axis is smaller: 99 with more absorbing states, the random walk will be absorbed sooner." ></td>
	<td class="line x" title="103:254	The procedure is repeated until all items are ranked." ></td>
	<td class="line x" title="104:254	The name GRASSHOPPER reflects the hopping behavior on the peaks." ></td>
	<td class="line x" title="105:254	It is therefore important to compute the expected number of visits in an absorbing Markov chain." ></td>
	<td class="line x" title="106:254	Let G be the set of items ranked so far." ></td>
	<td class="line x" title="107:254	We turn the states g  G into absorbing states by setting Pgg = 1 and Pgi = 0,i negationslash= g. If we arrange items so that ranked ones are listed before unranked ones, we can write P as P = bracketleftbigg I G 0 R Q bracketrightbigg." ></td>
	<td class="line x" title="108:254	(2) Here IG is the identity matrix on G. Submatrices R and Q correspond to rows of unranked items, those from (1)." ></td>
	<td class="line x" title="109:254	It is known that the fundamental matrix N = (IQ)1 (3) gives the expected number of visits in the absorbing random walk (Doyle and Snell, 1984)." ></td>
	<td class="line x" title="110:254	In particular Nij is the expected number of visits to state j before absorption, if the random walk started at state i. We then average over all starting states to obtain vj, the expected number of visits to state j. In matrix notation, v = N 1 n|G|, (4) where |G| is the size of G. We select the state with the largest expected number of visits as the next item g|G|+1 in GRASSHOPPER ranking: g|G|+1 = argmaxni=|G|+1 vi." ></td>
	<td class="line x" title="111:254	(5) The complete GRASSHOPPER algorithm is summarized in Figure 2." ></td>
	<td class="line x" title="112:254	2.4 Some Discussions To see how  controls the tradeoff, note when  = 1 we ignore the user-supplied prior ranking r, while when  = 0 one can show that GRASSHOPPER returns the ranking specified by r. Our data in Figure 1(a) has a cluster structure." ></td>
	<td class="line x" title="113:254	Many methods have exploited such structure, e.g., (Hearst and Pedersen, 1996; Leuski, 2001; Liu and Croft, 2004)." ></td>
	<td class="line x" title="114:254	In fact, a heuristic algorithm is to first cluster the items, then pick the central items from each cluster in turn." ></td>
	<td class="line x" title="115:254	But it can be difficult to Input: W, r,  1." ></td>
	<td class="line x" title="116:254	Create the initial Markov chain P from W,r, (1)." ></td>
	<td class="line x" title="117:254	2." ></td>
	<td class="line x" title="118:254	Compute Ps stationary distribution pi." ></td>
	<td class="line x" title="119:254	Pick the first item g1 = argmaxi pii." ></td>
	<td class="line x" title="120:254	3." ></td>
	<td class="line x" title="121:254	Repeat until all items are ranked: (a) Turn ranked items into absorbing states (2)." ></td>
	<td class="line x" title="122:254	(b) Compute the expected number of visits v for all remaining items (4)." ></td>
	<td class="line x" title="123:254	Pick the next item g|G|+1 = argmaxi vi Figure 2: The GRASSHOPPER algorithm determine the appropriate number and control the shape of clusters." ></td>
	<td class="line x" title="124:254	In contrast, GRASSHOPPER does not involve clustering." ></td>
	<td class="line x" title="125:254	However it is still able to automatically take advantage of cluster structures in the data." ></td>
	<td class="line x" title="126:254	In each iteration we need to compute the fundamental matrix (3)." ></td>
	<td class="line x" title="127:254	This involves inverting an (n  |G|)  (n  |G|) matrix, which is expensive." ></td>
	<td class="line x" title="128:254	However the Q matrix is reduced by one row and one column in every iteration, but is otherwise unchanged." ></td>
	<td class="line x" title="129:254	This allows us to apply the matrix inversion lemma (Sherman-Morrison-Woodbury formula) (Press et al. , 1992)." ></td>
	<td class="line x" title="130:254	Then we only need to invert the matrix once in the first iteration, but not in subsequent iterations." ></td>
	<td class="line x" title="131:254	Space precludes a full discussion, but we point out that it presents a significant speed up." ></td>
	<td class="line x" title="132:254	A Matlab implementation can be found at http://www.cs.wisc.edu/jerryzhu/ pub/grasshopper.m. 3 Experiments 3.1 Text Summarization Multi-document extractive text summarization is a prime application for GRASSHOPPER." ></td>
	<td class="line x" title="133:254	In this task, we must select and rank sentences originating from a set of documents about a particular topic or event." ></td>
	<td class="line x" title="134:254	The goal is to produce a summary that includes all the relevant facts, yet avoids repetition that may result from using similar sentences from multiple documents." ></td>
	<td class="line x" title="135:254	In this section, we demonstrate that 100 GRASSHOPPERs balance of centrality and diversity makes it successful at this task." ></td>
	<td class="line x" title="136:254	We present empirical evidence that GRASSHOPPER achieves results competitive with the top text summarizers in the 2004 Document Understanding Conference (http: //duc.nist.gov)." ></td>
	<td class="line x" title="137:254	DUC is a yearly text summarization community evaluation, with several tasks in recent years concentrating on multi-document summarization (described in more detail below)." ></td>
	<td class="line x" title="138:254	Many successful text summarization systems achieve a balance between sentence centrality and diversity in a two-step process." ></td>
	<td class="line x" title="139:254	Here we review the LexRank system (Erkan and Radev, 2004), which is most similar to our current approach." ></td>
	<td class="line x" title="140:254	LexRank works by placing sentences in a graph, with edges based on the lexical similarity between the sentences (as determined by a cosine measure)." ></td>
	<td class="line x" title="141:254	Each sentence is then assigned a centrality score by finding its probability under the stationary distribution of a random walk on this graph." ></td>
	<td class="line x" title="142:254	Unlike the similar PageRank algorithm (Page et al. , 1998), LexRank uses an undirected graph of sentences rather than Web pages, and the edge weights are either cosine values or 0/1 with thresholding." ></td>
	<td class="line x" title="143:254	The LexRank centrality can be combined with other centrality measures, as well as sentence position information." ></td>
	<td class="line x" title="144:254	After this first step of computing centrality, a second step performs re-ranking to avoid redundancy in the highly ranked sentences." ></td>
	<td class="line x" title="145:254	LexRank uses crosssentence informational subsumption (Radev, 2000) to this end, but MMR (Carbonell and Goldstein, 1998) has also been widely used in the text summarization community." ></td>
	<td class="line x" title="146:254	These methods essentially disqualify sentences that are too lexically similar to sentences ranked higher by centrality." ></td>
	<td class="line x" title="147:254	In short, similar graph-based approaches to text summarization rely on two distinct processes to measure each sentences importance and ensure some degree of diversity." ></td>
	<td class="line x" title="148:254	GRASSHOPPER, on the other hand, achieves the same goal in a unified procedure." ></td>
	<td class="line x" title="149:254	We apply GRASSHOPPER to text summarization in the following manner." ></td>
	<td class="line x" title="150:254	Our graph contains nodes for all the sentences in a document set." ></td>
	<td class="line x" title="151:254	We used the Clair Library (http://tangra.si." ></td>
	<td class="line x" title="152:254	umich.edu/clair/clairlib) to split documents into sentences, apply stemming, and create a cosine matrix for the stemmed sentences." ></td>
	<td class="line x" title="153:254	Cosine values are computed using TF-IDF vectors." ></td>
	<td class="line x" title="154:254	As in LexRank, edges in the graph correspond to text similarity." ></td>
	<td class="line x" title="155:254	To create a sparse graph, we use the cosine threshold value of 0.1 obtained in (Erkan and Radev, 2004)." ></td>
	<td class="line x" title="156:254	Specifically, the edge weight between sentence vectors si and sj is defined as wij = braceleftBigg 1 if si sjbardblsibardblbardblsjbardbl > 0.1 0 otherwise ." ></td>
	<td class="line x" title="157:254	(6) The second input for GRASSHOPPER is an initial ranking distribution, which we derive from the position of each sentence in its originating document." ></td>
	<td class="line x" title="158:254	Position forms the basis for lead-based summaries (i.e. , using the first N sentences as the summary) and leads to very competitive summaries (Brandow et al. , 1995)." ></td>
	<td class="line x" title="159:254	We form an initial ranking for each sentence by computing p, where p is the position of the sentence in its document, and  is a positive parameter trained on a development dataset." ></td>
	<td class="line x" title="160:254	We then normalize over all sentences in all documents to form a valid distribution r  p that gives high probability to sentences closer to the beginning of documents." ></td>
	<td class="line x" title="161:254	With a larger , the probability assigned to later sentences decays more rapidly." ></td>
	<td class="line x" title="162:254	To evaluate GRASSHOPPER, we experimented with DUC datasets." ></td>
	<td class="line x" title="163:254	We train our parameters ( and ) using the DUC 2003 Task 2 data." ></td>
	<td class="line x" title="164:254	This dataset contains 30 document sets, each with an average of 10 documents about a news event." ></td>
	<td class="line x" title="165:254	We test GRASSHOPPERs performance on the DUC 2004 Task 2, Tasks 4a and 4b data." ></td>
	<td class="line x" title="166:254	DUC 2004 Task 2 has 50 document sets of 10 documents each." ></td>
	<td class="line x" title="167:254	Tasks 4a and 4b explored cross-lingual summarization." ></td>
	<td class="line x" title="168:254	These datasets consist of Arabic-to-English translations of news stories." ></td>
	<td class="line x" title="169:254	The documents in Task 4a are machine-translated, while Task 4bs are manually-translated." ></td>
	<td class="line x" title="170:254	Note that we handle the translated documents in exactly the same manner as the English documents." ></td>
	<td class="line x" title="171:254	We evaluate our results using the standard text summarization metric ROUGE (http://www." ></td>
	<td class="line x" title="172:254	isi.edu/cyl/ROUGE/)." ></td>
	<td class="line x" title="173:254	This is a recall-based measure of text co-occurrence between a machinegenerated summary and model summaries manually created by judges." ></td>
	<td class="line x" title="174:254	ROUGE metrics exist based on bigram, trigram, and 4-gram overlap, but ROUGE-1 (based on unigram matching) has been found to correlate best with human judgments (Lin and Hovy, 2003)." ></td>
	<td class="line x" title="175:254	101 Using the DUC 2003 training data, we tuned  and  on a small grid (  {0.125,0.25,0.5,1.0};   {0.0,0.0625,0.125,0.25,0.5,0.95})." ></td>
	<td class="line x" title="176:254	Specifically, for each of the 30 DUC 2003 Task 2 document sets, we computed ROUGE-1 scores comparing our generated summary to 4 model summaries." ></td>
	<td class="line x" title="177:254	We averaged the resulting ROUGE-1 scores across all 30 sets to produce a single average ROUGE-1 score to assess a particular parameter configuration." ></td>
	<td class="line x" title="178:254	After examining the results for all 24 configurations, we selected the best one:  = 0.25 and  = 0.5." ></td>
	<td class="line x" title="179:254	Table 1 presents our results using these parameter values to generate summaries for the three DUC 2004 datasets." ></td>
	<td class="line x" title="180:254	Note that the averages listed are actually averages over 4 model summaries per set, and over all the sets." ></td>
	<td class="line x" title="181:254	Following the standard DUC protocol, we list the confidence intervals calculated by ROUGE using a bootstrapping technique." ></td>
	<td class="line x" title="182:254	The final column compares our results to the official systems that participated in the DUC 2004 evaluation." ></td>
	<td class="line x" title="183:254	GRASSHOPPER is highly competitive in these text summarization tasks: in particular it ranks between the 1st and 2nd automatic systems on 2004 Task 2." ></td>
	<td class="line x" title="184:254	The lower performance in Task 4a is potentially due to the documents being machine-translated." ></td>
	<td class="line x" title="185:254	If they contain poorly translated sentences, graph edges based on cosine similarity could be less meaningful." ></td>
	<td class="line x" title="186:254	For such a task, more advanced text processing is probably required." ></td>
	<td class="line x" title="187:254	3.2 Social Network Analysis As another application of GRASSHOPPER, we identify the nodes in a social network that are the most prominent, and at the same time maximally cover the network." ></td>
	<td class="line x" title="188:254	A nodes prominence comes from its intrinsic stature, as well as the prominence of the nodes it touches." ></td>
	<td class="line x" title="189:254	However, to ensure that the topranked nodes are representative of the larger graph structure, it is important to make sure the results are not dominated by a small group of highly prominent nodes who are closely linked to one another." ></td>
	<td class="line x" title="190:254	This requirement makes GRASSHOPPER a useful algorithm for this task." ></td>
	<td class="line x" title="191:254	We created a dataset from the Internet Movie Database (IMDb) that consists of all comedy movies produced between 2000 and 2006, and have received more than 500 votes by IMDb users." ></td>
	<td class="line x" title="192:254	This results in 1027 movies." ></td>
	<td class="line x" title="193:254	We form a social network of actors by co-star relationship." ></td>
	<td class="line x" title="194:254	Not surprisingly, actors from the United States dominate our dataset, although a total of 30 distinct countries are represented." ></td>
	<td class="line x" title="195:254	We seek an actor ranking such that the top actors are prominent." ></td>
	<td class="line x" title="196:254	However, we also want the top actors to be diverse, so they represent comedians from around the world." ></td>
	<td class="line x" title="197:254	This problem is framed as a GRASSHOPPER ranking problem." ></td>
	<td class="line x" title="198:254	For each movie, we considered only the main stars, i.e., the first five cast members, who tend to be the most important." ></td>
	<td class="line x" title="199:254	The resulting list contains 3452 unique actors." ></td>
	<td class="line x" title="200:254	We formed a social network where the nodes are the actors, and undirected weighted edges connect actors who have appeared in a movie together." ></td>
	<td class="line x" title="201:254	The edge weights are equal to the number of movies from our dataset in which both actors were main stars." ></td>
	<td class="line x" title="202:254	Actors are also given a selfedge with weight 1." ></td>
	<td class="line x" title="203:254	The co-star graph is given to GRASSHOPPER as an input." ></td>
	<td class="line x" title="204:254	For the prior actor ranking, we simply let r be proportional to the number of movies in our dataset in which an actor has appeared." ></td>
	<td class="line x" title="205:254	We set the weight  = 0.95." ></td>
	<td class="line x" title="206:254	It is important to note that no country information is ever given to GRASSHOPPER." ></td>
	<td class="line x" title="207:254	We use two measurements, country coverage and movie coverage, to study the diversity and prominence of the ranking produced by GRASSHOPPER." ></td>
	<td class="line x" title="208:254	We compare GRASSHOPPER to two baselines: ranking based solely on the number of movies an actor has appeared in, MOVIECOUNT, and a randomly generated ranking, RANDOM." ></td>
	<td class="line x" title="209:254	First, we calculate country coverage as the number of different countries represented by the top k actors, for all k values." ></td>
	<td class="line x" title="210:254	Each actor represents a single countrythe country that the actor has appeared in the most." ></td>
	<td class="line x" title="211:254	We hypothesize that actors are more likely to have co-star connections to actors within the same country, so our social network may have, to some extent, a clustering structure by country." ></td>
	<td class="line x" title="212:254	Country coverage approximates the number of clusters represented at different ranks." ></td>
	<td class="line x" title="213:254	Figure 3(a) shows that country coverage grows much more rapidly for GRASSHOPPER than for MOVIECOUNT." ></td>
	<td class="line x" title="214:254	That is, we see more comedians from around the world ranked highly by GRASSHOPPER." ></td>
	<td class="line x" title="215:254	In contrast, the top ranks of MOVIECOUNT are dominated by US actors, due to the relative abundance of US movies on IMDb." ></td>
	<td class="line x" title="216:254	Many other countries are 102 Number of Average GRASSHOPPER Dataset Doc." ></td>
	<td class="line x" title="217:254	Sets ROUGE-1 95% C.I. Unofficial Rank DUC 2004 Task 2 50 0.3755 [0.3622, 0.3888] Between 1 & 2 of 34 DUC 2004 Task 4a 24 0.3785 [0.3613, 0.3958] Between 5 & 6 of 11 DUC 2004 Task 4b 24 0.4067 [0.3883, 0.4251] Between 2 & 3 of 11 Table 1: Text summarization results on DUC 2004 datasets." ></td>
	<td class="line x" title="218:254	GRASSHOPPER was configured using parameters tuned on the DUC 2003 Task 2 dataset." ></td>
	<td class="line x" title="219:254	The rightmost column lists what our rank would have been if we had participated in the DUC 2004 evaluation." ></td>
	<td class="line x" title="220:254	not represented until further down in the ranked list." ></td>
	<td class="line x" title="221:254	This demonstrates that GRASSHOPPER ranking is successful in returning a more diverse ranking." ></td>
	<td class="line x" title="222:254	Because of the absorbing states in GRASSHOPPER, the first few highly ranked US actors encourage the selection of actors from other regions of the co-star graph, which roughly correspond to different countries." ></td>
	<td class="line x" title="223:254	RANDOM achieves even higher country coverage initially, but is quickly surpassed by GRASSHOPPER." ></td>
	<td class="line x" title="224:254	The initial high coverage comes from the random selection of actors." ></td>
	<td class="line x" title="225:254	However these randomly selected actors are often not prominent, as we show next." ></td>
	<td class="line x" title="226:254	Second, we calculate movie coverage as the total number of unique movies the top k actors are in." ></td>
	<td class="line x" title="227:254	We expect that actors who have been in more movies are more prominent." ></td>
	<td class="line x" title="228:254	This is reasonable because we count an actor in a movie only if the actor is among the top five actors from that movie." ></td>
	<td class="line x" title="229:254	Our counts thus exclude actors who had only small roles in numerous movies." ></td>
	<td class="line x" title="230:254	Therefore high movie coverage roughly corresponds to ranking more prominent actors highly." ></td>
	<td class="line x" title="231:254	It is worth noting that this measure also partially accounts for diversity, since an actor whose movies completely overlap with those of higher-ranked actors contributes nothing to movie coverage (i.e. , his/her movies are already covered by higher-ranked actors)." ></td>
	<td class="line x" title="232:254	Figure 3(b) shows that the movie coverage of GRASSHOPPER grows more rapidly than MOVIECOUNT, and much more rapidly than RANDOM." ></td>
	<td class="line x" title="233:254	The results show that, while the RANDOM ranking is diverse, it is not of high quality because it fails to include many prominent actors in its high ranks." ></td>
	<td class="line x" title="234:254	This is to be expected of a random ranking." ></td>
	<td class="line x" title="235:254	Since the vast majority of the actors appear in only one movie, the movie coverage curve is roughly linear in the number of actors." ></td>
	<td class="line x" title="236:254	By ranking more prominent actors highly, the GRASSHOPPER and MOVIECOUNT movie coverage curves grow faster." ></td>
	<td class="line x" title="237:254	Many of the US actors highly ranked by MOVIECOUNT are co-stars of one another, so GRASSHOPPER outperforms MOVIECOUNT in terms of movie coverage too." ></td>
	<td class="line x" title="238:254	We inspect the GRASSHOPPER ranking, and find the top 5 actors to be Ben Stiller, Anthony Anderson, Johnny Knoxville, Eddie Murphy and Adam Sandler." ></td>
	<td class="line x" title="239:254	GRASSHOPPER also brings many countries, and major stars from those countries, into the high ranks." ></td>
	<td class="line x" title="240:254	Examples include Mads Mikkelsen (synonym to the great success the Danish film industry has had), Cem Yilmaz (famous Turkish comedy actor, caricaturist and scenarist), Jun Ji-Hyun (face of South Korean cinema), Tadanobu Asano (Japans answer to Johnny Depp), Aamir Khan (prominent Bollywood film actor), and so on3." ></td>
	<td class="line x" title="241:254	These actors are ranked significantly lower by MOVIECOUNT." ></td>
	<td class="line x" title="242:254	These results indicate that GRASSHOPPER achieves both prominence and diversity in ranking actors in the IMDb co-star graph." ></td>
	<td class="line x" title="243:254	4 Conclusions GRASSHOPPER ranking provides a unified approach for achieving both diversity and centrality." ></td>
	<td class="line x" title="244:254	We have shown its effectiveness in text summarization and social network analysis." ></td>
	<td class="line x" title="245:254	As future work, one direction is partial absorption, where at each absorbing state the random walk has an escape probability to continue the random walk instead of being absorbed." ></td>
	<td class="line x" title="246:254	Tuning the escape probability creates a continuum between PageRank (if the walk always escapes) and GRASSHOPPER (if always absorbed)." ></td>
	<td class="line x" title="247:254	In addition, we will explore the issue of parameter learning, and 3Quotes from IMDb and Wikipedia." ></td>
	<td class="line x" title="248:254	103 0 100 200 300 400 5000 5 10 15 20 25 30 k (number of actors) Number of countries covered GRASSHOPPER MOVIECOUNT RANDOM 0 100 200 300 400 5000 100 200 300 400 500 600 700 800 900 1000 k (number of actors) Number of movies covered GRASSHOPPER MOVIECOUNT RANDOM (a) Country coverage (b) Movie coverage Figure 3: (a) Country coverage at ranks up to 500, showing that GRASSHOPPER and RANDOM rankings are more diverse than MOVIECOUNT." ></td>
	<td class="line x" title="249:254	(b) Movie coverage at ranks up to 500, showing that GRASSHOPPER and MOVIECOUNT have more prominent actors than RANDOM." ></td>
	<td class="line x" title="250:254	Overall, GRASSHOPPER is the best." ></td>
	<td class="line x" title="251:254	user feedback (e.g. , This item should be ranked higher.)." ></td>
	<td class="line x" title="252:254	We also plan to apply GRASSHOPPER to a variety of tasks, including information retrieval (for example ranking news articles on the same event as in Google News, where many newspapers might use the same report and thus result in a lack of diversity), image collection summarization, and social network analysis for national security and business intelligence." ></td>
	<td class="line x" title="253:254	Acknowledgment We thank Mark Craven and the anonymous reviewers for helpful comments." ></td>
	<td class="line x" title="254:254	This work is supported in part by Wisconsin Alumni Research Foundation (WARF) and NLM training grant 5T15LM07359." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N07-1026
Data-Driven Graph Construction for Semi-Supervised Graph-Based Learning in NLP
Alexandrescu, Andrei;Kirchhoff, Katrin;"></td>
	<td class="line x" title="1:191	Proceedings of NAACL HLT 2007, pages 204211, Rochester, NY, April 2007." ></td>
	<td class="line x" title="2:191	c2007 Association for Computational Linguistics Data-Driven Graph Construction for Semi-Supervised Graph-Based Learning in NLP Andrei Alexandrescu Dept. of Computer Science and Engineering University of Washington Seattle, WA, 98195 andrei@cs.washington.edu Katrin Kirchhoff Dept. of Electrical Engineering University of Washington Seattle, WA 98195 katrin@ee.washington.edu Abstract Graph-based semi-supervised learning has recently emerged as a promising approach to data-sparse learning problems in natural language processing." ></td>
	<td class="line x" title="3:191	All graph-based algorithms rely on a graph that jointly represents labeled and unlabeled data points." ></td>
	<td class="line x" title="4:191	The problem of how to best construct this graph remains largely unsolved." ></td>
	<td class="line x" title="5:191	In this paper we introduce a data-driven method that optimizes the representation of the initial feature space for graph construction by means of a supervised classi er." ></td>
	<td class="line x" title="6:191	We apply this technique in the framework of label propagation and evaluate it on two different classi cation tasks, a multi-class lexicon acquisition task and a word sense disambiguation task." ></td>
	<td class="line x" title="7:191	Signi cant improvements are demonstrated over both label propagation using conventional graph construction and state-of-the-art supervised classi ers." ></td>
	<td class="line x" title="8:191	1 Introduction Natural Language Processing (NLP) applications bene t from the availability of large amounts of annotated data." ></td>
	<td class="line x" title="9:191	However, such data is often scarce, particularly for non-mainstream languages." ></td>
	<td class="line x" title="10:191	Semisupervised learning addresses this problem by combining large amounts of unlabeled data with a small set of labeled data in order to learn a classi cation function." ></td>
	<td class="line x" title="11:191	One class of semi-supervised learning algorithms that has recently attracted increased interest is graph-based learning." ></td>
	<td class="line x" title="12:191	Graph-based techniques represent labeled and unlabeled data points as nodes in a graph with weighted edges encoding the similarity of pairs of samples." ></td>
	<td class="line x" title="13:191	Various techniques are then available for transferring class labels from the labeled to the unlabeled data points." ></td>
	<td class="line x" title="14:191	These approaches have shown good performance in cases where the data is characterized by an underlying manifold structure and samples are judged to be similar by local similarity measures." ></td>
	<td class="line x" title="15:191	However, the question of how to best construct the graph forming the basis of the learning procedure is still an underinvestigated research problem." ></td>
	<td class="line x" title="16:191	NLP learning tasks present additional problems since they often rely on discrete or heterogeneous feature spaces for which standard similarity measures (such as Euclidean or cosine distance) are suboptimal." ></td>
	<td class="line x" title="17:191	We propose a two-pass data-driven technique for graph construction in the framework of label propagation (Zhu, 2005)." ></td>
	<td class="line x" title="18:191	First, we use a supervised classi er trained on the labeled subset to transform the initial feature space (consisting of e.g. lexical, contextual, or syntactic features) into a continuous representation in the form of soft label predictions." ></td>
	<td class="line x" title="19:191	This representation is then used as a basis for measuring similarity among samples that determines the structure of the graph used for the second, semisupervised learning step." ></td>
	<td class="line x" title="20:191	It is important to note that, rather than simply cascading the supervised and the semi-supervised learner, we optimize the combination with respect to the properties required of the graph." ></td>
	<td class="line x" title="21:191	We present several techniques for such optimization, including regularization of the rst-pass classi er, biasing by class priors, and linear combi204 nation of classi er predictions with known features." ></td>
	<td class="line x" title="22:191	The proposed approach is evaluated on a lexicon learning task using the Wall Street Journal (WSJ) corpus, and on the SENSEVAL-3 word sense disambiguation task." ></td>
	<td class="line x" title="23:191	In both cases our technique signi cantly outperforms our baseline systems (label propagation using standard graph construction and discriminatively trained supervised classi ers)." ></td>
	<td class="line oc" title="24:191	2 Background Several graph-based learning techniques have recently been developed and applied to NLP problems: minimum cuts (Pang and Lee, 2004), random walks (Mihalcea, 2005; Otterbacher et al. , 2005), graph matching (Haghighi et al. , 2005), and label propagation (Niu et al. , 2005)." ></td>
	<td class="line x" title="25:191	Here we focus on label propagation as a learning technique." ></td>
	<td class="line x" title="26:191	2.1 Label propagation The basic label propagation (LP) algorithm (Zhu and Ghahramani, 2002; Zhu, 2005) has as inputs:  a labeled set(x1,y1), (x2,y2),,, (xn,yn)}, where xi are samples (feature vectors) and yi  {1, 2,,,C}are their corresponding labels;  an unlabeled setxn+1,,,xN};  a distance measure d(i,j) i,j {1, N}dened on the feature space." ></td>
	<td class="line x" title="27:191	The goal is to infer the labels {yn+1,,,yN} for the unlabeled set." ></td>
	<td class="line x" title="28:191	The algorithm represents all N data points as vertices in an undirected graph with weighted edges." ></td>
	<td class="line x" title="29:191	Initially, only the known data vertices are labeled." ></td>
	<td class="line x" title="30:191	The edge linking vertices i and j has weight: wij = exp parenleftbigg d(i,j) 2 2 parenrightbigg (1) where  is a hyperparameter that needs to be empirically chosen or learned separately." ></td>
	<td class="line x" title="31:191	wij indicates the label af nity of vertices: the larger wij is, the more likely it is that i and j have the same label." ></td>
	<td class="line x" title="32:191	The LP algorithm constructs a row-normalized NN transition probability matrix P as follows: Pij = P(ij) = wijsummationtextN k=1 wik (2) The algorithm probabilistically pushes labels from the labeled nodes to the unlabeled nodes." ></td>
	<td class="line x" title="33:191	To do so, it de nes the nC hard labels matrix Y and the NC soft labels matrix f, whose rst n rows are identical to Y. The hard labels matrix Y is invariant through the algorithm and is initialized with probability 1 for the known label and 0 for all other labels: Yic = (yi,C) (3) where  is Kroneckers delta function." ></td>
	<td class="line x" title="34:191	The algorithm iterates as follows: 1." ></td>
	<td class="line x" title="35:191	fprimeP f 2." ></td>
	<td class="line x" title="36:191	fprime[rows 1 to n] Y 3." ></td>
	<td class="line x" title="37:191	If fprime= f, stop 4." ></td>
	<td class="line x" title="38:191	f fprime 5." ></td>
	<td class="line x" title="39:191	Repeat from step 1 In each iteration, step 2 xes the known labels, which might otherwise be overriden by propagated labels." ></td>
	<td class="line x" title="40:191	The resulting labels for each feature xi, where i{n + 1,,,N}, are: li = arg max j=1,,C fij (4) It is important that the distance measure is locally accurate, i.e. nodes connected by an edge with a high weight should have the same label." ></td>
	<td class="line x" title="41:191	The global distance is less relevant since label information will be propagated from labeled points through the entire space." ></td>
	<td class="line x" title="42:191	This is why LP works well with a local distance measure that might be unsuitable as a global distance measure." ></td>
	<td class="line x" title="43:191	Applications of LP include handwriting recognition (Zhu and Ghahramani, 2002), image classi cation (Balcan et al. , 2005) and retrieval (Qin et al. , 2005), and protein classi cation (Weston et al. , 2003)." ></td>
	<td class="line x" title="44:191	In NLP, label propagation has been used for word sense disambiguation (Niu et al. , 2005), document classi cation (Zhu, 2005), sentiment analysis (Goldberg and Zhu, 2006), and relation extraction (Chen et al. , 2006)." ></td>
	<td class="line x" title="45:191	2.2 Graph construction One of the main problems in LP, as well as other graph-based learning techniques, is how to best construct the graph." ></td>
	<td class="line x" title="46:191	Currently, graph construction is more of an art than science (Zhu, 2005)." ></td>
	<td class="line x" title="47:191	Typically, edge weights are derived from a simple Euclidean or cosine distance measure, regardless of the nature of the underlying features." ></td>
	<td class="line x" title="48:191	Edges are then established either by connecting all nodes, by applying a single global threshold to the edge weights, or by connecting each node to its k nearest neighbors according to the edge weights." ></td>
	<td class="line x" title="49:191	This procedure is often suboptimal: Euclidean distance relies on a model of normally distributed i.i.d. random variables; cosine 205 distance likewise assumes that the different feature vector dimensions are uncorrelated." ></td>
	<td class="line x" title="50:191	However, many applications, particularly in NLP, rely on feature spaces with correlated dimensions." ></td>
	<td class="line x" title="51:191	Moreover, features may have different ranges and different types (e.g. continuous, binary, multi-valued), which entails the need for normalization, binning, or scaling." ></td>
	<td class="line x" title="52:191	Finally, common distance measures do not take advantage of domain knowledge that might be available." ></td>
	<td class="line x" title="53:191	Some attempts have been made at improving the standard method of graph construction." ></td>
	<td class="line x" title="54:191	For instance, in a face identi cation task (Balcan et al. , 2005), domain knowledge was used to identify three different edge sets based on time, color and face features, associating a different hyperparameter with each." ></td>
	<td class="line x" title="55:191	The resulting graph was then created by superposing edge sets." ></td>
	<td class="line x" title="56:191	Zhu (Zhu, 2005, Ch." ></td>
	<td class="line x" title="57:191	7) describes graph construction using separate  hyperparameters for each feature dimension, and presents a datadriven way (evidence maximization) for learning the values of the parameters." ></td>
	<td class="line x" title="58:191	3 Data-driven graph construction Unlike previous work, we propose to optimize the feature representation used for graph construction by learning it with a rst-pass supervised classier." ></td>
	<td class="line x" title="59:191	Under this approach, similarity of samples is de ned as similarity of the output values produced by a classi er applied to the original feature representation of the samples." ></td>
	<td class="line x" title="60:191	This idea bears similarity to classi er cascading (Alpaydin and Kaynak, 1998), where classi ers are trained around a ruleexceptions paradigm; however, in our case, the classi ers work together, the rst acting as a jointly optimized feature mapping function for the second." ></td>
	<td class="line x" title="61:191	1." ></td>
	<td class="line x" title="62:191	Train a rst-pass supervised classi er that outputs soft label predictions Zi for all samples i  {1, N}, e.g. a posterior probability distribution over target labels: Zi = pi1,pi2,,,piC; 2." ></td>
	<td class="line x" title="63:191	Apply postprocessing to Zi if needed." ></td>
	<td class="line x" title="64:191	3." ></td>
	<td class="line x" title="65:191	Use vectors Zi and an appropriately chosen distance measure to construct a graph for LP." ></td>
	<td class="line x" title="66:191	4." ></td>
	<td class="line x" title="67:191	Perform label propagation over the constructed graph to nd the labeling of the test samples." ></td>
	<td class="line x" title="68:191	The advantages of this procedure are:  Uniform range and type of features: The output from a rst-pass classi er can produce wellde ned features, e.g. posterior probability distributions." ></td>
	<td class="line x" title="69:191	This eliminates the problem of input features of different ranges and types (e.g. binary vs. multivalued, continuous vs. categorical attributes) which are often used in combination." ></td>
	<td class="line x" title="70:191	 Feature postprocessing: The transformation of features into a different space also opens up possibilities for postprocessing (e.g. probability distribution warping) depending on the requirements of the second-pass learner." ></td>
	<td class="line x" title="71:191	In addition, different distance functions (e.g. those de ned on probability spaces) can be used, which avoids violating assumptions made by metrics such as Euclidean and cosine distance." ></td>
	<td class="line x" title="72:191	 Optimizing class separation: The learned representation of labeled training samples might reveal better clusters in the data than the original representation: a discriminatively-trained rst pass classi er will attempt to maximize the separation of samples belonging to different classes." ></td>
	<td class="line x" title="73:191	Moreover, the rstpass classi er may learn a feature transformation that suppresses noise in the original input space." ></td>
	<td class="line x" title="74:191	Dif culties with the proposed approach might arise when the rst-pass classi er yields con dent but wrong predictions, especially for outlier samples in the original space." ></td>
	<td class="line x" title="75:191	For this reason, the rst-pass classi er and the graph-based learner should not simply be concatenated without modi cation, but the rst classi er should be optimized with respect to the requirements of the second." ></td>
	<td class="line x" title="76:191	In our case, the choice of rst-pass classi er and joint optimization techniques are determined by the particular learning task and are detailed below." ></td>
	<td class="line x" title="77:191	4 Tasks 4.1 Lexicon acquisition task Our rst task is a part-of-speech (POS) lexicon acquisition task, i.e. the labels to be predicted are the sets of POS tags associated with each word in a lexicon." ></td>
	<td class="line x" title="78:191	Note that this is not a tagging task: we are not attempting to identify the correct POS of each word in running text." ></td>
	<td class="line x" title="79:191	Rather, for each word in the vocabulary, we attempt to infer the set of possible POS tags." ></td>
	<td class="line x" title="80:191	Our choice of this task is motivated by our long-term goal of applying this technique to lexicon acquisition for resource-poor languages: POS lexi206 cons are one of the most basic language resources, which enable subsequent training of taggers, chunkers, etc. We assume that a small set of words can be reliably annotated, and that POS-sets for the remaining words can be inferred by semi-supervised learning." ></td>
	<td class="line x" title="81:191	Rather than choosing a genuinely resource-poor language for this task, we use the English Wall Street Journal (WSJ) corpus and arti cially limit the size of the labeled set." ></td>
	<td class="line x" title="82:191	This is because the WSJ corpus is widely obtainable and allows easy replication of our experiments." ></td>
	<td class="line x" title="83:191	We use sections 0-18 of the Wall Street Journal corpus (N = 44, 492)." ></td>
	<td class="line x" title="84:191	Words have between 1 and 4 POS tags, with an average of 1.1 per word." ></td>
	<td class="line x" title="85:191	The number of POS tags is 36, and we treat every POS combination as a unique class, resulting in C = 158 distinct labels." ></td>
	<td class="line x" title="86:191	We use three different randomly selected training sets of various sizes: 5000, 10000, and 15000 words, representing about 11%, 22%, and 34% of the entire data set respectively; the rest of the data was used for testing." ></td>
	<td class="line x" title="87:191	In order to avoid experimental bias, we run all experiments on ve different randomly chosen labeled subsets and report averages and standard deviations." ></td>
	<td class="line x" title="88:191	Due to the random sampling of the data it is possible that some labels never occur in the training set or only occur once." ></td>
	<td class="line x" title="89:191	We train our classi ers only on those labels that occur at least twice, which results in 60-63 classes." ></td>
	<td class="line x" title="90:191	Labels not present in the training set will therefore not be hypothesized and are guaranteed to be errors." ></td>
	<td class="line x" title="91:191	We delete samples with unknown labels from our unlabeled set since their percentage is less than 0.5% on average." ></td>
	<td class="line x" title="92:191	We use the following features to represent samples:  Integer: the three-letter suf x of the word;  Integer: The four-letter suf x of the word;  Integer 4: The indices of the four most frequent words that immediately precede the word in the WSJ text;  Boolean: word contains capital letters;  Boolean: word consists only of capital letters;  Boolean: word contains digits;  Boolean: word contains a hyphen;  Boolean: word contains other special characters (e.g. & )." ></td>
	<td class="line x" title="93:191	We have also experimented with shorter suf xes and with pre xes but those features tended to degrade performance." ></td>
	<td class="line x" title="94:191	4.2 SENSEVAL-3 word sense disambiguation task The second task is word sense disambiguation using the SENSEVAL-3 corpus (Mihalcea et al. , 2004), to enable a comparison of our method with previously published results." ></td>
	<td class="line x" title="95:191	The goal is to disambiguate the different senses of each of 57 words given the sentences within which they occur." ></td>
	<td class="line x" title="96:191	There are 7860 samples for training and 3944 for testing." ></td>
	<td class="line x" title="97:191	In line with existing work (Lee and Ng, 2002; Niu et al. , 2005), we use the following features:  Integer  7: seven features consisting of the POS of the previous three words, the POS of the next three words, and the POS of the word itself." ></td>
	<td class="line x" title="98:191	We used the MXPOST tagger (Ratnaparkhi, 1996) for POS annotation." ></td>
	<td class="line x" title="99:191	 Integervariable length: a bag of all words in the surrounding context." ></td>
	<td class="line x" title="100:191	 Integer  15: Local collocations Cij (i, j are the bounds of the collocation window) word combinations from the context of the word to disambiguate." ></td>
	<td class="line x" title="101:191	In addition to the 11 collocations used in similar work (Lee and Ng, 2002), we also used C3,1, C3,2, C2,3, C1,3." ></td>
	<td class="line x" title="102:191	Note that syntactic features, which have been used in some previous studies on this dataset (Mohammad and Pedersen, 2004), were not included." ></td>
	<td class="line x" title="103:191	We apply a simple feature selection method: a feature X is selected if the conditional entropy H(Y|X) is above a xed threshold (1 bit) in the training set, and if X also occurs in the test set (note that no label information from the test data is used for this purpose)." ></td>
	<td class="line x" title="104:191	5 Experiments For both tasks we compare the performance of a supervised classi er, label propagation using the standard input features and either Euclidean or cosine distance, and LP using the output from a rst-pass supervised classi er." ></td>
	<td class="line x" title="105:191	5.1 Lexicon acquisition task 5.1.1 First-pass classi er For this task, the rst-pass classi er is a multilayer perceptron (MLP) with the topology shown in Fig." ></td>
	<td class="line x" title="106:191	1." ></td>
	<td class="line x" title="107:191	The input features are mapped to con207 x 2 x 4 x 1 x 3 P(y | x) M i h o Wih W ho A Figure 1: Architecture of rst-pass supervised classi er (MLP) for lexicon acquisition . tinuous values by a discrete-to-continuous mapping layer M, which is itself learned during the MLP training process." ></td>
	<td class="line x" title="108:191	This layer connects to the hidden layer h, which in turn is connected to the output layer o. The entire network is trained via backpropagation." ></td>
	<td class="line x" title="109:191	The training criterion maximizes the regularized log-likelihood of the training data: L = 1n nsummationdisplay t=1 log P(yt|xt,) + R() (5) The use of an additional continuous mapping layer is similar to the use of hidden continuous word representations in neural language modeling (Bengio et al. , 2000) and yields better results than a standard 3-layer MLP topology." ></td>
	<td class="line x" title="110:191	Problems caused by data scarcity arise when some of the input features of the unlabeled words have never been seen in the training set, resulting in untrained, randomly-initialized values for those feature vector components." ></td>
	<td class="line x" title="111:191	We address this problem by creating an approximation layer A that nds the known input feature vector xprime that is most similar to x (by measuring the cosine similarity between the vectors)." ></td>
	<td class="line x" title="112:191	Then xk is replaced with xprimek, resulting in vector x =x1,,,xk1,xprimek,xk+1,,,xfthat has no unseen features and is closest to the original vector." ></td>
	<td class="line x" title="113:191	5.1.2 LP Setup We use a dense graph approach." ></td>
	<td class="line x" title="114:191	The WSJ set has a total of 44,492 words, therefore the P matrix that the algorithm requires would have 44, 492 44, 492= 2109 elements." ></td>
	<td class="line x" title="115:191	Due to the matrix size, we avoid the analytical solution of the LP problem, which requires inverting the P matrix, and choose the iterative approach described above (Sec." ></td>
	<td class="line x" title="116:191	2.1) instead." ></td>
	<td class="line x" title="117:191	Convergence is stopped when the maximum relative difference between each cell of f and the corresponding cell of fprime is less than 1%." ></td>
	<td class="line x" title="118:191	Also for data size reasons, we apply LP in chunks." ></td>
	<td class="line x" title="119:191	While the training set stays in memory, the test data is loaded in xed-size chunks, labeled, and discarded." ></td>
	<td class="line x" title="120:191	This approach has yielded similar results for various chunk sizes, suggesting that chunking is a good approximation of whole-set label propagation.1 LP in chunks is also amenable to parallelization: Our system labels different chunks in parallel." ></td>
	<td class="line x" title="121:191	We trained the  hyperparameter by three-fold cross-validation on the training data, using a geometric progression with limits 0.1 and 10 and ratio 2." ></td>
	<td class="line x" title="122:191	We set xed upper limits of edges between an unlabeled node and its labeled neighbors to 15, and between an unlabeled node and its unlabeled neighbors to 5." ></td>
	<td class="line x" title="123:191	The approach of setting different limits among different kinds of nodes is also used in related work (Goldberg and Zhu, 2006)." ></td>
	<td class="line x" title="124:191	For graph construction we tested: (a) the original discrete input representation with cosine distance; (b) the classi er output features (probability distributions) with the Jeffries-Matusita distance." ></td>
	<td class="line x" title="125:191	5.2 Combination optimization The static parameters of the MLP (learning rate, regularization rate, and number of hidden units) were optimized for the LP step by 5-fold cross-validation on the training data." ></td>
	<td class="line x" title="126:191	This process is important because overspecialization is detrimental to the combined system: an overspecialized rst-pass classier may output very con dent but wrong predictions for unseen patterns, thus placing such samples at large distances from all correctly labeled samples." ></td>
	<td class="line x" title="127:191	A strongly regularized neural network, by contrast, will output smoother probability distributions for unseen patterns." ></td>
	<td class="line x" title="128:191	Such outputs also result in a smoother graph, which in turn helps the LP process." ></td>
	<td class="line x" title="129:191	Thus, we found that a network with only 12 hidden units and relatively high R() in Eq." ></td>
	<td class="line x" title="130:191	5 (10% of the weight value) performed best in combination with LP (at an insigni cant cost in accuracy when used 1In fact, experiments have shown that performance tends to degrade for larger chunk sizes, suggesting that whole-set LP might be affected by artifact clusters that are not related to the labels." ></td>
	<td class="line x" title="131:191	208 as an isolated classi er)." ></td>
	<td class="line x" title="132:191	5.2.1 Results We rst conducted an experiment to measure the smoothness of the underlying graph, S(G), in the two LP experiments according to the following formula: S(G) = summationdisplay yinegationslash=yj,(i>nj>n) wij (6) where yi is the label of sample i." ></td>
	<td class="line x" title="133:191	(Lower values are better as they re ect less af nity between nodes of different labels)." ></td>
	<td class="line x" title="134:191	The value of S(G) was in all cases signi cantly better on graphs constructed with our proposed technique than on graphs constructed in the standard way (see Table 1)." ></td>
	<td class="line x" title="135:191	Table 1 also shows the performance comparison between LP over the discrete representation and cosine distance ( LP ), the neural network itself ( NN ), and LP over the continuous representation ( NN+LP ), on all different subsets and for different training sizes." ></td>
	<td class="line x" title="136:191	For scarce labeled data (5000 samples) the neural network, which uses a strictly supervised training procedure, is at a clear disadvantage." ></td>
	<td class="line x" title="137:191	However, for a larger training set the network is able to perform more accurately than the LP learner that uses the discrete features directly." ></td>
	<td class="line x" title="138:191	The third, combined technique outperforms the rst two signi cantly.2 The differences are more pronounced for smaller training set sizes." ></td>
	<td class="line x" title="139:191	Interestingly, the LP is able to extract information from largely erroneous (noisy) distributions learned by the neural network." ></td>
	<td class="line x" title="140:191	5.3 Word Sense Disambiguation We compare the performance of an SVM classi er, an LP learner using the same input features as the SVM, and an LP learner using the SVM outputs as input features." ></td>
	<td class="line x" title="141:191	To analyze the in uence of training set size on accuracy, we randomly sample subsets of the training data (25%, 50%, and 75%) and use the remaining training data plus the test data as unlabeled data, similarly to the procedure followed in related work (Niu et al. , 2005)." ></td>
	<td class="line x" title="142:191	The results are averaged over ve different random samplings." ></td>
	<td class="line x" title="143:191	The samplings were chosen such that there was at least one sample for each label in the training set." ></td>
	<td class="line x" title="144:191	SENSEVAL-3 sports multi-labeled samples and 2Signi cance was tested using a difference of proportions signi cance test; the signi cance level is 0.01 or smaller in all cases." ></td>
	<td class="line x" title="145:191	samples with the unknown label." ></td>
	<td class="line x" title="146:191	We eliminate all samples labeled as unknown and retain only the rst label for the multi-labeled instances." ></td>
	<td class="line x" title="147:191	5.3.1 SVM setup The use of SVM vs. MLP in this case was justied by the very small training data set." ></td>
	<td class="line x" title="148:191	An MLP has many parameters and needs a considerable amount of data for effective training, so for this task with only on the order of 102 training samples per classier, an SVM was deemed more appropriate." ></td>
	<td class="line x" title="149:191	We use the SVMlight package to build a set of binary classi ers in a one-versus-all formulation of the multiclass classi cation problem." ></td>
	<td class="line x" title="150:191	The features input to each SVM consist of the discrete features described above (Sec." ></td>
	<td class="line x" title="151:191	4.2) after feature selection." ></td>
	<td class="line x" title="152:191	After training SVMs for each target label against the union of all others, we evaluate the SVM approach against the test set by using the winner-takes-all strategy: the predicted label corresponds to the SVM that outputs the largest value." ></td>
	<td class="line x" title="153:191	5.3.2 LP setup Again we set up two LP systems: one using the original feature space (after feature selection, which bene ted all of the tested systems) and one using the SVM outputs." ></td>
	<td class="line x" title="154:191	Both use a cosine distance measure." ></td>
	<td class="line x" title="155:191	The  parameter (see Eq." ></td>
	<td class="line x" title="156:191	1) is optimized through 3-fold cross-validation on the training set." ></td>
	<td class="line x" title="157:191	5.4 Combination optimization Unlike MLPs, SVMs do not compute a smooth output distribution but base the classi cation decision on the sign of the output values." ></td>
	<td class="line x" title="158:191	In order to smooth output values with a view towards graph construction we applied the following techniques: 1." ></td>
	<td class="line x" title="159:191	Combining SVM predictions and perfect feature vectors: After training, the SVM actually outputs wrong label predictions for a small number (5%) of training samples." ></td>
	<td class="line x" title="160:191	These outputs could simply be replaced with the perfect SVM predictions (1 for the true class, -1 elsewhere) since the labels are known." ></td>
	<td class="line x" title="161:191	However, the second-pass learner might actually benet from the information contained in the misclassi cations." ></td>
	<td class="line x" title="162:191	We therefore linearly combine the SVM predictions with the perfect feature 209 Initial labels Model S(G) avg." ></td>
	<td class="line x" title="163:191	Accuracy (%) Set 1 Set 2 Set 3 Set 4 Set 5 Average 5000 NN  50.70 59.22 63.77 60.09 54.58 57.67  4.55 LP 451.54 58.37 59.91 60.88 62.01 59.47 60.13  1.24 NN+LP 409.79 58.03 63.91 66.62 65.93 57.76 62.45  3.83 10000 NN  65.86 60.19 67.52 65.68 65.64 64.98  2.49 LP 381.16 58.27 60.04 60.85 61.99 62.06 60.64  1.40 NN+LP 315.53 69.36 64.73 69.50 70.26 67.71 68.31  1.97 15000 NN  69.85 66.42 70.88 70.71 72.18 70.01  1.94 LP 299.10 58.51 61.00 60.94 63.53 60.98 60.99  1.59 NN+LP 235.83 70.59 69.45 69.99 71.20 73.45 70.94  1.39 Table 1: Accuracy results of neural classi cation (NN), LP with discrete features (LP), and combined (NN+LP), over 5 random samplings of 5000, 10000, and 15000 labeled words in the WSJ lexicon acquisition task." ></td>
	<td class="line x" title="164:191	S(G) is the smoothness of the graph vectors v that contain 1 at the correct label position and -1 elsewhere: sprimei = si + (1)vi (7) where si, sprimei are the ith input and output feature vectors and  a parameter xed at 0.5." ></td>
	<td class="line x" title="165:191	2. Biasing uninformative distributions: For some training samples, although the predicted class label was correct, the outputs of the SVM were relatively close to one another, i.e. the decision was borderline." ></td>
	<td class="line x" title="166:191	We decided to bias these SVM outputs in the right direction by using the same formula as in equation 7." ></td>
	<td class="line x" title="167:191	3. Weighting by class priors: For each training sample, a corresponding sample with the perfect output features was added, thus doubling the total number of labeled nodes in the graph." ></td>
	<td class="line x" title="168:191	These synthesized nodes are akin to the dongle nodes (Goldberg and Zhu, 2006)." ></td>
	<td class="line x" title="169:191	The difference is that, while dongle nodes are only linked to one node, our arti cial nodes are treated like any other node and as such can connect to several other nodes." ></td>
	<td class="line x" title="170:191	The role of the articial nodes is to serve as authorities during the LP process and to emphasize class priors." ></td>
	<td class="line x" title="171:191	5.4.1 Results As before, we measured the smoothness of the graphs in the two label propagation setups and found that in all cases the smoothness of the graph produced with our method was better when compared to the graphs produced using the standard approach, as shown in Table 3, which also shows accuracy results for the SVM ( SVM label), LP over the standard graph ( LP ), and label propagation over SVM outputs ( SVM+LP )." ></td>
	<td class="line x" title="172:191	The latter system consistently performs best in all cases, although the most marked gains occur in the upper range of labeled samples percentage." ></td>
	<td class="line x" title="173:191	The gain of the best data-driven LP over the knowledge-based LP is signi cant in the 100% and 75% cases." ></td>
	<td class="line x" title="174:191	# System Acc." ></td>
	<td class="line x" title="175:191	(%) 1 htsa3 (Grozea, 2004) 72.9 2 IRST-kernels (Strapparava et al. , 2004) 72.6 3 nusels (Lee et al. , 2004) 72.4 4 SENSEVAL-3 contest baseline 55.2 5 Niu et al.(Niu et al. , 2005) LP/J-S 70.3 6 Niu et al. LP/cosine 68.4 7 Niu et al. SVM 69.7 Table 2: Accuracy results of other published systems on SENSEVAL-3." ></td>
	<td class="line x" title="177:191	1-3 use syntactic features; 5-7 are directly comparably to our system." ></td>
	<td class="line x" title="178:191	For comparison purposes, Table 2 shows results of other published systems against the SENSEVAL corpus." ></td>
	<td class="line x" title="179:191	The htsa3, IRST-kernels, and nusels systems were the winners of the SENSEVAL-3 contest and used extra input features (syntactic relations)." ></td>
	<td class="line x" title="180:191	The Niu et al. work (Niu et al. , 2005) is most comparable to ours." ></td>
	<td class="line x" title="181:191	We attribute the slightly higher performance of our SVM due to our feature selection process." ></td>
	<td class="line x" title="182:191	The LP/cosine system is a system similar to our LP system using the discrete features, and the LP/Jensen-Shannon system is also similar but uses a distance measure derived from JensenShannon divergence." ></td>
	<td class="line x" title="183:191	6 Conclusions We have presented a data-driven graph construction technique for label propagation that utilizes a rst210 Initial labels Model S(G) avg." ></td>
	<td class="line x" title="184:191	Accuracy (%) Set 1 Set 2 Set 3 Set 4 Set 5 Average 25% SVM  62.94 62.53 62.69 63.52 62.99 62.93  0.34 LP 44.71 63.27 61.84 63.26 62.96 63.30 62.93  0.56 SVM+LP 39.67 63.39 63.20 63.95 63.68 63.91 63.63  0.29 50% SVM  67.90 66.75 67.57 67.44 66.79 67.29  0.45 LP 33.17 67.84 66.57 67.35 66.52 66.35 66.93  0.57 SVM+LP 24.19 67.95 67.54 67.93 68.21 68.11 67.95  0.23 75% SVM  69.54 70.19 68.75 69.80 68.73 69.40  0.58 LP 29.93 68.87 68.65 68.58 68.42 67.19 68.34  0.59 SVM+LP 16.19 69.98 70.05 69.69 70.38 68.94 69.81  0.49 100% SVM  70.74 LP 21.72 69.69 SVM+LP 13.17 71.72 Table 3: Accuracy results of support vector machine (SVM), label propagation over discrete features (LP), and label propagation over SVM outputs (SVM+LP), each trained with 25%, 50%, 75% (5 random samplings each), and 100% of the train set." ></td>
	<td class="line x" title="185:191	The improvements of SVM+LP are signi cant over LP in the 75% and 100% cases." ></td>
	<td class="line x" title="186:191	S(G) is the graph smoothness pass supervised classi er." ></td>
	<td class="line x" title="187:191	The outputs from this classi er (especially when optimized for the secondpass learner) were shown to serve as a better representation for graph-based semi-supervised learning." ></td>
	<td class="line x" title="188:191	Classi cation results on two learning tasks showed signi cantly better performance compared to LP using standard graph construction and the supervised classi er alone." ></td>
	<td class="line x" title="189:191	Acknowledgments This work was funded by NSF under grant no." ></td>
	<td class="line x" title="190:191	IIS-0326276." ></td>
	<td class="line x" title="191:191	Any opinions, ndings and conclusions, or recommendations expressed herein are those of the authors and do not necessarily re ect the views of this agency." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N07-1033
Using ``Annotator Rationales'' to Improve Machine Learning for Text Categorization
Zaidan, Omar F.;Eisner, Jason M.;Piatko, Christine;"></td>
	<td class="line x" title="1:231	Proceedings of NAACL HLT 2007, pages 260267, Rochester, NY, April 2007." ></td>
	<td class="line x" title="2:231	c2007 Association for Computational Linguistics Using Annotator Rationales to Improve Machine Learning for Text Categorization Omar F. Zaidan and Jason Eisner Department of Computer Science Johns Hopkins University Baltimore, MD 21218, USA {ozaidan,jason}@cs.jhu.edu Christine D. Piatko JHU Applied Physics Laboratory 11100 Johns Hopkins Road Laurel, MD 20723 USA christine.piatko@jhuapl.edu Abstract We propose a new framework for supervised machine learning." ></td>
	<td class="line x" title="3:231	Our goal is to learn from smaller amounts of supervised training data, by collecting a richer kind of training data: annotations with rationales. When annotating an example, the human teacher will also highlight evidence supporting this annotationthereby teaching the machine learner why the example belongs to the category." ></td>
	<td class="line x" title="4:231	We provide some rationale-annotated data and present a learning method that exploits the rationales during trainingtoboostperformancesignificantlyonasample task, namely sentiment classification of movie reviews." ></td>
	<td class="line x" title="5:231	We hypothesize that in some situations, providing rationales is a more fruitful use of an annotators time than annotating more examples." ></td>
	<td class="line x" title="6:231	1 Introduction Annotation cost is a bottleneck for many natural language processing applications." ></td>
	<td class="line x" title="7:231	While supervised machine learning systems are effective, it is laborintensive and expensive to construct the many training examples needed." ></td>
	<td class="line x" title="8:231	Previous research has exploredactiveorsemi-supervisedlearningaspossible ways to lessen this burden." ></td>
	<td class="line x" title="9:231	Weproposeanewwayofbreakingthisannotation bottleneck." ></td>
	<td class="line x" title="10:231	Annotators currently indicate what the correct answers are on training data." ></td>
	<td class="line x" title="11:231	We propose that they should also indicate why, at least by coarse hints." ></td>
	<td class="line x" title="12:231	We suggest new machine learning approaches that can benefit from this why information." ></td>
	<td class="line x" title="13:231	For example, an annotator who is categorizing phrases or documents might also be asked to highlight a few substrings that significantly influenced her judgment." ></td>
	<td class="line x" title="14:231	We call such clues rationales. They need not correspond to machine learning features." ></td>
	<td class="line x" title="15:231	This work was supported by the JHU WSE/APL Partnership Fund; National Science Foundation grant No. 0347822 to the second author; and an APL Hafstad Fellowship to the third." ></td>
	<td class="line x" title="16:231	In some circumstances, rationales should not be too expensive or time-consuming to collect." ></td>
	<td class="line x" title="17:231	As long as the annotator is spending the time to study example xi and classify it, it may not require much extra effort for her to mark reasons for her classification." ></td>
	<td class="line x" title="18:231	2 Using Rationales to Aid Learning We will not rely exclusively on the rationales, but use them only as an added source of information." ></td>
	<td class="line x" title="19:231	The idea is to help direct the learning algorithms attentionhelping it tease apart signal from noise." ></td>
	<td class="line x" title="20:231	Machine learning algorithms face a well-known credit assignment problem." ></td>
	<td class="line x" title="21:231	Given a complex datum xi and the desired response yi, many features of xi could be responsible for the choice of yi." ></td>
	<td class="line x" title="22:231	The learning algorithm must tease out which features were actually responsible." ></td>
	<td class="line x" title="23:231	This requires a lot of training data, and often a lot of computation as well." ></td>
	<td class="line x" title="24:231	Our rationales offer a shortcut to solving this credit assignment problem, by providing the learning algorithm with hints as to which features of xi were relevant." ></td>
	<td class="line x" title="25:231	Rationales should help guide the learning algorithm toward the correct classification function, by pushing it toward a function that correctly pays attention to each examples relevant features." ></td>
	<td class="line x" title="26:231	This should help the algorithm learn from lessdataandavoidgettingtrappedinlocalmaxima.1 In this paper, we demonstrate the annotator rationales technique on a text categorization problem previously studied by others." ></td>
	<td class="line x" title="27:231	1To understand the local maximum issue, consider the hard problem of training a standard 3-layer feed-forward neural network." ></td>
	<td class="line x" title="28:231	If the activations of the hidden layers features (nodes) were observed at training time, then the network would decompose into a pair of independent 2-layer perceptrons." ></td>
	<td class="line x" title="29:231	This turns an NP-hard problem with local maxima (Blum and Rivest, 1992) to a polytime-solvable convex problem." ></td>
	<td class="line x" title="30:231	Although rationales might only provide indirect evidence of the hidden layer, this would still modify the objective function (see section 8) in a way that tended to make the correct weights easier to discover." ></td>
	<td class="line x" title="31:231	260 3 Discriminative Approach One popular approach for text categorization is to use a discriminative model such as a Support Vector Machine (SVM) (e.g.(Joachims, 1998; Dumais, 1998))." ></td>
	<td class="line x" title="33:231	We propose that SVM training can in general incorporate annotator rationales as follows." ></td>
	<td class="line x" title="34:231	From the rationale annotations on a positive example xi, we will construct one or more not-quiteas-positive contrast examples vij." ></td>
	<td class="line x" title="35:231	In our text categorization experiments below, each contrast document vij was obtained by starting with the original and masking out one or all of the several rationale substrings that the annotator had highlighted (rij)." ></td>
	<td class="line x" title="36:231	The intuition is that the correct model should be less sureofapositiveclassificationonthecontrastexample vij than on the original example vectorxi, because vij lacks evidence that the annotator found significant." ></td>
	<td class="line x" title="37:231	We can translate this intuition into additional constraints on the correct model, i.e., on the weight vector vectorw." ></td>
	<td class="line x" title="38:231	In addition to the usual SVM constraint on positive examples that vectorwxi  1, we also want (for each j) that vectorw vectorxi  vectorw vij  , where   0 controls the size of the desired margin between original and contrast examples." ></td>
	<td class="line x" title="39:231	An ordinary soft-margin SVM chooses vectorw and vector to minimize 1 2bardblvectorwbardbl 2 + C(summationdisplay i i) (1) subject to the constraints (i) vectorw xi yi  1i (2) (i) i  0 (3) where xi is a training example, yi  {1,+1} is its desired classification, and i is a slack variable that allows training example xi to miss satisfying the margin constraint if necessary." ></td>
	<td class="line x" title="40:231	The parameter C > 0 controls the cost of taking such slack, and should generally be lower for noisier or less linearly separable datasets." ></td>
	<td class="line x" title="41:231	We add the contrast constraints (i,j) vectorw (xi vij)yi  (1ij), (4) where vij is one of the contrast examples constructed from example xi, and ij  0 is an associated slack variable." ></td>
	<td class="line x" title="42:231	Just as these extra constraints have their own margin , their slack variables have theirowncost, sotheobjectivefunction(1)becomes 1 2bardblvectorwbardbl 2 + C(summationdisplay i i) + Ccontrast( summationdisplay i,j ij) (5) The parameter Ccontrast  0 determines the importance of satisfying the contrast constraints." ></td>
	<td class="line x" title="43:231	It should generally be less than C if the contrasts are noisier than the training examples.2 Inpractice, itispossibletosolvethisoptimization using a standard soft-margin SVM learner." ></td>
	<td class="line x" title="44:231	Dividing equation (4) through by , it becomes (i,j) vectorw xij yi  1ij, (6) where xij def= xivij." ></td>
	<td class="line x" title="45:231	Since equation (6) takes the same form as equation (2), we simply add the pairs (xij,yi) to the training set as pseudoexamples, weighted by Ccontrast rather than C so that the learner will use the objective function (5)." ></td>
	<td class="line x" title="46:231	There is one subtlety." ></td>
	<td class="line x" title="47:231	To allow a biased hyperplane, we use the usual trick of prepending a 1 element to each training example." ></td>
	<td class="line x" title="48:231	Thus we require vectorw  (1,xi)  1  i (which makes w0 play the role of a bias term)." ></td>
	<td class="line x" title="49:231	This means, however, that we must prepend a 0 element to each pseudoexample: vectorw  (1,vectorxi)(1,vij) = vectorw (0,xij)  1ij." ></td>
	<td class="line x" title="50:231	In our experiments, we optimize , C, and Ccontrast on held-out data (see section 5.2)." ></td>
	<td class="line x" title="51:231	4 Rationale Annotation for Movie Reviews In order to demonstrate that annotator rationales help machine learning, we needed annotated data that included rationales for the annotations." ></td>
	<td class="line pc" title="52:231	We chose a dataset that would be enjoyable to reannotate: the movie review dataset of (Pang et al. , 2002; Pang and Lee, 2004).3 The dataset consists of 1000 positive and 1000 negative movie reviews obtained from the Internet Movie Database (IMDb) review archive, all written before 2002 by a total of 312 authors, with a cap of 20 reviews per author per 2Taking Ccontrast to be constant means that all rationales are equally valuable." ></td>
	<td class="line x" title="53:231	One might instead choose, for example, to reduce Ccontrast for examples xi that have many rationales, to prevent xis contrast examples vij from together dominating the optimization." ></td>
	<td class="line x" title="54:231	However, in this paper we assume that an xi with more rationales really does provide more evidence about the true classifier vectorw." ></td>
	<td class="line x" title="55:231	3Polarity dataset version 2.0." ></td>
	<td class="line x" title="56:231	261 category." ></td>
	<td class="line o" title="57:231	Pang and Lee have divided the 2000 documents into 10 folds, each consisting of 100 positive reviews and 100 negative reviews." ></td>
	<td class="line n" title="58:231	The dataset is arguably artificial in that it keeps only reviews where the reviewer provided a rather high or rather low numerical rating, allowing Pang and Lee to designate the review as positive or negative." ></td>
	<td class="line x" title="59:231	Nonetheless, most reviews contain a difficult mix of praise, criticism, and factual description." ></td>
	<td class="line x" title="60:231	In fact, it is possible for a mostly critical review to give a positive overall recommendation, or vice versa." ></td>
	<td class="line x" title="61:231	4.1 Annotation procedure Rationale annotators were given guidelines4 that read, in part: Each review was intended to give either a positive or a negative overall recommendation." ></td>
	<td class="line x" title="62:231	You will be asked to justify why a review is positive or negative." ></td>
	<td class="line x" title="63:231	To justify why a review is positive, highlight the most important words and phrases that would tell someone to see the movie." ></td>
	<td class="line x" title="64:231	To justify why a review is negative, highlight words and phrases that would tell someone not to see the movie." ></td>
	<td class="line x" title="65:231	These words and phrases are called rationales." ></td>
	<td class="line x" title="66:231	You can highlight the rationales as you notice them, which should result in several rationales per review." ></td>
	<td class="line x" title="67:231	Do your best to mark enough rationales to provide convincing support for the class of interest." ></td>
	<td class="line x" title="68:231	You do not need to go out of your way to mark everything." ></td>
	<td class="line x" title="69:231	You are probably doing too much work if you find yourself going back to a paragraph to look for even more rationales in it." ></td>
	<td class="line x" title="70:231	Furthermore, it is perfectly acceptable to skim through sections that you feel would not contain many rationales, such as a reviewers plot summary, even if that might cause you to miss a rationale here and there." ></td>
	<td class="line x" title="71:231	The last two paragraphs were intended to provide some guidance on how many rationales to annotate." ></td>
	<td class="line x" title="72:231	Even so, as section 4.2 shows, some annotators were considerably more thorough (and slower)." ></td>
	<td class="line x" title="73:231	Annotators were also shown the following examples5 of positive rationales:  you will enjoy the hell out of American Pie." ></td>
	<td class="line x" title="74:231	 fortunately, they managed to do it in an interesting and funny way." ></td>
	<td class="line x" title="75:231	 he is one of the most exciting martial artists on the big screen, continuing to perform his own stunts and dazzling audiences with his flashy kicks and punches." ></td>
	<td class="line x" title="76:231	 the romance was enchanting." ></td>
	<td class="line x" title="77:231	and the following examples5 of negative rationales: 4Available at http://cs.jhu.edu/ozaidan/rationales." ></td>
	<td class="line x" title="78:231	5For our controlled study of annotation time (section 4.2), different examples were given with full document context." ></td>
	<td class="line x" title="79:231	Figure 1: Histograms of rationale counts per document (A0s annotations)." ></td>
	<td class="line x" title="80:231	The overall mean of 8.55 is close to that of the four annotators in Table 1." ></td>
	<td class="line x" title="81:231	The median and mode are 8 and 7." ></td>
	<td class="line x" title="82:231	 A woman in peril." ></td>
	<td class="line x" title="83:231	A confrontation." ></td>
	<td class="line x" title="84:231	An explosion." ></td>
	<td class="line x" title="85:231	The end." ></td>
	<td class="line x" title="86:231	Yawn." ></td>
	<td class="line x" title="87:231	Yawn." ></td>
	<td class="line x" title="88:231	Yawn." ></td>
	<td class="line x" title="89:231	 when a film makes watching Eddie Murphy a tedious experience, you know something is terribly wrong." ></td>
	<td class="line x" title="90:231	 the movie is so badly put together that even the most casualviewermaynoticethemiserablepacingandstray plot threads." ></td>
	<td class="line x" title="91:231	 dont go see this movie The annotation involves boldfacing the rationale phrases using an HTML editor." ></td>
	<td class="line x" title="92:231	Note that a fancier annotation tool would be necessary for a task like namedentitytagging, whereanannotatormustmark many named entities in a single document." ></td>
	<td class="line x" title="93:231	At any given moment, such a tool should allow the annotator to highlight, view, and edit only the several rationales for the current annotated entity (the one most recently annotated or re-selected)." ></td>
	<td class="line x" title="94:231	One of the authors (A0) annotated folds 08 of the movie review set (1,800 documents) with rationales that supported the gold-standard classifications." ></td>
	<td class="line x" title="95:231	This training/development set was used for all of the learning experiments in sections 56." ></td>
	<td class="line x" title="96:231	A histogram of rationale counts is shown in Figure 1." ></td>
	<td class="line x" title="97:231	As mentioned in section 3, the rationale annotations were just textual substrings." ></td>
	<td class="line x" title="98:231	The annotator did not require knowledge of the classifier features." ></td>
	<td class="line x" title="99:231	Thus, our rationale dataset is a new resource4 that could also be used to study exploitation of rationales under feature sets or learning methods other than those considered here (see section 8)." ></td>
	<td class="line x" title="100:231	4.2 Inter-annotator agreement To study the annotation process, we randomly selected 150 documents from the dataset." ></td>
	<td class="line x" title="101:231	The doc262 Rationales % rationales also % rationales also % rationales also % rationales also % rationales also per document annotated by A1 annotated by A2 annotated by AX annotated by AY ann." ></td>
	<td class="line x" title="102:231	by anyone else A1 5.02 (100) 69.6 63.0 80.1 91.4 A2 10.14 42.3 (100) 50.2 67.8 80.9 AX 6.52 49.0 68.0 (100) 79.9 90.9 AY 11.36 39.7 56.2 49.3 (100) 75.5 Table 1: Average number of rationales and inter-annotator agreement for Tasks 2 and 3." ></td>
	<td class="line x" title="103:231	A rationale by Ai (I think this is a great movie!) is considered to have been annotated also by Aj if at least one of Ajs rationales overlaps it (I think this is a great movie!)." ></td>
	<td class="line x" title="104:231	In computing pairwise agreement on rationales, we ignored documents where Ai and Aj disagreed on the class." ></td>
	<td class="line x" title="105:231	Notice that the most thorough annotatorAYcaught most rationales marked by the others (exhibiting high recall), and that most rationales enjoyed some degree of consensus, especially those marked by the least thorough annotator A1 (exhibiting high precision)." ></td>
	<td class="line x" title="106:231	uments were split into three groups, each consisting of 50 documents (25 positive and 25 negative)." ></td>
	<td class="line x" title="107:231	Each subset was used for one of three tasks:6  Task 1: Given the document, annotate only the class (positive/negative)." ></td>
	<td class="line x" title="108:231	 Task 2: Given the document and its class, annotate some rationales for that class." ></td>
	<td class="line x" title="109:231	 Task 3: Given the document, annotate both the class and some rationales for it." ></td>
	<td class="line x" title="110:231	We carried out a pilot study (annotators AX and AY: two of the authors) and a later, more controlled study (annotators A1 and A2: paid students)." ></td>
	<td class="line x" title="111:231	The latter was conducted in a more controlled environment where both annotators used the same annotation tool and annotation setup as each other." ></td>
	<td class="line x" title="112:231	Their guidelines were also more detailed (see section 4.1)." ></td>
	<td class="line x" title="113:231	In addition, the documents for the different tasks were interleaved to avoid any practice effect." ></td>
	<td class="line o" title="114:231	The annotators classification accuracies in Tasks 1 and 3 (against Pang & Lees labels) ranged from 92%97%, with 4-way agreement on the class for 89% of the documents, and pairwise agreement also ranging from 92%97%." ></td>
	<td class="line x" title="115:231	Table 1 shows how many rationales the annotators provided and how well their rationales agreed." ></td>
	<td class="line x" title="116:231	Interestingly, in Task 3, four of AXs rationales for a positive class were also partially highlighted by AY as support for AYs (incorrect) negative classifications, such as: 6Each task also had a warmup set of 10 documents to be annotated before that taskss 50 documents." ></td>
	<td class="line x" title="117:231	Documents for Tasks 2 and 3 would automatically open in an HTML editor while Task 1 documents opened in an HTML viewer with no editing option." ></td>
	<td class="line x" title="118:231	The annotators recorded their classifications for Tasks 1 and 3 on a spreadsheet." ></td>
	<td class="line x" title="119:231	min./KB A1 time A2 time AX time AY time Task 1 0.252 0.112 0.150 0.422 Task 2 0.396 0.537 0.242 0.626 Task 3 0.399 0.505 0.288 1.01 min./doc." ></td>
	<td class="line x" title="120:231	A1 time A2 time AX time AY time Task 1 1.04 0.460 0.612 1.73 min./rat." ></td>
	<td class="line x" title="121:231	A1 time A2 time AX time AY time Task 2 0.340 0.239 0.179 0.298 Task 3 0.333 0.198 0.166 0.302 Table 2: Average annotation rates on each task." ></td>
	<td class="line x" title="122:231	 Even with its numerous flaws, the movie all comes together, if only for those who   Beloved acts like an incredibly difficult chamber drama paired with a ghost story." ></td>
	<td class="line x" title="123:231	4.3 Annotation time Average annotation times are in Table 2." ></td>
	<td class="line x" title="124:231	As hoped, rationales did not take too much extra time for most annotators to provide." ></td>
	<td class="line x" title="125:231	For each annotator except A2, providing rationales only took roughly twice the time (Task 3 vs. Task 1), even though it meant marking an average of 511 rationales in addition to the class." ></td>
	<td class="line x" title="126:231	Why this low overhead?" ></td>
	<td class="line x" title="127:231	Because marking the class already required the Task 1 annotator to read the document and find some rationales, even if s/he did not mark them." ></td>
	<td class="line x" title="128:231	The only extra work in Task 3 is in making them explicit." ></td>
	<td class="line x" title="129:231	This synergy between class annotation and rationale annotation is demonstrated by the fact that doing both at once (Task 3) was faster than doing them separately (Tasks 1+2)." ></td>
	<td class="line x" title="130:231	We remark that this taskbinary classification on full documentsseems to be almost a worst-case scenario for the annotation of rationales." ></td>
	<td class="line x" title="131:231	At a purely mechanical level, it was rather heroic of A0 to attach 89 new rationale phrases rij to every bit yi of ordinary annotation." ></td>
	<td class="line x" title="132:231	Imagine by contrast a more local task of identifying entities or relations." ></td>
	<td class="line x" title="133:231	Each 263 lower-level annotation yi will tend to have fewer rationalesrij, whileyi itselfwillbemorecomplexand hence more difficult to mark." ></td>
	<td class="line x" title="134:231	Thus, we expect that the overhead of collecting rationales will be less in many scenarios than the factor of 2 we measured." ></td>
	<td class="line x" title="135:231	Annotation overhead could be further reduced." ></td>
	<td class="line x" title="136:231	Foramulti-classproblemlikerelationdetection,one couldasktheannotatortoproviderationales only for the rarer classes." ></td>
	<td class="line x" title="137:231	This small amount of extra time where the data is sparsest would provide extra guidance where it was most needed." ></td>
	<td class="line x" title="138:231	Another possibility is passive collection of rationales via eye tracking." ></td>
	<td class="line oc" title="139:231	5 Experimental Procedures 5.1 Feature extraction Although this dataset seems to demand discourselevel features that contextualize bits of praise and criticism, we exactly follow Pang et al.(2002) and Pang and Lee (2004) in merely using binary unigram features, corresponding to the 17,744 unstemmed word or punctuation types with count  4 in the full 2000-document corpus." ></td>
	<td class="line x" title="141:231	Thus, each document is reduced to a 0-1 vector with 17,744 dimensions, which is then normalized to unit length.7 We used the method of section 3 to place additionalconstraintsonalinearclassifier." ></td>
	<td class="line x" title="142:231	Givenatrainingdocument, wecreateseveralcontrastdocuments, each by deleting exactly one rationale substring from the training document." ></td>
	<td class="line x" title="143:231	Converting documents to feature vectors, we obtained an original example xi and several contrast examples vi1,vi2,8 Again, our training method required each original document to be classified more confidently (by a margin ) than its contrast documents." ></td>
	<td class="line x" title="144:231	Ifwewereusingmorethanunigramfeatures,then simply deleting a rationale substring would not always be the best way to create a contrast document, as the resulting ungrammatical sentences might cause deep feature extraction to behave strangely (e.g. , parseerrorsduringpreprocessing)." ></td>
	<td class="line x" title="145:231	Thegoalin creating the contrast document is merely to suppress 7The vectors are normalized before prepending the 1 corresponding to the bias term feature (mentioned in section 3)." ></td>
	<td class="line x" title="146:231	8The contrast examples were not normalized to precisely unitlength, butinsteadwerenormalizedbythesamefactorused to normalize xi." ></td>
	<td class="line x" title="147:231	This conveniently ensured that the pseudoexamples xij def= vectorxivij were sparse vectors, with 0 coordinates for all words not in the jth rationale." ></td>
	<td class="line x" title="148:231	features (n-grams, parts of speech, syntactic dependencies ) that depend in part on material in one or more rationales." ></td>
	<td class="line x" title="149:231	This could be done directly by modifying the feature extractors, or if one prefers to use existing feature extractors, by masking rather thandeletingtherationalesubstringe.g. , replacing each of its word tokens with a special MASK token that is treated as an out-of-vocabulary word." ></td>
	<td class="line x" title="150:231	5.2 Training and testing procedures We transformed this problem to an SVM problem (seesection3)andappliedSVMlight fortrainingand testing, using the default linear kernel." ></td>
	<td class="line x" title="151:231	We used only A0s rationales and the true classifications." ></td>
	<td class="line x" title="152:231	Fold 9 was reserved as a test set." ></td>
	<td class="line x" title="153:231	All accuracy results reported in the paper are the result of testing on fold 9, after training on subsets of folds 08." ></td>
	<td class="line x" title="154:231	Our learning curves show accuracy after training on T < 9 folds (i.e. , 200T documents), for various T. To reduce the noise in these results, the accuracy we report for training on T folds is actually the average of 9 different experiments with different (albeit overlapping) training sets that cover folds 08: 1 9 8summationdisplay i=0 acc(F9 | ,Fi+1 Fi+T) (7) where Fj denotes the fold numbered j mod 9, and acc(Z | ,Y ) means classification accuracy on the set Z after training on Y with hyperparameters ." ></td>
	<td class="line x" title="155:231	To evaluate whether two different training methods A and B gave significantly different averageaccuracy values, we used a paired permutation test (generalizing a sign test)." ></td>
	<td class="line x" title="156:231	The test assumes independence among the 200 test examples but not among the 9 overlapping training sets." ></td>
	<td class="line x" title="157:231	For each of the 200 test examples in fold 9, we measured (ai,bi), where ai (respectively bi) is the number of the 9 training sets under which A (respectively B) classified the example correctly." ></td>
	<td class="line x" title="158:231	The p value is the probability that the absolute difference between the average-accuracy values would reach or exceed the observed absolute difference, namely | 1200summationtext200i=1 aibi9 |, ifeach(ai,bi)hadanindependent 1/2 chance of being replaced with (bi,ai), as per the null hypothesis that A and B are indistinguishable." ></td>
	<td class="line x" title="159:231	For any given value of T and any given training method, we chose hyperparameters  = 264 Figure 2: Classification accuracy under five different experimental setups (S1S5)." ></td>
	<td class="line x" title="160:231	At each training size, the 5 accuraciesarepairwisesignificantlydifferent(pairedpermutationtest, p < 0.02; see section 5.2), except for {S3,S4} or {S4,S5} at some sizes." ></td>
	<td class="line x" title="161:231	(C,,Ccontrast) to maximize the following crossvalidation performance:9  = argmax  8summationdisplay i=0 acc(Fi | ,Fi+1 Fi+T) (8) Weusedasimplealternatingoptimizationprocedure that begins at 0 = (1.0,1.0,1.0) and cycles repeatedly through the three dimensions, optimizing along each dimension by a local grid search with resolution 0.1.10 Of course, when training without rationales, we did not have to optimize  or Ccontrast." ></td>
	<td class="line x" title="162:231	6 Experimental Results 6.1 The value of rationales The top curve (S1) in Figure 2 shows that performance does increase when we introduce rationales for the training examples as contrast examples (section 3)." ></td>
	<td class="line x" title="163:231	S1 is significantly higher than the baseline curve (S2) immediately below it, which trains an ordinary SVM classifier without using rationales." ></td>
	<td class="line x" title="164:231	At the largest training set size, rationales raise the accuracy from 88.5% to 92.2%, a 32% error reduction." ></td>
	<td class="line x" title="165:231	9One might obtain better performance (across all methods being compared) by choosing a separate  for each of the 9 training sets." ></td>
	<td class="line x" title="166:231	However, to simulate real limited-data training conditions, one should then find the  for each {i,,j} using a separate cross-validation withini,,j}only; this would slow down the experiments considerably." ></td>
	<td class="line x" title="167:231	10For optimizing along the C dimension, one could use the efficient method of Beineke et al.(2004), but not in SVMlight." ></td>
	<td class="line x" title="169:231	The lower three curves (S3S5) show that learning is separately helped by the rationale and the non-rationale portions of the documents." ></td>
	<td class="line x" title="170:231	S3S5 are degraded versions of the baseline S2: they are ordinary SVM classifiers that perform significantly worse than S2 (p < 0.001)." ></td>
	<td class="line x" title="171:231	Removing the rationale phrases from the training documents (S3) made the test documents much harder to discriminate (compared to S2)." ></td>
	<td class="line x" title="172:231	This suggests that annotator A0s rationales often covered most of the usable evidence for the true class." ></td>
	<td class="line x" title="173:231	However, the pieces to solving the classification puzzle cannot be found solely in the short rationale phrases." ></td>
	<td class="line x" title="174:231	Removing all non-rationale text from the training documents (S5) was even worse than removing the rationales (S3)." ></td>
	<td class="line x" title="175:231	In other words, we cannot hope to do well simply by training on just the rationales (S5), although that approach is improved somewhat in S4 by treating each rationale (similarly to S1) as a separate SVM training example." ></td>
	<td class="line x" title="176:231	This presents some insight into why our method gives the best performance." ></td>
	<td class="line x" title="177:231	The classifier in S1 is able to extract subtle patterns from the corpus, like S2, S3, or any other standard machine learning method, but it is also able to learn from a human annotators decision-making strategy." ></td>
	<td class="line x" title="178:231	6.2 Using fewer rationales In practice, one might annotate rationales for only some training documentseither when annotating a new corpus or when adding rationales post hoc to an existing corpus." ></td>
	<td class="line x" title="179:231	Thus, a range of options can be found between curves S2 and S1 of Figure 2." ></td>
	<td class="line x" title="180:231	Figure 3 explores this space, showing how far the learning curve S2 moves upward if one has time to annotate rationales for a fixed number of documents R. The key useful discovery is that much of the benefit can actually be obtained with relatively few rationales." ></td>
	<td class="line x" title="181:231	For example, with 800 training documents, annotating(0%,50%,100%)ofthemwithrationales gives accuracies of (86.9%, 89.2%, 89.3%)." ></td>
	<td class="line x" title="182:231	With the maximum of 1600 training documents, annotating (0%, 50%, 100%) with rationales gives (88.5%, 91.7%, 92.2%)." ></td>
	<td class="line x" title="183:231	To make this point more broadly, we find that the R = 200 curve is significantly above the R = 0 curve (p < 0.05) at all T  1200." ></td>
	<td class="line x" title="184:231	By contrast, the R = 800,R = 1000,R = 1600 points at each T 265 Figure 3: Classification accuracy for T  {200,400,,1600} training documents (x-axis) when only R  {0,200,,T} of them are annotated with rationales (different curves)." ></td>
	<td class="line x" title="185:231	The R = 0 curve above corresponds to the baseline S2 from Figure 2." ></td>
	<td class="line x" title="186:231	S1s points are found above as the leftmost points on the other curves, where R = T. value are all-pairs statistically indistinguishable." ></td>
	<td class="line x" title="187:231	The figure also suggests that rationales and documents may be somewhat orthogonal in their benefit." ></td>
	<td class="line x" title="188:231	When one has many documents and few rationales, there is no longer much benefit in adding more documents (the curve is flattening out), but adding more rationales seems to provide a fresh benefit: rationales have not yet reached their point of diminishing returns." ></td>
	<td class="line x" title="189:231	(While this fresh benefit was often statistically significant, and greater than the benefit from more documents, our experiments did not establish that it was significantly greater.)" ></td>
	<td class="line x" title="190:231	Theaboveexperimentskeep all ofA0srationales on a fraction of training documents." ></td>
	<td class="line x" title="191:231	We also experimented with keeping a fraction of A0s rationales (chosen randomly with randomized rounding) on all training documents." ></td>
	<td class="line x" title="192:231	This yielded no noteworthy or statistically significant differences from Figure 3." ></td>
	<td class="line x" title="193:231	These latter experiments simulate a lazy annotator who is less assiduous than A0." ></td>
	<td class="line x" title="194:231	Such annotators may be common in the real world." ></td>
	<td class="line x" title="195:231	We also suspect that they will be more desirable." ></td>
	<td class="line x" title="196:231	First, they should be able to add more rationales per hour than the A0style annotator from Figure 3: some rationales are simplymorenoticeablethanothers, andalazyannotatorwillquicklyfindthemostnoticeableoneswithout wasting time tracking down the rest." ></td>
	<td class="line x" title="197:231	Second, the most noticeable rationales that they mark may be the most effective ones for learning, although our random simulation of laziness could not test that." ></td>
	<td class="line x" title="198:231	7 Related Work Our rationales resemble side information in machine learningsupplementary information about the target function that is available at training time." ></td>
	<td class="line x" title="199:231	Side information is sometimes encoded as virtual examples like our contrast examples or pseudoexamples." ></td>
	<td class="line x" title="200:231	However, past work generates these by automatically transforming the training examples in ways that are expected to preserve or alter the classification (Abu-Mostafa, 1995)." ></td>
	<td class="line x" title="201:231	In another formulation, virtual examples are automatically generated but must be manually annotated (Kuusela and Ocone, 2004)." ></td>
	<td class="line x" title="202:231	Our approach differs because a human helps to generate the virtual examples." ></td>
	<td class="line x" title="203:231	Enforcing a margin between ordinary examples and contrast examples also appears new." ></td>
	<td class="line x" title="204:231	Other researchers have considered how to reduce annotation effort." ></td>
	<td class="line x" title="205:231	In active learning, the annotator classifies only documents where the system so far is less confident (Lewis and Gale, 1994), or in an information extraction setting, incrementally corrects details of the systems less confident entity segmentationsandlabelings(CulottaandMcCallum, 2005)." ></td>
	<td class="line x" title="206:231	Raghavan et al.(2005) asked annotators to identify globally relevant features." ></td>
	<td class="line x" title="208:231	In contrast, our approach does not force the annotator to evaluate the importance of features individually, nor in a global context outside any specific document, nor even to know the learners feature space." ></td>
	<td class="line x" title="209:231	Annotators only mark text that supports their classification decision." ></td>
	<td class="line x" title="210:231	Our methods then consider the combined effect of this text on the feature vector, which may include complex features not known to the annotator." ></td>
	<td class="line x" title="211:231	8 Future Work: Generative models Our SVM contrast method (section 3) is not the only possible way to use rationales." ></td>
	<td class="line x" title="212:231	We would like to explicitly model rationale annotation as a noisy process that reflects, imperfectly and incompletely, the annotators internal decision procedure." ></td>
	<td class="line x" title="213:231	A natural approach would start with log-linear models in place of SVMs." ></td>
	<td class="line x" title="214:231	We can define a probabilistic classifier p(y | x) def= 1Z(x) exp ksummationdisplay h=1 hfh(x,y) (9) 266 where vectorf() extracts a feature vector from a classified document." ></td>
	<td class="line x" title="215:231	A standard training method would be to choose  to maximize the conditional likelihood of the training classifications: argmax vector nproductdisplay i=1 p(yi | xi) (10) When a rationale ri is also available for each (xi,yi), we propose to maximize a likelihood that tries to predict these rationale data as well: argmax vector nproductdisplay i=1 p(yi | xi)pprime(ri | xi,yi,) (11) Notice that a given guess of  might make equation (10) large, yet accord badly with the annotators rationales." ></td>
	<td class="line x" title="216:231	In that case, the second term of equation (11) will exert pressure on  to change to something that conforms more closely to the rationales." ></td>
	<td class="line x" title="217:231	If the annotator is correct, such a  will generalize better beyond the training data." ></td>
	<td class="line x" title="218:231	Inequation(11),pprime modelsthestochasticprocess of rationale annotation." ></td>
	<td class="line x" title="219:231	What is an annotator actually doing when she annotates rationales?" ></td>
	<td class="line x" title="220:231	In particular, how do her rationales derive from the true value of  and thereby tell us about ?" ></td>
	<td class="line x" title="221:231	Building a good model pprime of rationale annotation will require some exploratory data analysis." ></td>
	<td class="line x" title="222:231	Roughly, we expect that if hfh(xi,y) is much higher for y = yi than for other values of y, then the annotators ri is correspondingly more likely to indicate in some way that feature fh strongly influenced annotation yi." ></td>
	<td class="line x" title="223:231	However, we must also model the annotators limited patience (she may not annotate all important features), sloppiness (she may indicate only indirectly that fh is important), and bias (tendency to annotate some kinds of features at the expense of others)." ></td>
	<td class="line x" title="224:231	One advantage of this generative approach is that it eliminates the need for contrast examples." ></td>
	<td class="line x" title="225:231	Consider a non-textual example in which an annotator highlights the line crossing in a digital image of the digit 8 to mark the rationale that distinguishes it from 0. In this case it is not clear how to mask out that highlighted rationale to create a contrast example in which relevant features would not fire.11 11One cannot simply flip those highlighted pixels to white 9 Conclusions We have proposed a quite simple approach to improving machine learning by exploiting the cleverness of annotators, asking them to provide enriched annotations for training." ></td>
	<td class="line x" title="226:231	We developed and tested a particular discriminative method that can use annotator rationaleseven on a fraction of the training setto significantly improve sentiment classification of movie reviews." ></td>
	<td class="line x" title="227:231	We found fairly good annotator agreement on the rationales themselves." ></td>
	<td class="line x" title="228:231	Most annotators provided several rationales per classification without taking too much extra time, even in our text classification scenario, where the rationales greatly outweigh the classifications in number and complexity." ></td>
	<td class="line x" title="229:231	Greater speed might be possible through an improved user interface or passive feedback (e.g. , eye tracking)." ></td>
	<td class="line x" title="230:231	In principle, many machine learning methods might be modified to exploit rationale data." ></td>
	<td class="line x" title="231:231	While our experiments in this paper used a discriminative SVM, we plan to explore generative approaches." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N07-1039
Extracting Appraisal Expressions
Bloom, Kenneth;Garg, Navendu;Argamon, Shlomo;"></td>
	<td class="line x" title="1:46	Proceedings of NAACL HLT 2007, pages 308315, Rochester, NY, April 2007." ></td>
	<td class="line x" title="2:46	c2007 Association for Computational Linguistics Extracting Appraisal Expressions Kenneth Bloom and Navendu Garg and Shlomo Argamon Computer Science Department Illinois Institute of Technology 10 W. 31st St. Chicago, IL 60616 {kbloom1,gargnav,argamon}@iit.edu Abstract Sentiment analysis seeks to characterize opinionated or evaluative aspects of natural language text." ></td>
	<td class="line x" title="3:46	We suggest here that appraisal expression extraction should be viewed as a fundamental task in sentiment analysis." ></td>
	<td class="line x" title="4:46	An appraisal expression is a textual unit expressing an evaluative stance towards some target." ></td>
	<td class="line x" title="5:46	The task is to find and characterize the evaluative attributes of such elements." ></td>
	<td class="line x" title="6:46	This paper describes a system for effectively extracting and disambiguating adjectival appraisal expressions in English outputting a generic representation in terms of their evaluative function in the text." ></td>
	<td class="line x" title="7:46	Data mining on appraisal expressions gives meaningful and non-obvious insights." ></td>
	<td class="line x" title="8:46	1 Introduction Sentiment analysis, which seeks to analyze opinion in natural language text, has grown in interest in recent years." ></td>
	<td class="line oc" title="9:46	Sentiment analysis includes a variety of different problems, including: sentiment classification techniques to classify reviews as positive or negative, based on bag of words (Pang et al. , 2002) or positive and negative words (Turney, 2002; Mullen and Collier, 2004); classifying sentences in a document as either subjective or objective (Riloff and Wiebe, 2003; Pang and Lee, 2004); identifying or classifying appraisal targets (Nigam and Hurst, 2004); identifying the source of an opinion in a text (Choi et al. , 2005), whether the author is expressing the opinion, or whether he is attributing the opinion to someone else; and developing interactive and visual opinion mining methods (Gamon et al. , 2005; Popescu and Etzioni, 2005)." ></td>
	<td class="line n" title="10:46	Much of this work has utilized the fundamental concept of semantic orientation, (Turney, 2002); however, sentiment analysis still lacks a unified field theory." ></td>
	<td class="line x" title="11:46	We propose in this paper that a fundamental task underlying many of these formulations is the extraction and analysis of appraisal expressions, defined as those structured textual units which express an evaluation of some object." ></td>
	<td class="line x" title="12:46	An appraisal expression hasthreemaincomponents: an attitude (whichtakes an evaluative stance about an object), a target (the object of the stance), and a source (the person taking the stance) which may be implied." ></td>
	<td class="line x" title="13:46	The idea of appraisal extraction is a generalization of problem formulations developed in earlier works." ></td>
	<td class="line x" title="14:46	Mullen and Colliers (2004) notion of classifying appraisal terms using a multidimensional set of attributes is closely tied to the definition of an appraisal expression, which is classified along several dimensions." ></td>
	<td class="line x" title="15:46	In previous work (Whitelaw et al. , 2005), we presented a related technique of finding opinion phrases, using a multidimensional set of attributes and modeling the semantics of modifiers in these phrases." ></td>
	<td class="line x" title="16:46	The use of multiple text classifiers by Wiebe and colleagues (Wilson et al. , 2005; Wiebe et al. , 2004) for various kinds of sentiment classification can also be viewed as a sentencelevel technique for analyzing appraisal expressions." ></td>
	<td class="line x" title="17:46	Nigam and Hursts (2004) work on detecting opinions about a certain topic presages our notion of connecting attitudes to targets, while Popescu and Etzionis (2005) opinion mining technique also fits well into our framework." ></td>
	<td class="line x" title="18:46	In this paper we describe a system for extracting adjectival appraisal expressions, based on a handbuilt lexicon, a combination of heuristic shallow parsing and dependency parsing, and expectationmaximization word sense disambiguation." ></td>
	<td class="line x" title="19:46	Each ex308 tracted appraisal expression is represented as a set of feature values in terms of its evaluative function in thetext." ></td>
	<td class="line x" title="20:46	Wehaveappliedthissystemtotwodomains of texts: product reviews, and movie reviews." ></td>
	<td class="line x" title="21:46	Manual evaluation of the extraction shows our system to work well, as well as giving some directions for improvement." ></td>
	<td class="line x" title="22:46	We also show how straightforward data mining can give users very useful information about public opinion." ></td>
	<td class="line x" title="23:46	2 Appraisal Expressions We define an appraisal expression to be an elementary linguistic unit that conveys an attitude of some kind towards some target." ></td>
	<td class="line x" title="24:46	An appraisal expression is defined to comprise a source, an attitude, and a target, each represented by various attributes." ></td>
	<td class="line x" title="25:46	For example, in I found the movie quite monotonous, the speaker (the Source) expresses a negative Attitude (quite monotonous) towards the movie (the Target)." ></td>
	<td class="line x" title="26:46	Note that attitudes come in different types; for example, monotonous describes an inherent quality of the Target, while loathed would describe the emotional reaction of the Source." ></td>
	<td class="line x" title="27:46	Attitude may be expressed through nouns, verbs, adjectives and metaphors." ></td>
	<td class="line x" title="28:46	Extracting all of this information accurately for all of these types of appraisal expressions is a very difficult problem." ></td>
	<td class="line x" title="29:46	We therefore restrict ourselves for now to adjectival appraisal expressions that are each contained in a single sentence." ></td>
	<td class="line x" title="30:46	Additionally, we focus here only on extracting and analyzing the attitude and the target, but not the source." ></td>
	<td class="line x" title="31:46	Even with these restrictions, we obtain interesting results (Sec." ></td>
	<td class="line x" title="32:46	7)." ></td>
	<td class="line x" title="33:46	2.1 Appraisal attributes Our method is grounded in Appraisal Theory, developed by Martin and White (2005), which analyzes the way opinion is expressed." ></td>
	<td class="line x" title="34:46	Following Martin and White, we define: Attitude type is type of appraisal being expressedone of affect, appreciation, or judgment (Figure 1)." ></td>
	<td class="line x" title="35:46	Affect refers to an emotional state (e.g. , happy, angry), and is the most explicitly subjective type of appraisal." ></td>
	<td class="line x" title="36:46	The other two types express evaluation of external entities, differentiating between intrinsic appreciation of object properties (e.g. , slender, ugly) and social judgment (e.g. , heroic, idiotic)." ></td>
	<td class="line x" title="37:46	Orientation is whether the attitude is positive Attitude Type Appreciation Composition Balance: consistent, discordant, Complexity: elaborate, convoluted,  Reaction Impact: amazing, compelling, dull,  Quality: beautiful, elegant, hideous,  Valuation: innovative, profound, inferior,  Affect: happy, joyful, furious,  Judgment Social Esteem Capacity: clever, competent, immature,  Tenacity: brave, hard-working, foolhardy,  Normality: famous, lucky, obscure,  Social Sanction Propriety: generous, virtuous, corrupt,  Veracity: honest, sincere, sneaky,  Figure 1: The Attitude Type taxonomy, with examples of adjectives from the lexicon." ></td>
	<td class="line x" title="38:46	(good) or negative (bad)." ></td>
	<td class="line x" title="39:46	Force describes the intensity of the appraisal." ></td>
	<td class="line x" title="40:46	Force is largely expressed via modifiers such as very (increased force), or slightly (decreased force), but may also be expressed lexically, for example greatest vs. great vs. good." ></td>
	<td class="line x" title="41:46	Polarity of an appraisal is marked if it is scoped in a polarity marker (such as not), or unmarked otherwise." ></td>
	<td class="line x" title="42:46	Other attributes of appraisal are affected by negation; e.g., not good also has the opposite orientation from good." ></td>
	<td class="line x" title="43:46	Target type is a domain-dependent semantic type for the target." ></td>
	<td class="line x" title="44:46	This attribute takes on values fromadomain-dependenttaxonomy,representing important (and easily extractable) distinctions between targets in the domain." ></td>
	<td class="line x" title="45:46	2.2 Target taxonomies Two domain-dependent target type taxonomies are shown in Figure 2." ></td>
	<td class="line x" title="46:46	In both, the primary distinction is between a direct naming of a kind of Thing or a deictic/pronominal reference (e.g. , those or it), since the system does not currently rely on coreference resolution." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-1053
Opinion Mining using Econometrics: A Case Study on Reputation Systems
Ghose, Anindya;Ipeirotis, Panagiotis;Sundararajan, Arun;"></td>
	<td class="line x" title="1:201	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 416423, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:201	c2007 Association for Computational Linguistics Opinion Mining Using Econometrics: A Case Study on Reputation Systems Anindya Ghose Panagiotis G. Ipeirotis Department of Information, Operations, and Management Sciences Leonard N. Stern School of Business, New York University {aghose,panos,arun}@stern.nyu.edu Arun Sundararajan Abstract Deriving the polarity and strength of opinions is an important research topic, attracting significant attention over the last few years." ></td>
	<td class="line x" title="3:201	In this work, to measure the strength and polarity of an opinion, we consider the economic context in which the opinion is evaluated, instead of using human annotators or linguistic resources." ></td>
	<td class="line x" title="4:201	We rely on the fact that text in on-line systems influences the behavior of humans and this effect can be observed using some easy-to-measure economic variables, such as revenues or product prices." ></td>
	<td class="line x" title="5:201	By reversing the logic, we infer the semantic orientation and strength of an opinion by tracing the changes in the associated economic variable." ></td>
	<td class="line x" title="6:201	In effect, we use econometrics to identify the economic value of text and assign a dollar value to each opinion phrase, measuring sentiment effectively and without the need for manual labeling." ></td>
	<td class="line x" title="7:201	We argue that by interpreting opinions using econometrics, we have the first objective, quantifiable, and contextsensitive evaluation of opinions." ></td>
	<td class="line x" title="8:201	We make the discussion concrete by presenting results on the reputation system of Amazon.com." ></td>
	<td class="line x" title="9:201	We show that user feedback affects the pricing power of merchants and by measuring their pricing power we can infer the polarity and strength of the underlying feedback postings." ></td>
	<td class="line x" title="10:201	1 Introduction A significant number of websites today allow users to post articles where they express opinions about products, firms, people, and so on." ></td>
	<td class="line x" title="11:201	For example, users on Amazom.com post reviews about products they bought and users on eBay.com post feedback describing their experiences with sellers." ></td>
	<td class="line x" title="12:201	The goal of opinion mining systems is to identify such pieces of the text that express opinions (Breck et al. , 2007; Konig and Brill, 2006) and then measure the polarity and strength of the expressed opinions." ></td>
	<td class="line x" title="13:201	While intuitively the task seems straightforward, there are multiple challenges involved." ></td>
	<td class="line x" title="14:201	 What makes an opinion positive or negative?" ></td>
	<td class="line x" title="15:201	Is there an objective measure for this task?" ></td>
	<td class="line x" title="16:201	 How can we rank opinions according to their strength?" ></td>
	<td class="line x" title="17:201	Can we define an objective measure for ranking opinions?" ></td>
	<td class="line x" title="18:201	 How does the context change the polarity and strength of an opinion and how can we take the context into consideration?" ></td>
	<td class="line x" title="19:201	To evaluate the polarity and strength of opinions, most of the existing approaches rely either on training from human-annotated data (Hatzivassiloglou and McKeown, 1997), or use linguistic resources (Hu and Liu, 2004; Kim and Hovy, 2004) like WordNet, or rely on co-occurrence statistics (Turney, 2002) between words that are unambiguously positive (e.g. , excellent) and unambiguously negative (e.g. , horrible)." ></td>
	<td class="line oc" title="20:201	Finally, other approaches rely on reviews with numeric ratings from websites (Pang and Lee, 2002; Dave et al. , 2003; Pang and Lee, 2004; Cui et al. , 2006) and train (semi-)supervised learning algorithms to classify reviews as positive or negative, or in more fine-grained scales (Pang and Lee, 2005; Wilson et al. , 2006)." ></td>
	<td class="line o" title="21:201	Implicitly, the supervised learning techniques assume that numeric ratings fully encapsulate the sentiment of the review.416 In this paper, we take a different approach and instead consider the economic context in which an opinion is evaluated." ></td>
	<td class="line x" title="22:201	We observe that the text in on-line systems influence the behavior of the readers." ></td>
	<td class="line x" title="23:201	This effect can be measured by observing some easy-tomeasure economic variable, such as product prices." ></td>
	<td class="line x" title="24:201	For instance, online merchants on eBay with positive feedback can sell products for higher prices than competitors with negative evaluations." ></td>
	<td class="line x" title="25:201	Therefore, each of these (positive or negative) evaluations has a (positive or negative) effect on the prices that the merchant can charge." ></td>
	<td class="line x" title="26:201	For example, everything else being equal, a seller with speedy delivery may be able to charge $10 more than a seller with slow delivery." ></td>
	<td class="line x" title="27:201	Using this information, we can conclude that speedy is better than slow when applied to delivery and their difference is $10." ></td>
	<td class="line x" title="28:201	Thus, we can infer the semantic orientation and the strength of an evaluation from the changes in the observed economic variable." ></td>
	<td class="line x" title="29:201	Following this idea, we use techniques from econometrics to identify the economic value of text and assign a dollar value to each text snippet, measuring sentiment strength and polarity effectively and without the need for labeling or any other resource." ></td>
	<td class="line x" title="30:201	We argue that by interpreting opinions within an econometric framework, we have the first objective and context-sensitive evaluation of opinions." ></td>
	<td class="line x" title="31:201	For example, consider the comment good packaging, posted by a buyer to evaluate a merchant." ></td>
	<td class="line x" title="32:201	This comment would have been considered unambiguously positive by the existing opinion mining systems." ></td>
	<td class="line x" title="33:201	We observed, though, that within electronic markets, such as eBay, a posting that contains the words good packaging has actually negative effect on the power of a merchant to charge higher prices." ></td>
	<td class="line x" title="34:201	This surprising effect reflects the nature of the comments in online marketplaces: buyers tend to use superlatives and highly enthusiastic language to praise a good merchant, and a lukewarm good packaging is interpreted as negative." ></td>
	<td class="line x" title="35:201	By introducing the econometric interpretation of opinions we can effortlessly capture such challenging scenarios, something that is impossible to achieve with the existing approaches." ></td>
	<td class="line x" title="36:201	We focus our paper on reputation systems in electronic markets and we examine the effect of opinions on the pricing power of merchants in the marketplace of Amazon.com." ></td>
	<td class="line x" title="37:201	(We discuss more applications in Section 7)." ></td>
	<td class="line x" title="38:201	We demonstrate the value of our technique using a dataset with 9,500 transactions that took place over 180 days." ></td>
	<td class="line x" title="39:201	We show that textual feedback affects the power of merchants to charge higher prices than the competition, for the same product, and still make a sale." ></td>
	<td class="line x" title="40:201	We then reverse the logic and determine the contribution of each comment in the pricing power of a merchant." ></td>
	<td class="line x" title="41:201	Thus, we discover the polarity and strength of each evaluation without the need for human annotation or any other form of linguistic resource." ></td>
	<td class="line x" title="42:201	The structure of the rest of the paper is as follows." ></td>
	<td class="line x" title="43:201	Section 2 gives the basic background on reputation systems." ></td>
	<td class="line x" title="44:201	Section 3 describes our methodology for constructing the data set that we use in our experiments." ></td>
	<td class="line x" title="45:201	Section 4 shows how we combine established techniques from econometrics with text mining techniques to identify the strength and polarity of the posted feedback evaluations." ></td>
	<td class="line x" title="46:201	Section 5 presents the experimental evaluations of our techniques." ></td>
	<td class="line x" title="47:201	Finally, Section 6 discusses related work and Section 7 discusses further applications and concludes the paper." ></td>
	<td class="line x" title="48:201	2 Reputation Systems and Price Premiums When buyers purchase products in an electronic market, they assess and pay not only for the product they wish to purchase but for a set of fulfillment characteristics as well, e.g., packaging, delivery, and the extent to which the product description matches the actual product." ></td>
	<td class="line x" title="49:201	Electronic markets rely on reputation systems to ensure the quality of these characteristics for each merchant, and the importance of such systems is widely recognized in the literature (Resnick et al. , 2000; Dellarocas, 2003)." ></td>
	<td class="line x" title="50:201	Typically, merchants reputation in electronic markets is encoded by a reputation profile that includes: (a) the number of past transactions for the merchant, (b) a summary of numeric ratings from buyers who have completed transactions with the seller, and (c) a chronological list of textual feedback provided by these buyers." ></td>
	<td class="line x" title="51:201	Studies of online reputation, thus far, base a merchants reputation on the numeric rating that characterizes the seller (e.g. , average number of stars and number of completed transactions) (Melnik and Alm, 2002)." ></td>
	<td class="line x" title="52:201	The general conclusion of these studies show that merchants with higher (numeric) reputation can charge higher prices than the competition, for the same products, and still manage to make a sale." ></td>
	<td class="line x" title="53:201	This price premium that the merchants can command over the competition is a measure of their reputation." ></td>
	<td class="line x" title="54:201	Definition 2.1 Consider a set of merchants s1,,sn selling a product for prices p1,,pn." ></td>
	<td class="line x" title="55:201	If si makes417 Figure 1: A set of merchants on Amazon.com selling an identical product for different prices the sale for price pi, then si commands a price premium equal to pi  pj over sj and a relative price premium equal to pipjpi." ></td>
	<td class="line x" title="56:201	Hence, a transaction that involves n competing merchants generates n 1 price premiums.1 The average price premium for the transaction is summationtext jnegationslash=i(pipj) n1 and the average relative price premium is summationtext jnegationslash=i(pipj) pi(n1) . a50 Example 2.1 Consider the case in Figure 1 where three merchants sell the same product for $631.95, $632.26, and $637.05, respectively." ></td>
	<td class="line x" title="57:201	If GameHog sells the product, then the price premium against XP Passport is $4.79 (= $637.05$632.26) and against the merchant BuyPCsoft is $5.10." ></td>
	<td class="line x" title="58:201	The relative price premium is 0.75% and 0.8%, respectively." ></td>
	<td class="line x" title="59:201	Similarly, the average price premium for this transaction is $4.95 and the average relative price premium 0.78%." ></td>
	<td class="line x" title="60:201	a50 Different sellers in these markets derive their reputation from different characteristics: some sellers have a reputation for fast delivery, while some others have a reputation of having the lowest price among their peers." ></td>
	<td class="line x" title="61:201	Similarly, while some sellers are praised for their packaging in the feedback, others get good comments for selling high-quality goods but are criticized for being rather slow with shipping." ></td>
	<td class="line x" title="62:201	Even though previous studies have established the positive correlation between higher (numeric) reputation and higher price premiums, they ignored completely the role of the textual feedback and, in turn, the multi-dimensional nature of reputation in electronic markets." ></td>
	<td class="line x" title="63:201	We show that the textual feedback adds significant additional value to the numerical scores, and affects the pricing power of the merchants." ></td>
	<td class="line x" title="64:201	1As an alternative definition we can ignore the negative price premiums." ></td>
	<td class="line x" title="65:201	The experimental results are similar for both versions." ></td>
	<td class="line x" title="66:201	3 Data We compiled a data set using software resellers from publicly available information on software product listings at Amazon.com." ></td>
	<td class="line x" title="67:201	Our data set includes 280 individual software titles." ></td>
	<td class="line x" title="68:201	The sellers reputation matters when selling identical goods, and the price variation observed can be attributed primarily to variation in the merchants reputation." ></td>
	<td class="line x" title="69:201	We collected the data using Amazon Web Services over a period of 180 days, between October 2004 and March 2005." ></td>
	<td class="line x" title="70:201	We describe below the two categories of data that we collected." ></td>
	<td class="line x" title="71:201	Transaction Data: The first part of our data set contains details of the transactions that took place on the marketplace of Amazon.com for each of the software titles." ></td>
	<td class="line x" title="72:201	The Amazon Web Services associates a unique transaction ID for each unique product listed by a seller." ></td>
	<td class="line x" title="73:201	This transaction ID enables us to distinguish between multiple or successive listings of identical products sold by the same merchant." ></td>
	<td class="line x" title="74:201	Keeping with the methodology in prior research (Ghose et al. , 2006), we crawl the Amazons XML listings every 8 hours and when a transaction ID associated with a particular listing is removed, we infer that the listed product was successfully sold in the prior 8 hour window.2 For each transaction that takes place, we keep the price at which the product was sold and the merchants reputation at the time of the transaction (more on this later)." ></td>
	<td class="line x" title="75:201	Additionally, for each of the competing listings for identical products, we keep the listed price along with the competitors reputation." ></td>
	<td class="line x" title="76:201	Using the collected data, we compute the price premium variables for each transaction3 using Definition 2.1." ></td>
	<td class="line x" title="77:201	Overall, our data set contains 1,078 merchants, 9,484 unique transactions and 107,922 price premiums (recall that each transaction generates multiple price premiums)." ></td>
	<td class="line x" title="78:201	Reputation Data: The second part of our data set contains the reputation history of each merchant that had a (monitored) product for sale during our 180-day window." ></td>
	<td class="line x" title="79:201	Each of these merchants has a feedback profile, which consists of numerical scores and text-based feedback, posted by buyers." ></td>
	<td class="line x" title="80:201	We had an average of 4,932 postings per merchant." ></td>
	<td class="line x" title="81:201	The numerical ratings 2Amazon indicates that their seller listings remain on the site indefinitely until they are sold and sellers can change the price of the product without altering the transaction ID. 3Ideally, we would also include the tax and shipping cost charged by each merchant in the computation of the price premiums." ></td>
	<td class="line x" title="82:201	Unfortunately, we could not capture these costs using our methodology." ></td>
	<td class="line x" title="83:201	Assuming that the fees for shipping and tax are independent of the merchants reputation, our analysis is not affected.418 are provided on a scale of one to five stars." ></td>
	<td class="line x" title="84:201	These ratings are averaged to provide an overall score to the seller." ></td>
	<td class="line x" title="85:201	Note that we collect all feedback (both numerical and textual) associated with a seller over the entire lifetime of the seller and we reconstruct each sellers exact feedback profile at the time of each transaction." ></td>
	<td class="line x" title="86:201	4 Econometrics-based Opinion Mining In this section, we describe how we combine econometric techniques with NLP techniques to derive the semantic orientation and strength of the feedback evaluations." ></td>
	<td class="line x" title="87:201	Section 4.1 describes how we structure the textual feedback and Section 4.2 shows how we use econometrics to estimate the polarity and strength of the evaluations." ></td>
	<td class="line x" title="88:201	4.1 Retrieving the Dimensions of Reputation We characterize a merchant using a vector of reputation dimensions X = (X1,X2,,Xn), representing its ability on each of n dimensions." ></td>
	<td class="line x" title="89:201	We assume that each of these n dimensions is expressed by a noun, noun phrase, verb, or a verb phrase chosen from the set of all feedback postings, and that a merchant is evaluated on these n dimensions." ></td>
	<td class="line x" title="90:201	For example, dimension 1 might be shipping, dimension 2 might be packaging and so on." ></td>
	<td class="line x" title="91:201	In our model, each of these dimensions is assigned a numerical score." ></td>
	<td class="line x" title="92:201	Of course, when posting textual feedback, buyers do not assign explicit numeric scores to any dimension." ></td>
	<td class="line x" title="93:201	Rather, they use modifiers (typically adjectives or adverbs) to evaluate the seller along each of these dimensions (we describe how we assign numeric scores to each modifier in Section 4.2)." ></td>
	<td class="line x" title="94:201	Once we have identified the set of all dimensions, we can then parse each of the feedback postings, associate a modifier with each dimension, and represent a feedback posting as an n-dimensional vector  of modifiers." ></td>
	<td class="line x" title="95:201	Example 4.1 Suppose dimension 1 is delivery, dimension 2 is packaging, and dimension 3 is service. The feedback posting I was impressed by the speedy delivery!" ></td>
	<td class="line x" title="96:201	Great service! is then encoded as 1 = [speedy,NULL,great], while the posting The item arrived in awful packaging, and the delivery was slow is encoded as 2 = [slow,awful,NULL]." ></td>
	<td class="line x" title="97:201	a50 Let M = {NULL,1,,M} be the set of modifiers and consider a seller si with p postings in its reputation profile." ></td>
	<td class="line x" title="98:201	We denote with ijk M the modifier that appears in the j-th posting and is used to assess the k-th reputation dimension." ></td>
	<td class="line x" title="99:201	We then structure the merchants feedback as an np matrix M(si) whose rows are the p encoded vectors of modifiers associated with the seller." ></td>
	<td class="line x" title="100:201	We construct M(si) as follows: 1." ></td>
	<td class="line x" title="101:201	Retrieve the postings associated with a merchant." ></td>
	<td class="line x" title="102:201	2." ></td>
	<td class="line x" title="103:201	Parse the postings to identify the dimensions across which the buyer evaluates a seller, keeping4 the nouns, noun phrases, verbs, and verbal phrases as reputation characteristics.5." ></td>
	<td class="line x" title="104:201	3. Retrieve adjectives and adverbs that refer to6 dimensions (Step 2) and construct the  vectors." ></td>
	<td class="line x" title="105:201	We have implemented this algorithm on the feedback postings of each of our sellers." ></td>
	<td class="line x" title="106:201	Our analysis yields 151 unique dimensions, and a total of 142 modifiers (note that the same modifier can be used to evaluate multiple dimensions)." ></td>
	<td class="line x" title="107:201	4.2 Scoring the Dimensions of Reputation As discussed above, the textual feedback profile of merchant si is encoded as a np matrix M(si); the elements of this matrix belong to the set of modifiers M. In our case, we are interested in computing the score a(,d,j) that a modifier   M assigns to the dimension d, when it appears in the j-th posting." ></td>
	<td class="line x" title="108:201	Since buyers tend to read only the first few pages of text-based feedback, we weight higher the influence of recent text postings." ></td>
	<td class="line x" title="109:201	We model this by assuming that K is the number of postings that appear on each page (K = 25 on Amazon.com), and that c is the probability of clicking on the Next link and moving the next page of evaluations.7 This assigns a posting-specific weight rj = cfloorleft jKfloorright/summationtextpq=1 cfloorleft qKfloorright for the jth posting, where j is the rank of the posting, K is the number of postings per page, and p is the total number of postings for the given seller." ></td>
	<td class="line x" title="110:201	Then, we set a(,d,j) = rj a(,d) where a(,d) is the global score that modifier  assigns to dimension d. Finally, since each reputation dimension has potentially a different weight, we use a weight vector w to 4We eliminate all dimensions appearing in the profiles of less than 50 (out of 1078) merchants, since we cannot extract statistically meaningful results for such sparse dimensions 5The technique as described in this paper, considers words like shipping and  delivery as separate dimensions, although they refer to the same real-life dimension." ></td>
	<td class="line x" title="111:201	We can use Latent Dirichlet Allocation (Blei et al. , 2003) to reduce the number of dimensions, but this is outside the scope of this paper." ></td>
	<td class="line x" title="112:201	6To associate the adjectives and adverbs with the correct dimensions, we use the Collins HeadFinder capability of the Stanford NLP Parser." ></td>
	<td class="line x" title="113:201	7We report only results for c = 0.5." ></td>
	<td class="line x" title="114:201	We conducted experiments other values of c as well and the results are similar.419 weight the contribution of each reputation dimension to the overall reputation score (si) of seller si: (si) = rT A(M(si))w (1) where rT = [r1,r2,rp] is the vector of the postingspecific weights and A(M(i)) is a matrix that contains as element the score a(j,dk) where M(si) contains the modifier j in the column of the dimension dk." ></td>
	<td class="line x" title="115:201	If we model the buyers preferences as independently distributed along each dimension and each modifier score a(,dk) also as an independent random variable, then the random variable (si) is a sum of random variables." ></td>
	<td class="line x" title="116:201	Specifically, we have: (si) = Msummationdisplay j=1 nsummationdisplay k=1 (wk a(j,dk))R(j,dk) (2) where R(j,dk) is equal to the sum of the ri weights across all postings in which the modifier j modifies dimension dk." ></td>
	<td class="line x" title="117:201	We can easily compute the R(j,dk) values by simply counting appearances and weighting each appearance using the definition of ri." ></td>
	<td class="line x" title="118:201	The question is, of course, how to estimate the values of wk  a(j,dk), which determine the polarity and intensity of the modifier j modifying the dimension dk." ></td>
	<td class="line x" title="119:201	For this, we observe that the appearance of such modifier-dimension opinion phrases has an effect on the price premiums that a merchant can charge." ></td>
	<td class="line x" title="120:201	Hence, there is a correlation between the reputation scores () of the merchants and the price premiums observed for each transaction." ></td>
	<td class="line x" title="121:201	To discover the level of association, we use regression." ></td>
	<td class="line x" title="122:201	Since we are dealing with panel data, we estimate ordinary-leastsquares (OLS) regression with fixed effects (Greene, 2002), where the dependent variable is the price premium variable, and the independent variables are the reputation scores () of the merchants, together with a few other control variables." ></td>
	<td class="line x" title="123:201	Generally, we estimate models of the form: PricePremiumij = summationdisplay c Xcij +fij +epsilon1ij+ t1 (merchant)ij +t2 (competitor)ij (3) where PricePremiumij is one of the variations of price premium as given in Definition 2.1 for a seller si and product j, c, t1, and t2 are the regressor coefficients, Xc are the control variables, () are the text reputation scores (see Equation 1), fij denotes the fixed effects and epsilon1 is the error term." ></td>
	<td class="line x" title="124:201	In Section 5, we give the details about the control variables and the regression settings." ></td>
	<td class="line x" title="125:201	Interestingly, if we expand the () variables according to Equation 2, we can run the regression using the modifier-dimension pairs as independent variables, whose values are equal to the R(j,dk) values." ></td>
	<td class="line x" title="126:201	After running the regression, the coefficients assigned to each modifier-dimension pair correspond to the value wk  a(j,dk) for each modifier-dimension pair." ></td>
	<td class="line x" title="127:201	Therefore, we can easily estimate in economic terms the value of a particular modifier when used to evaluate a particular dimension." ></td>
	<td class="line x" title="128:201	5 Experimental Evaluation In this section, we first present the experimental settings (Section 5.1), and then we describe the results of our experimental evaluation (Section 5.2)." ></td>
	<td class="line x" title="129:201	5.1 Regression Settings In Equation 3 we presented the general form of the regression for estimating the scores a(j,dk)." ></td>
	<td class="line x" title="130:201	Since we want to eliminate the effect of any other factors that may influence the price premiums, we also use a set of control variables." ></td>
	<td class="line x" title="131:201	After all the control factors are taken into consideration, the modifier scores reflect the additional value of the text opinions." ></td>
	<td class="line x" title="132:201	Specifically, we used as control variables the products price on Amazon, the average star rating of the merchant, the number of merchants past transactions, and the number of sellers for the product." ></td>
	<td class="line x" title="133:201	First, we ran OLS regressions with product-seller fixed effects controlling for unobserved heterogeneity across sellers and products." ></td>
	<td class="line x" title="134:201	These fixed effects control for average product quality and differences in seller characteristics." ></td>
	<td class="line x" title="135:201	We run multiple variations of our model, using different versions of the price premium variable as listed in Definition 2.1." ></td>
	<td class="line x" title="136:201	We also tested variations where we include as independent variable not the individual reputation scores but the difference (merchant)(competitor)." ></td>
	<td class="line x" title="137:201	All regressions yielded qualitatively similar results, so due to space restrictions we only report results for the regressions that include all the control variables and all the text variables; we report results using the price premium as the dependent variable." ></td>
	<td class="line x" title="138:201	Our regressions in this setting contain 107,922 observations, and a total of 547 independent variables." ></td>
	<td class="line x" title="139:201	5.2 Experimental Results Recall of Extraction: The first step of our experimental evaluation is to examine whether the opinion extraction technique of Section 4.1 indeed captures all the reputation characteristics expressed in the feed-420 Dimension Human Recall Computer Recall Product Condition 0.76 0.76 Price 0.91 0.61 Package 0.96 0.66 Overall Experience 0.65 0.55 Delivery Speed 0.96 0.92 Item Description 0.22 0.43 Product Satisfaction 0.68 0.58 Problem Response 0.30 0.37 Customer Service 0.57 0.50 Average 0.66 0.60 Table 1: The recall of our technique compared to the recall of the human annotators back (recall) and whether the dimensions that we capture are accurate (precision)." ></td>
	<td class="line x" title="140:201	To examine the recall question, we used two human annotators." ></td>
	<td class="line x" title="141:201	The annotators read a random sample of 1,000 feedback postings, and identified the reputation dimensions mentioned in the text." ></td>
	<td class="line x" title="142:201	Then, they examined the extracted modifierdimension pairs for each posting and marked whether the modifier-dimension pairs captured the identified real reputation dimensions mentioned in the posting and which pairs were spurious, non-opinion phrases." ></td>
	<td class="line x" title="143:201	Both annotators identified nine reputation dimensions (see Table 1)." ></td>
	<td class="line x" title="144:201	Since the annotators did not agree in all annotations, we computed the average human recall hRecd = agreeddalld for each dimension d, where agreedd is the number of postings for which both annotators identified the reputation dimension d, and alld is the number of postings in which at least one annotator identified the dimension d. Based on the annotations, we computed the recall of our algorithm against each annotator." ></td>
	<td class="line x" title="145:201	We report the average recall for each dimension, together with the human recall in Table 1." ></td>
	<td class="line x" title="146:201	The recall of our technique is only slightly inferior to the performance of humans, indicating that the technique of Section 4.1 extracts the majority of the posted evaluations.8 Interestingly, precision is not an issue in our setting." ></td>
	<td class="line x" title="147:201	In our framework, if an particular modifier-dimension pair is just noise, then it is almost impossible to have a statistically significant correlation with the price premiums." ></td>
	<td class="line x" title="148:201	The noisy opinion phrases are statistically guaranteed to be filtered out by the regression." ></td>
	<td class="line x" title="149:201	Estimating Polarity and Strength: In Table 2, 8In the case of Item Description, where the computer recall was higher than the human recall, our technique identified almost all the phrases of one annotator, but the other annotator had a more liberal interpretation of Item Description dimension and annotated significantly more postings with the dimension Item Description than the other annotator, thus decreasing the human recall." ></td>
	<td class="line x" title="150:201	we present the modifier-dimension pairs (positive and negative) that had the strongest dollar value and were statistically significant across all regressions." ></td>
	<td class="line x" title="151:201	(Due to space issues, we cannot list the values for all pairs)." ></td>
	<td class="line x" title="152:201	These values reflect changes in the merchantss pricing power after taking their average numerical score and level of experience into account, and also highlight the additional the value contained in textbased reputation." ></td>
	<td class="line x" title="153:201	The examples that we list here illustrate that our technique generates a natural ranking of the opinion phrases, inferring the strength of each modifier within the context in which this opinion is evaluated." ></td>
	<td class="line x" title="154:201	This holds true even for misspelled evaluations that would break existing techniques based on annotation or on resources like WordNet." ></td>
	<td class="line x" title="155:201	Furthermore, these values reflect the context in which the opinion is evaluated." ></td>
	<td class="line x" title="156:201	For example, the pair good packaging has a dollar value of -$0.58." ></td>
	<td class="line x" title="157:201	Even though this seems counterintuitive, it actually reflects the nature of an online marketplace where most of the positive evaluations contain superlatives, and a mere good is actually interpreted by the buyers as a lukewarm, slightly negative evaluation." ></td>
	<td class="line x" title="158:201	Existing techniques cannot capture such phenomena." ></td>
	<td class="line x" title="159:201	Price Premiums vs. Ratings: One of the natural comparisons is to examine whether we could reach similar results by just using the average star rating associated with each feedback posting to infer the score of each opinion phrase." ></td>
	<td class="line x" title="160:201	The underlying assumption behind using the ratings is that the review is perfectly summarized by the star rating, and hence the text plays mainly an explanatory role and carries no extra information, given the star rating." ></td>
	<td class="line x" title="161:201	For this, we examined the R2 fit of the regression, with and without the use of the text variables." ></td>
	<td class="line x" title="162:201	Without the use of text variables, the R2 was 0.35, while when using only the text-based regressors, the R2 fit increased to 0.63." ></td>
	<td class="line x" title="163:201	This result clearly indicates that the actual text contains significantly more information than the ratings." ></td>
	<td class="line x" title="164:201	We also experimented with predicting which merchant will make a sale, if they simultaneously sell the same product, based on their listed prices and on their numeric and text reputation." ></td>
	<td class="line x" title="165:201	Our C4.5 classifier (Quinlan, 1992) takes a pair of merchants and decides which of the two will make a sale." ></td>
	<td class="line x" title="166:201	We used as training set the transactions that took place in the first four months and as test set the transactions in the last two months of our data set." ></td>
	<td class="line x" title="167:201	Table 3 summarizes the results for different sets of features used." ></td>
	<td class="line x" title="168:201	The 55%421 Modifier Dimension Dollar Value [wonderful experience] $5.86 [outstanding seller] $5.76 [excellant service] $5.27 [lightning delivery] $4.84 [highly recommended] $4.15 [best seller] $3.80 [perfectly packaged] $3.74 [excellent condition] $3.53 [excellent purchase] $3.22 [excellent seller] $2.70 [excellent communication] $2.38 [perfect item] $1.92 [terrific condition] $1.87 [top quality] $1.67 [awesome service] $1.05 [A+++ seller] $1.03 [great merchant] $0.93 [friendly service] $0.81 [easy service] $0.78 [never received] -$7.56 [defective product] -$6.82 [horible experience] -$6.79 [never sent] -$6.69 [never recieved] -$5.29 [bad experience] -$5.26 [cancelled order] -$5.01 [never responded] -$4.87 [wrong product] -$4.39 [not as advertised] -$3.93 [poor packaging] -$2.92 [late shipping] -$2.89 [wrong item] -$2.50 [not yet received] -$2.35 [still waiting] -$2.25 [wrong address] -$1.54 [never buy] -$1.48 Table 2: The highest scoring opinion phrases, as determined by the product wk a(j,dk)." ></td>
	<td class="line x" title="169:201	accuracy when using only prices as features indicates that customers rarely choose a product based solely on price." ></td>
	<td class="line x" title="170:201	Rather, as indicated by the 74% accuracy, they also consider the reputation of the merchants." ></td>
	<td class="line x" title="171:201	However, the real value of the postings relies on the text and not on the numeric ratings: the accuracy is 87%89% when using the textual reputation variables." ></td>
	<td class="line x" title="172:201	In fact, text subsumes the numeric variables but not vice versa, as indicated by the results in Table 3." ></td>
	<td class="line x" title="173:201	6 Related Work To the best of our knowledge, our work is the first to use economics for measuring the effect of opinions and deriving their polarity and strength in an econometric manner." ></td>
	<td class="line x" title="174:201	A few papers in the past tried to combine text analysis with economics (Das and Chen, 2006; Lewitt and Syverson, 2005), but the text analysis was limited to token counting and did not use Features Accuracy on Test Set Price 55% Price + Numeric Reputation 74% Price + Numeric Reputation 89% + Text Reputation Price + Text Reputation 87% Table 3: Predicting the merchant who makes the sale." ></td>
	<td class="line x" title="175:201	any NLP techniques." ></td>
	<td class="line x" title="176:201	The technique of Section 4.1 is based on existing research in sentiment analysis." ></td>
	<td class="line x" title="177:201	For instance, (Hatzivassiloglou and McKeown, 1997; Nigam and Hurst, 2004) use annotated data to create a supervised learning technique to identify the semantic orientation of adjectives." ></td>
	<td class="line x" title="178:201	We follow the approach by Turney (2002), who note that the semantic orientation of an adjective depends on the noun that it modifies and suggest using adjective-noun or adverb-verb pairs to extract semantic orientation." ></td>
	<td class="line x" title="179:201	However, we do not rely on linguistic resources (Kamps and Marx, 2002) or on search engines (Turney and Littman, 2003) to determine the semantic orientation, but rather rely on econometrics for this task." ></td>
	<td class="line x" title="180:201	Hu and Liu (2004), whose study is the closest to our work, use WordNet to compute the semantic orientation of product evaluations and try to summarize user reviews by extracting the positive and negative evaluations of the different product features." ></td>
	<td class="line x" title="181:201	Similarly, Snyder and Barzilay (2007) decompose an opinion across several dimensions and capture the sentiment across each dimension." ></td>
	<td class="line x" title="182:201	Other work in this area includes (Lee, 2004; Popescu and Etzioni, 2005) which uses text mining in the context product reviews, but none uses the economic context to evaluate the opinions." ></td>
	<td class="line x" title="183:201	7 Conclusion and Further Applications We demonstrated the value of using econometrics for extracting a quantitative interpretation of opinions." ></td>
	<td class="line x" title="184:201	Our technique, additionally, takes into consideration the context within which these opinions are evaluated." ></td>
	<td class="line x" title="185:201	Our experimental results show that our techniques can capture the pragmatic meaning of the expressed opinions using simple economic variables as a form of training data." ></td>
	<td class="line x" title="186:201	The source code with our implementation together with the data set used in this paper are available from http://economining.stern.nyu.edu." ></td>
	<td class="line x" title="187:201	There are many other applications beyond reputation systems." ></td>
	<td class="line x" title="188:201	For example, using sales rank data from Amazon.com, we can examine the effect of product reviews on product sales and detect the weight that422 customers put on different product features; furthermore, we can discover how customer evaluations on individual product features affect product sales and extract the pragmatic meaning of these evaluations." ></td>
	<td class="line x" title="189:201	Another application is the analysis of the effect of news stories on stock prices: we can examine what news topics are important for the stock market and see how the views of different opinion holders and the wording that they use can cause the market to move up or down." ></td>
	<td class="line x" title="190:201	In a slightly different twist, we can analyze news stories and blogs in conjunction with results from prediction markets and extract the pragmatic effect of news and blogs on elections or other political events." ></td>
	<td class="line x" title="191:201	Another research direction is to examine the effect of summarizing product descriptions on product sales: short descriptions reduce the cognitive load of consumers but increase their uncertainty about the underlying product characteristics; a longer description has the opposite effect." ></td>
	<td class="line x" title="192:201	The optimum description length is the one that balances both effects and maximizes product sales." ></td>
	<td class="line x" title="193:201	Similar approaches can improve the state of art in both economics and computational linguistics." ></td>
	<td class="line x" title="194:201	In economics and in social sciences in general, most researchers handle textual data manually or with simplistic token counting techniques; in the worst case they ignore text data altogether." ></td>
	<td class="line x" title="195:201	In computational linguistics, researchers often rely on human annotators to generate training data, a laborious and errorprone task." ></td>
	<td class="line x" title="196:201	We believe that cross-fertilization of ideas between the fields of computational linguistics and econometrics can be beneficial for both fields." ></td>
	<td class="line x" title="197:201	Acknowledgments The authors would like to thank Elena Filatova for the useful discussions and the pointers to related literature." ></td>
	<td class="line x" title="198:201	We also thank Sanjeev Dewan, Alok Gupta, Bin Gu, and seminar participants at Carnegie Mellon University, Columbia University, Microsoft Research, New York University, Polytechnic University, and University of Florida for their comments and feedback." ></td>
	<td class="line x" title="199:201	We thank Rhong Zheng for assistance in data collection." ></td>
	<td class="line x" title="200:201	This work was partially supported by a Microsoft Live Labs Search Award, a Microsoft Virtual Earth Award, and by NSF grants IIS-0643847 and IIS-0643846." ></td>
	<td class="line x" title="201:201	Any opinions, findings, and conclusions expressed in this material are those of the authors and do not necessarily reflect the views of the Microsoft Corporation or of the National Science Foundation." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-1055
Structured Models for Fine-to-Coarse Sentiment Analysis
McDonald, Ryan;Hannan, Kerry;Neylon, Tyler;Wells, Mike;Reynar, Jeffrey C.;"></td>
	<td class="line x" title="1:215	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 432439, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:215	c2007 Association for Computational Linguistics Structured Models for Fine-to-Coarse Sentiment Analysis Ryan McDonald Kerry Hannan Tyler Neylon Mike Wells Jeff Reynar Google, Inc. 76 Ninth Avenue New York, NY 10011 Contact email: ryanmcd@google.com Abstract In this paper we investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity." ></td>
	<td class="line x" title="3:215	Inference in the model is based on standard sequence classification techniques using constrained Viterbi to ensure consistent solutions." ></td>
	<td class="line x" title="4:215	The primary advantage of such a model is that it allows classification decisions from one level in the text to influence decisions at another." ></td>
	<td class="line x" title="5:215	Experiments show that this method can significantly reduce classification error relative to models trained in isolation." ></td>
	<td class="line x" title="6:215	1 Introduction Extractingsentimentfromtextisachallengingproblem with applications throughout Natural Language Processing and Information Retrieval." ></td>
	<td class="line oc" title="7:215	Previous workonsentimentanalysishascoveredawiderange of tasks, including polarity classification (Pang et al. , 2002; Turney, 2002), opinion extraction (Pang and Lee, 2004), and opinion source assignment (Choi et al. , 2005; Choi et al. , 2006)." ></td>
	<td class="line oc" title="8:215	Furthermore, these systems have tackled the problem at different levels of granularity, from the document level (Pang et al. , 2002), sentence level (Pang and Lee, 2004; Mao and Lebanon, 2006), phrase level (Turney, 2002; Choi et al. , 2005), as well as the speaker level in debates (Thomas et al. , 2006)." ></td>
	<td class="line x" title="9:215	The ability to classify sentiment on multiple levels is importantsincedifferentapplicationshavedifferentneeds." ></td>
	<td class="line x" title="10:215	For example, a summarization system for product reviews might require polarity classification at the sentence or phrase level; a question answering system would most likely require the sentiment of paragraphs; and a system that determines which articles from an online news source are editorial in nature would require a document level analysis." ></td>
	<td class="line x" title="11:215	This work focuses on models that jointly classify sentimentonmultiplelevelsofgranularity." ></td>
	<td class="line x" title="12:215	Consider the following example, This is the first Mp3 player that I have used I thought it sounded great  After only a few weeks, it started having trouble with the earphone connection  I wont be buying another." ></td>
	<td class="line x" title="13:215	Mp3 player review from Amazon.com Thisexcerptexpressesanoverallnegativeopinionof the product being reviewed." ></td>
	<td class="line x" title="14:215	However, not all parts of the review are negative." ></td>
	<td class="line x" title="15:215	The first sentence merely provides some context on the reviewers experience with such devices and the second sentence indicates that, at least in one regard, the product performed well." ></td>
	<td class="line x" title="16:215	We call the problem of identifying the sentiment of the document and of all its subcomponents, whether at the paragraph, sentence, phrase or word level, fine-to-coarse sentiment analysis." ></td>
	<td class="line x" title="17:215	The simplest approach to fine-to-coarse sentiment analysis would be to create a separate system for each level of granularity." ></td>
	<td class="line x" title="18:215	There are, however, obvious advantages to building a single model that classifies each level in tandem." ></td>
	<td class="line x" title="19:215	Consider the sentence, My 11 year old daughter has also been using it and it is a lot harder than it looks." ></td>
	<td class="line x" title="20:215	Inisolation, thissentenceappearstoconveynegative sentiment." ></td>
	<td class="line x" title="21:215	However, it is part of a favorable review 432 for a piece of fitness equipment, where hard essentially means good workout." ></td>
	<td class="line x" title="22:215	In this domain, hards sentiment can only be determined in context (i.e. , hard toassembleversusa hard workout)." ></td>
	<td class="line x" title="23:215	Iftheclassifierknewtheoverallsentimentofadocument, then disambiguating such cases would be easier." ></td>
	<td class="line x" title="24:215	Conversely, document level analysis can benefit from finer level classification by taking advantage of common discourse cues, such as the last sentence being a reliable indicator for overall sentiment in reviews." ></td>
	<td class="line x" title="25:215	Furthermore, during training, the model will not need to modify its parameters to explain phenomena like the typically positive word great appearing in a negative text (as is the case above)." ></td>
	<td class="line x" title="26:215	The model can also avoid overfitting to features derived from neutral or objective sentences." ></td>
	<td class="line pc" title="27:215	In fact, it has already been established that sentence level classification can improve document level analysis (Pang and Lee, 2004)." ></td>
	<td class="line o" title="28:215	This line of reasoning suggests that a cascaded approach would also be insufficient." ></td>
	<td class="line x" title="29:215	Valuable information is passed in both directions, which means any model of fine-to-coarse analysis should account for this." ></td>
	<td class="line x" title="30:215	In Section 2 we describe a simple structured model that jointly learns and infers sentiment on different levels of granularity." ></td>
	<td class="line x" title="31:215	In particular, we reduce the problem of joint sentence and document level analysis to a sequential classification problem using constrained Viterbi inference." ></td>
	<td class="line x" title="32:215	Extensions to the model that move beyond just two-levels of analysis are also presented." ></td>
	<td class="line x" title="33:215	In Section 3 an empirical evaluation of the model is given that shows significant gains in accuracy over both single level classifiers and cascaded systems." ></td>
	<td class="line x" title="34:215	1.1 Related Work The models in this work fall into the broad class of globalstructuredmodels, whicharetypicallytrained with structured learning algorithms." ></td>
	<td class="line x" title="35:215	Hidden Markov models (Rabiner, 1989) are one of the earliest structured learning algorithms, which have recently been followedbydiscriminativelearningapproachessuch as conditional random fields (CRFs) (Lafferty et al. , 2001; Sutton and McCallum, 2006), the structured perceptron (Collins, 2002) and its large-margin variants (Taskar et al. , 2003; Tsochantaridis et al. , 2004; McDonald et al. , 2005; Daume III et al. , 2006)." ></td>
	<td class="line x" title="36:215	These algorithms are usually applied to sequential labeling or chunking, but have also been applied to parsing (Taskar et al. , 2004; McDonald et al. , 2005), machine translation (Liang et al. , 2006) and summarization (Daume III et al. , 2006)." ></td>
	<td class="line x" title="37:215	Structured models have previously been used for sentiment analysis." ></td>
	<td class="line x" title="38:215	Choi et al.(2005, 2006) use CRFs to learn a global sequence model to classify and assign sources to opinions." ></td>
	<td class="line x" title="40:215	Mao and Lebanon (2006) used a sequential CRF regression model to measure polarity on the sentence level in order to determine the sentiment flow of authors in reviews." ></td>
	<td class="line x" title="41:215	Here we show that fine-to-coarse models of sentiment can often be reduced to the sequential case." ></td>
	<td class="line oc" title="42:215	Cascaded models for fine-to-coarse sentiment analysis were studied by Pang and Lee (2004)." ></td>
	<td class="line o" title="43:215	In that work an initial model classified each sentence as being subjective or objective using a global mincut inference algorithm that considered local labeling consistencies." ></td>
	<td class="line x" title="44:215	The top subjective sentences are then input into a standard document level polarity classifier with improved results." ></td>
	<td class="line o" title="45:215	The current work differs from that in Pang and Lee through the use of a single joint structured model for both sentence and document level analysis." ></td>
	<td class="line x" title="46:215	Many problems in natural language processing can be improved by learning and/or predicting multiple outputs jointly." ></td>
	<td class="line x" title="47:215	This includes parsing and relation extraction (Miller et al. , 2000), entity labeling and relation extraction (Roth and Yih, 2004), and part-of-speech tagging and chunking (Sutton et al. , 2004)." ></td>
	<td class="line x" title="48:215	One interesting work on sentiment analysis isthatofPopescuandEtzioni(2005)whichattempts to classify the sentiment of phrases with respect to possible product features." ></td>
	<td class="line x" title="49:215	To do this an iterative algorithm is used that attempts to globally maximize the classification of all phrases while satisfying local consistency constraints." ></td>
	<td class="line x" title="50:215	2 Structured Model In this section we present a structured model for fine-to-coarse sentiment analysis." ></td>
	<td class="line x" title="51:215	We start by examining the simple case with two-levels of granularity  the sentence and document  and show that the problem can be reduced to sequential classification with constrained inference." ></td>
	<td class="line x" title="52:215	We then discuss the feature space and give an algorithm for learning the parameters based on large-margin structured learning." ></td>
	<td class="line x" title="53:215	433 Extensions to the model are also examined." ></td>
	<td class="line x" title="54:215	2.1 A Sentence-Document Model Let Y(d) be a discrete set of sentiment labels at the document level and Y(s) be a discrete set of sentiment labels at the sentence level." ></td>
	<td class="line x" title="55:215	As input a system is given a document containing sentences s = s1,,sn and must produce sentiment labels for the document, yd  Y(d), and each individual sentence, ys = ys1,,ysn, where ysi  Y(s)  1  i  n. Define y = (yd,ys) = (yd,ys1,,ysn) as the joint labeling of the document and sentences." ></td>
	<td class="line oc" title="56:215	For instance, in Pang and Lee (2004), yd would be the polarity of the document and ysi would indicate whether sentence si is subjective or objective." ></td>
	<td class="line x" title="57:215	The models presented here are compatible with arbitrary sets of discrete output labels." ></td>
	<td class="line x" title="58:215	Figure 1 presents a model for jointly classifying the sentiment of both the sentences and the document." ></td>
	<td class="line x" title="59:215	In this undirected graphical model, the label of each sentence is dependent on the labels of its neighbouring sentences plus the label of the document." ></td>
	<td class="line x" title="60:215	The label of the document is dependent on the label of every sentence." ></td>
	<td class="line x" title="61:215	Note that the edges between the input (each sentence) and the output labels are not solid, indicating that they are given as input and are not being modeled." ></td>
	<td class="line x" title="62:215	The fact that the sentiment of sentences is dependent not only on the local sentiment of other sentences, but also the global document sentiment  and vice versa  allows the model to directly capture the importance of classification decisions across levels in fine-tocoarse sentiment analysis." ></td>
	<td class="line oc" title="63:215	The local dependencies between sentiment labels on sentences is similar to the work of Pang and Lee (2004) where soft local consistency constraints were created between every sentence in adocument and inference wassolved using a min-cut algorithm." ></td>
	<td class="line n" title="64:215	However, jointly modeling the document label and allowing for non-binary labelscomplicatesmin-cut stylesolutionsasinference becomes intractable." ></td>
	<td class="line x" title="65:215	Learning and inference in undirected graphical models is a well studied problem in machine learning and NLP." ></td>
	<td class="line x" title="66:215	For example, CRFs define the probability over the labels conditioned on the input using the property that the joint probability distribution over the labels factors over clique potentials in undirected graphical models (Lafferty et al. , 2001)." ></td>
	<td class="line x" title="67:215	Figure 1: Sentence and document level model." ></td>
	<td class="line x" title="68:215	In this work we will use structured linear classifiers (Collins, 2002)." ></td>
	<td class="line x" title="69:215	We denote the score of a labeling y for an input s as score(y,s) and define this score as the sum of scores over each clique, score(y,s) = score((yd,ys),s) = score((yd,ys1,,ysn),s) = nsummationdisplay i=2 score(yd,ysi1,ysi,s) where each clique score is a linear combination of features and their weights, score(yd,ysi1,ysi,s) = wf(yd,ysi1,ysi,s) (1) and f is a high dimensional feature representation of the clique and w a corresponding weight vector." ></td>
	<td class="line x" title="70:215	Note that s is included in each score since it is given as input and can always be conditioned on." ></td>
	<td class="line x" title="71:215	Ingeneral, inferenceinundirectedgraphicalmodels is intractable." ></td>
	<td class="line x" title="72:215	However, for the common case of sequences(a.k.a.linear-chain models)theViterbialgorithm can be used (Rabiner, 1989; Lafferty et al. , 2001)." ></td>
	<td class="line x" title="73:215	Fortunately there is a simple technique that reduces inference in the above model to sequence classification with a constrained version of Viterbi." ></td>
	<td class="line x" title="74:215	2.1.1 Inference as Sequential Labeling The inference problem is to find the highest scoring labeling y for an input s, i.e., argmax y score(y,s) If the document label yd is fixed, then inference in the model from Figure 1 reduces to the sequential case." ></td>
	<td class="line x" title="75:215	This is because the search space is only over the sentence labels ysi, whose graphical structure forms a chain." ></td>
	<td class="line x" title="76:215	Thus the problem of finding the 434 Input: s = s1,,sn 1." ></td>
	<td class="line x" title="77:215	y = null 2." ></td>
	<td class="line x" title="78:215	for each yd  Y(d) 3." ></td>
	<td class="line x" title="79:215	ys = argmaxys score((yd,ys),s) 4." ></td>
	<td class="line x" title="80:215	yprime = (yd,ys) 5." ></td>
	<td class="line x" title="81:215	if score(yprime,s) > score(y,s) or y = null 6." ></td>
	<td class="line x" title="82:215	y = yprime 7." ></td>
	<td class="line x" title="83:215	return y Figure 2: Inference algorithm for model in Figure 1." ></td>
	<td class="line x" title="84:215	The argmax in line 3 can be solved using Viterbis algorithm since yd is fixed." ></td>
	<td class="line x" title="85:215	highest scoring sentiment labels for all sentences, given a particular document label yd, can be solved efficiently using Viterbis algorithm." ></td>
	<td class="line x" title="86:215	The general inference problem can then be solved by iterating over each possible yd, finding ys maximizing score((yd,ys),s) and keeping the single best y = (yd,ys)." ></td>
	<td class="line x" title="87:215	This algorithm is outlined in Figure 2 and has a runtime of O(|Y(d)||Y(s)|2n), due to running Viterbi |Y(d)| times over a label space of size |Y(s)|." ></td>
	<td class="line x" title="88:215	The algorithm can be extended to produce exact k-best lists." ></td>
	<td class="line x" title="89:215	This is achieved by using k-best Viterbi techniques to return the k-best global labelings for each document label in line 3." ></td>
	<td class="line x" title="90:215	Merging these sets will produce the final k-best list." ></td>
	<td class="line x" title="91:215	It is possible to view the inference algorithm in Figure 2 as a constrained Viterbi search since it is equivalent to flattening the model in Figure 1 to a sequential model with sentence labels from the set Y(s)  Y(d)." ></td>
	<td class="line x" title="92:215	The resulting Viterbi search would then need to be constrained to ensure consistent solutions, i.e., the label assignments agree on the document label over all sentences." ></td>
	<td class="line x" title="93:215	If viewed this way, it is also possible to run a constrained forwardbackward algorithm and learn the parameters for CRFs as well." ></td>
	<td class="line x" title="94:215	2.1.2 Feature Space In this section we define the feature representation for each clique, f(yd,ysi1,ysi,s)." ></td>
	<td class="line x" title="95:215	Assume that each sentence si is represented by a set of binary predicates P(si)." ></td>
	<td class="line x" title="96:215	This set can contain any predicate over the input s, but for the present purposes it will include all the unigram, bigram and trigrams in the sentence si conjoined with their part-of-speech (obtained from an automatic classifier)." ></td>
	<td class="line x" title="97:215	Back-offs of each predicate are also included where one or more word is discarded." ></td>
	<td class="line x" title="98:215	For instance, if P(si) contains the predicate a:DT great:JJ product:NN, then it would also have the predicates a:DT great:JJ *:NN, a:DT *:JJ product:NN, *:DT great:JJ product:NN, a:DT *:JJ *:NN, etc. Each predicate, p, is then conjoined with the label information to construct a binary feature." ></td>
	<td class="line x" title="99:215	For example, if the sentence label set is Y(s) = {subj,obj} and the document set is Y(d) = {pos,neg}, then the system might contain the following feature, f(j)(yd,ysi1,ysi,s) =     1 if p  P(si) and ysi1 = obj and ysi = subj and yd = neg 0 otherwise Where f(j) is the jth dimension of the feature space." ></td>
	<td class="line x" title="100:215	For each feature, a set of back-off features are included that only consider the document label yd, the current sentence label ysi, the current sentence and document label ysi and yd, and the current and previous sentence labels ysi and ysi1." ></td>
	<td class="line x" title="101:215	Note that through these back-off features the joint models feature set will subsume the feature set of any individual level model." ></td>
	<td class="line x" title="102:215	Only features observed in the training data were considered." ></td>
	<td class="line x" title="103:215	Depending on the data set, the dimension of the feature vector f ranged from 350K to 500K." ></td>
	<td class="line x" title="104:215	Though the feature vectors can be sparse, the feature weights will be learned using large-margin techniques that are well known to be robust to large and sparse feature representations." ></td>
	<td class="line x" title="105:215	2.1.3 Training the Model Let Y = Y(d)  Y(s)n be the set of all valid sentence-document labelings for an input s. The weights, w, are set using the MIRA learning algorithm, which is an inference based online largemargin learning technique (Crammer and Singer, 2003; McDonald et al. , 2005)." ></td>
	<td class="line x" title="106:215	An advantage of this algorithm is that it relies only on inference to learn the weight vector (see Section 2.1.1)." ></td>
	<td class="line x" title="107:215	MIRA has been shown to provide state-of-the-art accuracy for many language processing tasks including parsing, chunking and entity extraction (McDonald, 2006)." ></td>
	<td class="line x" title="108:215	The basic algorithm is outlined in Figure 3." ></td>
	<td class="line x" title="109:215	The algorithm works by considering a single training instance during each iteration." ></td>
	<td class="line x" title="110:215	The weight vector w is updated in line 4 through a quadratic programming problem." ></td>
	<td class="line x" title="111:215	This update modifies the weight vector so 435 Training data: T = {(yt,st)}Tt=1 1." ></td>
	<td class="line x" title="112:215	w(0) = 0; i = 0 2." ></td>
	<td class="line x" title="113:215	for n : 1N 3." ></td>
	<td class="line x" title="114:215	for t : 1T 4." ></td>
	<td class="line x" title="115:215	w(i+1) = argminw*  w*w(i)   s.t. score(yt,st) score(yprime,s)  L(yt,yprime) relative to w* yprime  C  Y, where |C| = k 5." ></td>
	<td class="line x" title="116:215	i = i + 1 6." ></td>
	<td class="line x" title="117:215	return w(NT) Figure 3: MIRA learning algorithm." ></td>
	<td class="line x" title="118:215	that the score of the correct labeling is larger than the score of every labeling in a constraint set C with a margin proportional to the loss." ></td>
	<td class="line x" title="119:215	The constraint set C can be chosen arbitrarily, but it is usually taken to be the k labelings that have the highest score under the old weight vector w(i) (McDonald et al. , 2005)." ></td>
	<td class="line x" title="120:215	In this manner, the learning algorithm can update its parameters relative to those labelings closest to the decisionboundary." ></td>
	<td class="line x" title="121:215	Ofalltheweightvectorsthatsatisfy these constraints, MIRA chooses the one that is as close as possible to the previous weight vector in order to retain information about previous updates." ></td>
	<td class="line x" title="122:215	The loss function L(y,yprime) is a positive real valued function and is equal to zero when y = yprime." ></td>
	<td class="line x" title="123:215	This function is task specific and is usually the hamming loss for sequence classification problems (Taskar et al. , 2003)." ></td>
	<td class="line x" title="124:215	Experiments with different loss functions forthe jointsentence-document modelon adevelopment data set indicated that the hamming loss over sentence labels multiplied by the 0-1 loss over document labels worked best." ></td>
	<td class="line x" title="125:215	An important modification that was made to the learning algorithm deals with how the k constraints arechosenfortheoptimization." ></td>
	<td class="line x" title="126:215	Typicallytheseconstraints are the k highest scoring labelings under the current weight vector." ></td>
	<td class="line x" title="127:215	However, early experiments showed that the model quickly learned to discard any labeling with an incorrect document label for the instances in the training set." ></td>
	<td class="line x" title="128:215	As a result, the constraints were dominated by labelings that only differed over sentence labels." ></td>
	<td class="line x" title="129:215	This did not allow the algorithm adequate opportunity to set parameters relative to incorrect document labeling decisions." ></td>
	<td class="line x" title="130:215	To combat this, k was divided by the number of document labels, to get a new value kprime." ></td>
	<td class="line x" title="131:215	For each document label, the kprime highest scoring labelings were Figure 4: An extension to the model from Figure 1 incorporating paragraph level analysis." ></td>
	<td class="line x" title="132:215	extracted." ></td>
	<td class="line x" title="133:215	Each of these sets were then combined to produce the final constraint set." ></td>
	<td class="line x" title="134:215	This allowed constraints to be equally distributed amongst different document labels." ></td>
	<td class="line x" title="135:215	Based on performance on the development data set the number of training iterations was set to N = 5 and the number of constraints to k = 10." ></td>
	<td class="line x" title="136:215	Weight averaging was also employed (Collins, 2002), which helped improve performance." ></td>
	<td class="line x" title="137:215	2.2 Beyond Two-Level Models To this point, we have focused solely on a model for two-level fine-to-coarse sentiment analysis not only for simplicity, but because the experiments in Section 3 deal exclusively with this scenario." ></td>
	<td class="line x" title="138:215	In this section, we briefly discuss possible extensions for more complex situations." ></td>
	<td class="line x" title="139:215	For example, longer documents might benefit from an analysis on the paragraph level as well as the sentence and document levels." ></td>
	<td class="line x" title="140:215	One possible model for this case is given in Figure 4, which essentially inserts an additional layer between the sentence and document level from the original model." ></td>
	<td class="line x" title="141:215	Sentence level analysis is dependent on neighbouring sentences as well as the paragraph level analysis, and the paragraph analysis is dependent on each of the sentences within it, the neighbouring paragraphs, and the document level analysis." ></td>
	<td class="line x" title="142:215	This can be extended to an arbitrary level of fine-to-coarse sentiment analysis by simply inserting new layers in this fashion to create more complex hierarchical models." ></td>
	<td class="line x" title="143:215	The advantage of using hierarchical models of this form is that they are nested, which keeps inference tractable." ></td>
	<td class="line x" title="144:215	Observe that each pair of adjacent levels in the model is equivalent to the original model from Figure 1." ></td>
	<td class="line x" title="145:215	As a result, the scores of the every label at each node in the graph can be calculated with a straight-forward bottom-up dynamic programming algorithm." ></td>
	<td class="line x" title="146:215	Details are omitted 436 Sentence Stats Document Stats Pos Neg Neu Tot Pos Neg Tot Car 472 443 264 1179 98 80 178 Fit 568 635 371 1574 92 97 189 Mp3 485 464 214 1163 98 89 187 Tot 1525 1542 849 3916 288 266 554 Table 1: Data statistics for corpus." ></td>
	<td class="line x" title="147:215	Pos = positive polarity, Neg = negative polarity, Neu = no polarity." ></td>
	<td class="line x" title="148:215	for space reasons." ></td>
	<td class="line x" title="149:215	Other models are possible where dependencies occur across non-neighbouring levels, e.g., by inserting edges between the sentence level nodes and the document level node." ></td>
	<td class="line x" title="150:215	In the general case, inference is exponential in the size of each clique." ></td>
	<td class="line x" title="151:215	Both the models in Figure 1 and Figure 4 have maximum clique sizes of three." ></td>
	<td class="line x" title="152:215	3 Experiments 3.1 Data To test the model we compiled a corpus of 600 online product reviews from three domains: car seats forchildren, fitnessequipment, andMp3players." ></td>
	<td class="line x" title="153:215	Of the original 600 reviews that were gathered, we discarded duplicate reviews, reviews with insufficient text, and spam." ></td>
	<td class="line x" title="154:215	All reviews were labeled by onlinecustomersashavingapositiveornegativepolarity on the document level, i.e., Y(d) = {pos,neg}." ></td>
	<td class="line x" title="155:215	Each review was then split into sentences and every sentence annotated by a single annotator as either being positive, negative or neutral, i.e., Y(s) = {pos,neg,neu}." ></td>
	<td class="line x" title="156:215	Data statistics for the corpus are given in Table 1." ></td>
	<td class="line x" title="157:215	All sentences were annotated based on their context within the document." ></td>
	<td class="line x" title="158:215	Sentences were annotated as neutral if they conveyed no sentiment or had indeterminate sentiment from their context." ></td>
	<td class="line x" title="159:215	Many neutral sentences pertain to the circumstances under which the product was purchased." ></td>
	<td class="line x" title="160:215	A common class of sentences were those containing product features." ></td>
	<td class="line x" title="161:215	These sentences were annotated as having positive or negative polarity if the context supported it." ></td>
	<td class="line x" title="162:215	This could include punctuation such as exclamation points, smiley/frowny faces, question marks, etc. The supporting evidence could also come from another sentence, e.g., I love it." ></td>
	<td class="line x" title="163:215	It has 64Mb of memory and comes with a set of earphones." ></td>
	<td class="line x" title="164:215	3.2 Results Three baseline systems were created,  Document-Classifier is a classifier that learns to predict the document label only." ></td>
	<td class="line x" title="165:215	 Sentence-Classifier is a classifier that learns to predict sentence labels in isolation of one another, i.e., without consideration for either the document or neighbouring sentences sentiment." ></td>
	<td class="line x" title="166:215	 Sentence-Structured is another sentence classifier, but this classifier uses a sequential chain model to learn and classify sentences." ></td>
	<td class="line x" title="167:215	The thirdbaselineisessentiallythemodelfromFigure1withoutthetopleveldocumentnode." ></td>
	<td class="line x" title="168:215	This baselinewillhelptogagetheempiricalgainsof the different components of the joint structured model on sentence level classification." ></td>
	<td class="line x" title="169:215	The model described in Section 2 will be called Joint-Structured." ></td>
	<td class="line x" title="170:215	All models use the same basic predicate space: unigram, bigram, trigram conjoined with part-of-speech, plus back-offs of these (see Section 2.1.2 for more)." ></td>
	<td class="line x" title="171:215	However, due to the structure of the model and its label space, the feature space of each might be different, e.g., the document classifier will only conjoin predicates with the document label to create the feature set." ></td>
	<td class="line x" title="172:215	All models are trained using the MIRA learning algorithm." ></td>
	<td class="line x" title="173:215	Results for each model are given in the first four rows of Table 2." ></td>
	<td class="line x" title="174:215	These results were gathered using 10-fold cross validation with one fold for development and the other nine folds for evaluation." ></td>
	<td class="line x" title="175:215	This table shows that classifying sentences in isolation from one another is inferior to accounting for a more global context." ></td>
	<td class="line x" title="176:215	A significant increase in performance can be obtained when labeling decisions between sentences are modeled (Sentence-Structured)." ></td>
	<td class="line x" title="177:215	More interestingly, even further gains can be had when document level decisions are modeled (JointStructured)." ></td>
	<td class="line x" title="178:215	In many cases, these improvements are highly statistically significant." ></td>
	<td class="line x" title="179:215	On the document level, performance can also be improved by incorporating sentence level decisions  though these improvements are not consistent." ></td>
	<td class="line x" title="180:215	This inconsistency may be a result of the model overfitting on the small set of training data." ></td>
	<td class="line x" title="181:215	We 437 suspect this because the document level error rate on the Mp3 training set converges to zero much more rapidly for the Joint-Structured model than the Document-Classifier." ></td>
	<td class="line x" title="182:215	This suggests that the JointStructured model might be relying too much on the sentence level sentiment features  in order to minimize its error rate  instead of distributing the weights across all features more evenly." ></td>
	<td class="line x" title="183:215	One interesting application of sentence level sentiment analysis is summarizing product reviews on retail websites like Amazon.com or review aggregators like Yelp.com." ></td>
	<td class="line x" title="184:215	In this setting the correct polarity of a document is often known, but we wish to label sentiment on the sentence or phrase level to aid in generating a cohesive and informative summary." ></td>
	<td class="line x" title="185:215	The joint model can be used to classify sentences in this setting by constraining inference to the known fixed document label for a review." ></td>
	<td class="line x" title="186:215	If this is done, then sentiment accuracy on the sentence level increases substantially from 62.6% to 70.3%." ></td>
	<td class="line x" title="187:215	Finally we should note that experiments using CRFs to train the structured models and logistic regression to train the local models yielded similar results to those in Table 2." ></td>
	<td class="line x" title="188:215	3.2.1 Cascaded Models Another approach to fine-to-coarse sentiment analysis is to use a cascaded system." ></td>
	<td class="line x" title="189:215	In such a system, a sentence level classifier might first be run on the data, and then the results input into a document level classifier  or vice-versa.1 Two cascaded systems were built." ></td>
	<td class="line x" title="190:215	The first uses the SentenceStructured classifier to classify all the sentences from a review, then passes this information to the document classifier as input." ></td>
	<td class="line x" title="191:215	In particular, for every predicate in the original document classifier, an additional predicate that specifies the polarity of the sentence in which this predicate occurred was created." ></td>
	<td class="line x" title="192:215	The second cascaded system uses the document classifier to determine the global polarity, then passes this information as input into the SentenceStructured model, constructing predicates in a similar manner." ></td>
	<td class="line x" title="193:215	The results for these two systems can be seen in the last two rows of Table 2." ></td>
	<td class="line oc" title="194:215	In both cases there 1Alternatively, decisions from the sentence classifier can guide which input is seen by the document level classifier (Pang and Lee, 2004)." ></td>
	<td class="line x" title="195:215	is a slight improvement in performance suggesting that an iterative approach might be beneficial." ></td>
	<td class="line x" title="196:215	That is, a system could start by classifying documents, use the document information to classify sentences, use the sentence information to classify documents, and repeat until convergence." ></td>
	<td class="line x" title="197:215	However, experiments showedthatthisdidnotimproveaccuracyoverasingle iteration and often hurt performance." ></td>
	<td class="line x" title="198:215	Improvements from the cascaded models are far less consistent than those given from the joint structure model." ></td>
	<td class="line x" title="199:215	This is because decisions in the cascaded system are passed to the next layer as the gold standard at test time, which results in errors from the first classifier propagating to errors in the second." ></td>
	<td class="line x" title="200:215	This could be improved by passing a lattice of possibilities from the first classifier to the second withcorrespondingconfidences." ></td>
	<td class="line x" title="201:215	However, solutions such as these are really just approximations of the joint structured model that was presented here." ></td>
	<td class="line x" title="202:215	4 Future Work One important extension to this work is to augment the models for partially labeled data." ></td>
	<td class="line x" title="203:215	It is realistic to imagine a training set where many examples do not have every level of sentiment annotated." ></td>
	<td class="line x" title="204:215	For example, there are thousands of online product reviews with labeled document sentiment, but a much smaller amount where sentences are also labeled." ></td>
	<td class="line x" title="205:215	Work on learning with hidden variables can be used for both CRFs (Quattoni et al. , 2004) and for inference based learning algorithms like those used in this work (Liang et al. , 2006)." ></td>
	<td class="line x" title="206:215	Another area of future work is to empirically investigate the use of these models on longer documents that require more levels of sentiment analysis than product reviews." ></td>
	<td class="line x" title="207:215	In particular, the relative position of a phrase to a contrastive discourse connective or a cue phrase like in conclusion or to summarize may lead to improved performance since higher level classifications can learn to weigh information passed from these lower level components more heavily." ></td>
	<td class="line x" title="208:215	5 Discussion In this paper we have investigated the use of a global structured model that learns to predict sentiment on different levels of granularity for a text." ></td>
	<td class="line x" title="209:215	We de438 Sentence Accuracy Document Accuracy Car Fit Mp3 Total Car Fit Mp3 Total Document-Classifier 72.8 80.1 87.2 80.3 Sentence-Classifier 54.8 56.8 49.4 53.1 Sentence-Structured 60.5 61.4 55.7 58.8 Joint-Structured 63.5 65.2 60.1 62.6 81.5 81.9 85.0 82.8 Cascaded Sentence  Document 60.5 61.4 55.7 58.8 75.9 80.7 86.1 81.1 Cascaded Document  Sentence 59.7 61.0 58.3 59.5 72.8 80.1 87.2 80.3 Table 2: Fine-to-coarse sentiment accuracy." ></td>
	<td class="line x" title="210:215	Significance calculated using McNemars test between top two performing systems." ></td>
	<td class="line x" title="211:215	Statistically significant p < 0.05." ></td>
	<td class="line x" title="212:215	Statistically significant p < 0.005." ></td>
	<td class="line x" title="213:215	scribed a simple model for sentence-document analysis and showed that inference in it is tractable." ></td>
	<td class="line x" title="214:215	Experiments show that this model obtains higher accuracy than classifiers trained in isolation as well as cascaded systems that pass information from one leveltoanotherattesttime." ></td>
	<td class="line x" title="215:215	Furthermore, extensions to the sentence-document model were discussed and it was argued that a nested hierarchical structure would be beneficial since it would allow for efficient inference algorithms." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-1123
Learning Multilingual Subjective Language via Cross-Lingual Projections
Mihalcea, Rada;Banea, Carmen;Wiebe, Janyce M.;"></td>
	<td class="line x" title="1:193	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 976983, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:193	c2007 Association for Computational Linguistics Learning Multilingual Subjective Language via Cross-Lingual Projections Rada Mihalcea and Carmen Banea Department of Computer Science University of North Texas rada@cs.unt.edu, carmenb@unt.edu Janyce Wiebe Department of Computer Science University of Pittsburgh wiebe@cs.pitt.edu Abstract This paper explores methods for generating subjectivity analysis resources in a new language by leveraging on the tools and resources available in English." ></td>
	<td class="line x" title="3:193	Given a bridge between English and the selected target language (e.g. , a bilingual dictionary or a parallel corpus), the methods can be used to rapidly create tools for subjectivity analysis in the new language." ></td>
	<td class="line x" title="4:193	1 Introduction There is growing interest in the automatic extraction of opinions, emotions, and sentiments in text (subjectivity), to provide tools and support for various natural language processing applications." ></td>
	<td class="line x" title="5:193	Most of the research to date has focused on English, which is mainly explained by the availability of resources for subjectivity analysis, such as lexicons and manually labeled corpora." ></td>
	<td class="line x" title="6:193	In this paper, we investigate methods to automatically generate resources for subjectivity analysis for a new target language by leveraging on the resources and tools available for English, which in many cases took years of work to complete." ></td>
	<td class="line x" title="7:193	Specifically, through experiments with cross-lingual projection of subjectivity, we seek answers to the following questions." ></td>
	<td class="line x" title="8:193	First, can we derive a subjectivity lexicon for a new language using an existing English subjectivity lexicon and a bilingual dictionary?" ></td>
	<td class="line x" title="9:193	Second, can we derive subjectivity-annotated corpora in a new language using existing subjectivity analysis tools for English and a parallel corpus?" ></td>
	<td class="line x" title="10:193	Finally, third, can we build tools for subjectivity analysis for a new target language by relying on these automatically generated resources?" ></td>
	<td class="line x" title="11:193	We focus our experiments on Romanian, selected as a representative of the large number of languages that have only limited text processing resources developed to date." ></td>
	<td class="line x" title="12:193	Note that, although we work with Romanian, the methods described are applicable to any other language, as in these experiments we (purposely) do not use any language-specific knowledge of the target language." ></td>
	<td class="line x" title="13:193	Given a bridge between English and the selected target language (e.g. , a bilingual dictionary or a parallel corpus), the methods can be applied to other languages as well." ></td>
	<td class="line x" title="14:193	After providing motivations, we present two approaches to developing sentence-level subjectivity classifiers for a new target language." ></td>
	<td class="line x" title="15:193	The first uses a subjectivity lexicon translated from an English one." ></td>
	<td class="line x" title="16:193	The second uses an English subjectivity classifier and a parallel corpus to create target-language training data for developing a statistical classifier." ></td>
	<td class="line xc" title="17:193	2 Motivation Automatic subjectivity analysis methods have been used in a wide variety of text processing applications, such as tracking sentiment timelines in online forums and news (Lloyd et al. , 2005; Balog et al. , 2006), review classification (Turney, 2002; Pang et al. , 2002), mining opinions from product reviews (Hu and Liu, 2004), automatic expressive text-to-speech synthesis (Alm et al. , 2005), text semantic analysis (Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2006), and question answering (Yu and Hatzivassiloglou, 2003)." ></td>
	<td class="line x" title="18:193	976 While much recent work in subjectivity analysis focuses on sentiment (a type of subjectivity, namely positive and negative emotions, evaluations, and judgments), we opt to focus on recognizing subjectivity in general, for two reasons." ></td>
	<td class="line pc" title="19:193	First, even when sentiment is the desired focus, researchers in sentiment analysis have shown that a two-stage approach is often beneficial, in which subjective instances are distinguished from objective ones, and then the subjective instances are further classified according to polarity (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al. , 2005; Kim and Hovy, 2006)." ></td>
	<td class="line x" title="20:193	In fact, the problem of distinguishing subjective versus objective instances has often proved to be more difficult than subsequent polarity classification, so improvements in subjectivity classification promise to positively impact sentiment classification." ></td>
	<td class="line x" title="21:193	This is reported in studies of manual annotation of phrases (Takamura et al. , 2006), recognizing contextual polarity of expressions (Wilson et al. , 2005), and sentiment tagging of words and word senses (Andreevskaia and Bergler, 2006; Esuli and Sebastiani, 2006)." ></td>
	<td class="line x" title="22:193	Second, an NLP application may seek a wide range of types of subjectivity attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments." ></td>
	<td class="line x" title="23:193	For instance, the opinion tracking system Lydia (Lloyd et al. , 2005) gives separate ratings for subjectivity and sentiment." ></td>
	<td class="line x" title="24:193	These can be detected with subjectivity analysis but not by a method focused only on sentiment." ></td>
	<td class="line x" title="25:193	There is world-wide interest in text analysis applications." ></td>
	<td class="line x" title="26:193	While work on subjectivity analysis in other languages is growing (e.g. , Japanese data are used in (Takamura et al. , 2006; Kanayama and Nasukawa, 2006), Chinese data are used in (Hu et al. , 2005), and German data are used in (Kim and Hovy, 2006)), much of the work in subjectivity analysis has been applied to English data." ></td>
	<td class="line x" title="27:193	Creating corpora and lexical resources for a new language is very time consuming." ></td>
	<td class="line x" title="28:193	In general, we would like to leverage resources already developed for one language to more rapidly create subjectivity analysis tools for a new one." ></td>
	<td class="line x" title="29:193	This motivates our exploration and use of cross-lingual lexicon translations and annotation projections." ></td>
	<td class="line x" title="30:193	Most if not all work on subjectivity analysis has been carried out in a monolingual framework." ></td>
	<td class="line x" title="31:193	We are not aware of multi-lingual work in subjectivity analysis such as that proposed here, in which subjectivity analysis resources developed for one language are used to support developing resources in another." ></td>
	<td class="line x" title="32:193	3 A Lexicon-Based Approach Many subjectivity and sentiment analysis tools rely on manually or semi-automatically constructed lexicons (Yu and Hatzivassiloglou, 2003; Riloff and Wiebe, 2003; Kim and Hovy, 2006)." ></td>
	<td class="line x" title="33:193	Given the success of such techniques, the first approach we take to generating a target-language subjectivity classifier is to create a subjectivity lexicon by translating an existing source language lexicon, and then build a classifier that relies on the resulting lexicon." ></td>
	<td class="line x" title="34:193	Below, we describe the translation process and discuss the results of an annotation study to assess the quality of the translated lexicon." ></td>
	<td class="line x" title="35:193	We then describe and evaluate a lexicon-based target-language classifier." ></td>
	<td class="line x" title="36:193	3.1 Translating a Subjectivity Lexicon The subjectivity lexicon we use is from OpinionFinder (Wiebe and Riloff, 2005), an English subjectivity analysis system which, among other things, classifies sentences as subjective or objective." ></td>
	<td class="line x" title="37:193	The lexicon was compiled from manually developed resources augmented with entries learned from corpora." ></td>
	<td class="line x" title="38:193	It contains 6,856 unique entries, out of which 990 are multi-word expressions." ></td>
	<td class="line x" title="39:193	The entries in the lexicon have been labeled for part of speech, and for reliability  those that appear most often in subjective contexts are strong clues of subjectivity, while those that appear less often, but still more often than expected by chance, are labeled weak." ></td>
	<td class="line x" title="40:193	To perform the translation, we use two bilingual dictionaries." ></td>
	<td class="line x" title="41:193	The first is an authoritative EnglishRomanian dictionary, consisting of 41,500 entries,1 which we use as the main translation resource for the lexicon translation." ></td>
	<td class="line x" title="42:193	The second dictionary, drawn from the Universal Dictionary download site (UDP, 2007) consists of 4,500 entries written largely by Web volunteer contributors, and thus is not error free." ></td>
	<td class="line x" title="43:193	We use this dictionary only for those entries that do not appear in the main dictionary." ></td>
	<td class="line x" title="44:193	1Unique English entries, each with multiple Romanian translations." ></td>
	<td class="line x" title="45:193	977 There were several challenges encountered in the translation process." ></td>
	<td class="line x" title="46:193	First, although the English subjectivity lexicon contains inflected words, we must use the lemmatized form in order to be able to translate the entries using the bilingual dictionary." ></td>
	<td class="line x" title="47:193	However, words may lose their subjective meaning once lemmatized." ></td>
	<td class="line x" title="48:193	For instance, the inflected form of memories becomes memory." ></td>
	<td class="line x" title="49:193	Once translated into Romanian (as memorie), its main meaning is objective, referring to the power of retaining information as in Iron supplements may improve a womans memory." ></td>
	<td class="line x" title="50:193	Second, neither the lexicon nor the bilingual dictionary provides information on the sense of the individual entries, and therefore the translation has to rely on the most probable sense in the target language." ></td>
	<td class="line x" title="51:193	Fortunately, the bilingual dictionary lists the translations in reverse order of their usage frequencies." ></td>
	<td class="line x" title="52:193	Nonetheless, the ambiguity of the words and the translations still seems to represent an important source of error." ></td>
	<td class="line x" title="53:193	Moreover, the lexicon sometimes includes identical entries expressed through different parts of speech, e.g., grudge has two separate entries, for its noun and verb roles, respectively." ></td>
	<td class="line x" title="54:193	On the other hand, the bilingual dictionary does not make this distinction, and therefore we have again to rely on the most frequent heuristic captured by the translation order in the bilingual dictionary." ></td>
	<td class="line x" title="55:193	Finally, the lexicon includes a significant number (990) of multi-word expressions that pose translation difficulties, sometimes because their meaning is idiomatic, and sometimes because the multi-word expression is not listed in the bilingual dictionary and the translation of the entire phrase is difficult to reconstruct from the translations of the individual words." ></td>
	<td class="line x" title="56:193	To address this problem, when a translation is not found in the dictionary, we create one using a word-by-word approach." ></td>
	<td class="line x" title="57:193	These translations are then validated by enforcing that they occur at least three times on the Web, using counts collected from the AltaVista search engine." ></td>
	<td class="line x" title="58:193	The multi-word expressions that are not validated in this process are discarded, reducing the number of expressions from an initial set of 990 to a final set of 264." ></td>
	<td class="line x" title="59:193	The final subjectivity lexicon in Romanian contains 4,983 entries." ></td>
	<td class="line x" title="60:193	Table 1 shows examples of entries in the Romanian lexicon, together with their corresponding original English form." ></td>
	<td class="line x" title="61:193	The table Romanian English attributes nfrumuseta beautifying strong, verb notabil notable weak, adj plin de regret full of regrets strong, adj sclav slaves weak, noun Table 1: Examples of entries in the Romanian subjectivity lexicon also shows the reliability of the expression (weak or strong) and the part of speech  attributes that are provided in the English subjectivity lexicon." ></td>
	<td class="line x" title="62:193	Manual Evaluation." ></td>
	<td class="line x" title="63:193	We want to assess the quality of the translated lexicon, and compare it to the quality of the original English lexicon." ></td>
	<td class="line x" title="64:193	The English subjectivity lexicon was evaluated in (Wiebe and Riloff, 2005) against a corpus of English-language news articles manually annotated for subjectivity (the MPQA corpus (Wiebe et al. , 2005))." ></td>
	<td class="line x" title="65:193	According to this evaluation, 85% of the instances of the clues marked as strong and 71.5% of the clues marked as weak are in subjective sentences in the MPQA corpus." ></td>
	<td class="line x" title="66:193	Since there is no comparable Romanian corpus, an alternate way to judge the subjectivity of a Romanian lexicon entry is needed." ></td>
	<td class="line x" title="67:193	Two native speakers of Romanian annotated the subjectivity of 150 randomly selected entries." ></td>
	<td class="line x" title="68:193	Each annotator independently read approximately 100 examples of each drawn from the Web, including a large number from news sources." ></td>
	<td class="line x" title="69:193	The subjectivity of a word was consequently judged in the contexts where it most frequently appears, accounting for its most frequent meanings on the Web." ></td>
	<td class="line x" title="70:193	The tagset used for the annotations consists of S(ubjective), O(bjective), and B(oth)." ></td>
	<td class="line x" title="71:193	A W(rong) label is also used to indicate a wrong translation." ></td>
	<td class="line x" title="72:193	Table 2 shows the contingency table for the two annotators judgments on this data." ></td>
	<td class="line x" title="73:193	S O B W Total S 53 6 9 0 68 O 1 27 1 0 29 B 5 3 18 0 26 W 0 0 0 27 27 Total 59 36 28 27 150 Table 2: Agreement on 150 entries in the Romanian lexicon Without counting the wrong translations, the agreement is measured at 0.80, with a Kappa  = 978 0.70, which indicates consistent agreement." ></td>
	<td class="line x" title="74:193	After the disagreements were reconciled through discussions, the final set of 123 correctly translated entries does include 49.6% (61) subjective entries, but fully 23.6% (29) were found in the study to have primarily objective uses (the other 26.8% are mixed)." ></td>
	<td class="line x" title="75:193	Thus, this study suggests that the Romanian subjectivity clues derived through translation are less reliable than the original set of English clues." ></td>
	<td class="line x" title="76:193	In several cases, the subjectivity is lost in the translation, mainly due to word ambiguity in either the source or target language, or both." ></td>
	<td class="line x" title="77:193	For instance, the word fragile correctly translates into Romanian as fragil, yet this word is frequently used to refer to breakable objects, and it loses its subjective meaning of delicate." ></td>
	<td class="line x" title="78:193	Other words, such as one-sided, completely lose subjectivity once translated, as it becomes in Romanian cu o singura latura, meaning with only one side (as of objects)." ></td>
	<td class="line x" title="79:193	Interestingly, the reliability of clues in the English lexicon seems to help preserve subjectivity." ></td>
	<td class="line x" title="80:193	Out of the 77 entries marked as strong, 11 were judged to be objective in Romanian (14.3%), compared to 14 objective Romanian entries obtained from the 36 weak English clues (39.0%)." ></td>
	<td class="line x" title="81:193	3.2 Rule-based Subjectivity Classifier Using a Subjectivity Lexicon Starting with the Romanian lexicon, we developed a lexical classifier similar to the one introduced by (Riloff and Wiebe, 2003)." ></td>
	<td class="line x" title="82:193	At the core of this method is a high-precision subjectivity and objectivity classifier that can label large amounts of raw text using only a subjectivity lexicon." ></td>
	<td class="line x" title="83:193	Their method is further improved with a bootstrapping process that learns extraction patterns." ></td>
	<td class="line x" title="84:193	In our experiments, however, we apply only the rule-based classification step, since the extraction step cannot be implemented without tools for syntactic parsing and information extraction not available in Romanian." ></td>
	<td class="line x" title="85:193	The classifier relies on three main heuristics to label subjective and objective sentences: (1) if two or more strong subjective expressions occur in the same sentence, the sentence is labeled Subjective; (2) if no strong subjective expressions occur in a sentence, and at most two weak subjective expressions occur in the previous, current, and next sentence combined, then the sentence is labeled Objective; (3) otherwise, if none of the previous rules apply, the sentence is labeled Unknown." ></td>
	<td class="line x" title="86:193	The quality of the classifier was evaluated on a Romanian gold-standard corpus annotated for subjectivity." ></td>
	<td class="line x" title="87:193	Two native Romanian speakers (Ro1 and Ro2) manually annotated the subjectivity of the sentences of five randomly selected documents (504 sentences) from the Romanian side of an EnglishRomanian parallel corpus, according to the annotation scheme in (Wiebe et al. , 2005)." ></td>
	<td class="line x" title="88:193	Agreement between annotators was measured, and then their differences were adjudicated." ></td>
	<td class="line x" title="89:193	The baseline on this data set is 54.16%, which can be obtained by assigning a default Subjective label to all sentences." ></td>
	<td class="line x" title="90:193	(More information about the corpus and annotations are given in Section 4 below, where agreement between English and Romanian aligned sentences is also assessed.)" ></td>
	<td class="line x" title="91:193	As mentioned earlier, due to the lexicon projection process that is performed via a bilingual dictionary, the entries in our Romanian subjectivity lexicon are in a lemmatized form." ></td>
	<td class="line x" title="92:193	Consequently, we also lemmatize the gold-standard corpus, to allow for the identification of matches with the lexicon." ></td>
	<td class="line x" title="93:193	For this purpose, we use the Romanian lemmatizer developed by Ion and Tufis (Ion, 2007), which has an estimated accuracy of 98%.2 Table 3 shows the results of the rule-based classifier." ></td>
	<td class="line x" title="94:193	We show the precision, recall, and F-measure independently measured for the subjective, objective, and all sentences." ></td>
	<td class="line x" title="95:193	We also evaluated a variation of the rule-based classifier that labels a sentence as objective if there are at most three weak expressions in the previous, current, and next sentence combined, which raises the recall of the objective classifier." ></td>
	<td class="line x" title="96:193	Our attempts to increase the recall of the subjective classifier all resulted in significant loss in precision, and thus we kept the original heuristic." ></td>
	<td class="line x" title="97:193	In its original English implementation, this system was proposed as being high-precision but low coverage." ></td>
	<td class="line x" title="98:193	Evaluated on the MPQA corpus, it has subjective precision of 90.4, subjective recall of 34.2, objective precision of 82.4, and objective recall of 30.7; overall, precision is 86.7 and recall is 32.6 (Wiebe and Riloff, 2005)." ></td>
	<td class="line x" title="99:193	We see a similar behavior on Romanian for subjective sentences." ></td>
	<td class="line x" title="100:193	The subjective precision is good, albeit at the cost of low 2Dan Tufis, personal communication." ></td>
	<td class="line x" title="101:193	979 Measure Subjective Objective All subj = at least two strong; obj = at most two weak Precision 80.00 56.50 62.59 Recall 20.51 48.91 33.53 F-measure 32.64 52.52 43.66 subj = at least two strong; obj = at most three weak Precision 80.00 56.85 61.94 Recall 20.51 61.03 39.08 F-measure 32.64 58.86 47.93 Table 3: Evaluation of the rule-based classifier recall, and thus the classifier could be used to harvest subjective sentences from unlabeled Romanian data (e.g. , for a subsequent bootstrapping process)." ></td>
	<td class="line x" title="102:193	The system is not very effective for objective classification, however." ></td>
	<td class="line x" title="103:193	Recall that the objective classifier relies on the weak subjectivity clues, for which the transfer of subjectivity in the translation process was particularly low." ></td>
	<td class="line x" title="104:193	4 A Corpus-Based Approach Given the low number of subjective entries found in the automatically generated lexicon and the subsequent low recall of the lexical classifier, we decided to also explore a second, corpus-based approach." ></td>
	<td class="line x" title="105:193	This approach builds a subjectivity-annotated corpus for the target language through projection, and then trains a statistical classifier on the resulting corpus (numerous statistical classifiers have been trained for subjectivity or sentiment classification, e.g., (Pang et al. , 2002; Yu and Hatzivassiloglou, 2003))." ></td>
	<td class="line x" title="106:193	The hypothesis is that we can eliminate some of the ambiguities (and consequent loss of subjectivity) observed during the lexicon translation by accounting for the context of the ambiguous words, which is possible in a corpus-based approach." ></td>
	<td class="line x" title="107:193	Additionally, we also hope to improve the recall of the classifier, by addressing those cases not covered by the lexicon-based approach." ></td>
	<td class="line x" title="108:193	In the experiments reported in this section, we use a parallel corpus consisting of 107 documents from the SemCor corpus (Miller et al. , 1993) and their manual translations into Romanian.3 The corpus consists of roughly 11,000 sentences, with approximately 250,000 tokens on each side." ></td>
	<td class="line x" title="109:193	It is a balanced corpus covering a number of topics in sports, politics, fashion, education, and others." ></td>
	<td class="line x" title="110:193	3The translation was carried out by a Romanian native speaker, student in a department of Foreign Languages and Translations in Romania." ></td>
	<td class="line x" title="111:193	Below, we begin with a manual annotation study to assess the quality of annotation and preservation of subjectivity in translation." ></td>
	<td class="line x" title="112:193	We then describe the automatic construction of a target-language training set, and evaluate a classifier trained on that data." ></td>
	<td class="line x" title="113:193	Annotation Study." ></td>
	<td class="line x" title="114:193	We start by performing an agreement study meant to determine the extent to which subjectivity is preserved by the cross-lingual projections." ></td>
	<td class="line x" title="115:193	In the study, three annotators  one native English speaker (En) and two native Romanian speakers (Ro1 and Ro2)  first trained on 3 randomly selected documents (331 sentences)." ></td>
	<td class="line x" title="116:193	They then independently annotated the subjectivity of the sentences of two randomly selected documents from the parallel corpus, accounting for 173 aligned sentence pairs." ></td>
	<td class="line x" title="117:193	The annotators had access exclusively to the version of the sentences in their language, to avoid any bias that could be introduced by seeing the translation in the other language." ></td>
	<td class="line x" title="118:193	Note that the Romanian annotations (after all differences between the Romanian annotators were adjudicated) of all 331 + 173 sentences make up the gold standard corpus used in the experiments reported in Sections 3.2 and 4.1." ></td>
	<td class="line x" title="119:193	Before presenting the results of the annotation study, we give some examples." ></td>
	<td class="line x" title="120:193	The following are English subjective sentences and their Romanian translations (the subjective elements are shown in bold)." ></td>
	<td class="line x" title="121:193	[en] The desire to give Broglio as many starts as possible." ></td>
	<td class="line x" title="122:193	[ro] Dorinta de a-i da lui Broglio cat mai multe starturi posibile." ></td>
	<td class="line x" title="123:193	[en] Suppose he did lie beside Lenin, would it be permanent ? [ro] Sa presupunem ca ar fi asezat alaturi de Lenin, oare va fi pentru totdeauna?" ></td>
	<td class="line x" title="124:193	The following are examples of objective parallel sentences." ></td>
	<td class="line x" title="125:193	[en]The Pirates have a 9-6 record this year and the Redbirds are 7-9." ></td>
	<td class="line x" title="126:193	[ro] Piratii au un palmares de 9 la 6 anul acesta si Pasarile Rosii au 7 la 9." ></td>
	<td class="line x" title="127:193	[en] One of the obstacles to the easy control of a 2-year old child is a lack of verbal communication." ></td>
	<td class="line x" title="128:193	[ro] Unul dintre obstacolele n controlarea unui copil de 2 ani este lipsa comunicarii verbale." ></td>
	<td class="line x" title="129:193	980 The annotators were trained using the MPQA annotation guidelines (Wiebe et al. , 2005)." ></td>
	<td class="line x" title="130:193	The tagset consists of S(ubjective), O(bjective) and U(ncertain)." ></td>
	<td class="line x" title="131:193	For the U tags, a class was also given; OU means, for instance, that the annotator is uncertain but she is leaning toward O. Table 4 shows the pairwise agreement figures and the Kappa () calculated for the three annotators." ></td>
	<td class="line x" title="132:193	The table also shows the agreement when the borderline uncertain cases are removed." ></td>
	<td class="line x" title="133:193	all sentences Uncertain removed pair agree  agree  (%) removed Ro1 & Ro2 0.83 0.67 0.89 0.77 23 En & Ro1 0.77 0.54 0.86 0.73 26 En & Ro2 0.78 0.55 0.91 0.82 20 Table 4: Agreement on the data set of 173 sentences." ></td>
	<td class="line x" title="134:193	Annotations performed by three annotators: one native English speaker (En) and two native Romanian speakers (Ro1 and Ro2) When all the sentences are included, the agreement between the two Romanian annotators is measured at 0.83 ( = 0.67)." ></td>
	<td class="line x" title="135:193	If we remove the borderline cases where at least one annotators tag is Uncertain, the agreement rises to 0.89 with  = 0.77." ></td>
	<td class="line x" title="136:193	These figures are somewhat lower than the agreement observed during previous subjectivity annotation studies conducted on English (Wiebe et al. , 2005) (the annotators were more extensively trained in those studies), but they nonetheless indicate consistent agreement." ></td>
	<td class="line x" title="137:193	Interestingly, when the agreement is conducted cross-lingually between an English and a Romanian annotator, the agreement figures, although somewhat lower, are comparable." ></td>
	<td class="line x" title="138:193	In fact, once the Uncertain tags are removed, the monolingual and cross-lingual agreement and  values become almost equal, which suggests that in most cases the sentence-level subjectivity is preserved." ></td>
	<td class="line x" title="139:193	The disagreements were reconciled first between the labels assigned by the two Romanian annotators, followed by a reconciliation between the resulting Romanian gold-standard labels and the labels assigned by the English annotator." ></td>
	<td class="line x" title="140:193	In most cases, the disagreement across the two languages was found to be due to a difference of opinion about the sentence subjectivity, similar to the differences encountered in monolingual annotations." ></td>
	<td class="line x" title="141:193	However, there are cases where the differences are due to the subjectivity being lost in the translation." ></td>
	<td class="line x" title="142:193	Sometimes, this is due to several possible interpretations for the translated sentence." ></td>
	<td class="line x" title="143:193	For instance, the following sentence: [en] They honored the battling Billikens last night." ></td>
	<td class="line x" title="144:193	[ro] Ei i-au celebrat pe Billikens seara trecuta. is marked as Subjective in English (in context, the English annotator interpreted honored as referring to praises of the Billikens)." ></td>
	<td class="line x" title="145:193	However, the Romanian translation of honored is celebrat which, while correct as a translation, has the more frequent interpretation of having a party." ></td>
	<td class="line x" title="146:193	The two Romanian annotators chose this interpretation, which correspondingly lead them to mark the sentence as Objective." ></td>
	<td class="line x" title="147:193	In other cases, in particular when the subjectivity is due to figures of speech such as irony, the translation sometimes misses the ironic aspects." ></td>
	<td class="line x" title="148:193	For instance, the translation of egghead was not perceived as ironic by the Romanian annotators, and consequently the following sentence labeled Subjective in English is annotated as Objective in Romanian." ></td>
	<td class="line x" title="149:193	[en] I have lived for many years in a Connecticut commuting town with a high percentage of [] business executives of egghead tastes." ></td>
	<td class="line x" title="150:193	[ro] Am trait multi ani ntr-un oras din apropiere de Connecticut ce avea o mare proportie de [] oameni de afaceri cu gusturi intelectuale." ></td>
	<td class="line x" title="151:193	4.1 Translating a Subjectivity-Annotated Corpus and Creating a Machine Learning Subjectivity Classifier To further validate the corpus-based projection of subjectivity, we developed a subjectivity classifier trained on Romanian subjectivity-annotated corpora obtained via cross-lingual projections." ></td>
	<td class="line x" title="152:193	Ideally, one would generate an annotated Romanian corpus by translating English documents manually annotated for subjectivity such as the MPQA corpus." ></td>
	<td class="line x" title="153:193	Unfortunately, the manual translation of this corpus would be prohibitively expensive, both timewise and financially." ></td>
	<td class="line x" title="154:193	The other alternative  automatic machine translation  has not yet reached a level that would enable the generation of a highquality translated corpus." ></td>
	<td class="line x" title="155:193	We therefore decided to use a different approach where we automatically annotate the English side of an existing EnglishRomanian corpus, and subsequently project the annotations onto the Romanian side of the parallel cor981 Precision Recall F-measure high-precision 86.7 32.6 47.4 high-coverage 79.4 70.6 74.7 Table 5: Precision, recall, and F-measure for the two OpinionFinder classifiers, as measured on the MPQA corpus." ></td>
	<td class="line x" title="156:193	pus across the sentence-level alignments available in the corpus." ></td>
	<td class="line x" title="157:193	For the automatic subjectivity annotations, we generated two sets of the English-side annotations, one using the high-precision classifier and one using the high-coverage classifier available in the OpinionFinder tool." ></td>
	<td class="line x" title="158:193	The high-precision classifier in OpinionFinder uses the clues of the subjectivity lexicon to harvest subjective and objective sentences from a large amount of unannotated text; this data is then used to automatically identify a set of extraction patterns, which are then used iteratively to identify a larger set of subjective and objective sentences." ></td>
	<td class="line x" title="159:193	In addition, in OpinionFinder, the high-precision classifier is used to produce an English labeled data set for training, which is used to generate its Naive Bayes high-coverage subjectivity classifier." ></td>
	<td class="line x" title="160:193	Table 5 shows the performance of the two classifiers on the MPQA corpus as reported in (Wiebe and Riloff, 2005)." ></td>
	<td class="line x" title="161:193	Note that 55% of the sentences in the MPQA corpus are subjective  which represents the baseline for this data set." ></td>
	<td class="line x" title="162:193	The two OpinionFinder classifiers are used to label the training corpus." ></td>
	<td class="line x" title="163:193	After removing the 504 test sentences, we are left with 10,628 sentences that are automatically annotated for subjectivity." ></td>
	<td class="line x" title="164:193	Table 6 shows the number of subjective and objective sentences obtained with each classifier." ></td>
	<td class="line x" title="165:193	Classifier Subjective Objective All high-precision 1,629 2,334 3,963 high-coverage 5,050 5,578 10,628 Table 6: Subjective and objective training sentences automatically annotated with OpinionFinder." ></td>
	<td class="line x" title="166:193	Next, the OpinionFinder annotations are projected onto the Romanian training sentences, which are then used to develop a probabilistic classifier for the automatic labeling of subjectivity in Romanian sentences." ></td>
	<td class="line x" title="167:193	Similar to, e.g., (Pang et al. , 2002), we use a Naive Bayes algorithm trained on word features cooccurring with the subjective and the objective classifications." ></td>
	<td class="line x" title="168:193	We assume word independence, and we use a 0.3 cut-off for feature selection." ></td>
	<td class="line x" title="169:193	While recent work has also considered more complex syntactic features, we are not able to generate such features for Romanian as they require tools currently not available for this language." ></td>
	<td class="line x" title="170:193	We create two classifiers, one trained on each data set." ></td>
	<td class="line x" title="171:193	The quality of the classifiers is evaluated on the 504-sentence Romanian gold-standard corpus described above." ></td>
	<td class="line x" title="172:193	Recall that the baseline on this data set is 54.16%, the percentage of sentences in the corpus that are subjective." ></td>
	<td class="line x" title="173:193	Table 7 shows the results." ></td>
	<td class="line x" title="174:193	Subjective Objective All projection source: OF high-precision classifier Precision 65.02 69.62 64.48 Recall 82.41 47.61 64.48 F-measure 72.68 56.54 64.68 projection source: OF high-coverage classifier Precision 66.66 70.17 67.85 Recall 81.31 52.17 67.85 F-measure 72.68 56.54 67.85 Table 7: Evaluation of the machine learning classifier using training data obtained via projections from data automatically labeled by OpinionFinder (OF)." ></td>
	<td class="line x" title="175:193	Our best classifier has an F-measure of 67.85, and is obtained by training on projections from the high-coverage OpinionFinder annotations." ></td>
	<td class="line x" title="176:193	Although smaller than the 74.70 F-measure obtained by the English high-coverage classifier (see Table 5), the result appears remarkable given that no language-specific Romanian information was used." ></td>
	<td class="line x" title="177:193	The overall results obtained with the machine learning approach are considerably higher than those obtained from the rule-based classifier (except for the precision of the subjective sentences)." ></td>
	<td class="line x" title="178:193	This is most likely due to the lexicon translation process, which as mentioned in the agreement study in Section 3.1, leads to ambiguity and loss of subjectivity." ></td>
	<td class="line x" title="179:193	Instead, the corpus-based translations seem to better account for the ambiguity of the words, and the subjectivity is generally preserved in the sentence translations." ></td>
	<td class="line x" title="180:193	5 Conclusions In this paper, we described two approaches to generating resources for subjectivity annotations for a new 982 language, by leveraging on resources and tools available for English." ></td>
	<td class="line x" title="181:193	The first approach builds a target language subjectivity lexicon by translating an existing English lexicon using a bilingual dictionary." ></td>
	<td class="line x" title="182:193	The second generates a subjectivity-annotated corpus in a target language by projecting annotations from an automatically annotated English corpus." ></td>
	<td class="line x" title="183:193	These resources were validated in two ways." ></td>
	<td class="line x" title="184:193	First, we carried out annotation studies measuring the extent to which subjectivity is preserved across languages in each of the two resources." ></td>
	<td class="line x" title="185:193	These studies show that only a relatively small fraction of the entries in the lexicon preserve their subjectivity in the translation, mainly due to the ambiguity in both the source and the target languages." ></td>
	<td class="line x" title="186:193	This is consistent with observations made in previous work that subjectivity is a property associated not with words, but with word meanings (Wiebe and Mihalcea, 2006)." ></td>
	<td class="line x" title="187:193	In contrast, the sentence-level subjectivity was found to be more reliably preserved across languages, with cross-lingual inter-annotator agreements comparable to the monolingual ones." ></td>
	<td class="line x" title="188:193	Second, we validated the two automatically generated subjectivity resources by using them to build a tool for subjectivity analysis in the target language." ></td>
	<td class="line x" title="189:193	Specifically, we developed two classifiers: a rulebased classifier that relies on the subjectivity lexicon described in Section 3.1, and a machine learning classifier trained on the subjectivity-annotated corpus described in Section 4.1." ></td>
	<td class="line x" title="190:193	While the highest precision for the subjective classification is obtained with the rule-based classifier, the overall best result of 67.85 F-measure is due to the machine learning approach." ></td>
	<td class="line x" title="191:193	This result is consistent with the annotation studies, showing that the corpus projections preserve subjectivity more reliably than the lexicon translations." ></td>
	<td class="line x" title="192:193	Finally, neither one of the classifiers relies on language-specific information, but rather on knowledge obtained through projections from English." ></td>
	<td class="line x" title="193:193	A similar method can therefore be used to derive tools for subjectivity analysis in other languages." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-3007
Kinds of Features for Chinese Opinionated Information Retrieval
Zagibalov, Taras;"></td>
	<td class="line x" title="1:134	Proceedings of the ACL 2007 Student Research Workshop, pages 3742, Prague, June 2007." ></td>
	<td class="line x" title="2:134	c2007 Association for Computational Linguistics Kinds of Features for Chinese Opinionated Information Retrieval Taras Zagibalov Department of Informatics University of Sussex United Kingdom T.Zagibalov@sussex.ac.uk Abstract This paper presents the results of experiments in which we tested different kinds of features for retrieval of Chinese opinionated texts." ></td>
	<td class="line x" title="3:134	We assume that the task of retrieval of opinionated texts (OIR) can be regarded as a subtask of general IR, but with some distinct features." ></td>
	<td class="line x" title="4:134	The experiments showed that the best results were obtained from the combination of character-based processing, dictionary look up (maximum matching) and a negation check." ></td>
	<td class="line x" title="5:134	1 Introduction The extraction of opinionated information has recently become an important research topic." ></td>
	<td class="line x" title="6:134	Business and governmental institutions often need to have information about how their products or actions are perceived by people." ></td>
	<td class="line x" title="7:134	Individuals may be interested in other peoples opinions on various topics ranging from political events to consumer products." ></td>
	<td class="line x" title="8:134	At the same time globalization has made the whole world smaller, and a notion of the world as a global village does not surprise people nowadays." ></td>
	<td class="line x" title="9:134	In this context we assume information in Chinese to be of particular interest as the Chinese world (the mainland China, Taiwan, Hong Kong, Singapore and numerous Chinese communities all over the world) is getting more and more influential over the world economy and politics." ></td>
	<td class="line x" title="10:134	We therefore believe that a system capable of providing access to opinionated information in other languages (especially in Chinese) might be of great use for individuals as well as for institutions involved in international trade or international relations." ></td>
	<td class="line x" title="11:134	The sentiment classification experiments presented in this paper were done in the context of Opinionated Information Retrieval which is planned to be a module in a Cross-Language Opinion Extraction system (CLOE)." ></td>
	<td class="line x" title="12:134	The main goal of this system is to provide access to opinionated information on any topic ad-hoc in a language different to the language of a query." ></td>
	<td class="line x" title="13:134	To implement the idea the CLOE system which is the context for the experiments described in the paper will consist of four main modules: 1." ></td>
	<td class="line x" title="14:134	Query translation 2." ></td>
	<td class="line x" title="15:134	Opinionated Information Retrieval 3." ></td>
	<td class="line x" title="16:134	Opinionated Information Extraction 4." ></td>
	<td class="line x" title="17:134	Results presentation The OIR module will process complex queries consisting of a word sequence indicating a topic and sentiment information." ></td>
	<td class="line x" title="18:134	An example of such a query is: Asus laptop + OPINIONS, another, more detailed query, might be Asus laptop + POSITIVE OPINIONS." ></td>
	<td class="line x" title="19:134	Another possible approach to the architecture of the CLOE system would be to implement the processing as a pipeline consisting, first, of using IR to retrieve certain articles relevant to the topic followed by second stage of classifying them according to sentiment polarity." ></td>
	<td class="line x" title="20:134	But such an approach probably would be too inefficient, as the search will produce a lot of irrelevant results (containing no opinionated information)." ></td>
	<td class="line x" title="21:134	37 2 Chinese NLP and Feature Selection Problem One of the central problems in Chinese NLP is what the basic unit1 of processing should be." ></td>
	<td class="line x" title="22:134	The problem is caused by a distinctive feature of the Chinese language absence of explicit word boundaries, while it is widely assumed that a word is of extreme importance for any NLP task." ></td>
	<td class="line x" title="23:134	This problem is also crucial for the present study as the basic unit definition affects the kinds of features to be used." ></td>
	<td class="line x" title="24:134	In this study we use a mixed approached, based both on words (tokens consisting of more than one character) and characters as basic units." ></td>
	<td class="line x" title="25:134	It is also important to note, that we use notion of words in the sense of Vocabulary Word as it was stated by Li (2000)." ></td>
	<td class="line x" title="26:134	This means that we use only tokens that are listed in a dictionary, and do not look for all words (including grammar words)." ></td>
	<td class="line x" title="27:134	3 Related Work Processing of subjective texts and opinions has received a lot of interest recently." ></td>
	<td class="line xc" title="28:134	Most of the authors traditionally use a classification-based approach for sentiment extraction and sentiment polarity detection (for example, Pang et al.(2002), Turney (2002), Kim and Hovy (2004) and others), however, the research described in this paper uses the information retrieval (IR) paradigm which has also been used by some researchers." ></td>
	<td class="line x" title="30:134	Several sentiment information retrieval models were proposed in the framework of probabilistic language models by Eguchi and Lavrenko (2006)." ></td>
	<td class="line x" title="31:134	The setting for the study was a situation when a users query specifies not only terms expressing a certain topic and also specifies a sentiment polarity of interest in some manner, which makes this research very similar to the present one." ></td>
	<td class="line x" title="32:134	However, we use sentiment scores (not probabilistic language models) for sentiment retrieval (see Section 4.1)." ></td>
	<td class="line x" title="33:134	Dave et al.(Dave et al. , 2003) described a tool for sifting through and synthesizing product reviews, automating the sort of work done by aggregation sites or clipping services." ></td>
	<td class="line x" title="35:134	The authors of this paper used probability scores of arbitrary-length substrings that provide optimal classification." ></td>
	<td class="line x" title="36:134	Unlike this approach 1In the context of this study terms feature and basic unit are used interchangeably." ></td>
	<td class="line x" title="37:134	we use a combination of sentiment weights of characters and words (see Section 4)." ></td>
	<td class="line x" title="38:134	Recently several works on sentiment extraction from Chinese texts were published." ></td>
	<td class="line x" title="39:134	In a paper by Ku et al.(2006a) a dictionary-based approach was used in the context of sentiment extraction and summarization." ></td>
	<td class="line x" title="41:134	The same authors describe a corpus of opinionated texts in another paper (2006b)." ></td>
	<td class="line x" title="42:134	This paper also defines the annotations for opinionated materials." ></td>
	<td class="line x" title="43:134	Although we use the same dictionary in our research, we do not use only word-based approach to sentiment detection, but we also use scores for characters obtained by processing the dictionary as a training corpus (see Section 4)." ></td>
	<td class="line x" title="44:134	4 Experiments In this paper we present the results of sentiment classification experiments in which we tested different kinds of features for retrieval of Chinese opinionated information." ></td>
	<td class="line x" title="45:134	As stated earlier (see Section 1), we assume that the task of retrieval of opinionated texts (OIR) can be regarded as a subtask of general IR with a query consisting of two parts: (1) words indicating topic and (2) a semantic class indicating sentiment (OPINIONS)." ></td>
	<td class="line x" title="46:134	The latter part of the query cannot be specified in terms that can be instantly used in the process of retrieval." ></td>
	<td class="line x" title="47:134	The sentiment part of the query can be further detailed into subcategories such as POSITIVE OPINIONS, NEGATIVE OPINIONS, NEUTRAL OPINIONS each of which can be split according to sentiment intensity (HIGHLY POSITIVE OPINIONS, SLIGHTLY NEGATIVE OPINIONS etc.)." ></td>
	<td class="line x" title="48:134	But whatever level of categorisation we use, the query is still too abstract and cannot be used in practice." ></td>
	<td class="line x" title="49:134	It therefore needs to be put into words and most probably expanded." ></td>
	<td class="line x" title="50:134	The texts should also be indexed with appropriate sentiment tags which in the context of sentiment processing implies classification of the texts according to presence / absence of a sentiment and, if the texts are opinionated, according to their sentiment polarity." ></td>
	<td class="line x" title="51:134	To test the proposed approach we designed two experiments." ></td>
	<td class="line x" title="52:134	The purpose of the first experiment was to find the most effective kind of features for sentiment polar38 ity discrimination (detection) which can be used for OIR 2." ></td>
	<td class="line x" title="53:134	Nie et al.(2000) found that for Chinese IR the most effective kinds of features were a combination of dictionary look up (longest-match algorithm) together with unigrams (single characters)." ></td>
	<td class="line x" title="55:134	The approach was tested in the first experiment." ></td>
	<td class="line x" title="56:134	The second experiment was designed to test the found set of features for text classification (indexing) for an OIR query of the first level (finds opinionated information) and for an OIR query of the second level (finds opinionated information with sentiment direction detection), thus the classifier should 1) detect opinionated texts and 2) classify the found items either as positive or as negative." ></td>
	<td class="line x" title="57:134	As training corpus for the second experiment we use the NTU sentiment dictionary (NTUSD) (by Ku et al.(2006a))3 as well as a list of sentiment scores of Chinese characters obtained from processing of the same dictionary." ></td>
	<td class="line x" title="59:134	Dictionary look up used the longest-match algorithm." ></td>
	<td class="line x" title="60:134	The dictionary has 2809 items in the positive part and 8273 items in the negative." ></td>
	<td class="line x" title="61:134	The same dictionary was also used as a corpus for calculating the sentiment scores of Chinese characters." ></td>
	<td class="line x" title="62:134	The use of the dictionary as a training corpus for obtaining the sentiment scores of characters is justified by two reasons: 1) it is domain-independent and 2) it contains only relevant (sentiment-related) information." ></td>
	<td class="line x" title="63:134	The above mentioned parts of the dictionary used as the corpus comprised 24308 characters in the negative part and 7898 characters in the positive part." ></td>
	<td class="line x" title="64:134	4.1 Experiment 1 A corpus of E-Bay4 customers reviews of products and services was used as a test corpus." ></td>
	<td class="line x" title="65:134	The total number of reviews is 128, of which 37 are negative (average length 64 characters) and 91 are positive (average length 18 characters), all of the reviews were tagged as positive or negative by the 2For simplicity we used only binary polarity in both experiments: positive or negative." ></td>
	<td class="line x" title="66:134	Thus terms sentiment polarity and sentiment direction are used interchangeably in this paper." ></td>
	<td class="line x" title="67:134	3Ku et al.(2006a) automatically generated the dictionary by enlarging an initial manually created seed vocabulary by consulting two thesauri, including tong2yi4ci2ci2lin2 and the Academia Sinica Bilingual Ontological Wordnet 3." ></td>
	<td class="line x" title="69:134	4http://www.ebay.com.cn/ reviewers5." ></td>
	<td class="line x" title="70:134	We computed two scores for each item (a review): one for positive sentiment, another for negative sentiment." ></td>
	<td class="line x" title="71:134	The decision about an items sentiment polarity was made every time by finding the biggest score of the two." ></td>
	<td class="line x" title="72:134	For every phrase (a chunk of characters between punctuation marks) a score was calculated as: Scphrase =summationdisplay(Scdictionary) +summationdisplay(Sccharacter) where Scdictionary is a dictionary based score calculated using following formula: Scdictionary = LdL s 100 where Ld length of a dictionary item, Ls length of a phrase." ></td>
	<td class="line x" title="73:134	The constant value 100 is used to weight the score, obtained by a series of preliminary tests as a value that most significantly improved the accuracy." ></td>
	<td class="line x" title="74:134	The sentiment scores for characters were obtained by the formula: Sci = Fi/F(i+j) where Sci is the sentiment score for a character for a given class i, Fi the characters relative frequency in a class i, F(i+j) the characters relative frequency in both classes i and j taken as one unit." ></td>
	<td class="line x" title="75:134	The relative frequency of character c is calculated as Fc = summationtextN csummationtext N(1n) where summationtextNc is a number of the characters occurrences in the corpus, andsummationtextN(1n) is the number of all characters in the same corpus." ></td>
	<td class="line x" title="76:134	Preliminary tests showed that inverting all the characters for which Sci  1 improves accuracy." ></td>
	<td class="line x" title="77:134	The inverting is calculated as follows: Scinverted = Sci 1 We compute scores rather than probabilities since we are combining information from two distinct sources (characters and words)." ></td>
	<td class="line x" title="78:134	5The corpus is available at http://www.informatics.sussex.ac.uk/users/tz21/corpSmall.zip." ></td>
	<td class="line x" title="79:134	39 In addition to the features specified (characters and dictionary items) we also used a simple negation check." ></td>
	<td class="line x" title="80:134	The system checked two most widely used negations in Chinese: bu and mei." ></td>
	<td class="line x" title="81:134	Every phrase was compared with the following pattern: negation+ 0-2 characters+ phrase." ></td>
	<td class="line x" title="82:134	The scores of all the unigrams in the phrase that matched the pattern were multiplied by -1." ></td>
	<td class="line x" title="83:134	Finally, the score was calculated for an item as the sum of the phrases scores modified by the negation check: Scitem =summationdisplay(Scphrase NegCheck) For sentiment polarity detection the item scores for each of the two polarities were compared to each other: the polarity with bigger score was assigned to the item." ></td>
	<td class="line x" title="84:134	SentimentPolarity = argmax(Sci|Scj) where Sci is an item score for one polarity and Scj is an item score for the other." ></td>
	<td class="line x" title="85:134	The main evaluation measure was accuracy of sentiment identification, expressed in percent." ></td>
	<td class="line x" title="86:134	4.1.1 Results of Experiment 1 To find out which kinds of features perform best for sentiment polarity detection the system was run several times with different settings." ></td>
	<td class="line x" title="87:134	Running without character scores (with dictionary longest-match only) gave the following results: almost 64% of positive and near 65% for negative reviews were detected correctly, which is 64% accuracy for the whole corpus (note that a baseline classifier tagging all items as positive achieves an accuracy of 71.1%)." ></td>
	<td class="line x" title="88:134	Characters with sentiment scores alone performed much better on negative reviews (84% accuracy) rather than on positive (65%), but overall performance was still better: 70%." ></td>
	<td class="line x" title="89:134	Both methods combined gave a significant increase on positive reviews (73%) and no improvement on negative (84%), giving 77% overall." ></td>
	<td class="line x" title="90:134	The last run was with the dictionary look up, the characters and the negation check." ></td>
	<td class="line x" title="91:134	The results were: 77% for positive and 89% for negative, 80% corpus-wide (see Table 1)." ></td>
	<td class="line x" title="92:134	Judging from the results it is possible to suggest that both the word-based dictionary look up method Method Positive Negative All Dictionary 63.7 64.8 64.0 Characters 64.8 83.7 70.3 Characters+Dictionary 73.6 83.7 76.5 Chars+Dictionary+negation 76.9 89.1 80.4 Table 1: Results of Experiment 1 (accuracy in percent)." ></td>
	<td class="line x" title="93:134	and character-based method contributed to the final result." ></td>
	<td class="line x" title="94:134	It also corresponds to the results obtained by Nie et al.(2000) for Chinese information retrieval, where the same combination of features (characters and words) also performed best." ></td>
	<td class="line x" title="96:134	The negation check increased the performance by 3% overall, up to 80%." ></td>
	<td class="line x" title="97:134	Although the performance gain is not very high, the computational cost of this feature is very low." ></td>
	<td class="line x" title="98:134	As we used a non-balanced corpus (71% of the reviews are positive), it is quite difficult to compare the results with the results obtained by other authors." ></td>
	<td class="line x" title="99:134	But the proposed classifier outperformed some standart classifiers on the same data set: a Naive Bayes (multinomial) classifier gained only 49.6 % of accuracy (63 items tagged correctly) while a Support vector machine classifier got 64.5 % of accuracy (82 items).6 4.2 Experiment 2 The second experiment included two parts: determining whether texts are opinionated which is a precondition for the processing of the OPINION part of the query; and tagging found texts with relevant sentiment for processing a more detailed form of this query POSITIVE/NEGATIVE OPINION." ></td>
	<td class="line x" title="100:134	For this experiment we used the features that showed the best performance as described in section 4.1: the dictionary items and the characters with the sentiment scores." ></td>
	<td class="line x" title="101:134	The test corpus for this experiment consisted of 282 items, where every item is a paragraph." ></td>
	<td class="line x" title="102:134	We used paragraphs as basic items in this experiment because of two reasons: 1." ></td>
	<td class="line x" title="103:134	opinionated texts (reviews) are usually quite short (in our corpus all of them are one paragraph), while texts of other genres are usually much longer; and 2." ></td>
	<td class="line x" title="104:134	for IR tasks it is more usual to retrieve units longer then a sentence." ></td>
	<td class="line x" title="105:134	6We used WEKA 3.4.10 (http://www.cs.waikato.ac.nz/ ml/weka ) 40 The test corpus has following structure: 128 items are opinionated, of which 91 are positive and 37 are negative (all the items are the reviews used in the first experiment, see 4.1)." ></td>
	<td class="line x" title="106:134	154 items are not opinionated, of which 97 are paragraphs taken from a scientific book on Chinese linguistics and 57 items are from articles taken form a Chinese on-line encyclopedia Baidu Baike7." ></td>
	<td class="line x" title="107:134	For the first task we used the following technique: every item was assigned a score (a sum of the characters scores and dictionary scores described in 4.1)." ></td>
	<td class="line x" title="108:134	The score was divided by the number of characters in the item to obtain the average score: averScitem = ScitemL item where Scitem is the item score, and Litem is the length of an item (number of characters in it)." ></td>
	<td class="line x" title="109:134	A positive and a negative average score is computed for each item." ></td>
	<td class="line x" title="110:134	4.2.1 Results of Experiment 2 To determine whether an item is opinionated (for OPINION query), the maximum of the two scores was compared to a threshold value." ></td>
	<td class="line x" title="111:134	The best performance was achieved with the threshold value of 1.6 more than 85% of accuracy8 (see Table 2)." ></td>
	<td class="line x" title="112:134	Next task (NEGATIVE/POSITIVE OPINIONS) was processed by comparing the negative and positive scores for each found item (see Table 2)." ></td>
	<td class="line x" title="113:134	Query Recall Precision F-measure OPINION 71.8 85.1 77.9 POS/NEG OPINION 64.0 75.9 69.4 Table 2: Results of Experiment 2 (in percent)." ></td>
	<td class="line x" title="114:134	Although the unopinionated texts are very different from the opinionated ones in terms of genre and topic, the standard classifiers (Naive Bayes (multinomial) and SVM) failed to identify any nonopinionated texts." ></td>
	<td class="line x" title="115:134	The most probable explanation for this is that there were no items tagged unopinionated in the training corpus (the sentiment dictionary) and there were only words and phrases with predominant sentiment meaning rather then topicrelated." ></td>
	<td class="line x" title="116:134	7http://baike.baidu.com/ 8A random choice could have approximately 55% of accuracy if tagged all items as negative." ></td>
	<td class="line oc" title="117:134	It is worth noting that we observed the same relation between subjectivity detection and polarity classification accuracy as described by Pang and Lee (2004) and Eriksson (2006)." ></td>
	<td class="line x" title="118:134	The accuracy of the sentiment detection of opinionated texts (excluding erroneously detected unopinionated texts) in Experiment 2 has increased by 13% for positive reviews and by 6% for negative reviews (see Table 3)." ></td>
	<td class="line x" title="119:134	Query Positive Negative Experiment 1 76.9 89.1 Experiment 2 89.9 95.6 Table 3: Accuracy of sentiment polarity detection of opinionated texts (in percent)." ></td>
	<td class="line x" title="120:134	5 Conclusion and Future Work These preliminary experiments showed that using single characters and dictionary items modified by the negation check can produce reasonable results: about 78% F-measure for sentiment detection (see 4.1.1) and almost 70% F-measure for sentiment polarity identification (see 4.2.1) in the context of domain-independent opinionated information retrieval." ></td>
	<td class="line x" title="121:134	However, since the test corpus is very small the results obtained need further validation on bigger corpora." ></td>
	<td class="line x" title="122:134	The use of the dictionary as a training corpus helped to avoid domain-dependency, however, using a dictionary as a training corpus makes it impossible to obtain grammar information by means of analysis of punctuation marks and grammar word frequencies." ></td>
	<td class="line x" title="123:134	More intensive use of context information could improve the accuracy." ></td>
	<td class="line x" title="124:134	The dictionary-based processing may benefit from the use of word relations information: some words have sentiment information only when used with others." ></td>
	<td class="line x" title="125:134	For example, a noun dongxi (a thing) does not seem to have any sentiment information on its own, although it is tagged as negative in the dictionary." ></td>
	<td class="line x" title="126:134	Some manual filtering of the dictionary may improve the output." ></td>
	<td class="line x" title="127:134	It might also be promising to test the influence on performance of the different classes of words in the dictionary, for example, to use only adjectives or adjectives and nouns together (excluding adverbials)." ></td>
	<td class="line x" title="128:134	Another technique to be tested is computing the 41 positive and negative scores for the characters used only in one class, but absent in another." ></td>
	<td class="line x" title="129:134	In the current system, characters are assigned only one score (for the class they are present in)." ></td>
	<td class="line x" title="130:134	It might improve accuracy if such characters have an appropriate negative score for the other class." ></td>
	<td class="line x" title="131:134	Finally, the average sentiment score may be used for sentiment scaling." ></td>
	<td class="line x" title="132:134	For example, if in our experiments items with a score less than 1.6 were considered not to be opinionated, then ones with score more than 1.6 can be put on a scale where higher scores are interpreted as evidence for higher sentiment intensity (the highest score was 52)." ></td>
	<td class="line x" title="133:134	The scaling approach could help to avoid the problem of assigning documents to more than one sentiment category as the approach uses a continuous scale rather than a predefined number of rigid classes." ></td>
	<td class="line x" title="134:134	The scale (or the scores directly) may be used as a means of indexing for a search engine comprising OIR functionality." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-2013
SemEval-2007 Task 14: Affective Text
Strapparava, Carlo;Mihalcea, Rada;"></td>
	<td class="line x" title="1:121	Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 70??4, Prague, June 2007." ></td>
	<td class="line x" title="2:121	c2007 Association for Computational Linguistics SemEval-2007 Task 14: Affective Text Carlo Strapparava FBK ??irst Istituto per la Ricerca Scientifica e Tecnologica I-38050, Povo, Trento, Italy strappa@itc.it Rada Mihalcea Department of Computer Science University of North Texas Denton, TX, 76203, USA rada@cs.unt.edu Abstract The ?Affective Text??task focuses on the classification of emotions and valence (positive/negative polarity) in news headlines, and is meant as an exploration of the connection between emotions and lexical semantics." ></td>
	<td class="line x" title="3:121	In this paper, we describe the data set used in the evaluation and the results obtained by the participating systems." ></td>
	<td class="line x" title="4:121	1 Introduction All words can potentially convey affective meaning." ></td>
	<td class="line x" title="5:121	Every word, even those apparently neutral, can evoke pleasant or painful experiences due to their semantic relation with emotional concepts or categories." ></td>
	<td class="line x" title="6:121	Some words have emotional meaning with respect to an individual story, while for many others the affective power is part of the collective imagination (e.g. , words such as ?mum??" ></td>
	<td class="line x" title="7:121	?ghost??" ></td>
	<td class="line x" title="8:121	?war??." ></td>
	<td class="line x" title="9:121	The automatic detection of emotion in texts is becoming increasingly important from an applicative point of view." ></td>
	<td class="line x" title="10:121	Consider for example the tasks of opinion mining and market analysis, affective computing, or natural language interfaces such as e-learning environments or educational/edutainment games." ></td>
	<td class="line x" title="11:121	Possible beneficial effects of emotions on memory and attention of the users, and in general on fostering their creativity are also well-known in the field of psychology." ></td>
	<td class="line x" title="12:121	For instance, the following represent examples of applicative scenarios in which affective analysis would give valuable and interesting contributions: Sentiment Analysis." ></td>
	<td class="line x" title="13:121	Text categorization according to affective relevance, opinion exploration for market analysis, etc. are just some examples of application of these techniques." ></td>
	<td class="line x" title="14:121	While positive/negative valence annotation is an active field of sentiment analysis, we believe that a fine-grained emotion annotation would increase the effectiveness of these applications." ></td>
	<td class="line x" title="15:121	Computer Assisted Creativity." ></td>
	<td class="line x" title="16:121	The automated generation of evaluative expressions with a bias on some polarity orientation are a key component for automatic personalized advertisement and persuasive communication." ></td>
	<td class="line x" title="17:121	Verbal Expressivity in Human Computer Interaction." ></td>
	<td class="line x" title="18:121	Future human-computer interaction, according to a widespread view, will emphasize naturalness and effectiveness and hence the incorporation of models of possibly many human cognitive capabilities, including affective analysis and generation." ></td>
	<td class="line x" title="19:121	For example, emotion expression by synthetic characters (e.g. , embodied conversational agents) is considered now a key element for their believability." ></td>
	<td class="line x" title="20:121	Affective words selection and understanding is crucial for realizing appropriate and expressive conversations." ></td>
	<td class="line x" title="21:121	The ?Affective Text??task was intended as an exploration of the connection between lexical semantics and emotions, and an evaluation of various automatic approaches to emotion recognition." ></td>
	<td class="line x" title="22:121	The task is not easy." ></td>
	<td class="line x" title="23:121	Indeed, as (Ortony et al. , 1987) indicates, besides words directly referring to emotional states (e.g. , ?fear??" ></td>
	<td class="line x" title="24:121	?cheerful??" ></td>
	<td class="line x" title="25:121	and for which an appropriate lexicon would help, there are words that act only as an indirect reference to 70 emotions depending on the context (e.g. ?monster??" ></td>
	<td class="line x" title="26:121	?ghost??." ></td>
	<td class="line x" title="27:121	We can call the former direct affective words and the latter indirect affective words (Strapparava et al. , 2006)." ></td>
	<td class="line x" title="28:121	2 Task Definition We proposed to focus on the emotion classification of news headlines extracted from news web sites." ></td>
	<td class="line x" title="29:121	Headlines typically consist of a few words and are often written by creative people with the intention to ?provoke??emotions, and consequently to attract the readers??attention." ></td>
	<td class="line x" title="30:121	These characteristics make this type of text particularly suitable for use in an automatic emotion recognition setting, as the affective/emotional features (if present) are guaranteed to appear in these short sentences." ></td>
	<td class="line x" title="31:121	The structure of the task was as follows: Corpus: News titles, extracted from news web sites (such as Google news, CNN) and/or newspapers." ></td>
	<td class="line x" title="32:121	In the case of web sites, we can easily collect a few thousand titles in a short amount of time." ></td>
	<td class="line x" title="33:121	Objective: Provided a set of predefined six emotion labels (i.e. , Anger, Disgust, Fear, Joy, Sadness, Surprise), classify the titles with the appropriate emotion label and/or with a valence indication (positive/negative)." ></td>
	<td class="line x" title="34:121	The emotion labeling and valence classification were seen as independent tasks, and thus a team was able to participate in one or both tasks." ></td>
	<td class="line x" title="35:121	The task was carried out in an unsupervised setting, and consequently no training was provided." ></td>
	<td class="line x" title="36:121	The reason behind this decision is that we wanted to emphasize the study of emotion lexical semantics, and avoid biasing the participants toward simple ?text categorization??approaches." ></td>
	<td class="line x" title="37:121	Nonetheless supervised systems were not precluded from participation, and in such cases the teams were allowed to create their own supervised training sets." ></td>
	<td class="line x" title="38:121	Participants were free to use any resources they wanted." ></td>
	<td class="line x" title="39:121	We provided a set words extracted from WordNet Affect (Strapparava and Valitutti, 2004), relevant to the six emotions of interest." ></td>
	<td class="line x" title="40:121	However, the use of this list was entirely optional." ></td>
	<td class="line x" title="41:121	2.1 Data Set The data set consisted of news headlines drawn from major newspapers such as New York Times, CNN, and BBC News, as well as from the Google News search engine." ></td>
	<td class="line x" title="42:121	We decided to focus our attention on headlines for two main reasons." ></td>
	<td class="line x" title="43:121	First, news have typically a high load of emotional content, as they describe major national or worldwide events, and are written in a style meant to attract the attention of the readers." ></td>
	<td class="line x" title="44:121	Second, the structure of headlines was appropriate for our goal of conducting sentence-level annotations of emotions." ></td>
	<td class="line x" title="45:121	Two data sets were made available: a development data set consisting of 250 annotated headlines, and a test data set with 1,000 annotated headlines." ></td>
	<td class="line x" title="46:121	2.2 Data Annotation To perform the annotations, we developed a Webbased annotation interface that displayed one headline at a time, together with six slide bars for emotions and one slide bar for valence." ></td>
	<td class="line x" title="47:121	The interval for the emotion annotations was set to [0, 100], where 0 means the emotion is missing from the given headline, and 100 represents maximum emotional load." ></td>
	<td class="line x" title="48:121	The interval for the valence annotations was set to [??00, 100], where 0 represents a neutral headline, ??00 represents a highly negative headline, and 100 corresponds to a highly positive headline." ></td>
	<td class="line nc" title="49:121	Unlike previous annotations of sentiment or subjectivity (Wiebe et al. , 2005; Pang and Lee, 2004), which typically relied on binary 0/1 annotations, we decided to use a finer-grained scale, hence allowing the annotators to select different degrees of emotional load." ></td>
	<td class="line x" title="50:121	The test data set was independently labeled by six annotators." ></td>
	<td class="line x" title="51:121	The annotators were instructed to select the appropriate emotions for each headline based on the presence of words or phrases with emotional content, as well as the overall feeling invoked by the headline." ></td>
	<td class="line x" title="52:121	Annotation examples were also provided, including examples of headlines bearing two or more emotions to illustrate the case where several emotions were jointly applicable." ></td>
	<td class="line x" title="53:121	Finally, the annotators were encouraged to follow their ?first intuition,??and to use the full-range of the annotation scale bars." ></td>
	<td class="line x" title="54:121	71 2.3 Inter-Annotator Agreement We conducted inter-tagger agreement studies for each of the six emotions and for the valence annotations." ></td>
	<td class="line x" title="55:121	The agreement evaluations were carried out using the Pearson correlation measure, and are shown in Table 1." ></td>
	<td class="line x" title="56:121	To measure the agreement among the six annotators, we first measured the agreement between each annotator and the average of the remaining five annotators, followed by an average over the six resulting agreement figures." ></td>
	<td class="line x" title="57:121	EMOTIONS Anger 49.55 Disgust 44.51 Fear 63.81 Joy 59.91 Sadness 68.19 Surprise 36.07 VALENCE Valence 78.01 Table 1: Pearson correlation for inter-annotator agreement 2.4 Fine-grained and Coarse-grained Evaluations Fine-grained evaluations were conducted using the Pearson measure of correlation between the system scores and the gold standard scores, averaged over all the headlines in the data set." ></td>
	<td class="line x" title="58:121	We have also run a coarse-grained evaluation, where each emotion was mapped to a 0/1 classification (0 = [0,50), 1 = [50,100]), and each valence was mapped to a -1/0/1 classification (-1 = [-100,-50], 0 = (-50,50), 1 = [50,100])." ></td>
	<td class="line x" title="59:121	For the coarse-grained evaluations, we calculated accuracy, precision, and recall." ></td>
	<td class="line x" title="60:121	Note that the accuracy is calculated with respect to all the possible classes, and thus it can be artificially high in the case of unbalanced datasets (as some of the emotions are, due to the high number of neutral headlines)." ></td>
	<td class="line x" title="61:121	Instead, the precision and recall figures exclude the neutral annotations." ></td>
	<td class="line x" title="62:121	3 Participating Systems Five teams have participated in the task, with five systems for valence classification and three systems for emotion labeling." ></td>
	<td class="line x" title="63:121	The following represents a short description of the systems." ></td>
	<td class="line x" title="64:121	UPAR7: This is a rule-based system using a linguistic approach." ></td>
	<td class="line x" title="65:121	A first pass through the data ?uncapitalizes??common words in the news title." ></td>
	<td class="line x" title="66:121	The system then used the Stanford syntactic parser on the modified title, and tried to identify what is being said about the main subject by exploiting the dependency graph obtained from the parser." ></td>
	<td class="line x" title="67:121	Each word was first rated separately for each emotion (the six emotions plus Compassion) and for valence." ></td>
	<td class="line x" title="68:121	Next, the main subject rating was boosted." ></td>
	<td class="line x" title="69:121	Contrasts and accentuations between ?good??or ?bad??were detected, making it possible to identify surprising good or bad news." ></td>
	<td class="line x" title="70:121	The system also takes into account: human will (as opposed to illness or natural disasters); negation and modals; high-tech context; celebrities." ></td>
	<td class="line x" title="71:121	The lexical resource used was a combination of SentiWordNet (Esuli and Sebastiani, 2006) and WordNetAffect (Strapparava and Valitutti, 2004), which were semi-automatically enriched on the basis of the original trial data." ></td>
	<td class="line x" title="72:121	SICS: The SICS team used a very simple approach for valence annotation based on a word-space model and a set of seed words." ></td>
	<td class="line x" title="73:121	The idea was to create two points in a high-dimensional word space one representing positive valence, the other representing negative valence and then projecting each headline into this space, choosing the valence whose point was closer to the headline." ></td>
	<td class="line x" title="74:121	The word space was produced from a lemmatized and stop list filtered version of the LA times corpus (consisting of documents from 1994, released for experimentation in the Cross Language Evaluation Forum (CLEF)) using documents as contexts and standard TFIDF weighting of frequencies." ></td>
	<td class="line x" title="75:121	No dimensionality reduction was used, resulting in a 220,220-dimensional word space containing predominantly syntagmatic relations between words." ></td>
	<td class="line x" title="76:121	Valence vectors were created in this space by summing the context vectors of a set of manually selected seed words (8 positive and 8 negative words)." ></td>
	<td class="line x" title="77:121	For each headline in the test data, stop words and words with frequency above 10,000 in the LA times corpus were removed." ></td>
	<td class="line x" title="78:121	The context vectors of the remaining words were then summed, and the cosine of the angles between the summed vector and each of the valence vectors were computed, and the headline was ascribed the valence value (computed as 72 [cosine * 100 + 50]) of the closest valence vector (headlines that were closer to the negative valence vector were assigned a negative valence value)." ></td>
	<td class="line x" title="79:121	In 11 cases, a value of -0.0 was ascribed either because no words were left in the headline after frequency and stop word filtering, or because none of the remaining words occurred in the LA times corpus and thus did not have any context vector." ></td>
	<td class="line x" title="80:121	CLaC: This team submitted two systems to the competition: an unsupervised knowledge-based system (ClaC) and a supervised corpus-based system (CLaC-NB)." ></td>
	<td class="line x" title="81:121	Both systems were used for assigning positive/negative and neutral valence to headlines on the scale [-100,100]." ></td>
	<td class="line x" title="82:121	CLaC: The CLaC system relies on a knowledgebased domain-independent unsupervised approach to headline valence detection and scoring." ></td>
	<td class="line x" title="83:121	The system uses three main kinds of knowledge: a list of sentiment-bearing words, a list of valence shifters and a set of rules that define the scope and the result of the combination of sentiment-bearing words and valence shifters." ></td>
	<td class="line x" title="84:121	The unigrams used for sentence/headline classification were learned from WordNet dictionary entries." ></td>
	<td class="line x" title="85:121	In order to take advantage of the special properties of WordNet glosses and relations, we developed a system that used the list of human-annotated adjectives from (Hatzivassiloglou and McKeown, 1997) as a seed list and learned additional unigrams from WordNet synsets and glosses." ></td>
	<td class="line x" title="86:121	The list was then expanded by adding to it all the words annotated with Positive or Negative tags in the General Inquirer." ></td>
	<td class="line x" title="87:121	Each unigram in the resulting list had the degree of membership in the category of positive or negative sentiment assigned to it using the fuzzy Net Overlap Score method described in the team?s earlier work (Andreevskaia and Bergler, 2006)." ></td>
	<td class="line x" title="88:121	Only words with fuzzy membership score not equal to zero were retained in the list." ></td>
	<td class="line x" title="89:121	The resulting list contained 10,809 sentimentbearing words of different parts of speech." ></td>
	<td class="line x" title="90:121	The fuzzy Net Overlap Score counts were complemented with the capability to discern and take into account some relevant elements of syntactic structure of the sentences." ></td>
	<td class="line x" title="91:121	Two components were added to the system to enable this capability: (1) valence shifter handling rules and (2) parse tree analysis." ></td>
	<td class="line x" title="92:121	The list of valence shifters was a combination of a list of common English negations and a subset of the list of automatically obtained words with increase/decrease semantics, complemented with manual annotation." ></td>
	<td class="line x" title="93:121	The full list consists of 450 words and expressions." ></td>
	<td class="line x" title="94:121	Each entry in the list of valence shifters has an action and scope associated with it, which are used by special handling rules that enable the system to identify such words and phrases in the text and take them into account in sentence sentiment determination." ></td>
	<td class="line x" title="95:121	In order to correctly determine the scope of valence shifters in a sentence, the system used a parse tree analysis using MiniPar." ></td>
	<td class="line x" title="96:121	As a result of this processing, every headline received a system score assigned based on the combined fuzzy Net Overlap Score of its constituents." ></td>
	<td class="line x" title="97:121	This score was then mapped into the [-100 to 100] scale as required by the task." ></td>
	<td class="line x" title="98:121	CLaC-NB: In order to assess the performance of basic Machine Learning techniques on headlines, a second system ClaC-NB was also implemented." ></td>
	<td class="line x" title="99:121	This system used a Nave Bayes classifier in order to assign valence to headlines." ></td>
	<td class="line x" title="100:121	It was trained on a small corpus composed of the development corpus of 250 headlines provided for this competition, plus an additional 200 headlines manually annotated and 400 positive and negative news sentences." ></td>
	<td class="line x" title="101:121	The probabilities assigned by the classifier were mapped to the [100, 100] scale as follows: all negative headlines received the score of -100, all positive headlines were assigned the score of +100, and the neutral headlines obtained the score of 0." ></td>
	<td class="line x" title="102:121	UA: In order to determine the kind and the amount of emotions in a headline, statistics were gathered from three different web Search Engines: MyWay, AlltheWeb and Yahoo." ></td>
	<td class="line x" title="103:121	This information was used to observe the distribution of the nouns, the verbs, the adverbs and the adjectives extracted from the headline and the different emotions." ></td>
	<td class="line x" title="104:121	The emotion scores were obtained through Pointwise Mutual Information (PMI)." ></td>
	<td class="line x" title="105:121	First, the number of documents obtained from the three web search engines using a query that contains all the headline words and an emotion (the words occur in an independent proximity across the web documents) was divided by the number of documents containing only an emotion and the number of documents containing all the headline words." ></td>
	<td class="line x" title="106:121	Second, an associative score between each content word and an emotion was es73 timated and used to weight the final PMI score." ></td>
	<td class="line x" title="107:121	The obtained results were normalized in the 0-100 range." ></td>
	<td class="line x" title="108:121	SWAT: SWAT is a supervised system using an unigram model trained to annotate emotional content." ></td>
	<td class="line x" title="109:121	Synonym expansion on the emotion label words was also performed, using the Roget Thesaurus." ></td>
	<td class="line x" title="110:121	In addition to the development data provided by the task organizers, the SWAT team annotated an additional set of 1000 headlines, which was used for training." ></td>
	<td class="line x" title="111:121	Fine Coarse r Acc." ></td>
	<td class="line x" title="112:121	Prec." ></td>
	<td class="line x" title="113:121	Rec." ></td>
	<td class="line x" title="114:121	F1 CLaC 47.70 55.10 61.42 9.20 16.00 UPAR7 36.96 55.00 57.54 8.78 15.24 SWAT 35.25 53.20 45.71 3.42 6.36 CLaC-NB 25.41 31.20 31.18 66.38 42.43 SICS 20.68 29.00 28.41 60.17 38.60 Table 2: System results for valence annotations Fine Coarse r Acc." ></td>
	<td class="line x" title="115:121	Prec." ></td>
	<td class="line x" title="116:121	Rec." ></td>
	<td class="line x" title="117:121	F1 Anger SWAT 24.51 92.10 12.00 5.00 7.06 UA 23.20 86.40 12.74 21.6 16.03 UPAR7 32.33 93.60 16.67 1.66 3.02 Disgust SWAT 18.55 97.20 0.00 0.00 UA 16.21 97.30 0.00 0.00 UPAR7 12.85 95.30 0.00 0.00 Fear SWAT 32.52 84.80 25.00 14.40 18.27 UA 23.15 75.30 16.23 26.27 20.06 UPAR7 44.92 87.90 33.33 2.54 4.72 Joy SWAT 26.11 80.60 35.41 9.44 14.91 UA 2.35 81.80 40.00 2.22 4.21 UPAR7 22.49 82.20 54.54 6.66 11.87 Sadness SWAT 38.98 87.70 32.50 11.92 17.44 UA 12.28 88.90 25.00 0.91 1.76 UPAR7 40.98 89.00 48.97 22.02 30.38 Surprise SWAT 11.82 89.10 11.86 10.93 11.78 UA 7.75 84.60 13.70 16.56 15.00 UPAR7 16.71 88.60 12.12 1.25 2.27 Table 3: System results for emotion annotations 4 Results Tables 2 and 3 show the results obtained by the participating systems." ></td>
	<td class="line x" title="118:121	The tables show both the finegrained Pearson correlation measure and the coarsegrained accuracy, precision and recall figures." ></td>
	<td class="line x" title="119:121	While further analysis is still needed, the results indicate that the task of emotion annotation is difficult." ></td>
	<td class="line x" title="120:121	Although the Pearson correlation for the intertagger agreement is not particularly high, the gap between the results obtained by the systems and the upper bound represented by the annotator agreement suggests that there is room for future improvements." ></td>
	<td class="line x" title="121:121	Acknowledgments Carlo Strapparava was partially supported by the HUMAINE Network of Excellence." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-2022
CLaC and CLaC-NB: Knowledge-based and corpus-based approaches to sentiment tagging
Andreevskaia, Alina;Bergler, Sabine;"></td>
	<td class="line x" title="1:73	Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 117??20, Prague, June 2007." ></td>
	<td class="line x" title="2:73	c2007 Association for Computational Linguistics CLaC and CLaC-NB: Knowledge-based and corpus-based approaches to sentiment tagging Alina Andreevskaia Concordia University 1455 de Maisonneuve Blvd. Montreal, Canada andreev@cs.concordia.ca Sabine Bergler Concordia University 1455 de Maisonneuve Blvd. Montreal, Canada bergler@cs.concordia.ca Abstract For the Affective Text task at Semeval1/Senseval-4, the CLaC team compared a knowledge-based, domain-independent approach and a standard, statistical machine learning approach to ternary sentiment annotation of news headlines." ></td>
	<td class="line x" title="3:73	In this paper we describe the two systems submitted to the competition and evaluate their results." ></td>
	<td class="line x" title="4:73	We show that the knowledge-based unsupervised method achieves high accuracy and precision but low recall, while supervised statistical approach trained on small amount of in-domain data provides relatively high recall at the cost of low precision." ></td>
	<td class="line x" title="5:73	1 Introduction Sentiment tagging of short text spans ??sentences, headlines, or clauses ??poses considerable challenges for automatic systems due to the scarcity of sentiment clues in these units: sometimes, the decision about the text span sentiment has to be based on just a single sentiment clue and the cost of every error is high." ></td>
	<td class="line x" title="6:73	This is particularly true for headlines, which are typically very short." ></td>
	<td class="line x" title="7:73	Therefore, an ideal system for sentiment tagging of headlines has to use a large set of features with dependable sentiment annotations and to be able to reliably deduce the sentiment of the headline from the sentiment of its components." ></td>
	<td class="line x" title="8:73	The valence labeling subtask of the Affective Text task requires ternary ??positive vs. negative vs. neutral ??classification of headlines." ></td>
	<td class="line x" title="9:73	While such categorization at the sentence level remains relatively unexplored1, the two related sentence-level, binary classification tasks ??positive vs. negative and subjective vs. objective ??have attracted considerable attention in the recent years (Hu and Liu, 2004; Kim and Hovy, 2005; Riloff et al. , 2006; Turney and Littman, 2003; Yu and Hatzivassiloglou, 2003)." ></td>
	<td class="line x" title="10:73	Unsupervised knowledge-based methods are the preferred approach to classification of sentences into positive and negative, mostly due to the lack of adequate amounts of labeled training data (Gamon and Aue, 2005)." ></td>
	<td class="line x" title="11:73	These approaches rely on presence and scores of sentiment-bearing words that have been acquired from dictionaries (Kim and Hovy, 2005) or corpora (Yu and Hatzivassiloglou, 2003)." ></td>
	<td class="line x" title="12:73	Their accuracy on news sentences is between 65 and 68%." ></td>
	<td class="line pc" title="13:73	Sentence-level subjectivity detection, where training data is easier to obtain than for positive vs. negative classification, has been successfully performed using supervised statistical methods alone (Pang and Lee, 2004) or in combination with a knowledgebased approach (Riloff et al. , 2006)." ></td>
	<td class="line n" title="14:73	Since the extant literature does not provide clear evidence for the choice between supervised machine learning methods and unsupervised knowledgebased approaches for the task of ternary sentiment classification of sentences or headlines, we developed two systems for the Affective Text task at SemEval-2007." ></td>
	<td class="line x" title="15:73	The first system (CLaC) relies on the knowledge-rich approach that takes into consid1To our knowledge, the only work that attempted such classification at the sentence level is (Gamon and Aue, 2005) that classified product reviews." ></td>
	<td class="line x" title="16:73	117 eration multiple clues, such as a list of sentimentbearing unigrams and valence shifters, and makes use of sentence structure in order to combine these clues into an overall sentiment of the headline." ></td>
	<td class="line x" title="17:73	The second system (CLaC-NB) explores the potential of a statistical method trained on a small amount of manually labeled news headlines and sentences." ></td>
	<td class="line x" title="18:73	2 CLaC System: Syntax-Aware Dictionary-Based Approach The CLaC system relies on a knowledge-based, domain-independent, unsupervised approach to headline sentiment detection and scoring." ></td>
	<td class="line x" title="19:73	The system uses three main knowledge inputs: a list of sentiment-bearing unigrams, a list of valence shifters (Polanyi and Zaenen, 2006), and a set of rules that define the scope and results of combination of sentiment-bearing words with valence shifters." ></td>
	<td class="line x" title="20:73	2.1 List of sentiment-bearing words The unigrams used for sentence/headline classification were learned from WordNet (Fellbaum, 1998) dictionary entries using the STEP system described in (Andreevskaia and Bergler, 2006b)." ></td>
	<td class="line x" title="21:73	In order to take advantage of the special properties of WordNet glosses and relations, we developed a system that used the human-annotated adjectives from (Hatzivassiloglou and McKeown, 1997) as a seed list and learned additional unigrams from WordNet synsets and glosses." ></td>
	<td class="line x" title="22:73	The STEP algorithm starts with a small set of manually annotated seed words that is expanded using synonymy and antonymy relations in WordNet." ></td>
	<td class="line x" title="23:73	Then the system searches all WordNet glosses and selects the synsets that contain sentiment-bearing words from the expanded seed list in their glosses." ></td>
	<td class="line x" title="24:73	In order to eliminate errors produced by part-of-speech ambiguity of some of the seed words, the glosses are processed by Brill?s part-of-speech tagger (Brill, 1995) and only the seed words with matching part-of-speech tags are considered." ></td>
	<td class="line x" title="25:73	Headwords with sentiment-bearing seed words in their definitions are then added to the positive or negative categories depending on the seed-word sentiment." ></td>
	<td class="line x" title="26:73	Finally, words that were assigned contradicting ??positive and negative ??sentiment within the same run were eliminated." ></td>
	<td class="line x" title="27:73	The average accuracy of 60 runs with non-intersecting seed lists when compared to General Inquirer (Stone et al. , 1966) was 74%." ></td>
	<td class="line x" title="28:73	In order to improve the list coverage, the words annotated as ?Positiv??or ?Negativ??in the General Inquirer that were not picked up by STEP were added to the final list." ></td>
	<td class="line x" title="29:73	Since sentiment-bearing words in English have different degree of centrality to the category of sentiment, we have constructed a measure of word centrality to the category of positive or negative sentiment described in our earlier work (Andreevskaia and Bergler, 2006a)." ></td>
	<td class="line x" title="30:73	The measure, termed Net Overlap Score (NOS), is based on the number of ties that connect a given word to other words in the category." ></td>
	<td class="line x" title="31:73	The number of such ties is reflected in the number of times each word was retrieved from WordNet by multiple independent STEP runs with nonintersecting seed lists." ></td>
	<td class="line x" title="32:73	This approach allowed us to assign NOSs to each unigram captured by multiple STEP runs." ></td>
	<td class="line x" title="33:73	Only words with fuzzy membership score not equal to zero were retained in the list." ></td>
	<td class="line x" title="34:73	The resulting list contained 10,809 sentimentbearing words of different parts of speech." ></td>
	<td class="line x" title="35:73	2.2 Valence Shifters The brevity of the headlines compared to typical news sentences2 requires that the system is able to make a correct decision based on very few sentiment clues." ></td>
	<td class="line x" title="36:73	Due to the scarcity of sentiment clues, the additional factors, such as presence of valence shifters, have a greater impact on the system performance on headlines than on sentences or texts, where impact of a single error can often be compensated by a number of other, correctly identified sentiment clues." ></td>
	<td class="line x" title="37:73	For this reason, we complemented the system based on fuzzy score counts with the capability to discern and take into account some relevant elements of syntactic structure of sentences." ></td>
	<td class="line x" title="38:73	We added to the system two components in order to enable this capability: (1) valence shifter handling rules and (2) parse tree analysis." ></td>
	<td class="line x" title="39:73	Valence shifters can be defined as words that modify the sentiment expressed by a sentiment-bearing word (Polanyi and Zaenen, 2006)." ></td>
	<td class="line x" title="40:73	The list of valence shifters used in our experiments was a com2An average length of a sentence in a news corpus is over 20 words, while the average length of headlines in the test corpus was only 7 words." ></td>
	<td class="line x" title="41:73	118 bination of (1) a list of common English negations, (2) a subset of the list of automatically obtained words with increase/decrease semantics, and (3) words picked up in manual annotation conducted for other research projects by two trained linguists." ></td>
	<td class="line x" title="42:73	The full list consists of 490 words and expressions." ></td>
	<td class="line x" title="43:73	Each entry in the list of valence shifters has an action and scope associated with it." ></td>
	<td class="line x" title="44:73	The action and scope tags are used by special handling rules that enable our system to identify such words and phrases in the text and take them into account in sentence sentiment determination." ></td>
	<td class="line x" title="45:73	In order to correctly determine the scope of valence shifters in a sentence, we introduced into the system the analysis of the parse trees produced by MiniPar (Lin, 1998)." ></td>
	<td class="line x" title="46:73	As a result of this processing, every headline received a score according to the combined fuzzy NOS of its constituents." ></td>
	<td class="line x" title="47:73	We then mapped this score, which ranged between -1.2 and 0.99, into the [-100, 100] scale as required by the competition organizers." ></td>
	<td class="line pc" title="48:73	3 CLaC-NB System: Nave Bayes Supervised statistical methods have been very successful in sentiment tagging of texts and in subjectivity detection at sentence level: on movie review texts they reach an accuracy of 85-90% (Aue and Gamon, 2005; Pang and Lee, 2004) and up to 92% accuracy on classifying movie review snippets into subjective and objective using both Nave Bayes and SVM (Pang and Lee, 2004)." ></td>
	<td class="line x" title="49:73	These methods perform particularly well when a large volume of labeled data from the same domain as the test set is available for training (Aue and Gamon, 2005)." ></td>
	<td class="line x" title="50:73	The lack of sufficient data for training appears to be the main reason for the virtual absence of experiments with statistical classifiers in sentiment tagging at the sentence level." ></td>
	<td class="line x" title="51:73	In order to explore the potential of statistical approaches on sentiment classification of headlines, we implemented a basic Nave Bayes classifier with smoothing using Lidstone?s law of succession (with =0.1)." ></td>
	<td class="line x" title="52:73	No feature selection was performed." ></td>
	<td class="line x" title="53:73	The development set for the Affective Text task consisted of only 250 headlines, which is not sufficient for training of a statistical classifier." ></td>
	<td class="line x" title="54:73	In order to increase the size of the training corpus, we augmented it with a balanced set of 900 manually annotated news sentences on a variety of topics extracted from the Canadian NewsStand database3 and 200 headlines from different domains collected from Google News in January 20074." ></td>
	<td class="line x" title="55:73	The probabilities assigned by the classifier were mapped to [-100, 100] as follows: all negative headlines received a score of -100, all positive headlines +100, and neutral headlines 0." ></td>
	<td class="line x" title="56:73	4 Results and Discussion Table 1 shows the results of the two CLaC systems for valence labeling subtask of Affective Text task compared to all participating systems average." ></td>
	<td class="line x" title="57:73	The best subtask scores are highlighted in bold." ></td>
	<td class="line x" title="58:73	System Pearson Acc." ></td>
	<td class="line x" title="59:73	Prec." ></td>
	<td class="line x" title="60:73	Rec." ></td>
	<td class="line x" title="61:73	F1 correl." ></td>
	<td class="line x" title="62:73	CLaC 47.7 55.1 61.4 9.2 16 CLaC-NB 25.4 31.2 31.2 66.4 42 Task average 33.2 44.7 44.85 29.6 23.7 Table 1: System results The comparison between the two CLaC systems clearly demonstrates the relative advantages of the two approaches." ></td>
	<td class="line x" title="63:73	The knowledge-based unsupervised system performed well above average on three main measures: the Pearson correlation between fine-grained sentiment assigned by CLaC system and the human annotation; the accuracy for ternary classification; and the precision of binary (positive vs. negative) classification." ></td>
	<td class="line x" title="64:73	These results demonstrate that an accurately annotated list of sentimentbearing words combined with sophisticated valence shifter handling produces acceptably accurate sentiment labels even for such difficult data as news headlines." ></td>
	<td class="line x" title="65:73	This system, however, was not able to provide good recall." ></td>
	<td class="line x" title="66:73	On the contrary, supervised machine learning has very good recall, but low accuracy relative to the results of the unsupervised knowledge-based approach." ></td>
	<td class="line x" title="67:73	This shortcoming could be in part reduced if more uniformly labeled headlines were available 3http://www.il.proquest.com/productspq/ descriptions/Canadian newsstand.shtml 4The interannotator agreement for this data, as measured by Kappa, was 0.74." ></td>
	<td class="line x" title="68:73	119 for training." ></td>
	<td class="line x" title="69:73	However, we can hardly expect large amounts of such manually annotated data to be handy in real-life situations." ></td>
	<td class="line x" title="70:73	5 Conclusions The two CLaC systems that we submitted to the Affective Text task have tested the applicability of two main sentiment tagging approaches to news headlines annotation." ></td>
	<td class="line x" title="71:73	The results of the two systems indicate that the knowledge-based unsupervised approach that relies on an automatically acquired list of sentiment-bearing unigrams and takes into account the combinatorial properties of valence shifters, can produce high quality sentiment annotations, but may miss many sentiment-laden headlines." ></td>
	<td class="line x" title="72:73	On the other hand, supervised machine learning has good recall even with a relatively small training set, but its precision and accuracy are low." ></td>
	<td class="line x" title="73:73	In our future work we will explore the potential of combining the two approaches in a single system in order to improve both recall and precision of sentiment annotation." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1101
Discourse Level Opinion Interpretation
Somasundaran, Swapna;Wiebe, Janyce M.;Ruppenhofer, Josef;"></td>
	<td class="line x" title="1:239	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 801808 Manchester, August 2008 Discourse Level Opinion Interpretation  Swapna Somasundaran Dept. of Computer Science University of Pittsburgh Pittsburgh, PA 15260 swapna@cs.pitt.edu Janyce Wiebe Dept. of Computer Science University of Pittsburgh Pittsburgh, PA 15260 wiebe@cs.pitt.edu Josef Ruppenhofer Intelligent Systems Program University of Pittsburgh Pittsburgh, PA 15260 josefr@cs.pitt.edu Abstract This work proposes opinion frames as a representation of discourse-level associations which arise from related opinion topics." ></td>
	<td class="line x" title="2:239	We illustrate how opinion frames help gather more information and also assist disambiguation." ></td>
	<td class="line x" title="3:239	Finally we present the results of our experiments to detect these associations." ></td>
	<td class="line x" title="4:239	1 Introduction Opinions have been investigated at the phrase, sentence, and document levels." ></td>
	<td class="line x" title="5:239	However, little work has been carried out regarding interpreting opinions at the level of the discourse." ></td>
	<td class="line x" title="6:239	Consider the following excerpt from a dialog about designing a remote control for a television (the opinion targets  what the opinions are about  are shown in italics)." ></td>
	<td class="line x" title="7:239	(1) D :: And I thought not too edgy and like a box, more kind of hand-held not as computery, yeah, more organic shape I think." ></td>
	<td class="line x" title="8:239	Simple designs, like the last one we just saw, not too many buttons." ></td>
	<td class="line x" title="9:239	Speaker D expresses an opinion in favor of a design that is simple and organic in shape, and against an alternative design which is not." ></td>
	<td class="line x" title="10:239	Several individual opinions are expressed in this passage." ></td>
	<td class="line x" title="11:239	The first is a negative opinion about the design being too edgy and box-like, the next is a positive opinion toward a hand-held design, followed by a negative opinion toward a computery shape, and so on." ></td>
	<td class="line x" title="12:239	While recognizing individual expressions This research was supported in part by the Department of Homeland Security under grant N000140710152." ></td>
	<td class="line x" title="13:239	c2008." ></td>
	<td class="line x" title="14:239	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="15:239	Some rights reserved." ></td>
	<td class="line x" title="16:239	of opinions and their properties is important, discourse interpretation is needed as well." ></td>
	<td class="line x" title="17:239	It is by understanding the passage as a discourse that we see edgy, like a box, computery, and many buttons as descriptions of the type of design D does not prefer, and hand-held, organic shape, and simple designs as descriptions of the type he does." ></td>
	<td class="line x" title="18:239	These descriptions are not in general synonyms/antonyms of one another; for example, there are hand-held computery devices and simple designs that are edgy." ></td>
	<td class="line x" title="19:239	The unison/opposition among the descriptions is due to how they are used in the discourse." ></td>
	<td class="line x" title="20:239	This paper focuses on such relations between the targets of opinions in discourse." ></td>
	<td class="line x" title="21:239	Specifically, in this work, we propose a scheme of opinion frames, which consist of two opinions that are related by virtue of having united or opposed targets." ></td>
	<td class="line x" title="22:239	We argue that recognizing opinion frames will provide more opinion information for NLP applications than recognizing individual opinions alone." ></td>
	<td class="line x" title="23:239	Further, if there is uncertainty about any one of the components, we believe opinion frames are an effective representation incorporating discourse information to make an overall coherent interpretation (Hobbs et al., 1993)." ></td>
	<td class="line x" title="24:239	Finally, we also report the first results of experiments in recognizing the presence of these opinion frames." ></td>
	<td class="line x" title="25:239	We introduce our data in Section 2, present opinion frames in Section 3 and illustrate their utility in Section 4." ></td>
	<td class="line x" title="26:239	Our experiments are in Section 5, related work is discussed in Section 6, and conclusions are in Section 7." ></td>
	<td class="line x" title="27:239	2 Data The data used in this work is the AMI meeting corpus (Carletta et al., 2005) which contains multi-modal recordings of group meetings." ></td>
	<td class="line x" title="28:239	Each meeting has rich transcription and seg801 ment (turn/utterance) information for each speaker." ></td>
	<td class="line x" title="29:239	Each utterance consists of one or more sentences." ></td>
	<td class="line x" title="30:239	We also use some of the accompanying manual annotations (like adjacency pairs) as features in our machine learning experiments." ></td>
	<td class="line x" title="31:239	3 Opinion Frames In this section, we lay out definitions relating to opinion frames, illustrate with examples how these are manifested in our data, and consider them in the context of discourse relations." ></td>
	<td class="line x" title="32:239	3.1 Definitions The components of opinion frames are individual opinions and the relationships between their targets." ></td>
	<td class="line x" title="33:239	Following (Wilson and Wiebe, 2005; Somasundaran et al., 2007), we address two types of opinions, sentiment and arguing." ></td>
	<td class="line x" title="34:239	Sentiment includes positive and negative evaluations, emotions, and judgments." ></td>
	<td class="line x" title="35:239	Arguing includes arguing for or against something, and arguing that something should or should not be done." ></td>
	<td class="line x" title="36:239	Opinions have a polarity that can be positive or negative." ></td>
	<td class="line x" title="37:239	1 The target of an opinion is the entity or proposition that the opinion is about." ></td>
	<td class="line x" title="38:239	We establish relations between targets, in the process relating their respective opinions." ></td>
	<td class="line x" title="39:239	We address two types of relations, same and alternative." ></td>
	<td class="line x" title="40:239	The same relation holds between targets that refer to the same entity, property, or proposition." ></td>
	<td class="line x" title="41:239	Observing the relations marked by annotators, we found that same covers not only identity, but also part-whole, synonymy, generalization, specialization, entity-attribute, instantiation, cause-effect, epithets and implicit background topic, i.e., relations that have been studied by many researchers in the context of anaphora and co-reference (e.g.(Clark, 1975; Vieira and Poesio, 2000; Mueller and Strube, 2001))." ></td>
	<td class="line x" title="43:239	Actually, same relations holding between entities often involve co-reference (where co-reference is broadly conceived to include relations such as part-whole listed above)." ></td>
	<td class="line x" title="44:239	However, there are no morphosyntactic constraints on what targets may be." ></td>
	<td class="line x" title="45:239	Thus, same relations may also hold between adjective phrases, verb phrases, and clauses." ></td>
	<td class="line x" title="46:239	An instance of this is Example 1, where the same target relation holds between the adjectives edgy and computery." ></td>
	<td class="line x" title="47:239	1Polarity can also be neutral or both (Wilson and Wiebe, 2005), but these values are not significant for our opinion frames." ></td>
	<td class="line x" title="48:239	SPSPsame, SNSNsame, APAPsame, ANANsame, SPAPsame, APSPsame, SNANsame, ANSNsame, SPSNalt, SNSPalt, APANalt, ANAPalt, SPANalt, SNAPalt, APSNalt, ANSPalt SPSNsame, SNSPsame, APANsame, ANAPsame, SPANsame, APSNsame, SNAPsame, ANSPsame, SPSPalt, SNSNalt, APAPalt, ANANalt, SPAPalt, SNANalt, APSPalt, ANSNalt Table 1: Opinion Frames The alternative relation holds between targets that are related by virtue of being opposing (mutually exclusive) options in the context of the discourse." ></td>
	<td class="line x" title="49:239	For example, in the domain of TV remote controls, the set of all shapes are alternatives to one another, since a remote control may have only one shape at a time." ></td>
	<td class="line x" title="50:239	In such scenarios, a positive opinion regarding one choice may imply a negative opinion toward competing choices, and vice versa." ></td>
	<td class="line x" title="51:239	Objects appear as alternatives via world and domain knowledge (for example, shapes of a remote); the context of the discourse (for example, Hillary Clinton and Barak Obama are alternatives in discussions of the primaries, but not in discussions of the general election); and the way the objects are juxtaposed while expressing opinions (for instance hand-held and computery in Example 1)." ></td>
	<td class="line x" title="52:239	While same and alternative are not the only possible relations between targets, they commonly occur in task-oriented dialogs such as those in the data we use." ></td>
	<td class="line x" title="53:239	Now that we have all the ingredients, we can define opinion frames." ></td>
	<td class="line x" title="54:239	An opinion frame is defined as a structure composed of two opinions and their respective targets connected via their target relations." ></td>
	<td class="line x" title="55:239	With four opinion type/polarity pairs (SN,SP,AN,AP), for each of two opinion slots, and two possible target relations, we have 4 * 4 * 2 = 32 types of frame, listed in Table 1." ></td>
	<td class="line x" title="56:239	3.2 Examples We will now illustrate how the frames are applied with the following meeting snippets from the AMI meeting corpus." ></td>
	<td class="line x" title="57:239	In our examples, the lexical anchors revealing the opinion type (as the words are interpreted in context) are indicated in bold face." ></td>
	<td class="line x" title="58:239	The text span capturing the target of the opinion (as interpreted in context) is indicated in italics." ></td>
	<td class="line x" title="59:239	To make it easier to understand the opinion frames, we separately list each opinion, followed by the major relation between the targets and, in parentheses, the relevant subtype of the major relation." ></td>
	<td class="line x" title="60:239	In the passage below, the speaker D expresses 802 his preferences about the material for the TV remote." ></td>
	<td class="line x" title="61:239	(2) D::  this kind of rubbery material, its a bit more bouncy, like you said they get chucked around a lot." ></td>
	<td class="line x" title="62:239	A bit more durable and that 2 can also be ergonomic and it kind of feels a bit different from all the other remote controls." ></td>
	<td class="line x" title="63:239	Opinion Span target Span Type O1 bit more bouncy its [t1] SP O2 bit more durable ellipsis [t2] SP O3 ergonomic that [t3] SP O4 a bit different from all the other remote it [t4] SP Target target Rel t1 t2 same (ellipsis) t3 t4 same (identity) t1 t3 same (identity) The speakers positive sentiment regarding the rubbery material is apparent from the text spans bit more bouncy (Sentiment Positive or SP), bit more durable (SP), ergonomic (SP) and a bit different from all the other remote controls (SP)." ></td>
	<td class="line x" title="64:239	As shown, the targets of these opinions (its [t1], that [t3], and it [t4]) are related by the same relation." ></td>
	<td class="line x" title="65:239	The ellipsis occurs with bit more durable." ></td>
	<td class="line x" title="66:239	Target [t2] represents the (implicit) target of that opinion, and [t2] has a same relation to [t1], the target of the bit more bouncy opinion." ></td>
	<td class="line x" title="67:239	The opinion frames occurring throughout this passage are all SPSPsame denoting that both the opinion components are sentiments with positive polarity with a same relation between their targets." ></td>
	<td class="line x" title="68:239	One frame occurs between O1 and O2, another between O3 and O4, and so on." ></td>
	<td class="line x" title="69:239	Example 2 illustrates relatively simple same relations between targets." ></td>
	<td class="line x" title="70:239	Now let us consider the more involved passage below, in which a meeting participant analyzes two leading remotes on the market." ></td>
	<td class="line x" title="71:239	(3) D:: These are two leading remote controls at the moment." ></td>
	<td class="line x" title="72:239	You know theyre grey, this ones got loads of buttons, its hard to tell from here what they actually do, and they dont look very exciting at all." ></td>
	<td class="line x" title="73:239	Opinion Span target Span Rel O1 leading remote controls [t1] SP O2 grey they [t2] SN O3 loads of buttons this one [t3] SN O4 hard to tell they [t4] SN O5 dont look very exciting at all they [t5] SN Target target Rel t1 t2 same (identity) t2 t3 same (t3 subset of t2) 2Note that the that refers to the property of being durable; however, as our annotation scheme is not hierarchical, we connect it to the entity the opinion is about  in this case the rubbery material." ></td>
	<td class="line x" title="74:239	t3 t4 same (t4 partof t3) t5 t1 same (identity) Target [t2] is the set of two leading remotes; [t3], which is in a same relation with [t2], is one of those remotes." ></td>
	<td class="line x" title="75:239	Target [t4], which is also in a same relation with [t3], is a part of that remote, namely its buttons." ></td>
	<td class="line x" title="76:239	Thus, opinion O3 is directly about one of the remotes, and indirectly about the set of both remotes." ></td>
	<td class="line x" title="77:239	Similarly, O4 is directly about the buttons of one of the remotes, and indirectly about that remote itself." ></td>
	<td class="line x" title="78:239	The assessments at different levels accrue toward the analysis of the main topic under consideration." ></td>
	<td class="line x" title="79:239	Moving on to alternative (alt) relations, consider the passage below, where the speaker is arguing for the curved shape." ></td>
	<td class="line x" title="80:239	(4) C:: . . ." ></td>
	<td class="line x" title="81:239	shapes should be curved, so round shapes." ></td>
	<td class="line x" title="82:239	Nothing square-like." ></td>
	<td class="line x" title="83:239	C:: . . ." ></td>
	<td class="line x" title="84:239	So we shouldnt have too square corners and that kind of thing." ></td>
	<td class="line x" title="85:239	B:: Yeah okay." ></td>
	<td class="line x" title="86:239	Not the old box look." ></td>
	<td class="line x" title="87:239	Opinion Span target Span Rel O1 should be curved [t1] AP O2 Nothing square-like [t2] AN O3 shouldnt have square corners [t3] AN O4 too square corners [t3] SN O5 Not the old box look [t4] AN O6 the old box look the old box look [t4] SN Target target Rel t1 -t2 alternatives t2 t3 same (specification) t3 t4 same (epithet) Opinion O1 argues for a curved shape, O2 argues against a square shape, and O3 argues against square corners." ></td>
	<td class="line x" title="88:239	Note that square corners is also the target of a negative sentiment, O4, expressed here by too." ></td>
	<td class="line x" title="89:239	Opinion O5 argues against the old box look." ></td>
	<td class="line x" title="90:239	In addition, the wording old box look implies a negative sentiment  O6 (we list the target span as old box look, which refers to the look of having square corners)." ></td>
	<td class="line x" title="91:239	There is an alt relation between [t1] and [t2]." ></td>
	<td class="line x" title="92:239	Thus, we have an opinion frame of type APANalt between O1 and O2." ></td>
	<td class="line x" title="93:239	From this frame, we are able to understand that a positive opinion is expressed toward something and a negative opinion is expressed toward its alternative." ></td>
	<td class="line x" title="94:239	3.3 Link Transitivity When individual targets are linked, they form a chain-like structure." ></td>
	<td class="line x" title="95:239	Due to this, a connecting path may exist between targets that were not directly 803 linked by the human annotators." ></td>
	<td class="line x" title="96:239	This path can be traversed to create links between new pairs of targets, which in turn results in new opinion frame relations." ></td>
	<td class="line x" title="97:239	Let us illustrate this idea with Example 4." ></td>
	<td class="line x" title="98:239	The frames with direct relations are O1O2 APANalt." ></td>
	<td class="line x" title="99:239	By following the alt link from [t1] to [t2] and the same link from [t2] to [t3], we have an alt link between [t1] and [t3], and the additional frames O1O3 APANalt and O1O4 APSNalt." ></td>
	<td class="line x" title="100:239	Repeating this process would finally link speaker Cs opinion O1 with Bs opinion O6 via a APSNalt frame." ></td>
	<td class="line x" title="101:239	Simple recipes such as this can be used by applications such as QA to gather more information from the discourse." ></td>
	<td class="line x" title="102:239	3.4 Frame Types In our corpus, we found that the 32 frames of Table 1 can be categorized into two functional types: reinforcing frames and non-reinforcing frames." ></td>
	<td class="line x" title="103:239	The set of frames that occur in scenarios where the speaker intends to fortify or reinforce his opinion/stance are called reinforcing frames." ></td>
	<td class="line x" title="104:239	These are the ones in the top row of the Table 1." ></td>
	<td class="line x" title="105:239	Note that these frames cover all opinion types, polarities and target relations." ></td>
	<td class="line x" title="106:239	It is the particular combination of these frame components that bring about the reinforcement of the opinion in the discourse." ></td>
	<td class="line x" title="107:239	On the other hand, the frames at the bottom row of the table are non-reinforcing." ></td>
	<td class="line x" title="108:239	In our corpus, these frames occur when a speaker is ambivalent or weighing pros and cons." ></td>
	<td class="line x" title="109:239	Example 2 is characterized by opinion frames in which the opinions reinforce one another  that is, individual positive sentiments (SP) occurring throughout the passage fortify the positive regard for the rubbery material via the same target relations and the resulting SPSPsame frames." ></td>
	<td class="line x" title="110:239	Interestingly, interplays among different opinion types may show the same type of reinforcement." ></td>
	<td class="line x" title="111:239	For instance, Example 4 is characterized by mixtures of opinion types, polarities, and target relations." ></td>
	<td class="line x" title="112:239	However, the opinions are still unified in the intention to argue for a particular type of shape." ></td>
	<td class="line x" title="113:239	3.5 Discourse Relations and Opinion Frames Opinion-frame recognition and discourse interpretation go hand in hand; together, they provide richer overall interpretations." ></td>
	<td class="line x" title="114:239	For example, consider the opinion frames and the Penn Discourse Treebank relations (Prasad et al., 2007) for Example 2." ></td>
	<td class="line x" title="115:239	PDTB would see a list or conjunction relation between the clauses containing opinions bit more durable (O2) and ergonomic (O3), as well as between the clauses containing opinions ergonomic (O3) and a bit different from all the other remote controls (O4)." ></td>
	<td class="line x" title="116:239	All of our opinion frames for this passage are of type SPSPsame, a reinforcing frame type." ></td>
	<td class="line x" title="117:239	This passage illustrates the case in which discourse relations nicely correspond to opinion frames." ></td>
	<td class="line x" title="118:239	The opinion frames flesh out the discourse relations: we have lists specifically of positive sentiments toward related objects." ></td>
	<td class="line x" title="119:239	However, opinion-frame and discourse-relation schemes are not redundant." ></td>
	<td class="line x" title="120:239	Consider the following three passages." ></td>
	<td class="line x" title="121:239	(e1) Non-reinforcing opinion frame (SNSPsame); Contrast discourse relation D:: . . ." ></td>
	<td class="line x" title="122:239	I draw for you this schema that can be maybe too technical for you but is very important for me . ." ></td>
	<td class="line x" title="123:239	(e2) Reinforcing opinion frame (SNAPalt); Contrast discourse relation D:: not too edgy and like a box, more kind of handheld (e3) Reinforcing opinion frame (SPSPsame); no discourse relation . . ." ></td>
	<td class="line x" title="124:239	they want something thats easier to use straight away, more intuitive perhaps." ></td>
	<td class="line x" title="125:239	In both e1 and e2, the discourse relation between the two opinions is contrast (too technical is contrasted with very important, and not too edgy and like a box is contrasted with more kind of hand-held)." ></td>
	<td class="line x" title="126:239	However, the opinion frame in e1 is SNSPsame, which is a non-reinforcing frame, while the opinion frame in e2 is SNAPalt, which is a reinforcing frame." ></td>
	<td class="line x" title="127:239	In e3, the opinion frame holds between targets within a subordinated clause (easier to use and more intuitive are two desired targets); most discourse theories dont predict any discourse relation in this situation." ></td>
	<td class="line x" title="128:239	Generally speaking, we find that there are not definitive mappings between opinion frames and the relations of popular discourse theories." ></td>
	<td class="line x" title="129:239	For example, Hobbs (Hobbs et al., 1993) contrast covers at least four of our frames (SPSPalt, APAPalt, APANsame, SPSNsame), while, for instance, our SPSPsame frame can map to both the elaboration and explanation relations." ></td>
	<td class="line x" title="130:239	4 Benefits of Discourse Opinion Frames This section argues for two motivations for opinion frames: they may unearth additional information over and above the individual opinions stated in the text, and they may contribute toward arriving 804 Positive Negative Counting only individual opinions Accepted Items 120 20 Rejected Items 9 12 individual + opinions via Reinforcing Opinion frames Accepted Items 252 63 Rejected Items 22 26 Table 2: Opinion Polarity Distribution for Accepted/Rejected Items at a coherent interpretation (Hobbs et al., 1993) of the opinions in the discourse." ></td>
	<td class="line x" title="131:239	4.1 Gathering More Information Frame relations provide a mechanism to relate opinions expressed in non-local contexts the opinion may occur elsewhere in the discourse, but will become relevant to a given target due to a relation between its target and the given target." ></td>
	<td class="line x" title="132:239	For instance, in Example 3, there is one direct evaluation of the leading remotes (O1) and two evaluations via identity (O2, O5)." ></td>
	<td class="line x" title="133:239	Following frames constructed via t2-t3 and t3-t4, we get two more opinions (O3 and O4) for the leading remotes." ></td>
	<td class="line x" title="134:239	Furthermore, opinions regarding something not lexically or even anaphorically related can become relevant, providing more opinion information." ></td>
	<td class="line x" title="135:239	This is particularly interesting when alt relations are involved, as opinions towards one alternative imply opinions of opposite polarity toward the competing options." ></td>
	<td class="line x" title="136:239	For instance in Example 4, if we consider only the explicitly stated opinions, there is only one (positive) opinion, O1, about the curved shape." ></td>
	<td class="line x" title="137:239	However, the speaker expresses several other opinions which reinforce his positivity toward the curved shape." ></td>
	<td class="line x" title="138:239	Thus, by using the frame information, it is possible to gather more opinions regarding curved shapes for TV remotes." ></td>
	<td class="line x" title="139:239	As a simple proof of concept, we counted the number of positive and negative opinions towards the items that were accepted or rejected in the meetings (information about accepted and rejected items is obtained from the manual abstractive summaries provided by the AMI corpus)." ></td>
	<td class="line x" title="140:239	Counts are obtained, over opinions manually annotated in the data, for two conditions: with and without frame information." ></td>
	<td class="line x" title="141:239	The items in our meeting data are mainly options for the new TV remote, which include attributes and features like different shapes, materials, designs, and functionalities." ></td>
	<td class="line x" title="142:239	We observed that for the accepted items, the number of positive opinions is higher and, for rejected items, the number of negative opinions is higher." ></td>
	<td class="line x" title="143:239	The top section of Table 2 shows a contingency table of counts of positive/negative opinions for accepted/rejected items for 5 AMI meetings." ></td>
	<td class="line x" title="144:239	Then we counted the number of reinforcing opinions that were expressed regarding these items." ></td>
	<td class="line x" title="145:239	This meant also counting additional opinions that were related via reinforcing frames." ></td>
	<td class="line x" title="146:239	The bottom section of Table 2 shows the counts when the reinforcing frames are considered." ></td>
	<td class="line x" title="147:239	Compared to the counts of only individual opinions, we see that the numbers in each cell have increased, while maintaining the same pattern of distribution." ></td>
	<td class="line x" title="148:239	Thus, in effect we have procured more instances of opinions for the items." ></td>
	<td class="line x" title="149:239	We believe this added information would help applications like meeting summarizers and QA systems to make more informed decisions." ></td>
	<td class="line x" title="150:239	4.2 Interdependent Interpretation We believe that our opinion frames, anaphoric relations and discourse relations can symbiotically help disambiguate each other in the discourse." ></td>
	<td class="line x" title="151:239	In particular, suppose that some aspect of an individual opinion, such as polarity, is unclear." ></td>
	<td class="line x" title="152:239	If the discourse suggests certain opinion frames, this may in turn resolve the underlying ambiguity." ></td>
	<td class="line x" title="153:239	Revisiting Example 2 from above, we see that out of context, the polarities of bouncy and different from other remotes are unclear (bounciness and being different may be negative attributes for another type of object)." ></td>
	<td class="line x" title="154:239	However, the polarities of two of the opinions are clear (durable and ergonomic)." ></td>
	<td class="line x" title="155:239	There is evidence in this passage of discourse continuity and same relations such as the pronouns, the lack of contrastive cue phrases, and so on." ></td>
	<td class="line x" title="156:239	This evidence suggests that the speaker expresses similar opinions throughout the passage, making the opinion frame SPSPsame more likely throughout." ></td>
	<td class="line x" title="157:239	Recognizing the frames would resolve the polarity ambiguities of bouncy and different." ></td>
	<td class="line x" title="158:239	In the following example (5), the positive sentiment (SP) towards the this and the positive arguing (AP) for the it are clear." ></td>
	<td class="line x" title="159:239	These two individual opinions can be related by a same/alt target relation, be unrelated, or have some other relation not covered by our scheme (in which case we would not have a relation between them)." ></td>
	<td class="line x" title="160:239	There is evidence in the discourse that makes one interpretation more likely than others." ></td>
	<td class="line x" title="161:239	The so indicates that the two clauses are highly likely to be related by a cause discourse 805 relation (PDTB)." ></td>
	<td class="line x" title="162:239	This information confirms a discourse continuity, as well as makes a reinforcing scenario likely, which makes the reinforcing frame SPAPsame highly probable." ></td>
	<td class="line x" title="163:239	This increase in likelihood will in turn help a coreference system to increase its confidence that the that and the it co-refer." ></td>
	<td class="line x" title="164:239	(5) B ::  and this will definitely enhance our market sales, so we should take it into consideration also." ></td>
	<td class="line x" title="165:239	Opinion Span target Span Rel O1 definitely enhance our market sales this [t1] SP O2 so we should it [t2] AP Target target Rel t1 -t2 same (identity) 5 Experiments There has been much work on recognizing individual aspects of opinions like extracting individual opinions from phrases or sentences and recognizing opinion type and polarity." ></td>
	<td class="line x" title="166:239	Accordingly, in our machine learning experiments we assume oracle opinion and polarity information." ></td>
	<td class="line x" title="167:239	Our experiments thus focus on the new question: Given two opinion sentences, determine if they participate in any frame relation. Here, an opinion sentence is a sentence containing one or more sentiment or arguing expression." ></td>
	<td class="line x" title="168:239	In this work, we consider frame detection only between sentence pairs belonging to the same speaker." ></td>
	<td class="line x" title="169:239	5.1 Annotation of Gold Standard Creating gold-standard opinion-frame data is accomplished by annotating frame components and then building the frames from those underlying annotations." ></td>
	<td class="line x" title="170:239	We began with annotations created by Somasundaran et al.(2007), namely four meetings of the AMI meeting corpus annotated for sentiment and arguing opinions (text anchor and type)." ></td>
	<td class="line x" title="172:239	Following that annotation scheme, we annotated an additional meeting." ></td>
	<td class="line x" title="173:239	This gave us a corpus of 4436 sentences or 2942 segments (utterances)." ></td>
	<td class="line x" title="174:239	We added attributes to the existing opinion annotations, namely polarity and target-id. The targetid attribute links the opinion to its local target span." ></td>
	<td class="line x" title="175:239	Relations between targets were then annotated." ></td>
	<td class="line x" title="176:239	When a newly annotated target is similar (or opposed) to a set of targets already participating in same relations, then the same (or alt) link is made only to one of them the one that seems most natural." ></td>
	<td class="line x" title="177:239	This is often the one that is physically closest." ></td>
	<td class="line x" title="178:239	Content Word overlap between the sentence pair Focus space overlap between the sentence pair Anaphoric indicator in the second sentence Time difference between the sentence pair Number of intervening sentences Existence of adjacency pair between the sentence pair Bag of words for each sentence Table 3: Features for Opinion Frame detection Link transitivity is then used to connect targets that are not explicitly linked by the annotators." ></td>
	<td class="line x" title="179:239	All annotations were performed by two of the co-authors of this paper by consensus labeling." ></td>
	<td class="line x" title="180:239	The details of our annotation scheme and interannotator agreement studies are presented in (Somasundaran et al., 2008)." ></td>
	<td class="line x" title="181:239	Once the individual frame components are annotated, conceptually, a frame exists for a pair of opinions if their polarities are either positive or negative and their targets are in a same or alt relation." ></td>
	<td class="line x" title="182:239	For our experiments, if a path exists between two targets, then their opinions are considered to be participating in an opinion-frame relation." ></td>
	<td class="line x" title="183:239	The experimental data consists of pairs of opinion sentences and the gold-standard information whether there exists a frame between them." ></td>
	<td class="line x" title="184:239	We approximate continuous discourse by only pairing sentences that are not more than 10 sentences apart." ></td>
	<td class="line x" title="185:239	We also filter out sentences that are less than two words in length in order to handle data skewness." ></td>
	<td class="line x" title="186:239	This filters out very small sentences (e.g., Cool.) which rarely participate in frames." ></td>
	<td class="line x" title="187:239	The experiments were performed on a total of 2539 sentence pairs, of which 551 are positive instances." ></td>
	<td class="line x" title="188:239	5.2 Features The factor that determines if two opinions are related is primarily the target relations between them." ></td>
	<td class="line x" title="189:239	Instead of first finding the target span for each opinion sentence and then inferring if they should be related, we directly try to encode target relation information in our features." ></td>
	<td class="line x" title="190:239	By this approach, even in the absence of explicit target-span information, we are able to determine if the opinion sentence pairs are related." ></td>
	<td class="line x" title="191:239	We explored a number of features to incorporate this." ></td>
	<td class="line x" title="192:239	The set that give the best performance are listed in Table 3." ></td>
	<td class="line x" title="193:239	The content word overlap feature captures the degree of topic overlap between the sentence pair, and looks for target relations via identity." ></td>
	<td class="line x" title="194:239	The focus space overlap feature is motivated by our observation that partici806 Acc." ></td>
	<td class="line x" title="195:239	Prec." ></td>
	<td class="line x" title="196:239	Recall F-measure False 78.3% 0% Distribution 66% 21.7% 21.7% 21.4% Random 50.0% 21.5% 49.4% 29.8 % True 21.7% 21.6% 100% 35.5 % System 67.6% 36.8% 64.9% 46% Table 4: Automatic Detection of Opinion Frames pants refer to an established discourse topic without explicitly referring to it." ></td>
	<td class="line x" title="197:239	Thus, we construct a focus space for each sentence containing recently used NP chunks." ></td>
	<td class="line x" title="198:239	The feature is the percent overlap between the focus spaces of the two opinion sentences." ></td>
	<td class="line x" title="199:239	The anaphoric indicator feature checks for the presence of pronouns such as it and that in the second sentence to account for target relations via anaphora." ></td>
	<td class="line x" title="200:239	The time difference between the sentences and the number of intervening sentences are useful features to capture the idea that topics shift with time." ></td>
	<td class="line x" title="201:239	The existence of an adjacency pair 3 between the sentences can clue the system that the opinions in the sentences are related too." ></td>
	<td class="line x" title="202:239	Finally, standard bag of words features are included for each sentence." ></td>
	<td class="line x" title="203:239	5.3 Results We performed 5-fold cross validation experiments, using the standard SVMperf package (Joachims, 2005), an implementation of SVMs designed for optimizing multivariate performance measures." ></td>
	<td class="line x" title="204:239	We found that, on our skewed data, optimizing on F-measure obtains the best results." ></td>
	<td class="line x" title="205:239	Our system is compared to four baselines in Table 4." ></td>
	<td class="line x" title="206:239	The majority class baseline which always guesses false (False) has good accuracy but zero recall." ></td>
	<td class="line x" title="207:239	The baseline that always guesses true (True) has 100% recall and the best f-measure among the baselines, but poor accuracy." ></td>
	<td class="line x" title="208:239	We also constructed a baseline that guesses true/false over the test set based on the distribution in the training data (Distribution)." ></td>
	<td class="line x" title="209:239	This baseline is smarter than the other baselines, as it does not indiscriminately guess any one of the class." ></td>
	<td class="line x" title="210:239	The last baseline Random guesses true 50% of the time." ></td>
	<td class="line x" title="211:239	The bottom row of Table 4 shows the performance of our system (System)." ></td>
	<td class="line x" title="212:239	The skewness of the data affects the baselines as well as our system." ></td>
	<td class="line x" title="213:239	Our system beats the best baseline f-measure by over 10 percentage points, and the best baseline precision by 14 percentage points." ></td>
	<td class="line x" title="214:239	Comparing 3Adjacency Pairs are manual dialog annotations available in the AMI corpus." ></td>
	<td class="line x" title="215:239	it to the baseline which has comparable accuracy, namely Distribution, we see that our system improves in f-measure by 24 percentage points." ></td>
	<td class="line x" title="216:239	Our results are encouraging even using simple features to capture target relations achieves considerable improvement over the baselines." ></td>
	<td class="line x" title="217:239	However, there is much room for improvement." ></td>
	<td class="line x" title="218:239	Using more detailed target and discourse information promises to further improve system performance." ></td>
	<td class="line x" title="219:239	These are avenues for future work." ></td>
	<td class="line oc" title="220:239	6 Related work Evidence from the surrounding context has been used previously to determine if the current sentence should be subjective/objective (Riloff et al., 2003; Pang and Lee, 2004) and adjacency pair information has been used to predict congressional votes (Thomas et al., 2006)." ></td>
	<td class="line x" title="221:239	However, these methods do not explicitly model the relations between opinions." ></td>
	<td class="line x" title="222:239	An application of the idea of alternative targets can be seen in Kim and Hovys (2007) work on election prediction." ></td>
	<td class="line x" title="223:239	They assume that if a speaker expresses support for one party, all mentions of the competing parties have negative polarity, thus creating automatically labeled training data." ></td>
	<td class="line x" title="224:239	In the field of product review mining, sentiments and features (aspects) have been mined (Popescu and Etzioni, 2005), where the aspects correspond to our definition of targets." ></td>
	<td class="line x" title="225:239	However, the aspects themselves are not related to each other in any fashion." ></td>
	<td class="line x" title="226:239	Polanyi and Zaenen (2006), in their discussion on contextual valence shifters, have also observed the phenomena described in this work namely that a central topic may be divided into subtopics in order to perform evaluations, and that discourse structure can influence the overall interpretation of valence." ></td>
	<td class="line x" title="227:239	Snyder and Barzilay (2007) combine an agreement model based on contrastive RST relations with a local aspect model to make a more informed overall decision for sentiment classification." ></td>
	<td class="line x" title="228:239	In our scheme, their aspects would be related as same and their high contrast relations would correspond to the non-reinforcing frames SPSNsame, SNSPsame." ></td>
	<td class="line x" title="229:239	Additionally, our frame relations would link the sentiments across nonadjacent clauses, and make connections via alt target relations." ></td>
	<td class="line x" title="230:239	With regard to meetings, the most closely related work includes the dialog-related annotation 807 schemes for various available corpora of conversation (e.g., Carletta et al.(2005) for AMI)." ></td>
	<td class="line x" title="232:239	As shown by Somasundaran et al.(2007), dialog structure information and opinions are in fact complementary." ></td>
	<td class="line x" title="234:239	We believe that, like the discourse relations, the dialog information will additionally help in arriving at an overall coherent interpretation." ></td>
	<td class="line x" title="235:239	7 Conclusions In this paper, we described the idea of opinion frames as a representation capturing discourse level relations that arise from related opinion targets and which are common in task-oriented dialogs." ></td>
	<td class="line x" title="236:239	We introduced the alternative relations that hold between targets by virtue of being opposing in the discourse context." ></td>
	<td class="line x" title="237:239	We discussed how our opinion-frame scheme and discourse relations go hand in hand to provide a richer overall interpretation." ></td>
	<td class="line x" title="238:239	We also illustrated that such discourse level opinion associations have useful benefits, namely they help gather more opinion information and help interdependent interpretation." ></td>
	<td class="line x" title="239:239	Finally, we showed via our machine learning experiments that the presence of opinion frames can be automatically detected." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1104
From Words to Senses: A Case Study of Subjectivity Recognition
Su, Fangzhong;Markert, Katja;"></td>
	<td class="line x" title="1:203	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 825832 Manchester, August 2008 From Words to Senses: A Case Study of Subjectivity Recognition Fangzhong Su School of Computing University of Leeds, UK fzsu@comp.leeds.ac.uk Katja Markert School of Computing University of Leeds, UK markert@comp.leeds.ac.uk Abstract We determine the subjectivity of word senses." ></td>
	<td class="line x" title="2:203	To avoid costly annotation, we evaluate how useful existing resources established in opinion mining are for this task." ></td>
	<td class="line x" title="3:203	We show that results achieved with existing resources that are not tailored towards word sense subjectivity classification can rival results achieved with supervision on a manually annotated training set." ></td>
	<td class="line x" title="4:203	However, results with different resources vary substantially and are dependent on the different definitions of subjectivity used in the establishment of the resources." ></td>
	<td class="line x" title="5:203	1 Introduction In recent years, subjectivity analysis and opinion mining have attracted considerable attention in the NLP community." ></td>
	<td class="line x" title="6:203	Unlike traditional information extraction and document classification tasks which usually focus on extracting facts or categorizing documents into topics (e.g., sports, politics, medicine), subjectivity analysis focuses on determiningwhetheralanguageunit(suchasaword, sentence or document) expresses a private state, opinion or attitude and, if so, what polarity is expressed, i.e. a positive or negative attitude." ></td>
	<td class="line x" title="7:203	Inspired by Esuli and Sebastiani (2006) and Wiebe and Mihalcea (2006), we explore the automatic detection of the subjectivity of word senses, in contrast to the more frequently explored task of determining the subjectivity of words (see Section 2)." ></td>
	<td class="line x" title="8:203	This is motivated by many words being c2008." ></td>
	<td class="line x" title="9:203	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="10:203	Some rights reserved." ></td>
	<td class="line x" title="11:203	subjectivity-ambiguous, i.e. having both subjective and objective senses, such as the word positive with its two example senses given below.1 (1) positive, electropositivehaving a positive electric charge;protons are positive (objective) (2) plus, positiveinvolving advantage or good; a plus (or positive) factor (subjective) Subjectivity labels for senses add an additional layerofannotationtoelectroniclexicaandallowto group many fine-grained senses into higher-level classes based on subjectivity/objectivity." ></td>
	<td class="line x" title="12:203	This can increase the lexicas usability." ></td>
	<td class="line x" title="13:203	As an example, Wiebe and Mihalcea (2006) prove that subjectivity information for WordNet senses can improve word sense disambiguation tasks for subjectivityambiguous words (such as positive)." ></td>
	<td class="line x" title="14:203	In addition, Andreevskaia and Bergler (2006) show that the performance of automatic annotation of subjectivity at the word level can be hurt by the presence of subjectivity-ambiguous words in the training sets they use." ></td>
	<td class="line x" title="15:203	Moreover, the prevalence of different word senses in different domains also means that a subjective or an objective sense of a word might be dominant in different domains; thus, in a science text positive is likely not to have a subjective reading." ></td>
	<td class="line x" title="16:203	Theannotationofwordsassubjectiveand objective or positive and negative independent of senseordomaindoesnotcapturesuchdistinctions." ></td>
	<td class="line x" title="17:203	In this paper, we validate whether word sense subjectivity labeling can be achieved with existing resources for subjectivity analysis at the word and sentence level without creating a dedicated, manually annotated training set of WordNet senses labeled for subjectivity.2 We show that such an ap1All examples in this paper are from WordNet 2.0." ></td>
	<td class="line x" title="18:203	2We use a subset of WordNet senses that are manually annotated for subjectivity as test set (see Section 3)." ></td>
	<td class="line x" title="19:203	825 proach  even using a simple rule-based unsupervised algorithm  can compete with a standard supervised approach and also compares well to prior research on word sense subjectivity labeling." ></td>
	<td class="line x" title="20:203	However, success depends to a large degree on the definition of subjectivity used in the establishment of the prior resources." ></td>
	<td class="line x" title="21:203	The remainder of this paper is organized as follows." ></td>
	<td class="line x" title="22:203	Section 2 discusses previous work." ></td>
	<td class="line x" title="23:203	Section 3 introduces our human annotation scheme for word sense subjectivity and also shows that subjectivity-ambiguous words are frequent." ></td>
	<td class="line x" title="24:203	Section 4 describes our proposed classification algorithms in detail." ></td>
	<td class="line x" title="25:203	Section 5 presents the experimental results and evaluation, followed by conclusions and future work in Section 6." ></td>
	<td class="line oc" title="26:203	2 Related Work There has been extensive research in opinion mining at the document level, for example on product and movie reviews (Pang et al., 2002; Pang and Lee, 2004; Dave et al., 2003; Popescu and Etzioni, 2005)." ></td>
	<td class="line x" title="27:203	Several other approaches focus on the subjectivity classification of sentences (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff and Wiebe, 2003)." ></td>
	<td class="line x" title="28:203	They often build on the presence of subjective words in the sentence to be classified." ></td>
	<td class="line x" title="29:203	Closer to our work is the large body of work on the automatic, context-independent classification of words according to their polarity, i.e as positive or negative (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005)." ></td>
	<td class="line x" title="30:203	They use either co-occurrence patterns in corpora or dictionarybased methods." ></td>
	<td class="line x" title="31:203	Many papers assume that subjectivity recognition, i.e. separating subjective from objective words, has already been achieved prior to polarity recognition and test against word lists containing subjective words only (Hatzivassiloglou and McKeown, 1997; Takamura et al., 2005)." ></td>
	<td class="line x" title="32:203	However, Kim and Hovy (2004) and Andreevskaia and Bergler (2006) also address the classification into subjective/objective words and show this to be a potentially harder task than polarity classification with lower human agreement and automatic performance." ></td>
	<td class="line x" title="33:203	There are only two prior approaches addressing word sense subjectivity or polarity classification." ></td>
	<td class="line x" title="34:203	Esuli and Sebastiani (2006) determine the polarity of word senses in WordNet, distinguishing among positive, negative and objective." ></td>
	<td class="line x" title="35:203	They expand a small, manually determined seed set of strongly positive/negative WordNet senses by following WordNet relations and use the resulting larger training set for supervised classification." ></td>
	<td class="line x" title="36:203	The resulting labeled WordNet gives three scores for each sense, representing the positive, negative and objective score respectively." ></td>
	<td class="line x" title="37:203	However, there is no evaluation as to the accuracy of their approach." ></td>
	<td class="line x" title="38:203	They then extend their work (Esuli and Sebastiani, 2007) by applying the Page Rank algorithm to ranking the WordNet senses in terms of how strongly a sense possesses a given semantic property (e.g., positive or negative)." ></td>
	<td class="line x" title="39:203	Wiebe and Mihalcea (2006) label word senses in WordNet as subjective or objective." ></td>
	<td class="line x" title="40:203	They use a method relying on distributional similarity as well as an independent, large manually annotated opinion corpus (MPQA) (Wiebe et al., 2005) for determining subjectivity." ></td>
	<td class="line x" title="41:203	One of the disadvantages of their algorithm is that it is restricted to senses that have distributionally similar words in the MPQA corpus, excluding 23.2% of their test data from automatic classification." ></td>
	<td class="line x" title="42:203	3 Human Annotation of Word Sense Subjectivity and Polarity In contrast to other researchers (Hatzivassiloglou and McKeown, 1997; Takamura et al., 2005), we do not see polarity as a category that is dependent on prior subjectivity assignment and therefore applicable to subjective senses only." ></td>
	<td class="line x" title="43:203	We follow Wiebe and Mihalcea (2006) in that we see subjective expressions as private states that are not open to objective observation or verification." ></td>
	<td class="line x" title="44:203	This includes direct references to emotions, beliefs and judgements (such as anger, criticise) as well as expressions that let a private state be inferred, for example by referring to a doctor as a quack." ></td>
	<td class="line x" title="45:203	In contrast, polarity refers to positive or negative associations of a word or sense." ></td>
	<td class="line x" title="46:203	Whereas there is a dependency in that most subjective senses have a relatively clear polarity, polarity can be attached to objective words/senses as well." ></td>
	<td class="line x" title="47:203	For example, tuberculosis is not subjective  it does not describe a private state, is objectively verifiable and would not cause a sentence containing it to carry an opinion, but it does carry negative associations for the vast majority of people." ></td>
	<td class="line x" title="48:203	We therefore annotate subjectivity of word senses similarly to Wiebe and Mihalcea (2006), 826 distinguishing between subjective (S), objective (O) or both (B)." ></td>
	<td class="line x" title="49:203	Both is used if a literal and metaphoric sense of a word are collapsed into one WordNet synset or if a WordNet synset contains both opinionated and objective expressions (such as bastard and illegitimate child in Ex." ></td>
	<td class="line x" title="50:203	3 below)." ></td>
	<td class="line x" title="51:203	We expand their annotation scheme by also annotatingpolarity, usingthelabelspositive(P),negative (N) and varied (V)." ></td>
	<td class="line x" title="52:203	The latter is used when a senses polarity varies strongly with the context, such as Example 8 below, where we would expect uncompromising to be a judgement but this judgement will be positive or negative depending on what a person is uncompromising about." ></td>
	<td class="line x" title="53:203	To avoid prevalence of personalised associations, annotators were told to only annotate polarity for subjective senses, as well as objective senses that carry a strong association likely to be shared by most people at least in Western culture (such as the negative polarity for words referring to diseases and crime)." ></td>
	<td class="line x" title="54:203	Other objective senses would receive the label O:NoPol." ></td>
	<td class="line x" title="55:203	Therefore, we have 7 sub categories in total: O:NoPol, O:P, O:N, S:P, S:N, S:V, and B. The notation before and after the colon represents the subjectivity and polarity label respectively." ></td>
	<td class="line x" title="56:203	We list some annotated examples below." ></td>
	<td class="line x" title="57:203	(3) bastard, by-blow, love child, illegitimate child, illegitimate, whoreson the illegitimate offspring of unmarried parents (B) (4) atrophyundergo atrophy; Muscles that are not used will atrophy (O:N) (5) guard, safety, safety devicea device designed to prevent injury (O:P) (6) nasty, awfuloffensive or even (of persons) malicious;in a nasty mood;a nasty accident; a nasty shock (S:N) (7) happyenjoying or showing or marked by joy or pleasure or good fortune; a happy smile;spent many happy days on the beach; a happy marriage (S:P) (8) uncompromising, inflexiblenot making concessions; took an uncompromising stance in the peace talks (S:V) As far as we are aware, this is the first annotation scheme for both subjectivity and polarity of word senses." ></td>
	<td class="line x" title="58:203	We believe both are relevant for opinion extraction: subjectivity for finding and analysing directly expressed opinions, and polarity for either classifying these further or extracting objective words that, however, serve to colour a text or present bias rather than explicitly stated opinions." ></td>
	<td class="line x" title="59:203	Su and Markert (2008) describe the annotation scheme and agreement study in full." ></td>
	<td class="line x" title="60:203	3.1 Agreement Study WeusedtheMicro-WNOpcorpuscontaining1105 WordNet synsets to test our annotation scheme.3 The Micro-WNOp corpus is representative of the part-of-speech distribution in WordNet." ></td>
	<td class="line x" title="61:203	Twoannotators(bothnear-nativeEnglishspeakers) independently annotated 606 synsets of the Micro-WNOp corpus for subjectivity and polarity." ></td>
	<td class="line x" title="62:203	One annotator is the second author of this paper whereas the other is not a linguist." ></td>
	<td class="line x" title="63:203	The overall agreement using all 7 categories is 84.6%, with a kappa of 0.77, showing high reliability for a difficult pragmatic task." ></td>
	<td class="line x" title="64:203	This at first seems at odds with the notion of sentiment as a fuzzy category as expressed in (Andreevskaia and Bergler, 2006) but we believe is due to three factors:  The annotation of senses instead of words splits most subjectivity-ambiguous words into several senses, removing one source of annotation difficulty." ></td>
	<td class="line x" title="65:203	 The annotation of senses in a dictionary provided the annotators with sense descriptions in form of Wordnet glosses as well as relatedsenses, providingmoreinformationthan a pure word annotation task." ></td>
	<td class="line x" title="66:203	 The split of subjectivity and polarity annotation made the task clearer and the annotation of only very strong connotations for objective word senses de-individualized the task." ></td>
	<td class="line x" title="67:203	As in this paper we are only interested in subjectivity recognition, we collapse S:V, S:P, and S:N into a single label S and O:NoPol, O:P, and O:N into a single label O. Label B remains unchanged." ></td>
	<td class="line x" title="68:203	For this three-way annotation overall percentage agreement is 90.1%, with a kappa of 0.79." ></td>
	<td class="line x" title="69:203	3.2 Gold Standard After cases with disagreement were negotiated between the two annotators, a gold standard annotation was agreed upon." ></td>
	<td class="line x" title="70:203	Our test set consists of this agreed set as well as the remainder of the MicroWNOp corpus annotated by one of the annotators alone after agreement was established." ></td>
	<td class="line x" title="71:203	This set is available for research purposes at http://www." ></td>
	<td class="line x" title="72:203	comp.leeds.ac.uk/markert/data." ></td>
	<td class="line x" title="73:203	3The corpus has originally been annotated by the providers (Esuli and Sebastiani, 2007) with scores for positive, negative and objective/no polarity, thus a mixture of subjectivity and polarity annotation." ></td>
	<td class="line x" title="74:203	We re-annotated the corpus with our annotation scheme." ></td>
	<td class="line x" title="75:203	827 How many words are subjectivity-ambiguous?" ></td>
	<td class="line x" title="76:203	As the number of senses increases with word frequency, we expect rare words to be less likely to be subjectivity-ambiguous than frequent words." ></td>
	<td class="line x" title="77:203	The Micro-WNOp corpus contains relatively frequent words so we will get an overestimation of subjective-ambiguous word types from this corpus, though not necessarily of word tokens." ></td>
	<td class="line x" title="78:203	It includes 298 different words with all their synsets in WordNet 2.0." ></td>
	<td class="line x" title="79:203	Of all words, 97 (32.5%) are subjectivity-ambiguous, a substantial number." ></td>
	<td class="line x" title="80:203	4 Algorithms In this section, we present experiments using five different resources as training sets or clue sets for thistask." ></td>
	<td class="line x" title="81:203	ThefirstistheMicro-WNOpcorpuswith our own dedicated word sense subjectivity annotation which is used in a standard supervised approach as training and test set via 10-fold crossvalidation." ></td>
	<td class="line x" title="82:203	This technique presupposes a manual annotation effort tailored directly to our task to provide training data." ></td>
	<td class="line x" title="83:203	As it is costly to create such training sets, we investigate whether existing resources such as two different subjective sentence lists (Section 4.2) and two different subjective word lists (Section 4.3) can be adapted to provide training data or clue sets although they do not provide any information about word senses." ></td>
	<td class="line x" title="84:203	All resources are used to create training data for supervised approaches; the subjective word lists are also used in a simple rule-based unsupervised approach." ></td>
	<td class="line x" title="85:203	All algorithms were tested on the Micro-WNOp corpus by comparing to the human gold standard annotation." ></td>
	<td class="line x" title="86:203	However, we excluded all senses with the label both from Micro-WNOp for testing the automatic algorithms, resulting in a final 1061 senses, with 703 objective and 358 subjective senses." ></td>
	<td class="line x" title="87:203	We also compare all algorithms to a baseline of always assigning the most frequent category (objective) to each sense, which results in an overall accuracy of 66.3%." ></td>
	<td class="line x" title="88:203	4.1 Standard Supervised Approach: 10-fold Cross-validation (CV) on Micro-WNOp We use 10-fold cross validation for training and testing on the annotated synsets in the MicroWNOp corpus." ></td>
	<td class="line x" title="89:203	We applied a Naive Bayes classifier, 4 using the following three types of features: 4We also experimented with KNN, Maximum Entropy, Rocchio and SVM algorithms and overall Naive Bayes perLexical Features: These are unigrams in the glosses." ></td>
	<td class="line x" title="90:203	Weuseabag-of-wordsapproachandfilter out stop words." ></td>
	<td class="line x" title="91:203	As glosses are usually quite short, using a bagof-word feature representation will result in highdimensional and sparse feature vectors, which often deteriorate classification performance." ></td>
	<td class="line x" title="92:203	In order toaddressthisproblemtosomedegree,wealsoexplored other features which are available as training and test instances are WordNet synsets." ></td>
	<td class="line x" title="93:203	Part-of-Speech (POS) Features: each sense gets its POS as a feature (adjective, noun, verb or adverb)." ></td>
	<td class="line x" title="94:203	Relation Features: WordNet relations are good indicators for determining subjectivity as many of them are subjectivity-preserving." ></td>
	<td class="line x" title="95:203	For example, if sense A is subjective, then its antonym sense B is likely to be subjective." ></td>
	<td class="line x" title="96:203	We employ 8 relations hereantonym, similar-to, derivedfrom, attribute, also-see, direct-hyponym, directhypernym, and extended-antonym." ></td>
	<td class="line x" title="97:203	Each relation R leads to 2 features that describe for a sense A how many links of that type it has to synsets in the subjectiveortheobjectivetrainingsetrespectively." ></td>
	<td class="line x" title="98:203	Finally, we represent the feature weights through a TF*IDF measure." ></td>
	<td class="line x" title="99:203	Considering the size of WordNet (115,424 synsetsinWordNet2.0), thelabeledMicro-WNOp corpus is small." ></td>
	<td class="line x" title="100:203	Therefore, the question arises whether it is possible to adapt other data sources that provide subjectivity information to our task." ></td>
	<td class="line x" title="101:203	4.2 Sentence Collections: Movie and MPQA It is reasonable to cast word sense subjectivity classification as a sentence classification task, with the glosses that WordNet provides for each sense as the sentences to be classified." ></td>
	<td class="line x" title="102:203	Then we can in theory feed any collection of annotated subjective and objective sentences as training data into our classifier while the annotated Micro-WNOp corpus is used as test data." ></td>
	<td class="line x" title="103:203	We experimented with two different available data sets to test this assumption." ></td>
	<td class="line oc" title="104:203	Movie-domainSubjectivityDataSet(Movie): Pang and Lee (2004) used a collection of labeled subjective and objective sentences in their work on review classification.5 The data set contains 5000 subjective sentences, extracted from movie reviews collected from the Rotten Tomatoes web formed best." ></td>
	<td class="line x" title="105:203	5Available at http://www.cs.cornell.edu/ People/pabo/movie-review-data/ 828 site.6 The 5000 objective sentences were collected from movie plot summaries from the Internet Movie Database (IMDB)." ></td>
	<td class="line x" title="106:203	The assumption is that all the snippets from the Rotten Tomatoes pages are subjective (as they come from a review site), while all the sentences from IMDB are objective (as they focus on movie plot descriptions)." ></td>
	<td class="line x" title="107:203	The MPQA Corpus contains news articles manually annotated at the phrase level for opinions, their polarity and their strength." ></td>
	<td class="line x" title="108:203	The corpus (Version 1.2) contains 11,112 sentences." ></td>
	<td class="line x" title="109:203	We convert it into a corpus of subjective and objective sentencesfollowingexactlytheapproachin(Riloff et al., 2003; Riloff and Wiebe, 2003) and obtain 6127 subjective and 4985 objective sentences respectively." ></td>
	<td class="line x" title="110:203	Basically any sentence that contains at least one strong subjective annotation at the phrase level is seen as a subjective sentence." ></td>
	<td class="line x" title="111:203	We again use a Naive Bayes algorithm with lexical unigram features." ></td>
	<td class="line x" title="112:203	Note that part-of-speech and relation features are not applicable here as the trainingsetconsistsofcorpussentences,notWordNet synsets." ></td>
	<td class="line x" title="113:203	4.3 Word Lists: General Inquirer and Subjectivity List Several word lists annotated for subjectivity or polaritysuchastheGeneralInquirer(GI) 7orthesubjectivity clues list (SL) collated by Janyce Wiebe and her colleagues8 are available." ></td>
	<td class="line x" title="114:203	The General Inquirer (GI) was developed by Philip Stone and colleagues in the 1960s." ></td>
	<td class="line x" title="115:203	It concentrates on word polarity." ></td>
	<td class="line x" title="116:203	Here we make the simple assumption that both positive and negative words in the GI list are subjective clues whereas all other words are objective." ></td>
	<td class="line x" title="117:203	The Subjectivity Lexicon (SL) centers on subjectivity so that it is ideally suited for our task." ></td>
	<td class="line x" title="118:203	It provides fine-grained information for each clue, such as part-of-speech, subjectivity strength (strong/weak), and prior polarity (positive, negative, or neutral)." ></td>
	<td class="line x" title="119:203	For example, object(verb) is a subjective clue whereas object(noun) is objective." ></td>
	<td class="line x" title="120:203	Regarding strength, the adjective evil is marked as strong subjective whereas the adjective exposed is marked as a weak subjective clue." ></td>
	<td class="line x" title="121:203	Both lexica do not include any information about word senses and therefore cannot be used directly for subjectivity assignment at the sense 6http://www.rottentomatoes.com/ 7http://www.wjh.harvard.edu/inquirer/ 8http://www.cs.pitt.edu/mpqa/ level." ></td>
	<td class="line x" title="122:203	For example, at least one sense of any subjectivity-ambiguouswordwillbelabeledincorrectly if we just adopt a word-based label." ></td>
	<td class="line x" title="123:203	In addition, theselistsarefarfromcomplete: comparedto the over 100,000 synsets in WordNet, GI contains 11,788 words marked for polarity (1915 positive, 2291 negative and 7582 no-polarity words) and the SL list contains about 8,000 subjective words." ></td>
	<td class="line x" title="124:203	Still, it is a reasonable assumption that any gloss that contains several subjective words indicates a subjective sense overall." ></td>
	<td class="line x" title="125:203	This intuition is strengthened by the characteristics of glosses." ></td>
	<td class="line x" title="126:203	They normallyareshortandconcisewithoutacomplexsyntactic structure, thus the occurrence of subjective words in such a short string is likely to indicate a subjective sense overall." ></td>
	<td class="line x" title="127:203	This contrasts, for example, with sentences in newspapers where one clause might express an opinion, whereas other parts of the sentence are objective." ></td>
	<td class="line x" title="128:203	Therefore, for the rule-based unsupervised algorithm we lemmatized and POS-tagged the glosses in the Micro-WNOp test set." ></td>
	<td class="line x" title="129:203	Then we compute a subjectivity score S for each synset by summing up the weight values of all subjectivity clues in its gloss." ></td>
	<td class="line x" title="130:203	If S is equal or higher than an agreed threshold T, then the synset is classified as subjective, otherwise as objective." ></td>
	<td class="line x" title="131:203	For the GI lexicon, all subjectivity clues have the same weight 1, whereas for the SL list we assign a weight value 2 to strongly subjective clues and 1 to weakly subjective clues." ></td>
	<td class="line x" title="132:203	We experimented with several thresholds T and report here the results for the best thresholds, which were 2 for SL and 4 for the GI word list." ></td>
	<td class="line x" title="133:203	The corresponding methods are called Rule-SL and Rule-GI." ></td>
	<td class="line x" title="134:203	This approach does not allow us to easily integrate relational WordNet features." ></td>
	<td class="line x" title="135:203	It might also suffer from the incompleteness of the lexica and the fact that it has to make decisions for borderline cases (at the value of the threshold set)." ></td>
	<td class="line x" title="136:203	We therefore explored instead to generate larger, more reliable training data consisting of WordNet synsets from the word lists." ></td>
	<td class="line x" title="137:203	To achieve this, we assign a subjectivity score S as above to all WordNetsynsets(excludingsynsetsinthetestset)." ></td>
	<td class="line x" title="138:203	If S is higher or equal to a threshold T1 it is added to the subjective training set, if it is lower or equal to T2 it is added to the objective training set." ></td>
	<td class="line x" title="139:203	This allows us to choose quite clear thresholds so that borderline cases with a score between T1 and T2 arenotinthetrainingset." ></td>
	<td class="line x" title="140:203	Italsoallowstousepart829 of-speech and relational features as the training set then consists of WordNet synsets." ></td>
	<td class="line x" title="141:203	In this way, we can automatically generate (potentially noisy) training data of WordNet senses marked for subjectivity without annotating any WordNet senses manually for subjectivity." ></td>
	<td class="line x" title="142:203	We experimented with several different thresholdsetsbutwefoundthattheyactuallyhaveaminimal impact on the final results." ></td>
	<td class="line x" title="143:203	We report here the best results for a threshold T1 of 4 and T2 of 2 for the SL lexicon and of 3 and 1 respectively for the GI word list." ></td>
	<td class="line x" title="144:203	5 Experiments and Evaluation We measure the classification performance with overall accuracy as well as precision, recall and balancedF-scoreforbothcategories(objectiveand subjective)." ></td>
	<td class="line x" title="145:203	All results are summarised in Table 1." ></td>
	<td class="line x" title="146:203	Results are compared to the baseline of majority classification using a McNemar test at the significance level of 5%." ></td>
	<td class="line x" title="147:203	5.1 Experimental Results Table 1 shows that SL performs best among all the methodologies." ></td>
	<td class="line x" title="148:203	All CV, Rule-SL and SL methods significantly beat the baseline." ></td>
	<td class="line x" title="149:203	In addition, if we compare the results of methods with and without additional parts-of-speech and WordNet relation features, we see a small but consistent improvement when we use additional features." ></td>
	<td class="line x" title="150:203	It is also worthwhile to expand the rule-based unsupervised method into a method for generating training data and use additional features as SL significantly outperforms Rule-SL." ></td>
	<td class="line x" title="151:203	5.2 Discussion WordLists." ></td>
	<td class="line x" title="152:203	Surprisingly,usingSLgreatlyoutperforms GI, regardless of whether we use the supervised or unsupervised method or whether we use lexical features only or the other features as well.9 There are several reasons for this." ></td>
	<td class="line x" title="153:203	First, the GI lexicon is annotated for polarity, not subjectivity." ></td>
	<td class="line x" title="154:203	More specifically, words that we see as objective but with a strong positive or negative association (such as words for crimes) and words that we see as subjective are annotated with the same polarity label in the GI lexicon." ></td>
	<td class="line x" title="155:203	Therefore, the GI definition of subjectivity does not match ours." ></td>
	<td class="line x" title="156:203	Also, 9This pattern is repeated for all threshold combinations, which are not reported here." ></td>
	<td class="line x" title="157:203	the GI lexicon does not operate with a clearly expressed polarity definition, leading to conflicting annotations and casting doubt on its widespread use in the opinion mining community as a gold standard (Turney and Littman, 2003; Takamura et al., 2005; Andreevskaia and Bergler, 2006)." ></td>
	<td class="line x" title="158:203	For example, amelioration is seen as non-polar in GI but improvement is annotated with positive polarity." ></td>
	<td class="line x" title="159:203	Second, in contrast to SL, GI does not consider different parts-of-speech of a word and subjectivity strength (strong/weak subjectivity)." ></td>
	<td class="line x" title="160:203	Third, GI contains many fewer subjective clues than SL." ></td>
	<td class="line x" title="161:203	Sentence Data." ></td>
	<td class="line x" title="162:203	When using the Movie dataset and MPQA corpus as training data, the results are not satisfactory." ></td>
	<td class="line x" title="163:203	We first checked the purity of these two datasets to see whether they are too noisy." ></td>
	<td class="line x" title="164:203	For this purpose, we used a naive Bayes algorithm with unigram features and conducted a 10-foldcrossvalidationexperimentonrecognizing subjective/objective sentences within the Movie dataset and MPQA independently." ></td>
	<td class="line x" title="165:203	Interestingly, the accuracy for the Movie dataset and MPQA corpus achieved 91% and 76% respectively." ></td>
	<td class="line x" title="166:203	Considering that they are balanced datasets with a most frequent category baseline of about 50%, this accuracy is high, especially for the Movie dataset." ></td>
	<td class="line x" title="167:203	However, again the subjectivity definition in the Movie corpus does not seem to match ours." ></td>
	<td class="line x" title="168:203	Recall that we see a word sense or a sentence as subjective if it expresses a private state (i.e., emotion, opinion, sentiment, etc.), and objective otherwise." ></td>
	<td class="line x" title="169:203	Inspecting the movie data set, we found that indeed the sentences included in its subjective set would mostly be seen as subjective in our sense as well as they contain opinions about the movie such as it desperately wants to be a wacky , screwball comedy , but the most screwy thing here is how so many talented people were convinced to waste their time." ></td>
	<td class="line x" title="170:203	It is also true that the sentences (plot descriptions) in its objective data set relatively rarely contain opinions about the movie." ></td>
	<td class="line x" title="171:203	However, they still contain other opinionated content like opinions and emotions of the characters in the movie such as the obsession of a character with John Lennon in the beatles fan is a drama about Albert, a psychotic prisoner who is a devoted fan of John Lennon and the beatles." ></td>
	<td class="line x" title="172:203	Since the data sets definition of subjective sentences is closer to ours than the one for objective sentences, we conducted a one-class learning approach (Li and Liu, 2003) using Movies subjective sentences as 830 Table 1: Results Method Subjective Objective Accuracy Precision Recall F-score Precision Recall F-score Baseline N/A 0 N/A 66.3% 100% 79.7% 66.3% CV 65.2% 52.8% 58.3% 78.1% 85.6% 81.7% 74.6% CV 69.5% 55.3% 61.6% 79.4% 87.6% 83.3% 76.7% Movie 43.8% 60.1% 50.6% 74.9% 60.7% 67.1% 60.5% MPQA 44.5% 78.5% 56.8% 82.1% 50.1% 62.2% 59.7% GI 50.4% 39.4% 44.2% 72.2% 80.2% 76.0% 66.4% GI 54.5% 33.5% 41.5% 71.7% 85.8% 78.1% 68.1% SL 64.3% 62.8% 63.6% 81.3% 82.2% 81.8% 75.7% SL 66.2% 64.5% 65.3% 82.2% 83.2% 82.7% 76.9% Rule-GI 38.5% 5.6% 9.8% 66.5% 95.4% 78.4% 65.1% Rule-SL 59.7% 70.4% 64.6% 83.4% 75.8% 79.4% 74.0% 1 CV, GI and SL correspond to methods using lexical features only." ></td>
	<td class="line x" title="173:203	2 CV, GI and SL correspond to methods using a feature combination of lexical, part-of-speech, and WordNet relations." ></td>
	<td class="line x" title="174:203	3  indicates results significantly better than the baseline." ></td>
	<td class="line x" title="175:203	the only training data." ></td>
	<td class="line x" title="176:203	The algorithm 10 combines Expectation Maximization and Naive Bayes algorithms, and we used randomly extracted 50,000 unlabeled synsets in WordNet as the necessary unlabeled data." ></td>
	<td class="line x" title="177:203	This approach achieves an accuracy of 69.4% on Micro-WNOp, which is significantly better than the baseline." ></td>
	<td class="line x" title="178:203	The subjectivity definition in the MPQA corpus is quite close to ours." ></td>
	<td class="line x" title="179:203	However, our mapping from its phrase annotation to sentence annotation might betoocoarse-grainedasmanysentencesinthecorpus span several clauses containing both opinions andfactualdescription." ></td>
	<td class="line x" title="180:203	Weassumethatthisispossibly also the reason why its purity is lower than in the Movie dataset." ></td>
	<td class="line x" title="181:203	We therefore experimented again with a one-class learning approach using just the subjective phrases in MPQA as training data." ></td>
	<td class="line x" title="182:203	The accuracy does improve to 67.6% but is still not significantly higher than the baseline." ></td>
	<td class="line x" title="183:203	5.3 Comparison to Prior Approaches Esuli and Sebastiani (2006) make their labeled WordNet SentiWordNet 1.0 publically available.11 Recall that they actually use polarity classification: however, as there is a dependency between polarity and subjectivity classification for subjective senses, we map their polarity scores to our subjectivity labels as follows." ></td>
	<td class="line x" title="184:203	If the sum of positive and 10Available at http://www.cs.uic.edu/liub/LPU/." ></td>
	<td class="line x" title="185:203	11Available at http://sentiwordnet.isti.cnr." ></td>
	<td class="line x" title="186:203	it/ negativescoresofasenseinSentiWordNetismore than or equal to 0.5, then it is subjective and otherwise objective.12 Using this mapping, it achieves an accuracy of 75.3% on the Micro-WNOp corpus, compared to our gold standard." ></td>
	<td class="line x" title="187:203	Therefore our methods CV and SL perform slightly better than theirs, although the improvement is not significant." ></td>
	<td class="line x" title="188:203	The task definition in Wiebe and Mihalcea (2006) is much more similar to ours but they use different annotated test data, which is not publicallyavailable,soanexactcomparisonisnotpossible." ></td>
	<td class="line x" title="189:203	Both data sets, however, seem to include relatively frequent words." ></td>
	<td class="line x" title="190:203	One disadvantage of their method is that it is not applicable to all WordNet senses as it is dependent on distributionally similar words being available in the MPQA." ></td>
	<td class="line x" title="191:203	Thus, 23% of their test data is excluded from evaluation, whereas our methods can be used on any WordNet sense." ></td>
	<td class="line x" title="192:203	They measure precision and recall for subjective senses in a precision/recall curve: Precision is about 48/9% at a recall of 60% for subjective senses whereas our best SL method has a precision of 66% at about the same recall." ></td>
	<td class="line x" title="193:203	Although this suggests better performance of our method, it is not possible to draw final conclusions from this comparison due to the data set differences." ></td>
	<td class="line x" title="194:203	12We experimented with slightly different mappings but this mapping gave SentiWordNet the best possible result." ></td>
	<td class="line x" title="195:203	There is a relatively large number of cases with a 0.5/0.5 split in SentiWordNet, making it hard to decide between subjective and objective senses." ></td>
	<td class="line x" title="196:203	831 6 Conclusion and Future Work We proposed different ways of extracting training dataandcluesetsforwordsensesubjectivitylabeling from existing opinion mining resources." ></td>
	<td class="line x" title="197:203	The effectiveness of the resulting algorithms depends greatlyonthegeneratedtrainingdata, morespecifically on the different definitions of subjectivity used in resource creation." ></td>
	<td class="line x" title="198:203	However, we were able to show that at least one of these methods (based on the SL word list) resulted in a classifier that performed on a par with a supervised classifier that useddedicatedtrainingdatadevelopedforthistask (CV)." ></td>
	<td class="line x" title="199:203	Thus, it is possible to avoid any manual annotation for the subjectivity classification of word senses." ></td>
	<td class="line x" title="200:203	Our future work will explore new methodologies in feature representation by importing more background information (e.g., syntactic information)." ></td>
	<td class="line x" title="201:203	Furthermore, our current method of integrating the rich relation information in WordNet (using them as standard features) does not use joint classification of several senses." ></td>
	<td class="line x" title="202:203	Instead, we think it will be more promising to use the relations to construct graphs for semi-supervised graph-based learningofwordsensesubjectivity." ></td>
	<td class="line x" title="203:203	Inaddition,we will also explore whether the derived sense labels improve applications such as sentence classification and clustering WordNet senses." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1135
Automatic Seed Word Selection for Unsupervised Sentiment Classification of Chinese Text
Zagibalov, Taras;Carroll, John A.;"></td>
	<td class="line x" title="1:153	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 10731080 Manchester, August 2008 Automatic Seed Word Selection for Unsupervised Sentiment Classification of Chinese Text Taras Zagibalov    John Carroll University of Sussex Department of Informatics Brighton  BN1 9QH, UK {T.Zagibalov,J.A.Carroll}@sussex.ac.uk Abstract We describe and evaluate a new method of automatic seed word selection for unsupervised sentiment classification of product reviews in Chinese." ></td>
	<td class="line x" title="2:153	The whole method is unsupervised and does not require any annotated training data; it only requires information about commonly occurring negations and adverbials." ></td>
	<td class="line x" title="3:153	Unsupervised techniques are promising for this task since they avoid problems of domain-dependency typically associated with supervised methods." ></td>
	<td class="line x" title="4:153	The results obtained are close to those of supervised classifiers and sometimes better, up to an F1 of 92%." ></td>
	<td class="line x" title="5:153	1 Introduction Automatic classification of document sentiment (and more generally extraction of opinion from text) has recently attracted a lot of interest." ></td>
	<td class="line x" title="6:153	One of the main reasons for this is the importance of such  information  to  companies,  other organizations, and individuals." ></td>
	<td class="line x" title="7:153	Applications include marketing research tools that help a company see market or media reaction towards their brands, products or services, or search engines that help potential purchasers make an informed choice of a product they want to buy." ></td>
	<td class="line x" title="8:153	Sentiment classification research has drawn on and contributed to research in text classification, unsupervised machine learning, and crossdomain adaptation." ></td>
	<td class="line x" title="9:153	This paper presents a new, automatic approach to automatic seed word selection as part of sentiment classification of product reviews written in Chinese, which addresses the problem of do  2008." ></td>
	<td class="line x" title="10:153	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="11:153	Some rights reserved." ></td>
	<td class="line x" title="12:153	main-dependency of sentiment classification that has been observed in previous work." ></td>
	<td class="line x" title="13:153	It may also facilitate building sentiment classification systems in other languages since the approach assumes a very small amount of linguistic knowledge: the only language-specific information required is a basic description of the most frequent negated adverbial constructions in the language." ></td>
	<td class="line x" title="14:153	The paper is structured as follows." ></td>
	<td class="line x" title="15:153	Section 2 surveys related work in sentiment classification, unsupervised machine learning and Chinese language processing." ></td>
	<td class="line x" title="16:153	Section 3 motivates our approach, which is described in detail in Section 4." ></td>
	<td class="line x" title="17:153	The data used for experiments and baselines, as well as the results of experiments are covered in Section 5." ></td>
	<td class="line x" title="18:153	Section 6 discusses the lessons learned and proposes directions for future work." ></td>
	<td class="line x" title="19:153	2 Related Work 2.1 Sentiment Classification Most work on sentiment classification has used approaches based on supervised machine learning." ></td>
	<td class="line x" title="20:153	For example, Pang et al.(2002) collected movie reviews that had been annotated with respect to sentiment by the authors of the reviews, and used this data to train supervised classifiers." ></td>
	<td class="line x" title="22:153	A number of studies have investigated the impact on classification accuracy of different factors, including choice of feature set, machine learning algorithm, and pre-selection of the segments of text to be classified." ></td>
	<td class="line x" title="23:153	For example, Dave et al.(2003) experiment with the use of linguistic, statistical and n-gram features and measures for feature selection and weighting." ></td>
	<td class="line oc" title="25:153	Pang and Lee (2004) use a graph-based technique to identify and analyze only subjective parts of texts." ></td>
	<td class="line x" title="26:153	Yu and Hatzivassiloglou (2003) use semanticallyoriented words for identification of polarity at the sentence level." ></td>
	<td class="line x" title="27:153	Most of this work assumes binary classification (positive and negative), some1073 times with the addition of a neutral class (in terms of polarity, representing lack of sentiment)." ></td>
	<td class="line x" title="28:153	While supervised systems generally achieve reasonably high accuracy, they do so only on test data that is similar to the training data." ></td>
	<td class="line x" title="29:153	To move to another domain one would have to collect annotated data in the new domain and retrain the classifier." ></td>
	<td class="line x" title="30:153	Engstrm (2004) reports decreased accuracy in cross-domain classification since sentiment in different domains is often expressed in different ways." ></td>
	<td class="line x" title="31:153	However, it is impossible in practice to have annotated data for all possible domains of interest." ></td>
	<td class="line x" title="32:153	Aue and Gamon (2005) attempt to solve the problem of the absence of large amounts of labeled data by customizing sentiment classifiers to new domains using training data from other domains." ></td>
	<td class="line x" title="33:153	Blitzer et al.(2007) investigate domain adaptation for sentiment classifiers using structural correspondence learning." ></td>
	<td class="line x" title="35:153	Read (2005) also observed significant differences between the accuracy of classification of reviews in the same domain but published in different time periods." ></td>
	<td class="line x" title="36:153	Recently, there has been a shift of interest towards more fine-grained approaches to processing of sentiment, in which opinion is extracted at the sentence level, sometimes including information about different features of a product that are commented on and/or the opinion holder (Hu and Liu, 2004; Ku et al., 2006)." ></td>
	<td class="line x" title="37:153	But even in such approaches, McDonald et al.(2007) note that information about the overall sentiment orientation of a document facilitates more accurate extraction of more specific information from the text." ></td>
	<td class="line x" title="39:153	2.2 Unsupervised Approach One way of tackling the problem of domain dependency could be to use an approach that does not rely on annotated data." ></td>
	<td class="line x" title="40:153	Turney (2002) describes a method of sentiment classification using two human-selected seed words (the words poor and excellent) in conjunction with a very large text corpus; the semantic orientation of phrases is computed as their association with the seed words (as measured by pointwise mutual information)." ></td>
	<td class="line x" title="41:153	The sentiment of a document is calculated as the average semantic orientation of all such phrases." ></td>
	<td class="line x" title="42:153	Yarowsky (1995) describes a 'semi-unsupervised' approach to the problem of sense disambiguation of words, also using a set of initial seeds, in this case a few high quality sense annotations." ></td>
	<td class="line x" title="43:153	These annotations are used to start an iterative process of learning information about the contexts in which senses of words appear, in each iteration labeling senses of previously unlabeled word tokens using information from the previous iteration." ></td>
	<td class="line x" title="44:153	2.3 Chinese Language Processing A major issue in processing Chinese text is the fact that words are not delimited in the written language." ></td>
	<td class="line x" title="45:153	In many cases, NLP researchers working with Chinese use an initial segmentation module that is intended to break a text into words." ></td>
	<td class="line x" title="46:153	Although this can facilitate the use of subsequent computational techniques, there is no a clear definition of what a 'word' is in the modern Chinese language, so the use of such segmenters is of dubious theoretical status; indeed, good results have been reported from systems which do not assume such pre-processing (Foo and Li, 2004; Xu et al., 2004)." ></td>
	<td class="line x" title="47:153	2.4 Seed Word Selection We are not aware of any sentiment analysis system that uses unsupervised seed word selection." ></td>
	<td class="line x" title="48:153	However, Pang et al.(2002) showed that it is difficult to get good coverage of a target domain from manually selected words, and even simple corpus frequency counts may produce a better list of features for supervised classification: human-created lists resulted in 64% accuracy on a movie review corpus, while a list of frequent words scored 69%." ></td>
	<td class="line x" title="50:153	Pang et al. also observed that some words without any significant emotional orientation were quite good features: for example, the word still turned out to be a good indicator of positive reviews as it was often used in sentences such as Still, though, it was worth seeing''." ></td>
	<td class="line x" title="51:153	3 Our Approach Our main goal is to overcome the problem of domain-dependency in sentiment classification." ></td>
	<td class="line x" title="52:153	Unsupervised approaches seem promising in this regard, since they do not require annotated training data, just access to sufficient raw text in each domain." ></td>
	<td class="line x" title="53:153	We base our approach on a previously described, 'almost-unsupervised' system that starts with only a single, human-selected seed  (good) and uses an iterative method to extract a training sub-corpus (Zagibalov & Carroll, 2008)." ></td>
	<td class="line x" title="54:153	The approach does not use a word segmentation module; in this paper we use the term 'lexical item' to denote any sequence of Chinese characters that is treated by the system as a unit, whatever it is linguistically  a morpheme, a word or a phrase." ></td>
	<td class="line x" title="55:153	1074 Our initial aim was to investigate ways of improving the classifier by automatically finding a better seed, because Zagibalov & Carroll indicate that in different domains they could, by manual trial and error, find a seed other than  (good) which produced better results." ></td>
	<td class="line x" title="56:153	To find such a seed automatically, we make two assumptions: 1." ></td>
	<td class="line x" title="57:153	Attitude is often expressed through the negation of vocabulary items with the opposite meaning; for example in Chinese it is more common to say not good than bad (Tan, 2002)." ></td>
	<td class="line x" title="58:153	Zagibalov & Carroll's system uses this observation to find negative lexical items while nevertheless starting only from a positive seed." ></td>
	<td class="line x" title="59:153	This leads us to believe that it is possible to find candidate seeds themselves by looking for sequences of characters which are used with negation." ></td>
	<td class="line x" title="60:153	2." ></td>
	<td class="line x" title="61:153	The polarity of a candidate seed needs to be determined." ></td>
	<td class="line x" title="62:153	To do this we assume we can use the lexical item   (good) as a gold standard for positive lexical items and compare the pattern of contexts a candidate seed occurs in to the pattern exhibited by the gold standard." ></td>
	<td class="line x" title="63:153	Looking at product review corpora, we observed that good is always more often used without negation in positive texts, while in negative texts it is more often used with negation (e.g. not good)." ></td>
	<td class="line x" title="64:153	Also, good occurs more often in positive texts than negative, and more frequently without negation than with it." ></td>
	<td class="line x" title="65:153	We use the latter observation as the basis for identifying seed lexical items, finding those which occur with negation but more frequently occur without it." ></td>
	<td class="line x" title="66:153	As well as detecting negation1 we also use adverbials2 to avoid hypothesizing non-contentful seeds: the characters following the sequence of a negation and an adverbial are in general contentful units, as opposed to parts of words, function words, etc. In what follows we refer to such constructions as negated adverbial constructions." ></td>
	<td class="line x" title="67:153	1We use only six frequently occurring negations:  (bu),   (buhui),  (meiyou),  (baituo),  (mianqu), and  (bimian)." ></td>
	<td class="line x" title="68:153	We are trying to be as language-independent as possible so we take a simplistic approach to detecting negation." ></td>
	<td class="line x" title="69:153	2We use five frequently occurring adverbials:  (hen),  (feichang),  (tai),  (zui), and  (bijiao)." ></td>
	<td class="line x" title="70:153	Similarly to negation, we deliberately take a simplistic approach." ></td>
	<td class="line x" title="71:153	4 Method We use a similar sentiment classifier and iterative retraining technique to the almost-unsupervised system of Zagibalov & Carroll (2008), summarized below in Sections 4.2 and 4.3." ></td>
	<td class="line x" title="72:153	The main new contributions of this paper are techniques for automatically finding the seeds from raw text in a particular domain (Section 4.1), and for detecting when the process should stop (Section 4.4)." ></td>
	<td class="line x" title="73:153	This new system therefore differs from that of Zagibalov & Carroll (2008) in being completely unsupervised and not depending on arbitrary iteration limits." ></td>
	<td class="line x" title="74:153	(The evaluation also differs since we focus in this paper on the effects of domain on sentiment classification accuracy)." ></td>
	<td class="line x" title="75:153	4.1 Seed Lexical Item Identification The first step is to identify suitable positive seeds for the given corpus." ></td>
	<td class="line x" title="76:153	The intuition behind the way this is done is outlined above in Section 3." ></td>
	<td class="line x" title="77:153	The algorithm is as follows: 1." ></td>
	<td class="line x" title="78:153	find all sequences of characters between non-character symbols (i.e. punctuation marks, digits and so on) that contain negation and an adverbial, split the sequence at the negation, and store the character sequence that follows the negated adverbial construction; 2." ></td>
	<td class="line x" title="79:153	count the number of occurrences of each distinct sequence that follows a negated adverbial construction (X); 3." ></td>
	<td class="line x" title="80:153	count the number of occurrences of each distinct sequence without the construction (Y); 4." ></td>
	<td class="line x" title="81:153	find all sequences with Y  X > 0." ></td>
	<td class="line x" title="82:153	4.2 Sentiment Classification This approach to Chinese language processing does not use pre-segmentation (in the sense discussed in Section 2.3) or grammatical analysis: the basic unit of processing is the 'lexical item', each of which is a sequence of one or more Chinese characters excluding punctuation marks (so a lexical item may actually form part of a word, a whole word or a sequence of words), and 'zones', each of which is a sequence of characters delimited by punctuation marks." ></td>
	<td class="line x" title="83:153	Each zone is classified as either positive or negative based whether positive or negative vocabulary items predominate." ></td>
	<td class="line x" title="84:153	As there are two parts of the vocabulary (positive and negative), we correspondingly calculate two scores (Si , 1075 where i is either positive or negative) using Equation (1), where Ld is the length in characters of a matching lexical item (raised to the power of two to increase the significance of longer items which capture more context), Lphrase is the length of the current zone in characters, Sd is the current sentiment score of the matching lexical item (initially 1.0), and Nd is a negation check coefficient." ></td>
	<td class="line x" title="85:153	Si= Ld 2 LphraseSd Nd                    (1) The negation check is a regular expression which determines if the lexical item is preceded by a negation within its enclosing zone." ></td>
	<td class="line x" title="86:153	If a negation is found then Nd is set to 1." ></td>
	<td class="line x" title="87:153	The sentiment score of a zone is the sum of sentiment of all the items found in it." ></td>
	<td class="line x" title="88:153	To determine the sentiment orientation of the whole document, the classifier computes the difference between the number of positive and negative zones." ></td>
	<td class="line x" title="89:153	If the result is greater than zero the document is classified as positive, and vice versa." ></td>
	<td class="line x" title="90:153	4.3 Iterative Retraining Iterative retraining is used to enlarge the initial seed vocabulary into a comprehensive vocabulary list of sentiment-bearing lexical items." ></td>
	<td class="line x" title="91:153	In each iteration, the current version of the classifier is run on the input corpus to classify each document, resulting in a training subcorpus of positive and a negative documents." ></td>
	<td class="line x" title="92:153	The subcorpus is used to adjust the scores of existing positive and negative vocabulary items and to find new items to be included in the vocabulary." ></td>
	<td class="line x" title="93:153	Each lexical item that occurs at least twice in the corpus is a candidate for inclusion in the vocabulary list." ></td>
	<td class="line x" title="94:153	After candidate items are found, the system calculates their relative frequencies in both the positive and negative parts of the current training subcorpus." ></td>
	<td class="line x" title="95:153	The system also checks for negation while counting occurrences: if a lexical item is preceded by a negation, its count is reduced by one." ></td>
	<td class="line x" title="96:153	For all candidate items we compare their relative frequencies in the positive and negative documents in the subcorpus using Equation (2)." ></td>
	<td class="line x" title="97:153	difference= F pFnF pFn/2         (2) If difference < 1, then the frequencies are similar and the item does not have enough distinguishing power, so it is not included in the vocabulary." ></td>
	<td class="line x" title="98:153	Otherwise the sentiment score of the item is (re-)calculated  according to Equation (3) for positive items, and analogously for negative items." ></td>
	<td class="line x" title="99:153	F pFn         (3) Finally, the adjusted vocabulary list with the new scores is ready for the next iteration3." ></td>
	<td class="line x" title="100:153	4.4 Iteration Control To maximize the number of productive iterations while avoiding unnecessary processing and arbitrary iteration limits, iterative retraining is stopped when there is no change to the classification of any document over the previous two iterations." ></td>
	<td class="line x" title="101:153	5 Experiments 5.1 Data As our approach is unsupervised, we do not use an annotated training corpus, but run our iterative procedure on the raw data extracted from an annotated test corpus, and evaluate the final accuracy of the system with respect to the annotations in that corpus." ></td>
	<td class="line x" title="102:153	Our test corpus is derived from product reviews harvested from the website IT1684." ></td>
	<td class="line x" title="103:153	All the reviews were tagged by their authors as either positive or negative overall." ></td>
	<td class="line x" title="104:153	Most reviews consist of two or three distinct parts: positive opinions, negative opinions, and comments ('other')  although some reviews have only one part." ></td>
	<td class="line x" title="105:153	We removed duplicate reviews automatically using approximate matching, giving a corpus of 29531 reviews of which 23122 are positive (78%) and 6409 are negative (22%)." ></td>
	<td class="line x" title="106:153	The total number of different products in the corpus is 10631, the number of product categories is 255, and most of the reviewed products are either software products or consumer electronics." ></td>
	<td class="line x" title="107:153	Unfortunately, it appears that some users misuse the sentiment 3An alternative approach might be to use point-wise mutual information instead of relative frequencies of newly found features in a subcorpus produced in the previous iteration." ></td>
	<td class="line x" title="108:153	However, in preliminary experiments, SO-PMI did not produce good corpora from the first iteration." ></td>
	<td class="line x" title="109:153	Also, it is not clear how to manage subsequent iterations since PMI would have to be calculated between thousands of new vocabulary items and every newly found sequence of characters, which would be computationally intractable." ></td>
	<td class="line x" title="110:153	4http://product.it168.com 1076 tagging facility on the website so quite a lot of reviews have incorrect tags." ></td>
	<td class="line x" title="111:153	However, the parts of the reviews are much more reliably identified as being positive or negative so we used these as the items of the test corpus." ></td>
	<td class="line x" title="112:153	In the experiments described below we use 10 subcorpora containing a total of 7982 reviews, distributed between product types as shown in Table 1." ></td>
	<td class="line x" title="113:153	Corpus/product type Reviews Monitors 683 Mobile phones 2317 Digital cameras 1705 MP3 players 779 Computer parts (CD-drives, motherboards) 308 Video cameras and lenses 361 Networking (routers, network cards) 350 Office equipment (copiers, multifunction devices, scanners) 611 Printers (laser, inkjet) 569 Computer peripherals (mice, keyboards, speakers) 457 Table 1." ></td>
	<td class="line x" title="114:153	Product types and sizes of the test corpora." ></td>
	<td class="line x" title="115:153	We constructed five of the corpora by combining smaller ones of 100250 reviews each (as indicated in parentheses in Table 1) in order to have reasonable amounts of data." ></td>
	<td class="line x" title="116:153	Each corpus has equal numbers of positive and negative reviews so we can derive upper bounds from the corpora (Section 5.2) by applying supervised classifiers." ></td>
	<td class="line x" title="117:153	We balance the corpora since (at least on this data) these classifiers perform less well with skewed class distributions5." ></td>
	<td class="line x" title="118:153	5.2 Baseline and Upper Bound Since the corpora are balanced with respect to sentiment orientation the nave (unsupervised) baseline is 50%." ></td>
	<td class="line x" title="119:153	We also produced an upper bound using Naive Bayes multinomial (NBm) and Support Vector Machine (SVM)6 classifiers with the NTU Sentiment Dictionary (Ku et al., 2006) vocabulary items as the feature set." ></td>
	<td class="line x" title="120:153	The dictionary contains 2809 items in the 'positive' part and 8273 items in the 'negative'." ></td>
	<td class="line x" title="121:153	We ran 5We have made this corpus publicly available at http:// www.informatics.sussex.ac.uk/users/tz21/coling08.zip 6We used WEKA 3.4.11 (http://www.cs.waikato.ac.nz/ml/ weka ) both classifiers in 10-fold stratified cross-validation mode, resulting in the accuracies shown in Table 2." ></td>
	<td class="line x" title="122:153	The macroaveraged accuracies across all 10 corpora are 82.78% (NBm) and 80.89% (SVM)." ></td>
	<td class="line x" title="123:153	Corpus Nbm (%) SVM (%) Monitors 86.21 83.87 Mobile phones 86.52 84.49 Digital cameras 82.27 82.04 MP3 players 82.64 79.43 Computer parts 81.10 79.47 Video cameras and lenses 83.05 84.16 Networking 77.65 75.35 Office equipment 82.13 80.00 Printers 81.33 79.57 Computer peripherals 84.86 80.48 Table 2." ></td>
	<td class="line x" title="124:153	Upper bound accuracies." ></td>
	<td class="line x" title="125:153	We also tried adding the negations and adverbials specified in Section 3 to the feature set, and this resulted in slightly improved accuracies, of 83.90% (Nbm) and 82.49% (SVM)." ></td>
	<td class="line x" title="126:153	An alternative approach would have been to automatically segment the reviews and then derive a feature set of a manageable size by setting a threshold on word frequencies; however the extra processing means that this is a less valid upper bound." ></td>
	<td class="line x" title="127:153	Another possible comparison could be with a version of Turney's (2002) sentiment classification method applied to Chinese." ></td>
	<td class="line x" title="128:153	However, the results would not be comparable since Turney's method would require the additional use of very large text corpus and the manual selection of positive and negative seed words." ></td>
	<td class="line x" title="129:153	5.3 Experiment 1 To be able to compare to the accuracy of the almost-unsupervised approach of Zagibalov & Carroll (2008), we ran our system using the seed  (good) for each corpus." ></td>
	<td class="line x" title="130:153	The results are shown in Table 3." ></td>
	<td class="line x" title="131:153	We compute precision, recall and F1 measure rather than just accuracy, since our classifier can omit some reviews whereas the supervised classifiers attempt to classify all reviews." ></td>
	<td class="line x" title="132:153	The macroaveraged F1 measure is 80.55, which beats the nave baseline by over 30 percentage points, and approaches the two upper bounds." ></td>
	<td class="line x" title="133:153	1077 Corpus Iter P R F1 Monitors 12 86.62 86.24 86.43 Mobile phones 11 90.15 89.68 89.91 Digital cameras 13 81.33 80.23 80.78 MP3 players 13 86.10 85.10 85.60 Computer parts 10 69.10 67.53 68.31 Video cameras and lenses 10 82.81 81.44 82.12 Networking 11 69.28 68.29 68.78 Office equipment 12 81.83 80.36 81.09 Printers 12 81.04 79.61 80.32 Computer peripherals 10 82.20 81.84 82.02 Macroaverage 81.05 80.03 80.54 Table 3." ></td>
	<td class="line x" title="134:153	Results with the single, manually chosen seed  (good) for each corpus." ></td>
	<td class="line x" title="135:153	5.4 Experiment 2 We then ran our full system, including the seed identifier." ></td>
	<td class="line x" title="136:153	Appendix A shows that for most of the corpora the algorithm found different (highly domain-salient) seeds." ></td>
	<td class="line x" title="137:153	Table 4 shows the results achieved." ></td>
	<td class="line x" title="138:153	Corpus Iter P R F1 Monitors 11 85.57 85.07 85.32 Mobile phones 10 92.63 92.19 92.41 Digital cameras 13 84.92 83.58 84.24 MP3 players 13 88.69 87.55 88.11 Computer parts 12 77.78 77.27 77.52 Video cameras and lenses 11 83.62 81.99 82.8 Networking 13 72.83 72.00 72.41 Office equipment 10 82.42 81.34 81.88 Printers 12 81.04 79.61 80.32 Computer peripherals 10 82.24 82.06 82.15 Macroaverage 83.17 82.27 82.72 Table 4." ></td>
	<td class="line x" title="139:153	Results with the seeds automatically identified for each corpus." ></td>
	<td class="line x" title="140:153	Across all 10 subcorpora, the improvement using automatically identified seed words compared with just using the seed good is significant (paired t-test, P<0.0001), and the F1 measure lies between the two upper bounds." ></td>
	<td class="line x" title="141:153	6 Conclusions and Future Work The unsupervised approach to seed words selection for sentiment classification presented in this paper produces results which in most cases are close to the results of supervised classifiers and to the previous almost-unsupervised approach: eight out of ten results showed improvement over the human selected seed word and three results outperformed the supervised approach, while three other results were less than 1% inferior to the supervised ones." ></td>
	<td class="line x" title="142:153	How does it happen that the chosen seed is usually (in our dataset  always) positive?" ></td>
	<td class="line x" title="143:153	We think that this happens due to the socially accepted norm of behaviour: as a rule one needs to be friendly to communicate with others." ></td>
	<td class="line x" title="144:153	This in turn defines linguistic means of expressing ideas  they will be at least slightly positive overall." ></td>
	<td class="line x" title="145:153	The higher prevalence of positive reviews has been observed previously: for example, in our corpus before we balanced it almost 80% of reviews were positive; Pang et al.(2002) constructed their move review corpus from an original dataset of 1301 positive and 752 negative reviews (63% positive)." ></td>
	<td class="line x" title="147:153	Ghose et al.(2007) quote typical examples of highly positive language used in the online marketplace." ></td>
	<td class="line x" title="149:153	We can make a preliminary conclusion that a relatively high frequency of positive words is determined by the usage of language that reflects the social behaviour of people." ></td>
	<td class="line x" title="150:153	In future work we intend to explore these issues of positivity of language use." ></td>
	<td class="line x" title="151:153	We will also apply our approach to other genres containing some quantity of evaluative language (for example newspaper articles), and see if it works equally well for languages other than Chinese." ></td>
	<td class="line x" title="152:153	It is also likely we can use a smaller set of negation words and adverbials to produce the seed lists." ></td>
	<td class="line x" title="153:153	Acknowledgements The first author is supported by the Ford Foundation International Fellowships Program." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-2004
The Power of Negative Thinking: Exploiting Label Disagreement in the Min-cut Classification Framework
Bansal, Mohit;Cardie, Claire;Lee, Lillian;"></td>
	<td class="line x" title="1:78	Coling 2008: Companion volume  Posters and Demonstrations, pages 1518 Manchester, August 2008 The power of negative thinking: Exploiting label disagreement in the min-cut classification framework Mohit Bansal Dept. of Computer Science & Engineering Indian Institute of Technology Kanpur mbansal47@gmail.com Claire Cardie and Lillian Lee Dept. of Computer Science Cornell University {cardie,llee}@cs.cornell.edu Abstract Treating classification as seeking minimum cuts in the appropriate graph has proven effective in a number of applications." ></td>
	<td class="line x" title="2:78	The power of this approach lies in its ability to incorporate label-agreement preferences among pairs of instances in a provably tractable way." ></td>
	<td class="line x" title="3:78	Label disagreement preferences are another potentially rich source of information, but prior NLP work within the minimum-cut paradigm has not explicitly incorporated it." ></td>
	<td class="line x" title="4:78	Here, we report on work in progress that examines several novel heuristics for incorporating such information." ></td>
	<td class="line x" title="5:78	Our results, produced within the context of a politically-oriented sentiment-classification task, demonstrate that these heuristics allow for the addition of label-disagreement information in a way that improves classification accuracy while preserving the efficiency guarantees of the minimum-cut framework." ></td>
	<td class="line x" title="6:78	1 Introduction Classification algorithms based on formulating the classification task as one of finding minimum s-t cuts in edge-weighted graphs  henceforth minimum cuts or min cuts  have been successfully employed in vision, computational biology, and natural language processing." ></td>
	<td class="line oc" title="7:78	Within NLP, applications include sentiment-analysis problems (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006) and content selection for text generation (Barzilay and Lapata, 2005)." ></td>
	<td class="line x" title="8:78	c2008." ></td>
	<td class="line x" title="9:78	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="10:78	Some rights reserved." ></td>
	<td class="line x" title="11:78	As a classification framework, the minimumcut approach is quite attractive." ></td>
	<td class="line x" title="12:78	First, it provides a principled, yet flexible mechanism for allowing problem-specific relational information  including several types of both hard and soft constraints  to influence a collection of classification decisions." ></td>
	<td class="line x" title="13:78	Second, in many important cases, such as when all the edge weights are non-negative, finding a minimum cut can be done in a provably efficient manner." ></td>
	<td class="line x" title="14:78	To date, however, researchers have restricted the semantics of the constraints mentioned above to encode pair-wise agreement information only." ></td>
	<td class="line x" title="15:78	There is a computational reason for this restriction: agreement and disagreement information are arguably most naturally expressed via positive and negative edge weights, respectively; but in general, the inclusion of even a relatively small number of negative edge weights makes finding a minimum cut NP-hard (McCormick et al., 2003)." ></td>
	<td class="line x" title="16:78	To avoid this computational issue, we propose several heuristics that encode disagreement information with non-negative edge weights." ></td>
	<td class="line x" title="17:78	We instantiate our approach on a sentiment-polarity classification task  determining whether individual conversational turns in U.S. Congressional floor debates support or oppose some given legislation." ></td>
	<td class="line x" title="18:78	Our preliminary results demonstrate promising improvements over the prior work of Thomas et al.(2006), who considered only the use of agreement information in this domain." ></td>
	<td class="line x" title="20:78	2 Method 2.1 Min-cut classification framework Binary classification problems are usually approached by considering each classification decision in isolation." ></td>
	<td class="line x" title="21:78	More formally, let Xtest = 15 {x1,x2,,xn}be the test instances, drawn from some universe X, and let C = {c1,c2} be the two possible classes." ></td>
	<td class="line x" title="22:78	Then, the usual approach can often be framed as labeling each xi according to some individual-preference function Ind:XCRfractur, such as the signed distance to the dividing hyperplane according to an SVM or the posterior class probability assigned by a Naive Bayes classifier." ></td>
	<td class="line x" title="23:78	But when it is difficult to accurately classify a particular xi in isolation, there is a key insight that can help: knowing that xi has the same label as an easily-categorized xj makes labeling xi easy." ></td>
	<td class="line x" title="24:78	Thus, suppose we also have an associationpreference function Assoc:XXRfractur expressing a reward for placing two items in the same class; an example might be the output of an agreement classifier or a similarity function." ></td>
	<td class="line x" title="25:78	Then, we can search for a classification function c(xi|Xtest)  note that all of Xtest can affect an instances label  that minimizes the total pining of the test items for the class they were not assigned to due to either their individual or associational preferences: summationdisplay i Ind(xi,c(xi|Xtest)) +  summationdisplay i,j:c(xi|Xtest)=c(xj|Xtest) Assoc(xi,xj), where c(xi|Xtest) is the class opposite to c(xi|Xtest), and the free parameter  regulates the emphasis on agreement information." ></td>
	<td class="line x" title="26:78	Solutions to the above minimization problem correspond to minimum s-t cuts in a certain graph, and if both Ind and Assoc are non-negative functions, then, surprisingly, minimum cuts can be found in polynomial time; see Kleinberg and Tardos (2006, Section 7.10) for details." ></td>
	<td class="line x" title="27:78	But, as mentioned above, allowing negative values makes finding a solution intractable in the general case." ></td>
	<td class="line x" title="28:78	2.2 Prior work discards some negative values The starting point for our work is Thomas et al.(2006) (henceforth TPL)." ></td>
	<td class="line x" title="30:78	The reason for this choice is that TPL used minimum-cut-based classification wherein signed distances to dividing SVM hyperplanes were employed to define Ind(x,c) and Assoc(x,xprime)." ></td>
	<td class="line x" title="31:78	It was natural to use SVMs, since association was determined by classification rather than similarity  specifically, categorizing references by one congressperson to another as reflecting agreement or not  but as a result, negative association-preferences (e.g., negative distance to a hyperplane) had to be accounted for." ></td>
	<td class="line x" title="32:78	We formalize TPLs approach at a high level as follows." ></td>
	<td class="line x" title="33:78	Let Indprime:XCRfractur and Assocprime:XXRfractur be initial individualand association-preference functions, such as the signed distances mentioned above." ></td>
	<td class="line x" title="34:78	TPL create two non-negative conversion functions f:Rfractur[0,1] and g:Rfractur[0,1], and then define Ind(xi,c) := f(Indprime(xi,c)) Assoc(xi,xj) := g(Assocprime(xi,xj)) so that an optimal classification can be found in polynomial time, as discussed above." ></td>
	<td class="line x" title="35:78	We omit the exact definitions of f and g in order to focus on what is important here: roughly speaking, f and g normalize values and handle outliers1, with the following crucial exception." ></td>
	<td class="line x" title="36:78	While negative initial individual preferences for one class can be translated into positive individual preferences for the other, there is no such mechanism for negative values of Assocprime; so TPL resort to defining g to be 0 for negative arguments." ></td>
	<td class="line x" title="37:78	They thus discard potentially key information regarding the strength of label disagreement preferences." ></td>
	<td class="line x" title="38:78	2.3 Encoding negative associations Instead of discarding the potentially crucial labeldisagreement information represented by negative Assocprime values, we propose heuristics that seek to incorporate this valuable information, but that keep Ind and Assoc non-negative (by piggy-backing off of TPLs pre-existing conversion-function strategy2) to preserve the min-cut-classification efficiency guarantees." ></td>
	<td class="line x" title="39:78	We illustrate our heuristics with a running example." ></td>
	<td class="line x" title="40:78	Consider a simplified setting with only two instances x1 and x2; f(z) = z; g(z) = 0 if z < 0, 1 otherwise; and Indprime values (numbers labeling left or right arrows in the diagrams below, e.g., Indprime(x1,c1) = .7) and Assocprime value (the -2 labeling the up-and-down arrow) as depicted here: [.7] x1 [.3] c1 arrowdblbothv[2] c2 [.6] x2 [.4] Then, the resulting TPL Ind and Assoc values are 1Thus, strictly speaking, f and g also depend on Indprime, Assocprime, and Xtest, but we suppress this dependence for notational compactness." ></td>
	<td class="line x" title="41:78	2Our approach also applies to definitions of f and g different from TPLs. 16 [.7] x1 [.3] c1 arrowdblbothv[0] c2 [.6] x2 [.4] Note that since the initial -2 association value is ignored, c(x1|Xtest) = c(x2|Xtest) = c1 appears to be a good classification according to TPL." ></td>
	<td class="line x" title="42:78	The Scale all up heuristic Rather than discard disagreement information, a simple strategy is to just scale up all initial association preferences by a sufficiently large positive constant N: Ind(xi,c) := f(Indprime(xi,c)) Assoc(xi,xj) := g(Assocprime(xi,xj) + N) For N = 3 in our example, we get [.7] x1 [.3] c1 arrowdblbothv[1] c2 [.6] x2 [.4] This heuristic ensures that the more negative the Assocprime value, the lower the cost of separating the relevant item pair (whereas TPL dont distinguish between negative Assocprime values)." ></td>
	<td class="line x" title="43:78	The heuristic below tries to be more proactive, forcing such pairs to receive different labels." ></td>
	<td class="line x" title="44:78	The SetTo heuristic We proceed through x1,x2, in order." ></td>
	<td class="line x" title="45:78	Each time we encounter an xi where Assocprime(xi,xj) < 0 for some j > i, we try to force xi and xj into different classes by altering the four relevant individual-preferences affecting this pair of instances, namely, f(Indprime(xi,c1)), f(Indprime(xi,c2)), f(Indprime(xj,c1)), and f(Indprime(xj,c2))." ></td>
	<td class="line x" title="46:78	Assume without loss of generality that the largest of these values is the first one." ></td>
	<td class="line x" title="47:78	If we respect that preference to put xi in c1, then according to the association-preference information, it follows that we should put xj in c2." ></td>
	<td class="line x" title="48:78	We can instantiate this chain of reasoning by setting Ind(xi,c1) := max(,f(Indprime(xi,c1))) Ind(xi,c2) := min(1,f(Indprime(xi,c2))) Ind(xj,c1) := min(1,f(Indprime(xj,c1))) Ind(xj,c2) := max(,f(Indprime(xj,c2))) for some constant   (.5,1], and making no change to TPLs definition of Assoc." ></td>
	<td class="line x" title="49:78	For  = .8 in our example, we get [.8] x1 [.2] c1 arrowdblbothv[0] c2 [.2] x2 [.8] Note that as we proceed through x1,x2, in order, some earlier changes may be undone." ></td>
	<td class="line x" title="50:78	The IncBy heuristic A more conservative version of the above heuristic is to increment and decrement the individual-preference values so that they are somewhat preserved, rather than completely replace them with fixed constants: Ind(xi,c1) := min(1,f(Indprime(xi,c1)) + ) Ind(xi,c2) := max(0,f(Indprime(xi,c2))) Ind(xj,c1) := max(0,f(Indprime(xj,c1))) Ind(xj,c2) := min(1,f(Indprime(xj,c2)) + ) For  = .1, our example becomes [.8] x1 [.2] c1 arrowdblbothv[0] c2 [.5] x2 [.5] 3 Evaluation For evaluation, we adopt the sentimentclassification problem tackled by TPL: classifying speech segments (individual conversational turns) in a U.S. Congressional floor debate as to whether they support or oppose the legislation under discussion." ></td>
	<td class="line x" title="51:78	TPL describe many reasons why this is an important problem." ></td>
	<td class="line x" title="52:78	For our purposes, this task is also very convenient because all of TPLs computed raw and converted Indprime and Assocprime data are publicly available at www.cs.cornell.edu/home/llee/data/convote.html." ></td>
	<td class="line x" title="53:78	Thus, we used their calculated values to implement our algorithms as well as to reproduce their original results.3 One issue of note is that TPL actually inferred association preferences between speakers, not speech segments." ></td>
	<td class="line x" title="54:78	We do the same when applying SetTo or IncBy to a pair {xi,xj} by considering the average of f(Indprime(xk,c1)) over all xk uttered by the speaker of xi, instead of just f(Indprime(xi,c1))." ></td>
	<td class="line x" title="55:78	The other three relevant individual values are treated similarly." ></td>
	<td class="line x" title="56:78	We also make appropriate modifications (according to SetTo and IncBy) to the individual preferences of all such xk simultaneously, not just xi, and similarly for xj." ></td>
	<td class="line x" title="57:78	A related issue is that TPL assume that all speech segments by the same speaker should have the same label." ></td>
	<td class="line x" title="58:78	To make experimental comparisons meaningful, we follow TPL in considering two different instantiations of this assumption." ></td>
	<td class="line x" title="59:78	In segment-based classification, Assoc(xi,xj) is set to an arbitrarily high positive constant if the same speaker uttered both xi and xj." ></td>
	<td class="line x" title="60:78	In speaker-based classification, Indprime(xi,c) is produced by running 3For brevity, we omit TPLs high-threshold variants." ></td>
	<td class="line x" title="61:78	17  60  62  64  66  68  70  72  74  76  78 SetTo(.6)SVM SetTo(1) IncBy(.25)IncBy(.15)TPL IncBy(.05)Scale all up SetTo(.8) percent correct ALGORITHMS Test-set classification accuracies, using held-out parameter estimation segment-based, test speaker-based, test best TPL, test Figure 1: Experimental results." ></td>
	<td class="line x" title="62:78	SVM: classification using only individual-preference information." ></td>
	<td class="line x" title="63:78	Values of  are indicated in parentheses next to the relevant algorithm names." ></td>
	<td class="line x" title="64:78	an SVM on the concatenation of all speeches uttered by xis speaker." ></td>
	<td class="line x" title="65:78	Space limits preclude inclusion of further details; please see TPL for more information." ></td>
	<td class="line x" title="66:78	3.1 Results and future plans The association-emphasis parameter  was trained on held-out data, with ties broken in favor of the largest  in order to emphasize association information." ></td>
	<td class="line x" title="67:78	We used Andrew Goldbergs HIPR code (http://avglab.com/andrew/soft.html) to compute minimum cuts." ></td>
	<td class="line x" title="68:78	The resultant test-set classification accuracies are presented in Figure 1." ></td>
	<td class="line x" title="69:78	We see that Scale all up performs worse than TPL, but the more proactive heuristics (SetTo, IncBy) almost always outperform TPL on segment-based classification, sometimes substantially so, and outperform TPL on speaker-based classification for half of the variations." ></td>
	<td class="line x" title="70:78	We therefore conclude that label disagreement information is indeed valuable; and that incorporating label disagreement information on top of the (positive) label agreement information that TPL leveraged can be achieved using simple heuristics; and that good performance enhancements result without any concomitant significant loss of efficiency." ></td>
	<td class="line x" title="71:78	These results are preliminary, and the divergence in behaviors between different heuristics in different settings requires investigation." ></td>
	<td class="line x" title="72:78	Additional future work includes investigating more sophisticated (but often therefore less tractable) formalisms for joint classification; and looking at whether approximation algorithms for finding minimum cuts in graphs with negative edge capacities can be effective." ></td>
	<td class="line x" title="73:78	Acknowledgments We thank Jon Kleinberg and the reviewers for helpful comments." ></td>
	<td class="line x" title="74:78	Portions of this work were done while the first author was visiting Cornell University." ></td>
	<td class="line x" title="75:78	This paper is based upon work supported in part by the National Science Foundation under grant nos." ></td>
	<td class="line x" title="76:78	IIS0329064, BCS-0624277, and IIS-0535099, a Cornell University Provosts Award for Distinguished Scholarship, a Yahoo!" ></td>
	<td class="line x" title="77:78	Research Alliance gift, an Alfred P. Sloan Research Fellowship, and by DHS grant N0014-07-1-0152." ></td>
	<td class="line x" title="78:78	Any opinions, findings, and conclusions or recommendations expressed are those of the authors and do not necessarily reflect the views or official policies, either expressed or implied, of any sponsoring institutions, the U.S. government, or any other entity." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D08-1004
Modeling Annotators: A Generative Approach to Learning from Annotator Rationales
Zaidan, Omar F.;Eisner, Jason M.;"></td>
	<td class="line x" title="1:277	Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 3140, Honolulu, October 2008." ></td>
	<td class="line x" title="2:277	c2008 Association for Computational Linguistics Modeling Annotators: A Generative Approach to Learning from Annotator Rationales Omar F. Zaidan and Jason Eisner Dept. of Computer Science, Johns Hopkins University Baltimore, MD 21218, USA {ozaidan,jason}@cs.jhu.edu Abstract A human annotator can provide hints to a machine learner by highlighting contextual rationales for each of his or her annotations (Zaidan et al., 2007)." ></td>
	<td class="line x" title="3:277	How can one exploit this side information to better learn the desired parameters ?" ></td>
	<td class="line x" title="4:277	We present a generative model of how a given annotator, knowing the true , stochastically chooses rationales." ></td>
	<td class="line x" title="5:277	Thus, observing the rationales helps us infer the true ." ></td>
	<td class="line nc" title="6:277	We collect substring rationales for a sentiment classification task (Pang and Lee, 2004) and use them to obtain significant accuracy improvements for each annotator." ></td>
	<td class="line n" title="7:277	Our new generative approach exploits the rationales more effectively than our previous masking SVM approach." ></td>
	<td class="line x" title="8:277	It is also more principled, and could be adapted to help learn other kinds of probabilistic classifiers for quite different tasks." ></td>
	<td class="line x" title="9:277	1 Background Many recent papers aim to reduce the amount of annotated data needed to train the parameters of a statistical model." ></td>
	<td class="line x" title="10:277	Well-known paradigms include active learning, semi-supervised learning, and either domain adaptation or cross-lingual transfer from existing annotated data." ></td>
	<td class="line x" title="11:277	A rather different paradigm is to change the actual task that is given to annotators, giving them a greater hand in shaping the learned classifier." ></td>
	<td class="line x" title="12:277	After all, human annotators themselves are more than just black-box classifiers to be run on training data." ></td>
	<td class="line x" title="13:277	They possess some introspective knowledge about their own classification procedure." ></td>
	<td class="line x" title="14:277	The hope is to mine this knowledge rapidly via appropriate questions and use it to help train a machine classifier." ></td>
	<td class="line x" title="15:277	How to do this, however, is still being explored." ></td>
	<td class="line x" title="16:277	1.1 Hand-crafted rules An obvious option is to have the annotators directly express their knowledge by hand-crafting rules." ></td>
	<td class="line x" title="17:277	This This work was supported by National Science Foundation grant No. 0347822 and the JHU WSE/APL Partnership Fund." ></td>
	<td class="line x" title="18:277	Special thanks to Christine Piatko for many useful discussions." ></td>
	<td class="line x" title="19:277	approach remains data-driven if the annotators repeatedly refine their system against a corpus of labeled or unlabeled examples." ></td>
	<td class="line x" title="20:277	This achieves high performance in some domains, such as NP chunking (Brill and Ngai, 1999), but requires more analytical skill from the annotators." ></td>
	<td class="line x" title="21:277	One empirical study (Ngai and Yarowsky, 2000) found that it also required more annotation time than active learning." ></td>
	<td class="line x" title="22:277	1.2 Feature selection by humans More recent work has focused on statistical classifiers." ></td>
	<td class="line x" title="23:277	Training such classifiers faces the credit assignment problem. Given a training examplexwith many features, which features are responsible for its annotated class y?" ></td>
	<td class="line x" title="24:277	It may take many training examples to distinguish useful vs. irrelevant features.1 To reduce the number of training examples needed, one can ask annotators to examine or propose some candidate features." ></td>
	<td class="line x" title="25:277	This is possible even for the very large feature sets that are typically used in NLP." ></td>
	<td class="line x" title="26:277	In document classification, Raghavan et al.(2006) show that feature selection by an oracle could be helpful, and that humans are both rapid and reasonably good at distinguishing highly usefuln-gram features from randomly chosen ones, even when viewing these n-grams out of context." ></td>
	<td class="line x" title="28:277	Druck et al.(2008) show annotators some features f from a fixed feature set, and ask them to choose a class labely such thatp(y|f) is as high as possible." ></td>
	<td class="line x" title="30:277	Haghighi and Klein (2006) do the reverse: for each class label y, they ask the annotators to propose a few prototypical featuresf such thatp(y|f) is as high as possible." ></td>
	<td class="line x" title="31:277	1.3 Feature selection in context The above methods consider features out of context." ></td>
	<td class="line x" title="32:277	An annotator might have an easier time examining 1Most NLP systems use thousands or millions of features, because it is helpful to include lexical features over a large vocabulary, often conjoined with lexical or non-lexical context." ></td>
	<td class="line x" title="33:277	31 features in context to recognize whether they appear relevant." ></td>
	<td class="line x" title="34:277	This is particularly true for features that are only modestly or only sometimes helpful, which may be abundant in NLP tasks." ></td>
	<td class="line x" title="35:277	Thus, Raghavan et al.(2006) propose an active learning method in which, while classifying a training document, the annotator also identifies some features of that document as particularly relevant." ></td>
	<td class="line x" title="37:277	E.g., the annotator might highlight particular unigrams as he or she reads the document." ></td>
	<td class="line x" title="38:277	In their proposal, a feature that is highlighted in any document is assumed to be globally more relevant." ></td>
	<td class="line x" title="39:277	Its dimension in feature space is scaled by a factor of 10 so that this feature has more influence on distances or inner products, and hence on the learned classifier." ></td>
	<td class="line x" title="40:277	1.4 Concerns about marking features Despite the success of the above work, we have several concerns about asking annotators to identify globally relevant features." ></td>
	<td class="line x" title="41:277	First, a feature in isolation really does not have a well-defined worth." ></td>
	<td class="line x" title="42:277	A feature may be useful only in conjunction with other features,2 or be useful only to the extent that other correlated features are not selected to do the same work." ></td>
	<td class="line x" title="43:277	Second, it is not clear how an annotator would easily view and highlight features in context, except for the simplest feature sets." ></td>
	<td class="line x" title="44:277	In the phrase Apple shares up 3%, there may be several features that fire on the substring Appleresponding to the string Apple, its case-invariant form apple, its lemma apple(which would also respond to apples), its context-dependent sense Apple2, its part of speech noun, etc. How does the annotator indicate which of these features are relevant?" ></td>
	<td class="line x" title="45:277	Third, annotating features is only appropriate when the feature set can be easily understood by a human." ></td>
	<td class="line x" title="46:277	This is not always the case." ></td>
	<td class="line x" title="47:277	It would be hard for annotators to read, write, or evaluate a description of a complex syntactic configuration in NLP or a convolution filter in machine vision." ></td>
	<td class="line x" title="48:277	Fourth, traditional annotation efforts usually try to remain agnostic about the machine learning methods 2For example, a linear classifier can learn that most training examples satisfyAB by settingA =5 andAB = +5, but this solution requires selecting bothAandAB as features." ></td>
	<td class="line x" title="49:277	More simply, a polynomial kernel can consider the conjunction AB only if both A and B are selected as features." ></td>
	<td class="line x" title="50:277	and features to be used." ></td>
	<td class="line x" title="51:277	The projects cost is justified by saying that the annotations will be reused by many researchers (perhaps in a shared task), who are free to compete on how they tackle the learning problem." ></td>
	<td class="line x" title="52:277	Unfortunately, feature annotation commits to a particular feature set at annotation time." ></td>
	<td class="line x" title="53:277	Subsequent research cannot easily adjust the definition of the features, or obtain annotation of new features." ></td>
	<td class="line x" title="54:277	2 Annotating Rationales To solve these problems, we propose that annotators should not select features but rather mark relevant portions of the example." ></td>
	<td class="line x" title="55:277	In earlier work (Zaidan et al., 2007), we called these markings rationales. For example, when classifying a movie review as positive or negative, the annotator would also highlight phrases that supported that judgment." ></td>
	<td class="line x" title="56:277	Figure 1 shows two such rationales." ></td>
	<td class="line x" title="57:277	A multi-annotator timing study (Zaidan et al., 2007) found that highlighting rationale phrases while reading movie reviews only doubled annotation time, although annotators marked 511 rationale substrings in addition to the simple binary class." ></td>
	<td class="line x" title="58:277	The benefit justified the extra time." ></td>
	<td class="line x" title="59:277	Furthermore, much of the benefit could have been obtained by giving rationales for only a fraction of the reviews." ></td>
	<td class="line x" title="60:277	In the visual domain, when classifying an image as containing a zoo, the annotator might circle some animals or cages and the sign reading Zoo. The Peekaboom game (von Ahn et al., 2006) was in fact built to elicit such approximate yet relevant regions of images." ></td>
	<td class="line x" title="61:277	Further scenarios were discussed in (Zaidan et al., 2007): rationale annotation for named entities, linguistic relations, or handwritten digits." ></td>
	<td class="line x" title="62:277	Annotating rationales does not require the annotator to think about the feature space, nor even to know anything about it." ></td>
	<td class="line x" title="63:277	Arguably this makes annotation easier and more flexible." ></td>
	<td class="line x" title="64:277	It also preserves the reusability of the annotated data." ></td>
	<td class="line x" title="65:277	Anyone is free to reuse our collected rationales (section 4) to aid in learning a classifier with richer features, or a different kind of classifier altogether, using either our procedures or novel procedures." ></td>
	<td class="line x" title="66:277	3 Modeling Rationale Annotations As rationales are more indirect than explicit features, they present a trickier machine learning problem." ></td>
	<td class="line x" title="67:277	32 We wish to learn the parameters  of some classifier." ></td>
	<td class="line x" title="68:277	How can the annotators rationales help us to do this without many training examples?" ></td>
	<td class="line x" title="69:277	We will have to exploit a presumed relationship between the rationales and the optimal value of  (i.e., the value that we would learn on an infinite training set)." ></td>
	<td class="line x" title="70:277	This paper exploits an explicit, parametric model of that relationship." ></td>
	<td class="line x" title="71:277	The models parameters  are intended to capture what that annotator is doing when he or she marks rationales." ></td>
	<td class="line x" title="72:277	Most importantly, they capture how he or she is influenced by the true ." ></td>
	<td class="line x" title="73:277	Given this, our learning method will prefer values of  that would adequately explain the rationales (as well as the training classifications)." ></td>
	<td class="line x" title="74:277	3.1 A generative approach For concreteness, we will assume that the task is document classification." ></td>
	<td class="line x" title="75:277	Our training data consists ofntriples{(x1,y1,r1),,(xn,yn,rn)}), wherexi is a document, yi is its annotated class, and ri is its rationale markup." ></td>
	<td class="line x" title="76:277	At test time we will have to predict yn+1 from xn+1, without any rn+1." ></td>
	<td class="line x" title="77:277	We propose to jointly choose parameter vectors  and  to maximize the following regularized conditional likelihood:3 nproductdisplay i=1 p(yi,ri|xi,,)pprior(,) (1) def= nproductdisplay i=1 p(yi|xi)p(ri|xi,yi,)pprior(,) Here we are trying to model all the annotations, both yi and ri." ></td>
	<td class="line x" title="78:277	The first factor predicts yi using an ordinary probabilistic classifier p, while the novel second factor predicts ri using a model p of how annotators generate the rationale annotations." ></td>
	<td class="line x" title="79:277	The crucial point is that the second factor depends on  (since ri is supposed to reflect the relation between xi and yi that is modeled by )." ></td>
	<td class="line x" title="80:277	As a result, the learner has an incentive to modify  in a way that increases the second factor, even if this somewhat decreases the first factor on training data.4 3It would be preferable to integrate out  (and even ), but more difficult." ></td>
	<td class="line x" title="81:277	4Interestingly, even examples where the annotation yi is wrong or unhelpful can provide useful information about  via the pair (yi,ri)." ></td>
	<td class="line x" title="82:277	Two annotators marking the same movie review might disagree on whether it is overall a positive or negaAfter training, one should simply use the first factor p(y|x) to classify test documents x. The second factor is irrelevant for test documents, since they have not been annotated with rationales r. The second factor may likewise be omitted for any training documents i that have not been annotated with rationales, as there is no ri to predict in those cases." ></td>
	<td class="line x" title="83:277	In the extreme case where no documents are annotated with rationales, equation (1) reduces to the standard training procedure." ></td>
	<td class="line x" title="84:277	3.2 Noisy channel design of rationale models Like ordinary class annotations, rationale annotations present us with a credit assignment problem, albeit a smaller one that is limited to features that fire in the vicinity of the rationale r. Some of these -features were likely responsible for the classification y and hence triggered the rationale." ></td>
	<td class="line x" title="85:277	Other such -features were just innocent bystanders." ></td>
	<td class="line x" title="86:277	Thus, the interesting part of our model is p(r | x,y,), which models the rationale annotation process." ></td>
	<td class="line x" title="87:277	The rationales r reflect , but in noisy ways." ></td>
	<td class="line x" title="88:277	Taking this noisy channel idea seriously, p(r | x,y,) should consider two questions when assessing whether r is a plausible set of rationales given ." ></td>
	<td class="line x" title="89:277	First, it needs a language model of rationales: does r consist of rationales that are well-formed a priori, i.e., before  is considered?" ></td>
	<td class="line x" title="90:277	Second, it needs a channel model: does r faithfully signal the features of  that strongly support classifying x as y?" ></td>
	<td class="line x" title="91:277	If a feature contributes heavily to the classification of document x as class y, then the channel model should tell us which parts of document x tend to be highlighted as a result." ></td>
	<td class="line x" title="92:277	The channel model must know about the particular kinds of features that are extracted by f and scored by ." ></td>
	<td class="line x" title="93:277	Suppose the feature not . . ." ></td>
	<td class="line x" title="94:277	gripping,5 with weighth, is predictive of the annotated classy." ></td>
	<td class="line x" title="95:277	This raises the probabilities of the annotators highlighting each of various words, or combinations of words, in a phrase like not the most gripping banquet on film." ></td>
	<td class="line x" title="96:277	The channel model parameters in  tive reviewbut the second factor still allows learning positive features from the first annotators positive rationales, and negative features from the second annotators negative rationales." ></td>
	<td class="line x" title="97:277	5Our current experiments use only unigram features, to match past work, but we use this example to outline how our approach generalizes to complex linguistic (or visual) features." ></td>
	<td class="line x" title="98:277	33 should specify how much each of these probabilities is raised, based on the magnitude of h  R, the class y, and the fact that the feature is an instance of the template <Neg> . . .<Adjective>." ></td>
	<td class="line x" title="99:277	(Thus,  has no parameters specific to the word gripping; it is a low-dimensional vector that only describes the annotators general style in translating  into r.)" ></td>
	<td class="line x" title="100:277	The language model, however, is independent of the feature set ." ></td>
	<td class="line x" title="101:277	It models what rationales tend to look like in the input domaine.g., documents or images." ></td>
	<td class="line x" title="102:277	In the document case,  should describe: How frequent and how long are typical rationales?" ></td>
	<td class="line x" title="103:277	Do their edges tend to align with punctuation or major syntactic boundaries in x?" ></td>
	<td class="line x" title="104:277	Are they rarer in the middle of a document, or in certain documents?6 Thanks to the language model, we do not need to posit high features to explain every word in a rationale." ></td>
	<td class="line x" title="105:277	The language model can explain away some words as having been highlighted only because this annotator prefers not to end a rationale in midphrase, or prefers to sweep up close-together features with a single long rationale rather than many short ones." ></td>
	<td class="line x" title="106:277	Similarly, the language model can help explain why some words, though important, might not have been included in any rationale of r. If there are multiple annotators, one can learn different  parameters for each annotator, reflecting their different annotation styles.7 We found this to be useful (section 8.2)." ></td>
	<td class="line x" title="107:277	We remark that our generative modeling approach (equation (1)) would also apply if r were not rationale markup, but some other kind of so-called side information, such as the feature annotations discussed in section 1." ></td>
	<td class="line x" title="108:277	For example, Raghavan et al.(2006) assume that if feature h is relevanta bi6Our current experiments do not model this last point." ></td>
	<td class="line x" title="110:277	However, we imagine that if the document only has a few -features that support the classification, the annotator will probably mark most of them, whereas if such features are abundant, the annotator may lazily mark only a few of the strongest ones." ></td>
	<td class="line x" title="111:277	A simple approach would equip  with a different bias or threshold parameter x for each rationale training document x, to modulate the a priori probability of marking a rationale in x. By fitting this bias parameter, we deduce how lazy the annotator was (for whatever reason) on document x. If desired, a prior on x could consider whether x has many strong -features, whether the annotator has recently had a coffee break, etc. 7Given insufficient rationale data to recover some annotatorswell, one could smooth using data from other annotators." ></td>
	<td class="line x" title="112:277	But in our situation,  had relatively few parameters to learn." ></td>
	<td class="line x" title="113:277	nary distinctioniff it was selected in at least one document." ></td>
	<td class="line x" title="114:277	But it might be more informative to observe that h was selected in 3 of the 10 documents where it appeared, and to predict this via a model p(3 of 10|h), wheredescribes (e.g.) how to derive a binomial parameter nonlinearly from h. This approach would not how oftenhwas marked and infer how relevant is feature h (i.e., infer h)." ></td>
	<td class="line x" title="115:277	In this case, p is a simple channel that transforms relevant features into direct indicators of the feature." ></td>
	<td class="line x" title="116:277	Our side information merely requires a more complex transformationfrom relevant features into wellformed rationales, modulated by documents." ></td>
	<td class="line oc" title="117:277	4 Experimental Data: Movie Reviews In Zaidan et al.(2007), we introduced the Movie Review Polarity Dataset Enriched with Annotator Rationales.8 It is based on the dataset of Pang and Lee (2004),9 which consists of 1000 positive and 1000 negative movie reviews, tokenized and divided into 10 folds (F0F9)." ></td>
	<td class="line x" title="119:277	All our experiments use F9 as their final blind test set." ></td>
	<td class="line x" title="120:277	The enriched dataset adds rationale annotations produced by an annotator A0, who annotated folds F0F8 of the movie review set with rationales (in the form of textual substrings) that supported the goldstandard classifications." ></td>
	<td class="line x" title="121:277	We will use A0s data to determine the improvement of our method over a (log-linear) baseline model without rationales." ></td>
	<td class="line x" title="122:277	We also use A0 to compare against the masking SVM method and SVM baseline of Zaidan et al.(2007)." ></td>
	<td class="line x" title="124:277	Since  can be tuned to a particular annotator, we would also like to know how well this works with data from annotators other than A0." ></td>
	<td class="line x" title="125:277	We randomly selected 100 reviews (50 positive and 50 negative) and collected both class and rationale annotation data from each of six new annotators A3A8,10 following the same procedures as (Zaidan et al., 2007)." ></td>
	<td class="line x" title="126:277	We report results using only data from A3A5, since we used the data from A6A8 as development data in the early stages of our work." ></td>
	<td class="line x" title="127:277	We use this new rationale-enriched dataset8 to determine if our method works well across annotators." ></td>
	<td class="line x" title="128:277	We will only be able to carry out that comparison 8Available at http://cs.jhu.edu/ozaidan/rationales." ></td>
	<td class="line x" title="129:277	9Polarity dataset version 2.0." ></td>
	<td class="line x" title="130:277	10We avoid annotator names A1A2, which were already used in (Zaidan et al., 2007)." ></td>
	<td class="line x" title="131:277	34 Figure 1: Rationales as sequence annotation: the annotator highlighted two textual segments as rationales for a positive class." ></td>
	<td class="line x" title="132:277	Highlighted words in vectorx are tagged I in vectorr, and other words are tagged O. The figure also shows some -features." ></td>
	<td class="line x" title="133:277	For instance, gO(,)-I is a count of O-I transitions that occur with a comma as the left word." ></td>
	<td class="line x" title="134:277	Notice also that grel is the sum of the underlined values." ></td>
	<td class="line x" title="135:277	at small training set sizes, due to limited data from A3A8." ></td>
	<td class="line x" title="136:277	The larger A0 dataset will still allow us to evaluate our method on a range of training set sizes." ></td>
	<td class="line x" title="137:277	5 Detailed Models 5.1 Modeling class annotations with p We define the basic classifierp in equation (1) to be a standard conditional log-linear model: p(y|x) def= exp( vectorvectorf(x,y)) Z(x) def= u(x,y) Z(x) (2) where vectorf() extracts a feature vector from a classified document, vector are the corresponding weights of those features, and Z(x) def=summationtextyu(x,y) is a normalizer." ></td>
	<td class="line oc" title="138:277	We use the same set of binary features as in previous work on this dataset (Pang et al., 2002; Pang and Lee, 2004; Zaidan et al., 2007)." ></td>
	<td class="line x" title="139:277	Specifically, let V = {v1,,v17744} be the set of word types with count4 in the full 2000-document corpus." ></td>
	<td class="line x" title="140:277	Define fh(x,y) to be y if vh appears at least once in x, and 0 otherwise." ></td>
	<td class="line x" title="141:277	Thus R17744, and positive weights in favor class labely = +1 and equally discourage y =1, while negative weights do the opposite." ></td>
	<td class="line x" title="142:277	This standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales." ></td>
	<td class="line x" title="143:277	Future work should consider more complex features and how they are signaled by rationales, as discussed in section 3.2." ></td>
	<td class="line x" title="144:277	5.2 Modeling rationale annotations with p The rationales collected in this task are textual segments of a document to be classified." ></td>
	<td class="line x" title="145:277	The document itself is a word token sequencevectorx = x1,,xM." ></td>
	<td class="line x" title="146:277	We encode its rationales as a corresponding tag sequence vectorr = r1,,rM, as illustrated in Figure 1." ></td>
	<td class="line x" title="147:277	Here rm {I,O} according to whether the token xm is in a rationale (i.e.,xm was at least partly highlighted) or outside all rationales." ></td>
	<td class="line x" title="148:277	x1 and xM are special boundary symbols, tagged with O. We predict the full tag sequence vectorr at once using a conditional random field (Lafferty et al., 2001)." ></td>
	<td class="line x" title="149:277	A CRF is just another conditional log-linear model: p(r|x,y,vector) def= exp( vectorvectorg(r,x,y,vector)) Z(x,y,vector) def=u(r,x,y,vector) Z(x,y,vector) where vectorg() extracts a feature vector, vector are the corresponding weights of those features, and Z(x,y,vector) def=summationtextru(r,x,y,vector) is a normalizer." ></td>
	<td class="line x" title="150:277	As usual for linear-chain CRFs, vectorg() extracts two kinds of features: first-order emission features that relate rm to (xm,y,), and second-order transition features that relate rm to rm1 (although some of these also look at x)." ></td>
	<td class="line x" title="151:277	These two kinds of features respectively capture the channel model and language model of section 3.2." ></td>
	<td class="line x" title="152:277	The former says rm is I because xm is associated with a relevant -feature." ></td>
	<td class="line x" title="153:277	The latter says rm is I simply because it is next to another I. 5.3 Emission -features (channel model) Recall that our -features (at present) correspond to unigrams." ></td>
	<td class="line x" title="154:277	Given (vectorx,y,vector), let us say that a unigram w  vectorx is relevant, irrelevant, or anti-relevant if yw is respectivelygreatermuch0,0, orlessmuch0." ></td>
	<td class="line x" title="155:277	That is, w is relevant if its presence in x strongly supports the annotated class y, and anti-relevant if its presence strongly supports the opposite classy. 35 Figure 2: The function family Bs in equation (3), shown for s  {10,2,2,10}." ></td>
	<td class="line x" title="156:277	We would like to learn the extent rel to which annotators try to include relevant unigrams in their rationales, and the (usually lesser) extent antirel to which they try to exclude anti-relevant unigrams." ></td>
	<td class="line x" title="157:277	This will help us infer vector from the rationales." ></td>
	<td class="line x" title="158:277	The details are as follows." ></td>
	<td class="line x" title="159:277	rel and antirel are the weights of two emission features extracted byvectorg: grel(vectorx,y,vectorr,vector) def= Msummationdisplay m=1 I(rm = I)B10(yxm) gantirel(vectorx,y,vectorr,vector) def= Msummationdisplay m=1 I(rm = I)B10(yxm) Here I() denotes the indicator function, returning 1 or 0 according to whether its argument is true or false." ></td>
	<td class="line x" title="160:277	Relevance and negated anti-relevance are respectively measured by the differentiable nonlinear functions B10 and B10, which are defined by Bs(a) = (log(1 + exp(as))log(2))/s (3) and graphed in Figure 2." ></td>
	<td class="line x" title="161:277	Sample values of B10 and grel are shown in Figure 1." ></td>
	<td class="line x" title="162:277	How does this work?" ></td>
	<td class="line x" title="163:277	The grel feature is a sum over all unigrams in the document vectorx." ></td>
	<td class="line x" title="164:277	It does not fire strongly on the irrelevant or anti-relevant unigrams, since B10 is close to zero there.11 But it fires positively on relevant unigramsw if they are tagged with I, and the strength of such firing increases approximately linearly withw. Since the weightrel > 0 in practice, this means that raising a relevant unigrams w (if y = +1) will proportionately raise its logodds of being tagged with I. Symmetrically, since antirel > 0 in practice, lowering an anti-relevant unigrams w (if y = +1) will proportionately lower 11B10 sets the threshold for relevance to be about 0." ></td>
	<td class="line x" title="165:277	One could also include versions of the grel feature that set a higher threshold, using B10(yxmthreshold)." ></td>
	<td class="line x" title="166:277	its log-odds of being tagged with I, though not necessarily at the same rate as for relevant unigrams.12 Should  also include traditional CRF emission features, which would recognize that particular words like great tend to be tagged as I?" ></td>
	<td class="line x" title="167:277	No! Such features would undoubtedly do a better job predicting the rationales and hence increasing equation (1)." ></td>
	<td class="line x" title="168:277	However, crucially, our true goal is not to predict the rationales but to recover the classifier parameters ." ></td>
	<td class="line x" title="169:277	Thus, if great tends to be highlighted, then the model should not be permitted to explain this directly by increasing some feature great, but only indirectly by increasing great." ></td>
	<td class="line x" title="170:277	We therefore permit our rationale prediction model to consider only the two emission features grel and gantirel, which see the words in vectorx only through their -values." ></td>
	<td class="line x" title="171:277	5.4 Transition -features (language model) Annotators highlight more than just the relevant unigrams." ></td>
	<td class="line x" title="172:277	(After all, they arent told that our current -features are unigrams.)" ></td>
	<td class="line x" title="173:277	They tend to mark full phrases, though perhaps taking care to exclude antirelevant portions." ></td>
	<td class="line x" title="174:277	models these phrases shape, via weights for several language model features." ></td>
	<td class="line x" title="175:277	Most important are the 4 traditional CRF tag transition features gO-O,gO-I,gI-I,gI-O." ></td>
	<td class="line x" title="176:277	For example, gO-I counts the number of O-to-I transitions in vectorr (see Figure 1)." ></td>
	<td class="line x" title="177:277	Other things equal, an annotator with high O-I is predicted to have many rationales per 1000 words." ></td>
	<td class="line x" title="178:277	And if I-I is high, rationales are predicted to be long phrases (including more irrelevant unigrams around or between the relevant ones)." ></td>
	<td class="line x" title="179:277	We also learn more refined versions of these features, which consider how the transition probabilities are influenced by the punctuation and syntax of the document vectorx (independent of vector)." ></td>
	<td class="line x" title="180:277	These refined features are more specific and hence more sparsely trained." ></td>
	<td class="line x" title="181:277	Their weights reflect deviations from the simpler, backed-off transition features such as gO-I." ></td>
	<td class="line x" title="182:277	(Again, see Figure 1 for examples.)" ></td>
	<td class="line x" title="183:277	Conditioning on left word." ></td>
	<td class="line x" title="184:277	A feature of the form gt1(v)-t2 is specified by a pair of tag types t1,t2  {I,O}and a vocabulary word type v. It counts the 12If the two rates are equal (rel = antirel), we get a simpler model in which the log-odds change exactly linearly withw for each w, regardless of ws relevance/irrelevance/anti-relevance." ></td>
	<td class="line x" title="185:277	This follows from the fact thatBs(a)+Bs(a) simplifies toa." ></td>
	<td class="line x" title="186:277	36 number of times an t1t2 transition occurs invectorr conditioned on v appearing as the first of the two word tokens where the transition occurs." ></td>
	<td class="line x" title="187:277	Our experiments include gt1(v)-t2 features that tie I-O and O-I transitions to the 4 most frequent punctuation marks v (comma, period, ?, !)." ></td>
	<td class="line x" title="188:277	Conditioning on right word." ></td>
	<td class="line x" title="189:277	A feature gt1-t2(v) is similar, but v must appear as the second of the two word tokens where the transition occurs." ></td>
	<td class="line x" title="190:277	Again here, we use gt1-t2(v) features that tie I-O and O-I transitions to the four punctuation marks mentioned above." ></td>
	<td class="line x" title="191:277	We also include five features that tie O-I transitions to the words no, not, so, very, and quite, since in our development data, those words were more likely than others to start rationales.13 Conditioning on syntactic boundary." ></td>
	<td class="line x" title="192:277	We parsed each rationale-annotated training document (no parsing is needed at test time).14 We then marked each word bigram x1-x2 with three nonterminals: NEnd is the nonterminal of the largest constituent that contains x1 and not x2, NStart is the nonterminal of the largest constituent that contains x2 and not x1, and NCross is the nonterminal of the smallest constituent that contains both x1 and x2." ></td>
	<td class="line x" title="193:277	For a nonterminalN and pair of tag types (t1,t2), we define three features, gt1-t2/E=N, gt1-t2/S=N, and gt1-t2/C=N, which count the number of times a t1-t2 transition occurs in vectorr with N matching the NEnd, NStart, or NCross nonterminal, respectively." ></td>
	<td class="line x" title="194:277	Our experiments include these features for 11 common nonterminal types N (DOC, TOP, S, SBAR, FRAG, PRN, NP, VP, PP, ADJP, QP)." ></td>
	<td class="line x" title="195:277	6 Training: Joint Optimization of  and  To train our model, we use L-BFGS to locally maximize the log of the objective function (1):15 13These are the function words with count40 in a random sample of 100 documents, and which were associated with the O-I tag transition at more than twice the average rate." ></td>
	<td class="line x" title="196:277	We do not use any other lexical-features that referencevectorx, for fear that they would enable the learner to explain the rationales without changing  as desired (see the end of section 5.3)." ></td>
	<td class="line x" title="197:277	14We parse each sentence with the Collins parser (Collins, 1999)." ></td>
	<td class="line x" title="198:277	Then the document has one big parse tree, whose root is DOC, with each sentence being a child of DOC." ></td>
	<td class="line x" title="199:277	15One might expect this function to be convex becausep and p are both log-linear models with no hidden variables." ></td>
	<td class="line x" title="200:277	However, logp(ri|xi,yi,) is not necessarily convex in ." ></td>
	<td class="line x" title="201:277	nsummationdisplay i=1 logp(yi|xi) 122  bardblbardbl2 +C( nsummationdisplay i=1 logp(ri|xi,yi,)) 122  bardblbardbl2 (4) This definespprior from (1) to be a standard diagonal Gaussian prior, with variances 2 and 2 for the two sets of parameters." ></td>
	<td class="line x" title="202:277	We optimize 2 in our experiments." ></td>
	<td class="line x" title="203:277	As for 2, different values did not affect the results, since we have a large number of {I,O} rationale tags to train relatively few  weights; so we simply use 2 = 1 in all of our experiments." ></td>
	<td class="line x" title="204:277	Note the new C factor in equation (4)." ></td>
	<td class="line x" title="205:277	Our initial experiments showed that optimizing equation (4) without C led to an increase in the likelihood of the rationale data at the expense of classification accuracy, which degraded noticeably." ></td>
	<td class="line x" title="206:277	This is because the second sum in (4) has a much larger magnitude than the first: in a set of 100 documents, it predicts around 74,000 binary {I,O} tags, versus the one hundred binary class labels." ></td>
	<td class="line x" title="207:277	While we are willing to reduce the log-likelihood of the training classifications (the first sum) to a certain extent, focusing too much on modeling rationales (the second sum) is clearly not our ultimate goal, and so we optimize C on development data to achieve some balance between the two terms of equation (4)." ></td>
	<td class="line x" title="208:277	Typical values of C range from 1300 to 150.16 We perform alternating optimization on  and : 1." ></td>
	<td class="line x" title="209:277	Initialize  to maximize equation (4) but with C = 0 (i.e. based only on class data)." ></td>
	<td class="line x" title="210:277	2." ></td>
	<td class="line x" title="211:277	Fix , and find  that maximizes equation (4)." ></td>
	<td class="line x" title="212:277	3." ></td>
	<td class="line x" title="213:277	Fix , and find  that maximizes equation (4)." ></td>
	<td class="line x" title="214:277	4." ></td>
	<td class="line x" title="215:277	Repeat 2 and 3 until convergence." ></td>
	<td class="line x" title="216:277	The L-BFGS method requires calculating the gradient of the objective function (4)." ></td>
	<td class="line x" title="217:277	The partial derivatives with respect to components of  and  involve calculating expectations of the feature functions, which can be computed in linear time (with respect to the size of the training set) using the forward-backward algorithm for CRFs." ></td>
	<td class="line x" title="218:277	The partial derivatives also involve the derivative of (3), to determine how changing  will affect the firing strength of the emission features grel and gantirel." ></td>
	<td class="line x" title="219:277	16C also balances our confidence in the classifications y against our confidence in the rationales r; either may be noisy." ></td>
	<td class="line x" title="220:277	37 7 Experimental Procedures We report on two sets of experiments." ></td>
	<td class="line x" title="221:277	In the first set, we use the annotation data that A3A5 provided for the small set of 100 documents (as well as the data from A0 on those same 100 documents)." ></td>
	<td class="line x" title="222:277	In the second set, we used A0s abundant annotation data to evaluate our method with training set sizes up to 1600 documents, and compare it with three other methods: log-linear baseline, SVM baseline, and the SVM masking method of (Zaidan et al., 2007)." ></td>
	<td class="line x" title="223:277	7.1 Learning curves The learning curves reported in section 8.1 are generated exactly as in (Zaidan et al., 2007)." ></td>
	<td class="line x" title="224:277	Each curve shows classification accuracy at training set sizes T = 1,2,,9 folds (i.e. 200,400,,1600 training documents)." ></td>
	<td class="line x" title="225:277	For a given size T, the reported accuracy is an average of 9 experiments with different subsets of the entire training set, each of size T: 1 9 8summationdisplay i=0 acc(F9|Fi+1Fi+T) (5) where Fj denotes the fold numbered j mod 9, and acc(F9 | Y) means classification accuracy on the held-out test set F9 after training on set Y. We use an appropriate paired permutation test, detailed in (Zaidan et al., 2007), to test differences in (5)." ></td>
	<td class="line x" title="226:277	We call a difference significant at p< 0.05." ></td>
	<td class="line x" title="227:277	7.2 Comparison to masking SVM method We compare our method to the masking SVM method of (Zaidan et al., 2007)." ></td>
	<td class="line x" title="228:277	Briefly, that method used rationales to construct several so-called contrast examples from every training example." ></td>
	<td class="line x" title="229:277	A contrast example is obtained by masking out one of the rationales highlighted to support the training examples class." ></td>
	<td class="line x" title="230:277	A good classifier should have more trouble on this modified example." ></td>
	<td class="line x" title="231:277	Hence, Zaidan et al.(2007) required the learned SVM to classify each contrast example with a smaller margin than the corresponding original example (and did not require it to be classified correctly)." ></td>
	<td class="line x" title="233:277	The masking SVM learner relies on a simple geometric principle; is trivial to implement on top of an existing SVM learner; and works well." ></td>
	<td class="line x" title="234:277	However, we believe that the generative method we present here is more interesting and should apply more broadly." ></td>
	<td class="line x" title="235:277	Figure 3: Classification accuracy curves for the 4 methods: the two baseline learners that only utilize class data, and the two learners that also utilize rationale annotations." ></td>
	<td class="line x" title="236:277	The SVM curves are from (Zaidan et al., 2007)." ></td>
	<td class="line x" title="237:277	First, the masking method is specific to improving an SVM learner, whereas our method can be used to improve any classifier by adding a rationale-based regularizer (the second half of equation (4)) to its objective function during training." ></td>
	<td class="line x" title="238:277	More important, there are tasks where it is unclear how to generate contrast examples." ></td>
	<td class="line x" title="239:277	For the movie review task, it was natural to mask out a rationale by pretending its words never occurred in the document." ></td>
	<td class="line x" title="240:277	After all, most word types do not appear in most documents, so it is natural to consider the nonpresence of a word as a default state to which we can revert." ></td>
	<td class="line x" title="241:277	But in an image classification task, how should one modify the images features to ignore some spatial region marked as a rationale?" ></td>
	<td class="line x" title="242:277	There is usually no natural default value to which we could set the pixels." ></td>
	<td class="line x" title="243:277	Our method, on the other hand, eliminates contrast examples altogether." ></td>
	<td class="line x" title="244:277	8 Experimental Results and Analysis 8.1 The added benefit of rationales Fig." ></td>
	<td class="line x" title="245:277	3 shows learning curves for four methods." ></td>
	<td class="line x" title="246:277	A log-linear model shows large and significant improvements, at all training sizes, when we incorporate rationales into its training via equation (4)." ></td>
	<td class="line x" title="247:277	Moreover, the resulting classifier consistently outperforms17 prior work, the masking SVM, which starts with a slightly better baseline classifier (an SVM) but incorporates the rationales more crudely." ></td>
	<td class="line x" title="248:277	17Differences are not significant at sizes 200, 1000, and 1600." ></td>
	<td class="line x" title="249:277	38 size A0 A3 A4 A5 SVM baseline 100 72.0 72.0 72.0 70.0 SVM+contrasts 100 75.0 73.0 74.0 72.0 Log-linear baseline 100 71.0 73.0 71.0 70.0 Log-linear+rats 100 76.0 76.0 77.0 74.0 SVM baseline 20 63.4 62.2 60.4 62.6 SVM+contrasts 20 65.4 63.4 62.4 64.8 Log-linear baseline 20 63.0 62.2 60.2 62.4 Log-linear+rats 20 65.8 63.6 63.4 64.8 Table 1: Accuracy rates using each annotators data." ></td>
	<td class="line x" title="250:277	In a given column, a value in italics is not significantly different from the highest value in that column, which is boldfaced." ></td>
	<td class="line x" title="251:277	The size=20 results average over 5 experiments." ></td>
	<td class="line x" title="252:277	To confirm that we could successfully model annotators other than A0, we performed the same comparison for annotators A3A5; each had provided class and rationale annotations on a small 100document training set." ></td>
	<td class="line x" title="253:277	We trained a separate  for each annotator." ></td>
	<td class="line x" title="254:277	Table 1 shows improvements over baseline, usually significant, at 2 training set sizes." ></td>
	<td class="line x" title="255:277	8.2 Analysis Examining the learned weights vector gives insight into annotator behavior." ></td>
	<td class="line x" title="256:277	High weights include I-O and O-I transitions conditioned on punctuation, e.g., I(.)-O = 3.55,18 as well as rationales ending at the end of a major phrase, e.g., I-O/E=VP = 1.88." ></td>
	<td class="line x" title="257:277	The large emission feature weights, e.g., rel = 14.68 and antirel = 15.30, tie rationales closely to  values, as hoped." ></td>
	<td class="line x" title="258:277	For example, in Figure 1, the word w = succeeds, with w = 0.13, drives up p(I)/p(O) by a factor of 7 (in a positive document) relative to a word with w = 0." ></td>
	<td class="line x" title="259:277	In fact, feature ablation experiments showed that almost all the classification benefit from rationales can be obtained by using only these 2 emission -features and the 4 unconditioned transition features." ></td>
	<td class="line x" title="260:277	Our full  (115 features) merely improves our ability to predict the rationales (whose likelihood does increase significantly with more features)." ></td>
	<td class="line x" title="261:277	We also checked that annotators styles differ enough that it helps to tune  to the target annotatorAwho gave the rationales." ></td>
	<td class="line x" title="262:277	Table 3 shows that a model trained onAs own rationales does best at predicting new rationales fromA." ></td>
	<td class="line x" title="263:277	Table 2 shows that as 18When trained on folds F4F8 with A0s rationales." ></td>
	<td class="line x" title="264:277	A0 A3 A4 A5 Baseline A0 76.0 73.0 74.0 73.0 71.0 A3 73.0 76.0 74.0 73.0 73.0 A4 75.0 73.0 77.0 74.0 71.0 A5 74.0 71.0 72.0 74.0 70.0 Table 2: Accuracy rate for an annotators  (rows) obtained when using some other annotators  (columns)." ></td>
	<td class="line x" title="265:277	Notice that the diagonal entries and the baseline column are taken from rows of Table 1 (size=100)." ></td>
	<td class="line x" title="266:277	Trivial A0 A3 A4 A5 model L(rA0) 0.073 0.086 0.077 0.088 0.135 L(rA3) 0.084 0.068 0.071 0.068 0.130 L(rA4) 0.088 0.084 0.075 0.085 0.153 L(rA5) 0.058 0.044 0.047 0.044 0.111 Table 3: Cross-entropy per tag of rationale annotations vectorr for each annotator (rows), when predicted from that annotators vectorx and vector via a possibly different annotators  (columns)." ></td>
	<td class="line x" title="267:277	For comparison, the trivial model is a bigram model ofvectorr, which is trained on the target annotator but ignores vectorx and vector." ></td>
	<td class="line x" title="268:277	5-fold cross-validation on the 100document set was used to prevent testing on training data." ></td>
	<td class="line x" title="269:277	a result, classification performance on the test set is usually best if it wasAs ownthat was used to help learn  from As rationales." ></td>
	<td class="line x" title="270:277	In both cases, however, a different annotators  is better than nothing." ></td>
	<td class="line x" title="271:277	9 Conclusions We have demonstrated a effective method for eliciting extra knowledge from naive annotators, in the form of lightweight rationales for their annotations." ></td>
	<td class="line x" title="272:277	By explicitly modeling the annotators rationale-marking process, we are able to infer a better model of the original annotations." ></td>
	<td class="line x" title="273:277	We showed that our method performs significantly better than two strong baseline classifiers, and also outperforms our previous discriminative method for exploiting rationales (Zaidan et al., 2007)." ></td>
	<td class="line x" title="274:277	We also saw that it worked across four annotators who have different rationale-marking styles." ></td>
	<td class="line x" title="275:277	In future, we are interested in new domains that can adaptively solicit rationales for some or all training examples." ></td>
	<td class="line x" title="276:277	Our new method, being essentially Bayesian inference, is potentially extensible to many other situationsother tasks, classifier architectures, and more complex features." ></td>
	<td class="line x" title="277:277	39" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D08-1058
Using Bilingual Knowledge and Ensemble Techniques for Unsupervised Chinese Sentiment Analysis
Wan, Xiaojun;"></td>
	<td class="line x" title="1:180	Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 553561, Honolulu, October 2008." ></td>
	<td class="line x" title="2:180	c2008 Association for Computational Linguistics Using Bilingual Knowledge and Ensemble Techniques for Unsupervised Chinese Sentiment Analysis  Xiaojun Wan Institute of Compute Science and Technology Peking University Beijing 100871, China wanxiaojun@icst.pku.edu.cn  Abstract It is a challenging task to identify sentiment polarity of Chinese reviews because the resources for Chinese sentiment analysis are limited." ></td>
	<td class="line x" title="3:180	Instead of leveraging only monolingual Chinese knowledge, this study proposes a novel approach to leverage reliable English resources to improve Chinese sentiment analysis." ></td>
	<td class="line x" title="4:180	Rather than simply projecting English resources onto Chinese resources, our approach first translates Chinese reviews into English reviews by machine translation services, and then identifies the sentiment polarity of English reviews by directly leveraging English resources." ></td>
	<td class="line x" title="5:180	Furthermore, our approach performs sentiment analysis for both Chinese reviews and English reviews, and then uses ensemble methods to combine the individual analysis results." ></td>
	<td class="line x" title="6:180	Experimental results on a dataset of 886 Chinese product reviews demonstrate the effectiveness of the proposed approach." ></td>
	<td class="line x" title="7:180	The individual analysis of the translated English reviews outperforms the individual analysis of the original Chinese reviews, and the combination of the individual analysis results further improves the performance." ></td>
	<td class="line x" title="8:180	1 Introduction In recent years, sentiment analysis (including subjective/objective analysis, polarity identification, opinion extraction, etc.) has drawn much attention in the NLP field." ></td>
	<td class="line x" title="9:180	In this study, the objective of sentiment analysis is to annotate a given text for polarity orientation (positive/negative)." ></td>
	<td class="line x" title="10:180	Polarity orientation identification has many useful applications, including opinion summarization (Ku et al., 2006) and sentiment retrieval (Eguchi and Lavrenko, 2006)." ></td>
	<td class="line x" title="11:180	To date, most of the research focuses on English and a variety of reliable English resources for sentiment analysis are available, including polarity lexicon, contextual valence shifters, etc. However, the resources for other languages are limited." ></td>
	<td class="line x" title="12:180	In particular, few reliable resources are available for Chinese sentiment analysis 1  and it is not a trivial task to manually label reliable Chinese sentiment resources." ></td>
	<td class="line x" title="13:180	Instead of using only the limited Chinese knowledge, this study aims to improve Chinese sentiment analysis by making full use of bilingual knowledge in an unsupervised way, including both Chinese resources and English resources." ></td>
	<td class="line x" title="14:180	Generally speaking, there are two unsupervised scenarios for borrowing English resources for sentiment analysis in other languages: one is to generate resources in a new language by leveraging on the resources available in English via cross-lingual projections, and then perform sentiment analysis in the English language based on the generated resources, which has been investigated by Mihalcea et al.(2007); the other is to translate the texts in a new language into English texts, and then perform sentiment analysis in the English language, which has not yet been investigated." ></td>
	<td class="line x" title="16:180	In this study, we first translate Chinese reviews into English reviews by using machine translation services, and then identify the sentiment polarity of English reviews by directly leveraging English resources." ></td>
	<td class="line x" title="17:180	Furthermore, ensemble methods are employed to combine the individual analysis results in each language (i.e. Chinese and English) in order to obtain improved results." ></td>
	<td class="line x" title="18:180	Given machine translation services between the selected target language and English, the proposed approach can be applied to any other languages as well." ></td>
	<td class="line x" title="19:180	Experiments have been performed on a dataset of 886 Chinese product reviews." ></td>
	<td class="line x" title="20:180	Two commercial  1  This study focuses on Simplified Chinese." ></td>
	<td class="line x" title="21:180	553 machine translation services (i.e. Google Translate and Yahoo Babel Fish) and a baseline dictionarybased system are used for translating Chinese reviews into English reviews." ></td>
	<td class="line x" title="22:180	Experimental results show that the analysis of English reviews translated by the commercial translation services outperforms the analysis of original Chinese reviews." ></td>
	<td class="line x" title="23:180	Moreover, the analysis performance can be further improved by combining the individual analysis results in different languages." ></td>
	<td class="line x" title="24:180	The results also demonstrate that our proposed approach is more effective than the approach that leverages generated Chinese resources." ></td>
	<td class="line x" title="25:180	The rest of this paper is organized as follows: Section 2 introduces related work." ></td>
	<td class="line x" title="26:180	The proposed approach is described in detail in Section 3." ></td>
	<td class="line x" title="27:180	Section 4 shows the experimental results." ></td>
	<td class="line x" title="28:180	Lastly we conclude this paper in Section 5." ></td>
	<td class="line x" title="29:180	2 Related Work Polarity identification can be performed on word level, sentence level or document level." ></td>
	<td class="line x" title="30:180	Related work for word-level polarity identification includes (Hatzivassiloglou and McKeown, 1997; Kim and Hovy." ></td>
	<td class="line x" title="31:180	2004; Takamura et al., 2005; Yao et al. 2006; Kaji and Kitsuregawa, 2007), and related work for sentence-level polarity identification includes (Yu and Hatzivassiloglou, 2003; Kim and Hovy." ></td>
	<td class="line x" title="32:180	2004) Word-level or sentence-level sentiment analysis is not the focus of this paper." ></td>
	<td class="line x" title="33:180	Generally speaking, document-level polarity identification methods can be categorized into unsupervised and supervised." ></td>
	<td class="line x" title="34:180	Unsupervised methods involve deriving a sentiment metric for text without training corpus." ></td>
	<td class="line x" title="35:180	Turney (2002) predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs, which is denoted as the semantic oriented method." ></td>
	<td class="line x" title="36:180	Kim and Hovy (2004) build three models to assign a sentiment category to a given sentence by combining the individual sentiments of sentiment-bearing words." ></td>
	<td class="line x" title="37:180	Hiroshi et al.(2004) use the technique of deep language analysis for machine translation to extract sentiment units in text documents." ></td>
	<td class="line x" title="39:180	Kennedy and Inkpen (2006) determine the sentiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers." ></td>
	<td class="line x" title="40:180	Devitt and Ahmad (2007) explore a computable metric of positive or negative polarity in financial news text." ></td>
	<td class="line x" title="41:180	Supervised methods consider the sentiment analysis task as a classification task and use labeled corpus to train the classifier." ></td>
	<td class="line oc" title="42:180	Since the work of Pang et al.(2002), various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005a; Read, 2005)." ></td>
	<td class="line x" title="44:180	Most recently, McDonald et al.(2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity." ></td>
	<td class="line x" title="46:180	Blitzer et al.(2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products." ></td>
	<td class="line x" title="48:180	Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpusbased classifier and a lexicon-based classifier with precision-based vote weighting." ></td>
	<td class="line x" title="49:180	Research work focusing on Chinese sentiment analysis includes (Tsou et al., 2005; Ye et al., 2006; Li and Sun, 2007; Wang et al., 2007)." ></td>
	<td class="line x" title="50:180	Such work represents heuristic extensions of the unsupervised or supervised methods for English sentiment analysis." ></td>
	<td class="line x" title="51:180	To date, the most closely related work is Mihalcea et al.(2007), which explores cross-lingual projections to generate subjectivity analysis resources in Romanian by leveraging on the tools and resources available in English." ></td>
	<td class="line x" title="53:180	They have investigated two approaches: a lexicon-based approach based on Romanian subjectivity lexicon translated from English lexicon, and a corpus-based approach based on Romanian subjectivity-annotated corpora obtained via cross-lingual projections." ></td>
	<td class="line x" title="54:180	In this study, we focus on unsupervised sentiment polarity identification and we only investigate the lexicon-based approach in the experiments." ></td>
	<td class="line x" title="55:180	Other related work includes subjective/objective analysis (Hatzivassiloglon and Wiebe, 2000; Riloff and Wiebe, 2003) and opinion mining and summarization (Liu et al., 2005; Popescu and Etzioni." ></td>
	<td class="line x" title="56:180	2005; Choi et al., 2006; Ku et al., 2006; Titov and McDonald, 2008)." ></td>
	<td class="line x" title="57:180	3 The Proposed Approach 3.1 Overview The motivation of our approach is to make full use of bilingual knowledge to improve sentiment analysis in a target language, where the resources 554 for sentiment analysis are limited or unreliable." ></td>
	<td class="line x" title="58:180	This study focuses on unsupervised polarity identification of Chinese product reviews by using both the rich English knowledge and the limited Chinese knowledge." ></td>
	<td class="line x" title="59:180	The framework of our approach is illustrated in Figure 1." ></td>
	<td class="line x" title="60:180	A Chinese review is translated into the corresponding English review using machine translation services, and then the Chinese review and the English review are analyzed based on Chinese resources and English resources, respectively." ></td>
	<td class="line x" title="61:180	The analysis results are then combined to obtain more accurate results under the assumption that the individual sentiment analysis can complement each other." ></td>
	<td class="line x" title="62:180	Note that in the framework, different machine translation services can be used to obtain different English reviews, and the analysis of English reviews translated by a specific machine translation service is conducted separately." ></td>
	<td class="line x" title="63:180	For simplicity, we consider the English reviews translated by different machine translation services as reviews in different languages, despite the fact that in essence, they are still in English." ></td>
	<td class="line x" title="64:180	Figure 1." ></td>
	<td class="line x" title="65:180	Framework of our approach Formally, give a review rev 0  in the target language (i.e. Chinese), the corresponding review rev i in the ith language is obtained by using a translation function:  rev i =f  i Trans (rev 0 ) where 1ip and p is the total number of machine translation services." ></td>
	<td class="line x" title="66:180	For each review rev k  in the kth language (0kp), we employ the semantic oriented approach to assign a semantic orientation value f  k SO (rev k ) to the review, and the polarity orientation of the review can be simply predicated based on the value by using a threshold." ></td>
	<td class="line x" title="67:180	Given a set of semantic orientation values F SO ={f  k SO (rev k ) | 0kp}, the ensemble methods aim to derive a new semantic orientation value )( 0 revf Ensemble SO based on the values in F SO , which can be used to better classify the review as positive or negative." ></td>
	<td class="line x" title="68:180	The steps of review translation, individual semantic orientation value computation and ensemble combination are described in details in the next sections, respectively." ></td>
	<td class="line x" title="69:180	3.2 Review Translation Translation of a Chinese review into an English review is the first step of the proposed approach." ></td>
	<td class="line x" title="70:180	Manual translation is time-consuming and laborintensive, and it is not feasible to manually translate a large amount of Chinese product reviews in real applications." ></td>
	<td class="line x" title="71:180	Fortunately, machine translation techniques have been well developed in the NLP field, though the translation performance is far from satisfactory." ></td>
	<td class="line x" title="72:180	A few commercial machine translation services can be publicly accessed." ></td>
	<td class="line x" title="73:180	In this study, the following two commercial machine translation services and one baseline system are used to translate Chinese reviews into English reviews." ></td>
	<td class="line x" title="74:180	Google Translate 2  (GoogleTrans): Google Translate is one of the state-of-the-art commercial machine translation systems used today." ></td>
	<td class="line x" title="75:180	Google Translate applies statistical learning techniques to build a translation model based on both monolingual text in the target language and aligned text consisting of examples of human translations between the languages." ></td>
	<td class="line x" title="76:180	Yahoo Babel Fish 3  (YahooTrans): Different from Google Translate, Yaho Babel Fish uses SYSTRANs rule-based translation engine." ></td>
	<td class="line x" title="77:180	SYSTRAN was one of the earliest developers of machine translation software." ></td>
	<td class="line x" title="78:180	SYSTRAN applies complex sets of specific rules defined by linguists to analyze and then transfer the grammatical structure of the source language into the target language." ></td>
	<td class="line x" title="79:180	Baseline Translate (DictTrans): We simply develop a translation method based only on one-toone term translation in a large Chinese-to-English  2  http://translate.google.com/translate_t 3  http://babelfish.yahoo.com/translate_txt Chinese review Chinese Resource English review Machine translation Chinese sentiment analysis Ensemble English sentiment analysis English Resource Polarity Value Polarity Value Pos\Neg 555 dictionary." ></td>
	<td class="line x" title="80:180	Each term in a Chinese review is translated by the first corresponding term in the Chinese-to-English dictionary, without any other processing steps." ></td>
	<td class="line x" title="81:180	In this study, we use the LDC_CE_DIC2.0 4  constructed by LDC as the dictionary for translation, which contains 128366 Chinese terms and their corresponding English terms." ></td>
	<td class="line x" title="82:180	The Chinese-to-English translation performances of the two commercial systems are deemed much better than the weak baseline system." ></td>
	<td class="line x" title="83:180	Google Translate has achieved very good results on the Chinese-to-English translation tracks of NIST open machine translation test (MT) 5  and it ranks the first on most tracks." ></td>
	<td class="line x" title="84:180	In the Chinese-to-English task of MT2005, the BLEU-4 score of Google Translate is 0.3531, and the BLEU-4 score of SYSTRAN is 0.1471." ></td>
	<td class="line x" title="85:180	We can deduce that Google Translate is better than Yahoo Babel Fish, without considering the recent improvements of the two systems." ></td>
	<td class="line x" title="86:180	Here are two running example of Chinese reviews and the translated English reviews (HumanTrans refers to human translation): Positive Example: , HumanTrans: Many advantages and very good shape." ></td>
	<td class="line x" title="87:180	GoogleTrans: Many advantages, the shape is also very good." ></td>
	<td class="line x" title="88:180	YahooTrans: Merit very many, the contour very is also good." ></td>
	<td class="line x" title="89:180	DictTrans: merit very many figure also very good Negative example:  HumanTrans: The memory is too small to support IR." ></td>
	<td class="line x" title="90:180	GoogleTrans: Memory is too small not to support IR." ></td>
	<td class="line x" title="91:180	YahooTrans:The memory too is small does not support infrared." ></td>
	<td class="line x" title="92:180	DictTrans: memory highest small negative not to be in favor of ir." ></td>
	<td class="line x" title="93:180	3.3 Individual Semantic Orientation Value Computation For any specific language, we employ the semantic orientated approach (Kennedy and Inkpen, 2006) to compute the semantic orientation value of a review." ></td>
	<td class="line x" title="94:180	The unsupervised approach is quite  straightforward and it makes use of the following sentiment lexicons: positive Lexicon (Positive_Dic) including terms expressing positive polarity, Negative Lexicon (Negative_Dic) including terms expressing negative polarity, Negation  4  http://projects.ldc.upenn.edu/Chinese/LDC_ch.htm 5  http://www.nist.gov/speech/tests/mt/ Lexicon (Negation_Dic) including terms that are used to reverse the semantic polarity of a particular term, and Intensifier Lexicon (Intensifier_Dic) including terms that are used to change the degree to which a term is positive or negative." ></td>
	<td class="line x" title="95:180	In this study, we conduct our experiments within two languages, and we collect and use the following popular and available Chinese and English sentiment lexicons 6 , without any further filtering and labeling: 1) Chinese lexicons Positive_Dic cn : 3730 Chinese positive terms were collected from the Chinese Vocabulary for Sentiment Analysis (VSA) 7  released by HOWNET." ></td>
	<td class="line x" title="96:180	Negative_Dic cn : 3116 Chinese negative terms were collected from Chinese Vocabulary for Sentiment Analysis (VSA) released by HOWNET." ></td>
	<td class="line x" title="97:180	Negation_Dic cn : 13 negation terms were collected from related papers." ></td>
	<td class="line x" title="98:180	Intensifier_Dic cn : 148 intensifier terms were collected from Chinese Vocabulary for Sentiment Analysis (VSA) released by HOWNET." ></td>
	<td class="line x" title="99:180	2) English lexicons Positive_Dic en : 2718 English positive terms were collected from the feature file subjclueslen1HLTEMNLP05.tff 8  containing the subjectivity clues used in the work (Wilson et al., 2005a; Wilson et al., 2005b)." ></td>
	<td class="line x" title="100:180	The clues in this file were collected from a number of sources." ></td>
	<td class="line x" title="101:180	Some were culled from manually developed resources, e.g. general inquirer 9  (Stone et al., 1966)." ></td>
	<td class="line x" title="102:180	Others were identified automatically using both annotated and unannotated data." ></td>
	<td class="line x" title="103:180	A majority of the clues were collected as part of work reported in Riloff and Wiebe (2003)." ></td>
	<td class="line x" title="104:180	Negative_Dic en : 4910 English negative terms were collected from the same file described above." ></td>
	<td class="line x" title="105:180	Negation_Dic en : 88 negation terms were collected from the feature file valenceshifters.tff used in the work (Wilson et al., 2005a; Wilson et al., 2005b)." ></td>
	<td class="line x" title="106:180	Intensifier_Dic en : 244 intensifier terms were collected from the feature file intensifiers2.tff used in the work (Wilson et al., 2005a; Wilson et al., 2005b)." ></td>
	<td class="line x" title="107:180	6  In this study, we focus on using a few popular resources in both Chinese and English for comparative study, instead of trying to collect and use all available resources." ></td>
	<td class="line x" title="108:180	7  http://www.keenage.com/html/e_index.html 8  http://www.cs.pitt.edu/mpqa/ 9  http://www.wjh.harvard.edu/~inquirer/homecat.htm 556 The semantic orientation value f  k SO (rev k ) for rev k  is computed by summing the polarity values of all words in the review, making use of both the word polarity defined in the positive and negative lexicons and the contextual valence shifters defined in the negation and intensifier lexicons." ></td>
	<td class="line x" title="109:180	The algorithm is illustrated in Figure 2." ></td>
	<td class="line x" title="110:180	Input: a review rev k  in the kth language." ></td>
	<td class="line x" title="111:180	Four lexicons in the kth language: Positive_Dic k , Negative_Dic k , Negation_Dic k , Intensifier_Dic k , which are either Chinese or English lexicons; Output: Polarity Value f  k SO (rev k ); Algorithm Compute_SO: 1." ></td>
	<td class="line x" title="112:180	Tokenize review rev k  into sentence set S and each sentence sS  is tokenized into word set W s ; 2." ></td>
	<td class="line x" title="113:180	For any word w in a sentence sS, compute its SO value SO(w) as follows: 1) if wPositive_Dic k , SO(w)=PosValue; 2) If wNegative_Dic k , SO(w)=NegValue; 3) Otherwise, SO(w)=0; 4) Within the window of q words previous to w, if there is a term wNegation_Dic k , SO(w)= SO(w); 5) Within the window of q words previous to w, if there is a term wIntensifier_Dic k , SO(w) = SO(w); 3." ></td>
	<td class="line x" title="114:180	  = SsWw kk SO s wSOrevf )()( ; Figure 2." ></td>
	<td class="line x" title="115:180	The algorithm for semantic orientation value computation In the above algorithm, PosValue and NegValue are the polarity values for positive words and negative words respectively." ></td>
	<td class="line x" title="116:180	We empirically set PosValue=1 and NegValue= 2 because negative words usually contribute more to the overall semantic orientation of the review than positive words, according to our empirical analysis." ></td>
	<td class="line x" title="117:180	 >1 aims to intensify the polarity value and we simply set  =2." ></td>
	<td class="line x" title="118:180	q is the parameter controlling the window size within which the negation terms and intensifier terms have influence on the polarity words and here q is set to 2 words." ></td>
	<td class="line x" title="119:180	Note that the above parameters are tuned only for Chinese sentiment analysis, and they are used for sentiment analysis in the English language without further tuning." ></td>
	<td class="line x" title="120:180	The tokenization of Chinese reviews involves Chinese word segmentation." ></td>
	<td class="line x" title="121:180	Usually, if the semantic orientation value of a review is less than 0, the review is labeled as negative, otherwise, the review is labeled as positive." ></td>
	<td class="line x" title="122:180	3.4 Ensemble Combination After obtaining the set of semantic orientation values F SO ={f  k SO (rev k ) | 0kp} by using the semantic oriented approach, where p is the number of English translations for each Chinese review, we exploit the following ensemble methods for deriving a new semantic orientation value )( 0 revf Ensemble SO : 1) Average It is the most intuitive combination method and the new value is the average of the values in F SO : 1 )( )( 00 + =  = p revf revf p k kk SO Ensemble SO  Note that after the new value of a review is obtained, the polarity tag of the review is assigned in the same way as described in Section 3.3." ></td>
	<td class="line x" title="123:180	2) Weighted Average This combination method improves the average combination method by associating each individual value with a weight, indicating the relative confidence in the value." ></td>
	<td class="line x" title="124:180	 = = p k kk SOk Ensemble SO revfrevf 0 0 )()(   where  k [0, 1] is the weight associated with f  k SO (rev k )." ></td>
	<td class="line x" title="125:180	The weights can be set in the following two ways: Weighting Scheme1: The weight of f  k SO (rev k ) is set to the accuracy of the individual analysis in the kth language." ></td>
	<td class="line x" title="126:180	Weighting Scheme2: The weight of f  k SO (rev k ) is set to be the maximal correlation coefficient between the analysis results in the kth language and the analysis results in any other language." ></td>
	<td class="line x" title="127:180	The underlying idea is that if the analysis results in one language are highly consistent with the analysis results in another language, the results are deemed to be more reliable." ></td>
	<td class="line x" title="128:180	Given two lists of semantic values for all reviews, we use the Pearsons correlation coefficient to measure the correlation between them." ></td>
	<td class="line x" title="129:180	The weight associated with function f  k SO (rev k ) is then defined as the maximal Pearsons correlation coefficient between the reviews values in the kth language and the reviews values in any other language." ></td>
	<td class="line x" title="130:180	3) Max 557 The new value is the maximum value in F SO : { }pkrevfrevf kk SO Ensemble SO = 0|)(max)( 0  4) Min The new value is the minimum value in F SO : { }pkrevfrevf kk SO Ensemble SO = 0|)(min)( 0  5) Average Max&Min The new value is the average of the maximum value and the minimum value in F SO : {}{} 2 0|)(min0|)(max )( 0 pkrevfpkrevf revf kk SO kk SOEnsemble SO + =  6) Majority Voting This combination method relies on the final polarity tags, instead of the semantic orientation values." ></td>
	<td class="line x" title="131:180	A review can obtain p+1 polarity tags based on the individual analysis results in the p+1 languages." ></td>
	<td class="line x" title="132:180	The polarity tag receiving more votes is chosen as the final polarity tag of the review." ></td>
	<td class="line x" title="133:180	4 Empirical Evaluation 4.1 Dataset and Evaluation Metrics In order to assess the performance of the proposed approach, we collected 1000 product reviews from a popular Chinese IT product web site-IT168 10 . The reviews were posted by users and they focused on such products as mp3 players, mobile phones, digital camera and laptop computers." ></td>
	<td class="line x" title="134:180	Users usually selected for each review an icon indicating postive or negative." ></td>
	<td class="line x" title="135:180	The reviews were first categorized into positive and negative classes according to the associated icon." ></td>
	<td class="line x" title="136:180	The polarity labels for the reviews were then checked by subjects." ></td>
	<td class="line x" title="137:180	Finally, the dataset contained 886 product reviews with accurate polarity labels." ></td>
	<td class="line x" title="138:180	All the 886 reviews were used as test set." ></td>
	<td class="line x" title="139:180	We used the standard precision, recall and Fmeasure to measure the performance of positive and negative class, respectively, and used the MacroF measure and accuracy metric to measure the overall performance of the system." ></td>
	<td class="line x" title="140:180	The metrics are defined the same as in general text categorization." ></td>
	<td class="line x" title="141:180	4.2 Individual Analysis Results In this section, we investigate the following individual sentiment analysis results in each specified language: CN: This method uses only Chinese lexicons to analyze Chinese reviews;  10  http://www.it168.com GoogleEN: This method uses only English lexicons to analyze English reviews translated by GoogleTrans; YahooEN: This method uses only English lexicons to analyze English reviews translated by YahooTrans; DictEN: This method uses only English lexicons to analyze English reviews translated by DictTrans; In addition to the above methods for using English resources, the lexicon-based method investigated in Mihalcea et al.(2007) can also use English resources by directly projecting English lexicons into Chinese lexicons." ></td>
	<td class="line x" title="143:180	We use a large English-to-Chinese dictionary LDC_EC_DIC2.0 11  with 110834 entries for projecting English lexicons into Chinese lexicons via one-to-one translation." ></td>
	<td class="line x" title="144:180	Based on the generated Chinese lexicons, two other individual methods are investigated in the experiments: CN2: This method uses only the generated Chinese Resources to analyze Chinese reviews." ></td>
	<td class="line x" title="145:180	CN3: This method combines the original Chinese lexicons and the generated Chinese lexicons and uses the extended lexicons to analyze Chinese reviews." ></td>
	<td class="line x" title="146:180	Table 1 provides the performance values of all the above individual methods." ></td>
	<td class="line x" title="147:180	Seen from the table, the performances of GoogleEN and YahooEN are much better than the baseline CN method, and even the DictEN performs as well as CN." ></td>
	<td class="line x" title="148:180	The results demonstrate that the use of English resources for sentiment analysis of translated English reviews is an effective way for Chinese sentiment analysis." ></td>
	<td class="line x" title="149:180	We can also see that the English sentiment analysis performance relies positively on the translation performance, and GoogleEN performs the best while DictEN performs the worst, which is consistent with the fact the GoogleTrans is deemed the best of the three machine translation systems, while DictTrans is the weakest one." ></td>
	<td class="line x" title="150:180	Furthermore, the CN method outperforms the CN2 and CN3 methods, and the CN2 method performs the worst, which shows that the generated Chinese lexicons do not give any contributions to the performance of Chinese sentiment analysis." ></td>
	<td class="line x" title="151:180	We explain the results by the fact that the term-based one-to-one translation is inaccurate and the generated Chinese lexicons are not reliable." ></td>
	<td class="line x" title="152:180	Overall, the  11  http://projects.ldc.upenn.edu/Chinese/LDC_ch.htm 558 approach through cross-lingual lexicon translation does not work well for Chinese sentiment analysis in our experiments." ></td>
	<td class="line x" title="153:180	4.3 Ensemble Results In this section, we first use the simple average ensemble method to combine different individual analysis results." ></td>
	<td class="line x" title="154:180	Table 2 provides the performance values of the average ensemble results based on different individual methods." ></td>
	<td class="line x" title="155:180	Seen from Tables 1 and 2, almost all of the average ensembles outperforms the baseline CN method and the corresponding individual methods, which shows that each individual methods have their own evidences for sentiment analysis, and thus fusing the evidences together can improve performance." ></td>
	<td class="line x" title="156:180	For the methods of CN+GoogleEN, CN+YahooEN and CN+DictEN, we can see the ensemble performance is not positively relying on the translation performance: CN+YahooEN performs better than CN+GoogleEN, and even CN+DictEN performs as well as CN+GoogleEN." ></td>
	<td class="line x" title="157:180	The results show that the individual methods in the ensembles can complement each other, and even the combination of two weak individual methods can achieve good performance." ></td>
	<td class="line x" title="158:180	However, the DictEN method is not effective when the ensemble methods have already included GoogleEN and YahooEN." ></td>
	<td class="line x" title="159:180	Overall, the performances of the ensemble methods rely on the performances of the most effective constituent individual methods: the methods including both GoogleEN and YahooEN perform much better than other methods, and CN+GoogleEN+YahooEN performs the best out of all the methods." ></td>
	<td class="line x" title="160:180	We further show the results of four typical average ensembles by varying the combination weights." ></td>
	<td class="line x" title="161:180	The combination weights are respectively specified as  CN+(1- )GoogleEN,  CN+(1 )YahooEN,  CN+(1- )DictEN,  1 CN+ 2 GoogleEN+(1- 1 - 2 )YahooEN." ></td>
	<td class="line x" title="162:180	The results over the MacroF metric are shown in Figures 3 and 4 respectively." ></td>
	<td class="line x" title="163:180	We can see from the figures that GoogleEN and YahooEN are dominant factors in the ensemble methods." ></td>
	<td class="line x" title="164:180	We then investigate to use other ensemble methods introduced in Section 3.4 to combine the CN, GoogleEN and YahooEN methods." ></td>
	<td class="line x" title="165:180	Table 3 gives the comparison results." ></td>
	<td class="line x" title="166:180	The methods of Weighted Average1 and Weighted Average2 are two weighted average ensembles using the two weighing schemes, respectively." ></td>
	<td class="line x" title="167:180	We can see that all the ensemble methods outperform the constituent individual method, while the two weighted average ensembles perform the best." ></td>
	<td class="line x" title="168:180	The results further demonstrate the good effectiveness of the ensemble combination of individual analysis results for Chinese sentiment analysis." ></td>
	<td class="line x" title="169:180	Positive Negative Total Individual Method Precision Recall F-measure Precision Recall F-measure MacroF Accuracy CN 0.681 0.929 0.786 0.882 0.549 0.677 0.732 0.743 CN2 0.615 0.772 0.684 0.678 0.499 0.575 0.630 0.638 CN3 0.702 0.836 0.763 0.788 0.632 0.702 0.732 0.736 GoogleEN 0.764 0.914 0.832 0.888 0.708 0.787 0.810 0.813 YahooEN 0.763 0.871 0.814 0.844 0.720 0.777 0.795 0.797 DictEN 0.738 0.761 0.749 0.743 0.720 0.731 0.740 0.740 Table 1." ></td>
	<td class="line x" title="170:180	Individual analysis results Positive Negative Total Average Ensemble Precision Recall F-measure Precision Recall F-measure MacroF Accuracy GoogleEN+YahooEN 0.820 0.900 0.858 0.885 0.795 0.838 0.848 0.848 GoogleEN+YahooEN +DictEN 0.841 0.845 0.843 0.838 0.834 0.836 0.840 0.840 CN+GoogleEN 0.754 0.949 0.840 0.928 0.678 0.784 0.812 0.816 CN+YahooEN 0.784 0.925 0.848 0.904 0.736 0.811 0.830 0.832 CN+DictEN 0.790 0.867 0.827 0.847 0.761 0.801 0.814 0.815 CN+GoogleEN +YahooEN 0.813 0.927 0.866 0.911 0.779 0.840 0.853 0.854 CN+GoogleEN+ YahooEN+DictEN 0.831 0.891 0.860 0.878 0.811 0.843 0.852 0.852 Table 2." ></td>
	<td class="line x" title="171:180	Average combination results 559 0.72 0.74 0.76 0.78 0.8 0.82 0.84 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  Mac r oF CN+GoogleEN CN+YahooEN CN+DictEN  0 0.3 0.6 0.9 0 0.2 0.4 0.6 0.8 1 0.65 0.7 0.75 0.8 0.85 0.9 MacroF  1  2 0.85-0.9 0.8-0.85 0.75-0.8 0.7-0.75 0.65-0.7  Figure 3." ></td>
	<td class="line x" title="172:180	Ensemble performance vs. weight   for  CN+(1- )GoogleEN/YahooEN/DictEN Figure 4." ></td>
	<td class="line x" title="173:180	Ensemble performance vs. weights  1 and  2  for  1 CN+ 2 GoogleEN+(1- 1 - 2 ) YahooEN  Positive Negative Total Ensemble Method Precision Recall F-measure Precision Recall F-measure MacroF Accuracy Average 0.813 0.927 0.866 0.911 0.779 0.840 0.853 0.854 Weighted Average1 0.825 0.922 0.871 0.908 0.798 0.849 0.860 0.861 Weighted Average2 0.822 0.922 0.869 0.908 0.793 0.847 0.858 0.859 Max 0.765 0.940 0.844 0.919 0.701 0.795 0.820 0.823 Min 0.901 0.787 0.840 0.805 0.910 0.854 0.847 0.848 Average Max&Min 0.793 0.936 0.859 0.918 0.747 0.824 0.841 0.843 Majority Voting 0.765 0.940 0.844 0.919 0.701 0.795 0.820 0.823 Table 3." ></td>
	<td class="line x" title="174:180	Ensemble results for CN & GoogleEN & YahooEN 5 Conclusion and Future Work This paper proposes a novel approach to use English sentiment resources for Chinese sentiment analysis by employing machine translation and ensemble techniques." ></td>
	<td class="line x" title="175:180	Chinese reviews are translated into English reviews and the analysis results of both Chinese reviews and English reviews are combined to improve the overall accuracy." ></td>
	<td class="line x" title="176:180	Experimental results demonstrate the encouraging performance of the proposed approach." ></td>
	<td class="line x" title="177:180	In future work, more additional English resources will be used to further improve the results." ></td>
	<td class="line x" title="178:180	We will also apply the idea to supervised Chinese sentiment analysis." ></td>
	<td class="line x" title="179:180	Acknowledgments This work was supported by the National Science Foundation of China (No.60703064), the Research Fund for the Doctoral Program of Higher Education of China (No.20070001059) and the National High Technology Research and Development Program of China (No.2008AA01Z421)." ></td>
	<td class="line x" title="180:180	We also thank the anonymous reviewers for their useful comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="I08-1039
Learning to Shift the Polarity of Words for Sentiment Classification
Ikeda, Daisuke;Takamura, Hiroya;Ratinov, Lev;Okumura, Manabu;"></td>
	<td class="line x" title="1:199	Learning to Shift the Polarity of Words for Sentiment Classification Daisuke Ikeday Hiroya Takamuraz Lev-Arie Ratinovyy Manabu Okumuraz yDepartment of Computational Intelligence and Systems Science, Tokyo Institute of Technology ikeda@lr.pi.titech.ac.jp yyDepartment of Computer Science, University of Illinois at Urbana-Champaign ratinov2@uiuc.edu zPrecision and Intelligence Laboratory, Tokyo Institute of Technology ftakamura,okug@pi.titech.ac.jp Abstract We propose a machine learning based method of sentiment classification of sentences using word-level polarity." ></td>
	<td class="line x" title="2:199	The polarities of words in a sentence are not always the same as that of the sentence, because there can be polarity-shifters such as negation expressions." ></td>
	<td class="line x" title="3:199	The proposed method models the polarity-shifters." ></td>
	<td class="line x" title="4:199	Our model can be trained in two different ways: word-wise and sentence-wise learning." ></td>
	<td class="line x" title="5:199	In sentence-wise learning, the model can be trained so that the prediction of sentence polarities should be accurate." ></td>
	<td class="line x" title="6:199	The model can also be combined with features used in previous work such as bag-of-words and n-grams." ></td>
	<td class="line x" title="7:199	We empirically show that our method almost always improves the performance of sentiment classification of sentences especially when we have only small amount of training data." ></td>
	<td class="line x" title="8:199	1 Introduction Due to the recent popularity of the internet, individuals have been able to provide various information to the public easily and actively (e.g., by weblogs or online bulletin boards)." ></td>
	<td class="line x" title="9:199	The information often includes opinions or sentiments on a variety of things such as new products." ></td>
	<td class="line x" title="10:199	A huge amount of work has been devoted to analysis of the information, which is called sentiment analysis." ></td>
	<td class="line x" title="11:199	The sentiment analysis has been done at different levels including words, sentences, and documents." ></td>
	<td class="line x" title="12:199	Among them, we focus on the sentiment classification of sentences, the task to classify sentences into positive or negative, because this task is fundamental and has a wide applicability in sentiment analysis." ></td>
	<td class="line x" title="13:199	For example, we can retrieve individuals opinions that are related to a product and can find whether they have the positive attitude to the product." ></td>
	<td class="line x" title="14:199	There has been much work on the identification of sentiment polarity of words." ></td>
	<td class="line x" title="15:199	For instance, beautiful is positively oriented, while dirty is negatively oriented." ></td>
	<td class="line x" title="16:199	We use the term sentiment words to refer to those words that are listed in a predefined polarity dictionary." ></td>
	<td class="line x" title="17:199	Sentiment words are a basic resource for sentiment analysis and thus believed to have a great potential for applications." ></td>
	<td class="line x" title="18:199	However, it is still an open problem how we can effectively use sentiment words to improve performance of sentiment classification of sentences or documents." ></td>
	<td class="line x" title="19:199	The simplest way for that purpose would be the majority voting by the number of positive words and the number of negative words in the given sentence." ></td>
	<td class="line x" title="20:199	However, the polarities of words in a sentence are not always the same as that of the sentence, because there can be polarity-shifters such as negation expressions." ></td>
	<td class="line x" title="21:199	This inconsistency of word-level polarity and sentence-level polarity often causes errors in classification by the simple majority voting method." ></td>
	<td class="line x" title="22:199	A manual list of polarity-shifters, which are the words that can shift the sentiment polarity of another word (e.g., negations), has been suggested." ></td>
	<td class="line x" title="23:199	However, it has limitations due to the diversity of expressions." ></td>
	<td class="line x" title="24:199	Therefore, we propose a machine learning based method that models the polarity-shifters." ></td>
	<td class="line x" title="25:199	The model can be trained in two different ways: word-wise 296 and sentence-wise." ></td>
	<td class="line x" title="26:199	While the word-wise learning focuses on the prediction of polarity shifts, the sentence-wise learning focuses more on the prediction of sentence polarities." ></td>
	<td class="line x" title="27:199	The model can also be combined with features used in previous work such as bag-of-words, n-grams and dependency trees." ></td>
	<td class="line x" title="28:199	We empirically show that our method almost always improves the performance of sentiment classification of sentences especially when we have only small amount of training data." ></td>
	<td class="line x" title="29:199	The rest of the paper is organized as follows." ></td>
	<td class="line x" title="30:199	In Section 2, we briefly present the related work." ></td>
	<td class="line x" title="31:199	In Section 3, we discuss well-known methods that use word-level polarities and describe our motivation." ></td>
	<td class="line x" title="32:199	In Section 4, we describe our proposed model, how to train the model, and how to classify sentences using the model." ></td>
	<td class="line x" title="33:199	We present our experiments and results in Section 5." ></td>
	<td class="line x" title="34:199	Finally in Section 6, we conclude our work and mention possible future work." ></td>
	<td class="line xc" title="35:199	2 Related Work Supervised machine learning methods including Support Vector Machines (SVM) are often used in sentiment analysis and shown to be very promising (Pang et al., 2002; Matsumoto et al., 2005; Kudo and Matsumoto, 2004; Mullen and Collier, 2004; Gamon, 2004)." ></td>
	<td class="line x" title="36:199	One of the advantages of these methods is that a wide variety of features such as dependency trees and sequences of words can easily be incorporated (Matsumoto et al., 2005; Kudo and Matsumoto, 2004; Pang et al., 2002)." ></td>
	<td class="line x" title="37:199	Our attempt in this paper is not to use the information included in those substructures of sentences, but to use the word-level polarities, which is a resource usually at hand." ></td>
	<td class="line x" title="38:199	Thus our work is an instantiation of the idea to use a resource on one linguistic layer (e.g., word level) to the analysis of another layer (sentence level)." ></td>
	<td class="line x" title="39:199	There have been some pieces of work which focus on multiple levels in text." ></td>
	<td class="line x" title="40:199	Mao and Lebanon (2006) proposed a method that captures local sentiment flow in documents using isotonic conditional random fields." ></td>
	<td class="line oc" title="41:199	Pang and Lee (2004) proposed to eliminate objective sentences before the sentiment classification of documents." ></td>
	<td class="line x" title="42:199	McDonald et al.(2007) proposed a model for classifying sentences and documents simultaneously." ></td>
	<td class="line x" title="44:199	They experimented with joint classification of subjectivity for sentence-level, and sentiment for document-level, and reported that their model obtained higher accuracy than the standard document classification model." ></td>
	<td class="line x" title="45:199	Although these pieces of work aim to predict not sentence-level but document-level sentiments, their concepts are similar to ours." ></td>
	<td class="line x" title="46:199	However, all the above methods require annotated corpora for all levels, such as both subjectivity for sentences and sentiments for documents, which are fairly expensive to obtain." ></td>
	<td class="line x" title="47:199	Although we also focus on two different layers, our method does not require such expensive labeled data." ></td>
	<td class="line x" title="48:199	What we require is just sentence-level labeled training data and a polarity dictionary of sentiment words." ></td>
	<td class="line x" title="49:199	3 Simple Voting by Sentiment Words One of the simplest ways to classify sentences using word-level polarities would be a majority voting, where the occurrences of positive words and those of negative words in the given sentence are counted and compared with each other." ></td>
	<td class="line x" title="50:199	However, this majority voting method has several weaknesses." ></td>
	<td class="line x" title="51:199	First, the majority voting cannot take into account at all the phenomenon that the word-level polarity is not always the same as the polarity of the sentence." ></td>
	<td class="line x" title="52:199	Consider the following example: I have not had any distortion problems with this phone and am more pleased with this phone than any Ive used before." ></td>
	<td class="line x" title="53:199	where negative words are underlined and positive words are double-underlined." ></td>
	<td class="line x" title="54:199	The example sentence has the positive polarity, though it locally contains negative words." ></td>
	<td class="line x" title="55:199	The majority voting would misclassify it because of the two negative words." ></td>
	<td class="line x" title="56:199	This kind of inconsistency between sentence-level polarity and word-level polarity often occurs and causes errors in the majority voting." ></td>
	<td class="line x" title="57:199	The reason is that the majority voting cannot take into account negation expressions or adversative conjunctions, e.g., I have not had any  in the example above." ></td>
	<td class="line x" title="58:199	Therefore, taking such polarity-shifting into account is important for classification of sentences using a polarity dictionary." ></td>
	<td class="line x" title="59:199	To circumvent this problem, Kennedy and Inkpen (2006) and Hu and Liu (2004) proposed to use a manually-constructed list of polarity-shifters." ></td>
	<td class="line x" title="60:199	However, it has limitations due to the diversity of expressions." ></td>
	<td class="line x" title="61:199	297 Another weakness of the majority voting is that it cannot be easily combined with existing methods that use the n-gram model or tree structures of the sentence as features." ></td>
	<td class="line x" title="62:199	The method we propose here can easily be combined with existing methods and show better performance." ></td>
	<td class="line x" title="63:199	4 Word-Level Polarity-Shifting Model We assume that when the polarity of a word is different from the polarity of the sentence, the polarity of the word is shifted by its context to adapt to the polarity of the sentence." ></td>
	<td class="line x" title="64:199	Capturing such polarityshifts will improve the classification performance of the majority voting classifier as well as of more sophisticated classifiers." ></td>
	<td class="line x" title="65:199	In this paper, we propose a word polarity-shifting model to capture such phenomena." ></td>
	<td class="line x" title="66:199	This model is a kind of binary classification model which determines whether the polarity is shifted by its context." ></td>
	<td class="line x" title="67:199	The model assigns a score sshift(x,S) to the sentiment word x in the sentence S. If the polarity of x is shifted in S, sshift(x,S) > 0." ></td>
	<td class="line x" title="68:199	If the polarity of x is not shifted in S, sshift(x,S)  0." ></td>
	<td class="line x" title="69:199	Let w be a parameter vector of the model and ` be a pre-defined feature function." ></td>
	<td class="line x" title="70:199	Function sshift is defined as sshift(x,S) = w`(x,S)." ></td>
	<td class="line x" title="71:199	(1) Since this model is a linear discriminative model, there are well-known algorithms to estimate the parameters of the model." ></td>
	<td class="line x" title="72:199	Usually, such models are trained with each occurrence of words as one instance (word-wise learning)." ></td>
	<td class="line x" title="73:199	However, we can train our model more effectively with each sentence being one instance (sentencewise learning)." ></td>
	<td class="line x" title="74:199	In this section, we describe how to train our model in two different ways and how to apply the model to a sentence classification." ></td>
	<td class="line x" title="75:199	4.1 Word-wise Learning In this learning method, we train the word-level polarity-shift model with each occurrence of sentiment words being an instance." ></td>
	<td class="line x" title="76:199	Training examples are automatically extracted by finding sentiment words in labeled sentences." ></td>
	<td class="line x" title="77:199	In the example of Section 3, for instance, both negative words (distortion or problems) and a positive word (pleased) appear in a positive sentence." ></td>
	<td class="line x" title="78:199	We regard distortion and problems, whose polarities are different from that of the sentence, as belonging to the polarityshifted class." ></td>
	<td class="line x" title="79:199	On the contrary, we regard pleased, whose polarity is the same as that of the sentence, as not belonging to polarity-shifted class." ></td>
	<td class="line x" title="80:199	We can use the majority voting by those (possibly polarity-shifted) sentiment words." ></td>
	<td class="line x" title="81:199	Specifically, we first classify each sentiment word in the sentence according to whether the polarity is shifted or not." ></td>
	<td class="line x" title="82:199	Then we use the majority voting to determine the polarity of the sentence." ></td>
	<td class="line x" title="83:199	If the first classifier classifies a positive word into the polarity-shifted class, we treat the word as a negative one." ></td>
	<td class="line x" title="84:199	We expect that the majority voting with polarity-shifting will outperform the simple majority voting without polarityshifting." ></td>
	<td class="line x" title="85:199	We actually use the weighted majority voting, where the polarity-shifting score for each sentiment word is used as the weight of the vote by the word." ></td>
	<td class="line x" title="86:199	We expect that the score works as a confidence measure." ></td>
	<td class="line x" title="87:199	We can formulate this method as follows." ></td>
	<td class="line x" title="88:199	Here, N and P are respectively defined as the sets of negative sentiment words and positive sentiment words." ></td>
	<td class="line x" title="89:199	For instance, x 2 N means that x is a negative word." ></td>
	<td class="line x" title="90:199	We also write x 2 S to express that the word x occurs in S. First, let us define two scores, scorep(S) and scoren(S), for the input sentence S. The scorep(S) and the scoren(S) respectively represent the number of votes for S being positive and the number of votes for S being negative." ></td>
	<td class="line x" title="91:199	If scorep(S) > scoren(S), we regard the sentence S as having the positive polarity, otherwise negative." ></td>
	<td class="line x" title="92:199	We suppose that the following relations hold for the scores: scorep(S) =X x2P\S sshift(x,S) + X x2N\S sshift(x,S), (2) scoren(S) =X x2P\S sshift(x,S) + X x2N\S sshift(x,S)." ></td>
	<td class="line x" title="93:199	(3) When either a polarity-unchanged positive word (sshift(x,S)  0) or a polarity-shifted negative word occurs in the sentence S, scorep(S) increases." ></td>
	<td class="line x" title="94:199	We can easily obtain the following relation between two scores: scorep(S) = scoren(S)." ></td>
	<td class="line x" title="95:199	(4) 298 Since, according to this relation, scorep(S) > scoren(S) is equivalent to scorep(S) > 0, we use only scorep(S) for the rest of this paper." ></td>
	<td class="line x" title="96:199	4.2 Sentence-wise Learning The equation (2) can be rewritten as scorep(S) = X x2S sshift(x,S)I(x) = X x2S w`(x,S)I(x) = w (X x2S `(x,S)I(x) ) , (5) where I(x) is the function defined as follows: I(x) = 8 >< >: +1 if x 2 N, 1 if x 2 P, 0 otherwise." ></td>
	<td class="line x" title="97:199	(6) This scorep(S) can also be seen as a linear discriminative model and the parameters of the model can be estimated directly (i.e., without carrying out wordwise learning)." ></td>
	<td class="line x" title="98:199	Each labeled sentence in a corpus can be used as a training instance for the model." ></td>
	<td class="line x" title="99:199	In this method, the model is learned so that the predictive ability for sentence classification is optimized, instead of the predictive ability for polarityshifting." ></td>
	<td class="line x" title="100:199	Therefore, this model can remain indecisive on the classification of word instances that have little contextual evidence about whether polarityshifting occurs or not." ></td>
	<td class="line x" title="101:199	The model can rely more heavily on word instances that have much evidence." ></td>
	<td class="line x" title="102:199	In contrast, the word-wise learning trains the model with all the sentiment words appearing in a corpus." ></td>
	<td class="line x" title="103:199	It is assumed here that all the sentiment words have relations with the sentence-level polarity, and that we can always find the evidence of the phenomena that the polarity of a word is different from that of a sentence." ></td>
	<td class="line x" title="104:199	Obviously, this assumption is not always correct." ></td>
	<td class="line x" title="105:199	As a result, the word-wise learning sometimes puts a large weight on a context word that is irrelevant to the polarity-shifting." ></td>
	<td class="line x" title="106:199	This might degrade the performance of sentence classification as well as of polarity-shifting." ></td>
	<td class="line x" title="107:199	4.3 Hybrid Model Both methods described in Sections 4.1 and 4.2 are to predict the sentence-level polarity only with the word-level polarity." ></td>
	<td class="line x" title="108:199	On the other hand, several methods that use another set of features, for example, bag-of-words, n-grams or dependency trees, were proposed for the sentence or document classification tasks." ></td>
	<td class="line x" title="109:199	We propose to combine our method with existing methods." ></td>
	<td class="line x" title="110:199	We refer to it as hybrid model." ></td>
	<td class="line x" title="111:199	In recent work, discriminative models including SVM are often used with many different features." ></td>
	<td class="line x" title="112:199	These methods are generally represented as score0p(X) = w0 `0(X), (7) where X indicates the target of classification, for example, a sentence or a document." ></td>
	<td class="line x" title="113:199	If score0p(X) > 0, X is classified into the target class." ></td>
	<td class="line x" title="114:199	`0(X) is a feature function." ></td>
	<td class="line x" title="115:199	When the method uses the bag-ofwords model, `0 maps X to a vector with each element corresponding to a word." ></td>
	<td class="line x" title="116:199	Here, we define new score function scorecomb(S) as a linear combination of scorep(S), the score function of our sentence-wise learning, and score0p(S), the score function of an existing method." ></td>
	<td class="line x" title="117:199	Using this, we can write the function as scorecomb(S) = scorep(S) + (1  )score0p(S) = X x2S w`(x,S)I(x) + (1  )w0 `0(S) = wcomb * X x2S `(x,S)I(x), (1  )`0(S) + ." ></td>
	<td class="line x" title="118:199	(8) Note that hi indicates the concatenation of two vectors, wcomb is defined as hw, w0i and  is a parameter which controls the influence of the word-level polarity-shifting model." ></td>
	<td class="line x" title="119:199	This model is also a discriminative model and we can estimate the parameters with a variety of algorithms including SVMs." ></td>
	<td class="line x" title="120:199	We can incorporate additional information like bagof-words or dependency trees by `0(S)." ></td>
	<td class="line x" title="121:199	4.4 Discussions on the Proposed Model Features such as n-grams or dependency trees can also capture some negations or polarity-shifters." ></td>
	<td class="line x" title="122:199	For example, although satisfy is positive, the bigram model will learn not satisfy as a feature correlated with negative polarity if it appears in the training data." ></td>
	<td class="line x" title="123:199	However, the bigram model cannot generalize the learned knowledge to other features such 299 Table 1: Statistics of the corpus customer movie # of Labeled Sentences 1,700 10,662 Available 1,436 9,492 # of Sentiment Words 3,276 26,493 Inconsistent Words 1,076 10,674 as not great or not disappoint." ></td>
	<td class="line x" title="124:199	On the other hand, our polarity-shifter model learns that the word not causes polarity-shifts." ></td>
	<td class="line x" title="125:199	Therefore, even if there was no not disappoint in training data, our model can determine that not disappoint has correlation with positive class, because the dictionary contains disappoint as a negative word." ></td>
	<td class="line x" title="126:199	For this reason, the polarity-shifting model can be learned even with smaller training data." ></td>
	<td class="line x" title="127:199	What we can obtain from the proposed method is not only a set of polarity-shifters." ></td>
	<td class="line x" title="128:199	We can also obtain the weight vector w, which indicates the strength of each polarity-shifter and is learned so that the predictive ability of sentence classification is optimized especially in the sentence-wise learning." ></td>
	<td class="line x" title="129:199	It is impossible to manually determine such weights for numerous features." ></td>
	<td class="line x" title="130:199	It is also worth noting that all the models proposed in this paper can be represented as a kernel function." ></td>
	<td class="line x" title="131:199	For example, the hybrid model can be seen as the following kernel: Kcomb(S1,S2) =  X xi2S1 X xj2S2 K((xi,S1),(xj,S2)) +(1  )K0(S1,S2)." ></td>
	<td class="line x" title="132:199	(9) Here, K means the kernel function between words and K0 means the kernel function between sentences respectively." ></td>
	<td class="line x" title="133:199	In addition,P xi P xjK((xi,S1),(xj,S2)) can be seen as an instance of convolution kernels, which was proposed by Haussler (1999)." ></td>
	<td class="line x" title="134:199	Convolution kernels are a general class of kernel functions which are calculated on the basis of kernels between substructures of inputs." ></td>
	<td class="line x" title="135:199	Our proposed kernel treats sentences as input, and treats sentiment words as substructures of sentences." ></td>
	<td class="line x" title="136:199	We can use high degree polynomial kernels as both K which is a kernel between substructures, i.e. sentiment words, of sentences, and K0 which is a kernel between sentences to make the classifiers take into consideration the combination of features." ></td>
	<td class="line o" title="137:199	5 Evaluation 5.1 Datasets We used two datasets, customer reviews 1 (Hu and Liu, 2004) and movie reviews 2 (Pang and Lee, 2005) to evaluate sentiment classification of sentences." ></td>
	<td class="line o" title="138:199	Both of these two datasets are often used for evaluation in sentiment analysis researches." ></td>
	<td class="line x" title="139:199	The number of examples and other statistics of the datasets are shown in Table 1." ></td>
	<td class="line x" title="140:199	Our method cannot be applied to sentences which contain no sentiment words." ></td>
	<td class="line x" title="141:199	We therefore eliminated such sentences from the datasets." ></td>
	<td class="line x" title="142:199	Available in Table 1 means the number of examples to which our method can be applied." ></td>
	<td class="line x" title="143:199	Sentiment Words shows the number of sentiment words that are found in the given sentences." ></td>
	<td class="line x" title="144:199	Please remember that sentiment words are defined as those words that are listed in a predefined polarity dictionary in this paper." ></td>
	<td class="line x" title="145:199	Inconsistent Words shows the number of the words whose polarities conflicted with the polarity of the sentence." ></td>
	<td class="line x" title="146:199	We performed 5-fold cross-validation and used the classification accuracy as the evaluation measure." ></td>
	<td class="line x" title="147:199	We extracted sentiment words from General Inquirer (Stone et al., 1996) and constructed a polarity dictionary." ></td>
	<td class="line x" title="148:199	After some preprocessing, the dictionary contains 2,084 positive words and 2,685 negative words." ></td>
	<td class="line x" title="149:199	5.2 Experimental Settings We employed the Max Margin Online Learning Algorithms for parameter estimation of the model (Crammer et al., 2006; McDonald et al., 2007)." ></td>
	<td class="line x" title="150:199	In preliminary experiments, this algorithm yielded equal or better results compared to SVMs." ></td>
	<td class="line x" title="151:199	As the feature representation, `(x,S), of polarity-shifting model, we used the local context of three words to the left and right of the target sentiment word." ></td>
	<td class="line x" title="152:199	We used the polynomial kernel of degree 2 for polarity-shifting model and the linear kernel for oth1http://www.cs.uic.edu/liub/FBS/FBS." ></td>
	<td class="line x" title="153:199	html 2http://www.cs.cornell.edu/people/pabo/ movie-review-data/ 300 Table 2: Experimental results of the sentence classification methods customer movie Baseline 0.638 0.504 BoW 0.790 0.724 2gram 0.809 0.756 3gram 0.800 0.762 Simple-Voting 0.716 0.624 Negation Voting 0.733 0.658 Word-wise 0.783 0.699 Sentence-wise 0.806 0.718 Hybrid BoW 0.827 0.748 Hybrid 2gram 0.840 0.755 Hybrid 3gram 0.837 0.758 Opt 0.840 0.770 ers, and feature vectors are normalized to 1." ></td>
	<td class="line x" title="154:199	In hybrid models, the feature vectors, Px2S `(x,S)I(x) and `0(S) are normalized respectively." ></td>
	<td class="line x" title="155:199	5.3 Comparison of the Methods We compared the following methods:  Baseline classifies all sentences as positive." ></td>
	<td class="line x" title="156:199	 BoW uses unigram features." ></td>
	<td class="line x" title="157:199	2gram uses unigrams and bigrams." ></td>
	<td class="line x" title="158:199	3gram uses unigrams, bigrams, and 3grams." ></td>
	<td class="line x" title="159:199	 Simple-Voting is the most simple majority voting with word-level polarity (Section 3)." ></td>
	<td class="line x" title="160:199	 Negation Voting proposed by Hu and Liu (2004) is the majority voting that takes negations into account." ></td>
	<td class="line x" title="161:199	As negations, we employed not, no, yet, never, none, nobody, nowhere, nothing, and neither, which are taken from (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2006; Hu and Liu, 2004) (Section 3)." ></td>
	<td class="line x" title="162:199	 Word-wise was described in Section 4.1." ></td>
	<td class="line x" title="163:199	 Sentence-wise was described in Section 4.2." ></td>
	<td class="line x" title="164:199	 Hybrid BoW, hybrid 2gram, hybrid 3gram are combinations of sentence-wise model and respectively BoW, 2gram and 3gram (Section 4.3)." ></td>
	<td class="line x" title="165:199	We set  = 0.5." ></td>
	<td class="line x" title="166:199	Table 2 shows the results of these experiments." ></td>
	<td class="line x" title="167:199	Hybrid 3gram, which corresponds to the proposed method, obtained the best accuracy on customer review dataset." ></td>
	<td class="line o" title="168:199	However, on movie review dataset, the proposed method did not outperform 3gram." ></td>
	<td class="line x" title="169:199	In Section 5.4, we will discuss this result in details." ></td>
	<td class="line x" title="170:199	Comparing word-wise to simple-voting, the accuracy increased by about 7 points." ></td>
	<td class="line x" title="171:199	This means that the polarity-shifting model can capture the polarityshifts and it is an important factor for sentiment classification." ></td>
	<td class="line x" title="172:199	In addition, we can see the effectiveness of sentence-wise, by comparing it to word-wise in accuracy." ></td>
	<td class="line x" title="173:199	Opt in Table 2 shows the results of hybrid models with optimal  and combination of models." ></td>
	<td class="line x" title="174:199	The optimal results of hybrid models achieved the best accuracy on both datasets." ></td>
	<td class="line x" title="175:199	We show some dominating polarity-shifters obtained through learning." ></td>
	<td class="line x" title="176:199	We obtained many negations (e.g., no, not, nt, never), modal verbs (e.g., might, would, may), prepositions (e.g., without, despite), comma with a conjunction (e.g., , but as in the case is strong and stylish, but lacks a window), and idiomatic expressions (e.g., hard resist as in it is hard to resist, and real snooze)." ></td>
	<td class="line x" title="177:199	5.4 Effect of Training Data Size When we have a large amount of training data, the ngram classifier can learn well whether each n-gram tends to appear in the positive class or the negative class." ></td>
	<td class="line x" title="178:199	However, when we have only a small amount of training data, the n-gram classifier cannot capture such tendency." ></td>
	<td class="line x" title="179:199	Therefore the external knowledge, such as word-level polarity, could be more valuable information for classification." ></td>
	<td class="line x" title="180:199	Thus it is expected that the sentence-wise model and the hybrid model will outperform n-gram classifier which does not take word-level polarity into account, more largely with few training data." ></td>
	<td class="line x" title="181:199	To verify this conjecture, we conducted experiments by changing the number of the training examples, i.e., the labeled sentences." ></td>
	<td class="line o" title="182:199	We evaluated three models: sentence-wise, 3gram model and hybrid 3gram on both customer review and movie review." ></td>
	<td class="line o" title="183:199	Figures 1 and 2 show the results on customer review and movie review respectively." ></td>
	<td class="line o" title="184:199	When the size of the training data is small, sentence-wise outper301 Figure 1: Experimental results on customer review Figure 2: Experimental results on movie review forms 3gram on both datasets." ></td>
	<td class="line x" title="185:199	We can also see that the advantage of sentence-wise becomes smaller as the amount of training data increases, and that the hybrid 3gram model almost always achieved the best accuracy among the three models." ></td>
	<td class="line x" title="186:199	Similar behaviour was observed when we ran the same experiments with 2gram or BoW model." ></td>
	<td class="line x" title="187:199	From these results, we can conclude that, as we expected above, the wordlevel polarity is especially effective when we have only a limited amount of training data, and that the hybrid model can combine two models effectively." ></td>
	<td class="line x" title="188:199	6 Conclusion We proposed a model that captures the polarityshifting of sentiment words in sentences." ></td>
	<td class="line x" title="189:199	We also presented two different learning methods for the model and proposed an augmented hybrid classifier that is based both on the model and on existing classifiers." ></td>
	<td class="line x" title="190:199	We evaluated our method and reported that the proposed method almost always improved the accuracy of sentence classification compared with other simpler methods." ></td>
	<td class="line x" title="191:199	The improvement was more significant when we have only a limited amount of training data." ></td>
	<td class="line x" title="192:199	For future work, we plan to explore new feature sets appropriate for our model." ></td>
	<td class="line x" title="193:199	The feature sets we used for evaluation in this paper are not necessarily optimal and we can expect a better performance by exploring appropriate features." ></td>
	<td class="line x" title="194:199	For example, dependency relations between words or appearances of conjunctions will be useful." ></td>
	<td class="line x" title="195:199	The position of a word in the given sentence is also an important factor in sentiment analysis (Taboada and Grieve, 2004)." ></td>
	<td class="line x" title="196:199	Furthermore, we should directly take into account the fact that some words do not affect the polarity of the sentence, though the proposed method tackled this problem indirectly." ></td>
	<td class="line x" title="197:199	We cannot avoid this problem to use word-level polarity more effectively." ></td>
	<td class="line x" title="198:199	Lastly, since we proposed a method for the sentence-level sentiment prediction, our next step is to extend the method to the document-level sentiment prediction." ></td>
	<td class="line x" title="199:199	Acknowledgement This research was supported in part by Overseas Advanced Educational Research Practice Support Program by Ministry of Education, Culture, Sports, Science and Technology." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="I08-1040
Unsupervised Classification of Sentiment and Objectivity in Chinese Text
Zagibalov, Taras;Carroll, John A.;"></td>
	<td class="line x" title="1:190	UnsupervisedClassificationofSentimentandObjectivity inChineseText TarasZagibalov JohnCarroll UniversityofSussex DepartmentofInformatics BrightonBN19QH,UK {T.Zagibalov,J.A.Carroll}@sussex.ac.uk Abstract We address the problem of sentiment and objectivity classification of product reviews in Chinese." ></td>
	<td class="line x" title="2:190	Our approach is distinctive in that it treats both positive / negative sentiment and subjectivity / objectivity not as distinct classes but rather as a continuum; we arguethatthis is desirablefrom the perspective of would-be customers who read the reviews." ></td>
	<td class="line x" title="3:190	We use novel unsupervised techniques, including a one-word 'seed' vocabulary and iterative retraining for sentiment processing, and a criterion of 'sentiment density' for determining the extent to which a document is opinionated." ></td>
	<td class="line x" title="4:190	The classifier achieves up to 87% F-measureforsentimentpolaritydetection." ></td>
	<td class="line x" title="5:190	1 Introduction Automatic classification of sentiment has been a focus of a number of recent research efforts (e.g.(Turney, 2002; Pang et al., 2002; Dave at al., 2003)." ></td>
	<td class="line x" title="7:190	An important potential application of such work is in business intelligence: brands and company image are valuable property,so organizations want to know how they are viewed by the media (what the 'spin' is on news stories, and editorials), business analysts (as expressed in stock market reports), customers (for example on product review sites) and their own employees." ></td>
	<td class="line x" title="8:190	Another important application is to help people find out others' views about products they have purchased(e.g. consumer electronics), services and entertainment (e.g. movies), stocks and shares (from investor bulletin boards), and so on." ></td>
	<td class="line x" title="9:190	In the work reported in this paper we focus onproduct reviews, with the intended usersoftheprocessingbeingwould-becustomers." ></td>
	<td class="line x" title="10:190	Our approach is based on the insight that positive and negative sentiments are extreme points in a continuum of sentiment, and that intermediate points in this continuum are of potential interest." ></td>
	<td class="line x" title="11:190	For instance, in one scenario, someone might want to get an idea of the types of things people are saying about a particular product through reading a sample of reviews covering the spectrum from highly positive, through balanced, to highly negative." ></td>
	<td class="line x" title="12:190	(Wecall a review balanced if it is an opinionated text with an undecided or weak sentiment direction)." ></td>
	<td class="line x" title="13:190	In another scenario, a would-be customer might only be interested in reading balanced reviews, since they often present more reasoned arguments with fewer unsupported claims." ></td>
	<td class="line x" title="14:190	Such a person might therefore want to avoid reviews such asExample(1)writtenbyaChinesepurchaserof amobilephone(ourEnglishgloss)." ></td>
	<td class="line x" title="15:190	(1) q" ></td>
	<td class="line x" title="16:190	H HZ l " ></td>
	<td class="line x" title="17:190	Y9" ></td>
	<td class="line x" title="18:190	ml ^ y " ></td>
	<td class="line x" title="19:190	C`" ></td>
	<td class="line x" title="20:190	vrTQ" ></td>
	<td class="line x" title="21:190	 ^1lT9 ." ></td>
	<td class="line x" title="22:190	 Q1" ></td>
	<td class="line x" title="23:190	  P " ></td>
	<td class="line x" title="24:190	 " ></td>
	<td class="line x" title="25:190	q V Thesoftwareisbad,somesentSMSareneverreceivedbytheaddressee;compatibility isalsobad,onsomemobilephonesthereceivedmessagesareinascrambledencoding!Andsometimesthephone'dies'!Photos arehorrible!Itdoesn'thaveacyclicorpro304 grammablealarm-clock,youhavetosetit everytime,howcumbersome!Thebackcoverdoesnotfit!Theoriginalsoftwarehas manyholes!" ></td>
	<td class="line x" title="26:190	In a third scenario, someone might decide they would like only to read opinionated, weakly negative reviews such as Example (2), since these often contain good argumentation while still identifying themostsalientbadaspectsofaproduct." ></td>
	<td class="line x" title="27:190	(2) 0Q w" ></td>
	<td class="line x" title="28:190	A 1 ,#[/?l" ></td>
	<td class="line x" title="29:190	9 .1 .2" ></td>
	<td class="line x" title="30:190	 1{ .29zP" ></td>
	<td class="line x" title="31:190	HH
" ></td>
	<td class="line x" title="32:190	 'HW" ></td>
	<td class="line x" title="33:190	'0W+" ></td>
	<td class="line x" title="34:190	 q" ></td>
	<td class="line x" title="35:190	W" ></td>
	<td class="line x" title="36:190	9L" ></td>
	<td class="line x" title="37:190	 ^'L^9" ></td>
	<td class="line x" title="38:190	h." ></td>
	<td class="line x" title="39:190	Theresponsetimeofthismobileisvery long,MMSshouldbelessthan30kbonlyto bedownloaded,alsoitdoesn'tsupportMP3 ringtones,(while)thebuilt-intunesarenot good,andfromtimetotimeit'dies',but whenIwasbuyingitIreallylikedit:very original,verynicelymatchingredandwhite colours,ithasitsindividuality,alsoit'snot expensive,butwhenuseditalwayscauses trouble,makesone'sheadache The review contains both positive and negative sentiment coveringdifferentaspects oftheproduct, and the fact that it contains a balance of views means that it is likely to be useful for a would-be customer." ></td>
	<td class="line x" title="40:190	Moving beyond review classification, more advanced tasks such as automatic summarization of reviews (e.g. Feiguina & LaPalme, 2007) might also benefit from techniques which could distinguish more shades of sentiment than justabinarypositive/negativedistinction." ></td>
	<td class="line x" title="41:190	A second dimension, orthogonal to positive / negative, is opinionated / unopinionated (or equivalently subjective / objective)." ></td>
	<td class="line x" title="42:190	When shopping for a product, one might be interested in the physical characteristics of the product or what features the product has, rather than opinions about how well these features work or about how well the product as a whole functions." ></td>
	<td class="line x" title="43:190	Thus, if one is looking for a review that contains more factual information than opinion, one might be interested in reviews like Example(3)." ></td>
	<td class="line x" title="44:190	(3) 9$p" ></td>
	<td class="line x" title="45:190	L" ></td>
	<td class="line x" title="46:190	 7" ></td>
	<td class="line x" title="47:190	1" ></td>
	<td class="line x" title="48:190	9 " ></td>
	<td class="line x" title="49:190	 H" ></td>
	<td class="line x" title="50:190	 " ></td>
	<td class="line x" title="51:190	 '" ></td>
	<td class="line x" title="52:190	|AU" ></td>
	<td class="line x" title="53:190	HWy " ></td>
	<td class="line x" title="54:190	8'1 
" ></td>
	<td class="line x" title="55:190	V" ></td>
	<td class="line x" title="56:190	:Y'b (My)overallfeelingaboutthismobileisnot bad,itfeatures:5alarm-clocksthatswitch thephoneon(off),phonebookfor800items (500people),lunarandsolarcalendars, fastswitchingbetweentimeanddatemodes, WAPnetworking,organizer,notebookand soon." ></td>
	<td class="line x" title="57:190	This review is mostly neutral (unopinionated), but contains information that could be useful to a would-be customer which might not be in a product specification document, e.g. fast switching between different operating modes." ></td>
	<td class="line x" title="58:190	Similarly,wouldbe customers might be interested in retrieving completely unopinionated documents such as technical descriptions and usermanuals.Again, aswith sentiment classification, we argue that opinionated and unopinionated texts are not easily distinguishable separate sets, but form a continuum." ></td>
	<td class="line x" title="59:190	In this continuum, intermediate points are of interest as wellastheextremes." ></td>
	<td class="line x" title="60:190	A major obstacle for automatic classification of sentiment and objectivity is lack of training data, which limits the applicability of approaches based on supervised machine learning." ></td>
	<td class="line x" title="61:190	With the rapid growth in textual data and the emergence of new domains of knowledge it is virtually impossible to maintain corpora of tagged data that cover all  or even most  areas of interest." ></td>
	<td class="line x" title="62:190	The cost of manual taggingalsoaddstotheproblem.Reusingthesame corpus for training classifiers for new domains is also not effective: several studies report decreased accuracy in cross-domain classification (Engstrm, 2004; Aue & Gamon, 2005) a similar problem has also been observed in classification of documents createdoverdifferenttimeperiods(Read,2005)." ></td>
	<td class="line x" title="63:190	Inthispaperwedescribeanunsupervised classification technique which is able to build its own sentiment vocabulary starting from a very small seed vocabulary, using iterative retraining to enlarge the vocabulary.In order to avoid problems of domain dependence, the vocabulary is built using textfrom the samesourceas thetextwhich isto be classified." ></td>
	<td class="line x" title="64:190	In this paper we work with Chinese, but using a very small seed vocabulary may mean that this approach would in principle need very little linguistic adjustment to be applied to a different 305 language." ></td>
	<td class="line x" title="65:190	Written Chinese has some specific features, one of which is the absence of explicitly markedwordboundaries,whichmakesword-based processing problematic." ></td>
	<td class="line x" title="66:190	In keeping with our unsupervised, knowledge-poor approach, we do not use any preliminary word segmentation tools or higher levelgrammaticalanalysis." ></td>
	<td class="line x" title="67:190	The paper is structured as follows." ></td>
	<td class="line x" title="68:190	Section 2 reviews related work in sentiment classification and more generally in unsupervised training of classifiers." ></td>
	<td class="line x" title="69:190	Section 3 describes our datasets, and Section 4 the techniques we use for unsupervised classification and iterative retraining." ></td>
	<td class="line x" title="70:190	Sections 5 and 6 describe a number of experiments into how well the approacheswork,andSection7concludes." ></td>
	<td class="line x" title="71:190	2 RelatedWork 2.1 Sentiment Classification Most previous work on the problem of categorizing opinionated texts has focused on the binary classification of positive and negative sentiment (Turney, 2002; Pang et al., 2002; Dave at al., 2003)." ></td>
	<td class="line x" title="72:190	However, Pang & Lee (2005) describe an approach closer to ours in which they determine an author's evaluation with respect to a multi-point scale, similar to the 'five-star' sentiment scale widely used on review sites." ></td>
	<td class="line x" title="73:190	However, authors of reviews are inconsistent in assigning fine-grained ratings and quite often star systems are not consistent between critics." ></td>
	<td class="line x" title="74:190	This makes their approach very author-dependent." ></td>
	<td class="line x" title="75:190	The main differences are that Pang and Lee use discrete classes (although more than two), not a continuum as in our approach, and use supervisedmachine learningrather than unsupervised techniques." ></td>
	<td class="line x" title="76:190	A similar approach was adopted by Hagedorn et al.(2007), applied to news stories: they defined five classes encoding sentiment intensity and trained their classifier on a manually tagged training corpus." ></td>
	<td class="line x" title="78:190	They note that world knowledge is necessary for accurate classificationinsuchopen-endeddomains." ></td>
	<td class="line oc" title="79:190	There has also been previous work on determining whether a given text is factual or expresses opinion (Yu& Hatzivassiloglu, 2003; Pang & Lee, 2004); again this work uses a binary distinction, and supervised rather than unsupervised approaches." ></td>
	<td class="line x" title="80:190	Recent work on classification of terms with respect to opinion (Esuli & Sebastiani, 2006) uses a three-category system to characterize the opinionrelated properties of word meanings, assigning numerical scores to Positive, Negative and Objective categories." ></td>
	<td class="line x" title="81:190	The visualization of these scores somewhat resembles our graphs in Section 5, although we use two orthogonal scales rather than three categories; we are also concerned with classification ofdocumentsratherthanterms." ></td>
	<td class="line x" title="82:190	2.2 Unsupervised Classification Abney (2002) compares two major kinds of unsupervised approachto classification (co-training and the Yarowskyalgorithm)." ></td>
	<td class="line x" title="83:190	As we do not use multiple classifiers our approach is quite far from cotraining." ></td>
	<td class="line x" title="84:190	But it is close to the paradigm described by Yarowsky (1995) and Turney (2002) as it also employs self-training based on a relatively small seed data set which is incrementally enlarged with unlabelled samples." ></td>
	<td class="line x" title="85:190	But our approach does not use point-wise mutual information." ></td>
	<td class="line x" title="86:190	Instead we use relative frequencies of newly found features in a training subcorpus produced by the previous iteration of the classifier." ></td>
	<td class="line x" title="87:190	Wealso use the smallest possible seed vocabulary, containing just a single word; however there are no restrictions regarding the maximum number of items in the seed vocabulary." ></td>
	<td class="line x" title="88:190	3 Data 3.1 Seed Vocabulary Our approach starts out with a seed vocabulary consisting of a single word, z (good)." ></td>
	<td class="line x" title="89:190	This word is tagged as a positive vocabulary item; initially there are no negative items." ></td>
	<td class="line x" title="90:190	The choice of word was arbitrary, and other words with strongly positive or negative meaning would also be plausible seeds." ></td>
	<td class="line x" title="91:190	Indeed, z might not be the best possible seed, as it is relatively ambiguous: in some contexts it means to like or acts as the adverbial very, and is often used as part of other words (although usually contributing a positive meaning)." ></td>
	<td class="line x" title="92:190	But since it is one of the most frequent units in the Chinese language, it is likely to occur in a relatively large number of reviews, which is important for the rapidgrowthofthevocabularylist." ></td>
	<td class="line x" title="93:190	3.2 TestCorpus Our test corpus is derived from product reviews harvested from the website IT1681." ></td>
	<td class="line x" title="94:190	All the reviews were tagged by their authors as either positive or negative overall." ></td>
	<td class="line x" title="95:190	Most reviews consist of two or three distinct parts: positive opinions, negative opinions, and comments ('other')  although some 1http://product.it168.com 306 reviews have only one part." ></td>
	<td class="line x" title="96:190	Weremoved duplicate reviews automatically using approximate matching, giving a corpus of 29531 reviews of which 23122 are positive (78%) and 6409 are negative (22%)." ></td>
	<td class="line x" title="97:190	The total number of different products in the corpus is 10631, the number of product categories is 255, and most of the reviewed products are either software products or consumer electronics." ></td>
	<td class="line x" title="98:190	Unfortunately, it appears that some users misused the sentiment tagging facility on the website so quite a lot of reviews have incorrect tags." ></td>
	<td class="line x" title="99:190	However, the parts of the reviews are much more reliably identified as being positive or negative so we usedtheseastheitemsofthetestcorpus.Intheexperiments described in this paper we used 2317 reviews of mobile phones of which 1158 are negative and 1159 are positive." ></td>
	<td class="line x" title="100:190	Thus random choice would have approximately 50% accuracy if all itemsweretaggedeitherasnegativeorpositive2." ></td>
	<td class="line x" title="101:190	4 Method 4.1 Sentiment Classification As discussed in Section 1, we do not carry out any word segmentation or grammatical processing of input documents." ></td>
	<td class="line x" title="102:190	We use a very broad notion of words (or phrases) in the Chinese language." ></td>
	<td class="line x" title="103:190	The basic units of processing are 'lexical items', each of which is a sequence of one or more Chinese characters excluding punctuation marks (which may actually form part of a word, a whole word or a sequence of words), and `zones', each of which is a sequence of characters delimited by punctuation marks." ></td>
	<td class="line x" title="104:190	Each zone is classified as either positive or negative based whether positive or negative vocabulary items predominate." ></td>
	<td class="line x" title="105:190	In more detail, a simple maximum match algorithm is used to find all lexical items (character sequences) in the zone that are in the vocabulary list." ></td>
	<td class="line x" title="106:190	As there are two parts of the vocabulary (positiveand negative), wecorrespondinglycalculatetwoscoresusingEquation(1)3, Si= LdL phrase Sd Nd (1) where Ld is the length in characters of a matching lexical item, Lphrase is the length of the current zone 2Thiscorpusispubliclyavailableathttp://www.informatics." ></td>
	<td class="line x" title="107:190	sussex.ac.uk/users/tz21/it168test.zip 3Inthefirstiteration,whenwehaveonlyoneiteminthevocabulary,negativezonesarefoundbymeansofthenegation check(sonot+good=negativeitem)." ></td>
	<td class="line x" title="108:190	in characters, Sd is the current sentiment score of the matching lexical item (initially1.0), and Nd is a negation check coefficient.Thenegation check is a regular expression which determines if the lexical item is preceded by a negation within its enclosing zone." ></td>
	<td class="line x" title="109:190	If a negation is found then Nd is set to 1." ></td>
	<td class="line x" title="110:190	The check looks for six frequently occurring negations:(bu),  (buhui),  (meiyou),  (baituo),  (mianqu),and E  (bimian)." ></td>
	<td class="line x" title="111:190	The sentiment score of a zone is the sum of sentiment scores of all the items found in it." ></td>
	<td class="line x" title="112:190	In fact there are twocompeting sentiment scores for every zone: one positive (the sum of all scores of items found in the positive part of the vocabulary list) and one negative (the sum of the scores for the items in the negative part)." ></td>
	<td class="line x" title="113:190	The sentiment direction of a zone is determined from the maximum of the absolutevaluesofthetwocompetingscoresforthe zone." ></td>
	<td class="line x" title="114:190	This procedure is applied to all zones in a document, classifying each zone as positive, negative, or neither (in cases where there are no positive or negative vocabulary items in the zone)." ></td>
	<td class="line x" title="115:190	To determine the sentiment direction of the whole document, the classifier computes the difference between the number of positive and negative zones." ></td>
	<td class="line x" title="116:190	If the result is greater than zero the document is classifiedas positive,andviceversa.Iftheresultis zero the document is balanced or neutral for sentiment." ></td>
	<td class="line x" title="117:190	4.2 IterativeRetraining The task of iterative retraining is to enlargethe initial seed vocabulary (consisting of a single wordas discussed in Section 3.1) into a comprehensive vocabulary list of sentiment-bearing lexical items." ></td>
	<td class="line x" title="118:190	In each iteration, the current version of the classifier isrunontheproductreviewcorpustoclassifyeach document, resulting in a training subcorpus of positive and a negative documents." ></td>
	<td class="line x" title="119:190	The subcorpus is used to adjust the scores of existing positive and negative vocabulary items and to find new itemsto beincludedinthevocabulary." ></td>
	<td class="line x" title="120:190	Eachlexicalitemthatoccursatleasttwiceinthe corpus is a candidate for inclusion in the vocabulary list." ></td>
	<td class="line x" title="121:190	After candidate items are found, the system calculates their relative frequencies in both the positive and negative parts of the current training subcorpus." ></td>
	<td class="line x" title="122:190	The system also checks for negation whilecountingoccurrences:ifalexicalitemispreceded by a negation, its count is reduced by one." ></td>
	<td class="line x" title="123:190	This results in negative counts (and thus negative relativefrequenciesandscores)forthoseitemsthat 307 are usually used with negation; for example,     (thequalityisfartoobad)isinthe positive part of the vocabulary with a score of 1.70." ></td>
	<td class="line x" title="124:190	This meansthattheitemwasfoundinreviewsclassified by the system as positive but it was preceded by a negation." ></td>
	<td class="line x" title="125:190	If during classification this item is found in a document it will reduce the positive score for that document (as it is in the positive part of the vocabulary), unlessthe item is preceded bya negation." ></td>
	<td class="line x" title="126:190	In this situation the score will be reversed (multiplied by 1), and the positive score will be increasedseeEquation(1)above." ></td>
	<td class="line x" title="127:190	Forallcandidateitemswecomparetheirrelative frequencies in the positive and negative documents inthesubcorpususingEquation(2)." ></td>
	<td class="line x" title="128:190	difference= FpFnF pFn/2 (2) If difference < 1, then the frequencies are similar and the item does not have enough distinguishing power,so it is not included in the vocabulary.Otherwise the the sentiment score of the item is (re-) calculated  according to Equation (3) for positive items,andanalogouslyfornegativeitems." ></td>
	<td class="line x" title="129:190	Fp FpFn (3) Finally, the adjusted vocabulary list with the new scoresisreadyforthenextiteration." ></td>
	<td class="line x" title="130:190	4.3 ObjectivityClassification Given a sentiment classification for each zone in a document, we compute sentiment density as the proportion of opinionated zones with respect to the total number of zones in the document." ></td>
	<td class="line x" title="131:190	Sentiment densitymeasurestheproportionofopinionatedtext in a document, and thus the degree to which the documentasawholeisopinionated." ></td>
	<td class="line x" title="132:190	It should be noted that neither sentiment score nor sentiment density are absolute values, but are relative and only valid for comparing one document with other." ></td>
	<td class="line x" title="133:190	Thus, a sentiment density of 0.5 does not mean that the review is half opinionated, half not." ></td>
	<td class="line x" title="134:190	It means that the review is less opinionatedthanareviewwithdensity0.9." ></td>
	<td class="line x" title="135:190	5 Experiments We ran the system on the product review corpus (Section 3.2) for 20iterations." ></td>
	<td class="line x" title="136:190	Theresults for binary sentiment classification are shown in Table 1." ></td>
	<td class="line x" title="137:190	Wesee increasing F-measure up to iteration 18, after which both precision and recall start to descrease; we therefore use the version of the classifier as it stood after iteration 184." ></td>
	<td class="line x" title="138:190	These figures are only indicative of the classification accuracy of the system." ></td>
	<td class="line x" title="139:190	Accuracy might be lower for unseen text, although since our approach is unsupervised we could in principle perform further retraining iterations on any sample of new text to tune the vocabularylisttoit." ></td>
	<td class="line x" title="140:190	We also computed a (strong) baseline, using as the vocabulary list the NTU Sentiment Dictionary (Ku et al., 2006)5 which is intended to contain only sentiment-related words and phrases." ></td>
	<td class="line x" title="141:190	We assigned each positive and negative vocabulary item a score of 1 or 1 respectively." ></td>
	<td class="line x" title="142:190	This setup achieved 87.77 precision and 77.09 recall on the product review corpus." ></td>
	<td class="line x" title="143:190	InSection1wearguedthatsentimentandobjectivityshouldbothbeconsideredascontinuums,not Table1.Resultsforbinarysentimentclassificationduringiterativeretraining." ></td>
	<td class="line x" title="144:190	4Thesizeofthesentimentvocabularyafteriteration18was 22530(13462positiveand9068negative)." ></td>
	<td class="line x" title="145:190	5Kuetal.automaticallygeneratedthedictionarybyenlarging aninitialmanuallycreatedseedvocabularybyconsultingtwo thesauri,includingtong2yi4ci2ci2lin2andthe AcademiaSinicaBilingualOntologicalWordNet3." ></td>
	<td class="line x" title="146:190	Iteration Precision Recall F-measure 1 77.62 28.43 41.62 2 76.15 73.81 74.96 3 81.15 80.07 80.61 4 83.54 82.79 83.16 5 84.66 83.78 84.22 6 85.51 84.77 85.14 7 86.59 85.76 86.17 8 86.78 86.11 86.44 9 87.15 86.32 86.74 10 87.01 86.37 86.69 11 86.9 86.15 86.53 12 87.05 86.41 86.73 13 86.87 86.19 86.53 14 87.35 86.67 87.01 15 87.13 86.45 86.79 16 87.14 86.5 86.82 17 86.8 86.24 86.52 18 87.57 86.89 87.22 19 87.23 86.67 86.95 20 87.18 86.54 86.86 308 binary distinctions." ></td>
	<td class="line x" title="147:190	Section 4.1 describes how our approach compares the number of positive and negativezonesforadocumentandtreatsthedifference as a measure of the 'positivity' or 'negativity' of a review.The document in Example(2), with 12 zones, is assigned a score of 1 (the least negative score possible): the review contains some positive sentiment but the overall sentiment direction of the review is negative." ></td>
	<td class="line x" title="148:190	In contrast, Example (1) is identified as a highly negative review,as would be expected,withascoreof8,from11zones." ></td>
	<td class="line x" title="149:190	Similarly, with regard to objectivity, the sentiment density of the text in Example (3) is 0.53, which reflects its more factual character compared to Example (1), which has a score of 0.91." ></td>
	<td class="line x" title="150:190	Wecan represent sentiment and objectivity on the followingscales: Negative Balanced Positive Unopinionated Neutral Opinionated The scales are orthogonal, so we can combine themintoasinglecoordinatesystem: Opinionated Negative Positive We would expect most product reviews to be placedtowardsthetopofthethecoordinatesystem (i.e.opinionated),andstretchfromlefttoright." ></td>
	<td class="line x" title="151:190	Figure1plotstheresultsofsentimentandobjectivityclassificationofthetestcorpusinthistwodimensional coordinate system, where X represents sentiment (with scores scaled with respect to the number of zones so that 100 is the most negative possible and +100 the most positive), and Y represents sentiment density (0 being unopinionatedand 1beinghighlyopinionated)." ></td>
	<td class="line x" title="152:190	Most of the reviewsare located in the upper part of the coordinate system, indicating that they have been classified as opinionated, with either positive or negative sentiment direction." ></td>
	<td class="line x" title="153:190	Looking at the overall shape of the plot, more opinionated documents tend to have more explicit sentiment direction, while less opinionated texts stay closer to the balanced/neutralregion(aroundX=0)." ></td>
	<td class="line x" title="154:190	Figure1.Reviewsclassifiedaccordingto sentiment(Xaxis)anddegreeof opinionation(Yaxis)." ></td>
	<td class="line x" title="155:190	6 Discussion As can be seen in Figure 1, the classifier managed to map the reviews onto the coordinate system." ></td>
	<td class="line x" title="156:190	However, there are very few points in the neutral region, that is, on the same X = 0 line as balanced but with low sentiment density." ></td>
	<td class="line x" title="157:190	By inspection, we know that there are neutral reviews in our data set." ></td>
	<td class="line x" title="158:190	Wetherefore conducted a further experiment to investigate what the problem might be." ></td>
	<td class="line x" title="159:190	We took Wikipedia6 articles written in Chinese on mobile telephony and related issues, as well as several articles about the technology,the market and the history of mobile telecommunications, and split them into small parts (about a paragraph long, to make their size close to the size of the reviews) resulting in a corpus of 115documents, which we assume to be mostly unopinionated." ></td>
	<td class="line x" title="160:190	Weprocessed these documents with the trained classifier and found that they were mapped almost exactly where balanced documentsshouldbe(seeFigure2)." ></td>
	<td class="line x" title="161:190	Most of these documents have weak sentiment direction (X = 5 to +10), but are classified as relatively opinionated (Y > 0.5)." ></td>
	<td class="line x" title="162:190	The former is to be expected, whereas the latter is not." ></td>
	<td class="line x" title="163:190	When investigatingthepossiblereasonsforthisbehaviorwenoticed that the classifier found not only feature descriptions (like mz nice touch) or expressions which describe attitude ( (one) like(s)), butalsoproductfeatures(forexample,  MMS or jTV) to be opinionated." ></td>
	<td class="line x" title="164:190	This is because the presence of some advanced features such as MMS inmobilephonesisoftenregardedasapositiveby 6www.wikipedia.org -40 -30 -20 -10 0 10 20 30 40 50 60 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 309 Figure2.Classificationofasampleofarticles fromWikipedia." ></td>
	<td class="line x" title="165:190	Figure3.Classificationofasampleofarticles fromWikipedia,usingtheNTUSentiment Dictionaryasthevocabularylist." ></td>
	<td class="line x" title="166:190	authors of reviews." ></td>
	<td class="line x" title="167:190	In addition, the classifier found words that were used in reviews to describe situations connected with a product and its features: for example,  (service)wasoftenusedindescriptionsofquiteunpleasantsituationswhenauserhad toturntoamanufacturer'spost-salesserviceforrepairorreplacementofa malfunctioningphone,and  (user) was often used to describe what one candowithsomeadvancedfeatures.Thustheclassifier was able to capture some product-specific as well as market-specific sentiment markers, however, it was not able to distinguish the context these generally objective words were used in." ></td>
	<td class="line x" title="168:190	This resulted in relatively high sentiment density of neutral texts which contained these words but used in othertypesofcontext." ></td>
	<td class="line x" title="169:190	To verify this hypothesis we applied the same processing to our corpus derived from Wikipedia articles, but using as the vocabulary list the NTU Sentiment Dictionary." ></td>
	<td class="line x" title="170:190	The results (Figure 3) show that most of the neutral texts are now mapped to the lower part of the opinionation scale (Y < 0.5), as expected." ></td>
	<td class="line x" title="171:190	Therefore, to successfully distinguish between balanced reviewsand neutral documents a classifier should be able to detect when product features are used as sentiment markers and when theyarenot." ></td>
	<td class="line x" title="172:190	7 Conclusionsand FutureWork Wehave described an approach to classification of documents with respect to sentiment polarity and objectivity, representing both as a continuum, and mapping classified documents onto a coordinate system that also represents the difference between balanced and neutral text." ></td>
	<td class="line x" title="173:190	We have presented a novel, unsupervised, iterative retraining procedure for deriving the classifier, starting from the most minimal size seed vocabulary, in conjunction with a simple negation check." ></td>
	<td class="line x" title="174:190	Wehave verified that the approach produces reasonable results." ></td>
	<td class="line x" title="175:190	The approach is extremely minimal in terms of language processing technology, giving it good possibilities for porting to different genres, domains and languages." ></td>
	<td class="line x" title="176:190	We also found that the accuracy of the method depends a lot on the seed word chosen." ></td>
	<td class="line x" title="177:190	If the word has a relatively low frequency or does not have a definite sentiment-related meaning, the results may be very poor.For example,an antonymous wordto (good) in Chinese is(bad), but the latter is notafrequentword:theChineseprefertosay (not good)." ></td>
	<td class="line x" title="178:190	When this word was used as the seed word, accuracy was little more than 15%." ></td>
	<td class="line x" title="179:190	Although the first iteration produced high precision (82%), the size of the extracted subcorpus was only 24 items, resulting in the system being unable to produce a good classifier for the following iterations." ></td>
	<td class="line x" title="180:190	Every new iteration produced an even poorer result as each new extracted corpus was of lower accuracy." ></td>
	<td class="line x" title="181:190	On the other hand, it seems that a seed list consisting of several low-frequency one-character words can compensate each other and produce better results by capturing a larger part of the corpus (thus increasing recall)." ></td>
	<td class="line x" title="182:190	Nevertheless a single word may also produce results even better than those for multiword seed lists." ></td>
	<td class="line x" title="183:190	For example, the two-character wordM((comfortable) as seed reached 91% -40 -30 -20 -10 0 10 20 30 40 50 60 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 -40 -30 -20 -10 0 10 20 30 40 50 60 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 310 accuracy with 90% recall." ></td>
	<td class="line x" title="184:190	We can conclude that our method relies on the quality of the seed word." ></td>
	<td class="line x" title="185:190	Wetherefore need to investigate ways of choosing 'lucky'seedsandavoiding'unlucky'ones." ></td>
	<td class="line x" title="186:190	Future work should also focus on improving classification accuracy: adding a little languagespecific knowledge to be able to detect some word boundariesshouldhelp;wealsoplantoexperiment with more sophisticated methods of sentiment score calculation." ></td>
	<td class="line x" title="187:190	In addition, the notion of 'zone' needs refining and language-specific adjustments (for example, a 'reversed comma' should not be considered to be a zone boundary marker, since thispunctuationmarkisgenerallyusedfortheenumerationofrelatedobjects)." ></td>
	<td class="line x" title="188:190	More experiments are also necessary to determine how the approach works across domains, and further investigation into methods for distinguishingbetweenbalancedandneutraltext." ></td>
	<td class="line x" title="189:190	Finally, we need to produce a new corpus that would enable us to evaluate the performance of a pre-trained version of the classifier that did not have any prior access to the documents it was classifying: we need the reviews to be tagged not in a binary way as they are now, but in a way that reflects the two continuums we use (sentiment and objectivity)." ></td>
	<td class="line x" title="190:190	Acknowledgements The first author is supported by the Ford FoundationInternationalFellowshipsProgram." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="I08-1041
Using Roget's Thesaurus for Fine-grained Emotion Recognition
Aman, Saima;Szpakowicz, Stan;"></td>
	<td class="line x" title="1:138	Using Rogets Thesaurus for Fine-grained Emotion Recognition Saima Aman School of Information Technology and Engineering University of Ottawa, Ottawa, Canada   saman071@site.uottawa.ca Stan Szpakowicz School of Information Technology and Engineering University of Ottawa, Ottawa, Canada ICS, Polish Academy of Sciences Warszawa, Poland szpak@site.uottawa.ca   Abstract Recognizing the emotive meaning of text can add another dimension to the understanding of text." ></td>
	<td class="line x" title="2:138	We study the task of automatically categorizing sentences in a text into Ekmans six basic emotion categories." ></td>
	<td class="line x" title="3:138	We experiment with corpus-based features as well as features derived from two emotion lexicons." ></td>
	<td class="line x" title="4:138	One lexicon is automatically built using the classification system of Rogets Thesaurus, while the other consists of words extracted from WordNet-Affect." ></td>
	<td class="line x" title="5:138	Experiments on the data obtained from blogs show that a combination of corpus-based unigram features with emotion-related features provides superior classification performance." ></td>
	<td class="line x" title="6:138	We achieve Fmeasure values that outperform the rulebased baseline method for all emotion classes." ></td>
	<td class="line x" title="7:138	1 Introduction Recognizing emotions conveyed by a text can provide an insight into the authors intent and sentiment, and can lead to better understanding of the texts content." ></td>
	<td class="line x" title="8:138	Emotion recognition in text has recently attracted increased attention of the NLP community (Alm et al., 2005; Liu et al, 2003; Mihalcea and Liu, 2006); it is also one of the tasks at Semeval-2007 1 . Automatic recognition of emotions can be applied in the development of affective interfaces for  1  Affective Text: Semeval Task at the 4th International Workshop on Semantic Evaluations, 2007, Prague (nlp.cs.swarthmore.edu/semeval/tasks/task14/summary.shtml)." ></td>
	<td class="line x" title="9:138	Computer-Mediated Communication and HumanComputer Interaction." ></td>
	<td class="line x" title="10:138	Other areas that can potentially benefit from automatic emotion analysis are personality modeling and profiling (Liu and Maes, 2004), affective interfaces and communication systems (Liu et al, 2003; Neviarouskaya et al., 2007a) consumer feedback analysis, affective tutoring in e-learning systems (Zhang et al., 2006), and textto-speech synthesis (Alm et al., 2005)." ></td>
	<td class="line x" title="11:138	In this study, we address the task of automatically assigning an emotion label to each sentence in the given dataset, indicating the predominant emotion type expressed in the sentence." ></td>
	<td class="line x" title="12:138	The possible labels are happiness, sadness, anger, disgust, surprise, fear and no-emotion." ></td>
	<td class="line x" title="13:138	Those are Ekmans (1992) six basic emotion categories, and an additional label to account for the absence of a clearly discernible emotion." ></td>
	<td class="line x" title="14:138	We experiment with two types of features for representing text in emotion classification based on machine learning (ML)." ></td>
	<td class="line x" title="15:138	Features of the first type are a corpus-based unigram representation of text." ></td>
	<td class="line x" title="16:138	Features of the second type comprise words that appear in emotion lexicons." ></td>
	<td class="line x" title="17:138	One such lexicon consists of words that we automatically extracted from Rogets Thesaurus (1852)." ></td>
	<td class="line x" title="18:138	We chose words for their semantic similarity to a basic set of terms that represent each emotion category." ></td>
	<td class="line x" title="19:138	Another lexicon builds on lists of words for each emotion category, extracted from WordNet-Affect (Strapparava and Valitutti, 2004)." ></td>
	<td class="line x" title="20:138	We compare the classification results for groups of features of these two types." ></td>
	<td class="line x" title="21:138	We get good results when the features are combined in a series of ML experiments." ></td>
	<td class="line x" title="22:138	312 2 Related Work Research in emotion recognition has focused on discerning emotions along the dimensions of valence (positive / negative) and arousal (calm / excited), and on recognizing distinct emotion categories." ></td>
	<td class="line x" title="23:138	We focus on the latter." ></td>
	<td class="line x" title="24:138	Liu et al.(2003) use a real-world commonsense knowledge base to classify sentences into Ekmans (1992) basic emotion categories." ></td>
	<td class="line x" title="26:138	They use an ensemble of rule-based affect models to determine the emotional affinity of individual sentences." ></td>
	<td class="line x" title="27:138	Neviarouskaya et al.(2007b) also use rules to determine the emotions in sentences in blog posts; their analysis relies on a manually prepared database of words, abbreviations and emoticons labeled with emotion categories." ></td>
	<td class="line x" title="29:138	Since these papers do not report conventional performance metrics such as precision and recall, the effectiveness of their methods cannot be judged empirically." ></td>
	<td class="line x" title="30:138	They also disregard statistical learning methods as ineffective for emotion recognition at sentence level." ></td>
	<td class="line x" title="31:138	They surmise that the small size of the text input (a sentence) gives insufficient data for statistical analysis, and that statistical methods cannot handle negation." ></td>
	<td class="line x" title="32:138	In this paper, we show that ML-based approach with the appropriate combination of features can be applied to distinguishing emotions in text." ></td>
	<td class="line x" title="33:138	Previous work has used lexical resources such as WordNet to automatically acquire emotion-related words for emotion classification experiments." ></td>
	<td class="line x" title="34:138	Starting from a set of primary emotion adjectives, Alm et al.(2005) retrieve similar words from WordNet utilizing all senses of all words in the synsets that contain the adjectives." ></td>
	<td class="line x" title="36:138	They also exploit the synonym and hyponym relations in WordNet to manually find words similar to nominal emotion words." ></td>
	<td class="line x" title="37:138	Kamps and Marx (2002) use WordNets synset relations to determine the affective meaning of words." ></td>
	<td class="line x" title="38:138	They assign multidimensional scores to individual words based on the minimum path length between them and a pair of polar words (such as good and bad) in WordNets structure." ></td>
	<td class="line x" title="39:138	There is also a corpus-driven method of determining the emotional affinity of words: learn probabilistic affective scores of words from large corpora." ></td>
	<td class="line x" title="40:138	Mihalcea and Liu (2006) have used this method to assign a happiness factor to words depending on the frequency of their occurrences in happy-labeled blogposts compared to their total frequency in the corpus." ></td>
	<td class="line x" title="41:138	In this paper, we study a new approach to automatically acquiring a wide variety of words that express emotions or emotion-related concepts, using Rogets Thesaurus (1852)." ></td>
	<td class="line x" title="42:138	3 Emotion-Labeled Data We have based our study on data collected from blogs." ></td>
	<td class="line x" title="43:138	We chose blogs as data source because they are potentially rich in emotion content, and contain good examples of real-world instances of emotions expressed in text." ></td>
	<td class="line x" title="44:138	Additionally, text in blogs does not conform to the style of any particular genre per se, and thus offers a variety in writing styles, choice and combination of words, as well as topics." ></td>
	<td class="line x" title="45:138	So, the methods learned for discerning emotion using blog data are quite general and therefore applicable to a variety of genres rather than to blogs only." ></td>
	<td class="line x" title="46:138	We retrieved blogs using seed words for all emotion categories." ></td>
	<td class="line x" title="47:138	Four human judges manually annotated the blog posts with emotion-related information every sentence received two judgments." ></td>
	<td class="line x" title="48:138	The annotators were required to mark each sentence with one of the eight labels: happiness, sadness, anger, disgust, surprise, fear, mixed-emotion, and no-emotion." ></td>
	<td class="line x" title="49:138	The mixedemotion label was included to handle those sentences that had more than one type of emotion or whose emotion content could not fit into any of the given emotion categories." ></td>
	<td class="line x" title="50:138	Sample sentences from the annotated corpus are shown in Fig." ></td>
	<td class="line x" title="51:138	1." ></td>
	<td class="line x" title="52:138	We measured the inter-annotator agreement using Cohens (1960) kappa." ></td>
	<td class="line x" title="53:138	The average pair-wise agreement for different emotion categories ranged from 0.6 to 0.79." ></td>
	<td class="line x" title="54:138	In the experiments reported in this paper, we use only those sentences for which there was agreement between both judgments (to form a benchmark for the evaluation of the results of automatic classification)." ></td>
	<td class="line x" title="55:138	The distribution of emotion categories in the corpus used in our experiments is shown in Table 1." ></td>
	<td class="line x" title="56:138	313  Emotion Class Number of sentences Happiness 536 Sadness 173 Anger 179 Disgust 172 Surprise 115 Fear 115 No-emotion 600 Table 1." ></td>
	<td class="line x" title="57:138	Distribution of emotion classes 4 A Baseline Approach We are interested in investigating if emotion in text can be discerned on the basis of its lexical content." ></td>
	<td class="line x" title="58:138	A nave approach to determining the emotional orientation of text is to look for obvious emotion words, such as happy, afraid or astonished." ></td>
	<td class="line x" title="59:138	The presence of one or more words of a particular emotion category in a sentence provides a good premise for interpreting the overall emotion of the sentence." ></td>
	<td class="line x" title="60:138	This approach relies on a list of words with prior information about their emotion type, and uses it for sentence-level classification." ></td>
	<td class="line x" title="61:138	The obvious advantage is that no training data are required." ></td>
	<td class="line x" title="62:138	For evaluation purposes, we took this approach to develop a baseline system that counts the number of emotion words of each category in a sentence, and then assigns this sentence the category with the largest number of words." ></td>
	<td class="line x" title="63:138	Ties were resolved by choosing the emotion label according to an arbitrarily predefined ordering of emotion classes." ></td>
	<td class="line x" title="64:138	A sentence containing no emotion word of any type was assigned the no emotion category." ></td>
	<td class="line x" title="65:138	This system worked with word lists 2  extracted  2  Emotion words from WordNet-Affect (http://www.cse.unt.edu/~rada/affectivetext/data/WordNet AffectEmotionLists.tar.gz) from WordNet-Affect (Strapparava and Valitutti, 2004) for six basic emotion categories." ></td>
	<td class="line x" title="66:138	Table 2 shows the precision, recall, and Fmeasure values for the baseline system." ></td>
	<td class="line x" title="67:138	As we have seven classes in our experiments, the class imbalance makes accuracy values less relevant than precision, recall and F-measure." ></td>
	<td class="line x" title="68:138	That is why we do not report accuracy values in our results." ></td>
	<td class="line x" title="69:138	The baseline system shows precision values above 50% for all but two classes." ></td>
	<td class="line x" title="70:138	This shows the usefulness of this approach." ></td>
	<td class="line x" title="71:138	This method, however, fails in the absence of obvious emotion words in the sentence, as indicated by low recall values." ></td>
	<td class="line x" title="72:138	Thus, in order to improve recall, we need to increase the ambit of words that are considered emotion-related." ></td>
	<td class="line x" title="73:138	An alternative approach is to use ML to learn automatically rules that classify emotion in text." ></td>
	<td class="line x" title="74:138	Class Precision Recall F-Measure Happiness 0.589 0.390 0.469 Sadness 0.527 0.283 0.368 Anger 0.681 0.262 0.379 Disgust 0.944 0.099 0.179 Surprise 0.318 0.296 0.306 Fear 0.824 0.365 0.506 No-emotion 0.434 0.867 0.579 Table 2." ></td>
	<td class="line x" title="75:138	Performance metrics of the baseline system 5 Approach Based on Machine Learning We study two types of features: corpus-based features and features based on emotion lexicons." ></td>
	<td class="line x" title="76:138	5.1 Corpus-based features The corpus-based features exploit the statistical characteristics of the data on the basis of the ngram distribution." ></td>
	<td class="line x" title="77:138	In our experiments, we take unigrams (n=1) as features." ></td>
	<td class="line x" title="78:138	Unigram models have been previously shown to give good results in sentiment classification tasks (Kennedy and Inkpen, 2006; Pang et al., 2002): unigram representations can capture a variety of lexical combinations and distributions, including those of emotion words." ></td>
	<td class="line x" title="79:138	This is particularly important in the case of blogs, whose language is often characterized by frequent use of new words, acronyms (such as lol), onomatopoeic words (haha, grrr), and slang, most of which can be captured in a unigram representaThis was the best summer I have ever experienced." ></td>
	<td class="line x" title="80:138	(happiness) I dont feel like I ever have that kind of privacy where I can talk to God and cry and figure things out." ></td>
	<td class="line x" title="81:138	(sadness) Finally, I got fed up." ></td>
	<td class="line x" title="82:138	(disgust) I cant believe she is finally here!" ></td>
	<td class="line x" title="83:138	(surprise) Fig 1." ></td>
	<td class="line x" title="84:138	Sample sentences from the corpus 314 tion." ></td>
	<td class="line x" title="85:138	Another advantage of a unigram representation is that it does not require any prior knowledge about the data under investigation or the classes to be identified." ></td>
	<td class="line x" title="86:138	For our experiments, we selected all unigrams that occur more than three times in the corpus." ></td>
	<td class="line x" title="87:138	This eliminates rare words, as well as foreign-language words and spelling mistakes, which are quite common in blogs." ></td>
	<td class="line x" title="88:138	We also excluded words that occur in a list of stopwords primarily function words that do not generally have emotional connotations." ></td>
	<td class="line x" title="89:138	We used the SMART list of stopword 3 , with minor modifications." ></td>
	<td class="line x" title="90:138	For instance, we removed from the stop list words such as what and why, which may be used in the context of expressing surprise." ></td>
	<td class="line x" title="91:138	5.2 Features derived from Rogets Thesaurus We utilized Rogets Thesaurus (Jarmasz and Szpakowicz, 2001) to automatically build a lexicon  3  SMART stopwords list." ></td>
	<td class="line x" title="92:138	Used with the SMART information retrieval system at Cornell University (ftp://ftp.cs.cornell.edu/pub/smart/english.stop) of emotion-related words." ></td>
	<td class="line x" title="93:138	The features based on an emotion lexiconrequire prior knowledge about emotion relatedness of words." ></td>
	<td class="line x" title="94:138	We extracted this knowledge from the classification system in Rogets, which groups related concepts into various levels of a hierarchy." ></td>
	<td class="line x" title="95:138	For a detailed account of this classification structure, see Jarmasz and Szpakowicz (2001)." ></td>
	<td class="line x" title="96:138	Rogets structure allows the calculation of semantic relatedness between words, based on the path length between the nodes in the structure that represent those words." ></td>
	<td class="line x" title="97:138	In case of multiple paths, the shortest path is considered." ></td>
	<td class="line x" title="98:138	Jarmasz and Szpakowicz (2004) have introduced a similarity measure derived from path length, which assigns scores ranging from a maximum of 16 to most semantically related words to a minimum of 0 to least related words." ></td>
	<td class="line x" title="99:138	They have shown that on semantic similarity tests this measure outperforms several other methods." ></td>
	<td class="line x" title="100:138	To build a lexicon of emotion-related words utilizing Rogets structure, we need first to make two decisions: select a primary set of emotion words starting with which we can extract other similar Similarity Score Happiness Sadness Anger Disgust Surprise Fear 16 family, home, friends, life, house, loving, partying, bed, pleasure, rest, close, event, lucks, times crying, lost, wounds, bad, pills, falling, messed, spot, unhappy, pass, black, events, hurts, shocked pride, fits, stormed, abandoned, bothered, mental, anger, feelings, distractions shock, disgust, dislike, loathing plans, catch, expected, early, slid, slipped, earlier, caught, act nervous, cry, terror, panic, feelings, run, fog, fire, turn, police, faith, battle, war, sounds 14 love, like, feel, pretty, lovely, better, smiling, nice, beautiful, hope, cutest celebrations, warm, desires ill, bored, feeling, ruin, blow, down, wrong, awful, evil, worry, crushing, bug, death, trouble, dark hate, burn, upset, dislike, wrong, blood, ill, flaws, bar, defects, bitter, growled, black, slow hate, pain, horrifying, ill, pills, sad, wear, blood, appalling, end, work, weighed, regrets, bad left, swing, noticed, worry, times, amazing, stolen, break, interesting, attention falling, life, stunned, pay, broken, hate, blast, times, hanging, hope, broken, blood, blue 12 gift, treats, adorable, fun, hug, kidding, bigger, great, lighting, won, stars, enjoy, favourite, social, divine defeat, nasty, boring, ugly, loser, end, victim, sick, hard, serious, aggravating, bothering, burning lose, throw, offended, hit, power, feel, flaring, pills, broken, life, forgot, ranting feel, fun, lies, drawn, lose, missed, deprived, lack, sighs, defeat, down, hurt, tears, insulted realize, pick, wake, sense, jumped, new, late, magic, omen, forget, popped, feel, question, late, throw fearful, spy, night, upset, feel, chased, hazardous, tomorrow, victim, grim, terrorists, apprehensive Table 3." ></td>
	<td class="line x" title="101:138	Emotion-related words automatically extracted from Rogets Thesaurus 315 words, and choose an appropriate similarity score to serve as cutoff for determining semantic relatedness between words." ></td>
	<td class="line x" title="102:138	The primary set of words that we selected consists of one word for each emotion category, representing the base form of the name of the category: {happy, sad, anger, disgust, surprise, fear}." ></td>
	<td class="line x" title="103:138	Experiments performed on Miller and Charles similarity data (1991), reported in Jarmasz and Szpakowicz (2004), have shown that pairs of words with a semantic similarity value of 16 have high similarity, while those with a score of 12 to 14 have intermediate similarity." ></td>
	<td class="line x" title="104:138	Therefore, we select the score of 12 as cutoff, and include in the lexicon all words that have similarity scores of 12 or higher with respect to the words in the primary set." ></td>
	<td class="line x" title="105:138	This selection of cutoff therefore serves as a form of feature selection." ></td>
	<td class="line x" title="106:138	In Table 3, we present sample words from the lexicon with similarity scores of 16, 14, and 12 for each emotion category." ></td>
	<td class="line x" title="107:138	These words represent three different levels of relatedness to each emotion category." ></td>
	<td class="line x" title="108:138	We are able to identify a large variety of emotion-related words belonging to different parts of speech that go well beyond the stereotypical words associated with different emotions." ></td>
	<td class="line x" title="109:138	We particularly note some generic neutral words, such as feel, life, and times associated with many emotion categories, indicating their conceptual relevance to emotions." ></td>
	<td class="line x" title="110:138	5.3 Features derived from WordNet-Affect WordNet-Affect is an affective lexical resource that assigns a variety of affect-related labels to a subset Model Class Precision Recall F-Measure Baseline F-Measure Happiness 0.840 0.675 0.740 0.469 Sadness 0.619 0.301 0.405 0.368 Anger 0.634 0.358 0.457 0.379 Disgust 0.772 0.453 0.571 0.179 Surprise 0.813 0.339 0.479 0.306 Fear 0.889 0.487 0.629 0.506 Unigrams No-emotion 0.581 0.342 0.431 0.579 Happiness 0.772 0.562 0.650 0.469 Sadness 0.574 0.225 0.324 0.368 Anger 0.638 0.246 0.355 0.379 Disgust 0.729 0.297 0.421 0.179 Surprise 0.778 0.243 0.371 0.306 Fear 0.857 0.470 0.607 0.506 Rogets Thesaurus (RT) Features No-emotion 0.498 0.258 0.340 0.579 Happiness 0.809 0.705 0.754 0.469 Sadness 0.577 0.370 0.451 0.368 Anger 0.636 0.419 0.505 0.379 Disgust 0.686 0.471 0.559 0.179 Surprise 0.717 0.374 0.491 0.306 Fear 0.831 0.513 0.634 0.506 Unigrams + RT Features No-emotion 0.586 0.512 0.546 0.579 Happiness 0.813 0.698 0.751 0.469 Sadness 0.605 0.416 0.493 0.368 Anger 0.650 0.436 0.522 0.379 Disgust 0.672 0.488 0.566 0.179 Surprise 0.723 0.409 0.522 0.306 Fear 0.868 0.513 0.645 0.506 Unigrams + RT Features + WNA Features No-emotion 0.587 0.625 0.605 0.579 * Highest precision, recall, and F-measure values for each class are shown in bold Table 4 ML Classification Results 316 of WordNet synsets comprising affective concepts." ></td>
	<td class="line x" title="111:138	We used lists of words extracted from it for each of the six emotion categories." ></td>
	<td class="line x" title="112:138	6 Experiments and Results We train classifiers with unigram features for each emotion class using Support Vector Machine (SVM) for predicting the emotion category of the sentences in our corpus." ></td>
	<td class="line pc" title="113:138	SVM has been shown to be useful for text classification tasks (Joachims, 1998), and has previously given good performance in sentiment classification experiments (Kennedy and Inkpen, 2006; Mullen and Collier, 2004; Pang and Lee, 2004; Pang et al., 2002)." ></td>
	<td class="line x" title="114:138	In Table 4, we report results from ten-fold cross-validation experiments conducted using the SMO implementation of SVM in Weka (Witten and Frank, 2005)." ></td>
	<td class="line x" title="115:138	In each experiment, we represent a sentence by a vector indicating the number of times each feature occurs." ></td>
	<td class="line x" title="116:138	In the first experiment, we use only corpusbased unigram features." ></td>
	<td class="line x" title="117:138	We obtain high precision values for all emotion classes (as shown in Table 4), and the recall and F-measure values surpass baseline values for all classes except no-emotion." ></td>
	<td class="line x" title="118:138	This validates our premise that unigrams can help learn lexical distributions well to accurately predict emotion categories." ></td>
	<td class="line x" title="119:138	Next, we use as features all words in the emotion lexicon acquired from Rogets Thesaurus (RT)." ></td>
	<td class="line x" title="120:138	The F-measure scores beat the baseline for four out of seven classes." ></td>
	<td class="line x" title="121:138	When we combine both corpus-based unigrams with RT features, we can increase recall values across all seven classes." ></td>
	<td class="line x" title="122:138	Finally, we add features from WordNet-Affect to the feature set containing corpus unigrams and RT features." ></td>
	<td class="line x" title="123:138	This leads to further improvement in overall performance." ></td>
	<td class="line x" title="124:138	Combining all features, we achieve highest recall values across all but one class." ></td>
	<td class="line x" title="125:138	The resulting F-measure values (ranging from 0.493 to 0.751) surpass the baseline values across all seven classes." ></td>
	<td class="line x" title="126:138	This increase was found to be statistically significant (paired t-test, p=0.05)." ></td>
	<td class="line x" title="127:138	7 Discussion We observe that corpus-based features and emotion-related features together contribute to improved performance, better than given by any one type of feature group alone." ></td>
	<td class="line x" title="128:138	Any automatic way of recognizing emotion should inevitably take into account a wide variety of words that are semantically connected to emotions." ></td>
	<td class="line x" title="129:138	While some words are obviously affective, many more are only potentially affective." ></td>
	<td class="line x" title="130:138	The latter derive their affective property from their associations with emotional concepts." ></td>
	<td class="line x" title="131:138	For instance, words like family, friends, home are not inherently emotional, but because of their wellknown semantic association with emotion concepts, their presence in a sentence can be taken as an indicator of emotion expression in the sentence." ></td>
	<td class="line x" title="132:138	We can interpret the results as indicators of how much correlation the classifiers can find between the features and the predicted class." ></td>
	<td class="line x" title="133:138	Considering our best results using all features, we find that this correlation is highest for the happy class, indicated by a precision of 0.813 and recall of 0.698, the highest among all classes." ></td>
	<td class="line x" title="134:138	We can therefore conclude that it is easier to discern happiness in text than Ekmans other basic emotions." ></td>
	<td class="line x" title="135:138	8 Conclusions Working on a corpus of blog sentences annotated with emotion labels, we were able to demonstrate that a combination of corpus-based unigram features and features derived from emotion lexicons can help automatically distinguish basic emotion categories in written text." ></td>
	<td class="line x" title="136:138	When used together in an SVM-based learning environment, these features increased recall in all cases and the resulting F-measure values significantly surpassed the baseline scores for all emotion categories." ></td>
	<td class="line x" title="137:138	In addition, we described a method of building an emotion lexicon derived from Rogets Thesaurus on the basis of semantic relatedness of words to a set of basic emotion words for each emotion category." ></td>
	<td class="line x" title="138:138	The effectiveness of this emotion lexicon was demonstrated in the emotion classification tasks." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-1034
When Specialists and Generalists Work Together: Overcoming Domain Dependence in Sentiment Tagging
Andreevskaia, Alina;Bergler, Sabine;"></td>
	<td class="line x" title="1:177	Proceedings of ACL-08: HLT, pages 290298, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:177	c2008 Association for Computational Linguistics When Specialists and Generalists Work Together: Overcoming Domain Dependence in Sentiment Tagging Alina Andreevskaia Concordia University Montreal, Quebec andreev@cs.concordia.ca Sabine Bergler Concordia University Montreal, Canada bergler@cs.concordia.ca Abstract This study presents a novel approach to the problem of system portability across different domains: a sentiment annotation system that integrates a corpus-based classifier trained on a small set of annotated in-domain data and a lexicon-based system trained on WordNet." ></td>
	<td class="line x" title="3:177	The paper explores the challenges of system portability across domains and text genres (movie reviews, news, blogs, and product reviews), highlights the factors affecting system performance on out-of-domain and smallset in-domain data, and presents a new system consisting of the ensemble of two classifiers with precision-based vote weighting, that provides significant gains in accuracy and recall over the corpus-based classifier and the lexicon-based system taken individually." ></td>
	<td class="line x" title="4:177	1 Introduction One of the emerging directions in NLP is the development of machine learning methods that perform well not only on the domain on which they were trained, but also on other domains, for which training data is not available or is not sufficient to ensure adequate machine learning." ></td>
	<td class="line x" title="5:177	Many applications require reliable processing of heterogeneous corpora, such as the World Wide Web, where the diversity of genres and domains present in the Internet limits the feasibility of in-domain training." ></td>
	<td class="line x" title="6:177	In this paper, sentiment annotation is defined as the assignment of positive, negative or neutral sentiment values to texts, sentences, and other linguistic units." ></td>
	<td class="line x" title="7:177	Recent experiments assessing system portability across different domains, conducted by Aue and Gamon (2005), demonstrated that sentiment annotation classifiers trained in one domain do not perform well on other domains." ></td>
	<td class="line x" title="8:177	A number of methods has been proposed in order to overcome this system portability limitation by using out-of-domain data, unlabelled in-domain corpora or a combination of in-domain and out-of-domain examples (Aue and Gamon, 2005; Bai et al., 2005; Drezde et al., 2007; Tan et al., 2007)." ></td>
	<td class="line x" title="9:177	In this paper, we present a novel approach to the problem of system portability across different domains by developing a sentiment annotation system that integrates a corpus-based classifier with a lexicon-based system trained on WordNet." ></td>
	<td class="line x" title="10:177	By adopting this approach, we sought to develop a system that relies on both general and domainspecific knowledge, as humans do when analyzing a text." ></td>
	<td class="line x" title="11:177	The information contained in lexicographical sources, such as WordNet, reflects a lay persons general knowledge about the world, while domainspecific knowledge can be acquired through classifier training on a small set of in-domain data." ></td>
	<td class="line x" title="12:177	The first part of this paper reviews the extant literature on domain adaptation in sentiment analysis and highlights promising directions for research." ></td>
	<td class="line x" title="13:177	The second part establishes a baseline for system evaluation by drawing comparisons of system performance across four different domains/genres movie reviews, news, blogs, and product reviews." ></td>
	<td class="line x" title="14:177	The final, third part of the paper presents our system, composed of an ensemble of two classifiers  one trained on WordNet glosses and synsets and the other trained on a small in-domain training set." ></td>
	<td class="line x" title="15:177	290 2 Domain Adaptation in Sentiment Research Most text-level sentiment classifiers use standard machine learning techniques to learn and select features from labeled corpora." ></td>
	<td class="line x" title="16:177	Such approaches work well in situations where large labeled corpora are available for training and validation (e.g., movie reviews), but they do not perform well when training data is scarce or when it comes from a different domain (Aue and Gamon, 2005; Read, 2005), topic (Read, 2005) or time period (Read, 2005)." ></td>
	<td class="line x" title="17:177	There are two alternatives to supervised machine learning that can be used to get around this problem: on the one hand, general lists of sentiment clues/features can be acquired from domain-independent sources such as dictionaries or the Internet, on the other hand, unsupervised and weakly-supervised approaches can be used to take advantage of a small number of annotated in-domain examples and/or of unlabelled indomain data." ></td>
	<td class="line x" title="18:177	The first approach, using general word lists automatically acquired from the Internet or from dictionaries, outperforms corpus-based classifiers when such classifiers use out-of-domain training data or when the training corpus is not sufficiently large to accumulate the necessary feature frequency information." ></td>
	<td class="line x" title="19:177	But such general word lists were shown to perform worse than statistical models built on sufficiently large in-domain training sets of movie reviews (Pang et al., 2002)." ></td>
	<td class="line x" title="20:177	On other domains, such as product reviews, the performance of systems that use general word lists is comparable to the performance of supervised machine learning approaches (Gamon and Aue, 2005)." ></td>
	<td class="line x" title="21:177	The recognition of major performance deficiencies of supervised machine learning methods with insufficient or out-of-domain training brought about an increased interest in unsupervised and weaklysupervised approaches to feature learning." ></td>
	<td class="line x" title="22:177	For instance, Aue and Gamon (2005) proposed training on a samll number of labeled examples and large quantities of unlabelled in-domain data." ></td>
	<td class="line x" title="23:177	This system performed well even when compared to systems trained on a large set of in-domain examples: on feedback messages from a web survey on knowledge bases, Aue and Gamon report 73.86% accuracy using unlabelled data compared to 77.34% for in-domain and 72.39% for the best out-of-domain training on a large training set." ></td>
	<td class="line x" title="24:177	Drezde et al.(2007) applied structural correspondence learning (Drezde et al., 2007) to the task of domain adaptation for sentiment classification of product reviews." ></td>
	<td class="line x" title="26:177	They showed that, depending on the domain, a small number (e.g., 50) of labeled examples allows to adapt the model learned on another corpus to a new domain." ></td>
	<td class="line x" title="27:177	However, they note that the success of such adaptation and the number of necessary in-domain examples depends on the similarity between the original domain and the new one." ></td>
	<td class="line x" title="28:177	Similarly, Tan et al.(2007) suggested to combine out-of-domain labeled examples with unlabelled ones from the target domain in order to solve the domain-transfer problem." ></td>
	<td class="line x" title="30:177	They applied an outof-domain-trained SVM classifier to label examples from the target domain and then retrained the classifier using these new examples." ></td>
	<td class="line x" title="31:177	In order to maximize the utility of the examples from the target domain, these examples were selected using Similarity Ranking and Relative Similarity Ranking algorithms (Tan et al., 2007)." ></td>
	<td class="line x" title="32:177	Depending on the similarity between domains, this method brought up to 15% gain compared to the baseline SVM." ></td>
	<td class="line x" title="33:177	Overall, the development of semi-supervised approaches to sentiment tagging is a promising direction of the research in this area but so far, based on reported results, the performance of such methods is inferior to the supervised approaches with indomain training and to the methods that use general word lists." ></td>
	<td class="line x" title="34:177	It also strongly depends on the similarity between the domains as has been shown by (Drezde et al., 2007; Tan et al., 2007)." ></td>
	<td class="line x" title="35:177	3 Factors Affecting System Performance The comparison of system performance across different domains involves a number of factors that can significantly affect system performance  from training set size to level of analysis (sentence or entire document), document domain/genre and many other factors." ></td>
	<td class="line x" title="36:177	In this section we present a series of experiments conducted to assess the effects of different external factors (i.e., factors unrelated to the merits of the system itself) on system performance in order to establish the baseline for performance comparisons across different domains/genres." ></td>
	<td class="line oc" title="37:177	291 3.1 Level of Analysis Research on sentiment annotation is usually conducted at the text (Aue and Gamon, 2005; Pang et al., 2002; Pang and Lee, 2004; Riloff et al., 2006; Turney, 2002; Turney and Littman, 2003) or at the sentence levels (Gamon and Aue, 2005; Hu and Liu, 2004; Kim and Hovy, 2005; Riloff et al., 2006)." ></td>
	<td class="line x" title="38:177	It should be noted that each of these levels presents different challenges for sentiment annotation." ></td>
	<td class="line x" title="39:177	For example, it has been observed that texts often contain multiple opinions on different topics (Turney, 2002; Wiebe et al., 2001), which makes assignment of the overall sentiment to the whole document problematic." ></td>
	<td class="line x" title="40:177	On the other hand, each individual sentence contains a limited number of sentiment clues, which often negatively affects the accuracy and recall if that single sentiment clue encountered in the sentence was not learned by the system." ></td>
	<td class="line x" title="41:177	Since the comparison of sentiment annotation system performance on texts and on sentences has not been attempted to date, we also sought to close this gap in the literature by conducting the first set of our comparative experiments on data sets of 2,002 movie review texts and 10,662 movie review snippets (5331 with positive and 5331 with negative sentiment) provided by Bo Pang (http://www.cs.cornell.edu/People/pabo/moviereview-data/)." ></td>
	<td class="line x" title="42:177	3.2 Domain Effects The second set of our experiments explores system performance on different domains at sentence level." ></td>
	<td class="line x" title="43:177	For this we used four different data sets of sentences annotated with sentiment tags:  A set of movie review snippets (further: movie) from (Pang and Lee, 2005)." ></td>
	<td class="line x" title="44:177	This dataset of 10,662 snippets was collected automatically from www.rottentomatoes.com website." ></td>
	<td class="line x" title="45:177	All sentences in reviews marked rotten were considered negative and snippets from fresh reviews were deemed positive." ></td>
	<td class="line x" title="46:177	In order to make the results obtained on this dataset comparable to other domains, a randomly selected subset of 1066 snippets was used in the experiments." ></td>
	<td class="line x" title="47:177	 A balanced corpus of 800 manually annotated sentences extracted from 83 newspaper texts (further, news)." ></td>
	<td class="line x" title="48:177	The full set of sentences was annotated by one judge." ></td>
	<td class="line x" title="49:177	200 sentences from this corpus (100 positive and 100 negative) were also randomly selected from the corpus for an inter-annotator agreement study and were manually annotated by two independent annotators." ></td>
	<td class="line x" title="50:177	The pairwise agreement between annotators was calculated as the percent of same tags divided by the number of sentences with this tag in the gold standard." ></td>
	<td class="line x" title="51:177	The pair-wise agreement between the three annotators ranged from 92.5 to 95.9% (=0.74 and 0.75 respectively) on positive vs. negative tags." ></td>
	<td class="line x" title="52:177	 A set of sentences taken from personal weblogs (further, blogs) posted on LiveJournal (http://www.livejournal.com) and on http://www.cyberjournalist.com." ></td>
	<td class="line x" title="53:177	This corpus is composed of 800 sentences (400 sentences with positive and 400 sentences with negative sentiment)." ></td>
	<td class="line x" title="54:177	In order to establish the interannotator agreement, two independent judges were asked to annotate 200 sentences from this corpus." ></td>
	<td class="line x" title="55:177	The agreement between the two annotators on positive vs. negative tags reached 99% (=0.97)." ></td>
	<td class="line x" title="56:177	 A set of 1200 product review (PR) sentences extracted from the annotated corpus made available by Bing Liu (Hu and Liu, 2004) (http://www.cs.uic.edu/ liub/FBS/FBS.html)." ></td>
	<td class="line x" title="57:177	The data set sizes are summarized in Table 1." ></td>
	<td class="line x" title="58:177	Movies News Blogs PR Text level 2002 texts n/a n/a n/a Sentence level 10662 800 800 1200 snippets sent." ></td>
	<td class="line x" title="59:177	sent." ></td>
	<td class="line x" title="60:177	sent." ></td>
	<td class="line pc" title="61:177	Table 1: Datasets 3.3 Establishing a Baseline for a Corpus-based System (CBS) Supervised statistical methods have been very successful in sentiment tagging of texts: on movie review texts they reach accuracies of 85-90% (Aue and Gamon, 2005; Pang and Lee, 2004)." ></td>
	<td class="line p" title="62:177	These methods perform particularly well when a large volume of labeled data from the same domain as the 292 test set is available for training (Aue and Gamon, 2005)." ></td>
	<td class="line x" title="63:177	For this reason, most of the research on sentiment tagging using statistical classifiers was limited to product and movie reviews, where review authors usually indicate their sentiment in a form of a standardized score that accompanies the texts of their reviews." ></td>
	<td class="line x" title="64:177	The lack of sufficient data for training appears to be the main reason for the virtual absence of experiments with statistical classifiers in sentiment tagging at the sentence level." ></td>
	<td class="line x" title="65:177	To our knowledge, the only work that describes the application of statistical classifiers (SVM) to sentence-level sentiment classification is (Gamon and Aue, 2005)1." ></td>
	<td class="line x" title="66:177	The average performance of the system on ternary classification (positive, negative, and neutral) was between 0.50 and 0.52 for both average precision and recall." ></td>
	<td class="line x" title="67:177	The results reported by (Riloff et al., 2006) for binary classification of sentences in a related domain of subjectivity tagging (i.e., the separation of sentiment-laden from neutral sentences) suggest that statistical classifiers can perform well on this task: the authors have reached 74.9% accuracy on the MPQA corpus (Riloff et al., 2006)." ></td>
	<td class="line x" title="68:177	In order to explore the performance of different approaches in sentiment annotation at the text and sentence levels, we used a basic Nave Bayes classifier." ></td>
	<td class="line oc" title="69:177	It has been shown that both Nave Bayes and SVMs perform with similar accuracy on different sentiment tagging tasks (Pang and Lee, 2004)." ></td>
	<td class="line x" title="70:177	These observations were confirmed with our own experiments with SVMs and Nave Bayes (Table 3)." ></td>
	<td class="line x" title="71:177	We used the Weka package (http://www.cs.waikato.ac.nz/ml/weka/) with default settings." ></td>
	<td class="line x" title="72:177	In the sections that follow, we describe a set of comparative experiments with SVMs and Nave Bayes classifiers (1) on texts and sentences and (2) on four different domains (movie reviews, news, blogs, and product reviews)." ></td>
	<td class="line x" title="73:177	System runs with unigrams, bigrams, and trigrams as features and with different training set sizes are presented." ></td>
	<td class="line x" title="74:177	1Recently, a similar task has been addressed by the Affective Text Task at SemEval-1 where even shorter units  headlines  were classified into positive, negative and neutral categories using a variety of techniques (Strapparava and Mihalcea, 2007)." ></td>
	<td class="line x" title="75:177	4 Experiments 4.1 System Performance on Texts vs. Sentences The experiments comparing in-domain trained system performance on texts vs. sentences were conducted on 2,002 movie review texts and on 10,662 movie review snippets." ></td>
	<td class="line x" title="76:177	The results with 10-fold cross-validation are reported in Table 22." ></td>
	<td class="line x" title="77:177	Trained on Texts Trained on Sent." ></td>
	<td class="line x" title="78:177	Tested on Tested on Tested on Tested on Texts Sent." ></td>
	<td class="line x" title="79:177	Texts Sent." ></td>
	<td class="line x" title="80:177	1gram 81.1 69.0 66.8 77.4 2gram 83.7 68.6 71.2 73.9 3gram 82.5 64.1 70.0 65.4 Table 2: Accuracy of Nave Bayes on movie reviews." ></td>
	<td class="line x" title="81:177	Consistent with findings in the literature (Cui et al., 2006; Dave et al., 2003; Gamon and Aue, 2005), on the large corpus of movie review texts, the indomain-trained system based solely on unigrams had lower accuracy than the similar system trained on bigrams." ></td>
	<td class="line x" title="82:177	But the trigrams fared slightly worse than bigrams." ></td>
	<td class="line x" title="83:177	On sentences, however, we have observed an inverse pattern: unigrams performed better than bigrams and trigrams." ></td>
	<td class="line x" title="84:177	These results highlight a special property of sentence-level annotation: greater sensitivity to sparseness of the model: On texts, classifier error on one particular sentiment marker is often compensated by a number of correctly identified other sentiment clues." ></td>
	<td class="line x" title="85:177	Since sentences usually contain a much smaller number of sentiment clues than texts, sentence-level annotation more readily yields errors when a single sentiment clue is incorrectly identified or missed by the system." ></td>
	<td class="line x" title="86:177	Due to lower frequency of higher-order n-grams (as opposed to unigrams), higher-order ngram language models are more sparse, which increases the probability of missing a particular sentiment marker in a sentence (Table 33)." ></td>
	<td class="line x" title="87:177	Very large 2All results are statistically significant at = 0.01 with two exceptions: the difference between trigrams and bigrams for the system trained and tested on texts is statistically significant at alpha=0.1 and for the system trained on sentences and tested on texts is not statistically significant at  = 0.01." ></td>
	<td class="line x" title="88:177	3The results for movie reviews are lower than those reported in Table 2 since the dataset is 10 times smaller, which results in less accurate classification." ></td>
	<td class="line x" title="89:177	The statistical significance of the 293 training sets are required to overcome this higher ngram sparseness in sentence-level annotation." ></td>
	<td class="line x" title="90:177	Dataset Movie News Blogs PRs Dataset size 1066 800 800 1200 unigrams SVM 68.5 61.5 63.85 76.9 NB 60.2 59.5 60.5 74.25 nb features 5410 4544 3615 2832 bigrams SVM 59.9 63.2 61.5 75.9 NB 57.0 58.4 59.5 67.8 nb features 16286 14633 15182 12951 trigrams SVM 54.3 55.4 52.7 64.4 NB 53.3 57.0 56.0 69.7 nb features 20837 18738 19847 19132 Table 3: Accuracy of unigram, bigram and trigram models across domains." ></td>
	<td class="line x" title="91:177	4.2 System Performance on Different Domains In the second set of experiments we sought to compare system results on sentences using in-domain and out-of-domain training." ></td>
	<td class="line x" title="92:177	Table 4 shows that indomain training, as expected, consistently yields superior accuracy than out-of-domain training across all four datasets: movie reviews (Movies), news, blogs, and product reviews (PRs)." ></td>
	<td class="line x" title="93:177	The numbers for in-domain trained runs are highlighted in bold." ></td>
	<td class="line x" title="94:177	Test Data Training Data Movies News Blogs PRs Movies 68.5 55.2 53.2 60.7 News 55.0 61.5 56.25 57.4 Blogs 53.7 49.9 63.85 58.8 PRs 55.8 55.9 56.25 76.9 Table 4: Accuracy of SVM with unigram model results depends on the genre and size of the n-gram: on product reviews, all results are statistically significant at  = 0.025 level; on movie reviews, the difference between Nave Bayes and SVM is statistically significant at  = 0.01 but the significance diminishes as the size of the n-gram increases; on news, only bi-grams produce a statistically significant ( = 0.01) difference between the two machine learning methods, while on blogs the difference between SVMs and Nave Bayes is most pronounced when unigrams are used ( = 0.025)." ></td>
	<td class="line x" title="95:177	It is interesting to note that on sentences, regardless of the domain used in system training and regardless of the domain used in system testing, unigrams tend to perform better than higher-order ngrams." ></td>
	<td class="line x" title="96:177	This observation suggests that, given the constraints on the size of the available training sets, unigram-based systems may be better suited for sentence-level sentiment annotation." ></td>
	<td class="line x" title="97:177	5 Lexicon-Based Approach The search for a base-learner that can produce greatest synergies with a classifier trained on small-set in-domain data has turned our attention to lexiconbased systems." ></td>
	<td class="line x" title="98:177	Since the benefits from combining classifiers that always make similar decisions is minimal, the two (or more) base-learners should complement each other (Alpaydin, 2004)." ></td>
	<td class="line x" title="99:177	Since a system based on a fairly different learning approach is more likely to produce a different decision under a given set of circumstances, the diversity of approaches integrated in the ensemble of classifiers was expected to have a beneficial effect on the overall system performance." ></td>
	<td class="line x" title="100:177	A lexicon-based approach capitalizes on the fact that dictionaries, such as WordNet (Fellbaum, 1998), contain a comprehensive and domainindependent set of sentiment clues that exist in general English." ></td>
	<td class="line x" title="101:177	A system trained on such general data, therefore, should be less sensitive to domain changes." ></td>
	<td class="line x" title="102:177	This robustness, however is expected to come at some cost, since some domain-specific sentiment clues may not be covered in the dictionary." ></td>
	<td class="line x" title="103:177	Our hypothesis was, therefore, that a lexiconbased system will perform worse than an in-domain trained classifier but possibly better than a classifier trained on out-of domain data." ></td>
	<td class="line x" title="104:177	One of the limitations of general lexicons and dictionaries, such as WordNet (Fellbaum, 1998), as training sets for sentiment tagging systems is that they contain only definitions of individual words and, hence, only unigrams could be effectively learned from dictionary entries." ></td>
	<td class="line x" title="105:177	Since the structure of WordNet glosses is fairly different from that of other types of corpora, we developed a system that used the list of human-annotated adjectives from (Hatzivassiloglou and McKeown, 1997) as a seed list and then learned additional unigrams 294 from WordNet synsets and glosses with up to 88% accuracy, when evaluated against General Inquirer (Stone et al., 1966) (GI) on the intersection of our automatically acquired list with GI." ></td>
	<td class="line x" title="106:177	In order to expand the list coverage for our experiments at the text and sentence levels, we then augmented the list by adding to it all the words annotated with Positiv or Negativ tags in GI, that were not picked up by the system." ></td>
	<td class="line x" title="107:177	The resulting list of features contained 11,000 unigrams with the degree of membership in the category of positive or negative sentiment assigned to each of them." ></td>
	<td class="line x" title="108:177	In order to assign the membership score to each word, we did 58 system runs on unique nonintersecting seed lists drawn from manually annotated list of positive and negative adjectives from (Hatzivassiloglou and McKeown, 1997)." ></td>
	<td class="line x" title="109:177	The 58 runs were then collapsed into a single set of 7,813 unique words." ></td>
	<td class="line x" title="110:177	For each word we computed a score by subtracting the total number of runs assigning this word a negative sentiment from the total of the runs that consider it positive." ></td>
	<td class="line x" title="111:177	The resulting measure, termed Net Overlap Score (NOS), reflected the number of ties linking a given word with other sentimentladen words in WordNet, and hence, could be used as a measure of the words centrality in the fuzzy category of sentiment." ></td>
	<td class="line x" title="112:177	The NOSs were then normalized into the interval from -1 to +1 using a sigmoid fuzzy membership function (Zadeh, 1975)4." ></td>
	<td class="line x" title="113:177	Only words with fuzzy membership degree not equal to zero were retained in the list." ></td>
	<td class="line x" title="114:177	The resulting list contained 10,809 sentiment-bearing words of different parts of speech." ></td>
	<td class="line x" title="115:177	The sentiment determination at the sentence and text level was then done by summing up the scores of all identified positive unigrams (NOS>0) and all negative unigrams (NOS<0) (Andreevskaia and Bergler, 2006)." ></td>
	<td class="line x" title="116:177	5.1 Establishing a Baseline for the Lexicon-Based System (LBS) The baseline performance of the Lexicon-Based System (LBS) described above is presented in Table 5, along with the performance results of the indomainand out-of-domain-trained SVM classifier." ></td>
	<td class="line x" title="117:177	Table 5 confirms the predicted pattern: the LBS performs with lower accuracy than in-domain4With coefficients: =1, =15." ></td>
	<td class="line x" title="118:177	Movies News Blogs PRs LBS 57.5 62.3 63.3 59.3 SVM in-dom." ></td>
	<td class="line x" title="119:177	68.5 61.5 63.85 76.9 SVM out-of-dom." ></td>
	<td class="line x" title="120:177	55.8 55.9 56.25 60.7 Table 5: System accuracy on best runs on sentences trained corpus-based classifiers, and with similar or better accuracy than the corpus-based classifiers trained on out-of-domain data." ></td>
	<td class="line x" title="121:177	Thus, the lexiconbased approach is characterized by a bounded but stable performance when the system is ported across domains." ></td>
	<td class="line x" title="122:177	These performance characteristics of corpus-based and lexicon-based approaches prompt further investigation into the possibility to combine the portability of dictionary-trained systems with the accuracy of in-domain trained systems." ></td>
	<td class="line x" title="123:177	6 Integrating the Corpus-based and Dictionary-based Approaches The strategy of integration of two or more systems in a single ensemble of classifiers has been actively used on different tasks within NLP." ></td>
	<td class="line x" title="124:177	In sentiment tagging and related areas, Aue and Gamon (2005) demonstrated that combining classifiers can be a valuable tool in domain adaptation for sentiment analysis." ></td>
	<td class="line x" title="125:177	In the ensemble of classifiers, they used a combination of nine SVM-based classifiers deployed to learn unigrams, bigrams, and trigrams on three different domains, while the fourth domain was used as an evaluation set." ></td>
	<td class="line x" title="126:177	Using then an SVM meta-classifier trained on a small number of target domain examples to combine the nine base classifiers, they obtained a statistically significant improvement on out-of-domain texts from book reviews, knowledge-base feedback, and product support services survey data." ></td>
	<td class="line x" title="127:177	No improvement occurred on movie reviews." ></td>
	<td class="line oc" title="128:177	Pang and Lee (2004) applied two different classifiers to perform sentiment annotation in two sequential steps: the first classifier separated subjective (sentiment-laden) texts from objective (neutral) ones and then they used the second classifier to classify the subjective texts into positive and negative." ></td>
	<td class="line x" title="129:177	Das and Chen (2004) used five classifiers to determine market sentiment on Yahoo!" ></td>
	<td class="line x" title="130:177	postings." ></td>
	<td class="line x" title="131:177	Simple majority vote was applied to make decisions within 295 the ensemble of classifiers and achieved accuracy of 62% on ternary in-domain classification." ></td>
	<td class="line x" title="132:177	In this study we describe a system that attempts to combine the portability of a dictionary-trained system (LBS) with the accuracy of an in-domain trained corpus-based system (CBS)." ></td>
	<td class="line x" title="133:177	The selection of these two classifiers for this system, thus, was theorybased." ></td>
	<td class="line x" title="134:177	The section that follows describes the classifier integration and presents the performance results of the system consisting of an ensemble CBS and LBS classifier and a precision-based vote weighting procedure." ></td>
	<td class="line x" title="135:177	6.1 The Classifier Integration Procedure and System Evaluation The comparative analysis of the corpus-based and lexicon-based systems described above revealed that the errors produced by CBS and LBS were to a great extent complementary (i.e., where one classifier makes an error, the other tends to give the correct answer)." ></td>
	<td class="line x" title="136:177	This provided further justification to the integration of corpus-based and lexicon-based approaches in a single system." ></td>
	<td class="line x" title="137:177	Table 6 below illustrates the complementarity of the performance CBS and LBS classifiers on the positive and negative categories." ></td>
	<td class="line x" title="138:177	In this experiment, the corpus-based classifier was trained on 400 annotated product review sentences5." ></td>
	<td class="line x" title="139:177	The two systems were then evaluated on a test set of another 400 product review sentences." ></td>
	<td class="line x" title="140:177	The results reported in Table 6 are statistically significant at  = 0.01." ></td>
	<td class="line x" title="141:177	CBS LBS Precision positives 89.3% 69.3% Precision negatives 55.5% 81.5% Pos/Neg Precision 58.0% 72.1% Table 6: Base-learners precision and recall on product reviews on test data." ></td>
	<td class="line x" title="142:177	Table 6 shows that the corpus-based system has a very good precision on those sentences that it classifies as positive but makes a lot of errors on those sentences that it deems negative." ></td>
	<td class="line x" title="143:177	At the same time, the lexicon-based system has low precision on positives 5The small training set explains relatively low overall performance of the CBS system." ></td>
	<td class="line x" title="144:177	and high precision on negatives6." ></td>
	<td class="line x" title="145:177	Such complementary distribution of errors produced by the two systems was observed on different data sets from different domains, which suggests that the observed distribution pattern reflects the properties of each of the classifiers, rather than the specifics of the domain/genre." ></td>
	<td class="line x" title="146:177	In order to take advantage of the observed complementarity of the two systems, the following procedure was used." ></td>
	<td class="line x" title="147:177	First, a small set of in-domain data was used to train the CBS system." ></td>
	<td class="line x" title="148:177	Then both CBS and LBS systems were run separately on the same training set, and for each classifier, the precision measures were calculated separately for those sentences that the classifier considered positive and those it considered negative." ></td>
	<td class="line x" title="149:177	The chance-level performance (50%) was then subtracted from the precision figures to ensure that the final weights reflect by how much the classifiers precision exceeds the chance level." ></td>
	<td class="line x" title="150:177	The resulting chance-adjusted precision numbers of the two classifiers were then normalized, so that the weights of CBS and LBS classifiers sum up to 100% on positive and to 100% on negative sentences." ></td>
	<td class="line x" title="151:177	These weights were then used to adjust the contribution of each classifier to the decision of the ensemble system." ></td>
	<td class="line x" title="152:177	The choice of the weight applied to the classifier decision, thus, varied depending on whether the classifier scored a given sentence as positive or as negative." ></td>
	<td class="line x" title="153:177	The resulting system was then tested on a separate test set of sentences7." ></td>
	<td class="line x" title="154:177	The small-set training and evaluation experiments with the system were performed on different domains using 3-fold validation." ></td>
	<td class="line x" title="155:177	The experiments conducted with the Ensemble system were designed to explore system performance under conditions of limited availability of annotated data for classifier training." ></td>
	<td class="line x" title="156:177	For this reason, the numbers reported for the corpus-based classifier do not reflect the full potential of machine learning approaches when sufficient in-domain training data is available." ></td>
	<td class="line x" title="157:177	Table 7 presents the results of these experiments by domain/genre." ></td>
	<td class="line x" title="158:177	The results 6These results are consistent with an observation in (Kennedy and Inkpen, 2006), where a lexicon-based system performed with a better precision on negative than on positive texts." ></td>
	<td class="line x" title="159:177	7The size of the test set varied in different experiments due to the availability of annotated data for a particular domain." ></td>
	<td class="line x" title="160:177	296 are statistically significant at  = 0.01, except the runs on movie reviews where the difference between the LBS and Ensemble classifiers was significant at  = 0.05." ></td>
	<td class="line x" title="161:177	LBS CBS Ensemble News Acc 67.8 53.2 73.3 F 0.82 0.71 0.85 Movies Acc 54.5 53.5 62.1 F 0.73 0.72 0.77 Blogs Acc 61.2 51.1 70.9 F 0.78 0.69 0.83 PRs Acc 59.5 58.9 78.0 F 0.77 0.75 0.88 Average Acc 60.7 54.2 71.1 F 0.77 0.72 0.83 Table 7: Performance of the ensemble classifier Table 7 shows that the combination of two classifiers into an ensemble using the weighting technique described above leads to consistent improvement in system performance across all domains/genres." ></td>
	<td class="line x" title="162:177	In the ensemble system, the average gain in accuracy across the four domains was 16.9% relative to CBS and 10.3% relative to LBS." ></td>
	<td class="line x" title="163:177	Moreover, the gain in accuracy and precision was not offset by decreases in recall: the net gain in recall was 7.4% relative to CBS and 13.5% vs. LBS." ></td>
	<td class="line x" title="164:177	The ensemble system on average reached 99.1% recall." ></td>
	<td class="line x" title="165:177	The F-measure has increased from 0.77 and 0.72 for LBS and CBS classifiers respectively to 0.83 for the whole ensemble system." ></td>
	<td class="line x" title="166:177	7 Discussion The development of domain-independent sentiment determination systems poses a substantial challenge for researchers in NLP and artificial intelligence." ></td>
	<td class="line x" title="167:177	The results presented in this study suggest that the integration of two fairly different classifier learning approaches in a single ensemble of classifiers can yield substantial gains in system performance on all measures." ></td>
	<td class="line x" title="168:177	The most substantial gains occurred in recall, accuracy, and F-measure." ></td>
	<td class="line x" title="169:177	This study permits to highlight a set of factors that enable substantial performance gains with the ensemble of classifiers approach." ></td>
	<td class="line x" title="170:177	Such gains are most likely when (1) the errors made by the classifiers are complementary, i.e., where one classifier makes an error, the other tends to give the correct answer, (2) the classifier errors are not fully random and occur more often in a certain segment (or category) of classifier results, and (3) there is a way for a system to identify that low-precision segment and reduce the weights of that classifiers results on that segment accordingly." ></td>
	<td class="line x" title="171:177	The two classifiers used in this study  corpus-based and lexicon-based  provided an interesting illustration of potential performance gains associated with these three conditions." ></td>
	<td class="line x" title="172:177	The use of precision of classifier results on the positives and negatives proved to be an effective technique for classifier vote weighting within the ensemble." ></td>
	<td class="line x" title="173:177	8 Conclusion This study contributes to the research on sentiment tagging, domain adaptation, and the development of ensembles of classifiers (1) by proposing a novel approach for sentiment determination at sentence level and delineating the conditions under which greatest synergies among combined classifiers can be achieved, (2) by describing a precision-based technique for assigning differential weights to classifier results on different categories identified by the classifier (i.e., categories of positive vs. negative sentences), and (3) by proposing a new method for sentiment annotation in situations where the annotated in-domain data is scarce and insufficient to ensure adequate performance of the corpus-based classifier, which still remains the preferred choice when large volumes of annotated data are available for system training." ></td>
	<td class="line x" title="174:177	Among the most promising directions for future research in the direction laid out in this paper is the deployment of more advanced classifiers and feature selection techniques that can further enhance the performance of the ensemble of classifiers." ></td>
	<td class="line x" title="175:177	The precision-based vote weighting technique may prove to be effective also in situations, where more than two classifiers are integrated into a single system." ></td>
	<td class="line x" title="176:177	We expect that these more advanced ensemble-ofclassifiers systems would inherit the benefits of multiple complementary approaches to sentiment annotation and will be able to achieve better and more stable accuracy on in-domain, as well as on out-ofdomain data." ></td>
	<td class="line x" title="177:177	297" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-1041
Summarizing Emails with Conversational Cohesion and Subjectivity
Carenini, Giuseppe;Ng, Raymond;Zhou, Xiaodong;"></td>
	<td class="line x" title="1:250	Proceedings of ACL-08: HLT, pages 353361, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:250	c2008 Association for Computational Linguistics Summarizing Emails with Conversational Cohesion and Subjectivity Giuseppe Carenini, Raymond T. Ng and Xiaodong Zhou Department of Computer Science University of British Columbia Vancouver, BC, Canada {carenini, rng, xdzhou}@cs.ubc.ca Abstract In this paper, we study the problem of summarizing email conversations." ></td>
	<td class="line x" title="3:250	We first build a sentence quotation graph that captures the conversation structure among emails." ></td>
	<td class="line x" title="4:250	We adopt three cohesion measures: clue words, semantic similarity and cosine similarity as the weight of the edges." ></td>
	<td class="line x" title="5:250	Second, we use two graph-based summarization approaches, Generalized ClueWordSummarizer and PageRank, to extract sentences as summaries." ></td>
	<td class="line x" title="6:250	Third, we propose a summarization approach based on subjective opinions and integrate it with the graph-based ones." ></td>
	<td class="line x" title="7:250	The empirical evaluation shows that the basic clue words have the highest accuracy among the three cohesion measures." ></td>
	<td class="line x" title="8:250	Moreover, subjective words can significantly improve accuracy." ></td>
	<td class="line x" title="9:250	1 Introduction With the ever increasing popularity of emails, it is very common nowadays that people discuss specific issues, events or tasks among a group of people by emails(Fisher and Moody, 2002)." ></td>
	<td class="line x" title="10:250	Those discussions can be viewed as conversations via emails and are valuable for the user as a personal information repository(Ducheneaut and Bellotti, 2001)." ></td>
	<td class="line x" title="11:250	In this paper, we study the problem of summarizing email conversations." ></td>
	<td class="line x" title="12:250	Solutions to this problem can help users access the information embedded in emails more effectively." ></td>
	<td class="line x" title="13:250	For instance, 10 minutes before a meeting, a user may want to quickly go through a previous discussion via emails that is going to be discussed soon." ></td>
	<td class="line x" title="14:250	In that case, rather than reading each individual email one by one, it would be preferable to read a concise summary of the previous discussion with the major information summarized." ></td>
	<td class="line x" title="15:250	Email summarization is also helpful for mobile email users on a small screen." ></td>
	<td class="line x" title="16:250	Summarizing email conversations is challenging due to the characteristics of emails, especially the conversational nature." ></td>
	<td class="line x" title="17:250	Most of the existing methods dealing with email conversations use the email thread to represent the email conversation structure, which is not accurate in many cases (Yeh and Harnly, 2006)." ></td>
	<td class="line x" title="18:250	Meanwhile, most existing email summarization approaches use quantitative features to describe the conversation structure, e.g., number of recipients and responses, and apply some general multi-document summarization methods to extract some sentences as the summary (Rambow et al., 2004) (Wan and McKeown, 2004)." ></td>
	<td class="line x" title="19:250	Although such methods consider the conversation structure somehow, they simplify the conversation structure into several features and do not fully utilize it into the summarization process." ></td>
	<td class="line x" title="20:250	In contrast, in this paper, we propose new summarization approaches by sentence extraction, which rely on a fine-grain representation of the conversation structure." ></td>
	<td class="line x" title="21:250	We first build a sentence quotation graph by content analysis." ></td>
	<td class="line x" title="22:250	This graph not only captures the conversation structure more accurately, especially for selective quotations, but it also represents the conversation structure at the finer granularity of sentences." ></td>
	<td class="line x" title="23:250	As a second contribution of this paper, we study several ways to measure the cohesion between parent and child sentences in the quotation graph: clue words (re-occurring words in the reply) 353 (Carenini et al., 2007), semantic similarity and cosine similarity." ></td>
	<td class="line x" title="24:250	Hence, we can directly evaluate the importance of each sentence in terms of its cohesion with related ones in the graph." ></td>
	<td class="line x" title="25:250	The extractive summarization problem can be viewed as a node ranking problem." ></td>
	<td class="line x" title="26:250	We apply two summarization algorithms, Generalized ClueWordSummarizer and Page-Rank to rank nodes in the sentence quotation graph and to select the corresponding most highly ranked sentences as the summary." ></td>
	<td class="line x" title="27:250	Subjective opinions are often critical in many conversations." ></td>
	<td class="line x" title="28:250	As a third contribution of this paper, we study how to make use of the subjective opinions expressed in emails to support the summarization task." ></td>
	<td class="line x" title="29:250	We integrate our best cohesion measure together with the subjective opinions." ></td>
	<td class="line x" title="30:250	Our empirical evaluations show that subjective words and phrases can significantly improve email summarization." ></td>
	<td class="line x" title="31:250	To summarize, this paper is organized as follows." ></td>
	<td class="line x" title="32:250	In Section 2, we discuss related work." ></td>
	<td class="line x" title="33:250	After building a sentence quotation graph to represent the conversation structure in Section 3, we apply two summarization methods in Section 4." ></td>
	<td class="line x" title="34:250	In Section 5, we study summarization approaches with subjective opinions." ></td>
	<td class="line x" title="35:250	Section 6 presents the empirical evaluation of our methods." ></td>
	<td class="line x" title="36:250	We conclude this paper and propose future work in Section 7." ></td>
	<td class="line x" title="37:250	2 Related Work Rambow et al. proposed a sentence extraction summarization approach for email threads (Rambow et al., 2004)." ></td>
	<td class="line x" title="38:250	They described each sentence in an email conversations by a set of features and used machine learning to classify whether or not a sentence should be included into the summary." ></td>
	<td class="line x" title="39:250	Their experiments showed that features about emails and the email thread could significantly improve the accuracy of summarization." ></td>
	<td class="line x" title="40:250	Wan et al. proposed a summarization approach for decision-making email discussions (Wan and McKeown, 2004)." ></td>
	<td class="line x" title="41:250	They extracted the issue and response sentences from an email thread as a summary." ></td>
	<td class="line x" title="42:250	Similar to the issue-response relationship, Shrestha et al.(Shrestha and McKeown, 2004) proposed methods to identify the question-answer pairs from an email thread." ></td>
	<td class="line x" title="43:250	Once again, their results showed that including features about the email thread could greatly improve the accuracy." ></td>
	<td class="line x" title="44:250	Similar results were obtained by Corston-Oliver et al. They studied how to identify action sentences in email messages and use those sentences as a summary(Corston-Oliver et al., 2004)." ></td>
	<td class="line x" title="45:250	All these approaches used the email thread as a coarse representation of the underlying conversation structure." ></td>
	<td class="line x" title="46:250	In our recent study (Carenini et al., 2007), we built a fragment quotation graph to represent an email conversation and developed a ClueWordSummarizer (CWS) based on the concept of clue words." ></td>
	<td class="line x" title="47:250	Our experiments showed that CWS had a higher accuracy than the email summarization approach in (Rambow et al., 2004) and the generic multidocument summarization approach MEAD (Radev et al., 2004)." ></td>
	<td class="line x" title="48:250	Though effective, the CWS method still suffers from the following four substantial limitations." ></td>
	<td class="line x" title="49:250	First, we used a fragment quotation graph to represent the conversation, which has a coarser granularity than the sentence level." ></td>
	<td class="line x" title="50:250	For email summarization by sentence extraction, the fragment granularity may be inadequate." ></td>
	<td class="line x" title="51:250	Second, we only adopted one cohesion measure (clue words that are based on stemming), and did not consider more sophisticated ones such as semantically similar words." ></td>
	<td class="line x" title="52:250	Third, we did not consider subjective opinions." ></td>
	<td class="line x" title="53:250	Finally, we did not compared CWS to other possible graph-based approaches as we propose in this paper." ></td>
	<td class="line x" title="54:250	Other than for email summarization, other document summarization methods have adopted graphranking algorithms for summarization, e.g., (Wan et al., 2007), (Mihalcea and Tarau, 2004) and (Erkan and Radev, 2004)." ></td>
	<td class="line x" title="55:250	Those methods built a complete graph for all sentences in one or multiple documents and measure the similarity between every pair of sentences." ></td>
	<td class="line x" title="56:250	Graph-ranking algorithms, e.g., PageRank (Brin and Page, 1998), are then applied to rank those sentences." ></td>
	<td class="line x" title="57:250	Our method is different from them." ></td>
	<td class="line x" title="58:250	First, instead of using the complete graph, we build the graph based on the conversation structure." ></td>
	<td class="line x" title="59:250	Second, we try various ways to compute the similarity among sentences and the ranking of the sentences." ></td>
	<td class="line x" title="60:250	Several studies in the NLP literature have explored the reoccurrence of similar words within one document due to text cohesion." ></td>
	<td class="line x" title="61:250	The idea has been formalized in the construct of lexical chains (Barzilay and Elhadad, 1997)." ></td>
	<td class="line x" title="62:250	While our approach and lexical chains both rely on lexical cohesion, they are 354 quite different with respect to the kind of linkages considered." ></td>
	<td class="line x" title="63:250	Lexical chain is only based on similarities between lexical items in contiguous sentences." ></td>
	<td class="line x" title="64:250	In contrast, in our approach, the linkage is based on the existing conversation structure." ></td>
	<td class="line x" title="65:250	In our approach, the chain is not only lexical but also conversational, and typically spans over several emails." ></td>
	<td class="line x" title="66:250	3 Extracting Conversations from Multiple Emails In this section, we first review how to build a fragment quotation graph through an example." ></td>
	<td class="line x" title="67:250	Then we extend this structure into a sentence quotation graph, which can allow us to capture the conversational relationship at the level of sentences." ></td>
	<td class="line x" title="68:250	3.1 Building the Fragment Quotation Graph b > a E2 c > b > > a E3 E4 d e > c > > b > > > a E5 g h > > d > f > > e E6 > g i > h j a E1 (a) Conversation involving 6 Emails ba c e d f h g i j (b) Fragment Quotation Graph Figure 1: A Real Example Figure 1(a) shows a real example of a conversation from a benchmark data set involving 6 emails." ></td>
	<td class="line x" title="69:250	For the ease of representation, we do not show the original content but abbreviate them as a sequence of fragments." ></td>
	<td class="line x" title="70:250	In the first step, all new and quoted fragments are identified." ></td>
	<td class="line x" title="71:250	For instance, email E3 is decomposed into 3 fragments: new fragment c and quoted fragments b, which in turn quoted a. E4 is decomposed into de, c, b and a. Then, in the second step, to identify distinct fragments (nodes), fragments are compared with each other and overlaps are identified." ></td>
	<td class="line x" title="72:250	Fragments are split if necessary (e.g., fragment gh in E5 is split into g and h when matched with E6), and duplicates are removed." ></td>
	<td class="line x" title="73:250	At the end, 10 distinct fragments a,,j give rise to 10 nodes in the graph shown in Figure 1(b)." ></td>
	<td class="line x" title="74:250	As the third step, we create edges, which represent the replying relationship among fragments." ></td>
	<td class="line x" title="75:250	In general, it is difficult to determine whether one fragment is actually replying to another fragment." ></td>
	<td class="line x" title="76:250	We assume that any new fragment is a potential reply to neighboring quotations  quoted fragments immediately preceding or following it." ></td>
	<td class="line x" title="77:250	Let us consider E6 in Figure 1(a)." ></td>
	<td class="line x" title="78:250	there are two edges from node i to g and h, while there is only a single edge from j to h. For E3, there are the edges (c,b) and (c,a)." ></td>
	<td class="line x" title="79:250	Because of the edge (b,a), the edge (c,a) is not included in Figure 1(b)." ></td>
	<td class="line x" title="80:250	Figure 1(b) shows the fragment quotation graph of the conversation shown in Figure 1(a) with all the redundant edges removed." ></td>
	<td class="line x" title="81:250	In contrast, if threading is done at the coarse granularity of entire emails, as adopted in many studies, the threading would be a simple chain from E6 to E5, E5 to E4 and so on." ></td>
	<td class="line x" title="82:250	Fragment f reflects a special and important phenomenon, where the original email of a quotation does not exist in the users folder." ></td>
	<td class="line x" title="83:250	We call this as the hidden email problem." ></td>
	<td class="line x" title="84:250	This problem and its influence on email summarization were studied in (Carenini et al., 2005) and (Carenini et al., 2007)." ></td>
	<td class="line x" title="85:250	3.2 Building the Sentence Quotation Graph A fragment quotation graph can only represent the conversation in the fragment granularity." ></td>
	<td class="line x" title="86:250	We notice that some sentences in a fragment are more relevant to the conversation than the remaining ones." ></td>
	<td class="line x" title="87:250	The fragment quotation graph is not capable of representing this difference." ></td>
	<td class="line x" title="88:250	Hence, in the following, we describe how to build a sentence quotation graph from the fragment quotation graph and introduce several ways to give weight to the edges." ></td>
	<td class="line x" title="89:250	In a sentence quotation graph GS, each node represents a distinct sentence in the email conversation, and each edge (u,v) represents the replying relationship between node u and v. The algorithm to create the sentence quotation graph contains the following 3 steps: create nodes, create edges and assign weight to edges." ></td>
	<td class="line x" title="90:250	In the following, we first illustrate how to create nodes and edges." ></td>
	<td class="line x" title="91:250	In Section 3.3, we discuss different ways to assign weight to edges." ></td>
	<td class="line x" title="92:250	Given a fragment quotation graph GF, we first split each fragment into a set of sentences." ></td>
	<td class="line x" title="93:250	For each sentence, we create a node in the sentence quotation graph GS." ></td>
	<td class="line x" title="94:250	In this way, each sentence in the email conversation is represented by a distinct node in GS." ></td>
	<td class="line x" title="95:250	As the second step, we create the edges in GS." ></td>
	<td class="line x" title="96:250	The edges in GS are based on the edges in GF 355 Pk s1 s2 sn P1 C1 Ck (a) Fragment Quotation Graph (b) Sentence Quotation Graph F: Ct s1, s2,,sn   P1 C1 Pk  Figure 2: Create the Sentence Quotation Graph from the Fragment Quotation Graph because the edges in GF already reflect the replying relationship among fragments." ></td>
	<td class="line x" title="97:250	For each edge (u,v)  GF, we create edges from each sentence of u to each sentence of v in the sentence quotation graph GS." ></td>
	<td class="line x" title="98:250	This is illustrated in Figure 2." ></td>
	<td class="line x" title="99:250	Note that when each distinct sentence in an email conversation is represented as one node in the sentence quotation graph, the extractive email summarization problem is transformed into a standard node ranking problem within the sentence quotation graph." ></td>
	<td class="line x" title="100:250	Hence, general node ranking algorithms, e.g., Page-Rank, can be used for email summarization as well." ></td>
	<td class="line x" title="101:250	3.3 Measuring the Cohesion Between Sentences After creating the nodes and edges in the sentence quotation graph, a key technical question is how to measure the degree that two sentences are related to each other, e.g., a sentence su is replying to or being replied by sv." ></td>
	<td class="line x" title="102:250	In this paper, we use text cohesion between two sentences su and sv to make this assessment and assign this as the weight of the corresponding edge (su,sv)." ></td>
	<td class="line x" title="103:250	We explore three types of cohesion measures: (1) clue words that are based on stems, (2) semantic distance based on WordNet and (3) cosine similarity that is based on the word TFIDF vector." ></td>
	<td class="line x" title="104:250	In the following, we discuss these three methods separately in detail." ></td>
	<td class="line x" title="105:250	3.3.1 Clue Words Clue words were originally defined as reoccurring words with the same stem between two adjacent fragments in the fragment quotation graph." ></td>
	<td class="line x" title="106:250	In this section, we re-define clue words based on the sentence quotation graph as follows." ></td>
	<td class="line x" title="107:250	A clue word in a sentence S is a non-stop word that also appears (modulo stemming) in a parent or a child node (sentence) of S in the sentence quotation graph." ></td>
	<td class="line x" title="108:250	The frequency of clue words in the two sentences measures their cohesion as described in Equation 1." ></td>
	<td class="line x" title="109:250	weight(su,sv) = summationdisplay wisu freq(wi,sv) (1) 3.3.2 Semantic Similarity Based on WordNet Other than stems, when people reply to previous messages they may also choose some semantically related words, such as synonyms and antonyms, e.g., talk vs. discuss." ></td>
	<td class="line x" title="110:250	Based on this observation, we propose to use semantic similarity to measure the cohesion between two sentences." ></td>
	<td class="line x" title="111:250	We use the wellknown lexical database WordNet to get the semantic similarity of two words." ></td>
	<td class="line x" title="112:250	Specifically, we use the package by (Pedersen et al., 2004), which includes several methods to compute the semantic similarity." ></td>
	<td class="line x" title="113:250	Among those methods, we choose lesk and jcn, which are considered two of the best methods in (Jurafsky and Martin, 2008)." ></td>
	<td class="line x" title="114:250	Similar to the clue words, we measure the semantic similarity of two sentences by the total semantic similarity of the words in both sentences." ></td>
	<td class="line x" title="115:250	This is described in the following equation." ></td>
	<td class="line x" title="116:250	weight(su,sv) = summationdisplay wisu summationdisplay wjsv (wi,wj), (2) 3.3.3 Cosine Similarity Cosine similarity is a popular metric to compute the similarity of two text units." ></td>
	<td class="line x" title="117:250	To do so, each sentence is represented as a word vector of TFIDF values." ></td>
	<td class="line x" title="118:250	Hence, the cosine similarity of two sentences su and sv is then computed as susv||s u||||sv|| . 356 4 Summarization Based on the Sentence Quotation Graph Having built the sentence quotation graph with different measures of cohesion, in this section, we develop two summarization approaches." ></td>
	<td class="line x" title="119:250	One is the generalization of the CWS algorithm in (Carenini et al., 2007) and one is the well-known PageRank algorithm." ></td>
	<td class="line x" title="120:250	Both algorithms compute a score, SentScore(s), for each sentence (node) s, which is used to select the top-k% sentences as the summary." ></td>
	<td class="line x" title="121:250	4.1 Generalized ClueWordSummarizer Given the sentence quotation graph, since the weight of an edge (s,t) represents the extent that s is related to t, a natural assumption is that the more relevant a sentence (node) s is to its parents and children, the more important s is. Based on this assumption, we compute the weight of a node s by summing up the weight of all the outgoing and incoming edges of s. This is described in the following equation." ></td>
	<td class="line x" title="122:250	SentScore(s) = summationdisplay (s,t)GS weight(s,t) + summationdisplay (p,s)GS weight(p,s) (3) The weight of an edge (s,t) can be any of the three metrics described in the previous section." ></td>
	<td class="line x" title="123:250	Particularly, when the weight of the edge is based on clue words as in Equation 1, this method is equivalent to Algorithm CWS in (Carenini et al., 2007)." ></td>
	<td class="line x" title="124:250	In the rest of this paper, let CWS denote the Generalized ClueWordSummarizer when the edge weight is based on clue words, and let CWS-Cosine and CWSSemantic denote the summarizer when the edge weight is cosine similarity and semantic similarity respectively." ></td>
	<td class="line x" title="125:250	Semantic can be either lesk or jcn." ></td>
	<td class="line x" title="126:250	4.2 Page-Rank-based Summarization The Generalized ClueWordSummarizer only considers the weight of the edges without considering the importance (weight) of the nodes." ></td>
	<td class="line x" title="127:250	This might be incorrect in some cases." ></td>
	<td class="line x" title="128:250	For example, a sentence replied by an important sentence should get some of its importance." ></td>
	<td class="line x" title="129:250	This intuition is similar to the one inspiring the well-known Page-Rank algorithm." ></td>
	<td class="line x" title="130:250	The traditional Page-Rank algorithm only considers the outgoing edges." ></td>
	<td class="line x" title="131:250	In email conversations, what we want to measure is the cohesion between sentences no matter which one is being replied to." ></td>
	<td class="line x" title="132:250	Hence, we need to consider both incoming and outgoing edges and the corresponding sentences." ></td>
	<td class="line x" title="133:250	Given the sentence quotation graph Gs, the PageRank-based algorithm is described in Equation 4." ></td>
	<td class="line x" title="134:250	PR(s) is the Page-Rank score of a node (sentence) s. d is the dumping factor, which is initialized to 0.85 as suggested in the Page-Rank algorithm." ></td>
	<td class="line x" title="135:250	In this way, the rank of a sentence is evaluated globally based on the graph." ></td>
	<td class="line x" title="136:250	5 Summarization with Subjective Opinions Other than the conversation structure, the measures of cohesion and the graph-based summarization methods we have proposed, the importance of a sentence in emails can be captured from other aspects." ></td>
	<td class="line oc" title="137:250	In many applications, it has been shown that sentences with subjective meanings are paid more attention than factual ones(Pang and Lee, 2004)(Esuli and Sebastiani, 2006)." ></td>
	<td class="line x" title="138:250	We evaluate whether this is also the case in emails, especially when the conversation is about decision making, giving advice, providing feedbacks, etc. A large amount of work has been done on determining the level of subjectivity of text (Shanahan et al., 2005)." ></td>
	<td class="line x" title="139:250	In this paper we follow a very simple approach that, if successful, could be extended in future work." ></td>
	<td class="line x" title="140:250	More specifically, in order to assess the degree of subjectivity of a sentence s, we count the frequency of words and phrases in s that are likely to bear subjective opinions." ></td>
	<td class="line x" title="141:250	The assumption is that the more subjective words s contains, the more likely that s is an important sentence for the purpose of email summarization." ></td>
	<td class="line x" title="142:250	Let SubjScore(s) denote the number of words with a subjective meaning." ></td>
	<td class="line x" title="143:250	Equation 5 illustrates how SubjScore(s) is computed." ></td>
	<td class="line x" title="144:250	SubjList is a list of words and phrases that indicate subjective opinions." ></td>
	<td class="line x" title="145:250	SubjScore(s) = summationdisplay wiSubjList,wis freq(wi) (5) The SubjScore(s) alone can be used to evaluate the importance of a sentence." ></td>
	<td class="line x" title="146:250	In addition, we can combine SubjScore with any of the sentence scores based on the sentence quotation graph." ></td>
	<td class="line x" title="147:250	In this paper, we use a simple approach by adding them up as the final sentence score." ></td>
	<td class="line x" title="148:250	357 PR(s) = (1d)+ d summationdisplay sichild(s) weight(s,si)PR(si)+ summationdisplay sjparent(s) weight(sj,s)PR(sj) summationdisplay sichild(s) weight(s,si)+ summationdisplay sjparent(s) weight(sj,s) (4) As to the subjective words and phrases, we consider the following two lists generated by researchers in this area." ></td>
	<td class="line x" title="149:250	 OpFind: The list of subjective words in (Wilson et al., 2005)." ></td>
	<td class="line x" title="150:250	The major source of this list is from (Riloff and Wiebe, 2003) with additional words from other sources." ></td>
	<td class="line x" title="151:250	This list contains 8,220 words or phrases in total." ></td>
	<td class="line x" title="152:250	 OpBear: The list of opinion bearing words in (Kim and Hovy, 2005)." ></td>
	<td class="line x" title="153:250	This list contains 27,193 words or phrases in total." ></td>
	<td class="line x" title="154:250	6 Empirical Evaluation 6.1 Dataset Setup There are no publicly available annotated corpora to test email summarization techniques." ></td>
	<td class="line x" title="155:250	So, the first step in our evaluation was to develop our own corpus." ></td>
	<td class="line x" title="156:250	We use the Enron email dataset, which is the largest public email dataset." ></td>
	<td class="line x" title="157:250	In the 10 largest inbox folders in the Enron dataset, there are 296 email conversations." ></td>
	<td class="line x" title="158:250	Since we are studying summarizing email conversations, we required that each selected conversation contained at least 4 emails." ></td>
	<td class="line x" title="159:250	In total, 39 conversations satisfied this requirement." ></td>
	<td class="line x" title="160:250	We use the MEAD package to segment the text into 1,394 sentences (Radev et al., 2004)." ></td>
	<td class="line x" title="161:250	We recruited 50 human summarizers to review those 39 selected email conversations." ></td>
	<td class="line x" title="162:250	Each email conversation was reviewed by 5 different human summarizers." ></td>
	<td class="line x" title="163:250	For each given email conversation, human summarizers were asked to generate a summary by directly selecting important sentences from the original emails in that conversation." ></td>
	<td class="line x" title="164:250	We asked the human summarizers to select 30% of the total sentences in their summaries." ></td>
	<td class="line x" title="165:250	Moreover, human summarizers were asked to classify each selected sentence as either essential or optional." ></td>
	<td class="line x" title="166:250	The essential sentences are crucial to the email conversation and have to be extracted in any case." ></td>
	<td class="line x" title="167:250	The optional sentences are not critical but are useful to help readers understand the email conversation if the given summary length permits." ></td>
	<td class="line x" title="168:250	By classifying essential and optional sentences, we can distinguish the core information from the supporting ones and find the most convincing sentences that most human summarizers agree on." ></td>
	<td class="line x" title="169:250	As essential sentences are more important than the optional ones, we give more weight to the essential selections." ></td>
	<td class="line x" title="170:250	We compute a GSV alue for each sentence to evaluate its importance according to the human summarizers selections." ></td>
	<td class="line x" title="171:250	The score is designed as follows: for each sentence s, one essential selection has a score of 3, one optional selection has a score of 1." ></td>
	<td class="line x" title="172:250	Thus, the GSValue of a sentence ranges from 0 to 15 (5 human summarizers x 3)." ></td>
	<td class="line x" title="173:250	The GSValue of 8 corresponds to 2 essential and 2 optional selections." ></td>
	<td class="line x" title="174:250	If a sentence has a GSValue no less than 8, we take it as an overall essential sentence." ></td>
	<td class="line x" title="175:250	In the 39 conversations, we have about 12% overall essential sentences." ></td>
	<td class="line x" title="176:250	6.2 Evaluation Metrics Evaluation of summarization is believed to be a difficult problem in general." ></td>
	<td class="line x" title="177:250	In this paper, we use two metrics to measure the accuracy of a system generated summary." ></td>
	<td class="line x" title="178:250	One is sentence pyramid precision, and the other is ROUGE recall." ></td>
	<td class="line x" title="179:250	As to the statistical significance, we use the 2-tail pairwise student t-test in all the experiments to compare two specific methods." ></td>
	<td class="line x" title="180:250	We also use ANOVA to compare three or more approaches together." ></td>
	<td class="line x" title="181:250	The sentence pyramid precision is a relative precision based on the GSValue." ></td>
	<td class="line x" title="182:250	Since this idea is borrowed from the pyramid metric by Nenkova et al.(Nenkova et al., 2007), we call it the sentence pyramid precision." ></td>
	<td class="line x" title="183:250	In this paper, we simplify it as the pyramid precision." ></td>
	<td class="line x" title="184:250	As we have discussed above, with the reviewers selections, we get a GSValue for each sentence, which ranges from 0 to 15." ></td>
	<td class="line x" title="185:250	With this GSValue, we rank all sentences in a descendant order." ></td>
	<td class="line x" title="186:250	We also group all sentences with the same GSValue together as one tier Ti, where i is the corre358 sponding GSValue; i is called the level of the tier Ti." ></td>
	<td class="line x" title="187:250	In this way, we organize all sentences into a pyramid: a sequence of tiers with a descendant order of levels." ></td>
	<td class="line x" title="188:250	With the pyramid of sentences, the accuracy of a summary is evaluated over the best summary we can achieve under the same summary length." ></td>
	<td class="line x" title="189:250	The best summary of k sentences are the top k sentences in terms of GSValue." ></td>
	<td class="line x" title="190:250	Other than the sentence pyramid precision, we also adopt the ROUGE recall to evaluate the generated summary with a finer granularity than sentences, e.g., n-gram and longest common subsequence." ></td>
	<td class="line x" title="191:250	Unlike the pyramid method which gives more weight to sentences with a higher GSValue, ROUGE is not sensitive to the difference between essential and optional selections (it considers all sentences in one summary equally)." ></td>
	<td class="line x" title="192:250	Directly applying ROUGE may not be accurate in our experiments." ></td>
	<td class="line x" title="193:250	Hence, we use the overall essential sentences as the gold standard summary for each conversation, i.e., sentences in tiers no lower than T8." ></td>
	<td class="line x" title="194:250	In this way, the ROUGE metric measures the similarity of a system generated summary to a gold standard summary that is considered important by most human summarizers." ></td>
	<td class="line x" title="195:250	Specifically, we choose ROUGE-2 and ROUGE-L as the evaluation metric." ></td>
	<td class="line x" title="196:250	6.3 Evaluating the Weight of Edges In Section 3.3, we developed three ways to compute the weight of an edge in the sentence quotation graph, i.e., clue words, semantic similarity based on WordNet and cosine similarity." ></td>
	<td class="line x" title="197:250	In this section, we compare them together to see which one is the best." ></td>
	<td class="line x" title="198:250	It is well-known that the accuracy of the summarization method is affected by the length of the summary." ></td>
	<td class="line x" title="199:250	In the following experiments, we choose the summary length as 10%, 12%, 15%, 20% and 30% of the total sentences and use the aggregated average accuracy to evaluate different algorithms." ></td>
	<td class="line x" title="200:250	Table 1 shows the aggregated pyramid precision over all five summary lengths of CWS, CWSCosine, two semantic similarities, i.e., CWS-lesk and CWS-jcn." ></td>
	<td class="line x" title="201:250	We first use ANOVA to compare the four methods." ></td>
	<td class="line x" title="202:250	For the pyramid precision, the F ratio is 50, and the p-value is 2.1E-29." ></td>
	<td class="line x" title="203:250	This shows that the four methods are significantly different in the average accuracy." ></td>
	<td class="line x" title="204:250	In Table 1, by comparing CWS with the other methods, we can see that CWS obtains the CWS CWS-Cosine CWS-lesk CWS-jcn Pyramid 0.60 0.39 0.57 0.57 p-value <0.0001 0.02 0.005 ROUGE-2 0.46 0.31 0.39 0.35 p-value <0.0001 <0.001 <0.001 ROUGE-L 0.54 0.43 0.49 0.45 p-value <0.0001 <0.001 <0.001 Table 1: Generalized CWS with Different Edge Weights highest precision (0.60)." ></td>
	<td class="line x" title="205:250	The widely used cosine similarity does not perform well." ></td>
	<td class="line x" title="206:250	Its precision (0.39) is about half of the precision of CWS with a p-value less than 0.0001." ></td>
	<td class="line x" title="207:250	This clearly shows that CWS is significantly better than CWS-Cosine." ></td>
	<td class="line x" title="208:250	Meanwhile, both semantic similarities have lower accuracy than CWS, and the differences are also statistically significant even with the conservative Bonferroni adjustment (i.e., the p-values in Table 1 are multiplied by three)." ></td>
	<td class="line x" title="209:250	The above experiments show that the widely used cosine similarity and the more sophisticated semantic similarity in WordNet are less accurate than the basic CWS in the summarization framework." ></td>
	<td class="line x" title="210:250	This is an interesting result and can be viewed at least from the following two aspects." ></td>
	<td class="line x" title="211:250	First, clue words, though straight forward, are good at capturing the important sentences within an email conversation." ></td>
	<td class="line x" title="212:250	The higher accuracy of CWS may suggest that people tend to use the same words to communicate in email conversations." ></td>
	<td class="line x" title="213:250	Some related words in the previous emails are adopted exactly or in another similar format (modulo stemming)." ></td>
	<td class="line x" title="214:250	This is different from other documents such as newspaper articles and formal reports." ></td>
	<td class="line x" title="215:250	In those cases, the authors are usually professional in writing and choose their words carefully, even intentionally avoid repeating the same words to gain some diversity." ></td>
	<td class="line x" title="216:250	However, for email conversation summarization, this does not appear to be the case." ></td>
	<td class="line x" title="217:250	Moreover, in the previous discussion we only considered the accuracy in precision without considering the runtime issue." ></td>
	<td class="line x" title="218:250	In order to have an idea of the runtime of the two methods, we did the following comparison." ></td>
	<td class="line x" title="219:250	We randomly picked 1000 pairs of words from the 20 conversations and compute their semantic distance in jcn." ></td>
	<td class="line x" title="220:250	It takes about 0.056 seconds to get the semantic similarity for one pair on the 359 average." ></td>
	<td class="line x" title="221:250	In contrast, when the weight of edges are computed based on clue words, the average runtime to compute the SentScore for all sentences in a conversation is only 0.05 seconds, which is even a little less than the time to compute the semantic similarity of one pair of words." ></td>
	<td class="line x" title="222:250	In other words, when CWS has generated the summary of one conversation, we can only get the semantic distance between one pair of words." ></td>
	<td class="line x" title="223:250	Note that for each edge in the sentence quotation graph, we need to compute the distance for every pair of words in each sentence." ></td>
	<td class="line x" title="224:250	Hence, the empirical results do not support the use of semantic similarity." ></td>
	<td class="line x" title="225:250	In addition, we do not discuss the runtime performance of CWS-cosine here because of its extremely low accuracy." ></td>
	<td class="line x" title="226:250	6.4 Comparing Page-Rank and CWS Table 2 compares Page-Rank and CWS under different edge weights." ></td>
	<td class="line x" title="227:250	We compare Page-Rank only with CWS because CWS is better than the other Generalized CWS methods as shown in the previous section." ></td>
	<td class="line x" title="228:250	This table shows that Page-Rank has a lower accuracy than that of CWS and the difference is significant in all four cases." ></td>
	<td class="line x" title="229:250	Moreover, when we compare Table 1 and 2 together, we can find that, for each kind of edge weight, Page-Rank has a lower accuracy than the corresponding Generalized CWS." ></td>
	<td class="line x" title="230:250	Note that Page-Rank computes a nodes rank based on all the nodes and edges in the graph." ></td>
	<td class="line x" title="231:250	In contrast, CWS only considers the similarity between neighboring nodes." ></td>
	<td class="line x" title="232:250	The experimental result indicates that for email conversation, the local similarity based on clue words is more consistent with the human summarizers selections." ></td>
	<td class="line x" title="233:250	6.5 Evaluating Subjective Opinions Table 3 shows the result of using subjective opinions described in Section 5." ></td>
	<td class="line x" title="234:250	The first 3 columns in this table are pyramid precision of CWS and using 2 lists of subjective words and phrases alone." ></td>
	<td class="line x" title="235:250	We can see that by using subjective words alone, the precision of each subjective list is lower than that of CWS." ></td>
	<td class="line x" title="236:250	However, when we integrate CWS and subjective words together, as shown in the remaining 2 columns, the precisions get improved consistently for both lists." ></td>
	<td class="line x" title="237:250	The increase in precision is at least 0.04 with statistical significance." ></td>
	<td class="line x" title="238:250	A natural question to ask is whether clue words and subjective words overlap much." ></td>
	<td class="line x" title="239:250	Our CWS PR-Clue PR-Cosine PR-lesk PR-jcn Pyramid 0.60 0.51 0.37 0.54 0.50 p-value < 0.0001 < 0.0001 < 0.0001 < 0.0001 ROUGE-2 0.46 0.4 0.26 0.36 0.39 p-value 0.05 < 0.0001 0.001 0.02 ROUGE-L 0.54 0.49 0.36 0.44 0.48 p-value 0.06 < 0.0001 0.0005 0.02 Table 2: Compare Page-Rank with CWS CWS OpFind OpBear CWS+OpFind CWS+OpBear Pyramid 0.60 0.52 0.59 0.65 0.64 p-value 0.0003 0.8 <0.0001 0.0007 ROUGE-2 0.46 0.37 0.44 0.50 0.49 p-value 0.0004 0.5 0.004 0.06 ROUGE-L 0.54 0.48 0.56 0.60 0.59 p-value 0.01 0.6 0.0002 0.002 Table 3: Accuracy of Using Subjective Opinions analysis shows that the overlap is minimal." ></td>
	<td class="line x" title="240:250	For the list of OpFind, the overlapped words are about 8% of clue words and 4% of OpFind that appear in the conversations." ></td>
	<td class="line x" title="241:250	This result clearly shows that clue words and subjective words capture the importance of sentences from different angles and can be used together to gain a better accuracy." ></td>
	<td class="line x" title="242:250	7 Conclusions We study how to summarize email conversations based on the conversational cohesion and the subjective opinions." ></td>
	<td class="line x" title="243:250	We first create a sentence quotation graph to represent the conversation structure on the sentence level." ></td>
	<td class="line x" title="244:250	We adopt three cohesion metrics, clue words, semantic similarity and cosine similarity, to measure the weight of the edges." ></td>
	<td class="line x" title="245:250	The Generalized ClueWordSummarizer and Page-Rank are applied to this graph to produce summaries." ></td>
	<td class="line x" title="246:250	Moreover, we study how to include subjective opinions to help identify important sentences for summarization." ></td>
	<td class="line x" title="247:250	The empirical evaluation shows the following two discoveries: (1) The basic CWS (based on clue words) obtains a higher accuracy and a better runtime performance than the other cohesion measures." ></td>
	<td class="line x" title="248:250	It also has a significant higher accuracy than the Page-Rank algorithm." ></td>
	<td class="line x" title="249:250	(2) By integrating clue words and subjective words (phrases), the accuracy of CWS is improved significantly." ></td>
	<td class="line x" title="250:250	This reveals an interesting phenomenon and will be further studied." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-2004
Dimensions of Subjectivity in Natural Language
Chen, Wei;"></td>
	<td class="line x" title="1:109	Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 1316, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:109	c2008 Association for Computational Linguistics Dimensions of Subjectivity in Natural Language Wei Chen Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA weichen@cs.cmu.edu Abstract Current research in automatic subjectivity analysis deals with various kinds of subjective statements involving human attitudes and emotions." ></td>
	<td class="line x" title="3:109	While all of them are related to subjectivity, these statements usually touch on multiple dimensions such as non-objectivity1, uncertainty, vagueness, non-objective measurability, imprecision, and ambiguity, which are inherently different." ></td>
	<td class="line x" title="4:109	This paper discusses the differences and relations of six dimensions of subjectivity." ></td>
	<td class="line x" title="5:109	Conceptual and linguistic characteristics of each dimension will be demonstrated under different contexts." ></td>
	<td class="line x" title="6:109	1 Introduction Natural language involves statements that do not contain complete, exact, and unbiased information." ></td>
	<td class="line x" title="7:109	Many of these are subjective, which share the common property described in narrative theory (Banfield, 1982) as (subjective statements) must all be referred to the speaking subject for interpretation." ></td>
	<td class="line x" title="8:109	Wiebe (1990) further adapted this definition of subjectivity to be the linguistic expression of private states (Quirk et al., 1985)." ></td>
	<td class="line oc" title="9:109	So far, linguistic cues have played an important role in research of subjectivity recognition (e.g.(Wilson et al., 2006)), sentiment analysis (e.g.(Wilson et al., 2005; Pang and Lee, 2004)), and emotion studies (e.g.(Pennebaker et al., 2001))." ></td>
	<td class="line x" title="13:109	While most linguistic cues 1We use the term non-objectivity to refer to the property of creating a bias from a speakers point of view that is not supported by sufficient objective evidence." ></td>
	<td class="line x" title="14:109	It is not identical to the subjectivity that involves all the dimensions we discuss in this paper." ></td>
	<td class="line x" title="15:109	are grouped under the general rubric of subjectivity, they are usually originated from different dimensions, including:  non-objectivity  uncertainty  vagueness  non-objective measurability  imprecision  ambiguity These dimensions all mingle in various applications that deal with subjective statements." ></td>
	<td class="line x" title="16:109	For example, opinion extraction processes statements involving non-objectivity and uncertainty." ></td>
	<td class="line x" title="17:109	Evaluation and sentiment analysis deal with vague words, which often covers the issue of non-objective measurability andimprecision." ></td>
	<td class="line x" title="18:109	Ambiguitysometimesinvolvesimplicit subjectivity that is hard to recognize from linguistic patterns, which leads to great challenge of identifyingandunderstandingsubjectivestatements." ></td>
	<td class="line x" title="19:109	Since multiple dimensions are involved in subjectivity, discriminating them may be helpful in understanding subjectivity and related concepts." ></td>
	<td class="line x" title="20:109	The following sections discuss characteristics and relations of the six dimensions of subjectivity." ></td>
	<td class="line x" title="21:109	2 Dimensions of Subjective Statements 2.1 Non-objectivity In this paper, we define non-objectivity as the property of creating a bias according to personal beliefs, judgments and emotions." ></td>
	<td class="line x" title="22:109	This does not include the kind of subjectivity originated from particular properties of linguistic units that lead to personal interpretations." ></td>
	<td class="line x" title="23:109	Non-objectivity exists in subjective 13 statements such as opinions, evaluations, and persuasive statements." ></td>
	<td class="line x" title="24:109	Non-objectivity can be recognized from linguistic patterns including words explicitly expressing thoughts, beliefs, speculations, and postulations such as think, believe, hope and guess." ></td>
	<td class="line x" title="25:109	Although linguistic cues are found to be reliable, there are cases of non-objectivity that cannot be identified merely from lexical, syntactical or morphological cues." ></td>
	<td class="line x" title="26:109	For example, sentence (1) and sentence (2) are very similar in linguistic structures, but only sentence (2) is non-objective." ></td>
	<td class="line x" title="27:109	(1) Living things cannot survive without water." ></td>
	<td class="line x" title="28:109	(2) He cannot survive without music." ></td>
	<td class="line x" title="29:109	Apart from linguistic patterns and conceptual characteristics of non-objectivity, there are two main issues in non-objectivity recognition." ></td>
	<td class="line x" title="30:109	First, non-objectivity cannot be clearly identified without knowledge about its source (Wiebe et al., 2005)." ></td>
	<td class="line x" title="31:109	For example, Bob says the red team is about to win is objective with respect to the position of the speaker of the sentence, who objectively stated a speech event." ></td>
	<td class="line x" title="32:109	But the fragment the red team is about to win is an opinion of Bob." ></td>
	<td class="line x" title="33:109	Hence, whether a statement is an opinion depends on both the scope of the statement and the source of that statement." ></td>
	<td class="line x" title="34:109	Second, non-objectivity always lies in a context, which cannot be ignored (Wiebe, 1990)." ></td>
	<td class="line x" title="35:109	For example, Pinocchios nose is likely to be objective when used within the context of the famous fairy tale." ></td>
	<td class="line x" title="36:109	But the same phrase can be used subjectively as a metaphor in other contexts, where it may indicate non-objectivity." ></td>
	<td class="line x" title="37:109	2.2 Uncertainty Uncertainty can indicate either subjectivity or objectivity." ></td>
	<td class="line x" title="38:109	Flagged by words such as probably and maybe, statements expressing uncertainty are usually considered subjective because being uncertain itself can be a subjective mental activity." ></td>
	<td class="line x" title="39:109	However, uncertainty is not a subtype of subjectivity." ></td>
	<td class="line x" title="40:109	Consider the following sentences: (3) Bob has probably already finished his homework." ></td>
	<td class="line x" title="41:109	(4) A poll of recent public opinions shows that Bob is likely to win the nomination." ></td>
	<td class="line x" title="42:109	Sentence (3) is a subjective statement, where the speaker expresses his/her postulation of Bob finished his homework through the uncertainty indicated by probably." ></td>
	<td class="line x" title="43:109	On the contrary, sentence (4) is an objective statement, although uncertainty about a future event exists." ></td>
	<td class="line x" title="44:109	This sentence reports a conclusion drawnfrom sufficient evidentthat Bobtakes the majority vote based on the survey, which does not rely on a particular speaking subject for interpretation." ></td>
	<td class="line x" title="45:109	In this case, uncertainty does not necessarily imply subjectivity." ></td>
	<td class="line x" title="46:109	On the other hand, people sometimes explicitly indicate uncertainty to avoid being subjective." ></td>
	<td class="line x" title="47:109	(5) It is possible that the red team will win." ></td>
	<td class="line x" title="48:109	(6) It is likely that the red team will win." ></td>
	<td class="line x" title="49:109	(7) The red team will win." ></td>
	<td class="line x" title="50:109	We could easily imagine a scenario where sentence (5) is more objective than sentence (6) and (7)." ></td>
	<td class="line x" title="51:109	For example, the speaker may believe that the red team will lose, but in order to avoid personal bias, he/she may instead say: It is possible that the red team will win (but the blue team has a better chance). In general, explicitly showing uncertainty can imply postulation, but it can also convey the intention of being objective by not excluding other possibilities." ></td>
	<td class="line x" title="52:109	Uncertainty sometimes exists in statements where no linguistic cues are present." ></td>
	<td class="line x" title="53:109	For example, the linguistic pattern of sentence (7) is similar to that of I will have an exam tomorrow, but the later one is usually used to describe an objective future event while sentence (7) can be semantically identical to sentence (6)2, although the indicator of uncertainty in sentence (7) is not shown explicitly." ></td>
	<td class="line x" title="54:109	2.3 Vagueness, Non-objective Measurability, and Imprecision Vagueness refers to a property of the concepts that have no precise definitions." ></td>
	<td class="line x" title="55:109	For example, gradable words such as small and popular are sometimes treated as linguistic cues of vagueness, and they are found to be good indicators of subjectivity (Hatzivassiloglou and Wiebe, 2000)." ></td>
	<td class="line x" title="56:109	Especially, gradable words are vague if there is no well-defined frame of reference." ></td>
	<td class="line x" title="57:109	This in some cases 2These two are identical as long as the game is not fixed." ></td>
	<td class="line x" title="58:109	14 leads to two issues: comparison class and boundary." ></td>
	<td class="line x" title="59:109	InthesentenceElephantsarebig, thecomparison class of elephants is unclear: we could compare the size of elephants with either land animals or all the animals including both land and aquatic creatures3." ></td>
	<td class="line x" title="60:109	Also, there is no clear boundary between being small and not being small." ></td>
	<td class="line x" title="61:109	Different individuals usually have their own fuzzy boundaries for vague concepts." ></td>
	<td class="line x" title="62:109	As such, vague words are usually treated as important cues for subjectivity." ></td>
	<td class="line x" title="63:109	However, learning which words are vague is non-trivial, because vagueness cannot be hard-coded into lexicons." ></td>
	<td class="line x" title="64:109	For example, the gradable word cold is vague in sentence (8) but not in sentence (9)." ></td>
	<td class="line x" title="65:109	The difference between these two is the one in sentence (9) has a known boundary which is the temperature for liquid water to exist, and the one in sentence (8) simply reflects personal perception." ></td>
	<td class="line x" title="66:109	(8) It is cold outside." ></td>
	<td class="line x" title="67:109	(9) It is too cold during the night on the moon for liquid water to exist." ></td>
	<td class="line x" title="68:109	Vagueness is often a strong indicator of subjectivity because it involves personal explanation of a concept." ></td>
	<td class="line x" title="69:109	But there are exceptions." ></td>
	<td class="line x" title="70:109	For example, the definition of traditional education can be vague, but talking about traditional education may not necessarily imply subjectivity." ></td>
	<td class="line x" title="71:109	When speaking of qualities, there are two major dimensions related to vagueness: non-objective measurability and imprecision." ></td>
	<td class="line x" title="72:109	Attributes like height, length, weight, temperature, and time are objectively measurable, whereas things like beauty and wisdom are usually not objectively measurable." ></td>
	<td class="line x" title="73:109	Vagueness exists at different levels for nonobjectively and objectively measurable qualities." ></td>
	<td class="line x" title="74:109	For non-objectively measurable qualities, vagueness existsattheconceptuallevel,whereitintersectswith non-objectivity." ></td>
	<td class="line x" title="75:109	In the sentence He is not as charming as his brother, the word charming refers to a quality whose interpretation may vary among different cultures and different individuals." ></td>
	<td class="line x" title="76:109	For objectively measurable qualities, vagueness exists at the boundary-setting level, where either subjectivity or common sense comes into play." ></td>
	<td class="line x" title="77:109	Sentence 3Other comparison classes are also possible." ></td>
	<td class="line x" title="78:109	(10) shows an example of the objectively measurable quality long time indicating an opinion that the speaker is unsatisfied with someones work." ></td>
	<td class="line x" title="79:109	On the contrary, an objective meaning of long time in sentence (11) can be resolved by common sense." ></td>
	<td class="line x" title="80:109	(10) You finally finished the work, but it took you a long time." ></td>
	<td class="line x" title="81:109	(11) Intelligent life took a long time to develop on Earth.4 Statements involving objectively measurable quantities often have an imprecision problem, where vagueness is usually resolved from common agreements on small variations of values." ></td>
	<td class="line x" title="82:109	For example, Bob is six feet tall usually implies that the height is around six feet5, with a commonly acceptable precision of about an inch." ></td>
	<td class="line x" title="83:109	Generally, specific precisions are determined by variations tied to measurement technologies for specific quantities: the precision for the size of a cell may be around a micron, and the error tolerance for the distance between stars can be on the order of light years." ></td>
	<td class="line x" title="84:109	Imprecision can also indicate subjectivity when used for subjective estimation." ></td>
	<td class="line x" title="85:109	For instance, Bob needs two days to finish his homework is usually not telling an exact period of time, but a personal estimation." ></td>
	<td class="line x" title="86:109	2.4 Ambiguity While vagueness exists at the conceptual level, ambiguity lies at the level of linguistic expressions." ></td>
	<td class="line x" title="87:109	In other words, an ambiguous statement contains linguistic expressions that can refer to multiple explanations, whereas a vague statement carries a concept with unclear or soft definition." ></td>
	<td class="line x" title="88:109	Previous studies have explored the relationship between ambiguity and subjectivity." ></td>
	<td class="line x" title="89:109	They have shown that subjectivity annotations can be helpful for word sense disambiguation when a word has distinct subjective senses and objective senses (Wiebe and Mihalcea, 2006)." ></td>
	<td class="line x" title="90:109	Lexical and syntactical ambiguity usually can be resolved from contextual information and/or common consensus." ></td>
	<td class="line x" title="91:109	But when ambiguity is used intentionality, identifying and understanding the ambiguity become creative and interactive procedures, 4Sentence fragment adapted from Astrobiology Magazine (Dec 02, 2002)." ></td>
	<td class="line x" title="92:109	5It could also mean at least six feet tall in some cases." ></td>
	<td class="line x" title="93:109	15 which usually indicate subjectivity." ></td>
	<td class="line x" title="94:109	The sentence Id like to see more of you is an example of this kind,whichcouldbeusedtoindicatemultiplemeanings under the same context 6." ></td>
	<td class="line x" title="95:109	3 Mixtures of Multiple Dimensions In many cases, subjective statements involve multiple of the dimensions discussed in previous sections." ></td>
	<td class="line x" title="96:109	For example, the subjectivity of the sentence Its a nice car comes from three dimensions: nonobjectivity, vagueness and ambiguity." ></td>
	<td class="line x" title="97:109	First, a car being nice is usually a personal opinion which may not be commonly acceptable." ></td>
	<td class="line x" title="98:109	Second, the gradable word nice indicates vagueness, since there is no clear boundary for being nice." ></td>
	<td class="line x" title="99:109	Third, the sentence is also ambiguous because nice could refer to appearance, acceleration, angle rate, and many other metrics that might affect personal evaluations." ></td>
	<td class="line x" title="100:109	For information retrieval systems, processing natural queries such as find me the popular movies of 2007 requires proper understanding of the vague word popular." ></td>
	<td class="line x" title="101:109	Besides, non-objectivity and ambiguity also take part in the query: on the nonobjectivity side, the definition of popular may differ according to different individuals; on the ambiguity side, the word popular may refer to different metrics related to the popularity of a movie such as movie ratings and box office performance." ></td>
	<td class="line x" title="102:109	In applications requiring certain level of language-understanding, things can get even more complicated while different dimensions weave together." ></td>
	<td class="line x" title="103:109	As in sentence (5), the speaker may bias towards the blue team while he/she shows uncertainty towards the red team." ></td>
	<td class="line x" title="104:109	Correctly understanding this kind of subjective statements would probably need some investigation in different dimensions of subjectivity." ></td>
	<td class="line x" title="105:109	4 Conclusion In this paper, we demonstrated that subjectivity in naturallanguageisacomplexphenomenonthatcontains multiple dimensions including non-objectivity, uncertainty, vagueness, non-objective measurability, imprecision and ambiguity." ></td>
	<td class="line x" title="106:109	These dimensions pattern together in various kinds of subjective state6Kent Bach, Ambiguity." ></td>
	<td class="line x" title="107:109	Routledge Encyclopedia of Philosophy, http://online.sfsu.edu/ kbach/ambguity.html ments such as opinions, evaluations and natural queries." ></td>
	<td class="line x" title="108:109	Since these dimensions have different behaviors in subjective statements, discriminating them in both linguistic and psychological aspects would be necessary in subjectivity analysis." ></td>
	<td class="line x" title="109:109	Acknowledgments The author would like to thank Scott Fahlman for the original motivation of the idea and helpful discussions." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W08-0122
Discourse Level Opinion Relations: An Annotation Study
Somasundaran, Swapna;Ruppenhofer, Josef;Wiebe, Janyce M.;"></td>
	<td class="line x" title="1:217	Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 129137, Columbus, June 2008." ></td>
	<td class="line x" title="2:217	c2008 Association for Computational Linguistics Discourse Level Opinion Relations: An Annotation Study Swapna Somasundaran Dept. of Computer Science University of Pittsburgh Pittsburgh, PA 15260 swapna@cs.pitt.edu Josef Ruppenhofer Intelligent Systems Program University of Pittsburgh Pittsburgh, PA 15260 josefr@cs.pitt.edu Janyce Wiebe Dept. of Computer Science University of Pittsburgh Pittsburgh, PA 15260 wiebe@cs.pitt.edu Abstract This work proposes opinion frames as a representation of discourse-level associations that arise from related opinion targets and which are common in task-oriented meeting dialogs." ></td>
	<td class="line x" title="3:217	We define the opinion frames and explain their interpretation." ></td>
	<td class="line x" title="4:217	Additionally we present an annotation scheme that realizes the opinion frames and via human annotation studies, we show that these can be reliably identified." ></td>
	<td class="line x" title="5:217	1 Introduction There has been a great deal of research in recent years on opinions and subjectivity." ></td>
	<td class="line x" title="6:217	Opinions have been investigated at the phrase, sentence, and document levels." ></td>
	<td class="line x" title="7:217	However, little work has been carried out at the level of discourse." ></td>
	<td class="line x" title="8:217	Consider the following excerpt from a dialog about designing a remote control for a television (the opinion targets what the opinions are about are shown in italics)." ></td>
	<td class="line x" title="9:217	(1) D:: And I thought not too edgy and like a box, more kind of hand-held not as computery, yeah, more organic shape I think." ></td>
	<td class="line x" title="10:217	Simple designs, like the last one we just saw, not too many buttons  Speaker D expresses an opinion in favor of a design that is simple and organic in shape, and against an alternative design which is not." ></td>
	<td class="line x" title="11:217	Several individual opinions are expressed in this passage." ></td>
	<td class="line x" title="12:217	The first is a negative opinion about the design being too edgy and box-like, the next is a positive opinion toward a hand-held design, followed by a negative opinion toward a computery shape, and so on." ></td>
	<td class="line x" title="13:217	While we believe that recognizing individual expressions of opinions, their properties, and components is important, we believe that discourse interpretation is needed as well." ></td>
	<td class="line x" title="14:217	It is by understanding the passage as a discourse that we see edgy, like a box, computery, and many buttons as descriptions of the type of design D does not prefer, and hand-held, organic shape, and simple designs as descriptions of the type he does." ></td>
	<td class="line x" title="15:217	These descriptions are not in general synonyms/antonyms of one another; for example, there are hand-held computery devices and simple designs that are edgy." ></td>
	<td class="line x" title="16:217	The unison/opposition among the descriptions is due to how they are used in the discourse." ></td>
	<td class="line x" title="17:217	This paper focuses on such relations between the targets of opinions in discourse." ></td>
	<td class="line x" title="18:217	Specifically, we propose opinion frames, which consist of two opinions which are related by virtue of having united or opposed targets." ></td>
	<td class="line x" title="19:217	We believe that recognizing opinion frames will provide more information for NLP applications than recognizing their individual components alone." ></td>
	<td class="line x" title="20:217	Further, if there is uncertainty about any one of the components, we believe opinion frames are an effective representation incorporating discourse information to make an overall coherent interpretation (Hobbs, 1979; Hobbs, 1983)." ></td>
	<td class="line x" title="21:217	To our knowledge, this is the first work to extend a manual annotation scheme to relate opinions in the discourse." ></td>
	<td class="line x" title="22:217	In this paper, we present opinion frames, and motivate their usefulness through examples." ></td>
	<td class="line x" title="23:217	Then we provide an annotation scheme for capturing these opinion frames." ></td>
	<td class="line x" title="24:217	Finally we perform fine-grained annotation studies to measure the human reliability in recognizing of these opinion frames." ></td>
	<td class="line x" title="25:217	129 Opinion frames are presented in Section 2, our annotation scheme is described in Section 3, the interannotator agreement studies are presented in Section 4, related work is discussed in Section 5, and conclusions are in Section 6." ></td>
	<td class="line x" title="26:217	2 Opinion Frames 2.1 Introduction The components of opinion frames are individual opinions and the relationships between their targets." ></td>
	<td class="line x" title="27:217	We address two types of opinions, sentiment and arguing." ></td>
	<td class="line x" title="28:217	Following (Wilson and Wiebe, 2005; Somasundaran et al., 2007), sentiment includes positive and negative evaluations, emotions, and judgments, while arguing includes arguing for or against something, and arguing that something should or should not be done." ></td>
	<td class="line x" title="29:217	In our examples, the lexical anchors revealing the opinion type (as the words are interpreted in context) are indicated in bold face." ></td>
	<td class="line x" title="30:217	In addition, the text span capturing the target of the opinion (again, as interpreted in context) is indicated in italics." ></td>
	<td class="line x" title="31:217	(2) D::  this kind of rubbery material, its a bit more bouncy, like you said they get chucked around a lot." ></td>
	<td class="line x" title="32:217	A bit more durable and that can also be ergonomic and it kind of feels a bit different from all the other remote controls." ></td>
	<td class="line x" title="33:217	Speaker D expresses his preference for the rubbery material for the remote." ></td>
	<td class="line x" title="34:217	He reiterates his opinion with a number of positive evaluations like bit more bouncy, bit more durable, ergonomic and so on." ></td>
	<td class="line x" title="35:217	All opinions in this example are related to the others via opinion frames by virtue of having the same targets, i.e., the opinions are essentially about the same things (the rubbery material for the remote)." ></td>
	<td class="line x" title="36:217	For example, the opinions ergonomic and a bit different from all the other remote controls are related in a frame of type SPSPsame, meaning the first opinion is a S(entiment) with polarity P(ositive); the second also is a S(entiment) with polarity P(ositive); and the targets of the opinions are in a same (target) relation." ></td>
	<td class="line x" title="37:217	The specific target relations addressed in this paper are the relations of either being the same or being alternatives to one another." ></td>
	<td class="line x" title="38:217	While these are not the only possible relations, they are not infrequent, and SPSPsame, SNSNsame, APAPsame, ANANsame, SPAPsame, APSPsame, SNANsame, ANSNsame, SPSNalt, SNSPalt, APANalt, ANAPalt, SPANalt, SNAPalt, APSNalt, ANSPalt SPSNsame, SNSPsame, APANsame, ANAPsame, SPANsame, APSNsame, SNAPsame, ANSPsame, SPSPalt, SNSNalt, APAPalt, ANANalt, SPAPalt, SNANalt, APSPalt, ANSNalt Table 1: Opinion Frames they commonly occur in task-oriented dialogs such as those in our data." ></td>
	<td class="line x" title="39:217	With four opinion type polarity pairs (SN, SP, AN, AP), for each of two opinion slots, and two possible target relations, we have 4 * 4 * 2 = 32 types of frame, listed in Table 1." ></td>
	<td class="line x" title="40:217	In the remainder of this section, we elaborate further on the same target relation (in 2.2) the alternative target relation (in 2.3) and explain a method by which these relationships can be propagated (in 2.4)." ></td>
	<td class="line x" title="41:217	Finally, we illustrate the usefulness of opinion frames in discourse interpretation (in 2.5)." ></td>
	<td class="line x" title="42:217	2.2 Same Targets Our notion of sameness for targets includes cases of anaphora and ellipses, lexically similar items, as well as less direct relations such as part-whole, subset, inferable, and instance-class." ></td>
	<td class="line x" title="43:217	Looking at the opinion frames for Example 2 in more detail, we separately list the opinions, followed by the relations between targets." ></td>
	<td class="line x" title="44:217	Opinion Span target Span Type O1 bit more bouncy its [t1] SP O2 bit more durable ellipsis [t2] SP O3 ergonomic that [t3] SP O4 a bit different from all the other remote it [t4] SP Target target Rel t1 t2 same t1 t3 same t3 t4 same Ellipsis occurs with bit more durable." ></td>
	<td class="line x" title="45:217	[t2] represents the (implicit) target of that opinion, and [t2] has a same relation to [t1], the target of the bit more bouncy opinion." ></td>
	<td class="line x" title="46:217	(Note that the interpretation of the first target, [t1], would require anaphora resolution of its target span with a previous noun phrase, rubbery material.)" ></td>
	<td class="line x" title="47:217	Let us now consider the following passage, in which a meeting participant analyzes two leading re130 motes on the market.1 (3) D:: These are two leading remote controls at the moment." ></td>
	<td class="line x" title="48:217	You know theyre grey, this ones got loads of buttons, its hard to tell from here what they actually do, and they dont look very exciting at all." ></td>
	<td class="line x" title="49:217	Opinion Span target Span Rel O1 leading remote controls [t1] SP O2 grey they [t2] SN O3 loads of buttons this one [t3] SN O4 hard to tell they [t4] SN O5 dont look very exciting at all they [t5] SN Target target Rel t1 t2 same t2 t3 same t3 t4 same t5 t1 same Target [t2] is the set of two leading remotes, and [t3], which is in a same relation with [t2], is one of those remotes." ></td>
	<td class="line x" title="50:217	Target [t4], which is also in a same relation with [t3], is an aspect of that remote, namely its buttons." ></td>
	<td class="line x" title="51:217	Thus, opinion O3 is directly about one of the remotes, and indirectly about the set of both remotes." ></td>
	<td class="line x" title="52:217	Similarly, opinion O4 is directly about the buttons of one of the remotes, and indirectly about that remote itself." ></td>
	<td class="line x" title="53:217	2.3 Alternative Targets The alt(ernative) target relation arises when multiple choices are available, and only one can be selected." ></td>
	<td class="line x" title="54:217	For example, in the domain of TV remote controls, the set of all shapes are alternatives to one another, since a remote control may have only one shape at a time." ></td>
	<td class="line x" title="55:217	In such scenarios, a positive opinion regarding one choice may imply a negative opinion toward the rest of the choices, and vice versa." ></td>
	<td class="line x" title="56:217	As an example, let us now consider the following passage (some intervening utterances have been removed for clarity)." ></td>
	<td class="line x" title="57:217	(4) C::  shapes should be curved, so round shapes2 Nothing square-like." ></td>
	<td class="line x" title="58:217	C::  So we shouldnt have too square corners and that kind of thing." ></td>
	<td class="line x" title="59:217	B:: Yeah okay." ></td>
	<td class="line x" title="60:217	Not the old box look." ></td>
	<td class="line x" title="61:217	1In the other examples in this paper, the source (holder) of the opinions is the speaker." ></td>
	<td class="line x" title="62:217	The leading opinion in this example is an exception: its source is implicit; it is a consensus opinion that is not necessarily shared by the speaker (i.e., it is a nested source (Wiebe et al., 2005))." ></td>
	<td class="line x" title="63:217	2In the context of the dialogs, the annotators read the so round shapes as a summary statement." ></td>
	<td class="line x" title="64:217	Had the so been interpreted as Arguing, the round shapes would have been annotated as a target (and linked to curved)." ></td>
	<td class="line x" title="65:217	Opinion Span target Span Rel O1 should be curved [t1] AP O2 Nothing square-like [t2] AN O3 shouldnt have square corners [t3] AN O4 too square corners [t3] SN O5 Not the old box look [t4] AN O6 the old box look the old box look [t4] SN Target target Rel t1 -t2 alternatives t2 t3 same t3 t4 same There is an alt relation between, for example, [t1] and [t2]." ></td>
	<td class="line x" title="66:217	Thus, we have an opinion frame between O1 and O2, whose type is APANalt." ></td>
	<td class="line x" title="67:217	From this frame, we understand that a positive opinion is expressed toward something and a negative opinion is expressed toward its alternative." ></td>
	<td class="line x" title="68:217	2.4 Link Transitivity When individual targets are linked, they form a chain-like structure." ></td>
	<td class="line x" title="69:217	Due to this, a connecting path may exist between targets that were not directly linked by the human annotators." ></td>
	<td class="line x" title="70:217	This path may be traversed to create links between new pairs of targets which in turn results in new opinion frame relations." ></td>
	<td class="line x" title="71:217	For instance, in Example 4, the frame with direct relation is O1O2 APANalt." ></td>
	<td class="line x" title="72:217	By following the alt link from [t1] to [t2] and the same link from [t2] to [t3], we have an alt link between [t1] and [t3], and the additional frames O1O3 APANalt and O1O4 APSNalt." ></td>
	<td class="line x" title="73:217	Repeating this process would finally link speaker Cs opinion O1 with Bs opinion O6, yielding a APSNalt frame." ></td>
	<td class="line x" title="74:217	2.5 Interpretation This section illustrates two motivations for opinion frames: they may unearth additional information over and above the individual opinions stated in the text, and they may contribute toward arriving at a coherent interpretation (Hobbs, 1979; Hobbs, 1983) of the opinions in the discourse." ></td>
	<td class="line x" title="75:217	Through opinion frames, opinions regarding something not explicitly mentioned in the local context and not even lexically related can become relevant, providing more information about someones opinions." ></td>
	<td class="line x" title="76:217	This is particularly interesting when alt relations are involved, as opinions towards one alternative imply opinions of opposite polarity toward the remaining options." ></td>
	<td class="line x" title="77:217	For instance in Example 4 131 above, if we consider only the explicitly stated opinions, there is only one (positive) opinion about the curved shape, namely O1." ></td>
	<td class="line x" title="78:217	However, the speaker expresses several other opinions which reinforce his positivity toward the curved shape." ></td>
	<td class="line x" title="79:217	These are in fact opinion frames in which the other opinion has the opposite polarity as O1 and the target relation is alt (for example frames such as O1O3 APANalt and O1O4 APSNalt)." ></td>
	<td class="line x" title="80:217	In the dialog, notice that speaker B agrees with C and exhibits his own reinforcing opinions." ></td>
	<td class="line x" title="81:217	These would be similarly linked via targets resulting in frames like O1O6 APSNalt." ></td>
	<td class="line x" title="82:217	Turning to our second point, arriving at a coherent interpretation obviously involves disambiguation." ></td>
	<td class="line x" title="83:217	Suppose that some aspect of an individual opinion, such as polarity, is unclear." ></td>
	<td class="line x" title="84:217	If the discourse suggests certain opinion frames, this may in turn resolve the underlying ambiguity." ></td>
	<td class="line x" title="85:217	For instance in Example 2, we see that out of context, the polarities of bouncy and different from other remotes are unclear (bounciness and being different may be negative attributes for another type of object)." ></td>
	<td class="line x" title="86:217	However, the polarities of two of the opinions are clear (durable and ergonomic)." ></td>
	<td class="line x" title="87:217	There is evidence in this passage of discourse continuity and same relations, such as the pronouns, the lack of contrastive cue phrases, and so on." ></td>
	<td class="line x" title="88:217	This evidence suggests that the speaker expresses similar opinions throughout the passage, making the opinion frame SPSPsame more likely throughout." ></td>
	<td class="line x" title="89:217	Recognizing the frames would resolve the polarity ambiguities of bouncy and different." ></td>
	<td class="line x" title="90:217	Example 2 is characterized by opinion frames in which the opinions reinforce one other." ></td>
	<td class="line x" title="91:217	Interestingly, interplays among different opinion types may show the same type of reinforcement." ></td>
	<td class="line x" title="92:217	As we analyzed above, Example 4 is characterized by mixtures of opinion types, polarities, and target relations." ></td>
	<td class="line x" title="93:217	However, the opinions are still unified in the intention to argue for a particular type of shape." ></td>
	<td class="line x" title="94:217	There is evidence in this passage suggesting reinforcing frames: the negations are applied to targets that are alternative to the desired option, and the passage is without contrastive discourse cues." ></td>
	<td class="line x" title="95:217	If we are able to recognize the best overall set of opinion frames for the passage, the polarity ambiguities will be resolved." ></td>
	<td class="line x" title="96:217	On the other hand, evidence for non-reinforcing opinions would suggest other frames, potentially resulting in different interpretations of polarity and relations among targets." ></td>
	<td class="line x" title="97:217	Such non-reinforcing associations between opinions and often occur when the speaker is ambivalent or weighing pros and cons." ></td>
	<td class="line x" title="98:217	Table 1 lists the frames that occur in reinforcing scenarios in the top row, and the frames that occur in non-reinforcing scenarios in the bottom row." ></td>
	<td class="line x" title="99:217	3 Annotation Scheme Our annotation scheme began with the definition and basics of the opinion annotation from previous work (Wilson and Wiebe, 2005; Somasundaran et al., 2007)." ></td>
	<td class="line x" title="100:217	We then add to it the attributes and components that are necessary to make an Opinion Frame." ></td>
	<td class="line x" title="101:217	First, the text span that reveals the opinion expression is identified." ></td>
	<td class="line x" title="102:217	Then, the text spans corresponding to the targets are marked, if there exist any (we also allow span-less targets)." ></td>
	<td class="line x" title="103:217	Then, the type and polarity of the opinion in the context of the discourse is marked." ></td>
	<td class="line x" title="104:217	Finally the targets that are related (again in the context of the discourse) are linked." ></td>
	<td class="line x" title="105:217	Specifically, the components that form the Annotation of the frame are as follows: Opinion Span: This is a span of text that reveals the opinion." ></td>
	<td class="line x" title="106:217	Type: This attribute specifies the opinion type as either Arguing or SentimentPackage." ></td>
	<td class="line x" title="107:217	Polarity: This attribute identifies the valence of an opinion and can be one of: positive, negative, neutral, both, unknown." ></td>
	<td class="line x" title="108:217	Target Span: This is a span of text that captures what an opinion is about." ></td>
	<td class="line x" title="109:217	This can be a proposition or an entity." ></td>
	<td class="line x" title="110:217	Target Link: This is an attribute of a target and records all the targets in the discourse that the target is related to." ></td>
	<td class="line x" title="111:217	Link Type: The link between two targets is specified by this attribute as either same or alternative." ></td>
	<td class="line x" title="112:217	132 In addition to these definitions, our annotation manual has guidelines detailing how to deal with grammatical issues, disfluencies, etc. Appendix A illustrates how this annotation scheme is applied to the utterances of Example 4." ></td>
	<td class="line x" title="113:217	Links between targets can be followed in either direction to construct chains." ></td>
	<td class="line x" title="114:217	In this work, we consider target relations to be commutative, i.e., Link(t1,t2) => Link(t2,t1)." ></td>
	<td class="line x" title="115:217	When a newly annotated target is similar (or opposed) to a set of targets already participating in same relations, then the same (or alt) link is made only to one of them the one that looks most natural." ></td>
	<td class="line x" title="116:217	This is often the one that is closest." ></td>
	<td class="line x" title="117:217	4 Annotation Studies Construction of an opinion frame is a stepwise process where first the text spans revealing the opinions and their targets are selected, the opinion text spans are classified by type and polarity and finally the targets are linked via one of the possible relations." ></td>
	<td class="line x" title="118:217	We split our annotation process into these 3 intuitive stages and use an evaluation that is most applicable for the task at that stage." ></td>
	<td class="line x" title="119:217	Two annotators (both co-authors on the paper) underwent training at each stage, and the annotation manual was revised after each round of training." ></td>
	<td class="line x" title="120:217	In order to prevent errors incurred at earlier stages from affecting the evaluation of later stages, the annotators produced a consensus version at the end of each stage, and used that consensus annotation as the starting point for the next annotation stage." ></td>
	<td class="line x" title="121:217	In producing these consensus files, one annotator first annotated a document, and the other annotator reviewed the annotations, making changes if needed." ></td>
	<td class="line x" title="122:217	This prevented any discussion between the annotators from influencing the tagging task of the next stage." ></td>
	<td class="line x" title="123:217	In the following subsections, we first introduce the data and then present our results for annotation studies for each stage, ending with discussion." ></td>
	<td class="line x" title="124:217	4.1 Data The data used in this work is the AMI meeting corpus (Carletta et al., 2005) which contains multimodal recordings of group meetings." ></td>
	<td class="line x" title="125:217	We annotated meetings from the scenario based meetings, where Gold Exact Lenient Subset ANN-1 53 89 87 ANN-2 44 76 74 Table 2: Inter-Annotator agreement on Opinion Spans four participants collaborate to design a new TV remote control in a series of four meetings." ></td>
	<td class="line x" title="126:217	The meetings represent different project phases, namely project kick-off, functional design, conceptual design, and detailed design." ></td>
	<td class="line x" title="127:217	Each meeting has rich transcription and segment (turn/utterance) information for each speaker." ></td>
	<td class="line x" title="128:217	Each utterance consists of one or more sentences." ></td>
	<td class="line x" title="129:217	At each agreement stage we used approximately 250 utterances from a meeting for evaluation." ></td>
	<td class="line x" title="130:217	The annotators also used the audio and video recordings in the annotation of meetings." ></td>
	<td class="line x" title="131:217	4.2 Opinion Spans and Target Spans In this step, the annotators selected text spans and labeled them as opinion or target We calculated our agreement for text span retrieval similar to Wiebe et al.(2005)." ></td>
	<td class="line x" title="133:217	This agreement metric corresponds to the Precision metric in information retrieval, where annotations from one annotator are considered the gold standard, and the other annotators annotations are evaluated against it." ></td>
	<td class="line x" title="134:217	Table 2 shows the inter-annotator agreement (in percentages)." ></td>
	<td class="line x" title="135:217	For the first row, the annotations produced by Annotator-1 (ANN-1) are taken as the gold standard and, for the second row, the annotations from annotator-2 form the gold standard." ></td>
	<td class="line x" title="136:217	The Exact column reports the agreement when two text spans have to match exactly to be considered correct." ></td>
	<td class="line x" title="137:217	The Lenient column shows the results if an overlap relation between the two annotators retrieved spans is also considered to be a hit." ></td>
	<td class="line x" title="138:217	Wiebe et al.(2005) use this approach to measure agreement for a (somewhat) similar task of subjectivity span retrieval in the news corpus." ></td>
	<td class="line x" title="140:217	Our agreement numbers for this column is comparable to theirs." ></td>
	<td class="line x" title="141:217	Finally, the third column, Subset, shows the agreement for a more strict constraint, namely, that one of the spans must be a subset of the other to be considered a match." ></td>
	<td class="line x" title="142:217	Two opinion spans that satisfy this relation are ensured to share all the opinion words of the smaller span." ></td>
	<td class="line x" title="143:217	The numbers indicate that, while the annotators 133 Gold Exact Lenient Subset ANN-1 54 73 71 ANN-2 54 75 74 Table 3: Inter-Annotator agreement on Target Spans Gold Exact Lenient Subset ANN-1 74 87 87 ANN-2 76 90 90 Table 4: Inter-Annotator agreement on Targets with Perfect Opinion spans do not often retrieve the exact same span, they reliably retrieve approximate spans." ></td>
	<td class="line x" title="144:217	Interestingly, the agreement numbers between Lenient and Subset columns are close." ></td>
	<td class="line x" title="145:217	This implies that, in the cases of inexact matches, the spans retrieved by the two annotators are still close." ></td>
	<td class="line x" title="146:217	They agree on the opinion words and differ mostly on the inclusion of function words (e.g. articles) and observation of syntactic boundaries." ></td>
	<td class="line x" title="147:217	In similar fashion, Table 3 gives the interannotator agreement for target span retrieval." ></td>
	<td class="line x" title="148:217	Additionally, Table 4 shows the inter-annotator agreement for target span retrieval when opinions that do not have an exact match are filtered out." ></td>
	<td class="line x" title="149:217	That is, Table 4 shows results only for targets of the opinions on which the annotators perfectly agree." ></td>
	<td class="line x" title="150:217	As targets are annotated with respect to the opinions, this second evaluation removes any effects of disagreements in the opinion detection task." ></td>
	<td class="line x" title="151:217	As seen in Table 4, this improves the inter-coder agreement." ></td>
	<td class="line x" title="152:217	4.3 Opinion Type and Polarity In this step, the annotators began with the consensus opinion span and target span annotations." ></td>
	<td class="line x" title="153:217	We hypothesized that given the opinion expression, determining whether it is Arguing or Sentiment would not be difficult." ></td>
	<td class="line x" title="154:217	Similarly, we hypothesized that target information would make the polarity labeling task clearer." ></td>
	<td class="line x" title="155:217	As every opinion instance is tagged with a type Type Tagging Polarity Tagging Accuracy 97.8% 98.5%  0.95 0.952 Table 5: Inter-Annotator agreement on Opinion Types and Polarity and polarity, we use Accuracy and Cohens Kappa () metric (Cohen, 1960)." ></td>
	<td class="line x" title="156:217	The  metric measures the inter-annotator agreement above chance agreement." ></td>
	<td class="line x" title="157:217	The results, in Table 5, show that  both for type and polarity tagging is very high." ></td>
	<td class="line x" title="158:217	This confirms our hypothesis that Sentiment and Arguing can be reliably distinguished once the opinion spans are known." ></td>
	<td class="line x" title="159:217	Our polarity detection task shows an improvement in  over a similar polarity assignment task by Wilson et al.(2005) for the news corpus ( of 0.72)." ></td>
	<td class="line x" title="161:217	We believe this improvement can partly be attributed to the target information available to our annotators." ></td>
	<td class="line x" title="162:217	4.4 Target Linking As an intuitive first step in evaluating target linking, we treat target links in the discourse similarly to anaphoric chains and apply methods developed for co-reference resolution (Passonneau, 2004) for our evaluation." ></td>
	<td class="line x" title="163:217	Passonneaus method is based on Krippendorfs  metric (Krippendorff, 2004) and allows for partial matches between anaphoric chains." ></td>
	<td class="line x" title="164:217	In addition to this, we evaluate links identified by both annotators for the type (same / alternative) labeling task with the help of the  metric." ></td>
	<td class="line x" title="165:217	Passonneau (2004) reports that in her co-reference task on spoken monologs,  varies with the difficulty of the corpus (from 0.46 to 0.74)." ></td>
	<td class="line x" title="166:217	This is true in our case too." ></td>
	<td class="line x" title="167:217	Table 6 shows our agreement for the four types of meetings in the AMI corpus: the kickoff meeting (a), the functional design (b), the conceptual design (c) and the detailed design (d)." ></td>
	<td class="line x" title="168:217	Of the meetings, the kickoff meeting (a) we use has relatively clear discussions." ></td>
	<td class="line x" title="169:217	The conceptual design meeting (c) is the toughest, as as participants are expressing opinions about a hypothetical (desirable) remote." ></td>
	<td class="line x" title="170:217	In our detailed design meeting (d), there are two final designs being evaluated." ></td>
	<td class="line x" title="171:217	On analyzing the chains from the two annotators, we discovered that one annotator had maintained two separate chains for the two remotes as there is no explicit linguistic indication (within the 250 utterances) that these two are alternatives." ></td>
	<td class="line x" title="172:217	The second annotator, on the other hand, used the knowledge that the goal of the meeting is to design a single TV remote to link them as alternatives." ></td>
	<td class="line x" title="173:217	Thus by changing just two links in the second annotators file to account for this, our  for this meeting went up from 0.52 134 Meeting: a b c d Target linking () 0.79 0.74 0.59 0.52 Relation Labeling () 1 1 0.91 1 Table 6: Inter-Annotator agreement on Target relation identification to 0.70." ></td>
	<td class="line x" title="174:217	We plan to further explore other evaluation methodologies that account for severity of differences in linking and are more relevant for our task." ></td>
	<td class="line x" title="175:217	Nonetheless, the resulting numbers indicate that there is sufficient information in the discourse to provide for reliable linking of targets." ></td>
	<td class="line x" title="176:217	The high  for the relation type identification shows that once the presence of a link is detected, it is not difficult to determine if the targets are similar or alternatives to each other." ></td>
	<td class="line x" title="177:217	4.5 Discussion Our agreement studies help to identify the aspects of opinion frames that are straightforward, and those that need complex reasoning." ></td>
	<td class="line x" title="178:217	Our results indicate that while the labeling tasks such as opinion type, opinion polarity and target relation type are relatively reliable for humans, retrieval of opinions spans, target spans and target links is more difficult." ></td>
	<td class="line x" title="179:217	A common cause of annotation disagreement is different interpretation of the utterance, particularly in the presence of disfluencies and restarts." ></td>
	<td class="line x" title="180:217	For example consider the following utterance where a participant is evaluating the drawing of another participant on the white board." ></td>
	<td class="line x" title="181:217	(5) Its a baby shark , it looks to me,  One annotator interpreted this it looks to me as an arguing for the belief that it was indeed a drawing of a baby shark (positive Arguing)." ></td>
	<td class="line x" title="182:217	The second annotator on the other hand looked at it as a neutral viewpoint/evaluation (Sentiment) being expressed regarding the drawing." ></td>
	<td class="line x" title="183:217	Thus even though both annotators felt an opinion is being expressed, they differed on its type and polarity." ></td>
	<td class="line x" title="184:217	There are some opinions that are inherently on the borderline of Sentiment and Arguing." ></td>
	<td class="line x" title="185:217	For example, consider the following utterance where there is an appeal to importance: (6) Also important for you all is um the production cost must be maximal twelve Euro and fifty cents." ></td>
	<td class="line x" title="186:217	Here, also important might be taken as an assessment of the high value of adhering to the budget (relative to other constraints), or simply as an argument for adhering to the budget." ></td>
	<td class="line x" title="187:217	One potential source of problems to the targetlinking process consists of cases where the same item becomes involved in more than one opposition." ></td>
	<td class="line x" title="188:217	For instance, in the example below, speaker D initially sets up an alternative between speech recognition and buttons as a possible interface for navigation." ></td>
	<td class="line x" title="189:217	But later, speaker A re-frames the choice as between having speech recognition only and having both options." ></td>
	<td class="line x" title="190:217	Connecting up all references to speech recognition as a target respects the co-reference but it also results in incorrect conclusions: the speech recognition is an alternative to having both speech recognition and buttons." ></td>
	<td class="line x" title="191:217	(7) A:: One thing is interesting is talking about speech recognition in a remote control D::  So that we dont need any button on the remote control it would be all based on speech." ></td>
	<td class="line x" title="192:217	A::  I think that would not work so well." ></td>
	<td class="line x" title="193:217	You wanna have both options." ></td>
	<td class="line oc" title="194:217	5 Related Work Evidence from the surrounding context has been used previously to determine if the current sentence should be subjective/objective (Riloff et al., 2003; Pang and Lee, 2004)) and adjacency pair information has been used to predict congressional votes (Thomas et al., 2006)." ></td>
	<td class="line n" title="195:217	However, these methods do not explicitly model the relations between opinions." ></td>
	<td class="line x" title="196:217	Additionally, in our scheme opinions that are not in the immediate context may be allowed to influence the interpretation of a given opinion via target chains." ></td>
	<td class="line x" title="197:217	Polanyi and Zaenen (2006), in their discussion on contextual valence shifters, have also observed the phenomena described in this work namely that a central topic may be divided into subtopics in order to perform evaluations, and that discourse structure can influence the overall interpretation of valence." ></td>
	<td class="line x" title="198:217	Snyder and Barzilay (2007) combine an agreement model based on contrastive RST relations with a local aspect (or target) model to make a more informed overall decision for sentiment classification." ></td>
	<td class="line x" title="199:217	The contrastive cue indicates a change in the sentiment polarity." ></td>
	<td class="line x" title="200:217	In our scheme, their aspects would be related as same and their high contrast relations would result in frames such as SPSNsame, SNSPsame." ></td>
	<td class="line x" title="201:217	Additionally, our frame relations would link sentiments across non-adjacent clauses, and make connections via alt target relations." ></td>
	<td class="line x" title="202:217	135 Considering the discourse relation annotations in the PDTB (Prasad et al., 2006), there can be alignment between discourse relations (like contrast) and our opinion frames when the frames represent dominant relations between two clauses." ></td>
	<td class="line x" title="203:217	However, when the relation between opinions is not the most prominent one between two clauses, the discourse relation may not align with the opinion frames." ></td>
	<td class="line x" title="204:217	And when an opinion frame is between two opinions in the same clause, there would be no discourse relation counterpart at all." ></td>
	<td class="line x" title="205:217	Further, opinion frames assume particular intentions that are not necessary for the establishment of ostensibly similar discourse relations." ></td>
	<td class="line x" title="206:217	For example, we may not impose an opinion frame even if there are contrastive cues." ></td>
	<td class="line x" title="207:217	(Please refer to Appendix B for examples) With regard to meetings, the most closely related work includes the dialog-related annotation schemes for various available corpora of conversation (Dhillon et al.(2003) for ICSI MRDA; Carletta et al.(2005) for AMI ) As shown by Somasundaran et al.(2007), dialog structure information and opinions are in fact complementary." ></td>
	<td class="line x" title="211:217	We believe that, like discourse relations, dialog information will additionally help in arriving at an overall coherent interpretation." ></td>
	<td class="line x" title="212:217	6 Conclusion and Future work This is the first work that extends an opinion annotation scheme to relate opinions via target relations." ></td>
	<td class="line x" title="213:217	We first introduced the idea of opinion frames as a representation capturing discourse level relations that arise from related opinion targets and which are common in task-oriented dialogs such as our data." ></td>
	<td class="line x" title="214:217	We built an annotation scheme that would capture these relationships." ></td>
	<td class="line x" title="215:217	Finally, we performed extensive inter-annotator agreement studies in order to find the reliability of human judgment in recognizing frame components." ></td>
	<td class="line x" title="216:217	Our results and analysis provide insights into the complexities involved in recognizing discourse level relations between opinions." ></td>
	<td class="line x" title="217:217	Acknowledgments This research was supported in part by the Department of Homeland Security under grant N000140710152." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1017
Review Sentiment Scoring via a Parse-and-Paraphrase Paradigm
Liu, Jingjing;Seneff, Stephanie;"></td>
	<td class="line x" title="1:229	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 161169, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:229	c 2009 ACL and AFNLP Review Sentiment Scoring via a Parse-and-Paraphrase Paradigm   Jingjing Liu, Stephanie Seneff MIT Computer Science & Artificial Intelligence Laboratory 32 Vassar Street, Cambridge, MA 02139 {jingl, seneff}@csail.mit.edu    Abstract  This paper presents a parse-and-paraphrase paradigm to assess the degrees of sentiment for product reviews." ></td>
	<td class="line x" title="3:229	Sentiment identification has been well studied; however, most previous work provides binary polarities only (positive and negative), and the polarity of sentiment is simply reversed when a negation is detected." ></td>
	<td class="line x" title="4:229	The extraction of lexical features such as unigram/bigram also complicates the sentiment classification task, as linguistic structure such as implicit long-distance dependency is often disregarded." ></td>
	<td class="line x" title="5:229	In this paper, we propose an approach to extracting adverb-adjective-noun phrases based on clause structure obtained by parsing sentences into a hierarchical representation." ></td>
	<td class="line x" title="6:229	We also propose a robust general solution for modeling the contribution of adverbials and negation to the score for degree of sentiment." ></td>
	<td class="line x" title="7:229	In an application involving extracting aspect-based pros and cons from restaurant reviews, we obtained a 45% relative improvement in recall through the use of parsing methods, while also improving precision." ></td>
	<td class="line x" title="8:229	1 Introduction Online product reviews have provided an extensive collection of free-style texts as well as product ratings prepared by general users, which in return provide grassroots contributions to users interested in a particular product or service as assistance." ></td>
	<td class="line x" title="9:229	Yet, valuable as they are, free-style reviews contain much noisy data and are tedious to read through in order to reach an overall conclusion." ></td>
	<td class="line x" title="10:229	Thus, we conducted this study to automatically process and evaluate product reviews in order to generate both numerical evaluation and textual summaries of users opinions, with the ultimate goal of adding value to real systems such as a restaurant-guide dialogue system." ></td>
	<td class="line xc" title="11:229	Sentiment summarization has been well studied in the past decade (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004a, 2004b; Carenini et al., 2006; Liu et al., 2007)." ></td>
	<td class="line x" title="12:229	The polarity of users sentiments in each segment of review texts is extracted, and the polarities of individual sentiments are aggregated among all the sentences/segments of texts to give a numerical scaling on sentiment orientation." ></td>
	<td class="line x" title="13:229	Most of the work done for sentiment analysis so far has employed shallow parsing features such as part-of-speech tagging." ></td>
	<td class="line x" title="14:229	Frequent adjectives and nouns/noun phrases are extracted as opinion words and representative product features." ></td>
	<td class="line x" title="15:229	However, the linguistic structure of the sentence is usually not taken into consideration." ></td>
	<td class="line x" title="16:229	High level linguistic features, if well utilized and accurately extracted, can provide much insight into the semantic meaning of user opinions and contribute to the task of sentiment identification." ></td>
	<td class="line x" title="17:229	Furthermore, in addition to adjectives and nouns, adverbials and negation also play an important role in determining the degree of the orientation level." ></td>
	<td class="line x" title="18:229	For example, very good and good certainly express different degrees of positive sentiment." ></td>
	<td class="line x" title="19:229	Also, in previous studies, when negative expressions are identified, the polarity of sentiment in the associated segment of text is simply reversed." ></td>
	<td class="line x" title="20:229	However, semantic expressions are quite different from the absolute opposite values in mathematics." ></td>
	<td class="line x" title="21:229	For example, not bad does not express the opposite meaning of bad, which would be highly positive." ></td>
	<td class="line x" title="22:229	Simply reversing the polarity of sentiment on the appearance of negations may result in inaccurate interpretation of sentiment expressions." ></td>
	<td class="line x" title="23:229	Thus, a system which attempts to quantify sentiment while ignoring adverbials is missing a significant component of the sentiment score, especially if the adverbial is a negative word." ></td>
	<td class="line x" title="24:229	161 Another challenging aspect of negation is proper scoping of the negative reference over the right constituent, which we argue, can be handled quite well with careful linguistic analysis." ></td>
	<td class="line x" title="25:229	Take the sentence I dont think the place is very clean as example." ></td>
	<td class="line x" title="26:229	A linguistic approach associating long-distance elements with semantic relations can identify that the negation not scopes over the complement clause, thus extracting not very clean instead of very clean." ></td>
	<td class="line x" title="27:229	Our goal in modeling adverbials is to investigate whether a simple linear correction model can capture the polarity contribution of all adverbials." ></td>
	<td class="line x" title="28:229	Furthermore, is it also appropriate to adjust for multiple adverbs, including negation, via a linear additive model?" ></td>
	<td class="line x" title="29:229	I.e., can not very good be modeled as not(very(good))?" ></td>
	<td class="line x" title="30:229	The fact that not very good seems to be less negative than not good suggests that such an algorithm might work well." ></td>
	<td class="line x" title="31:229	From these derivations we have developed a model which treats negations in the exact same way as modifying adverbs, via an accumulative linear offset model." ></td>
	<td class="line x" title="32:229	This yields a very generic and straightforward solution to modeling the strength of sentiment expression." ></td>
	<td class="line x" title="33:229	In this paper we utilize a parse-and-paraphrase paradigm to identify semantically related phrases in review texts, taking quantifiers (e.g., modifying adverbs) and qualifiers (e.g., negations) into special consideration." ></td>
	<td class="line x" title="34:229	The approach makes use of a lexicalized probabilistic syntactic grammar to identify and extract sets of adverb-adjectivenoun phrases that match review-related patterns." ></td>
	<td class="line x" title="35:229	Such patterns are constructed based on wellformed linguistic structure; thus, relevant phrases can be extracted reliably." ></td>
	<td class="line x" title="36:229	We also propose a cumulative linear offset model to calculate the degree of sentiment for joint adjectives and quantifiers/qualifiers." ></td>
	<td class="line x" title="37:229	The proposed sentiment prediction model takes modifying adverbs and negations as universal scales on strength of sentiment, and conducts cumulative calculation on the degree of sentiment for the associated adjective." ></td>
	<td class="line oc" title="38:229	With this model, we can provide not only qualitative textual summarization such as good food and bad service, but also a numerical scoring of sentiment, i.e., how good the food is and how bad the service is. 2 Related Work There have been many studies on sentiment classification and opinion summarization (Pang and Lee, 2004, 2005; Gamon et al., 2005; Popescu and Etzioni, 2005; Liu et al., 2005; Zhuang et al., 2006; Kim and Hovy, 2006)." ></td>
	<td class="line x" title="39:229	Specifically, aspect rating as an interesting topic has also been widely studied (Titov and McDonald, 2008a; Snyder and Barzilay, 2007; Goldberg and Zhu, 2006)." ></td>
	<td class="line x" title="40:229	Recently, Baccianella et." ></td>
	<td class="line x" title="41:229	al." ></td>
	<td class="line x" title="42:229	(2009) conducted a study on multi-facet rating of product reviews with special emphasis on how to generate vectorial representations of the text by means of POS tagging, sentiment analysis, and feature selection for ordinal regression learning." ></td>
	<td class="line x" title="43:229	Titov and McDonald (2008b) proposed a joint model of text and aspect ratings which utilizes a modified LDA topic model to build topics that are representative of ratable aspects, and builds a set of sentiment predictors." ></td>
	<td class="line x" title="44:229	Branavan et al.(2008) proposed a method for leveraging unstructured annotations in product reviews to infer semantic document properties, by clustering user annotations into semantic properties and tying the induced clusters to hidden topics in the text." ></td>
	<td class="line x" title="46:229	3 System Overview Our review summarization task is to extract sets of descriptor-topic pairs (e.g., excellent service) from a set of reviews (e.g., for a particular restaurant), and to cluster the extracted phrases into representative aspects on a set of dimensions (e.g., food, service and atmosphere)." ></td>
	<td class="line x" title="47:229	Driven by this motivation, we propose a three-stage system that automatically processes reviews." ></td>
	<td class="line x" title="48:229	A block diagram is given in Figure 1." ></td>
	<td class="line x" title="49:229	Figure 1." ></td>
	<td class="line x" title="50:229	Framework of review processing." ></td>
	<td class="line x" title="51:229	The first stage is sentence-level data filtering." ></td>
	<td class="line x" title="52:229	Review data published by general users is often in free-style, and a large fraction of the data is either ill-formed or not relevant to the task." ></td>
	<td class="line x" title="53:229	We classify these as out of domain sentences." ></td>
	<td class="line x" title="54:229	To filter out such noisy data, we collect unigram statistics on all the relevant words in the corpus, and select high frequency adjectives and nouns." ></td>
	<td class="line x" title="55:229	Any sentence that contains none of the highfrequency nouns or adjectives is rejected from further analysis." ></td>
	<td class="line x" title="56:229	The remaining in-domain sentences are subjected to the second stage, parse 162 analysis and semantic understanding, for topic extraction." ></td>
	<td class="line x" title="57:229	From the parsable sentences we extract descriptor-topic phrase patterns based on a carefully-designed generation grammar." ></td>
	<td class="line x" title="58:229	We then apply LM (language model) based topic clustering to group the extracted phrases into representative aspects." ></td>
	<td class="line x" title="59:229	The third stage scores the degree of sentiment for adjectives, as well as the strength of sentiment for modifying adverbs and negations, which further refine the degree of sentiment of the associated adjectives." ></td>
	<td class="line x" title="60:229	We then run a linear additive model to assign a combined sentiment score for each extracted phrase." ></td>
	<td class="line x" title="61:229	The rest of the paper is structured as follows: In Section 4, we explain the linguistic analysis." ></td>
	<td class="line x" title="62:229	In Section 5, we describe the cumulative model for assessing the degree of sentiment." ></td>
	<td class="line x" title="63:229	Section 6 provides a systematic evaluation, conducted on real data in the restaurant review domain harvested from the Web." ></td>
	<td class="line x" title="64:229	Section 7 provides a discussion analyzing the results." ></td>
	<td class="line x" title="65:229	Section 8 summarizes the paper as well as pointing to future work." ></td>
	<td class="line x" title="66:229	4 Linguistic Analysis 4.1 Parse-and-Paraphrase Our linguistic analysis is based on a parse-andparaphrase paradigm." ></td>
	<td class="line x" title="67:229	Instead of the flat structure of a surface string, the parser provides a hierarchical representation, which we call a linguistic frame (Xu et al., 2008)." ></td>
	<td class="line x" title="68:229	It preserves linguistic structure by encoding different layers of semantic dependencies." ></td>
	<td class="line x" title="69:229	The grammar captures syntactic structure through a set of carefully constructed context free grammar rules, and employs a feature-passing mechanism to enforce long distance constraints." ></td>
	<td class="line x" title="70:229	The grammar is lexicalized, and uses a statistical model to rank order competing hypotheses." ></td>
	<td class="line x" title="71:229	It knows explicitly about 9,000 words, with all unknown words being interpreted as nouns." ></td>
	<td class="line x" title="72:229	The grammar probability model was trained automatically on the corpus of review sentences." ></td>
	<td class="line x" title="73:229	To produce the phrases, a set of generation rules is carefully constructed to only extract sets of related adverbs, adjectives and nouns." ></td>
	<td class="line x" title="74:229	The adjective-noun relationships are captured from the following linguistic patterns: (1) all adjectives attached directly to a noun in a noun phrase, (2) adjectives embedded in a relative clause modifying a noun, and (3) adjectives related to nouns in a subject-predicate relationship in a clause." ></td>
	<td class="line x" title="75:229	These patterns are compatible, i.e., if a clause contains both a modifying adjective and a predicate adjective related to the same noun, two adjective-noun pairs are generated by different patterns." ></td>
	<td class="line x" title="76:229	As in, The efficient waitress was nonetheless very courteous. It is a parse-andparaphrase-like paradigm: the paraphrase tries to preserve the original words intact, while reordering them and/or duplicating them into multiple NP units." ></td>
	<td class="line x" title="77:229	Since they are based on syntactic structure, the generation rules can also be applied in any other domain involving opinion mining." ></td>
	<td class="line x" title="78:229	An example linguistic frame is shown in Figure 2, which encodes the sentence The caesar with salmon or chicken is really quite good. In this example, for the adjective good, the nearby noun chicken would be associated with it if only proximity is considered." ></td>
	<td class="line x" title="79:229	From the linguistic frame, however, we can easily associate caesar with good by extracting the head of the topic sub-frame and the head of the predicate subframe, which are encoded in the same layer (root layer) of the linguistic frame." ></td>
	<td class="line x" title="80:229	Also, we can tell from the predicate sub-frame that there is an adverb quite modifying the head word good." ></td>
	<td class="line x" title="81:229	The linguistic frame also encodes an adverb really in the upstairs layer." ></td>
	<td class="line x" title="82:229	A well-constructed generation grammar can create customized adverb-adjective-noun phrases such as quite good caesar or really quite good caesar." ></td>
	<td class="line x" title="83:229	{c cstatement   :topic {q caesar              :quantifier 'def'              :pred {p with :topic {q salmon                                          :pred {p conjunction                                            :or {q chicken  }}}}   :adv 'really'    :pred {p adj_complement             :pred {p adjective                     :adv 'quite'                 :pred {p quality :topic 'good' }}}} Figure 2." ></td>
	<td class="line x" title="84:229	Linguistic frame for The caesar with salmon or chicken is really quite good. Interpreting negation in English is not straightforward, and it is often impossible to do correctly without a deep linguistic analysis." ></td>
	<td class="line x" title="85:229	Xuehui Wu (2005) wrote: The scope of negation is a complex linguistic phenomenon." ></td>
	<td class="line x" title="86:229	It is easy to perceive but hard to be defined from a syntactic point of view." ></td>
	<td class="line x" title="87:229	Misunderstanding or ambiguity may occur when the negative scope is not understood clearly and correctly. The majority rule for negation is that it scopes over the remainder of its containing clause, and this works well for most cases." ></td>
	<td class="line x" title="88:229	For example, Figure 3 shows 163 the linguistic frame for the sentence Their menu was a good one that didnt try to do too much. {c cstatement :topic {q menu   :poss 'their' } }    :complement {q pronoun   :name one              :adj_clause {c cstatement                            :conjn 'that'                            :negate 'not'                            :pred {p try :to_clause  {p do                                            :topic {q object                                            :adv 'too'                                            :quant 'much' }}}}    :pred {p adjective                 :pred {p quality :topic 'good' }}} Figure 3." ></td>
	<td class="line x" title="89:229	Linguistic frame for Their menu was a good one that didnt try to do too much. Traditional approaches which do not consider the linguistic structure would treat the appearance of not as a negation and simply reverse the sentiment of the sentence to negative polarity, which is wrong as the sentence actually expresses positive opinion for the topic menu." ></td>
	<td class="line x" title="90:229	In our approach, the negation not is identified as under the sub-frame of the complement clause, instead of in the same or higher layer of the adjective sub-frame; thus it is considered as unrelated to the adjective good." ></td>
	<td class="line x" title="91:229	In this way we can successfully predict the scope of the reference of the negation over the correct constituent of a sentence and create proper association between negation and its modified words." ></td>
	<td class="line x" title="92:229	4.2 LM-based Topic Clustering To categorize the extracted phrases into representative aspects, we automatically group the identified topics into a set of clusters based on LM probabilities." ></td>
	<td class="line x" title="93:229	The LM-based algorithm assumes that topics which are semantically related have high probability of co-occurring with similar descriptive words." ></td>
	<td class="line x" title="94:229	For example, delicious might co-occur frequently with both pizza and dessert." ></td>
	<td class="line x" title="95:229	By examining the distribution of bigram probability of these topics with corresponding descriptive words, we can group pizza and dessert into the same cluster of food." ></td>
	<td class="line x" title="96:229	We select a small set of the most common topics, i.e., topics with the highest frequency counts, and put them into an initial set I. Then, for each candidate topic g1872g1855 outside set I, we calculate its probability given each topic g1872g1861 within the initial set I, given by:        g1842g4666g1872g3030| g1872g3036g4667g3404 g1842g4666g1872g3030|g1853g4667g1842g4666g1853|g1872g3036g4667g3028g1488g3002                       g3404  g3017g4666g3028,g3047g3278g4667g3017g4666g3028g4667 g3017g4666g3028,g3047g3284g4667g3017g4666g3047 g3284g4667g3028g1488g3002                     g3404 g2869g3017g4666g3047 g3284g4667  g2869g3017g4666g3028g4667g1842g4666g1853,g1872g3030g4667g1842g4666g1853,g1872g3036g4667g3028g1488g3002        (1) where A represents the set of all the adjectives in the corpus." ></td>
	<td class="line x" title="97:229	For each candidate topic g1872g1855 , we choose the cluster of the initial topic g1872g1861  with which it has the highest probability score." ></td>
	<td class="line x" title="98:229	There are also cases where a meaningful adjective occurs in the absence of an associated topic, e.g., It is quite expensive. We call such cases the widow-adjective case." ></td>
	<td class="line x" title="99:229	Without hardcoded ontology matching, it is difficult to identify expensive as a price-related expression." ></td>
	<td class="line x" title="100:229	To discover such cases and associate them with related topics, we propose a surrogate topic matching approach based on bigram probability." ></td>
	<td class="line x" title="101:229	As aforementioned, the linguistic frame organizes all adjectives into separate clauses." ></td>
	<td class="line x" title="102:229	Thus, we create a surrogate topic category in the linguistic frames for widow-adjective cases, which makes it easy to detect descriptors that are affiliated with uninformative topics like the pronoun it." ></td>
	<td class="line x" title="103:229	We then have it generate phrases such as expensive surrogate_topic and use bigram probability statistics to automatically map each sufficiently strongly associated adjective to its most common topic among our major classes, e.g., mapping expensive with its surrogate topic price." ></td>
	<td class="line x" title="104:229	Therefore, we can generate sets of additional phrases in which the topic is hallucinated from the widow-adjective." ></td>
	<td class="line x" title="105:229	5 Assessment of Sentiment Strength 5.1 Problem Formulation Given the sets of adverb-adjective-noun phrases extracted by linguistic analysis, our goal is to assign a score for the degree of sentiment to each phrase and calculate an average rating for each aspect." ></td>
	<td class="line x" title="106:229	An example summary is given in Table 1." ></td>
	<td class="line x" title="107:229	Table 1." ></td>
	<td class="line x" title="108:229	Example of review summary." ></td>
	<td class="line x" title="109:229	Aspect Extracted phrases Rating Atmosphere very nice ambiance, outdoor patio 4.8 Food not bad meal, quite authentic food 4.1 Place not great place, very smoky restaurant 2.8 Price so high bill, high cost, not cheap price 2.2 To calculate the numerical degree of sentiment, there are three major problems to solve: 1) how to associate numerical scores with textual sentiment; 2) whether to calculate sentiment scores for adjectives and adverbs jointly or separately; 3) 164 whether to treat negations as special cases or in the same way as modifying adverbs." ></td>
	<td class="line x" title="110:229	There have been studies on building sentiment lexicons to define the strength of sentiment of words." ></td>
	<td class="line x" title="111:229	Esuli and Sebastiani (2006) constructed a lexical resource, SentiWordNet, a WordNet-like lexicon emphasizing sentiment orientation of words and providing numerical scores of how objective, positive and negative these words are." ></td>
	<td class="line x" title="112:229	However, lexicon-based methods can be tedious and inefficient and may not be accurate due to the complex cross-relations in dictionaries like WordNet." ></td>
	<td class="line x" title="113:229	Instead, our primary approach to sentiment scoring is to make use of collective data such as user ratings." ></td>
	<td class="line x" title="114:229	In product reviews collected from online forums, the format of a review entry often consists of three parts: pros/cons, free-style text and user rating." ></td>
	<td class="line x" title="115:229	We assume that user rating is normally consistent with the tone of the review text published by the same user." ></td>
	<td class="line x" title="116:229	By associating user ratings with each phrase extracted from review texts, we can easily associate numerical scores with textual sentiment." ></td>
	<td class="line x" title="117:229	A simple strategy of rating assignment is to take each extracted adverb-adjective pair as a composite unit." ></td>
	<td class="line x" title="118:229	However, this method is likely to lead to a large number of rare combinations, thus suffering from sparse data problems." ></td>
	<td class="line x" title="119:229	Therefore, an interesting question to ask is whether it is feasible to assign to each adverb a perturbation score, which adjusts the score of the associated adjective up or down by a fixed scalar value." ></td>
	<td class="line x" title="120:229	This approach thus hypothesizes that very expensive is as much worse than expensive as very romantic is better than romantic." ></td>
	<td class="line x" title="121:229	This allows us to pool all instances of a given adverb regardless of which adjective it is associated with, in order to compute the absolute value of the perturbation score for that adverb." ></td>
	<td class="line x" title="122:229	Therefore, we consider adverbs and adjectives separately when calculating the sentiment score, treating each modifying adverb as a universal quantifier which consistently scales up/down the strength of sentiment for the adjectives it modifies." ></td>
	<td class="line x" title="123:229	Furthermore, instead of treating negation as a special case, the universal model works for all adverbials." ></td>
	<td class="line x" title="124:229	The model hypothesizes that not bad is as much better than bad as not good is worse than good, i.e., negations push positive/negative adjectives to the other side of sentiment polarity by a universal scale." ></td>
	<td class="line x" title="125:229	This again, allows us to pool all instances of a given negation and compute the absolute value of the perturbation score for that negation, in the same way as dealing with modifying adverbs." ></td>
	<td class="line x" title="126:229	5.2 Linear Additive Model For each adjective, we average all its ratings given by: g1845g1855g1867g1870g1857g4666g1853g1856g1862g4667g3404   g3263g3289g3293 g3284   g3045g3284g3284g1488g3265   g3263g3289g3293 g3284 g3293g3284             (2) where g1842 represents the set of appearances of adjective g1853g1856g1862, g1870g3036 represents the associated user rating in each appearance of g1853g1856g1862, g1840 represents the number of entities (e.g., restaurants) in the entire data set, and g1866g3045g3284 represents the number of entities with rating g1870g3036." ></td>
	<td class="line x" title="127:229	The score is averaged over all the appearances, weighted by the frequency count of each category of rating to remove bias towards any category." ></td>
	<td class="line x" title="128:229	As for adverbs, using a slightly modified version of equation (2), we can get a rating table for all adverb-adjective pairs." ></td>
	<td class="line x" title="129:229	For each adverb adv, we get a list of all its possible combinations with adjectives." ></td>
	<td class="line x" title="130:229	Then, for each adj in the list, we calculate the distance between the rating of adv-adj and the rating of the adj alone." ></td>
	<td class="line x" title="131:229	We then aggregate the distances among all the pairs of adv-adj and adj in the list, weighted by the frequency count of each adv-adj pair: g1845g1855g1867g1870g1857g4666g1853g1856g1874g4667g3404  g3030g3042g3048g3041g3047g4666g3028g3031g3049,g3028g3031g3037g3295g4667 g3030g3042g3048g3041g3047g3435g3028g3031g3049,g3028g3031g3037 g3285g3439g3285g1488g3250g3047g1488g3002  g1842g1867g1864g4666g1853g1856g1862g3047g4667g4666g1870g4666g1853g1856g1874,g1853g1856g1862g3047g4667g3398g1870g4666g1853g1856g1862g3047g4667g4667          (3) where g1855g1867g1873g1866g1872g4666g1853g1856g1874, g1853g1856g1862g3047g4667 represents the count of the combination g1853g1856g1874g3398g1853g1856g1862g3047, g1827 represents the set of adjectives that co-occur with g1853g1856g1874 , g1870g4666g1853g1856g1874,g1853g1856g1862g3047g4667 represents the sentiment rating of the combination g1853g1856g1874g3398g1853g1856g1862g3047 , and g1870g4666g1853g1856g1862g3047g4667 represents the sentiment rating of the adjective g1853g1856g1862g3047 alone." ></td>
	<td class="line x" title="132:229	g1842g1867g1864g4666g1853g1856g1862g3047g4667 represents the polarity of g1853g1856g1862g3047, assigned as 1 if g1853g1856g1862g3047 is positive, and -1 if negative." ></td>
	<td class="line x" title="133:229	Specifically, negations are well handled by the same scoring strategy, treated exactly the same as modifying adverbs, except that they get such strong negative scores that the sentiment of the associated adjectives is pushed to the other side of the polarity scale." ></td>
	<td class="line x" title="134:229	After obtaining the strength rating for adverbs and the sentiment rating for adjectives, the next step is to assign the strength of sentiment to each phrase (negation-adverb-adjective-noun) extracted by linguistic analysis, as given by: g1845g1855g1867g1870g1857g4672g1866g1857g1859g3435g1853g1856g1874g4666g1853g1856g1862g4667g3439g4673g3404g1870g4666g1853g1856g1862g4667g3397 g1842g1867g1864g4666g1853g1856g1862g4667g1870g4666g1853g1856g1874g4667g3397g1842g1867g1864g4666g1853g1856g1862g4667g1870g4666g1866g1857g1859g4667       (4) 165 where g1870g4666g1853g1856g1862g4667 represents the rating of adjective g1853g1856g1862, g1870g4666g1853g1856g1874g4667 represents the rating of adverb g1853g1856g1874, and g1870g4666g1866g1857g1859g4667 represents the rating of negation g1866g1857g1859." ></td>
	<td class="line x" title="135:229	g1842g1867g1864g4666g1853g1856g1862g4667 represents the polarity of g1853g1856g1862, assigned as 1 if g1853g1856g1862 is positive, and -1 if negative." ></td>
	<td class="line x" title="136:229	Thus, if g1853g1856g1862 is positive, we assign a combined rating g1870g4666g1853g1856g1862g4667g3397g1870g4666g1853g1856g1874g4667 to this phrase." ></td>
	<td class="line x" title="137:229	If it is negative, we assign g1870g4666g1853g1856g1862g4667g3398g1870g4666g1853g1856g1874g4667." ></td>
	<td class="line x" title="138:229	Specifically, if it is a negation case, we further assign a linear offset g1870g4666g1866g1857g1859g4667 if g1853g1856g1862 is positive or g3398g1870g4666g1866g1857g1859g4667 if g1853g1856g1862 is negative." ></td>
	<td class="line x" title="139:229	For example, given the ratings <good: 4.5>, <bad: 1.5>, <very: 0.5> and <not: -3.0>, we would assign 5.0 to very good (score(very(good))=4.5+0.5), 1.0 to very bad (score(very(bad))=1.5-0.5), and 2.0 to not very good (score(not(very(good)))= 4.5+0.53.0)." ></td>
	<td class="line x" title="140:229	The corresponding sequence of different degrees of sentiment is: very good: 5.0 > good: 4.5 > not very good: 2.0 > bad: 1.5 > very bad: 1.0." ></td>
	<td class="line x" title="141:229	6 Experiments In this section we present a systematic evaluation of the proposed approaches conducted on real data." ></td>
	<td class="line x" title="142:229	We crawled a data collection of 137,569 reviews on 24,043 restaurants in 9 cities in the U.S. from an online restaurant evaluation website1." ></td>
	<td class="line x" title="143:229	Most of the reviews have both pros/cons and free-style text." ></td>
	<td class="line x" title="144:229	For the purpose of evaluation, we take those reviews containing pros/cons as the experimental set, which is 72.7% (99,147 reviews) of the original set." ></td>
	<td class="line x" title="145:229	6.1 Topic Extraction Based on the experimental set, we first filtered out-of-domain sentences based on frequency count, leaving a set of 857,466 in-domain sentences (67.5%)." ></td>
	<td class="line x" title="146:229	This set was then subjected to parse analysis; 78.6% of them are parsable." ></td>
	<td class="line x" title="147:229	Given the parsing results in the format of linguistic frame, we used a set of language generation rules to extract relevant adverb-adjectivenoun phrases." ></td>
	<td class="line x" title="148:229	We then selected the most frequent 6 topics that represented appropriate dimensions for the restaurant domain (place, food, service, price, atmosphere and portion) as the initial set, and clustered the extracted topic mentions into different aspect categories by creating a set of topic mappings with the LMbased clustering method." ></td>
	<td class="line x" title="149:229	Phrases not belonging to any category are filtered out." ></td>
	<td class="line x" title="150:229	1  http://www.citysearch.com To evaluate the performance of the proposed approach (LING) to topic extraction, we compare it with a baseline method similar to (Hu and Liu, 2004a, 2004b; Liu et al., 2005)." ></td>
	<td class="line x" title="151:229	We performed part-of-speech tagging on both parsable and unparsable sentences, extracted each pair of noun and adjective that has the smallest proximity, and filtered out those with low frequency counts." ></td>
	<td class="line x" title="152:229	Adverbs and negation words that are adjacent to the identified adjectives were also extracted along with the adjective-noun pairs." ></td>
	<td class="line x" title="153:229	We call this the neighbor baseline (NB)." ></td>
	<td class="line x" title="154:229	The proposed method is unable to make use of the non-parsable sentences, which make up over 20% of the data." ></td>
	<td class="line x" title="155:229	Hence, it seems plausible to utilize a back-off mechanism for these sentences via a combined system (COMB) incorporating NB only for the sentences that fail to parse." ></td>
	<td class="line x" title="156:229	In considering how to construct the ground truth set of pros and cons for particular aspects, our goal was to minimize error as much as possible without requiring exorbitant amounts of manual labeling." ></td>
	<td class="line x" title="157:229	We also wanted to assure that the methods were equally fair to both systems (LING and NB)." ></td>
	<td class="line x" title="158:229	To these ends, we decided to pool together all of the topic mappings and surrogate topic hallucinations obtained automatically from both systems, and then to manually edit the resulting list to eliminate any that were deemed unreasonable." ></td>
	<td class="line x" title="159:229	We then applied these edited mappings in an automatic procedure to the adjective-noun pairs in the user-provided pros and cons of all the restaurant reviews." ></td>
	<td class="line x" title="160:229	The resulting aspect-categorized phrase lists are taken as the ground truth." ></td>
	<td class="line x" title="161:229	Each system then used its own (unedited) set of mappings in processing the associated review texts." ></td>
	<td class="line x" title="162:229	We also needed an algorithm to decide on a particular set of reviews for consideration, again, with the goal of omitting bias towards either of the two systems." ></td>
	<td class="line x" title="163:229	We decided to retain as the evaluation set all reviews which obtained at least one topic extraction from both systems." ></td>
	<td class="line x" title="164:229	Thus the two systems processed exactly the same data with exactly the same definitions of ground truth." ></td>
	<td class="line x" title="165:229	Performance was evaluated on this set of 62,588 reviews in terms of recall (percentage of topics in the ground truth that are also identified from the review body) and precision (percentage of extracted topics that are also in the ground truth)." ></td>
	<td class="line x" title="166:229	These measures are computed separately for each review, and then averaged over all reviews." ></td>
	<td class="line x" title="167:229	As shown in Table 2, without clustering, the LING approach gets 4.6% higher recall than the 166 NB baseline." ></td>
	<td class="line x" title="168:229	And the recall from the COMB approach is 3.9% higher than that from the LING approach and 8.5% higher than that from the NB baseline." ></td>
	<td class="line x" title="169:229	With topic clustering, the COMB approach also gets the highest recall, with a 4.9% and 17.5% increase from the LING approach and the NB baseline respectively." ></td>
	<td class="line x" title="170:229	The precision is quite close among the different approaches, around 60%." ></td>
	<td class="line x" title="171:229	Table 2 also shows that the topic clustering approach increases the recall by 4.8% for the NB baseline, 12.8% for the LING approach, and 13.8% for the COMB approach." ></td>
	<td class="line x" title="172:229	Table 2." ></td>
	<td class="line x" title="173:229	Experimental results of topic extraction by the NB baseline, the proposed LING approach and a combined system (COMB)." ></td>
	<td class="line x" title="174:229	No Clustering NB LING COMB Recall 39.6% 44.2% 48.1% Precision 60.2% 60.0% 59.8%  With Clustering NB LING COMB Recall 44.4% 57.0% 61.9% Precision 56.8% 61.1% 60.8% 6.2 Sentiment Scoring To score the degree of sentiment for each extracted phrase, we built a table of sentiment score (<adjective: score>) for adjectives and a table of strength score (<adverb: score>) for adverbs." ></td>
	<td class="line x" title="175:229	The pros/cons often contain short and wellstructured phrases, and have better parsing quality than the long and complex sentences in freestyle texts; pros/cons also have clear sentiment orientations." ></td>
	<td class="line x" title="176:229	Thus, we use pros/cons to score the sentiment of adjectives, which requires strong polarity association." ></td>
	<td class="line x" title="177:229	To obtain reliable ratings, we associate the adjectives in the pros of review entries which have a user rating 4 or 5, and associate the adjectives in the cons of review entries with user ratings 1 or 2 (the scale of user rating is 1 to 5)." ></td>
	<td class="line x" title="178:229	Reviews with rating 3 are on the boundary of sentiment, so we associate both sides with the overall rating." ></td>
	<td class="line x" title="179:229	On the other hand, the frequencies of adverbs in free-style texts are much higher than those in pros/cons, as pros/cons mostly contain adjective-noun patterns." ></td>
	<td class="line x" title="180:229	Thus, we use free-style texts instead of pros/cons to score the strength of adverbs." ></td>
	<td class="line x" title="181:229	Partial results of the sentiment scoring are shown in Tables 3 and 4." ></td>
	<td class="line x" title="182:229	As shown in Table 3, the polarity of sentiment as well as the degree of polarity of an adjective can be distinguished by its score." ></td>
	<td class="line x" title="183:229	The higher the sentiment score is, the more positive the adjective is. Table 3." ></td>
	<td class="line x" title="184:229	Sentiment scoring for selected adjectives." ></td>
	<td class="line x" title="185:229	Adjective Rating Adjective Rating Excellent  5.0 Awesome  4.8 Easy  4.1 Great  4.4 Good  3.9 Limited  3.4 Inattentive  2.75 Overpriced  2.3 Rude  1.69 Horrible  1.3 Table 4 gives the scores of strength for most common adverbs." ></td>
	<td class="line x" title="186:229	The higher the strength score is, the more the adverb scales up/down the degree of sentiment of the adjective it modifies." ></td>
	<td class="line x" title="187:229	While not gets a strong negative score, some adverbs such as a little (-0.65) and a bit (0.83) also get negative scores, indicating slightly less sentiment for the associated adjectives." ></td>
	<td class="line x" title="188:229	Table 4." ></td>
	<td class="line x" title="189:229	Strength scoring for selected adverbs." ></td>
	<td class="line x" title="190:229	Adverb Rating Adverb Rating Super  0.58 Fairly  0.13 Extremely  0.54 Pretty 0.07 Incredibly  0.49 A little  -0.65 Very 0.44 A bit -0.83 Really  0.39 Not -3.10 To evaluate the performance of sentiment scoring, we randomly selected a subset of 1,000 adjective-noun phrases and asked two annotators to independently rate the sentiment of each phrase on a scale of 1 to 5." ></td>
	<td class="line x" title="191:229	We compared the sentiment scoring between our system and the annotations in a measurement of mean distance: g1856g1861g1871g1872g1853g1866g1855g1857 g3404 g2869|g3020| |g1870g3036g3043g3043g3106g3020 g3398g1870g3028g3043|      (5) where g1845  represents the set of phrases, g1868 represents each phrase in the set g1845, g1870g3036g3043 represents the rating on phrase g1868 from our sentiment scoring system, and g1870g3028g3043 represents the annotated rating on phrase g1868." ></td>
	<td class="line x" title="192:229	As shown in Table 5, the obtained mean distance between the scoring from our approach and that from each annotation set is 0.46 and 0.43 respectively, based on the absolute rating scale from 1 to 5." ></td>
	<td class="line x" title="193:229	This shows that the scoring of sentiment from our system is quite close to human annotation." ></td>
	<td class="line x" title="194:229	The kappa agreement between the two annotation sets is 0.68, indicating high consistency between the annotators." ></td>
	<td class="line x" title="195:229	The reliability of these results gives us sufficient confidence to make use of the scores of sentiments for summarization." ></td>
	<td class="line x" title="196:229	To examine the prediction of sentiment polarity, for each annotation set, we pooled the phrases with rating 4/5 into positive, rating 1/2 into negative, and rating 3 into neutral." ></td>
	<td class="line x" title="197:229	Then we rounded up the sentiment scores from our system to integers and pooled the scores into three polar167 ity sets (positive, negative and neutral) in the same way." ></td>
	<td class="line x" title="198:229	As shown in Table 5, the obtained kappa agreement between the result from our system and that from each annotation set is 0.55 and 0.60 respectively." ></td>
	<td class="line x" title="199:229	This shows reasonably high agreement on the polarity of sentiment between our system and human evaluation." ></td>
	<td class="line x" title="200:229	Table 5." ></td>
	<td class="line x" title="201:229	Comparison of sentiment scoring between the proposed approach and two annotation sets." ></td>
	<td class="line x" title="202:229	Annotation 1 Annotation 2 Mean distance 0.46 0.43 Kappa agreement 0.55 0.60 Table 6." ></td>
	<td class="line x" title="203:229	Experimental results of topic extraction based on sentiment polarity matching." ></td>
	<td class="line x" title="204:229	No Clustering NB LING COMB Recall 34.5% 38.9% 42.2% Precision 53.8% 54.0% 53.3%  With Clustering NB LING COMB Recall 37.4% 49.7% 54.1% Precision 48.5% 52.9% 51.4% To evaluate the combination of topic extraction and sentiment identification, we repeated the topic extraction experiments presented in Table 2, but this time requiring as well a correct polarity assignment to obtain a match with the pros/cons ground truth." ></td>
	<td class="line x" title="205:229	As shown in Table 6, the COMB approach gets the highest recall both with and without topic clustering, and the recall from the LING approach is higher than that from the NB baseline in both cases as well, indicating the superiority of the proposed approach." ></td>
	<td class="line x" title="206:229	The precision is stable among the different approaches, consistent with the case without the consideration of sentiment polarity." ></td>
	<td class="line x" title="207:229	7 Discussion It is surprising that the parse-and-paraphrase method performs so well, despite the fact that it utilizes less than 80% of the data (parsable set)." ></td>
	<td class="line x" title="208:229	In this section, we will discuss two experiments that were done to tease apart the contributions of different variables." ></td>
	<td class="line x" title="209:229	In both experiments, we compared the change in relative improvement in recall between NB and LING, relative to the values in Table 6, in the with-clustering condition." ></td>
	<td class="line x" title="210:229	In the table, LING obtains a score of 49.7% for recall, which is a 33% relative increase from the score for NB (37.4%)." ></td>
	<td class="line x" title="211:229	Three distinct factors could play a role in the improvement: the widowadjective topic hallucinations, the topic mapping for clustering, and the extracted phrases themselves." ></td>
	<td class="line x" title="212:229	An experiment involving omitting topic hallucinations from widow adjectives determined that these account for 12% of the relative increase." ></td>
	<td class="line x" title="213:229	To evaluate the contribution of clustering, we replaced the mapping tables used by both systems with the edited one used by the ground truth computation." ></td>
	<td class="line x" title="214:229	Thus, both systems made use of the same mapping table, removing this variable from consideration." ></td>
	<td class="line x" title="215:229	This improved the performance of both systems (NB and LING), but resulted in a decrease of LINGs relative improvement by 17%." ></td>
	<td class="line x" title="216:229	This implies that LINGs mapping table is superior." ></td>
	<td class="line x" title="217:229	Since both systems use the same sentiment scores for adjectives and adverbs, the remainder of the difference (71%) must be due simply to higher quality extracted phrases." ></td>
	<td class="line x" title="218:229	We suspected that over-generated phrases (the 40% of phrases that find no mappings in the pros/cons) might not really be a problem." ></td>
	<td class="line x" title="219:229	To test this hypothesis, we selected 100 reviews for their high density of extracted phrases, and manually evaluated all the over-generated phrases." ></td>
	<td class="line x" title="220:229	We found that over 80% were well formed, correct, and informative." ></td>
	<td class="line x" title="221:229	Therefore, a lower precision here does not necessarily mean poor performance, but instead shows that the pros/cons provided by users are often incomplete." ></td>
	<td class="line x" title="222:229	By extracting summaries from review texts we can recover additional valuable information." ></td>
	<td class="line x" title="223:229	8 Conclusions & Future Work This paper presents a parse-and-paraphrase approach to assessing the degree of sentiment for product reviews." ></td>
	<td class="line x" title="224:229	A general purpose context free grammar is employed to parse review sentences, and semantic understanding methods are developed to extract representative negation-adverbadjective-noun phrases based on well-defined semantic rules." ></td>
	<td class="line x" title="225:229	A language modeling-based method is proposed to cluster topics into respective categories." ></td>
	<td class="line x" title="226:229	We also introduced in this paper a cumulative linear offset model for supporting the assessment of the strength of sentiment in adjectives and quantifiers/qualifiers (including negations) on a numerical scale." ></td>
	<td class="line x" title="227:229	We demonstrated that the parse-and-paraphrase method can perform substantially better than a neighbor baseline on topic extraction from reviews even with less data." ></td>
	<td class="line x" title="228:229	The future work focuses in two directions: (1) building a relational database from the summaries and ratings and using it to enhance users experiences in a multimodal spoken dialogue system; and (2) applying our techniques to other domains to demonstrate generality." ></td>
	<td class="line x" title="229:229	168" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1018
Supervised and Unsupervised Methods in Employing Discourse Relations for Improving Opinion Polarity Classification
Somasundaran, Swapna;Namata, Galileo;Wiebe, Janyce M.;Getoor, Lise;"></td>
	<td class="line x" title="1:233	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 170179, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:233	c 2009 ACL and AFNLP Supervised and Unsupervised Methods in Employing Discourse Relations for Improving Opinion Polarity Classification Swapna Somasundaran Univ. of Pittsburgh Pittsburgh, PA 15260 swapna@cs.pitt.edu Galileo Namata Univ. of Maryland College Park, MD 20742 namatag@cs.umd.edu Janyce Wiebe Univ. of Pittsburgh Pittsburgh, PA 15260 wiebe@cs.pitt.edu Lise Getoor Univ. of Maryland College Park, MD 20742 getoor@cs.umd.edu Abstract This work investigates design choices in modeling a discourse scheme for improving opinion polarity classification." ></td>
	<td class="line x" title="3:233	For this, two diverse global inference paradigms are used: a supervised collective classification framework and an unsupervised optimization framework." ></td>
	<td class="line x" title="4:233	Both approaches perform substantially better than baseline approaches, establishing the efficacy of the methods and the underlying discourse scheme." ></td>
	<td class="line x" title="5:233	We also present quantitative and qualitative analyses showing how the improvements are achieved." ></td>
	<td class="line x" title="6:233	1 Introduction The importance of discourse in opinion analysis is being increasingly recognized (Polanyi and Zaenen, 2006)." ></td>
	<td class="line x" title="7:233	Motivated by the need to enable discourse-based opinion analysis, previous research (Asher et al., 2008; Somasundaran et al., 2008) developed discourse schemes and created manually annotated corpora." ></td>
	<td class="line x" title="8:233	However, it was not known whether and how well these linguistic ideas and schemes can be translated into effective computational implementations." ></td>
	<td class="line x" title="9:233	In this paper, we first investigate ways in which an opinion discourse scheme can be computationally modeled, and then how it can be utilized to improve polarity classification." ></td>
	<td class="line x" title="10:233	Specifically, the discourse scheme we use is from Somasundaran et al.(2008), which was developed to support a global, interdependent polarity interpretation." ></td>
	<td class="line x" title="12:233	To achieve discourse-based global inference, we explore two different frameworks." ></td>
	<td class="line x" title="13:233	The first is a supervised framework that learns interdependent opinion interpretations from training data." ></td>
	<td class="line x" title="14:233	The second is an unsupervised optimization framework which uses constraints to express the ideas of coherent opinion interpretation embodied in the scheme." ></td>
	<td class="line x" title="15:233	For the supervised framework, we use Iterative Collective Classification (ICA), which facilitates machine learning using relational information." ></td>
	<td class="line x" title="16:233	The unsupervised optimization is implemented as an Integer Linear Programming (ILP) problem." ></td>
	<td class="line x" title="17:233	Via our implementations, we aim to empirically test if discourse-based approaches to opinion analysis are useful." ></td>
	<td class="line x" title="18:233	Our results show that both of our implementations achieve significantly better accuracies in polarity classification than classifiers using local information alone." ></td>
	<td class="line x" title="19:233	This confirms the hypothesis that the discourse-based scheme is useful, and also shows that both of our design choices are effective." ></td>
	<td class="line x" title="20:233	We also find that there is a difference in the way ICA and ILP achieve improvements, and a simple hybrid approach, which incorporates the strengths of both, is able to achieve significant overall improvements over both." ></td>
	<td class="line x" title="21:233	Our analyses show that even when our discourse-based methods bootstrap from noisy classifications, they can achieve good improvements." ></td>
	<td class="line x" title="22:233	The rest of this paper is organized as follows: we discuss related work in Section 2 and the discourse scheme in Section 3." ></td>
	<td class="line x" title="23:233	We present our discourse-based implementations in Section 4, experiments in Section 5, discussions in Section 6 and conclusions in Section 7." ></td>
	<td class="line x" title="24:233	2 Related Work Previous work on polarity disambiguation has used contextual clues and reversal words (Wilson et al., 2005; Kennedy and Inkpen, 2006; Kanayama and Nasukawa, 2006; Devitt and Ahmad, 2007; Sadamitsu et al., 2008)." ></td>
	<td class="line x" title="25:233	However, these do not capture discourse-level relations." ></td>
	<td class="line x" title="26:233	Researchers, such as (Polanyi and Zaenen, 2006), have discussed how the discourse structure can influence opinion interpretation; and previous work, such as (Asher et al., 2008; Somasundaran et al., 2008), have developed annota170 tion schemes for interpreting opinions with discourse relations." ></td>
	<td class="line x" title="27:233	However, they do not empirically demonstrate how automatic methods can use their ideas to improve polarity classification." ></td>
	<td class="line x" title="28:233	In this work, we demonstrate concrete ways in which a discourse-based scheme can be modeled using global inference paradigms." ></td>
	<td class="line x" title="29:233	Joint models have been previously explored for other NLP problems (Haghighi et al., 2005; Moschitti et al., 2006; Moschitti, 2009)." ></td>
	<td class="line x" title="30:233	Our global inference model focuses on opinion polarity recognition task." ></td>
	<td class="line x" title="31:233	The biggest difference between this work and previous work in opinion analysis that use global inference methods is in the type of linguistic relations used to achieve the global inference." ></td>
	<td class="line x" title="32:233	Some of the work is not related to discourse at all (e.g., lexical similarities (Takamura et al., 2007), morphosyntactic similarities (Popescu and Etzioni, 2005) and word-based measures like TFIDF (Goldberg and Zhu, 2006))." ></td>
	<td class="line oc" title="33:233	Others use sentence cohesion (Pang and Lee, 2004), agreement/disagreement between speakers (Thomas et al., 2006; Bansal et al., 2008), or structural adjacency." ></td>
	<td class="line x" title="34:233	In contrast, our work focuses on discoursebased relations for global inference." ></td>
	<td class="line x" title="35:233	Another difference from the above work is that our work is over multi-party conversations." ></td>
	<td class="line x" title="36:233	Previous work on emotion and subjectivity detection in multi-party conversations has explored using prosodic information (Neiberg et al., 2006), combining linguistic and acoustic information (Raaijmakers et al., 2008) and combining lexical and dialog information (Somasundaran et al., 2007)." ></td>
	<td class="line x" title="37:233	Our work is focused on harnessing discourse-based knowledge and on interdependent inference." ></td>
	<td class="line x" title="38:233	There are several collective classification frameworks, including (Neville and Jensen, 2000; Lu and Getoor, 2003; Taskar et al., 2004; Richardson and Domingos, 2006; Bilgic et al., 2007)." ></td>
	<td class="line x" title="39:233	In this paper, we use an approach by (Lu and Getoor, 2003) which iteratively predicts class values using local and relational features." ></td>
	<td class="line x" title="40:233	ILP has been used on other NLP tasks, e.g., (Denis and Baldridge, 2007; Choi et al., 2006; Roth and Yih, 2004)." ></td>
	<td class="line x" title="41:233	In this work, we employ ILP for modeling discourse constraints for polarity classification." ></td>
	<td class="line x" title="42:233	3 Discourse Scheme and Data The scheme in Somasundaran et al.(2008) has been developed and annotated over the AMI meeting corpus (Carletta et al., 2005).1 This scheme annotates opinions, their polarities (positive, negative, neutral) and their targets (a target is what the opinion is about)." ></td>
	<td class="line x" title="44:233	The targets of opinions are related via two types of relations: the same relation, which relates targets referring to the same entity or proposition, and the alternative relation, which relates targets referring to mutually exclusive options in the context of the discourse." ></td>
	<td class="line x" title="45:233	Additionally, the scheme relates opinions via two types of frame relations: the reinforcing and nonreinforcing relations." ></td>
	<td class="line x" title="46:233	The frame relations represent discourse scenarios: reinforcing relations exist between opinions when they contribute to the same overall stance, while non-reinforcing relations exist between opinions that show ambivalence." ></td>
	<td class="line x" title="47:233	The opinion annotations are text-span based, while in this work, we use Dialog Act (DA) based segmentation of meetings.2 As the DAs are our units of classification, we map opinion annotations to the DA units as follows." ></td>
	<td class="line x" title="48:233	If a DA unit contains an opinion annotation, the label is transferred upwards to the containing DA." ></td>
	<td class="line x" title="49:233	When a DA contains multiple opinion annotations, each with a different polarity, one of them is randomly chosen as the label for the DA." ></td>
	<td class="line x" title="50:233	The discourse relations existing between opinions are also transferred upwards, between the DAs containing each of these annotations." ></td>
	<td class="line x" title="51:233	We recreate an example from Somasundaran et al.(2008) using DA segmentation in Example 1." ></td>
	<td class="line x" title="53:233	Here, the speaker has a positive opinion towards the rubbery material for the TV remote." ></td>
	<td class="line x" title="54:233	(1) DA-1:  this kind of rubbery material, DA-2: its a bit more bouncy, DA-3: like you said they get chucked around a lot." ></td>
	<td class="line x" title="55:233	DA-4: A bit more durable and that can also be ergonomic and DA-5: it kind of feels a bit different from all the other remote controls." ></td>
	<td class="line x" title="56:233	In the example, the individual opinion expressions (shown in bold) are essentially regarding the same thing  the rubbery material." ></td>
	<td class="line x" title="57:233	Thus, the explicit targets (shown in italics), its, that, and it, and the implicit target of a bit more durable are all linked 1The AMI corpus contains a set of scenario-based meetings where participants have to design a new TV remote prototype." ></td>
	<td class="line x" title="58:233	2DA segmentation is provided with the AMI corpus." ></td>
	<td class="line x" title="59:233	171 Figure 1: Discourse Relations between DA segments for Example 1." ></td>
	<td class="line x" title="60:233	with same target relations." ></td>
	<td class="line x" title="61:233	Also, notice that the opinions reinforce a particular stance, i.e., a prorubbery-material stance." ></td>
	<td class="line x" title="62:233	Thus, the scheme links the opinions via reinforcing relations." ></td>
	<td class="line x" title="63:233	Figure 1 illustrates the corresponding discourse relations between the containing DA units." ></td>
	<td class="line x" title="64:233	4 Implementing the Discourse Model The hypothesis in using discourse information for polarity classification is that the global discourse view will improve upon a classification with only a local view." ></td>
	<td class="line x" title="65:233	Thus, we implement a local classifier to bootstrap the classification process, and then implement classifiers that use discourse information from the scheme annotations, over it." ></td>
	<td class="line x" title="66:233	We explore two approaches for implementing our discourse-based classifier." ></td>
	<td class="line x" title="67:233	The first is ICA, where discourse relations and the neighborhood information brought in by these relations are incorporated as features into the learner." ></td>
	<td class="line x" title="68:233	The second approach is ILP optimization, which tries to maximize the class distributions predicted by the local classifier, subject to constraints imposed by discourse relations." ></td>
	<td class="line x" title="69:233	Both classifiers thus accommodate preferences of the local classifier and for coherence with discourse neighbors." ></td>
	<td class="line x" title="70:233	4.1 Local Classifier A supervised local classifier, Local, is used to provide the classifications to bootstrap the discoursebased classifiers.3 It is important to make Local as reliable as possible; otherwise, the discourse relations will propagate misclassifications." ></td>
	<td class="line x" title="71:233	Thus, we build Local using a variety of knowledge sources that have been shown to be useful for opinion analysis in previous work." ></td>
	<td class="line x" title="72:233	Specifically, we construct features using polarity lexicons (used by (Wilson et al., 2005)), DA tags (used by (Somasundaran 3Local is supervised, as previous work has shown that supervised methods are effective in opinion analysis." ></td>
	<td class="line x" title="73:233	Even though this makes the final end-to-end system with the ILP implementation semi-supervised, note that the discoursebased ILP part is itself unsupervised." ></td>
	<td class="line oc" title="74:233	et al., 2007)) and unigrams (used by many researchers, e.g., (Pang and Lee, 2004))." ></td>
	<td class="line x" title="75:233	Note that, as our discourse-based classifiers attempt to improve upon the local classifications, Local is also a baseline for our experiments." ></td>
	<td class="line x" title="76:233	4.2 Iterative Collective Classification We use a variant of ICA (Lu and Getoor, 2003; Neville and Jensen, 2000), which is a collective classification algorithm shown to perform consistently well over a wide variety of relational data." ></td>
	<td class="line x" title="77:233	Algorithm 1 ICA Algorithm for each instance i do{bootstrapping} Compute polarity for i using local attributes end for repeat{iterative} Generate ordering I over all instances for each i in I do Compute polarity for i using local and relational attributes end for until Stopping criterion is met ICA uses two classifiers: a local classifier and a relational classifier." ></td>
	<td class="line x" title="78:233	The local classifier is trained to predict the DA labels using only the local features." ></td>
	<td class="line x" title="79:233	We use Local, described in Section 4.1, for this purpose." ></td>
	<td class="line x" title="80:233	The relational classifier is trained using the local features, and an additional set of features commonly referred to as relational features." ></td>
	<td class="line x" title="81:233	The value of a relational feature, for a given DA, depends on the polarity of the discourse neighbors of that DA." ></td>
	<td class="line x" title="82:233	Thus, the relational features incorporate discourse and neighbor information; that is, they incorporate the information about the frame and target relations in conjunction with the polarity of the discourse neighbors." ></td>
	<td class="line x" title="83:233	Intuitively, our motivation for this approach can be explained using Example 1." ></td>
	<td class="line x" title="84:233	Here, in interpreting the ambiguous opinion a bit different as being positive, we use the knowledge that it participates in a reinforcing discourse, and that all its neighbors (e.g., ergonomic, durable) are positive opinions regarding the same thing." ></td>
	<td class="line x" title="85:233	On the other hand, if it had been a non-reinforcing discourse, then the polarity of a bit different, when viewed with respect to the other opinions, could have been interpreted as negative." ></td>
	<td class="line x" title="86:233	Table 1 lists the relational features we defined for our experiments where each row represents a 172 Percent of neighbors with polarity type a related via frame relation fprime Percent of neighbors with polarity type a related via target relation tprime Percent of neighbors with polarity type a related via frame relation f and target relation t Percent of neighbors with polarity type a and same speaker related via frame relation fprime Percent of neighbors with polarity type a and same speaker related via target relation tprime Percent of neighbors with polarity type a related via a frame relation or target relation Percent of neighbors with polarity type a related via a reinforcing frame relation or same target relation Percent of neighbors with polarity type a related via a non-reinforcing frame relation or alt target relation Most common polarity type of neighbors related via a same target relation Most common polarity type of neighbors related via a reinforcing frame relation and same target relation Table 1: Relational features: a  {non-neutral (i.e., positive or negative), positive, negative}, t  {same, alt}, f  {reinforcing, non-reinforcing}, tprime  {same or alt, same, alt}, fprime  {reinforcing or non-reinforcing, reinforcing, nonreinforcing} set of features." ></td>
	<td class="line x" title="87:233	Features are generated for all combinations of a, t, tprime, f and fprime for each row." ></td>
	<td class="line x" title="88:233	For example, one of the features in the first row is Percent of neighbors with polarity type positive, that are related via a reinforcing frame relation." ></td>
	<td class="line x" title="89:233	Thus, each feature for the relational classifier identifies neighbors for a given instance via a specific relation (f, t, fprime or tprime, obtained from the scheme annotations) and factors in their polarity values (a, obtained from the classifier predictions from the previous round)." ></td>
	<td class="line x" title="90:233	This adds a total of 59 relational features to the already existing local features." ></td>
	<td class="line x" title="91:233	ICA has two main phases: the bootstrapping and iterative phases." ></td>
	<td class="line x" title="92:233	In the bootstrapping phase, the polarity of each instance is initialized to the most likely value given only the local classifier and its features." ></td>
	<td class="line x" title="93:233	In the iterative phase, we create a random ordering of all the instances and, in turn, apply the relational classifier to each instance where the relational features, for a given instance, are computed using the most recent polarity assignments of its neighbors." ></td>
	<td class="line x" title="94:233	We repeat this until some stopping criterion is met." ></td>
	<td class="line x" title="95:233	For our experiments, we use a fixed number of 30 iterations, which has been found to be sufficient in most data sets for ICA to converge to a solution." ></td>
	<td class="line x" title="96:233	The pseudocode for the algorithm is shown in Algorithm 1." ></td>
	<td class="line x" title="97:233	4.3 Integer Linear Programming First, we explain the intuition behind viewing discourse relations as enforcing constraints on polarity interpretation." ></td>
	<td class="line x" title="98:233	Then, we explain how the constraints are encoded in the optimization problem." ></td>
	<td class="line x" title="99:233	4.3.1 Discourse Constraints on Polarity The discourse relations between opinions can provide coherence constraints on the way their polarity is interpreted." ></td>
	<td class="line x" title="100:233	Consider a discourse scenario in which a speaker expresses multiple opinions regarding the same thing, and is reinforcing his stance in the process (as in Example 1)." ></td>
	<td class="line x" title="101:233	The set of individual polarity assignments that is most coherent with this global scenario is the one where all the opinions have the same (equal) polarity." ></td>
	<td class="line x" title="102:233	On the other hand, a pair of individual polarity assignments most consistent with a discourse scenario where a speaker reinforces his stance via opinions towards alternative options, is one with opinions having mutually opposite polarity." ></td>
	<td class="line x" title="103:233	For instance, in the utterance Shapes should be curved, nothing square-like, the speaker reinforces his procurved stance via his opinions about the alternative shapes: curved and square-like." ></td>
	<td class="line x" title="104:233	And, we see that the first opinion is positive and the second is negative." ></td>
	<td class="line x" title="105:233	Table 2 lists the discourse relations (target and frame relation combinations) found in the corpus, and the likely polarity interpretation for the related instances." ></td>
	<td class="line x" title="106:233	Target relation + Frame relation Polarity same+reinforcing equal (e) same+non-reinforcing opposite (o) alternative+reinforcing opposite (o) alternative+non-reinforcing equal (e) Table 2: Discourse relations and their polarity constraints on the related instances." ></td>
	<td class="line x" title="107:233	4.3.2 Optimization Problem For each DA instance i in a dataset, the local classifier provides a class distribution [pi,qi,ri], where pi, qi and ri correspond to the probabilities that i belongs to positive, negative and neutral categories, respectively." ></td>
	<td class="line x" title="108:233	The optimization problem is formulated as an ILP minimization of the objective function in Equation 1." ></td>
	<td class="line x" title="109:233	1summationdisplay i (pixi+qiyi+rizi)+summationdisplay i,j epsilon1ij+summationdisplay i,j ij (1) 173 where the xi, yi and zi are binary class variables corresponding to positive, negative and neutral classes, respectively." ></td>
	<td class="line x" title="110:233	When a class variable is 1, the corresponding class is chosen." ></td>
	<td class="line x" title="111:233	Variables epsilon1ij and ij are binary slack variables that correspond to the discourse constraints between two distinct DA instances i and j. When a given slack variable is 1, the corresponding discourse constraint is violated." ></td>
	<td class="line x" title="112:233	Note that the objective function tries to achieve two goals." ></td>
	<td class="line x" title="113:233	The first part (summationtextipixi+qiyi+rizi) is a maximization that tries to choose a classification for the instances that maximizes the probabilities provided by the local classifier." ></td>
	<td class="line x" title="114:233	The second part (summationtexti,jepsilon1ij+summationtexti,jij) is a minimization that tries to minimize the number of slack variables used, that is, minimize the number of discourse constraints violated." ></td>
	<td class="line x" title="115:233	Constraints in Equations 2 and 3 listed below impose binary constraints on the variables." ></td>
	<td class="line x" title="116:233	The constraint in Equation 4 ensures that, for each instance i, only one class variable is set to 1." ></td>
	<td class="line x" title="117:233	xi{0,1},yi{0,1},zi{0,1}, i (2) epsilon1ij {0,1},ij {0,1}, inegationslash=j (3) xi +yi +zi = 1, i (4) We pair distinct DA instances i and j as ij, and if there exists a discourse relation between them, they can be subject to the corresponding polarity constraints listed in Table 2." ></td>
	<td class="line x" title="118:233	For this, we define two binary discourse-constraint constants: the equal-polarity constant, eij and the oppositepolarity constant, oij." ></td>
	<td class="line x" title="119:233	If a given DA pair ij is related by either a same+reinforcing relation or an alternative+non-reinforcing relation (rows 1, 4 of Table 2), then eij = 1; otherwise it is zero." ></td>
	<td class="line x" title="120:233	Similarly, if it is related by either a same+nonreinforcing relation or an alternative+reinforcing relation (rows 2, 3 of Table 2), then oij = 1." ></td>
	<td class="line x" title="121:233	Both eij and oij are zero if the instance pair is unrelated in the discourse." ></td>
	<td class="line x" title="122:233	For each DA instance pair ij, equal-polarity constraints are applied to the polarity variables ofi (xi, yi) and j (xj, yj) via the following equations: |xixj|1eij +epsilon1ij , inegationslash=j (5) |yiyj|1eij +epsilon1ij , inegationslash=j (6) (xi +yi)li , i (7) When eij = 1, the Equation 5 constrains xi and xj to be of the same value (both zero or both one)." ></td>
	<td class="line x" title="123:233	Similarly, Equation 6 constrains yi and yj to be of the same value." ></td>
	<td class="line x" title="124:233	Via these equations, we ensure that the instances i and j do not have the opposite polarity when eij = 1." ></td>
	<td class="line x" title="125:233	However, notice that, if we use just Equations 5 and 6, the optimization can converge to the same, non-polar (neutral) category." ></td>
	<td class="line x" title="126:233	To guide the convergence to the same polar (positive or negative) category, we use Equation 7." ></td>
	<td class="line x" title="127:233	Here li = 1 if the instance i participates in one or more discourse relations." ></td>
	<td class="line x" title="128:233	Wheneij = 0,xi andxj (and yi and yj), can take on assignments independently of one another." ></td>
	<td class="line x" title="129:233	Notice that both constraints 5 and 6 are relaxed when epsilon1ij = 1; thus, xi and xj (or yi and yj) can take on values independently of one another, even if eij = 1." ></td>
	<td class="line x" title="130:233	Next, the opposite-polarity constraints are applied via the following equations: |xi +xj1|1oij +ij , inegationslash=j (8) |yi +yj1|1oij +ij , inegationslash=j (9) In the above equations, when oij = 1, xi and xj (and yi and yj) take on opposite values; for example, if xi = 1 then xj = 0 and vice versa." ></td>
	<td class="line x" title="131:233	When oij = 0, the variable assignments are independent of one another." ></td>
	<td class="line x" title="132:233	This set of constraints is relaxed when ij = 1." ></td>
	<td class="line x" title="133:233	In general, in our ILP formulation, notice that if an instance does not have a discourse relation to any other instance in the data, its classification is unaffected by the optimization." ></td>
	<td class="line x" title="134:233	Also, as the underlying discourse scheme poses constraints only on the interpretation of the polarity of the related instances, discourse constraints are applied only to the polarity variables x and y, and not to the neutral class variable, z. Finally, even though slack variables are used, we discourage the ILP system from indiscriminately setting the slack variables to 1 by making them a part of the objective function that is minimized." ></td>
	<td class="line x" title="135:233	5 Experiments In this work, we are particularly interested in improvements due to discourse-based methods." ></td>
	<td class="line x" title="136:233	Thus, we report performance under three conditions: over only those instances that are related via discourse relations (Connected), over instances not related via discourse relations (Singletons), and over all instances (All)." ></td>
	<td class="line x" title="137:233	The annotated data consists of 7 scenario-based, multi-party meetings from the AMI meeting corpus." ></td>
	<td class="line x" title="138:233	We filter out very small DAs (DAs with fewer than 3 tokens, punctuation included)." ></td>
	<td class="line x" title="139:233	This gives 174 us a total of 4606 DA instances, of which 1935 (42%) have opinion annotations." ></td>
	<td class="line x" title="140:233	For our experiments, the DAs with no opinion annotations as well as those with neutral opinions are considered as neutral." ></td>
	<td class="line x" title="141:233	Table 3 shows the class distributions in the data for the three conditions." ></td>
	<td class="line x" title="142:233	Pos Neg Neutral Total Connected 643 343 81 1067 Singleton 553 233 2753 3539 All 1196 576 2834 4606 Table 3: Class distribution over connected, single and all instances." ></td>
	<td class="line x" title="143:233	5.1 Classifiers Our first baseline, Base, is a simple distributionbased classifier that classifies the test data based on the overall distribution of the classes in the training data." ></td>
	<td class="line x" title="144:233	However, in Table 3, the class distribution is different for the Connected and Singleton conditions." ></td>
	<td class="line x" title="145:233	We incorporate this in a smarter baseline, Base-2, which constructs separate distributions for connected instances and singletons." ></td>
	<td class="line x" title="146:233	Thus, given a test instance, depending on whether it is connected, Base-2 uses the corresponding distribution to make its prediction." ></td>
	<td class="line x" title="147:233	The third baseline is the supervised classifier, Local, described in Section 4.1." ></td>
	<td class="line x" title="148:233	It is implemented using the SVM classifiers from the Weka toolkit (Witten and Frank, 2002).4 Our supervised discourse-based classifier, ICA from Section 4.2, also uses a similar SVM implementation for its relational classifier." ></td>
	<td class="line x" title="149:233	We implement our ILP approach from Section 4.3 using the optimization toolbox from Mathworks (http://www.mathworks.com) and GNU Linear Programming Kit." ></td>
	<td class="line x" title="150:233	We observed that the ILP system performs better than the ICA system on instances that are connected, while ICA performs better on singletons." ></td>
	<td class="line x" title="151:233	Thus, we also implemented a simple hybrid classifier (HYB), which selects the ICA prediction for classification of singletons and the ILP prediction for classification of connected instances." ></td>
	<td class="line x" title="152:233	5.2 Results We performed 7-fold cross validation experiments, where six meetings are used for training 4We use the SMO implementation, which, when used with the logistic regression, has an output that can be viewed as a posterior probability distribution." ></td>
	<td class="line x" title="153:233	and the seventh is used for testing the supervised classifiers (Base, Base-2, Local and ICA)." ></td>
	<td class="line x" title="154:233	In the case of ILP, the optimization is applied to the output of Local for each test fold." ></td>
	<td class="line x" title="155:233	Table 4 reports the accuracies of the classifiers, averaged over 7 folds." ></td>
	<td class="line x" title="156:233	First, we observe that Base performs poorly over connected instances, but performs considerably better over singletons." ></td>
	<td class="line x" title="157:233	This is expected as the overall majority class is neutral and the singletons are more likely to be neutral." ></td>
	<td class="line x" title="158:233	Base-2, which incorporates the differentiated distributions, performs substantially better than Base." ></td>
	<td class="line x" title="159:233	Local achieves an overall performance improvement over Base and Base-2 by 23 percentage points and 9 percentage points, respectively." ></td>
	<td class="line x" title="160:233	In general, Local outperforms Base for all three conditions (p < 0.001), and Base-2 for the Singleton and All conditions (p< 0.001)." ></td>
	<td class="line x" title="161:233	This overall improvement in Locals accuracy corroborates the utility of the lexical, unigram and DA based features for polarity detection in this corpus." ></td>
	<td class="line x" title="162:233	Turning to the discourse-based classifiers, ICA, ILP and HYB, all of these perform better than Base and Base-2 for all conditions." ></td>
	<td class="line x" title="163:233	ICA improves over Local by 9 percentage points for Connected, 3 points for Singleton and 4 points for All." ></td>
	<td class="line x" title="164:233	ILPs improvement over Local for Connected and All is even more substantial: 28 percentage points and 6 points, respectively." ></td>
	<td class="line x" title="165:233	Notice that ILP has the same performance as Local for Singletons, as the discourse constraints are not applied over unconnected instances." ></td>
	<td class="line x" title="166:233	Finally, HYB significantly outperforms Local under all conditions." ></td>
	<td class="line x" title="167:233	The significance levels of the improvements over Local are highlighted in Table 4." ></td>
	<td class="line x" title="168:233	These improvements also signify that the underlying discourse scheme is effective, and adaptable to different implementations." ></td>
	<td class="line x" title="169:233	Interestingly, ICA and ILP improve over Local in different ways." ></td>
	<td class="line x" title="170:233	While ILP sharply improves the performance over the connected instances, ICA shows relatively modest improvements over both connected and singletons." ></td>
	<td class="line x" title="171:233	ICAs improvement over singletons is interesting because it indicates that, even though the features in Table 1 are focused on discourse relations, ICA utilizes them to learn the classification of singletons too." ></td>
	<td class="line x" title="172:233	Comparing our discourse-based approaches, ILP does significantly better than ICA over connected instances (p < 0.001), while ICA does significantly better than ILP over singletons (p < 175 Base Base-2 Local ICA ILP HYB Connected 24.4 47.56 46.66 55.64 75.07 75.07 Singleton 51.72 63.23 75.73 78.72 75.73 78.72 All 45.34 59.46 68.72 73.31 75.35 77.72 Table 4: Accuracies of the classifiers measured over Connected, Singleton and All instances." ></td>
	<td class="line x" title="173:233	Performance significantly better than Local are indicated in bold for p<0.001 and underline for p<0.01." ></td>
	<td class="line x" title="174:233	0.01)." ></td>
	<td class="line x" title="175:233	However, there is no significant difference between ICA and ILP for the All condition." ></td>
	<td class="line x" title="176:233	The HYB classifier outperforms ILP for the Singleton condition (p < 0.01) and ICA for the Connected condition (p < 0.001)." ></td>
	<td class="line x" title="177:233	Interestingly, over all instances (the All condition), HYB also performs significantly better than both ICA (p<0.001) and ILP (p<0.01)." ></td>
	<td class="line x" title="178:233	5.3 Analysis Amongst our two approaches, ILP performs better, and hence we further analyze its behavior to understand how the improvements are achieved." ></td>
	<td class="line x" title="179:233	Table 5 reports the performance of ILP and Local for the precision, recall and f-measure metrics (averaged over 7 test folds), measured separately for each of the opinion categories." ></td>
	<td class="line x" title="180:233	The most prominent improvement by ILP is observed for the recall of the polar categories under the Connected condition: 40 percentage points for the positive class, and 29 percentage points for the negative class." ></td>
	<td class="line x" title="181:233	The gain in recall is not accompanied by a significant loss in precision." ></td>
	<td class="line x" title="182:233	This results in an improvement in f-measure for the polar categories (24 points for positive and 16 points for negative)." ></td>
	<td class="line x" title="183:233	Also note that, by virtue of the constraint in Equation 7, ILP does not classify any connected instance as neutral; thus the precision is NaN, recall is 0 and the f-meaure is NaN." ></td>
	<td class="line x" title="184:233	This is indicated as * in the Table." ></td>
	<td class="line x" title="185:233	The improvement of ILP for the All condition, for the polar classes, follows a similar trend for recall (18 to 21 point improvement) and f-measure (9 to 13 point improvement)." ></td>
	<td class="line x" title="186:233	In addition to this, the ILP has an overall improvement in precision over Local." ></td>
	<td class="line x" title="187:233	This may seem counterintuitive, as in Table 5, ILPs precision for connected nodes is similar to, or lower than, that of Local." ></td>
	<td class="line x" title="188:233	This is explained by the fact that, while going from connected to overall conditions, Locals polar predictions increase by threefold (565 to 1482), but its correct polar predictions increase by only twofold (430 to 801)." ></td>
	<td class="line x" title="189:233	Thus, the ratio of change in the total Gold Local Pos Neg Neut Total Pos 551 113 532 1196 Neg 121 250 205 576 Neut 312 135 2387 2834 Total 984 498 3124 4606 Gold ILP Pos Neg Neut Total Pos 817 157 222 1196 Neg 147 358 71 576 Neut 358 147 2329 2834 Total 1322 662 2622 4606 Table 6: Contingency table over all instances." ></td>
	<td class="line x" title="190:233	polar predictions to the correct polar predictions is 3 : 2." ></td>
	<td class="line x" title="191:233	On the other hand, while polar predictions by ILP increase by only twofold (1067 to 1984), its correct polar predictions increase by 1.5 times (804 to 1175)." ></td>
	<td class="line x" title="192:233	Here, the ratio of change in the total polar predictions to the correct polar predictions is 4 : 3, a smaller ratio." ></td>
	<td class="line x" title="193:233	The contingency table (Table 6) shows how Local and ILP compare against the gold standard annotations." ></td>
	<td class="line x" title="194:233	Notice here, that even though ILP makes more polar guesses as compared to Local, a greater proportion of the ILP guesses are correct." ></td>
	<td class="line x" title="195:233	The number of non-diagonal elements are much smaller for ILP, resulting in the accuracy improvements seen in Table 4." ></td>
	<td class="line x" title="196:233	6 Examples and Discussion The results in Table 4 show that Local, which provides the classifications for bootstrapping ICA and ILP, predicts an incorrect class for more than 50% of the connected instances." ></td>
	<td class="line x" title="197:233	Methods starting with noisy starting points are in danger of propagating the errors and hence worsening the performance." ></td>
	<td class="line x" title="198:233	Interestingly, in spite of starting with so many bad classifications, ILP is able to achieve a large performance improvement." ></td>
	<td class="line x" title="199:233	We discovered that, given a set of connected instances, even when Local has only one correct guess, ILP is able to use this to rectify the related instances." ></td>
	<td class="line x" title="200:233	We illustrate this situation in Figure 2, which reproduces the connected DAs for Example 1." ></td>
	<td class="line x" title="201:233	It shows the classifications 176 Positive Negative Neutral Local ILP Local ILP Local ILP Connected-Prec 78.1 78.2 71.9 69.8 12.1 Connected-Recall 45.3 86.3 44.1 73.4 62.8 * Connected-F1 56.8 81.5 54.0 70.7 18.5 All-Prec 56.2 61.3 52.3 54.6 76.3 88.3 All-Recall 46.6 67.7 44.3 62.5 83.9 81.5 All-F1 50.4 64.0 46.0 57.1 79.6 84.6 Table 5: Precision, Recall, Fmeasure for each Polarity category." ></td>
	<td class="line x" title="202:233	Performance significantly better than Local are indicated in bold (p< 0.001), underline (p< 0.01) and italics (p< 0.05)." ></td>
	<td class="line x" title="203:233	The * denotes that ILP does not retrieve any connected node as neutral." ></td>
	<td class="line x" title="204:233	Figure 2: Discourse Relations and Classifications for Example 1." ></td>
	<td class="line x" title="205:233	for each DA from the gold standard (G), the Local classifier (L) and the ILP classifier (ILP)." ></td>
	<td class="line x" title="206:233	Observe that Local predicts the correct positive class (+) for only DA-4 (the DA containing bit more durable and ergonomic)." ></td>
	<td class="line x" title="207:233	Notice that these are clear cases of positive evaluation." ></td>
	<td class="line x" title="208:233	It incorrectly predicts the polarity of DA-2 (containing bit more bouncy) as neutral (*), and DA-5 (containing a bit different from all the other remote controls) as negative (-)." ></td>
	<td class="line x" title="209:233	DA-2 and DA-5 exemplify the fact that polarity classification is a complex and difficult problem: being bouncy is a positive evaluation in this particular discourse context, and may not be so elsewhere." ></td>
	<td class="line x" title="210:233	Thus, naturally, lexicons and unigram-based learning would fail to capture this positive evaluation." ></td>
	<td class="line x" title="211:233	Similarly, being different could be deemed negative in other discourse contexts." ></td>
	<td class="line x" title="212:233	However, ILP is able to arrive at the correct predictions for all the instances." ></td>
	<td class="line x" title="213:233	As the DA-4 is connected to both DA-2 and DA-5 via a discourse relation that enforces an equal-polarity constraint (same+reinforcing relation of row 1, Table 2), both of the misclassifications are rectified." ></td>
	<td class="line x" title="214:233	Presumably, the incorrect predictions made by Local are low confidence estimates, while the predictions of the correct cases have high confidence, which makes it possible for ILP to make the corrections." ></td>
	<td class="line x" title="215:233	We also observed the propagation of the correct classification for other types of discourse relations, for more complex types of connectivity, and also for conditions where an instance is not directly connected to the correctly predicted instance." ></td>
	<td class="line x" title="216:233	The meeting snippet below (Example 2) and its corresponding DA relations (Figure 3) illustrate this." ></td>
	<td class="line x" title="217:233	This example is a reinforcing discourse where the speaker is arguing for the number keypad, which is an alternative to the scrolling option." ></td>
	<td class="line x" title="218:233	Thus, he argues against the scrolling, and argues for entering the number (which is a capability of the number keypad)." ></td>
	<td class="line x" title="219:233	(2) D-1: I reckon youre gonna have to have a number keypad anyway for the amount of channels these days, D-2: You wouldnt want to just have to scroll through all the channels to get to the one you want D-3: You wanna enter just the number of it , if you know it D-4: I reckon were gonna have to have a number keypad anyway In Figure 3, we see that, DA-2 is connected via an alternative+reinforcing discourse relation to each of its neighbors DA-1 and DA-3, which encourages the optimization to choose a class for it that is opposite to DA-1 and DA-3." ></td>
	<td class="line x" title="220:233	Notice also, that even though Local predicts only DA-4 correctly, this correct classification finally influences the correct choice for all the instances, including the remotely connected DA-2." ></td>
	<td class="line x" title="221:233	7 Conclusions and Future Work This work focuses on the first step to ascertain whether discourse relations are useful for improving opinion polarity classification, whether they can be modeled and what modeling choices can be used." ></td>
	<td class="line x" title="222:233	To this end, we explored two distinct paradigms: the supervised ICA and the unsupervised ILP." ></td>
	<td class="line x" title="223:233	We showed that both of our approaches are effective in exploiting discourse relations to 177 Figure 3: Discourse Relations and Classifications for Example 2." ></td>
	<td class="line x" title="224:233	significantly improve polarity classification." ></td>
	<td class="line x" title="225:233	We found that there is a difference in how ICA and ILP achieve improvements, and that combining the two in a hybrid approach can lead to further overall improvement." ></td>
	<td class="line x" title="226:233	Quantitatively, we showed that our approach is able to achieve a large increase in recall of the polar categories without harming the precision, which results in the performance improvements." ></td>
	<td class="line x" title="227:233	Qualitatively, we illustrated how, even if the bootstrapping process is noisy, the optimization and discourse constraints effectively rectify the misclassifications." ></td>
	<td class="line x" title="228:233	The improvements of our diverse global inference approaches indicate that discourse information can be adapted in different ways to augment and improve existing opinion analysis techniques." ></td>
	<td class="line x" title="229:233	The automation of the discourse-relation recognition is the next step in this research." ></td>
	<td class="line x" title="230:233	The behavior of ICA and ILP can change, depending on the automation of discourse level recognition." ></td>
	<td class="line x" title="231:233	The implementation and comparison of the two methods under full automation is the focus of our future work." ></td>
	<td class="line x" title="232:233	Acknowledgments This research was supported in part by the Department of Homeland Security under grant N000140710152 and NSF Grant No. 0746930." ></td>
	<td class="line x" title="233:233	We would also like to thank the anonymous reviewers for their helpful comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1020
Subjectivity Word Sense Disambiguation
Akkaya, Cem;Wiebe, Janyce M.;Mihalcea, Rada;"></td>
	<td class="line x" title="1:249	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 190199, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:249	c 2009 ACL and AFNLP Subjectivity Word Sense Disambiguation Cem Akkaya and Janyce Wiebe University of Pittsburgh {cem,wiebe}@cs.pitt.edu Rada Mihalcea University of North Texas rada@cs.unt.edu Abstract This paper investigates a new task, subjectivity word sense disambiguation (SWSD), which is to automatically determine which word instances in a corpus are being used with subjective senses, and which are being used with objective senses." ></td>
	<td class="line x" title="3:249	We provide empirical evidence that SWSD is more feasible than full word sense disambiguation, and that it can be exploited to improve the performance of contextual subjectivity and sentiment analysis systems." ></td>
	<td class="line x" title="4:249	1 Introduction The automatic extraction of opinions, emotions, and sentiments in text (subjectivity analysis) to support applications such as product review mining, summarization, question answering, and information extraction is an active area of research in NLP." ></td>
	<td class="line x" title="5:249	Many approaches to opinion, sentiment, and subjectivity analysis rely on lexicons of words that may be used to express subjectivity." ></td>
	<td class="line x" title="6:249	Examples of such words are the following (in bold): (1) He is a disease to every team he has gone to." ></td>
	<td class="line x" title="7:249	Converting to SMF is a headache." ></td>
	<td class="line x" title="8:249	The concert left me cold." ></td>
	<td class="line x" title="9:249	That guy is such a pain." ></td>
	<td class="line x" title="10:249	Knowing the meaning (and thus subjectivity) of these words would help a system recognize the negative sentiments in these sentences." ></td>
	<td class="line x" title="11:249	Most subjectivity lexicons are compiled as lists of keywords, rather than word meanings (senses)." ></td>
	<td class="line x" title="12:249	However, many keywords have both subjective and objective senses." ></td>
	<td class="line x" title="13:249	False hits  subjectivity clues used with objective senses  are a significant source of error in subjectivity and sentiment analysis." ></td>
	<td class="line x" title="14:249	For example, even though the following sentence contains all of the negative keywords above, it is nevertheless objective, as they are all false hits: (2) Early symptoms of the disease include severe headaches, red eyes, fevers and cold chills, body pain, and vomiting." ></td>
	<td class="line x" title="15:249	To tackle this source of error, we define a new task, subjectivity word sense disambiguation (SWSD), which is to automatically determine which word instances in a corpus are being used with subjective senses, and which are being used with objective senses." ></td>
	<td class="line x" title="16:249	We hypothesize that SWSD is more feasible than full word sense disambiguation, because it is more coarse grained  often, the exact sense need not be pinpointed." ></td>
	<td class="line x" title="17:249	We also hypothesize that SWSD can be exploited to improve the performance of contextual subjectivity analysis systems via sense-aware classification." ></td>
	<td class="line x" title="18:249	The paper consists of two parts." ></td>
	<td class="line x" title="19:249	In the first part, we build and evaluate a targeted supervised SWSD system that aims to disambiguate members of a subjectivity lexicon." ></td>
	<td class="line x" title="20:249	It labels clue instances as having a subjective sense or an objective sense in context." ></td>
	<td class="line x" title="21:249	The system relies on common machine learning features for word sense disambiguation (WSD)." ></td>
	<td class="line x" title="22:249	The performance is substantially above both baseline and the performance of full WSD on the same data, suggesting that the task is feasible, and that subjectivity provides a natural coarsegrained grouping of senses." ></td>
	<td class="line x" title="23:249	The second part demonstrates the promise of SWSD for contextual subjectivity analysis." ></td>
	<td class="line x" title="24:249	First, we show that subjectivity sense ambiguity is highly prevalent in the MPQA opinion-annotated corpus (Wiebe et al., 2005; Wilson, 2008), thus establishing the potential benefit of performing SWSD." ></td>
	<td class="line x" title="25:249	Then, we exploit SWSD to improve performance on several subjectivity analysis tasks, from subjective/objective sentence-level classification to positive/negative/neutral expressionlevel classification." ></td>
	<td class="line x" title="26:249	To our knowledge, this is the 190 first attempt to explicitly use sense-level subjectivity tags in contextual subjectivity and sentiment analysis." ></td>
	<td class="line x" title="27:249	2 Background We adopt the definitions of subjective and objective from (Wiebe et al., 2005; Wiebe and Mihalcea, 2006; Wilson, 2008)." ></td>
	<td class="line x" title="28:249	Subjective expressions are words and phrases being used to express mental and emotional states, such as speculations, evaluations, sentiments, and beliefs." ></td>
	<td class="line x" title="29:249	A general covering term for such states is private state (Quirk et al., 1985), an internal state that cannot be directly observed or verified by others." ></td>
	<td class="line x" title="30:249	(Wiebe and Mihalcea, 2006) give the following examples: (3) His alarm grew." ></td>
	<td class="line x" title="31:249	He absorbed the information quickly." ></td>
	<td class="line x" title="32:249	UCC/Disciples leaders roundly condemned the Iranian Presidents verbal assault on Israel." ></td>
	<td class="line x" title="33:249	Whats the catch?" ></td>
	<td class="line x" title="34:249	Polarity (also called semantic orientation) is also important to NLP applications." ></td>
	<td class="line x" title="35:249	In review mining, for example, we want to know whether an opinion about a product is positive or negative." ></td>
	<td class="line x" title="36:249	Nonetheless, as argued by (Wiebe and Mihalcea, 2006; Su and Markert, 2008), there are also motivations for a separate subjective/objective (S/O) classification." ></td>
	<td class="line x" title="37:249	First, expressions may be subjective but not have any particular polarity." ></td>
	<td class="line x" title="38:249	An example given by (Wilson et al., 2005a) is Jerome says the hospital feels no different than a hospital in the states." ></td>
	<td class="line x" title="39:249	An NLP application system may want to find a wide range of private states attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments." ></td>
	<td class="line oc" title="40:249	Second, benefits for sentiment analysis can be realized by decomposing the problem into S/O (or neutral versus polar) and polarity classification (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005a; Kim and Hovy, 2006)." ></td>
	<td class="line x" title="41:249	We will see further evidence of this in Section 4.2.3 in this paper." ></td>
	<td class="line x" title="42:249	The contextual subjectivity analysis experiments in Section 4 include both S/O and polarity classifications." ></td>
	<td class="line x" title="43:249	The data used in those experiments is from the MPQA Corpus (Wiebe et al., 2005; Wilson, 2008),1 which consists of texts from the world press annotated for subjective expressions." ></td>
	<td class="line x" title="44:249	1Available at http://www.cs.pitt.edu/mpqa In the MPQA Corpus, subjective expressions of varying lengths are marked, from single words to long phrases." ></td>
	<td class="line x" title="45:249	In addition, other properties are annotated, including polarity." ></td>
	<td class="line x" title="46:249	For SWSD, we need the notions of subjective and objective senses of words in a dictionary." ></td>
	<td class="line x" title="47:249	We adopt the definitions from (Wiebe and Mihalcea, 2006), who describe the annotation scheme as follows." ></td>
	<td class="line x" title="48:249	Classifying a sense as S means that, when the sense is used in a text or conversation, one expects it to express subjectivity, and also that the phrase or sentence containing it expresses subjectivity." ></td>
	<td class="line x" title="49:249	As noted in (Wiebe and Mihalcea, 2006), sentences containing objective senses may not be objective." ></td>
	<td class="line x" title="50:249	Thus, objective senses are defined as follows: Classifying a sense as O means that, when the sense is used in a text or conversation, one does not expect it to express subjectivity and, if the phrase or sentence containing it is subjective, the subjectivity is due to something else." ></td>
	<td class="line x" title="51:249	Finally, classifying a sense as B means it covers both subjective and objective usages." ></td>
	<td class="line x" title="52:249	The following subjective examples are given in (Wiebe and Mihalcea, 2006): His alarm grew." ></td>
	<td class="line x" title="53:249	alarm, dismay, consternation  (fear resulting from the awareness of danger) => fear, fearfulness, fright  (an emotion experienced in anticipation of some specific pain or danger (usually accompanied by a desire to flee or fight)) Whats the catch?" ></td>
	<td class="line x" title="54:249	catch  (a hidden drawback; it sounds good but whats the catch?) => drawback  (the quality of being a hindrance; he pointed out all the drawbacks to my plan) They give the following objective examples: The alarm went off." ></td>
	<td class="line x" title="55:249	alarm, warning device, alarm system  (a device that signals the occurrence of some undesirable event) => device  (an instrumentality invented for a particular purpose; the device is small enough to wear on your wrist; a device intended to conserve water) He sold his catch at the market." ></td>
	<td class="line x" title="56:249	catch, haul  (the quantity that was caught; the catch was only 10 fish) => indefinite quantity  (an estimated quantity) Wiebe and Mihalcea performed an agreement study and report that good agreement (=0.74) can be achieved between human annotators labeling the subjectivity of senses." ></td>
	<td class="line x" title="57:249	For a similar task, (Su and Markert, 2008) also report good agreement (=0.79)." ></td>
	<td class="line x" title="58:249	191 3 Subjectivity Word Sense Disambiguation 3.1 Task Definition and Method We now turn to SWSD, and our method for performing it." ></td>
	<td class="line x" title="59:249	Note that SWSD is midway between pure dictionary classification and pure contextual interpretation." ></td>
	<td class="line x" title="60:249	For SWSD, the context of the word is considered in order to perform the task, but the subjectivity is determined solely by the dictionary." ></td>
	<td class="line x" title="61:249	In contrast, full contextual interpretation can deviate from a senses subjectivity label in the dictionary." ></td>
	<td class="line x" title="62:249	As noted above, words used with objective senses may appear in subjective expressions." ></td>
	<td class="line x" title="63:249	For example, an SWSD system would label the following examples of alarm as S, O and O, respectively." ></td>
	<td class="line x" title="64:249	On the other hand, a sentence-level subjectivity classifier would label the sentences as S, S, and O, respectively." ></td>
	<td class="line x" title="65:249	(4) His alarm grew." ></td>
	<td class="line x" title="66:249	Will someone shut that darn alarm off?" ></td>
	<td class="line x" title="67:249	The alarm went off." ></td>
	<td class="line x" title="68:249	We use a supervised approach to SWSD." ></td>
	<td class="line x" title="69:249	We train a different classifier for each lexicon entry for which we have training data." ></td>
	<td class="line x" title="70:249	Thus, our approach is like targeted WSD (in contrast to allwords WSD), with two labels: S and O. We borrow machine learning features which have been successfully used in WSD." ></td>
	<td class="line x" title="71:249	Specifically, given an ambiguous target word, we use the following features from (Mihalcea, 2002): CW : the target word itself CP : POS of the target word CF : surrounding context of 3 words and their POS HNP : the head of the noun phrase to which the target word belongs NB : the first noun before the target word VB : the first verb before the target word NA : the first noun after the target word VA : the first verb after the target word SK : at most 10 context words occurring at least 5 times; determined for each sense 3.2 Lexicon and Data Our target words are members of a subjectivity lexicon, because, since they are in such a lexicon, we know they have subjective usages." ></td>
	<td class="line x" title="72:249	Specifically, we use the lexicon of (Wilson et al., 2005b; Wilson, 2008).2 The entries have been divided into 2Available at http://www.cs.pitt.edu/mpqa those that are strongly subjective (strongsubj) and those that are weakly subjective (weaksubj), reflecting their reliability as subjectivity clues." ></td>
	<td class="line x" title="73:249	The sources of the entries in the lexicon are identified in (Wilson, 2008)." ></td>
	<td class="line x" title="74:249	In the second part of this paper, we evaluate systems against the MPQA corpus." ></td>
	<td class="line x" title="75:249	Wilson also uses this corpus for her evaluations." ></td>
	<td class="line x" title="76:249	To enable this, entries were added to the lexicon independently from the MPQA corpus (that is, none of the entries were derived using the MPQA corpus)." ></td>
	<td class="line x" title="77:249	The training and test data for SWSD consists of word instances in a corpus labeled as S or O, indicating whether they are used with a subjective or objective sense." ></td>
	<td class="line x" title="78:249	Because we do not have data labeled with the S/O coarse-grained senses and we did not want to undertake the annotation effort at this stage, we created an annotated corpus by combining two types of sense annotations: (1) labels of senses within a dictionary as S or O (i.e., subjectivity sense labels), and (2) sense tags of word instances in a corpus (i.e., sense-tagged data)." ></td>
	<td class="line x" title="79:249	The subjectivity sense labels are used to collapse the sense labels in the sense-tagged data into the two new senses, S and O. Our sense-tagged data are the lexical sample corpora (training and test data) from SENSEVAL1 (Kilgarriff and Palmer, 2000), SENSEVAL2 (Preiss and Yarowsky, 2001), and SENSEVAL3 (Mihalcea and Edmonds, 2004)." ></td>
	<td class="line x" title="80:249	We selected all of the SENSEVAL words that are also in the subjectivity lexicon, and labeled their dictionary senses as S, O, or B according to the annotation scheme described above in Section 2." ></td>
	<td class="line x" title="81:249	We did this subjectivity sense labeling according to the sense inventory of the underlying corpus (Hector for SENSEVAL1; WordNet1.7 for SENSEVAL2; and WordNet1.7.1 for SENSEVAL3)." ></td>
	<td class="line x" title="82:249	Among the words, we found that 11 are not ambiguous either they have only S or only O senses (in the corresponding sense inventory), or the senses of their instances in the SENSEVAL data are all S or all O. So as not to inflate our results, we removed those 11 from the data, leaving 39 words." ></td>
	<td class="line x" title="83:249	In addition, we excluded the senses labeled B (a total of 10 senses)." ></td>
	<td class="line x" title="84:249	This leaves a total of 372 senses: 9 words (64 senses) from SENSEVAL1, 18 words (201 senses) from SENSEVAL2, and 12 words (107 senses) from SENSEVAL3." ></td>
	<td class="line x" title="85:249	192 Base Acc SP SR SF OP OR OF IB EB(%) All 79.9 88.3 89.3 89.1 89.2 87.1 87.4 87.2 8.4 41.8 S1 57.9 80.7 81.1 78.3 79.7 80.2 82.9 81.5 22.8 54.2 S2 81.1 87.3 86.5 85.2 85.8 87.9 89.0 88.4 6.2 32.8 S3 95.0 96.4 96.5 99.0 97.7 96.3 87.8 91.8 1.4 28.0 Table 1: Overall SWSD results (micro averages)." ></td>
	<td class="line x" title="86:249	Base is majority-class baseline; Acc is accuracy; SP, SR, and SF are subjective precision, recall and F-measure; similarly for OP, OR, and OF." ></td>
	<td class="line x" title="87:249	IB is absolute improvement in Acc over Base; EB is percent error reduction in Acc." ></td>
	<td class="line x" title="88:249	3.3 SWSD Experiments In this section, we evaluate our SWSD system, and compare its performance to an WSD system on the same data." ></td>
	<td class="line x" title="89:249	Note that, although generally in the SENSEVAL datasets, training and test data are provided separately, a few target words from SENSEVAL1 do not have both training and testing data." ></td>
	<td class="line x" title="90:249	Thus, we opted to combine the training and test data into one dataset, and then perform 10-fold cross validation experiments." ></td>
	<td class="line x" title="91:249	For our classifier, we use the SVM classifier from the Weka package (Witten and Frank., 2005) with its default settings." ></td>
	<td class="line x" title="92:249	We were interested in how well the system would perform on more and less ambiguous words." ></td>
	<td class="line x" title="93:249	Thus, we split the words into three subsets according to their majority-class baselines, and report separate results: S1 (9 words), S2 (18 words), and S3 (12 words) have majority-class baselines in the intervals [50%,70%) , [70%,90%), and [90%,100%), respectively." ></td>
	<td class="line x" title="94:249	Table 1 contains the results, giving the overall results (micro averages), as well as results for the subsets S1, S2, and S3." ></td>
	<td class="line x" title="95:249	The improvement for SWSD over baseline is especially high for the less skewed set, S1." ></td>
	<td class="line x" title="96:249	This is very encouraging because these words are the more ambiguous words, and thus are the ones that most need SWSD (assuming the SENSEVAL priors are similar to the priors in the corpus)." ></td>
	<td class="line x" title="97:249	The average error reduction over baseline for S1 words is 54.2%." ></td>
	<td class="line x" title="98:249	Even for the more skewed sets S2 and S3, reductions are 32.8% and 28.0%, respectively, with an overall reduction of 41.8%." ></td>
	<td class="line x" title="99:249	To compare SWSD with WSD, we re-ran the 10-fold cross validation experiments, but this time using the original sense labels, rather than S and O. The (micro-averaged) accuracy is 67.9%, much lower than the overall accuracy for SWSD (88.3%)." ></td>
	<td class="line x" title="100:249	The positive results provide evidence that SWSD is a feasible variant of WSD, and that the S/O sense groupings are natural ones, since the system is able to learn to distinguish between them with high accuracy." ></td>
	<td class="line x" title="101:249	There is also potential for improvement by using a richer feature set, including subjectivity features." ></td>
	<td class="line x" title="102:249	4 Opinion Analysis with Subjectivity Word Sense Disambiguation In this section, we explore the promise of SWSD for contextual subjectivity analysis." ></td>
	<td class="line x" title="103:249	First, we provide evidence that a subjectivity lexicon can have substantial coverage of the subjective expressions in a corpus, yet still be responsible for significant subjectivity sense ambiguity in that corpus." ></td>
	<td class="line x" title="104:249	Then, we exploit SWSD in several contextual opinion analysis systems, comparing the performance of sense-aware and non-sense-aware versions." ></td>
	<td class="line x" title="105:249	They are all variations of components of the OpinionFinder opinion recognition system.3 4.1 Coverage and Ambiguity of Lexicon Entries in the MPQA Corpus In this section, we consider the distribution of lexicon entries in the MPQA corpus." ></td>
	<td class="line x" title="106:249	The lexicon covers a substantial subset of the subjective expressions in the corpus: 67.1% of the subjective expressions contain one or more lexicon entries." ></td>
	<td class="line x" title="107:249	On the other hand, fully 42.9% of the instances of the lexicon entries in the MPQA corpus are not in subjective expressions." ></td>
	<td class="line x" title="108:249	An instance that is not in a subjective expression is, by definition, being used with an objective sense." ></td>
	<td class="line x" title="109:249	Thus, these instances are false hits of subjectivity clues." ></td>
	<td class="line x" title="110:249	As mentioned above, the entries in the lexicon have been pre-classified as either more (strongsubj) or less (weaksubj) reliable." ></td>
	<td class="line x" title="111:249	We see this difference reflected in their degree of ambiguity  53% of the 3Available at http://www.cs.pitt.edu/opin 193 weaksubj instances are false hits, while only 22% of the strongsubj instances are." ></td>
	<td class="line x" title="112:249	The high coverage of the lexicon demonstrates its potential usefulness for opinion analysis systems, while its degree of ambiguity, in the form of false hits in a subjectivity annotated corpus, shows the potential benefit to opinion analysis of performing SWSD." ></td>
	<td class="line x" title="113:249	As mentioned above, our experiments involve only lexicon entries that are covered by the SENSEVAL data, as we did not perform manual sense tagging for this work." ></td>
	<td class="line x" title="114:249	We have hope to expand the systems coverage in the future, as more wordsense tagged data is produced (e.g., ONTONOTES (Hovy et al., 2006))." ></td>
	<td class="line x" title="115:249	We also have evidence that a moderate amount of manual annotation would be worth the effort." ></td>
	<td class="line x" title="116:249	For example, let us order the lexicon entries from highest to lowest by frequency in the MPQA corpus." ></td>
	<td class="line x" title="117:249	The top 20 are responsible for 25% of all false hits in the corpus; the top 40 are responsible for 34%; and the top 80 are responsible for 44%." ></td>
	<td class="line x" title="118:249	If the SWSD system could be trained for these words, the potential impact on reducing false hits could be substantial, especially considering the good performance of the SWSD system on the more ambiguous words." ></td>
	<td class="line x" title="119:249	Note that we do not want to simply discard these clues." ></td>
	<td class="line x" title="120:249	The top 20 cover 9.4% of all subjective expressions; the top 40 cover 15.4%; and the top 80 cover 29.5%." ></td>
	<td class="line x" title="121:249	Note that SWSD only needs the data annotated with the coarse-grained binary labels, which should be less time consuming to produce than full word sense tags." ></td>
	<td class="line x" title="122:249	4.2 Contextual Classification We found in Section 3.3 that SWSD is a feasible task and then in Section 4.1 that there is a great deal of subjectivity sense ambiguity in a standard subjectivity-annotated corpus (MPQA)." ></td>
	<td class="line x" title="123:249	We now turn to exploiting the results of SWSD to automatically recognize subjectivity and sentiment in the MPQA corpus." ></td>
	<td class="line x" title="124:249	A motivation for using the MPQA data is that many types of classifiers have been evaluated on it, and we can directly test the effect of SWSD on these classifiers." ></td>
	<td class="line x" title="125:249	Note that, for the SWSD experiments, the number of words does not limit the amount of data, as SENSEVAL provides data for each word." ></td>
	<td class="line x" title="126:249	However, the only parts of the MPQA corpus for which SWSD could affect performance is the subset containing instances of the words in the SWSD systems coverage." ></td>
	<td class="line x" title="127:249	Thus, for the classifiers in this section, the data used is the SenMPQA dataset, which consists of the sentences in the MPQA Corpus that contain at least one instance of the 39 keywords." ></td>
	<td class="line x" title="128:249	There are 689 such sentences (containing, in total, 723 instances of the 39 keywords)." ></td>
	<td class="line x" title="129:249	Even though this dataset is smaller than the one used above, it gives us enough data to draw conclusions according to McNemars test for statistical significance." ></td>
	<td class="line x" title="130:249	4.2.1 Rule-based Classifier We first apply SWSD to the rule-based classifier from (Riloff and Wiebe, 2003)." ></td>
	<td class="line x" title="131:249	The classifier, which is a sentence-level S/O classifier, has low subjective and objective recall but high subjective and objective precision." ></td>
	<td class="line x" title="132:249	It is useful for creating training data for subsequent processing by applying it to large amounts of unannotated data." ></td>
	<td class="line x" title="133:249	The classifier is a good candidate for directly measuring the effects of SWSD on contextual subjectivity analysis, because it classifies sentences only by looking for the presence of subjectivity keywords." ></td>
	<td class="line x" title="134:249	Performance will improve if false hits can be ignored." ></td>
	<td class="line x" title="135:249	The classifier labels a sentence as S if it contains two or more strongsubj clues." ></td>
	<td class="line x" title="136:249	On the other hand, it considers three conditions to classify a sentence as O: there are no strongsubj clues in the current sentence, there are together at most one strongsubj clue in the previous and next sentence, and there are together at most 2 weaksubj clues in the current, previous, and next sentence." ></td>
	<td class="line x" title="137:249	A sentence that is not labeled S or O is labeled unknown." ></td>
	<td class="line x" title="138:249	The rule-based classifier is made sense aware by making it blind to the target word instances labeled O by the SWSD system, as these represent false hits of subjectivity keywords." ></td>
	<td class="line x" title="139:249	We compare this sense-aware method (SE), with the original classifier (ORB), in order to see if SWSD would improve performance." ></td>
	<td class="line x" title="140:249	We also built another modified rule-based classifier RE to demonstrate the effect of randomly ignoring subjectivity keywords." ></td>
	<td class="line x" title="141:249	RE ignores a keyword instance randomly with a probability of 0.429, the expected value of false hits in the MPQA corpus." ></td>
	<td class="line x" title="142:249	The results are listed in Table 2." ></td>
	<td class="line x" title="143:249	The rule-based classifier looks for the presence of the keywords to find subjective sentences and for the absence of the keywords to find objective sentences." ></td>
	<td class="line x" title="144:249	It is obvious that a variant working on 194 Acc OP OR OF SP SR SF ORB 27.0 50.0 4.1 7.6 92.7 36.0 51.8 SE 28.3 62.1 9.3 16.1 92.7 35.8 51.6 RE 27.6 48.4 7.7 13.3 92.6 35.4 51.2 Table 2: Effect of SWSD on the rule-based classifiers." ></td>
	<td class="line x" title="145:249	fewer keyword instances than ORB will always have the same or higher objective recall and the same or lower subjective recall than ORB." ></td>
	<td class="line x" title="146:249	That is the case for both SE and RE." ></td>
	<td class="line x" title="147:249	The real benefit we see is in objective precision, which is substantially higher for SE than ORB." ></td>
	<td class="line x" title="148:249	For our experiments, OP gives a better idea of the impact of SWSD, because most of the keyword instances SWSD disambiguates are weaksubj clues, and weaksubj keywords figure more prominently in objective classification." ></td>
	<td class="line x" title="149:249	On the other hand, RE has both lower OP and SP than ORB." ></td>
	<td class="line x" title="150:249	Note that accuracy for all three systems is low, because all unknown predictions are counted as incorrect." ></td>
	<td class="line x" title="151:249	These findings suggest that SWSD performs well on disambiguating keyword instances in the MPQA corpus,4 and demonstrates a positive impact of SWSD on sentence-level subjectivity classification." ></td>
	<td class="line x" title="152:249	4.2.2 Subjective/Objective Classifier We now move to more fine-grained expressionlevel subjectivity classification." ></td>
	<td class="line x" title="153:249	Since sentences often contain multiple subjective expressions, expression-level classification is more informative than sentence-level classification." ></td>
	<td class="line x" title="154:249	The classifier in this section is an implementation of the neutral/polar supervised classifier of (Wilson et al., 2005a) (using the same features), except that the classes are S/O rather than neutral/polar." ></td>
	<td class="line x" title="155:249	These classifiers label instances of lexicon entries." ></td>
	<td class="line x" title="156:249	The gold standard is defined on the MPQA Corpus as follows: If an instance is in a subjective expression, it is contextually S. If the instance is in an objective expression, it is contextually O. We evaluate the system on the 723 clue instances in the SenMPQA dataset." ></td>
	<td class="line x" title="157:249	We incorporate SWSD information into the contextual subjectivity classifier in a straightforward fashion: outputs are modified according to simple, intuitive rules." ></td>
	<td class="line x" title="158:249	4which we cannot evaluate directly, as the MPQA corpus is not sense tagged." ></td>
	<td class="line x" title="159:249	Our strategy is defined by the relation between sense subjectivity and contextual subjectivity and involves two rules, R1 and R2." ></td>
	<td class="line x" title="160:249	We know that a keyword instance used with a S sense must be in a subjective expression." ></td>
	<td class="line x" title="161:249	R1 is to simply trust SWSD: If the contextual classifier labels an instance as O, but SWSD determines that it has an S sense, then R1 flips the contextual classifiers label to S. Things are not as simple in the case of O senses, since they may appear in both subjective and objective expressions." ></td>
	<td class="line x" title="162:249	We will state R2, and then explain it: If the contextual classifier labels an instance as S, but (1) SWSD determines that it has an O sense, (2) the contextual classifiers confidence is low, and (3) there is no other subjective keyword in the same expression, then R2 flips the contextual classifiers label to O. First, consider confidence: though a keyword with an O sense may appear in either subjective or objective expressions, it is more likely to appear in an objective expression." ></td>
	<td class="line x" title="163:249	We assume that this is reflected to some extent in the contextual classifiers confidence." ></td>
	<td class="line x" title="164:249	Second, if a keyword with an O sense appears in a subjective expression, then the subjectivity is not due to that keyword but rather due to something else." ></td>
	<td class="line x" title="165:249	Thus, the presence of another lexicon entry explains away the presence of the O sense in the subjective expression, and we do not want SWSD to overrule the contextual classifier." ></td>
	<td class="line x" title="166:249	Only when the contextual classifier isnt certain and only when there isnt another keyword does R2 flip the label to O. Our definition of low confidence is in terms of the label weights assigned by BoosTexter (Schapire and Singer, 2000), which is the underlying machine learning algorithm of the classifier." ></td>
	<td class="line x" title="167:249	We use the difference between the largest label weight and the second largest label weight as a measure of confidence, as suggested in the BoosTexter documentation." ></td>
	<td class="line x" title="168:249	The threshold we use is 0.0008.5 We apply the contextual classifier and the SWSD system to the data, and compare the performance of the original system (OS/O) and three sense-aware variants: one using only R1, one us5As will be noted below, we experimented with three thresholds for the classifier in Section 4.2.3, with no significant difference in accuracy." ></td>
	<td class="line x" title="169:249	Here, we simply adopt 0.0008, without further experimentation." ></td>
	<td class="line x" title="170:249	In addition, we did not experiment with other conditions than those incorporated in the two rules in this section and the two rules in Section 4.2.3 below." ></td>
	<td class="line x" title="171:249	195 Acc OP OR OF SP SR SF OS/O 75.4 68.0 62.9 65.4 79.2 82.7 80.9 R1 77.7 75.5 58.8 66.1 78.6 88.8 83.4 R2 79.0 67.3 83.9 74.7 89.0 76.1 82.0 R1R2 81.3 72.5 79.8 75.9 87.4 82.2 84.8 Table 3: Effect of SWSD on the subjective/objective classifier ing only R2, and one using both (R1R2)." ></td>
	<td class="line x" title="172:249	The results are in Table 3." ></td>
	<td class="line x" title="173:249	The R1 variant shows an improvement of 2.3 points in accuracy (a 9.4% error reduction)." ></td>
	<td class="line x" title="174:249	The R2 variant shows an improvement of 3.6 points in accuracy (a 14.6% error reduction)." ></td>
	<td class="line x" title="175:249	Applying both rules (R1R2) gives an improvement of 5.9 percentage points in accuracy (a 24% error reduction)." ></td>
	<td class="line x" title="176:249	In our case, a paired t-test is not appropriate to measure statistical significance, as we are not doing multiple runs." ></td>
	<td class="line x" title="177:249	Thus, we apply McNemars test, which is a non-parametric method for algorithms that can be executed only once, meaning training once and testing once (Dietterich, 1998)." ></td>
	<td class="line x" title="178:249	For R1, the improvement in accuracy is statistically significant at the p < .05 level." ></td>
	<td class="line x" title="179:249	For R2 and R1R2, the improvement in accuracy is statistically significant at the p < .01 level." ></td>
	<td class="line x" title="180:249	Moreover, in all cases, we see improvement in both objective and subjective F-measure." ></td>
	<td class="line x" title="181:249	4.2.3 Contextual Polarity Classifier We now apply SWSD to contextual polarity classification (positive/negative/neutral), in the hope that avoiding false hits of subjectivity keywords will also lead to performance improvement in contextual sentiment analysis." ></td>
	<td class="line x" title="182:249	We use an implementation of the classifier of (Wilson et al., 2005a)." ></td>
	<td class="line x" title="183:249	This classifier labels instances of lexicon entries." ></td>
	<td class="line x" title="184:249	The gold standard is defined on the MPQA Corpus as follows: If an instance is in a positive subjective expression, it is contextually positive (Ps); if in a negative subjective expression, it is contextually negative (Ng); and if it is in an objective expression or a neutral subjective expression, then it is contextually N(eutral)." ></td>
	<td class="line x" title="185:249	As above, we evaluate the system on the keyword instances in the SenMPQA dataset." ></td>
	<td class="line x" title="186:249	Wilson et al. use a two step approach." ></td>
	<td class="line x" title="187:249	The first step classifies keyword instances as being in a polar (positive or negative) or a neutral context." ></td>
	<td class="line x" title="188:249	The first step is performed by the neutral/polar classifier mentioned above in Section 4.2.2." ></td>
	<td class="line x" title="189:249	The second step decides the contextual polarity (positive or negative) of the instances classified as polar in the first step, and is performed by a separate classifier." ></td>
	<td class="line x" title="190:249	To make a sense-aware version of the system, we use rules to change some of the answers of the neutral/polar classifier." ></td>
	<td class="line x" title="191:249	Unfortunately, we cannot simply trust SWSD when it labels a keyword as an S sense, because an S sense might be in a N(eutral) expression (since there are neutral subjective expressions)." ></td>
	<td class="line x" title="192:249	But, an S sense is more likely to appear in a P(olar) expression." ></td>
	<td class="line x" title="193:249	Thus, we consider confidence (rule R3): If the contextual classifier labels an instance as N, but SWSD determines it has an S sense and the contextual classifiers confidence is low,6 then R3 flips the contextual classifiers label to P. Rule R4 is analogous to R2 in the previous section: If the contextual classifier labels an instance as P, but (1) SWSD determines that it has an O sense, (2) the contextual classifiers confidence is low, and (3) there is no other subjective keyword in the same expression, then R2 flips the contextual classifiers label to N. We compare the performance of the original neutral/polar classifier (ON/P) and sense-aware variants using R3 and R4." ></td>
	<td class="line x" title="194:249	The results are in Table 4." ></td>
	<td class="line x" title="195:249	This time, the table does not include a combined method, because only R4 improves performance." ></td>
	<td class="line x" title="196:249	This is consistent with the finding in (Wilson et al., 2005a) that most errors are caused by subjectivity keywords with non-neutral prior polarity appearing in phrases with neutral contextual polarity." ></td>
	<td class="line x" title="197:249	R4 targets these cases." ></td>
	<td class="line x" title="198:249	It is promising to see that SWSD provides enough information to fix some of them." ></td>
	<td class="line x" title="199:249	There is a 2.6 point improvement in accuracy (a 12.4% error reduction)." ></td>
	<td class="line x" title="200:249	The improvement in accuracy is statistically significant at the p < .01 level with McNemars test." ></td>
	<td class="line x" title="201:249	The improvement in accuracy is accompanied by improvements in both neutral and polar F-measure." ></td>
	<td class="line x" title="202:249	We wanted to see if the improvements in the 6As in the previous section, low confidence is defined in terms of the difference between the largest label weight and the second largest label weight assigned by BoosTexter." ></td>
	<td class="line x" title="203:249	We tried three thresholds, 0.0007, 0.0008, and 0.0009, resulting in only a slight difference in accuracy: 0.0007 and 0.0009 both give 81.5 accuracy compared to 81.6 accuracy for 0.0008." ></td>
	<td class="line x" title="204:249	We report results using 0.0008, though the accuracy using the other thresholds is statistically significantly better than the accuracy of the original classifier at the same level." ></td>
	<td class="line x" title="205:249	196 Acc NP NR NF NgP NgR NgF PsP PsR PsF OPs/Ng/N 77.6 80.9 94.6 87.2 60.4 29.4 39.5 52.2 32.4 40.0 R4 80.6 81.2 98.7 89.1 82.1 29.4 43.2 68.6 32.4 44.0 Table 5: Effect of SWSD on the contextual polarity classifier Acc NP NR NF PP PR PF ON/P 79.0 81.5 92.5 86.7 65.8 40.7 50.3 R3 70.0 83.7 73.8 78.4 44.4 59.3 50.8 R4 81.6 81.7 96.8 88.6 81.1 38.6 52.3 Table 4: Effect of SWSD on the neutral/polar classifier first step of Wilson et als system can be propagated to their second step, yielding an overall improvement in positive /negative/neutral (Ps/Ng/N) classification." ></td>
	<td class="line x" title="206:249	The sense-aware variant of the overall two-part system is the same as the original except that we apply R4 to the output of the first step (flipping some of the neutral/polar classifiers P labels to N)." ></td>
	<td class="line x" title="207:249	Thus, since the second step in Wilson et al.s classifier processes only those instances labeled P in the first step, in the sense-aware system, fewer instances are passed from the first to the second step." ></td>
	<td class="line x" title="208:249	Table 5 reports results for the original system (OPs/Ng/N) and the sense-aware variant (R4)." ></td>
	<td class="line x" title="209:249	These results are for the entire SenMPQA dataset, not just those labeled P in the first step." ></td>
	<td class="line x" title="210:249	The accuracy improves 3 percentage points (a 13.4% error reduction)." ></td>
	<td class="line x" title="211:249	The improvement in accuracy is statistically significant at the p < .01 level with McNemars test." ></td>
	<td class="line x" title="212:249	We see the real benefit when we look at the precision of the positive and negative classes." ></td>
	<td class="line x" title="213:249	Negative precision goes from 60.4 to 82.1 and positive precision goes from 52.2 to 68.6, with no loss in recall." ></td>
	<td class="line x" title="214:249	This is evidence that the SWSD system is doing a good job of removing some false hits of subjectivity clues that harm the original version of the system." ></td>
	<td class="line x" title="215:249	5 Comparisons to Previous Work Several researchers exploit lexical resources for contextual subjectivity and sentiment analysis." ></td>
	<td class="line x" title="216:249	These systems typically look for the presence of subjective or sentiment-bearing words in the text." ></td>
	<td class="line x" title="217:249	They may rely only on this information (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003)), or they may combine it with additional information as well (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Wilson et al., 2005a))." ></td>
	<td class="line x" title="218:249	We apply SWSD to some of those systems to show the effect of SWSD on contextual subjectivity and sentiment analysis." ></td>
	<td class="line x" title="219:249	Another set of related work is on subjectivity and polarity labeling of word senses (e.g.(Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Su and Markert, 2008))." ></td>
	<td class="line x" title="221:249	They label senses of words in a dictionary." ></td>
	<td class="line x" title="222:249	In comparison, we label senses of word instances in a corpus." ></td>
	<td class="line x" title="223:249	Moreover, our work extends findings in (Wiebe and Mihalcea, 2006) and (Su and Markert, 2008)." ></td>
	<td class="line x" title="224:249	(Wiebe and Mihalcea, 2006) demonstrates that subjectivity is a property that can be associated with word senses." ></td>
	<td class="line x" title="225:249	We show that it is a natural grouping of word senses and that it provides a principled way for clustering senses." ></td>
	<td class="line x" title="226:249	They also demonstrate that subjectivity helps with WSD." ></td>
	<td class="line x" title="227:249	We show that a coarse-grained WSD variant (SWSD) helps with subjectivity and sentiment analysis." ></td>
	<td class="line x" title="228:249	Both (Wiebe and Mihalcea, 2006) and (Su and Markert, 2008) show that even reliable subjectivity clues have objective senses." ></td>
	<td class="line x" title="229:249	We demonstrate that this ambiguity is also prevalent in a corpus." ></td>
	<td class="line x" title="230:249	Several researchers (e.g., (Palmer et al., 2004; Navigli, 2006; Snow et al., 2007; Hovy et al., 2006)) work on reducing the granularity of sense inventories for WSD." ></td>
	<td class="line x" title="231:249	They aim for a more coarsegrained sense inventory to overcome performance shortcomings related to fine-grained sense distinctions." ></td>
	<td class="line x" title="232:249	Our work is similar in the sense that we reduce all senses of a word to two senses (S/O)." ></td>
	<td class="line x" title="233:249	The difference is the criterion driving the grouping." ></td>
	<td class="line x" title="234:249	Related work concentrates on syntactic and semantic similarity between senses to group them." ></td>
	<td class="line x" title="235:249	In contrast, our grouping is driven by subjectivity with a specific application area in mind, namely subjectivity and sentiment analysis." ></td>
	<td class="line x" title="236:249	6 Conclusions and Future Work We introduced the task of subjectivity word sense disambiguation (SWSD), and evaluated a supervised method inspired by research in WSD." ></td>
	<td class="line x" title="237:249	The 197 system achieves high accuracy, especially on highly ambiguous words, and substantially outperforms WSD on the same data." ></td>
	<td class="line x" title="238:249	The positive results provide evidence that SWSD is a feasible variant of WSD, and that the S/O sense groupings are natural ones." ></td>
	<td class="line x" title="239:249	We also explored the promise of SWSD for contextual subjectivity analysis." ></td>
	<td class="line x" title="240:249	We showed that a subjectivity lexicon can have substantial coverage of the subjective expressions in the corpus, yet still be responsible for significant sense ambiguity." ></td>
	<td class="line x" title="241:249	This demonstrates the potential benefit to opinion analysis of performing SWSD." ></td>
	<td class="line x" title="242:249	We then exploit SWSD in several contextual opinion analysis systems, including positive/negative/neutral sentiment classification." ></td>
	<td class="line x" title="243:249	Improvements in performance were realized for all of the systems." ></td>
	<td class="line x" title="244:249	We plan several future directions which promise to further increase the impact of SWSD on subjectivity and sentiment analysis." ></td>
	<td class="line x" title="245:249	We will manually annotate a moderate number of strategically chosen words, namely frequent ones which are highly ambiguous." ></td>
	<td class="line x" title="246:249	In addition, we will add features to the SWSD system reflecting the subjectivity of the surrounding context." ></td>
	<td class="line x" title="247:249	Finally, there are more sophisticated strategies to explore for improving subjectivity and sentiment analysis via SWSD than the simple, intuitive rules we began with in this paper." ></td>
	<td class="line x" title="248:249	Acknowledgments This material is based in part upon work supported by National Science Foundation awards #0840632 and #0840608." ></td>
	<td class="line x" title="249:249	Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E09-1004
Contextual Phrase-Level Polarity Analysis Using Lexical Affect Scoring and Syntactic N-Grams
Agarwal, Apoorv;Biadsy, Fadi;Mckeown, Kathleen;"></td>
	<td class="line x" title="1:247	Proceedings of the 12th Conference of the European Chapter of the ACL, pages 2432, Athens, Greece, 30 March  3 April 2009." ></td>
	<td class="line x" title="2:247	c2009 Association for Computational Linguistics Contextual Phrase-Level Polarity Analysis using Lexical Affect Scoring and Syntactic N-grams Apoorv Agarwal Department of Computer Science Columbia University New York, USA aa2644@columbia.edu Fadi Biadsy Department of Computer Science Columbia University New York, USA fadi@cs.columbia.edu Kathleen R. Mckeown Department of Computer Science Columbia University New York, USA kathy@cs.columbia.edu Abstract We present a classifier to predict contextual polarity of subjective phrases in a sentence." ></td>
	<td class="line x" title="3:247	Our approach features lexical scoring derived from the Dictionary of Affect in Language (DAL) and extended through WordNet, allowing us to automatically score the vast majority of words in our input avoiding the need for manual labeling." ></td>
	<td class="line x" title="4:247	We augment lexical scoring with n-gram analysis to capture the effect of context." ></td>
	<td class="line x" title="5:247	We combine DAL scores with syntactic constituents and then extract ngrams of constituents from all sentences." ></td>
	<td class="line x" title="6:247	We also use the polarity of all syntactic constituents within the sentence as features." ></td>
	<td class="line x" title="7:247	Our results show significant improvement over a majority class baseline as well as a more difficult baseline consisting of lexical n-grams." ></td>
	<td class="line x" title="8:247	1 Introduction Sentiment analysis is a much-researched area that deals with identification of positive, negative and neutral opinions in text." ></td>
	<td class="line x" title="9:247	The task has evolved from document level analysis to sentence and phrasal level analysis." ></td>
	<td class="line x" title="10:247	Whereas the former is suitable for classifying news (e.g., editorials vs. reports) into positive and negative, the latter is essential for question-answering and recommendation systems." ></td>
	<td class="line x" title="11:247	A recommendation system, for example, must be able to recommend restaurants (or movies, books, etc.) based on a variety of features such as food, service or ambience." ></td>
	<td class="line x" title="12:247	Any single review sentence may contain both positive and negative opinions, evaluating different features of a restaurant." ></td>
	<td class="line x" title="13:247	Consider the following sentence (1) where the writer expresses opposing sentiments towards food and service of a restaurant." ></td>
	<td class="line x" title="14:247	In tasks such as this, therefore, it is important that sentiment analysis be done at the phrase level." ></td>
	<td class="line x" title="15:247	(1) The Taj has great food but I found their service to be lacking." ></td>
	<td class="line x" title="16:247	Subjective phrases in a sentence are carriers of sentiments in which an experiencer expresses an attitude, often towards a target." ></td>
	<td class="line x" title="17:247	These subjective phrases may express neutral or polar attitudes depending on the context of the sentence in which they appear." ></td>
	<td class="line x" title="18:247	Context is mainly determined by content and structure of the sentence." ></td>
	<td class="line x" title="19:247	For example, in the following sentence (2), the underlined subjective phrase seems to be negative, but in the larger context of the sentence, it is positive.1 (2) The robber entered the store but his efforts were crushed when the police arrived on time." ></td>
	<td class="line x" title="20:247	Our task is to predict contextual polarity of subjective phrases in a sentence." ></td>
	<td class="line x" title="21:247	A traditional approach to this problem is to use a prior polarity lexicon of words to first set priors on target phrases and then make use of the syntactic and semantic information in and around the sentence to make the final prediction." ></td>
	<td class="line x" title="22:247	As in earlier approaches, we also use a lexicon to set priors, but we explore new uses of a Dictionary of Affect in Language (DAL) (Whissel, 1989) extended using WordNet (Fellbaum, 1998)." ></td>
	<td class="line x" title="23:247	We augment this approach with n-gram analysis to capture the effect of context." ></td>
	<td class="line x" title="24:247	We present a system for classification of neutral versus positive versus negative and positive versus negative polarity (as is also done by (Wilson et al., 2005))." ></td>
	<td class="line x" title="25:247	Our approach is novel in the use of following features:  Lexical scores derived from DAL and extended through WordNet: The Dictionary of Affect has been widely used to aid in interpretation of emotion in speech (Hirschberg 1We assign polarity to phrases based on Wiebe (Wiebe et al., 2005); the polarity of all examples shown here is drawn from annnotations in the MPQA corpus." ></td>
	<td class="line x" title="26:247	Clearly the assignment of polarity chosen in this corpus depends on general cultural norms." ></td>
	<td class="line x" title="27:247	24 et al., 2005)." ></td>
	<td class="line x" title="28:247	It contains numeric scores assigned along axes of pleasantness, activeness and concreteness." ></td>
	<td class="line x" title="29:247	We introduce a method for setting numerical priors on words using these three axes, which we refer to as a scoring scheme throughout the paper." ></td>
	<td class="line x" title="30:247	This scheme has high coverage of the phrases for classification and requires no manual intervention when tagging words with prior polarities." ></td>
	<td class="line x" title="31:247	 N-gram Analysis: exploiting automatically derived polarity of syntactic constituents We compute polarity for each syntactic constituent in the input phrase using lexical affect scores for its words and extract n-grams over these constituents." ></td>
	<td class="line x" title="32:247	N-grams of syntactic constituents tagged with polarity provide patterns that improve prediction of polarity for the subjective phrase." ></td>
	<td class="line x" title="33:247	 Polarity of Surrounding Constituents: We use the computed polarity of syntactic constituents surrounding the phrase we want to classify." ></td>
	<td class="line x" title="34:247	These features help to capture the effect of context on the polarity of the subjective phrase." ></td>
	<td class="line x" title="35:247	We show that classification of subjective phrases using our approach yields better accuracy than two baselines, a majority class baseline and a more difficult baseline of lexical n-gram features." ></td>
	<td class="line x" title="36:247	We also provide an analysis of how the different component DAL scores contribute to our results through the introduction of a norm that combines the component scores, separating polar words that are less subjective (e.g., Christmas , murder) from neutral words that are more subjective (e.g., most, lack)." ></td>
	<td class="line x" title="37:247	Section 2 presents an overview of previous work, focusing on phrasal level sentiment analysis." ></td>
	<td class="line x" title="38:247	Section 3 describes the corpus and the gold standard we used for our experiments." ></td>
	<td class="line x" title="39:247	In section 4, we give a brief description of DAL, discussing its utility and previous uses for emotion and for sentiment analysis." ></td>
	<td class="line x" title="40:247	Section 5 presents, in detail, our polarity classification framework." ></td>
	<td class="line x" title="41:247	Here we describe our scoring scheme and the features we extract from sentences for classification tasks." ></td>
	<td class="line x" title="42:247	Experimental set-up and results are presented in Section 6." ></td>
	<td class="line x" title="43:247	We conclude with Section 7 where we also look at future directions for this research." ></td>
	<td class="line oc" title="44:247	2 Literature Survey The task of sentiment analysis has evolved from document level analysis (e.g., (Turney., 2002); (Pang and Lee, 2004)) to sentence level analysis (e.g., (Hu and Liu., 2004); (Kim and Hovy., 2004); (Yu and Hatzivassiloglou, 2003))." ></td>
	<td class="line o" title="45:247	These researchers first set priors on words using a prior polarity lexicon." ></td>
	<td class="line x" title="46:247	When classifying sentiment at the sentence level, other types of clues are also used, including averaging of word polarities or models for learning sentence sentiment." ></td>
	<td class="line x" title="47:247	Research on contextual phrasal level sentiment analysis was pioneered by Nasukawa and Yi (2003), who used manually developed patterns to identify sentiment." ></td>
	<td class="line x" title="48:247	Their approach had high precision, but low recall." ></td>
	<td class="line x" title="49:247	Wilson et al., (2005) also explore contextual phrasal level sentiment analysis, using a machine learning approach that is closer to the one we present." ></td>
	<td class="line x" title="50:247	Both of these researchers also follow the traditional approach and first set priors on words using a prior polarity lexicon." ></td>
	<td class="line x" title="51:247	Wilson et al.(2005) use a lexicon of over 8000 subjectivity clues, gathered from three sources ((Riloff and Wiebe, 2003); (Hatzivassiloglou and McKeown, 1997) and The General Inquirer2)." ></td>
	<td class="line x" title="53:247	Words that were not tagged as positive or negative were manually labeled." ></td>
	<td class="line x" title="54:247	Yi et al.(2003) acquired words from GI, DAL and WordNet." ></td>
	<td class="line x" title="56:247	From DAL, only words whose pleasantness score is one standard deviation away from the mean were used." ></td>
	<td class="line x" title="57:247	Nasukawa as well as other researchers (Kamps and Marx, 2002)) also manually tag words with prior polarities." ></td>
	<td class="line x" title="58:247	All of these researchers use categorical tags for prior lexical polarity; in contrast, we use quantitative scores, making it possible to use them in computation of scores for the full phrase." ></td>
	<td class="line x" title="59:247	While Wilson et al.(2005) aim at phrasal level analysis, their system actually only gives each clue instance its own label [p. 350]." ></td>
	<td class="line x" title="61:247	Their gold standard is also at the clue level and assigns a value based on the clues appearance in different expressions (e.g., if a clue appears in a mixture of negative and neutral expressions, its class is negative)." ></td>
	<td class="line x" title="62:247	They note that they do not determine subjective expression boundaries and for this reason, they classify at the word level." ></td>
	<td class="line x" title="63:247	This approach is quite different from ours, as we compute the polarity of the full phrase." ></td>
	<td class="line x" title="64:247	The average length of the subjective phrases in the corpus was 2.7 words, with a standard deviation of 2.3." ></td>
	<td class="line x" title="65:247	Like Wilson et al. 2http://www.wjh.harvard.edu/ inquirer 25 (2005) we do not attempt to determine the boundary of subjective expressions; we use the labeled boundaries in the corpus." ></td>
	<td class="line x" title="66:247	3 Corpus We used the Multi-Perspective QuestionAnswering (MPQA version 1.2) Opinion corpus (Wiebe et al., 2005) for our experiments." ></td>
	<td class="line x" title="67:247	We extracted a total of 17,243 subjective phrases annotated for contextual polarity from the corpus of 535 documents (11,114 sentences)." ></td>
	<td class="line x" title="68:247	These subjective phrases are either direct subjective or expressive subjective." ></td>
	<td class="line x" title="69:247	Direct subjective expressions are explicit mentions of a private state (Quirk et al., 1985) and are much easier to classify." ></td>
	<td class="line x" title="70:247	Expressive subjective phrases are indirect or implicit mentions of private states and therefore are harder to classify." ></td>
	<td class="line x" title="71:247	Approximately one third of the phrases we extracted were direct subjective with non-neutral expressive intensity whereas the rest of the phrases were expressive subjective." ></td>
	<td class="line x" title="72:247	In terms of polarity, there were 2779 positive, 6471 negative and 7993 neutral expressions." ></td>
	<td class="line x" title="73:247	Our Gold Standard is the manual annotation tag given to phrases in the corpus." ></td>
	<td class="line x" title="74:247	4 DAL DAL is an English language dictionary built to measure emotional meaning of texts." ></td>
	<td class="line x" title="75:247	The samples employed to build the dictionary were gathered from different sources such as interviews, adolescents descriptions of their emotions and university students essays." ></td>
	<td class="line x" title="76:247	Thus, the 8742 word dictionary is broad and avoids bias from any one particular source." ></td>
	<td class="line x" title="77:247	Each word is given three kinds of scores (pleasantness  also called evaluation, ee, activeness, aa and imagery, ii) on a scale of 1 (low) to 3 (high)." ></td>
	<td class="line x" title="78:247	Pleasantness is a measure of polarity." ></td>
	<td class="line x" title="79:247	For example, in Table 1, affection is given a pleasantness score of 2.77 which is closer to 3.0 and is thus a highly positive word." ></td>
	<td class="line x" title="80:247	Likewise, activeness is a measure of the activation or arousal level of a word, which is apparent from the activeness scores of slug and energetic in the table." ></td>
	<td class="line x" title="81:247	The third score, imagery, is a measure of the ease with which a word forms a mental picture." ></td>
	<td class="line x" title="82:247	For example, affect cannot be imagined easily and therefore has a score closer to 1, as opposed to flower which is a very concrete and therefore has an imagery score of 3." ></td>
	<td class="line x" title="83:247	A notable feature of the dictionary is that it has different scores for various inflectional forms of a word ( affect and affection) and thus, morphological parsing, and the possibility of resulting errors, is avoided." ></td>
	<td class="line x" title="84:247	Moreover, Cowie et al., (2001) showed that the three scores are uncorrelated; this implies that each of the three scores provide complementary information." ></td>
	<td class="line x" title="85:247	Word ee aa ii Affect 1.75 1.85 1.60 Affection 2.77 2.25 2.00 Slug 1.00 1.18 2.40 Energetic 2.25 3.00 3.00 Flower 2.75 1.07 3.00 Table 1: DAL scores for words The dictionary has previously been used for detecting deceptive speech (Hirschberg et al., 2005) and recognizing emotion in speech (Athanaselis et al., 2006)." ></td>
	<td class="line x" title="86:247	5 The Polarity Classification Framework In this section, we present our polarity classification framework." ></td>
	<td class="line x" title="87:247	The system takes a sentence marked with a subjective phrase and identifies the most likely contextual polarity of this phrase." ></td>
	<td class="line x" title="88:247	We use a logistic regression classifier, implemented in Weka, to perform two types of classification: Three way (positive, negative, vs. neutral) and binary (positive vs. negative)." ></td>
	<td class="line x" title="89:247	The features we use for classification can be broadly divided into three categories: I. Prior polarity features computed from DAL and augmented using WordNet (Section 5.1)." ></td>
	<td class="line x" title="90:247	II." ></td>
	<td class="line x" title="91:247	lexical features including POS and word n-gram features (Section 5.3), and III." ></td>
	<td class="line x" title="92:247	the combination of DAL scores and syntactic features to allow both n-gram analysis and polarity features of neighbors (Section 5.4)." ></td>
	<td class="line x" title="93:247	5.1 Scoring based on DAL and WordNet DAL is used to assign three prior polarity scores to each word in a sentence." ></td>
	<td class="line x" title="94:247	If a word is found in DAL, scores of pleasantness (ee), activeness (aa), and imagery (ii) are assigned to it." ></td>
	<td class="line x" title="95:247	Otherwise, a list of the words synonyms and antonyms is created using WordNet." ></td>
	<td class="line x" title="96:247	This list is sequentially traversed until a match is found in DAL or the list ends, in which case no scores are assigned." ></td>
	<td class="line x" title="97:247	For example, astounded, a word absent in DAL, was scored by using its synonym amazed." ></td>
	<td class="line x" title="98:247	Similarly, in-humane was scored using the reverse polarity of 26 its antonym humane, present in DAL." ></td>
	<td class="line x" title="99:247	These scores are Z-Normalized using the mean and standard deviation measures given in the dictionarys manual (Whissel, 1989)." ></td>
	<td class="line x" title="100:247	It should be noted that in our current implementation all function words are given zero scores since they typically do not demonstrate any polarity." ></td>
	<td class="line x" title="101:247	The next step is to boost these normalized scores depending on how far they lie from the mean." ></td>
	<td class="line x" title="102:247	The reason for doing this is to be able to differentiate between phrases like fairly decent advice and excellent advice." ></td>
	<td class="line x" title="103:247	Without boosting, the pleasantness scores of both phrases are almost the same." ></td>
	<td class="line x" title="104:247	To boost the score, we multiply it by the number of standard deviations it lies from the mean." ></td>
	<td class="line x" title="105:247	After the assignment of scores to individual words, we handle local negations in a sentence by using a simple finite state machine with two states: RETAIN and INVERT." ></td>
	<td class="line x" title="106:247	In the INVERT state, the sign of the pleasantness score of the current word is inverted, while in the RETAIN state the sign of the score stays the same." ></td>
	<td class="line x" title="107:247	Initially, the first word in a given sentence is fed to the RETAIN state." ></td>
	<td class="line x" title="108:247	When a negation (e.g., not, no, never, cannot, didnt) is encountered, the state changes to the INVERT state." ></td>
	<td class="line x" title="109:247	While in the INVERT state, if but is encountered, it switches back to the RETAIN state." ></td>
	<td class="line x" title="110:247	In this machine we also take care of not only which serves as an intensifier rather than negation (Wilson et al., 2005)." ></td>
	<td class="line x" title="111:247	To handle phrases like no better than evil and could not be clearer, we also switch states from INVERT to RETAIN when a comparative degree adjective is found after not." ></td>
	<td class="line x" title="112:247	For example, the words in phrase in Table (2) are given positive pleasantness scores labeled with positive prior polarity." ></td>
	<td class="line x" title="113:247	Phrase has no greater desire POS VBZ DT JJR NN (ee) 0 0 3.37 0.68 State RETAIN INVERT RETAIN RETAIN Table 2: Example of scoring scheme using DAL We observed that roughly 74% of the content words in the corpus were directly found in DAL." ></td>
	<td class="line x" title="114:247	Synonyms of around 22% of the words in the corpus were found to exist in DAL." ></td>
	<td class="line x" title="115:247	Antonyms of only 1% of the words in the corpus were found in DAL." ></td>
	<td class="line x" title="116:247	Our system failed to find prior semantic orientations of roughly 3% of the total words in the corpus." ></td>
	<td class="line x" title="117:247	These were rarely occurring words like apartheid, apocalyptic and ulterior." ></td>
	<td class="line x" title="118:247	We assigned zero scores for these words." ></td>
	<td class="line x" title="119:247	In our system, we assign three DAL scores, using the above scheme, for the subjective phrase in a given sentence." ></td>
	<td class="line x" title="120:247	The features are (1) ee, the mean of the pleasantness scores of the words in the phrase, (2) aa, the mean of the activeness scores of the words in the phrase, and similarly (3) ii, the mean of the imagery scores." ></td>
	<td class="line x" title="121:247	5.2 Norm We gave each phrase another score, which we call the norm, that is a combination of the three scores from DAL." ></td>
	<td class="line x" title="122:247	Cowie et al.(2001) suggest a mechanism of mapping emotional states to a 2-D continuous space using an Activation-Evaluation space (AE) representation." ></td>
	<td class="line x" title="124:247	This representation makes use of the pleasantness and activeness scores from DAL and divides the space into four quadrants: delightful, angry, serene, and depressed." ></td>
	<td class="line x" title="125:247	Whissel (2008), observes that tragedies, which are easily imaginable in general, have higher imagery scores than comedies." ></td>
	<td class="line x" title="126:247	Drawing on these approaches and our intuition that neutral expressions tend to be more subjective, we define the norm in the following equation (1)." ></td>
	<td class="line x" title="127:247	norm= ee2 +aa2 ii (1) Words of interest to us may fall into the following four broad categories: 1." ></td>
	<td class="line x" title="128:247	High AE score and high imagery: These are words that are highly polar and less subjective (e.g., angel and lively)." ></td>
	<td class="line x" title="129:247	2." ></td>
	<td class="line x" title="130:247	Low AE score and low imagery: These are highly subjective neutral words (e.g., generally and ordinary)." ></td>
	<td class="line x" title="131:247	3." ></td>
	<td class="line x" title="132:247	High AE score and low imagery: These are words that are both highly polar and subjective (e.g., succeed and good)." ></td>
	<td class="line x" title="133:247	4." ></td>
	<td class="line x" title="134:247	Low AE score and high imagery: These are words that are neutral and easily imaginable (e.g., car and door)." ></td>
	<td class="line x" title="135:247	It is important to differentiate between these categories of words, because highly subjective words may change orientation depending on context; less subjective words tend to retain their prior orientation." ></td>
	<td class="line x" title="136:247	For instance, in the example sentence from Wilson et al.(2005)., the underlined phrase 27 seems negative, but in the context it is positive." ></td>
	<td class="line x" title="137:247	Since a subjective word like succeed depends on what one succeeds in, it may change its polarity accordingly." ></td>
	<td class="line x" title="138:247	In contrast, less subjective words, like angel, do not depend on the context in which they are used; they evoke the same connotation as their prior polarity." ></td>
	<td class="line x" title="139:247	(3) They havent succeeded and will never succeed in breaking the will of this valiant people." ></td>
	<td class="line x" title="140:247	As another example, AE space scores of goodies and good turn out to be the same." ></td>
	<td class="line x" title="141:247	What differentiates one from the another is the imagery score, which is higher for the former." ></td>
	<td class="line x" title="142:247	Therefore, value of the norm is lower for goodies than for good." ></td>
	<td class="line x" title="143:247	Unsurprisingly, this feature always appears in the top 10 features when the classification task contains neutral expressions as one of the classes." ></td>
	<td class="line x" title="144:247	5.3 Lexical Features We extract two types of lexical features, part of speech (POS) tags and n-gram word features." ></td>
	<td class="line x" title="145:247	We count the number of occurrences of each POS in the subjective phrase and represent each POS as an integer in our feature vector.3 For each subjective phrase, we also extract a subset of unigram, bigrams, and trigrams of words (selected automatically, see Section 6)." ></td>
	<td class="line x" title="146:247	We represent each n-gram feature as a binary feature." ></td>
	<td class="line x" title="147:247	These types of features were used to approximate standard n-gram language modeling (LM)." ></td>
	<td class="line x" title="148:247	In fact, we did experiment with a standard trigram LM, but found that it did not improve performance." ></td>
	<td class="line x" title="149:247	In particular, we trained two LMs, one on the polar subjective phrases and another on the neutral subjective phrases." ></td>
	<td class="line x" title="150:247	Given a sentence, we computed two perplexities of the two LMs on the subjective phrase in the sentence and added them as features in our feature vectors." ></td>
	<td class="line x" title="151:247	This procedure provided us with significant improvement over a chance baseline but did not outperform our current system." ></td>
	<td class="line x" title="152:247	We speculate that this was caused by the split of training data into two parts, one for training the LMs and another for training the classifier." ></td>
	<td class="line x" title="153:247	The resulting small quantity of training data may be the reason for bad performance." ></td>
	<td class="line x" title="154:247	Therefore, we decided to back off to only binary n-gram features as part of our feature vector." ></td>
	<td class="line x" title="155:247	3We use the Stanford Tagger to assign parts of speech tags to sentences." ></td>
	<td class="line x" title="156:247	(Toutanova and Manning, 2000) 5.4 Syntactic Features In this section, we show how we can combine the DAL scores with syntactic constituents." ></td>
	<td class="line x" title="157:247	This process involves two steps." ></td>
	<td class="line x" title="158:247	First, we chunk each sentence to its syntactic constituents (NP, VP, PP, JJP, and Other) using a CRF Chunker.4 If the marked-up subjective phrase does not contain complete chunks (i.e., it partially overlaps with other chunks), we expand the subjective phrase to include the chunks that it overlaps with." ></td>
	<td class="line x" title="159:247	We term this expanded phrase as the target phrase, see Figure 1." ></td>
	<td class="line x" title="160:247	Second, each chunk in a sentence is then assigned a 2-D AE space score as defined by Cowie et al., (2001) by adding the individual AE space scores of all the words in the chunk and then normalizing it by the number of words." ></td>
	<td class="line x" title="161:247	At this point, we are only concerned with the polarity of the chunk (i.e., whether it is positive or negative or neutral) and imagery will not help in this task; the AE space score is determined from pleasantness and activeness alone." ></td>
	<td class="line x" title="162:247	A threshold, determined empirically by analyzing the distributions of positive (pos), negative (neg) and neutral (neu) expressions, is used to define ranges for these classes of expressions." ></td>
	<td class="line x" title="163:247	This enables us to assign each chunk a prior semantic polarity." ></td>
	<td class="line x" title="164:247	Having the semantic orientation (positive, negative, neutral) and phrasal tags, the sentence is then converted to a sequence of encodings [PhrasalTag]polarity." ></td>
	<td class="line x" title="165:247	We mark each phrase that we want to classify as a target to differentiate it from the other chunks and attach its encoding." ></td>
	<td class="line x" title="166:247	As mentioned, if the target phrase partially overlaps with chunks, it is simply expanded to subsume the chunks." ></td>
	<td class="line x" title="167:247	This encoding is illustrated in Figure 1." ></td>
	<td class="line x" title="168:247	After these two steps, we extract a set of features that are used in classifying the target phrase." ></td>
	<td class="line x" title="169:247	These include n-grams of chunks from the all sentences, minimum and maximum pleasantness scores from the chunks in the target phrase itself, and the syntactic categories that occur in the context of the target phrase." ></td>
	<td class="line x" title="170:247	In the remainder of this section, we describe how these features are extracted." ></td>
	<td class="line x" title="171:247	We extract unigrams, bigrams and trigrams of chunks from all the sentences." ></td>
	<td class="line x" title="172:247	For example, we may extract a bigram from Figure 1 of [VP]neu followed by [PP]targetneg . Similar to the lexical 4Xuan-Hieu Phan, CRFChunker: CRF English Phrase Chunker, http://crfchunker.sourceforge.net/, 2006." ></td>
	<td class="line x" title="173:247	28 !''# !'# $%&($ !'#$ %& !'#$ %& !''# $%& !'#$%&()%*+,-./% !'#$%&()*+,+ -%." ></td>
	<td class="line x" title="174:247	&$%,+-%.#-'%)&#,()$ %* (/+,&0(%12%-'+%# +3&0(&4%,+/#&5% ! !" ></td>
	<td class="line x" title="175:247	# Figure 1: Converting a sentence with a subjective phrase to a sequence of chunks with their types and polarities n-grams, for the sentence containing the target phrase, we add binary values in our feature vector such that the value is 1 if the sentence contains that chunk n-gram." ></td>
	<td class="line x" title="176:247	We also include two features related to the target phrase." ></td>
	<td class="line x" title="177:247	The target phrase often consists of many chunks." ></td>
	<td class="line x" title="178:247	To detect if a chunk of the target phrase is highly polar, minimum and maximum pleasantness scores over all the chunks in the target phrase are noted." ></td>
	<td class="line x" title="179:247	In addition, we add features which attempt to capture contextual information using the prior semantic polarity assigned to each chunk both within the target phrase itself and within the context of the target phrase." ></td>
	<td class="line x" title="180:247	In cases where the target phrase is in the beginning of the sentence or at the end, we simply assign zero scores." ></td>
	<td class="line x" title="181:247	Then we compute the frequency of each syntactic type (i.e., NP, VP, PP, JJP) and polarity (i.e., positive, negative, neutral) to the left of the target, to the right of the target and for the target." ></td>
	<td class="line x" title="182:247	This additional set of contextual features yields 36 features in total: three polarities:{positive, negative, neutral}* three contexts: {left, target, right} * four chunk syntactic types: {NP, VP, PP, JJP}." ></td>
	<td class="line x" title="183:247	The full set of features captures different types of information." ></td>
	<td class="line x" title="184:247	N-grams look for certain patterns that may be specific to either polar or neutral sentiments." ></td>
	<td class="line x" title="185:247	Minimum and maximum scores capture information about the target phrase standalone." ></td>
	<td class="line x" title="186:247	The last set of features incorporate information about the neighbors of the target phrase." ></td>
	<td class="line x" title="187:247	We performed feature selection on this full set of n-gram related features and thus, a small subset of these n-gram related features, selected automatically (see section 6) were used in the experiments." ></td>
	<td class="line x" title="188:247	6 Experiments and Results Subjective phrases from the MPQA corpus were used in 10-fold cross-validation experiments." ></td>
	<td class="line x" title="189:247	The MPQA corpus includes gold standard tags for each Feature Types Accuracy Pos.* Neg.* Neu.* Chance baseline 33.33% N-gram baseline 59.05% 0.602 0.578 0.592 DAL scores only 59.66% 0.635 0.635 0.539 + POS 60.55% 0.621 0.542 0.655 + Chunks 64.72% 0.681 0.665 0.596 + N-gram (all) 67.51% 0.703 0.688 0.632 All (unbalanced) 70.76% 0.582 0.716 0.739 Table 3: Results of 3 way classification (Positive, Negative, and Neutral)." ></td>
	<td class="line x" title="190:247	In the unbalanced case, majority class baseline is 46.3% (*F-Measure)." ></td>
	<td class="line x" title="191:247	Feature Types Accuracy Pos.* Neg.* Chance baseline 50% N-gram baseline 73.21% 0.736 0.728 DAL scores only 77.02% 0.763 0.728 + POS 79.02% 0.788 0.792 + Chunks 80.72% 0.807 0.807 + N-gram (all) 82.32% 0.802 0.823 All (unbalanced) 84.08% 0.716 0.889 Table 4: Positive vs. Negative classification results." ></td>
	<td class="line x" title="192:247	Baseline is the majority class." ></td>
	<td class="line x" title="193:247	In the unbalanced case, majority class baseline is 69.74%." ></td>
	<td class="line x" title="194:247	(* F-Measure) phrase." ></td>
	<td class="line x" title="195:247	A logistic classifier was used for two polarity classification tasks, positive versus negative versus neutral and positive versus negative." ></td>
	<td class="line x" title="196:247	We report accuracy, and F-measure for both balanced and unbalanced data." ></td>
	<td class="line x" title="197:247	6.1 Positive versus Negative versus Neutral Table 3 shows results for a 3-way classifier." ></td>
	<td class="line x" title="198:247	For the balanced data-set, each class has 2799 instances and hence the chance baseline is 33%." ></td>
	<td class="line x" title="199:247	For the unbalanced data-set, there are 2799 instances of positive, 6471 instances of negative and 7993 instances of neutral phrases and thus the baseline is about 46%." ></td>
	<td class="line x" title="200:247	Results show that the accuracy increases as more features are added." ></td>
	<td class="line x" title="201:247	It may be seen from the table that prior polarity scores do not do well alone, but when used in conjunction with other features they play an important role in achieving an accuracy much higher than both baselines (chance and lexical n-grams)." ></td>
	<td class="line x" title="202:247	To re29 Figure 2: (a) An example sentence with three annotated subjective phrases in the same sentence." ></td>
	<td class="line x" title="203:247	(b) Part of the sentence with the target phrase (B) and their chunks with prior polarities." ></td>
	<td class="line x" title="204:247	confirm if prior polarity scores add value, we experimented by using all features except the prior polarity scores and noticed a drop in accuracy by about 4%." ></td>
	<td class="line x" title="205:247	This was found to be true for the other classification task as well." ></td>
	<td class="line x" title="206:247	The table shows that parts of speech and lexical n-grams are good features." ></td>
	<td class="line x" title="207:247	A significant improvement in accuracy (over 4%, p-value = 4.2e-15) is observed when chunk features (i.e., n-grams of constituents and polarity of neighboring constituents) are used in conjunction with prior polarity scores and part of speech features.5 This improvement may be explained by the following observation." ></td>
	<td class="line x" title="208:247	The bigram [Other]targetneu [NP]neu was selected as a top feature by the Chi-square feature selector." ></td>
	<td class="line x" title="209:247	So were unigrams, [Other]targetneu and [Other]targetneg . We thus learned n-gram patterns that are characteristic of neutral expressions (the just mentioned bigram and the first of the unigrams) as well as a pattern found mostly in negative expressions (the latter unigram)." ></td>
	<td class="line x" title="210:247	It was surprising to find another top chunk feature, the bigram [Other]targetneu [NP]neg (i.e., a neutral chunk of syntactic type Other preceding a negative noun phrase), present in neutral expressions six times more than in polar expressions." ></td>
	<td class="line x" title="211:247	An instance where these chunk features could have been responsible for the correct prediction of a target phrase is shown in Figure 2." ></td>
	<td class="line x" title="212:247	Figure 2(a) shows an example sentence from the MPQA corpus, which has three annotated subjective phrases." ></td>
	<td class="line x" title="213:247	The manually labeled polarity of phrases (A) and (C) is negative and that of (B) is neutral." ></td>
	<td class="line x" title="214:247	Figure 2(b) shows the 5We use the binomial test procedure to test statistical significance throughout the paper." ></td>
	<td class="line x" title="215:247	relevant chunk bigram which is used to predict the contextual polarity of the target phrase (B)." ></td>
	<td class="line x" title="216:247	It was interesting to see that the top 10 features consisted of all categories (i.e., prior DAL scores, lexical n-grams and POS, and syntactic) of features." ></td>
	<td class="line x" title="217:247	In this and the other experiment, pleasantness, activation and the norm were among the top 5 features." ></td>
	<td class="line x" title="218:247	We ran a significance test to show the importance of the norm feature in our classification task and observed that it exerted a significant increase in accuracy (2.26%, p-value = 1.45e-5)." ></td>
	<td class="line x" title="219:247	6.2 Positive versus Negative Table 4 shows results for positive versus negative classification." ></td>
	<td class="line x" title="220:247	We show results for both balanced and unbalanced data-sets." ></td>
	<td class="line x" title="221:247	For balanced, there are 2779 instances of each class." ></td>
	<td class="line x" title="222:247	For the unbalanced data-set, there are 2779 instances of positive and 6471 instances of neutral, thus our chance baseline is around 70%." ></td>
	<td class="line x" title="223:247	As in the earlier classification, accuracy and F-measure increase as we add features." ></td>
	<td class="line x" title="224:247	While the increase of adding the chunk features, for example, is not as great as in the previous classification, it is nonetheless significant (p-value = 0.0018) in this classification task." ></td>
	<td class="line x" title="225:247	The smaller increase lends support to our hypothesis that polar expressions tend to be less subjective and thus are less likely to be affected by contextual polarity." ></td>
	<td class="line x" title="226:247	Another thing that supports our hypothesis that neutral expressions are more subjective is the fact that the rank of imagery (ii), dropped significantly in this classification task as compared to the previous classification task." ></td>
	<td class="line x" title="227:247	This implies that imagery has a much lesser role to play when we are dealing with non-neutral expressions." ></td>
	<td class="line x" title="228:247	30 7 Conclusion and Future Work We present new features (DAL scores, norm scores computed using DAL, n-gram over chunks with polarity) for phrasal level sentiment analysis." ></td>
	<td class="line x" title="229:247	They work well and help in achieving high accuracy in a three-way classification of positive, negative and neutral expressions." ></td>
	<td class="line x" title="230:247	We do not require any manual intervention during feature selection, and thus our system is fully automated." ></td>
	<td class="line x" title="231:247	We also introduced a 3-D representation that maps different classes to spatial coordinates." ></td>
	<td class="line x" title="232:247	It may seem to be a limitation of our system that it requires accurate expression boundaries." ></td>
	<td class="line x" title="233:247	However, this is not true for the following two reasons: first, Wiebe et al., (2005) declare that while marking the span of subjective expressions and hand annotating the MPQA corpus, the annotators were not trained to mark accurate expression boundaries." ></td>
	<td class="line x" title="234:247	The only constraint was that the subjective expression should be within the mark-ups for all annotators." ></td>
	<td class="line x" title="235:247	Second, we expanded the marked subjective phrase to subsume neighboring phrases at the time of chunking." ></td>
	<td class="line x" title="236:247	A limitation of our scoring scheme is that it does not handle polysemy, since words in DAL are not provided with their parts of speech." ></td>
	<td class="line x" title="237:247	Statistics show, however, that most words occurred with primarily one part of speech only." ></td>
	<td class="line x" title="238:247	For example, will occurred as modal 1272 times in the corpus, whereas it appeared 34 times as a noun." ></td>
	<td class="line x" title="239:247	The case is similar for like and just, which mostly occur as a preposition and an adverb, respectively." ></td>
	<td class="line x" title="240:247	Also, in our state machine, we havent accounted for the impact of connectives such as but or although; we propose drawing on work in argumentative orientation to do so ((Anscombre and Ducrot, 1983); (Elhadad and McKeown, 1990))." ></td>
	<td class="line x" title="241:247	For future work, it would be interesting to do subjectivity and intensity classification using the same scheme and features." ></td>
	<td class="line x" title="242:247	Particularly, for the task of subjectivity analysis, we speculate that the imagery score might be useful for tagging chunks with subjective and objective instead of positive, negative, and neutral." ></td>
	<td class="line x" title="243:247	Acknowledgments This work was supported by the National Science Foundation under the KDD program." ></td>
	<td class="line x" title="244:247	Any opinions, ndings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reect the views of the National Science Foundation." ></td>
	<td class="line x" title="245:247	score." ></td>
	<td class="line x" title="246:247	We would like to thank Julia Hirschberg for useful discussion." ></td>
	<td class="line x" title="247:247	We would also like to acknowledge Narayanan Venkiteswaran for implementing parts of the system and Amal El Masri, Ashleigh White and Oliver Elliot for their useful comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E09-1077
Semi-Supervised Polarity Lexicon Induction
Rao, Delip;Ravichandran, Deepak;"></td>
	<td class="line x" title="1:202	Proceedings of the 12th Conference of the European Chapter of the ACL, pages 675682, Athens, Greece, 30 March  3 April 2009." ></td>
	<td class="line x" title="2:202	c2009 Association for Computational Linguistics Semi-Supervised Polarity Lexicon Induction Delip Rao Department of Computer Science Johns Hopkins University Baltimore, MD delip@cs.jhu.edu Deepak Ravichandran Google Inc. 1600 Amphitheatre Parkway Mountain View, CA deepakr@google.com Abstract We present an extensive study on the problem of detecting polarity of words." ></td>
	<td class="line x" title="3:202	We consider the polarity of a word to be either positive or negative." ></td>
	<td class="line x" title="4:202	For example, words such as good, beautiful, and wonderful are considered as positive words; whereas words such as bad, ugly, and sad are considered negative words." ></td>
	<td class="line x" title="5:202	We treat polarity detection as a semi-supervised label propagation problem in a graph." ></td>
	<td class="line x" title="6:202	In the graph, each node represents a word whose polarity is to be determined." ></td>
	<td class="line x" title="7:202	Each weighted edge encodes a relation that exists between two words." ></td>
	<td class="line x" title="8:202	Each node (word) can have two labels: positive or negative." ></td>
	<td class="line x" title="9:202	We study this framework in two different resource availability scenarios using WordNet and OpenOffice thesaurus when WordNet is not available." ></td>
	<td class="line x" title="10:202	We report our results on three different languages: English, French, and Hindi." ></td>
	<td class="line x" title="11:202	Our results indicate that label propagation improves significantly over the baseline and other semisupervised learning methods like Mincuts and Randomized Mincuts for this task." ></td>
	<td class="line x" title="12:202	1 Introduction Opinionated texts are characterized by words or phrases that communicate positive or negative sentiment." ></td>
	<td class="line x" title="13:202	Consider the following example of two movie reviews1 shown in Figure 1." ></td>
	<td class="line x" title="14:202	The positive review is peppered with words such as enjoyable, likeable, decent, breathtakingly and the negative Work done as a summer intern at Google Inc. 1Source: Live Free or Die Hard, rottentomatoes.com Figure 1: Movie Reviews with positive (left) and negative (right) sentiment." ></td>
	<td class="line x" title="15:202	comment uses words like ear-shattering, humorless, unbearable." ></td>
	<td class="line x" title="16:202	These terms and prior knowledge of their polarity could be used as features in a supervised classification framework to determine the sentiment of the opinionated text (E.g., (Esuli and Sebastiani, 2006))." ></td>
	<td class="line x" title="17:202	Thus lexicons indicating polarity of such words are indispensable resources not only in automatic sentiment analysis but also in other natural language understanding tasks like textual entailment." ></td>
	<td class="line x" title="18:202	This motivation was seen in the General Enquirer effort by Stone et al.(1966) and several others who manually construct such lexicons for the English language.2 While it is possible to manually build these resources for a language, the ensuing effort is onerous." ></td>
	<td class="line x" title="20:202	This motivates the need for automatic language-agnostic methods for building sentiment lexicons." ></td>
	<td class="line x" title="21:202	The importance of this problem has warranted several efforts in the past, some of which will be reviewed here." ></td>
	<td class="line x" title="22:202	We demonstrate the application of graph-based semi-supervised learning for induction of polarity lexicons." ></td>
	<td class="line x" title="23:202	We try several graph-based semi2The General Inquirer tries to classify English words along several dimensions, including polarity." ></td>
	<td class="line x" title="24:202	675 supervised learning methods like Mincuts, Randomized Mincuts, and Label Propagation." ></td>
	<td class="line x" title="25:202	In particular, we define a graph with nodes consisting of the words or phrases to be classified either as positive or negative." ></td>
	<td class="line x" title="26:202	The edges between the nodes encode some notion of similarity." ></td>
	<td class="line x" title="27:202	In a transductive fashion, a few of these nodes are labeled using seed examples and the labels for the remaining nodes are derived using these seeds." ></td>
	<td class="line x" title="28:202	We explore natural word-graph sources like WordNet and exploit different relations within WordNet like synonymy and hypernymy." ></td>
	<td class="line x" title="29:202	Our method is not just confined to WordNet; any source listing synonyms could be used." ></td>
	<td class="line x" title="30:202	To demonstrate this, we show the use of OpenOffice thesaurus  a free resource available in several languages.3 We begin by discussing some related work in Section 2 and briefly describe the learning methods we use, in Section 3." ></td>
	<td class="line x" title="31:202	Section 4 details our evaluation methodology along with detailed experiments for English." ></td>
	<td class="line x" title="32:202	In Section 5 we demonstrate results in French and Hindi, as an example of how the method could be easily applied to other languages as well." ></td>
	<td class="line x" title="33:202	2 Related Work The literature on sentiment polarity lexicon induction can be broadly classified into two categories, those based on corpora and the ones using WordNet." ></td>
	<td class="line x" title="34:202	2.1 Corpora based approaches One of the earliest work on learning polarity of terms was by Hatzivassiloglou and McKeown (1997) who deduce polarity by exploiting constraints on conjoined adjectives in the Wall Street Journal corpus." ></td>
	<td class="line x" title="35:202	For example, the conjunction and links adjectives of the same polarity while but links adjectives of opposite polarity." ></td>
	<td class="line x" title="36:202	However the applicability of this method for other important classes of sentiment terms like nouns and verbs is yet to be demonstrated." ></td>
	<td class="line x" title="37:202	Further they assume linguistic features specific to English." ></td>
	<td class="line x" title="38:202	Wiebe (2000) uses Lin (1998a) style distributionally similar adjectives in a cluster-and-label process to generate sentiment lexicon of adjectives." ></td>
	<td class="line x" title="39:202	In a different work, Riloff et al.(2003) use manually derived pattern templates to extract subjective nouns by bootstrapping." ></td>
	<td class="line x" title="41:202	3http://www.openoffice.org Another corpora based method due to Turney and Littman (2003) tries to measure the semantic orientation O(t) for a term t by O(t) = summationdisplay tiS+ PMI(t,ti) summationdisplay tjS PMI(t,tj) where S+ and S are minimal sets of polar terms that contain prototypical positive and negative terms respectively, and PMI(t,ti) is the pointwise mutual information (Lin, 1998b) between the terms t and ti." ></td>
	<td class="line x" title="42:202	While this method is general enough to be applied to several languages our aim was to develop methods that exploit more structured sources like WordNet to leverage benefits from the rich network structure." ></td>
	<td class="line x" title="43:202	Kaji and Kitsuregawa (2007) outline a method of building sentiment lexicons for Japanese using structural cues from HTML documents." ></td>
	<td class="line x" title="44:202	Apart from being very specific to Japanese, excessive dependence on HTML structure makes their method brittle." ></td>
	<td class="line x" title="45:202	2.2 WordNet based approaches These approaches use lexical relations defined in WordNet to derive sentiment lexicons." ></td>
	<td class="line x" title="46:202	A simple but high-precision method proposed by Kim and Hovy (2006) is to add all synonyms of a polar word with the same polarity and its antonyms with reverse polarity." ></td>
	<td class="line x" title="47:202	As demonstrated later, the method suffers from low recall and is unsuitable in situations when the seed polar words are too few  not uncommon in low resource languages." ></td>
	<td class="line x" title="48:202	In line with Turneys work, Kamps et." ></td>
	<td class="line x" title="49:202	al." ></td>
	<td class="line x" title="50:202	(2004) try to determine sentiments of adjectives in WordNet by measuring relative distance of the term from exemplars, such as good and bad." ></td>
	<td class="line x" title="51:202	The polarity orientation of a term t is measured as follows O(t) = d(t,good) d(t,bad)d(good,bad) where d(.)" ></td>
	<td class="line x" title="52:202	is a WordNet based relatedness measure (Pedersen et al., 2004)." ></td>
	<td class="line x" title="53:202	Again they report results for adjectives alone." ></td>
	<td class="line x" title="54:202	Another relevant example is the recent work by Mihalcea et." ></td>
	<td class="line x" title="55:202	al." ></td>
	<td class="line x" title="56:202	(2007) on multilingual sentiment analysis using cross-lingual projections." ></td>
	<td class="line x" title="57:202	This is achieved by using bridge resources like dictionaries and parallel corpora to build sentence subjectivity classifiers for the target language (Romanian)." ></td>
	<td class="line x" title="58:202	An interesting result from their work is that 676 only a small fraction of the lexicon entries preserve their polarities under translation." ></td>
	<td class="line x" title="59:202	The primary contributions of this paper are :  An application of graph-based semisupervised learning methods for inducing sentiment lexicons from WordNet and other thesauri." ></td>
	<td class="line x" title="60:202	The label propagation method naturally allows combining several relations from WordNet." ></td>
	<td class="line x" title="61:202	 Our approach works on all classes of words and not just adjectives  Though we report results for English, Hindi, and French, our methods can be easily replicated for other languages where WordNet is available.4 In the absence of WordNet, any thesaurus listing synonyms could be used." ></td>
	<td class="line x" title="62:202	We present one such result using the OpenOffice thesaurus  a freely available multilingual resource scarcely used in NLP literature." ></td>
	<td class="line x" title="63:202	3 Graph based semi-supervised learning Most natural language data has some structure that could be exploited even in the absence of fully annotated data." ></td>
	<td class="line x" title="64:202	For instance, documents are similar in the terms they contain, words could be synonyms of each other, and so on." ></td>
	<td class="line x" title="65:202	Such information can be readily encoded as a graph where the presence of an edge between two nodes would indicate a relationship between the two nodes and, optionally, the weight on the edge could encode strength of the relationship." ></td>
	<td class="line x" title="66:202	This additional information aids learning when very few annotated examples are present." ></td>
	<td class="line x" title="67:202	We review three well known graph based semi-supervised learning methods  mincuts, randomized mincuts, and label propagation  that we use in induction of polarity lexicons." ></td>
	<td class="line x" title="68:202	3.1 Mincuts A mincut of a weighted graph G(V,E) is a partitioning the vertices V into V1 and V2 such that sum of the edge weights of all edges between V1 and V2 is minimal (Figure 2)." ></td>
	<td class="line x" title="69:202	Mincuts for semi-supervised learning proposed by Blum and Chawla (2001) tries to classify datapoints by partitioning the similarity graph such that it minimizes the number of similar points being labeled differently." ></td>
	<td class="line oc" title="70:202	Mincuts have been used 4As of this writing, WordNet is available for more than 40 world languages (http://www.globalwordnet.org) Figure 2: Semi-supervised classification using mincuts in semi-supervised learning for various tasks, including document level sentiment analysis (Pang and Lee, 2004)." ></td>
	<td class="line x" title="71:202	We explore the use of mincuts for the task of sentiment lexicon learning." ></td>
	<td class="line x" title="72:202	3.2 Randomized Mincuts An improvement to the basic mincut algorithm was proposed by Blum et." ></td>
	<td class="line x" title="73:202	al." ></td>
	<td class="line x" title="74:202	(2004)." ></td>
	<td class="line x" title="75:202	The deterministic mincut algorithm, solved using max-flow, produces only one of the several possible mincuts." ></td>
	<td class="line x" title="76:202	Some of these cuts could be skewed thereby negatively effecting the results." ></td>
	<td class="line x" title="77:202	As an extreme example consider the graph in Figure 3a." ></td>
	<td class="line x" title="78:202	Let the nodes with degree one be labeled as positive and negative respectively, and for the purpose of illustration let all edges be of the same weight." ></td>
	<td class="line x" title="79:202	The graph in Figure 3a." ></td>
	<td class="line x" title="80:202	can be partitioned in four equal cost cuts  two of which are shown in (b) and (c)." ></td>
	<td class="line x" title="81:202	The minFigure 3: Problem with mincuts cut algorithm, depending on the implementation, will return only one of the extreme cuts (as in (b)) while the desired classification might be as shown in Figure 3c." ></td>
	<td class="line x" title="82:202	The randomized mincut approach tries to address this problem by randomly perturbing the adjacency matrix by adding random noise.5 Mincut is then performed on this perturbed graph." ></td>
	<td class="line x" title="83:202	This is 5We use a Gaussian noise N(0,1)." ></td>
	<td class="line x" title="84:202	677 repeated several times and unbalanced partitions are discarded." ></td>
	<td class="line x" title="85:202	Finally the remaining partitions are used to deduce the final classification by majority voting." ></td>
	<td class="line x" title="86:202	In the unlikely event of the voting resulting in a tie, we refrain from making a decision thus favoring precision over recall." ></td>
	<td class="line x" title="87:202	3.3 Label propagation Another semi-supervised learning method we use is label propagation by Zhu and Ghahramani (2002)." ></td>
	<td class="line x" title="88:202	The label propagation algorithm is a transductive learning framework which uses a few examples, or seeds, to label a large number of unlabeled examples." ></td>
	<td class="line x" title="89:202	In addition to the seed examples, the algorithm also uses a relation between the examples." ></td>
	<td class="line x" title="90:202	This relation should have two requirements: 1." ></td>
	<td class="line x" title="91:202	It should be transitive." ></td>
	<td class="line x" title="92:202	2." ></td>
	<td class="line x" title="93:202	It should encode some notion of relatedness between the examples." ></td>
	<td class="line x" title="94:202	To name a few, examples of such relations include, synonymy, hypernymy, and similarity in some metric space." ></td>
	<td class="line x" title="95:202	This relation between the examples can be easily encoded as a graph." ></td>
	<td class="line x" title="96:202	Thus every node in the graph is an example and the edge represents the relation." ></td>
	<td class="line x" title="97:202	Also associated with each node, is a probability distribution over the labels for the node." ></td>
	<td class="line x" title="98:202	For the seed nodes, this distribution is known and kept fixed." ></td>
	<td class="line x" title="99:202	The aim is to derive the distributions for the remaining nodes." ></td>
	<td class="line x" title="100:202	Consider a graph G(V,E,W) with vertices V , edges E, and an n  n edge weight matrix W = [wij], where n = |V|." ></td>
	<td class="line x" title="101:202	The label propagation algorithm minimizes a quadratic energy function E = 12 summationdisplay (i,j)  E wij(yi yj)2 where yi and yj are the labels assigned to the nodes i and j respectively.6 Thus, to derive the labels at yi, we set yiE = 0 to obtain the following update equation yi = summationdisplay (i,j)E wijyj summationdisplay (i,j)E wij In practice, we use the following iterative algorithm as noted by Zhu and Ghahramani (2002)." ></td>
	<td class="line x" title="102:202	A 6For binary classification yk  {1,+1}." ></td>
	<td class="line x" title="103:202	nn stochastic transition matrix T is derived by row-normalizing W as follows: Tij = P(j  i) = wijsummationtextn k=1 wkj where Tij can be viewed as the transition probability from node j to node i. The algorithm proceeds as follows: 1." ></td>
	<td class="line x" title="104:202	Assign a n C matrix Y with the initial assignment of labels, where C is the number of classes." ></td>
	<td class="line x" title="105:202	2." ></td>
	<td class="line x" title="106:202	Propagate labels for all nodes by computing Y = TY 3." ></td>
	<td class="line x" title="107:202	Row-normalize Y such that each row adds up to one." ></td>
	<td class="line x" title="108:202	4." ></td>
	<td class="line x" title="109:202	Clamp the seed examples in Y to their original values 5." ></td>
	<td class="line x" title="110:202	Repeat 2-5 until Y converges." ></td>
	<td class="line x" title="111:202	There are several points to be noted." ></td>
	<td class="line x" title="112:202	First, we add a special label DEFAULT to existing set of labels and set P(DEFAULT| node = u) = 1 for all unlabeled nodes u. For all the seed nodes s with class label Lwe define P(L|node = s) = 1." ></td>
	<td class="line x" title="113:202	This ensures nodes that cannot be labeled at all7 will retain P(DEFAULT) = 1 thereby leading to a quick convergence." ></td>
	<td class="line x" title="114:202	Second, the algorithm produces a probability distribution over the labels for all unlabeled points." ></td>
	<td class="line x" title="115:202	This makes this method specially suitable for classifier combination approaches." ></td>
	<td class="line x" title="116:202	For this paper, we simply select the most likely label as the predicted label for the point." ></td>
	<td class="line x" title="117:202	Third, the algorithm eventually converges." ></td>
	<td class="line x" title="118:202	For details on the proof for convergence we refer the reader to Zhu and Ghahramani (2002)." ></td>
	<td class="line x" title="119:202	4 Evaluation and Experiments We use the General Inquirer (GI)8 data for evaluation." ></td>
	<td class="line x" title="120:202	General Inquirer is lexicon of English words hand-labeled with categorical information along several dimensions." ></td>
	<td class="line x" title="121:202	One such dimension is called valence, with 1915 words labeled Positiv (sic) and 2291 words labeled Negativ for words with positive and negative sentiments respectively." ></td>
	<td class="line x" title="122:202	Since we want to evaluate the performance of the 7As an example of such a situation, consider a disconnected component of unlabeled nodes with no seed in it." ></td>
	<td class="line x" title="123:202	8http://www.wjh.harvard.edu/inquirer/ 678 algorithms alone and not the recall issues in using WordNet, we only consider words from GI that also occur in WordNet." ></td>
	<td class="line x" title="124:202	This leaves us the distribution of words as enumerated in Table 1." ></td>
	<td class="line x" title="125:202	PoS type No." ></td>
	<td class="line x" title="126:202	of Positives No." ></td>
	<td class="line x" title="127:202	of Negatives Nouns 517 579 Verbs 319 562 Adjectives 547 438 Table 1: English evaluation data from General Inquirer All experiments reported in Sections 4.1 to 4.5 use the data described above with a 50-50 split so that the first half is used as seeds and the second half is used for test." ></td>
	<td class="line x" title="128:202	Note that all the experiments described below did not involve any parameter tuning thus obviating the need for a separate development test set." ></td>
	<td class="line x" title="129:202	The effect of number of seeds on learning is described in Section 4.6." ></td>
	<td class="line x" title="130:202	4.1 Kim-Hovy method and improvements Kim and Hovy (2006) enrich their sentiment lexicon from WordNet as follows." ></td>
	<td class="line x" title="131:202	Synonyms of a positive word are positive while antonyms are treated as negative." ></td>
	<td class="line x" title="132:202	This basic version suffers from a very poor recall as shown in the Figure 4 for adjectives (see iteration 1)." ></td>
	<td class="line x" title="133:202	The recall can be improved for a slight trade-off in precision if we re-run the above algorithm on the output produced at the previous level." ></td>
	<td class="line x" title="134:202	This could be repeated iteratively until there is no noticeable change in precision/recall." ></td>
	<td class="line x" title="135:202	We consider this as the best possible F1-score produced by the Kim-Hovy method." ></td>
	<td class="line x" title="136:202	The classwise F1 for this method is shown in Table 2." ></td>
	<td class="line x" title="137:202	We use these scores as our baseline." ></td>
	<td class="line x" title="138:202	Figure 4: Kim-Hovy method PoS type P R F1 Nouns 92.59 21.43 34.80 Verbs 87.89 38.31 53.36 Adjectives 92.95 31.71 47.28 Table 2: Precision/Recall/F1-scores for KimHovy method 4.2 Using prototypes We now consider measuring semantic orientation from WordNet using prototypical examples such as good and bad similar to Kamps et al.(2004)." ></td>
	<td class="line x" title="140:202	Kamps et." ></td>
	<td class="line x" title="141:202	al., report results only for adjectives though their method could be used for other part-of-speech types." ></td>
	<td class="line x" title="142:202	The results for using prototypes are listed in Table 3." ></td>
	<td class="line x" title="143:202	Note that the seed data was fully unused except for the examples good and bad." ></td>
	<td class="line x" title="144:202	We still test on the same test data as earlier for comparing results." ></td>
	<td class="line x" title="145:202	Also note that the recall need not be 100 in this case as we refrain from making a decision when d(t,good) = d(t,bad)." ></td>
	<td class="line x" title="146:202	PoS type P R F1 Nouns 48.03 99.82 64.86 Verbs 58.12 100.00 73.51 Adjectives 57.35 99.59 72.78 Table 3: Precision/Recall/F1-scores for prototype method 4.3 Using mincuts and randomized mincuts We now report results for mincuts and randomized mincuts algorithm using the WordNet synonym graph." ></td>
	<td class="line x" title="147:202	As seen in Table 4, we only observed a marginal improvement (for verbs) over mincuts by using randomized mincuts." ></td>
	<td class="line x" title="148:202	But the overall improvement of using graphbased semi-supervised learning methods over the Kim-Hovy and Prototype methods is quite significant." ></td>
	<td class="line x" title="149:202	4.4 Using label propagation We extract the synonym graph from WordNet with an edge between two nodes being defined iff one is a synonym of the other." ></td>
	<td class="line x" title="150:202	When label propagation is performed on this graph results in Table 5 are observed." ></td>
	<td class="line x" title="151:202	The results presented in Tables 2-5 need deeper inspection." ></td>
	<td class="line x" title="152:202	The iterated KimHovy method suffers from poor recall." ></td>
	<td class="line x" title="153:202	However both mincut methods and the prototype method by 679 P R F1 Nouns Mincut 68.25 100.00 81.13 RandMincut 68.32 99.09 80.08 Verbs Mincut 72.34 100.00 83.95 RandMincut 73.06 99.02 84.19 Adjectives Mincut 73.78 100.00 84.91 RandMincut 73.58 100.00 84.78 Table 4: Precision/Recall/F1-scores using mincuts and randomized mincuts PoS type P R F1 Nouns 82.55 58.58 58.53 Verbs 81.00 85.94 83.40 Adjectives 84.76 64.02 72.95 Table 5: Precision/Recall/F1-scores for Label Propogation Kamps et." ></td>
	<td class="line x" title="154:202	al., have high recall as they end up classifying every node as either positive or negative." ></td>
	<td class="line x" title="155:202	Note that the recall for randomized mincut is not 100 as we do not make a classification decision when there is a tie in majority voting (refer Section 3.2)." ></td>
	<td class="line x" title="156:202	Observe that the label propagation method performs significantly better than previous graph based methods in precision." ></td>
	<td class="line x" title="157:202	The reason for lower recall is attributed to the lack of connectivity between plausibly related nodes, thereby not facilitating the spread of labels from the labeled seed nodes to the unlabeled nodes." ></td>
	<td class="line x" title="158:202	We address this problem by adding additional edges to the synonym graph in the next section." ></td>
	<td class="line x" title="159:202	4.5 Incorporating hypernyms The main reason for low recall in label propagation is that the WordNet synonym graph is highly disconnected." ></td>
	<td class="line x" title="160:202	Even nodes which are logically related have paths missing between them." ></td>
	<td class="line x" title="161:202	For example the positive nouns compliment and laud belong to different synonym subgraphs without a path between them." ></td>
	<td class="line x" title="162:202	But incorporating the hypernym edges the two are connected by the noun praise." ></td>
	<td class="line x" title="163:202	So, we incorporated hypernyms of every node to improve connectivity." ></td>
	<td class="line x" title="164:202	Performing label propagation on this combined graph gives much better results (Table 6) with much higher recall and even slightly better precision." ></td>
	<td class="line x" title="165:202	In Table 6., we do not report results for adjectives as WordNet does not define hypernyms for adjectives." ></td>
	<td class="line x" title="166:202	A natural quesPoS type P R F1 Nouns 83.88 99.64 91.08 Verbs 85.49 100.00 92.18 Adjectives N/A N/A N/A Table 6: Effect of adding hypernyms tion to ask is if we can use other WordNet relations too." ></td>
	<td class="line x" title="167:202	We will defer this until section 6." ></td>
	<td class="line x" title="168:202	4.6 Effect of number of seeds The results reported in Sections 4.1 to 4.5 fixed the number of seeds." ></td>
	<td class="line x" title="169:202	We now investigate the performance of the various methods on the number of seeds used." ></td>
	<td class="line x" title="170:202	In particular, we are interested in performance under conditions when the number of seeds are few  which is the motivation for using semi-supervised learning in the first place." ></td>
	<td class="line x" title="171:202	Figure 5 presents our results for English." ></td>
	<td class="line x" title="172:202	Observe that Label Propagation performs much better than our baseline even when the number of seeds is as low as ten." ></td>
	<td class="line x" title="173:202	Thus label propagation is especially suited when annotation data is extremely sparse." ></td>
	<td class="line x" title="174:202	One reason for mincuts performing badly with few seeds is because they generate degenrate cuts." ></td>
	<td class="line x" title="175:202	5 Adapting to other languages In order to demonstrate the ease of adaptability of our method for other languages, we used the Hindi WordNet9 to derive the adjective synonym graph." ></td>
	<td class="line x" title="176:202	We selected 489 adjectives at random from a list of 10656 adjectives and this list was annotated by two native speakers of the language." ></td>
	<td class="line x" title="177:202	The annotated list was then split 50-50 into seed and test sets." ></td>
	<td class="line x" title="178:202	Label propagation was performed using the seed list and evaluated on the test list." ></td>
	<td class="line x" title="179:202	The results are listed in Table 7." ></td>
	<td class="line x" title="180:202	Hindi P R F1 90.99 95.10 93.00 Table 7: Evaluation on Hindi dataset WordNet might not be freely available for all languages or may not exist." ></td>
	<td class="line x" title="181:202	In such cases building graph from an existing thesaurus might also suffice." ></td>
	<td class="line x" title="182:202	As an example, we consider French." ></td>
	<td class="line x" title="183:202	Although the French WordNet is available10, we 9http://www.cfilt.iitb.ac.in/wordnet/webhwn/ 10http://www.illc.uva.nl/EuroWordNet/consortiumewn.html 680 Figure 5: Effect of number of seeds on the F-score for Nouns, Verbs, and Adjectives." ></td>
	<td class="line x" title="184:202	The X-axis is number of seeds and the Y-axis is the F-score." ></td>
	<td class="line x" title="185:202	found the cost prohibitive to obtain it." ></td>
	<td class="line x" title="186:202	Observe that if we are using only the synonymy relation in WordNet then any thesaurus can be used instead." ></td>
	<td class="line x" title="187:202	To demonstrate this, we consider the OpenOffice thesaurus for French, that is freely available." ></td>
	<td class="line x" title="188:202	The synonym graph of French adjectives has 9707 vertices and 1.6M edges." ></td>
	<td class="line x" title="189:202	We manually annotated a list of 316 adjectives and derived seed and test sets using a 50-50 split." ></td>
	<td class="line x" title="190:202	The results of label propagation on such a graph is shown in Table 8." ></td>
	<td class="line x" title="191:202	French P R F1 73.65 93.67 82.46 Table 8: Evaluation on French dataset The reason for better results in Hindi compared to French can be attributed to (1) higher interannotator agreement ( = 0.7) in Hindi compared that in French ( = 0.55).11 (2) The Hindi experiment, like English, used WordNet while the French experiment was performed on graphs derived from the OpenOffice thesaurus due lack of freely available French WordNet." ></td>
	<td class="line x" title="192:202	11We do not have  scores for English dataset derived from the Harvard Inquirer project." ></td>
	<td class="line x" title="193:202	6 Conclusions and Future Work This paper demonstrated the utility of graph-based semi-supervised learning framework for building sentiment lexicons in a variety of resource availability situations." ></td>
	<td class="line x" title="194:202	We explored how the structure of WordNet could be leveraged to derive polarity lexicons." ></td>
	<td class="line x" title="195:202	The paper combines, for the first time, relationships like synonymy and hypernymy to improve label propagation results." ></td>
	<td class="line x" title="196:202	All of our methods are independent of language as shown in the French and Hindi cases." ></td>
	<td class="line x" title="197:202	We demonstrated applicability of our approach on alternative thesaurus-derived graphs when WordNet is not freely available, as in the case of French." ></td>
	<td class="line x" title="198:202	Although our current work uses WordNet and other thesauri, in resource poor situations when only monolingual raw text is available we can perform label propagation on nearest neighbor graphs derived directly from raw text using distributional similarity methods." ></td>
	<td class="line x" title="199:202	This is work in progress." ></td>
	<td class="line x" title="200:202	We are also currently working on the possibility of including WordNet relations other than synonymy and hypernymy." ></td>
	<td class="line x" title="201:202	One relation that is interesting and useful is antonymy." ></td>
	<td class="line x" title="202:202	Antonym edges cannot be added in a straight-forward way to the 681 graph for label propagation as antonymy encodes negative similarity (or dissimilarity) and the dissimilarity relation is not transitive." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-1001
Subjectivity Recognition on Word Senses via Semi-supervised Mincuts
Su, Fangzhong;Markert, Katja;"></td>
	<td class="line x" title="1:199	Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 19, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:199	c 2009 Association for Computational Linguistics Subjectivity Recognition on Word Senses via Semi-supervised Mincuts Fangzhong Su School of Computing University of Leeds fzsu@comp.leeds.ac.uk Katja Markert School of Computing University of Leeds markert@comp.leeds.ac.uk Abstract We supplement WordNet entries with information on the subjectivity of its word senses." ></td>
	<td class="line x" title="3:199	Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data." ></td>
	<td class="line x" title="4:199	The resulting data sparseness problem is aggravated by the fact that dictionary definitions are very short." ></td>
	<td class="line x" title="5:199	We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure." ></td>
	<td class="line x" title="6:199	The experimental results show that it outperformssupervisedminimumcutaswellasstandard supervised, non-graph classification, reducing the error rate by 40%." ></td>
	<td class="line x" title="7:199	In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data." ></td>
	<td class="line x" title="8:199	1 Introduction There is considerable academic and commercial interest in processing subjective content in text, where subjective content refers to any expression of a private state such as an opinion or belief (Wiebe et al., 2005)." ></td>
	<td class="line x" title="9:199	Important strands of work include the identification of subjective content and the determination of its polarity, i.e. whether a favourable or unfavourable opinion is expressed." ></td>
	<td class="line x" title="10:199	Automatic identification of subjective content often relies on word indicators, such as unigrams (Pang et al., 2002) or predetermined sentiment lexica (Wilson et al., 2005)." ></td>
	<td class="line x" title="11:199	Thus, the word positive in the sentence This deal is a positive development for our company. gives a strong indication that the sentence contains a favourable opinion." ></td>
	<td class="line x" title="12:199	However, such word-based indicators can be misleading for two reasons." ></td>
	<td class="line x" title="13:199	First, contextual indicators such as irony and negation can reverse subjectivity or polarity indications (Polanyi and Zaenen, 2004)." ></td>
	<td class="line x" title="14:199	Second, different word senses of a single word can actually be of different subjectivity or polarity." ></td>
	<td class="line x" title="15:199	A typical subjectivity-ambiguous word, i.e. a word that has at least one subjective and at least one objective sense, is positive, as shown by the two example senses given below.1 (1) positive, electropositivehaving a positive electric charge;protons are positive (objective) (2) plus, positiveinvolving advantage or good; a plus (or positive) factor (subjective) We concentrate on this latter problem by automatically creating lists of subjective senses, instead of subjective words, via adding subjectivity labels for senses to electronic lexica, using the example of WordNet." ></td>
	<td class="line x" title="16:199	This is important as the problem of subjectivity-ambiguity is frequent: We (Su and Markert, 2008) find that over 30% of words in our dataset are subjectivity-ambiguous." ></td>
	<td class="line x" title="17:199	Information on subjectivity of senses can also improve other tasks such as word sense disambiguation (Wiebe and Mihalcea, 2006)." ></td>
	<td class="line x" title="18:199	Moreover, Andreevskaia and Bergler (2006) show that the performance of automaticannotationofsubjectivityatthewordlevelcan be hurt by the presence of subjectivity-ambiguous words in the training sets they use." ></td>
	<td class="line x" title="19:199	1All examples in this paper are from WordNet 2.0." ></td>
	<td class="line x" title="20:199	1 We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Ouralgorithmoutperformssupervisedminimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%." ></td>
	<td class="line x" title="21:199	In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data." ></td>
	<td class="line x" title="22:199	Our approach also outperforms prior approaches to the subjectivity recognition of word senses and performs well across two different data sets." ></td>
	<td class="line x" title="23:199	The remainder of this paper is organized as follows." ></td>
	<td class="line x" title="24:199	Section 2 discusses previous work." ></td>
	<td class="line x" title="25:199	Section 3 describes our proposed semi-supervised minimum cut framework in detail." ></td>
	<td class="line x" title="26:199	Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5." ></td>
	<td class="line oc" title="27:199	2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level." ></td>
	<td class="line x" title="28:199	An up-to-date overview is given in Pang and Lee (2008)." ></td>
	<td class="line oc" title="29:199	Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do." ></td>
	<td class="line nc" title="30:199	We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences, used in Pang and Lee (2004)) at the word sense level." ></td>
	<td class="line x" title="31:199	At the word level Takamura et al.(2005) use a semi-supervised spin model for word polarity determination, where the graph 2It can be argued that subjectivity labels are maybe rather more graded than the clear-cut binary distinction we assign." ></td>
	<td class="line x" title="33:199	However, in Su and Markert (2008a) as well as Wiebe and Mihalcea (2006) we find that human can assign the binary distinction to word senses with a high level of reliability." ></td>
	<td class="line x" title="34:199	is constructed using a variety of information such as gloss co-occurrences and WordNet links." ></td>
	<td class="line x" title="35:199	Apart from using a different graph-based model from ours, theyassumethatsubjectivityrecognitionhasalready been achieved prior to polarity recognition and test against word lists containing subjective words only." ></td>
	<td class="line x" title="36:199	However, Kim and Hovy (2004) and Andreevskaia and Bergler (2006) show that subjectivity recognition might be the harder problem with lower human agreement and automatic performance." ></td>
	<td class="line x" title="37:199	In addition, we deal with classification at the word sense level, treating also subjectivity-ambiguous words, which goes beyond the work in Takamura et al.(2005)." ></td>
	<td class="line x" title="39:199	Word Sense Level: There are three prior approaches addressing word sense subjectivity or polarity classification." ></td>
	<td class="line x" title="40:199	Esuli and Sebastiani (2006) determine the polarity (positive/negative/objective) of word senses in WordNet." ></td>
	<td class="line x" title="41:199	However, there is no evaluation as to the accuracy of their approach." ></td>
	<td class="line x" title="42:199	They then extend their work (Esuli and Sebastiani, 2007) by applying the Page Rank algorithm to rank the WordNet senses in terms of how strongly a sense possesses a given semantic property (e.g., positive or negative)." ></td>
	<td class="line x" title="43:199	Apart from us tackling subjectivity instead of polarity, their Page Rank graph is also constructed focusing on WordNet glosses (linking glosses containing the same words), whereas we concentrate on the use of WordNet relations." ></td>
	<td class="line x" title="44:199	Both Wiebe and Mihalcea (2006) and our prior work (Su and Markert, 2008) present an annotation scheme for word sense subjectivity and algorithms for automatic classification." ></td>
	<td class="line x" title="45:199	Wiebe and Mihalcea (2006) use an algorithm relying on distributional similarity and an independent, large manually annotated opinion corpus (MPQA) (Wiebe et al., 2005)." ></td>
	<td class="line x" title="46:199	Oneofthedisadvantagesoftheiralgorithmis thatitisrestrictedtosensesthathavedistributionally similar words in the MPQA corpus, excluding 23% of their test data from automatic classification." ></td>
	<td class="line x" title="47:199	Su and Markert (2008) present supervised classifiers, which rely mostly on WordNet glosses and do not effectively exploit WordNets relation structure." ></td>
	<td class="line x" title="48:199	3 Semi-Supervised Mincuts 3.1 Minimum Cuts: The Main Idea Binary classification with minimum cuts (Mincuts) in graphs is based on the idea that similar items 2 should be grouped in the same cut." ></td>
	<td class="line x" title="49:199	All items in the training/test data are seen as vertices in a graph with undirected weighted edges between them specifying how strong the similarity/association between two vertices is. We use minimum s-t cuts: the graph contains two particular vertices s (source, corresponds to subjective) and t (sink, corresponds to objective) and each vertex u is connected to s and t via a weighted edge that can express how likely u is to be classified as s or t in isolation." ></td>
	<td class="line x" title="50:199	Binary classification of the vertices is equivalent to splitting the graph into two disconnected subsets of all vertices, S and T with s  S and t  T. This corresponds to removing a set of edges from the graph." ></td>
	<td class="line x" title="51:199	As similar items should be in the same part of the split, the best split is one which removes edges with low weights." ></td>
	<td class="line x" title="52:199	In other words, a minimum cut problem is to find a partition of the graph which minimizes the following formula, where w(u,v) expresses the weight of an edge between two vertices." ></td>
	<td class="line oc" title="53:199	W(S,T) = summationdisplay uS,vT w(u,v) Globally optimal minimum cuts can be found in polynomial time and near-linear running time in practice, using the maximum flow algorithm (Pang and Lee, 2004; Cormen et al., 2002)." ></td>
	<td class="line x" title="54:199	3.2 Why might Semi-supervised Minimum Cuts Work?" ></td>
	<td class="line x" title="55:199	We propose semi-supervised mincuts for subjectivity recognition on senses for several reasons." ></td>
	<td class="line x" title="56:199	First, our problem satisfies two major conditions necessary for using minimum cuts." ></td>
	<td class="line x" title="57:199	It is a binary classification problem (subjective vs. objective senses) as is needed to divide the graph into two components." ></td>
	<td class="line x" title="58:199	Our dataset also lends itself naturally to s-t Mincuts as we have two different views on the data." ></td>
	<td class="line x" title="59:199	Thus, the edges of a vertex (=sense) to the source/sink can be seen as the probability of a sense being subjective or objective without taking similaritytoothersensesintoaccount, forexampleviaconsidering only the sense gloss." ></td>
	<td class="line x" title="60:199	In contrast, the edges between two senses can incorporate the WordNet relation hierarchy, which is a good source of similarity for our problem as many WordNet relations are subjectivity-preserving, i.e. if two senses are connected via such a relation they are likely to be both subjective or both objective.3 An example here is the antonym relation, where two antonyms such as goodmorally admirable and evil, wickedmorally bad or wrong are both subjective." ></td>
	<td class="line x" title="61:199	Second, Mincuts can be easily expanded into a semi-supervised framework (Blum and Chawla, 2001)." ></td>
	<td class="line x" title="62:199	This is essential as the existing labeled datasets for our problem are small." ></td>
	<td class="line x" title="63:199	In addition, glosses are short, leading to sparse high dimensional vectors in standard feature representations." ></td>
	<td class="line x" title="64:199	Also, WordNet connections between different parts of the WordNet hierarchy can also be sparse, leading to relatively isolated senses in a graph in a supervised framework." ></td>
	<td class="line x" title="65:199	Semi-supervised Mincuts allow us to import unlabeled data that can serve as bridges to isolated components." ></td>
	<td class="line x" title="66:199	More importantly, as the unlabeled data can be chosen to be related to the labeled and test data, they might help pull test data to the right cuts (categories)." ></td>
	<td class="line x" title="67:199	3.3 Formulation of Semi-supervised Mincuts The formulation of our semi-supervised Mincut for sense subjectivity classification involves the following steps, which we later describe in more detail." ></td>
	<td class="line x" title="68:199	1." ></td>
	<td class="line x" title="69:199	We define two vertices s (source) and t (sink), which correspond to the subjective and objective category, respectively." ></td>
	<td class="line x" title="70:199	Following the definition in Blum and Chawla (2001), we call the vertices s and t classification vertices, and all other vertices (labeled, test, and unlabeled data) example vertices." ></td>
	<td class="line x" title="71:199	Each example vertex corresponds to one WordNet sense and is connected to both s and t via a weighted edge." ></td>
	<td class="line x" title="72:199	The latter guarantees that the graph is connected." ></td>
	<td class="line x" title="73:199	2." ></td>
	<td class="line x" title="74:199	For the test and unlabeled examples, we see the edges to the classification vertices as the probability of them being subjective/objective disregarding other example vertices." ></td>
	<td class="line x" title="75:199	We use a supervised classifier to set these edge weights." ></td>
	<td class="line x" title="76:199	Forthelabeledtrainingexamples, theyareconnected by edges with a high constant weight to the classification vertices that they belong to." ></td>
	<td class="line x" title="77:199	3." ></td>
	<td class="line x" title="78:199	WordNet relations are used to construct the edges between two example vertices." ></td>
	<td class="line x" title="79:199	Such 3See Kamps et al.(2004) for an early indication of such properties for some WordNet relations." ></td>
	<td class="line x" title="81:199	3 edges can exist between any pair of example vertices, for example between two unlabeled examples." ></td>
	<td class="line x" title="82:199	4." ></td>
	<td class="line x" title="83:199	After graph construction we then employ a maximum-flow algorithm to find the minimum s-t cuts of the graph." ></td>
	<td class="line x" title="84:199	The cut in which the sourcevertexsliesisclassified assubjective, andthecutinwhichthesinkvertextliesisobjective." ></td>
	<td class="line x" title="85:199	We now describe the above steps in more detail." ></td>
	<td class="line x" title="86:199	Selection of unlabeled data: Random selection of unlabeled data might hurt the performance of Mincuts, as they might not be related to any sense in our training/test data (denoted by A)." ></td>
	<td class="line x" title="87:199	Thus a basic principle is that the selected unlabeled senses should be related to the training/test data by WordNet relations." ></td>
	<td class="line x" title="88:199	WethereforesimplyscaneachsenseinA, and collect all senses related to it via one of the WordNet relations in Table 1." ></td>
	<td class="line x" title="89:199	All such senses that are not in A are collected in the unlabeled data set." ></td>
	<td class="line x" title="90:199	Weighting of edges to the classification vertices: The edge weight to s and t represents how likely it is that an example vertex is initially put in the cut in which s (subjective) or t (objective) lies." ></td>
	<td class="line x" title="91:199	For unlabeled and test vertices, we use a supervised classifier (SVM4) with the labeled data as training data to assign the edge weights." ></td>
	<td class="line x" title="92:199	The SVM is also used as a baseline and its features are described in Section 4.3." ></td>
	<td class="line x" title="93:199	As we do not wish the Mincut to reverse labels of the labeled training data, we assign a high constant weight of 5 to the edge between a labeledvertexanditscorrespondingclassificationvertex, and a low weight of 0.01 to the edge to the other classification vertex." ></td>
	<td class="line x" title="94:199	Assigning weights to WordNet relations: We connect two vertices that are linked by one of the ten WordNet relations in Table 1 via an edge." ></td>
	<td class="line x" title="95:199	Not all WordNet relations we use are subjectivitypreserving to the same degree: for example, hyponyms (such as simpleton) of objective senses (such as person) do not have to be objective." ></td>
	<td class="line x" title="96:199	However, we aim for high graph connectivity and we can assign different weights to different relations 4We employ LIBSVM, available at http://www.csie." ></td>
	<td class="line x" title="97:199	ntu.edu.tw/cjlin/libsvm/." ></td>
	<td class="line x" title="98:199	Linear kernel and probability estimates are used in this work." ></td>
	<td class="line x" title="99:199	to reflect the degree to which they are subjectivitypreserving." ></td>
	<td class="line x" title="100:199	Therefore, we experiment with two methods of weight assignment." ></td>
	<td class="line x" title="101:199	Method 1 (NoSL) assigns the same constant weight of 1.0 to all WordNet relations." ></td>
	<td class="line x" title="102:199	Method 2 (SL) reflects different degrees of preserving subjectivity." ></td>
	<td class="line x" title="103:199	To do this, we adapt an unsupervised method of generating a large noisy set of subjective and objective senses from our previous work (Su and Markert, 2008)." ></td>
	<td class="line x" title="104:199	This method uses a list of subjective words (SL)5 to classify each WordNet sense with at least two subjective words in its gloss as subjective and all other senses as objective." ></td>
	<td class="line x" title="105:199	We then count how often two senses related via a given relation have the same or a different subjectivity label." ></td>
	<td class="line x" title="106:199	The weight is computed by #same/(#same+#different)." ></td>
	<td class="line x" title="107:199	Results are listed in Table 1." ></td>
	<td class="line x" title="108:199	Table 1: Relation weights (Method 2) Method #Same #Different Weight Antonym 2,808 309 0.90 Similar-to 6,887 1,614 0.81 Derived-from 4,630 947 0.83 Direct-Hypernym 71,915 8,600 0.89 Direct-Hyponym 71,915 8,600 0.89 Attribute 350 109 0.76 Also-see 1,037 337 0.75 Extended-Antonym 6,917 1,651 0.81 Domain 4,387 892 0.83 Domain-member 4,387 892 0.83 Example graph: An example graph is shown in Figure 1." ></td>
	<td class="line x" title="109:199	The three example vertices correspond to the senses religiousextremely scrupulous and conscientious, scrupuloushaving scruples; arising from a sense of right and wrong; principled; and flicker, spark, glinta momentary flash of light respectively." ></td>
	<td class="line x" title="110:199	The vertex scrupulous is unlabeled data derived from the vertex religious(a test item) by the relation similar-to." ></td>
	<td class="line x" title="111:199	4 Experiments and Evaluation 4.1 Datasets We conduct the experiments on two different gold standard datasets." ></td>
	<td class="line x" title="112:199	One is the Micro-WNOp corpus, 5Available at http://www.cs.pitt.edu/mpqa 4 scrupulous religious subjective objective flicker 0.24 0.76 0.83 0.17 0.16 0.84 0.81similar-to Figure 1: Graph of Word Senses which is representative of the part-of-speech distribution in WordNet 6." ></td>
	<td class="line x" title="113:199	It includes 298 words with 703 objective and 358 subjective WordNet senses." ></td>
	<td class="line x" title="114:199	The second one is the dataset created by Wiebe and Mihalcea (2006).7 It only contains noun and verb senses, and includes 60 words with 236 objective and 92 subjective WordNet senses." ></td>
	<td class="line x" title="115:199	As the Micro-WNOp set is larger and also contains adjective and adverb senses, we describe our results in more detail on that corpus in the Section 4.3 and 4.4." ></td>
	<td class="line x" title="116:199	In Section 4.5, we shortly discuss results on Wiebe&Mihalceas dataset." ></td>
	<td class="line x" title="117:199	4.2 Baseline and Evaluation We compare to a baseline that assigns the most frequent category objective to all senses, which achievesanaccuracyof66.3%and72.0%onMicroWNOp and Wiebe&Mihalceas dataset respectively." ></td>
	<td class="line x" title="118:199	We use the McNemar test at the significance level of 5% for significance statements." ></td>
	<td class="line x" title="119:199	All evaluations are carried out by 10-fold cross-validation." ></td>
	<td class="line x" title="120:199	4.3 Standard Supervised Learning We use an SVM classifier to compare our proposed semi-supervised Mincut approach to a reasonable 6Available at http://www.comp.leeds.ac.uk/ markert/data." ></td>
	<td class="line x" title="121:199	This dataset was first used with a different annotation scheme in Esuli and Sebastiani (2007) and we also used it in Su and Markert (2008)." ></td>
	<td class="line x" title="122:199	7Available at http://www.cs.pitt.edu/wiebe/ pubs/papers/goldstandard.total.acl06." ></td>
	<td class="line x" title="123:199	baseline.8 Three different feature types are used." ></td>
	<td class="line x" title="124:199	LexicalFeatures(L):a bag-of-words representation of the sense glosses with stop word filtering." ></td>
	<td class="line x" title="125:199	RelationFeatures (R): First, we use two features for each of the ten WordNet relations in Table 1, describing how many relations of that type the sense hastosensesinthesubjectiveorobjectivepartofthe training set, respectively." ></td>
	<td class="line x" title="126:199	This provides a non-graph summary of subjectivity-preserving links." ></td>
	<td class="line x" title="127:199	Second, we manually collected a small set (denoted by SubjSet) of seven subjective verb and noun senses which are close to the root in WordNets hypernym tree." ></td>
	<td class="line x" title="128:199	A typical example element of SubjSet is psychological feature a feature of the mental life of a living organism, which indicates subjectivity for its hyponyms such as hope  the general feeling that some desire will be fulfilled." ></td>
	<td class="line x" title="129:199	A binary feature describes whether a noun/verb sense is a hyponym of an element of SubjSet." ></td>
	<td class="line x" title="130:199	Monosemous Feature (M): for each sense, we scan if a monosemous word is part of its synset." ></td>
	<td class="line x" title="131:199	If so, we further check if the monosemous word is collected in the subjective word list (SL)." ></td>
	<td class="line x" title="132:199	The intuition is that if a monosemous word is subjective, obviously its (single) sense is subjective." ></td>
	<td class="line x" title="133:199	For example, the sense uncompromising, inflexiblenot making concessions is subjective, as uncompromising is a monosemous word and also in SL." ></td>
	<td class="line x" title="134:199	We experiment with different combinations of features and the results are listed in Table 2, prefixed by SVM." ></td>
	<td class="line x" title="135:199	All combinations perform significantly better than the more frequent category baseline and similarly to the supervised Naive Bayes classifier (see S&M in Table 2) we used in Su and Markert (2008)." ></td>
	<td class="line x" title="136:199	However, improvements by adding more features remain small." ></td>
	<td class="line x" title="137:199	In addition, we compare to a supervised classifier (see Lesk in Table 2) that just assigns each sense the subjectivity label of its most similar sense in the training data, using Lesks similarity measure from Pedersens WordNet similarity package9." ></td>
	<td class="line x" title="138:199	We use Lesk as it is one of the few measures applicable across all parts-of-speech." ></td>
	<td class="line x" title="139:199	8This SVM is also used to provide the edge weights to the classification vertices in the Mincut approach." ></td>
	<td class="line x" title="140:199	9Availableathttp://www.d.umn.edu/tpederse/ similarity.html." ></td>
	<td class="line x" title="141:199	5 Table 2: Results of SVM and Mincuts with different settings of feature Method Subjective Objective Accuracy Precision Recall F-score Precision Recall F-score Baseline N/A 0 N/A 66.3% 100% 79.7% 66.3% S&M 66.2% 64.5% 65.3% 82.2% 83.2% 82.7% 76.9% Lesk 65.6% 50.3% 56.9% 77.5% 86.6% 81.8% 74.4% SVM-L 69.6% 37.7% 48.9% 74.3% 91.6% 82.0% 73.4% L-SL 82.0% 43.3% 56.7% 76.7% 95.2% 85.0% 77.7% L-NoSL 80.8% 43.6% 56.6% 76.7% 94.7% 84.8% 77.5% SVM-LM 68.9% 42.2% 52.3% 75.4% 90.3% 82.2% 74.1% LM-SL 83.2% 44.4% 57.9% 77.1% 95.4% 85.3% 78.2% LM-NoSL 83.6% 44.1% 57.8% 77.1% 95.6% 85.3% 78.2% SVM-LR 68.4% 45.3% 54.5% 76.2% 89.3% 82.3% 74.5% LR-SL 82.7% 65.4% 73.0% 84.1% 93.0% 88.3% 83.7% LR-NoSL 82.4% 65.4% 72.9% 84.0% 92.9% 88.2% 83.6% SVM-LRM 69.8% 47.2% 56.3% 76.9% 89.6% 82.8% 75.3% LRM-SL 85.5% 65.6% 74.2% 84.4% 94.3% 89.1% 84.6% LRM-NoSL 84.6% 65.9% 74.1% 84.4% 93.9% 88.9% 84.4% 1 L, R and M correspond to the lexical, relation and monosemous features respectively." ></td>
	<td class="line x" title="142:199	2 SVM-LcorrespondstousinglexicalfeaturesonlyfortheSVMclassifier." ></td>
	<td class="line x" title="143:199	Likewise,SVMLRM corresponds to using a combination for lexical, relation, and monosemous features for the SVM classifier." ></td>
	<td class="line x" title="144:199	3 L-SL corresponds to the Mincut that uses only lexical features for the SVM classifier, and subjective list (SL) to infer the weight of WordNet relations." ></td>
	<td class="line x" title="145:199	Likewise, LM-NoSL corresponds to the Mincut algorithm that uses lexical and monosemous features for the SVM, and predefined constants for WordNet relations (without subjective list)." ></td>
	<td class="line x" title="146:199	4.4 Semi-supervised Graph Mincuts Using our formulation in Section 3.3, we import 3,220 senses linked by the ten WordNet relations to any senses in Micro-WNOp as unlabeled data." ></td>
	<td class="line x" title="147:199	We construct edge weights to classification vertices using the SVM discussed above and use WordNet relations for links between example vertices, weighted by either constants (NoSL) or via the method illustrated in Table 1 (SL)." ></td>
	<td class="line x" title="148:199	The results are also summarized in Table 2." ></td>
	<td class="line x" title="149:199	Semi-supervised Mincuts always significantly outperform the corresponding SVM classifiers, regardless of whether the subjectivity list is used for setting edge weights." ></td>
	<td class="line x" title="150:199	We can also see that we achieve good results without using any other knowledge sources (setting LR-NoSL)." ></td>
	<td class="line x" title="151:199	The example in Figure 1 explains why semisupervised Mincuts outperforms the supervised approach." ></td>
	<td class="line x" title="152:199	The vertex religious is initially assigned the subjective/objective probabilities 0.24/0.76 by theSVMclassifier, leadingtoawrongclassification." ></td>
	<td class="line x" title="153:199	However, inourgraph-basedMincutframework, the vertex religious might link to other vertices (for example, it links to the vertex scrupulous in the unlabeled data by the relation similar-to)." ></td>
	<td class="line x" title="154:199	The mincut algorithm will put vertices religious and scrupulousinthesamecut(subjectivecategory)as this results in the least cost 0.93 (ignoring the cost of assigning the unrelated sense of flicker)." ></td>
	<td class="line x" title="155:199	In other words, the edges between the vertices are likely to correct some initially wrong classification and pull the vertices into the right cuts." ></td>
	<td class="line x" title="156:199	In the following we will analyze the best minimum cut algorithm LRM-SL in more detail." ></td>
	<td class="line x" title="157:199	We measure its accuracy for each part-of-speech in the Micro-WNOp dataset." ></td>
	<td class="line x" title="158:199	The number of noun, adjective, adverb and verb senses in Micro-WNOp is 484, 265, 31 and 281, respectively." ></td>
	<td class="line x" title="159:199	The result is listed in Table 3." ></td>
	<td class="line x" title="160:199	The significantly better performance of semi-supervised mincuts holds across all parts-ofspeech but the small set of adverbs, where there is no significant difference between the baseline, SVM and the Mincut algorithm." ></td>
	<td class="line x" title="161:199	6 Table 3: Accuracy for Different Part-Of-Speech Method Noun Adjective Adverb Verb Baseline 76.9% 61.1% 77.4% 72.6% SVM 81.4% 63.4% 83.9% 75.1% Mincut 88.6% 78.9% 77.4% 84.0% We will now investigate how LRM-SL performs with different sizes of labeled and unlabeled data." ></td>
	<td class="line x" title="162:199	All learning curves are generated via averaging 10 learning curves from 10-fold cross-validation." ></td>
	<td class="line x" title="163:199	Performance with different sizes of labeled data: we randomly generate subsets of labeled data A1, A2 An, and guarantee that A1  A2  An." ></td>
	<td class="line x" title="164:199	Results for the best SVM (LRM) and the best minimum cut (LRM-SL) are listed in Table 4, and the corresponding learning curve is shown in Figure 2." ></td>
	<td class="line x" title="165:199	As can be seen, the semi-supervised Mincuts is consistently better than SVM." ></td>
	<td class="line x" title="166:199	Moreover, the semisupervised Mincut with only 200 labeled data items performs even better than SVM with 954 training items (78.9% vs 75.3%), showing that our semisupervised framework allows for a training data reduction of more than 80%." ></td>
	<td class="line x" title="167:199	Table 4: Accuracy with different sizes of labeled data # labeled data SVM Mincuts 100 69.1% 72.2% 200 72.6% 78.9% 400 74.4% 82.7% 600 75.5% 83.7% 800 76.0% 84.1% 900 75.6% 84.8% 954 (all) 75.3% 84.6% Performance with different sizes of unlabeled data: We propose two different settings." ></td>
	<td class="line x" title="168:199	Option1: Use a subset of the ten relations to generate the unlabeled data (and edges between example vertices)." ></td>
	<td class="line x" title="169:199	For example, we first use {antonym, similar-to} only to obtain a unlabeled dataset U1, then use a larger subset of the relations like {antonym, similar-to, direct-hyponym, directhypernym} to generate another unlabeled dataset U2, and so forth." ></td>
	<td class="line x" title="170:199	Obviously, Ui is a subset of Ui+1." ></td>
	<td class="line x" title="171:199	Option2: Use all the ten relations to generate the unlabeled data U. We then randomly select subsets of U, such as subset U1, U2 and U3, and guarantee that U1  U2  U3  U.  68  71  74  77  80  83  86  89  100  200  300  400  500  600  700  800  900  1000 Accuracy(%) Size of Labeled Data Mincuts SVM Figure 2: Learning curve with different sizes of labeled data The results are listed in Table 5 and Table 6 respectively." ></td>
	<td class="line x" title="172:199	The corresponding learning curves are shown in Figure 3." ></td>
	<td class="line x" title="173:199	We see that performance improves with the increase of unlabeled data." ></td>
	<td class="line x" title="174:199	In addition, the curves seem to converge when the size of unlabeled data is larger than 3,000." ></td>
	<td class="line x" title="175:199	From the results in Tabel 5 one can also see that hyponymy is the relation accounting for the largest increase." ></td>
	<td class="line x" title="176:199	Table 6: Accuracy with different sizes of unlabeled data (random selection) # unlabeled data Accuracy 0 75.9% 200 76.5% 500 78.6% 1000 80.2% 2000 82.8% 3000 84.0% 3220 84.6% Furthermore, these results also show that a supervised mincut without unlabeled data performs only on a par with other supervised classifiers (75.9%)." ></td>
	<td class="line x" title="177:199	The reason is that if we exclude the unlabeled data, there are only 67 WordNet relations/edges between senses in the small Micro-WNOp dataset." ></td>
	<td class="line x" title="178:199	In contrast, the use of unlabeled data adds more edges (4,586) to the graph, which strongly affects the graph cut partition (see also Figure 1)." ></td>
	<td class="line x" title="179:199	4.5 Comparison to Prior Approaches In our previous work (Su and Markert, 2008), we report 76.9% as the best accuracy on the same Micro7 Table 5: Accuracy with different sizes of unlabeled data from WordNet relation Relation # unlabeled data Accuracy {} 0 75.3% {similar-to} 418 79.1% {similar-to, antonym} 514 79.5% {similar-to, antonym, direct-hypernym, directhyponym} 2,721 84.4% {similar-to, antonym, direct-hypernym, directhyponym, also-see, extended-antonym} 3,004 84.4% {similar-to, antonym, direct-hypernym, directhyponym, also-see, extended-antonym, derived-from, attribute, domain, domain-member} 3,220 84.6%  75  77  79  81  83  85  87  89  0  500  1000  1500  2000  2500  3000  3500 Accuracy(%) Size of Unlabeled Data Option1Option2 Figure3: Learningcurvewithdifferentsizesofunlabeled data WNOp dataset used in the previous sections, using a supervised Naive Bayes (S&M in Tabel 2)." ></td>
	<td class="line x" title="180:199	Our best result from Mincuts is significantly better at 84.6% (see LRM-SL in Table 2)." ></td>
	<td class="line x" title="181:199	For comparison to Wiebe and Mihalcea (2006), we use their dataset for testing, henceforth called Wiebe (see Section 4.1 for a description)." ></td>
	<td class="line x" title="182:199	Wiebe and Mihalcea (2006) report their results in precision andrecallcurvesforsubjectivesenses,suchasaprecision of about 55% at a recall of 50% for subjective senses." ></td>
	<td class="line x" title="183:199	Their F-score for subjective senses seems to remain relatively static at 0.52 throughout their precision/recall curve." ></td>
	<td class="line x" title="184:199	We run our best Mincut LRM-SL algorithm with two different settings on Wiebe." ></td>
	<td class="line x" title="185:199	Using MicroWNOp as training set and Wiebe as test set, we achieveanaccuracyof83.2%,whichissimilartothe results on the Micro-WNOp dataset." ></td>
	<td class="line x" title="186:199	At the recall of 50% we achieve a precision of 83.6% (in comparisontotheirprecisionof55%atthesamerecall)." ></td>
	<td class="line x" title="187:199	Our F-score is 0.63 (vs. 0.52)." ></td>
	<td class="line x" title="188:199	Tocheckwhetherthehighperformanceisjustdue to our larger training set, we also conduct 10-fold cross-validation on Wiebe." ></td>
	<td class="line x" title="189:199	The accuracy achieved is 81.1% and the F-score 0.56 (vs. 0.52), suggesting that our algorithm performs better." ></td>
	<td class="line x" title="190:199	Our algorithm can be used on all WordNet senses whereas theirs is restricted to senses that have distributionally similar words in the MPQA corpus (see Section 2)." ></td>
	<td class="line x" title="191:199	However, they use an unsupervised algorithm i.e. they do not need labeled word senses, although they do need a large, manually annotated opinion corpus." ></td>
	<td class="line x" title="192:199	5 Conclusion and Future Work We propose a semi-supervised minimum cut algorithm for subjectivity recognition on word senses." ></td>
	<td class="line x" title="193:199	The experimental results show that our proposed approach is significantly better than a standard supervised classification framework as well as a supervised Mincut." ></td>
	<td class="line x" title="194:199	Overall, we achieve a 40% reduction in error rates (from an error rate of about 25% to an error rate of 15%)." ></td>
	<td class="line x" title="195:199	To achieve the results of standard supervised approaches with our model, we need less than20%oftheirtrainingdata." ></td>
	<td class="line x" title="196:199	Inaddition, wecompare our algorithm to previous state-of-the-art approaches, showing that our model performs better on the same datasets." ></td>
	<td class="line x" title="197:199	Future work will explore other graph construction methods, such as the use of morphological relations as well as thesaurus and distributional similarity measures." ></td>
	<td class="line x" title="198:199	We will also explore other semisupervised algorithms." ></td>
	<td class="line x" title="199:199	8" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-1002
Integrating Knowledge for Subjectivity Sense Labeling
Gyamfi, Yaw;Wiebe, Janyce M.;Mihalcea, Rada;Akkaya, Cem;"></td>
	<td class="line x" title="1:226	Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 1018, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:226	c 2009 Association for Computational Linguistics Integrating Knowledge for Subjectivity Sense Labeling Yaw Gyamfi and Janyce Wiebe University of Pittsburgh {anti,wiebe}@cs.pitt.edu Rada Mihalcea University of North Texas rada@cs.unt.edu Cem Akkaya University of Pittsburgh cem@cs.pitt.edu Abstract This paper introduces an integrative approach to automatic word sense subjectivity annotation." ></td>
	<td class="line x" title="3:226	We use features that exploit the hierarchical structure and domain information in lexical resources such as WordNet, as well as other types of features that measure the similarity of glosses and the overlap among sets of semantically related words." ></td>
	<td class="line x" title="4:226	Integrated in a machine learning framework, the entire set of features is found to give better results than any individual type of feature." ></td>
	<td class="line x" title="5:226	1 Introduction Automatic extraction of opinions, emotions, and sentiments in text (subjectivity analysis) to support applications such as product review mining, summarization, question answering, and information extraction is an active area of research in NLP." ></td>
	<td class="line x" title="6:226	Many approaches to opinion, sentiment, and subjectivity analysis rely on lexicons of words that may be used to express subjectivity." ></td>
	<td class="line x" title="7:226	However, words may have both subjective and objective senses, which is a source of ambiguity in subjectivity and sentiment analysis." ></td>
	<td class="line x" title="8:226	We show that even words judged in previous work to be reliable clues of subjectivity have significant degrees of subjectivity sense ambiguity." ></td>
	<td class="line x" title="9:226	To address this ambiguity, we present a method for automatically assigning subjectivity labels to word senses in a taxonomy, which uses new features and integrates more diverse types of knowledge than in previous work." ></td>
	<td class="line x" title="10:226	We focus on nouns, which are challenging and have received less attention in automatic subjectivity and sentiment analysis." ></td>
	<td class="line x" title="11:226	A common approach to building lexicons for subjectivity analysis is to begin with a small set of seeds which are prototypically subjective (or positive/negative, in sentiment analysis), and then follow semantic links in WordNet-like resources." ></td>
	<td class="line x" title="12:226	By far, the emphasis has been on horizontal relations, such as synonymy and antonymy." ></td>
	<td class="line x" title="13:226	Exploiting vertical links opens the door to taking into account the information content of ancestor concepts of senses with known and unknown subjectivity." ></td>
	<td class="line x" title="14:226	We develop novel features that measure the similarity of a target word sense with a seed set of senses known to be subjective, where the similarity between two concepts is determined by the extent to which they share information, measured by the information content associated with their least common subsumer (LCS)." ></td>
	<td class="line x" title="15:226	Further, particularizing the LCS features to domain greatly reduces calculation while still maintaining effective features." ></td>
	<td class="line x" title="16:226	We find that our new features do lead to significant improvements over methods proposed in previous work, and that the combination of all features gives significantly better performance than any single type of feature alone." ></td>
	<td class="line x" title="17:226	We also ask, given that there are many approaches to finding subjective words, if it would make sense for wordand sense-level approaches to work in tandem, or should we best view them as competing approaches?" ></td>
	<td class="line x" title="18:226	We give evidence suggesting that first identifying subjective words and then disambiguating their senses would be an effective approach to subjectivity sense labeling." ></td>
	<td class="line x" title="19:226	10 There are several motivations for assigning subjectivity labels to senses." ></td>
	<td class="line x" title="20:226	First, (Wiebe and Mihalcea, 2006) provide evidence that word sense labels, together with contextual subjectivity analysis, can be exploited to improve performance in word sense disambiguation." ></td>
	<td class="line x" title="21:226	Similarly, given subjectivity sense labels, word-sense disambiguation may potentially help contextual subjectivity analysis." ></td>
	<td class="line x" title="22:226	In addition, as lexical resources such as WordNet are developed further, subjectivity labels would provide principled criteria for refining word senses, as well as for clustering similar meanings to create more coursegrained sense inventories." ></td>
	<td class="line x" title="23:226	For many opinion mining applications, polarity (positive, negative) is also important." ></td>
	<td class="line x" title="24:226	The overall framework we envision is a layered approach: classifying instances as objective or subjective, and further classifying the subjective instances by polarity." ></td>
	<td class="line x" title="25:226	Decomposing the problem into subproblems has been found to be effective for opinion mining." ></td>
	<td class="line x" title="26:226	This paper addresses the first of these subproblems." ></td>
	<td class="line x" title="27:226	2 Background We adopt the definitions of subjective and objective from Wiebe and Mihalcea (2006) (hereafter WM)." ></td>
	<td class="line x" title="28:226	Subjective expressions are words and phrases being used to express opinions, emotions, speculations, etc. WM give the following examples: His alarm grew." ></td>
	<td class="line x" title="29:226	He absorbed the information quickly." ></td>
	<td class="line x" title="30:226	UCC/Disciples leaders roundly condemned the Iranian Presidents verbal assault on Israel." ></td>
	<td class="line x" title="31:226	Whats the catch?" ></td>
	<td class="line x" title="32:226	Polarity (also called semantic orientation) is also important to NLP applications in sentiment analysis and opinion extraction." ></td>
	<td class="line x" title="33:226	In review mining, for example, we want to know whether an opinion about a product is positive or negative." ></td>
	<td class="line x" title="34:226	Even so, we believe there are strong motivations for a separate subjective/objective (S/O) classification as well." ></td>
	<td class="line x" title="35:226	First, expressions may be subjective but not have any particular polarity." ></td>
	<td class="line x" title="36:226	An example given by (Wilson et al., 2005) is Jerome says the hospital feels no different than a hospital in the states." ></td>
	<td class="line x" title="37:226	An NLP application system may want to find a wide range of private states attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments." ></td>
	<td class="line x" title="38:226	Second, distinguishing S and O instances has often proven more difficult than subsequent polarity classification." ></td>
	<td class="line x" title="39:226	Researchers have found this at various levels of analysis, including the manual annotation of phrases (Takamura et al., 2006), sentiment classification of phrases (Wilson et al., 2005), sentiment tagging of words (Andreevskaia and Bergler, 2006b), and sentiment tagging of word senses (Esuli and Sebastiani, 2006a)." ></td>
	<td class="line x" title="40:226	Thus, effective methods for S/O classification promise to improve performance for sentiment classification." ></td>
	<td class="line oc" title="41:226	In fact, researchers in sentiment analysis have realized benefits by decomposing the problem into S/O and polarity classification (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005; Kim and Hovy, 2006)." ></td>
	<td class="line x" title="42:226	One reason is that different features may be relevant for the two subproblems." ></td>
	<td class="line x" title="43:226	For example, negation features are more important for polarity classification than for subjectivity classification." ></td>
	<td class="line x" title="44:226	Note that some of our features require vertical links that are present in WordNet for nouns and verbs but not for other parts of speech." ></td>
	<td class="line x" title="45:226	Thus we address nouns (leaving verbs to future work)." ></td>
	<td class="line x" title="46:226	There are other motivations for focusing on nouns." ></td>
	<td class="line x" title="47:226	Relatively little work in subjectivity and sentiment analysis has focused on subjective nouns." ></td>
	<td class="line x" title="48:226	Also, a study (Bruce and Wiebe, 1999) showed that, of the major parts of speech, nouns are the most ambiguous with respect to the subjectivity of their instances." ></td>
	<td class="line x" title="49:226	Turning to word senses, we adopt the definitions from WM." ></td>
	<td class="line x" title="50:226	First, subjective: Classifying a sense as S means that, when the sense is used in a text or conversation, we expect it to express subjectivity; we also expect the phrase or sentence containing it to be subjective [WM, pp." ></td>
	<td class="line x" title="51:226	2-3]. In WM, it is noted that sentences containing objective senses may not be objective, as in the sentence Will someone shut that darn alarm off?" ></td>
	<td class="line x" title="52:226	Thus, objective senses are defined as follows: Classifying a sense as O means that, when the sense is used in a text or conversation, we do not expect it to express subjectivity and, if the phrase or sentence containing it is subjective, the subjectivity is due to something else [WM, p 3]. The following subjective examples are given in 11 WM: His alarm grew." ></td>
	<td class="line x" title="53:226	alarm, dismay, consternation  (fear resulting from the awareness of danger) => fear, fearfulness, fright  (an emotion experienced in anticipation of some specific pain or danger (usually accompanied by a desire to flee or fight)) Whats the catch?" ></td>
	<td class="line x" title="54:226	catch  (a hidden drawback; it sounds good but whats the catch?) => drawback  (the quality of being a hindrance; he pointed out all the drawbacks to my plan) The following objective examples are given in WM: The alarm went off." ></td>
	<td class="line x" title="55:226	alarm, warning device, alarm system  (a device that signals the occurrence of some undesirable event) =>device  (an instrumentality invented for a particular purpose; the device is small enough to wear on your wrist; a device intended to conserve water) He sold his catch at the market." ></td>
	<td class="line x" title="56:226	catch, haul  (the quantity that was caught; the catch was only 10 fish) => indefinite quantity  (an estimated quantity) WM performed an agreement study and report that good agreement (=0.74) can be achieved between human annotators labeling the subjectivity of senses." ></td>
	<td class="line x" title="57:226	For a similar task, (Su and Markert, 2008) also report good agreement." ></td>
	<td class="line x" title="58:226	3 Related Work Many methods have been developed for automatically identifying subjective (opinion, sentiment, attitude, affect-bearing, etc.) words, e.g., (Turney, 2002; Riloff and Wiebe, 2003; Kim and Hovy, 2004; Taboada et al., 2006; Takamura et al., 2006)." ></td>
	<td class="line x" title="59:226	Five groups have worked on subjectivity sense labeling." ></td>
	<td class="line x" title="60:226	WM and Su and Markert (2008) (hereafter SM) assign S/O labels to senses, while Esuli and Sebastiani (hereafter ES) (2006a; 2007), Andreevskaia and Bergler (hereafter AB) (2006b; 2006a), and (Valitutti et al., 2004) assign polarity labels." ></td>
	<td class="line x" title="61:226	WM, SM, and ES have evaluated their systems against manually annotated word-sense data." ></td>
	<td class="line x" title="62:226	WMs annotations are described above; SMs are similar." ></td>
	<td class="line x" title="63:226	In the scheme ES use (Cerini et al., 2007), senses are assigned three scores, for positivity, negativity, and neutrality." ></td>
	<td class="line x" title="64:226	There is no unambiguous mapping between the labels of WM/SM and ES, first because WM/SM use distinct classes and ES use numerical ratings, and second because WM/SM distinguish between objective senses on the one hand and neutral subjective senses on the other, while those are both neutral in the scheme used by ES." ></td>
	<td class="line x" title="65:226	WM use an unsupervised corpus-based approach, in which subjectivity labels are assigned to word senses based on a set of distributionally similar words in a corpus annotated with subjective expressions." ></td>
	<td class="line x" title="66:226	SM explore methods that use existing resources that do not require manually annotated data; they also implement a supervised system for comparison, which we will call SMsup." ></td>
	<td class="line x" title="67:226	The other three groups start with positive and negative seed sets and expand them by adding synonyms and antonyms, and traversing horizontal links in WordNet." ></td>
	<td class="line x" title="68:226	AB, ES, and SMsup additionally use information contained in glosses; AB also use hyponyms; SMsup also uses relation and POS features." ></td>
	<td class="line x" title="69:226	AB perform multiple runs of their system to assign fuzzy categories to senses." ></td>
	<td class="line x" title="70:226	ES use a semi-supervised, multiple-classifier learning approach." ></td>
	<td class="line x" title="71:226	In a later paper, (Esuli and Sebastiani, 2007), ES again use information in glosses, applying a random walk ranking algorithm to a graph in which synsets are linked if a member of the first synset appears in the gloss of the second." ></td>
	<td class="line x" title="72:226	Like ES and SMsup, we use machine learning, but with more diverse sources of knowledge." ></td>
	<td class="line x" title="73:226	Further, several of our features are novel for the task." ></td>
	<td class="line x" title="74:226	The LCS features (Section 6.1) detect subjectivity by measuring the similarity of a candidate word sense with a seed set." ></td>
	<td class="line x" title="75:226	WM also use a similarity measure, but as a way to filter the output of a measure of distributional similarity (selecting words for a given word sense), not as we do to cumulatively calculate the subjectivity of a word sense." ></td>
	<td class="line x" title="76:226	Another novel aspect of our similarity features is that they are particularized to domain, which greatly reduces calculation." ></td>
	<td class="line x" title="77:226	The domain subjectivity LCS features (Section 6.2) are also novel for our task." ></td>
	<td class="line x" title="78:226	So is augmenting seed sets with monosemous words, for greater coverage without requiring human intervention or sacrificing quality." ></td>
	<td class="line x" title="79:226	Note that none of our features as we specifically define them has been used in previous work; combining them together, our approach outperforms previous approaches." ></td>
	<td class="line x" title="80:226	12 4 Lexicon and Annotations We use the subjectivity lexicon of (Wiebe and Riloff, 2005)1 both to create a subjective seed set and to create the experimental data sets." ></td>
	<td class="line x" title="81:226	The lexicon is a list of words and phrases that have subjective uses, though only word entries are used in this paper (i.e., we do not address phrases at this point)." ></td>
	<td class="line x" title="82:226	Some entries are from manually developed resources, including the General Inquirer, while others were derived from corpora using automatic methods." ></td>
	<td class="line x" title="83:226	Through manual review and empirical testing on data, (Wiebe and Riloff, 2005) divided the clues into strong (strongsubj) and weak (weaksubj) subjectivity clues." ></td>
	<td class="line x" title="84:226	Strongsubj clues have subjective meanings with high probability, and weaksubj clues have subjective meanings with lower probability." ></td>
	<td class="line x" title="85:226	To support our experiments, we annotated the senses2 of polysemous nouns selected from the lexicon, using WMs annotation scheme described in Section 2." ></td>
	<td class="line x" title="86:226	Due to time constraints, only some of the data was labeled through consensus labeling by two annotators; the rest was labeled by one annotator." ></td>
	<td class="line x" title="87:226	Overall, 2875 senses for 882 words were annotated." ></td>
	<td class="line x" title="88:226	Even though all are senses of words from the subjectivity lexicon, only 1383 (48%) of the senses are subjective." ></td>
	<td class="line x" title="89:226	The words labeled strongsubj are in fact less ambiguous than those labeled weaksubj in our analysis, thus supporting the reliability classifications in the lexicon." ></td>
	<td class="line x" title="90:226	55% (1038/1924) of the senses of strongsubj words are subjective, while only 36% (345/951) of the senses of weaksubj words are subjective." ></td>
	<td class="line x" title="91:226	For the analysis in Section 7.3, we form subsets of the data annotated here to test performance of our method on different data compositions." ></td>
	<td class="line x" title="92:226	5 Seed Sets Both subjective and objective seed sets are used to define the features described below." ></td>
	<td class="line x" title="93:226	For seeds, a large number is desirable for greater coverage, although high quality is also important." ></td>
	<td class="line x" title="94:226	We begin to build our subjective seed set by adding the monosemous strongsubj nouns of the subjectivity lexicon (there are 397 of these)." ></td>
	<td class="line x" title="95:226	Since they are monosemous, they pose no problem of sense ambiguity." ></td>
	<td class="line x" title="96:226	We 1Available at http://www.cs.pitt.edu/mpqa 2In WordNet 2.0 then expand the set with their hyponyms, as they were found useful in previous work by AB (2006b; 2006a)." ></td>
	<td class="line x" title="97:226	This yields a subjective seed set of 645 senses." ></td>
	<td class="line x" title="98:226	After removing the word senses that belong to the same synset, so that only one word sense per synset is left, we ended up with 603 senses." ></td>
	<td class="line x" title="99:226	To create the objective seed set, two annotators manually annotated 800 random senses from WordNet, and selected for the objective seed set the ones they both agreed are clearly objective." ></td>
	<td class="line x" title="100:226	This creates an objective seed set of 727." ></td>
	<td class="line x" title="101:226	Again we removed multiple senses from the same synset leaving us with 722." ></td>
	<td class="line x" title="102:226	The other 73 senses they annotated are added to the mixed data set described below." ></td>
	<td class="line x" title="103:226	As this sampling shows, WordNet nouns are highly skewed toward objective senses, so finding an objective seed set is not difficult." ></td>
	<td class="line x" title="104:226	6 Features 6.1 Sense Subjectivity LCS Feature This feature measures the similarity of a target sense with members of the subjective seed set." ></td>
	<td class="line x" title="105:226	Here, similarity between two senses is determined by the extent to which they share information, measured by using the information content associated with their least common subsumer." ></td>
	<td class="line x" title="106:226	For an intuition behind this feature, consider this example." ></td>
	<td class="line x" title="107:226	In WordNet, the hypernym of the strong criticism sense of attack is criticism." ></td>
	<td class="line x" title="108:226	Several other negative subjective senses are descendants of criticism, including the relevant senses of fire, thrust, and rebuke." ></td>
	<td class="line x" title="109:226	Going up one more level, the hypernym of criticism is the expression of disapproval meaning of disapproval, which has several additional negative subjective descendants, such as the expression of opposition and disapproval sense of discouragement." ></td>
	<td class="line x" title="110:226	Our hypothesis is that the cases where subjectivity is preserved in the hypernym structure, or where hypernyms do lead from subjective senses to others, are the ones that have the highest least common subsumer score with the seed set of known subjective senses." ></td>
	<td class="line x" title="111:226	We calculate similarity using the informationcontent based measure proposed in (Resnik, 1995), as implemented in the WordNet::Similarity package (using the default option in which LCS values are computed over the SemCor corpus).3 Given a 3http://search.cpan.org/dist/WordNet-Similarity/ 13 taxonomy such as WordNet, the information content associated with a concept is determined as the likelihood of encountering that concept, defined as log(p(C)), where p(C) is the probability of seeing concept C in a corpus." ></td>
	<td class="line x" title="112:226	The similarity between two concepts is then defined in terms of information content as: LCSs(C1,C2) = max[log(p(C))], where C is the concept that subsumes both C1 and C2 and has the highest information content (i.e., it is the least common subsumer (LCS))." ></td>
	<td class="line x" title="113:226	For this feature, a score is assigned to a target sense based on its semantic similarity to the members of a seed set; in particular, the maximum such similarity is used." ></td>
	<td class="line x" title="114:226	For a target sense t and a seed set S, we could have used the following score: Score(t,S) = maxsS LCSs(t,s) However, several researchers have noted that subjectivity may be domain specific." ></td>
	<td class="line x" title="115:226	A version of WordNet exists, WordNet Domains (Gliozzo et al., 2005), which associates each synset with one of the domains in the Dewey Decimal library classification." ></td>
	<td class="line x" title="116:226	After sorting our subjective seed set into different domains, we observed that over 80% of the subjective seed senses are concentrated in six domains (the rest are distributed among 35 domains)." ></td>
	<td class="line x" title="117:226	Thus, we decided to particularize the semantic similarity feature to domain, such that only the subset of the seed set in the same domain as the target sense is used to compute the feature." ></td>
	<td class="line x" title="118:226	This involves much less calculation, as LCS values are calculated only with respect to a subset of the seed set." ></td>
	<td class="line x" title="119:226	We hypothesized that this would still be an effective feature, while being more efficient to calculate." ></td>
	<td class="line x" title="120:226	This will be important when this method is applied to large resources such as the entire WordNet." ></td>
	<td class="line x" title="121:226	Thus, for seed set S and target sense t which is in domain D, the feature is defined as the following score: SenseLCSscore(t,D,S) = max dDS LCSs(t,d) The seed set is a parameter, so we could have defined a feature reflecting similarity to the objective seed set as well." ></td>
	<td class="line x" title="122:226	Since WordNet is already highly skewed toward objective noun senses, any naive classifier need only guess the majority class for high accuracy for the objective senses." ></td>
	<td class="line x" title="123:226	We included only a subjective feature to put more emphasis on the subjective senses." ></td>
	<td class="line x" title="124:226	In the future, features could be defined with respect to objectivity, as well as polarity and other properties of subjectivity." ></td>
	<td class="line x" title="125:226	6.2 Domain Subjectivity LCS Score We also include a feature reflecting the subjectivity of the domain of the target sense." ></td>
	<td class="line x" title="126:226	Domains are assigned scores as follows." ></td>
	<td class="line x" title="127:226	For domain D and seed set S: DomainLCSscore(D,S) = avedDSMemLCSscore(d,D,S) where: MemLCSscore(d,D,S) = max diDS,dinegationslash=d LCSs(d,di) The value of this feature for a sense is the score assigned to that senses domain." ></td>
	<td class="line x" title="128:226	6.3 Common Related Senses This feature is based on the intersection between the set of senses related (via WordNet relations) to the target sense and the set of senses related to members of a seed set." ></td>
	<td class="line x" title="129:226	First, for the target sense and each member of the seed set, a set of related senses is formed consisting of its synonyms, antonyms and direct hypernyms as defined by WordNet." ></td>
	<td class="line x" title="130:226	For a sense s, R(s) is s together with its related senses." ></td>
	<td class="line x" title="131:226	Then, given a target sense t and a seed set S we compute an average percentage overlap as follows: RelOverlap(t,S) = summationtext siS |R(t)R(si)| max(|R(t)|,|R(si)|) |S| The value of a feature is its score." ></td>
	<td class="line x" title="132:226	Two features are included in the experiments below, one for each of the subjective and objective seed sets." ></td>
	<td class="line x" title="133:226	6.4 Gloss-based features These features are Lesk-style features (Lesk, 1986) that exploit overlaps between glosses of target and seed senses." ></td>
	<td class="line x" title="134:226	We include two types in our work." ></td>
	<td class="line x" title="135:226	6.4.1 Average Percentage Gloss Overlap Features For a sense s, gloss(s) is the set of stems in the gloss of s (excluding stop words)." ></td>
	<td class="line x" title="136:226	Then, given a tar14 get sense t and a seed set S, we compute an average percentage overlap as follows: GlOverlap(t,S) = summationtext siS |gloss(t)rR(si)gloss(r)| max(|gloss(t)|,|rR(si)gloss(r)|) |S| As above, R(s) is considered for each seed sense s, but now only the target sense t is considered, not R(t)." ></td>
	<td class="line x" title="137:226	We did this because we hypothesized that the gloss can provide sufficient context for a given target sense, so that the addition of related words is not necessary." ></td>
	<td class="line x" title="138:226	We include two features, one for each of the subjective and objective seed sets." ></td>
	<td class="line x" title="139:226	6.4.2 Vector Gloss Overlap Features For this feature we also consider overlaps of stems in glosses (excluding stop words)." ></td>
	<td class="line x" title="140:226	The overlaps considered are between the gloss of the target sense t and the glosses of R(s) for all s in a seed set (for convenience, we will refer to these as seedRelationSets)." ></td>
	<td class="line x" title="141:226	A vector of stems is created, one for each stem (excluding stop words) that appears in a gloss of a member of seedRelationSets." ></td>
	<td class="line x" title="142:226	If a stem in the gloss of the target sense appears in this vector, then the vector entry for that stem is the total count of that stem in the glosses of the target sense and all members of seedRelationSets." ></td>
	<td class="line x" title="143:226	A feature is created for each vector entry whose value is the count at that position." ></td>
	<td class="line x" title="144:226	Thus, these features consider counts of individual stems, rather than average proportions of overlaps, as for the previous type of gloss feature." ></td>
	<td class="line x" title="145:226	Two vectors of features are used, one where the seed set is the subjective seed set, and one where it is the objective seed set." ></td>
	<td class="line x" title="146:226	6.5 Summary In summary, we use the following features (here, SS is the subjective seed set and OS is the objective one)." ></td>
	<td class="line x" title="147:226	1." ></td>
	<td class="line x" title="148:226	SenseLCSscore(t,D,SS) 2." ></td>
	<td class="line x" title="149:226	DomainLCSscore(D,SS) 3." ></td>
	<td class="line x" title="150:226	RelOverlap(t,SS) 4." ></td>
	<td class="line x" title="151:226	RelOverlap(t,OS) 5." ></td>
	<td class="line x" title="152:226	GlOverlap(t,SS) 6." ></td>
	<td class="line x" title="153:226	GlOverlap(t,OS) Features Acc P R F All 77.3 72.8 74.3 73.5 Standalone Ablation Results All 77.3 72.8 74.3 73.5 LCS 68.2 69.3 44.2 54.0 Gloss vector 74.3 71.2 68.5 69.8 Overlaps 69.4 75.8 40.6 52.9 Leave-One-Out Ablation Results All 77.3 72.8 74.3 73.5 LCS 75.2 70.9 70.6 70.7 Gloss vector 75.0 74.4 61.8 67.5 Overlaps 74.8 71.9 73.8 72.8 Table 1: Results for the mixed corpus (2354 senses, 57.82% O)) 7." ></td>
	<td class="line x" title="154:226	Vector of gloss words (SS) 8." ></td>
	<td class="line x" title="155:226	Vector of gloss words (OS) 7 Experiments We perform 10-fold cross validation experiments on several data sets, using SVM light (Joachims, 1999)4 under its default settings." ></td>
	<td class="line x" title="156:226	Based on our random sampling of WordNet, it appears that WordNet nouns are highly skewed toward objective senses." ></td>
	<td class="line x" title="157:226	(Esuli and Sebastiani, 2007) argue that random sampling from WordNet would yield a corpus mostly consisting of objective (neutral) senses, which would be pretty useless as a benchmark for testing derived lexical resources for opinion mining [p. 428]. So, they use a mixture of subjective and objective senses in their data set." ></td>
	<td class="line x" title="158:226	To create a mixed corpus for our task, we annotated a second random sample from WordNet (which is as skewed as the previously mentioned one)." ></td>
	<td class="line x" title="159:226	We added together all of the senses of words in the lexicon which we annotated, the leftover senses from the selection of objective seed senses, and this new sample." ></td>
	<td class="line x" title="160:226	We removed duplicates, multiple senses from the same synset, and any senses belonging to the same synset in either of the seed sets." ></td>
	<td class="line x" title="161:226	This resulted in a corpus of 2354 senses, 993 (42.18%) of which are subjective and 1361 (57.82%) of which are objective." ></td>
	<td class="line x" title="162:226	The results with all of our features on this mixed corpus are given in Row 1 of Table 1." ></td>
	<td class="line x" title="163:226	In Table 1, the 4http://svmlight.joachims.org/ 15 first column identifies the features, which in this case is all of them." ></td>
	<td class="line x" title="164:226	The next three columns show overall accuracy, and precision and recall for finding subjective senses." ></td>
	<td class="line x" title="165:226	The baseline accuracy for the mixed data set (guessing the more frequent class, which is objective) is 57.82%." ></td>
	<td class="line x" title="166:226	As the table shows, the accuracy is substantially above baseline.5 7.1 Analysis and Discussion In this section, we seek to gain insights by performing ablation studies, evaluating our method on different data compositions, and comparing our results to previous results." ></td>
	<td class="line x" title="167:226	7.2 Ablation Studies Since there are several features, we divided them into sets for the ablation studies." ></td>
	<td class="line x" title="168:226	The vector-ofgloss-words features are the most similar to ones used in previous work." ></td>
	<td class="line x" title="169:226	Thus, we opted to treat them as one ablation group (Gloss vector)." ></td>
	<td class="line x" title="170:226	The Overlaps group includes the RelOverlap(t,SS), RelOverlap(t,OS), GlOverlap(t,SS), and GlOverlap(t,OS) features." ></td>
	<td class="line x" title="171:226	Finally, the LCS group includes the SenseLCSscore and the DomainLCSscore features." ></td>
	<td class="line x" title="172:226	There are two types of ablation studies." ></td>
	<td class="line x" title="173:226	In the first, one group of features at a time is included." ></td>
	<td class="line x" title="174:226	Those results are in the middle section of Table 1." ></td>
	<td class="line x" title="175:226	Thus, for example, the row labeled LCS in this section is for an experiment using only the LCS features." ></td>
	<td class="line x" title="176:226	In comparison to performance when all features are used, F-measure for the Overlaps and LCS ablations is significantly different at the p < .01 level, and, for the Gloss Vector ablation, it is significantly different at the p = .052 level (one-tailed t-test)." ></td>
	<td class="line x" title="177:226	Thus, all of the features together have better performance than any single type of feature alone." ></td>
	<td class="line x" title="178:226	In the second type of ablation study, we use all the features minus one group of features at a time." ></td>
	<td class="line x" title="179:226	The results are in the bottom section of Table 1." ></td>
	<td class="line x" title="180:226	Thus, for example, the row labeled LCS in this section is for an experiment using all but the LCS features." ></td>
	<td class="line x" title="181:226	F-measures for LCS and Gloss vector are significantly different at the p = .056 and p = .014 levels, respectively." ></td>
	<td class="line x" title="182:226	However, F-measure for the Overlaps ablation is not significantly different (p = .39)." ></td>
	<td class="line x" title="183:226	5Note that, because the majority class is O, baseline recall (and thus F-measure) is 0." ></td>
	<td class="line x" title="184:226	Data (#senses) Acc P R F mixed (2354 57.8% O) 77.3 72.8 74.3 73.5 strong+weak (1132) 77.7 76.8 78.9 77.8 weaksubj (566) 71.3 70.3 71.1 70.7 strongsubj (566) 78.6 78.8 78.6 78.7 Table 2: Results for different data sets (all are 50% S, unless otherwise notes) These results provide evidence that LCS and Gloss vector are better together than either of them alone." ></td>
	<td class="line x" title="185:226	7.3 Results on Different Data Sets Several methods have been developed for identifying subjective words." ></td>
	<td class="line x" title="186:226	Perhaps an effective strategy would be to begin with a word-level subjectivity lexicon, and then perform subjectivity sense labeling to sort the subjective from objective senses of those words." ></td>
	<td class="line x" title="187:226	We also wondered about the relative effectiveness of our method on strongsubj versus weaksubj clues." ></td>
	<td class="line x" title="188:226	To answer these questions, we apply the full model (again in 10-fold cross validation experiments) to data sets composed of senses of polysemous words in the subjectivity lexicon." ></td>
	<td class="line x" title="189:226	To support comparison, all of the data sets in this section have a 50%-50% objective/subjective distribution.6 The results are presented in Table 2." ></td>
	<td class="line x" title="190:226	For comparison, the first row repeats the results for the mixed corpus from Table 1." ></td>
	<td class="line x" title="191:226	The second row shows results for a corpus of senses of a mixture of strongsubj and weaksubj words." ></td>
	<td class="line x" title="192:226	The corpus was created by selecting a mixture of strongsubj and weaksubj words, extracting their senses and the S/O labels applied to them in Section 4, and then randomly removing senses of the more frequent class until the distribution is uniform." ></td>
	<td class="line x" title="193:226	We see that the results on this corpus are better than on the mixed data set, even though the baseline accuracy is lower and the corpus is smaller." ></td>
	<td class="line x" title="194:226	This supports the idea that an effective strategy would be to first identify opinion-bearing words, and then apply our method to those words to sort out their subjective and objective senses." ></td>
	<td class="line x" title="195:226	The third row shows results for a weaksubj subset 6As with the mixed data set, we removed from these data sets multiple senses from the same synset and any senses in the same synset in either of the seed sets." ></td>
	<td class="line x" title="196:226	16 Method P R F Our method 56.8 66.0 61.1 WM, 60% recall 44.0 66.0 52.8 SentiWordNet mapping 60.0 17.3 26.8 Table 3: Results for WM Corpus (212 senses, 76% O) Method A P R F Our Method 81.3% 60.3% 63.3% 61.8% SM CV* 82.4% 70.8% 41.1% 52.0% SM SL* 78.3% 53.0% 57.4% 54.9% Table 4: Results for SM Corpus (484 senses, 76.9% O) of the strong+weak corpus and the fourth shows results for a strongsubj subset that is of the same size." ></td>
	<td class="line x" title="197:226	As expected, the results for the weaksubj senses are lower while those for the strongsubj senses are higher, as weaksubj clues are more ambiguous." ></td>
	<td class="line x" title="198:226	7.4 Comparisons with Previous Work WM and SM address the same task as we do." ></td>
	<td class="line x" title="199:226	To compare our results to theirs, we apply our full model (in 10-fold cross validation experiments) to their data sets.7 Table 3 has the WM data set results." ></td>
	<td class="line x" title="200:226	WM rank their senses and present their results in the form of precision recall curves." ></td>
	<td class="line x" title="201:226	The second row of Table 3 shows their results at the recall level achieved by our method (66%)." ></td>
	<td class="line x" title="202:226	Their precision at that level is substantially below ours." ></td>
	<td class="line x" title="203:226	Turning to ES, to create S/O annotations, we applied the following heuristic mapping (which is also used by SM for the purpose of comparison): any sense for which the sum of positive and negative scores is greater than or equal to 0.5 is S, otherwise it is O. We then evaluate the mapped tags against the gold standard of WM." ></td>
	<td class="line x" title="204:226	The results are in Row 3 of Table 3." ></td>
	<td class="line x" title="205:226	Note that this mapping is not fair to SentiWordNet, as the tasks are quite different, and we do not believe any conclusions can be drawn." ></td>
	<td class="line x" title="206:226	We include the results to eliminate the possibility that their method is as good ours on our task, despite the differences between the tasks." ></td>
	<td class="line x" title="207:226	Table 4 has the results for the noun subset of SMs 7The WM data set is available at http://www.cs.pitt.edu/www.cs.pitt.edu/wiebe." ></td>
	<td class="line x" title="208:226	ES applied their method in (2006b) to WordNet, and made the results available as SentiWordNet at http://sentiwordnet.isti.cnr.it/." ></td>
	<td class="line x" title="209:226	data set, which is the data set used by ES, reannotated by SM." ></td>
	<td class="line x" title="210:226	CV* is their supervised system and SL* is their best non-supervised one." ></td>
	<td class="line x" title="211:226	Our method has higher F-measure than the others.8 Note that the focus of SMs work is not supervised machine learning." ></td>
	<td class="line x" title="212:226	8 Conclusions In this paper, we introduced an integrative approach to automatic subjectivity word sense labeling which combines features exploiting the hierarchical structure and domain information of WordNet, as well as similarity of glosses and overlap among sets of semantically related words." ></td>
	<td class="line x" title="213:226	There are several contributions." ></td>
	<td class="line x" title="214:226	First, we learn several things." ></td>
	<td class="line x" title="215:226	We found (in Section 4) that even reliable lists of subjective (opinion-bearing) words have many objective senses." ></td>
	<td class="line x" title="216:226	We asked if wordand sense-level approaches could be used effectively in tandem, and found (in Section 7.3) that an effective strategy is to first identify opinion-bearing words, and then apply our method to sort out their subjective and objective senses." ></td>
	<td class="line x" title="217:226	We also found (in Section 7.2) that the entire set of features gives better results than any individual type of feature alone." ></td>
	<td class="line x" title="218:226	Second, several of the features are novel for our task, including those exploiting the hierarchical structure of a lexical resource, domain information, and relations to seed sets expanded with monosemous senses." ></td>
	<td class="line x" title="219:226	Finally, the combination of our particular features is effective." ></td>
	<td class="line x" title="220:226	For example, on senses of words from a subjectivity lexicon, accuracies range from 20 to 29 percentage points above baseline." ></td>
	<td class="line x" title="221:226	Further, our combination of features outperforms previous approaches." ></td>
	<td class="line x" title="222:226	Acknowledgments This work was supported in part by National Science Foundation awards #0840632 and #0840608." ></td>
	<td class="line x" title="223:226	The authors are grateful to Fangzhong Su and Katja Markert for making their data set available, and to the three paper reviewers for their helpful suggestions." ></td>
	<td class="line x" title="224:226	8We performed the same type of evaluation as in SMs paper." ></td>
	<td class="line x" title="225:226	That is, we assign a subjectivity label to one word sense for each synset, which is the same as applying a subjectivity label to a synset as a whole as done by SM." ></td>
	<td class="line x" title="226:226	17" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-1065
Graph-Cut-Based Anaphoricity Determination for Coreference Resolution
Ng, Vincent;"></td>
	<td class="line x" title="1:212	Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 575583, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:212	c 2009 Association for Computational Linguistics Graph-Cut-Based Anaphoricity Determination for Coreference Resolution Vincent Ng Human Language Technology Research Institute University of Texas at Dallas Richardson, TX 75083-0688 vince@hlt.utdallas.edu Abstract Recent work has shown that explicitly identifying and filtering non-anaphoric mentions prior to coreference resolution can improve the performance of a coreference system." ></td>
	<td class="line x" title="3:212	We present a novel approach to this task of anaphoricity determination based on graph cuts, and demonstrate its superiority to competing approaches by comparing their effectiveness in improving a learning-based coreference system on the ACE data sets." ></td>
	<td class="line x" title="4:212	1 Introduction Coreference resolution is the problem of identifying which noun phrases (NPs, or mentions) refer to the same real-world entity in a text or dialogue." ></td>
	<td class="line x" title="5:212	According to Webber (1979), coreference resolution can be decomposed into two complementary tasks: (1) identifying what a text potentially makes available for anaphoric reference and (2) constraining the candidate set of a given anaphoric expression down to one possible choice. The first task is nowadays typically formulated as an anaphoricity determination task, which aims to classify whether a given mention is anaphoric or not." ></td>
	<td class="line x" title="6:212	Knowledge of anaphoricity could improve the precision of a coreference system, since non-anaphoric mentions do not have an antecedent and therefore do not need to be resolved." ></td>
	<td class="line x" title="7:212	Previous work on anaphoricity determination can be broadly divided into two categories (see Poesio et al.(2004) for an overview)." ></td>
	<td class="line x" title="9:212	Research in the first category aims to identify specific types of nonanaphoric phrases, with some identifying pleonastic it (using heuristics [e.g., Paice and Husk (1987), Lappin and Leass (1994), Kennedy and Boguraev (1996)], supervised approaches [e.g., Evans (2001), Muller (2006), Versley et al.(2008)], and distributional methods [e.g., Bergsma et al.(2008)]), and others identifying non-anaphoric definite descriptions (using rule-based techniques [e.g., Vieira and Poesio (2000)] and unsupervised techniques [e.g., Bean and Riloff (1999)])." ></td>
	<td class="line x" title="12:212	On the other hand, research in the second category focuses on (1) determining the anaphoricity of all types of mentions, and (2) using the resulting anaphoricity information to improve coreference resolution." ></td>
	<td class="line x" title="13:212	For instance, Ng and Cardie (2002a) train an anaphoricity classifier to determine whether a mention is anaphoric, and let an independentlytrained coreference system resolve only those mentions that are classified as anaphoric." ></td>
	<td class="line x" title="14:212	Somewhat surprisingly, they report that using anaphoricity information adversely affects the performance of their coreference system, as a result of an overly conservative anaphoricity classifier that misclassifies many anaphoric mentions as non-anaphoric." ></td>
	<td class="line x" title="15:212	One solution to this problem is to use anaphoricity information as soft constraints rather than as hard constraints for coreference resolution." ></td>
	<td class="line x" title="16:212	For instance, when searching for the best partition of a set of mentions, Luo (2007) combines the probabilities returned by an anaphoricity model and a coreference model to score a coreference partition, such that a partition is penalized whenever an anaphoric mention is resolved." ></td>
	<td class="line x" title="17:212	Another, arguably more popular, solution is to improve the output of the anaphoricity classifier by exploiting the dependency between anaphoricity determination and coreference resolu575 tion." ></td>
	<td class="line x" title="18:212	For instance, noting that Ng and Cardies anaphoricity classifier is too conservative, Ng (2004) first parameterizes their classifier such that its conservativeness can be varied, and then tunes this parameter so that the performance of the coreference system is maximized." ></td>
	<td class="line x" title="19:212	As another example, Denis and Baldridge (2007) and Finkel and Manning (2008) perform joint inference for anaphoricity determination and coreference resolution, by using Integer Linear Programming (ILP) to enforce the consistency between the output of the anaphoricity classifier and that of the coreference classifier." ></td>
	<td class="line x" title="20:212	While this ILP approach and Ngs (2004) approach to improving the output of an anaphoricity classifier both result in increased coreference performance, they have complementary strengths and weaknesses." ></td>
	<td class="line x" title="21:212	Specifically, Ngs approach can directly optimize the desired coreference evaluation metric, but by treating the coreference system as a black box during the optimization process, it does not exploit the potentially useful pairwise probabilities provided by the coreference classifier." ></td>
	<td class="line x" title="22:212	On the other hand, the ILP approach does exploit such pairwise probabilities, but optimizes an objective function that does not necessarily have any correlation with the desired evaluation metric." ></td>
	<td class="line x" title="23:212	Our goals in this paper are two-fold." ></td>
	<td class="line x" title="24:212	First, motivated in part by previous work, we propose a graphcut-based approach to anaphoricity determination that combines the strengths of Ngs approach and the ILP approach, by exploiting pairwise coreference probabilities when co-ordinating anaphoricity and coreference decisions, and at the same time allowing direct optimization of the desired coreference evaluation metric." ></td>
	<td class="line x" title="25:212	Second, we compare our cut-based approach with the five aforementioned approaches to anaphoricity determination (namely, Ng and Cardie (2002a), Ng (2004), Luo (2007), Denis and Baldridge (2007), and Finkel and Manning (2008)) in terms of their effectiveness in improving a learning-based coreference system." ></td>
	<td class="line x" title="26:212	To our knowledge, there has been no attempt to perform a comparative evaluation of existing approaches to anaphoricity determination." ></td>
	<td class="line x" title="27:212	It is worth noting, in particular, that Luo (2007), Denis and Baldridge (2007), and Finkel and Manning (2008) evaluate their approaches on true mentions extracted from the answer keys." ></td>
	<td class="line x" title="28:212	Since true mentions are composed of all the NPs involved in coreference relations but only a subset of the singleton NPs (i.e., NPs that are not coreferent with any other NPs) in a text, evaluating the utility of anaphoricity determination on true mentions to some extent defeats the purpose of performing anaphoricity determination, which precisely aims to identify non-anaphoric mentions." ></td>
	<td class="line x" title="29:212	Hence, we hope that our evaluation on mentions extracted using an NP chunker can reveal their comparative strengths and weaknesses." ></td>
	<td class="line x" title="30:212	We perform our evaluation on three ACE coreference data sets using two commonly-used scoring programs." ></td>
	<td class="line x" title="31:212	Experimental results show that (1) employing our cut-based approach to anaphoricity determination yields a coreference system that achieves the best performance for all six dataset/scoring-program combinations, and (2) among the five existing approaches, none performs consistently better than the others." ></td>
	<td class="line x" title="32:212	The rest of the paper is organized as follows." ></td>
	<td class="line x" title="33:212	Section 2 describes our learning-based coreference system." ></td>
	<td class="line x" title="34:212	In Section 3, we give an overview of the five baseline approaches to anaphoricity determination." ></td>
	<td class="line x" title="35:212	Section 4 provides the details of our graph-cut-based approach." ></td>
	<td class="line x" title="36:212	Finally, we present evaluation results in Section 5 and conclude in Section 6." ></td>
	<td class="line x" title="37:212	2 Baseline Coreference Resolution System Our baseline coreference system implements the standard machine learning approach to coreference resolution (see Ng and Cardie (2002b), Ponzetto and Strube (2006), Yang and Su (2007), for instance), which consists of probabilistic classification and clustering, as described below." ></td>
	<td class="line x" title="38:212	2.1 The Standard Machine Learning Approach We use maximum entropy (MaxEnt) classification (Berger et al., 1996) in conjunction with the 33 features described in Ng (2007) to acquire a model, PC, for determining the probability that two mentions, mi and mj, are coreferent." ></td>
	<td class="line x" title="39:212	Hence, PC(mi,mj) = P(COREFERENT | mi,mj)." ></td>
	<td class="line x" title="40:212	In the rest of the paper, we will refer to PC(mi,mj) as the pairwise coreference probability between mi and mj." ></td>
	<td class="line x" title="41:212	To generate training instances, we employ Soon et al.s (2001) procedure, relying on the training texts to create (1) a positive instance for 576 each anaphoric mention, mj, and its closest antecedent, mi; and (2) a negative instance for mj paired with each of the intervening mentions, mi+1, mi+2,, mj1." ></td>
	<td class="line x" title="42:212	When training the feature-weight parameters of the MaxEnt model, we use 100 iterations of the improved iterative scaling (IIS) algorithm (Della Pietra et al., 1997) together with a Gaussian prior to prevent overfitting." ></td>
	<td class="line x" title="43:212	After training, the coreference model is used to select an antecedent for each mention in a test text." ></td>
	<td class="line x" title="44:212	Following Soon et al.(2001), we select as the antecedent of each mention, mj, the closest preceding mention that is classified as coreferent with mj, where mention pairs with pairwise probabilities of at least 0.5 are considered coreferent." ></td>
	<td class="line x" title="46:212	If no such mention exists, no antecedent will be selected for mj." ></td>
	<td class="line x" title="47:212	In essence, we use a closest-first clustering algorithm to impose a partitioning on the mentions." ></td>
	<td class="line x" title="48:212	3 Baseline Approaches to Anaphoricity Determination As mentioned previously, we will use five existing approaches to anaphoricity determination as baselines in our evaluation." ></td>
	<td class="line x" title="49:212	Common to all five approaches is the acquisition of an anaphoricity model, PA, for determining the probability that a mention, mj, is anaphoric." ></td>
	<td class="line x" title="50:212	Hence, PA(mj) = P(ANAPHORIC | mj) To train PA, we again employ MaxEnt modeling, and create one training instance from each mention in a training text." ></td>
	<td class="line x" title="51:212	Hence, each instance represents a single mention and consists of 37 features that are potentially useful for distinguishing anaphoric and non-anaphoric mentions (see Ng and Cardie (2002a) for a detailed description of these features).1 The classification of a training instance  one of ANAPHORIC or NOT ANAPHORIC  is derived directly from the coreference chains in the associated training text." ></td>
	<td class="line x" title="52:212	Like the coreference model, the anaphoricity model is trained by running 100 iterations of IIS with a Guassian prior." ></td>
	<td class="line x" title="53:212	The resulting model is then applied to a test text to determine the 1While we train the anaphoricity model using the Ng and Cardie (2002a) feature set, it should be clear that any features that are useful for distinguishing anaphoric and non-anaphoric mentions can be used (e.g., those proposed by Uryupina (2003) and Elsner and Charniak (2007))." ></td>
	<td class="line x" title="54:212	probability that a mention is anaphoric." ></td>
	<td class="line x" title="55:212	In the rest of this section, we provide an overview of the five baseline approaches to anaphoricity determination." ></td>
	<td class="line x" title="56:212	We will characterize each approach along two dimensions: (1) whether it attempts to improve PA, and if so, how; and (2) whether the resulting anaphoricity information is used as hard constraints or soft constraints by the coreference system." ></td>
	<td class="line x" title="57:212	3.1 Ng and Cardie (2002a) Ng and Cardie (N&C) do not attempt to improve PA, simply using the anaphoricity information it provides as hard constraints for coreference resolution." ></td>
	<td class="line x" title="58:212	Specifically, the coreference system resolves only those mentions that are determined as anaphoric by PA, where a mention is classified as anaphoric if the classification threshold is at least 0.5." ></td>
	<td class="line x" title="59:212	3.2 Ng (2004) PA may not be sufficiently accurate, however, as N&C report a significant drop in the performance of their coreference system after incorporating anaphoricity information, owing in part to their overly conservative anaphoricity model that misclassifies many anaphoric mentions as nonanaphoric." ></td>
	<td class="line x" title="60:212	To address this problem, Ng (2004) attempts to improve PA by introducing a threshold parameter c to adjust the conservativeness of PA as follows." ></td>
	<td class="line x" title="61:212	Given a specific c (0  c  1), a mention mj is classified as anaphoric by PA if and only if PA(mj)  c. It should be easy to see that decreasing c yields progressively less conservative anaphoricity models (i.e., more mentions will be classified as anaphoric)." ></td>
	<td class="line x" title="62:212	The parameter c is tuned using held-out development data to optimize the performance of the coreference system that employs anaphoricity information (as hard constraints)." ></td>
	<td class="line x" title="63:212	In essence, Ngs approach to improving PA treats the coreference system as a black box, merely selecting the value for c that yields the best score according to the desired coreference evaluation metric on the held-out data." ></td>
	<td class="line x" title="64:212	In particular, unlike some of the anaphoricity determination approaches discussed later on, this approach does not attempt to coordinate the anaphoricity decisions and the pairwise coreference decisions." ></td>
	<td class="line x" title="65:212	Nevertheless, as mentioned before, a unique strength of this approach lies in its ability to optimize directly the desired coreference 577 evaluation metric." ></td>
	<td class="line x" title="66:212	3.3 Luo (2007) Among the five anaphoricity determination approaches, Luos (2007) is the only one where anaphoricity information is exploited as soft constraints by the coreference model, PC." ></td>
	<td class="line x" title="67:212	Specifically, Luos algorithm attempts to find the most probable coreference partition of a given set of mentions." ></td>
	<td class="line x" title="68:212	To do so, it scores a partition using the probabilities provided by PA and PC." ></td>
	<td class="line x" title="69:212	Let us illustrate how this can be done via the following example." ></td>
	<td class="line x" title="70:212	Given a document with four mentions, m1,,m4, and a partition of the mentions, {[m1,m3,m4],[m2]}, automatically produced by some coreference system, Luos algorithm scores the partition by considering the mentions in the document in a left-to-right manner." ></td>
	<td class="line x" title="71:212	As the first mention in the document, m1 is not anaphoric, and the probability that it is non-anaphoric is 1  PA(m1)." ></td>
	<td class="line x" title="72:212	Then, the algorithm processes m2, which according to the partition is non-anaphoric, and the probability of its being non-anaphoric is 1  PA(m2)." ></td>
	<td class="line x" title="73:212	Next, it processes m3, which is coreferent with m1 with probability PC(m1,m3)." ></td>
	<td class="line x" title="74:212	Finally, it processes m4, which is coreferent with m1 and m3." ></td>
	<td class="line x" title="75:212	The probability that m4 is coreferent with the cluster consisting of m1 and m3 is defined to be max(PC(m1,m4),PC(m3,m4)), according to Luos algorithm." ></td>
	<td class="line x" title="76:212	The score of this partition is the product of these four probabilities, two provided by PA and two by PC." ></td>
	<td class="line x" title="77:212	As can be seen, a partition is penalized whenever a mention that is unlikely to be anaphoric (according to PA) is being resolved to some antecedent according to the partition." ></td>
	<td class="line x" title="78:212	Nevertheless, it is computationally infeasible to score all possible partitions given a set of mentions, as the number of partitions is exponential in the number of mentions." ></td>
	<td class="line x" title="79:212	To cope with this computational complexity, Luo employs the algorithm proposed in Luo et al.(2004) to heuristically search for the most probable partition by performing a beam search through a Bell tree." ></td>
	<td class="line x" title="81:212	In essence, only the most promising nodes in the tree are expanded at each step of the search process, where the promise of a node is defined in terms of the probabilities provided by PA and PC, as described above." ></td>
	<td class="line x" title="82:212	Details of this process can be found in Luo et al.(2004)." ></td>
	<td class="line x" title="84:212	3.4 Denis and Baldridge (2007) As mentioned before, Denis and Baldridge (D&B) aim to improve the outputs of PA and PC by employing Integer Linear Programming (ILP) to perform joint inference for anaphoricity determination and coreference resolution." ></td>
	<td class="line x" title="85:212	The ILP approach is motivated by the observation that the outputs of these two models have to satisfy certain constraints." ></td>
	<td class="line x" title="86:212	For instance, if PC determines that a mention, mj, is not coreferent with any other mentions in the associated text, then PA should determine that mj is non-anaphoric." ></td>
	<td class="line x" title="87:212	In practice, however, since PA and PC are trained independently of each other, this and other constraints cannot be enforced." ></td>
	<td class="line x" title="88:212	ILP provides a framework for jointly determining anaphoricity and coreference decisions for a given set of mentions based on the probabilities provided by PA and PC, such that the resulting joint decisions satisfy the desired constraints while respecting as much as possible the probabilistic decisions made by the independently-trained PA and PC." ></td>
	<td class="line x" title="89:212	Specifically, an ILP program is composed of an objective function to be optimized subject to a set of linear constraints, and is created for each test text D as follows." ></td>
	<td class="line x" title="90:212	Let M be the set of mentions in D, and P be the set of mention pairs formed from M (i.e., P = {(mi,mj) | mi,mj  M,i < j})." ></td>
	<td class="line x" title="91:212	Each ILP program has a set of indicator variables." ></td>
	<td class="line x" title="92:212	In our case, we have one binary-valued variable for each anaphoricity decision and coreference decision to be made by an ILP solver." ></td>
	<td class="line x" title="93:212	Following D&Bs notation, we use yj to denote the anaphoricity decision for mention mj, and xi,j to denote the coreference decision involving mentions mi and mj." ></td>
	<td class="line x" title="94:212	In addition, each variable is associated with an assignment cost." ></td>
	<td class="line x" title="95:212	Specifically, let cCi,j = log(PC(mi,mj)) be the cost of setting xi,j to 1, and cCi,j = log(1  PC(mi,mj)) be the complementary cost of setting xi,j to 0." ></td>
	<td class="line x" title="96:212	We can similarly define the cost associated with each yj, letting cAj = log(PA(mj)) be the cost of setting yj to 1, and cAj = log(1 PA(mj)) be the complementary cost of setting yj to 0." ></td>
	<td class="line x" title="97:212	Given these costs, we aim to optimize the following objective function: min summationdisplay (mi,mj)P cCi,j  xi,j + cCi,j  (1 xi,j) + summationdisplay mjM cAj  yj + cAj  (1 yj) 578 subject to a set of manually-specified linear constraints." ></td>
	<td class="line x" title="98:212	D&B specify four types of constraints: (1) each indicator variable can take on a value of 0 or 1; (2) if mi and mj are coreferent (xi,j=1), then mj is anaphoric (yj=1); (3) if mj is anaphoric (yj=1), then it must be coreferent with some preceding mention mi; and (4) if mj is non-anaphoric, then it cannot be coreferent with any mention." ></td>
	<td class="line x" title="99:212	Note that we are minimizing the objective function, since each assignment cost is expressed as a negative logarithm value." ></td>
	<td class="line x" title="100:212	We use lp solve2, an ILP solver, to solve this program." ></td>
	<td class="line x" title="101:212	It is easy to see that enforcing consistency using ILP amounts to employing anaphoricity information as hard constraints for the coreference system." ></td>
	<td class="line x" title="102:212	Since transitivity is not guaranteed by the above constraints, we follow D&B and use the aggressivemerge clustering algorithm to put any two mentions that are posited as coreferent into the same cluster." ></td>
	<td class="line x" title="103:212	3.5 Finkel and Manning (2008) Finkel and Manning (F&M) present one simple extension to D&Bs ILP approach: augmenting the set of linear constraints with the transitivity constraint." ></td>
	<td class="line x" title="104:212	This ensures that if xi,j=1 and xj,k=1, then xi,k=1." ></td>
	<td class="line x" title="105:212	As a result, the coreference decisions do not need to be co-ordinated by a separate clustering mechanism." ></td>
	<td class="line x" title="106:212	4 Cut-Based Anaphoricity Determination As mentioned in the introduction, our graph-cutbased approach to anaphoricity determination is motivated by Ngs (2004) and the ILP approach, aiming to combine the strengths of the two approaches." ></td>
	<td class="line x" title="107:212	Specifically, like Ng (2004), our approach allows direct optimization of the desired coreference evaluation metric; and like the ILP approach, our approach co-ordinates anaphoricity decisions and coreference decisions by exploiting the pairwise probabilities provided by a coreference model." ></td>
	<td class="line x" title="108:212	In this section, we will introduce our cut-based approach, starting by reviewing concepts related to minimum cuts." ></td>
	<td class="line x" title="109:212	4.1 The Minimum Cut Problem Setting Assume that we want to partition a set of n objects, {x1,x2,,xn}, into two sets, Y1 and Y2." ></td>
	<td class="line x" title="110:212	We have two types of scores concerning the xs and the Y s: 2Available from http://lpsolve.sourceforge.net/ membership scores and similarity scores." ></td>
	<td class="line x" title="111:212	The membership score, memYi(xj), is a non-negative quantity that approximates the affinity of xj to Yi." ></td>
	<td class="line x" title="112:212	On the other hand, the similarity score, sim(xj,xk), is a non-negative quantity that provides an estimate of the similarity between xj and xk." ></td>
	<td class="line x" title="113:212	Informally, our goal is to maximize each objects net happiness, which is computed by subtracting its membership score of the class it is not assigned to from its membership score of the class it is assigned to." ></td>
	<td class="line x" title="114:212	However, at the same time, we want to avoid assigning similar objects to different classes." ></td>
	<td class="line x" title="115:212	More formally, we seek to minimize the partition cost:summationdisplay xjY1,xkY2 sim(xj,xk)+ summationdisplay xY1 memY2(x)+ summationdisplay xY2 memY1(x) There exists an efficient algorithm for solving this seemingly intractable problem when it is recast as a graph problem." ></td>
	<td class="line x" title="116:212	So, let us construct a graph, G, based on the available scores as follows." ></td>
	<td class="line x" title="117:212	First, we create two nodes, s and t (called the source and the sink, respectively), to represent the two classes." ></td>
	<td class="line x" title="118:212	Then, we create one object node for each of the n objects." ></td>
	<td class="line x" title="119:212	For each object, xj, we add two directed edges, one from s to xj (with weight memY1(xj)) and the other from xj to t (with weight memY2(xj))." ></td>
	<td class="line x" title="120:212	Moreover, for each pair of object nodes, xj and xk, we add two directed edges (one from xj to xk and another from xk to xj), both of which have weight sim(xj,xk)." ></td>
	<td class="line x" title="121:212	A cut in G is defined as a partition of the nodes into two sets, S and T, such that s  S, t  T; and the cost of the cut, cost(S,T), is the sum of the weights of the edges going from S to T. A minimum cut is a cut that has the lowest cost among all the cuts of G. It can be proved that finding a minimum cut of G is equivalent to minimizing the partition cost defined as above." ></td>
	<td class="line x" title="122:212	The main advantage of recasting the above minimization problem as a graph-cut problem is that there exist polynomialtime maxflow algorithms for finding a minimum cut." ></td>
	<td class="line x" title="123:212	4.2 Graph Construction Next, we show how to construct the graph to which the mincut-finding algorithm will be applied." ></td>
	<td class="line x" title="124:212	The ultimate goal is to use the mincut finder to partition a given set of mentions into two subsets, so that our coreference system will attempt to resolve only those mentions that are in the subset corresponding to ANAPHORIC." ></td>
	<td class="line x" title="125:212	In other words, the resulting 579 anaphoricity information will be used to identify and filter non-anaphoric mentions prior to coreference resolution." ></td>
	<td class="line x" title="126:212	The graph construction process, which takes as input a set of mentions in a test text, is composed of three steps, as described below." ></td>
	<td class="line x" title="127:212	Step 1: Mimicking Ng and Cardie (2002a) To construct the desired graph, G, we first create the source, s, and the sink, t, that represent the classes ANAPHORIC and NOT ANAPHORIC, respectively." ></td>
	<td class="line x" title="128:212	Then, for each mention mn in the input text, we create one node, n, and two edges, sn and nt, connecting n to s and t. Next, we compute wsn and wnt, the weights associated with sn and nt." ></td>
	<td class="line x" title="129:212	A natural choice would be to use PA(mn) as the weight of sn and (1wsn) as the weight of nt." ></td>
	<td class="line x" title="130:212	(We will assume throughout that wnt is always equal to 1  wsn.)" ></td>
	<td class="line x" title="131:212	If we apply the mincut finder to the current G, it should be easy to see that (1) any node n where wsn > 0.5 will be assigned to s, (2) any node n where wsn < 0.5 will be assigned to t, and (3) any remaining node will be assigned to one of them." ></td>
	<td class="line x" title="132:212	(Without loss of generality, we assume that such nodes are assigned to s.)" ></td>
	<td class="line x" title="133:212	Hence, the set of mentions determined as anaphoric by the mincut finder is identical to the set of mentions classified as anaphoric by PA, thus yielding a coreference system that is functionally equivalent to N&Cs. This also implies that G shares the same potential weakness as PA: being overly conservative in determining a mention as anaphoric." ></td>
	<td class="line x" title="134:212	Step 2: Mimicking Ng (2004) One way to improve G is to make it functionally equivalent to Ngs (2004) approach." ></td>
	<td class="line x" title="135:212	Specifically, our goal in Step 2 is to modify the edge weights in G (without adding new edges or nodes) such that the mincut finder classifies a node n as anaphoric if and only if PA(mn)  c for some c  [0,1]." ></td>
	<td class="line x" title="136:212	Now, recall from Step 1 that the mincut finder classifies a node n as anaphoric if and only if wsn  0.5." ></td>
	<td class="line x" title="137:212	Hence, to achieve the aforementioned goal, we just need to ensure the property that wsn  0.5 if and only if PA(mn)  c. Consequently, we compute wsn using a sigmoid function: wsn = 11 + e(P A(mn)c) where  is a constant that controls the steepness of the sigmoid.3 It should be easy to verify that the sigmoid satisfies the aforementioned property." ></td>
	<td class="line x" title="138:212	As noted before, wnt = 1  wsn for each node n. Inspired by Ng (2004), the value of the parameter c will be tuned based on held-out development data to maximize coreference performance." ></td>
	<td class="line x" title="139:212	Step 3: Incorporating coreference probabilities Like Ngs (2004) approach, the current G suffers from the weakness of not exploiting the pairwise probabilities provided by PC." ></td>
	<td class="line x" title="140:212	Fortunately, these probabilities can be naturally incorporated into G as similarity scores." ></td>
	<td class="line x" title="141:212	To see why these pairwise probabilities are potentially useful, consider two mentions, mi and mj, in a text D that are coreferent and are both anaphoric." ></td>
	<td class="line x" title="142:212	Assume that the graph G constructed from D has these edge weights: wsi = 0.8, wsj = 0.3, and wij = wji = 0.8." ></td>
	<td class="line x" title="143:212	Without the similarity scores, the mincut finder will correctly determine mi as anaphoric but incorrectly classify mj as non-anaphoric." ></td>
	<td class="line x" title="144:212	On the other hand, if the similarity scores are taken into account, the mincut finder will correctly determine both mentions as anaphoric." ></td>
	<td class="line x" title="145:212	The above discussion suggests that it is desirable to incorporate edges between two nodes, i and j, when mi and mj are likely to be coreferent (i.e., PC(mi,mj)  c2 for some constant c2)." ></td>
	<td class="line x" title="146:212	In our implementation, we tune this new parameter, c2, jointly with c (see Step 2) on development data to maximize coreference performance." ></td>
	<td class="line x" title="147:212	While it is possible to imagine scenarios where incorporating pairwise probabilities is not beneficial, we believe that these probabilities represent a source of information that could be profitably exploited via learning appropriate values for c and c2.4 3One of the main reasons why we use a sigmoid function (rather than a linear function) is that the weights will still fall within the [0,1] interval after the transformation, a property that will turn out to be convenient when the pairwise coreference probabilities are incorporated (see Step 3)." ></td>
	<td class="line x" title="148:212	 is chosen so that the difference between two weights after the transformation is as close as possible to their difference before the transformation." ></td>
	<td class="line x" title="149:212	With this criterion in mind, we set  to 0.42 in our experiments." ></td>
	<td class="line x" title="150:212	4Incorporating the coreference probabilities can potentially identify some of the anaphoric mentions that would be misclassified otherwise." ></td>
	<td class="line x" title="151:212	However, note that the minimum cut algorithm does not maintain the notion of directionality that would allow one to determine that a discourse-new mention (i.e., the first mention of a coreference chain) is not anaphoric." ></td>
	<td class="line x" title="152:212	In particular, the algorithm tends to classify all members of a coreference chain, including the first mention, as anaphoric." ></td>
	<td class="line x" title="153:212	We did not ex580 5 Evaluation 5.1 Experimental Setup For evaluation, we use the ACE Phase II coreference corpus, which is composed of three sections: Broadcast News (BNEWS), Newspaper (NPAPER), and Newswire (NWIRE)." ></td>
	<td class="line x" title="154:212	Each section is in turn composed of a training set and a test set." ></td>
	<td class="line x" title="155:212	For each section, we train an anaphoricity model, PA, and a coreference model, PC, on the training set, and evaluate PC (when used in combination with different approaches to anaphoricity determination) on the test set." ></td>
	<td class="line x" title="156:212	As noted before, the mentions used are extracted automatically using an in-house NP chunker." ></td>
	<td class="line x" title="157:212	Results are reported in terms of recall (R), precision (P), and F-measure (F), obtained using two coreference scoring programs: the MUC scorer (Vilain et al., 1995) and the CEAF scorer (Luo, 2005)." ></td>
	<td class="line x" title="158:212	5.2 Results and Discussions No Anaphoricity baseline." ></td>
	<td class="line x" title="159:212	Our first baseline is the learning-based coreference system described in Section 2, which does not employ any anaphoricity determination algorithm." ></td>
	<td class="line x" title="160:212	Results using the MUC scorer and the CEAF scorer are shown in row 1 of Tables 1 and 2, respectively." ></td>
	<td class="line x" title="161:212	As we can see, MUC F-score ranges from 55.0 to 61.7 and CEAF F-score ranges from 55.3 to 61.2." ></td>
	<td class="line x" title="162:212	Duplicated Ng and Cardie (2002a) baseline." ></td>
	<td class="line x" title="163:212	Next, we evaluate our second baseline, which is N&Cs coreference system." ></td>
	<td class="line x" title="164:212	As seen from row 2 of Tables 1 and 2, MUC F-score ranges from 50.5 to 60.0 and CEAF F-score ranges from 54.5 to 59.4." ></td>
	<td class="line x" title="165:212	In comparison to the first baseline, we see drops in F-score in all cases as a result of considerable precipitation in recall, which can in turn be attributed to the misclassification of many anaphoric mentions by the anaphoricity model." ></td>
	<td class="line x" title="166:212	More specifically, MUC F-score decreases by 1.75.5%, whereas CEAF Fscore decreases by 0.51.8%." ></td>
	<td class="line x" title="167:212	These trends are consistent with those reported in N&Cs paper." ></td>
	<td class="line x" title="168:212	Duplicated Ng (2004) baseline." ></td>
	<td class="line x" title="169:212	Our third baseline is Ngs (2004) coreference system." ></td>
	<td class="line x" title="170:212	Recall that this resolver requires the tuning of the conservativeness parameter, c, on held-out data." ></td>
	<td class="line x" title="171:212	To ensure a fair comparison between different resolvers, we do not plicitly address this issue, simply letting the coreference clustering algorithm discover that first mentions are non-anaphoric." ></td>
	<td class="line x" title="172:212	rely on additional data for parameter tuning." ></td>
	<td class="line x" title="173:212	Rather, we reserve 13 of the available training data for tuning c, for which we tested values from 0 to 1 in steps of 0.01, and use the remaining 23 of the data for training PA and PC." ></td>
	<td class="line x" title="174:212	Results are shown in row 3 of Tables 1 and 2, where MUC F-score ranges from 57.0 to 61.9 and CEAF F-score ranges from 55.5 to 60.6." ></td>
	<td class="line x" title="175:212	In comparison to the first baseline, we obtain mixed results: MUC F-score increases by 2.0% and 0.2% for BNEWS and NPAPER, respectively, but drops by 0.1% for NWIRE; CEAF F-score increases by 0.2% and 1.1% for BNEWS and NPAPER, respectively, but drops by 0.6% for NWIRE." ></td>
	<td class="line x" title="176:212	Duplicated Luo (2007) baseline." ></td>
	<td class="line x" title="177:212	Results of our fourth baseline, in which the anaphoricity and pairwise coreference probabilities are combined to score a partition using Luos system, are shown in row 4 of Tables 1 and 2." ></td>
	<td class="line x" title="178:212	Here, we see that MUC F-score ranges from 55.8 to 62.1 and CEAF F-score ranges from 56.3 to 61.5." ></td>
	<td class="line x" title="179:212	In comparison to the first baseline, performance improves, though insignificantly,5 in all cases: MUC F-score increases by 0.20.8%, whereas CEAF F-score increases by 0.31.0%." ></td>
	<td class="line x" title="180:212	Duplicated Denis and Baldridge (2007) baseline." ></td>
	<td class="line x" title="181:212	Our fifth baseline performs joint inference for anaphoricity determination and coreference resolution using D&Bs ILP approach." ></td>
	<td class="line x" title="182:212	Results are shown in row 5 of Tables 1 and 2, where MUC F-score ranges from 56.2 to 63.8 and CEAF Fscore ranges from 56.9 to 61.5." ></td>
	<td class="line x" title="183:212	In comparison to the first baseline, MUC F-score always increases, with improvements ranging from 1.2% to 2.1%." ></td>
	<td class="line x" title="184:212	CEAF results are mixed: F-score increases significantly for BNEWS, drops insignificantly for NPAPER, and rises insignificantly for NWIRE." ></td>
	<td class="line x" title="185:212	The difference in performance trends between the two scorers can be attributed to the fact that the MUC scorer typically under-penalizes errors due to overmerging, which occurs as a result of D&Bs using the aggressive-merge clustering algorithm." ></td>
	<td class="line x" title="186:212	In addition, we can see that D&Bs approach performs at least as good as Luos approach in all but one case (NPAPER/CEAF)." ></td>
	<td class="line x" title="187:212	Duplicated Finkel and Manning (2008) baseline." ></td>
	<td class="line x" title="188:212	Our sixth baseline is F&Ms coreference system, 5Like the MUC organizers, we use Approximate Randomization (Noreen, 1989) for significance testing, with p=0.05." ></td>
	<td class="line x" title="189:212	581 Broadcast News Newspaper Newswire Approach to Anaphoricity Determination R P F R P F R P F 1 No Anaphoricity 57.7 52.6 55.0 60.8 62.6 61.7 59.1 58.1 58.6 2 Duplicated Ng and Cardie (2002a) 40.3 67.7 50.5 52.1 70.6 60.0 43.0 69.3 53.1 3 Duplicated Ng (2004) 51.9 63.2 57.0 60.0 63.8 61.9 59.3 57.7 58.5 4 Duplicated Luo (2007) 55.4 56.1 55.8 60.6 63.7 62.1 58.4 59.2 58.8 5 Duplicated Denis and Baldridge (2007) 57.3 55.1 56.2 63.8 63.7 63.8 60.4 59.3 59.8 6 Duplicated Finkel and Manning (2008) 56.4 55.3 55.8 63.8 63.7 63.8 59.7 59.2 59.5 7 Graph Minimum Cut 53.1 67.5 59.4 57.9 71.2 63.9 54.1 69.0 60.6 Table 1: MUC scores for the three ACE data sets." ></td>
	<td class="line x" title="190:212	F-scores that represent statistically significant gains and drops with respect to the No Anaphoricity baseline are marked with an asterisk (*) and a dagger (), respectively." ></td>
	<td class="line x" title="191:212	Broadcast News Newspaper Newswire Approach to Anaphoricity Determination R P F R P F R P F 1 No Anaphoricity 63.2 49.2 55.3 64.5 54.3 59.0 67.3 56.1 61.2 2 Duplicated Ng and Cardie (2002a) 55.9 53.3 54.5 60.7 56.3 58.5 60.6 58.2 59.4 3 Duplicated Ng (2004) 62.5 49.9 55.5 63.5 57.0 60.1 65.6 56.3 60.6 4 Duplicated Luo (2007) 62.7 51.1 56.3 64.6 55.4 59.6 67.0 56.8 61.5 5 Duplicated Denis and Baldridge (2007) 63.8 51.4 56.9 62.6 53.6 57.8 67.0 56.8 61.5 6 Duplicated Finkel and Manning (2008) 63.2 51.3 56.7 62.6 53.6 57.8 66.7 56.7 61.3 7 Graph Minimum Cut 61.4 57.6 59.4 64.1 59.4 61.7 65.7 61.9 63.8 Table 2: CEAF scores for the three ACE data sets." ></td>
	<td class="line x" title="192:212	F-scores that represent statistically significant gains and drops with respect to the No Anaphoricity baseline are marked with an asterisk (*) and a dagger (), respectively." ></td>
	<td class="line x" title="193:212	which is essentially D&Bs approach augmented with transitivity constraints." ></td>
	<td class="line x" title="194:212	Results are shown in row 6 of Tables 1 and 2, where MUC F-score ranges from 55.8 to 63.8 and CEAF F-score ranges from 56.7 to 61.3." ></td>
	<td class="line x" title="195:212	In comparison to the D&B baseline, we see that F-score never improves, regardless of which scoring program is used." ></td>
	<td class="line x" title="196:212	In fact, recall slightly deteriorates, and this can be attributed to F&Ms observation that transitivity constraints tend to produce smaller clusters." ></td>
	<td class="line x" title="197:212	Overall, these results suggest that enforcing transitivity for coreference resolution is not useful for improving coreference performance." ></td>
	<td class="line x" title="198:212	Our graph-cut-based approach." ></td>
	<td class="line x" title="199:212	Finally, we evaluate the coreference system using the anaphoricity information provided by our cut-based approach." ></td>
	<td class="line x" title="200:212	As before, we reserve 13 of the training data for jointly tuning the two parameters, c and c2, and use the remaining 23 for training PA and PC." ></td>
	<td class="line x" title="201:212	For tuning, we tested values from 0 to 1 in steps of 0.1 for both c and c2." ></td>
	<td class="line x" title="202:212	Results are shown in row 7 of Tables 1 and 2." ></td>
	<td class="line x" title="203:212	As we can see, MUC F-score ranges from 59.4 to 63.9 and CEAF F-score ranges from 59.4 to 63.8, representing a significant improvement over the first baseline in all six cases: MUC F-score rises by 2.04.4% and CEAF F-score rises by 2.6 4.1%." ></td>
	<td class="line x" title="204:212	Such an improvement can be attributed to a large gain in precision and a smaller drop in recall." ></td>
	<td class="line x" title="205:212	This implies that our mincut algorithm has successfully identified many non-anaphoric mentions, but in comparison to N&Cs approach, it misclassifies a smaller number of anaphoric mentions." ></td>
	<td class="line x" title="206:212	Moreover, our approach achieves the best F-score for each dataset/scoring-program combination, and significantly outperforms the best baseline (D&B) in all but two cases, NPAPER/MUC and NWIRE/MUC." ></td>
	<td class="line x" title="207:212	6 Conclusions We have presented a graph-cut-based approach to anaphoricity determination that (1) directly optimizes the desired coreference evaluation metric through parameterization and (2) exploits the probabilities provided by the coreference model when coordinating anaphoricity and coreference decisions." ></td>
	<td class="line x" title="208:212	Another major contribution of our work is the empirical comparison of our approach against five existing approaches to anaphoricity determination in terms of their effectiveness in improving a coreference system using automatically extracted mentions." ></td>
	<td class="line x" title="209:212	Our approach demonstrates effectiveness and robustness by achieving the best result on all three ACE data sets according to both the MUC scorer and the CEAF scorer." ></td>
	<td class="line x" title="210:212	We believe that our cut-based approach provides a flexible mechanism for coordinating anaphoricity and coreference decisions." ></td>
	<td class="line x" title="211:212	582 Acknowledgments We thank the three anonymous reviewers for their invaluable comments, Kazi Saidul Hasan for his help on using lp solve, and NSF for its gracious support of this work under Grant IIS-0812261." ></td>
	<td class="line oc" title="212:212	The description of the minimum cut framework in Section 4.1 was inspired by Pang and Lee (2004)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-3010
Interactive Annotation Learning with Indirect Feature Voting
Arora, Shilpa;Nyberg, Eric H.;"></td>
	<td class="line x" title="1:163	Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 5560, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:163	c 2009 Association for Computational Linguistics Interactive Annotation Learning with Indirect Feature Voting Shilpa Arora and Eric Nyberg Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA {shilpaa,ehn}@cs.cmu.edu Abstract We demonstrate that a supervised annotation learning approach using structured features derived from tokens and prior annotations performs better than a bag of words approach." ></td>
	<td class="line x" title="3:163	We present a general graph representation for automatically deriving these features from labeled data." ></td>
	<td class="line x" title="4:163	Automatic feature selection based on class association scores requires a large amount of labeled data and direct voting can be difficult and error-prone for structured features, even for language specialists." ></td>
	<td class="line x" title="5:163	We show that highlighted rationales from the user can be used for indirect feature voting and same performance can be achieved with less labeled data.We present our results on two annotation learning tasks for opinion mining from product and movie reviews." ></td>
	<td class="line x" title="6:163	1 Introduction Interactive Annotation Learning is a supervised approach to learning annotations with the goal of minimizing the total annotation cost." ></td>
	<td class="line x" title="7:163	In this work, we demonstrate that with additional supervision per example, such as distinguishing discriminant features, same performance can be achieved with less annotated data." ></td>
	<td class="line x" title="8:163	Supervision for simple features has been explored in the literature (Raghavan et al., 2006; Druck et al., 2008; Haghighi and Klein, 2006)." ></td>
	<td class="line x" title="9:163	In this work, we propose an approach that seeks supervision from the user on structured features." ></td>
	<td class="line x" title="10:163	Features that capture the linguistic structure in text such as n-grams and syntactic patterns, referred to as structured features in this work, have been found to be useful for supervised learning of annotations." ></td>
	<td class="line x" title="11:163	For example, Pradhan et al.(2004) show that using features like syntactic path from constituent to predicate improves performance of a semantic parser." ></td>
	<td class="line x" title="13:163	However, often such features are handcrafted by domain experts and do not generalize to other tasks and domains." ></td>
	<td class="line x" title="14:163	In this work, we propose a general graph representation for automatically extracting structured features from tokens and prior annotations such as part of speech, dependency triples, etc. Gamon (2004) shows that an approach using a large set of structured features and a feature selection procedure performs better than an approach that uses a few handcrafted features." ></td>
	<td class="line x" title="15:163	Our hypothesis is that structured features are important for supervised annotation learning and can be automatically derived from tokens and prior annotations." ></td>
	<td class="line x" title="16:163	We test our hypothesis and present our results for opinion mining from product reviews." ></td>
	<td class="line x" title="17:163	Deriving features from the annotation graph gives us a large number of very sparse features." ></td>
	<td class="line x" title="18:163	Feature selection based on class association scores such as mutual information and chi-square have often been used to identify the most discriminant features (Manning et al., 2008)." ></td>
	<td class="line x" title="19:163	However, these scores are calculated from labeled data and they are not very meaningful when the dataset is small." ></td>
	<td class="line x" title="20:163	Supervised feature selection, i.e. asking the user to vote for the most discriminant features, has been used as an alternative when the training dataset is small." ></td>
	<td class="line x" title="21:163	Raghavan et al.(2006) and Druck et al.(2008) seek feedback on unigram features from the user for document classification tasks." ></td>
	<td class="line x" title="24:163	Haghighi and Klein (2006) ask the user to suggest a few prototypes (examples) for each class and use those as features." ></td>
	<td class="line x" title="25:163	These approaches ask the annotators to identify globally rel55 evant features, but certain features are difficult to vote on without the context and may take on very different meanings in different contexts." ></td>
	<td class="line x" title="26:163	Also, all these approaches have been demonstrated for unigram features and it is not clear how they can be extended straightforwardly to structured features." ></td>
	<td class="line x" title="27:163	We propose an indirect approach to interactive feature selection that makes use of highlighted rationales from the user." ></td>
	<td class="line x" title="28:163	A rationale (Zaidan et al., 2007) is the span of text a user highlights in support of his/her annotation." ></td>
	<td class="line x" title="29:163	Rationales also allow us to seek feedback on features in context." ></td>
	<td class="line x" title="30:163	Our hypothesis is that with rationales, we can achieve same performance with lower annotation cost and we demonstrate this for opinion mining from movie reviews." ></td>
	<td class="line x" title="31:163	In Section 2, we describe the annotation graph representation and motivate the use of structured features with results on learning opinions from product reviews." ></td>
	<td class="line x" title="32:163	In Section 3, we show how rationales can be used for identifying the most discriminant features for opinion classification with less training data." ></td>
	<td class="line x" title="33:163	We then list the conclusions we can draw from this work, followed by suggestions for future work." ></td>
	<td class="line x" title="34:163	2 Learning with Structured Features In this section, we demonstrate that structured features help in improving performance and propose a formal graph representation for deriving these features automatically." ></td>
	<td class="line x" title="35:163	2.1 Opinions and Structured Features Unigram features such as tokens are not sufficient for recognizing all kinds of opinions." ></td>
	<td class="line x" title="36:163	For example, a unigram feature good may seem useful for identifying opinions, however, consider the following two comments in a review: 1) This camera has good features and 2) I did a good months worth of research before buying this camera." ></td>
	<td class="line x" title="37:163	In the first example, the unigram good is a useful feature." ></td>
	<td class="line x" title="38:163	However, in the second example, good is not complementing the camera and hence will mislead the classifier." ></td>
	<td class="line x" title="39:163	Structured features such as part-of-speech, dependency relations etc. are needed to capture the language structure that unigram features fail to capture." ></td>
	<td class="line x" title="40:163	2.2 Annotation Graph and Features We define the annotation graph as a quadruple: G = (N,E,,), where N is the set of nodes, E is the set of edges E  N N,  = N  E is a set of labels for nodes and edges." ></td>
	<td class="line x" title="41:163	 is the labeling function  : NE , that assigns labels to nodes and edges." ></td>
	<td class="line x" title="42:163	In this work, we define the set of labels for nodes, N as tokens, part of speech and dependency annotations and set of labels for edges, E as relations, E = {leftOf,parentOf,restricts}." ></td>
	<td class="line x" title="43:163	The leftOf relation is defined between two adjacent nodes." ></td>
	<td class="line x" title="44:163	The parentOf relation is defined between the dependency type and its attributes." ></td>
	<td class="line x" title="45:163	For example, for the dependency triple nsubj perfect camera, there is a parentOf relation between the dependency type nsubj and tokens perfect and camera." ></td>
	<td class="line x" title="46:163	The restricts relation exists between two nodes a and b if their textual spans overlap completely andarestricts howbis interpreted." ></td>
	<td class="line x" title="47:163	For a word with multiple senses the restricts relation between the word and its part of speech, restricts the way the word is interpreted, by capturing the sense of the word in the given context." ></td>
	<td class="line x" title="48:163	The Stanford POS tagger (Toutanova and Manning, 2000) and the Stanford parser (Klein and Manning, 2003) were used to produce the part of speech and dependency annotations." ></td>
	<td class="line x" title="49:163	Features are defined as subgraphs, Gprime = (Nprime,Eprime,prime,prime) in the annotation graph G, such that Nprime N,Eprime NprimeNprime andEprime E, prime = primeNprimeE where primeN N and primeE E andprime : NprimeEprime  prime." ></td>
	<td class="line x" title="50:163	For a bag of words approach that only uses tokens as features, primeN = T, where T is the token vocabulary and E =  and E =  (where  is the null set)." ></td>
	<td class="line x" title="51:163	We define thedegreeof a feature subgraph as the number of edges it contains." ></td>
	<td class="line x" title="52:163	For example, the unigram features are the feature subgraphs with no edges i.e. degree = 0." ></td>
	<td class="line x" title="53:163	Degree1 features are the feature subgraphs with two nodes and an edge." ></td>
	<td class="line x" title="54:163	In this paper, we present results for feature subgraphs with degree = 0 and degree = 1." ></td>
	<td class="line x" title="55:163	Figure 1 shows the partial annotation graph for two comments discussed above." ></td>
	<td class="line x" title="56:163	The feature subgraph that captures the opinion expressed in 1(a), can be described in simple words as camera has features that are good." ></td>
	<td class="line x" title="57:163	This kind of subject-object relationship with the same verb, between the camera and whats being modified by good, is not present in the second example (1(b))." ></td>
	<td class="line x" title="58:163	A slight modification of 1(b), I did a months worth of research before buying this good camera does express an opinion about the camera." ></td>
	<td class="line x" title="59:163	A bag of words approach that uses only unigram features will not be able to differ56 entiate between these two examples; structured features like dependency relation subgraphs can capture this linguistic distinction between the two examples." ></td>
	<td class="line x" title="60:163	P24:amod [16,29] P23:JJ [16,20] P22:dobj [12,29] P21:nsubj [5,15] restricts parentOf parentOf parentOf (a) (b) Figure1: The figure shows partial annotation graphs for two examples." ></td>
	<td class="line x" title="61:163	Only some of the nodes and edges are shown for clarity." ></td>
	<td class="line x" title="62:163	Spans of nodes in brackets are the character spans." ></td>
	<td class="line x" title="63:163	2.3 Experiments and Results The dataset we used is a collection of 244 Amazons customer reviews (2962 comments) for five products (Hu and Liu, 2004)." ></td>
	<td class="line x" title="64:163	A review comment is annotated as an opinion if it expresses an opinion about an aspect of the product and the aspect is explicitly mentioned in the sentence." ></td>
	<td class="line x" title="65:163	We performed 10-fold cross validation (CV) using the Support Vector Machine (SVM) classifier in MinorThird (Cohen, 2004) with the default linear kernel and chi-square feature selection to select the top 5000 features." ></td>
	<td class="line x" title="66:163	As can be seen in Table 1, an approach using degree0 features, i.e. unigrams, part of speech and dependency triples together, outperforms using any of those features alone and this difference is significant." ></td>
	<td class="line x" title="67:163	Using degree 1 features with two nodes and an edge improves performance further." ></td>
	<td class="line x" title="68:163	However, using degree0 features in addition todegree1 features does not improve performance." ></td>
	<td class="line x" title="69:163	This suggests that when using higher degree features, we may leave out the features with lower degree that they subsume." ></td>
	<td class="line x" title="70:163	Features Avg F1 Outperforms unigram [uni] 65.74 pos,dep pos-unigram [pos] 64 dep dependency [dep] 63.18 degree-0 [deg-0] 67.77 uni,pos,dep degree-1 [deg-1] 70.56 uni,pos,dep,deg-0, deg-* (deg-0 + deg-1) [deg-*] 70.12 uni,pos,dep,deg-0 Table1: The table reports the F-measure scores averaged over ten cross validation folds." ></td>
	<td class="line x" title="71:163	The value in bold in the Avg F1 column is the best performing feature combination." ></td>
	<td class="line x" title="72:163	For each feature combination in the row, outperforms column lists the feature combinations it outperforms, with significant differences highlighted in bold (paired t-test with p < 0.05 considered significant)." ></td>
	<td class="line x" title="73:163	3 Rationales & Indirect Feature voting We propose an indirect feature voting approach that uses user-highlighted rationales to identify the most discriminant features." ></td>
	<td class="line x" title="74:163	We present our results on Movie Review data annotated with rationales." ></td>
	<td class="line oc" title="75:163	3.1 Data and Experimental Setup The data set by Pang and Lee (2004) consists of 2000 movie reviews (1000-pos, 1000-neg) from the IMDb review archive." ></td>
	<td class="line x" title="76:163	Zaidan et al.(2007) provide rationales for 1800 reviews (900-pos, 900-neg)." ></td>
	<td class="line x" title="78:163	The annotation guidelines for marking rationales are described in (Zaidan et al., 2007)." ></td>
	<td class="line x" title="79:163	An example of a rationale is: the movie is so badly put together that even the most casual viewer may notice the miserable pacing and stray plot threads." ></td>
	<td class="line x" title="80:163	For a test dataset of 200 reviews, randomly selected from 1800 reviews, we varied the training data size from 50 to 500 reviews, adding 50 reviews at a time." ></td>
	<td class="line x" title="81:163	Training examples were randomly selected from the remaining 1600 reviews." ></td>
	<td class="line x" title="82:163	During testing, information about rationales is not used." ></td>
	<td class="line x" title="83:163	We used tokens1, part of speech and dependency triples as features." ></td>
	<td class="line x" title="84:163	We used the KStem stemmer (Krovetz, 1993) to stem the token features." ></td>
	<td class="line x" title="85:163	In order to compare the approaches at their best performing feature configuration, we varied the total number of features used, choosing from the set: {1000, 2000, 5000, 10000, 50000}." ></td>
	<td class="line x" title="86:163	We used chi-square feature selection (Manning et al., 2008) and the SVM learner with default settings from the Minorthird package (Cohen, 2004) for these experiments." ></td>
	<td class="line x" title="87:163	We compare the following approaches: BaseTrainingDataset(BTD): We train a model from the labeled data with no feature voting." ></td>
	<td class="line x" title="88:163	1filtering the stop words using the stop word list: http: //www.cs.cmu.edu/shilpaa/stop-words-ial-movie." ></td>
	<td class="line x" title="89:163	txt 57 Rationale annotated Training Dataset (RTD): We experimented with two different settings for indirect feature voting: 1) only using features that overlap with rationales (RTD(1,0)); 2) features from rationales weighted twice as much as features from other parts of the text (RTD(2,1))." ></td>
	<td class="line x" title="90:163	In general, R(i,j) describes an experimental condition where features from rationales are weighted i times and other features are weighted j times." ></td>
	<td class="line x" title="91:163	In Minorthird, weighing a feature two times more than other features is equivalent to that feature occurring twice as much." ></td>
	<td class="line x" title="92:163	Oracle voted Training Data (OTD): In order to compare indirect feature voting to direct voting on features, we simulate the users vote on the features with class association scores from a large dataset (all 1600 documents used for selecting training documents)." ></td>
	<td class="line x" title="93:163	This is based on the assumption that the class association scores, such as chi-square, from a large dataset can be used as a reliable discriminator of the most relevant features." ></td>
	<td class="line x" title="94:163	This approach of simulating the oracle with large amount of labeled data has been used previously in feature voting (Raghavan et al., 2006)." ></td>
	<td class="line x" title="95:163	3.2 Results and Discussion In Table 2, we present the accuracy results for the four approaches described in the previous section." ></td>
	<td class="line x" title="96:163	We compare the best performing feature configurations for three approaches BTD, RTD(1,0) and RTD (2,0)." ></td>
	<td class="line x" title="97:163	As can be seen, RTD(1,0) always performs better than BTD." ></td>
	<td class="line x" title="98:163	As expected, improvement with rationales is greater and it is significant when the training dataset is small." ></td>
	<td class="line x" title="99:163	The performance of all approaches converge as the training data size increases and hence we only present results up to training dataset size of 500 examples in this paper." ></td>
	<td class="line x" title="100:163	Since our goal is to evaluate the use of rationales independently of how many features the model uses, we also compared the four approaches in terms of the accuracy averaged over five feature configurations." ></td>
	<td class="line x" title="101:163	Due to space constraints, we do not include the table of results." ></td>
	<td class="line x" title="102:163	On average RTD(1,0) significantly outperforms BTD when the total training dataset is less than 350 examples." ></td>
	<td class="line x" title="103:163	When the training data has fewer than 400 examples, RTD(1,0) also significantly outperforms RTD(2,1)." ></td>
	<td class="line x" title="104:163	OTD with simulated user is an approximate up#Ex Approach Number of Features1000 2000 5000 10000 50000 50 OTD 67.63 66.30 62.90 52.17 55.03 BTD 58.10 57.47 52.67 51.80 55.03 RTD(1,0)* 55.43 55.93 61.63 61.63 61.63 RTD(2,1) 57.77 57.53 52.73 52.30 56.33 100 OTD 71.97 71.07 70.27 69.37 64.33 BTD 64.17 64.43 62.70 56.63 64.37 RTD(1,0)* 65.43 63.27 65.13 67.23 67.23 RTD(2,1) 64.27 63.93 62.47 56.10 63.77 150 OTD 73.83 74.83 74.20 74.00 63.83 BTD 66.17 67.77 68.60 64.33 60.47 RTD(1,0)* 69.30 68.30 67.27 71.30 71.87 RTD(2,1) 68.00 67.07 68.43 63.57 58.90 200 OTD 74.83 75.87 75.70 75.10 56.97 BTD 71.63 71.37 72.57 71.53 58.90 RTD(1,0) 72.23 72.63 71.63 73.80 73.93 RTD(2,1) 71.20 71.10 73.03 70.77 57.87 250 OTD 75.63 76.90 77.70 77.67 62.20 BTD 72.60 73.57 74.73 75.20 58.93 RTD(1,0) 73.00 73.57 73.57 74.70 76.70 RTD(2,1) 72.87 73.90 74.63 75.40 57.43 300 OTD 76.57 77.67 78.93 78.43 68.17 BTD 72.97 74.13 74.93 76.57 63.83 RTD(1,0) 74.43 74.83 74.67 74.73 77.67 RTD(2,1) 72.67 74.53 74.37 76.53 61.30 350 OTD 76.47 78.20 80.20 79.80 71.73 BTD 74.43 74.30 74.73 77.27 66.80 RTD(1,0) 75.07 76.20 75.80 75.20 78.53 RTD(2,1) 74.63 75.70 74.80 78.23 64.93 400 OTD 77.97 78.93 80.53 80.60 75.27 BTD 75.83 76.77 76.47 78.93 70.63 RTD(1,0) 75.17 76.40 75.83 76.00 79.23 RTD(2,1) 75.73 76.07 76.80 78.50 68.20 450 OTD 77.67 79.20 80.57 80.73 77.13 BTD 75.73 76.80 77.80 78.80 74.37 RTD(1,0)* 74.83 76.50 76.23 76.47 80.40 RTD(2,1) 75.87 76.87 77.87 78.87 71.80 500 OTD 78.03 80.10 81.27 81.67 79.87 BTD 75.27 77.33 79.37 80.30 75.73 RTD(1,0) 75.77 77.63 77.47 77.27 81.10 RTD(2,1) 75.83 77.47 79.50 79.70 74.50 Table 2: Accuracy performance for four approaches, five feature configurations and increasing training dataset size." ></td>
	<td class="line x" title="105:163	Accuracy reported is averaged over five random selection of training documents for three randomly selected test datasets." ></td>
	<td class="line x" title="106:163	The numbers in bold in a row represents the best performing feature configuration for a given approach and training dataset size." ></td>
	<td class="line x" title="107:163	The approach in bold represents the best performing approach among BTD, RTD(1,0) and RTD(2,1) for a given training dataset size." ></td>
	<td class="line x" title="108:163	* indicates significant improvement in performance over BTD (paired t-test with p < 0.05 considered significant)." ></td>
	<td class="line x" title="109:163	per bound for rationale based approaches." ></td>
	<td class="line x" title="110:163	It tells us how far we are from direct supervision on structured features." ></td>
	<td class="line x" title="111:163	On average, OTD significantly outperformed RTD(1,0) for training data size of 100, 150, 400, 450 and 500 examples but not always." ></td>
	<td class="line x" title="112:163	As can be seen from Table 2, difference between OTD and RTD(1,0) reduces with more training data, since with more data and hence more rationales we get better feature coverage." ></td>
	<td class="line x" title="113:163	Results presented here show that for a given training dataset, we can boost the performance by ask58 ing the user to label rationales." ></td>
	<td class="line x" title="114:163	However, there is an additional cost associated with the rationales." ></td>
	<td class="line x" title="115:163	It is important to evaluate how much total annotation cost rationales can save us while achieving the desired performance." ></td>
	<td class="line x" title="116:163	In Figure 2, we compare the number of training examples an approach needs to achieve a given level of performance." ></td>
	<td class="line x" title="117:163	As can be seen, RTD(1,0) needs fewer training examples to achieve the same performance as BTD." ></td>
	<td class="line x" title="118:163	The difference is large initially when the total number of training examples is small (50 forRTD(1,0) and 150 for BTD to achieve a performance between 6667)." ></td>
	<td class="line x" title="119:163	Figure 2: The Figure shows the number of examples needed by the two approaches, RTD(1,0) and BTD, to achieve an accuracy in the given range." ></td>
	<td class="line x" title="120:163	Comparison with Zaidan et al.(2007): Zaidan et al.(2007) conclude that using only features from rationales performs worse than both: 1) using all the features in the documents, and 2) using features that do not overlap with the rationales." ></td>
	<td class="line x" title="123:163	The results presented in this paper seem to contradict their results." ></td>
	<td class="line x" title="124:163	However, they only experimented with unigram features and only one approach to using features from rationales, RTD(1,0) and not RTD(2,1)." ></td>
	<td class="line x" title="125:163	In order to compare our work directly with theirs, we experimented with an equivalent set of unigram features." ></td>
	<td class="line x" title="126:163	In Table 3, we present the results using same number of total features (17744) as Zaidan et al.(2007)." ></td>
	<td class="line x" title="128:163	As can be seen from the table, when only unigram features are used,RTD(2,1) outperformsBTD but RTD(1,0) performs worse than BTD." ></td>
	<td class="line x" title="129:163	Thus, our results are consistent with (Zaidan et al., 2007) i.e. using unigram features only from the rationales does not boost performance." ></td>
	<td class="line x" title="130:163	From Table 3, we also analyze the improvement in performance when part of speech and dependency features are used in addition to the unigram features i.e. using all degree 0 subgraph fea#Ex Approach uni uni-pos uni-pos-dep 100 OTD 68.6 68.8 61.6 BTD 68.6 68.8 52.2 RTD(1,0) 68.2 68.1 69.0* RTD(2,0) 70.0 67.0 51.7 200 OTD 73.6 73.8 75.3 BTD 73.6 73.8 67.1 RTD(1,0) 73.9 73.2 73.9* RTD(2,0) 75.3* 70.3 65.2 300 OTD 76.2 76.1 79.1 BTD 76.2 76.1 73.7 RTD(1,0) 75.0 74.9 77.1* RTD(2,0) 77.5* 73.3 74.8 400 OTD 77.4 76.8 79.9 BTD 77.4 76.8 76.2 RTD(1,0) 75.9 75.9 77.0 RTD(2,0) 78.0 74.7 77.7* 500 OTD 78.1 78.1 80.0 BTD 78.1 78.1 78.4 RTD(1,0) 76.3 76.2 77.6 RTD(2,0) 78.2 75.4 79.0 Table 3: The Table reports accuracy for four approaches in a setting similar to (Zaidan et al., 2007)." ></td>
	<td class="line x" title="131:163	Accuracy reported is averaged over ten random selection of training documents for two randomly selected test datasets.The numbers in bold are the best among BTD, RTD(1,0), RTD(2,1) for a given feature combination." ></td>
	<td class="line x" title="132:163	* highlights the significant improvement in performance over BTD (using paired t-test, with p < 0.05 considered significant)." ></td>
	<td class="line x" title="133:163	tures." ></td>
	<td class="line x" title="134:163	For RTD(1,0), adding these features improves performance for all data sizes with significant improvement for dataset size of 300 and 500 examples." ></td>
	<td class="line x" title="135:163	RTD(1,0) also significantly outperforms BTD when all three features are used." ></td>
	<td class="line x" title="136:163	For direct voting on features (OTD), a significant improvement with these structured features is seen when the training dataset size is greater than 200 examples." ></td>
	<td class="line x" title="137:163	For BTD and RTD(2,1) approaches, there is no significant improvement with these additional features." ></td>
	<td class="line x" title="138:163	In the future, we plan to investigate further the benefit of using higher degree subgraph features for opinion mining from the movie review data." ></td>
	<td class="line x" title="139:163	Comparing ranking of features:We also compared the features that the rationales capture to what the oracle will vote for as the most relevant features." ></td>
	<td class="line x" title="140:163	Features are ranked based on chi-square scores used in feature selection." ></td>
	<td class="line x" title="141:163	We compare the ranked list of features from RTD(1,0), BTD and OTD and use a weighted F-measure score for evaluating the top 100 ranked features by each approach." ></td>
	<td class="line x" title="142:163	This measure is inspired by the Pyramid measure used in Summarization (Nenkova and Passonneau, 2004)." ></td>
	<td class="line x" title="143:163	Instead of using counts in calculating F-measure, we used the chi-square score assigned to the features by the oracle dataset, in order to give more weight to the more discriminant features." ></td>
	<td class="line x" title="144:163	As can be seen from 59 Table 4, RTD(1,0) outperforms BTD in capturing the important features when the datasize set is small (< 300) and this difference is significant." ></td>
	<td class="line x" title="145:163	Beyond 300 examples, as the data size increases,BTD outperforms RTD(1,0)." ></td>
	<td class="line x" title="146:163	This implies that the rationales alone are able to capture the most relevant features when the dataset is small." ></td>
	<td class="line x" title="147:163	100 200 300 400 500 600 700 RO 47.70 53.80 57.68 59.54 62.13 60.86 61.56 TO 31.22 44.43 52.98 60.57 64.61 67.10 70.39 Table 4: Weighted F-measure performance comparison of ranked list of features from RTD(1,0) & OTD(RO) and BTD & OTD(TO)." ></td>
	<td class="line x" title="148:163	Results are averaged over ten random selections of the training data for a randomly selected test dataset." ></td>
	<td class="line x" title="149:163	Significant differences are highlighted in bold (paired t-test with p < 0.05 considered significant)." ></td>
	<td class="line x" title="150:163	4 Conclusion and Future Work In this work, we demonstrated that using structured features boosts performance of supervised annotation learning." ></td>
	<td class="line x" title="151:163	We proposed a formal annotation graph representation that can be used to derive these features automatically." ></td>
	<td class="line x" title="152:163	However, the space of possible feature subgraphs can grow very large with more prior annotations." ></td>
	<td class="line x" title="153:163	Standard feature selection techniques based on class association scores are less effective when the dataset is small." ></td>
	<td class="line x" title="154:163	Feature voting from the user for identifying the relevant features is limited to simple features." ></td>
	<td class="line x" title="155:163	Supplementary input from the user in terms of highlighted rationales can be used instead to prune the feature space." ></td>
	<td class="line x" title="156:163	The proposed approach is general and can be applied to a variety of problems and features." ></td>
	<td class="line x" title="157:163	In this work, we presented our results with degree  0 and degree  1 feature subgraphs." ></td>
	<td class="line x" title="158:163	We will extend our algorithm to automatically extract higher degree features from the annotation graph." ></td>
	<td class="line x" title="159:163	For the rationale annotated training data (RTD(i,j)), we experimented with two possible values for i and j. We aim to learn these weights empirically using a held out dataset." ></td>
	<td class="line x" title="160:163	Rationales are associated with an additional cost per example and hence two approaches, with and without the rationales, are not directly comparable in terms of the number of examples." ></td>
	<td class="line x" title="161:163	In the future, we will conduct an annotation experiment with real users to evaluate the usefulness of rationales in terms of clock time." ></td>
	<td class="line x" title="162:163	Acknowledgments We would like to thank Dr. Carolyn P. Rose for her help with statistical analysis of the results." ></td>
	<td class="line x" title="163:163	We would also like to thank all the anonymous reviewers for their helpful comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1027
Co-Training for Cross-Lingual Sentiment Classification
Wan, Xiaojun;"></td>
	<td class="line x" title="1:195	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 235243, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:195	c2009 ACL and AFNLP Co-Training for Cross-Lingual Sentiment Classification  Xiaojun Wan Institute of Compute Science and Technology & Key Laboratory of Computational Linguistics, MOE Peking University, Beijing 100871, China wanxiaojun@icst.pku.edu.cn   Abstract The lack of Chinese sentiment corpora limits the research progress on Chinese sentiment classification." ></td>
	<td class="line x" title="3:195	However, there are many freely available English sentiment corpora on the Web." ></td>
	<td class="line x" title="4:195	This paper focuses on the problem of cross-lingual sentiment classification, which leverages an available English corpus for Chinese sentiment classification by using the English corpus as training data." ></td>
	<td class="line x" title="5:195	Machine translation services are used for eliminating the language gap between the training set and test set, and English features and Chinese features are considered as two independent views of the classification problem." ></td>
	<td class="line x" title="6:195	We propose a cotraining approach to making use of unlabeled Chinese data." ></td>
	<td class="line x" title="7:195	Experimental results show the effectiveness of the proposed approach, which can outperform the standard inductive classifiers and the transductive classifiers." ></td>
	<td class="line x" title="8:195	1 Introduction Sentiment classification is the task of identifying the sentiment polarity of a given text." ></td>
	<td class="line x" title="9:195	The sentiment polarity is usually positive or negative and the text genre is usually product review." ></td>
	<td class="line x" title="10:195	In recent years, sentiment classification has drawn much attention in the NLP field and it has many useful applications, such as opinion mining and summarization (Liu et al., 2005; Ku et al., 2006; Titov and McDonald, 2008)." ></td>
	<td class="line x" title="11:195	To date, a variety of corpus-based methods have been developed for sentiment classification." ></td>
	<td class="line x" title="12:195	The methods usually rely heavily on an annotated corpus for training the sentiment classifier." ></td>
	<td class="line x" title="13:195	The sentiment corpora are considered as the most valuable resources for the sentiment classification task." ></td>
	<td class="line x" title="14:195	However, such resources in different languages are very imbalanced." ></td>
	<td class="line x" title="15:195	Because most previous work focuses on English sentiment classification, many annotated corpora for English sentiment classification are freely available on the Web." ></td>
	<td class="line x" title="16:195	However, the annotated corpora for Chinese sentiment classification are scarce and it is not a trivial task to manually label reliable Chinese sentiment corpora." ></td>
	<td class="line x" title="17:195	The challenge before us is how to leverage rich English corpora for Chinese sentiment classification." ></td>
	<td class="line x" title="18:195	In this study, we focus on the problem of cross-lingual sentiment classification, which leverages only English training data for supervised sentiment classification of Chinese product reviews, without using any Chinese resources." ></td>
	<td class="line x" title="19:195	Note that the above problem is not only defined for Chinese sentiment classification, but also for various sentiment analysis tasks in other different languages." ></td>
	<td class="line x" title="20:195	Though pilot studies have been performed to make use of English corpora for subjectivity classification in other languages (Mihalcea et al., 2007; Banea et al., 2008), the methods are very straightforward by directly employing an inductive classifier (e.g. SVM, NB), and the classification performance is far from satisfactory because of the language gap between the original language and the translated language." ></td>
	<td class="line x" title="21:195	In this study, we propose a co-training approach to improving the classification accuracy of polarity identification of Chinese product reviews." ></td>
	<td class="line x" title="22:195	Unlabeled Chinese reviews can be fully leveraged in the proposed approach." ></td>
	<td class="line x" title="23:195	First, machine translation services are used to translate English training reviews into Chinese reviews and also translate Chinese test reviews and additional unlabeled reviews into English reviews." ></td>
	<td class="line x" title="24:195	Then, we can view the classification problem in two independent views: Chinese view with only Chinese features and English view with only English features." ></td>
	<td class="line x" title="25:195	We then use the co-training approach to making full use of the two redundant views of features." ></td>
	<td class="line x" title="26:195	The SVM classifier is adopted as the basic classifier in the proposed approach." ></td>
	<td class="line x" title="27:195	Experimental results show that the proposed approach can outperform the baseline inductive classifiers and the more advanced transductive classifiers." ></td>
	<td class="line x" title="28:195	The rest of this paper is organized as follows: Section 2 introduces related work." ></td>
	<td class="line x" title="29:195	The proposed 235 co-training approach is described in detail in Section 3." ></td>
	<td class="line x" title="30:195	Section 4 shows the experimental results." ></td>
	<td class="line x" title="31:195	Lastly we conclude this paper in Section 5." ></td>
	<td class="line x" title="32:195	2 Related Work 2.1 Sentiment Classification Sentiment classification can be performed on words, sentences or documents." ></td>
	<td class="line x" title="33:195	In this paper we focus on document sentiment classification." ></td>
	<td class="line x" title="34:195	The methods for document sentiment classification can be generally categorized into lexicon-based and corpus-based." ></td>
	<td class="line x" title="35:195	Lexicon-based methods usually involve deriving a sentiment measure for text based on sentiment lexicons." ></td>
	<td class="line x" title="36:195	Turney (2002) predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs, which is denoted as the semantic oriented method." ></td>
	<td class="line x" title="37:195	Kim and Hovy (2004) build three models to assign a sentiment category to a given sentence by combining the individual sentiments of sentimentbearing words." ></td>
	<td class="line x" title="38:195	Hiroshi et al.(2004) use the technique of deep language analysis for machine translation to extract sentiment units in text documents." ></td>
	<td class="line x" title="40:195	Kennedy and Inkpen (2006) determine the sentiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers." ></td>
	<td class="line x" title="41:195	Devitt and Ahmad (2007) explore a computable metric of positive or negative polarity in financial news text." ></td>
	<td class="line x" title="42:195	Corpus-based methods usually consider the sentiment analysis task as a classification task and they use a labeled corpus to train a sentiment classifier." ></td>
	<td class="line oc" title="43:195	Since the work of Pang et al.(2002), various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005; Read, 2005)." ></td>
	<td class="line x" title="45:195	Most recently, McDonald et al.(2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity." ></td>
	<td class="line x" title="47:195	Blitzer et al.(2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products." ></td>
	<td class="line x" title="49:195	Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpus-based classifier and a lexicon-based classifier with precision-based vote weighting." ></td>
	<td class="line x" title="50:195	Chinese sentiment analysis has also been studied (Tsou et al., 2005; Ye et al., 2006; Li and Sun, 2007) and most such work uses similar lexiconbased or corpus-based methods for Chinese sentiment classification." ></td>
	<td class="line x" title="51:195	To date, several pilot studies have been performed to leverage rich English resources for sentiment analysis in other languages." ></td>
	<td class="line x" title="52:195	Standard Nave Bayes and SVM classifiers have been applied for subjectivity classification in Romanian (Mihalcea et al., 2007; Banea et al., 2008), and the results show that automatic translation is a viable alternative for the construction of resources and tools for subjectivity analysis in a new target language." ></td>
	<td class="line x" title="53:195	Wan (2008) focuses on leveraging both Chinese and English lexicons to improve Chinese sentiment analysis by using lexicon-based methods." ></td>
	<td class="line x" title="54:195	In this study, we focus on improving the corpus-based method for crosslingual sentiment classification of Chinese product reviews by developing novel approaches." ></td>
	<td class="line x" title="55:195	2.2 Cross-Domain Text Classification Cross-domain text classification can be considered as a more general task than cross-lingual sentiment classification." ></td>
	<td class="line x" title="56:195	In the problem of crossdomain text classification, the labeled and unlabeled data come from different domains, and their underlying distributions are often different from each other, which violates the basic assumption of traditional classification learning." ></td>
	<td class="line x" title="57:195	To date, many semi-supervised learning algorithms have been developed for addressing the cross-domain text classification problem by transferring knowledge across domains, including Transductive SVM (Joachims, 1999), EM(Nigam et al., 2000), EM-based Nave Bayes classifier (Dai et al., 2007a), Topic-bridged PLSA (Xue et al., 2008), Co-Clustering based classification (Dai et al., 2007b), two-stage approach (Jiang and Zhai, 2007)." ></td>
	<td class="line x" title="58:195	DaumIII and Marcu (2006) introduce a statistical formulation of this problem in terms of a simple mixture model." ></td>
	<td class="line x" title="59:195	In particular, several previous studies focus on the problem of cross-lingual text classification, which can be considered as a special case of general cross-domain text classification." ></td>
	<td class="line x" title="60:195	Bel et al.(2003) present practical and cost-effective solutions." ></td>
	<td class="line x" title="62:195	A few novel models have been proposed to address the problem, e.g. the EM-based algorithm (Rigutini et al., 2005), the information bottleneck approach (Ling et al., 2008), the multilingual domain models (Gliozzo and Strapparava, 2005), etc. To the best of our knowledge, cotraining has not yet been investigated for crossdomain or cross-lingual text classification." ></td>
	<td class="line x" title="63:195	236 3 The Co-Training Approach 3.1 Overview The purpose of our approach is to make use of the annotated English corpus for sentiment polarity identification of Chinese reviews in a supervised framework, without using any Chinese resources." ></td>
	<td class="line x" title="64:195	Given the labeled English reviews and unlabeled Chinese reviews, two straightforward methods for addressing the problem are as follows: 1) We first learn a classifier based on the labeled English reviews, and then translate Chinese reviews into English reviews." ></td>
	<td class="line x" title="65:195	Lastly, we use the classifier to classify the translated English reviews." ></td>
	<td class="line x" title="66:195	2) We first translate the labeled English reviews into Chinese reviews, and then learn a classifier based on the translated Chinese reviews with labels." ></td>
	<td class="line x" title="67:195	Lastly, we use the classifier to classify the unlabeled Chinese reviews." ></td>
	<td class="line x" title="68:195	The above two methods have been used in (Banea et al., 2008) for Romanian subjectivity analysis, but the experimental results are not very promising." ></td>
	<td class="line x" title="69:195	As shown in our experiments, the above two methods do not perform well for Chinese sentiment classification, either, because the underlying distribution between the original language and the translated language are different." ></td>
	<td class="line x" title="70:195	In order to address the above problem, we propose to use the co-training approach to make use of some amounts of unlabeled Chinese reviews to improve the classification accuracy." ></td>
	<td class="line x" title="71:195	The co-training approach can make full use of both the English features and the Chinese features in a unified framework." ></td>
	<td class="line x" title="72:195	The framework of the proposed approach is illustrated in Figure 1." ></td>
	<td class="line x" title="73:195	The framework consists of a training phase and a classification phase." ></td>
	<td class="line x" title="74:195	In the training phase, the input is the labeled English reviews and some amounts of unlabeled Chinese reviews 1 . The labeled English reviews are translated into labeled Chinese reviews, and the unlabeled Chinese reviews are translated into unlabeled English reviews, by using machine translation services." ></td>
	<td class="line x" title="75:195	Therefore, each review is associated with an English version and a Chinese version." ></td>
	<td class="line x" title="76:195	The English features and the Chinese features for each review are considered two independent and redundant views of the review." ></td>
	<td class="line x" title="77:195	The co-training algorithm is then applied to learn two classifiers  1  The unlabeled Chinese reviews used for co-training do not include the unlabeled Chinese reviews for testing, i.e., the Chinese reviews for testing are blind to the training phase." ></td>
	<td class="line x" title="78:195	and finally the two classifiers are combined into a single sentiment classifier." ></td>
	<td class="line x" title="79:195	In the classification phase, each unlabeled Chinese review for testing is first translated into English review, and then the learned classifier is applied to classify the review into either positive or negative." ></td>
	<td class="line x" title="80:195	The steps of review translation and the cotraining algorithm are described in details in the next sections, respectively." ></td>
	<td class="line x" title="81:195	Figure 1." ></td>
	<td class="line x" title="82:195	Framework of the proposed approach 3.2 Review Translation In order to overcome the language gap, we must translate one language into another language." ></td>
	<td class="line x" title="83:195	Fortunately, machine translation techniques have been well developed in the NLP field, though the translation performance is far from satisfactory." ></td>
	<td class="line x" title="84:195	A few commercial machine translation services can be publicly accessed, e.g. Google Translate 2 , Yahoo Babel Fish 3  and Windows Live Translate 4 .  2  http://translate.google.com/translate_t 3  http://babelfish.yahoo.com/translate_txt 4  http://www.windowslivetranslator.com/ Unlabeled Chinese Reviews Labeled English Reviews Machine Translation (CN-EN) Co-Training Machine Translation (EN-CN) Labeled Chinese Reviews Unlabeled English Reviews Pos\Neg Chinese View English View Test Chinese Review Sentiment Classifier Machine Translation (CN-EN) Test English Review Training Phase Classification Phase 237 In this study, we adopt Google Translate for both English-to-Chinese Translation and Chinese-toEnglish Translation, because it is one of the state-of-the-art commercial machine translation systems used today." ></td>
	<td class="line x" title="85:195	Google Translate applies statistical learning techniques to build a translation model based on both monolingual text in the target language and aligned text consisting of examples of human translations between the languages." ></td>
	<td class="line x" title="86:195	3.3 The Co-Training Algorithm The co-training algorithm (Blum and Mitchell, 1998) is a typical bootstrapping method, which starts with a set of labeled data, and increase the amount of annotated data using some amounts of unlabeled data in an incremental way." ></td>
	<td class="line x" title="87:195	One important aspect of co-training is that two conditional independent views are required for cotraining to work, but the independence assumption can be relaxed." ></td>
	<td class="line x" title="88:195	Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001)." ></td>
	<td class="line x" title="89:195	In the context of cross-lingual sentiment classification, each labeled English review or unlabeled Chinese review has two views of features: English features and Chinese features." ></td>
	<td class="line x" title="90:195	Here, a review is used to indicate both its Chinese version and its English version, until stated otherwise." ></td>
	<td class="line x" title="91:195	The co-training algorithm is illustrated in Figure 2." ></td>
	<td class="line x" title="92:195	In the algorithm, the class distribution in the labeled data is maintained by balancing the parameter values of p and n at each iteration." ></td>
	<td class="line x" title="93:195	The intuition of the co-training algorithm is that if one classifier can confidently predict the class of an example, which is very similar to some of labeled ones, it can provide one more training example for the other classifier." ></td>
	<td class="line x" title="94:195	But, of course, if this example happens to be easy to be classified by the first classifier, it does not mean that this example will be easy to be classified by the second classifier, so the second classifier will get useful information to improve itself and vice versa (Kiritchenko and Matwin, 2001)." ></td>
	<td class="line x" title="95:195	In the co-training algorithm, a basic classification algorithm is required to construct C en  and C cn . Typical text classifiers include Support Vector Machine (SVM), Nave Bayes (NB), Maximum Entropy (ME), K-Nearest Neighbor (KNN), etc. In this study, we adopt the widely-used SVM classifier (Joachims, 2002)." ></td>
	<td class="line x" title="96:195	Viewing input data as two sets of vectors in a feature space, SVM constructs a separating hyperplane in the space by maximizing the margin between the two data sets." ></td>
	<td class="line x" title="97:195	The English or Chinese features used in this study include both unigrams and bigrams 5  and the feature weight is simply set to term frequency 6 . Feature selection methods (e.g. Document Frequency (DF), Information Gain (IG), and Mutual Information (MI)) can be used for dimension reduction." ></td>
	<td class="line x" title="98:195	But we use all the features in the experiments for comparative analysis, because there is no significant performance improvement after applying the feature selection techniques in our empirical study." ></td>
	<td class="line x" title="99:195	The output value of the SVM classifier for a review indicates the confidence level of the reviews classification." ></td>
	<td class="line x" title="100:195	Usually, the sentiment polarity of a review is indicated by the sign of the prediction value." ></td>
	<td class="line x" title="101:195	Given: F en  and F cn  are redundantly sufficient sets of features, where F en  represents the English features, F cn  represents the Chinese features; L is a set of labeled training reviews; U is a set of unlabeled reviews; Loop for I iterations: 1." ></td>
	<td class="line x" title="102:195	Learn the first classifier C en  from L based on F en ; 2." ></td>
	<td class="line x" title="103:195	Use C en  to label reviews from U based on F en ; 3." ></td>
	<td class="line x" title="104:195	Choose p positive and n negative the most confidently predicted reviews E en  from U; 4." ></td>
	<td class="line x" title="105:195	Learn the second classifier C cn  from L based on F cn ; 5." ></td>
	<td class="line x" title="106:195	Use C cn  to label reviews from U based on F cn ; 6." ></td>
	<td class="line x" title="107:195	Choose p positive and n negative the most confidently predicted reviews E cn  from U; 7." ></td>
	<td class="line x" title="108:195	Removes reviews E en E cn  from U 7 ; 8." ></td>
	<td class="line x" title="109:195	Add reviews E en E cn  with the corresponding labels to L; Figure 2." ></td>
	<td class="line x" title="110:195	The co-training algorithm In the training phase, the co-training algorithm learns two separate classifiers: C en  and C cn .  5  For Chinese text, a unigram refers to a Chinese word and a bigram refers to two adjacent Chinese words." ></td>
	<td class="line x" title="111:195	6  Term frequency performs better than TFIDF by our empirical analysis." ></td>
	<td class="line x" title="112:195	7  Note that the examples with conflicting labels are not included in E en E cn In other words, if an example is in both E en  and E cn , but the labels for the example is conflicting, the example will be excluded from E en E cn." ></td>
	<td class="line x" title="113:195	238 Therefore, in the classification phase, we can obtain two prediction values for a test review." ></td>
	<td class="line x" title="114:195	We normalize the prediction values into [-1, 1] by dividing the maximum absolute value." ></td>
	<td class="line x" title="115:195	Finally, the average of the normalized values is used as the overall prediction value of the review." ></td>
	<td class="line x" title="116:195	4 Empirical Evaluation 4.1 Evaluation Setup 4.1.1 Data set The following three datasets were collected and used in the experiments: Test Set (Labeled Chinese Reviews): In order to assess the performance of the proposed approach, we collected and labeled 886 product reviews (451 positive reviews + 435 negative reviews) from a popular Chinese IT product web site-IT168 8 . The reviews focused on such products as mp3 players, mobile phones, digital camera and laptop computers." ></td>
	<td class="line x" title="117:195	Training Set (Labeled English Reviews): There are many labeled English corpora available on the Web and we used the corpus constructed for multi-domain sentiment classification (Blitzer et al., 2007) 9 , because the corpus was large-scale and it was within similar domains as the test set." ></td>
	<td class="line x" title="118:195	The dataset consisted of 8000 Amazon product reviews (4000 positive reviews + 4000 negative reviews) for four different product types: books, DVDs, electronics and kitchen appliances." ></td>
	<td class="line x" title="119:195	Unlabeled Set (Unlabeled Chinese Reviews): We downloaded additional 1000 Chinese product reviews from IT168 and used the reviews as the unlabeled set." ></td>
	<td class="line x" title="120:195	Therefore, the unlabeled set and the test set were in the same domain and had similar underlying feature distributions." ></td>
	<td class="line x" title="121:195	Each Chinese review was translated into English review, and each English review was translated into Chinese review." ></td>
	<td class="line x" title="122:195	Therefore, each review has two independent views: English view and Chinese view." ></td>
	<td class="line x" title="123:195	A review is represented by both its English view and its Chinese view." ></td>
	<td class="line x" title="124:195	Note that the training set and the unlabeled set are used in the training phase, while the test set is blind to the training phase." ></td>
	<td class="line x" title="125:195	4.1.2 Evaluation Metric We used the standard precision, recall and Fmeasure to measure the performance of positive and negative class, respectively, and used the  8  http://www.it168.com 9  http://www.cis.upenn.edu/~mdredze/datasets/sentiment/ accuracy metric to measure the overall performance of the system." ></td>
	<td class="line x" title="126:195	The metrics are defined the same as in general text categorization." ></td>
	<td class="line x" title="127:195	4.1.3 Baseline Methods In the experiments, the proposed co-training approach (CoTrain) is compared with the following baseline methods: SVM(CN): This method applies the inductive SVM with only Chinese features for sentiment classification in the Chinese view." ></td>
	<td class="line x" title="128:195	Only Englishto-Chinese translation is needed." ></td>
	<td class="line x" title="129:195	And the unlabeled set is not used." ></td>
	<td class="line x" title="130:195	SVM(EN): This method applies the inductive SVM with only English features for sentiment classification in the English view." ></td>
	<td class="line x" title="131:195	Only Chineseto-English translation is needed." ></td>
	<td class="line x" title="132:195	And the unlabeled set is not used." ></td>
	<td class="line x" title="133:195	SVM(ENCN1): This method applies the inductive SVM with both English and Chinese features for sentiment classification in the two views." ></td>
	<td class="line x" title="134:195	Both English-to-Chinese and Chinese-toEnglish translations are required." ></td>
	<td class="line x" title="135:195	And the unlabeled set is not used." ></td>
	<td class="line x" title="136:195	SVM(ENCN2): This method combines the results of SVM(EN) and SVM(CN) by averaging the prediction values in the same way with the co-training approach." ></td>
	<td class="line x" title="137:195	TSVM(CN): This method applies the transductive SVM with only Chinese features for sentiment classification in the Chinese view." ></td>
	<td class="line x" title="138:195	Only English-to-Chinese translation is needed." ></td>
	<td class="line x" title="139:195	And the unlabeled set is used." ></td>
	<td class="line x" title="140:195	TSVM(EN): This method applies the transductive SVM with only English features for sentiment classification in the English view." ></td>
	<td class="line x" title="141:195	Only Chinese-to-English translation is needed." ></td>
	<td class="line x" title="142:195	And the unlabeled set is used." ></td>
	<td class="line x" title="143:195	TSVM(ENCN1): This method applies the transductive SVM with both English and Chinese features for sentiment classification in the two views." ></td>
	<td class="line x" title="144:195	Both English-to-Chinese and Chinese-toEnglish translations are required." ></td>
	<td class="line x" title="145:195	And the unlabeled set is used." ></td>
	<td class="line x" title="146:195	TSVM(ENCN2): This method combines the results of TSVM(EN) and TSVM(CN) by averaging the prediction values." ></td>
	<td class="line x" title="147:195	Note that the first four methods are straightforward methods used in previous work, while the latter four methods are strong baselines because the transductive SVM has been widely used for improving the classification accuracy by leveraging additional unlabeled examples." ></td>
	<td class="line x" title="148:195	239 4.2 Evaluation Results 4.2.1 Method Comparison In the experiments, we first compare the proposed co-training approach (I=40 and p=n=5) with the eight baseline methods." ></td>
	<td class="line x" title="149:195	The three parameters in the co-training approach are empirically set by considering the total number (i.e. 1000) of the unlabeled Chinese reviews." ></td>
	<td class="line x" title="150:195	In our empirical study, the proposed approach can perform well with a wide range of parameter values, which will be shown later." ></td>
	<td class="line x" title="151:195	Table 1 shows the comparison results." ></td>
	<td class="line x" title="152:195	Seen from the table, the proposed co-training approach outperforms all eight baseline methods over all metrics." ></td>
	<td class="line x" title="153:195	Among the eight baselines, the best one is TSVM(ENCN2), which combines the results of two transductive SVM classifiers." ></td>
	<td class="line x" title="154:195	Actually, TSVM(ENCN2) is similar to CoTrain because CoTrain also combines the results of two classifiers in the same way." ></td>
	<td class="line x" title="155:195	However, the co-training approach can train two more effective classifiers, and the accuracy values of the component English and Chinese classifiers are 0.775 and 0.790, respectively, which are higher than the corresponding TSVM classifiers." ></td>
	<td class="line x" title="156:195	Overall, the use of transductive learning and the combination of English and Chinese views are beneficial to the final classification accuracy, and the cotraining approach is more suitable for making use of the unlabeled Chinese reviews than the transductive SVM." ></td>
	<td class="line x" title="157:195	4.2.2 Influences of Iteration Number (I) Figure 3 shows the accuracy curve of the cotraining approach (Combined Classifier) with different numbers of iterations." ></td>
	<td class="line x" title="158:195	The iteration number I is varied from 1 to 80." ></td>
	<td class="line x" title="159:195	When I is set to 1, the co-training approach is degenerated into SVM(ENCN2)." ></td>
	<td class="line x" title="160:195	The accuracy curves of the component English and Chinese classifiers learned in the co-training approach are also shown in the figure." ></td>
	<td class="line x" title="161:195	We can see that the proposed co-training approach can outperform the best baselineTSVM(ENCN2) after 20 iterations." ></td>
	<td class="line x" title="162:195	After a large number of iterations, the performance of the cotraining approach decreases because noisy training examples may be selected from the remaining unlabeled set." ></td>
	<td class="line x" title="163:195	Finally, the performance of the approach does not change any more, because the algorithm runs out of all possible examples in the unlabeled set." ></td>
	<td class="line x" title="164:195	Fortunately, the proposed approach performs well with a wide range of iteration numbers." ></td>
	<td class="line x" title="165:195	We can also see that the two component classifier has similar trends with the cotraining approach." ></td>
	<td class="line x" title="166:195	It is encouraging that the component Chinese classifier alone can perform better than the best baseline when the iteration number is set between 40 and 70." ></td>
	<td class="line x" title="167:195	4.2.3 Influences of Growth Size (p, n) Figure 4 shows how the growth size at each iteration (p positive and n negative confident examples) influences the accuracy of the proposed co-training approach." ></td>
	<td class="line x" title="168:195	In the above experiments, we set p=n, which is considered as a balanced growth." ></td>
	<td class="line x" title="169:195	When p differs very much from n, the growth is considered as an imbalanced growth." ></td>
	<td class="line x" title="170:195	Balanced growth of (2, 2), (5, 5), (10, 10) and (15, 15) examples and imbalanced growth of (1, 5), (5, 1) examples are compared in the figure." ></td>
	<td class="line x" title="171:195	We can see that the performance of the cotraining approach with the balanced growth can be improved after a few iterations." ></td>
	<td class="line x" title="172:195	And the performance of the co-training approach with large p and n will more quickly become unchanged, because the approach runs out of the limited examples in the unlabeled set more quickly." ></td>
	<td class="line x" title="173:195	However, the performance of the co-training approaches with the two imbalanced growths is always going down quite rapidly, because the labeled unbalanced examples hurt the performance badly at each iteration." ></td>
	<td class="line x" title="174:195	Positive Negative Total Method Precision Recall F-measure Precision Recall F-measure Accuracy SVM(CN) 0.733 0.865 0.793 0.828 0.674 0.743 0.771 SVM(EN) 0.717 0.803 0.757 0.766 0.671 0.716 0.738 SVM(ENCN1) 0.744 0.820 0.781 0.792 0.708 0.748 0.765 SVM(ENCN2) 0.746 0.847 0.793 0.816 0.701 0.754 0.775 TSVM(CN) 0.724 0.878 0.794 0.838 0.653 0.734 0.767 TSVM(EN) 0.732 0.860 0.791 0.823 0.674 0.741 0.769 TSVM(ENCN1) 0.743 0.878 0.805 0.844 0.685 0.756 0.783 TSVM(ENCN2) 0.744 0.896 0.813 0.863 0.680 0.761 0.790 CoTrain (I=40; p=n=5) 0.768 0.905 0.831 0.879 0.717 0.790 0.813 Table 1." ></td>
	<td class="line x" title="175:195	Comparison results 240 0.72 0.73 0.74 0.75 0.76 0.77 0.78 0.79 0.8 0.81 0.82 1 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 Iteration Number (I ) Acc u ra c y English Classifier(CoTrain) Chinese Classifier(CoTrain) Combined Classifier(CoTrain) TSVM(ENCN2)  Figure 3." ></td>
	<td class="line x" title="176:195	Accuracy vs. number of iterations for co-training (p=n=5) 0.5 0.55 0.6 0.65 0.7 0.75 0.8 1 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 Iteration Number (I ) Ac cura c y (p=2,n=2) (p=5,n=5) (p=10,n=10) (p=15,n=15) (p=1,n=5) (p=5,n=1)  Figure 4." ></td>
	<td class="line x" title="177:195	Accuracy vs. different (p, n) for co-training 0.76 0.77 0.78 0.79 0.8 0.81 0.82 25% 50% 75% 100% Feature size Acc u r a c y TSVM(ENCN1) TSVM(ENCN2) CoTrain (I=40; p=n=5)  Figure 5." ></td>
	<td class="line x" title="178:195	Influences of feature size  241 4.2.4 Influences of Feature Selection In the above experiments, all features (unigram + bigram) are used." ></td>
	<td class="line x" title="179:195	As mentioned earlier, feature selection techniques are widely used for dimension reduction." ></td>
	<td class="line x" title="180:195	In this section, we further conduct experiments to investigate the influences of feature selection techniques on the classification results." ></td>
	<td class="line x" title="181:195	We use the simple but effective document frequency (DF) for feature selection." ></td>
	<td class="line x" title="182:195	Figures 6 show the comparison results of different feature sizes for the co-training approach and two strong baselines." ></td>
	<td class="line x" title="183:195	The feature size is measured as the proportion of the selected features against the total features (i.e. 100%)." ></td>
	<td class="line x" title="184:195	We can see from the figure that the feature selection technique has very slight influences on the classification accuracy of the methods." ></td>
	<td class="line x" title="185:195	It can be seen that the co-training approach can always outperform the two baselines with different feature sizes." ></td>
	<td class="line x" title="186:195	The results further demonstrate the effectiveness and robustness of the proposed cotraining approach." ></td>
	<td class="line x" title="187:195	5 Conclusion and Future Work In this paper, we propose to use the co-training approach to address the problem of cross-lingual sentiment classification." ></td>
	<td class="line x" title="188:195	The experimental results show the effectiveness of the proposed approach." ></td>
	<td class="line x" title="189:195	In future work, we will improve the sentiment classification accuracy in the following two ways: 1) The smoothed co-training approach used in (Mihalcea, 2004) will be adopted for sentiment classification." ></td>
	<td class="line x" title="190:195	The approach has the effect of smoothing the learning curves." ></td>
	<td class="line x" title="191:195	During the bootstrapping process of smoothed co-training, the classifier at each iteration is replaced with a majority voting scheme applied to all classifiers constructed at previous iterations." ></td>
	<td class="line x" title="192:195	2) The feature distributions of the translated text and the natural text in the same language are still different due to the inaccuracy of the machine translation service." ></td>
	<td class="line x" title="193:195	We will employ the structural correspondence learning (SCL) domain adaption algorithm used in (Blitzer et al., 2007) for linking the translated text and the natural text." ></td>
	<td class="line x" title="194:195	Acknowledgments This work was supported by NSFC (60873155), RFDP (20070001059), Beijing Nova Program (2008B03), National High-tech R&D Program (2008AA01Z421) and NCET (NCET-08-0006)." ></td>
	<td class="line x" title="195:195	We also thank the anonymous reviewers for their useful comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1028
A Non-negative Matrix Tri-factorization Approach to Sentiment Classification with Lexical Prior Knowledge
Li, Tao;Zhang, Yi;Sindhwani, Vikas;"></td>
	<td class="line x" title="1:191	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 244252, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:191	c2009 ACL and AFNLP A Non-negative Matrix Tri-factorization Approach to Sentiment Classification with Lexical Prior Knowledge Tao Li Yi Zhang School of Computer Science Florida International University {taoli,yzhan004}@cs.fiu.edu Vikas Sindhwani Mathematical Sciences IBM T.J. Watson Research Center vsindhw@us.ibm.com Abstract Sentiment classi cation refers to the task of automatically identifying whether a given piece of text expresses positive or negative opinion towards a subject at hand." ></td>
	<td class="line x" title="3:191	The proliferation of user-generated web content such as blogs, discussion forums and online review sites has made it possible to perform large-scale mining of public opinion." ></td>
	<td class="line x" title="4:191	Sentiment modeling is thus becoming a critical component of market intelligence and social media technologies that aim to tap into the collective wisdom of crowds." ></td>
	<td class="line x" title="5:191	In this paper, we consider the problem of learning high-quality sentiment models with minimal manual supervision." ></td>
	<td class="line x" title="6:191	We propose a novel approach to learn from lexical prior knowledge in the form of domain-independent sentimentladen terms, in conjunction with domaindependent unlabeled data and a few labeled documents." ></td>
	<td class="line x" title="7:191	Our model is based on a constrained non-negative tri-factorization of the term-document matrix which can be implemented using simple update rules." ></td>
	<td class="line x" title="8:191	Extensive experimental studies demonstrate the effectiveness of our approach on a variety of real-world sentiment prediction tasks." ></td>
	<td class="line x" title="9:191	1 Introduction Web 2.0 platforms such as blogs, discussion forums and other such social media have now given a public voice to every consumer." ></td>
	<td class="line x" title="10:191	Recent surveys have estimated that a massive number of internet users turn to such forums to collect recommendations for products and services, guiding their own choices and decisions by the opinions that other consumers have publically expressed." ></td>
	<td class="line x" title="11:191	Gleaning insights by monitoring and analyzing large amounts of such user-generated data is thus becoming a key competitive differentiator for many companies." ></td>
	<td class="line x" title="12:191	While tracking brand perceptions in traditional media is hardly a new challenge, handling the unprecedented scale of unstructured user-generated web content requires new methodologies." ></td>
	<td class="line x" title="13:191	These methodologies are likely to be rooted in natural language processing and machine learning techniques." ></td>
	<td class="line x" title="14:191	Automatically classifying the sentiment expressed in a blog around selected topics of interest is a canonical machine learning task in this discussion." ></td>
	<td class="line x" title="15:191	A standard approach would be to manually label documents with their sentiment orientation and then apply off-the-shelf text classi cation techniques." ></td>
	<td class="line x" title="16:191	However, sentiment is often conveyed with subtle linguistic mechanisms such as the use of sarcasm and highly domain-speci c contextual cues." ></td>
	<td class="line x" title="17:191	This makes manual annotation of sentiment time consuming and error-prone, presenting a bottleneck in learning high quality models." ></td>
	<td class="line x" title="18:191	Moreover, products and services of current focus, and associated community of bloggers with their idiosyncratic expressions, may rapidly evolve over time causing models to potentially lose performance and become stale." ></td>
	<td class="line x" title="19:191	This motivates the problem of learning robust sentiment models from minimal supervision." ></td>
	<td class="line x" title="20:191	In their seminal work, (Pang et al., 2002) demonstrated that supervised learning signi cantly outperformed a competing body of work where hand-crafted dictionaries are used to assign sentiment labels based on relative frequencies of positive and negative terms." ></td>
	<td class="line x" title="21:191	As observed by (Ng et al., 2006), most semi-automated dictionary-based approaches yield unsatisfactory lexicons, with either high coverage and low precision or vice versa." ></td>
	<td class="line x" title="22:191	However, the treatment of such dictionaries as forms of prior knowledge that can be incorporated in machine learning models is a relatively less explored topic; even lesser so in conjunction with semi-supervised models that attempt to utilize un244 labeled data." ></td>
	<td class="line x" title="23:191	This is the focus of the current paper." ></td>
	<td class="line x" title="24:191	Our models are based on a constrained nonnegative tri-factorization of the term-document matrix, which can be implemented using simple update rules." ></td>
	<td class="line x" title="25:191	Treated as a set of labeled features, the sentiment lexicon is incorporated as one set of constraints that enforce domain-independent prior knowledge." ></td>
	<td class="line x" title="26:191	A second set of constraints introduce domain-speci c supervision via a few document labels." ></td>
	<td class="line x" title="27:191	Together these constraints enable learning from partial supervision along both dimensions of the term-document matrix, in what may be viewed more broadly as a framework for incorporating dual-supervision in matrix factorization models." ></td>
	<td class="line x" title="28:191	We provide empirical comparisons with several competing methodologies on four, very different domains  blogs discussing enterprise software products, political blogs discussing US presidential candidates, amazon.com product reviews and IMDB movie reviews." ></td>
	<td class="line x" title="29:191	Results demonstrate the effectiveness and generality of our approach." ></td>
	<td class="line x" title="30:191	The rest of the paper is organized as follows." ></td>
	<td class="line x" title="31:191	We begin by discussing related work in Section 2." ></td>
	<td class="line x" title="32:191	Section 3 gives a quick background on Nonnegative Matrix Tri-factorization models." ></td>
	<td class="line x" title="33:191	In Section 4, we present a constrained model and computational algorithm for incorporating lexical knowledge in sentiment analysis." ></td>
	<td class="line x" title="34:191	In Section 5, we enhance this model by introducing document labels as additional constraints." ></td>
	<td class="line x" title="35:191	Section 6 presents an empirical study on four datasets." ></td>
	<td class="line x" title="36:191	Finally, Section 7 concludes this paper." ></td>
	<td class="line x" title="37:191	2 Related Work We point the reader to a recent book (Pang and Lee, 2008) for an in-depth survey of literature on sentiment analysis." ></td>
	<td class="line x" title="38:191	In this section, we briskly cover related work to position our contributions appropriately in the sentiment analysis and machine learning literature." ></td>
	<td class="line x" title="39:191	Methods focussing on the use and generation of dictionaries capturing the sentiment of words have ranged from manual approaches of developing domain-dependent lexicons (Das and Chen, 2001) to semi-automated approaches (Hu and Liu, 2004; Zhuang et al., 2006; Kim and Hovy, 2004), and even an almost fully automated approach (Turney, 2002)." ></td>
	<td class="line x" title="40:191	Most semi-automated approaches have met with limited success (Ng et al., 2006) and supervised learning models have tended to outperform dictionary-based classi cation schemes (Pang et al., 2002)." ></td>
	<td class="line pc" title="41:191	A two-tier scheme (Pang and Lee, 2004) where sentences are  rst classi ed as subjective versus objective, and then applying the sentiment classi er on only the subjective sentences further improves performance." ></td>
	<td class="line o" title="42:191	Results in these papers also suggest that using more sophisticated linguistic models, incorporating parts-of-speech and n-gram language models, do not improve over the simple unigram bag-of-words representation." ></td>
	<td class="line x" title="43:191	In keeping with these  ndings, we also adopt a unigram text model." ></td>
	<td class="line x" title="44:191	A subjectivity classi cation phase before our models are applied may further improve the results reported in this paper, but our focus is on driving the polarity prediction stage with minimal manual effort." ></td>
	<td class="line x" title="45:191	In this regard, our model brings two interrelated but distinct themes from machine learning to bear on this problem: semi-supervised learning and learning from labeled features." ></td>
	<td class="line x" title="46:191	The goal of the former theme is to learn from few labeled examples by making use of unlabeled data, while the goal of the latter theme is to utilize weak prior knowledge about term-class af nities (e.g., the term  awful indicates negative sentiment and therefore may be considered as a negatively labeled feature)." ></td>
	<td class="line x" title="47:191	Empirical results in this paper demonstrate that simultaneously attempting both these goals in a single model leads to improvements over models that focus on a single goal." ></td>
	<td class="line x" title="48:191	(Goldberg and Zhu, 2006) adapt semi-supervised graph-based methods for sentiment analysis but do not incorporate lexical prior knowledge in the form of labeled features." ></td>
	<td class="line x" title="49:191	Most work in machine learning literature on utilizing labeled features has focused on using them to generate weakly labeled examples that are then used for standard supervised learning: (Schapire et al., 2002) propose one such framework for boosting logistic regression; (Wu and Srihari, 2004) build a modi ed SVM and (Liu et al., 2004) use a combination of clustering and EM based methods to instantiate similar frameworks." ></td>
	<td class="line x" title="50:191	By contrast, we incorporate lexical knowledge directly as constraints on our matrix factorization model." ></td>
	<td class="line x" title="51:191	In recent work, Druck et al.(Druck et al., 2008) constrain the predictions of a multinomial logistic regression model on unlabeled instances in a Generalized Expectation formulation for learning from labeled features." ></td>
	<td class="line x" title="53:191	Unlike their approach which uses only unlabeled instances, our method uses both labeled and unlabeled documents in conjunction with labeled and 245 unlabeled words." ></td>
	<td class="line x" title="54:191	The matrix tri-factorization models explored in this paper are closely related to the models proposed recently in (Li et al., 2008; Sindhwani et al., 2008)." ></td>
	<td class="line x" title="55:191	Though, their techniques for proving algorithm convergence and correctness can be readily adapted for our models, (Li et al., 2008) do not incorporate dual supervision as we do." ></td>
	<td class="line x" title="56:191	On the other hand, while (Sindhwani et al., 2008) do incorporate dual supervision in a non-linear kernelbased setting, they do not enforce non-negativity or orthogonality  aspects of matrix factorization models that have shown bene ts in prior empirical studies, see e.g., (Ding et al., 2006)." ></td>
	<td class="line x" title="57:191	We also note the very recent work of (Sindhwani and Melville, 2008) which proposes a dualsupervision model for semi-supervised sentiment analysis." ></td>
	<td class="line x" title="58:191	In this model, bipartite graph regularization is used to diffuse label information along both sides of the term-document matrix." ></td>
	<td class="line x" title="59:191	Conceptually, their model implements a co-clustering assumption closely related to Singular Value Decomposition (see also (Dhillon, 2001; Zha et al., 2001) for more on this perspective) while our model is based on Non-negative Matrix Factorization." ></td>
	<td class="line x" title="60:191	In another recent paper (Sandler et al., 2008), standard regularization models are constrained using graphs of word co-occurences." ></td>
	<td class="line x" title="61:191	These are very recently proposed competing methodologies, and we have not been able to address empirical comparisons with them in this paper." ></td>
	<td class="line x" title="62:191	Finally, recent efforts have also looked at transfer learning mechanisms for sentiment analysis, e.g., see (Blitzer et al., 2007)." ></td>
	<td class="line x" title="63:191	While our focus is on single-domain learning in this paper, we note that cross-domain variants of our model can also be orthogonally developed." ></td>
	<td class="line x" title="64:191	3 Background 3.1 Basic Matrix Factorization Model Our proposed models are based on non-negative matrix Tri-factorization (Ding et al., 2006)." ></td>
	<td class="line x" title="65:191	In these models, an m n term-document matrix X is approximated by three factors that specify soft membership of terms and documents in one of kclasses: X FSGT ." ></td>
	<td class="line x" title="66:191	(1) where F is an m k non-negative matrix representing knowledge in the word space, i.e., i-th row of F represents the posterior probability of word i belonging to the k classes, G is an n k nonnegative matrix representing knowledge in document space, i.e., the i-th row of G represents the posterior probability of document i belonging to the k classes, and S is an k k nonnegative matrix providing a condensed view of X. The matrix factorization model is similar to the probabilistic latent semantic indexing (PLSI) model (Hofmann, 1999)." ></td>
	<td class="line x" title="67:191	In PLSI, X is treated as the joint distribution between words and documents by the scaling X ! flX = X/i j Xi j thus i j flXi j = 1)." ></td>
	<td class="line x" title="68:191	flX is factorized as flX WSDT , k Wik = 1, k D jk = 1, k Skk = 1." ></td>
	<td class="line x" title="69:191	(2) where X is the m n word-document semantic matrix, X = WSD, W is the word classconditional probability, and D is the document class-conditional probability and S is the class probability distribution." ></td>
	<td class="line x" title="70:191	PLSI provides a simultaneous solution for the word and document class conditional distribution." ></td>
	<td class="line x" title="71:191	Our model provides simultaneous solution for clustering the rows and the columns of X. To avoid ambiguity, the orthogonality conditions FT F = I, GT G = I." ></td>
	<td class="line x" title="72:191	(3) can be imposed to enforce each row of F and G to possess only one nonzero entry." ></td>
	<td class="line x" title="73:191	Approximating the term-document matrix with a tri-factorization while imposing non-negativity and orthogonality constraints gives a principled framework for simultaneously clustering the rows (words) and columns (documents) of X. In the context of coclustering, these models return excellent empirical performance, see e.g., (Ding et al., 2006)." ></td>
	<td class="line x" title="74:191	Our goal now is to bias these models with constraints incorporating (a) labels of features (coming from a domain-independent sentiment lexicon), and (b) labels of documents for the purposes of domainspeci c adaptation." ></td>
	<td class="line x" title="75:191	These enhancements are addressed in Sections 4 and 5 respectively." ></td>
	<td class="line x" title="76:191	4 Incorporating Lexical Knowledge We used a sentiment lexicon generated by the IBM India Research Labs that was developed for other text mining applications (Ramakrishnan et al., 2003)." ></td>
	<td class="line x" title="77:191	It contains 2,968 words that have been human-labeled as expressing positive or negative sentiment." ></td>
	<td class="line x" title="78:191	In total, there are 1,267 positive (e.g.  great ) and 1,701 negative (e.g.,  bad ) unique 246 terms after stemming." ></td>
	<td class="line x" title="79:191	We eliminated terms that were ambiguous and dependent on context, such as  dear and   ne . It should be noted, that this list was constructed without a speci c domain in mind; which is further motivation for using training examples and unlabeled data to learn domain speci c connotations." ></td>
	<td class="line x" title="80:191	Lexical knowledge in the form of the polarity of terms in this lexicon can be introduced in the matrix factorization model." ></td>
	<td class="line x" title="81:191	By partially specifying term polarities via F, the lexicon in uences the sentiment predictions G over documents." ></td>
	<td class="line x" title="82:191	4.1 Representing Knowledge in Word Space Let F0 represent prior knowledge about sentimentladen words in the lexicon, i.e., if word i is a positive word (F0)i1 = 1 while if it is negative (F0)i2 = 1." ></td>
	<td class="line x" title="83:191	Note that one may also use soft sentiment polarities though our experiments are conducted with hard assignments." ></td>
	<td class="line x" title="84:191	This information is incorporated in the tri-factorization model via a squared loss term, min F;G;S kX FSGTk2 + Trbracketleftbig(F F0)TC1(F F0)bracketrightbig (4) where the notation Tr(A) means trace of the matrix A. Here,  > 0 is a parameter which determines the extent to which we enforce F F0, C1 is a m m diagonal matrix whose entry (C1)ii = 1 if the category of the i-th word is known (i.e., speci ed by the i-th row of F0) and (C1)ii = 0 otherwise." ></td>
	<td class="line x" title="85:191	The squared loss terms ensure that the solution for F in the otherwise unsupervised learning problem be close to the prior knowledge F0." ></td>
	<td class="line x" title="86:191	Note that if C1 = I, then we know the class orientation of all the words and thus have a full speci cation of F0, Eq.(4) is then reduced to min F;G;S kX FSGTk2 + kF F0k2 (5) The above model is generic and it allows certain  exibility." ></td>
	<td class="line x" title="87:191	For example, in some cases, our prior knowledge on F0 is not very accurate and we use smaller  so that the  nal results are not dependent on F0 very much, i.e., the results are mostly unsupervised learning results." ></td>
	<td class="line x" title="88:191	In addition, the introduction of C1 allows us to incorporate partial knowledge on word polarity information." ></td>
	<td class="line x" title="89:191	4.2 Computational Algorithm The optimization problem in Eq.( 4) can be solved using the following update rules G jk G jk (X T FS)jk (GGT XT FS)jk , (6) Sik  Sik (F T XG)ik (FT FSGT G)ik ." ></td>
	<td class="line x" title="90:191	(7) Fik Fik (XGS T + C1F0)ik (FFT XGST + C1F)ik ." ></td>
	<td class="line x" title="91:191	(8) The algorithm consists of an iterative procedure using the above three rules until convergence." ></td>
	<td class="line x" title="92:191	We call this approach Matrix Factorization with Lexical Knowledge (MFLK) and outline the precise steps in the table below." ></td>
	<td class="line x" title="93:191	Algorithm 1 Matrix Factorization with Lexical Knowledge (MFLK) begin 1." ></td>
	<td class="line x" title="94:191	Initialization: Initialize F = F0 G to K-means clustering results, S = (FT F) 1FT XG(GT G) 1." ></td>
	<td class="line x" title="95:191	2. Iteration: Update G:  xing F,S, updating G Update F:  xing S,G, updating F Update S:  xing F,G, updating S end 4.3 Algorithm Correctness and Convergence Updating F,G,S using the rules above leads to an asymptotic convergence to a local minima." ></td>
	<td class="line x" title="96:191	This can be proved using arguments similar to (Ding et al., 2006)." ></td>
	<td class="line x" title="97:191	We outline the proof of correctness for updating F since the squared loss term that involves F is a new component in our models." ></td>
	<td class="line x" title="98:191	Theorem 1 The above iterative algorithm converges." ></td>
	<td class="line x" title="99:191	Theorem 2 At convergence, the solution satisfies the Karuch, Kuhn, Tucker optimality condition, i.e., the algorithm converges correctly to a local optima." ></td>
	<td class="line x" title="100:191	Theorem 1 can be proved using the standard auxiliary function approach used in (Lee and Seung, 2001)." ></td>
	<td class="line x" title="101:191	Proof of Theorem 2." ></td>
	<td class="line x" title="102:191	Following the theory of constrained optimization (Nocedal and Wright, 1999), 247 we minimize the following function L(F) =jjX FSGTjj2 +Trbracketleftbig(F F0)TC1(F F0)bracketrightbig Note that the gradient of L is, L F = 2XGS T + 2FSGT GST + 2C1(F F0)." ></td>
	<td class="line x" title="103:191	(9) The KKT complementarity condition for the nonnegativity of Fik gives [ 2XGST + FSGT GST + 2C1(F F0)]ikFik = 0." ></td>
	<td class="line x" title="104:191	(10) This is the  xed point relation that local minima for F must satisfy." ></td>
	<td class="line x" title="105:191	Given an initial guess of F, the successive update of F using Eq.(8) will converge to a local minima." ></td>
	<td class="line x" title="106:191	At convergence, we have Fik = Fik (XGS T + C1F0)ik (FFT XGST + C1F)ik . which is equivalent to the KKT condition of Eq.(10)." ></td>
	<td class="line x" title="107:191	The correctness of updating rules for G in Eq.(6) and S in Eq.(7) have been proved in (Ding et al., 2006)." ></td>
	<td class="line x" title="108:191	u Note that we do not enforce exact orthogonality in our updating rules since this often implies softer class assignments." ></td>
	<td class="line x" title="109:191	5 Semi-Supervised Learning With Lexical Knowledge So far our models have made no demands on human effort, other than unsupervised collection of the term-document matrix and a one-time effort in compiling a domain-independent sentiment lexicon." ></td>
	<td class="line x" title="110:191	We now assume that a few documents are manually labeled for the purposes of capturing some domain-speci c connotations leading to a more domain-adapted model." ></td>
	<td class="line x" title="111:191	The partial labels on documents can be described using G0 where (G0)i1 = 1 if the document expresses positive sentiment, and (G0)i2 = 1 for negative sentiment." ></td>
	<td class="line x" title="112:191	As with F0, one can also use soft sentiment labeling for documents, though our experiments are conducted with hard assignments." ></td>
	<td class="line x" title="113:191	Therefore, the semi-supervised learning with lexical knowledge can be described as min F;G;S kX FSGTk2 + Trbracketleftbig(F F0)TC1(F F0)bracketrightbig+ Trbracketleftbig(G G0)TC2(G G0)bracketrightbig Where  > 0, > 0 are parameters which determine the extent to which we enforce F  F0 and G G0 respectively, C1 and C2 are diagonal matrices indicating the entries of F0 and G0 that correspond to labeled entities." ></td>
	<td class="line x" title="114:191	The squared loss terms ensure that the solution for F,G, in the otherwise unsupervised learning problem, be close to the prior knowledge F0 and G0." ></td>
	<td class="line x" title="115:191	5.1 Computational Algorithm The optimization problem in Eq.( 4) can be solved using the following update rules G jk G jk (X T FS + C2G0)jk (GGT XT FS + GGTC2G0)jk (11) Sik  Sik (F T XG)ik (FT FSGT G)ik ." ></td>
	<td class="line x" title="116:191	(12) Fik Fik (XGS T + C1F0)ik (FFT XGST + C1F)ik ." ></td>
	<td class="line x" title="117:191	(13) Thus the algorithm for semi-supervised learning with lexical knowledge based on our matrix factorization framework, referred as SSMFLK, consists of an iterative procedure using the above three rules until convergence." ></td>
	<td class="line x" title="118:191	The correctness and convergence of the algorithm can also be proved using similar arguments as what we outlined earlier for MFLK in Section 4.3." ></td>
	<td class="line x" title="119:191	A quick word about computational complexity." ></td>
	<td class="line x" title="120:191	The term-document matrix is typically very sparse with z nm non-zero entries while k is typically also much smaller than n,m. By using sparse matrix multiplications and avoiding dense intermediate matrices, the updates can be very ef ciently and easily implemented." ></td>
	<td class="line x" title="121:191	In particular, updating F,S,G each takes O(k2(m + n) + kz) time per iteration which scales linearly with the dimensions and density of the data matrix." ></td>
	<td class="line x" title="122:191	Empirically, the number of iterations before practical convergence is usually very small (less than 100)." ></td>
	<td class="line x" title="123:191	Thus, computationally our approach scales to large datasets even though our experiments are run on relatively small-sized datasets." ></td>
	<td class="line x" title="124:191	6 Experiments 6.1 Datasets Description Four different datasets are used in our experiments." ></td>
	<td class="line x" title="125:191	Movies Reviews: This is a popular dataset in sentiment analysis literature (Pang et al., 2002)." ></td>
	<td class="line x" title="126:191	It consists of 1000 positive and 1000 negative movie reviews drawn from the IMDB archive of the rec.arts.movies.reviews newsgroups." ></td>
	<td class="line x" title="127:191	248 Lotus blogs: The data set is targeted at detecting sentiment around enterprise software, specifically pertaining to the IBM Lotus brand (Sindhwani and Melville, 2008)." ></td>
	<td class="line x" title="128:191	An unlabeled set of blog posts was created by randomly sampling 2000 posts from a universe of 14,258 blogs that discuss issues relevant to Lotus software." ></td>
	<td class="line x" title="129:191	In addition to this unlabeled set, 145 posts were chosen for manual labeling." ></td>
	<td class="line x" title="130:191	These posts came from 14 individual blogs, 4 of which are actively posting negative content on the brand, with the rest tending to write more positive or neutral posts." ></td>
	<td class="line x" title="131:191	The data was collected by downloading the latest posts from each bloggers RSS feeds, or accessing the blogs archives." ></td>
	<td class="line x" title="132:191	Manual labeling resulted in 34 positive and 111 negative examples." ></td>
	<td class="line x" title="133:191	Political candidate blogs: For our second blog domain, we used data gathered from 16,742 political blogs, which contain over 500,000 posts." ></td>
	<td class="line x" title="134:191	As with the Lotus dataset, an unlabeled set was created by randomly sampling 2000 posts." ></td>
	<td class="line x" title="135:191	107 posts were chosen for labeling." ></td>
	<td class="line x" title="136:191	A post was labeled as having positive or negative sentiment about a speci c candidate (Barack Obama or Hillary Clinton) if it explicitly mentioned the candidate in positive or negative terms." ></td>
	<td class="line x" title="137:191	This resulted in 49 positively and 58 negatively labeled posts." ></td>
	<td class="line x" title="138:191	Amazon Reviews: The dataset contains product reviews taken from Amazon.com from 4 product types: Kitchen, Books, DVDs, and Electronics (Blitzer et al., 2007)." ></td>
	<td class="line x" title="139:191	The dataset contains about 4000 positive reviews and 4000 negative reviews and can be obtained from http://www.cis.upenn." ></td>
	<td class="line x" title="140:191	edu/mdredze/datasets/sentiment/." ></td>
	<td class="line x" title="141:191	For all datasets, we picked 5000 words with highest document-frequency to generate the vocabulary." ></td>
	<td class="line x" title="142:191	Stopwords were removed and a normalized term-frequency representation was used." ></td>
	<td class="line x" title="143:191	Genuinely unlabeled posts for Political and Lotus were used for semi-supervised learning experiments in section 6.3; they were not used in section 6.2 on the effect of lexical prior knowledge." ></td>
	<td class="line x" title="144:191	In the experiments, we set , the parameter determining the extent to which to enforce the feature labels, to be 1/2, and , the corresponding parameter for enforcing document labels, to be 1." ></td>
	<td class="line x" title="145:191	6.2 Sentiment Analysis with Lexical Knowledge Of course, one can remove all burden on human effort by simply using unsupervised techniques." ></td>
	<td class="line x" title="146:191	Our interest in the  rst set of experiments is to explore the bene ts of incorporating a sentiment lexicon over unsupervised approaches." ></td>
	<td class="line x" title="147:191	Does a one-time effort in compiling a domainindependent dictionary and using it for different sentiment tasks pay off in comparison to simply using unsupervised methods?" ></td>
	<td class="line x" title="148:191	In our case, matrix tri-factorization and other co-clustering methods form the obvious unsupervised baseline for comparison and so we start by comparing our method (MFLK) with the following methods:  Four document clustering methods: Kmeans, Tri-Factor Nonnegative Matrix Factorization (TNMF) (Ding et al., 2006), Information-Theoretic Co-clustering (ITCC) (Dhillon et al., 2003), and Euclidean Co-clustering algorithm (ECC) (Cho et al., 2004)." ></td>
	<td class="line x" title="149:191	These methods do not make use of the sentiment lexicon." ></td>
	<td class="line x" title="150:191	Feature Centroid (FC): This is a simple dictionary-based baseline method." ></td>
	<td class="line x" title="151:191	Recall that each word can be expressed as a  bagof-documents vector." ></td>
	<td class="line x" title="152:191	In this approach, we compute the centroids of these vectors, one corresponding to positive words and another corresponding to negative words." ></td>
	<td class="line x" title="153:191	This yields a two-dimensional representation for documents, on which we then perform K-means clustering." ></td>
	<td class="line x" title="154:191	Performance Comparison Figure 1 shows the experimental results on four datasets using accuracy as the performance measure." ></td>
	<td class="line x" title="155:191	The results are obtained by averaging 20 runs." ></td>
	<td class="line x" title="156:191	It can be observed that our MFLK method can effectively utilize the lexical knowledge to improve the quality of sentiment prediction." ></td>
	<td class="line x" title="157:191	Movies Lotus Political Amazon0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Accuracy   MFLK FC TNMF ECC ITCC KMeans Figure 1: Accuracy results on four datasets 249 Size of Sentiment Lexicon We also investigate the effects of the size of the sentiment lexicon on the performance of our model." ></td>
	<td class="line x" title="158:191	Figure 2 shows results with random subsets of the lexicon of increasing size." ></td>
	<td class="line x" title="159:191	We observe that generally the performance increases as more and more lexical supervision is provided." ></td>
	<td class="line x" title="160:191	0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 Fraction of sentiment words labeled Accuracy   Movies Lotus Political Amazon Figure 2: MFLK accuracy as size of sentiment lexicon (i.e., number of words in the lexicon) increases on the four datasets Robustness to Vocabulary Size High dimensionality and noise can have profound impact on the comparative performance of clustering and semi-supervised learning algorithms." ></td>
	<td class="line x" title="161:191	We simulate scenarios with different vocabulary sizes by selecting words based on information gain." ></td>
	<td class="line x" title="162:191	It should, however, be kept in mind that in a truely unsupervised setting document labels are unavailable and therefore information gain cannot be practically computed." ></td>
	<td class="line x" title="163:191	Figure 3 and Figure 4 show results for Lotus and Amazon datasets respectively and are representative of performance on other datasets." ></td>
	<td class="line x" title="164:191	MLFK tends to retain its position as the best performing method even at different vocabulary sizes." ></td>
	<td class="line x" title="165:191	ITCC performance is also noteworthy given that it is a completely unsupervised method." ></td>
	<td class="line x" title="166:191	0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 Fraction of Original Vocabulary Accuracy   MFLK FC TNMF KMeans ITCC ECC Figure 3: Accuracy results on Lotus dataset with increasing vocabulary size 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 Fraction of Original Vocabulary Accuracy   MFLK FC TNMF KMeans ITCC ECC Figure 4: Accuracy results on Amazon dataset with increasing vocabulary size 6.3 Sentiment Analysis with Dual Supervision We now assume that together with labeled features from the sentiment lexicon, we also have access to a few labeled documents." ></td>
	<td class="line x" title="167:191	The natural question is whether the presence of lexical constraints leads to better semi-supervised models." ></td>
	<td class="line x" title="168:191	In this section, we compare our method (SSMFLK) with the following three semi-supervised approaches: (1) The algorithm proposed in (Zhou et al., 2003) which conducts semi-supervised learning with local and global consistency (Consistency Method); (2) Zhu et al.s harmonic Gaussian  eld method coupled with the Class Mass Normalization (HarmonicCMN) (Zhu et al., 2003); and (3) Greens function learning algorithm (Greens Function) proposed in (Ding et al., 2007)." ></td>
	<td class="line x" title="169:191	We also compare the results of SSMFLK with those of two supervised classi cation methods: Support Vector Machine (SVM) and Naive Bayes." ></td>
	<td class="line x" title="170:191	Both of these methods have been widely used in sentiment analysis." ></td>
	<td class="line x" title="171:191	In particular, the use of SVMs in (Pang et al., 2002) initially sparked interest in using machine learning methods for sentiment classi cation." ></td>
	<td class="line x" title="172:191	Note that none of these competing methods utilizes lexical knowledge." ></td>
	<td class="line x" title="173:191	The results are presented in Figure 5, Figure 6, Figure 7, and Figure 8." ></td>
	<td class="line x" title="174:191	We note that our SSMFLK method either outperforms all other methods over the entire range of number of labeled documents (Movies, Political), or ultimately outpaces other methods (Lotus, Amazon) as a few document labels come in." ></td>
	<td class="line x" title="175:191	Learning Domain-Specific Connotations In our  rst set of experiments, we incorporated the sentiment lexicon in our models and learnt the sentiment orientation of words and documents via F,G factors respectively." ></td>
	<td class="line x" title="176:191	In the second set of 250 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.50.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 Number of documents labeled as a fraction of the original set of labeled documents Accuracy   SSMFLK Consistency Method HomonicCMN Green Function SVM Naive Bays Figure 5: Accuracy results with increasing number of labeled documents on Movies dataset 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.50.3 0.4 0.5 0.6 0.7 0.8 0.9 Number of documents labeled as a fraction of the original set of labeled documents Accuracy   SSMFLK Consistency Method HomonicCMN Green Function SVM Naive Bayes Figure 6: Accuracy results with increasing number of labeled documents on Lotus dataset experiments, we additionally introduced labeled documents for domain-speci c adjustments." ></td>
	<td class="line x" title="177:191	Between these experiments, we can now look for words that switch sentiment polarity." ></td>
	<td class="line x" title="178:191	These words are interesting because their domain-speci c connotation differs from their lexical orientation." ></td>
	<td class="line x" title="179:191	For amazon reviews, the following words switched polarity from positive to negative: fan, important, learning, cons, fast, feature, happy, memory, portable, simple, small, work while the following words switched polarity from negative to positive: address, finish, lack, mean, budget, rent, throw." ></td>
	<td class="line x" title="180:191	Note that words like fan, memory probably refer to product or product components (i.e., computer fan and memory) in the amazon review context but have a very different connotation say in the context of movie reviews where they probably refer to movie fanfare and memorable performances." ></td>
	<td class="line x" title="181:191	We were surprised to see happy switch polarity!" ></td>
	<td class="line x" title="182:191	Two examples of its negative-sentiment usage are: I ended up buying a Samsung and I couldnt be more happy and BORING, not one single exciting thing about this book." ></td>
	<td class="line x" title="183:191	I was happy when my lunch break ended so I could go back to work and stop reading." ></td>
	<td class="line x" title="184:191	0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.50.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 Number of documents labeled as a fraction of the original set of labeled documents Accuracy   SSMFLK Consistency Method HomonicCMN Green Function SVM Naive Bays Figure 7: Accuracy results with increasing number of labeled documents on Political dataset 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.50.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 Number of documents labeled as a fraction of the original set of labeled documents Accuracy   SSMFLK Consistency Method HomonicCMN Green Function SVM Naive Bays Figure 8: Accuracy results with increasing number of labeled documents on Amazon dataset 7 Conclusion The primary contribution of this paper is to propose and benchmark new methodologies for sentiment analysis." ></td>
	<td class="line x" title="185:191	Non-negative Matrix Factorizations constitute a rich body of algorithms that have found applicability in a variety of machine learning applications: from recommender systems to document clustering." ></td>
	<td class="line x" title="186:191	We have shown how to build effective sentiment models by appropriately constraining the factors using lexical prior knowledge and document annotations." ></td>
	<td class="line x" title="187:191	To more effectively utilize unlabeled data and induce domain-speci c adaptation of our models, several extensions are possible: facilitating learning from related domains, incorporating hyperlinks between documents, incorporating synonyms or co-occurences between words etc. As a topic of vigorous current activity, there are several very recently proposed competing methodologies for sentiment analysis that we would like to benchmark against." ></td>
	<td class="line x" title="188:191	These are topics for future work." ></td>
	<td class="line x" title="189:191	Acknowledgement: The work of T. Li is partially supported by NSF grants DMS-0844513 and CCF-0830659." ></td>
	<td class="line x" title="190:191	We would also like to thank Prem Melville and Richard Lawrence for their support." ></td>
	<td class="line x" title="191:191	251" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1078
A Framework of Feature Selection Methods for Text Categorization
Li, Shoushan;Xia, Rui;Zong, Chengqing;Huang, Chu-Ren;"></td>
	<td class="line x" title="1:174	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 692700, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:174	c2009 ACL and AFNLP A Framework of Feature Selection Methods for Text Categorization   Shoushan Li1  Rui Xia2  Chengqing Zong2  Chu-Ren Huang1  1 Department of Chinese and Bilingual Studies The Hong Kong Polytechnic University {shoushan.li,churenhuang} @gmail.com  2 National Laboratory of Pattern Recognition  Institute of Automation  Chinese Academy of Sciences {rxia,cqzong}@nlpr.ia.ac.cn    Abstract In text categorization, feature selection (FS) is a strategy that aims at making text classifiers more efficient and accurate." ></td>
	<td class="line x" title="3:174	However, when dealing with a new task, it is still difficult to quickly select a suitable one from various FS methods provided by many previous studies." ></td>
	<td class="line x" title="4:174	In this paper, we propose a theoretic framework of FS methods based on two basic measurements: frequency measurement and ratio measurement." ></td>
	<td class="line x" title="5:174	Then six popular FS methods are in detail discussed under this framework." ></td>
	<td class="line x" title="6:174	Moreover, with the guidance of our theoretical analysis, we propose a novel method called weighed frequency and odds (WFO) that combines the two measurements with trained weights." ></td>
	<td class="line x" title="7:174	The experimental results on data sets from both topic-based and sentiment classification tasks show that this new method is robust across different tasks and numbers of selected features." ></td>
	<td class="line x" title="8:174	1 Introduction With the rapid growth of online information, text classification, the task of assigning text documents to one or more predefined categories, has become one of the key tools for automatically handling and organizing text information." ></td>
	<td class="line x" title="9:174	The problems of text classification normally involve the difficulty of extremely high dimensional feature space which sometimes makes learning algorithms intractable." ></td>
	<td class="line x" title="10:174	A standard procedure to reduce the feature dimensionality is called feature selection (FS)." ></td>
	<td class="line x" title="11:174	Various FS methods, such as document frequency (DF), information gain (IG), mutual information (MI), 2 -test (CHI), Bi-Normal Separation (BNS), and weighted log-likelihood ratio (WLLR), have been proposed for the tasks (Yang and Pedersen, 1997; Nigam et al., 2000; Forman, 2003) and make text classification more efficient and accurate." ></td>
	<td class="line x" title="12:174	However, comparing these FS methods appears to be difficult because they are usually based on different theories or measurements." ></td>
	<td class="line x" title="13:174	For example, MI and IG are based on information theory, while CHI is mainly based on the measurements of statistic independence." ></td>
	<td class="line x" title="14:174	Previous comparisons of these methods have mainly depended on empirical studies that are heavily affected by the experimental sets." ></td>
	<td class="line x" title="15:174	As a result, conclusions from those studies are sometimes inconsistent." ></td>
	<td class="line x" title="16:174	In order to better understand the relationship between these methods, building a general theoretical framework provides a fascinating perspective." ></td>
	<td class="line x" title="17:174	Furthermore, in real applications, selecting an appropriate FS method remains hard for a new task because too many FS methods are available due to the long history of FS studies." ></td>
	<td class="line x" title="18:174	For example, merely in an early survey paper (Sebastiani, 2002), eight methods are mentioned." ></td>
	<td class="line x" title="19:174	These methods are provided by previous work for dealing with different text classification tasks but none of them is shown to be robust across different classification applications." ></td>
	<td class="line x" title="20:174	In this paper, we propose a framework with two basic measurements for theoretical comparison of six FS methods which are widely used in text classification." ></td>
	<td class="line x" title="21:174	Moreover, a novel method is set forth that combines the two measurements and tunes their influences considering different application domains and numbers of selected features." ></td>
	<td class="line x" title="22:174	The remainder of this paper is organized as follows." ></td>
	<td class="line x" title="23:174	Section 2 introduces the related work on 692 feature selection for text classification." ></td>
	<td class="line x" title="24:174	Section 3 theoretically analyzes six FS methods and proposes a new FS approach." ></td>
	<td class="line x" title="25:174	Experimental results are presented and analyzed in Section 4." ></td>
	<td class="line x" title="26:174	Finally, Section 5 draws our conclusions and outlines the future work." ></td>
	<td class="line x" title="27:174	2 Related Work FS is a basic problem in pattern recognition and has been a fertile field of research and development since the 1970s." ></td>
	<td class="line x" title="28:174	It has been proven to be effective on removing irrelevant and redundant features, increasing efficiency in learning tasks, and improving learning performance." ></td>
	<td class="line x" title="29:174	FS methods fall into two broad categories, the filter model and the wrapper model (John et al., 1994)." ></td>
	<td class="line x" title="30:174	The wrapper model requires one predetermined learning algorithm in feature selection and uses its performance to evaluate and determine which features are selected." ></td>
	<td class="line x" title="31:174	And the filter model relies on general characteristics of the training data to select some features without involving any specific learning algorithm." ></td>
	<td class="line x" title="32:174	There is evidence that wrapper methods often perform better on small scale problems (John et al, 1994), but on large scale problems, such as text classification, wrapper methods are shown to be impractical because of its high computational cost." ></td>
	<td class="line x" title="33:174	Therefore, in text classification, filter methods using feature scoring metrics are popularly used." ></td>
	<td class="line x" title="34:174	Below we review some recent studies of feature selection on both topic-based and sentiment classification." ></td>
	<td class="line x" title="35:174	In the past decade, FS studies mainly focus on topic-based classification where the classification categories are related to the subject content, e.g., sport or education." ></td>
	<td class="line x" title="36:174	Yang and Pedersen (1997) investigate five FS metrics and report that good FS methods improve the categorization accuracy with an aggressive feature removal using DF, IG, and CHI." ></td>
	<td class="line x" title="37:174	More recently, Forman (2003) empirically compares twelve FS methods on 229 text classification problem instances and proposes a new method called 'Bi-Normal Separation' (BNS)." ></td>
	<td class="line x" title="38:174	Their experimental results show that BNS can perform very well in the evaluation metrics of recall rate and F-measure." ></td>
	<td class="line x" title="39:174	But for the metric of precision, it often loses to IG." ></td>
	<td class="line x" title="40:174	Besides these two comparison studies, many others contribute to this topic (Yang and Liu, 1999; Brank et al., 2002; Gabrilovich and Markovitch, 2004) and more and more new FS methods are generated, such as, Gini index (Shang et al., 2007), Distance to Transition Point (DTP) (Moyotl-Hernandez and Jimenez-Salazar, 2005), Strong Class Information Words (SCIW) (Li and Zong, 2005) and parameter tuning based FS for Rocchio classifier (Moschitti, 2003)." ></td>
	<td class="line x" title="41:174	Recently, sentiment classification has become popular because of its wide applications (Pang et al., 2002)." ></td>
	<td class="line x" title="42:174	Its criterion of classification is the attitude expressed in the text (e.g., recommended or not recommended, positive or negative) rather than some facts (e.g., sport or education)." ></td>
	<td class="line x" title="43:174	To our best knowledge, yet no related work has focused on comparison studies of FS methods on this special task." ></td>
	<td class="line x" title="44:174	There are only some scattered reports in their experimental studies." ></td>
	<td class="line x" title="45:174	Riloff et al.(2006) report that the traditional FS method (only using IG method) performs worse than the baseline in some cases." ></td>
	<td class="line x" title="47:174	However, Cui et al.(2006) present the experiments on the sentiment classification for large-scale online product reviews to show that using the FS method of CHI does not degrade the performance but can significantly reduce the dimension of the feature vector." ></td>
	<td class="line pc" title="49:174	Moreover, Ng et al.(2006) examine the FS of the weighted log-likelihood ratio (WLLR) on the movie review dataset and achieves an accuracy of 87.1%, which is higher than the result reported by Pang and Lee (2004) with the same dataset." ></td>
	<td class="line x" title="51:174	From the analysis above, we believe that the performance of the sentiment classification system is also dramatically affected by FS." ></td>
	<td class="line x" title="52:174	3 Our Framework In the selection process, each feature (term, or single word) is assigned with a score according to a score-computing function." ></td>
	<td class="line x" title="53:174	Then those with higher scores are selected." ></td>
	<td class="line x" title="54:174	These mathematical definitions of the score-computing functions are often defined by some probabilities which are estimated by some statistic information in the documents across different categories." ></td>
	<td class="line x" title="55:174	For the convenience of description, we give some notations of these probabilities below." ></td>
	<td class="line x" title="56:174	( )P t : the probability that a document x  contains term t ; ( )iP c : the probability that a document x  does not belong to category ic ; ( , )iP t c : the joint probability that a document x contains term t  and also belongs to category ic ; ( | )iP c t : the probability that a document x belongs to category ic g712under the condition that it contains term t. 693 ( | )iP t c : the probability that, a document x does not contain term t with the condition that x belongs to category ic ; Some other probabilities, such as ( )P t , ( )iP c , ( | )iP t c , ( | )iP t c , ( | )iP c t ,  and ( | )iP c t , are similarly defined." ></td>
	<td class="line x" title="57:174	In order to estimate these probabilities, statistical information from the training data is needed, and notations about the training data are given as follows: 1{ } m i ic = : the set of categories; iA : the number of the documents that contain the term t  and also belong to category ic ; iB : the number of the documents that contain the term t  but do not belong to category ic ; iN : the total number of the documents that belong to category ic ; allN : the total number of all documents from the training data." ></td>
	<td class="line x" title="58:174	iC : the number of the documents that do not contain the term t  but belong to category ic , i.e., i iN A iD : the number of the documents that neither contain the term t  nor belong to category ic , i.e., all i iN N B  ; In this section, we would analyze theoretically six popular methods, namely DF, MI, IG, CHI, BNS, and WLLR." ></td>
	<td class="line x" title="59:174	Although these six FS methods are defined differently with different scoring measurements, we believe that they are strongly related." ></td>
	<td class="line x" title="60:174	In order to connect them, we define two basic measurements which are discussed as follows." ></td>
	<td class="line x" title="61:174	The first measurement is to compute the document frequency in one category, i.e., iA . The second measurement is the ratio between the document frequencies in one category and the other categories, i.e., /i iA B . The terms with a high ratio are often referred to as the terms with high category information." ></td>
	<td class="line x" title="62:174	These two measurements form the basis for all the measurements that are used by the FS methods throughout this paper." ></td>
	<td class="line x" title="63:174	In particular, we show that DF and MI are using the first and second measurement respectively." ></td>
	<td class="line x" title="64:174	Other complicated FS methods are combinations of these two measurements." ></td>
	<td class="line x" title="65:174	Thus, we regard the two measurements as basic, which are referred to as the frequency measurement and ratio measurement." ></td>
	<td class="line x" title="66:174	3.1 Document Frequency (DF) DF is the number of documents in which a term occurs." ></td>
	<td class="line x" title="67:174	It is defined as 1( ) m iiDF A== The terms with low or high document frequency are often referred to as rare or common terms, respectively." ></td>
	<td class="line x" title="68:174	It is easy to see that this FS method is based on the first basic measurement." ></td>
	<td class="line x" title="69:174	It assumes that the terms with higher document frequency are more informative for classification." ></td>
	<td class="line x" title="70:174	But sometimes this assumption does not make any sense, for example, the stop words (e.g., the, a, an) hold very high DF scores, but they seldom contribute to classification." ></td>
	<td class="line x" title="71:174	In general, this simple method performs very well in some topic-based classification tasks (Yang and Pedersen, 1997)." ></td>
	<td class="line x" title="72:174	3.2 Mutual Information (MI) The mutual information between term t  and class ic  is defined as ( | )( , ) log ( ) i i P t cI t c P t= And it is estimated as log ( )( )i all i i i i A NMI A C A B = + + Let us consider the following formula (using Bayes theorem) ( | ) ( | )( , ) log log ( ) ( ) i i i i P t c P c tI t c P t P c= = Therefore, ( , )= log ( | ) log ( )i i iI t c P c t P c And it is estimated as log log       log log 1      log(1 ) log / i i i i all i i i i all i i i all A NMI A B N A B N A N N A B N = + +=   =  +   From this formula, we can see that the MI score is based on the second basic measurement." ></td>
	<td class="line x" title="73:174	This method assumes that the term with higher category ratio is more effective for classification." ></td>
	<td class="line x" title="74:174	It is reported that this method is biased towards low frequency terms and the bias becomes extreme when ( )P t  is near zero." ></td>
	<td class="line x" title="75:174	It can be seen in the following formula (Yang and Pedersen, 1997) ( , ) log( ( | )) log( ( ))i iI t c P t c P t=  694 Therefore, this method might perform badly when common terms are informative for classification." ></td>
	<td class="line x" title="76:174	Taking into account mutual information of all categories, two types of MI score are commonly used: the maximum score ( )maxI t  and the average score ( )avgI t , i.e., 1( ) max { ( , )} m max i iI t I t c== , 1( ) ( ) ( , ) m avg i iiI t P c I t c==  . We choose the maximum score since it performs better than the average score (Yang and Pedersen, 1997)." ></td>
	<td class="line x" title="77:174	It is worth noting that the same choice is made for other methods, including CHI, BNS, and WLLR in this paper." ></td>
	<td class="line x" title="78:174	3.3 Information Gain (IG) IG measures the number of bits of information obtained for category prediction by recognizing the presence or absence of a term in a document (Yang and Pedersen, 1997)." ></td>
	<td class="line x" title="79:174	The function is 1 1 1 ( ) { ( ) log ( )}             +{ ( )[ ( | )log ( | )]            ( )[ ( | ) log ( | )]} m i ii m i ii m i ii G t P c P c P t P c t P c t P t P c t P c t = = = =  +     And it is estimated as 1 1 1 1 1 { log }     +( / )[ log ]   ( / )[ log ] m i i i all all m m i i i alli i i i i i m m i i i alli i i i i i N NIG N N A AA N A B A B C CC N C D C D = = = = = =  + + + + +      From the definition, we know that the information gain is the weighted average of the mutual information ( , )iI t c and ( , )iI t c  where the weights are the joint probabilities ( , )iP t c and ( , )iP t c : 1 1( ) ( , ) ( , ) ( , ) ( , ) m m i i i ii iG t P t c I t c P t c I t c= == +  Since ( , )iP t c is closely related to the document frequency iA  and the mutual information ( , )iI t c  is shown to be based on the second measurement, we can say that the IG score is influenced by the two basic measurements." ></td>
	<td class="line x" title="80:174	3.4 2  Statistic (CHI) The CHI measurement (Yang and Pedersen, 1997) is defined as 2( ) ( ) ( ) ( ) ( ) all i i i i i i i i i i i i N A D C BCHI A C B D A B C D  = +  +  +  + In order to get the relationship between CHI and the two measurements, the above formula is rewritten as follows 2[ ( ) ( ) ] ( ) ( ) [ ( )] all i all i i i i i i all i i i all i i N A N N B N A BCHI N N N A B N A B     =    +   +  For simplicity, we assume that there are two categories and the numbers of the training documents in the two categories are the same ( 2all iN N= )." ></td>
	<td class="line x" title="81:174	The CHI score then can be written as  2 2 2 ( ) ( ) [2 ( )] 2 ( / 1) 2( / 1) [ / ( / 1)] i i i i i i i i i i i i i i i i i i i N A BCHI A B N A B N A B NA B A B A B A = +   + = +    +  From the above formula, we see that the CHI score is related to both the frequency measurement iA  and ratio measurement /i iA B . Also, when keeping the same ratio value, the terms with higher document frequencies will yield higher CHI scores." ></td>
	<td class="line x" title="82:174	3.5 Bi-Normal Separation (BNS) BNS method is originally proposed by Forman (2003) and it is defined as 1 1( , ) ( ( | )) ( ( | ) i i iBNS t c F P t c F P t c  =  It is calculated using the following formula 1 1( ) ( )i i i all i A BBNS F F N N N  =   where ( )F x  is the cumulative probability function of standard normal distribution." ></td>
	<td class="line x" title="83:174	For simplicity, we assume that there are two categories and the numbers of the training documents in the two categories are the same, i.e., 2all iN N=  and we also assume that i iA B> . It should be noted that this assumption is only to allow easier analysis but will not be applied in our experiment implementation." ></td>
	<td class="line x" title="84:174	In addition, we only consider the case when / 0.5i iA N  . In fact, most terms take the document frequency iA which is less than half of iN . Under these conditions, the BNS score can be shown in Figure 1 where the area of the shadow part represents ( / / )i i i iA N B N  and the length of the projection to the x  axis represents the BNS score." ></td>
	<td class="line x" title="85:174	695 From Figure 1, we can easily draw the two following conclusions: 1) Given the same value of iA , the BNS score increases with the increase of i iA B . 2) Given the same value of i iA B , BNS score increase with the decrease of iA .  Figure 1." ></td>
	<td class="line x" title="86:174	View of BNS using the normal probability distribution." ></td>
	<td class="line x" title="87:174	Both the left and right graphs have shadowed areas of the same size." ></td>
	<td class="line x" title="88:174	And the value of i iA B  can be rewritten as the following 1(1 ) / i i i i i i i i i A BA B A A A A B  =  =   The above analysis gives the following conclusions regarding the relationship between BNS and the two basic measurements: 1) Given the same iA , the BNS score increases with the increase of /i iA B . 2) Given the same /i iA B , when iA  increases, i iA B  also increase." ></td>
	<td class="line x" title="89:174	It seems that the BNS score does not show a clear relationship with iA . In summary, the BNS FS method is biased towards the terms with the high category ratio but cannot be said to be sensitive to document frequency." ></td>
	<td class="line x" title="90:174	3.6 Weighted Log Likelihood Ratio (WLLR) WLLR method (Nigam et al., 2000) is defined as ( | )( , ) ( | )log ( | ) i i i i P t cWLLR t c P t c P t c= And it is estimated as ( )logi i all i i i i A A N NWLLR N B N  =  The formula shows WLLR is proportional to the frequency measurement and the logarithm of the ratio measurement." ></td>
	<td class="line x" title="91:174	Clearly, WLLR is biased towards the terms with both high category ratio and high document frequency and the frequency measurement seems to take a more important place than the ratio measurement." ></td>
	<td class="line x" title="92:174	3.7 Weighed Frequency and Odds (WFO) So far in this section, we have shown that the two basic measurements constitute the six FS methods." ></td>
	<td class="line x" title="93:174	The class prior probabilities, ( ), 1,2,,iP c i m= , are also related to the selection methods except for the two basic measurements." ></td>
	<td class="line x" title="94:174	Since they are often estimated according to the distribution of the documents in the training data and are identical for all the terms in a class, we ignore the discussion of their influence on the selection measurements." ></td>
	<td class="line x" title="95:174	In the experiment, we consider the case when training data have equal class prior probabilities." ></td>
	<td class="line x" title="96:174	When training data are unbalanced, we need to change the forms of the two basic measurements to /i iA N  and ( ) / ( )i all i i iA N N B N   . Because some methods are expressed in complex forms, it is difficult to explain their relationship with the two basic measurements, for example, which one prefers the category ratio most." ></td>
	<td class="line x" title="97:174	Instead, we will give the preference analysis in the experiment by analyzing the features in real applications." ></td>
	<td class="line x" title="98:174	But the following two conclusions are drawn without doubt according to the theoretical analysis given above." ></td>
	<td class="line x" title="99:174	1) Good features are features with high document frequency; 2) Good features are features with high category ratio." ></td>
	<td class="line x" title="100:174	These two conclusions are consistent with the original intuition." ></td>
	<td class="line x" title="101:174	However, using any single one does not provide competence in selecting the best set of features." ></td>
	<td class="line x" title="102:174	For example, stop words, such as a, the and as, have very high document frequency but are useless for the classification." ></td>
	<td class="line x" title="103:174	In real applications, we need to mix these two measurements to select good features." ></td>
	<td class="line x" title="104:174	Because of different distribution of features in different domains, the importance of each measurement may differ a lot in different applications." ></td>
	<td class="line x" title="105:174	Moreover, even in a given domain, when different numbers of features are to be selected, different combinations of the two measurements are required to provide the best performance." ></td>
	<td class="line x" title="106:174	Although a great number of FS methods is available, none of them can appropriately change the preference of the two measurements." ></td>
	<td class="line x" title="107:174	A better way is to tune the importance according to the application rather than to use a predetermined combination." ></td>
	<td class="line x" title="108:174	Therefore, we propose a new FS method called Weighed Frequency and Odds (WFO), which is defined as 696  ( | ) / ( | ) 1i iwhen P t c P t c > 1( | )( , ) ( | ) [log ] ( | ) i i i i P t cWFO t c P t c P t c  =                  ( , ) 0i else WFO t c = And it is estimated as 1( )( ) (log )i i all i i i i A A N NWFO N B N   =  where   is the parameter for tuning the weight between frequency and odds." ></td>
	<td class="line x" title="109:174	The value of  varies from 0 to 1." ></td>
	<td class="line x" title="110:174	By assigning different value to   we can adjust the preference of each measurement." ></td>
	<td class="line x" title="111:174	Specially, when 0 = , the algorithm prefers the category ratio that is equivalent to the MI method; when 1 = , the algorithm is similar to DF; when 0.5 = , the algorithm is exactly the WLLR method." ></td>
	<td class="line x" title="112:174	In real applications, a suitable parameter   needs to be learned by using training data." ></td>
	<td class="line x" title="113:174	4 Experimental Studies 4.1 Experimental Setup Data Set:  The experiments are carried out on both topic-based and sentiment text classification datasets." ></td>
	<td class="line x" title="114:174	In topic-based text classification, we use two popular data sets: one subset of Reuters-21578 referred to as R2 and the 20 Newsgroup dataset referred to as 20NG." ></td>
	<td class="line x" title="115:174	In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578." ></td>
	<td class="line pc" title="116:174	And 20NG is a collection of approximately 20,000 20-category documents 1 . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset2 (Pang and Lee, 2004) and one dataset from product reviews of domain DVD3 (Blitzer et al., 2007)." ></td>
	<td class="line x" title="117:174	Both of them are 2-category tasks and each consists of 2,000 reviews." ></td>
	<td class="line x" title="118:174	In our experiments, the document numbers of all data sets are (nearly) equally distributed cross all categories." ></td>
	<td class="line x" title="119:174	Classification Algorithm: Many classification algorithms are available for text classification, such as Nave Bayes, Maximum Entropy, k-NN, and SVM." ></td>
	<td class="line o" title="120:174	Among these methods, SVM is shown to perform better than other methods (Yang and Pedersen, 1997; Pang et al.,  1 http://people.csail.mit.edu/~jrennie/20Newsgroups/ 2 http://www.cs.cornell.edu/People/pabo/movie-review-data/ 3 http://www.seas.upenn.edu/~mdredze/datasets/sentiment/  2002)." ></td>
	<td class="line x" title="121:174	Hence we apply SVM algorithm with the help of the LIBSVM 4  tool." ></td>
	<td class="line x" title="122:174	Almost all parameters are set to their default values except the kernel function which is changed from a polynomial kernel function to a linear one because the linear one usually performs better for text classification tasks." ></td>
	<td class="line x" title="123:174	Experiment Implementation: In the experiments, each dataset is randomly and evenly split into two subsets: 90% documents as the training data and the remaining 10% as testing data." ></td>
	<td class="line x" title="124:174	The training data are used for training SVM classifiers, learning parameters in WFO method and selecting 'good' features for each FS method." ></td>
	<td class="line x" title="125:174	The features are single words with a bool weight (0 or 1), representing the presence or absence of a feature." ></td>
	<td class="line x" title="126:174	In addition to the principled FS methods, terms occurring in less than three documents ( 3DF  ) in the training set are removed." ></td>
	<td class="line x" title="127:174	4.2 Relationship between FS Methods and the Two Basic Measurements To help understand the relationship between FS methods and the two basic measurements, the empirical study is presented as follows." ></td>
	<td class="line x" title="128:174	Since the methods of DF and MI only utilize the document frequency and category information respectively, we use the DF scores and MI scores to represent the information of the two basic measurements." ></td>
	<td class="line x" title="129:174	Thus we would select the top-2% terms with each method and then investigate the distribution of their DF and MI scores." ></td>
	<td class="line x" title="130:174	First of all, for clear comparison, we normalize the scores coming from all the methods using Min-Max normalization method which is designed to map a score s  to 's  in the range [0, 1] by computing ' s Mins Max Min=  where Min  and Max  denote the minimum and maximum values respectively in all terms scores using one FS method." ></td>
	<td class="line x" title="131:174	Table 1 shows the mean values of all top-2% terms MI scores and DF scores of all the six FS methods in each domain." ></td>
	<td class="line x" title="132:174	From this table, we can apparently see the relationship between each method and the two basic measurements." ></td>
	<td class="line x" title="133:174	For instance, BNS most distinctly prefers the terms with high MI scores and low DF scores." ></td>
	<td class="line x" title="134:174	According to the degree of this preference, we  4 http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 697 FS Methods Domain 20NG R2 Movie DVD DF score MI score DF score MI score DF score MI score DF score MI score MI 0.004 0.870 0.047 0.959 0.003 0.888 0.004 0.881 BNS 0.005 0.864 0.117 0.922 0.008 0.881 0.006 0.880 CHI 0.015 0.814 0.211 0.748 0.092 0.572 0.055 0.676 IG 0.087 0.525 0.209 0.792 0.095 0.559 0.066 0.669 WLLR 0.026 0.764 0.206 0.805 0.168 0.414 0.127 0.481 DF 0.122 0.252 0.268 0.562 0.419 0.09 0.321 0.111  Table 1." ></td>
	<td class="line x" title="135:174	The mean values of all top-2% terms MI and DF scores using six FS methods in each domain  can rank these six methods as MI, BNS IG, CHI, WLLR DFf f , where x yf means method x  prefers the terms with higher MI scores (higher category information) and lower DF scores (lower document frequency) than method y. This empirical discovery is in agreement with the finding that WLLR is biased towards the high frequency terms and also with the finding that BNS is biased towards high category information (cf.Section 3 theoretical analysis)." ></td>
	<td class="line x" title="137:174	Also, we can find that CHI and IG share a similar preference of these two measurements in 2-category domains, i.e., R2, movie, and DVD." ></td>
	<td class="line x" title="138:174	This gives a good explanation that CHI and IG are two similar-performed methods for 2-category tasks, which have been found by Forman (2003) in their experimental studies." ></td>
	<td class="line x" title="139:174	According to the preference, we roughly cluster FS methods into three groups." ></td>
	<td class="line x" title="140:174	The first group includes the methods which dramatically prefer the category information, e.g., MI and BNS; the second one includes those which prefer both kinds of information, e.g., CHI, IG, and WLLR; and the third one includes those which strongly prefer frequency information, e.g., DF." ></td>
	<td class="line x" title="141:174	4.3 Performances of Different FS Methods It is worth noting that learning parameters in WFO is very important for its good performance." ></td>
	<td class="line x" title="142:174	We use 9-fold cross validation to help learning the parameter   so as to avoid over-fitting." ></td>
	<td class="line x" title="143:174	Specifically, we run nine times by using every 8 fold documents as a new training data set and the remaining one fold documents as a development data set." ></td>
	<td class="line x" title="144:174	In each running with one fixed feature number m, we get the best ,i m best  (i=1,, 9) value through varying ,i m  from 0 to 1 with the step of 0.1 to get the best performance in the development data set." ></td>
	<td class="line x" title="145:174	The average value m best  , i.e., 9 ,1( ) / 9m best i m besti  ==  is used for further testing." ></td>
	<td class="line x" title="146:174	Figure 2 shows the experimental results when using all FS methods with different selected feature numbers." ></td>
	<td class="line x" title="147:174	The red line with star tags represents the results of WFO." ></td>
	<td class="line x" title="148:174	At the first glance, in R2 domain, the differences of performances across all are very noisy when the feature number is larger than 1,000, which makes the comparison meaningless." ></td>
	<td class="line x" title="149:174	We think that this is because the performances themselves in this task are very high (nearly 98%) and the differences between two FS methods cannot be very large (less than one percent)." ></td>
	<td class="line x" title="150:174	Even this, WFO method do never get the worst performance and can also achieve the top performance in about half times, e.g., when feature numbers are 20, 50, 100, 500, 3000." ></td>
	<td class="line x" title="151:174	Let us pay more attention to the other three domains and discuss the results in the following two cases." ></td>
	<td class="line x" title="152:174	In the first case when the feature number is low (about less than 1,000), the FS methods in the second group including IG, CHI, WLLR, always perform better than those in the other two groups." ></td>
	<td class="line x" title="153:174	WFO can also perform well because its parameters m best   are successfully learned to be around 0.5, which makes it consistently belong to the second group." ></td>
	<td class="line x" title="154:174	Take 500 feature number for instance, the parameters 500 best   are 0.42, 0.50, and 0.34 in these three domains respectively." ></td>
	<td class="line x" title="155:174	In the second case when the feature number is large, among the six traditional methods, MI and BNS take the leads in the domains of 20NG and Movie while IG and CHI seem to be better and more stable than others in the domain of DVD." ></td>
	<td class="line x" title="156:174	As for WFO, its performances are excellent cross all these three domains and different feature numbers." ></td>
	<td class="line x" title="157:174	In each domain, it performs similarly as or better than the top methods due to its well-learned parameters." ></td>
	<td class="line x" title="158:174	For example, in 20NG, the parameters m best   are 0.28, 0.20, 0.08, and 0.01 when feature numbers are 10,000, 15,000, 20,000, and 30,000." ></td>
	<td class="line x" title="159:174	These values are close to 0 698 (WFO equals MI when 0 = ) while MI is the top one in this domain." ></td>
	<td class="line x" title="160:174	10 20 50 100 200 500 1000 2000 3000 4227 0.88 0.9 0.92 0.94 0.96 0.98 1 feature number ac cu rac y Topic R2   DF MI IG BNS CHI WLLR WFO  200 500 1000 2000 5000 10000 15000 20000 30000 320910.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 feature number ac cu rac y Topic 20NG   DF MI IG BNS CHI WLLR WFO  50 200 500 1000 4000 7000 10000 13000 151760.55 0.6 0.65 0.7 0.75 0.8 0.85 feature number ac cu rac y Sentiment Movie   DF MI IG BNS CHI WLLR WFO  20 50 100 500 1000 1500 2000 3000 4000 58240.5 0.55 0.6 0.65 0.7 0.75 0.8 feature number ac cu rac y Sentiment DVD   DF MI IG BNS CHI WLLR WFO  Figure 2." ></td>
	<td class="line x" title="161:174	The classification accuracies of the four domains using seven different FS methods while increasing the number of selected features." ></td>
	<td class="line x" title="162:174	From Figure 2, we can also find that FS does help sentiment classification." ></td>
	<td class="line x" title="163:174	At least, it can dramatically decrease the feature numbers without losing classification accuracies (see Movie domain, using only 500-4000 features is as good as using all 15176 features)." ></td>
	<td class="line x" title="164:174	5 Conclusion and Future Work In this paper, we propose a framework with two basic measurements and use it to theoretically analyze six FS methods." ></td>
	<td class="line x" title="165:174	The differences among them mainly lie in how they use these two measurements." ></td>
	<td class="line x" title="166:174	Moreover, with the guidance of the analysis, a novel method called WFO is proposed, which combine these two measurements with trained weights." ></td>
	<td class="line x" title="167:174	The experimental results show that our framework helps us to better understand and compare different FS methods." ></td>
	<td class="line x" title="168:174	Furthermore, the novel method WFO generated from the framework, can perform robustly across different domains and feature numbers." ></td>
	<td class="line x" title="169:174	In our study, we use four data sets to test our new method." ></td>
	<td class="line x" title="170:174	There are much more data sets on text categorization which can be used." ></td>
	<td class="line x" title="171:174	In additional, we only focus on using balanced samples in each category to do the experiments." ></td>
	<td class="line x" title="172:174	It is also necessary to compare the FS methods on some unbalanced data sets, which are common in real-life applications (Forman, 2003; Mladeni and Marko, 1999)." ></td>
	<td class="line x" title="173:174	These matters will be dealt with in the future work." ></td>
	<td class="line x" title="174:174	Acknowledgments The research work described in this paper has been partially supported by Start-up Grant for Newly Appointed Professors, No. 1-BBZM in the Hong Kong Polytechnic University." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1079
Mine the Easy, Classify the Hard: A Semi-Supervised Approach to Automatic Sentiment Classification
Dasgupta, Sajib;Ng, Vincent;"></td>
	<td class="line x" title="1:254	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 701709, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:254	c2009 ACL and AFNLP Mine the Easy, Classify the Hard: A Semi-Supervised Approach to Automatic Sentiment Classification Sajib Dasgupta and Vincent Ng Human Language Technology Research Institute University of Texas at Dallas Richardson, TX 75083-0688 {sajib,vince}@hlt.utdallas.edu Abstract Supervised polarity classification systems are typically domain-specific." ></td>
	<td class="line x" title="3:254	Building these systems involves the expensive process of annotating a large amount of data for each domain." ></td>
	<td class="line x" title="4:254	A potential solution to this corpus annotation bottleneck is to build unsupervised polarity classification systems." ></td>
	<td class="line x" title="5:254	However, unsupervised learning of polarity is difficult, owing in part to the prevalence of sentimentally ambiguous reviews, where reviewers discuss both the positive and negative aspects of a product." ></td>
	<td class="line x" title="6:254	To address this problem, we propose a semi-supervised approach to sentiment classification where we first mine the unambiguous reviews using spectral techniques and then exploit them to classify the ambiguous reviews via a novel combination of active learning, transductive learning, and ensemble learning." ></td>
	<td class="line x" title="7:254	1 Introduction Sentiment analysis has recently received a lot of attention in the Natural Language Processing (NLP) community." ></td>
	<td class="line x" title="8:254	Polarity classification, whose goal is to determine whether the sentiment expressed in a document is thumbs up or thumbs down, is arguably one of the most popular tasks in document-level sentiment analysis." ></td>
	<td class="line x" title="9:254	Unlike topic-based text classification, where a high accuracy can be achieved even for datasets with a large number of classes (e.g., 20 Newsgroups), polarity classification appears to be a more difficult task." ></td>
	<td class="line x" title="10:254	One reason topic-based text classification is easier than polarity classification is that topic clusters are typically well-separated from each other, resulting from the fact that word usage differs considerably between two topically-different documents." ></td>
	<td class="line x" title="11:254	On the other hand, many reviews are sentimentally ambiguous for a variety of reasons." ></td>
	<td class="line x" title="12:254	For instance, an author of a movie review may have negative opinions of the actors but at the same time talk enthusiastically about how much she enjoyed the plot." ></td>
	<td class="line x" title="13:254	Here, the review is ambiguous because she discussed both the positive and negative aspects of the movie, which is not uncommon in reviews." ></td>
	<td class="line x" title="14:254	As another example, a large portion of a movie review may be devoted exclusively to the plot, with the author only briefly expressing her sentiment at the end of the review." ></td>
	<td class="line x" title="15:254	In this case, the review is ambiguous because the objective material in the review, which bears no sentiment orientation, significantly outnumbers its subjective counterpart." ></td>
	<td class="line x" title="16:254	Realizing the challenges posed by ambiguous reviews, researchers have explored a number of techniques to improve supervised polarity classifiers." ></td>
	<td class="line oc" title="17:254	For instance, Pang and Lee (2004) train an independent subjectivity classifier to identify and remove objective sentences from a review prior to polarity classification." ></td>
	<td class="line x" title="18:254	Koppel and Schler (2006) use neutral reviews to help improve the classification of positive and negative reviews." ></td>
	<td class="line x" title="19:254	More recently, McDonald et al.(2007) have investigated a model for jointly performing sentenceand document-level sentiment analysis, allowing the relationship between the two tasks to be captured and exploited." ></td>
	<td class="line x" title="21:254	However, the increased sophistication of supervised polarity classifiers has also resulted in their increased dependence on annotated data." ></td>
	<td class="line x" title="22:254	For instance, Koppel and Schler needed to manually identify neutral reviews to train their polarity classifier, and McDonald et al.s joint model requires that each sentence in a review be labeled with polarity information." ></td>
	<td class="line x" title="23:254	Given the difficulties of supervised polarity classification, it is conceivable that unsupervised polarity classification is a very challenging task." ></td>
	<td class="line x" title="24:254	Nevertheless, a solution to unsupervised polarity classification is of practical significance." ></td>
	<td class="line x" title="25:254	One reason is that the vast majority of supervised polarity 701 classification systems are domain-specific." ></td>
	<td class="line x" title="26:254	Hence, when given a new domain, a large amount of annotated data from the domain typically needs to be collected in order to train a high-performance polarity classification system." ></td>
	<td class="line x" title="27:254	As Blitzer et al.(2007) point out, this data collection process can be prohibitively expensive, especially since product features can change over time." ></td>
	<td class="line x" title="29:254	Unfortunately, to our knowledge, unsupervised polarity classification is largely an under-investigated task in NLP." ></td>
	<td class="line x" title="30:254	Turneys (2002) work is perhaps one of the most notable examples of unsupervised polarity classification." ></td>
	<td class="line x" title="31:254	However, while his system learns the semantic orientation of phrases in a review in an unsupervised manner, such information is used to heuristically predict the polarity of a review." ></td>
	<td class="line x" title="32:254	At first glance, it may seem plausible to apply an unsupervised clustering algorithm such as kmeans to cluster the reviews according to their polarity." ></td>
	<td class="line x" title="33:254	However, there is reason to believe that such a clustering approach is doomed to fail: in the absence of annotated data, an unsupervised learner is unable to identify which features are relevant for polarity classification." ></td>
	<td class="line x" title="34:254	The situation is further complicated by the prevalence of ambiguous reviews, which may contain a large amount of irrelevant and/or contradictory information." ></td>
	<td class="line x" title="35:254	In light of the difficulties posed by ambiguous reviews, we differentiate between ambiguous and unambiguous reviews in our classification process by addressing the task of semi-supervised polarity classification via a mine the easy, classify the hard approach." ></td>
	<td class="line x" title="36:254	Specifically, we propose a novel system architecture where we first automatically identify and label the unambiguous (i.e., easy) reviews, then handle the ambiguous (i.e., hard) reviews using a discriminative learner to bootstrap from the automatically labeled unambiguous reviews and a small number of manually labeled reviews that are identified by an active learner." ></td>
	<td class="line x" title="37:254	It is worth noting that our system differs from existing work on unsupervised/active learning in two aspects." ></td>
	<td class="line x" title="38:254	First, while existing unsupervised approaches typically rely on clustering or learning via a generative model, our approach distinguishes between easy and hard instances and exploits the strengths of discriminative models to classify the hard instances." ></td>
	<td class="line x" title="39:254	Second, while existing active learners typically start with manually labeled seeds, our active learner relies only on seeds that are automatically extracted from the data." ></td>
	<td class="line x" title="40:254	Experimental results on five sentiment classification datasets demonstrate that our system can generate high-quality labeled data from unambiguous reviews, which, together with a small number of manually labeled reviews selected by the active learner, can be used to effectively classify ambiguous reviews in a discriminative fashion." ></td>
	<td class="line x" title="41:254	The rest of the paper is organized as follows." ></td>
	<td class="line x" title="42:254	Section 2 gives an overview of spectral clustering, which will facilitate the presentation of our approach to unsupervised sentiment classification in Section 3." ></td>
	<td class="line x" title="43:254	We evaluate our approach in Section 4 and present our conclusions in Section 5." ></td>
	<td class="line x" title="44:254	2 Spectral Clustering In this section, we give an overview of spectral clustering, which is at the core of our algorithm for identifying ambiguous reviews." ></td>
	<td class="line x" title="45:254	2.1 Motivation When given a clustering task, an important question to ask is: which clustering algorithm should be used?" ></td>
	<td class="line x" title="46:254	A popular choice is k-means." ></td>
	<td class="line x" title="47:254	Nevertheless, it is well-known that k-means has the major drawback of not being able to separate data points that are not linearly separable in the given feature space (e.g, see Dhillon et al.(2004))." ></td>
	<td class="line x" title="49:254	Spectral clustering algorithms were developed in response to this problem with k-means clustering." ></td>
	<td class="line x" title="50:254	The central idea behind spectral clustering is to (1) construct a low-dimensional space from the original (typically high-dimensional) space while retaining as much information about the original space as possible, and (2) cluster the data points in this lowdimensional space." ></td>
	<td class="line x" title="51:254	2.2 Algorithm Although there are several well-known spectral clustering algorithms in the literature (e.g., Weiss (1999), Meila and Shi (2001), Kannan et al.(2004)), we adopt the one proposed by Ng et al.(2002), as it is arguably the most widely used." ></td>
	<td class="line x" title="54:254	The algorithm takes as input a similarity matrix S created by applying a user-defined similarity function to each pair of data points." ></td>
	<td class="line x" title="55:254	Below are the main steps of the algorithm: 1." ></td>
	<td class="line x" title="56:254	Create the diagonal matrix G whose (i,i)th entry is the sum of the i-th row of S, and then construct the Laplacian matrix L = G1/2SG1/2." ></td>
	<td class="line x" title="57:254	2. Find the eigenvalues and eigenvectors of L. 702 3." ></td>
	<td class="line x" title="58:254	Create a new matrix from the m eigenvectors that correspond to the m largest eigenvalues.1 4." ></td>
	<td class="line x" title="59:254	Each data point is now rank-reduced to a point in the m-dimensional space." ></td>
	<td class="line x" title="60:254	Normalize each point to unit length (while retaining the sign of each value)." ></td>
	<td class="line x" title="61:254	5." ></td>
	<td class="line x" title="62:254	Cluster the resulting data points using kmeans." ></td>
	<td class="line x" title="63:254	In essence, each dimension in the reduced space is defined by exactly one eigenvector." ></td>
	<td class="line x" title="64:254	The reason why eigenvectors with large eigenvalues are retained is that they capture the largest variance in the data." ></td>
	<td class="line x" title="65:254	Therefore, each of them can be thought of as revealing an important dimension of the data." ></td>
	<td class="line x" title="66:254	3 Our Approach While spectral clustering addresses a major drawback of k-means clustering, it still cannot be expected to accurately partition the reviews due to the presence of ambiguous reviews." ></td>
	<td class="line x" title="67:254	Motivated by this observation, rather than attempting to cluster all the reviews at the same time, we handle them in different stages." ></td>
	<td class="line x" title="68:254	As mentioned in the introduction, we employ a mine the easy, classify the hard approach to polarity classification, where we (1) identify and classify the easy (i.e., unambiguous) reviews with the help of a spectral clustering algorithm; (2) manually label a small number of hard (i.e., ambiguous) reviews selected by an active learner; and (3) using the reviews labeled thus far, apply a transductive learner to label the remaining (ambiguous) reviews." ></td>
	<td class="line x" title="69:254	In this section, we discuss each of these steps in detail." ></td>
	<td class="line x" title="70:254	3.1 Identifying Unambiguous Reviews We begin by preprocessing the reviews to be classified." ></td>
	<td class="line x" title="71:254	Specifically, we tokenize and downcase each review and represent it as a vector of unigrams, using frequency as presence." ></td>
	<td class="line x" title="72:254	In addition, we remove from the vector punctuation, numbers, words of length one, and words that occur in a single review only." ></td>
	<td class="line x" title="73:254	Finally, following the common practice in the information retrieval community, we remove words with high document frequency, many of which are stopwords or domainspecific general-purpose words (e.g., movies in the movie domain)." ></td>
	<td class="line x" title="74:254	A preliminary examination of our evaluation datasets reveals that these words 1For brevity, we will refer to the eigenvector with the n-th largest eigenvalue simply as the n-th eigenvector." ></td>
	<td class="line x" title="75:254	typically comprise 12% of a vocabulary." ></td>
	<td class="line x" title="76:254	The decision of exactly how many terms to remove from each dataset is subjective: a large corpus typically requires more removals than a small corpus." ></td>
	<td class="line x" title="77:254	To be consistent, we simply sort the vocabulary by document frequency and remove the top 1.5%." ></td>
	<td class="line x" title="78:254	Recall that in this step we use spectral clustering to identify unambiguous reviews." ></td>
	<td class="line x" title="79:254	To make use of spectral clustering, we first create a similarity matrix, defining the similarity between two reviews as the dot product of their feature vectors, but following Ng et al.(2002), we set its diagonal entries to 0." ></td>
	<td class="line x" title="81:254	We then perform an eigen-decomposition of this matrix, as described in Section 2.2." ></td>
	<td class="line x" title="82:254	Finally, using the resulting eigenvectors, we partition the length-normalized reviews into two sets." ></td>
	<td class="line x" title="83:254	As Ng et al. point out, different authors still disagree on which eigenvectors to use, and how to derive clusters from them." ></td>
	<td class="line x" title="84:254	To create two clusters, the most common way is to use only the second eigenvector, as Shi and Malik (2000) proved that this eigenvector induces an intuitively ideal partition of the data  the partition induced by the minimum normalized cut of the similarity graph2, where the nodes are the data points and the edge weights are the pairwise similarity values of the points." ></td>
	<td class="line x" title="85:254	Clustering in a one-dimensional space is trivial: since we have a linearization of the points, all we need to do is to determine a threshold for partitioning the points." ></td>
	<td class="line x" title="86:254	A common approach is to set the threshold to zero." ></td>
	<td class="line x" title="87:254	In other words, all points whose value in the second eigenvector is positive are classified as positive, and the remaining points are classified as negative." ></td>
	<td class="line x" title="88:254	However, we found that the second eigenvector does not always induce a partition of the nodes that corresponds to the minimum normalized cut." ></td>
	<td class="line x" title="89:254	One possible reason is that Shi and Maliks proof assumes the use of a Laplacian matrix that is different from the one used by Ng et al. To address this problem, we use the first five eigenvectors: for each eigenvector, we (1) use each of its n elements as a threshold to independently generate n partitions, (2) compute the normalized cut value for each partition, and (3) find the minimum of the n cut values." ></td>
	<td class="line x" title="90:254	We then select the eigenvector that corresponds to the smallest of the five minimum cut values." ></td>
	<td class="line x" title="91:254	Next, we identify the ambiguous reviews from 2Using the normalized cut (as opposed to the usual cut) ensures that the size of the two clusters are relatively balanced, avoiding trivial cuts where one cluster is empty and the other is full." ></td>
	<td class="line x" title="92:254	See Shi and Malik (2000) for details." ></td>
	<td class="line x" title="93:254	703 the resulting partition." ></td>
	<td class="line x" title="94:254	To see how this is done, consider the example in Figure 1, where the goal is to produce two clusters from five data points." ></td>
	<td class="line x" title="95:254	parenleftBigg1 1 1 0 0 1 1 1 0 00 0 1 1 0 0 0 0 1 10 0 0 1 1 parenrightBigg parenleftBigg0.6983 0.7158 0.6983 0.7158 0.9869 0.1616 0.6224 0.7827 0.6224 0.7827 parenrightBigg Figure 1: Sample data and the top two eigenvectors of its Laplacian In the matrix on the left, each row is the feature vector generated for Di, the i-th data point." ></td>
	<td class="line x" title="96:254	By inspection, one can identify two clusters, {D1,D2} and {D4,D5}." ></td>
	<td class="line x" title="97:254	D3 is ambiguous, as it bears resemblance to the points in both clusters and therefore can be assigned to any of them." ></td>
	<td class="line x" title="98:254	In the matrix on the right, the two columns correspond to the top two eigenvectors obtained via an eigendecomposition of the Laplacian matrix formed from the five data points." ></td>
	<td class="line x" title="99:254	As we can see, the second eigenvector gives us a natural cluster assignment: all the points whose corresponding values in the second eigenvector are strongly positive will be in one cluster, and the strongly negative points will be in another cluster." ></td>
	<td class="line x" title="100:254	Being ambiguous, D3 is weakly negative and will be assigned to the negative cluster." ></td>
	<td class="line x" title="101:254	Before describing our algorithm for identifying ambiguous data points, we make two additional observations regarding D3." ></td>
	<td class="line x" title="102:254	First, if we removed D3, we could easily cluster the remaining (unambiguous) points, since the similarity graph becomes more disconnected as we remove more ambiguous data points." ></td>
	<td class="line x" title="103:254	The question then is: why is it important to produce a good clustering of the unambiguous points?" ></td>
	<td class="line x" title="104:254	Recall that the goal of this step is not only to identify the unambiguous reviews, but also to annotate them as POSITIVE or NEGATIVE, so that they can serve as seeds for semi-supervised learning in a later step." ></td>
	<td class="line x" title="105:254	If we have a good 2-way clustering of the seeds, we can simply annotate each cluster (by sampling a handful of its reviews) rather than each seed." ></td>
	<td class="line x" title="106:254	To reiterate, removing the ambiguous data points can help produce a good clustering of their unambiguous counterparts." ></td>
	<td class="line x" title="107:254	Second, as an ambiguous data point, D3 can in principle be assigned to any of the two clusters." ></td>
	<td class="line x" title="108:254	According to the second eigenvector, it should be assigned to the negative cluster; but if feature #4 were irrelevant, it should be assigned to the positive cluster." ></td>
	<td class="line x" title="109:254	In other words, the ability to determine the relevance of each feature is crucial to the accurate clustering of the ambiguous data points." ></td>
	<td class="line x" title="110:254	However, in the absence of labeled data, it is not easy to assess feature relevance." ></td>
	<td class="line x" title="111:254	Even if labeled data were present, the ambiguous points might be better handled by a discriminative learning system than a clustering algorithm, as discriminative learners are more sophisticated, and can handle ambiguous feature space more effectively." ></td>
	<td class="line x" title="112:254	Taking into account these two observations, we aim to (1) remove the ambiguous data points while clustering their unambiguous counterparts, and then (2) employ a discriminative learner to label the ambiguous points in a later step." ></td>
	<td class="line x" title="113:254	The question is: how can we identify the ambiguous data points?" ></td>
	<td class="line x" title="114:254	To do this, we exploit an important observation regarding eigendecomposition." ></td>
	<td class="line x" title="115:254	In the computation of eigenvalues, each data point factors out the orthogonal projections of each of the other data points with which they have an affinity." ></td>
	<td class="line x" title="116:254	Ambiguous data points receive the orthogonal projections from both the positive and negative data points, and hence they have near-zero values in the pivot eigenvectors." ></td>
	<td class="line x" title="117:254	Given this observation, our algorithm uses the eight steps below to remove the ambiguous points in an iterative fashion and produce a clustering of the unambiguous points." ></td>
	<td class="line x" title="118:254	1." ></td>
	<td class="line x" title="119:254	Create a similarity matrix S from the data points D. 2." ></td>
	<td class="line x" title="120:254	Form the Laplacian matrix L from S. 3." ></td>
	<td class="line x" title="121:254	Find the top five eigenvectors of L. 4." ></td>
	<td class="line x" title="122:254	Row-normalize the five eigenvectors." ></td>
	<td class="line x" title="123:254	5." ></td>
	<td class="line x" title="124:254	Pick the eigenvector e for which we get the minimum normalized cut." ></td>
	<td class="line x" title="125:254	6." ></td>
	<td class="line x" title="126:254	Sort D according to e and remove  points in the middle of D (i.e., the points indexed from |D| 2   2 +1 to |D| 2 +  2 ).7." ></td>
	<td class="line x" title="127:254	If |D| = , goto Step 8; else goto Step 1." ></td>
	<td class="line x" title="128:254	8. Run 2-means on e to cluster the points in D. This algorithm can be thought of as the opposite of self-training." ></td>
	<td class="line x" title="129:254	In self-training, we iteratively train a classifier on the data labeled so far, use it to classify the unlabeled instances, and augment the labeled data with the most confidently labeled instances." ></td>
	<td class="line x" title="130:254	In our algorithm, we start with an initial clustering of all of the data points, and then iteratively remove the  most ambiguous points from the dataset and cluster the remaining points." ></td>
	<td class="line x" title="131:254	Given this analogy, it should not be difficult to see the advantage of removing the data points in an iterative fashion (as opposed to removing them in a 704 single iteration): the clusters produced in a given iteration are supposed to be better than those in the previous iterations, as subsequent clusterings are generated from less ambiguous points." ></td>
	<td class="line x" title="132:254	In our experiments, we set  to 50 and  to 500.3 Finally, we label the two clusters." ></td>
	<td class="line x" title="133:254	To do this, we first randomly sample 10 reviews from each cluster and manually label each of them as POSITIVE or NEGATIVE." ></td>
	<td class="line x" title="134:254	Then, we label a cluster as POSITIVE if more than half of the 10 reviews from the cluster are POSITIVE; otherwise, it is labeled as NEGATIVE." ></td>
	<td class="line x" title="135:254	For each of our evaluation datasets, this labeling scheme always produces one POSITIVE cluster and one NEGATIVE cluster." ></td>
	<td class="line x" title="136:254	In the rest of the paper, we will refer to these 500 automatically labeled reviews as seeds." ></td>
	<td class="line x" title="137:254	A natural question is: can this algorithm produce high-quality seeds?" ></td>
	<td class="line x" title="138:254	To answer this question, we show in the middle column of Table 1 the labeling accuracy of the 500 reviews produced by our iterative algorithm for our five evaluation datasets (see Section 4.1 for details on these datasets)." ></td>
	<td class="line x" title="139:254	To better understand whether it is indeed beneficial to remove the ambiguous points in an iterative fashion, we also show the results of a version of this algorithm in which we remove all but the 500 least ambiguous points in just one iteration (see the rightmost column)." ></td>
	<td class="line x" title="140:254	As we can see, for three datasets (Movie, Kitchen, and Electronics), the accuracy is above 80%." ></td>
	<td class="line x" title="141:254	For the remaining two (Book and DVD), the accuracy is not particularly good." ></td>
	<td class="line x" title="142:254	One plausible reason is that the ambiguous reviews in Book and DVD are relatively tougher to identify." ></td>
	<td class="line x" title="143:254	Another reason can be attributed to the failure of the chosen eigenvector to capture the sentiment dimension." ></td>
	<td class="line x" title="144:254	Recall that each eigenvector captures an important dimension of the data, and if the eigenvector that corresponds to the minimum normalized cut (i.e., the eigenvector that we chose) does not reveal the sentiment dimension, the resulting clustering (and hence the seed accuracy) will be poor." ></td>
	<td class="line x" title="145:254	However, even with imperfectly labeled seeds, we will show in the next section how we exploit these seeds to learn a better classifier." ></td>
	<td class="line x" title="146:254	3.2 Incorporating Active Learning Spectral clustering allows us to focus on a small number of dimensions that are relevant as far as creating well-separated clusters is concerned, but 3Additional experiments indicate that the accuracy of our approach is not sensitive to small changes to these values." ></td>
	<td class="line x" title="147:254	Dataset Iterative Single Step Movie 89.3 86.5 Kitchen 87.9 87.1 Electronics 80.4 77.6 Book 68.5 70.3 DVD 66.3 65.4 Table 1: Seed accuracies on five datasets." ></td>
	<td class="line x" title="148:254	they are not necessarily relevant for creating polarity clusters." ></td>
	<td class="line x" title="149:254	In fact, owing to the absence of labeled data, unsupervised clustering algorithms are unable to distinguish between useful and irrelevant features for polarity classification." ></td>
	<td class="line x" title="150:254	Nevertheless, being able to distinguish between relevant and irrelevant information is important for polarity classification, as discussed before." ></td>
	<td class="line x" title="151:254	Now that we have a small, high-quality seed set, we can potentially make better use of the available features by training a discriminative classifier on the seed set and having it identify the relevant and irrelevant features for polarity classification." ></td>
	<td class="line x" title="152:254	Despite the high quality of the seed set, the resulting classifier may not perform well when applied to the remaining (unlabeled) points, as there is no reason to believe that a classifier trained solely on unambiguous reviews can achieve a high accuracy when classifying ambiguous reviews." ></td>
	<td class="line x" title="153:254	We hypothesize that a high accuracy can be achieved only if the classifier is trained on both ambiguous and unambiguous reviews." ></td>
	<td class="line x" title="154:254	As a result, we apply active learning (Cohn et al., 1994) to identify the ambiguous reviews." ></td>
	<td class="line x" title="155:254	Specifically, we train a discriminative classifier using the support vector machine (SVM) learning algorithm (Joachims, 1999) on the set of unambiguous reviews, and then apply the resulting classifier to all the reviews in the training folds4 that are not seeds." ></td>
	<td class="line x" title="156:254	Since this classifier is trained solely on the unambiguous reviews, it is reasonable to assume that the reviews whose labels the classifier is most uncertain about (and therefore are most informative to the classifier) are those that are ambiguous." ></td>
	<td class="line x" title="157:254	Following previous work on active learning for SVMs (e.g., Campbell et al.(2000), Schohn and Cohn (2000), Tong and Koller (2002)), we define the uncertainty of a data point as its distance from the separating hyperplane." ></td>
	<td class="line x" title="159:254	In other words, 4Following Dredze and Crammer (2008), we perform cross-validation experiments on the 2000 labeled reviews in each evaluation dataset, choosing the active learning points from the training folds." ></td>
	<td class="line x" title="160:254	Note that the seeds obtained in the previous step were also acquired using the training folds only." ></td>
	<td class="line x" title="161:254	705 points that are closer to the hyperplane are more uncertain than those that are farther away." ></td>
	<td class="line x" title="162:254	We perform active learning for five iterations." ></td>
	<td class="line x" title="163:254	In each iteration, we select the 10 most uncertain points from each side of the hyperplane for human annotation, and then re-train a classifier on all of the points annotated so far." ></td>
	<td class="line x" title="164:254	This yields a total of 100 manually labeled reviews." ></td>
	<td class="line x" title="165:254	3.3 Applying Transductive Learning Given that we now have a labeled set (composed of 100 manually labeled points selected by active learning and 500 unambiguous points) as well as a larger set of points that are yet to be labeled (i.e., the remaining unlabeled points in the training folds and those in the test fold), we aim to train a better classifier by using a weakly supervised learner to learn from both the labeled and unlabeled data." ></td>
	<td class="line x" title="166:254	As our weakly supervised learner, we employ a transductive SVM." ></td>
	<td class="line x" title="167:254	To begin with, note that the automatically acquired 500 unambiguous data points are not perfectly labeled (see Section 3.1)." ></td>
	<td class="line x" title="168:254	Since these unambiguous points significantly outnumber the manually labeled points, they could undesirably dominate the acquisition of the hyperplane and diminish the benefits that we could have obtained from the more informative and perfectly labeled active learning points otherwise." ></td>
	<td class="line x" title="169:254	We desire a system that can use the active learning points effectively and at the same time is noise-tolerant to the imperfectly labeled unambiguous data points." ></td>
	<td class="line x" title="170:254	Hence, instead of training just one SVM classifier, we aim to reduce classification errors by training an ensemble of five classifiers, each of which uses all 100 manually labeled reviews and a different subset of the 500 automatically labeled reviews." ></td>
	<td class="line x" title="171:254	Specifically, we partition the 500 automatically labeled reviews into five equal-sized sets as follows." ></td>
	<td class="line x" title="172:254	First, we sort the 500 reviews in ascending order of their corresponding values in the eigenvector selected in the last iteration of our algorithm for removing ambiguous points (see Section 3.1)." ></td>
	<td class="line x" title="173:254	We then put point i into set Li mod 5." ></td>
	<td class="line x" title="174:254	This ensures that each set consists of not only an equal number of positive and negative points, but also a mix of very confidently labeled points and comparatively less confidently labeled points." ></td>
	<td class="line x" title="175:254	Each classifier Ci will then be trained transductively, using the 100 manually labeled points and the points in Li as labeled data, and the remaining points (including all points in Lj, where i negationslash= j) as unlabeled data." ></td>
	<td class="line x" title="176:254	After training the ensemble, we classify each unlabeled point as follows: we sum the (signed) confidence values assigned to it by the five ensemble classifiers, labeling it as POSITIVE if the sum is greater than zero (and NEGATIVE otherwise)." ></td>
	<td class="line x" title="177:254	Since the points in the test fold are included in the unlabeled data, they are all classified in this step." ></td>
	<td class="line x" title="178:254	4 Evaluation 4.1 Experimental Setup For evaluation, we use five sentiment classification datasets, including the widely-used movie review dataset [MOV] (Pang et al., 2002) as well as four datasets that contain reviews of four different types of product from Amazon [books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT)] (Blitzer et al., 2007)." ></td>
	<td class="line x" title="179:254	Each dataset has 2000 labeled reviews (1000 positives and 1000 negatives)." ></td>
	<td class="line x" title="180:254	We divide the 2000 reviews into 10 equal-sized folds for cross-validation purposes, maintaining balanced class distributions in each fold." ></td>
	<td class="line x" title="181:254	It is important to note that while the test fold is accessible to the transductive learner (Step 3), only the reviews in training folds (but not their labels) are used for the acquisition of seeds (Step 1) and the selection of active learning points (Step 2)." ></td>
	<td class="line x" title="182:254	We report averaged 10-fold cross-validation results in terms of accuracy." ></td>
	<td class="line x" title="183:254	Following Kamvar et al.(2003), we also evaluate the clusters produced by our approach against the gold-standard clusters using Adjusted Rand Index (ARI)." ></td>
	<td class="line x" title="185:254	ARI ranges from 1 to 1; better clusterings have higher ARI values." ></td>
	<td class="line x" title="186:254	4.2 Baseline Systems Recall that our approach uses 100 hand-labeled reviews chosen by active learning." ></td>
	<td class="line x" title="187:254	To ensure a fair comparison, each of our three baselines has access to 100 labeled points chosen from the training folds." ></td>
	<td class="line x" title="188:254	Owing to the randomness involved in the choice of labeled data, all baseline results are averaged over ten independent runs for each fold." ></td>
	<td class="line x" title="189:254	Semi-supervised spectral clustering." ></td>
	<td class="line x" title="190:254	We implemented Kamvar et al.s (2003) semi-supervised spectral clustering algorithm, which incorporates labeled data into the clustering framework in the form of must-link and cannot-link constraints." ></td>
	<td class="line x" title="191:254	Instead of computing the similarity between each pair of points, the algorithm computes the similarity between a point and its k most similar points only." ></td>
	<td class="line x" title="192:254	Since its performance is highly sensitive to 706 Accuracy Adjusted Rand Index System Variation MOV KIT ELE BOO DVD MOV KIT ELE BOO DVD 1 Semi-supervised spectral learning 67.3 63.7 57.7 55.8 56.2 0.12 0.08 0.01 0.02 0.02 2 Transductive SVM 68.7 65.5 62.9 58.7 57.3 0.14 0.09 0.07 0.03 0.02 3 Active learning 68.9 68.1 63.3 58.6 58.0 0.14 0.14 0.08 0.03 0.03 4 Our approach (after 1st step) 69.8 70.8 65.7 58.6 55.8 0.15 0.17 0.10 0.03 0.01 5 Our approach (after 2nd step) 73.5 73.0 69.9 60.6 59.8 0.22 0.21 0.16 0.04 0.04 6 Our approach (after 3rd step) 76.2 74.1 70.6 62.1 62.7 0.27 0.23 0.17 0.06 0.06 Table 2: Results in terms of accuracy and Adjusted Rand Index for the five datasets." ></td>
	<td class="line x" title="193:254	k, we tested values of 10, 15, , 50 for k and reported in row 1 of Table 2 the best results." ></td>
	<td class="line x" title="194:254	As we can see, accuracy ranges from 56.2% to 67.3%, whereas ARI ranges from 0.02 to 0.12." ></td>
	<td class="line x" title="195:254	Transductive SVM." ></td>
	<td class="line x" title="196:254	We employ as our second baseline a transductive SVM5 trained using 100 points randomly sampled from the training folds as labeled data and the remaining 1900 points as unlabeled data." ></td>
	<td class="line x" title="197:254	Results of this baseline are shown in row 2 of Table 3." ></td>
	<td class="line x" title="198:254	As we can see, accuracy ranges from 57.3% to 68.7% and ARI ranges from 0.02 to 0.14, which are significantly better than those of semi-supervised spectral learning." ></td>
	<td class="line x" title="199:254	Active learning." ></td>
	<td class="line x" title="200:254	Our last baseline implements the active learning procedure as described in Tong and Koller (2002)." ></td>
	<td class="line x" title="201:254	Specifically, we begin by training an inductive SVM on one labeled example from each class, iteratively labeling the most uncertain unlabeled point on each side of the hyperplane and re-training the SVM until 100 points are labeled." ></td>
	<td class="line x" title="202:254	Finally, we train a transductive SVM on the 100 labeled points and the remaining 1900 unlabeled points, obtaining the results in row 3 of Table 1." ></td>
	<td class="line x" title="203:254	As we can see, accuracy ranges from 58% to 68.9%, whereas ARI ranges from 0.03 to 0.14." ></td>
	<td class="line x" title="204:254	Active learning is the best of the three baselines, presumably because it has the ability to choose the labeled data more intelligently than the other two." ></td>
	<td class="line x" title="205:254	4.3 Our Approach Results of our approach are shown in rows 46 of Table 2." ></td>
	<td class="line x" title="206:254	Specifically, rows 4 and 5 show the results of the SVM classifier when it is trained on the labeled data obtained after the first step (unsupervised extraction of unambiguous reviews) and the second step (active learning), respectively." ></td>
	<td class="line x" title="207:254	After the first step, our approach can already achieve 5All the SVM classifiers in this paper are trained using the SVMlight package (Joachims, 1999)." ></td>
	<td class="line x" title="208:254	All SVM-related learning parameters are set to their default values, except in transductive learning, where we set p (the fraction of unlabeled examples to be classified as positive) to 0.5 so that the system does not have any bias towards any class." ></td>
	<td class="line x" title="209:254	comparable results to the best baseline." ></td>
	<td class="line x" title="210:254	Performance increases substantially after the second step, indicating the benefits of active learning." ></td>
	<td class="line x" title="211:254	Row 6 shows the results of transductive learning with ensemble." ></td>
	<td class="line x" title="212:254	Comparing rows 5 and 6, we see that performance rises by 0.7%-2.9% for all five datasets after ensembled transduction." ></td>
	<td class="line x" title="213:254	This could be attributed to (1) the unlabeled data, which may have provided the transductive learner with useful information that are not accessible to the other learners, and (2) the ensemble, which is more noise-tolerant to the imperfect seeds." ></td>
	<td class="line x" title="214:254	4.4 Additional Experiments To gain insight into how the design decisions we made in our approach impact performance, we conducted the following additional experiments." ></td>
	<td class="line x" title="215:254	Importance of seeds." ></td>
	<td class="line x" title="216:254	Table 1 showed that for all but one dataset, the seeds obtained through multiple iterations are more accurate than those obtained in a single iteration." ></td>
	<td class="line x" title="217:254	To envisage the importance of seeds, we conducted an experiment where we repeated our approach using the seeds learned in a single iteration." ></td>
	<td class="line x" title="218:254	Results are shown in the first row of Table 3." ></td>
	<td class="line x" title="219:254	In comparison to row 6 of Table 2, we can see that results are indeed better when we bootstrap from higher-quality seeds." ></td>
	<td class="line x" title="220:254	To further understand the role of seeds, we experimented with a version of our approach that bootstraps from no seeds." ></td>
	<td class="line x" title="221:254	Specifically, we used the 500 seeds to guide the selection of active learning points, but trained a transductive SVM using only the active learning points as labeled data (and the rest as unlabeled data)." ></td>
	<td class="line x" title="222:254	As can be seen in row 2 of Table 3, the results are poor, suggesting that our approach yields better performance than the baselines not only because of the way the active learning points were chosen, but also because of contributions from the imperfectly labeled seeds." ></td>
	<td class="line x" title="223:254	We also experimented with training a transductive SVM using only the 100 least ambiguous seeds (i.e., the points with the largest unsigned 707 Accuracy Adjusted Rand Index System Variation MOV KIT ELE BOO DVD MOV KIT ELE BOO DVD 1 Single-step cluster purification 74.9 72.7 70.1 66.9 60.7 0.25 0.21 0.16 0.11 0.05 2 Using no seeds 58.3 55.6 59.7 54.0 56.1 0.04 0.04 0.02 0.01 0.01 3 Using the least ambiguous seeds 74.6 69.7 69.1 60.9 63.3 0.24 0.16 0.14 0.05 0.07 4 No Ensemble 74.1 72.7 68.8 61.5 59.9 0.23 0.21 0.14 0.05 0.04 5 Passive learning 74.1 72.4 68.0 63.7 58.6 0.23 0.20 0.13 0.07 0.03 6 Using 500 active learning points 82.5 78.4 77.5 73.5 73.4 0.42 0.32 0.30 0.22 0.22 7 Fully supervised results 86.1 81.7 79.3 77.6 80.6 0.53 0.41 0.34 0.30 0.38 Table 3: Additional results in terms of accuracy and Adjusted Rand Index for the five datasets." ></td>
	<td class="line x" title="224:254	second eigenvector values) in combination with the active learning points as labeled data (and the rest as unlabeled data)." ></td>
	<td class="line x" title="225:254	Note that the accuracy of these 100 least ambiguous seeds is 45% higher than that of the 500 least ambiguous seeds shown in Table 1." ></td>
	<td class="line x" title="226:254	Results are shown in row 3 of Table 3." ></td>
	<td class="line x" title="227:254	As we can see, using only 100 seeds turns out to be less beneficial than using all of them via an ensemble." ></td>
	<td class="line x" title="228:254	One reason is that since these 100 seeds are the most unambiguous, they may also be the least informative as far as learning is concerned." ></td>
	<td class="line x" title="229:254	Remember that SVM uses only the support vectors to acquire the hyperplane, and since an unambiguous seed is likely to be far away from the hyperplane, it is less likely to be a support vector." ></td>
	<td class="line x" title="230:254	Role of ensemble learning To get a better idea of the role of the ensemble in the transductive learning step, we used all 500 seeds in combination with the 100 active learning points to train a single transductive SVM." ></td>
	<td class="line x" title="231:254	Results of this experiment (shown in row 4 of Table 3) are worse than those in row 6 of Table 2, meaning that the ensemble has contributed positively to performance." ></td>
	<td class="line x" title="232:254	This should not be surprising: as noted before, since the seeds are not perfectly labeled, using all of them without an ensemble might overwhelm the more informative active learning points." ></td>
	<td class="line x" title="233:254	Passive learning." ></td>
	<td class="line x" title="234:254	To better understand the role of active learning in our approach, we replaced it with passive learning, where we randomly picked 100 data points from the training folds and used them as labeled data." ></td>
	<td class="line x" title="235:254	Results, shown in row 5 of Table 3, are averaged over ten independent runs for each fold." ></td>
	<td class="line x" title="236:254	In comparison to row 6 of Table 2, we see that employing points chosen by an active learner yields significantly better results than employing randomly chosen points, which suggests that the way the points are chosen is important." ></td>
	<td class="line x" title="237:254	Using more active learning points." ></td>
	<td class="line x" title="238:254	An interesting question is: how much improvement can we obtain if we employ more active learning points?" ></td>
	<td class="line x" title="239:254	In row 6 of Table 3, we show the results when the experiment in row 6 of Table 2 was repeated using 500 active learning points." ></td>
	<td class="line x" title="240:254	Perhaps not surprisingly, the 400 additional labeled points yield a 4 11% increase in accuracy." ></td>
	<td class="line x" title="241:254	For further comparison, we trained a fully supervised SVM classifier using all of the training data." ></td>
	<td class="line x" title="242:254	Results are shown in row 7 of Table 3." ></td>
	<td class="line x" title="243:254	As we can see, employing only 500 active learning points enables us to almost reach fully-supervised performance for three datasets." ></td>
	<td class="line x" title="244:254	5 Conclusions We have proposed a novel semi-supervised approach to polarity classification." ></td>
	<td class="line x" title="245:254	Our key idea is to distinguish between unambiguous, easy-tomine reviews and ambiguous, hard-to-classify reviews." ></td>
	<td class="line x" title="246:254	Specifically, given a set of reviews, we applied (1) an unsupervised algorithm to identify and classify those that are unambiguous, (2) an active learner that is trained solely on automatically labeled unambiguous reviews to identify a small number of prototypical ambiguous reviews for manual labeling, and (3) an ensembled transductive learner to train a sophisticated classifier on the reviews labeled so far to handle the ambiguous reviews." ></td>
	<td class="line x" title="247:254	Experimental results on five sentiment datasets demonstrate that our mine the easy, classify the hard approach, which only requires manual labeling of a small number of ambiguous reviews, can be employed to train a highperformance polarity classification system." ></td>
	<td class="line x" title="248:254	We plan to extend our approach by exploring two of its appealing features." ></td>
	<td class="line x" title="249:254	First, none of the steps in our approach is designed specifically for sentiment classification." ></td>
	<td class="line x" title="250:254	This makes it applicable to other text classification tasks." ></td>
	<td class="line x" title="251:254	Second, our approach is easily extensible." ></td>
	<td class="line x" title="252:254	Since the semisupervised learner is discriminative, our approach can adopt a richer representation that makes use of more sophisticated features such as bigrams or manually labeled sentiment-oriented words." ></td>
	<td class="line x" title="253:254	708 Acknowledgments We thank the three anonymous reviewers for their invaluable comments on an earlier draft of the paper." ></td>
	<td class="line x" title="254:254	This work was supported in part by NSF Grant IIS-0812261." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-1606
Investigation in Statistical Language-Independent Approaches for Opinion Detection in English, Chinese and Japanese
Zubaryeva, Olena;Savoy, Jacques;"></td>
	<td class="line x" title="1:178	Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 3845, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:178	c 2009 Association for Computational Linguistics Investigation in Statistical Language-Independent Approaches for Opinion Detection in English, Chinese and Japanese Olena Zubaryeva Jacques Savoy Institute of Informatics Institute of Informatics University of Neuchtel University of Neuchtel Emile-Argand, 11, 2009 Switzerland Emile-Argand, 11, 2009 Switzerland olena.zubaryeva@unine.ch jacques.savoy@unine.ch Abstract In this paper we present a new statistical approach to opinion detection and its evaluation on the English, Chinese and Japanese corpora." ></td>
	<td class="line x" title="3:178	Besides, the proposed method is compared  with  three  baselines,  namely Nave Bayes classifier, a language model and an approach based on significant collocations." ></td>
	<td class="line x" title="4:178	These models being language independent are improved with the use of language-dependent technique on the example of the English corpus." ></td>
	<td class="line x" title="5:178	We show that our method almost always gives better performance compared to the considered baselines." ></td>
	<td class="line x" title="6:178	1 Introduction The task of opinion mining has received attention from the research community and industry lately." ></td>
	<td class="line x" title="7:178	The main reasons for extensive research in the area are the growth of user needs and companies desire to analyze and exploit the user-generated content on the Web in the form of blogs and discussions." ></td>
	<td class="line x" title="8:178	Thus, users want to search for opinions on various topics from products that they want to buy to opinions about events and well-known persons." ></td>
	<td class="line x" title="9:178	A lot of businesses are interested in how their services are perceived by their customers." ></td>
	<td class="line x" title="10:178	Therefore, the detection of subjectivity in the searched information may add the additional value to the interpretation of the results and their relevancy to the searched topic." ></td>
	<td class="line x" title="11:178	The growth of user activity on the Web gives substantial amounts of data for these purposes." ></td>
	<td class="line x" title="12:178	In the context of globalization the possibility to provide search of opinionated information in different natural languages might be of great interest to organizations and communities around the world." ></td>
	<td class="line x" title="13:178	Our goal is to design a fully automatic system capable of working in a language-independent manner." ></td>
	<td class="line x" title="14:178	In order to compare our approach on different languages we chose English, traditional Chinese and Japanese corpora." ></td>
	<td class="line x" title="15:178	As a further possibility to improve the effectiveness of the language independent methods we also consider the additional application of language dependent techniques specific to the particular natural language." ></td>
	<td class="line x" title="16:178	The related work in opinion detection is presented in Section 2." ></td>
	<td class="line x" title="17:178	We describe our approach in detail with the three other baselines in Section 3." ></td>
	<td class="line x" title="18:178	The fourth section describes language specific approach used for the English corpus." ></td>
	<td class="line x" title="19:178	In Section 5 we present the evaluation of the three models using the NTCIR-6 and NTCIR-7 MOAT English, Chinese and Japanese test collections (Seki  et  al., 2008)." ></td>
	<td class="line x" title="20:178	The main findings of our study and future research possibilities are discussed in the last sections." ></td>
	<td class="line x" title="21:178	2 Related Work The focus of our work is to propose a general approach that can be easily deployed for different natural languages." ></td>
	<td class="line x" title="22:178	This task of opinion detection is important  in  many  areas  of  NLP  such  as question/answering, information retrieval, docu38 ment classification and summarization, and information filtering." ></td>
	<td class="line x" title="23:178	There are numerous challenges when trying to solve the task of opinion detection." ></td>
	<td class="line x" title="24:178	Some of them include the fact that the distinction between opinionated and factual could be denoted by a single word in the underlying text (e.g., The iPhone price is $600. vs. The iPhone price is high.)." ></td>
	<td class="line x" title="25:178	Most importantly evaluating whether or not a given sentence conveys an opinion could be questionable when judged by different people." ></td>
	<td class="line x" title="26:178	Further, the opinion classification can be done on different levels, from documents to clauses in the sentence." ></td>
	<td class="line x" title="27:178	We consider the opinion detection task on a sentence level." ></td>
	<td class="line x" title="28:178	After retrieving the relevant sentences using any IR system we automatically classify a sentence according to two classes: opinionated and not opinionated (factual)." ></td>
	<td class="line x" title="29:178	When viewing an opinion-finding task as a classification task, it is usually considered as a supervised learning problem where a statistical model performs a learning task by analyzing a pool of labeled sentences." ></td>
	<td class="line x" title="30:178	Two questions must therefore be solved, namely defining an effective classification algorithm and determining pertinent features that might effectively discriminate between opinionated and factual sentences." ></td>
	<td class="line x" title="31:178	From this perspective, during the last TREC opinion-finding task (Macdonald et  al., 2008) and the last NTCIR-7 workshop (Seki et al., 2008), a series of suggestions surfaced." ></td>
	<td class="line x" title="32:178	As the language-dependent approach various teams proposed using Levin defined verb categories (namely, characterize, declare, conjecture, admire, judge, assess, say, complain, advise) and their features (a verb corresponding to a given category occurring in the analyzed information item) that may be pertinent as a classification feature (Bloom  et  al., 2007)." ></td>
	<td class="line x" title="33:178	However, words such as these cannot always work correctly as clues, for example with the word said in the two sentences There were crowds and crowds of people at the concert, said Ann and There were more than 10,000 people at the concert, said Ann.  Both sentences contain the clue word said but only the first one contains an opinion on the target product." ></td>
	<td class="line x" title="34:178	Turney (2002) suggested comparing the frequency of phrase co-occurrences with words predetermined by the sentiment lexicon." ></td>
	<td class="line x" title="35:178	Specific to the opinion detection in Chinese language Ku et al.(2006) propose a dictionary-based approach for extraction and summarization." ></td>
	<td class="line x" title="37:178	For the Japanese language in the last NTCIR-6 and NTCIR-7 workshops the opinion finding methods included the use of supervised machine learning approaches with specific selection of certain parts-of-speech (POS) and sentence parts in the form of n-gram features to improve performance." ></td>
	<td class="line x" title="38:178	There has been a trend in applying language models for opinion detection task (Lavrenko, Croft, 2001)." ></td>
	<td class="line oc" title="39:178	Pang & Lee (2004) propose the use of language models for sentiment analysis task and subjectivity extraction." ></td>
	<td class="line x" title="40:178	Usually, language models are trained on the labeled data and as an output they give probabilities of classified tokens belonging to the class." ></td>
	<td class="line x" title="41:178	Eguchi & Lavrenko (2006) propose the use of probabilistic language models for ranking the results not only by sentiment but also by the topic relevancy." ></td>
	<td class="line x" title="42:178	As an alternative other teams during the last TREC and NTCIR evaluation campaigns have suggested variations of Nave Bayes classifier, language models and SVM, along with the use of such heuristics as word order, punctuation, sentence length, etc. We might also mention OpinionFinder (Wilson et al., 2005), a more complex system that performs subjectivity analyses to identify opinions as well as sentiments and other private states (speculations, dreams, etc.)." ></td>
	<td class="line x" title="43:178	This system is based on various classical computational linguistics components (tokenization, part-of-speech (POS) tagging (Toutanova & Manning, 2000) as well as classification tools." ></td>
	<td class="line x" title="44:178	For example, a Nave Bayes classifier (Witten & Frank, 2005) is used to distinguish between subjective and objective sentences." ></td>
	<td class="line x" title="45:178	A rule-based system is included to identify both speech events (said, according to) and direct subjective expressions (is happy, fears) within a given sentence." ></td>
	<td class="line x" title="46:178	Of course such learning system requires both a training set and a deeper knowledge of a given natural language (morphological components, syntactic analyses, semantic thesaurus)." ></td>
	<td class="line x" title="47:178	The lack of enough training data for the learning-based systems is clearly a drawback." ></td>
	<td class="line x" title="48:178	Moreover, it is difficult to objectively establish when a complex learning system has enough training data (and to objectively measure the amount of training data needed in a complex ML model)." ></td>
	<td class="line x" title="49:178	39 3 Language Independent Approaches In this section we propose our statistical approach for opinion detection as well as the description of the Nave Bayes and language model (LM) baselines." ></td>
	<td class="line x" title="50:178	3.1  Logistic Model Our system is based on two components: the extraction and weighting of useful features (limited to isolated words in this study) to allow an effective classification, and a classification scheme." ></td>
	<td class="line x" title="51:178	First, we present the feature extraction approach in the Section 3.1.1." ></td>
	<td class="line x" title="52:178	Next, we discuss our classification model." ></td>
	<td class="line x" title="53:178	Sections 3.2 and 3.3 describe the chosen baselines." ></td>
	<td class="line x" title="54:178	3.1.1 Features Extraction  In order to determine the features that can help distinguishing between factual and opinionated documents, we have selected the tokens." ></td>
	<td class="line x" title="55:178	As shown by Kilgarriff (2001), the selection of words (or in general features) in an effort to characterize a particular category is a difficult task." ></td>
	<td class="line x" title="56:178	The goal is therefore to design a method capable of selecting terms that clearly belong to one of the classes." ></td>
	<td class="line x" title="57:178	The approaches that use words and their frequencies or distributions are usually based on a contingency table (see Table 1)." ></td>
	<td class="line x" title="58:178	S C a b a+b not  c d c+d a+c b+d n=a+b+c +d Table 1." ></td>
	<td class="line x" title="59:178	Example of a contingency table." ></td>
	<td class="line x" title="60:178	In this table, the letter a represents the number of occurrences (tokens) of the word  in the document set S (corresponding to a subset of the larger corpus C in the current study)." ></td>
	<td class="line x" title="61:178	The letter b denotes the number of tokens of the same word  in the rest of the corpus (denoted C-) while a+b is the total number of occurrences in the entire corpus (denoted C with C=C-S)." ></td>
	<td class="line x" title="62:178	Similarly, a+c indicates the total number of tokens in S.  The entire corpus C corresponds to the union of the subset S and Cthat contains n tokens (n = a+b+c+d)." ></td>
	<td class="line x" title="63:178	Based on the MLE (Maximum Likelihood Estimation) principle the values shown in a contingency table could be used to estimate various probabilities." ></td>
	<td class="line x" title="64:178	For example we might calculate the probability of the occurrence of the word  in the entire corpus C as Pr() = (a+b)/n or the probability of finding in C a word belonging to the set S as Pr(S) = (a+c)/n. Now to define the discrimination power a term , we suggest defining a weight attached to it according to Muller's method (Muller, 1992)." ></td>
	<td class="line x" title="65:178	We assume that the distribution of the number of tokens of the word  follows a binomial distribution with the parameters p and n'." ></td>
	<td class="line x" title="66:178	The parameter p represented the probability of drawing a word  also denoted in the corpus C (or Pr()) and could be estimated as (a+b)/n.  If we repeat this drawing n' = a+c times, we will have an estimate of the number of word  included in the subset S by Pr().n'." ></td>
	<td class="line x" title="67:178	On the other hand, Table 1 gives also the number of observations of the word  in S, and this value is denoted by a. A large difference between a and the product Pr().n' is clearly an indication that the presence of a occurrences of the term  is not due by chance but corresponds to an intrinsic characteristic of the subset S compared to the subset C-." ></td>
	<td class="line x" title="68:178	In order to obtain a clear rule, we suggest computing the Z score attached to each word ." ></td>
	<td class="line x" title="69:178	If the mean of a binomial distribution is Pr().n', its variance is n'.Pr().(1-Pr())." ></td>
	<td class="line x" title="70:178	These two elements are needed to compute the standard score as described in Equation 1." ></td>
	<td class="line x" title="71:178	))Pr(1()Pr(` )Pr(`)(    = n naZscore         (1) As a decision rule we consider the words having a Z score between -2 and 2 as terms belonging to a common vocabulary, as compared to the reference corpus (as for example will, with, many, friend, or forced in our example)." ></td>
	<td class="line x" title="72:178	This threshold was chosen arbitrary." ></td>
	<td class="line x" title="73:178	A word having a Z score > 2 would be considered as overused (e.g., that, should, must,  not, or government  in MOAT NTCIR-6 English corpus), while a Z score < -2 would be interpreted as an underused term (e.g., police, cell, year, died, or according)." ></td>
	<td class="line x" title="74:178	The arbitrary threshold limit of 2 corresponds to the limit of the standard normal distribution, allowing us to find around 5% of the observa40 tions (around 2.5% less than -2 and 2.5% greater than 2)." ></td>
	<td class="line x" title="75:178	As shown in Figure 1, the difference between our arbitrary limit of 2 (drawn in solid line) and the limits delimiting the 2.5% of the observations (dotted line) are rather close." ></td>
	<td class="line x" title="76:178	Figure 1." ></td>
	<td class="line x" title="77:178	Distribution of the Z score (MOAT NTCIR-6 English corpus, opinionated)." ></td>
	<td class="line x" title="78:178	Based on a training sample, we were able to compute the Z score for different words and retain only those having a large or small Z score value." ></td>
	<td class="line x" title="79:178	Such a procedure is repeated for all classification categories (opinionated and factual)." ></td>
	<td class="line x" title="80:178	It is worth mentioning that such a general scheme may work with isolated words (as applied here) or n-gram (that could be a sequence of either characters or words), as well as with punctuations or other symbols (numbers, dollar signs), syntactic patterns (e.g., verb-adjective in comparative or superlative forms) or other features (presence of proper names, hyperlinks, etc.) 3.1.2 Classification Model When our system needs to determine the opinionatedness of a sentence, we first represent this sentence as a set of words." ></td>
	<td class="line x" title="81:178	For each word, we can then retrieve the Z scores for each category." ></td>
	<td class="line x" title="82:178	If all Z scores for all words are judged as belonging to the general vocabulary, our classification procedure selects the default category." ></td>
	<td class="line x" title="83:178	If not, we may increase the weight associated with the corresponding category (e.g., for the opinionated class if the underlying term is overused in this category)." ></td>
	<td class="line x" title="84:178	Such a simple additive process could be viewed as a first classification scheme, selecting the class having the highest score after enumerating all words occurring in a sentence." ></td>
	<td class="line x" title="85:178	This approach assumes that the word order does not have any impact." ></td>
	<td class="line x" title="86:178	We also assume that each sentence has a similar length." ></td>
	<td class="line x" title="87:178	For this model, we can define two variables, namely SumOP  indicating the sum of the Z score of terms overused in opinionated class (i.e. Z score > 2) and appearing in the input sentence." ></td>
	<td class="line x" title="88:178	Similarly, we can define SumNOOP for the other class." ></td>
	<td class="line x" title="89:178	However, a large SumOP value can be obtained by a single word or by a set of two (or more) words." ></td>
	<td class="line x" title="90:178	Thus, it could be useful to consider also the number of words (features) that are overused (or underused) in a sentence." ></td>
	<td class="line x" title="91:178	Therefore, we can define #OpOver  indicated the number of terms in the evaluated sentence that tends to be overused in opinionated documents (i.e. Z score > 2) while #OpUnder indicated the number of terms that tends to be underused in the class of  opinionated documents (i.e. Z score < -2)." ></td>
	<td class="line x" title="92:178	Similarly, we can define the variables #NoopOver, #NoopUnder, but for the non-opinionated category." ></td>
	<td class="line x" title="93:178	With these additional explanatory variables, we can compute the corresponding subjectivity score for each sentence as follows: NoopUnderNoopOver NoopOverscoreNoop OpUnderOpOver OpOverscoreOp ##  #_ ##  #_ += +=     (2) As a better way to combine different judgments we suggest following Le Calv & Savoy (2000) and normalize the scores using the logistic regression." ></td>
	<td class="line x" title="94:178	The logistic transformation (x) given by each logistic regression model is defined as: +  = = = + + k i ii k i ii x x e ex 10 10 1 )(   pi                   (3) where i are the coefficients obtained from the fitting, xi are the variables, and k is the number of explanatory variables." ></td>
	<td class="line x" title="95:178	These coefficients reflect the relative importance of each variable in the final score." ></td>
	<td class="line x" title="96:178	For each sentence, we can compute the (x) corresponding to the two possible categories and the final decision is simply to classify the sentence according to the max (x) value." ></td>
	<td class="line x" title="97:178	This approach takes account of the fact that some explanatory variables may have more importance than other in assigning the correct category." ></td>
	<td class="line x" title="98:178	41 3.2 Nave Bayes For comparison with our logistic model we chose three baselines: Nave Bayes and language model and finding significant collocations." ></td>
	<td class="line x" title="99:178	Despite its simplicity Nave Bayes classifier tends to perform relatively well for various text categorization problems (Witten, Frank, 2005)." ></td>
	<td class="line x" title="100:178	In accordance with our approach, we used word tokens as classification features for the English corpora." ></td>
	<td class="line x" title="101:178	For the Chinese and Japanese languages overlapping bigram approach was used (Savoy, 2005)." ></td>
	<td class="line x" title="102:178	The training method estimates the relative frequency of the probability that the chosen feature belongs to a specific category using add-one smoothing technique." ></td>
	<td class="line oc" title="103:178	3.3 Language Model (LM) As a second baseline we use the classification based on the language model using overlapping ngram sequences (n was set to 8) as suggested by Pang & Lee (2004, 2005) for the English language." ></td>
	<td class="line x" title="104:178	Using the overlapping 4-gram sequence for the word company, we obtain: comp, ompa, mpan, etc. For the Chinese and Japanese corpora bigram approach was applied." ></td>
	<td class="line x" title="105:178	As in Nave Bayes, the language model gives the probability of the sentence belonging to a specific class." ></td>
	<td class="line x" title="106:178	Working with relatively large n allows a lot of word tokens to be processed as is, at least for the English language." ></td>
	<td class="line x" title="107:178	3.4 Significant Collocations (SC) Another promising approach among the supervised learning schemes is the use of collocations of two or more words or features (Manning & Schtze, 2000)." ></td>
	<td class="line x" title="108:178	This method allows classification of instances based on significant collocations learned from the labeled data." ></td>
	<td class="line x" title="109:178	Some examples of the frequent collocations in the corpora would be in the, of the." ></td>
	<td class="line x" title="110:178	The idea of the method is to find significant collocations (SC) that occur more in the opinionated corpus than in the non-opinionated one." ></td>
	<td class="line x" title="111:178	In order to do so the model returns the collocations of two words for the English language based on the degree to which their counts in the opinionated corpus exceed their expected counts in the not opinionated one." ></td>
	<td class="line x" title="112:178	As an example for the English opinionated corpus the following collocations were found: are worried, pleaded guilty, eager to, expressed hope." ></td>
	<td class="line x" title="113:178	Clearly, overlooking the list of new found collocations it is possible to judge their relevancy." ></td>
	<td class="line x" title="114:178	However, it is not clear how to use this method with the Chinese and Japanese texts, since these languages do not have white space or other usual delimiters as in English." ></td>
	<td class="line x" title="115:178	In order to solve the problem of feature selection we chose  bigram  indexing  on  the  Chinese  and Japanese corpora and searched for significant new collocations of bigrams." ></td>
	<td class="line x" title="116:178	4 Language Dependent Approach As the language dependent technique to improve the obtained classification results we suggest the use of SentiWordNet for the English language (Esuli & Sebastiani, 2006)." ></td>
	<td class="line x" title="117:178	Since the vocabulary of words in SentiWordNet is quite limited it is not always clear how to combine the objectivity scores." ></td>
	<td class="line x" title="118:178	The SentiWordNet score was computed in the following way: to define the opinionated score of the sentence the sum of scores representing that the word belongs to opinionated category for each word in the sentence is calculated." ></td>
	<td class="line x" title="119:178	The not opinionated score of the sentence is computed in the same way with the difference that it is divided by the number of words in the sentence." ></td>
	<td class="line x" title="120:178	Thus, if opinionated score is more than not opinionated one, there is an opinion, otherwise not." ></td>
	<td class="line x" title="121:178	This is a heuristic approach that intuitively takes account of the rationalization that there are more not opinionated words than opinionated in the sentence." ></td>
	<td class="line x" title="122:178	At the same time the presence of opinionated word weighs more than the presence of the not opinionated ones." ></td>
	<td class="line x" title="123:178	Especially, this approach seems to give good result." ></td>
	<td class="line x" title="124:178	5 Experiments The experiment was carried out on the NTCIR-6 and NTCIR-7 English news corpora using 10-fold cross-validation model  on a lenient evaluation standard as described in Seki et al. We do not question the construction and structure of opinions in this data set, since those questions were addressed  at the NTCIR workshops." ></td>
	<td class="line x" title="125:178	Using the Chinese and Japanese corpora we can verify the quality of the suggested language-independent approaches." ></td>
	<td class="line x" title="126:178	42 5.1 Feature Selection & Evaluation in English For the evaluation of sentences in English, the assumption of isolated words (bag-of-words) previously stemmed was used by our system." ></td>
	<td class="line x" title="127:178	The corpora are comprised of more than 13,400 sentences, 4,859 (36.3%) of which are opinionated." ></td>
	<td class="line x" title="128:178	As the evaluation metrics precision, recall and F1-measure were used based on gold standard evaluation provided by NTCIR workshops (Seki et al., 2008)." ></td>
	<td class="line x" title="129:178	The precision and recall are weighted equally in our experiment but it should be recognized that based on the system's needs and focus there could be more accent on precision or recall." ></td>
	<td class="line x" title="130:178	Model Precision Recall F1-measure Logistic model 0.583 0.508 0.543 Nave Bayes 0.415 0.364 0.388 LM 0.350 0.339 0.343 SC 0.979 0.360 0.527 Table 2." ></td>
	<td class="line x" title="131:178	Evaluation results of 10-fold cross-validation on NTCIR-6 and NTCIR-7 English  corpora." ></td>
	<td class="line x" title="132:178	Comparing the results in Table 2 to the baselines of the Nave Bayes classifier and LM evaluated on the same training and testing sets, we see that logistic model outperforms the baselines." ></td>
	<td class="line x" title="133:178	In our opinion, this is due to the use of more explanatory variables that better discriminate between opinionated and factual sentences." ></td>
	<td class="line x" title="134:178	The use of language dependent techniques on the other hand might further improve the results." ></td>
	<td class="line x" title="135:178	Especially, this seems promising observing the results when using the SentiWordNet on the English corpus." ></td>
	<td class="line x" title="136:178	In Table 3 one can see that the first three models show improvement." ></td>
	<td class="line x" title="137:178	Specifically, the precision of the logistic model increased from 0.583 to 0.766 (by 31.4%)." ></td>
	<td class="line x" title="138:178	Model Precision Recall F1-measure Logistic model 0.766 0.488 0.597 Nave Bayes 0.667 0.486 0.562 LM 0.611 0.474 0.534 SC 0.979 0.420 0.588 Table 3." ></td>
	<td class="line x" title="139:178	Evaluation results of 10-fold cross-validation on NTCIR-6 and NTCIR-7 English corpora with SentiWordNet." ></td>
	<td class="line x" title="140:178	When considering the F1-measure, the impact of the language-dependent approach shows 9% of improvement, from 0.543 to 0.597." ></td>
	<td class="line x" title="141:178	The way that we incorporated the scores provided by SentiWordNet was done with the help of linear combination and normalization of scores for each of the models." ></td>
	<td class="line x" title="142:178	5.2 Feature Selection & Evaluation in Chinese We have assumed until now that words can be extracted from a sentence in order to define the needed features used to determine if the underlying information item conveys an opinion or not." ></td>
	<td class="line x" title="143:178	Working with the Chinese language this assumption does no longer hold." ></td>
	<td class="line x" title="144:178	Therefore, we need to determine indexing units by either applying an automating segmentation approach (based on either a morphological  (e.g.,  CSeg&Tag)  or  a  statistical method (Murata & Isahara, 2003)) or considering n-gram indexing approach (unigram or bigram, for example)." ></td>
	<td class="line x" title="145:178	Finally we may also consider a combination of both n-gram and word-based indexing strategies." ></td>
	<td class="line x" title="146:178	Based on the work of Savoy, 2005 we experimented with overlapping bigram and trigram indexing schemes for Chinese." ></td>
	<td class="line x" title="147:178	The experimental results show that bigram indexing outperforms trigram on all three considered statistical methods." ></td>
	<td class="line x" title="148:178	Therefore, as features for Chinese we used overlapping bigrams." ></td>
	<td class="line x" title="149:178	The NTCIR-6 and NTCIR-7 Chinese corpora consisted of more than 14,507 sentences, 9960 (68.7%) of which are opinionated." ></td>
	<td class="line x" title="150:178	The results of all three statistical models performed on the Chinese corpora are presented in Table 4." ></td>
	<td class="line x" title="151:178	Model Precision Recall F1-measure Logistic model 0.943 0.730 0.823 Nave Bayes 0.729 0.538 0.619 LM 0.581 0.634 0.606 SC 0.313 0.898 0.464 Table 4." ></td>
	<td class="line x" title="152:178	Evaluation results of 10-fold cross-validation on NTCIR-6 and NTCIR-7 Chinese corpora." ></td>
	<td class="line x" title="153:178	From the results in Table 4 we clearly see that our approach gives better performance and confirms the results presented in Tables 2 and 3." ></td>
	<td class="line x" title="154:178	The significant improvement in scores could be due to the fact that Chinese corpus contains more opinionated sentences in relevance to not opinionated once." ></td>
	<td class="line x" title="155:178	Thus, the training set for opinionated classi43 fication was much larger compared to the English language." ></td>
	<td class="line x" title="156:178	This proves the relevance of more training data for the learning-based systems." ></td>
	<td class="line x" title="157:178	But the direct comparison with the results on the English corpus is not possible." ></td>
	<td class="line x" title="158:178	5.3 Feature Selection & Evaluation in Japanese As with the Chinese language we face the same challenges in feature definition for the Japanese language." ></td>
	<td class="line x" title="159:178	After experimenting with bigram and trigram we chose bigram strategy for indexing and feature selection." ></td>
	<td class="line x" title="160:178	The NTCIR-6 and NTCIR-7 Japanese corpora consisted of more than 11,100 sentences with 4,622 opinionated sentences (representing 41.6% of the corpus)." ></td>
	<td class="line x" title="161:178	The results of the statistical models are shown in Table 5." ></td>
	<td class="line x" title="162:178	Model Precision Recall F-measure Logistic model 0.527 0.761 0.623 Nave Bayes 0.565 0.570 0.567 LM 0.657 0.667 0.662 SC 0.663 0.856 0.747 Table 5." ></td>
	<td class="line x" title="163:178	Evaluation results of 10-fold cross-validation on NTCIR-6 and NTCIR-7 Japanese corpora." ></td>
	<td class="line x" title="164:178	From the results we can see that the significant collocations model outperforms the others." ></td>
	<td class="line x" title="165:178	This could be due to the fewer number of opinionated sentences compared to the Chinese or English corpora." ></td>
	<td class="line x" title="166:178	This tends to indicate the necessity of an extensive training data for the logistic model in order to provide reliable opinion estimates." ></td>
	<td class="line x" title="167:178	6 Future Work and Conclusion In this paper we presented our language-independent approach based on using Z scores and the logistic model  to identify those terms that adequately characterize subsets of the corpus belonging to opinionated or non-opinionated classes." ></td>
	<td class="line x" title="168:178	In this selection, we focused only on the statistical aspect (distribution difference) of words or bigrams." ></td>
	<td class="line x" title="169:178	Our approach was compared to the three baselines, namely Nave Bayes classifier, language model and an approach based on finding significant collocations." ></td>
	<td class="line x" title="170:178	We have also demonstrated on the English corpora how we can use the language dependent techniques to identify the possibility of opinion expressed in the sentences that otherwise were classified as not opinionated by the system." ></td>
	<td class="line x" title="171:178	The use of SentiWordNet (Esuli & Sebastiani, 2006) in combination with our methods yields better results for the English language." ></td>
	<td class="line x" title="172:178	This study was limited to isolated words in English corpus but in further research we could easily consider longer word sequences to include both noun and verb phrases." ></td>
	<td class="line x" title="173:178	The most useful terms would also then be added to the query to improve the rank of opinionated documents." ></td>
	<td class="line x" title="174:178	As another approach, we could use the evaluation of co-occurrence terms of pronouns I and you mainly with verbs (e.g., believe, feel, think, hate) using part of speech tagging techniques in order to boost the rank of retrieved items." ></td>
	<td class="line x" title="175:178	Using freely available POS taggers, we could take POS information into account (Toutanova & Mannning, 2004) and hopefully develop a better classifier." ></td>
	<td class="line x" title="176:178	For example, the presence of proper names and their frequency or distribution might help us classify a document as being opinionated or not." ></td>
	<td class="line x" title="177:178	The presence of adjectives and adverbs, together with their superlative (e.g., best, most) or comparative (e.g., greater, more) forms could also be useful hints regarding the presence of opinionated versus factual information." ></td>
	<td class="line x" title="178:178	Acknowledgments We would like to thank the MOAT task organizers at NTCIR-7 for their valuable work." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-1904
Data Quality from Crowdsourcing: A Study of Annotation Selection Criteria
Hsueh, Pei-Yun;Melville, Prem;Sindhwani, Vikas;"></td>
	<td class="line x" title="1:182	Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 2735, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:182	c 2009 Association for Computational Linguistics Data Quality from Crowdsourcing: A Study of Annotation Selection Criteria Pei-Yun Hsueh, Prem Melville, Vikas Sindhwani IBM T.J. Watson Research Center 1101 Kitchawan Road, Route 134 Yorktown Heights, NY 10598, USA Abstract Annotation acquisition is an essential step in training supervised classifiers." ></td>
	<td class="line x" title="3:182	However, manual annotation is often time-consuming and expensive." ></td>
	<td class="line x" title="4:182	The possibility of recruiting annotators through Internet services (e.g., Amazon Mechanic Turk) is an appealing option that allows multiple labeling tasks to be outsourced in bulk, typically with low overall costs and fast completion rates." ></td>
	<td class="line x" title="5:182	In this paper, we consider the difficult problem of classifying sentiment in political blog snippets." ></td>
	<td class="line x" title="6:182	Annotation data from both expert annotators in a research lab and non-expert annotators recruited from the Internet are examined." ></td>
	<td class="line x" title="7:182	Three selection criteria are identified to select high-quality annotations: noise level, sentiment ambiguity, and lexical uncertainty." ></td>
	<td class="line x" title="8:182	Analysis confirm the utility of these criteria on improving data quality." ></td>
	<td class="line x" title="9:182	We conduct an empirical study to examine the effect of noisy annotations on the performance of sentiment classification models, and evaluate the utility of annotation selection on classification accuracy and efficiency." ></td>
	<td class="line x" title="10:182	1 Introduction Crowdsourcing (Howe, 2008) is an attractive solution to the problem of cheaply and quickly acquiring annotations for the purposes of constructing all kinds of predictive models." ></td>
	<td class="line x" title="11:182	To sense the potential of crowdsourcing, consider an observation in von Ahn et al.(2004): a crowd of 5,000 people playing an appropriately designed computer game 24 hours a day, could be made to label all images on Google (425,000,000 images in 2005) in a matter of just 31 days." ></td>
	<td class="line x" title="13:182	Several recent papers have studied the use of annotations obtained from Amazon Mechanical Turk, a marketplace for recruiting online workers (Su et al., 2007; Kaisser et al., 2008; Kittur et al., 2008; Sheng et al., 2008; Snow et al., 2008; Sorokin and Forsyth, 2008)." ></td>
	<td class="line x" title="14:182	With efficiency and cost-effectiveness, online recruitment of anonymous annotators brings a new set of issues to the table." ></td>
	<td class="line x" title="15:182	These workers are not usually specifically trained for annotation, and might not be highly invested in producing good-quality annotations." ></td>
	<td class="line x" title="16:182	Consequently, the obtained annotations may be noisy by nature, and might require additional validation or scrutiny." ></td>
	<td class="line x" title="17:182	Several interesting questions immediately arise in how to optimally utilize annotations in this setting: How does one handle differences among workers in terms of the quality of annotations they provide?" ></td>
	<td class="line x" title="18:182	How useful are noisy annotations for the end task of creating a model?" ></td>
	<td class="line x" title="19:182	Is it possible to identify genuinely ambiguous examples via annotator disagreements?" ></td>
	<td class="line x" title="20:182	How should these considerations be treated with respect to intrinsic informativeness of examples?" ></td>
	<td class="line x" title="21:182	These questions also hint at a strong connection to active learning, with annotation quality as a new dimension to the problem." ></td>
	<td class="line x" title="22:182	As a challenging empirical testbed for these issues, we consider the problem of sentiment classification on political blogs." ></td>
	<td class="line x" title="23:182	Given a snippet drawn from a political blog post, the desired output is a polarity score that indicates whether the sentiment expressed is positive or negative." ></td>
	<td class="line x" title="24:182	Such an analysis provides a view of the opinion around a subject of interest, e.g., US Presidential candidates, aggregated across the blogsphere." ></td>
	<td class="line x" title="25:182	Recently, sentiment analy27 sis is emerging as a critical methodology for social media analytics." ></td>
	<td class="line oc" title="26:182	Previous research has focused on classifying subjective-versus-objective expressions (Wiebe et al., 2004), and also on accurate sentiment polarity assignment (Turney, 2002; Yi et al., 2003; Pang and Lee, 2004; Sindhwani and Melville, 2008; Melville et al., 2009)." ></td>
	<td class="line x" title="27:182	The success of most prior work relies on the quality of their knowledge bases; either lexicons defining the sentiment polarity of words around a topic (Yi et al., 2003), or quality annotation data for statistical training." ></td>
	<td class="line x" title="28:182	While manual intervention for compiling lexicons has been significantly lessened by bootstrapping techniques (Yu and Hatzivassiloglou, 2003; Wiebe and Riloff, 2005), manual intervention in the annotation process is harder to avoid." ></td>
	<td class="line x" title="29:182	Moreover, the task of annotating blog-post snippets is challenging, particularly in a charged political atmosphere with complex discourse spanning many issues, use of cynicism and sarcasm, and highly domain-specific and contextual cues." ></td>
	<td class="line x" title="30:182	The downside is that high-performance models are generally difficult to construct, but the upside is that annotation and data-quality issues are more clearly exposed." ></td>
	<td class="line x" title="31:182	In this paper we aim to provide an empirical basis for the use of data selection criteria in the context of sentiment analysis in political blogs." ></td>
	<td class="line x" title="32:182	Specifically, we highlight the need for a set of criteria that can be applied to screen untrustworthy annotators and select informative yet unambiguous examples for the end goal of predictive modeling." ></td>
	<td class="line x" title="33:182	In Section 2, we first examine annotation data obtained by both the expert and non-expert annotators to quantify the impact of including non-experts." ></td>
	<td class="line x" title="34:182	Then, in Section 3, we quantify criteria that can be used to select annotators and examples for selective sampling." ></td>
	<td class="line x" title="35:182	Next, in Section 4, we address the questions of whether the noisy annotations are still useful for this task and study the effect of the different selection criteria on the performance of this task." ></td>
	<td class="line x" title="36:182	Finally, in Section 5 we present conclusion and future work." ></td>
	<td class="line x" title="37:182	2 Annotating Blog Sentiment This section introduces the Political Blog Snippet (PBS) corpus, describes our annotation procedure and the sources of noise, and gives an overview of the experiments on political snippet sentiments." ></td>
	<td class="line x" title="38:182	2.1 The Political Blog Snippet Corpus Our dataset comprises of a collection of snippets extracted from over 500,000 blog posts, spanning the activity of 16,741 political bloggers in the time period of Aug 15, 2008 to the election day Nov 4, 2008." ></td>
	<td class="line x" title="39:182	A snippet was defined as a window of text containing four consecutive sentences such that the head sentence contained either the term Obama or the term McCain, but both candidates were not mentioned in the same window." ></td>
	<td class="line x" title="40:182	The global discourse structure of a typical political blog post can be highly complicated with latent topics ranging from policies (e.g., financial situation, economics, the Iraq war) to personalities to voting preferences." ></td>
	<td class="line x" title="41:182	We therefore expected sentiment to be highly nonuniform over a blog post." ></td>
	<td class="line x" title="42:182	This snippetization procedure attempts to localize the text around a presidential candidate with the objective of better estimating aggregate sentiment around them." ></td>
	<td class="line x" title="43:182	In all, we extracted 631,224 snippets." ></td>
	<td class="line x" title="44:182	For learning classifiers, we passed the snippets through a stopword filter, pruned all words that occur in less than 3 snippets and created normalized term-frequency feature vectors over a vocabulary of 3,812 words." ></td>
	<td class="line x" title="45:182	2.2 Annotation Procedure The annotation process consists of two steps: Sentiment-class annotation: In the first step, as we are only interested in detecting sentiments related to the named candidate, the annotators were first asked to mark up the snippets irrelevant to the named candidates election campaign." ></td>
	<td class="line x" title="46:182	Then, the annotators were instructed to tag each relevant snippet with one of the following four sentiment polarity labels: Positive, Negative, Both, or Neutral." ></td>
	<td class="line x" title="47:182	Alignment annotation: In the second step, the annotators were instructed to mark up whether each snippet was written to support or oppose the target candidate therein named." ></td>
	<td class="line x" title="48:182	The motivation of adding this tag comes from our interest in building a classification system to detect positive and negative mentions of each candidate." ></td>
	<td class="line x" title="49:182	For the snippets that do not contain a clear political alignment, the annotators had the freedom to mark it as neutral or simply not alignment-revealing." ></td>
	<td class="line x" title="50:182	In our pilot study many bloggers were observed to endorse a named candidate by using negative ex28 pressions to denounce his opponent." ></td>
	<td class="line x" title="51:182	Therefore, in our annotation procedure, the distinction is made between the coding of manifest content, i.e., sentiments on the surface, and latent political alignment under these surface elements." ></td>
	<td class="line x" title="52:182	2.3 Agreement Study In this section, we compare the annotations obtained from the on-site expert annotators and those from the non-expert AMT annotators." ></td>
	<td class="line x" title="53:182	2.3.1 Expert (On-site) Annotation To assess the reliability of the sentiment annotation procedure, we conducted an agreement study with three expert annotators in our site, using 36 snippets randomly chosen from the PBS Corpus." ></td>
	<td class="line x" title="54:182	Overall agreement among the three annotators on the relevance of snippets is 77.8%." ></td>
	<td class="line x" title="55:182	Overall agreement on the four-class sentiment codings is 70.4%." ></td>
	<td class="line x" title="56:182	Analysis indicate that the annotators agreed better on some codings than the others." ></td>
	<td class="line x" title="57:182	For the task of determining whether a snippet is subjective or not1, the annotators agreed 86.1% of the time." ></td>
	<td class="line x" title="58:182	For the task of determining whether a snippet is positive or negative, they agreed 94.9% of the time." ></td>
	<td class="line x" title="59:182	To examine which pair of codings is the most difficult to distinguish, Table 1 summarizes the confusion matrix for the three pairs of annotators judgements on sentiment codings." ></td>
	<td class="line x" title="60:182	Each column describes the marginal probability of a coding and the probability distribution for this coding being recognized as another coding (including itself)." ></td>
	<td class="line x" title="61:182	As many bloggers use cynical expressions in their writings, the most confusing cases occur when the annotators have to determine whether a snippet is negative or neutral." ></td>
	<td class="line x" title="62:182	The effect of cynical expressions on % Neu Pos Both Neg Marginal 21.9 20.0 10.5 47.6 Neutral (Neu) 47.8 14.3 9.1 16.0 Positive (Pos) 13.0 61.9 18.2 6.0 Both (Both) 4.4 9.5 9.1 14.0 Negative (Neg) 34.8 14.3 63.6 64.0 Table 1: Summary matrix for the three on-site annotators sentiment codings." ></td>
	<td class="line x" title="63:182	1This is done by grouping the codings of Positive, Negative, and Both into the subjective class." ></td>
	<td class="line x" title="64:182	sentiment analysis in the political domain is also revealed in the second step of alignment annotation." ></td>
	<td class="line x" title="65:182	Only 42.5% of the snippets have been coded with alignment coding in the same direction as its sentiment coding  i.e., if a snippet is intended to support (oppose) a target candidate, it will contain positive (negative) sentiment." ></td>
	<td class="line x" title="66:182	The alignment coding task has been shown to be reliable, with the annotators agreeing 76.8% of the time overall on the three-level codings: Support/Against/Neutral." ></td>
	<td class="line x" title="67:182	2.3.2 Amazon Mechanical Turk Annotation To compare the annotation reliability between expert and non-expert annotators, we further conducted an agreement study with the annotators recruited from Amazon Mechanical Turk (AMT)." ></td>
	<td class="line x" title="68:182	We have collected 1,000 snippets overnight, with the cost of 4 cents per annotation." ></td>
	<td class="line x" title="69:182	In the agreement study, a subset of 100 snippets is used, and each snippet is annotated by five AMT annotators." ></td>
	<td class="line x" title="70:182	These annotations were completed by 25 annotators whom were selected based on the approval rate of their previous AMT tasks (over 95% of times).2 The AMT annotators spent on average 40 seconds per snippet, shorter than the average of two minutes reported by the on-site annotators." ></td>
	<td class="line x" title="71:182	The lower overall agreement on all four-class sentiment codings, 35.3%, conforms to the expectation that the non-expert annotators are less reliable." ></td>
	<td class="line x" title="72:182	The Turk annotators also agreed less on the three-level alignment codings, achieving only 47.2% of agreement." ></td>
	<td class="line x" title="73:182	However, a finer-grained analysis reveals that they still agree well on some codings: The overall agreement on whether a snippet is relevant, whether a snippet is subjective or not, and whether a snippet is positive or negative remain within a reasonable range: 81.0%, 81.8% and 61.9% respectively." ></td>
	<td class="line x" title="74:182	2.4 Gold Standard We defined the gold standard (GS) label of a snippet in terms of the coding that receives the majority votes.3 Column 1 in Table 2 (onsite-GS predic2Note that we do not enforce these snippets to be annotated by the same group of annotators." ></td>
	<td class="line x" title="75:182	However, Kappa statistics requires to compute the chance agreement of each annotator." ></td>
	<td class="line x" title="76:182	Due to the violation of this assumption, we do not measure the intercoder agreement with Kappa in this agreement study." ></td>
	<td class="line x" title="77:182	3In this study, we excluded 6 snippets whose annotations failed to reach majority vote by the three onsite annotators." ></td>
	<td class="line x" title="78:182	29 onsite-GS prediction onsite agreement AMT-GS prediction AMT agreement Sentiment (4-class) 0.767 0.704 0.614 0.353 Alignment (3-level) 0.884 0.768 0.669 0.472 Relevant or not 0.889 0.778 0.893 0.810 Subjective or not 0.931 0.861 0.898 0.818 Positive or negative 0.974 0.949 0.714 0.619 Table 2: Average prediction accuracy on gold standard (GS) using one-coder strategy and inter-coder agreement." ></td>
	<td class="line x" title="79:182	tion) shows the ratio of the onsite expert annotations that are consistent with the gold standard, and Column 3 (AMT-GS prediction) shows the same for the AMT annotations." ></td>
	<td class="line x" title="80:182	The level of consistency, i.e., the percentage agreement with the gold standard labels, can be viewed as a proxy of the quality of the annotations." ></td>
	<td class="line x" title="81:182	Among the AMT annotations, Columns 2 (onsite agreement) and 4 (AMT agreement) show the pair-wise intercoder agreement in the on-site expert and AMT annotations respectively." ></td>
	<td class="line x" title="82:182	The results suggest that it is possible to take one single expert annotators coding as the gold standard in a number of annotation tasks using binary classification." ></td>
	<td class="line x" title="83:182	For example, there is a 97.4% chance that one experts coding on the polarity of a snippet, i.e., whether it is positive or negative, will be consistent with the gold standard coding." ></td>
	<td class="line x" title="84:182	However, this one-annotator strategy is less reliable with the introduction of non-expert annotators." ></td>
	<td class="line x" title="85:182	Take the task of polarity annotation as an example, the intercoder agreement among the AMT workers goes down to 61.9% and the one-coder strategy can only yield 71.4% accuracy." ></td>
	<td class="line x" title="86:182	To determine reliable gold standard codings, multiple annotators are still necessary when non-expert annotators are recruited." ></td>
	<td class="line x" title="87:182	3 Annotation Quality Measures Given the noisy AMT annotations, in this section we discuss some summary statistics that are needed to control the quality of annotations." ></td>
	<td class="line x" title="88:182	3.1 Annotator-level noise To study the question of whether there exists a group of annotators who tend to yield more noisy annotations, we evaluate the accumulated noise level introduced by each of the annotators." ></td>
	<td class="line x" title="89:182	We define the noise level as the deviation from the gold standard labels." ></td>
	<td class="line x" title="90:182	Similar to the measure of individual error rates proposed in (Dawid and Skene, 1979), the noise level of a particular annotator j, i.e., noise(annoj), is then estimated by summing up the deviation of the annotations received from this annotator, with a small sampling correction for chance disagreement." ></td>
	<td class="line x" title="91:182	Analysis results demonstrate that there does exist a subset of annotators who yield more noisy annotations than the others." ></td>
	<td class="line x" title="92:182	20% of the annotators (who exceed the noise level 60%) result in annotations that have 70% disagreement with the gold standard." ></td>
	<td class="line x" title="93:182	In addition, we also evaluate how inclusion of noisy annotators reduces the mean agreement with Gold Standard." ></td>
	<td class="line x" title="94:182	The plot (left) in Figure 1 plots the mean agreement rate with GS over the subset of annotators that pass a noise threshold." ></td>
	<td class="line x" title="95:182	These results show that the data quality decreases with the inclusion of more untrustworthy annotators." ></td>
	<td class="line x" title="96:182	3.2 Snippet-level sentiment ambiguity We have observed that not all snippets are equally easy to annotate, with some containing more ambiguous expressions." ></td>
	<td class="line x" title="97:182	To incorporate this concern in the selection process, a key question to be answered is whether there exist snippets whose sentiment is substantially less distinguishable than the others." ></td>
	<td class="line x" title="98:182	We address this question by quantifying ambiguity measures with the two key properties shown as important in evaluating the controversiality of annotation snippets (Carenini and Cheung, 2008): (1) the strength of the annotators judgements and (2) the polarity of the annotations." ></td>
	<td class="line x" title="99:182	The measurement needs to satisfy the constraints demonstrated in the following snippets: (1) An example that has received three positive codings are more ambiguous than that has received five, and (2) an example that has received five positive codings is more ambiguous than the one that has received four positive and one negative coding." ></td>
	<td class="line x" title="100:182	In addition, as some snippets were shown to 30 Annotator noise level Prediction Accuracy 0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8 1.0 Annotator Noise 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Sentiment Ambigity Lexical Uncertainty Prediction Accuracy 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Lexical Uncertainty Figure 1: Data quality (consistency with GS) as a function of noise level (left), sentiment ambiguity (middle), and lexical uncertainty (right)." ></td>
	<td class="line x" title="101:182	be difficult to tell whether they contain negative or neutral sentiment, the measure of example ambiguity has to go beyond controversiality and incorporate codings of neutral and both." ></td>
	<td class="line x" title="102:182	To satisfy these constraints, we first enumerated through the codings of each snippet and counted the number of neutral, positive, both, and negative codings: We added (1) one to the positive (negative) category for each positive (negative) coding, (2) 0.5 to the neutral category with each neutral coding, and (3) 0.5 to both the positive and negative categories with each both coding." ></td>
	<td class="line x" title="103:182	The strength of codings in the three categories, i.e., str+(snipi), strneu(snipi), and str(snipi), were then summed up into str(snipi)." ></td>
	<td class="line x" title="104:182	The distribution were parameterized with +(snipi) = str+(snipi)/str(snipi) neu(snipi) = strneu(snipi)/str(snipi) (snipi) = str(snipi)/str(snipi) We then quantify the level of ambiguity in the annotators judgement as follows: H((snipi)) =+(snipi)log(+(snipi)) neu(snipi)log(neu(snipi)) (snipi)log((snipi)) Amb(snipi) = str(snipi)str max H((snipi)), where strmax is the maximum value of str among all the snippets in the collection." ></td>
	<td class="line x" title="105:182	The plot (middle) in Figure 1 shows that with the inclusion of snippets that are more ambiguous in sentiment disambiguation, the mean agreement with Gold Standard decreases as expected." ></td>
	<td class="line x" title="106:182	3.3 Combining measures on multiple annotations Having established the impact of noise and sentiment ambiguity on annotation quality, we then set out to explore how to integrate them for selection." ></td>
	<td class="line x" title="107:182	First, the ambiguity scores for each of the snippets are reweighed with respect to the noise level." ></td>
	<td class="line x" title="108:182	w(snipi) = summationdisplay j noise(annoj)(1e)(ij) Conf(snipi) = w(snipi)summationtext iw(snipi) Amb(snipi), where (ij) is an indicator function of whether a coding ofsnipi from annotatorj agrees with its gold standard coding." ></td>
	<td class="line x" title="109:182	w(expi) is thus computed as the aggregated noise level of all the annotators who labeled the ith snippet." ></td>
	<td class="line x" title="110:182	To understand the baseline performance of the selection procedure, we evaluate the the true predictions versus the false alarms resulting from using each of the quality measures separately to select annotations for label predictions." ></td>
	<td class="line x" title="111:182	In this context, a true prediction occurs when an annotation suggested by our measure as high-quality indeed matches the GS label, and a false alarm occurs when a high quality annotation suggested by our measure does not match the GS label." ></td>
	<td class="line x" title="112:182	The ROC (receiver operating characteristics) curves in Figure 2 reflect all the potential operating points with the different measures." ></td>
	<td class="line x" title="113:182	We used data from 2,895 AMT annotations on 579 snippets, including 63 snippets used in the agreement study." ></td>
	<td class="line x" title="114:182	This dataset is obtained by filtering out the snippets with their GS labels as 1 (irrelevant) and the snippets that do not receive any coding that has more than two votes." ></td>
	<td class="line x" title="115:182	31 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 False Alarm Rate True Prediction Rate a71a71a71 a71 a71 a71 a71 a71 a71 a71 0.10.20.3 0.4 0.5 0.6 0.7 0.8 0.9 1 (a) Match Prediction Before Removing Divisive Snippets 1confusion 1ambiguity 1noise 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 False Alarm Rate True Prediction Rate a71a71a71a71 a71 a71 a71 a71 a71 a71 a71 00.10.20.3 0.4 0.5 0.6 0.7 0.8 0.9 1 (b) Match Prediction After Removing Divisive Snippets 1confusion(all4codings) 1confusion(pos/neg) 1ambiguity(all4codings) 1ambiguity(pos/neg) 1noise Figure 2: Modified ROC curves for quality measures: (a) before removing divisive snippets, (b) after removing divisive snippets." ></td>
	<td class="line x" title="116:182	The numbers shown with the ROC curve are the values of the aggregated quality measure (1-confusion)." ></td>
	<td class="line x" title="117:182	Initially, three quality measures are tested: 1noise, 1-ambiguity, 1-confusion." ></td>
	<td class="line x" title="118:182	Examination of the snippet-level sentiment codings reveals that some snippets (12%) result in divisive codings, i.e., equal number of votes on two codings." ></td>
	<td class="line x" title="119:182	The ROC curves in Figure 2 (a) plot the baseline performance of the different quality measures." ></td>
	<td class="line x" title="120:182	Results show that before removing the subset of divisive snippets, the only effective selection criteria is obtained by monitoring the noise level of annotators." ></td>
	<td class="line x" title="121:182	Figure 2 (b) plots the performance after removing the divisive snippets." ></td>
	<td class="line x" title="122:182	In addition, our ambiguity scores are computed under two settings: (1) with only the polar codings (pos/neg), and (2) with all the four codings (all4codings)." ></td>
	<td class="line x" title="123:182	The ROC curves reveal that analyzing only the polar codings is not sufficient for annotation selection." ></td>
	<td class="line x" title="124:182	The results also demonstrate that confusion, an integrated measure, does perform best." ></td>
	<td class="line x" title="125:182	Confusion is just one way of combining these measures." ></td>
	<td class="line x" title="126:182	One may chose alternative combinations  the results here primarily illustrate the benefit of considering these different dimensions in tandem." ></td>
	<td class="line x" title="127:182	Moreover, the difference between plot (a) and (b) suggests that removing divisive snippets is essential for the quality measures to work well." ></td>
	<td class="line x" title="128:182	How to automatically identify the divisive snippets is therefore important to the success of the annotation selection process." ></td>
	<td class="line x" title="129:182	3.4 Effect of lexical uncertainty on divisive snippet detection In search of measures that can help identify the divisive snippets automatically, we consider the inherent lexical uncertainty of an example." ></td>
	<td class="line x" title="130:182	Uncertainty Sampling (Lewis and Catlett, 1994) is one common heuristic for the selection of informative instances, which select instances that the current classifier is most uncertain about." ></td>
	<td class="line x" title="131:182	Following on these lines we measure the uncertainty on instances, with the assumption that the most uncertain snippets are likely to be divisive." ></td>
	<td class="line x" title="132:182	In particular, we applied a lexical sentiment classifier (c.f. Section 4.1.1) to estimate the likelihood of an unseen snippet being of positive or negative sentiment, i.e., P+(expi), P(expi), by counting the sentiment-indicative word occurrences in the snippet." ></td>
	<td class="line x" title="133:182	As in our dataset the negative snippets far exceed the positive ones, we also take the prior probability into account to avoid class bias." ></td>
	<td class="line x" title="134:182	We then measure lexical uncertainty as follows." ></td>
	<td class="line x" title="135:182	Deviation(snipi) = 1 C|(log(P(+))log(P())) +(log(P+(snipi))log(P(snipi)))|, Uncertainty(snipi) =1Deviation(snipi), where class priors, P(+) and P(), are estimated with the dataset used in the agreement studies, and C is the normalization constant." ></td>
	<td class="line x" title="136:182	We then examine not only the utility of lexical uncertainty in identifying high-quality annotations, but 32 Classifier Accuracy AUC LC 49.60 0.614 NB 83.53 0.653 SVM 83.89 0.647 Pooling 84.51 0.700 Table 3: Accuracy of sentiment classification methods." ></td>
	<td class="line x" title="137:182	also the utility of such measure in identifying divisive snippets." ></td>
	<td class="line x" title="138:182	Figure 1 (right) shows the effect of lexical uncertainty on filtering out low-quality annotations." ></td>
	<td class="line x" title="139:182	Figure 3 demonstrates the effect of lexical uncertainty on divisive snippet detection, suggesting the potential use of lexical uncertainty measures in the selection process." ></td>
	<td class="line x" title="140:182	Lexical Uncertainty Divisive Snippet Detection Accuracy 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Figure 3: Divisive snippet detection accuracy as a function of lexical uncertainty." ></td>
	<td class="line x" title="141:182	4 Empirical Evaluation The analysis in Sec." ></td>
	<td class="line x" title="142:182	3 raises two important questions: (1) how useful are noisy annotations for sentiment analysis, and (2) what is the effect of online annotation selection on improving sentiment polarity classification?" ></td>
	<td class="line x" title="143:182	4.1 Polarity Classifier with Noisy Annotations To answer the first question raised above, we train classifiers based on the noisy AMT annotations to classify positive and negative snippets." ></td>
	<td class="line x" title="144:182	Four different types of classifiers are used: SVMs, Naive Bayes (NB), a lexical classifier (LC), and the lexical knowledge-enhanced Pooling Multinomial classifier, described below." ></td>
	<td class="line x" title="145:182	4.1.1 Lexical Classifier In the absence of any labeled data in a domain, one can build sentiment-classification models that rely solely on background knowledge, such as a lexicon defining the polarity of words." ></td>
	<td class="line x" title="146:182	Given a lexicon of positive and negative terms, one straightforward approach to using this information is to measure the frequency of occurrence of these terms in each document." ></td>
	<td class="line x" title="147:182	The probability that a test document belongs to the positive class can then be computed as P(+|D) = aa+b; where a and b are the number of occurrences of positive and negative terms in the document respectively." ></td>
	<td class="line x" title="148:182	A document is then classified as positive if P(+|D) > P(|D); otherwise, the document is classified as negative." ></td>
	<td class="line x" title="149:182	For this study, we used a lexicon of 1,267 positive and 1,701 negative terms, as labeled by human experts." ></td>
	<td class="line x" title="150:182	4.1.2 Pooling Multinomials The Pooling Multinomials classifier was introduced by the authors as an approach to incorporate prior lexical knowledge into supervised learning for better text classification." ></td>
	<td class="line x" title="151:182	In the context of sentiment analysis, such lexical knowledge is available in terms of the prior sentiment-polarity of words." ></td>
	<td class="line x" title="152:182	Pooling Multinomials classifies unlabeled examples just as in multinomial Nave Bayes classification (McCallum and Nigam, 1998), by predicting the class with the maximum likelihood, given by argmaxcjP(cj)producttextiP(wi|cj); where P(cj) is the prior probability of class cj, and P(wi|cj) is the probability of word wi appearing in a snippet of class cj." ></td>
	<td class="line x" title="153:182	In the absence of background knowledge about the class distribution, we estimate the class priors P(cj) solely from the training data." ></td>
	<td class="line x" title="154:182	However, unlike regular Nave Bayes, the conditional probabilities P(wi|cj) are computed using both the labeled examples and the lexicon of labeled features." ></td>
	<td class="line x" title="155:182	Given two models built using labeled examples and labeled features, the multinomial parameters of such models can be aggregated through a convex combination,P(wi|cj) = Pe(wi|cj)+(1)Pf(wi|cj); where Pe(wi|cj) and Pf(wi|cj) represent the probability assigned by using the example labels and feature labels respectively, and is the weight for combining these distributions." ></td>
	<td class="line x" title="156:182	The weight indicates a level of confidence in each source of information, and can be computed based on the training set accuracy of the two components." ></td>
	<td class="line x" title="157:182	The derivation and details of these models are not directly relevant to this paper, but can be found in (Melville et al., 2009)." ></td>
	<td class="line x" title="158:182	33 Q1 Q2 Q3 Q4 Accuracy AUC Accuracy AUC Accuracy AUC Accuracy AUC Noise 84.62% 0.688 74.36% 0.588 74.36% 0.512 79.49% 0.441 Ambiguity 84.21% 0.715 78.95% 0.618 68.42% 0.624 84.21% 0.691 Confusion 82.50% 0.831 82.50% 0.762 80.00% 0.814 80.00% 0.645 Table 4: Effect of annotation selection on classification accuracy." ></td>
	<td class="line x" title="159:182	4.1.3 Results on Polarity Classification We generated a data set of 504 snippets that had 3 or more labels for either the positive or negative class." ></td>
	<td class="line x" title="160:182	We compare the different classification approaches using 10-fold cross-validation and present our results in Table 3." ></td>
	<td class="line x" title="161:182	Results show that the Pooling Multinomial classifier, which makes predictions based on both the prior lexical knowledge and the training data, can learn the most from the labeled data to classify sentiments of the political blog snippets." ></td>
	<td class="line x" title="162:182	We observe that despite the significant level of noise and ambiguity in the training data, using majority-labeled data for training still results in classifiers with reasonable accuracy." ></td>
	<td class="line x" title="163:182	4.2 Effect of Annotation Selection We then evaluate the utility of the quality measures in a randomly split dataset (with 7.5% of the data in the test set)." ></td>
	<td class="line x" title="164:182	We applied each of the measures to rank the annotation examples and then divide them into 4 equal-sized training sets based on their rankings." ></td>
	<td class="line x" title="165:182	For example, Noise-Q1 contains only the least noisy quarter of annotations and Q4 the most noisy ones." ></td>
	<td class="line x" title="166:182	Results in Table 4 demonstrate that the classification performance declines with the decrease of each quality measure in general, despite exceptions in the subset with the highest sentiment ambiguity (Ambiguity-Q4), the most noisy subset Q4 (NoiseQ4), and the subset yielding less overall confusion (Confusion-Q2)." ></td>
	<td class="line x" title="167:182	The results also reveal the benefits of annotation selection on efficiency: using the subset of annotations predicted in the top quality quarter achieves similar performance as using the whole training set." ></td>
	<td class="line x" title="168:182	These preliminary results suggest that an active learning scheme which considers all three quality measures may indeed be effective in improving label quality and subsequent classification accuracy." ></td>
	<td class="line x" title="169:182	5 Conclusion In this paper, we have analyzed the difference between expert and non-expert annotators in terms of annotation quality, and showed that having a single non-expert annotator is detrimental for annotating sentiment in political snippets." ></td>
	<td class="line x" title="170:182	However, we confirmed that using multiple noisy annotations from different non-experts can still be very useful for modeling." ></td>
	<td class="line x" title="171:182	This finding is consistent with the simulated results reported in (Sheng et al., 2008)." ></td>
	<td class="line x" title="172:182	Given the availability of many non-expert annotators ondemand, we studied three important dimensions to consider when acquiring annotations: (1) the noise level of an annotator compared to others, (2) the inherent ambiguity of an examples class label, and (3) the informativeness of an example to the current classification model." ></td>
	<td class="line x" title="173:182	While the first measure has been studied with annotations obtained from experts (Dawid and Skene, 1979; Clemen and Reilly, 1999), the applicability of their findings on non-expert annotation selection has not been examined." ></td>
	<td class="line x" title="174:182	We showed how quality of labels can be improved by eliminating noisy annotators and ambiguous examples." ></td>
	<td class="line x" title="175:182	Furthermore, we demonstrated the quality measures are useful for selecting annotations that lead to more accurate classification models." ></td>
	<td class="line x" title="176:182	Our results suggest that a good active learning or online learning scheme in this setting should really consider all three dimensions." ></td>
	<td class="line x" title="177:182	The way we use to integrate the different dimensions now is still preliminary." ></td>
	<td class="line x" title="178:182	Also, our empirical findings suggest that some of the dimensions may have to be considered separately." ></td>
	<td class="line x" title="179:182	For example, due to the divisive tendency of the most informative examples, these examples may have to be disregarded in the initial stage of annotation selection." ></td>
	<td class="line x" title="180:182	Also, the way we use to combine these measures is still preliminary." ></td>
	<td class="line x" title="181:182	The design and testing of such schemes are avenues for future work." ></td>
	<td class="line x" title="182:182	34" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-2804
Optimization-based Content Selection for Opinion Summarization
Cheung, Jackie Chi Kit;Carenini, Giuseppe;Ng, Raymond T.;"></td>
	<td class="line x" title="1:204	Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 714, Suntec, Singapore, 6 August 2009." ></td>
	<td class="line x" title="2:204	c 2009 ACL and AFNLP Optimization-based Content Selection for Opinion Summarization Jackie Chi Kit Cheung Department of Computer Science University of Toronto Toronto, ON, M5S 3G4, Canada jcheung@cs.toronto.edu Giuseppe Carenini and Raymond T. Ng Department of Computer Science University of British Columbia Vancouver, BC, V6T 1Z4, Canada {carenini,rng}@cs.ubc.ca Abstract We introduce a content selection method for opinion summarization based on a well-studied, formal mathematical model, the p-median clustering problem from facility location theory." ></td>
	<td class="line x" title="3:204	Our method replaces a series of local, myopic steps to content selection with a global solution, and is designed to allow content and realization decisions to be naturally integrated." ></td>
	<td class="line x" title="4:204	We evaluate and compare our method against an existing heuristic-based method on content selection, using human selections as a gold standard." ></td>
	<td class="line x" title="5:204	We find that the algorithms perform similarly, suggesting that our content selection method is robust enough to support integration with other aspects of summarization." ></td>
	<td class="line x" title="6:204	1 Introduction It is now possible to find a large amount of information on peoples opinions on almost every subject online." ></td>
	<td class="line x" title="7:204	The ability to analyze such information is critical in complex, high-stakes decision making processes." ></td>
	<td class="line x" title="8:204	At the individual level, someone wishing to buy a laptop may read customer reviews from others who have purchased and used the product." ></td>
	<td class="line x" title="9:204	At the corporate level, customer feedback on a newly launched product may help to identify weaknesses and features that are in need of improvement (Dellarocas et al., 2004)." ></td>
	<td class="line x" title="10:204	Effective summarization systems are thus needed to convey peoples opinions to users." ></td>
	<td class="line x" title="11:204	A challenging problem in implementing this approach in a particular domain is to devise a content selection strategy that identifies what key information should be presented." ></td>
	<td class="line x" title="12:204	In general, content selection is a critical task at the core of both summarization and NLG and it represents a promising area for cross-fertilization." ></td>
	<td class="line x" title="13:204	Existing NLG systems tend to approach content selection by defining a heuristic based on several relevant factors, and maximizing this heuristic function." ></td>
	<td class="line x" title="14:204	ILEX (Intelligent Labelling Explorer) is a system for generating labels for sets of objects defined in a database, such as for museum artifacts (ODonnell et al., 2001)." ></td>
	<td class="line x" title="15:204	Its content selection strategy involves computing a heuristic relevance score for knowledge elements, and returning the items with the highest scores." ></td>
	<td class="line x" title="16:204	In GEA (Generator of Evaluative Arguments), evaluative arguments are generated to describe an entity as positive or negative (Carenini and Moore, 2006)." ></td>
	<td class="line x" title="17:204	An entity is decomposed into a hierarchy of features, and a relevance score is independently calculated for each feature, based on the preferences of the user and the value of that feature for the product." ></td>
	<td class="line x" title="18:204	Content selection involves selecting the most relevant features for the current user." ></td>
	<td class="line o" title="19:204	There is also work in sentiment analysis relying on optimization or clustering-based approaches." ></td>
	<td class="line oc" title="20:204	Pang and Lee (2004) frame the problem of detecting subjective sentences as finding the minimum cut in a graph representation of the sentences." ></td>
	<td class="line o" title="21:204	They produce compressed versions of movie reviews using just the subjective sentences, which retain the polarity information of the review." ></td>
	<td class="line x" title="22:204	Gamon et al.(2005) use a heuristic approach to cluster sentences drawn from car reviews, grouping sentences that share common terms, especially those salient in the domain such as drive or handling." ></td>
	<td class="line x" title="24:204	The resulting clusters are displayed by a Treemap visualization." ></td>
	<td class="line x" title="25:204	Our work is most similar to the content selection method of the multimedia conversation system RIA (Responsive Information Architect) (Zhou and Aggarwal, 2004)." ></td>
	<td class="line x" title="26:204	In RIA, content selection involves selecting dimensions (such as price in the real estate domain) in response to a query such that the desirability of the dimensions selected for the query is maximized while respect7 ing time and space constraints." ></td>
	<td class="line x" title="27:204	The maximization of desirability is implemented as an optimization problem similar to a knapsack problem." ></td>
	<td class="line x" title="28:204	RIAs content selection method performs similarly to expert human designers, but the evaluation is limited in scale (two designers, each annotating two series of queries to the system), and no heuristic alternative is compared against it." ></td>
	<td class="line x" title="29:204	Our work also frames content selection as a formal optimization problem, but we apply this model to the domain of opinion summarization." ></td>
	<td class="line x" title="30:204	A key advantage of formulating a content selection strategy as a p-median optimization problem is that the resulting framework can be extended to select other characteristics of the summary at the same time as the information content, such as the realization strategy with which the content is expressed." ></td>
	<td class="line x" title="31:204	The p-median clustering works as a module separate from its interpretation as the solution to a content selection problem, so we can freely modify the conversion process from the selection problem to the clustering problem." ></td>
	<td class="line x" title="32:204	Work in NLG and summarization has shown that content and realization decisions (including media allocation) are often dependent on each other, which should be reflected in the summarization process." ></td>
	<td class="line x" title="33:204	For example, in multi-modal summarization, complex information can be more effectively conveyed by combining graphics and text (Tufte et al., 1998)." ></td>
	<td class="line x" title="34:204	While graphics can present large amounts of data compactly and support the discovery of trends and relationships, text is much more effective at explaining key points about the data." ></td>
	<td class="line x" title="35:204	In another case specific to opinion summarization, the controversiality of the opinions in a corpus was found to correlate with the type of text summary, with abstractive summarization being preferred when the controversiality is high (Carenini and Cheung, 2008)." ></td>
	<td class="line x" title="36:204	We first test whether our optimization-based approach can achieve reasonable performance on content selection alone." ></td>
	<td class="line x" title="37:204	As a contribution of this paper, we compare our optimization-based approach to a previously proposed heuristic method." ></td>
	<td class="line x" title="38:204	Because our approach replaces a set of myopic decisions with an extensively studied procedure (the p-median problem) that is able to find a global solution, we hypothesized our approach would produce better selections." ></td>
	<td class="line x" title="39:204	The results of our study indicate that our optimization-based content selection strategy performs about as well as the heuristic method." ></td>
	<td class="line x" title="40:204	These results suggest that our framework is robust enough for integrating other aspects of summarization with content selection." ></td>
	<td class="line x" title="41:204	2 Previous Heuristic Approach 2.1 Assumed Input Information We now define the expected input into the summarization process, then describe a previous greedy heuristic method." ></td>
	<td class="line x" title="42:204	The first phase of the summarization process is to extract opinions about an entity from free text or some other source, such as surveys." ></td>
	<td class="line x" title="43:204	and express the extracted information in a structured format for further processing." ></td>
	<td class="line x" title="44:204	We adopt the approach to opinion extraction described by Carenini et al.(2006), which we summarize here." ></td>
	<td class="line x" title="46:204	Given a corpus of documents expressing opinions about an entity, the system extracts a set of evaluations on aspects or features of the product." ></td>
	<td class="line x" title="47:204	An evaluation consists of a polarity, a score for the strength of the opinion, and the feature being evaluated." ></td>
	<td class="line x" title="48:204	The polarity expresses whether the opinion is positive or negative, and the strength expresses the degree of the sentiment, which is represented as an integer from 1 to 3." ></td>
	<td class="line x" title="49:204	Possible polarity/strength (P/S) scores are thus [-3,2,-1,+1,+2,+3], with +3 being the most positive evaluation, and -3 the most negative." ></td>
	<td class="line x" title="50:204	For example, using a DVD player as the entity, the comment Excellent picture qualityon par with my Pioneer, Panasonic, and JVC players. contains an opinion on the picture quality, and is a very positive evaluation (+3)." ></td>
	<td class="line x" title="51:204	The features and their associated opinions are organized into a hierarchy of user-defined features (UDFs), so named because they can be defined by a user according to the users needs or interests.1 The outcome of the process of opinion extraction and structuring is a UDF hierarchy in which each node is annotated with all the evaluations it received in the corpus (See Figure 1 for an example)." ></td>
	<td class="line x" title="52:204	2.2 Heuristic Content Selection Strategy Using the input information described above, content selection is framed as the process of selecting a subset of those features that are deemed more 1Actually, the system first extracts a set of surface-level crude features (CFs) on which opinions were expressed, using methods described by Hu and Liu (2004)." ></td>
	<td class="line x" title="53:204	Next, the CFs are mapped onto the UDFs using term similarity scores." ></td>
	<td class="line x" title="54:204	The process of mapping CFs to UDFs groups together semantically similar CFs and reduces redundancy." ></td>
	<td class="line x" title="55:204	Our study abstracts away from this mapping process, as well as the process of creating the UDF structure." ></td>
	<td class="line x" title="56:204	We leave the explanation of the details to the original papers." ></td>
	<td class="line x" title="57:204	8 Camera Lens [+1,+1,+3,2,+2] Digital Zoom Optical Zoom . . ." ></td>
	<td class="line x" title="58:204	Editing/Viewing [+1,+1] Viewfinder [-2,2,-1] . . ." ></td>
	<td class="line x" title="59:204	Flash [+1,+1,+3,+2,+2] . . ." ></td>
	<td class="line x" title="60:204	Image Image Type TIFF JPEG . . ." ></td>
	<td class="line x" title="61:204	Resolution Effective Pixels Aspect Ratio . . ." ></td>
	<td class="line x" title="62:204	Figure 1: Partial view of assumed input information (UDF hierarchy annotated with user evaluations) for a digital camera." ></td>
	<td class="line x" title="63:204	important and relevant to the user." ></td>
	<td class="line x" title="64:204	This is done using an importance measure defined on the available features (UDFs)." ></td>
	<td class="line x" title="65:204	This measure is calculated from the P/S scores of the evaluations associated to each UDF." ></td>
	<td class="line x" title="66:204	Let PS(u) be the set of P/S scores that UDF u receives." ></td>
	<td class="line x" title="67:204	Then, a measure of importance is defined as some function of the P/S scores." ></td>
	<td class="line x" title="68:204	Previous work considered only summing the squares of the scores." ></td>
	<td class="line x" title="69:204	In this work, we also consider summing the absolute value of the scores." ></td>
	<td class="line x" title="70:204	So, the importance measure is defined as dir moi(u) = summationdisplay psepsilon1PS(u) ps2 or summationdisplay psepsilon1PS(u) |ps| where the term direct means the importance is derived only from that feature and not from its descendant features." ></td>
	<td class="line x" title="71:204	The basic premises of these metrics are that a features importance should be proportional to the number of evaluations of that feature in the corpus, and that stronger evaluations should be given more weight." ></td>
	<td class="line x" title="72:204	The two versions implement the latter differently, using the sum of squares or the absolute values respectively." ></td>
	<td class="line x" title="73:204	Notice that each non-leaf node in the feature hierarchy effectively serves a dual purpose." ></td>
	<td class="line x" title="74:204	It is both a feature upon which a user might comment, as well as a category for grouping its sub-features." ></td>
	<td class="line x" title="75:204	Thus, a non-leaf node should be important if either its descendants are important or the node itself is important." ></td>
	<td class="line x" title="76:204	To this end, a total measure of importance moi(u) is defined as moi(u) =      dir moi(u) if CH(u) =  [dir moi(u) + (1)summationtext vCH(u)moi(v)] otherwise where CH(u) refers to the children of u in the hierarchy and  is some real parameter in the range [0.5,1] that adjusts the relative weights of the parent and children." ></td>
	<td class="line x" title="77:204	We found in our experimentation that the parameter setting does not substantially change the performance of the system, so we select the value 0.9 for , following previous work." ></td>
	<td class="line x" title="78:204	As a result, the total importance of a node is a combination of its direct importance and of the importance of its children." ></td>
	<td class="line x" title="79:204	The selection procedure proceeds as follows." ></td>
	<td class="line x" title="80:204	First, the most obvious simple greedy selection strategy was consideredsort the nodes in the UDF by the measure of importance and select the most important node until a desired number of features is included." ></td>
	<td class="line x" title="81:204	However, since a node derives part of its importance from its children, it is possible for a nodes importance to be dominated by one or more of its children." ></td>
	<td class="line x" title="82:204	Including both the child and parent node would be redundant because most of the information is contained in the child." ></td>
	<td class="line x" title="83:204	Thus, a dynamic greedy selection algorithm was devised in which the importance of each node was recalculated after each round of selection, with all previously selected nodes removed from the tree." ></td>
	<td class="line x" title="84:204	In this way, if a node that dominates its parents importance is selected, its parents importance will be reduced during later rounds of selection." ></td>
	<td class="line x" title="85:204	Notice, however, that this greedy selection consists of a series of myopic steps to decide which features to include in the summary next, based on what has been selected already and what remains to be selected at this step." ></td>
	<td class="line x" title="86:204	Although this series of local decisions may be locally optimal, it may result in a suboptimal choice of contents overall." ></td>
	<td class="line x" title="87:204	3 Clustering-Based Optimization Strategy To address the limitation of local optimality of this initial strategy, we explore if the content selection problem for opinion summarization can be naturally and effectively solved by a global optimization-based approach." ></td>
	<td class="line x" title="88:204	Our approach assumes the same input information as the previous approach, and we also use the direct measure 9 of importance defined above." ></td>
	<td class="line x" title="89:204	Our framework is UDF-based in the following senses." ></td>
	<td class="line x" title="90:204	First, a UDF is the basic unit of content that is selected for inclusion in the summary." ></td>
	<td class="line x" title="91:204	Also, the information content that needs to be covered by the summary is the sum of the information content in all of the UDFs in the UDF hierarchy." ></td>
	<td class="line x" title="92:204	To reduce content selection to a clustering problem, we need the following components." ></td>
	<td class="line x" title="93:204	First, we need a cost function to quantify how well a UDF (if selected) can express the information content in another UDF." ></td>
	<td class="line x" title="94:204	We call this measure the information coverage cost." ></td>
	<td class="line x" title="95:204	To define this cost function, we need to define the semantic relatedness between the selected content and the covered content, which is domain-dependent." ></td>
	<td class="line x" title="96:204	For example, we can rely on similarity metrics such as ones based on WordNet similarity scores (Fellbaum and others, 1998)." ></td>
	<td class="line x" title="97:204	In the consumer product domain in which we test our method, we use the UDF hierarchy of the entity being summarized." ></td>
	<td class="line x" title="98:204	Second, we need a clustering paradigm that defines the quality of a proposed clustering; that is, a way to globally quantify how well all the information content is represented by the set of UDFs that we select." ></td>
	<td class="line x" title="99:204	The clustering paradigm that we found to most naturally fit our task is the p-median problem (also known as the k-median problem), from facility location theory." ></td>
	<td class="line x" title="100:204	In its original interpretation, p-median is used to find optimal locations for opening facilities which provide services to customers, such that the cost of serving all of the customers with these facilities is minimized." ></td>
	<td class="line x" title="101:204	This matches our intuition that the quality of a summary of opinions depends on how well it represents all of the opinions to be summarized." ></td>
	<td class="line x" title="102:204	Formally, given a set F of m potential locations for facilities, a set U of n customers, a cost function d : F U Rfractur representing the cost of serving a customer uU with a facility f F, and a constant pm, an optimal solution to the p-median problem is a subsetS ofF, such that the expression summationdisplay uU min fS d(f,u) is minimized, and|S|= p. The subsetS is exactly the set of UDFs that we would include in the summary, and the parameter p can be set to determine the summary length." ></td>
	<td class="line x" title="103:204	Although solving the p-median problem is NPhard in general (Kariv and Hakimi, 1979), viable approximation methods do exist." ></td>
	<td class="line x" title="104:204	We use POPSTAR, an implementation of an approximate solution (Resende and Werneck, 2004) which has an average error rate of less than 0.4% on all the problem classes it was tested on in terms of the pmedian problem value." ></td>
	<td class="line x" title="105:204	As an independent test of the programs efficacy, we compare the programs output to solutions which we obtained by bruteforce search on 12 of the 36 datasets we worked with which are small enough such that an exact solution can be feasibly found." ></td>
	<td class="line x" title="106:204	POPSTAR returned the exact solution in all 12 instances." ></td>
	<td class="line x" title="107:204	We now reinterpret the p-median problem for summarization content selection by specifying the sets U, F, and the information coverage cost d in terms of properties of the summarization process." ></td>
	<td class="line x" title="108:204	We define the basic unit of the summarization process to be UDFs, so the sets U and F correspond to the set of UDFs describing the product." ></td>
	<td class="line x" title="109:204	The constant p is a parameter to the p-median problem, determining the summary size in terms of the number of features." ></td>
	<td class="line x" title="110:204	The cost function is d(u,v), where u is a UDF that is being considered for inclusion in the summary, and v is the UDF to be covered by u. To specify this cost, we need to consider both the total amount of information in v as well as the semantic relationship between the two features." ></td>
	<td class="line x" title="111:204	We use the importance measure defined earlier, based on the number and strength of evaluations of the covered feature to quantify the former." ></td>
	<td class="line x" title="112:204	The raw importance score is modified by multipliers which depend on the relationship between u and v. One is the semantic relatedness between the two features, which is modelled by the UDF tree hierarchy." ></td>
	<td class="line x" title="113:204	We hypothesize that it is easier for a more general feature to cover information about a more specific feature than the reverse, and that features that are not in a ancestor-descendant relationship cannot cover information about each other because of the tenuous semantic connection between them." ></td>
	<td class="line x" title="114:204	For example, knowing that a camera is well-liked in general provides stronger evidence that its durability is also well-liked than the reverse." ></td>
	<td class="line x" title="115:204	Based on these assumptions, we define a multiplier for the above measure of importance based on the UDF tree structure, T(u,v), as follows." ></td>
	<td class="line x" title="116:204	T(u,v) =   Tupk, if u is a descendant of v k, if u is an ancestor of v , otherwise k is the length of the path from u to v in the UDF 10 hierarchy." ></td>
	<td class="line x" title="117:204	Tup is a parameter specifying the relative difficulty of covering information in a feature that is an ancestor in the UDF hierarchy." ></td>
	<td class="line x" title="118:204	Mirroring our experience with the heuristic method, the value of the parameter does not affect performance very much." ></td>
	<td class="line x" title="119:204	In our experiments and the example to follow, we pick the values Tup = 3, meaning that covering information in an ancestor node is three times more difficult than covering information in a descendant node." ></td>
	<td class="line x" title="120:204	Another multiplier to the opinion domain is the distribution of evaluations of the features." ></td>
	<td class="line x" title="121:204	Coverage is expected to be less if the features are evaluated differently; for example, if users rated a camera well overall but the feature zoom poorly, a sentence about how well the camera is rated in general does not provide much evidence that the zoom is not well liked, and vice versa." ></td>
	<td class="line x" title="122:204	Since evaluations are labelled with P/S ratings in our data, it is natural to define this multiplier based on the distributions of ratings for the features." ></td>
	<td class="line x" title="123:204	Given these P/S ratings between -3 and +3, we first aggregate the positive and negative evaluations." ></td>
	<td class="line x" title="124:204	As before, we test both summing absolute values and squared values." ></td>
	<td class="line x" title="125:204	Define: imp pos(u) = summationdisplay psPS(u)ps>0 ps2 or |ps| imp neg(u) = summationdisplay psPS(u)ps<0 ps2 or |ps| Then, we calculate the parameter to the Bernoulli distribution corresponding to the ratio of the importance of the two polarities." ></td>
	<td class="line x" title="126:204	That is, Bernoulli with parameter (u) = imp pos(u)/(imp pos(u)+imp neg(u)) The distribution-based multiplier E(u,v) is the Jensen-Shannon divergence from Ber((u)) to Ber((v)), plus one for multiplicative identity when the divergence is zero." ></td>
	<td class="line x" title="127:204	E(u,v) = JS((u),(v)) + 1 The final formula for the information coverage cost is thus d(u,v) = dir moi(v)T(u,v)E(u,v) Consider the following example consisting of four-node UDF tree and importance scores." ></td>
	<td class="line x" title="128:204	i. Covered ii." ></td>
	<td class="line x" title="129:204	Solutions A B C D p Selected Val." ></td>
	<td class="line x" title="130:204	Co vering A 0 50 30 240 1 A 320 B 165 0  120 2 A,D 80 C 165  0  3 A,B,D 30 D 330 150  0 4 A,B,C,D 0 Table 1: i. Information coverage cost scores for the worked example." ></td>
	<td class="line x" title="131:204	Rows represent the covering feature, while columns represent the covered feature." ></td>
	<td class="line x" title="132:204	ii." ></td>
	<td class="line x" title="133:204	Optimal solution to p-median problem in the worked example at different numbers of features selected." ></td>
	<td class="line x" title="134:204	A dir moi(A) = 55 arrowsouthwestarrowsoutheast B C dir moi(B) = 50,dir moi(C) = 30  D dir moi(D) = 120 With parameter Tup = 3 and setting the distribution-based multiplier E to 1 to simplify calculations (or for example, if the features received the same distributions of evaluations), this tree yields the information coverage cost scores found in Table 1i." ></td>
	<td class="line x" title="135:204	Running p-median on these values produces the optimal results found in Table 1ii." ></td>
	<td class="line x" title="136:204	This method trades off selecting centrally located nodes near the root of the UDF tree and the importance of the individual nodes." ></td>
	<td class="line x" title="137:204	In this example, D is selected after the root node A even though D has a greater importance value." ></td>
	<td class="line x" title="138:204	4 Comparative Evaluation 4.1 Stochastic Data Generation In our experiments we wanted to compare the two content selection strategies (heuristic vs. p-median optimization) on datasets that were both realistic and diverse." ></td>
	<td class="line x" title="139:204	Despite the widespread adoption of user reviews in online websites, there is to our knowledge no publicly available corpus of customer reviews of sufficient size which is annotated with features arranged in a hierarchy." ></td>
	<td class="line x" title="140:204	While smallscale corpora do exist for a small number of products, the size of the corpora is too small to be representative of all possible distributions of evaluations and feature hierarchies of products, which limits our ability to draw any meaningful conclusion from the dataset.2 Thus, we stochastically 2Using a constructed dataset based on real data where no resources or agreed-upon evaluation methodology yet exists has been done in other NLP tasks such as topic boundary detection (Reynar, 1994) and local coherence modelling (Barzilay and Lapata, 2005)." ></td>
	<td class="line x" title="141:204	We are encouraged, however, that subsequent to our experiment, more resources for opinion anal11 mean std." ></td>
	<td class="line x" title="142:204	# Features 55.3889 8.5547 # Evaluated Features 21.6667 5.9722 # Children (depth 0) 11.3056 0.7753 # Children (depth 1 fertile) 5.5495 1.7724 Table 2: Statistics on the 36 generated data sets." ></td>
	<td class="line x" title="143:204	At depth 1, 134 of the 407 features in total across the trees were barren." ></td>
	<td class="line x" title="144:204	The generated tree hierarchies were quite flat, with a maximum depth of 2." ></td>
	<td class="line x" title="145:204	generated the data for the products to mimic real product feature hierarchies and evaluations." ></td>
	<td class="line x" title="146:204	We did this by gathering statistics from existing corpora of customer reviews about electronics products (Hu and Liu, 2004), which contain UDF hierarchies and evaluations that have been defined and annotated." ></td>
	<td class="line x" title="147:204	Using these statistics, we created distributions over the characteristics of the data, such as the number of nodes in a UDF hierarchy, and sampled from these distributions to generate new UDF hierarchies and evaluations." ></td>
	<td class="line x" title="148:204	In total, we generated 36 sets of data, which covered a realistic set of possible scenarios in term of feature hierarchy structures as well as in term of distribution of evaluations for each feature." ></td>
	<td class="line x" title="149:204	Table 2 presents some statistics on the generated data sets." ></td>
	<td class="line x" title="150:204	4.2 Building a Human Performance Model We adopt the evaluation approach that a good content selection strategy should perform similarly to humans, which is the view taken by existing summarization evaluation schemes such as ROUGE (Lin, 2004) and the Pyramid method (Nenkova et al., 2007)." ></td>
	<td class="line x" title="151:204	For evaluating our content selection strategy, we conducted a user study asking human participants to perform a selection task to create gold standard selections." ></td>
	<td class="line x" title="152:204	Participants viewed and selected UDF features using a Treemap information visualization." ></td>
	<td class="line x" title="153:204	See Figure 2 for an example." ></td>
	<td class="line x" title="154:204	We recruited 25 university students or graduates, who were each presented with 19 to 20 of the cases we generated as described above." ></td>
	<td class="line x" title="155:204	Each case represented a different hypothetical product, which was represented by a UDF hierarchy, as well as P/S evaluations from -3 to +3." ></td>
	<td class="line x" title="156:204	These were displayed to the participants by a Treemap visualization (Shneiderman, 1992), which is able to give an overview of the feature hierarchy and the evaluations that each feature received." ></td>
	<td class="line x" title="157:204	Treemaps have been shown to be a generally successful tool for ysis such as a user review corpus by Constant et al.(2008) have been released, as an anonymous reviewer pointed out." ></td>
	<td class="line x" title="159:204	visualizing data in the customer review domain, even for novice users (Carenini et al., 2006)." ></td>
	<td class="line x" title="160:204	In a Treemap, the feature hierarchy is represented by nested rectangles, with parent features being larger rectangles, and children features being smaller rectangles contained within its parent rectangle." ></td>
	<td class="line x" title="161:204	The size of the rectangles depends on the number of evaluations that this feature received directly, as well as indirectly through its children features." ></td>
	<td class="line x" title="162:204	Each evaluation is also shown as a small rectangle, coloured according to its P/S rating, with -3 being bright red, and +3 being bright green." ></td>
	<td class="line x" title="163:204	Participants received 30 minutes of interactive training in using Treemaps, and were presented with a scenario in which they were told to take the role of a friend giving advice on the purchase of an electronics product based on existing customer reviews." ></td>
	<td class="line x" title="164:204	They were then shown 22 to 23 scenarios corresponding to different products and evaluations, and asked to select features which they think would be important to include in a summary to send to a friend." ></td>
	<td class="line x" title="165:204	We discarded the first three selections that participants made to allow them to become further accustomed to the visualization." ></td>
	<td class="line x" title="166:204	The number of features that participants were asked to select from each tree was 18% of the number of selectable features." ></td>
	<td class="line x" title="167:204	A feature is considered selectable if it appears in the Treemap visualization; that is, the feature receives at least one evaluation, or one of its descendant features does." ></td>
	<td class="line x" title="168:204	This proportion was the average proportion at which the selections made by the heuristic greedy strategy and p-median diverged the most when we were initially testing the algorithms." ></td>
	<td class="line x" title="169:204	Because each tree contained a different number of features, the actual number of features selected ranged from two to seven." ></td>
	<td class="line x" title="170:204	Features were given generic labels like Feature 34, so that participants cannot rely on preexisting knowledge about that Figure 2: A sample Treemap visualization of the customer review data sets shown to participants." ></td>
	<td class="line x" title="171:204	12 Selection method Cohens Kappa heuristic, squared moi 0.4839 heuristic, abs moi 0.4841 p-median, squared moi 0.4679 p-median, abs moi 0.4821 Table 3: Cohens kappa for heuristic greedy and p-median methods against human selections." ></td>
	<td class="line x" title="172:204	Two versions of the measure of importance were tested, one using squared P/S scores, the other using absolute values." ></td>
	<td class="line x" title="173:204	kind of product in their selections." ></td>
	<td class="line x" title="174:204	4.3 Evaluation Metrics Using this human gold standard, we can now compare the greedy heuristic and the p-median strategies." ></td>
	<td class="line x" title="175:204	We report the agreement between the human and machine selections in terms of kappa and a version of the Pyramid method." ></td>
	<td class="line x" title="176:204	The Pyramid method is a summarization evaluation scheme built upon the observation that human summaries can be equally informative despite being divergent in content (Nenkova et al., 2007)." ></td>
	<td class="line x" title="177:204	In the Pyramid method, Summary Content Units (SCUs) in a set of human-written model summaries are manually identified and annotated." ></td>
	<td class="line x" title="178:204	These SCUs are placed into a pyramid with different tiers, corresponding to the number of model (i.e. human) summaries in which each SCU appears." ></td>
	<td class="line x" title="179:204	A summary to be evaluated is similarly annotated by SCUs and is scored by the scores of its SCUs, which are the tier of the pyramid in which the SCU appears." ></td>
	<td class="line x" title="180:204	The Pyramid score is defined as the sum of the weights of the SCUs in the evaluated summary divided by the maximum score achievable with this number of SCUs, if we were to take SCUs starting from the highest tier of the pyramid." ></td>
	<td class="line x" title="181:204	Thus, a summary scores highly if its SCUs are found in many of the model summaries." ></td>
	<td class="line x" title="182:204	We use UDFs rather than text passages as SCUs, since UDFs are the basic units of content in our selections." ></td>
	<td class="line x" title="183:204	Moderate inter-annotator agreement between human feature selections shows that our data fits the assumption of the Pyramid method (i.e. diversity of human annotations); the Fleiss kappa (1971) scores for the human selections ranged from 0.2984 to 0.6151, with a mean of 0.4456 among all 33 sets which were evaluated." ></td>
	<td class="line x" title="184:204	A kappa value above 0.6 is generally taken to indicate substantial agreement (Landis and Koch, 1977)." ></td>
	<td class="line x" title="185:204	Figure 3: Pyramid scores for the two selection approaches at different numbers of features i. using the squared importance measure, ii." ></td>
	<td class="line x" title="186:204	using the absolute value importance measure." ></td>
	<td class="line x" title="187:204	4.4 Results The greedy heuristic method and p-median perform similarly at the number of features that the human participants were asked to select." ></td>
	<td class="line x" title="188:204	The difference is not statistically significant by a twotailed t-test." ></td>
	<td class="line x" title="189:204	Table 3 shows that using absolute values of P/S scores in the importance measure is better than using squares." ></td>
	<td class="line x" title="190:204	Squaring seems to give too much weight to extreme evaluations over more neutral evaluations." ></td>
	<td class="line x" title="191:204	P-median is particularly affected, which is not surprising as it uses the measure of importance both in the raw importance score and in the distribution-based multiplier." ></td>
	<td class="line x" title="192:204	The Pyramid method allows us to compare the algorithms at different numbers of features." ></td>
	<td class="line x" title="193:204	Figure 3 shows the average pyramid score for the two methods over the proportion of features that are selected." ></td>
	<td class="line x" title="194:204	Overall, both algorithms perform well, and reach a score of about 0.9 at 10% of features selected." ></td>
	<td class="line x" title="195:204	The heuristic method performs slightly better when the proportion is below 25%, but slightly worse above that proportion." ></td>
	<td class="line x" title="196:204	We consider several possible explanations for the surprising result that the heuristic greedy method and p-median methods perform similarly." ></td>
	<td class="line x" title="197:204	One possibility is that the approximate p-median solution we adopted (POPSTAR) is error-prone on this task, but this is unlikely as the approximate method has been rigorously tested both externally on much larger problems and internally on a subset of our data." ></td>
	<td class="line x" title="198:204	Another possibility is that the automatic methods have reached a ceiling in performance by these evaluation metrics." ></td>
	<td class="line x" title="199:204	Nevertheless, these results are encouraging in showing that our optimization-based method is a viable alternative to a heuristic strategy for content selection, and validate that incorporating other 13 summarization decisions into content selection is an option worth exploring." ></td>
	<td class="line x" title="200:204	5 Conclusions and Future Work We have proposed a formal optimization-based method for summarization content selection based on the p-median clustering paradigm, in which content selection is viewed as selecting clusters of related information." ></td>
	<td class="line x" title="201:204	We applied the framework to opinion summarization of customer reviews." ></td>
	<td class="line x" title="202:204	An experiment evaluating our p-median algorithm found that it performed about as well as a comparable existing heuristic approach designed for the opinion domain in terms of similarity to human selections." ></td>
	<td class="line x" title="203:204	These results suggest that the optimization-based approach is a good starting point for integration with other parts of the summarization/NLG process, which is a promising avenue of research." ></td>
	<td class="line x" title="204:204	6 Acknowledgements We would like to thank Lucas Rizoli, Gabriel Murray and the anonymous reviewers for their comments and suggestions." ></td>
</tr></table>
</div
</body></html>
