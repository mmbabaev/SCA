<html><body><head><link rel="stylesheet" type="text/css" href="style.css" /><script src="map.js"></script><script src="jquery-1.7.1.min.js"></script></head>
<div class="dstPaperData">
N04-1035 <div class="dstPaperTitle">What's In A Translation Rule?</div><div class="dstPaperAuthors">Galley, Michel;Hopkins, Mark;Knight, Kevin;Marcu, Daniel;</div>
</div>
<table cellspacing="0" cellpadding="0"><tr>
	<td class="srcData" >Source Paper</td>
	<td class="pp legend" ><input type="checkbox" id="cbIPositive" checked="true"/><label for="cbIPositive">Informal +<label></td>
	<td class="nn legend" ><input type="checkbox" id="cbINegative" checked="true"/><label for="cbINegative">Informal -<label></td>
	<td class="oo legend" ><input type="checkbox" id="cbIObjective" checked="true"/><label for="cbIObjective">Informal Neutral<label></td>
	<td class="ppc legend" ><input type="checkbox" id="cbEPositive" checked="true"/><label for="cbEPositive">Formal +</label></td>
	<td class="nnc legend" ><input type="checkbox" id="cbENegative" checked="true"/><label for="cbENegative">Formal -</label></td>
	<td class="ooc legend" ><input type="checkbox" id="cbEObjective" checked="true"/><label for="cbEObjective">Formal Neutral</label></td>
	<td class="lb"><input type="checkbox" id="cbSentenceBoundary"/><label for="cbSentenceBoundary">Sentence Boundary</label></td>
</tr></table>
<div class="dstPaper">
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P05-1066
Clause Restructuring For Statistical Machine Translation
Collins, Michael John;Koehn, Philipp;Kucerova, Ivona;"></td>
	<td class="line x" title="1:229	Proceedings of the 43rd Annual Meeting of the ACL, pages 531540, Ann Arbor, June 2005." ></td>
	<td class="line x" title="2:229	c2005 Association for Computational Linguistics Clause Restructuring for Statistical Machine Translation Michael Collins MIT CSAIL mcollins@csail.mit.edu Philipp Koehn School of Informatics University of Edinburgh pkoehn@inf.ed.ac.uk Ivona Kucerova MIT Linguistics Department kucerova@mit.edu Abstract We describe a method for incorporating syntactic information in statistical machine translation systems." ></td>
	<td class="line x" title="3:229	The first step of the method is to parse the source language string that is being translated." ></td>
	<td class="line x" title="4:229	The second step is to apply a series of transformations to the parse tree, effectively reordering the surface string on the source language side of the translation system." ></td>
	<td class="line x" title="5:229	The goal of this step is to recover an underlying word order that is closer to the target language word-order than the original string." ></td>
	<td class="line x" title="6:229	The reordering approach is applied as a pre-processing step in both the training and decoding phases of a phrase-based statistical MT system." ></td>
	<td class="line x" title="7:229	We describe experiments on translation from German to English, showing an improvement from 25.2% Bleu score for a baseline system to 26.8% Bleu score for the system with reordering, a statistically significant improvement." ></td>
	<td class="line x" title="8:229	1 Introduction Recent research on statistical machine translation (SMT) has lead to the development of phrasebased systems (Och et al. , 1999; Marcu and Wong, 2002; Koehn et al. , 2003)." ></td>
	<td class="line x" title="9:229	These methods go beyond the original IBM machine translation models (Brown et al. , 1993), by allowing multi-word units (phrases) in one language to be translated directly into phrases in another language." ></td>
	<td class="line x" title="10:229	A number of empirical evaluations have suggested that phrase-based systems currently represent the stateoftheart in statistical machine translation." ></td>
	<td class="line x" title="11:229	In spite of their success, a key limitation of phrase-based systems is that they make little or no direct use of syntactic information." ></td>
	<td class="line x" title="12:229	It appears likely that syntactic information will be crucial in accurately modeling many phenomena during translation, for example systematic differences between the word order of different languages." ></td>
	<td class="line x" title="13:229	For this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical machine translation systems (e.g. , see (Alshawi, 1996; Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Melamed, 2004; Graehl and Knight, 2004; Och et al. , 2004; Xia and McCord, 2004))." ></td>
	<td class="line x" title="14:229	In this paper we describe an approach for the use of syntactic information within phrase-based SMT systems." ></td>
	<td class="line x" title="15:229	The approach constitutes a simple, direct method for the incorporation of syntactic information in a phrasebased system, which we will show leads to significant improvements in translation accuracy." ></td>
	<td class="line x" title="16:229	The first step of the method is to parse the source language string that is being translated." ></td>
	<td class="line x" title="17:229	The second step is to apply a series of transformations to the resulting parse tree, effectively reordering the surface string on the source language side of the translation system." ></td>
	<td class="line x" title="18:229	The goal of this step is to recover an underlying word order that is closer to the target language word-order than the original string." ></td>
	<td class="line x" title="19:229	Finally, we apply a phrase-based system to the reordered string to give a translation into the target language." ></td>
	<td class="line x" title="20:229	We describe experiments involving machine translation from German to English." ></td>
	<td class="line x" title="21:229	As an illustrative example of our method, consider the following German sentence, together with a translation into English that follows the original word order: Original sentence: Ich werde Ihnen die entsprechenden Anmerkungen aushaendigen, damit Sie das eventuell bei der Abstimmung uebernehmen koennen." ></td>
	<td class="line x" title="22:229	English translation: I will to you the corresponding comments pass on, so that you them perhaps in the vote adopt can." ></td>
	<td class="line x" title="23:229	The German word order in this case is substantially different from the word order that would be seen in English." ></td>
	<td class="line x" title="24:229	As we will show later in this paper, translations of sentences of this type pose difficulties for phrase-based systems." ></td>
	<td class="line x" title="25:229	In our approach we reorder the constituents in a parse of the German sentence to give the following word order, which is much closer to the target English word order (words which have been moved are underlined): Reordered sentence: Ich werde aushaendigen Ihnen die entsprechenden Anmerkungen, damit Sie koennen uebernehmen das eventuell bei der Abstimmung." ></td>
	<td class="line x" title="26:229	English translation: I will pass on to you the corresponding comments, so that you can adopt them perhaps in the vote." ></td>
	<td class="line x" title="27:229	531 We applied our approach to translation from German to English in the Europarl corpus." ></td>
	<td class="line x" title="28:229	Source language sentences are reordered in test data, and also in training data that is used by the underlying phrasebased system." ></td>
	<td class="line x" title="29:229	Results using the method show an improvement from 25.2% Bleu score to 26.8% Bleu score (a statistically significant improvement), using a phrase-based system (Koehn et al. , 2003) which has been shown in the past to be a highly competitive SMT system." ></td>
	<td class="line x" title="30:229	2 Background 2.1 Previous Work 2.1.1 Research on Phrase-Based SMT The original work on statistical machine translation was carried out by researchers at IBM (Brown et al. , 1993)." ></td>
	<td class="line x" title="31:229	More recently, phrase-based models (Och et al. , 1999; Marcu and Wong, 2002; Koehn et al. , 2003) have been proposed as a highly successful alternative to the IBM models." ></td>
	<td class="line x" title="32:229	Phrase-based models generalize the original IBM models by allowing multiple words in one language to correspond to multiple words in another language." ></td>
	<td class="line x" title="33:229	For example, we might have a translation entry specifying that I will in English is a likely translation for Ich werde in German." ></td>
	<td class="line x" title="34:229	In this paper we use the phrase-based system of (Koehn et al. , 2003) as our underlying model." ></td>
	<td class="line x" title="35:229	This approach first uses the original IBM models to derive word-to-word alignments in the corpus of example translations." ></td>
	<td class="line x" title="36:229	Heuristics are then used to grow these alignments to encompass phrase-tophrase pairs." ></td>
	<td class="line x" title="37:229	The end result of the training process is a lexicon of phrase-to-phrase pairs, with associated costs or probabilities." ></td>
	<td class="line x" title="38:229	In translation with the system, a beam search method with left-to-right search is used to find a high scoring translation for an input sentence." ></td>
	<td class="line x" title="39:229	At each stage of the search, one or more English words are added to the hypothesized string, and one or more consecutive German words are absorbed (i.e. , marked as having already been translatednote that each word is absorbed at most once)." ></td>
	<td class="line x" title="40:229	Each step of this kind has a number of costs: for example, the log probability of the phrase-tophrase correspondance involved, the log probability from a language model, and some distortion score indicating how likely it is for the proposed words in the English string to be aligned to the corresponding position in the German string." ></td>
	<td class="line oc" title="41:229	2.1.2 Research on Syntax-Based SMT A number of researchers (Alshawi, 1996; Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Melamed, 2004; Graehl and Knight, 2004; Galley et al. , 2004) have proposed models where the translation process involves syntactic representations of the source and/or target languages." ></td>
	<td class="line o" title="42:229	One class of approaches make use of bitext grammars which simultaneously parse both the source and target languages." ></td>
	<td class="line o" title="43:229	Another class of approaches make use of syntactic information in the target language alone, effectively transforming the translation problem into a parsing problem." ></td>
	<td class="line o" title="44:229	Note that these models have radically different structures and parameterizations from phrasebased models for SMT." ></td>
	<td class="line n" title="45:229	As yet, these systems have not shown significant gains in accuracy in comparison to phrase-based systems." ></td>
	<td class="line x" title="46:229	Reranking methods have also been proposed as a method for using syntactic information (Koehn and Knight, 2003; Och et al. , 2004; Shen et al. , 2004)." ></td>
	<td class="line x" title="47:229	In these approaches a baseline system is used to generate a0 -best output." ></td>
	<td class="line x" title="48:229	Syntactic features are then used in a second model that reranks the a0 -best lists, in an attempt to improve over the baseline approach." ></td>
	<td class="line x" title="49:229	(Koehn and Knight, 2003) apply a reranking approach to the sub-task of noun-phrase translation." ></td>
	<td class="line x" title="50:229	(Och et al. , 2004; Shen et al. , 2004) describe the use of syntactic features in reranking the output of a full translation system, but the syntactic features give very small gains: for example the majority of the gain in performance in the experiments in (Och et al. , 2004) was due to the addition of IBM Model 1 translation probabilities, a non-syntactic feature." ></td>
	<td class="line x" title="51:229	An alternative use of syntactic information is to employ an existing statistical parsing model as a language model within an SMT system." ></td>
	<td class="line x" title="52:229	See (Charniak et al. , 2003) for an approach of this form, which shows improvements in accuracy over a baseline system." ></td>
	<td class="line x" title="53:229	2.1.3 Research on Preprocessing Approaches Our approach involves a preprocessing step, where sentences in the language being translated are modified before being passed to an existing phrasebased translation system." ></td>
	<td class="line x" title="54:229	A number of other re532 searchers (Berger et al. , 1996; Niessen and Ney, 2004; Xia and McCord, 2004) have described previous work on preprocessing methods." ></td>
	<td class="line x" title="55:229	(Berger et al. , 1996) describe an approach that targets translation of French phrases of the form NOUN de NOUN (e.g. , conflit dinteret)." ></td>
	<td class="line x" title="56:229	This was a relatively limited study, concentrating on this one syntactic phenomenon which involves relatively local transformations (a parser was not required in this study)." ></td>
	<td class="line x" title="57:229	(Niessen and Ney, 2004) describe a method that combines morphologicallysplit verbs in German, and also reorders questions in English and German." ></td>
	<td class="line x" title="58:229	Our method goes beyond this approach in several respects, for example considering phenomena such as declarative (non-question) clauses, subordinate clauses, negation, and so on." ></td>
	<td class="line x" title="59:229	(Xia and McCord, 2004) describe an approach for translation from French to English, where reordering rules are acquired automatically." ></td>
	<td class="line x" title="60:229	The reordering rules in their approach operate at the level of context-free rules in the parse tree." ></td>
	<td class="line x" title="61:229	Our method differs from that of (Xia and McCord, 2004) in a couple of important respects." ></td>
	<td class="line x" title="62:229	First, we are considering German, which arguably has more challenging word order phenonema than French." ></td>
	<td class="line x" title="63:229	German has relatively free word order, in contrast to both English and French: for example, there is considerable flexibility in terms of which phrases can appear in the first position in a clause." ></td>
	<td class="line x" title="64:229	Second, Xia et." ></td>
	<td class="line x" title="65:229	als (2004) use of reordering rules stated at the context-free level differs from ours." ></td>
	<td class="line x" title="66:229	As one example, in our approach we use a single transformation that moves an infinitival verb to the first position in a verb phrase." ></td>
	<td class="line x" title="67:229	Xia et." ></td>
	<td class="line x" title="68:229	als approach would require learning of a different rule transformation for every production of the form VP => In practice the German parser that we are using creates relatively flat structures at the VP and clause levels, leading to a huge number of context-free rules (the flatness is one consequence of the relatively free word order seen within VPs and clauses in German)." ></td>
	<td class="line x" title="69:229	There are clearly some advantages to learning reordering rules automatically, as in Xia et." ></td>
	<td class="line x" title="70:229	als approach." ></td>
	<td class="line x" title="71:229	However, we note that our approach involves a handful of linguisticallymotivated transformations and achieves comparable improvements (albeit on a different language pair) to Xia et." ></td>
	<td class="line x" title="72:229	als method, which in contrast involves over 56,000 transformations." ></td>
	<td class="line x" title="73:229	S PPER-SB Ich VAFIN-HD werde VP PPER-DA Ihnen NP-OA ART die ADJA entsprechenden NN Anmerkungen VVINF-HD aushaendigen,, S KOUS damit PPER-SB Sie VP PDS-OA das ADJD eventuell PP APPR bei ART der NN Abstimmung VVINF-HD uebernehmen VMFIN-HD koennen Figure 1: An example parse tree." ></td>
	<td class="line x" title="74:229	Key to non-terminals: PPER = personal pronoun; VAFIN = finite verb; VVINF = infinitival verb; KOUS = complementizer; APPR = preposition; ART = article; ADJA = adjective; ADJD = adverb; -SB = subject; -HD = head of a phrase; -DA = dative object; -OA = accusative object." ></td>
	<td class="line x" title="75:229	2.2 German Clause Structure In this section we give a brief description of the syntactic structure of German clauses." ></td>
	<td class="line x" title="76:229	The characteristics we describe motivate the reordering rules described later in the paper." ></td>
	<td class="line x" title="77:229	Figure 1 gives an example parse tree for a German sentence." ></td>
	<td class="line x" title="78:229	This sentence contains two clauses: Clause 1: Ich/I werde/will Ihnen/to you die/the entsprechenden/corresponding Anmerkungen/comments aushaendigen/pass on Clause 2: damit/so that Sie/you das/them eventuell/perhaps bei/in der/the Abstimmung/vote uebernehmen/adopt koennen/can These two clauses illustrate a number of syntactic phenomena in German which lead to quite different word order from English: Position of finite verbs." ></td>
	<td class="line x" title="79:229	In Clause 1, which is a matrix clause, the finite verb werde is in the second position in the clause." ></td>
	<td class="line x" title="80:229	Finite verbs appear rigidly in 2nd position in matrix clauses." ></td>
	<td class="line x" title="81:229	In contrast, in subordinate clauses, such as Clause 2, the finite verb comes last in the clause." ></td>
	<td class="line x" title="82:229	For example, note that koennen is a finite verb which is the final element of Clause 2." ></td>
	<td class="line x" title="83:229	Position of infinitival verbs." ></td>
	<td class="line x" title="84:229	In German, infinitival verbs are final within their associated verb 533 phrase." ></td>
	<td class="line x" title="85:229	For example, returning to Figure 1, notice that aushaendigen is the last element in its verb phrase, and that uebernehmen is the final element of its verb phrase in the figure." ></td>
	<td class="line x" title="86:229	Relatively flexible word ordering." ></td>
	<td class="line x" title="87:229	German has substantially freer word order than English." ></td>
	<td class="line x" title="88:229	In particular, note that while the verb comes second in matrix clauses, essentially any element can be in the first position." ></td>
	<td class="line x" title="89:229	For example, in Clause 1, while the subject Ich is seen in the first position, potentially any of the other constituents (e.g. , Ihnen) could also appear in this position." ></td>
	<td class="line x" title="90:229	Note that this often leads to the subject following the finite verb, something which happens very rarely in English." ></td>
	<td class="line x" title="91:229	There are many other phenomena which lead to differing word order between German and English." ></td>
	<td class="line x" title="92:229	Two others that we focus on in this paper are negation (the differing placement of items such as not in English and nicht in German), and also verb-particle constructions." ></td>
	<td class="line x" title="93:229	We describe our treatment of these phenomena later in this paper." ></td>
	<td class="line x" title="94:229	2.3 Reordering with Phrase-Based SMT We have seen in the last section that German syntax has several characteristics that lead to significantly different word order from that of English." ></td>
	<td class="line x" title="95:229	We now describe how these characteristics can lead to difficulties for phrasebased translation systems when applied to German to English translation." ></td>
	<td class="line x" title="96:229	Typically, reordering models in phrase-based systems are based solely on movement distance." ></td>
	<td class="line x" title="97:229	In particular, at each point in decoding a cost is associated with skipping over 1 or more German words." ></td>
	<td class="line x" title="98:229	For example, assume that in translating Ich werde Ihnen die entsprechenden Anmerkungen aushaendigen." ></td>
	<td class="line x" title="99:229	we have reached a state where Ich and werde have been translated into I will in English." ></td>
	<td class="line x" title="100:229	A potential decoding decision at this point is to add the phrase pass on to the English hypothesis, at the same time absorbing aushaendigen from the German string." ></td>
	<td class="line x" title="101:229	The cost of this decoding step will involve a number of factors, including a cost of skipping over a phrase of length 4 (i.e. , Ihnen die entsprechenden Anmerkungen) in the German string." ></td>
	<td class="line x" title="102:229	The ability to penalise skips of this type, and the potential to model multi-word phrases, are essentially the main strategies that the phrase-based system is able to employ when modeling differing word-order across different languages." ></td>
	<td class="line x" title="103:229	In practice, when training the parameters of an SMT system, for example using the discriminative methods of (Och, 2003), the cost for skips of this kind is typically set to a very high value." ></td>
	<td class="line x" title="104:229	In experiments with the system of (Koehn et al. , 2003) we have found that in practice a large number of complete translations are completely monotonic (i.e. , have a0 skips), suggesting that the system has difficulty learning exactly what points in the translation should allow reordering." ></td>
	<td class="line x" title="105:229	In summary, phrase-based systems have relatively limited potential to model word-order differences between different languages." ></td>
	<td class="line x" title="106:229	The reordering stage described in this paper attempts to modify the source language (e.g. , German) in such a way that its word order is very similar to that seen in the target language (e.g. , English)." ></td>
	<td class="line x" title="107:229	In an ideal approach, the resulting translation problem that is passed on to the phrase-based system will be solvable using a completely monotonic translation, without any skips, and without requiring extremely long phrases to be translated (for example a phrasal translation corresponding to Ihnen die entsprechenden Anmerkungen aushaendigen)." ></td>
	<td class="line x" title="108:229	Note than an additional benefit of the reordering phase is that it may bring together groups of words in German which have a natural correspondance to phrases in English, but were unseen or rare in the original German text." ></td>
	<td class="line x" title="109:229	For example, in the previous example, we might derive a correspondance between werde aushaendigen and will pass on that was not possible before reordering." ></td>
	<td class="line x" title="110:229	Another example concerns verb-particle constructions, for example in Wir machen die Tuer auf machen and auf form a verb-particle construction." ></td>
	<td class="line x" title="111:229	The reordering stage moves auf to precede machen, allowing a phrasal entry that auf machen is translated to to open in English." ></td>
	<td class="line x" title="112:229	Without the reordering, the particle can be arbitrarily far from the verb that it modifies, and there is a danger in this example of translating machen as to make, the natural translation when no particle is present." ></td>
	<td class="line x" title="113:229	534 Original sentence: Ich werde Ihnen die entsprechenden Anmerkungen aushaendigen, damit Sie das eventuell bei der Abstimmung uebernehmen koennen." ></td>
	<td class="line x" title="114:229	(I will to you the corresponding comments pass on, so that you them perhaps in the vote adopt can.)" ></td>
	<td class="line x" title="115:229	Reordered sentence: Ich werde aushaendigen Ihnen die entsprechenden Anmerkungen, damit Sie koennen uebernehmen das eventuell bei der Abstimmung." ></td>
	<td class="line x" title="116:229	(I will pass on to you the corresponding comments, so that you can adopt them perhaps in the vote.)" ></td>
	<td class="line x" title="117:229	Figure 2: An example of the reordering process, showing the original German sentence and the sentence after reordering." ></td>
	<td class="line x" title="118:229	3 Clause Restructuring We now describe the method we use for reordering German sentences." ></td>
	<td class="line x" title="119:229	As a first step in the reordering process, we parse the sentence using the parser described in (Dubey and Keller, 2003)." ></td>
	<td class="line x" title="120:229	The second step is to apply a sequence of rules that reorder the German sentence depending on the parse tree structure." ></td>
	<td class="line x" title="121:229	See Figure 2 for an example German sentence before and after the reordering step." ></td>
	<td class="line x" title="122:229	In the reordering phase, each of the following six restructuring steps were applied to a German parse tree, in sequence (see table 1 also, for examples of the reordering steps): [1] Verb initial In any verb phrase (i.e. , phrase with label VP-) find the head of the phrase (i.e. , the child with label -HD) and move it into the initial position within the verb phrase." ></td>
	<td class="line x" title="123:229	For example, in the parse tree in Figure 1, aushaendigen would be moved to precede Ihnen in the first verb phrase (VPOC), and uebernehmen would be moved to precede das in the second VP-OC." ></td>
	<td class="line x" title="124:229	The subordinate clause would have the following structure after this transformation: S-MO KOUS-CP damit PPER-SB Sie VP-OC VVINF-HD uebernehmen PDS-OA das ADJD-MO eventuell PP-MO APPR-DA bei ART-DA der NN-NK Abstimmung VMFIN-HD koennen [2] Verb 2nd In any subordinate clause labelled S-, with a complementizer KOUS, PREL, PWS or PWAV, find the head of the clause, and move it to directly follow the complementizer." ></td>
	<td class="line x" title="125:229	For example, in the subordinate clause in Figure 1, the head of the clause koennen would be moved to follow the complementizer damit, giving the following structure: S-MO KOUS-CP damit VMFIN-HD koennen PPER-SB Sie VP-OC VVINF-HD uebernehmen PDS-OA das ADJD-MO eventuell PP-MO APPR-DA bei ART-DA der NN-NK Abstimmung [3] Move Subject For any clause (i.e. , phrase with label S), move the subject to directly precede the head." ></td>
	<td class="line x" title="126:229	We define the subject to be the left-most child of the clause with label -SB or PPEREP, and the head to be the leftmost child with label -HD." ></td>
	<td class="line x" title="127:229	For example, in the subordinate clause in Figure 1, the subject Sie would be moved to precede koennen, giving the following structure: S-MO KOUS-CP damit PPER-SB Sie VMFIN-HD koennen VP-OC VVINF-HD uebernehmen PDS-OA das ADJD-MO eventuell PP-MO APPR-DA bei ART-DA der NN-NK Abstimmung [4] Particles In verb particle constructions, move the particle to immediately precede the verb." ></td>
	<td class="line x" title="128:229	More specifically, if a finite verb (i.e. , verb tagged as VVFIN) and a particle (i.e. , word tagged as PTKVZ) are found in the same clause, move the particle to precede the verb." ></td>
	<td class="line x" title="129:229	As one example, the following clause contains both a verb (forden) as well as a particle (auf): S PPER-SB Wir VVFIN-HD fordern NP-OA ART das NN Praesidium PTKVZ-SVP auf After the transformation, the clause is altered to: S PPER-SB Wir PTKVZ-SVP auf VVFIN-HD fordern NP-OA ART das NN Praesidium 535 Transformation Example Verb Initial Before: Ich werde Ihnen die entsprechenden Anmerkungen aushaendigen, a0a1a0a2a0 After: Ich werde aushaendigen Ihnen die entsprechenden Anmerkungen, a0a2a0a2a0 English: I shall be passing on to you some comments, a0a1a0a2a0 Verb 2nd Before: a0a2a0a2a0 damit Sie uebernehmen das eventuell bei der Abstimmung koennen." ></td>
	<td class="line x" title="130:229	After: a0a2a0a2a0 damit koennen Sie uebernehmen das eventuell bei der Abstimmung . English: a0a2a0a2a0 so that could you adopt this perhaps in the voting." ></td>
	<td class="line x" title="131:229	Move Subject Before: a0a2a0a2a0 damit koennen Sie uebernehmen das eventuell bei der Abstimmung." ></td>
	<td class="line x" title="132:229	After: a0a2a0a2a0 damit Sie koennen uebernehmen das eventuell bei der Abstimmung . English: a0a2a0a2a0 so that you could adopt this perhaps in the voting." ></td>
	<td class="line x" title="133:229	Particles Before: Wir fordern das Praesidium auf, a0a3a0a2a0 After: Wir auf fordern das Praesidium, a0a2a0a2a0 English: We ask the Bureau, a0a1a0a3a0 Infinitives Before: Ich werde der Sache nachgehen dann, a0a2a0a2a0 After: Ich werde nachgehen der Sache dann, a0a2a0a2a0 English: I will look into the matter then, a0a1a0a2a0 Negation Before: Wir konnten einreichen es nicht mehr rechtzeitig, a0a2a0a2a0 After: Wir konnten nicht einreichen es mehr rechtzeitig, a0a2a0a1a0 English: We could not hand it in in time, a0a2a0a2a0 Table 1: Examples for each of the reordering steps." ></td>
	<td class="line x" title="134:229	In each case the item that is moved is underlined." ></td>
	<td class="line x" title="135:229	[5] Infinitives In some cases, infinitival verbs are still not in the correct position after transformations [1][4]." ></td>
	<td class="line x" title="136:229	For this reason we add a second step that involves infinitives." ></td>
	<td class="line x" title="137:229	First, we remove all internal VP nodes within the parse tree." ></td>
	<td class="line x" title="138:229	Second, for any clause (i.e. , phrase labeled S), if the clause dominates both a finite and infinitival verb, and there is an argument (i.e. , a subject, or an object) between the two verbs, then the infinitive is moved to directly follow the finite verb." ></td>
	<td class="line x" title="139:229	As an example, the following clause contains an infinitival (einreichen) that is separated from a finite verb konnten by the direct object es: S PPER-SB Wir VMFIN-HD konnten PPER-OA es PTKNEG-NG nicht VP-OC VVINF-HD einreichen AP-MO ADV-MO mehr ADJD-HD rechtzeitig The transformation removes the VP-OC, and moves the infinitive, giving: S PPER-SB Wir VMFIN-HD konnten VVINF-HD einreichen PPER-OA es PTKNEG-NG nicht AP-MO ADV-MO mehr ADJD-HD rechtzeitig [6] Negation As a final step, we move negative particles." ></td>
	<td class="line x" title="140:229	If a clause dominates both a finite and infinitival verb, as well as a negative particle (i.e. , a word tagged as PTKNEG), then the negative particle is moved to directly follow the finite verb." ></td>
	<td class="line x" title="141:229	As an example, the previous example now has the negative particle nicht moved, to give the following clause structure: S PPER-SB Wir VMFIN-HD konnten PTKNEG-NG nicht VVINF-HD einreichen PPER-OA es AP-MO ADV-MO mehr ADJD-HD rechtzeitig 4 Experiments This section describes experiments with the reordering approach." ></td>
	<td class="line x" title="142:229	Our baseline is the phrase-based MT system of (Koehn et al. , 2003)." ></td>
	<td class="line x" title="143:229	We trained this system on the Europarl corpus, which consists of 751,088 sentence pairs with 15,256,792 German words and 16,052,269 English words." ></td>
	<td class="line x" title="144:229	Translation performance is measured on a 2000 sentence test set from a different part of the Europarl corpus, with average sentence length of 28 words." ></td>
	<td class="line x" title="145:229	We use BLEU scores (Papineni et al. , 2002) to measure translation accuracy." ></td>
	<td class="line x" title="146:229	We applied our re536 Annotator 2 Annotator 1 R B E R 33 2 5 B 2 13 5 E 9 4 27 Table 2: Table showing the level of agreement between two annotators on 100 translation judgements." ></td>
	<td class="line x" title="147:229	R gives counts corresponding to translations where an annotator preferred the reordered system; B signifies that the annotator preferred the baseline system; E means an annotator judged the two systems to give equal quality translations." ></td>
	<td class="line x" title="148:229	ordering method to both the training and test data, and retrained the system on the reordered training data." ></td>
	<td class="line x" title="149:229	The BLEU score for the new system was 26.8%, an improvement from 25.2% BLEU for the baseline system." ></td>
	<td class="line x" title="150:229	4.1 Human Translation Judgements We also used human judgements of translation quality to evaluate the effectiveness of the reordering rules." ></td>
	<td class="line x" title="151:229	We randomly selected 100 sentences from the test corpus where the English reference translation was between 10 and 20 words in length.1 For each of these 100 translations, we presented the two annotators with three translations: the reference (human) translation, the output from the baseline system, and the output from the system with reordering." ></td>
	<td class="line x" title="152:229	No indication was given as to which system was the baseline system, and the ordering in which the baseline and reordered translations were presented was chosen at random on each example, to prevent ordering effects in the annotators judgements." ></td>
	<td class="line x" title="153:229	For each example, we asked each of the annotators to make one of two choices: 1) an indication that one translation was an improvement over the other; or 2) an indication that the translations were of equal quality." ></td>
	<td class="line x" title="154:229	Annotator 1 judged 40 translations to be improved by the reordered model; 40 translations to be of equal quality; and 20 translations to be worse under the reordered model." ></td>
	<td class="line x" title="155:229	Annotator 2 judged 44 translations to be improved by the reordered model; 37 translations to be of equal quality; and 19 translations to be worse under the reordered model." ></td>
	<td class="line x" title="156:229	Table 2 gives figures indicating agreement rates between the annotators." ></td>
	<td class="line x" title="157:229	Note that if we only consider preferences where both annotators were in agree1We chose these shorter sentences for human evaluation because in general they include a single clause, which makes human judgements relatively straightforward." ></td>
	<td class="line x" title="158:229	ment (and consider all disagreements to fall into the equal category), then 33 translations improved under the reordering system, and 13 translations became worse." ></td>
	<td class="line x" title="159:229	Figure 3 shows a random selection of the translations where annotator 1 judged the reordered model to give an improvement; Figure 4 shows examples where the baseline system was preferred by annotator 1." ></td>
	<td class="line x" title="160:229	We include these examples to give a qualitative impression of the differences between the baseline and reordered system." ></td>
	<td class="line x" title="161:229	Our (no doubt subjective) impression is that the cases in figure 3 are more clear cut instances of translation improvements, but we leave the reader to make his/her own judgement on this point." ></td>
	<td class="line x" title="162:229	4.2 Statistical Significance We now describe statistical significance tests for our results." ></td>
	<td class="line x" title="163:229	We believe that applying significance tests to Bleu scores is a subtle issue, for this reason we go into some detail in this section." ></td>
	<td class="line x" title="164:229	We used the sign test (e.g. , see page 166 of (Lehmann, 1986)) to test the statistical significance of our results." ></td>
	<td class="line x" title="165:229	For a source sentence a0, the sign test requires a function a1a3a2a0a5a4 that is defined as follows: a6a8a7a10a9a12a11a14a13 a15a16 a16 a16 a16 a16 a16 a16 a17 a16 a16 a16 a16 a16 a16 a16a18 a19 If reordered system produces a better translation for a9 than the baseline a20 If baseline produces a better translation for a9 than the reordered system." ></td>
	<td class="line x" title="166:229	a13 If the two systems produce equal quality translations on a9 We assume that sentences a0 are drawn from some underlying distribution a21a22a2 a0a5a4, and that the test set consists of independently, identically distributed (IID) sentences from this distribution." ></td>
	<td class="line x" title="167:229	We can define the following probabilities: a23a25a24 a26 Probability a2a27a1a3a2 a0a28a4 a26a30a29 a4 (1) a23a32a31 a26 Probability a2a27a1a3a2 a0a28a4 a26a34a33 a4 (2) where the probability is taken with respect to the distribution a21a35a2 a0a28a4 . The sign test has the null hypothesis a36a35a37 a26 a38a39a23 a24a41a40 a23 a31a43a42 and the alternative hypothesis a36a45a44 a26 a38a39a23a46a24 a47a41a23a48a31 a42 . Given a sample of a49 test points a38 a0 a44a51a50a53a52a53a52a53a52a51a50 a0a55a54 a42, the sign test depends on calculation of the following counts: a56a53a24a57a26a59a58a60a38a51a61a63a62 a1a3a2 a0a55a64a65a4 a26a30a29 a42 a58, a56a66a31a67a26a68a58a60a38a51a61a69a62 a1a3a2 a0a55a64a70a4 a26a34a33 a42 a58, 537 and a56 a37 a26a59a58a60a38a51a61a69a62 a1a3a2 a0a55a64a70a4 a26 a0 a42 a58, where a58a0 a58 is the cardinality of the set a0 . We now come to the definition of a1a3a2 a0a28a4  how should we judge whether a translation from one system is better or worse than the translation from another system?" ></td>
	<td class="line x" title="168:229	A critical problem with Bleu scores is that they are a function of an entire test corpus and do not give translation scores for single sentences." ></td>
	<td class="line x" title="169:229	Ideally we would have some measure a1a2a1a69a2 a0a28a4a4a3a6a5 of the quality of the translation of sentence a0 under the reordered system, and a corresponding function a1a8a7 a2a0a5a4 that measures the quality of the baseline translation." ></td>
	<td class="line x" title="170:229	We could then define a1a3a2 a0a5a4 as follows: a1a3a2 a0a5a4 a26 a29 If a1a8a1a63a2 a0a28a4 a47 a1 a7 a2 a0a28a4 a1a3a2 a0a5a4 a26 a33 If a1a8a1a63a2 a0a28a4a10a9 a1 a7 a2 a0a28a4 a1a3a2 a0a5a4 a26 a0 If a1a8a1a63a2 a0a28a4 a26 a1 a7 a2 a0a28a4 Unfortunately Bleu scores do not give persentence measures a1a11a1a69a2a0a5a4 and a1 a7 a2a0a5a4, and thus do not allow a definition of a1a3a2 a0a5a4 in this way." ></td>
	<td class="line x" title="171:229	In general the lack of per-sentence scores makes it challenging to apply significance tests to Bleu scores.2 To get around this problem, we make the following approximation." ></td>
	<td class="line x" title="172:229	For any test sentence a0 a64, we calculate a1a3a2a0a55a64a70a4 as follows." ></td>
	<td class="line x" title="173:229	First, we define a12 to be the Bleu score for the test corpus when translated by the baseline model." ></td>
	<td class="line x" title="174:229	Next, we define a12a64 to be the Bleu score when all sentences other than a0a22a64 are translated by the baseline model, and where a0a22a64 itself is translated by the reordered model." ></td>
	<td class="line x" title="175:229	We then define a1a3a2 a0a55a64a27a4 a26a30a29 If a12 a64 a47 a12 a1a3a2 a0a55a64a27a4 a26a34a33 If a12 a64a13a9 a12 a1a3a2 a0a55a64a27a4 a26 a0 If a12 a64 a26 a12 Note that strictly speaking, this definition of a1a3a2 a0a3a64a70a4 is not valid, as it depends on the entire set of sample points a0 a44a48a52a53a52a53a52 a0a55a54 rather than a0 a64 alone." ></td>
	<td class="line x" title="176:229	However, we believe it is a reasonable approximation to an ideal 2The lack of per-sentence scores means that it is not possible to apply standard statistical tests such as the sign test or the ttest (which would test the hypothesis a14a10a15a6a17a16a25a7a10a9a12a11a19a18a21a20 a14a10a15a6a17a22a48a7a10a9a12a11a19a18, where a14a10a15 a0 a18 is the expected value under a9 )." ></td>
	<td class="line x" title="177:229	Note that previous work (Koehn, 2004; Zhang and Vogel, 2004) has suggested the use of bootstrap tests (Efron and Tibshirani, 1993) for the calculation of confidence intervals for Bleu scores." ></td>
	<td class="line x" title="178:229	(Koehn, 2004) gives empirical evidence that these give accurate estimates for Bleu statistics." ></td>
	<td class="line x" title="179:229	However, correctness of the bootstrap method relies on some technical properties of the statistic (e.g. , Bleu scores) being used (e.g. , see (Wasserman, 2004) theorem 8.3); (Koehn, 2004; Zhang and Vogel, 2004) do not discuss whether Bleu scores meet any such criteria, which makes us uncertain of their correctness when applied to Bleu scores." ></td>
	<td class="line x" title="180:229	function a1a3a2a0a5a4 that indicates whether the translations have improved or not under the reordered system." ></td>
	<td class="line x" title="181:229	Given this definition of a1a3a2a0a28a4, we found that a56a53a24a57a26a24a23 a0a11a25a11a26, a56a66a31 a26 a26a28a27a8a29, and a56 a37 a26 a27 a23 a25 ." ></td>
	<td class="line x" title="182:229	(Thus 52.85% of all test sentences had improved translations under the baseline system, 36.4% of all sentences had worse translations, and 10.75% of all sentences had the same quality as before)." ></td>
	<td class="line x" title="183:229	If our definition of a1a3a2a0a5a4 was correct, these values for a56 a24 and a56a66a31 would be significant at the level a23 a40 a0 a52 a0 a23 . We can also calculate confidence intervals for the results." ></td>
	<td class="line x" title="184:229	Define a21 to be the probability that the reordered system improves on the baseline system, given that the two systems do not have equal performance." ></td>
	<td class="line x" title="185:229	The relative frequency estimate of a21 is a30 a21 a26a31a23 a0a11a25a11a26a8a32 a2 a23 a0a11a25a11a26 a29 a26a28a27a8a29 a4 a26 a25a8a33a8a52a34a27a11a35 . Using a normal approximation (e.g. , see Example 6.17 from (Wasserman, 2004)) a 95% confidence interval for a sample size of 1785 is a30 a21a37a36a38a27a8a52a34a39a11a35, giving a 95% confidence interval of a40a41a25a8a42a8a52a34a33a11a35 a50a43a42 a23 a52a34a25a11a35a45a44 for a21 . 5 Conclusions We have demonstrated that adding knowledge about syntactic structure can significantly improve the performance of an existing state-of-the-art statistical machine translation system." ></td>
	<td class="line x" title="186:229	Our approach makes use of syntactic knowledge to overcome a weakness of tradition SMT systems, namely long-distance reordering." ></td>
	<td class="line x" title="187:229	We pose clause restructuring as a problem for machine translation." ></td>
	<td class="line x" title="188:229	Our current approach is based on hand-crafted rules, which are based on our linguistic knowledge of how German and English syntax differs." ></td>
	<td class="line x" title="189:229	In the future we may investigate data-driven approaches, in an effort to learn reordering models automatically." ></td>
	<td class="line x" title="190:229	While our experiments are on German, other languages have word orders that are very different from English, so we believe our methods will be generally applicable." ></td>
	<td class="line x" title="191:229	Acknowledgements We would like to thank Amit Dubey for providing the German parser used in our experiments." ></td>
	<td class="line x" title="192:229	Thanks to Brooke Cowan and Luke Zettlemoyer for providing the human judgements of translation performance." ></td>
	<td class="line x" title="193:229	Thanks also to Regina Barzilay for many helpful comments on an earlier draft of this paper." ></td>
	<td class="line x" title="194:229	Any remaining errors are of course our own." ></td>
	<td class="line x" title="195:229	Philipp Koehn was supported by a grant from NTT, Agmt." ></td>
	<td class="line x" title="196:229	dtd." ></td>
	<td class="line x" title="197:229	6/21/1998." ></td>
	<td class="line x" title="198:229	Michael Collins was supported by NSF grants IIS-0347631 and IIS-0415030." ></td>
	<td class="line x" title="199:229	538 R: the current difficulties should encourage us to redouble our efforts to promote cooperation in the euro-mediterranean framework." ></td>
	<td class="line x" title="200:229	C: the current problems should spur us to intensify our efforts to promote cooperation within the framework of the europamittelmeerprozesses." ></td>
	<td class="line x" title="201:229	B: the current problems should spur us, our efforts to promote cooperation within the framework of the europamittelmeerprozesses to be intensified." ></td>
	<td class="line x" title="202:229	R: propaganda of any sort will not get us anywhere." ></td>
	<td class="line x" title="203:229	C: with any propaganda to lead to nothing." ></td>
	<td class="line x" title="204:229	B: with any of the propaganda is nothing to do here." ></td>
	<td class="line x" title="205:229	R: yet we would point out again that it is absolutely vital to guarantee independent financial control." ></td>
	<td class="line x" title="206:229	C: however, we would like once again refer to the absolute need for the independence of the financial control." ></td>
	<td class="line x" title="207:229	B: however, we would like to once again to the absolute need for the independence of the financial control out." ></td>
	<td class="line x" title="208:229	R: i cannot go along with the aims mr brok hopes to achieve via his report." ></td>
	<td class="line x" title="209:229	C: i cannot agree with the intentions of mr brok in his report persecuted." ></td>
	<td class="line x" title="210:229	B: i can intentions, mr brok in his report is not agree with." ></td>
	<td class="line x" title="211:229	R: on method, i think the nice perspectives, from that point of view, are very interesting." ></td>
	<td class="line x" title="212:229	C: what the method is concerned, i believe that the prospects of nice are on this point very interesting." ></td>
	<td class="line x" title="213:229	B: what the method, i believe that the prospects of nice in this very interesting point." ></td>
	<td class="line x" title="214:229	R: secondly, without these guarantees, the fall in consumption will impact negatively upon the entire industry." ></td>
	<td class="line x" title="215:229	C: and, secondly, the collapse of consumption without these guarantees will have a negative impact on the whole sector." ></td>
	<td class="line x" title="216:229	B: and secondly, the collapse of the consumption of these guarantees without a negative impact on the whole sector." ></td>
	<td class="line x" title="217:229	R: awarding a diploma in this way does not contravene uk legislation and can thus be deemed legal." ></td>
	<td class="line x" title="218:229	C: since the award of a diploms is not in this form contrary to the legislation of the united kingdom, it can be recognised as legitimate." ></td>
	<td class="line x" title="219:229	B: since the award of a diploms in this form not contrary to the legislation of the united kingdom is, it can be recognised as legitimate." ></td>
	<td class="line x" title="220:229	R: i should like to comment briefly on the directive concerning undesirable substances in products and animal nutrition." ></td>
	<td class="line x" title="221:229	C: i would now like to comment briefly on the directive on undesirable substances and products of animal feed." ></td>
	<td class="line x" title="222:229	B: i would now like to briefly to the directive on undesirable substances and products in the nutrition of them." ></td>
	<td class="line x" title="223:229	R: it was then clearly shown that we can in fact tackle enlargement successfully within the eu s budget." ></td>
	<td class="line x" title="224:229	C: at that time was clear that we can cope with enlargement, in fact, within the framework drawn by the eu budget." ></td>
	<td class="line x" title="225:229	B: at that time was clear that we actually enlargement within the framework able to cope with the eu budget, the drawn." ></td>
	<td class="line x" title="226:229	Figure 3: Examples where annotator 1 judged the reordered system to give an improved translation when compared to the baseline system." ></td>
	<td class="line x" title="227:229	Recall that annotator 1 judged 40 out of 100 translations to fall into this category." ></td>
	<td class="line x" title="228:229	These examples were chosen at random from these 40 examples, and are presented in random order." ></td>
	<td class="line x" title="229:229	R is the human (reference) translation; C is the translation from the system with reordering; B is the output from the baseline system." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P05-3025
Interactively Exploring A Machine Translation Model
DeNeefe, Steve;Knight, Kevin;Chan, Hayward H.;"></td>
	<td class="line x" title="1:93	Proceedings of the ACL Interactive Poster and Demonstration Sessions, pages 97100, Ann Arbor, June 2005." ></td>
	<td class="line x" title="2:93	c2005 Association for Computational Linguistics Interactively Exploring a Machine Translation Model Steve DeNeefe, Kevin Knight, and Hayward H. Chan Information Sciences Institute and Department of Computer Science The Viterbi School of Engineering, University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 {sdeneefe,knight}@isi.edu, hhchan@umich.edu Abstract This paper describes a method of interactively visualizing and directing the process of translating a sentence." ></td>
	<td class="line x" title="3:93	The method allows a user to explore a model of syntax-based statistical machine translation (MT), to understand the models strengths and weaknesses, and to compare it to other MT systems." ></td>
	<td class="line x" title="4:93	Using this visualization method, we can find and address conceptual and practical problems in an MT system." ></td>
	<td class="line x" title="5:93	In our demonstration at ACL, new users of our tool will drive a syntaxbased decoder for themselves." ></td>
	<td class="line x" title="6:93	1 Introduction There are many new approaches to statistical machine translation, and more ideas are being suggested all the time." ></td>
	<td class="line x" title="7:93	However, it is difficult to determine how well a model will actually perform." ></td>
	<td class="line x" title="8:93	Experienced researchers have been surprised by the capability of unintuitive word-for-word models; at the same time, seemingly capable models often have serious hidden problems  intuition is no substitute for experimentation." ></td>
	<td class="line x" title="9:93	With translation ideas growing more complex, capturing aspects of linguistic structure in different ways, it becomes difficult to try out a new idea without a large-scale software development effort." ></td>
	<td class="line x" title="10:93	Anyone who builds a full-scale, trainable translation system using syntactic information faces this problem." ></td>
	<td class="line x" title="11:93	We know that syntactic models often do not fit the data." ></td>
	<td class="line x" title="12:93	For example, the syntactic system described in Yamada and Knight (2001) cannot translate n-to-m-word phrases and does not allow for multi-level syntactic transformations; both phenomena are frequently observed in real data." ></td>
	<td class="line x" title="13:93	In building a new syntax-based MT system which addresses these flaws, we wanted to find problems in our framework as early as possible." ></td>
	<td class="line x" title="14:93	So we decided to create a tool that could help us answer questions like: 1." ></td>
	<td class="line x" title="15:93	Does our framework allow good translations for real data, and if not, where does it get stuck?" ></td>
	<td class="line x" title="16:93	2." ></td>
	<td class="line x" title="17:93	How does our framework compare to existing state-of-the-art phrase-based statistical MT systems such as Och and Ney (2004)?" ></td>
	<td class="line x" title="18:93	The result is DerivTool, an interactive translation visualization tool." ></td>
	<td class="line x" title="19:93	It allows a user to build up a translation from one language to another, step by step, presenting the user with the myriad of choices available to the decoder at each point in the process." ></td>
	<td class="line x" title="20:93	DerivTool simplifies the users experience of exploring these choices by presenting only the decisions relevant to the context in which the user is working, and allowing the user to search for choices that fit a particular set of conditions." ></td>
	<td class="line x" title="21:93	Some previous tools have allowed the user to visualize word alignment information (Callison-Burch et al. , 2004; Smith and Jahr, 2000), but there has been no corresponding deep effort into visualizing the decoding experience itself." ></td>
	<td class="line x" title="22:93	Other tools use visualization to aid the user in manually developing a grammar (Copestake and Flickinger, 2000), while our tool visualizes 97 Starting with:  0  and applying the rule: NPB(DT(the) NNS(police))  0 we get:  NPB(DT(the) NNS(police))  If we then apply the rule: VBN(killed)   we get:  NPB(DT(the) NNS(police)) VBN(killed) Applying the next rule: NP-C(x0:NPB)  x0 results in:  NP-C(NPB(DT(the) NNS(police))) VBN(killed) Finally, applying the rule: VP(VBD(was) VP-C(x0:VBN PP(IN(by) x1:NP-C)))   x1 x0 results in the final phrase: VP(VBD(was) VP-C(VBN(killed) PP(IN(by) NP-C(NPB(DT(the) NNS(police)))))) Table 1: By applying applying four rules, a Chinese verb phrase is translated to English." ></td>
	<td class="line x" title="23:93	the translation process itself, using rules from very large, automatically learned rule sets." ></td>
	<td class="line x" title="24:93	DerivTool can be adapted to visualize other syntax-based MT models, other tree-to-tree or tree-to-string MT models, or models for paraphrasing." ></td>
	<td class="line x" title="25:93	2 Translation Framework It is useful at this point to give a brief description of the syntax-based framework that we work with, which is based on translating Chinese sentences into English syntax trees." ></td>
	<td class="line oc" title="26:93	Galley et al.(2004) describe how to learn hundreds of millions of treetransformation rules from a parsed, aligned Chinese/English corpus, and Galley et al.(submitted) describe probability estimators for those rules." ></td>
	<td class="line x" title="29:93	We decode a new Chinese sentence with a method similar to parsing, where we apply learned rules to build up a complete English tree hypothesis from the Chinese string." ></td>
	<td class="line x" title="30:93	The rule extractor learns rules for many situations." ></td>
	<td class="line x" title="31:93	Some are simple phrase-to-phrase rules such as: NPB(DT(the) NNS(police))  0 This rule should be read as follows: replace the Chinese word 0 with the noun phrase the police." ></td>
	<td class="line x" title="32:93	Others rules can take existing tree fragments and build upon them." ></td>
	<td class="line x" title="33:93	For example, the rule S(x0:NP-C x1:VP x2:)." ></td>
	<td class="line x" title="34:93	 x0 x1 x2 takes three parts of a sentence, a noun phrase (x0), a verb phrase (x1), and a period (x2) and ties them together to build a complete sentence." ></td>
	<td class="line x" title="35:93	Rules also can involve phrase re-ordering, as in NPB(x0:JJ x1:NN)  x1 x0 This rule builds an English noun phrase out of an adjective (x0) and a noun (x1), but in the Chinese, the order is reversed." ></td>
	<td class="line x" title="36:93	Multilevel rules can tie several of these concepts together; the rule VP(VBD(was) VP-C(x0:VBN PP(IN(by) x1:NP-C)))   x1 x0 takes a Chinese word  and two English constituents  x1, a noun phrase, and x0, a pastparticiple verb  and translates them into a phrase of the form was [verb] by [noun-phrase]." ></td>
	<td class="line x" title="37:93	Notice that the order of the constituents has been reversed in the resulting English phrase, and that English function words have been generated." ></td>
	<td class="line x" title="38:93	The decoder builds up a translation from the Chinese sentence into an English tree by applying these rules." ></td>
	<td class="line x" title="39:93	It follows the decoding-as-parsing idea exemplified by Wu (1996) and Yamada and Knight (2002)." ></td>
	<td class="line x" title="40:93	For example, the Chinese verb phrase  0  (literally, [passive] police kill) can be translated to English via four rules (see Table 1)." ></td>
	<td class="line oc" title="41:93	3 DerivTool In order to test whether good translations can be generated with rules learned by Galley et al.(2004), we created DerivTool as an environment for interactively using these rules as a decoder would." ></td>
	<td class="line x" title="43:93	A user starts with a Chinese sentence and applies rules one after another, building up a translation from Chinese to English." ></td>
	<td class="line x" title="44:93	After finishing the translation, the user can save the trace of rule-applications (the derivation tree) for later analysis." ></td>
	<td class="line x" title="45:93	We now outline the typical procedure for a user to translate a sentence with DerivTool." ></td>
	<td class="line x" title="46:93	To start, the user loads a set of sentences to translate and chooses a particular one to work with." ></td>
	<td class="line x" title="47:93	The tool then presents the user with a window split halfway up." ></td>
	<td class="line x" title="48:93	The top 98 Figure 1: DerivTool with a completed derivation." ></td>
	<td class="line x" title="49:93	half is the workspace where the user builds a translation." ></td>
	<td class="line x" title="50:93	It initially displays only the Chinese sentence, with each word as a separate node." ></td>
	<td class="line x" title="51:93	The bottom half presents a set of tabbed panels which allow the user to select rules to build up the translation." ></td>
	<td class="line x" title="52:93	See Figure 1 for a picture of the interface showing a completed derivation tree." ></td>
	<td class="line x" title="53:93	The most immediately useful panel is called Selecting Template, which shows a grid of possible English phrasal translations for Chinese phrases from the sentence." ></td>
	<td class="line x" title="54:93	This phrase grid contains both phrases learned in our extracted rules (e.g. , the police from earlier) and phrases learned by the phrasebased translation system (Och and Ney, 2004)1." ></td>
	<td class="line x" title="55:93	The user presses a grid button to choose a phrase to include in the translation." ></td>
	<td class="line x" title="56:93	At this point, a frequency1The phrase-based system serves as a sparring partner." ></td>
	<td class="line x" title="57:93	We display its best decoding in the center of the screen." ></td>
	<td class="line x" title="58:93	Note that in Figure 1 its output lacks an auxiliary verb and an article." ></td>
	<td class="line x" title="59:93	ordered list of rules will appear; these rules translate the Chinese phrase into the button-selected English phrase, and the user specifies which one to use." ></td>
	<td class="line x" title="60:93	Often there will be more than one rule (e.g. ,  may translate via the rule VBD(killed)   or VBN(killed)  ), and sometimes there are no rules available." ></td>
	<td class="line x" title="61:93	When there are no rules, the buttons are marked in red, telling us that the phrase-based system has access to this phrasal translation but our learned syntactic rules did not capture it." ></td>
	<td class="line x" title="62:93	Other buttons are marked green to represent translations from the specialized number/name/date system, and others are blue, indicating the phrases in the phrasebased decoders best output." ></td>
	<td class="line x" title="63:93	A purple button indicates both red and blue, i.e., the phrase was chosen by the phrase-based decoder but is unavailable in our syntactic framework." ></td>
	<td class="line x" title="64:93	This is a bad combination, showing us where rule learning is weak." ></td>
	<td class="line x" title="65:93	The 99 remaining buttons are gray." ></td>
	<td class="line x" title="66:93	Once the user has chosen the phrasal rules required for translating the sentence, the next step is to stitch these phrases together into a complete English syntax tree using more general rules." ></td>
	<td class="line x" title="67:93	These are found in another panel called Searching." ></td>
	<td class="line x" title="68:93	This panel allows a user to select a set of adjacent, top-level nodes in the tree and find a rule that will connect them together." ></td>
	<td class="line x" title="69:93	It is commonly used for building up larger constituents from smaller ones." ></td>
	<td class="line x" title="70:93	For example, if one has a noun-phrase, a verb-phrase, and a period, the user can search for the rule that connects them and builds an S on top, completing the sentence." ></td>
	<td class="line x" title="71:93	The results of a search are presented in a list, again ordered by frequency." ></td>
	<td class="line x" title="72:93	A few more features to note are: 1) loading and saving your work at any point, 2) adding free-form notes to the document (e.g. I couldnt find a rule that), and 3) manually typing rules if one cannot be found by the above methods." ></td>
	<td class="line x" title="73:93	This allows us to see deficiencies in the framework." ></td>
	<td class="line x" title="74:93	4 How DerivTool Helps First, DerivTool has given us confidence that our syntax-based framework can work, and that the rules we are learning are good." ></td>
	<td class="line x" title="75:93	We have been able to manually build a good translation for each sentence we tried, both for short and long sentences." ></td>
	<td class="line x" title="76:93	In fact, there are multiple good ways to translate sentences using these rules, because different DerivTool users translate sentences differently." ></td>
	<td class="line x" title="77:93	Ordering rules by frequency and/or probability helps us determine if the rules we want are also frequent and favored by our model." ></td>
	<td class="line x" title="78:93	DerivTool has also helped us to find problems with the framework and to see clearly how to fix them." ></td>
	<td class="line x" title="79:93	For example, in one of our first sentences we realized that there was no rule for translating a date  likewise for numbers, names, currency values, and times of day." ></td>
	<td class="line x" title="80:93	Our phrase-based system solves these problems with a specialized date/name/number translator." ></td>
	<td class="line x" title="81:93	Through the process of manually typing syntactic transformation rules for dates and numbers in DerivTool, it became clear that our current date/name/number translator did not provide enough information to create such syntactic rules automatically." ></td>
	<td class="line x" title="82:93	This sparked a new area of research before we had a fully-functional decoder." ></td>
	<td class="line x" title="83:93	We also found that multi-word noun phrases, such as Israeli Prime Minister Sharon and the French Ambassadors visit were often parsed in a way that did not allow us to learn good translation rules." ></td>
	<td class="line x" title="84:93	The flat structure of the constituents in the syntax tree makes it difficult to learn rules that are general enough to be useful." ></td>
	<td class="line x" title="85:93	Phrases with possessives also gave particular difficulty due to the awkward multilevel structure of the parsers output." ></td>
	<td class="line x" title="86:93	We are researching solutions to these problems involving restructuring the syntax trees before training." ></td>
	<td class="line x" title="87:93	Finally, our tool has helped us find bugs in our system." ></td>
	<td class="line x" title="88:93	We found many cases where rules we wanted to use were unexpectedly absent." ></td>
	<td class="line x" title="89:93	We eventually traced these bugs to our rule extraction system." ></td>
	<td class="line x" title="90:93	Our decoder would have simply worked around this problem, producing less desirable translations, but DerivTool allowed us to quickly spot the missing rules." ></td>
	<td class="line x" title="91:93	5 Conclusion We created DerivTool to test our MT framework against real-world data before building a fullyfunctional decoder." ></td>
	<td class="line x" title="92:93	By allowing us to play the role of a decoder and translate sentences manually, it has given us insight into how well our framework fits the data, what some of its weaknesses are, and how it compares to other systems." ></td>
	<td class="line x" title="93:93	We continue to use it as we try out new rule-extraction techniques and finish the decoding system." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W05-0803
Parsing Word-Aligned Parallel Corpora In A Grammar Induction Context
Kuhn, Jonas;"></td>
	<td class="line x" title="1:160	Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 1724, Ann Arbor, June 2005." ></td>
	<td class="line x" title="2:160	cAssociation for Computational Linguistics, 2005 Parsing Word-Aligned Parallel Corpora in a Grammar Induction Context Jonas Kuhn The University of Texas at Austin, Department of Linguistics jonask@mail.utexas.edu Abstract We present an Earley-style dynamic programming algorithm for parsing sentence pairs from a parallel corpus simultaneously, building up two phrase structure trees and a correspondence mapping between the nodes." ></td>
	<td class="line x" title="3:160	The intended use of the algorithm is in bootstrapping grammars for less studied languages by using implicit grammatical information in parallel corpora." ></td>
	<td class="line x" title="4:160	Therefore, we presuppose a given (statistical) word alignment underlying in the synchronous parsing task; this leads to a significant reduction of the parsing complexity." ></td>
	<td class="line x" title="5:160	The theoretical complexity results are corroborated by a quantitative evaluation in which we ran an implementation of the algorithm on a suite of test sentences from the Europarl parallel corpus." ></td>
	<td class="line x" title="6:160	1 Introduction The technical results presented in this paper1 are motivated by the following considerations: It is conceivable to use sentence pairs from a parallel corpus (along with the tentative word correspondences from a statistical word alignment) as training data for a grammar induction approach." ></td>
	<td class="line x" title="7:160	The goal is to induce monolingual grammars for the languages under consideration; but the implicit information about syntactic structure gathered from typical patterns in the alignment goes beyond what can be obtained from unlabeled monolingual data." ></td>
	<td class="line x" title="8:160	Consider for instance the sentence pair from the Europarl corpus (Koehn, 2002) in fig." ></td>
	<td class="line x" title="9:160	1 (shown with a hand-labeled word alignment): distributional patterns over this and similar sentences may show that in English, the subject 1This work was in part supported by the German Research Foundation DFG in the context of the authors Emmy Noether research group at Saarland University." ></td>
	<td class="line x" title="10:160	(the word block the situation) is in a fixed structural position, whereas in German, it can appear in various positions; similarly, the finite verb in German (here: stellt) systematically appears in second position in main clauses." ></td>
	<td class="line x" title="11:160	In a way, the translation of sentences into other natural languages serves as an approximation of a (much more costly) manual structural or semantic annotation  one might speak of automatic indirect supervision in learning." ></td>
	<td class="line x" title="12:160	The technique will be most useful for low-resource languages and languages for which there is no funding for treebanking activities." ></td>
	<td class="line x" title="13:160	The only requirement will be that a parallel corpus exist for the language under consideration and one or more other languages.2 Induction of grammars from parallel corpora is rarely viewed as a promising task in its own right; in work that has addressed the issue directly (Wu, 1997; Melamed, 2003; Melamed, 2004), the synchronous grammar is mainly viewed as instrumental in the process of improving the translation model in a noisy channel approach to statistical MT.3 In the present paper, we provide an important prerequisite for parallel corpus-based grammar induction work: an efficient algorithm for synchronous parsing of sentence pairs, given a word alignment." ></td>
	<td class="line x" title="14:160	This work represents a second pilot study (after (Kuhn, 2004)) for the longer-term PTOLEMAIOS project at Saarland University4 with the goal of learning linguistic grammars from parallel corpora (compare (Kuhn, 2005))." ></td>
	<td class="line x" title="15:160	The grammars should be robust and assign a 2In the present paper we use examples from English/German for illustration, but the approach is of course independent of the language pair under consideration." ></td>
	<td class="line x" title="16:160	3Of course, there is related work (e.g. , (Hwa et al. , 2002; Lu et al. , 2002)) using aligned parallel corpora in order to project bracketings or dependency structures from English to another language and exploit them for training a parser for the other language." ></td>
	<td class="line x" title="17:160	But note the conceptual difference: the parse projection approach departs from a given monolingual parser, with a particular style of analysis, whereas our project will explore to what extent it may help to design the grammar topology specifically for the parallel corpus case." ></td>
	<td class="line x" title="18:160	This means that the emerging English parser may be different from all existing ones." ></td>
	<td class="line x" title="19:160	4http://www.coli.uni-saarland.de/jonask/PTOLEMAIOS/ 17 Heute stellt sich die Lage jedoch vollig anders dar The situation now however is radically different Figure 1: Word-aligned German/English sentence pair from the Europarl corpus predicate-argument-modifier (or dependency) structure to sentences, such that they can be applied in the context of multilingual information extraction or question answering." ></td>
	<td class="line x" title="20:160	2 Synchronous grammars For the purpose of grammar induction from parallel corpora, we assume a fairly straightforward extension of context-free grammars to the synchronous grammar case (compare the transduction grammars of (Lewis II and Stearns, 1968)): Firstly, the terminal and non-terminal categories are pairs of symbols, one for each language; as a special case, one of the two symbols can be NIL for material realized in only one of the languages." ></td>
	<td class="line x" title="21:160	Secondly, the linear sequence of daughter categories that is specified in the rules can differ for the two languages; therefore, an explicit numerical ranking is used for the linear precedence in each language." ></td>
	<td class="line x" title="22:160	We use a compact rule notation with a numerical ranking for the linear precedence in each language." ></td>
	<td class="line x" title="23:160	The general form of a grammar rule for the case of two parallel languages is N0/M0  N1:i1/M1:j1." ></td>
	<td class="line x" title="24:160	Nk:ik/Mk:jk, where Nl, Ml are NIL or a terminal or nonterminal symbol for language L1 and L2, respectively, and il, jl are natural numbers for the rank of the phrase in the sequence for L1 and L2 respectively (for NIL categories a special rank 0 is assumed).5 Since linear ordering of daughters in both languages is explicitly encoded by the rank indices, the specification sequence in the rule is irrelevant from a declarative point of view." ></td>
	<td class="line x" title="25:160	To facilitate parsing we assume a normal form in which the right-hand side is ordered by the rank in L1, with the exception that the categories that are NIL in L1 come last." ></td>
	<td class="line x" title="26:160	If there are several such 5Note that in the probabilistic variants of these grammars, we will typically expect that any ordering of the right-hand side symbols is possible (but that the probability will of course vary  in a maximum entropy or log-linear model, the probability will be estimated based on a variety of learning features)." ></td>
	<td class="line x" title="27:160	This means that in parsing, the right-hand side categories will be accepted as they come in, and the relevant probability parameters are looked up accordingly." ></td>
	<td class="line x" title="28:160	NIL categories in the same rule, they are viewed as unordered with respect to each other.6 Fig." ></td>
	<td class="line x" title="29:160	2 illustrates our simple synchronous grammar formalism with some rules of a sample grammar and their application on a German/English sentence pair." ></td>
	<td class="line x" title="30:160	Derivation with a synchronous grammar gives rise to a multitree, which combines classical phrase structure trees for the languages involved and also encodes the phrase level correspondence across the languages." ></td>
	<td class="line x" title="31:160	Note that the two monolingual trees in fig." ></td>
	<td class="line x" title="32:160	2 for German and English are just two ways of unfolding the common underlying multitree." ></td>
	<td class="line x" title="33:160	Note that the simple formalism goes along with the continuity assumption that every complete constituent is continuous in both languages." ></td>
	<td class="line x" title="34:160	Various recent studies in the field of syntax-based Statistical MT have shown that such an assumption is problematic when based on typical treebank-style analyses." ></td>
	<td class="line x" title="35:160	As (Melamed, 2003) discusses for instance, in the context of binary branching structures even simple examples like the English/French pair a gift for you from France  un cadeau de France pour vouz [a gift from France for you] lead to discontinuity of a synchronous phrase in one of the two languages." ></td>
	<td class="line oc" title="36:160	(Gildea, 2003) and (Galley et al. , 2004) discuss different ways of generalizing the tree-level crosslinguistic correspondence relation, so it is not confined to single tree nodes, thereby avoiding a continuity assumption." ></td>
	<td class="line x" title="37:160	We believe that in order to obtain full coverage on real parallel corpora, some mechanism along these lines will be required." ></td>
	<td class="line x" title="38:160	However, if the typical rich phrase structure analyses (with fairly detailed fine structure) are replaced by flat, multiply branching analyses, most of the highly frequent problematic cases are resolved.7 In 6This detail will be relevant for the parsing inference rule (5) below." ></td>
	<td class="line x" title="39:160	7Compare the systematic study for English-French alignments by (Fox, 2002), who compared (i) treebank-parser style analyses, (ii) a variant with flattened VPs, and (iii) dependency structures." ></td>
	<td class="line x" title="40:160	The degree of cross-linguistic phrasal cohesion increases from (i) to (iii)." ></td>
	<td class="line x" title="41:160	With flat clausal trees, we will come close to dependency structures with respect to cohesion." ></td>
	<td class="line x" title="42:160	18 Synchronous grammar rules: S/S  NP:1/NP:2 Vfin:2/Vfin:3 Adv:3/Adv:1 NP:4/PP:5 Vinf:5/Vinf:4 NP/NP  Pron:1/Pron:1 NP/PP  Det:1/Det:2 N:2/N:4 NIL:0/P:1 NIL:0/Adj:3 Pron/Pron  wir:1/we:1 Vfin/Vfin  mussen:1/must:1 Adv/Adv  deshalb:1/so:1 NIL/P  NIL:0/at:1 Det/Det  die:1/the:1 NIL/Adj  NIL:0/agricultural:1 N/N  Agrarpolitik:1/policy:1 Vinf/Vinf  prufen:1/look:1 German tree: S NP Vfin Adv NP Vinf Pron Det N Wir mussen deshalb die Agrarpolitik prufen we must therefore the agr." ></td>
	<td class="line x" title="43:160	policy examine English tree: S Adv NP Vfin Vinf PP Pron P Det Adj N So we must look at the agricultural policy Multitree: S/S NP:1/NP:2 Vfin:2/Vfin:3 Adv:3/Adv:1 NP:4/PP:5 Vinf:5/Vinf:4 Pron:1/Pron:1 NIL:0/P:1 Det:1/Det:2 NIL:0/Adj:3 N:2/N:4 Wir/we mussen/must deshalb/so NIL/at die/the NIL/agricultural Agrarpolitik/policy prufen/look Figure 2: Sample rules and analysis for a synchronous grammar the flat representation that we assume, a clause is represented in a single subtree of depth 1, with all verbal elements and the argument/adjunct phrases (NPs or PPs) as immediate daughters of the clause node." ></td>
	<td class="line x" title="44:160	Similarly, argument/adjunct phrases are flat internally." ></td>
	<td class="line x" title="45:160	Such a flat representation is justified both from the point of view of linguistic learning and from the point of view of grammar application: (i) Language-specific principles of syntactic structure (e.g. , the strong configurationality of English), which are normally captured linguistically by the richer phrase structure, are available to be induced in learning as systematic patterns in the relative ordering of the elements of a clause." ></td>
	<td class="line x" title="46:160	(ii) The predicateargument-modifier structure relevant for application of the grammars, e.g., in information extraction can be directly read off the flat clausal representation." ></td>
	<td class="line x" title="47:160	It is a hypothesis of our longer-term project that a word alignment-based consensus structure which works with flat representations and under the continuity assumption is a very effective starting point for learning the basic language-specific constraints required for a syntactic grammar." ></td>
	<td class="line x" title="48:160	Linguistic phenomena that fall outside what can be captured in this confined framework (in particular unbounded dependencies spanning more than one clause and discontinuous argument phrases) will then be learned in a later bootstrapping step that provides a richer set of operations." ></td>
	<td class="line x" title="49:160	We are aware of a number of open practical questions, e.g.: Will the fact that real parallel corpora often contain rather free translations undermine our idea of using the consensus structure for learning basic syntactic constraints?" ></td>
	<td class="line x" title="50:160	Statistical alignments are imperfect  can the constraints imposed by the word alignment be relaxed accordingly without sacrificing tractability and the effect of indirect supervision?8 3 Alignment-guided synchronous parsing Our dynamic programming algorithm can be described as a variant of standard Earley-style chart parsing (Earley, 1970) and generation (Shieber, 1988; Kay, 1996)." ></td>
	<td class="line x" title="51:160	The chart is a data structure which stores all sub-analyses that cover part of the input string (in parsing) or meaning representation (in generation)." ></td>
	<td class="line x" title="52:160	Memoizing such partial results has the standard advantage of dynamic programming techniques  it helps one to avoid unnecessary recomputation of partial results." ></td>
	<td class="line x" title="53:160	The chart structure for context-free parsing is also exploited directly in dynamic programming algorithms for probabilistic context-free grammars (PCFGs): (i) the inside (or outside) algorithm for summing over the probabilities for every possible analysis of a given string, (ii) the Viterbi algorithm for determining the most likely analysis of a given string, and (iii) the in8Ultimately, bootstrapping of not only the grammars, but also of the word alignment should be applied." ></td>
	<td class="line x" title="54:160	19 side/outside algorithm for re-estimating the parameters of the PCFG in an Expectation-Maximization approach (i.e. , for iterative training of a PCFG on unlabeled data)." ></td>
	<td class="line x" title="55:160	This aspect is important for the intended later application of our parsing algorithm in a grammar induction context." ></td>
	<td class="line x" title="56:160	A convenient way of describing Earley-style parsing is by inference rules." ></td>
	<td class="line x" title="57:160	For instance, the central completion step in Earley parsing can be described by the rule9 (1) X    Y , [i, j], Y   , [j, k] X   Y  , [i, k] Synchronous parsing." ></td>
	<td class="line x" title="58:160	The input in synchronous parsing is not a one-dimensional string, but a pair of sentences, i.e., a two-dimensional array of possible word pairs (or a multidimensional array if we are looking at a multilingual corpus), as illustrated in fig." ></td>
	<td class="line x" title="59:160	3." ></td>
	<td class="line x" title="60:160	policy  agricultural the  at look  must  we  So  0 1 2 3 4 5 6 L 2 : L1: Wir mussen deshalb die Agrarprufen politik Figure 3: Synchronous parsing: two-dimensional input (with word alignment marked) The natural way of generalizing context-free parsing to synchronous grammars is thus to control the inference rules by string indices in both dimensions." ></td>
	<td class="line x" title="61:160	Graphically speaking, parsing amounts to identifying rectangular crosslinguistic constituents  by assembling smaller rectangles that will together cover the full string spans in both dimensions (compare (Wu, 1997; Melamed, 2003))." ></td>
	<td class="line x" title="62:160	For instance in fig." ></td>
	<td class="line x" title="63:160	4, the NP/NP rectangle [i1, j1, j2, k2] can be combined with the Vinf/Vinf rectangle [j1, k1, i2, j2] (assuming there is an appropriate rule in the grammar)." ></td>
	<td class="line x" title="64:160	9A chart item is specified through a position () in a production and a string span ([l1, l2])." ></td>
	<td class="line x" title="65:160	X    Y , [i, j] means that between string position i and j, the beginning of an X phrase has been found, covering , but still missing Y ." ></td>
	<td class="line x" title="66:160	Chart items for which the dot is at the end of a production (like Y  , [j, k]) are called passive items, the others active." ></td>
	<td class="line x" title="67:160	Vinf/Vinf NP/NP i1 j1 k1 k2 j2 i2 her interview sie interviewen Figure 4: Completion in two-dimensional chart: parsing part of Can I interview her?/Kann ich sie interviewen?" ></td>
	<td class="line x" title="68:160	More generally, we get the inference rules (2) and (3) (one for the case of parallel sequencing, one for crossed order across languages)." ></td>
	<td class="line x" title="69:160	(2) X1/X2    Y1:r1/Y2:r2 , [i1, j1, i2, j2], Y1/Y2   , [j1, k1, j2, k2] X1/X2   Y1:r1/Y2:r2  , [i1, k1, i2, k2] (3) X1/X2    Y1:r1/Y2:r2 , [i1, j1, j2, k2], Y1/Y2   , [j1, k1, i2, j2] X1/X2   Y1:r1/Y2:r2  , [i1, k1, i2, k2] Since each inference rule contains six free variables over string positions (i1, j1, k1, i2, j2, k2), we get a parsing complexity of order O(n6) for unlexicalized grammars (where n is the number of words in the longer of the two strings from language L1 and L2) (Wu, 1997; Melamed, 2003)." ></td>
	<td class="line x" title="70:160	For large-scale learning experiments this may be problematic, especially when one moves to lexicalized grammars, which involve an additional factor of n4.10 As a further issue, we observe that the inference rules are insufficient for multiply branching rules, in which partial constituents may be discontinuous in one dimension (only complete constituents need to be continuous in both dimensions)." ></td>
	<td class="line x" title="71:160	For instance, by parsing the first two words of the German string in fig." ></td>
	<td class="line x" title="72:160	1 (Heute stellt), we should get a partial chart item for a sentence, but the English correspondents for the two words (now and is) are discontinuous, so we couldnt apply rule (2) or (3)." ></td>
	<td class="line x" title="73:160	Correspondence-guided parsing." ></td>
	<td class="line x" title="74:160	As an alternative to the standard rectangular indexing approach 10The assumption here (following (Melamed, 2003)) is that lexicalization is not considered as just affecting the grammar constant, but that in parsing, every terminal symbol has to be considered as the potential head of every phrase of which it is a part." ></td>
	<td class="line x" title="75:160	Melamed demonstrates: If the number of different category symbols is taken into consideration as l, we get O(l2n6) for unlexicalized grammars, and O(l6n10) for lexicalized grammars; however there are some possible optimizations." ></td>
	<td class="line x" title="76:160	20 to synchronous parsing we propose a conceptually very simple asymmetric approach." ></td>
	<td class="line x" title="77:160	As we will show in sec." ></td>
	<td class="line x" title="78:160	4 and 5, this algorithm is both theoretically and practically efficient when applied to sentence pairs for which a word alignment has previously been determined." ></td>
	<td class="line x" title="79:160	The approach is asymmetric in that one of the languages is viewed as the master language, i.e., indexing in parsing is mainly based on this language (the primary index is the string span in L1 as in monolingual parsing)." ></td>
	<td class="line x" title="80:160	The other language contributes a secondary index, which is mainly used to guide parsing in the master language  i.e., certain options are eliminated." ></td>
	<td class="line x" title="81:160	The choice of the master language is in principle arbitrary, but for efficiency considerations it is better to pick the one that has more words without a correspondent." ></td>
	<td class="line x" title="82:160	A way of visualizing correspondence-guided parsing is that standard Earley parsing is applied to L1, with primary indexing by string position; as the chart items are assembled, the synchronous grammar and the information from the word alignment is used to check whether the string in L2 could be generated (essentially using chart-based generation techniques; cf.(Shieber, 1988; Neumann, 1998))." ></td>
	<td class="line x" title="84:160	The index for chart items consists of two components: the string span in L1 and a bit vector for the words in L2 which are covered." ></td>
	<td class="line x" title="85:160	For instance, based on fig." ></td>
	<td class="line x" title="86:160	3, the noun compound Agrarpolitik corresponding to agricultural policy in English will have the index [4, 5], [0, 0, 0, 0, 0, 0, 1, 1] (assuming for illustrative purposes that German is the master language in this case)." ></td>
	<td class="line x" title="87:160	The completion step in correspondence-guided parsing can be formulated as the following single inference rule:11 (4) X1/X2    Y1:r1/Y2:r2 ,[i, j],v, Y1/Y2   ,[j, k],w X1/X2   Y1:r1/Y2:r2  ,[i, k],u where (i) j negationslash= k; (ii) OR(v,w) = u; (iii) w is continuous (i.e. , it contains maximally one subsequence of 1s)." ></td>
	<td class="line x" title="88:160	Condition (iii) excludes discontinuity in passive chart items, i.e., complete constituents; active items 11We use the bold-faced variables v,w,u for bit vectors; the function OR performs bitwise disjunction on the vectors (e.g. , OR([0, 1, 1, 0, 0], [0, 0, 1, 0, 1]) = [0, 1, 1, 0, 1])." ></td>
	<td class="line x" title="89:160	(i.e., partial constituents) may well contain discontinuities." ></td>
	<td class="line x" title="90:160	The success condition for parsing a string with N words in L1 is that a chart item with index [0, N],1 has been found for the start category pair of the grammar." ></td>
	<td class="line x" title="91:160	Words in L2 with no correspondent in L1 (lets call them L1-NILs for short), for example the words at and agricultural in fig." ></td>
	<td class="line x" title="92:160	3,12 can in principle appear between any two words of L1." ></td>
	<td class="line x" title="93:160	Therefore they are represented with a variable empty L1string span like for instance in [i, i], [0, 0, 1, 0, 0]." ></td>
	<td class="line x" title="94:160	At first blush, such L1-NILs seem to introduce an extreme amount of non-determinism into the algorithm." ></td>
	<td class="line x" title="95:160	Note however that due to the continuity assumption for complete constituents, the distribution of the L1-NILs is constrained by the other words in L2." ></td>
	<td class="line x" title="96:160	This is exploited by the following inference rule, which is the only way of integrating L1-NILs into the chart: (5) X1/X2    NIL:0/Y2:r2 ,[i, j],v, NIL/Y2   ,[j, j],w X1/X2   NIL:0/Y2:r2  ,[i, j],u where (i) w is adjacent to v (i.e. , unioning vectors w and v does not lead to more 0-separated 1sequences than v contains already); (ii) OR(v,w) = u. The rule has the effect of finalizing a crosslinguistic constituent (i.e. , rectangle in the twodimensional array) after all the parts that have correspondents in both languages have been found." ></td>
	<td class="line x" title="97:160	13 4 Complexity We assume that the two-dimensional chart is initialized with the correspondences following from a word alignment." ></td>
	<td class="line x" title="98:160	Hence, for each terminal that is non-empty in L1, both components of the index are known." ></td>
	<td class="line x" title="99:160	When two items with known secondary indices are combined with rule (4), the new secondary 12It is conceivable that a word alignment would list agricultural as an additional correspondent for Agrarpolitik; but we use the given alignment for illustrative purposes." ></td>
	<td class="line x" title="100:160	13For instance, the L1-NILs in fig." ></td>
	<td class="line x" title="101:160	3  NIL/at and NIL/agricultural  have to be added to incomplete NP/PP constituent in the L1-string span from 3 to 5, consisting of the Det/Det die/the and the N/N Agrarpolitik/policy." ></td>
	<td class="line x" title="102:160	With two applications of rule (5), the two L1-NILs can be added." ></td>
	<td class="line x" title="103:160	Note that the conditions are met, and that as a result, we will have a continuous NP/PP constituent with index [3, 5], [0, 0, 0, 0, 1, 1, 1, 1], which can be used as a passive item Y1/Y2 in rule (4)." ></td>
	<td class="line x" title="104:160	21 index can be determined by bitwise disjunction of the bit vectors." ></td>
	<td class="line x" title="105:160	This operation is linear in the length of the L2-string (which is of the same order as the length of the L1-string) and has a very small constant factor.14 Since parsing with a simple, nonlexicalized context-free grammar has a time complexity of O(n3) (due to the three free variables for string positions in the completion rule), we get O(n4) for synchronous parsing of sentence pairs without any L1-NILs." ></td>
	<td class="line x" title="106:160	Note that words from L1 without a correspondent in L2 (which we would have to call L2-NILs) do not add to the complexity, so the language with more correspondent-less words can be selected as L1." ></td>
	<td class="line x" title="107:160	For the average complexity of correspondenceguided parsing of sentence pairs without L1-NILs we note an advantage over monolingual parsing: certain hypotheses for complete constituents that would have to be considered when parsing only L1, are excluded because the secondary index reveals a discontinuity." ></td>
	<td class="line x" title="108:160	An example from fig." ></td>
	<td class="line x" title="109:160	3 would be the sequence mussen deshalb, which is adjacent in L1, but doesnt go through as a continuous rectangle when L2 is taken into consideration (hence it cannot be used as a passive item in rule (4))." ></td>
	<td class="line x" title="110:160	The complexity of correspondence-guided parsing is certainly increased by the presence of L1NILs, since with them the secondary index can no longer be uniquely determined." ></td>
	<td class="line x" title="111:160	However, with the adjacency condition ((i) in rule (5)), the number of possible variants in the secondary index is a function of the number of L1-NILs." ></td>
	<td class="line x" title="112:160	Let us say there are m L1-NILs, i.e., the bit vectors contain m elements that we have to flip from 0 to 1 to obtain the final bit vector." ></td>
	<td class="line x" title="113:160	In each application of rule (5) we pick a vector v, with a variable for the leftmost and rightmost L1-NIL element (since this is not fully determined by the primary index)." ></td>
	<td class="line x" title="114:160	By the adjacency condition, 14Note that the operation does not have to be repeated when the completion rule is applied on additional pairs of items with identical indices." ></td>
	<td class="line x" title="115:160	This means that the extra time complexity factor of n doesnt go along with an additional factor of the grammar constant (which we are otherwise ignoring in the present considerations)." ></td>
	<td class="line x" title="116:160	In practical terms this means that changes in the size of the grammar are much more noticable than moving from monolingual parsing to alignment-guided parsing." ></td>
	<td class="line x" title="117:160	An additional advantage is that in an Expectation Maximization approach to grammar induction (with a fixed word alignment), the bit vectors have to be computed only in the first iteration of parsing the training corpus, later iterations are cubic." ></td>
	<td class="line x" title="118:160	either the leftmost or rightmost marks the boundary for adding the additional L1-NIL element NIL/Y2  hence we need only one new variable for the newly shifted boundary among the L1-NILs." ></td>
	<td class="line x" title="119:160	So, in addition to the n4 expense of parsing non-nil words, we get an expense of m3 for parsing the L1-NILs, and we conclude that for unlexicalized synchronous parsing, guided by an initial word alignment the complexity class is O(n4m3) (where n is the total number of words appearing in L1, and m is the number of words appearing in L2, without a correspondent in L1)." ></td>
	<td class="line x" title="120:160	Recall that the complexity for standard synchronous parsing is O(n6)." ></td>
	<td class="line x" title="121:160	Since typically the number of correspondent-less words is significantly lower than the total number of words (at least for one of the two languages), these results are encouraging for medium-to-large-scale grammar learning experiments using a synchronous parsing algorithm." ></td>
	<td class="line x" title="122:160	5 Empirical Evaluation In order to validate the theoretical complexity results empirically, we implemented the algorithm and ran it on sentence pairs from the Europarl parallel corpus." ></td>
	<td class="line x" title="123:160	At the present stage, we are interested in quantitative results on parsing time, rather than qualitative results of parsing accuracy (for which a more extensive training of the rule parameters would be required)." ></td>
	<td class="line x" title="124:160	Implementation." ></td>
	<td class="line x" title="125:160	We did a prototype implementation of the correspondence-guided parsing algorithm in SWI Prolog.15 Chart items are asserted to the knowledge base and efficiently retrieved using indexing by a hash function." ></td>
	<td class="line x" title="126:160	Besides chart construction, the Viterbi algorithm for selecting the most probable analysis has been implemented, but for the current quantitative results only chart construction was relevant." ></td>
	<td class="line x" title="127:160	Sample grammar extraction." ></td>
	<td class="line x" title="128:160	The initial probablistic grammar for our experiments was extracted from a small multitree bank of 140 German/English sentence pairs (short examples from the Europarl corpus)." ></td>
	<td class="line x" title="129:160	The multitree bank was annotated using the MMAX2 tool16 and a specially 15http://www.swi-prolog.org  The advantage of using Prolog is that it is very easy to experiment with various conditions on the inference rules in parsing." ></td>
	<td class="line x" title="130:160	16http://mmax.eml-research.de 22 tailored annotation scheme for flat correspondence structures as described in sec." ></td>
	<td class="line x" title="131:160	2." ></td>
	<td class="line x" title="132:160	A German and English part-of-speech tagger was used to determine word categories; they were mapped to a reduced category set and projected to the syntactic constituents." ></td>
	<td class="line x" title="133:160	To obtain parameters for a probabilistic grammar, we used maximum likelihood estimation from the small corpus, based on a rather simplistic generative model,17 which for each local subtree decides (i) what categories will be the two heads, (ii) how many daughters there will be, and for each nonhead sister (iii) whether it will be a nonterminal or a terminal (and in that case, what category pair), and (iv) in which position relative to the head to place it in both languages." ></td>
	<td class="line x" title="134:160	In order to obtain a realistically-sized grammar, we applied smoothing to all parameters; so effectively, every sequence of terminals/nonterminals of arbitrary length was possible in parsing." ></td>
	<td class="line x" title="135:160	Parsing sentences without NIL words 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 4 5 6 7 8 9 10 number of words (in L1) p a r s i n g t i m e [ s e c ] Monolingual parsing L1 CGSP Figure 5: Comparison of synchronous parsing with and without exploiting constraints from L2 Results." ></td>
	<td class="line x" title="136:160	To validate empirically that the proposed correspondence-guided synchronous parsing approach (CGSP) can effectively exploit L2 as a guide, thereby reducing the search space of L1 parses that have to be considered, we first ran a comparison on sentences without L1-NILs." ></td>
	<td class="line x" title="137:160	The results (average parsing time for Viterbi parsing with the sample grammar) are shown in fig." ></td>
	<td class="line x" title="138:160	5.18 The parser we call monolingual cannot exploit any 17For our learning experiments we intend to use a Maximum Entropy/log-linear model with more features." ></td>
	<td class="line x" title="139:160	18The experiments were run on a 1.4GHz Pentium M processor." ></td>
	<td class="line x" title="140:160	alignment-induced restrictions from L2.19 Note that CGSP takes clearly less time." ></td>
	<td class="line x" title="141:160	Comparison wrt." ></td>
	<td class="line x" title="142:160	# NIL words 0 0.2 0.4 0.6 0.8 1 1.2 1.4 5 6 7 8 9 10 number of words (in L1) p a r s i n g t i m e [ s e c ] 3 L1-NILs, CGSP 2 L1-NILs, CGSP 1 L1-NIL, CGSP no L1-NILs, CGSP monolingual parsing (L1) Figure 6: Synchronous parsing with a growing number of L1-NILs Fig." ></td>
	<td class="line x" title="143:160	6 shows our comparative results for parsing performance on sentences that do contain L1-NILs." ></td>
	<td class="line x" title="144:160	Here too, the theoretical results are corroborated that with a limited number of L1-NILs, the CGSP is still efficient." ></td>
	<td class="line x" title="145:160	The average chart size (in terms of the number of entries) for sentences of length 8 (in L1) was 212 for CGSP (and 80 for monolingual parsing)." ></td>
	<td class="line x" title="146:160	The following comparison shows the effect of L1-NILs (note that the values for 4 and more L1-NILs are based on only one or two cases): (6) Chart size for sentences of length 8 (in L1) Number of L1-NILs 0 1 2 3 4 5 6 Avg." ></td>
	<td class="line x" title="147:160	number of chart items 77 121 175 256 (330) (435) (849) We also simulated a synchronous parser which does not take advantage of a given word alignment (by providing an alignment link between any pair of words, plus the option that any word could be a NULL word)." ></td>
	<td class="line x" title="148:160	For sentences of length 5, this parser took an average time of 22.3 seconds (largely independent of the presence/absence of L1-NILs).20 19The monolingual parser used in this comparison parses two identical copies of the same string synchronously, with a strictly linear alignment." ></td>
	<td class="line x" title="149:160	20While our simulation may be significantly slower than a direct implementation of the algorithm (especially when some of the optimizations discussed in (Melamed, 2003) are taken into account), the fact that it is orders of magnitude slower does in23 Finally, we also ran an experiment in which the continuity condition (condition (iii) in rule (4)) was deactivated, i.e., complete constituents were allowed to be discontinuous in one of the languages." ></td>
	<td class="line x" title="150:160	The results in (7) underscore the importance of this condition  leaving it out leads to a tremendous increase in parsing time." ></td>
	<td class="line x" title="151:160	(7) Average parsing time in seconds with and without continuity condition Sentence length (with no L1NILs) 4 5 6 Avg." ></td>
	<td class="line x" title="152:160	parsing time with CGSP (incl." ></td>
	<td class="line x" title="153:160	continuity condition) 0.005 0.012 0.026 Avg." ></td>
	<td class="line x" title="154:160	parsing time without the continuity condition 0.035 0.178 1.025 6 Conclusion We proposed a conceptually simple, yet efficient algorithm for synchronous parsing in a context where a word alignment can be assumed as given  for instance in a bootstrapping learning scenario." ></td>
	<td class="line x" title="155:160	One of the two languages in synchronous parsing acts as the master language, providing the primary string span index, which is used as in classical Earley parsing." ></td>
	<td class="line x" title="156:160	The second language contributes a bit vector as a secondary index, inspired by work on chart generation." ></td>
	<td class="line x" title="157:160	Continuity assumptions make it possible to constrain the search space significantly, to the point that synchronous parsing for sentence pairs with few NULL words (which lack correspondents) may be faster than standard monolingual parsing." ></td>
	<td class="line x" title="158:160	We discussed the complexity both theoretically and provided a quantitative evaluation based on a prototype implementation." ></td>
	<td class="line x" title="159:160	The study we presented is part of the longer-term PTOLEMAIOS project." ></td>
	<td class="line x" title="160:160	The next step is to apply the synchronous parsing algorithm with probabilistic synchronous grammars in grammar induction experiments on parallel corpora." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N06-1001
Capitalizing Machine Translation
Wang, Wei;Knight, Kevin;Marcu, Daniel;"></td>
	<td class="line x" title="1:204	Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 18, New York, June 2006." ></td>
	<td class="line x" title="2:204	c2006 Association for Computational Linguistics Capitalizing Machine Translation Wei Wang and Kevin Knight and Daniel Marcu Language Weaver, Inc. 4640 Admiralty Way, Suite 1210 Marina del Rey, CA, 90292 {wwang, kknight, dmarcu}@languageweaver.com Abstract We present a probabilistic bilingual capitalization model for capitalizing machine translation outputs using conditional random fields." ></td>
	<td class="line x" title="3:204	Experiments carried out on three language pairs and a variety of experiment conditions show that our model significantly outperforms a strong monolingual capitalization model baseline, especially when working with small datasets and/or European language pairs." ></td>
	<td class="line x" title="4:204	1 Introduction Capitalization is the process of recovering case information for texts in lowercase." ></td>
	<td class="line x" title="5:204	It is also called truecasing (Lita et al. , 2003)." ></td>
	<td class="line x" title="6:204	Usually, capitalization itself tries to improve the legibility of texts." ></td>
	<td class="line x" title="7:204	It, however, can affect the word choice or order when interacting with other models." ></td>
	<td class="line x" title="8:204	In natural language processing, a good capitalization model has been shown useful for tasks like name entity recognition, automatic content extraction, speech recognition, modern word processors, and machine translation (MT)." ></td>
	<td class="line x" title="9:204	Capitalization can be viewed as a sequence labeling process." ></td>
	<td class="line x" title="10:204	The input to this process is a sentence in lowercase." ></td>
	<td class="line x" title="11:204	For each lowercased word in the input sentence, we have several available capitalization tags: initial capital (IU), all uppercase (AU), all lowercase (AL), mixed case (MX), and all having no case (AN)." ></td>
	<td class="line x" title="12:204	The output of capitalization is a capitalization tag sequence." ></td>
	<td class="line x" title="13:204	Associating a tag in the output with the corresponding lowercased word in the input results in a surface form of the word." ></td>
	<td class="line x" title="14:204	For example, we can tag the input sentence click ok to save your changes to /home/doc. into click IU ok AU to AL save AL your AL changes AL to AL /home/doc MX." ></td>
	<td class="line x" title="15:204	AN, getting the surface form Click OK to save your changes to /home/DOC .." ></td>
	<td class="line x" title="16:204	A capitalizer is a tagger that recovers the capitalization tag for each input lowercased word, outputting a well-capitalized sentence." ></td>
	<td class="line x" title="17:204	Since each lowercased word can have more than one tag, and associating a tag with a lowercased word can result in more than one surface form (e.g. , /home/doc MX can be either /home/DOC or /home/Doc), we need a capitalization model to solve the capitalization ambiguities." ></td>
	<td class="line x" title="18:204	For example, Lita et al.(2003) use a trigram language model estimated from a corpus with case information; Chelba and Acero (2004) use a maximum entropy Markov model (MEMM) combining features involving words and their cases." ></td>
	<td class="line x" title="20:204	Capitalization models presented in most previous approaches are monolingual because the models are estimated only from monolingual texts." ></td>
	<td class="line x" title="21:204	However, for capitalizing machine translation outputs, using only monolingual capitalization models is not enough." ></td>
	<td class="line x" title="22:204	For example, if the sentence click ok to save your changes to /home/doc . in the above example is the translation of the French sentence CLIQUEZ SUR OK POUR ENREGISTRER VOS MODIFICATIONS DANS /HOME/DOC ., the correct capitalization result should probably be CLICK OK TO SAVE YOUR CHANGES TO /HOME/DOC ., where all words are in all upper-case." ></td>
	<td class="line x" title="23:204	Without looking into the case 1 of the MT input, we can hardly get the correct capitalization result." ></td>
	<td class="line x" title="24:204	Although monolingual capitalization models in previous work can apply to MT output, a bilingual model is more desirable." ></td>
	<td class="line x" title="25:204	This is because MT outputs usually strongly preserve case from the input, and because monolingual capitalization models do not always perform as well on badly translated text as on well-formed syntactic texts." ></td>
	<td class="line x" title="26:204	In this paper, we present a bilingual capitalization model for capitalizing machine translation outputs using conditional random fields (CRFs) (Lafferty et al. , 2001)." ></td>
	<td class="line x" title="27:204	This model exploits case information from both the input sentence (source) and the output sentence (target) of the MT system." ></td>
	<td class="line x" title="28:204	We define a series of feature functions to incorporate capitalization knowledge into the model." ></td>
	<td class="line x" title="29:204	Experimental results are shown in terms of BLEU scores of a phrase-based SMT system with the capitalization model incorporated, and in terms of capitalization precision." ></td>
	<td class="line x" title="30:204	Experiments are performed on both French and English targeted MT systems with large-scale training data." ></td>
	<td class="line x" title="31:204	Our experimental results show that the CRF-based bilingual capitalization model performs better than a strong baseline capitalizer that uses a trigram language model." ></td>
	<td class="line x" title="32:204	2 Related Work A simple capitalizer is the 1-gram tagger: the case of a word is always the most frequent one observed in training data, with the exception that the sentenceinitial word is always capitalized." ></td>
	<td class="line x" title="33:204	A 1-gram capitalizer is usually used as a baseline for capitalization experiments (Lita et al. , 2003; Kim and Woodland, 2004; Chelba and Acero, 2004)." ></td>
	<td class="line x" title="34:204	Lita et al.(2003) view capitalization as a lexical ambiguity resolution problem, where the lexical choices for each lowercased word happen to be its different surface forms." ></td>
	<td class="line x" title="36:204	For a lowercased sentence e, a trigram language model is used to find the best capitalization tag sequence T that maximizes p(T,e) = p(E), resulting in a case-sensitive sentence E. Besides local trigrams, sentence-level contexts like sentence-initial position are employed as well." ></td>
	<td class="line x" title="37:204	Chelba and Acero (2004) frame capitalization as a sequence labeling problem, where, for each lowMT Decoder Train Monolingual Capitalization Model Monolingual Cap Model Capitalization Lower Case Lower Case f Lower Case e Finput Eoutput Train Translation Model Train Language Model Translation Model Languagel Model {F} {E} {f} {e} Figure 1: The monolingual capitalization scheme employed by most statistical MT systems." ></td>
	<td class="line x" title="38:204	ercased sentence e, they find the label sequence T that maximizes p(Tje)." ></td>
	<td class="line x" title="39:204	They use a maximum entropy Markov model (MEMM) to combine features of words, cases and context (i.e. , tag transitions)." ></td>
	<td class="line x" title="40:204	Gale et al.(1994) report good results on capitalizing 100 words." ></td>
	<td class="line x" title="42:204	Mikheev (1999) performs capitalization using simple positional heuristics." ></td>
	<td class="line x" title="43:204	3 Monolingual Capitalization Scheme Translation and capitalization are usually performed in two successive steps because removing case information from the training of translation models substantially reduces both the source and target vocabulary sizes." ></td>
	<td class="line x" title="44:204	Smaller vocabularies lead to a smaller translation model with fewer parameters to learn." ></td>
	<td class="line x" title="45:204	For example, if we do not remove the case information, we will have to deal with at least nine probabilities for the English-French word pair (click, cliquez)." ></td>
	<td class="line x" title="46:204	This is because either click or cliquez can have at least three tags (IU, AL, AU), and thus three surface forms." ></td>
	<td class="line x" title="47:204	A smaller translation model requires less training data, and can be estimated more accurately than otherwise from the same amount of training data." ></td>
	<td class="line x" title="48:204	A smaller translation model also means less memory usage." ></td>
	<td class="line x" title="49:204	Most statistical MT systems employ the monolingual capitalization scheme as shown in Figure 1." ></td>
	<td class="line x" title="50:204	In this scheme, the translation model and the target language model are trained from the lowercased corpora." ></td>
	<td class="line x" title="51:204	The capitalization model is trained from the case-sensitive target corpus." ></td>
	<td class="line x" title="52:204	In decoding, we first turn input into lowercase, then use the decoder to generate the lowercased translation, and finally ap2 HYDRAULIC HEADER TILT CYLINDER KIT Kit de verin dinclinaison hydraulique de la plate-forme haut-parleur avant droit + HAUT-PARLEUR AVANT DROIT + Seat Controls, Standard COMMANDES DU SIGE, STANDARD loading a saved legend Chargement dune legende sauvegarde Table 1: Errors made by monolingual capitalization model." ></td>
	<td class="line x" title="53:204	Each row contains a pair of MT input and MT output." ></td>
	<td class="line x" title="54:204	MT Decoder CapitalizationBilingual Cap Model Train Bilingual Cap Model alignment Word/Phrase Aligner f Lower Case e Finput Eoutput {F} {E} Figure 2: A bilingual capitalization scheme." ></td>
	<td class="line x" title="55:204	ply the capitalization model to recover the case of the decoding output." ></td>
	<td class="line x" title="56:204	The monolingual capitalization scheme makes many errors as shown in Table 1." ></td>
	<td class="line x" title="57:204	Each cell in the table contains the MT-input and the MT-output." ></td>
	<td class="line x" title="58:204	These errors are due to the capitalizer does not have access to the source sentence." ></td>
	<td class="line x" title="59:204	Regardless, estimating mixed-cased translation models, however, is a very interesting topic and worth future study." ></td>
	<td class="line x" title="60:204	4 Bilingual Capitalization Model 4.1 The Model Our probabilistic bilingual capitalization model exploits case information from both the input sentence to the MT system and the output sentence from the system (see Figure 2)." ></td>
	<td class="line x" title="61:204	An MT system translates a capitalized sentence F into a lowercased sentence e. A statistical MT system can also provide the alignment A between the input F and the output e; for example, a statistical phrase-based MT system could provide the phrase boundaries in F and e, and also the alignment between the phrases.1 1We shall explain our capitalization model within the phrase-based SMT framework, the model, however, could be OK Click OK Cliquez E F Ei Fj Figure 3: Alignment graph." ></td>
	<td class="line x" title="62:204	Brackets mean phrase boundaries." ></td>
	<td class="line x" title="63:204	The bilingual capitalization algorithm recovers the capitalized sentence E from e, according to the input sentence F, and the alignment A. Formally, we look for the best capitalized sentence E such that E = arg maxEGEN(e)p(EjF,A) (1) where GEN(e) is a function returning the set of possible capitalized sentences consistent with e. Notice that e does not appear in p(EjF,A) because we can uniquely obtain e from E. p(EjF,A) is the capitalization model of concern in this paper.2 To further decompose the capitalization model p(EjF,A), we make some assumptions." ></td>
	<td class="line x" title="64:204	As shown in Figure 3, input sentence F, capitalized output E, and their alignment can be viewed as a graph." ></td>
	<td class="line x" title="65:204	Vertices of the graph correspond to words in F and E. An edge connecting a word in F and a word in E corresponds to a word alignment." ></td>
	<td class="line x" title="66:204	An edge between two words in E represents the dependency between them captured by monolingual n-gram language models." ></td>
	<td class="line x" title="67:204	We also assume that both E and F have phrase boundaries available (denoted by the square brackets), and that A is the phrase alignment." ></td>
	<td class="line x" title="68:204	In Figure 3, Fj is the j-th phrase of F, Ei is the i-th phrase of E, and they align to each other." ></td>
	<td class="line x" title="69:204	We do not require a word alignment; instead we find it reasonable to think that a word in Ei can be aligned to any adapted to syntax-based machine translation, too." ></td>
	<td class="line oc" title="70:204	To this end, the translational correspondence is described within a translation rule, i.e., (Galley et al. , 2004) (or a synchronous production), rather than a translational phrase pair; and the training data will be derivation forests, instead of the phrase-aligned bilingual corpus." ></td>
	<td class="line x" title="71:204	2The capitalization model p(E|F, A) itself does not require the existence of e. This means that in principle this model can also be viewed as a capitalized translation model that performs translation and capitalization in an integrated step." ></td>
	<td class="line x" title="72:204	In our paper, however, we consider the case where the machine translation output e is given, which is reflected by the the fact that GEN(e) takes e as input in Formula 1." ></td>
	<td class="line x" title="73:204	3 word in Fj." ></td>
	<td class="line x" title="74:204	A probabilistic model defined on this graph is a Conditional Random Field." ></td>
	<td class="line x" title="75:204	Therefore, it is natural to formulate the bilingual capitalization model using CRFs:3 p(E|F, A) = 1Z(F, A, ) exp IX i=1 ifi(E, F, A) !" ></td>
	<td class="line x" title="76:204	(2) where Z(F, A, ) = X EGEN(e) exp IX i=1 ifi(E,F, A) !" ></td>
	<td class="line x" title="77:204	(3) fi(E,F,A),i = 1I are the I features, and  = (1,,I) is the feature weight vector." ></td>
	<td class="line x" title="78:204	Based on this capitalization model, the decoder in the capitalizer looks for the best E such that E = arg maxEGEN(e,F) Isummationdisplay i=1 ifi(E,F,A) (4) 4.2 Parameter Estimation Following Roark et al.(2004), Lafferty et al.(2001) and Chen and Rosenfeld (1999), we are looking for the set of feature weights  maximizing the regularized log-likelihood LLR() of the training data fE(n),F (n),A(n),n = 1,,Ng." ></td>
	<td class="line x" title="81:204	LLR() = NX n=1 log p  E(n)|F (n), A(n)   |||| 2 22 (5) The second term at the right-hand side of Formula 5 is a zero-mean Gaussian prior on the parameters." ></td>
	<td class="line x" title="82:204	 is the variance of the Gaussian prior dictating the cost of feature weights moving away from the mean  a smaller value of  keeps feature weights closer to the mean." ></td>
	<td class="line x" title="83:204	 can be determined by linear search on development data.4 The use of the Gaussian prior term in the objective function has been found effective in avoiding overfitting, leading to consistently better results." ></td>
	<td class="line x" title="84:204	The choice of LLR as an objective function can be justified as maximum a-posteriori (MAP) training within a Bayesian approach (Roark et al. , 2004)." ></td>
	<td class="line x" title="85:204	3We chose CRFs over other sequence labeling models (i.e. MEMM) because CRFs have no label bias and we do not need to compute the partition function during decoding." ></td>
	<td class="line x" title="86:204	4In our experiment, we use an empirical value  = 0.5 as in (Roark et al. , 2004)." ></td>
	<td class="line x" title="87:204	4.3 Feature Functions We define features based on the alignment graph in Figure 3." ></td>
	<td class="line x" title="88:204	Each feature function is defined on a word." ></td>
	<td class="line x" title="89:204	Monolingual language model feature." ></td>
	<td class="line x" title="90:204	The monolingual LM feature of word Ei is the logarithm of the probability of the n-gram ending at Ei: fLM(Ei,F,A) = log p(EijEi1,,Ein+1) (6) p should be appropriately smoothed such that it never returns zero." ></td>
	<td class="line x" title="91:204	Capitalized translation model feature." ></td>
	<td class="line x" title="92:204	Suppose E phrase Click OK is aligned to F phrase Cliquez OK." ></td>
	<td class="line x" title="93:204	The capitalized translation model feature of Click is computed as log p(ClickjCliquez)+log p(ClickjOK)." ></td>
	<td class="line x" title="94:204	Click is assumed to be aligned to any word in the F phrase." ></td>
	<td class="line x" title="95:204	The larger the probability that Click is translated from an F word, i.e., Cliquez, the more chances that Click preserves the case of Cliquez." ></td>
	<td class="line x" title="96:204	Formally, for word Ei, and an aligned phrase pair El and Fm, where Ei 2 El, the capitalized translation model feature of Ei is fcapt1(Ei,F,A) = log | Fm|summationdisplay k=1 p(Eij Fm,k) (7) p(Eij Fm,k) is the capitalized translation table." ></td>
	<td class="line x" title="97:204	It needs smoothing to avoid returning zero, and is estimated from a word-aligned bilingual corpus." ></td>
	<td class="line x" title="98:204	Capitalization tag translation feature." ></td>
	<td class="line x" title="99:204	The feature value of E word Click aligning to F phrase Cliquez OK is log p(IUjIU)p(clickjcliquez) + log p(IUjAU)p(clickjok)." ></td>
	<td class="line x" title="100:204	We see that this feature is less specific than the capitalized translation model feature." ></td>
	<td class="line x" title="101:204	It is computed in terms of the tag translation probability and the lowercased word translation probability." ></td>
	<td class="line x" title="102:204	The lowercased word translation probability, i.e., p(clickjok), is used to decide how much of the tag translation probability, i.e., p(IUjAU), will contribute to the final decision." ></td>
	<td class="line x" title="103:204	The smaller the word translation probability, i.e., p(clickjok), is, the smaller the chance that the surface form of click 4 preserves case from that of ok." ></td>
	<td class="line x" title="104:204	Formally, this feature is defined as fcaptagt1(Ei,F,A) = log | fm|summationdisplay k=1 p(eij fm,k) p((Ei)j( Fm,k)) (8) p(eij fm,k) is the t-table over lowercased word pairs, which is the usual t-table in a SMT system." ></td>
	<td class="line x" title="105:204	p((Ei)j( Fm,k)) is the probability of a target capitalization tag given a source capitalization tag and can be easily estimated from a word-aligned bilingual corpus." ></td>
	<td class="line x" title="106:204	This feature attempts to help when fcapt1 fails (i.e. , the capitalized word pair is unseen)." ></td>
	<td class="line x" title="107:204	Smoothing is also applied to both p(eij fm,k) and p((Ei)j( Fm,k)) to handle unseen words (or word pairs)." ></td>
	<td class="line x" title="108:204	Upper-case translation feature." ></td>
	<td class="line x" title="109:204	Word Ei is in all upper case if all words in the corresponding F phrase Fm are in upper case." ></td>
	<td class="line x" title="110:204	Although this feature can also be captured by the capitalization tag translation feature in the case where an AU tag in the input sentence is most probably preserved in the output sentence, we still define it to emphasize its effect." ></td>
	<td class="line x" title="111:204	This feature aims, for example, to translate ABC XYZ into UUU VVV even if all words are unseen." ></td>
	<td class="line x" title="112:204	Initial capitalization feature." ></td>
	<td class="line x" title="113:204	An E word is initially capitalized if it is the first word that contains letters in the E sentence." ></td>
	<td class="line x" title="114:204	For example, for sentence  Please click the button that starts with a bullet, the initial capitalization feature value of word please is 1 because   does not contain a letter." ></td>
	<td class="line x" title="115:204	Punctuation feature template." ></td>
	<td class="line x" title="116:204	An E word is initially capitalized if it follows a punctuation mark." ></td>
	<td class="line x" title="117:204	Non-sentence-ending punctuation marks like commas will usually get negative weights." ></td>
	<td class="line x" title="118:204	As one can see, our features are coarse-grained (e.g. , the language model feature)." ></td>
	<td class="line x" title="119:204	In contrast, Kim and Woodland (2004) and Roark et al.(2004) use fine-grained features." ></td>
	<td class="line x" title="121:204	They treat each n-gram as a feature for, respectively, monolingual capitalization and language modeling." ></td>
	<td class="line x" title="122:204	Feature weights tuned at a fine granularity may lead to better accuracy, but they require much more training data, and result in much slower training speed, especially for large-scale learning problems." ></td>
	<td class="line x" title="123:204	Coarse-grained features enable us to efficiently get the feature values from a very large training corpus, and quickly tune the weights on small development sets." ></td>
	<td class="line x" title="124:204	For example, we can train a bilingual capitalization model on a 70 million-word corpus in several hours with the coarse-grained features presented above, but in several days with fine-grained n-gram count features." ></td>
	<td class="line x" title="125:204	4.4 The GEN Function Function GEN generates the set of case-sensitive candidates from a lowercased token." ></td>
	<td class="line x" title="126:204	For example GEN(mt) = fmt,mT,Mt,MTg." ></td>
	<td class="line x" title="127:204	The following heuristics can be used to reduce the range of GEN. The returned set of GEN on a lower-cased token w is the union of: (i) fw,AU(w),IU(w)g, (ii) fvjv is seen in training data and AL(v) = wg, and (iii) f Fm,kjAL( Fm,k) = AL(w)g. The heuristic (iii) is designed to provide more candidates for w when it is translated from a very strange input word Fm,k in the F phrase Fm that is aligned to the phrase that w is in." ></td>
	<td class="line x" title="128:204	This heuristic creates good capitalization candidates for the translation of URLs, file names, and file paths." ></td>
	<td class="line x" title="129:204	5 Generating Phrase-Aligned Training Data Training the bilingual capitalization model requires a bilingual corpus with phrase alignments, which are usually produced from a phrase aligner." ></td>
	<td class="line x" title="130:204	In practice, the task of phrase alignment can be quite computationally expensive as it requires to translate the entire training corpus; also a phrase aligner is not always available." ></td>
	<td class="line x" title="131:204	We therefore generate the training data using a nave phrase aligner (NPA) instead of resorting to a real one." ></td>
	<td class="line x" title="132:204	The input to the NPA is a word-aligned bilingual corpus." ></td>
	<td class="line x" title="133:204	The NPA stochastically chooses for each sentence pair one segmentation and phrase alignment that is consistent with the word alignment." ></td>
	<td class="line x" title="134:204	An aligned phrase pair is consistent with the word alignment if neither phrase contains any word aligning to a word outside the other phrase (Och and Ney, 2004)." ></td>
	<td class="line x" title="135:204	The NPA chunks the source sentence into phrases according to a probabilistic distribution over source phrase lengths." ></td>
	<td class="line x" title="136:204	This distribution can be obtained from the trace output of a phrase-based MT 5 Entire Corpus (#W) Test-BLEU Languages Training Dev Test-Prec." ></td>
	<td class="line x" title="137:204	(#sents) EF (IT) 62M 13K 15K 763 FE (news) 144M 11K 22K 241 CE (news) 50M 8K 17K 919 Table 2: Corpora used in experiments." ></td>
	<td class="line x" title="138:204	decoder on a small development set." ></td>
	<td class="line x" title="139:204	The NPA has to retry if the current source phrase cannot find any consistent target phrase." ></td>
	<td class="line x" title="140:204	Unaligned target words are attached to the left phrase." ></td>
	<td class="line x" title="141:204	Heuristics are employed to prevent the NPA from not coming to a solution." ></td>
	<td class="line x" title="142:204	Obviously, the NPA is a special case of the phrase extractor in (Och and Ney, 2004) in that it considers only one phrase alignment rather than all possible ones." ></td>
	<td class="line x" title="143:204	Unlike a real phrase aligner, the NPA need not wait for the training of the translation model to finish, making it possible for parallelization of translation model training and capitalization model training." ></td>
	<td class="line x" title="144:204	However, we believe that a real phrase aligner may make phrase alignment quality higher." ></td>
	<td class="line x" title="145:204	6 Experiments 6.1 Settings We conducted capitalization experiments on three language pairs: English-to-French (E!F) with a bilingual corpus from the Information Technology (IT) domain; French-to-English (F!E) with a bilingual corpus from the general news domain; and Chinese-to-English (C!E) with a bilingual corpus from the general news domain as well." ></td>
	<td class="line x" title="146:204	Each language pair comes with a training corpus, a development corpus and two test sets (see Table 2)." ></td>
	<td class="line x" title="147:204	TestPrecision is used to test the capitalization precision of the capitalizer on well-formed sentences drawn from genres similar to those used for training." ></td>
	<td class="line x" title="148:204	TestBLEU is used to assess the impact of our capitalizer on end-to-end translation performance; in this case, the capitalizer may operate on ungrammatical sentences." ></td>
	<td class="line x" title="149:204	We chose to work with these three language pairs because we wanted to test our capitalization model on both English and French target MT systems and in cases where the source language has no case information (such as in Chinese)." ></td>
	<td class="line x" title="150:204	We estimated the feature functions, such as the log probabilities in the language model, from the training set." ></td>
	<td class="line x" title="151:204	Kneser-Ney smoothing (Kneser and Ney, 1995) was applied to features fLM, fcapt1, and fcaptagt1." ></td>
	<td class="line x" title="152:204	We trained the feature weights of the CRF-based bilingual capitalization model using the development set." ></td>
	<td class="line x" title="153:204	Since estimation of the feature weights requires the phrase alignment information, we efficiently applied the NPA on the development set." ></td>
	<td class="line x" title="154:204	We employed two LM-based capitalizers as baselines for performance comparison: a unigram-based capitalizer and a strong trigram-based one." ></td>
	<td class="line x" title="155:204	The unigram-based capitalizer is the usual baseline for capitalization experiments in previous work." ></td>
	<td class="line x" title="156:204	The trigram-based baseline is similar to the one in (Lita et al. , 2003) except that we used Kneser-Ney smoothing instead of a mixture." ></td>
	<td class="line x" title="157:204	A phrase-based SMT system (Marcu and Wong, 2002) was trained on the bitext." ></td>
	<td class="line x" title="158:204	The capitalizer was incorporated into the MT system as a postprocessing module  it capitalizes the lowercased MT output." ></td>
	<td class="line x" title="159:204	The phrase boundaries and alignments needed by the capitalizer were automatically inferred as part of the decoding process." ></td>
	<td class="line x" title="160:204	6.2 BLEU and Precision We measured the impact of our capitalization model in the context of an end-to-end MT system using BLEU (Papineni et al. , 2001)." ></td>
	<td class="line x" title="161:204	In this context, the capitalizer operates on potentially ill-formed, MTproduced outputs." ></td>
	<td class="line x" title="162:204	To this end, we first integrated our bilingual capitalizer into the phrase-based SMT system as a postprocessing module." ></td>
	<td class="line x" title="163:204	The decoder of the MT system was modified to provide the capitalizer with the case-preserved source sentence, the lowercased translation, and the phrase boundaries and their alignments." ></td>
	<td class="line x" title="164:204	Based on this information, our bilingual capitalizer recovers the case information of the lowercased translation, outputting a capitalized target sentence." ></td>
	<td class="line x" title="165:204	The case-restored machine translations were evaluated against the target test-BLEU set." ></td>
	<td class="line x" title="166:204	For comparison, BLEU scores were also computed for an MT system that used the two LM-based baselines." ></td>
	<td class="line x" title="167:204	We also assessed the performance of our capitalizer on the task of recovering case information for well-formed grammatical texts." ></td>
	<td class="line x" title="168:204	To this end, we used the precision metric that counted the number of cor6 rectly capitalized words produced by our capitalizer on well-formed, lowercased input precision = #correctly capitalized words#total words (9) To obtain the capitalization precision, we implemented the capitalizer as a standalone program." ></td>
	<td class="line x" title="169:204	The inputs to the capitalizer were triples of a casepreserved source sentence, a lowercased target sentence, and phrase alignments between them." ></td>
	<td class="line x" title="170:204	The output was the case-restored version of the target sentence." ></td>
	<td class="line x" title="171:204	In this evaluation scenario, the capitalizer output and the reference differ only in case information  word choices and word orders between them are the same." ></td>
	<td class="line x" title="172:204	Testing was conducted on TestPrecision." ></td>
	<td class="line x" title="173:204	We applied the NPA to the Test-Precision set to obtain the phrases and their alignments because they were needed to trigger the features in testing." ></td>
	<td class="line x" title="174:204	We used a Test-Precision set that was different from the Test-BLEU set because word alignments were by-products only of training of translation models on the MT training data and we could not put the Test-BLEU set into the MT training data." ></td>
	<td class="line x" title="175:204	Rather than implementing a standalone word aligner, we randomly divided the MT training data into three non-overlapping sets: Test-Precision set, CRF capitalizer training set and dev set." ></td>
	<td class="line x" title="176:204	6.3 Results The performance comparisons between our CRFbased capitalizer and the two LM-based baselines are shown in Table 3 and Table 4." ></td>
	<td class="line x" title="177:204	Table 3 shows the BLEU scores, and Table 4 shows the precision." ></td>
	<td class="line x" title="178:204	The BLEU upper bounds indicate the ceilings that a perfect capitalizer can reach, and are computed by ignoring the case information in both the capitalizer outputs and the reference." ></td>
	<td class="line x" title="179:204	Obviously, the precision upper bounds for all language pairs are 100%." ></td>
	<td class="line x" title="180:204	The precision and end-to-end BLEU based comparisons show that, for European language pairs, the CRF-based bilingual capitalization model outperforms significantly the strong LM-based baseline." ></td>
	<td class="line x" title="181:204	We got more than one BLEU point improvement on the MT translation between English and French, a 34% relative reduction in capitalization error rate for the French-to-English language pair, and a 42% relative error rate reduction for the English-to-French language pair." ></td>
	<td class="line x" title="182:204	These results show that source language information provides significant help for capitalizing machine translation outputs." ></td>
	<td class="line x" title="183:204	The results also show that when the source language does not have case, as in Chinese, the bilingual model equals a monolingual one." ></td>
	<td class="line x" title="184:204	The BLEU difference between the CRF-based capitalizer and the trigram one were larger than the precision difference." ></td>
	<td class="line x" title="185:204	This indicates that the CRF-based capitalizer performs much better on nongrammatical texts that are generated from an MT system due to the bilingual feature of the CRF capitalizer." ></td>
	<td class="line x" title="186:204	6.4 Effect of Training Corpus Size The experiments above were carried out on large data sets." ></td>
	<td class="line x" title="187:204	We also conducted experiments to examine the effect of the training corpus size on capitalization precision." ></td>
	<td class="line x" title="188:204	Figure 4 shows the effects." ></td>
	<td class="line x" title="189:204	The experiment was performed on the E!F corpus." ></td>
	<td class="line x" title="190:204	The bilingual capitalizer performed significantly better when the training corpus size was small (e.g. , under 8 million words)." ></td>
	<td class="line x" title="191:204	This is common in many domains: when the training corpus size increases, the difference between the two capitalizers decreases." ></td>
	<td class="line x" title="192:204	7 Conclusions In this paper, we have studied how to exploit bilingual information to improve capitalization performance on machine translation output, and evaluated the improvement over traditional methods that use only monolingual language models." ></td>
	<td class="line x" title="193:204	We first presented a probabilistic bilingual capitalization model for capitalizing machine translation outputs using conditional random fields." ></td>
	<td class="line x" title="194:204	This model exploits bilingual capitalization knowledge as well as monolingual information." ></td>
	<td class="line x" title="195:204	We defined a series of feature functions to incorporate capitalization knowledge into the model." ></td>
	<td class="line x" title="196:204	We then evaluated our CRF-based bilingual capitalization model both on well-formed texts in terms of capitalization precision, and on possibly ungrammatical end-to-end machine translation outputs in terms of BLEU scores." ></td>
	<td class="line x" title="197:204	Experiments were performed on both French and English target MT systems with large-scale training data." ></td>
	<td class="line x" title="198:204	Our experimental results showed that the CRF-based bilingual cap7 BLEU Scores Translation UnigramCapitalizer TrigramCapitalizer CRF-basedCapitalizer UpperBound FE 24.96 26.73 27.92 28.85 EF 32.63 34.66 36.10 36.17 CE 23.81 25.92 25.89 Table 3: Impact of CRF-based capitalizer on end-to-end translation performance compared with two LM-based baselines." ></td>
	<td class="line x" title="199:204	Capitalization Precision (%) Translation Unigramcapitalizer Trigramcapitalizer CRF-basedcapitalizer FE 94.03 98.79 99.20 EF 91.52 98.47 99.11 CE 90.77 96.40 96.76 Table 4: Impact of CRF-based capitalizer on capitalization precision compared with two LM-based baselines." ></td>
	<td class="line x" title="200:204	100 99 98 97 96 95 94 93 92 64.032.016.08.04.02.01.00.50.20.1 Precision (x%) Training Corpus Size (MWs) CRF-based capitalizer LM-based capitalizer Figure 4: Capitalization precision with respect to size of training corpus." ></td>
	<td class="line x" title="201:204	LM-based capitalizer refers to the trigram-based one." ></td>
	<td class="line x" title="202:204	Results were on EF corpus." ></td>
	<td class="line x" title="203:204	italization model performs significantly better than a strong baseline, monolingual capitalizer that uses a trigram language model." ></td>
	<td class="line x" title="204:204	In all experiments carried out at Language Weaver with customer (or domain specific) data, MT systems trained on lowercased data coupled with the CRF bilingual capitalizer described in this paper consistently outperformed both MT systems trained on lowercased data coupled with a strong monolingual capitalizer and MT systems trained on mixedcased data." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N06-1031
Relabeling Syntax Trees To Improve Syntax-Based Machine Translation Quality
Huang, Bryant;Knight, Kevin;"></td>
	<td class="line x" title="1:157	Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 240247, New York, June 2006." ></td>
	<td class="line x" title="2:157	c2006 Association for Computational Linguistics Relabeling Syntax Trees to Improve Syntax-Based Machine Translation Quality Bryant Huang Language Weaver, Inc. 4640 Admiralty Way, Suite 1210 Marina del Rey, CA 90292 bhuang@languageweaver.com Kevin Knight Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292 knight@isi.edu Abstract We identify problems with the Penn Treebank that render it imperfect for syntaxbased machine translation and propose methods of relabeling the syntax trees to improve translation quality." ></td>
	<td class="line x" title="3:157	We develop a system incorporating a handful of relabeling strategies that yields a statistically signi cant improvement of 2.3 BLEU points over a baseline syntax-based system." ></td>
	<td class="line x" title="4:157	1 Introduction Recent work in statistical machine translation (MT) has sought to overcome the limitations of phrasebased models (Marcu and Wong, 2002; Koehn et al. , 2003; Och and Ney, 2004) by making use of syntactic information." ></td>
	<td class="line x" title="5:157	Syntax-based MT offers the potential advantages of enforcing syntaxmotivated constraints in translation and capturing long-distance/non-contiguous dependencies." ></td>
	<td class="line x" title="6:157	Some approaches have used syntax at the core (Wu, 1997; Alshawi et al. , 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003; Hearne and Way, 2003; Melamed, 2004) while others have integrated syntax into existing phrase-based frameworks (Xia and McCord, 2004; Chiang, 2005; Collins et al. , 2005; Quirk et al. , 2005)." ></td>
	<td class="line oc" title="7:157	In this work, we employ a syntax-based model that applies a series of tree/string (xRS) rules (Galley et al. , 2004; Graehl and Knight, 2004) to a source language string to produce a target language phrase structure tree." ></td>
	<td class="line x" title="8:157	Figure 1 exempli es the translation process, which is called a derivation, from Chinese into English." ></td>
	<td class="line x" title="9:157	The source string to translate (a0a0a0a2a1a1a1 a3a3a3a5a4a4a4a7a6a6a6a9a8a8a8a11a10a10a10 )." ></td>
	<td class="line x" title="10:157	is shown at the top left." ></td>
	<td class="line x" title="11:157	Rule 1 replaces the Chinese word a4a4a4a2a6a6a6 (shaded) with the English NP-C police." ></td>
	<td class="line x" title="12:157	Rule 2 then builds a VP over thea3a3a3 NP-Ca8a8a8a12a10a10a10 sequence." ></td>
	<td class="line x" title="13:157	Next,a0a0a0a11a1a1a1 is translated as the NP-C the gunman by rule 3." ></td>
	<td class="line x" title="14:157	Finally, rule 4 combines the sequence of NP-C VP." ></td>
	<td class="line x" title="15:157	into an S, denoting a complete tree." ></td>
	<td class="line x" title="16:157	The yield of this tree gives the target translation: the gunman was killed by police . The Penn English Treebank (PTB) (Marcus et al. , 1993) is our source of syntactic information, largely due to the availability of reliable parsers." ></td>
	<td class="line x" title="17:157	It is not clear, however, whether this resource is suitable, as is, for the task of MT. In this paper, we argue that the overly-general tagset of the PTB is problematic for MT because it fails to capture important grammatical distinctions that are critical in translation." ></td>
	<td class="line x" title="18:157	As a solution, we propose methods of relabeling the syntax trees that effectively improve translation quality." ></td>
	<td class="line x" title="19:157	Consider the derivation in Figure 2." ></td>
	<td class="line x" title="20:157	The output translation has two salient errors: determiner/noun number disagreement (*this Turkish positions) and auxiliary/verb tense disagreement (*has demonstrate)." ></td>
	<td class="line x" title="21:157	The rst problem arises because the DT tag, which does not distinguish between singular and plural determiners, allows singular this to be used with plural NNS positions." ></td>
	<td class="line x" title="22:157	In the second problem, the VP-C tag fails to communicate that it is headed by the base verb (VB) demonstrate, which should prevent it from being used with the auxiliary VBZ has." ></td>
	<td class="line x" title="23:157	Information-poor tags like DT and VP-C can be relabeled to encourage more uent translations, which is the thrust of this paper." ></td>
	<td class="line x" title="24:157	240 Figure 1: A derivation from a Chinese sentence to an English tree." ></td>
	<td class="line x" title="25:157	Section 2 describes our data and experimental procedure." ></td>
	<td class="line x" title="26:157	Section 3 explores different relabeling approaches and their impact on translation quality." ></td>
	<td class="line x" title="27:157	Section 4 reports a substantial improvement in BLEU achieved by combining the most effective relabeling methods." ></td>
	<td class="line x" title="28:157	Section 5 concludes." ></td>
	<td class="line x" title="29:157	2 Experimental Framework Our training data consists of 164M+167M words of parallel Chinese/English text." ></td>
	<td class="line x" title="30:157	The English half was parsed with a reimplementation of Collins Model 2 (Collins, 1999) and the two halves were wordaligned using GIZA++ (Och and Ney, 2000)." ></td>
	<td class="line x" title="31:157	These three components Chinese strings, English parse trees, and their word alignments were inputs to our experimental procedure, which involved ve steps: (1) tree relabeling, (2) rule extraction, (3) decoding, (4) n-best reranking, (5) evaluation." ></td>
	<td class="line x" title="32:157	This paper focuses on step 1, in which the original English parse trees are transformed by one or more relabeling strategies." ></td>
	<td class="line oc" title="33:157	Step 2 involves extracting minimal xRS rules (Galley et al. , 2004) from the set of string/tree/alignments triplets." ></td>
	<td class="line x" title="34:157	These rules are then used in a CKY-type parser-decoder to translate the 878-sentence 2002 NIST MT evaluation test set (step 3)." ></td>
	<td class="line x" title="35:157	In step 4, the output 2,500-sentence nbest list is reranked using an n-gram language model trained on 800M words of English news text." ></td>
	<td class="line x" title="36:157	In the nal step, we score our translations with 4-gram BLEU (Papineni et al. , 2002)." ></td>
	<td class="line x" title="37:157	Separately for each relabeling method, we ran these ve steps and compared the resulting BLEU score with that of a baseline system with no relabeling." ></td>
	<td class="line x" title="38:157	To determine if a BLEU score increase or decrease is meaningful, we calculate statistical signi cance at 95% using paired bootstrap resampling (Koehn, 2004; Zhang et al. , 2004) on 1,000 samples." ></td>
	<td class="line x" title="39:157	Figure 3 shows the results from each relabeling experiment." ></td>
	<td class="line x" title="40:157	The second column indicates the change in the number of unique rules from the baseline number of 16.7M rules." ></td>
	<td class="line x" title="41:157	The third column gives the BLEU score along with an indication whether it is a statistically signi cant increase (a76), a statistically signi cant decrease (a77), or neither ()?" ></td>
	<td class="line x" title="42:157	over the baseline BLEU score." ></td>
	<td class="line x" title="43:157	241 Figure 2: A bad translation xable by relabeling." ></td>
	<td class="line x" title="44:157	242 Relabeling Variant  # Rules BLEU  BASELINE 20.06 LEX_PREP 1 +301.2K 20.2 a76 2 +254.8K 20.36 a76 3 +188.3K 20.14 a76 LEX_DT 1 +36.1K 20.15 a76 2 +29.6K 20.18 a76 LEX_AUX 1 +5.1K 20.09 a76 2 +8.0K 20.09 ? 3 +1.6K 20.11 a76 4 +13.8K 20.07 ? LEX_CC +3.3K 20.03 a77 LEX_% +0.3K 20.14 a76 TAG_VP +123.6K 20.28 a76 SISTERHOOD 1 +1.1M 21.33 a76 2 +935.5K 20.91 a76 3 +433.1K 20.36 a76 4 +407.0K 20.59 a76 PARENT 1 +1.1M 19.77 a77 2 +9.0K 20.01 a77 3 +2.9M 15.63 a77 COMP_IN +17.4K 20.36 a76 REM_NPB 3.5K 19.93 a77 REM_-C 143.4K 19.3 a77 REM_SG 9.4K 20.01 a77 Figure 3: For each relabeling method and variant, the impact on ruleset size and BLEU score over the baseline." ></td>
	<td class="line x" title="45:157	3 Relabeling The small tagset of the PTB has the advantage of being simple to annotate and to parse." ></td>
	<td class="line x" title="46:157	On the other hand, this can lead to tags that are overly generic." ></td>
	<td class="line x" title="47:157	Klein and Manning (2003) discuss this as a problem in parsing and demonstrate that annotating additional information onto the PTB tags leads to improved parsing performance." ></td>
	<td class="line x" title="48:157	We similarly propose methods of relabeling PTB trees that notably improve MT quality." ></td>
	<td class="line x" title="49:157	In the next two subsections, we explore relabeling strategies that fall under two categories introduced by Klein and Manning internal annotation and external annotation." ></td>
	<td class="line x" title="50:157	3.1 Internal Annotation Internal annotation reveals information about a node and its descendants to its surrounding nodes (ancestors, sisters, and other relatives) that is otherwise hidden." ></td>
	<td class="line x" title="51:157	This is paramount in MT because the contents of a node must be understood before the node can be reliably translated and positioned in a sentence." ></td>
	<td class="line x" title="52:157	Here we discuss two such strategies: lexiFigure 4: Rules before and after lexicalization." ></td>
	<td class="line x" title="53:157	calization and tag annotation." ></td>
	<td class="line x" title="54:157	3.1.1 Lexicalization Many state-of-the-art statistical parsers incorporate lexicalization to effectively capture wordspeci c behavior, which has proved helpful in our system as well." ></td>
	<td class="line x" title="55:157	We generalize lexicalization to allow a lexical item (terminal word) to be annotated onto any ancestor label, not only its parent." ></td>
	<td class="line x" title="56:157	Let us revisit the determiner/noun number disagreement problem in Figure 2 (*this Turkish positions)." ></td>
	<td class="line x" title="57:157	If we lexicalize all DTs in the parse trees, the problematic DT is relabeled more speci cally as DT_this, as seen in rule 2prime in Figure 4." ></td>
	<td class="line x" title="58:157	This also produces rules like 4prime, where both the determiner and the noun are plural (notice the DT_these), and 4primeprime, where both are singular." ></td>
	<td class="line x" title="59:157	With such a ruleset, 2prime could only combine with 4primeprime, not 4prime, enforcing the grammatical output this Turkish position." ></td>
	<td class="line x" title="60:157	We explored ve lexicalization strategies, each targeting a different grammatical category." ></td>
	<td class="line x" title="61:157	A common translation mistake was the improper choice of prepositions, e.g., responsibility to attacks." ></td>
	<td class="line x" title="62:157	Lexicalizing prepositions proved to be the most effective lexicalization method (LEX_PREP)." ></td>
	<td class="line x" title="63:157	We annotated a preposition onto both its parent (IN or TO) and its grandparent (PP) since the generic PP tag was often at fault." ></td>
	<td class="line x" title="64:157	We tried lexicalizing all prepositions (variant 1), the top 15 most common prepositions (variant 2), and the top 5 most common (variant 3)." ></td>
	<td class="line x" title="65:157	All gave statistically signi cant BLEU improvements, especially variant 2." ></td>
	<td class="line x" title="66:157	The second strategy was DT lexicalization 243 (LEX_DT), which we encountered previously in Figure 4." ></td>
	<td class="line x" title="67:157	This addresses two features of Chinese that are problematic in translation to English: the infrequent use of articles and the lack of overt number indicators on nouns." ></td>
	<td class="line x" title="68:157	We lexicalized these determiners: the, a, an, this, that, these, or those, and grouped together those with similar grammatical distributions (a/an, this/that, and these/those)." ></td>
	<td class="line x" title="69:157	Variant 1 included all the determiners mentioned above and variant 2 was restricted to the and a/an to focus only on articles." ></td>
	<td class="line x" title="70:157	The second slightly improved on the rst." ></td>
	<td class="line x" title="71:157	The third type was auxiliary lexicalization (LEX_AUX), in which all forms of the verb be are annotated with _be, and similarly with do and have." ></td>
	<td class="line x" title="72:157	The PTB purposely eliminated such distinctions; here we seek to recover them." ></td>
	<td class="line x" title="73:157	However, auxiliaries and verbs function very differently and thus cannot be treated identically." ></td>
	<td class="line x" title="74:157	Klein and Manning (2003) make a similar proposal but omit do." ></td>
	<td class="line x" title="75:157	Variants 1, 2, and 3, lexicalize have, be, and do, respectively." ></td>
	<td class="line x" title="76:157	The third variant slightly outperformed the other variants, including variant 4, which combines all three." ></td>
	<td class="line x" title="77:157	The last two methods are drawn directly from Klein and Manning (2003)." ></td>
	<td class="line x" title="78:157	In CC lexicalization (LEX_CC), both but and & are lexicalized since these two conjunctions are distributed very differently compared to other conjunctions." ></td>
	<td class="line x" title="79:157	Though helpful in parsing, it proved detrimental in our system." ></td>
	<td class="line x" title="80:157	In % lexicalization (LEX_%), the percent sign (%) is given its own PCT tag rather than its typical NN tag, which gave a statistically signi cant BLEU increase." ></td>
	<td class="line x" title="81:157	3.1.2 Tag Annotation In addition to propagating up a terminal word, we can also propagate up a nonterminal, which we call tag annotation." ></td>
	<td class="line x" title="82:157	This partitions a grammatical category into more speci c subcategories, but not as ne-grained as lexicalization." ></td>
	<td class="line x" title="83:157	For example, a VP headed by a VBG can be tag-annotated as VP_VBG to represent a progressive verb phrase." ></td>
	<td class="line x" title="84:157	Let us once again return to Figure 2 to address the auxiliary/verb tense disagreement error (*has demonstrate)." ></td>
	<td class="line x" title="85:157	The auxiliary has expects a VP-C, permitting the bare verb phrase demonstrate to be incorrectly used." ></td>
	<td class="line x" title="86:157	However, if we tag-annotate all VP-Cs, rule 6 would be relabeled as VP-C_VB in rule 6prime and rule 7as 7primein Figure 5." ></td>
	<td class="line x" title="87:157	Rule 6primecan no longer Figure 5: Rules before and after tag annotation." ></td>
	<td class="line x" title="88:157	join with 7prime, while the variant rule 6primeprime can, which produces the grammatical result has demonstrated." ></td>
	<td class="line x" title="89:157	We noticed many wrong verb tense choices, e.g., gerunds and participles used as main sentence verbs." ></td>
	<td class="line x" title="90:157	We resolved this by tag-annotating every VP and VPC with its head verb (TAG_VP)." ></td>
	<td class="line x" title="91:157	Note that we group VBZ and VBP together since they have very similar grammatical distributions and differ only by number." ></td>
	<td class="line x" title="92:157	This strategy gave a healthy BLEU improvement." ></td>
	<td class="line x" title="93:157	3.2 External Annotation In addition to passing information from inside a node to the outside, we can pass information from the external environment into the node through external annotation." ></td>
	<td class="line x" title="94:157	This allows us to make translation decisions based on the context in which a word or phrase is found." ></td>
	<td class="line x" title="95:157	In this subsection, we look at three such methods: sisterhood annotation, parent annotation, and complement annotation." ></td>
	<td class="line x" title="96:157	3.2.1 Sisterhood Annotation The single most effective relabeling scheme we tried was sisterhood annotation." ></td>
	<td class="line x" title="97:157	We annotate each nonterminal with #L if it has any sisters to the left, #R if any to the right, #LR if on both sides, and nothing if it has no sisters." ></td>
	<td class="line x" title="98:157	This distinguishes between words that tend to fall on the left or right border of a constituent (often head words, like NN#L in an NP or IN#R in a PP), in the middle of a constituent (often modi ers, like JJ#LR in an NP), or by themselves 244 Figure 6: A bad translation xable by sisterhood or parent annotation." ></td>
	<td class="line x" title="99:157	(often particles and pronouns, like RP and PRP)." ></td>
	<td class="line x" title="100:157	In our outputs, we frequently nd words used in positions where they should be disallowed or disfavored." ></td>
	<td class="line x" title="101:157	Figure 6 presents a derivation that leads to the ungrammatical output *deeply love she." ></td>
	<td class="line x" title="102:157	The subject pronoun she is incorrectly preferred over the object form her because the most popular NP-C translation for a0 a0 a0 is she." ></td>
	<td class="line x" title="103:157	We can sidestep this mistake through sisterhood-annotation, which yields the relabeled rules 3prime and 4prime in Figure 7." ></td>
	<td class="line x" title="104:157	Rule 4prime expects an NP-C on the right border of the constituent (NP-C#L)." ></td>
	<td class="line x" title="105:157	Since she never occurs in this position in the PTB, it should never be sisterhood-annotated as an NP-C#L. It does occur with sisters to the right, which gives the NP-C#R rule 3prime." ></td>
	<td class="line x" title="106:157	The object NP-C her, on the other hand, is frequently rightmost in a constituent, which is re ected in the NP-C#L rule 3primeprime." ></td>
	<td class="line x" title="107:157	Using this rule with rule 4prime gives the desired result deeply love her." ></td>
	<td class="line x" title="108:157	We experimented with four sisterhood annotation (SISTERHOOD) variants of decreasing complexity." ></td>
	<td class="line x" title="109:157	The rst was described above, which includes rightmost (#L), leftmost (#R), middle (#LR), and alone (no annotation)." ></td>
	<td class="line x" title="110:157	Variant 2 omitted #LR, variant 3 kept only #LR, and variant 4 only annotated nodes without sisters." ></td>
	<td class="line x" title="111:157	Variants 1 and 2 produced the largest gains from relabeling: 1.27 and 0.85 BLEU points, respectively." ></td>
	<td class="line x" title="112:157	Figure 7: Rules before and after sisterhood annotation." ></td>
	<td class="line x" title="113:157	Figure 8: Rules before and after parent annotation." ></td>
	<td class="line x" title="114:157	3.2.2 Parent Annotation Another common relabeling method in parsing is parent annotation (Johnson, 1998), in which a node is annotated with its parents label." ></td>
	<td class="line x" title="115:157	Typically, this is done only to nonterminals, but Klein and Manning (2003) found that annotating preterminals as well was highly effective." ></td>
	<td class="line x" title="116:157	It seemed likely that such contextual information could also bene t MT. Let us tackle the bad output from Figure 6 with parent annotation." ></td>
	<td class="line x" title="117:157	In Figure 8, rule 4is relabeled as rule 4primeand expects an NP-CVP, i.e., an NP-C with a VP parent." ></td>
	<td class="line x" title="118:157	In the PTB, we observe that the NP-C she never has a VP parent, while her does." ></td>
	<td class="line x" title="119:157	In fact, the most popular parent for the NP-C her is VP, while the most popular parent for she is S. Rule 3is relabeled as the NP-CS rule 3primeand her is expressed as the NPCVP rule 3primeprime." ></td>
	<td class="line x" title="120:157	Only rule 3primeprime can partner with rule 4prime, which produces the correct output deeply love her." ></td>
	<td class="line x" title="121:157	We tested three variants of parent annotation (PARENT): (1) all nonterminals are parentannotated, (2) only S nodes are parent-annotated, and (3) all nonterminals are parentand grandparentannotated (the annotation of a nodes parents parent)." ></td>
	<td class="line x" title="122:157	The rst and third variants yielded the largest ruleset sizes of all relabeling methods." ></td>
	<td class="line x" title="123:157	The second variant was restricted only to S to capture the difference between top-level clauses (STOP) and em245 bedded clauses (like SS-C)." ></td>
	<td class="line x" title="124:157	Unfortunately, all three variants turned out to be harmful in terms of BLEU." ></td>
	<td class="line x" title="125:157	3.2.3 Complement Annotation In addition to a nodes parent, we can also annotate a nodes complement." ></td>
	<td class="line x" title="126:157	This captures the fact that words have a preference of taking certain complements over others." ></td>
	<td class="line x" title="127:157	For instance, 96% of cases where the IN of takes one complement in the PTB, it takes NP-C." ></td>
	<td class="line x" title="128:157	On the other hand, although never takes NP-C but takes S-C 99% of the time." ></td>
	<td class="line x" title="129:157	Consider the derivation in Figure 9 that results in the bad output *postponed out May 6." ></td>
	<td class="line x" title="130:157	The IN out is incorrectly allowed despite the fact that it almost never takes an NP-C complement (0.6% of cases in the PTB)." ></td>
	<td class="line x" title="131:157	A way to restrict this is to annotate the INs complement." ></td>
	<td class="line x" title="132:157	Complement-annotated versions of rules 2 and 3 are given in Figure 10." ></td>
	<td class="line x" title="133:157	Rule 2 is relabeled as the IN/PP-C rule 2prime since PP-C is the most common complement for out (99% of the time)." ></td>
	<td class="line x" title="134:157	Since rule 3primeprime expects an IN/NP-C, rule 2prime is disquali ed." ></td>
	<td class="line x" title="135:157	The preposition from (rule 2primeprime), on the other hand, frequently takes NP-C as complement (82% of the time)." ></td>
	<td class="line x" title="136:157	Combining rule 2primeprime with rule 3prime ensures the correct output postponed from May 6." ></td>
	<td class="line x" title="137:157	Complement-annotating all IN tags with their complement if they had one and only one complement (COMP_IN) gave a signi cant BLEU improvement with only a modest increase in ruleset size." ></td>
	<td class="line x" title="138:157	3.3 Removal of Parser Annotations Many parsers, though trained on the PTB, do not preserve the original tagset." ></td>
	<td class="line x" title="139:157	They may omit function tags (like -TMP), indices, and null/gap elements or add annotations to increase parsing accuracy and provide useful grammatical information." ></td>
	<td class="line x" title="140:157	It is not obvious whether these modi cations are helpful for MT, so we explore the effects of removing them." ></td>
	<td class="line x" title="141:157	The statistical parser we used makes three relabelings: (1) base NPs are relabeled as NPB, (2) argument nonterminals are suf xed with -C, and (3) subjectless sentences are relabeled from S to SG." ></td>
	<td class="line x" title="142:157	We tried removing each annotation individually (REM_NPB, REM_-C, and REM_SG), but doing so signi cantly dropped the BLEU score." ></td>
	<td class="line x" title="143:157	This leads us to conclude these parser additions are helpful in MT. Figure 9: A bad translation xable by complement annotation." ></td>
	<td class="line x" title="144:157	Figure 10: Rules before and after complement annotation." ></td>
	<td class="line x" title="145:157	4 Evaluation To maximize the bene t of relabeling, we incorporated ve of the most promising relabeling strategies into one additive system: LEX_%, LEX_DT variant 246  # Rules BLEU Relabeling Variant Ind. Cum." ></td>
	<td class="line x" title="146:157	Ind. Cum." ></td>
	<td class="line x" title="147:157	BASELINE 20.06 20.06 LEX_% +0.3K +0.3K 20.14 20.14 LEX_DT 2 +29.6K +29.9K 20.18 20.3 TAG_VP +123.6K +153.5K 20.28 20.43 LEX_PREP 2 +254.8K +459.0K 20.36 21.25 SISTERHOOD 1 +1.1M +1.5M 21.33 22.38 Figure 11: Relabelings in the additive system and their individual/cumulative effects over the baseline." ></td>
	<td class="line x" title="148:157	2, TAG_VP, LEX_PREP variant 2, and SISTERHOOD variant 1." ></td>
	<td class="line x" title="149:157	These relabelings contributed to a 2.3 absolute (11.6% relative) BLEU point increase over the baseline, with a score of 22.38." ></td>
	<td class="line x" title="150:157	Figure 11 lists these relabelings in the order they were added." ></td>
	<td class="line x" title="151:157	5 Conclusion We have demonstrated that relabeling syntax trees for use in syntax-based machine translation can signi cantly boost translation performance." ></td>
	<td class="line x" title="152:157	It is na ve to assume that linguistic resources can be immediately useful out of the box, in our case, the Penn Treebank for MT. Rather, we targeted features of the PTB tagset that impair translatability and proposed relabeling strategies to overcome these weaknesses." ></td>
	<td class="line x" title="153:157	Many of our ideas effectively raised the BLEU score over a baseline system without relabeling." ></td>
	<td class="line x" title="154:157	Finally, we demonstrated through an additive system that relabelings can be combined together to achieve an even greater improvement in translation quality." ></td>
	<td class="line x" title="155:157	Acknowledgments This research was supported in part by NSF grant IIS-0428020." ></td>
	<td class="line x" title="156:157	We would like to thank Greg Langmead, Daniel Marcu, and Wei Wang for helpful comments." ></td>
	<td class="line x" title="157:157	This paper describes work conducted while the rst author was at the University of Southern California/Information Sciences Institute." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N06-1033
Synchronous Binarization For Machine Translation
Zhang, Hao;Huang, Liang;Gildea, Daniel;Knight, Kevin;"></td>
	<td class="line x" title="1:186	Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 256263, New York, June 2006." ></td>
	<td class="line x" title="2:186	c2006 Association for Computational Linguistics Synchronous Binarization for Machine Translation Hao Zhang Computer Science Department University of Rochester Rochester, NY 14627 zhanghao@cs.rochester.edu Liang Huang Dept. of Computer & Information Science University of Pennsylvania Philadelphia, PA 19104 lhuang3@cis.upenn.edu Daniel Gildea Computer Science Department University of Rochester Rochester, NY 14627 gildea@cs.rochester.edu Kevin Knight Information Sciences Institute University of Southern California Marina del Rey, CA 90292 knight@isi.edu Abstract Systems based on synchronous grammars and tree transducers promise to improve the quality of statistical machine translation output, but are often very computationally intensive." ></td>
	<td class="line x" title="3:186	The complexity is exponential in the size of individual grammar rules due to arbitrary re-orderings between the two languages, and rules extracted from parallel corpora can be quite large." ></td>
	<td class="line x" title="4:186	We devise a linear-time algorithm for factoring syntactic re-orderings by binarizing synchronous rules when possible and show that the resulting rule set significantly improves the speed and accuracy of a state-of-the-art syntax-based machine translation system." ></td>
	<td class="line oc" title="5:186	1 Introduction Several recent syntax-based models for machine translation (Chiang, 2005; Galley et al. , 2004) can be seen as instances of the general framework of synchronous grammars and tree transducers." ></td>
	<td class="line n" title="6:186	In this framework, both alignment (synchronous parsing) and decoding can be thought of as parsing problems, whose complexity is in general exponential in the number of nonterminals on the right hand side of a grammar rule." ></td>
	<td class="line x" title="7:186	To alleviate this problem, we investigate bilingual binarization to factor the synchronous grammar to a smaller branching factor, although it is not guaranteed to be successful for any synchronous rule with arbitrary permutation." ></td>
	<td class="line x" title="8:186	In particular:  We develop a technique called synchronous binarization and devise a fast binarization algorithm such that the resulting rule set allows efcient algorithms for both synchronous parsing and decoding with integrated n-gram language models." ></td>
	<td class="line x" title="9:186	 We examine the effect of this binarization method on end-to-end machine translation quality, compared to a more typical baseline method." ></td>
	<td class="line x" title="10:186	 We examine cases of non-binarizable rules in a large, empirically-derived rule set, and we investigate the effect on translation quality when excluding such rules." ></td>
	<td class="line x" title="11:186	Melamed (2003) discusses binarization of multitext grammars on a theoretical level, showing the importance and dif culty of binarization for ef cient synchronous parsing." ></td>
	<td class="line x" title="12:186	One way around this dif culty is to stipulate that all rules must be binary from the outset, as in inversion-transduction grammar (ITG) (Wu, 1997) and the binary synchronous context-free grammar (SCFG) employed by the Hiero system (Chiang, 2005) to model the hierarchical phrases." ></td>
	<td class="line oc" title="13:186	In contrast, the rule extraction method of Galley et al.(2004) aims to incorporate more syntactic information by providing parse trees for the target language and extracting tree transducer rules that apply to the parses." ></td>
	<td class="line o" title="15:186	This approach results in rules with many nonterminals, making good binarization techniques critical." ></td>
	<td class="line x" title="16:186	Suppose we have the following SCFG, where superscripts indicate reorderings (formal de nitions of 256 S NP Baoweier PP yu Shalong VP juxing le huitan S NP Powell VP held a meeting PP with Sharon Figure 1: A pair of synchronous parse trees in the SCFG (1)." ></td>
	<td class="line x" title="17:186	The dashed curves indicate pairs of synchronous nonterminals (and sub trees)." ></td>
	<td class="line x" title="18:186	SCFGs can be found in Section 2): (1) S NP(1) VP(2) PP(3), NP(1) PP(3) VP(2) NP Powell, Baoweier VP held a meeting, juxing le huitan PP with Sharon, yu Shalong Decoding can be cast as a (monolingual) parsing problem since we only need to parse the sourcelanguage side of the SCFG, as if we were constructing a CFG projected on Chinese out of the SCFG." ></td>
	<td class="line x" title="19:186	The only extra work we need to do for decoding is to build corresponding target-language (English) subtrees in parallel." ></td>
	<td class="line x" title="20:186	In other words, we build synchronous trees when parsing the source-language input, as shown in Figure 1." ></td>
	<td class="line x" title="21:186	To ef ciently decode with CKY, we need to binarize the projected CFG grammar.1 Rules can be binarized in different ways." ></td>
	<td class="line x" title="22:186	For example, we could binarize the rst rule left to right or right to left: S VNP-PP VP VNP-PP NP PP or S NP VPP-VP VPP-VP  PP VP We call those intermediate symbols (e.g. VPP-VP) virtual nonterminals and corresponding rules virtual rules, whose probabilities are all set to 1." ></td>
	<td class="line x" title="23:186	These two binarizations are no different in the translation-model-only decoding described above, just as in monolingual parsing." ></td>
	<td class="line x" title="24:186	However, in the source-channel approach to machine translation, we need to combine probabilities from the translation model (an SCFG) with the language model (an ngram), which has been shown to be very important for translation quality (Chiang, 2005)." ></td>
	<td class="line x" title="25:186	To do bigram-integrated decoding, we need to augment each chart item (X, i, j) with two target-language 1Other parsing strategies like the Earley algorithm use an internal binary representation (e.g. dotted-rules) of the original grammar to ensure cubic time complexity." ></td>
	<td class="line x" title="26:186	boundary words u and v to produce a bigram-item like parenleftBig u  v Xi j parenrightBig, following the dynamic programming algorithm of Wu (1996)." ></td>
	<td class="line x" title="27:186	Now the two binarizations have very different effects." ></td>
	<td class="line x" title="28:186	In the rst case, we rst combine NP with PP: parenleftbigg Powell  PowellNP 1 2 parenrightbigg : p parenleftbigg with  SharonPP 2 4 parenrightbigg : q parenleftbigg Powell  Powell  with  Sharon VNP-PP 1 4 parenrightbigg : pq where p and q are the scores of antecedent items." ></td>
	<td class="line x" title="29:186	This situation is unpleasant because in the targetlanguage NP and PP are not contiguous so we cannot apply language model scoring when we build the VNP-PP item." ></td>
	<td class="line x" title="30:186	Instead, we have to maintain all four boundary words (rather than two) and postpone the language model scoring till the next step where VNP-PP is combined with parenleftbigg held  meeting VP2 4 parenrightbigg to form an S item." ></td>
	<td class="line x" title="31:186	We call this binarization method monolingual binarization since it works only on the source-language projection of the rule without respecting the constraints from the other side." ></td>
	<td class="line x" title="32:186	This scheme generalizes to the case where we have n nonterminals in a SCFG rule, and the decoder conservatively assumes nothing can be done on language model scoring (because target-language spans are non-contiguous in general) until the real nonterminal has been recognized." ></td>
	<td class="line x" title="33:186	In other words, targetlanguage boundary words from each child nonterminal of the rule will be cached in all virtual nonterminals derived from this rule." ></td>
	<td class="line x" title="34:186	In the case of m-gram integrated decoding, we have to maintain 2(m1) boundary words for each child nonterminal, which leads to a prohibitive overall complexity of O(|w|3+2n(m1)), which is exponential in rule size (Huang et al. , 2005)." ></td>
	<td class="line x" title="35:186	Aggressive pruning must be used to make it tractable in practice, which in general introduces many search errors and adversely affects translation quality." ></td>
	<td class="line x" title="36:186	In the second case, however: parenleftbigg with  SharonPP 2 4 parenrightbigg : r parenleftbigg held  meeting VP4 7 parenrightbigg : s parenleftbigg held  Sharon VPP-VP 2 7 parenrightbigg : rsPr(with|meeting) Here since PP and VP are contiguous (but swapped) in the target-language, we can include the 257 NP NP PP VP VP PP target (English) source (Chinese) VPP-VP NP PP VP Chinese indices English boundary wor ds 1 2 4 7 Powell Powell held meeting with Sharon VPP-VP Figure 2: The alignment pattern (left) and alignment matrix (right) of the synchronous production." ></td>
	<td class="line x" title="37:186	language model score by adding Pr(with | meeting), and the resulting item again has two boundary words." ></td>
	<td class="line x" title="38:186	Later we add Pr(held | Powell) when the resulting item is combined with parenleftbigg Powell  PowellNP 1 2 parenrightbigg to form an S item." ></td>
	<td class="line x" title="39:186	As illustrated in Figure 2, VPP-VP has contiguous spans on both source and target sides, so that we can generate a binary-branching SCFG: (2) S NP (1) V PP-VP (2), NP(1) V PP-VP (2) VPP-VP  VP(1) PP(2), PP(2) VP(1) In this case m-gram integrated decoding can be done in O(|w|3+4(m1)) time which is much lowerorder polynomial and no longer depends on rule size (Wu, 1996), allowing the search to be much faster and more accurate facing pruning, as is evidenced in the Hiero system of Chiang (2005) where he restricts the hierarchical phrases to be a binary SCFG." ></td>
	<td class="line x" title="40:186	The bene t of binary grammars also lies in synchronous parsing (alignment)." ></td>
	<td class="line x" title="41:186	Wu (1997) shows that parsing a binary SCFG is in O(|w|6) while parsing SCFG is NP-hard in general (Satta and Peserico, 2005)." ></td>
	<td class="line x" title="42:186	The same reasoning applies to tree transducer rules." ></td>
	<td class="line oc" title="43:186	Suppose we have the following tree-to-string rules, following Galley et al.(2004): (3) S(x0:NP, VP(x2:VP, x1:PP))x0 x1 x2 NP(NNP(Powell))Baoweier VP(VBD(held), NP(DT(a) NPS(meeting))) juxing le huitan PP(TO(with), NP(NNP(Sharon)))yu Shalong where the reorderings of nonterminals are denoted by variables xi." ></td>
	<td class="line x" title="45:186	Notice that the rst rule has a multi-level lefthand side subtree." ></td>
	<td class="line x" title="46:186	This system can model nonisomorphic transformations on English parse trees to t another language, for example, learning that the (S (V O)) structure in English should be transformed into a (V S O) structure in Arabic, by looking at two-level tree fragments (Knight and Graehl, 2005)." ></td>
	<td class="line x" title="47:186	From a synchronous rewriting point of view, this is more akin to synchronous tree substitution grammar (STSG) (Eisner, 2003)." ></td>
	<td class="line x" title="48:186	This larger locality is linguistically motivated and leads to a better parameter estimation." ></td>
	<td class="line x" title="49:186	By imagining the left-hand-side trees as special nonterminals, we can virtually create an SCFG with the same generative capacity." ></td>
	<td class="line x" title="50:186	The technical details will be explained in Section 3.2." ></td>
	<td class="line x" title="51:186	In general, if we are given an arbitrary synchronous rule with many nonterminals, what are the good decompositions that lead to a binary grammar?" ></td>
	<td class="line x" title="52:186	Figure 2 suggests that a binarization is good if every virtual nonterminal has contiguous spans on both sides." ></td>
	<td class="line x" title="53:186	We formalize this idea in the next section." ></td>
	<td class="line x" title="54:186	2 Synchronous Binarization A synchronous CFG (SCFG) is a context-free rewriting system for generating string pairs." ></td>
	<td class="line x" title="55:186	Each rule (synchronous production) rewrites a nonterminal in two dimensions subject to the constraint that the sequence of nonterminal children on one side is a permutation of the nonterminal sequence on the other side." ></td>
	<td class="line x" title="56:186	Each co-indexed child nonterminal pair will be further rewritten as a unit.2 We de ne the language L(G) produced by an SCFG G as the pairs of terminal strings produced by rewriting exhaustively from the start symbol." ></td>
	<td class="line x" title="57:186	As shown in Section 3.2, terminals do not play an important role in binarization." ></td>
	<td class="line x" title="58:186	So we now write rules in the following notation: X X(1)1X(n)n, X(pi(1))pi(1) X(pi(n))pi(n) where each Xi is a variable which ranges over nonterminals in the grammar and pi is the permutation of the rule." ></td>
	<td class="line x" title="59:186	We also de ne an SCFG rule as n-ary if its permutation is of n and call an SCFG n-ary if its longest rule is n-ary." ></td>
	<td class="line x" title="60:186	Our goal is to produce an equivalent binary SCFG for an input n-ary SCFG." ></td>
	<td class="line x" title="61:186	2In making one nonterminal play dual roles, we follow the de nitions in (Aho and Ullman, 1972; Chiang, 2005), originally known as Syntax Directed Translation Schema (SDTS)." ></td>
	<td class="line x" title="62:186	An alternative de nition by Satta and Peserico (2005) allows co-indexed nonterminals taking different symbols in two dimensions." ></td>
	<td class="line x" title="63:186	Formally speaking, we can construct an equivalent SDTS by creating a cross-product of nonterminals from two sides." ></td>
	<td class="line x" title="64:186	See (Satta and Peserico, 2005, Sec." ></td>
	<td class="line x" title="65:186	4) for other details." ></td>
	<td class="line x" title="66:186	258 (2,3,5,4) (2,3) 2 3 (5,4) 5 4 (2,3,5,4) 2 (3,5,4) 3 (5,4) 5 4 (a) (b) (c) Figure 3: (a) and (b): two binarization patterns for (2, 3, 5, 4)." ></td>
	<td class="line x" title="67:186	(c): alignment matrix for the nonbinarizable permuted sequence (2, 4, 1, 3) However, not every SCFG can be binarized." ></td>
	<td class="line x" title="68:186	In fact, the binarizability of an n-ary rule is determined by the structure of its permutation, which can sometimes be resistant to factorization (Aho and Ullman, 1972)." ></td>
	<td class="line x" title="69:186	So we now start to rigorously de ne the binarizability of permutations." ></td>
	<td class="line x" title="70:186	2.1 Binarizable Permutations A permuted sequence is a permutation of consecutive integers." ></td>
	<td class="line x" title="71:186	For example, (3, 5, 4) is a permuted sequence while (2, 5) is not." ></td>
	<td class="line x" title="72:186	As special cases, single numbers are permuted sequences as well." ></td>
	<td class="line x" title="73:186	A sequence a is said to be binarizable if it is a permuted sequence and either 1." ></td>
	<td class="line x" title="74:186	a is a singleton, i.e. a = (a), or 2." ></td>
	<td class="line x" title="75:186	a can be split into two sub sequences, i.e. a = (b;c), where b and c are both binarizable permuted sequences." ></td>
	<td class="line x" title="76:186	We call such a division (b;c) a binarizable split of a. This is a recursive de nition." ></td>
	<td class="line x" title="77:186	Each binarizable permuted sequence has at least one hierarchical binarization pattern." ></td>
	<td class="line x" title="78:186	For instance, the permuted sequence (2, 3, 5, 4) is binarizable (with two possible binarization patterns) while (2, 4, 1, 3) is not (see Figure 3)." ></td>
	<td class="line x" title="79:186	2.2 Binarizable SCFG An SCFG is said to be binarizable if the permutation of each synchronous production is binarizable." ></td>
	<td class="line x" title="80:186	We denote the class of binarizable SCFGs as bSCFG." ></td>
	<td class="line x" title="81:186	This set represents an important subclass of SCFG that is easy to handle (parsable in O(|w|6)) and covers many interesting longer-than-two rules.3 3Although we factor the SCFG rules individually and dene bSCFG accordingly, there are some grammars (the dashed SCFG bSCFG SCFG-2 O(|w|6) parsable Figure 4: Subclasses of SCFG." ></td>
	<td class="line x" title="82:186	The thick arrow denotes the direction of synchronous binarization." ></td>
	<td class="line x" title="83:186	For clarity reasons, binary SCFG is coded as SCFG-2." ></td>
	<td class="line x" title="84:186	Theorem 1." ></td>
	<td class="line x" title="85:186	For each grammar G in bSCFG, there exists a binary SCFG Gprime, such that L(Gprime) = L(G)." ></td>
	<td class="line x" title="86:186	Proof." ></td>
	<td class="line x" title="87:186	Once we decompose the permutation of n in the original rule into binary permutations, all that remains is to decorate the skeleton binary parse with nonterminal symbols and attach terminals to the skeleton appropriately." ></td>
	<td class="line x" title="88:186	We explain the technical details in the next section." ></td>
	<td class="line x" title="89:186	3 Binarization Algorithms We have reduced the problem of binarizing an SCFG rule into the problem of binarizing its permutation." ></td>
	<td class="line x" title="90:186	This problem can be cast as an instance of synchronous ITG parsing (Wu, 1997)." ></td>
	<td class="line x" title="91:186	Here the parallel string pair that we are parsing is the integer sequence (1n) and its permutation (pi(1)pi(n))." ></td>
	<td class="line x" title="92:186	The goal of the ITG parsing is to nd a synchronous tree that agrees with the alignment indicated by the permutation." ></td>
	<td class="line x" title="93:186	In fact, as demonstrated previously, some permutations may have more than one binarization patterns among which we only need one." ></td>
	<td class="line x" title="94:186	Wu (1997, Sec." ></td>
	<td class="line x" title="95:186	7) introduces a non-ambiguous ITG that prefers left-heavy binary trees so that for each permutation there is a unique synchronous derivation (binarization pattern)." ></td>
	<td class="line x" title="96:186	However, this problem has more ef cient solutions." ></td>
	<td class="line x" title="97:186	Shapiro and Stephens (1991, p. 277) informally present an iterative procedure where in each pass it scans the permuted sequence from left to right and combines two adjacent sub sequences whenever possible." ></td>
	<td class="line x" title="98:186	This procedure produces a left-heavy binarization tree consistent with the unambiguous ITG and runs in O(n2) time since we need n passes in the worst case." ></td>
	<td class="line x" title="99:186	We modify this procedure and improve circle in Figure 4), which can be binarized only by analyzing interactions between rules." ></td>
	<td class="line x" title="100:186	Below is a simple example: S X(1) X(2) X(3) X(4), X(2) X(4) X(1) X(3) X a, a 259 iteration stack input action 1 5 3 4 2 1 5 3 4 2 shift 1 1 5 3 4 2 shift 2 1 5 3 4 2 shift 3 1 5 3 4 2 shift 1 5 3-4 2 reduce [3, 4] 1 3-5 2 reduce 5, [3, 4] 4 1 3-5 2 shift 1 2-5 reduce 2,5, [3, 4] 1-5 reduce [1,2,5, [3, 4]] Figure 5: Example of Algorithm 1 on the input (1, 5, 3, 4, 2)." ></td>
	<td class="line x" title="101:186	The rightmost column shows the binarization-trees generated at each reduction step." ></td>
	<td class="line x" title="102:186	it into a linear-time shift-reduce algorithm that only needs one pass through the sequence." ></td>
	<td class="line x" title="103:186	3.1 The linear-time skeleton algorithm The (unique) binarization tree bi(a) for a binarizable permuted sequence a is recursively de ned as follows:  if a = (a), then bi(a) = a;  otherwise let a = (b;c) to be the rightmost binarizable split of a. then bi(a) = braceleftBigg [bi(b), bi(c)] b1 < c1 bi(b), bi(c) b1 > c1." ></td>
	<td class="line x" title="104:186	For example, the binarization tree for (2, 3, 5, 4) is [[2, 3],5, 4], which corresponds to the binarization pattern in Figure 3(a)." ></td>
	<td class="line x" title="105:186	We use [] and  for straight and inverted combinations respectively, following the ITG notation (Wu, 1997)." ></td>
	<td class="line x" title="106:186	The rightmost split ensures left-heavy binary trees." ></td>
	<td class="line x" title="107:186	The skeleton binarization algorithm is an instance of the widely used left-to-right shift-reduce algorithm." ></td>
	<td class="line x" title="108:186	It maintains a stack for contiguous subsequences discovered so far, like 2-5, 1." ></td>
	<td class="line x" title="109:186	In each iteration, it shifts the next number from the input and repeatedly tries to reduce the top two elements on the stack if they are consecutive." ></td>
	<td class="line x" title="110:186	See Algorithm 1 for details and Figure 5 for an example." ></td>
	<td class="line x" title="111:186	Theorem 2." ></td>
	<td class="line x" title="112:186	Algorithm 1 succeeds if and only if the input permuted sequence a is binarizable, and in case of success, the binarization pattern recovered is the binarization tree of a. Proof." ></td>
	<td class="line x" title="113:186	: it is obvious that if the algorithm succeeds then a is binarizable using the binarization pattern recovered." ></td>
	<td class="line x" title="114:186	: by a complete induction on n, the length of a. Base case: n = 1, trivial." ></td>
	<td class="line x" title="115:186	Assume it holds for all nprime < n. If a is binarizable, then let a = (b;c) be its rightmost binarizable split." ></td>
	<td class="line x" title="116:186	By the induction hypothesis, the algorithm succeeds on the partial input b, reducing it to the single element s[0] on the stack and recovering its binarization tree bi(b)." ></td>
	<td class="line x" title="117:186	Let c = (c1;c2)." ></td>
	<td class="line x" title="118:186	If c1 is binarizable and triggers our binarizer to make a straight combination of (b;c1), based on the property of permutations, it must be true that (c1;c2) is a valid straight concatenation." ></td>
	<td class="line x" title="119:186	We claim that c2 must be binarizable in this situation." ></td>
	<td class="line x" title="120:186	So, (b,c1;c2) is a binarizable split to the right of the rightmost binarizable split (b;c), which is a contradiction." ></td>
	<td class="line x" title="121:186	A similar contradiction will arise if b and c1 can make an inverted concatenation." ></td>
	<td class="line x" title="122:186	Therefore, the algorithm will scan through the whole c as if from the empty stack." ></td>
	<td class="line x" title="123:186	By the induction hypothesis again, it will reduce c into s[1] on the stack and recover its binarization tree bi(c)." ></td>
	<td class="line x" title="124:186	Since b and c are combinable, the algorithm reduces s[0] and s[1] in the last step, forming the binarization tree for a, which is either [bi(b), bi(c)] or bi(b), bi(c)." ></td>
	<td class="line x" title="125:186	The running time of Algorithm 1 is linear in n, the length of the input sequence." ></td>
	<td class="line x" title="126:186	This is because there are exactly n shifts and at most n1 reductions, and each shift or reduction takes O(1) time." ></td>
	<td class="line x" title="127:186	3.2 Binarizing tree-to-string transducers Without loss of generality, we have discussed how to binarize synchronous productions involving only nonterminals through binarizing the corresponding skeleton permutations." ></td>
	<td class="line x" title="128:186	We still need to tackle a few technical problems in the actual system." ></td>
	<td class="line x" title="129:186	First, we are dealing with tree-to-string transducer rules." ></td>
	<td class="line x" title="130:186	We view each left-hand side subtree as a monolithic nonterminal symbol and factor each transducer rule into two SCFG rules: one from the root nonterminal to the subtree, and the other from the subtree to the leaves." ></td>
	<td class="line x" title="131:186	In this way we can uniquely reconstruct the tree-to-string derivation using the two-step SCFG derivation." ></td>
	<td class="line x" title="132:186	For example, 260 Algorithm 1 The Linear-time Binarization Algorithm 1: function BINARIZABLE(a) 2: top0 triangleright stack top pointer 3: PUSH(a1, a1) triangleright initial shift 4: for i2 to|a|do triangleright for each remaining element 5: PUSH(ai, ai) triangleright shift 6: while top > 1 and CONSECUTIVE(s[top], s[top1]) do triangleright keep reducing if possible 7: (p, q)COMBINE(s[top], s[top1]) 8: toptop2 9: PUSH(p, q) 10: return (top = 1) triangleright if reduced to a single element then the input is binarizable, otherwise not 11: function CONSECUTIVE((a, b), (c, d)) 12: return (b = c1) or (d = a1) triangleright either straight or inverted 13: function COMBINE((a, b), (c, d)) 14: return (min(a, c), max(b, d)) consider the following tree-to-string rule: ADJP x0:RB JJ responsible PP IN for NP-C NPB DT the x2:NN x1:PP  x0 fuze x1 de x2 We create a speci c nonterminal, say, T859, which is a unique identi er for the left-hand side subtree and generate the following two SCFG rules: ADJP  T859 (1), T859 (1) T859  RB (1) resp." ></td>
	<td class="line x" title="133:186	for the NN(2) PP(3), RB(1) fuze PP(3) de NN(2) Second, besides synchronous nonterminals, terminals in the two languages can also be present, as in the above example." ></td>
	<td class="line x" title="134:186	It turns out we can attach the terminals to the skeleton parse for the synchronous nonterminal strings quite freely as long as we can uniquely reconstruct the original rule from its binary parse tree." ></td>
	<td class="line x" title="135:186	In order to do so we need to keep track of sub-alignments including both aligned nonterminals and neighboring terminals." ></td>
	<td class="line x" title="136:186	When binarizing the second rule above, we rst run the skeleton algorithm to binarize the underlying permutation (1, 3, 2) to its binarization tree [1,3, 2]." ></td>
	<td class="line x" title="137:186	Then we do a post-order traversal to the skeleton tree, combining Chinese terminals (one at a time) at the leaf nodes and merging English terminals greedily at internal nodes: [1,3, 2] 1 3, 2 3 2  T859 [1,3,2] V[RB, fuze]1 RB fuze VV[PP, de], resp." ></td>
	<td class="line x" title="138:186	for the NN3,2 V[PP, de]3 PP de NN2 A pre-order traversal of the decorated binarization tree gives us the following binary SCFG rules: T859  V1(1) V2(2), V1(1) V2(2) V1  RB(1), RB(1) fuze V2  resp." ></td>
	<td class="line x" title="139:186	for the NN(1) V(2)3, V(2)3 NN(1) V3  PP(1), PP(1) de where the virtual nonterminals are: V1: V[RB, fuze] V2: VV[PP, de], resp." ></td>
	<td class="line x" title="140:186	for the NN V3: V[PP, de] Analogous to the dotted rules in Earley parsing for monolingual CFGs, the names we create for the virtual nonterminals re ect the underlying sub-alignments, ensuring intermediate states can be shared across different tree-to-string rules without causing ambiguity." ></td>
	<td class="line x" title="141:186	The whole binarization algorithm still runs in time linear in the number of symbols in the rule (including both terminals and nonterminals)." ></td>
	<td class="line x" title="142:186	4 Experiments In this section, we answer two empirical questions." ></td>
	<td class="line x" title="143:186	261 0 2e+06 4e+06 6e+06 8e+06 1e+07 0 5 10 15 20 25 30 35 40 0 20 40 60 80 100 # of rules percentage (%) length Figure 6: The solid-line curve represents the distribution of all rules against permutation lengths." ></td>
	<td class="line x" title="144:186	The dashed-line stairs indicate the percentage of non-binarizable rules in our initial rule set while the dotted-line denotes that percentage among all permutations." ></td>
	<td class="line x" title="145:186	4.1 How many rules are binarizable?" ></td>
	<td class="line x" title="146:186	It has been shown by Shapiro and Stephens (1991) and Wu (1997, Sec." ></td>
	<td class="line x" title="147:186	4) that the percentage of binarizable cases over all permutations of length n quickly approaches 0 as n grows (see Figure 6)." ></td>
	<td class="line x" title="148:186	However, for machine translation, it is more meaningful to compute the ratio of binarizable rules extracted from real text." ></td>
	<td class="line oc" title="149:186	Our rule set is obtained by rst doing word alignment using GIZA++ on a Chinese-English parallel corpus containing 50 million words in English, then parsing the English sentences using a variant of Collins parser, and nally extracting rules using the graph-theoretic algorithm of Galley et al.(2004)." ></td>
	<td class="line x" title="151:186	We did a spectrum analysis on the resulting rule set with 50,879,242 rules." ></td>
	<td class="line x" title="152:186	Figure 6 shows how the rules are distributed against their lengths (number of nonterminals)." ></td>
	<td class="line x" title="153:186	We can see that the percentage of non-binarizable rules in each bucket of the same length does not exceed 25%." ></td>
	<td class="line x" title="154:186	Overall, 99.7% of the rules are binarizable." ></td>
	<td class="line x" title="155:186	Even for the 0.3% nonbinarizable rules, human evaluations show that the majority of them are due to alignment errors." ></td>
	<td class="line x" title="156:186	It is also interesting to know that 86.8% of the rules have monotonic permutations, i.e. either taking identical or totally inverted order." ></td>
	<td class="line x" title="157:186	4.2 Does synchronous binarizer help decoding?" ></td>
	<td class="line x" title="158:186	We did experiments on our CKY-based decoder with two binarization methods." ></td>
	<td class="line x" title="159:186	It is the responsibility of the binarizer to instruct the decoder how to compute the language model scores from children nonterminals in each rule." ></td>
	<td class="line x" title="160:186	The baseline method is monolingual left-to-right binarization." ></td>
	<td class="line x" title="161:186	As shown in Section 1, decoding complexity with this method is exponential in the size of the longest rule and since we postpone all the language model scorings, pruning in this case is also biased." ></td>
	<td class="line x" title="162:186	system bleu monolingual binarization 36.25 synchronous binarization 38.44 alignment-template system 37.00 Table 1: Syntax-based systems vs. ATS To move on to synchronous binarization, we rst did an experiment using the above baseline system without the 0.3% non-binarizable rules and did not observe any difference in BLEU scores." ></td>
	<td class="line x" title="163:186	So we safely move a step further, focusing on the binarizable rules only." ></td>
	<td class="line x" title="164:186	The decoder now works on the binary translation rules supplied by an external synchronous binarizer." ></td>
	<td class="line x" title="165:186	As shown in Section 1, this results in a simpli ed decoder with a polynomial time complexity, allowing less aggressive and more effective pruning based on both translation model and language model scores." ></td>
	<td class="line x" title="166:186	We compare the two binarization schemes in terms of translation quality with various pruning thresholds." ></td>
	<td class="line x" title="167:186	The rule set is that of the previous section." ></td>
	<td class="line x" title="168:186	The test set has 116 Chinese sentences of no longer than 15 words." ></td>
	<td class="line x" title="169:186	Both systems use trigram as the integrated language model." ></td>
	<td class="line x" title="170:186	Figure 7 demonstrates that decoding accuracy is signi cantly improved after synchronous binarization." ></td>
	<td class="line x" title="171:186	The number of edges proposed during decoding is used as a measure of the size of search space, or time ef ciency." ></td>
	<td class="line x" title="172:186	Our system is consistently faster and more accurate than the baseline system." ></td>
	<td class="line x" title="173:186	We also compare the top result of our synchronous binarization system with the state-of-theart alignment-template approach (ATS) (Och and Ney, 2004)." ></td>
	<td class="line x" title="174:186	The results are shown in Table 1." ></td>
	<td class="line x" title="175:186	Our system has a promising improvement over the ATS 262 33.5 34.5 35.5 36.5 37.5 38.5 3e+09 4e+09 5e+09 6e+09 7e+09 bleu scores # of edges proposed during decoding synchronous binarization monolingual binarization Figure 7: Comparing the two binarization methods in terms of translation quality against search effort." ></td>
	<td class="line x" title="176:186	system which is trained on a larger data-set but tuned independently." ></td>
	<td class="line x" title="177:186	5 Conclusion Modeling reorderings between languages has been a major challenge for machine translation." ></td>
	<td class="line x" title="178:186	This work shows that the majority of syntactic reorderings, at least between languages like English and Chinese, can be ef ciently decomposed into hierarchical binary reorderings." ></td>
	<td class="line x" title="179:186	From a modeling perspective, on the other hand, it is bene cial to start with a richer representation that has more transformational power than ITG or binary SCFG." ></td>
	<td class="line x" title="180:186	Our work shows how to convert it back to a computationally friendly form without harming much of its expressiveness." ></td>
	<td class="line x" title="181:186	As a result, decoding with n-gram models can be fast and accurate, making it possible for our syntax-based system to overtake a comparable phrase-based system in BLEU score." ></td>
	<td class="line x" title="182:186	We believe that extensions of our technique to more powerful models such as synchronous tree-adjoining grammar (Shieber and Schabes, 1990) is an interesting area for further work." ></td>
	<td class="line x" title="183:186	Acknowledgments Much of this work was done when H. Zhang and L. Huang were visiting USC/ISI." ></td>
	<td class="line x" title="184:186	The authors wish to thank Wei Wang, Jonathan Graehl and Steven DeNeefe for help with the experiments." ></td>
	<td class="line x" title="185:186	We are also grateful to Daniel Marcu, Giorgio Satta, and Aravind Joshi for discussions." ></td>
	<td class="line x" title="186:186	This work was partially supported by NSF ITR IIS-09325646 and NSF ITR IIS-0428020." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N06-3004
Efficient Algorithms For Richer Formalisms: Parsing And Machine Translation
Huang, Liang;"></td>
	<td class="line x" title="1:68	Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 223226, New York, June 2006." ></td>
	<td class="line x" title="2:68	c2006 Association for Computational Linguistics Efficient Algorithms for Richer Formalisms: Parsing and Machine Translation Liang Huang Department of Computer and Information Science University of Pennsylvania lhuang3@cis.upenn.edu My PhD research has been on the algorithmic and formal aspects of computational linguistics, esp. in the areas of parsing and machine translation." ></td>
	<td class="line x" title="3:68	I am interested in developing efficient algorithms for formalisms with rich expressive power, so that we can have a better modeling of human languages without sacrificing efficiency." ></td>
	<td class="line x" title="4:68	In doing so, I hope to help integrating more linguistic and structural knowledge with modern statistical techniques, and in particular, for syntax-based machine translation (MT) systems." ></td>
	<td class="line x" title="5:68	Among other projects, I have been working on kbest parsing, synchronous binarization, and syntaxdirected translation." ></td>
	<td class="line x" title="6:68	1 k-best Parsing and Hypergraphs NLP systems are often cascades of several modules, e.g., part-of-speech tagging, then syntactic parsing, and finally semantic interpretation." ></td>
	<td class="line x" title="7:68	It is often the case that the 1-best output from one module is not always optimal for the next module." ></td>
	<td class="line x" title="8:68	So one might want to postpone some disambiguation by propagating k-best lists (instead of 1-best solutions) to subsequent phases, as in joint parsing and semantic role-labeling (Gildea and Jurafsky, 2002)." ></td>
	<td class="line x" title="9:68	This is also true for reranking and discriminative training, where the k-best list of candidates serves as an approximation of the full set (Collins, 2000; Och, 2003; McDonald et al. , 2005)." ></td>
	<td class="line x" title="10:68	In this way we can optimize some complicated objective function on the k-best set, rather than on the full search space which is usually exponentially large." ></td>
	<td class="line x" title="11:68	Previous algorithms for k-best parsing (Collins, 2000; Charniak and Johnson, 2005) are either suboptimal or slow and rely significantly on pruning techniques to make them tractable." ></td>
	<td class="line x" title="12:68	So I codeveloped several fast and exact algorithms for kbest parsing in the general framework of directed monotonic hypergraphs (Huang and Chiang, 2005)." ></td>
	<td class="line x" title="13:68	This formulation extends and refines Klein and Mannings work (2001) by introducing monotonic 1.5 2.5 3.5 4.5 5.5 6.5 7.5 1 10 100 1000 10000 Average Parsing Time (seconds) k Algorithm 0 Algorithm 1 Algorithm 3 Figure 1: Average parsing speed on the Section 23 of Penn Treebank (Algorithms 0, 1, and 3, log-log)." ></td>
	<td class="line x" title="14:68	weight functions, which is closely related to the optimal subproblem property in dynamic programming." ></td>
	<td class="line x" title="15:68	We first generalize the classical 1-best Viterbi algorithm to hypergraphs, and then present four k-best algorithms, each improving its predessor by delaying more work until necessary." ></td>
	<td class="line x" title="16:68	The final one, Algorithm 3, starts with a normal 1-best search for each vertex (or item, as in deductive frameworks), and then works backwards from the target vertex (final item) for its 2nd, 3rd,." ></td>
	<td class="line x" title="17:68	., kth best derivations, calling itself recursively only on demand, being the laziest of the four algorithms." ></td>
	<td class="line x" title="18:68	When tested on top of two state-of-the-art systems, the Collins/Bikel parser (Bikel, 2004) and Chiangs CKY-based Hiero decoder (Chiang, 2005), this algorithm is shown to have very little overhead even for quite large k (say, 106) (See Fig." ></td>
	<td class="line x" title="19:68	1 for experiments on Bikel parser)." ></td>
	<td class="line x" title="20:68	These algorithms have been re-implemented by other researchers in the field, including Eugene Charniak for his n-best parser, Ryan McDonald for his dependency parser (McDonald et al. , 2005), Microsoft Research NLP group (Simon Corston-Oliver and Kevin Duh, p.c)." ></td>
	<td class="line x" title="21:68	for a similar model, Jonathan Graehl for the ISI syntax-based MT decoder, David A. Smith for the Dyna language (Eisner et al. , 2005), 223 and Jonathan May for ISIs tree automata package Tiburon." ></td>
	<td class="line x" title="22:68	All of these experiments confirmed the findings in our work." ></td>
	<td class="line x" title="23:68	2 Synchronous Binarization for MT Machine Translation has made very good progress in recent times, especially, the so-called phrasebased statistical systems (Och and Ney, 2004)." ></td>
	<td class="line x" title="24:68	In order to take a substantial next-step it will be necessary to incorporate several aspects of syntax." ></td>
	<td class="line x" title="25:68	Many researchers have explored syntax-based methods, for instance, Wu (1996) and Chiang (2005) both uses binary-branching synchronous context-free grammars (SCFGs)." ></td>
	<td class="line pc" title="26:68	However, to be more expressive and flexible, it is often easier to start with a general SCFG or tree-transducer (Galley et al. , 2004)." ></td>
	<td class="line x" title="27:68	In this case, binarization of the input grammar is required for the use of the CKY algorithm (in order to get cubic-time complexity), just as we convert a CFG into the Chomsky Normal Form (CNF) for monolingual parsing." ></td>
	<td class="line x" title="28:68	For synchronous grammars, however, different binarization schemes may result in very different-looking chart items that greatly affect decoding efficiency." ></td>
	<td class="line x" title="29:68	For example, consider the following SCFG rule: (1) S  NP(1) VP(2) PP(3), NP(1) PP(3) VP(2) We can binarize it either left-to-right or right-to-left: S  VNP-PP VP VNP-PP NP PP or S  NP VPP-VP VPP-VP  PP VP The intermediate symbols (e.g. VPP-VP) are called virtual nonterminals." ></td>
	<td class="line x" title="30:68	We would certainly prefer the right-to-left binarization because the virtual nonterminal has consecutive span (see Fig." ></td>
	<td class="line x" title="31:68	2)." ></td>
	<td class="line x" title="32:68	The left-toright binarization causes discontinuities on the target side, which results in an exponential time complexity when decoding with an integrated n-gram model." ></td>
	<td class="line x" title="33:68	We develop this intuition into a technique called synchronous binarization (Zhang et al. , 2006) which binarizes a synchronous production or treetranduction rule on both source and target sides simultaneously." ></td>
	<td class="line x" title="34:68	It essentially converts an SCFG into an equivalent ITG (the synchronous extension of CNF) if possible." ></td>
	<td class="line x" title="35:68	We reduce this problem to the binarization of the permutation of nonterminal symbols between the source and target sides of a synchronous rule and devise a linear-time algorithm NP NP PP VP VP PP target (English) source (Chinese) VPP-VP NP PP VP Chinese indices English boundary wor ds 1 2 4 7 Powell Powell held meeting with Sharon VPP-VP Figure 2: The alignment pattern (left) and alignment matrix (right) of the SCFG rule." ></td>
	<td class="line x" title="36:68	system BLEU monolingual binarization 36.25 synchronous binarization 38.44 Table 1: Synchronous vs. monolingual binarization in terms of translation quality (BLEU score)." ></td>
	<td class="line x" title="37:68	for it." ></td>
	<td class="line nc" title="38:68	Experiments show that the resulting rule set significantly improves the speed and accuracy over monolingual binarization (see Table 1) in a stateof-the-art syntax-based machine translation system (Galley et al. , 2004)." ></td>
	<td class="line x" title="39:68	We also propose another trick (hook) for further speeding up the decoding with integrated n-gram models (Huang et al. , 2005)." ></td>
	<td class="line x" title="40:68	3 Syntax-Directed Translation Syntax-directed translation was originally proposed for compiling programming languages (Irons, 1961; Lewis and Stearns, 1968), where the source program is parsed into a syntax-tree that guides the generation of the object code." ></td>
	<td class="line x" title="41:68	These translations have been formalized as a synchronous context-free grammar (SCFG) that generates two languages simultaneously (Aho and Ullman, 1972), and equivalently, as a top-down tree-to-string transducer (Gecseg and Steinby, 1984)." ></td>
	<td class="line x" title="42:68	We adapt this syntaxdirected transduction process to statistical MT by applying stochastic operations at each node of the source-language parse-tree and searching for the best derivation (a sequence of translation steps) that converts the whole tree into some target-language string (Huang et al. , 2006)." ></td>
	<td class="line x" title="43:68	3.1 Extended Domain of Locality From a modeling perspective, however, the structural divergence across languages results in nonisomorphic parse-trees that are not captured by 224 SCFGs." ></td>
	<td class="line x" title="44:68	For example, the S(VO) structure in English is translated into a VSO order in Arabic, an instance of complex re-ordering (Fig." ></td>
	<td class="line x" title="45:68	4)." ></td>
	<td class="line x" title="46:68	To alleviate this problem, grammars with richer expressive power have been proposed which can grab larger fragments of the tree." ></td>
	<td class="line oc" title="47:68	Following Galley et al.(2004), we use an extended tree-to-string transducer (xRs) with multi-level left-hand-side (LHS) trees.1 Since the right-hand-side (RHS) string can be viewed as a flat one-level tree with the same nonterminal root from LHS (Fig." ></td>
	<td class="line x" title="49:68	4), this framework is closely related to STSGs in having extended domain of locality on the source-side except for remaining a CFG on the target-side." ></td>
	<td class="line oc" title="50:68	These rules can be learned from a parallel corpus using English parsetrees, Chinese strings, and word alignment (Galley et al. , 2004)." ></td>
	<td class="line x" title="51:68	3.2 A Running Example Consider the following English sentence and its Chinese translation (note the reordering in the passive construction): (2) the gunman was killed by the police . qiangshou [gunman] bei [passive] jingfang [police] jibi [killed]  . Figure 3 shows how the translator works." ></td>
	<td class="line x" title="52:68	The English sentence (a) is first parsed into the tree in (b), which is then recursively converted into the Chinese string in (e) through five steps." ></td>
	<td class="line x" title="53:68	First, at the root node, we apply the rule r1 which preserves the toplevel word-order and translates the English period into its Chinese counterpart: (r1) S (x1:NP-C x2:VP PUNC ()." ></td>
	<td class="line x" title="54:68	)  x1 x2  Then, the rule r2 grabs the whole sub-tree for the gunman and translates it as a phrase: (r2) NP-C ( DT (the) NN (gunman) )  qiangshou Now we get a partial Chinese, partial English sentence qiangshou VP  as shown in Fig." ></td>
	<td class="line x" title="55:68	3 (c)." ></td>
	<td class="line x" title="56:68	Our recursion goes on to translate the VP sub-tree." ></td>
	<td class="line x" title="57:68	Here we use the rule r3 for the passive construction: 1we will use LHS and source-side interchangeably (so are RHS and target-side)." ></td>
	<td class="line x" title="58:68	In accordance with our experiments, we also use English and Chinese as the source and target languages, opposite to the Foreign-to-English convention of Brown et al.(1993)." ></td>
	<td class="line x" title="60:68	(a) the gunman was killed by the police . parser  (b) S NP-C DT the NN gunman VP VBD was VP-C VBN killed PP IN by NP-C DT the NN police PUNC . r1, r2  (c) qiangshou VP VBD was VP-C VBN killed PP IN by NP-C DT the NN police  r3  (d) qiangshou bei NP-C DT the NN police VBN killed  r5  r4  (e) qiangshou bei jingfang jibi  Figure 3: A synatx-directed translation process." ></td>
	<td class="line x" title="61:68	S NP(1) VP VB(2) NP(3), S VB(2) NP(1) NP(3) Figure 4: An example of complex re-ordering." ></td>
	<td class="line x" title="62:68	225 (r3) VP VBD was VP-C x1:VBN PP IN by x2:NP-C  bei x2 x1 which captures the fact that the agent (NP-C, the police) and the verb (VBN, killed) are always inverted between English and Chinese in a passive voice." ></td>
	<td class="line x" title="63:68	Finally, we apply rules r4 and r5 which perform phrasal translations for the two remaining subtrees in (d), respectively, and get the completed Chinese string in (e)." ></td>
	<td class="line x" title="64:68	3.3 Translation Algorithm Given a fixed parse-tree , the search for the best derivation (as a sequence of conversion steps) can be done by a simple top-down traversal (or depthfirst search) from the root of the tree." ></td>
	<td class="line x" title="65:68	With memoizationm, we get a dynamic programming algorithm that is guaranteed to run in O(n) time where n is the length of the input string, since the size of the parsetree is proportional to n. Similar algorithms have also been proposed for dependency-based translation (Lin, 2004; Ding and Palmer, 2005)." ></td>
	<td class="line x" title="66:68	I am currently performing large-scale experiments on English-to-Chinese translation using the xRs rules." ></td>
	<td class="line x" title="67:68	We are not doing the usual direction of Chinese-to-English partly due to the lack of a sufficiently good Chinese parser." ></td>
	<td class="line x" title="68:68	Initial results show promising translation quality (in terms of BLEU scores) and fast translation speed." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-1077
Tree-To-String Alignment Template For Statistical Machine Translation
Liu, Yang;Liu, Qun;Lin, Shouxun;"></td>
	<td class="line x" title="1:252	Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 609616, Sydney, July 2006." ></td>
	<td class="line x" title="2:252	c2006 Association for Computational Linguistics Tree-to-String Alignment Template for Statistical Machine Translation Yang Liu, Qun Liu, and Shouxun Lin Institute of Computing Technology Chinese Academy of Sciences No.6 Kexueyuan South Road, Haidian District P. O. Box 2704, Beijing, 100080, China {yliu,liuqun,sxlin}@ict.ac.cn Abstract We present a novel translation model based on tree-to-string alignment template (TAT) which describes the alignment between a source parse tree and a target string." ></td>
	<td class="line x" title="3:252	A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels." ></td>
	<td class="line x" title="4:252	The model is linguistically syntaxbased because TATs are extracted automatically from word-aligned, source side parsed parallel texts." ></td>
	<td class="line x" title="5:252	To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string." ></td>
	<td class="line x" title="6:252	Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models." ></td>
	<td class="line x" title="7:252	1 Introduction Phrase-based translation models (Marcu and Wong, 2002; Koehn et al. , 2003; Och and Ney, 2004), which go beyond the original IBM translation models (Brown et al. , 1993) 1 by modeling translations of phrases rather than individual words, have been suggested to be the state-of-theart in statistical machine translation by empirical evaluations." ></td>
	<td class="line x" title="8:252	In phrase-based models, phrases are usually strings of adjacent words instead of syntactic constituents, excelling at capturing local reordering and performing translations that are localized to 1The mathematical notation we use in this paper is taken from that paper: a source string fJ1 = f1,,fj,,fJ is to be translated into a target string eI1 = e1,,ei,,eI." ></td>
	<td class="line x" title="9:252	Here, I is the length of the target string, and J is the length of the source string." ></td>
	<td class="line x" title="10:252	substrings that are common enough to be observed on training data." ></td>
	<td class="line x" title="11:252	However, a key limitation of phrase-based models is that they fail to model reordering at the phrase level robustly." ></td>
	<td class="line x" title="12:252	Typically, phrase reordering is modeled in terms of offset positions at the word level (Koehn, 2004; Och and Ney, 2004), making little or no direct use of syntactic information." ></td>
	<td class="line x" title="13:252	Recent research on statistical machine translation has lead to the development of syntax-based models." ></td>
	<td class="line x" title="14:252	Wu (1997) proposes Inversion Transduction Grammars, treating translation as a process of parallel parsing of the source and target language via a synchronized grammar." ></td>
	<td class="line x" title="15:252	Alshawi et al.(2000) represent each production in parallel dependency tree as a finite transducer." ></td>
	<td class="line x" title="17:252	Melamed (2004) formalizes machine translation problem as synchronous parsing based on multitext grammars." ></td>
	<td class="line x" title="18:252	Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers." ></td>
	<td class="line x" title="19:252	Chiang (2005) presents a hierarchical phrasebased model that uses hierarchical phrase pairs, which are formally productions of a synchronous context-free grammar." ></td>
	<td class="line x" title="20:252	Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, a version of synchronous grammars defined on dependency trees." ></td>
	<td class="line x" title="21:252	All these approaches, though different in formalism, make use of synchronous grammars or tree-based transduction rules to model both source and target languages." ></td>
	<td class="line x" title="22:252	Another class of approaches make use of syntactic information in the target language alone, treating the translation problem as a parsing problem." ></td>
	<td class="line x" title="23:252	Yamada and Knight (2001) use a parser in the target language to train probabilities on a set of 609 operations that transform a target parse tree into a source string." ></td>
	<td class="line x" title="24:252	Paying more attention to source language analysis, Quirk et al.(2005) employ a source language dependency parser, a target language word segmentation component, and an unsupervised word alignment component to learn treelet translations from parallel corpus." ></td>
	<td class="line x" title="26:252	In this paper, we propose a statistical translation model based on tree-to-string alignment template which describes the alignment between a source parse tree and a target string." ></td>
	<td class="line x" title="27:252	A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels." ></td>
	<td class="line x" title="28:252	The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side parsed parallel texts." ></td>
	<td class="line x" title="29:252	To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string." ></td>
	<td class="line x" title="30:252	One advantage of our model is that TATs can be automatically acquired to capture linguistically motivated reordering at both low (word) and high (phrase, clause) levels." ></td>
	<td class="line x" title="31:252	In addition, the training of TAT-based model is less computationally expensive than tree-to-tree models." ></td>
	<td class="line oc" title="32:252	Similarly to (Galley et al. , 2004), the tree-to-string alignment templates discussed in this paper are actually transformation rules." ></td>
	<td class="line x" title="33:252	The major difference is that we model the syntax of the source language instead of the target side." ></td>
	<td class="line o" title="34:252	As a result, the task of our decoder is to find the best target string while Galleys is to seek the most likely target tree." ></td>
	<td class="line x" title="35:252	2 Tree-to-String Alignment Template A tree-to-string alignment template z is a triple T, S, A, which describes the alignment A between a source parse tree T = T(FJprime1 ) 2 and a target string S = EIprime1." ></td>
	<td class="line x" title="36:252	A source string FJprime1, which is the sequence of leaf nodes of T(FJprime1 ), consists of both terminals (source words) and nonterminals (phrasal categories)." ></td>
	<td class="line x" title="37:252	A target string EIprime1 is also composed of both terminals (target words) and non-terminals (placeholders)." ></td>
	<td class="line x" title="38:252	An alignment A is defined as a subset of the Cartesian product of source and target symbol positions: A {(j,i) : j = 1,,Jprime;i = 1,,Iprime} (1) 2We use T() to denote a parse tree." ></td>
	<td class="line x" title="39:252	To reduce notational overhead, we use T(z) to represent the parse tree in z. Similarly, S(z) denotes the string in z. Figure 1 shows three TATs automatically learned from training data." ></td>
	<td class="line x" title="40:252	Note that when demonstrating a TAT graphically, we represent non-terminals in the target strings by blanks." ></td>
	<td class="line x" title="41:252	NP NR  I NN 9d LCP NP NR S CC  NR LC W NP DNP NPDEG NP President Bush betweenUnitedStatesand Figure 1: Examples of tree-to-string alignment templates obtained in training In the following, we formally describe how to introduce tree-to-string alignment templates into probabilistic dependencies to model Pr(eI1|fJ1 ) 3." ></td>
	<td class="line x" title="42:252	In a first step, we introduce the hidden variable T(fJ1 ) that denotes a parse tree of the source sentence fJ1 : Pr(eI1|fJ1 ) = summationdisplay T(fJ1 ) Pr(eI1,T(fJ1 )|fJ1 ) (2) = summationdisplay T(fJ1 ) Pr(T(fJ1 )|fJ1 )Pr(eI1|T(fJ1 ),fJ1 ) (3) Next, another hidden variable D is introduced to detach the source parse tree T(fJ1 ) into a sequence of K subtrees TK1 with a preorder transversal." ></td>
	<td class="line x" title="43:252	We assume that each subtree Tk produces a target string Sk." ></td>
	<td class="line x" title="44:252	As a result, the sequence of subtrees TK1 produces a sequence of target strings SK1, which can be combined serially to generate the target sentence eI1." ></td>
	<td class="line x" title="45:252	We assume that Pr(eI1|D,T(fJ1 ),fJ1 )  Pr(SK1 |TK1 ) because eI1 is actually generated by the derivation of SK1 . Note that we omit an explicit dependence on the detachment D to avoid notational overhead." ></td>
	<td class="line x" title="46:252	Pr(eI1|T(fJ1 ),fJ1 ) = summationdisplay D Pr(eI1,D|T(fJ1 ),fJ1 ) (4) = summationdisplay D Pr(D|T(fJ1 ),fJ1 )Pr(eI1|D,T(fJ1 ),fJ1 ) (5) = summationdisplay D Pr(D|T(fJ1 ),fJ1 )Pr(SK1 |TK1 ) (6) = summationdisplay D Pr(D|T(fJ1 ),fJ1 ) Kproductdisplay k=1 Pr(Sk|Tk) (7) 3The notational convention will be as follows." ></td>
	<td class="line x" title="47:252	We use the symbol Pr() to denote general probability distribution with no specific assumptions." ></td>
	<td class="line x" title="48:252	In contrast, for model-based probability distributions, we use generic symbol p()." ></td>
	<td class="line x" title="49:252	610 NP DNP NP NR S DEG  NP NN 6 NN ?Z NP DNP NPDEG  NP NP NR S NP NNNN NN 6 NN ?Z S6?Z parsing detachmentproduction of China economicdevelopment combination economicdevelopmentofChina Figure 2: Graphic illustration for translation process To further decompose Pr(S|T), the tree-tostring alignment template, denoted by the variable z, is introduced as a hidden variable." ></td>
	<td class="line x" title="50:252	Pr(S|T) = summationdisplay z Pr(S,z|T) (8) = summationdisplay z Pr(z|T)Pr(S|z, T) (9) Therefore, the TAT-based translation model can be decomposed into four sub-models: 1." ></td>
	<td class="line x" title="51:252	parse model: Pr(T(fJ1 )|fJ1 ) 2." ></td>
	<td class="line x" title="52:252	detachment model: Pr(D|T(fJ1 ),fJ1 ) 3." ></td>
	<td class="line x" title="53:252	TAT selection model: Pr(z|T) 4." ></td>
	<td class="line x" title="54:252	TAT application model: Pr(S|z, T) Figure 2 shows how TATs work to perform translation." ></td>
	<td class="line x" title="55:252	First, the input source sentence is parsed." ></td>
	<td class="line x" title="56:252	Next, the parse tree is detached into five subtrees with a preorder transversal." ></td>
	<td class="line x" title="57:252	For each subtree, a TAT is selected and applied to produce a string." ></td>
	<td class="line x" title="58:252	Finally, these strings are combined serially to generate the translation (we use X to denote the non-terminal): X1  X2 of X3  X2 of China  X3 X4 of China  economic X4 of China  economic development of China Following Och and Ney (2002), we base our model on log-linear framework." ></td>
	<td class="line x" title="59:252	Hence, all knowledge sources are described as feature functions that include the given source string fJ1, the target string eI1, and hidden variables." ></td>
	<td class="line x" title="60:252	The hidden variable T(fJ1 ) is omitted because we usually make use of only single best output of a parser." ></td>
	<td class="line x" title="61:252	As we assume that all detachment have the same probability, the hidden variable D is also omitted." ></td>
	<td class="line x" title="62:252	As a result, the model we actually adopt for experiments is limited because the parse, detachment, and TAT application sub-models are simplified." ></td>
	<td class="line x" title="63:252	Pr(eI1,zK1 |fJ1 ) = exp[ summationtextM m=1 mhm(eI1,fJ1,zK1 )]summationtext eprimeI1,zprimeK1 exp[ summationtextM m=1 mhm(eprime I1,fJ 1,zprime K1 )] eI1 = argmax eI1,zK1 braceleftbigg Msummationdisplay m=1 mhm(eI1,fJ1,zK1 ) bracerightbigg For our experiments we use the following seven feature functions 4 that are analogous to default feature set of Pharaoh (Koehn, 2004)." ></td>
	<td class="line x" title="64:252	To simplify the notation, we omit the dependence on the hidden variables of the model." ></td>
	<td class="line x" title="65:252	h1(eI1,fJ1 ) = log Kproductdisplay k=1 N(z)(T(z), Tk) N(T(z)) h2(eI1,fJ1 ) = log Kproductdisplay k=1 N(z)(T(z), Tk) N(S(z)) h3(eI1,fJ1 ) = log Kproductdisplay k=1 lex(T(z)|S(z))(T(z), Tk) h4(eI1,fJ1 ) = log Kproductdisplay k=1 lex(S(z)|T(z))(T(z), Tk) h5(eI1,fJ1 ) = K h6(eI1,fJ1 ) = log Iproductdisplay i=1 p(ei|ei2,ei1) h7(eI1,fJ1 ) = I 4When computing lexical weighting features (Koehn et al. , 2003), we take only terminals into account." ></td>
	<td class="line x" title="66:252	If there are no terminals, we set the feature value to 1." ></td>
	<td class="line x" title="67:252	We use lex() to denote lexical weighting." ></td>
	<td class="line x" title="68:252	We denote the number of TATs used for decoding by K and the length of target string by I. 611 Tree String Alignment ( NR I) Bush 1:1 ( NN9d) President 1:1 ( VV?V) made 1:1 ( NN) speech 1:1 ( NP ( NR ) ( NN ) ) X1 |X2 1:2 2:1 ( NP ( NR I) ( NN ) ) X | Bush 1:2 2:1 ( NP ( NR ) ( NN9d) ) President |X 1:2 2:1 ( NP ( NR I) ( NN9d) ) President | Bush 1:2 2:1 ( VP ( VV ) ( NN ) ) X1 | a |X2 1:1 2:3 ( VP ( VV?V) ( NN ) ) made | a |X 1:1 2:3 ( VP ( VV ) ( NN) ) X | a | speech 1:1 2:3 ( VP ( VV?V) ( NN) ) made | a | speech 1:1 2:3 ( IP ( NP ) ( VP ) ) X1 |X2 1:1 2:2 Table 1: Examples of TATs extracted from the TSA in Figure 3 with h = 2 and c = 2 3 Training To extract tree-to-string alignment templates from a word-aligned, source side parsed sentence pair T(fJ1 ),eI1,A, we need first identify TSAs (TreeString-Alignment) using similar criterion as suggested in (Och and Ney, 2004)." ></td>
	<td class="line x" title="69:252	A TSA is a triple T(fj2j1 ),ei2i1, A) that is in accordance with the following constraints: 1." ></td>
	<td class="line x" title="70:252	(i,j)  A : i1  i  i2  j1  j  j2 2." ></td>
	<td class="line x" title="71:252	T(fj2j1 ) is a subtree of T(fJ1 ) Given a TSA T(fj2j1 ),ei2i1, A, a triple T(fj4j3 ),ei4i3, A is its sub TSA if and only if: 1." ></td>
	<td class="line x" title="72:252	T(fj4j3 ),ei4i3, A is a TSA 2." ></td>
	<td class="line x" title="73:252	T(fj4j3 ) is rooted at the direct descendant of the root node of T(fj1j2 ) 3." ></td>
	<td class="line x" title="74:252	i1  i3  i4  i2 4." ></td>
	<td class="line x" title="75:252	(i,j)  A : i3  i  i4  j3  j  j4 Basically, we extract TATs from a TSA T(fj2j1 ),ei2i1, A using the following two rules: 1." ></td>
	<td class="line x" title="76:252	If T(fj2j1 ) contains only one node, then T(fj2j1 ),ei2i1, A is a TAT 2." ></td>
	<td class="line x" title="77:252	If the height of T(fj2j1 ) is greater than one, then build TATs using those extracted from sub TSAs of T(fj2j1 ),ei2i1, A." ></td>
	<td class="line x" title="78:252	IP NP NR  I NN 9d VP VV ?V NN  President Bushmadea speech Figure 3: An example of TSA Usually, we can extract a very large amount of TATs from training data using the above rules, making both training and decoding very slow." ></td>
	<td class="line x" title="79:252	Therefore, we impose three restrictions to reduce the magnitude of extracted TATs: 1." ></td>
	<td class="line x" title="80:252	A third constraint is added to the definition of TSA: jprime,jprimeprime : j1  jprime  j2 and j1  jprimeprime  j2 and (i1,jprime)  A and (i2,jprimeprime)  A This constraint requires that both the first and last symbols in the target string must be aligned to some source symbols." ></td>
	<td class="line x" title="81:252	2." ></td>
	<td class="line x" title="82:252	The height of T(z) is limited to no greater than h. 3." ></td>
	<td class="line x" title="83:252	The number of direct descendants of a node of T(z) is limited to no greater than c. Table 1 shows the TATs extracted from the TSA in Figure 3 with h = 2 and c = 2." ></td>
	<td class="line x" title="84:252	As we restrict that T(fj2j1 ) must be a subtree of T(fJ1 ), TATs may be treated as syntactic hierar612 chical phrase pairs (Chiang, 2005) with tree structure on the source side." ></td>
	<td class="line x" title="85:252	At the same time, we face the risk of losing some useful non-syntactic phrase pairs." ></td>
	<td class="line x" title="86:252	For example, the phrase pair  I9d?V President Bush made can never be obtained in form of TAT from the TSA in Figure 3 because there is no subtree for that source string." ></td>
	<td class="line x" title="87:252	4 Decoding We approach the decoding problem as a bottom-up beam search." ></td>
	<td class="line x" title="88:252	To translate a source sentence, we employ a parser to produce a parse tree." ></td>
	<td class="line x" title="89:252	Moving bottomup through the source parse tree, we compute a list of candidate translations for the input subtree rooted at each node with a postorder transversal." ></td>
	<td class="line x" title="90:252	Candidate translations of subtrees are placed in stacks." ></td>
	<td class="line x" title="91:252	Figure 4 shows the organization of candidate translation stacks." ></td>
	<td class="line x" title="92:252	NP DNP NP NR S DEG  NP NN 6 NN ?Z 8 4 7 2 3 5 6 1  1  2  3  4  5  6  7  8 Figure 4: Candidate translations of subtrees are placed in stacks according to the root index set by postorder transversal A candidate translation contains the following information: 1." ></td>
	<td class="line x" title="93:252	the partial translation 2." ></td>
	<td class="line x" title="94:252	the accumulated feature values 3." ></td>
	<td class="line x" title="95:252	the accumulated probability A TAT z is usable to a parse tree T if and only if T(z) is rooted at the root of T and covers part of nodes of T. Given a parse tree T, we find all usable TATs." ></td>
	<td class="line x" title="96:252	Given a usable TAT z, if T(z) is equal to T, then S(z) is a candidate translation of T. If T(z) covers only a portion of T, we have to compute a list of candidate translations for T by replacing the non-terminals of S(z) with candidate translations of the corresponding uncovered subtrees." ></td>
	<td class="line x" title="97:252	NP DNP NPDEG  NP 8 4 7 2 3 of  1  2  3  4  5  6  7  8 Figure 5: Candidate translation construction For example, when computing the candidate translations for the tree rooted at node 8, the TAT used in Figure 5 covers only a portion of the parse tree in Figure 4." ></td>
	<td class="line x" title="98:252	There are two uncovered subtrees that are rooted at node 2 and node 7 respectively." ></td>
	<td class="line x" title="99:252	Hence, we replace the third symbol with the candidate translations in stack 2 and the first symbol with the candidate translations in stack 7." ></td>
	<td class="line x" title="100:252	At the same time, the feature values and probabilities are also accumulated for the new candidate translations." ></td>
	<td class="line x" title="101:252	To speed up the decoder, we limit the search space by reducing the number of TATs used for each input node." ></td>
	<td class="line x" title="102:252	There are two ways to limit the TAT table size: by a fixed limit (tatTable-limit) of how many TATs are retrieved for each input node, and by a probability threshold (tatTable-threshold) that specify that the TAT probability has to be above some value." ></td>
	<td class="line x" title="103:252	On the other hand, instead of keeping the full list of candidates for a given node, we keep a top-scoring subset of the candidates." ></td>
	<td class="line x" title="104:252	This can also be done by a fixed limit (stack-limit) or a threshold (stack-threshold)." ></td>
	<td class="line x" title="105:252	To perform recombination, we combine candidate translations that share the same leading and trailing bigrams in each stack." ></td>
	<td class="line x" title="106:252	5 Experiments Our experiments were on Chinese-to-English translation." ></td>
	<td class="line x" title="107:252	The training corpus consists of 31,149 sentence pairs with 843,256 Chinese words and 613 System Features BLEU4 d + (e|f) 0.0573  0.0033 Pharaoh d + lm + (e|f) + wp 0.2019  0.0083 d + lm + (f|e) + lex(f|e) + (e|f) + lex(e|f) + pp + wp 0.2089  0.0089 h1 0.1639  0.0077 Lynx h1 + h6 + h7 0.2100  0.0089 h1 + h2 + h3 + h4 + h5 + h6 + h7 0.2178  0.0080 Table 2: Comparison of Pharaoh and Lynx with different feature settings on the test corpus 949,583 English words." ></td>
	<td class="line x" title="108:252	For the language model, we used SRI Language Modeling Toolkit (Stolcke, 2002) to train a trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on the 31,149 English sentences." ></td>
	<td class="line x" title="109:252	We selected 571 short sentences from the 2002 NIST MT Evaluation test set as our development corpus, and used the 2005 NIST MT Evaluation test set as our test corpus." ></td>
	<td class="line x" title="110:252	We evaluated the translation quality using the BLEU metric (Papineni et al. , 2002), as calculated by mteval-v11b.pl with its default setting except that we used case-sensitive matching of n-grams." ></td>
	<td class="line x" title="111:252	5.1 Pharaoh The baseline system we used for comparison was Pharaoh (Koehn et al. , 2003; Koehn, 2004), a freely available decoder for phrase-based translation models: p(e|f) = p(f|e) pLM(e)LM  pD(e,f)D length(e)W(e) (10) We ran GIZA++ (Och and Ney, 2000) on the training corpus in both directions using its default setting, and then applied the refinement rule diagand described in (Koehn et al. , 2003) to obtain a single many-to-many word alignment for each sentence pair." ></td>
	<td class="line x" title="112:252	After that, we used some heuristics, which including rule-based translation of numbers, dates, and person names, to further improve the alignment accuracy." ></td>
	<td class="line x" title="113:252	Given the word-aligned bilingual corpus, we obtained 1,231,959 bilingual phrases (221,453 used on test corpus) using the training toolkits publicly released by Philipp Koehn with its default setting." ></td>
	<td class="line x" title="114:252	To perform minimum error rate training (Och, 2003) to tune the feature weights to maximize the systems BLEU score on development set, we used optimizeV5IBMBLEU.m (Venugopal and Vogel, 2005)." ></td>
	<td class="line x" title="115:252	We used default pruning settings for Pharaoh except that we set the distortion limit to 4." ></td>
	<td class="line x" title="116:252	5.2 Lynx On the same word-aligned training data, it took us about one month to parse all the 31,149 Chinese sentences using a Chinese parser written by Deyi Xiong (Xiong et al. , 2005)." ></td>
	<td class="line x" title="117:252	The parser was trained on articles 1270 of Penn Chinese Treebank version 1.0 and achieved 79.4% (F1 measure) as well as a 4.4% relative decrease in error rate." ></td>
	<td class="line x" title="118:252	Then, we performed TAT extraction described in section 3 with h = 3 and c = 5 and obtained 350,575 TATs (88,066 used on test corpus)." ></td>
	<td class="line x" title="119:252	To run our decoder Lynx on development and test corpus, we set tatTable-limit = 20, tatTable-threshold = 0, stack-limit = 100, and stack-threshold = 0.00001." ></td>
	<td class="line x" title="120:252	5.3 Results Table 2 shows the results on test set using Pharaoh and Lynx with different feature settings." ></td>
	<td class="line x" title="121:252	The 95% confidence intervals were computed using Zhangs significance tester (Zhang et al. , 2004)." ></td>
	<td class="line x" title="122:252	We modified it to conform to NISTs current definition of the BLEU brevity penalty." ></td>
	<td class="line x" title="123:252	For Pharaoh, eight features were used: distortion model d, a trigram language model lm, phrase translation probabilities (f|e) and (e|f), lexical weightings lex(f|e) and lex(e|f), phrase penalty pp, and word penalty wp." ></td>
	<td class="line x" title="124:252	For Lynx, seven features described in section 2 were used." ></td>
	<td class="line x" title="125:252	We find that Lynx outperforms Pharaoh with all feature settings." ></td>
	<td class="line x" title="126:252	With full features, Lynx achieves an absolute improvement of 0.006 over Pharaoh (3.1% relative)." ></td>
	<td class="line x" title="127:252	This difference is statistically significant (p < 0.01)." ></td>
	<td class="line x" title="128:252	Note that Lynx made use of only 88,066 TATs on test corpus while 221,453 bilingual phrases were used for Pharaoh." ></td>
	<td class="line x" title="129:252	The feature weights obtained by minimum er614 FeaturesSystem d lm (f|e) lex(f|e) (e|f) lex(e|f) pp wp Pharaoh 0.0476 0.1386 0.0611 0.0459 0.1723 0.0223 0.3122 -0.2000 Lynx 0.3735 0.0061 0.1081 0.1656 0.0022 0.0824 0.2620 Table 3: Feature weights obtained by minimum error rate training on the development corpus BLEU4 tat 0.2178  0.0080 tat + bp 0.2240  0.0083 Table 4: Effect of using bilingual phrases for Lynx ror rate training for both Pharaoh and Lynx are shown in Table 3." ></td>
	<td class="line x" title="130:252	We find that (f|e) (i.e. h2) is not a helpful feature for Lynx." ></td>
	<td class="line x" title="131:252	The reason is that we use only a single non-terminal symbol instead of assigning phrasal categories to the target string." ></td>
	<td class="line x" title="132:252	In addition, we allow the target string consists of only non-terminals, making translation decisions not always based on lexical evidence." ></td>
	<td class="line x" title="133:252	5.4 Using bilingual phrases It is interesting to use bilingual phrases to strengthen the TAT-based model." ></td>
	<td class="line x" title="134:252	As we mentioned before, some useful non-syntactic phrase pairs can never be obtained in form of TAT because we restrict that there must be a corresponding parse tree for the source phrase." ></td>
	<td class="line x" title="135:252	Moreover, it takes more time to obtain TATs than bilingual phrases on the same training data because parsing is usually very time-consuming." ></td>
	<td class="line x" title="136:252	Given an input subtree T(Fj2j1 ), if Fj2j1 is a string of terminals, we find all bilingual phrases that the source phrase is equal to Fj2j1 . Then we build a TAT for each bilingual phrase fJprime1,eIprime1, A: the tree of the TAT is T(Fj2j1 ), the string is eIprime1, and the alignment is A. If a TAT built from a bilingual phrase is the same with a TAT in the TAT table, we prefer to the greater translation probabilities." ></td>
	<td class="line x" title="137:252	Table 4 shows the effect of using bilingual phrases for Lynx." ></td>
	<td class="line x" title="138:252	Note that these bilingual phrases are the same with those used for Pharaoh." ></td>
	<td class="line x" title="139:252	5.5 Results on large data We also conducted an experiment on large data to further examine our design philosophy." ></td>
	<td class="line x" title="140:252	The training corpus contains 2.6 million sentence pairs." ></td>
	<td class="line x" title="141:252	We used all the data to extract bilingual phrases and a portion of 800K pairs to obtain TATs." ></td>
	<td class="line x" title="142:252	Two trigram language models were used for Lynx." ></td>
	<td class="line x" title="143:252	One was trained on the 2.6 million English sentences and another was trained on the first 1/3 of the Xinhua portion of Gigaword corpus." ></td>
	<td class="line x" title="144:252	We also included rule-based translations of named entities, dates, and numbers." ></td>
	<td class="line x" title="145:252	By making use of these data, Lynx achieves a BLEU score of 0.2830 on the 2005 NIST Chinese-to-English MT evaluation test set, which is a very promising result for linguistically syntax-based models." ></td>
	<td class="line x" title="146:252	6 Conclusion In this paper, we introduce tree-to-string alignment templates, which can be automatically learned from syntactically-annotated training data." ></td>
	<td class="line x" title="147:252	The TAT-based translation model improves translation quality significantly compared with a stateof-the-art phrase-based decoder." ></td>
	<td class="line x" title="148:252	Treated as special TATs without tree on the source side, bilingual phrases can be utilized for the TAT-based model to get further improvement." ></td>
	<td class="line x" title="149:252	It should be emphasized that the restrictions we impose on TAT extraction limit the expressive power of TAT." ></td>
	<td class="line x" title="150:252	Preliminary experiments reveal that removing these restrictions does improve translation quality, but leads to large memory requirements." ></td>
	<td class="line x" title="151:252	We feel that both parsing and word alignment qualities have important effects on the TATbased model." ></td>
	<td class="line x" title="152:252	We will retrain the Chinese parser on Penn Chinese Treebank version 5.0 and try to improve word alignment quality using log-linear models as suggested in (Liu et al. , 2005)." ></td>
	<td class="line x" title="153:252	Acknowledgement This work is supported by National High Technology Research and Development Program contract Generally Technical Research and Basic Database Establishment of Chinese Platform(Subject No. 2004AA114010)." ></td>
	<td class="line x" title="154:252	We are grateful to Deyi Xiong for providing the parser and Haitao Mi for making the parser more efficient and robust." ></td>
	<td class="line x" title="155:252	Thanks to Dr. Yajuan Lv for many helpful comments on an earlier draft of this paper." ></td>
	<td class="line x" title="156:252	615 References Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas." ></td>
	<td class="line x" title="157:252	2000." ></td>
	<td class="line x" title="158:252	Learning dependency translation models as collections of finite-state head transducers." ></td>
	<td class="line x" title="159:252	Computational Linguistics, 26(1):45-60." ></td>
	<td class="line x" title="160:252	Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer." ></td>
	<td class="line x" title="161:252	1993." ></td>
	<td class="line x" title="162:252	The mathematics of statistical machine translation: Parameter estimation." ></td>
	<td class="line x" title="163:252	Computational Linguistics, 19(2):263-311." ></td>
	<td class="line x" title="164:252	Stanley F. Chen and Joshua Goodman." ></td>
	<td class="line x" title="165:252	1998." ></td>
	<td class="line x" title="166:252	Am empirical study of smoothing techniques for language modeling." ></td>
	<td class="line x" title="167:252	Technical Report TR-10-98, Harvard University Center for Research in Computing Technology." ></td>
	<td class="line x" title="168:252	David Chiang." ></td>
	<td class="line x" title="169:252	2005." ></td>
	<td class="line x" title="170:252	A hierarchical phrase-based model for statistical machine translation." ></td>
	<td class="line x" title="171:252	In Proceedings of 43rd Annual Meeting of the ACL, pages 263-270." ></td>
	<td class="line x" title="172:252	Yuan Ding and Martha Palmer." ></td>
	<td class="line x" title="173:252	2005." ></td>
	<td class="line x" title="174:252	Machine translation using probabilistic synchronous dependency insert grammars." ></td>
	<td class="line x" title="175:252	In Proceedings of 43rd Annual Meeting of the ACL, pages 541-548." ></td>
	<td class="line x" title="176:252	Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu." ></td>
	<td class="line x" title="177:252	2004." ></td>
	<td class="line x" title="178:252	Whats in a translation rule?" ></td>
	<td class="line x" title="179:252	In Proceedings of NAACL-HLT 2004, pages 273280." ></td>
	<td class="line x" title="180:252	Jonathan Graehl and Kevin Knight." ></td>
	<td class="line x" title="181:252	2004." ></td>
	<td class="line x" title="182:252	Training tree transducers." ></td>
	<td class="line x" title="183:252	In Proceedings of NAACL-HLT 2004, pages 105-112." ></td>
	<td class="line x" title="184:252	Philipp Koehn, Franz J. Och, and Daniel Marcu." ></td>
	<td class="line x" title="185:252	2003." ></td>
	<td class="line x" title="186:252	Statistical phrase-based translation." ></td>
	<td class="line x" title="187:252	In Proceedings of HLT-NAACL 2003, pages 127-133." ></td>
	<td class="line x" title="188:252	Philipp Koehn." ></td>
	<td class="line x" title="189:252	2004." ></td>
	<td class="line x" title="190:252	Pharaoh: a beam search decoder for phrase-based statistical machine trnaslation models." ></td>
	<td class="line x" title="191:252	In Proceedings of the Sixth Conference of the Association for Machine Translation in the Americas, pages 115-124." ></td>
	<td class="line x" title="192:252	Yang Liu, Qun Liu, and Shouxun Lin." ></td>
	<td class="line x" title="193:252	2005." ></td>
	<td class="line x" title="194:252	Loglinear models for word alignment." ></td>
	<td class="line x" title="195:252	In Proceedings of 43rd Annual Meeting of the ACL, pages 459-466." ></td>
	<td class="line x" title="196:252	Daniel Marcu and William Wong." ></td>
	<td class="line x" title="197:252	2002." ></td>
	<td class="line x" title="198:252	A phrasebased, joint probability model for statistical machine translation." ></td>
	<td class="line x" title="199:252	In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 133-139." ></td>
	<td class="line x" title="200:252	Dan Melamed." ></td>
	<td class="line x" title="201:252	2004." ></td>
	<td class="line x" title="202:252	Statistical machine translation by parsing." ></td>
	<td class="line x" title="203:252	In Proceedings of 42nd Annual Meeting of the ACL, pages 653-660." ></td>
	<td class="line x" title="204:252	Franz J. Och and Hermann Ney." ></td>
	<td class="line x" title="205:252	2000." ></td>
	<td class="line x" title="206:252	Improved statistical alignment models." ></td>
	<td class="line x" title="207:252	In Proceedings of 38th Annual Meeting of the ACL, pages 440-447." ></td>
	<td class="line x" title="208:252	Franz J. Och and Hermann Ney." ></td>
	<td class="line x" title="209:252	2002." ></td>
	<td class="line x" title="210:252	Discriminative training and maximum entropy models for statistical machine translation." ></td>
	<td class="line x" title="211:252	In Proceedings of 40th Annual Meeting of the ACL, pages 295-302." ></td>
	<td class="line x" title="212:252	Franz J. Och and Hermann Ney." ></td>
	<td class="line x" title="213:252	2004." ></td>
	<td class="line x" title="214:252	The alignment template approach to statistical machine translation." ></td>
	<td class="line x" title="215:252	Computational Linguistics, 30(4):417-449." ></td>
	<td class="line x" title="216:252	Franz J. Och." ></td>
	<td class="line x" title="217:252	2003." ></td>
	<td class="line x" title="218:252	Minimum error rate training in statistical machine translation." ></td>
	<td class="line x" title="219:252	In Proceedings of 41st Annual Meeting of the ACL, pages 160-167." ></td>
	<td class="line x" title="220:252	Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu." ></td>
	<td class="line x" title="221:252	2002." ></td>
	<td class="line x" title="222:252	BLEU: a method for automatic evaluation of machine translation." ></td>
	<td class="line x" title="223:252	In Proceedings of 40th Annual Meeting of the ACL, pages 311-318." ></td>
	<td class="line x" title="224:252	Chris Quirk, Arul Menezes, and Colin Cherry." ></td>
	<td class="line x" title="225:252	2005." ></td>
	<td class="line x" title="226:252	Dependency treelet translation: Syntactically informed phrasal SMT." ></td>
	<td class="line x" title="227:252	In Proceedings of 43rd Annual Meeting of the ACL, pages 271-279." ></td>
	<td class="line x" title="228:252	Andreas Stolcke." ></td>
	<td class="line x" title="229:252	2002." ></td>
	<td class="line x" title="230:252	SRILM an extensible language modeling toolkit." ></td>
	<td class="line x" title="231:252	In Proceedings of International Conference on Spoken Language Processing, volume 2, pages 901-904." ></td>
	<td class="line x" title="232:252	Ashish Venugopal and Stephan Vogel." ></td>
	<td class="line x" title="233:252	2005." ></td>
	<td class="line x" title="234:252	Considerations in maximum mutual information and minimum classification error training for statistical machine translation." ></td>
	<td class="line x" title="235:252	In Proceedings of the Tenth Conference of the European Association for Machine Translation (EAMT-05)." ></td>
	<td class="line x" title="236:252	Dekai Wu." ></td>
	<td class="line x" title="237:252	1997." ></td>
	<td class="line x" title="238:252	Stochastic inversion transduction grammars and bilingual parsing of parallel corpora." ></td>
	<td class="line x" title="239:252	Computational Linguistics, 23(3):377-403." ></td>
	<td class="line x" title="240:252	Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and Yueliang Qian." ></td>
	<td class="line x" title="241:252	2005." ></td>
	<td class="line x" title="242:252	Parsing the Penn Chinese treebank with semantic knowledge." ></td>
	<td class="line x" title="243:252	In Proceedings of IJCNLP 2005, pages 70-81." ></td>
	<td class="line x" title="244:252	Kenji Yamada and Kevin Knight." ></td>
	<td class="line x" title="245:252	2001." ></td>
	<td class="line x" title="246:252	A syntaxbased statistical translation model." ></td>
	<td class="line x" title="247:252	In Proceedings of 39th Annual Meeting of the ACL, pages 523-530." ></td>
	<td class="line x" title="248:252	Ying Zhang, Stephan Vogel, and Alex Waibel." ></td>
	<td class="line x" title="249:252	2004." ></td>
	<td class="line x" title="250:252	Interpreting BLEU/NIST scores: How much improvement do we need to have a better system?" ></td>
	<td class="line x" title="251:252	In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC), pages 2051-2054." ></td>
	<td class="line x" title="252:252	616" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-1121
Scalable Inference And Training Of Context-Rich Syntactic Translation Models
Galley, Michel;Graehl, Jonathan;Knight, Kevin;Marcu, Daniel;DeNeefe, Steve;Wang, Wei;Thayer, Ignacio;"></td>
	<td class="line x" title="1:176	Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 961968, Sydney, July 2006." ></td>
	<td class="line x" title="2:176	c2006 Association for Computational Linguistics Scalable Inference and Training of Context-Rich Syntactic Translation Models Michel Galley*, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang and Ignacio Thayer *Columbia University Dept. of Computer Science New York, NY 10027 galley@cs.columbia.edu, {graehl,knight,marcu,sdeneefe}@isi.edu, wwang@languageweaver.com, thayer@google.com University of Southern California Information Sciences Institute Marina del Rey, CA 90292 Language Weaver, Inc. 4640 Admiralty Way Marina del Rey, CA 90292 Abstract Statistical MT has made great progress in the last few years, but current translation models are weak on re-ordering and target language fluency." ></td>
	<td class="line x" title="3:176	Syntactic approaches seek to remedy these problems." ></td>
	<td class="line oc" title="4:176	In this paper, we take the framework for acquiring multi-level syntactic translation rules of (Galley et al. , 2004) from aligned tree-string pairs, and present two main extensions of their approach: first, instead of merely computing a single derivation that minimally explains a sentence pair, we construct a large number of derivations that include contextually richer rules, and account for multiple interpretations of unaligned words." ></td>
	<td class="line x" title="5:176	Second, we propose probability estimates and a training procedure for weighting these rules." ></td>
	<td class="line x" title="6:176	We contrast different approaches on real examples, show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated, and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules." ></td>
	<td class="line x" title="7:176	1 Introduction While syntactic approaches seek to remedy wordordering problems common to statistical machine translation (SMT) systems, many of the earlier modelsparticularly child re-ordering models fail to account for human translation behavior." ></td>
	<td class="line oc" title="8:176	Galley et al.(2004) alleviate this modeling problem and present a method for acquiring millions of syntactic transfer rules from bilingual corpora, which we review below." ></td>
	<td class="line x" title="10:176	Here, we make the following new contributions: (1) we show how to acquire larger rules that crucially condition on more syntactic context, and show how to compute multiple derivations for each training example, capturing both large and small rules, as well as multiple interpretations for unaligned words; (2) we develop probability models for these multilevel transfer rules, and give estimation methods for assigning probabilities to very large rule sets." ></td>
	<td class="line nc" title="11:176	We contrast our work with (Galley et al. , 2004), highlight some severe limitations of probability estimates computed from single derivations, and demonstrate that it is critical to account for many derivations for each sentence pair." ></td>
	<td class="line x" title="12:176	We also use real examples to show that our probability models estimated from a large number of derivations favor phrasal re-orderings that are linguistically well motivated." ></td>
	<td class="line x" title="13:176	An empirical evaluation against a state-of-the-art SMT system similar to (Och and Ney, 2004) indicates positive prospects." ></td>
	<td class="line nc" title="14:176	Finally, we show that our contextually richer rules provide a 3.63 BLEU point increase over those of (Galley et al. , 2004)." ></td>
	<td class="line x" title="15:176	2 Inferring syntactic transformations We assume we are given a source-language (e.g. , French) sentence f, a target-language (e.g. , English) parse tree pi, whose yield e is a translation of f, and a word alignment a between f and e. Our aim is to gain insight into the process of transforming pi into f and to discover grammaticallygrounded translation rules." ></td>
	<td class="line oc" title="16:176	For this, we need a formalism that is expressive enough to deal with cases of syntactic divergence between source and target languages (Fox, 2002): for any given (pi,f,a) triple, it is useful to produce a derivation that minimally explains the transformation between pi and f, while remaining consistent with a. Galley et al.(2004) present one such formalism (henceforth GHKM)." ></td>
	<td class="line x" title="18:176	2.1 Tree-to-string alignments It is appealing to model the transformation of pi into f using tree-to-string (xRs) transducers, since their theory has been worked out in an extensive literature and is well understood (see, e.g., (Graehl and Knight, 2004))." ></td>
	<td class="line oc" title="19:176	Formally, transformational rules ri presented in (Galley et al. , 2004) are equivalent to 1-state xRs transducers mapping a given pattern (subtree to match in pi) to a right hand side string." ></td>
	<td class="line o" title="20:176	We will refer to them as lhs(ri) and rhs(ri), respectively." ></td>
	<td class="line x" title="21:176	For example, some xRs 961 rules may describe the transformation of does not intone pasinFrench." ></td>
	<td class="line o" title="22:176	Aparticularinstancemay look like this: VP(AUX(does), RB(not), x0:VB)  ne, x0, pas lhs(ri) can be any arbitrary syntax tree fragment." ></td>
	<td class="line x" title="23:176	Its leaves are either lexicalized (e.g. does) or variables (x0, x1, etc)." ></td>
	<td class="line x" title="24:176	rhs(ri) is represented as a sequence of target-language words and variables." ></td>
	<td class="line o" title="25:176	Now we give a brief overview of how such transformational rules are acquired automatically in GHKM.1 In Figure 1, the (pi,f,a) triple is represented as a directed graph G (edges going downward), with no distinction between edges of pi and alignments." ></td>
	<td class="line x" title="26:176	Each node of the graph is labeled with its span and complement span (the latter in italic in the figure)." ></td>
	<td class="line x" title="27:176	The span of a node n is defined by the indices of the first and last word in f that are reachable from n. The complement span of n is the union of the spans of all nodes nprime in G that are neither descendants nor ancestors of n. Nodes of G whose spans and complement spans are nonoverlapping form the frontier set F  G. What is particularly interesting about the frontier set?" ></td>
	<td class="line x" title="28:176	For any frontier of graph G containing a given node n  F, spans on that frontier define an ordering between n and each other frontier node nprime." ></td>
	<td class="line x" title="29:176	For example, the span of VP[4-5] either precedes or follows, but never overlaps the span of any node nprime on any graph frontier." ></td>
	<td class="line x" title="30:176	This property does nothold fornodes outside of F. For instance, PP[4-5] and VBG[4] are two nodes of the same graph frontier, but they cannot be ordered because of their overlapping spans." ></td>
	<td class="line x" title="31:176	The purpose of xRs rules in this framework is to order constituents along sensible frontiers in G, and all frontiers containing undefined orderings, as between PP[4-5] and VBG[4], must be disregarded during rule extraction." ></td>
	<td class="line x" title="32:176	To ensure that xRs rules are prevented from attempting to re-order any such pair of constituents, these rules are designed in such a way that variables in their lhs can only match nodes of the frontier set." ></td>
	<td class="line o" title="33:176	Rules that satisfy this property are said to be induced by G.2 For example, rule (d) in Table 1 is valid according to GHKM, since the spans corresponding to 1Note that we use a slightly different terminology." ></td>
	<td class="line x" title="34:176	2Specifically, an xRs ruleri is extracted fromGby taking a subtree   pi as lhs(ri), appending a variable to each leaf node of  that is internal to pi, adding those variables to rhs(ri), ordering them in accordance to a, and if necessary inserting any word offto ensure thatrhs(ri)is a sequence of contiguous spans (e.g. , [4-5][6][7-8] for rule (f) in Table 1)." ></td>
	<td class="line x" title="35:176	c44c54 c43c44 c56c42c50 c4ec4ec53 c49c4e c4ec4ec50 c4ec50 c4ec4ec53 c56c42c47 c33 c32 c32 c31 c37c2dc38 c34 c34 c35 c39 c31 c32 c33 c34 c35 c36 c37 c38 c39 c33 c31c2dc32c2cc34c2dc39 c32 c31c2dc39 c32 c31c2dc39 c31 c32c2dc39 c37c2dc38 c31c2dc35c2cc39 c34 c31c2dc39 c34 c31c2dc39 c35 c31c2dc34c2cc37c2dc39 c39 c31c2dc38 c31c2dc32 c33c2dc39 c4ec50 c37c2dc38 c31c2dc35c2cc39 c4ec50 c35 c31c2dc34c2cc20c37c2dc39 c50c50 c34c2dc35 c31c2dc34c2cc37c2dc39 c56c50 c34c2dc35 c31c2dc33c2cc37c2dc39 c4ec50 c34c2dc38 c31c2dc33c2cc39 c56c50 c33c2dc38 c31c2dc32c2cc39 c53 c31c2dc39 c97 c37c21 c22c23c24 c25c26 c27c28 c29 c2ac2b c2c c2e c54c68c65c73c65 c70c65c6fc70c6cc65 c69c6ec63c6cc75c64c65 c61c73c74c72c6fc6ec61c75c74c73 c63c6fc6dc69c6ec67 c66c72c6fc6d c46c72c61c6ec63c65 c2ec2e c37 c2d Figure 1: Spans and complement-spans determine what rules are extracted." ></td>
	<td class="line x" title="36:176	Constituents in gray are members of the frontier set; a minimal rule is extracted from each of them." ></td>
	<td class="line x" title="37:176	(a) S(x0:NP, x1:VP, x2:)." ></td>
	<td class="line x" title="38:176	 x0, x1, x2 (b) NP(x0:DT, CD(7), NNS(people))  x0, 7a186 (c) DT(these) a217 (d) VP(x0:VBP, x1:NP)  x0, x1 (e) VBP(include) a45a5a236 (f) NP(x0:NP, x1:VP)  x1,a132, x0 (g) NP(x0:NNS)  x0 (h) NNS(astronauts) a135a42,a88 (i) VP(VBG(coming), PP(IN(from), x0:NP)) a101a234, x0 (j) NP(x0:NNP)  x0 (k) NNP(France) a213a253 (l) .()." ></td>
	<td class="line x" title="39:176	 . Table 1: A minimal derivation corresponding to Figure 1." ></td>
	<td class="line x" title="40:176	its rhs constituents (VBP[3] and NP[4-8]) do not overlap." ></td>
	<td class="line o" title="41:176	Conversely,NP(x0:DT,x1:CD:,x2:NNS) is not the lhs of any rule extractible from G, since its frontier constituents CD[2] and NNS[2] have overlapping spans.3 Finally, the GHKM procedure produces a single derivation from G, which is shown in Table 1." ></td>
	<td class="line o" title="42:176	The concern in GHKM was to extract minimal rules, whereas ours is to extract rules of any arbitrary size." ></td>
	<td class="line x" title="43:176	Minimal rules defined over G are those that cannot be decomposed into simpler rules induced by the same graph G, e.g., all rules in Table 1." ></td>
	<td class="line x" title="44:176	We call minimal a derivation that only contains minimal rules." ></td>
	<td class="line x" title="45:176	Conversely, a composed rule results from the composition of two or more minimal rules, e.g., rule (b) and (c) compose into: NP(DT(these), CD(7), NNS(people)) a217, 7a186 3It is generally reasonable to also require that the root n of lhs(ri) be part of F, because no rule induced by G can compose with ri at n, due to the restrictions imposed on the extraction procedure, and ri wouldnt be part of any valid derivation." ></td>
	<td class="line x" title="46:176	962 c4fc52 c4ec50c28c78c30c3ac4ec50c2cc20c78c31c3a c56c50c29c20 c21c78c31 c2cc21 c2cc20c78c30 c56c50c28c78c30c3ac56c42c50c2cc20c78c31c3a c4ec50c29c20 c21c78c30 c20c2cc20c78c31 c53c28c78c30c3ac4ec50c2cc20c78c31c3ac56c50c2cc20c78 c32c3ac2ec29c20 c21c78c30 c20c2cc20c78c31c2cc20c78 c32 c4ec50c28c78c30c3ac44c54c20c43c44c28c37c29c2cc20 c4ec4ec53c28c70c65c6fc70c6cc65c29c29c20 c21c78c30 c2cc20c37c22 c2ec28c2ec29c20 c21c2e c44c54c28c74c68c65c73c65c29c20 c21c23 c56c42c50c28c69c6ec63c6cc75c64c65c29c20 c21 c24c25c26 c4ec50c28c78c30c3ac4ec50c2cc20c78c31c3a c56c50c29c20 c21c78c31 c2cc20c78c30 c4ec50c28c78c30c3ac4ec50c2cc20c78c31c3a c56c50c29c20 c21c78c31 c2cc20c78c30 c56c50c28 c56c42 c47c28c63c6f c6dc69c6ec67 c29c2cc20 c50c50c28c49c4e c28c66c72c6f c6dc29c2cc20c78c30c3a c4ec50c29c29c20c20 c21c27c28 c2cc20c78c30c2cc20 c21 c56c50c28 c56c42 c47c28c63c6f c6dc69c6ec67 c29c2cc20 c50c50c28c49c4e c28c66c72c6f c6dc29c2cc20c78c30c3a c4ec50c29c29c20 c21c27c28 c2cc20c78c30 c4ec50c28c78c30c3ac4ec4ec53c29c20 c21c78c30 c4ec50c28c78c30c3ac4ec4ec53c29c20 c21c21 c2cc20c78c30 c4ec50c28c78c30c3ac4ec4ec50c29c20 c21c78c30 c2cc20c21 c4ec4ec50c28c46c72c61c6ec63c65c29c20 c21c29c2a c4ec4ec53c28c61c73c74c72c6fc6ec61c75c74c73c29c20 c21c2bc2c c2cc20c2d c4fc52 c4fc52 c4ec4ec53c28c61c73c74c72c6fc6ec61c75c74c73c29c20 c21c21 c2cc2bc2c c2cc20c2d c4fc52 c4ec50c28c78c30c3ac4ec4ec50c29c20 c21c78c30 c4ec50c28c78c30c3ac4ec4ec50c29c20 c21c78c30 c4ec4ec50c28c46c72c61c6ec63c65c29c20 c21c29c2a c2cc20c21 c4ec50c28c78c30c3ac4ec4ec53c29c20 c21c78c30 c56c50c28 c56c42 c47c28c63c6f c6dc69c6ec67 c29c2cc20 c50c50c28c49c4e c28c66c72c6f c6dc29c2cc20c78c30c3a c4ec50c29c29c20 c21c27c28 c2cc20c78c30 c63c6fc6d c69c6ec67 c66c72c6fc6d c4ec4e c53 c49c4e c4ec4e c50 c4ec50 c56c50 c4ec50 c56c42c47 c50c50 c4ec50 c37c2dc38 c35 c37c2dc38 c35 c37c2dc38 c34 c34 c35 c34 c35 c36 c37 c38 c34 c34 c34c2dc35 c34c2dc35 c34c2dc38 c4ec4ec50c28c46c72c61c6ec63c65c29c20 c21c29c2a c2cc20c21 c4ec50c28c78c30c3ac4ec4ec50c29c20 c21c78c30 c2cc20c21 c56c50c28c56c42c47c28c63c6f c6dc69c6ec67c29c2cc20 c50c50c28c49c4e c28c66c72c6fc6d c29c2cc20 c78c30c3ac4ec50c29c29c20c20 c21c27c28 c2cc20c78c30c2cc20 c21 c4ec4ec53c28c61c73c74c72c6fc6ec61c75c74c73c29c20 c21c21 c2cc20c2bc2c c2cc20c2d c4ec50c28c78c30c3ac4ec4ec53c29c20 c21c21 c2cc20c78c30 c4ec50c28c78c30c3ac4ec50c2cc20c78c31c3a c56c50c29c20 c21c78c31 c2cc20c21 c2cc20c78c30 c28c61c29 c28c62c29 c2d c27c28 c29c2a c21 c2bc2c c61c73c74c72c6f c6ec61c75c74 c73 c46c72c61c6ec63c65 Figure 2: (a) Multiple ways of aligninga132to constituents in the tree." ></td>
	<td class="line x" title="47:176	(b) Derivation corresponding to the parse tree in Figure 1, which takes into account all alignments ofa132pictured in (a)." ></td>
	<td class="line x" title="48:176	Note that these properties are dependent on G, and the above rule would be considered a minimal rule in a graph Gprime similar to G, but additionally containing a word alignment between 7 and a217." ></td>
	<td class="line x" title="49:176	We will see in Sections 3 and 5 why extracting only minimal rules can be highly problematic." ></td>
	<td class="line n" title="50:176	2.2 Unaligned words While the general theory presented in GHKM accounts for any kind of derivation consistent with G, it does not particularly discuss the case where some words of the source-language string f are not aligned to any word of e, thus disconnected from the rest of the graph." ></td>
	<td class="line x" title="51:176	This case is highly frequent: 24.1% of Chinese words in our 179 million word English-Chinese bilingual corpus are unaligned, and 84.8% of Chinese sentences contain at least one unaligned word." ></td>
	<td class="line x" title="52:176	The question is what to do with such lexical items, e.g., a132 in Figure 2(a)." ></td>
	<td class="line o" title="53:176	The approach of building one minimal derivation for G as in the algorithm described in GHKM assumes that we commit ourselves to a particular heuristic to attach the unaligned item to a certain constituent of pi, e.g., highest attachment (in the example, a132 is attached to NP[4-8] and the heuristic generates rule (f))." ></td>
	<td class="line x" title="54:176	A more reasonable approach is to invoke the principle of insufficient reason and make no a priori assumption about what is a correct way of assigning the item to a constituent, and return all derivations that are consistent with G. In Section 4, we will see how to use corpus evidence to give preference to unaligned-word attachments that are the most consistent across the data." ></td>
	<td class="line x" title="55:176	Figure 2(a) shows the six possible ways of attaching a132 to constituents of pi: besides the highest attachment (rule (f)),a132 can move along the ancestors of France, since it is to the right of the translation of that word, and be considered to be part of an NNP, NP, or VP rule." ></td>
	<td class="line x" title="56:176	We make the same reasoning to the left: a132 can either start the NNS of astronauts, or start an NP." ></td>
	<td class="line x" title="57:176	Our account of all possible ways of consistently attaching a132 to constituents means we must extract more than one derivation to explain transformations in G, even if we still restrict ourselves to minimal derivations (a minimal derivation for G is unique if and only if no source-language word in G is unaligned)." ></td>
	<td class="line x" title="58:176	While we could enumerate all derivations separately, it is much more efficient both in time and space to represent them as a derivation forest, as in Figure 2(b)." ></td>
	<td class="line x" title="59:176	Here, the forest covers all minimal derivations that correspond to G. It is necessary to ensure that for each derivation, each unaligned item (here a132) appears only once in the rules of that derivation, as shown in Figure 2 (which satisfies the property)." ></td>
	<td class="line x" title="60:176	That requirement will prove to be critical when we address the problem of estimating probabilities for our rules: if we allowed in our example to spuriously generatea132s in multiple successive steps of the same derivation, we would not only represent the transformation incorrectly, but also a132-rules would be disproportionately represented, leading to strongly biased estimates." ></td>
	<td class="line x" title="61:176	We will now see how to ensure this constraint is satisfied in our rule extraction and derivation building algorithm." ></td>
	<td class="line o" title="62:176	963 2.3 Algorithm The linear-time algorithm presented in GHKM is only a particular case of the more general one we describe here, which is used to extract all rules, minimal and composed, induced by G. Similarly to the GHKM algorithm, ours performs a topdown traversal of G, but differs in the operations it performs at each node n  F: we must explore all subtrees rooted at n, find all consistent ways of attaching unaligned words of f, and build valid derivations in accordance to these attachments." ></td>
	<td class="line x" title="63:176	We use a table or-dforest[x,y,c] to store ORnodes, in which each OR-node can be uniquely defined by a syntactic category c and a span [x,y] (which may cover unaligned words of f)." ></td>
	<td class="line x" title="64:176	This table is used to prevent the same partial derivation to be followed multiple times (the in-degrees of OR-nodes generally become large with composed rules)." ></td>
	<td class="line x" title="65:176	Furthermore, to avoid over-generating unaligned words, the root and variables in each rule are represented with their spans." ></td>
	<td class="line x" title="66:176	For example, in Figure 2(b), the second and third child of the topmost OR-node respectively span across [4-5][6-8] and [4-6][7-8] (after constituent reordering)." ></td>
	<td class="line x" title="67:176	In the former case, a132 will eventually be realized in an NP, and in the latter case, in a VP." ></td>
	<td class="line x" title="68:176	The preprocessing step consists of assigning spans and complement spans to nodes of G, in the first case by a bottom-up exploration of the graph, and in the latter by a top-down traversal." ></td>
	<td class="line x" title="69:176	To assign complement spans, we assign the complement span of any node n to each of its children, and for each of them, add the span of the child to the complement span of all other children." ></td>
	<td class="line x" title="70:176	In another traversal of G, we determine the minimal rule extractible from each node in F. We explore all tree fragments rooted at n by maintaining an open and a closed queue of rules extracted from n (qo and qc)." ></td>
	<td class="line x" title="71:176	At each step, we pick the smallest rule in qo, and for each of its variable nodes, try to discover new rules (successor rules) by means of composition with minimal rules, until a given threshold on rule size or maximum number of rules in qc is reached." ></td>
	<td class="line x" title="72:176	There may be more that one successor per rule, since we must account for all possible spans than can be assigned to non-lexical leaves of a rule." ></td>
	<td class="line x" title="73:176	Once a threshold is reached, or if the open queue is empty, we connect a new OR-node to all rules that have just been extracted from n, and add it to or-dforest." ></td>
	<td class="line x" title="74:176	Finally, weproceedrecursively, andextractnewrulesfrom eachnodeatthefrontieroftheminimalrulerooted at n. Once all nodes of F have been processed, the or-dforest table contains a representation encoding only valid derivations." ></td>
	<td class="line x" title="75:176	3 Probability models The overall goal of our translation system is to transform a given source-language sentence f into an appropriate translation e in the set E of all possible target-language sentences." ></td>
	<td class="line x" title="76:176	In a noisy-channel approach to SMT, we uses Bayes theorem and choose the English sentence e  E that maximizes:4 e = argmax eE braceleftBig Pr(e)  Pr(f|e) bracerightBig (1) Pr(e) is our language model, and Pr(f|e) our translation model." ></td>
	<td class="line x" title="77:176	In a grammatical approach to MT, we hypothesize that syntactic information can help produce good translation, and thus introduce dependencies on target-language syntax trees." ></td>
	<td class="line x" title="78:176	The function to optimize becomes: e = argmax eE braceleftBig Pr(e) summationdisplay pi(e) Pr(f|pi)Pr(pi|e) bracerightBig (2) (e) is the set of all English trees that yield the given sentence e. Estimating Pr(pi|e) is a problem equivalent to syntactic parsing and thus is not discussed here." ></td>
	<td class="line x" title="79:176	Estimating Pr(f|pi) is the task of syntax-based translation models (SBTM)." ></td>
	<td class="line x" title="80:176	Given a rule set R, our SBTM makes the common assumption that left-most compositions of xRs rules i = r1    rn are independent from one another in a given derivation i  , where  is the set of all derivations constructible from G = (pi,f,a) using rules of R. Assuming that  is the set of all subtree decompositions of pi corresponding to derivations in , we define the estimate: Pr(f|pi) = 1|| summationdisplay i productdisplay rji p(rhs(rj)|lhs(rj)) (3) under the assumption: summationdisplay rjR:lhs(rj)=lhs(ri) p(rhs(rj)|lhs(rj)) = 1 (4) It is important to notice that the probability distribution defined in Equation 3 requires a normalization factor (||) in order to be tight, i.e., sum to 1 over all strings fi  F that can be derived 4We denote general probability distributions with Pr() and use p() for probabilities assigned by our models." ></td>
	<td class="line x" title="81:176	964 c58 c61 c59 c62 c61c92 c62c92 c63c92c63 c28c21c2cc66 c31c2c c61 c31c29c3a c58 c61 c59 c62 c62c92 c61c92 c63c92c63 c28c21c2cc66 c32c2c c61 c32c29c3a Figure 3: Example corpus." ></td>
	<td class="line x" title="82:176	from pi." ></td>
	<td class="line x" title="83:176	A simple example suffices to demonstrate it is not tight without normalization." ></td>
	<td class="line x" title="84:176	Figure 3 contains a sample corpus from which four rules can be extracted: r1: X(a, Y(b, c))  a, b, c r2: X(a, Y(b, c))  b, a, c r3: X(a, x0:Y)  a, x0 r4: Y(b, c)  b, c From Equation 4, the probabilities of r3 and r4 must be 1, and those of r1 and r2 must sum to 1." ></td>
	<td class="line x" title="85:176	Thus, the total probability mass, which is distributed across two possible output strings abc and bac, is: p(abc|pi) + p(bac|pi) = p1 + p3  p4 + p2 = 2, where pi = p(rhs(ri)|lhs(ri))." ></td>
	<td class="line x" title="86:176	It is relatively easy to prove that the probabilities of all derivations that correspond to a given decomposition i   sum to 1 (the proof is omitted due to constraints on space)." ></td>
	<td class="line x" title="87:176	From this property we can immediately conclude that the model described by Equation 3 is tight.5 We examine two estimates p(rhs(r)|lhs(r))." ></td>
	<td class="line x" title="88:176	The first one is the relative frequency estimator conditioning on left hand sides: p(rhs(r)|lhs(r)) = f(r)summationtext rprime:lhs(rprime)=lhs(r) f(rprime) (5) f(r) represents the number of times rule r occurred in the derivations of the training corpus." ></td>
	<td class="line x" title="89:176	One of the major negative consequences of extracting only minimal rules from a corpus is that an estimator such as Equation 5 can become extremely biased." ></td>
	<td class="line x" title="90:176	This again can be observed from Figure 3." ></td>
	<td class="line o" title="91:176	In the minimal-rule extraction of GHKM, only three rules are extracted from the example corpus, i.e. rules r2, r3, and r4." ></td>
	<td class="line x" title="92:176	Lets assume now that the triple (pi,f1,a1) is represented 99 times, and (pi,f2,a2) only once." ></td>
	<td class="line x" title="93:176	Given a tree pi, the model trained on that corpus can generate the two strings abc and bac only through two derivations, r3  r4 and r2, respectively." ></td>
	<td class="line x" title="94:176	Since all rules in that example have probability 1, and 5If each tree fragment in pi is the lhs of some rule in R, then we have || = 2n, where n is the number of nodes of the frontier set F  G (each node is a binary choice point)." ></td>
	<td class="line x" title="95:176	given that the normalization factor || is 2, both probabilities p(abc|pi) and p(bac|pi) are 0.5." ></td>
	<td class="line x" title="96:176	On the other hand, if all rules are extracted and incorporated into our relative-frequency probability model, r1 seriously counterbalances r2 and the probabilityof abcbecomes: 12( 99100+1) = .995 (since it differs from .99, the estimator remains biased, but to a much lesser extent)." ></td>
	<td class="line x" title="97:176	An alternative to the conditional model of Equation 3 is to use a joint model conditioning on the root node instead of the entire left hand side: p(r|root(r)) = f(r)summationtext rprime:root(rprime)=root(r) f(rprime) (6) This can be particularly useful if no parser or syntax-based language model is available, and we need to rely on the translation model to penalize ill-formed parse trees." ></td>
	<td class="line x" title="98:176	Section 6 will describe an empirical evaluation based on this estimate." ></td>
	<td class="line x" title="99:176	4 EM training In our previous discussion of parameter estimation, we did not explore the possibility that one derivation in a forest may be much more plausible than the others." ></td>
	<td class="line x" title="100:176	If we knew which derivation in each forest was the true derivation, then we could straightforwardly collect rule counts off those derivations." ></td>
	<td class="line x" title="101:176	On the other hand, if we had good rule probabilities, we could compute the most likely (Viterbi) derivations for each training example." ></td>
	<td class="line x" title="102:176	This is a situation in which we can employ EM training, starting with uniform rule probabilities." ></td>
	<td class="line x" title="103:176	Foreachtrainingexample, wewouldlike to: (1) score each derivation i as a product of the probabilities of the rules it contains, (2) compute a conditional probability pi for each derivation i (conditioned on the observed training pair) by normalizing those scores to add to 1, and (3) collect weighted counts for each rule in each i, where the weight is pi." ></td>
	<td class="line x" title="104:176	We can then normalize the counts to get refined probabilities, and iterate; the corpus likelihood is guaranteed to improve with each iteration." ></td>
	<td class="line x" title="105:176	While it is infeasible to enumerate the millions of derivations in each forest, Graehl and Knight (2004) demonstrate an efficient algorithm." ></td>
	<td class="line x" title="106:176	They also analyze how to train arbitrary tree transducers into two steps." ></td>
	<td class="line x" title="107:176	The first step is to build a derivation forest for each training example, where the forest contains those derivations licensed by the (already supplied) transducers rules." ></td>
	<td class="line x" title="108:176	The second step employs EM on those derivation forests, running in time proportional to the size of the 965 Best minimal-rule derivation (Cm) p(r) (a) S(x0:NP-C x1:VP x2:)." ></td>
	<td class="line x" title="109:176	 x0 x1 x2 .845 (b) NP-C(x0:NPB)  x0 .82 (c) NPB(DT(the) x0:NNS)  x0 .507 (d) NNS(gunmen) a170a75 .559 (e) VP(VBD(were) x0:VP-C)  x0 .434 (f) VP-C(x0:VBN x1:PP)  x1 x0 .374 (g) PP(x0:IN x1:NP-C)  x0 x1 .64 (h) IN(by) a171 .0067 (i) NP-C(x0:NPB)  x0 .82 (j) NPB(DT(the) x0:NN)  x0 .586 (k) NN(police) a102a185 .0429 (l) VBN(killed) a251a217 .0072 (m) .()." ></td>
	<td class="line x" title="110:176	 . .981 c2ec20 c54c68c65c67 c75c6ec6dc65c6e c77c65c72c65 c6bc69c6cc6cc65c64 c62c79c74 c68c65c70c6fc6cc69c63c65 c2e c44c54 c56c42c44 c56c42c4e c44c54 c4ec4e c4ec50 c50c50 c56c50c2dc43 c56c50c53 c4ec4ec53 c49c4e c4ec50 c2e c21c22 c23c24 c25 c26c27 Best composed-rule derivation (C4) p(r) (o) S(NP-C(NPB(DT(the) NNS(gunmen))) x0:VP .())." ></td>
	<td class="line x" title="111:176	a170a75x0 . 1 (p) VP(VBD(were) VP-C(x0:VBN PP(IN(by) x1:NP-C))) a171x1 x0 0.00724 (q) NP-C(NPB(DT(the) NN(police))) a102a185 0.173 (r) VBN(killed) a251a217 0.00719 Figure 4: Two most probable derivations for the graph on the right: the top table restricted to minimal rules; the bottom one, much more probable, using a large set of composed rules." ></td>
	<td class="line x" title="112:176	Note: the derivations are constrained on the (pi,f,a) triple, and thus include some non-literal translations with relatively low probabilities (e.g. killed, which is more commonly translated asa123a161)." ></td>
	<td class="line x" title="113:176	rule nb." ></td>
	<td class="line x" title="114:176	of nb." ></td>
	<td class="line x" title="115:176	of derivEMset rules nodes time time Cm 4M 192M 2 h. 4 h. C3 142M 1255M 52 h. 34 h. C4 254M 2274M 134 h. 60 h. Table 2: Rules and derivation nodes for a 54M-word, 1.95M sentence pair English-Chinese corpus, and time to build derivations (on 10 cluster nodes) and run 50 EM iterations." ></td>
	<td class="line x" title="116:176	forests." ></td>
	<td class="line x" title="117:176	We only need to borrow the second step for our present purposes, as we construct our own derivation forests when we acquire our rule set." ></td>
	<td class="line x" title="118:176	A major challenge is to scale up this EM training to large data sets." ></td>
	<td class="line x" title="119:176	We have been able to run EM for 50 iterations on our Chinese-English 54million word corpus." ></td>
	<td class="line x" title="120:176	The derivation forests for this corpus contain 2.2 billion nodes; the largest forest contains 1.1 million nodes." ></td>
	<td class="line x" title="121:176	The outcome is to assign probabilities to over 254 million rules." ></td>
	<td class="line x" title="122:176	Our EM runs with either lhs normalization or lhsroot normalization." ></td>
	<td class="line x" title="123:176	In the former case, each lhs has an average of three corresponding rhss that compete with each other for probability mass. 5 Model coverage We now present some examples illustrating the benefit of composed rules." ></td>
	<td class="line x" title="124:176	We trained three p(rhs(ri)|lhs(ri)) models on a 54 million-word English-Chineseparallelcorpus(Table2): thefirst one (Cm) with only minimal rules, and the two others (C3 and C4) additionally considering composed rules with no more than three, respectively four, internal nodes in lhs(ri)." ></td>
	<td class="line x" title="125:176	We evaluated these models on a section of the NIST 2002 evaluation corpus, for which we built derivation forests and lhs: S(x0:NP-C VP(x1:VBD x2:NP-C) x3:.)" ></td>
	<td class="line x" title="126:176	corpus rhsi p(rhsi|lhs) Chinese x1 x0 x2 x3 .3681 (minimal) x0 x1 a44x3 x2 .0357 x2, x0 x1 x3 .0287 x0 x1 a44x3 x2 . .0267 Chinese x0 x1 x2 x3 .9047 (composed) x0 x1, x2 x3 .016 x0, x1 x2 x3 .0083 x0 x1 a134x2 x3 .0072 Arabic x1 x0 x2 x3 .5874 (composed) x0 x1 x2 x3 .4027 x1 x2 x0 x3 .0077 x1 x0 x2 ' x3 .0001 Table 3: Our model transforms English subject-verb-object (SVO) structures into Chinese SVO and into Arabic VSO." ></td>
	<td class="line x" title="127:176	With only minimal rules, Chinese VSO is wrongly preferred." ></td>
	<td class="line x" title="128:176	extracted the most probable one (Viterbi) for each sentence pair (based on an automatic alignment produced by GIZA)." ></td>
	<td class="line x" title="129:176	We noticed in general that Viterbi derivations according to C4 make extensive usage of composed rules, as it is the case in the example in Figure 4." ></td>
	<td class="line x" title="130:176	It shows the best derivation according to Cm and C4 on the unseen (pi,f,a) triple displayed on the right." ></td>
	<td class="line x" title="131:176	The second derivation (logp = 11.6) is much more probable than the minimal one (logp = 17.7)." ></td>
	<td class="line x" title="132:176	In the case of Cm, we can see that many small rules must be applied to explain the transformation, and at each step, thedecisionregardingthere-orderingofconstituents is made with little syntactic context." ></td>
	<td class="line x" title="133:176	For example, from the perspective of a decoder, the word by is immediately transformed into a preposition (IN), but it is in general useful to know which particular function word is present in the sentence to motivate good re-orderings in the up966 lhs1: NP-C(x0:NPB PP(IN(of) x1:NP-C)) (NP-of-NP) lhs2: PP(IN(of) NP-C(x0:NPB PP(IN(of) NP-C(x1:NPB x2:VP)))) (of-NP-of-NP-VP) lhs3: VP(VBD(said) SBAR-C(IN(that) x0:S-C)) (said-that-S) lhs4: SBAR(WHADVP(WRB(when)) S-C(x0:NP-C VP(VBP(are) x1:VP-C))) (when-NP-are-VP) rhs1i p(rhs1i|lhs1) rhs2i p(rhs2i|lhs2) rhs3i p(rhs3i|lhs3) rhs4i p(rhs4i|lhs4) x1 x0 .54 x2 a132x1 a132x0 .6754 a244, x0 .6062 a40x1 x0 a246 .6618 x0 x1 .2351 a40x2 a132x1 a132x0 .035 a244x0 .1073 a83x1 x0 a246 .0724 x1 a132x0 .0334 x2 a132x1 a132x0, .0263 a104a58, x0 .0591 a40x1 x0 a246, .0579 x1 x0 a132 .026 x2 a132x1 a132x0 a9 .0116 a214a244, x0 .0234, a40x1 x0 a246 .0289 Table 4: Translation probabilities promote linguistically motivated constituent re-orderings (for lhs1 and lhs2), and enable non-constituent (lhs3) and non-contiguous (lhs4) phrasal translations." ></td>
	<td class="line x" title="134:176	per levels of the tree." ></td>
	<td class="line x" title="135:176	A rule like (e) is particularly unfortunate, since it allows the word were to be added without any other evidence that the VP should be in passive voice." ></td>
	<td class="line x" title="136:176	On the other hand, the composed-rule derivation of C4 incorporates more linguistic evidence in its rules, and re-orderings are motivated by more syntactic context." ></td>
	<td class="line x" title="137:176	Rule (p) is particularly appropriate to create a passive VP construct, since it expects a Chinese passive marker (a171), an NP-C, and a verb in its rhs, and creates the were  by construction at once in the left hand side." ></td>
	<td class="line x" title="138:176	5.1 Syntactic translation tables We evaluate the promise of our SBTM by analyzinginstancesoftranslationtables(t-table)." ></td>
	<td class="line x" title="139:176	Table3 shows how a particular form of SVO construction is transformed into Chinese, which is also an SVOlanguage." ></td>
	<td class="line x" title="140:176	Whilethet-tableforChinesecomposed rules clearly gives good estimates for the correct x0 x1 ordering (p = .9), i.e. subject before verb, the t-table for minimal rules unreasonably gives preference to verb-subject ordering (x1 x0, p = .37), because the most probable transformation (x0 x1) does not correspond to a minimal rule." ></td>
	<td class="line x" title="141:176	We obtain different results with Arabic, an VSO language, and our model effectively learns to move the subject after the verb (p = .59)." ></td>
	<td class="line x" title="142:176	lhs1 in Table 4 shows that our model is able to learn large-scale constituent re-orderings, such as re-ordering NPs in a NP-of-NP construction, and put the modifier first as it is more commonly the case in Chinese (p = .54)." ></td>
	<td class="line x" title="143:176	If more syntactic context is available as in lhs2, our model provides much sharper estimates, and appropriately reverses the order of three constituents with highprobability(p = .68),insertingmodifiersfirst (possessive markersa132are needed here for better syntactic disambiguation)." ></td>
	<td class="line x" title="144:176	A limitation of earlier syntax-based systems is their poor handling of non-constituent phrases." ></td>
	<td class="line x" title="145:176	Table 4 shows that our model can learn rules for such phrases, e.g., said that (lhs3)." ></td>
	<td class="line x" title="146:176	While the that has no direct translation, our model effectively learnstoseparatea244(said)fromtherelativeclause with a comma, which is common in Chinese." ></td>
	<td class="line x" title="147:176	Anotherpromisingprospectofourmodelseems to lie in its ability to handle non-contiguous phrases, a feature that state of the art systems such as (Och and Ney, 2004) do not incorporate." ></td>
	<td class="line x" title="148:176	The when-NP-are-VP construction of lhs4 presents such a case." ></td>
	<td class="line x" title="149:176	Our model identifies that are needs to be deleted, that when translates into the phrasea40a246,andthattheNPneedstobemoved after the VP in Chinese (p = .66)." ></td>
	<td class="line x" title="150:176	6 Empirical evaluation The task of our decoder is to find the most likely English tree pi that maximizes all models involved in Equation 2." ></td>
	<td class="line x" title="151:176	Since xRs rules can be converted to context-free productions by increasing the number of non-terminals, we implemented our decoder as a standard CKY parser with beam search." ></td>
	<td class="line x" title="152:176	Its rule binarization is described in (Zhang et al. , 2006)." ></td>
	<td class="line x" title="153:176	We compare our syntax-based system against an implementation of the alignment template (AlTemp) approach to MT (Och and Ney, 2004), which is widely considered to represent the state of the art in the field." ></td>
	<td class="line x" title="154:176	We registered both systems in the NIST 2005 evaluation; results are presented in Table 5." ></td>
	<td class="line x" title="155:176	With a difference of 6.4 BLEU points for both language pairs, we consider the results of our syntax-based system particularly promising, since these are the highest scores to date that we know of using linguistic syntactic transformations." ></td>
	<td class="line x" title="156:176	Also, on the one hand, our AlTemp system represents quite mature technology, and incorporates highly tuned model parameters." ></td>
	<td class="line x" title="157:176	On the other hand, our syntax decoder is still work in progress: only one model was used during search, i.e., the EM-trained root-normalized SBTM, and as yet no language model is incorporated in the search (whereas the search in the AlTemp system uses two phrase-based translation models and 967 Syntactic AlTemp Arabic-to-English 40.2 46.6 Chinese-to-English 24.3 30.7 Table 5: BLEU-4 scores for the 2005 NIST test set." ></td>
	<td class="line x" title="158:176	Cm C3 C4 Chinese-to-English 24.47 27.42 28.1 Table6: BLEU-4scoresforthe2002NISTtestset, withrules of increasing sizes." ></td>
	<td class="line x" title="159:176	12 other feature functions)." ></td>
	<td class="line x" title="160:176	Furthermore, our decoder doesnt incorporate any syntax-based language model, and admittedly our ability to penalize ill-formed parse trees is still limited." ></td>
	<td class="line x" title="161:176	Finally, we evaluated our system on the NIST02 test set with the three different rule sets (see Table 6)." ></td>
	<td class="line x" title="162:176	The performance with our largest rule set represents a 3.63 BLEU point increase (14.8% relative) compared to using only minimal rules, which indicates positive prospects for using even larger rules." ></td>
	<td class="line x" title="163:176	While our rule inference algorithm scales to higher thresholds, one important area of future work will be the improvement of our decoder, conjointly with analyses of the impact in terms of BLEU of contextually richer rules." ></td>
	<td class="line x" title="164:176	7 Related work Similarly to (Poutsma, 2000; Wu, 1997; Yamada and Knight, 2001; Chiang, 2005), the rules discussed in this paper are equivalent to productions of synchronous tree substitution grammars." ></td>
	<td class="line x" title="165:176	We believe that our tree-to-string model has several advantages over tree-to-tree transformations such as the ones acquired by Poutsma (2000)." ></td>
	<td class="line x" title="166:176	While tree-to-tree grammars are richer formalisms that provide the potential benefit of rules that are linguistically better motivated, modeling the syntax of both languages comes as an extra cost, and it is admittedly more helpful to focus our syntactic modeling effort on the target language (e.g. , English) in cases where it has syntactic resources (parsers and treebanks) that are considerably more available than for the source language." ></td>
	<td class="line x" title="167:176	Furthermore, we think there is, overall, less benefit in modeling the syntax of the source language, since the input sentence is fixed during decoding and is generally already grammatical." ></td>
	<td class="line x" title="168:176	With the notable exception of Poutsma, most related works rely on models that are restricted to synchronous context-free grammars (SCFG)." ></td>
	<td class="line x" title="169:176	While the state-of-the-art hierarchical SMT system (Chiang, 2005) performs well despite stringent constraints imposed on its context-free grammar, we believe its main advantage lies in its ability to extract hierarchical rules across phrasal boundaries." ></td>
	<td class="line x" title="170:176	Context-free grammars (such as Penn Treebank and Chiangs grammars) make independence assumptions that are arguably often unreasonable, but as our work suggests, relaxations of these assumptions by using contextually richer rules results in translations of increasing quality." ></td>
	<td class="line x" title="171:176	We believe it will be beneficial to account for this finding in future work in syntax-based SMT and in efforts to improve upon (Chiang, 2005)." ></td>
	<td class="line oc" title="172:176	8 Conclusions In this paper, we developed probability models for the multi-level transfer rules presented in (Galley et al. , 2004), showed how to acquire larger rules that crucially condition on more syntactic context, and how to pack multiple derivations, including interpretations of unaligned words, into derivation forests." ></td>
	<td class="line nc" title="173:176	We presented some theoretical arguments for not limiting extraction to minimal rules, validated them on concrete examples, and presented experiments showing that contextually richer rules provide a 3.63 BLEU point increase over the minimal rules of (Galley et al. , 2004)." ></td>
	<td class="line x" title="174:176	Acknowledgments We would like to thank anonymous reviewers for their helpful comments and suggestions." ></td>
	<td class="line x" title="175:176	This work was partially supported under the GALE program of the Defense Advanced Research Projects Agency, Contract No." ></td>
	<td class="line x" title="176:176	HR001106-C-0022." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-1123
Empirical Lower Bounds On The Complexity Of Translational Equivalence
Wellington, Benjamin;Waxmonsky, Sonjia;Melamed, I. Dan;"></td>
	<td class="line x" title="1:221	Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 977984, Sydney, July 2006." ></td>
	<td class="line x" title="2:221	c2006 Association for Computational Linguistics Empirical Lower Bounds on the Complexity of Translational Equivalence  Benjamin Wellington Computer Science Dept. New York University New York, NY 10003 flastnameg@cs.nyu.edu Sonjia Waxmonsky Computer Science Dept. University of Chicago Chicago, IL, 60637 wax@cs.uchicago.edu I. Dan Melamed Computer Science Dept. New York University New York, NY, 10003 flastnameg@cs.nyu.edu Abstract This paper describes a study of the patterns of translational equivalence exhibited by a variety of bitexts." ></td>
	<td class="line x" title="3:221	The study found that the complexity of these patterns in every bitext was higher than suggested in the literature." ></td>
	<td class="line x" title="4:221	These findings shed new light on why syntactic constraints have not helped to improve statistical translation models, including finitestate phrase-based models, tree-to-string models, and tree-to-tree models." ></td>
	<td class="line x" title="5:221	The paper also presents evidence that inversion transduction grammars cannot generate some translational equivalence relations, even in relatively simple real bitexts in syntactically similar languages with rigid word order." ></td>
	<td class="line x" title="6:221	Instructions for replicating our experiments are at http://nlp.cs.nyu.edu/GenPar/ACL06 1 Introduction Translational equivalence is a mathematical relation that holds between linguistic expressions with the same meaning." ></td>
	<td class="line x" title="7:221	The most common explicit representations of this relation are word alignments between sentences that are translations of each other." ></td>
	<td class="line x" title="8:221	The complexity of a given word alignment can be measured by the difficulty of decomposing it into its atomic units under certain constraints detailed in Section 2." ></td>
	<td class="line x" title="9:221	This paper describes a study of the distribution of alignment complexity in a variety of bitexts." ></td>
	<td class="line x" title="10:221	The study considered word alignments both in isolation and in combination with independently generated parse trees for one or both sentences in each pair." ></td>
	<td class="line x" title="11:221	Thus, the study  Thanks to David Chiang, Liang Huang, the anonymous reviewers, and members of the NYU Proteus Project for helpful feedback." ></td>
	<td class="line x" title="12:221	This research was supported by NSF grant #s 0238406 and 0415933." ></td>
	<td class="line x" title="13:221	 SW made most of her contribution while at NYU." ></td>
	<td class="line x" title="14:221	is relevant to finite-state phrase-based models that use no parse trees (Koehn et al. , 2003), tree-tostring models that rely on one parse tree (Yamada and Knight, 2001), and tree-to-tree models that rely on two parse trees (Groves et al. , 2004, e.g.)." ></td>
	<td class="line x" title="15:221	The word alignments that are the least complex on our measure coincide with those that can be generated by an inversion transduction grammar (ITG)." ></td>
	<td class="line x" title="16:221	Following Wu (1997), the prevailing opinion in the research community has been that more complex patterns of word alignment in real bitexts are mostly attributable to alignment errors." ></td>
	<td class="line x" title="17:221	However, the experiments in Section 3 show that more complex patterns occur surprisingly often even in highly reliable alignments in relatively simple bitexts." ></td>
	<td class="line x" title="18:221	As discussed in Section 4, these findings shed new light on why syntactic constraints have not yet helped to improve the accuracy of statistical machine translation." ></td>
	<td class="line x" title="19:221	Our study used two kinds of data, each controlling a different confounding variable." ></td>
	<td class="line x" title="20:221	First, we wanted to study alignments that contained as few errors as possible." ></td>
	<td class="line x" title="21:221	So unlike some other studies (Zens and Ney, 2003; Zhang et al. , 2006), we used manually annotated alignments instead of automatically generated ones." ></td>
	<td class="line x" title="22:221	The results of our experiments on these data will remain relevant regardless of improvements in technology for automatic word alignment." ></td>
	<td class="line x" title="23:221	Second, we wanted to measure how much of the complexity is not attributable to systematic translation divergences, both in the languages as a whole (SVO vs. SOV), and in specific constructions (English not vs. French ne." ></td>
	<td class="line x" title="24:221	pas)." ></td>
	<td class="line x" title="25:221	To eliminate this source of complexity of translational equivalence, we used English/English bitexts." ></td>
	<td class="line x" title="26:221	We are not aware of any previous studies of word alignments in monolingual bitexts." ></td>
	<td class="line x" title="27:221	Even manually annotated word alignments vary in their reliability." ></td>
	<td class="line x" title="28:221	For example, annotators sometimes link many words in one sentence to many 977 (a) that, I believe we all find unacceptable, regardless of political party, je pense que, independamment de notre parti, nous trouvons tous cela inacceptable (b) (Y / Y,Y) > (D C / D,C) * (S / S) > (X A / X A X) (X / X,X) > (Y B / B Y,Y) X A Y B A D C B A B D A CY A Y B X A X S S believe party pense unacc that celaparti inacc Figure 1: (a) Part of a word alignment." ></td>
	<td class="line x" title="29:221	(b) Derivation of this word alignment using only binary and nullary productions requires one gap per nonterminal, indicated by commas in the production rules." ></td>
	<td class="line x" title="30:221	words in the other, instead of making the effort to tease apart more fine-grained distinctions." ></td>
	<td class="line x" title="31:221	A study of such word alignments might say more about the annotation process than about the translational equivalence relation in the data." ></td>
	<td class="line x" title="32:221	The inevitable noise in the data motivated us to focus on lower bounds, complementary to Fox (2002), who wrote that her results should be looked on as more of an upper bound. (p. 307) As explained in Section 3, we modified all unreliable alignments so that they cannot increase the complexity measure." ></td>
	<td class="line x" title="33:221	Thus, we arrived at complexity measurements that were underestimates, but reliably so." ></td>
	<td class="line x" title="34:221	It is almost certain that the true complexity of translational equivalence is higher than what we report." ></td>
	<td class="line x" title="35:221	2 A Measure of Alignment Complexity Any translation model can memorize a training sentence pair as a unit." ></td>
	<td class="line x" title="36:221	For example, given a sentence pair like (he left slowly / slowly he left) with the correct word alignment, a phrase-based translation model can add a single 3-word biphrase to its phrase table." ></td>
	<td class="line x" title="37:221	However, this biphrase would not help the model predict translations of the individual words in it." ></td>
	<td class="line x" title="38:221	Thats why phrase-based models typically decompose such training examples into their sub-biphrases and remember them too." ></td>
	<td class="line x" title="39:221	Decomposing the translational equivalence relations in the training data into smaller units of knowledge can improve a models ability to generalize (Zhang et al. , 2006)." ></td>
	<td class="line x" title="40:221	In the limit, to maximize the chances of covering arbitrary new data, a model should decompose the training data into the smallest possible units, and learn from them.1 For phrasebased models, this stipulation implies phrases of length one." ></td>
	<td class="line x" title="41:221	If the model is a synchronous rewriting system, then it should be able to generate every training sentence pair as the yield of a binary1Many popular models learn from larger units at the same time, but the size of the smallest learnable unit is whats important for our purposes." ></td>
	<td class="line x" title="42:221	branching synchronous derivation tree, where every word-to-word link is generated by a different derivation step." ></td>
	<td class="line x" title="43:221	For example, a model that uses production rules could generate the previous example using the synchronous productions (S, S) !" ></td>
	<td class="line x" title="44:221	(X Y / Y X); (X, X) !" ></td>
	<td class="line x" title="45:221	(U V / U V); (Y, Y) !" ></td>
	<td class="line x" title="46:221	(slowly, slowly); (U, U) !" ></td>
	<td class="line x" title="47:221	(he, he); and (V, V) !" ></td>
	<td class="line x" title="48:221	(left, left)." ></td>
	<td class="line x" title="49:221	A problem arises when this kind of decomposition is attempted for the alignment in Figure 1(a)." ></td>
	<td class="line x" title="50:221	If each link is represented by its own nonterminal, and production rules must be binary-branching, then some of the nonterminals involved in generating this alignment need discontinuities, or gaps." ></td>
	<td class="line x" title="51:221	Figure 1(b) illustrates how to generate the sentence pair and its word alignment in this manner." ></td>
	<td class="line x" title="52:221	The nonterminals X and Y have one discontinuity each." ></td>
	<td class="line x" title="53:221	More generally, for any positive integer k, it is possible to construct a word alignment that cannot be generated using binary production rules whose nonterminals all have fewer than k gaps (Satta and Peserico, 2005)." ></td>
	<td class="line x" title="54:221	Our study measured the complexity of a word alignment as the minimum number of gaps needed to generate it under the following constraints: 1." ></td>
	<td class="line x" title="55:221	Each step of the derivation generates no more than two different nonterminals." ></td>
	<td class="line x" title="56:221	2." ></td>
	<td class="line x" title="57:221	Each word-to-word link is generated from a separate nonterminal.2 Our measure of alignment complexity is analogous to what Melamed et al.(2004) call fanout.3 The least complex alignments on this measure  those that can be generated with zero gaps  are precisely those that can be generated by an 2If we imagine that each word is generated from a separate nonterminal as in GCNF (Melamed et al. , 2004), then constraint 2 becomes a special case of constraint 1." ></td>
	<td class="line x" title="59:221	3For grammars that generate bitexts, fan-out is equal to the maximum number of allowed gaps plus two." ></td>
	<td class="line x" title="60:221	978 bitext # SPs min median max 95% C.I. Chinese/English 491 4 24 52 .02 Romanian/English 200 2 19 76 .03 Hindi/English 90 1 10 40 .04 Spanish/English 199 4 23 49 .03 French/English 447 2 15 29 .01 Eng/Eng MTEval 5253 2 26 92 .01 Eng/Eng fiction 6263 2 15 97 .01 Table 1: Number of sentence pairs and minimum/median/maximum sentence lengths in each bitext." ></td>
	<td class="line x" title="61:221	All failure rates reported later have a 95% confidence interval that is no wider than the value shown for each bitext." ></td>
	<td class="line x" title="62:221	ITG." ></td>
	<td class="line x" title="63:221	For the rest of the paper, we restrict our attention to binary derivations, except where explicitly noted otherwise." ></td>
	<td class="line x" title="64:221	To measure the number of gaps needed to generate a given word alignment, we used a bottom-up hierarchical alignment algorithm to infer a binary synchronous parse tree that was consistent with the alignment, using as few gaps as possible." ></td>
	<td class="line x" title="65:221	A hierarchical alignment algorithm is a type of synchronous parser where, instead of constraining inferences by the production rules of a grammar, the constraints come from word alignments and possibly other sources (Wu, 1997; Melamed and Wang, 2005)." ></td>
	<td class="line x" title="66:221	A bottom-up hierarchical aligner begins with word-to-word links as constituents, where some of the links might be to nothing (NULL)." ></td>
	<td class="line x" title="67:221	It then repeatedly composes constituents with other constituents to make larger ones, trying to find a constituent that covers the entire input." ></td>
	<td class="line x" title="68:221	One of the important design choices in this kind of study is how to treat multiple links attached to the same word token." ></td>
	<td class="line x" title="69:221	Word aligners, both human and automatic, are often inconsistent about whether they intend such sets of links to be disjunctive or conjunctive." ></td>
	<td class="line x" title="70:221	In accordance with its focus on lower bounds, the present study treated them as disjunctive, to give the hierarchical alignment algorithm more opportunities to use fewer gaps." ></td>
	<td class="line x" title="71:221	This design decision is one of the main differences between our study and that of Fox (2002), who treated links to the same word conjunctively." ></td>
	<td class="line x" title="72:221	By treating many-to-one links disjunctively, our measure of complexity ignored a large class of discontinuities." ></td>
	<td class="line x" title="73:221	Many types of discontinuous constituents exist in text independently of any translation." ></td>
	<td class="line x" title="74:221	Simard et al.(2005) give examples such as English verb-particle constructions, and the French negation ne." ></td>
	<td class="line x" title="76:221	pas." ></td>
	<td class="line x" title="77:221	The disparate elements of such constituents would usually be aligned to the same word in a translation." ></td>
	<td class="line x" title="78:221	However, when PP NP b) V S leftGeorgeFriday George left on Friday VP S NP V PPleftGeorgeFriday George left on Friday on ona) Figure 2: a) With a parse tree constraining the top sentence, a hierarchical alignment is possible without gaps." ></td>
	<td class="line x" title="79:221	b) With a parse tree constraining the bottom sentence, no such alignment exists." ></td>
	<td class="line x" title="80:221	our hierarchical aligner saw two words linked to one word, it ignored one of the two links." ></td>
	<td class="line x" title="81:221	Our lower bounds would be higher if they accounted for this kind of discontinuity." ></td>
	<td class="line x" title="82:221	3 Experiments 3.1 Data We used two monolingual bitexts and five bilingual bitexts." ></td>
	<td class="line x" title="83:221	The Romanian/English and Hindi/English data came from Martin et al.(2005)." ></td>
	<td class="line x" title="85:221	For Chinese/English and Spanish/English, we used the data from Ayan et al.(2005)." ></td>
	<td class="line x" title="87:221	The French/English data were those used by Mihalcea and Pedersen (2003)." ></td>
	<td class="line x" title="88:221	The monolingual bitext labeled MTEval in the tables consists of multiple independent translations from Chinese to English (LDC, 2002)." ></td>
	<td class="line x" title="89:221	The other monolingual bitext, labeled fiction, consists of two independent translations from French to English of Jules Vernes novel 20,000 Leagues Under the Sea, sentencealigned by Barzilay and McKeown (2001)." ></td>
	<td class="line x" title="90:221	From the monolingual bitexts, we removed all sentence pairs where either sentence was longer than 100 words." ></td>
	<td class="line x" title="91:221	Table 1 gives descriptive statistics for the remaining data." ></td>
	<td class="line x" title="92:221	The table also shows the upper bound of the 95% confidence intervals for the coverage rates reported later." ></td>
	<td class="line x" title="93:221	The results of experiments on different bitexts are not directly comparable, due to the varying genres and sentence lengths." ></td>
	<td class="line x" title="94:221	3.2 Constraining Parse Trees One of the main independent variables in our experiments was the number of monolingual parse trees used to constrain the hierarchical alignments." ></td>
	<td class="line x" title="95:221	To induce models of translational equivalence, some researchers have tried to use such trees to constrain bilingual constituents: The span of every node in the constraining parse tree must coincide with the relevant monolingual span of some 979 crew astronautsincluded S NP VP NP VP VP S NP PP theinare crewincludedastronauts the Figure 3: A word alignment that cannot be generated without gaps in a manner consistent with both parse trees." ></td>
	<td class="line x" title="96:221	node in the bilingual derivation tree." ></td>
	<td class="line x" title="97:221	These additional constraints can thwart attempts at hierarchical alignment that might have succeeded otherwise." ></td>
	<td class="line x" title="98:221	Figure 2a shows a word alignment and a parse tree that can be hierarchically aligned without gaps." ></td>
	<td class="line x" title="99:221	George and left can be composed in both sentences into a constituent without crossing any phrase boundaries in the tree, as can on and Friday." ></td>
	<td class="line x" title="100:221	These two constituents can then be composed to cover the entire sentence pair." ></td>
	<td class="line x" title="101:221	On the other hand, if a constraining tree is applied to the other sentence as shown in Figure 2b, then the word alignment and tree constraint conflict." ></td>
	<td class="line x" title="102:221	The projection of the VP is discontinuous in the top sentence, so the links that it covers cannot be composed into a constituent without gaps." ></td>
	<td class="line x" title="103:221	On the other hand, if a gap is allowed, then the VP can compose as on Friday . . ." ></td>
	<td class="line x" title="104:221	left in the top sentence, where the ellipsis represents a gap." ></td>
	<td class="line x" title="105:221	This VP can then compose with the NP complete a synchronous parse tree." ></td>
	<td class="line x" title="106:221	Some authors have applied constraining parse trees to both sides of the bitext." ></td>
	<td class="line x" title="107:221	The example in Figure 3 can be hierarchically aligned using either one of the two constraining trees, but gaps are necessary to align it with both trees." ></td>
	<td class="line x" title="108:221	3.3 Methods We parsed the English side of each bilingual bitext and both sides of each English/English bitext using an off-the-shelf syntactic parser (Bikel, 2004), which was trained on sections 02-21 of the Penn English Treebank (Marcus et al. , 1993)." ></td>
	<td class="line x" title="109:221	Our bilingual bitexts came with manually annotated word alignments." ></td>
	<td class="line x" title="110:221	For the monolingual bitexts, we used an automatic word aligner based on a cognate heuristic and a list of 282 function words compiled by hand." ></td>
	<td class="line x" title="111:221	The aligner linked two words to each other only if neither of them was on the function word list and their longest common subsequence ratio (Melamed, 1995) was at least 0.75." ></td>
	<td class="line x" title="112:221	Words that were not linked to another word in this manner were linked to NULL." ></td>
	<td class="line x" title="113:221	For the purposes of this study, a word aligned to NULL is a non-constraint, because it can always be composed without a gap with some constituent that is adjacent to it on just one side of the bitext." ></td>
	<td class="line x" title="114:221	The number of automatically induced non-NULL links was lower than what would be drawn by hand." ></td>
	<td class="line x" title="115:221	We modified the word alignments in all bitexts to minimize the chances that alignment errors would lead to an over-estimate of alignment complexity." ></td>
	<td class="line x" title="116:221	All of the modifications involved adding links to NULL." ></td>
	<td class="line x" title="117:221	Due to our disjunctive treatment of conflicting links, the addition of a link to NULL can decrease but cannot increase the complexity of an alignment." ></td>
	<td class="line x" title="118:221	For example, if we added the links (cela, NULL) and (NULL, that) to the alignment in Figure 1, the hierarchical alignment algorithm could use them instead of the link between cela and that." ></td>
	<td class="line x" title="119:221	It could thus generate the modified alignment without using a gap." ></td>
	<td class="line x" title="120:221	We added NULL links in two situations." ></td>
	<td class="line x" title="121:221	First, if a subset of the links in an alignment formed a many-to-many mapping but did not form a bipartite clique (i.e. every word on one side linked to every word on the other side), then we added links from each of these words to NULL." ></td>
	<td class="line x" title="122:221	Second, if n words on one side of the bitext aligned to m words on the other side with m > n then we added NULL links for each of the words on the side with m words." ></td>
	<td class="line x" title="123:221	After modifying the alignments and obtaining monolingual parse trees, we measured the alignment complexity of each bitext using a hierarchical alignment algorithm, as described in Section 2." ></td>
	<td class="line x" title="124:221	Separate measurements were taken with zero, one, and two constraining parse trees." ></td>
	<td class="line x" title="125:221	The synchronous parser in the GenPar toolkit4 can be configured for all of these cases (Burbank et al. , 2005)." ></td>
	<td class="line oc" title="126:221	Unlike Fox (2002) and Galley et al.(2004), we measured failure rates per corpus rather than per sentence pair or per node in a constraining tree." ></td>
	<td class="line x" title="128:221	This design was motivated by the observation that if a translation model cannot correctly model a certain word alignment, then it is liable to make incorrect inferences about arbitrary parts of that alignment, not just the particular word links involved in a complex pattern." ></td>
	<td class="line x" title="129:221	The failure rates we report represent lower bounds on the fraction of training data 4http://nlp.cs.nyu.edu/GenPar 980 # of gaps allowed  0/0 0/1 or 1/0 Chinese/English 26 = 5% 0 = 0% Romanian/English 1 = 0% 0 = 0% Hindi/English 2 = 2% 0 = 0% Spanish/English 3 = 2% 0 = 0% French/English 3 = 1% 0 = 0% Table 2: Failure rates for hierarchical alignment of bilingual bitexts under word alignment constraints only." ></td>
	<td class="line x" title="130:221	# of gaps allowed on non-English side  0 1 2 Chinese/English 298 = 61% 28 = 6% 0 = 0% Romanian/English 82 = 41% 6 = 3% 1 = 0% Hindi/English 33 = 37% 1 = 1% 0 = 0% Spanish/English 75 = 38% 4 = 2% 0 = 0% French/English 67 = 15% 2 = 0% 0 = 0% Table 3: Failure rates for hierarchical alignment of bilingual bitexts under the constraints of a word alignment and a monolingual parse tree on the English side." ></td>
	<td class="line x" title="131:221	that is susceptible to misinterpretation by overconstrained translation models." ></td>
	<td class="line x" title="132:221	3.4 Summary Results Table 2 shows the lower bound on alignment failure rates with and without gaps for five languages paired with English." ></td>
	<td class="line x" title="133:221	This table represents the case where the only constraints are from word alignments." ></td>
	<td class="line x" title="134:221	Wu (1997) has been unable to find real examples of cases where hierarchical alignment would fail under these conditions, at least in fixed-word-order languages that are lightly inflected, such as English and Chinese. (p. 385)." ></td>
	<td class="line x" title="135:221	In contrast, we found examples in all bitexts that could not be hierarchically aligned without gaps, including at least 5% of the Chinese/English sentence pairs." ></td>
	<td class="line x" title="136:221	Allowing constituents with a single gap on one side of the bitext decreased the observed failure rate to zero for all five bitexts." ></td>
	<td class="line x" title="137:221	Table 3 shows what happened when we used monolingual parse trees to restrict the compositions on the English side." ></td>
	<td class="line x" title="138:221	The failure rates were above 35% for four of the five language pairs, and 61% for Chinese/English!" ></td>
	<td class="line x" title="139:221	Again, the failure rate fell dramatically when one gap was allowed on the unconstrained (non-English) side of the bitext." ></td>
	<td class="line x" title="140:221	Allowing two gaps on the non-English side led to almost complete coverage of these word alignments." ></td>
	<td class="line x" title="141:221	Table 3 does not specify the number of gaps allowed on the English side, because varying this parameter never changed the outcome." ></td>
	<td class="line x" title="142:221	The only way that a gap on that side could increase coverage is if there was a node in the constraining parse tree that # of gaps  0/0 0/1 0/2 0 CTs 171 = 3% 0 = 0% 0 = 0% 1 CTs 1792 = 34% 143 = 3% 7 = 0% 2 CTs 3227 = 61% 3227 = 61% 3227 = 61% Table 4: Failure rates for hierarchical alignment of the MTEval bitext, over varying numbers of gaps and constraining trees (CTs)." ></td>
	<td class="line x" title="143:221	# of gaps  0/0 0/1 0/2 0 CTs 23 = 0% 0 = 0% 0 = 0% 1 CTs 655 = 10% 22 = 0% 1 = 0% 2 CTs 1559 = 25% 1559 = 25% 1559 = 25% Table 5: Failure rates for hierarchical alignment of the fiction bitext, over varying numbers of gaps and constraining trees (CTs)." ></td>
	<td class="line x" title="144:221	had at least four children whose translations were in one of the complex permutations." ></td>
	<td class="line x" title="145:221	The absence of such cases in the data implies that the failure rates under the constraints of one parse tree would be identical even if we allowed production rules of rank higher than two." ></td>
	<td class="line x" title="146:221	Table 4 shows the alignment failure rates for the MTEval bitext." ></td>
	<td class="line x" title="147:221	With word alignment constraints only, 3% of the sentence pairs could not be hierarchically aligned without gaps." ></td>
	<td class="line x" title="148:221	Allowing a single gap on one side decreased this failure rate to zero." ></td>
	<td class="line x" title="149:221	With a parse tree constraining constituents on one side of the bitext and with no gaps, alignment failure rates rose from 3% to 34%, but allowing a single gap on the side of the bitext that was not constrained by a parse tree brought the failure rate back down to 3%." ></td>
	<td class="line x" title="150:221	With two constraining trees the failure rate was 61%, and allowing gaps did not lower it, for the same reasons that allowing gaps on the tree-constrained side made no difference in Table 3." ></td>
	<td class="line x" title="151:221	The trends in the fiction bitext (Table 5) were similar to those in the MTEval bitext, but the coverage was always higher, for two reasons." ></td>
	<td class="line x" title="152:221	First, the median sentence size was lower in the fiction bitext." ></td>
	<td class="line x" title="153:221	Second, the MTEval translators were instructed to translate as literally as possible, but the fiction translators paraphrased to make the fiction more interesting." ></td>
	<td class="line x" title="154:221	This freedom in word choice reduced the frequency of cognates and thus imposed fewer constraints on the hierarchical alignment, which resulted in looser estimates of the lower bounds." ></td>
	<td class="line oc" title="155:221	We would expect the opposite effect with hand-aligned data (Galley et al. , 2004)." ></td>
	<td class="line x" title="156:221	To study how sentence length correlates with the complexity of translational equivalence, we took subsets of each bitext while varying the max981 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 10 20 30 40 50 60 70 80 90 100 failure rate maximum length of shortest sentence 0 constraining trees Chinese/Eng MTeval fiction 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 10 20 30 40 50 60 70 80 90 100 failure rate maximum length of shorter sentence 1 constraining tree Chinese/Eng Romanian/Eng Hindi/Eng Spanish/Eng MTeval French/Eng fiction 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 10 20 30 40 50 60 70 80 90 100 failure rate maximum length of shorter sentence 2 constraining trees MTeval fiction Figure 4: Failure rates for hierarchical alignment without gaps vs. maximum length of shorter sentence." ></td>
	<td class="line x" title="157:221	category  1 2 3 valid reordering 12 10 5 parser error n/a 16 25 same word used differently 15 4 0 erroneous cognates 3 0 0 total sample size 30 30 30 initial failure rate (%) 3.25 31.9 38.4 % false negatives 607 667 843 adjusted failure rate (%) 1.3.22 112.2 61.1 Table 6: Detailed analysis of hierarchical alignment failures in MTEval bitext." ></td>
	<td class="line x" title="158:221	imum length of the shorter sentence in each pair.5 Figure 4 plots the resulting alignment failure rates with and without constraining parse trees." ></td>
	<td class="line x" title="159:221	The lines in these graphs are not comparable to each other because of the variety of genres involved." ></td>
	<td class="line x" title="160:221	3.5 Detailed Failure Analysis We examined by hand 30 random sentence pairs from the MTEval bitext in each of three different categories: (1) the set of sentence pairs that could not be hierarchically aligned without gaps, even without constraining parse trees; (2) the set of sentence pairs that could not be hierarchically aligned without gaps with one constraining parse tree, but that did not fall into category 1; and (3) the set of sentence pairs that could not be hierarchically aligned without gaps with two constraining parse trees, but that did not fall into category 1 or 2." ></td>
	<td class="line x" title="161:221	Table 6 shows the results of this analysis." ></td>
	<td class="line x" title="162:221	In category 1, 60% of the word alignments that could not be hierarchically aligned without gaps were caused by word alignment errors." ></td>
	<td class="line x" title="163:221	E.g.: 1a GlaxoSmithKlines second-best selling drug may have to face competition." ></td>
	<td class="line x" title="164:221	1b Drug maker GlaxoSmithKline may have to face competition on its second best selling product." ></td>
	<td class="line x" title="165:221	The word drug appears in both sentences, but for different purposes, so drug and drug should not 5The length of the shorter sentence is the upper bound on the number of non-NULL word alignments." ></td>
	<td class="line x" title="166:221	have been linked.6 Three errors were caused by words like targeted and started, which our word alignment algorithm deemed cognates." ></td>
	<td class="line x" title="167:221	12 of the hierarchical alignment failures in this category were true failures." ></td>
	<td class="line x" title="168:221	For example: 2a Cheney denied yesterday that the mission of his trip was to organize an assault on Iraq, while in Manama." ></td>
	<td class="line x" title="169:221	2b Yesterday in Manama, Cheney denied that the mission of his trip was to organize an assault on Iraq." ></td>
	<td class="line x" title="170:221	The alignment pattern of the words in bold is the familiar (3,1,4,2) permutation, as in Figure 1." ></td>
	<td class="line x" title="171:221	Most of the 12 true failures were due to movement of prepositional phrases." ></td>
	<td class="line x" title="172:221	The freedom of movement for such modifiers would be greater in bitexts that involve languages with less rigid word order than English." ></td>
	<td class="line x" title="173:221	Of the 30 sentence pairs in category 2, 16 could not be hierarchically aligned due to parser errors and 4 due to faulty word alignments." ></td>
	<td class="line x" title="174:221	10 were due to valid word reordering." ></td>
	<td class="line x" title="175:221	In the following example, a co-referring pronoun causes the word alignment to fail with a constraining tree on the second sentence: 3a But Chretien appears to have changed his stance after meeting with Bush in Washington last Thursday." ></td>
	<td class="line x" title="176:221	3b But after Chretien talked to Bush last Thursday in Washington, he seemed to change his original stance." ></td>
	<td class="line x" title="177:221	25 of the 30 sentence pairs in category 3 failed to align due to parser error." ></td>
	<td class="line x" title="178:221	5 examples failed because of valid word reordering." ></td>
	<td class="line x" title="179:221	1 of the 5 reorderings was due to a difference between active voice and passive voice, as in Figure 3." ></td>
	<td class="line x" title="180:221	The last row of Table 6 takes the various reasons for alignment failure into account." ></td>
	<td class="line x" title="181:221	It estimates what the failure rates would be if the monolingual parses and word alignments were perfect, with 95% confidence intervals." ></td>
	<td class="line x" title="182:221	These revised rates emphasize the importance of reliable word alignments for this kind of study." ></td>
	<td class="line x" title="183:221	6This sort of error is likely to happen with other word alignment algorithms too, because words and their common translations are likely to be linked even if theyre not translationally equivalent in the given sentence." ></td>
	<td class="line x" title="184:221	982 4 Discussion Figure 1 came from a real bilingual bitext, and Example 2 in Section 3.5 came from a real monolingual bitext.7 Neither of these examples can be hierarchically aligned correctly without gaps, even without constraining parse trees." ></td>
	<td class="line x" title="185:221	The received wisdom in the literature led us to expect no such examples in bilingual bitexts, let alone in monolingual bitexts." ></td>
	<td class="line x" title="186:221	See http://nlp.cs.nyu.edu/GenPar/ACL06 for more examples." ></td>
	<td class="line x" title="187:221	The English/English lower bounds are very loose, because the automatic word aligner would not link words that were not cognates." ></td>
	<td class="line x" title="188:221	Alignment failure rates on a hand aligned bitext would be higher." ></td>
	<td class="line x" title="189:221	We conclude that the ITG formalism cannot account for the natural complexity of translational equivalence, even when translation divergences are factored out." ></td>
	<td class="line x" title="190:221	Perhaps our most surprising results were those involving one constraining parse tree." ></td>
	<td class="line x" title="191:221	These results explain why constraints from independently generated monolingual parse trees have not improved statistical translation models." ></td>
	<td class="line x" title="192:221	For example, Koehn et al.(2003) reported that requiring constituents to be syntactically motivated does not lead to better constituent pairs, but only fewer constituent pairs, with loss of a good amount of valuable knowledge. This statement is consistent with our findings." ></td>
	<td class="line x" title="194:221	However, most of the knowledge loss could be prevented by allowing a gap." ></td>
	<td class="line x" title="195:221	With a parse tree constraining constituents on the English side, the coverage failure rate was 61% for the Chinese/English bitext (top row of Table 3), but allowing a gap decreased it to 6%." ></td>
	<td class="line x" title="196:221	Zhang and Gildea (2004) found that their alignment method, which did not use external syntactic constraints, outperformed the model of Yamada and Knight (2001)." ></td>
	<td class="line x" title="197:221	However, Yamada and Knights model could explain only the data that would pass the nogap test in our experiments with one constraining tree (first column of Table 3)." ></td>
	<td class="line x" title="198:221	Zhang and Gildeas conclusions might have been different if Yamada and Knights model were allowed to use discontinuous constituents." ></td>
	<td class="line x" title="199:221	The second row of Table 4 suggests that when constraining parse trees are used without gaps, at least 34% of training sentence pairs are likely to introduce noise into the model, even if systematic syntactic differences between languages are factored out." ></td>
	<td class="line x" title="200:221	We should not 7The examples were shortened for the sake of space and clarity." ></td>
	<td class="line x" title="201:221	0 10 20 30 40 50 60 70 80 90 100 0 10 20 30 40 50 60 70 cumulative %age of sentences span length Figure 5: Lengths of spans covering words in (3,1,4,2) permutations." ></td>
	<td class="line x" title="202:221	be surprised when such constraints do more harm than good." ></td>
	<td class="line x" title="203:221	To increase the chances that a translation model can explain complex word alignments, some authors have proposed various ways of extending a models domain of locality." ></td>
	<td class="line x" title="204:221	For example, Callison-Burch et al.(2005) have advocated for longer phrases in finite-state phrase-based translation models." ></td>
	<td class="line x" title="206:221	We computed the phrase length that would be necessary to cover the words involved in each (3,1,4,2) permutation in the MTEval bitext." ></td>
	<td class="line x" title="207:221	Figure 5 shows the cumulative percentage of these cases that would be covered by phrases up to a certain length." ></td>
	<td class="line x" title="208:221	Only 9 of the 171 cases (5.2%) could be covered by phrases of length 10 or less." ></td>
	<td class="line oc" title="209:221	Analogous techniques for tree-structured translation models involve either allowing each nonterminal to generate both terminals and other nonterminals (Groves et al. , 2004; Chiang, 2005), or, given a constraining parse tree, to flatten it (Fox, 2002; Zens and Ney, 2003; Galley et al. , 2004)." ></td>
	<td class="line n" title="210:221	Both of these approaches can increase coverage of the training data, but, as explained in Section 2, they risk losing generalization ability." ></td>
	<td class="line x" title="211:221	Our study suggests that there might be some benefits to an alternative approach using discontinuous constituents, as proposed, e.g., by Melamed et al.(2004) and Simard et al.(2005)." ></td>
	<td class="line x" title="214:221	The large differences in failure rates between the first and second columns of Table 3 are largely independent of the tightness of our lower bounds." ></td>
	<td class="line x" title="215:221	Synchronous parsing with discontinuities is computationally expensive in the worst case, but recently invented data structures make it feasible for typical inputs, as long as the number of gaps allowed per constituent is fixed at a small maximum (Waxmonsky and Melamed, 2006)." ></td>
	<td class="line x" title="216:221	More research is needed to investigate the trade-off between these costs and benefits." ></td>
	<td class="line x" title="217:221	983 5 Conclusions This paper presented evidence of phenomena that can lead to complex patterns of translational equivalence in bitexts of any language pair." ></td>
	<td class="line x" title="218:221	There were surprisingly many examples of such patterns that could not be analyzed using binary-branching structures without discontinuities." ></td>
	<td class="line x" title="219:221	Regardless of the languages involved, the translational equivalence relations in most real bitexts of non-trivial size cannot be generated by an inversion transduction grammar." ></td>
	<td class="line x" title="220:221	The low coverage rates without gaps under the constraints of independently generated monolingual parse trees might be the main reason why syntactic constraints have not yet increased the accuracy of SMT systems." ></td>
	<td class="line x" title="221:221	Allowing a single gap in bilingual phrases or other types of constituent can improve coverage dramatically." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-3601
A Syntax-Directed Translator With Extended Domain Of Locality
Huang, Liang;Knight, Kevin;Joshi, Aravind K.;"></td>
	<td class="line x" title="1:298	Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 18, New York City, New York, June 2006." ></td>
	<td class="line x" title="2:298	c2006 Association for Computational Linguistics A Syntax-Directed Translator with Extended Domain of Locality Liang Huang Dept. of Comp." ></td>
	<td class="line x" title="3:298	& Info." ></td>
	<td class="line x" title="4:298	Sci." ></td>
	<td class="line x" title="5:298	Univ. of Pennsylvania Philadelphia, PA 19104 lhuang3@cis.upenn.edu Kevin Knight Info." ></td>
	<td class="line x" title="6:298	Sci." ></td>
	<td class="line x" title="7:298	Inst." ></td>
	<td class="line x" title="8:298	Univ. of Southern California Marina del Rey, CA 90292 knight@isi.edu Aravind Joshi Dept. of Comp." ></td>
	<td class="line x" title="9:298	& Info." ></td>
	<td class="line x" title="10:298	Sci." ></td>
	<td class="line x" title="11:298	Univ. of Pennsylvania Philadelphia, PA 19104 joshi@linc.cis.upenn.edu Abstract A syntax-directed translator first parses the source-language input into a parsetree, and then recursively converts the tree into a string in the target-language." ></td>
	<td class="line x" title="12:298	We model this conversion by an extended treeto-string transducer that have multi-level trees on the source-side, which gives our system more expressive power and flexibility." ></td>
	<td class="line x" title="13:298	We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation." ></td>
	<td class="line x" title="14:298	The model is then extended to the general log-linear framework in order to rescore with other features like n-gram language models." ></td>
	<td class="line x" title="15:298	We devise a simple-yet-effective algorithm to generate non-duplicate k-best translations for n-gram rescoring." ></td>
	<td class="line x" title="16:298	Initial experimental results on English-to-Chinese translation are presented." ></td>
	<td class="line x" title="17:298	1 Introduction The concept of syntax-directed (SD) translation was originally proposed in compiling (Irons, 1961; Lewis and Stearns, 1968), where the source program is parsed into a tree representation that guides the generation of the object code." ></td>
	<td class="line x" title="18:298	Following Aho and Ullman (1972), a translation, as a set of string pairs, can be specified by a syntax-directed translation schema (SDTS), which is essentially a synchronous context-free grammar (SCFG) that generates two languages simultaneously." ></td>
	<td class="line x" title="19:298	An SDTS also induces a translator, a device that performs the transformation induces implements SD translator (source parser + recursive converter) specifies translation (string relation) SD translation schema (synchronous grammar) Figure 1: The relationship among SD concepts, adapted from (Aho and Ullman, 1972)." ></td>
	<td class="line x" title="20:298	    S NP(1) VP VB(2) NP(3), S VB(2) NP(1) NP(3)     Figure 2: An example of complex reordering represented as an STSG rule, which is beyond any SCFG." ></td>
	<td class="line x" title="21:298	from input string to output string." ></td>
	<td class="line x" title="22:298	In this context, an SD translator consists of two components, a sourcelanguage parser and a recursive converter which is usually modeled as a top-down tree-to-string transducer (Gecseg and Steinby, 1984)." ></td>
	<td class="line x" title="23:298	The relationship among these concepts is illustrated in Fig." ></td>
	<td class="line x" title="24:298	1." ></td>
	<td class="line x" title="25:298	This paper adapts the idea of syntax-directed translator to statistical machine translation (MT)." ></td>
	<td class="line x" title="26:298	We apply stochastic operations at each node of the source-language parse-tree and search for the best derivation (a sequence of translation steps) that converts the whole tree into some target-language string with the highest probability." ></td>
	<td class="line x" title="27:298	However, the structural divergence across languages often results in nonisomorphic parse-trees that is beyond the power of SCFGs." ></td>
	<td class="line x" title="28:298	For example, the S(VO) structure in English is translated into a VSO word-order in Arabic, an instance of complex reordering not captured by any 1 SCFG (Fig." ></td>
	<td class="line x" title="29:298	2)." ></td>
	<td class="line x" title="30:298	To alleviate the non-isomorphism problem, (synchronous) grammars with richer expressive power have been proposed whose rules apply to larger fragments of the tree." ></td>
	<td class="line x" title="31:298	For example, Shieber and Schabes (1990) introduce synchronous tree-adjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions." ></td>
	<td class="line x" title="32:298	STSGs and STAGs generate more tree relations than SCFGs, e.g. the non-isomorphic tree pair in Fig." ></td>
	<td class="line x" title="33:298	2." ></td>
	<td class="line x" title="34:298	This extra expressive power lies in the extended domain of locality (EDL) (Joshi and Schabes, 1997), i.e., elementary structures beyond the scope of onelevel context-free productions." ></td>
	<td class="line oc" title="35:298	Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox, 2002; Galley et al. , 2004)." ></td>
	<td class="line x" title="36:298	Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side." ></td>
	<td class="line x" title="37:298	Since an SD translator separates the sourcelanguage analysis from the recursive transformation, the domains of locality in these two modules are orthogonal to each other: in this work, we use a CFGbased Treebank parser but focuses on the extended domain in the recursive converter." ></td>
	<td class="line oc" title="38:298	Following Galley et al.(2004), we use a special class of extended tree-to-string transducer (xRs for short) with multilevel left-hand-side (LHS) trees.1 Since the righthand-side (RHS) string can be viewed as a flat onelevel tree with the same nonterminal root from LHS (Fig." ></td>
	<td class="line o" title="40:298	2), this framework is closely related to STSGs: they both have extended domain of locality on the source-side, while our framework remains as a CFG on the target-side." ></td>
	<td class="line x" title="41:298	For instance, an equivalent xRs rule for the complex reordering in Fig." ></td>
	<td class="line x" title="42:298	2 would be S(x1:NP, VP(x2:VB, x3:NP))x2 x1 x3 While Section 3 will define the model formally, we first proceed with an example translation from English to Chinese (note in particular that the inverted phrases between source and target): 1Throughout this paper, we will use LHS and source-side interchangeably (so are RHS and target-side)." ></td>
	<td class="line x" title="43:298	In accordance with our experiments, we also use English and Chinese as the source and target languages, opposite to the Foreign-to-English convention of Brown et al.(1993)." ></td>
	<td class="line x" title="45:298	(a) the gunman was [killed]1 by [the police]2." ></td>
	<td class="line x" title="46:298	parser (b) S NP-C DT the NN gunman VP VBD was VP-C VBN killed PP IN by NP-C DT the NN police PUNC . r1,r2 (c) qiangshou VP VBD was VP-C VBN killed PP IN by NP-C DT the NN police  r3 (d) qiangshou bei NP-C DT the NN police VBN killed  r5 r4 (e) qiangshou bei [jingfang]2 [jibi]1  Figure 3: A synatx-directed translation process for Example (1)." ></td>
	<td class="line x" title="47:298	(1) the gunman was killed by the police . qiangshou [gunman] bei [passive] jingfang [police] jibi [killed]  . Figure 3 shows how the translator works." ></td>
	<td class="line x" title="48:298	The English sentence (a) is first parsed into the tree in (b), which is then recursively converted into the Chinese string in (e) through five steps." ></td>
	<td class="line x" title="49:298	First, at the root node, we apply the rule r1 which preserves the toplevel word-order and translates the English period into its Chinese counterpart: (r1) S (x1:NP-C x2:VP PUNC ()." ></td>
	<td class="line x" title="50:298	)x1 x2  2 Then, the rule r2 grabs the whole sub-tree for the gunman and translates it as a phrase: (r2) NP-C ( DT (the) NN (gunman) )qiangshou Now we get a partial Chinese, partial English sentence qiangshou VP  as shown in Fig." ></td>
	<td class="line x" title="51:298	3 (c)." ></td>
	<td class="line x" title="52:298	Our recursion goes on to translate the VP sub-tree." ></td>
	<td class="line x" title="53:298	Here we use the rule r3 for the passive construction: (r3) VP VBD was VP-C x1:VBN PP IN by x2:NP-C  bei x2 x1 which captures the fact that the agent (NP-C, the police) and the verb (VBN, killed) are always inverted between English and Chinese in a passive voice." ></td>
	<td class="line x" title="54:298	Finally, we apply rules r4 and r5 which perform phrasal translations for the two remaining subtrees in (d), respectively, and get the completed Chinese string in (e)." ></td>
	<td class="line x" title="55:298	2 Previous Work It is helpful to compare this approach with recent efforts in statistical MT. Phrase-based models (Koehn et al. , 2003; Och and Ney, 2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order." ></td>
	<td class="line x" title="56:298	This is because the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), which is unable to process nested structures and long-distance dependencies in natural languages." ></td>
	<td class="line x" title="57:298	Syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems." ></td>
	<td class="line x" title="58:298	Both Yamada and Knight (2001) and Chiang (2005) use SCFGs as the underlying model, so their translation schemata are syntax-directed as in Fig." ></td>
	<td class="line x" title="59:298	1, but their translators are not: both systems do parsing and transformation in a joint search, essentially over a packed forest of parse-trees." ></td>
	<td class="line x" title="60:298	To this end, their translators are not directed by a syntactic tree." ></td>
	<td class="line x" title="61:298	Although their method potentially considers more than one single parse-tree as in our case, the packed representation of the forest restricts the scope of each transfer step to a one-level contextfree rule, while our approach decouples the sourcelanguage analyzer and the recursive converter, so that the latter can have an extended domain of locality." ></td>
	<td class="line x" title="62:298	In addition, our translator also enjoys a speedup by this decoupling, with each of the two stages having a smaller search space." ></td>
	<td class="line x" title="63:298	In fact, the recursive transfer step can be done by a a linear-time algorithm (see Section 5), and the parsing step is also fast with the modern Treebank parsers, for instance (Collins, 1999; Charniak, 2000)." ></td>
	<td class="line x" title="64:298	In contrast, their decodings are reported to be computationally expensive and Chiang (2005) uses aggressive pruning to make it tractable." ></td>
	<td class="line x" title="65:298	There also exists a compromise between these two approaches, which uses a k-best list of parse trees (for a relatively small k) to approximate the full forest (see future work)." ></td>
	<td class="line x" title="66:298	Besides, our model, as being linguistically motivated, is also more expressive than the formally syntax-based models of Chiang (2005) and Wu (1997)." ></td>
	<td class="line x" title="67:298	Consider, again, the passive example in rule r3." ></td>
	<td class="line x" title="68:298	In Chiangs SCFG, there is only one nonterminal X, so a corresponding rule would be was X(1) by X(2), bei X(2) X(1) which can also pattern-match the English sentence: I was [asleep]1 by [sunset]2 . and translate it into Chinese as a passive voice." ></td>
	<td class="line x" title="69:298	This produces very odd Chinese translation, because here was A by B in the English sentence is not a passive construction." ></td>
	<td class="line x" title="70:298	By contrast, our model applies rule r3 only if A is a past participle (VBN) and B is a noun phrase (NP-C)." ></td>
	<td class="line x" title="71:298	This example also shows that, one-level SCFG rule, even if informed by the Treebank as in (Yamada and Knight, 2001), is not enough to capture a common construction like this which is five levels deep (from VP to by)." ></td>
	<td class="line x" title="72:298	There are also some variations of syntax-directed translators where dependency structures are used in place of constituent trees (Lin, 2004; Ding and Palmer, 2005; Quirk et al. , 2005)." ></td>
	<td class="line x" title="73:298	Although they share with this work the basic motivations and similar speed-up, it is difficult to specify re-ordering information within dependency elementary structures, so they either resort to heuristics (Lin) or a separate ordering model for linearization (the other two 3 works).2 Our approach, in contrast, explicitly models the re-ordering of sub-trees within individual transfer rules." ></td>
	<td class="line x" title="74:298	3 Extended Tree-to-String Tranducers In this section, we define the formal machinery of our recursive transformation model as a special case of xRs transducers (Graehl and Knight, 2004) that has only one state, and each rule is linear (L) and non-deleting (N) with regarding to variables in the source and target sides (henth the name 1-xRLNs)." ></td>
	<td class="line x" title="75:298	Definition 1." ></td>
	<td class="line x" title="76:298	A 1-xRLNs transducer is a tuple (N,,,R) where N is the set of nonterminals,  is the input alphabet,  is the output alphabet, and R is a set of rules." ></td>
	<td class="line x" title="77:298	A rule in R is a tuple (t,s,) where: 1." ></td>
	<td class="line x" title="78:298	t is the LHS tree, whose internal nodes are labeled by nonterminal symbols, and whose frontier nodes are labeled terminals from  or variables from a setX ={x1,x2,}; 2." ></td>
	<td class="line x" title="79:298	s(X) is the RHS string; 3." ></td>
	<td class="line x" title="80:298	 is a mapping fromX to nonterminals N. We require each variable xiX occurs exactly once in t and exactly once in s (linear and non-deleting)." ></td>
	<td class="line pc" title="81:298	We denote (t) to be the root symbol of tree t. When writing these rules, we avoid notational overhead by introducing a short-hand form from Galley et al.(2004) that integrates the mapping into the tree, which is used throughout Section 1." ></td>
	<td class="line x" title="83:298	Following TSG terminology (see Figure 2), we call these variable nodes such as x2:NP-C substitution nodes, since when applying a rule to a tree, these nodes will be matched with a sub-tree with the same root symbol." ></td>
	<td class="line x" title="84:298	We also define|X|to be the rank of the rule, i.e., the number of variables in it." ></td>
	<td class="line x" title="85:298	For example, rules r1 and r3 in Section 1 are both of rank 2." ></td>
	<td class="line x" title="86:298	If a rule has no variable, i.e., it is of rank zero, then it is called a purely lexical rule, which performs a phrasal translation as in phrase-based models." ></td>
	<td class="line x" title="87:298	Rule r2, for instance, can be thought of as a phrase pairthe gunman, qiangshou." ></td>
	<td class="line x" title="88:298	Informally speaking, a derivation in a transducer is a sequence of steps converting a source-language 2Although hybrid approaches, such as dependency grammars augmented with phrase-structure information (Alshawi et al. , 2000), can do re-ordering easily." ></td>
	<td class="line x" title="89:298	r1 r2 r3 r4 r5 r1 r2 r6 r4 r7 r5 (a) (b) Figure 4: (a) the derivation in Figure 3; (b) another derviation producing the same output by replacing r3 with r6 and r7, which provides another way of translating the passive construction: (r6) VP ( VBD (was) VP-C (x1:VBN x2:PP ) )x2 x1 (r7) PP ( IN (by) x1:NP-C )bei x1 tree into a target-language string, with each step applying one tranduction rule." ></td>
	<td class="line x" title="90:298	However, it can also be formalized as a tree, following the notion of derivation-tree in TAG (Joshi and Schabes, 1997): Definition 2." ></td>
	<td class="line x" title="91:298	A derivation d, its source and target projections, noted E(d) and C(d) respectively, are recursively defined as follows: 1." ></td>
	<td class="line x" title="92:298	If r = (t,s,) is a purely lexical rule ( =), then d = r is a derivation, whereE(d) = t and C(d) = s; 2." ></td>
	<td class="line x" title="93:298	If r = (t,s,) is a rule, and di is a (sub-) derivation with the root symbol of its source projection matches the corresponding substitution node in r, i.e., (E(di)) = (xi), then d = r(d1,,dm) is also a derivation, where E(d) = [xi mapsto E(di)]t and C(d) = [xi mapsto C(di)]s. Note that we use a short-hand notation [ximapstoyi]t to denote the result of substituting each xi with yi in t, where xi ranges over all variables in t. For example, Figure 4 shows two derivations for the sentence pair in Example (1)." ></td>
	<td class="line x" title="94:298	In both cases, the source projection is the English tree in Figure 3 (b), and the target projection is the Chinese translation." ></td>
	<td class="line oc" title="95:298	Galley et al.(2004) presents a linear-time algorithm for automatic extraction of these xRs rules from a parallel corpora with word-alignment and parse-trees on the source-side, which will be used in our experiments in Section 6." ></td>
	<td class="line x" title="97:298	4 4 Probability Models 4.1 Direct Model Departing from the conventional noisy-channel approach of Brown et al.(1993), our basic model is a direct one: c = argmax c Pr(c|e) (2) where e is the English input string and c is the best Chinese translation according to the translation model Pr(c | e)." ></td>
	<td class="line x" title="99:298	We now marginalize over all English parse treesT(e) that yield the sentence e: Pr(c|e) = summationdisplay T (e) Pr(,c|e) = summationdisplay T (e) Pr( |e)Pr(c|) (3) Rather than taking the sum, we pick the best tree  and factors the search into two separate steps: parsing (4) (a well-studied problem) and tree-to-string translation (5) (Section 5):  = argmax T (e) Pr( |e) (4) c = argmax c Pr(c|) (5) In this sense, our approach can be considered as a Viterbi approximation of the computationally expensive joint search using (3) directly." ></td>
	<td class="line x" title="100:298	Similarly, we now marginalize over all derivations D() ={d|E(d) = } that translates English tree  into some Chinese string and apply the Viterbi approximation again to search for the best derivation d: c =C(d) =C(argmax dD() Pr(d)) (6) Assuming different rules in a derivation are applied independently, we approximate Pr(d) as Pr(d) = productdisplay rd Pr(r) (7) where the probability Pr(r) of the rule r is estimated by conditioning on the root symbol (t(r)): Pr(r) = Pr(t(r),s(r)|(t(r))) = c(r)summationtext rprime:(t(rprime))=(t(r)) c(rprime) (8) where c(r) is the count (or frequency) of rule r in the training data." ></td>
	<td class="line x" title="101:298	4.2 Log-Linear Model Following Och and Ney (2002), we extend the direct model into a general log-linear framework in order to incorporate other features: c = argmax c Pr(c|e)Pr(c)e|c| (9) where Pr(c) is the language model and e|c| is the length penalty term based on |c|, the length of the translation." ></td>
	<td class="line x" title="102:298	Parameters , , and  are the weights of relevant features." ></td>
	<td class="line x" title="103:298	Note that positive  prefers longer translations." ></td>
	<td class="line x" title="104:298	We use a standard trigram model for Pr(c)." ></td>
	<td class="line x" title="105:298	5 Search Algorithms We first present a linear-time algorithm for searching the best derivation under the direct model, and then extend it to the log-linear case by a new variant of k-best parsing." ></td>
	<td class="line x" title="106:298	5.1 Direct Model: Memoized Recursion Since our probability model is not based on the noisy channel, we do not call our search module a decoder as in most statistical MT work." ></td>
	<td class="line x" title="107:298	Instead, readers who speak English but not Chinese can view it as an encoder (or encryptor), which corresponds exactly to our direct model." ></td>
	<td class="line x" title="108:298	Given a fixed parse-tree , we are to search for the best derivation with the highest probability." ></td>
	<td class="line x" title="109:298	This can be done by a simple top-down traversal (or depth-first search) from the root of : at each node  in , try each possible rule r whose Englishside pattern t(r) matches the subtree  rooted at , and recursively visit each descendant node i in  that corresponds to a variable in t(r)." ></td>
	<td class="line x" title="110:298	We then collect the resulting target-language strings and plug them into the Chinese-side s(r) of rule r, getting a translation for the subtree  . We finally take the best of all translations." ></td>
	<td class="line x" title="111:298	With the extended LHS of our transducer, there may be many different rules applicable at one tree node." ></td>
	<td class="line x" title="112:298	For example, consider the VP subtree in Fig." ></td>
	<td class="line x" title="113:298	3 (c), where both r3 and r6 can apply." ></td>
	<td class="line x" title="114:298	As a result, the number of derivations is exponential in the size of the tree, since there are exponentially many 5 decompositions of the tree for a given set of rules." ></td>
	<td class="line x" title="115:298	This problem can be solved by memoization (Cormen et al. , 2001): we cache each subtree that has been visited before, so that every tree node is visited at most once." ></td>
	<td class="line x" title="116:298	This results in a dynamic programming algorithm that is guaranteed to run in O(npq) time where n is the size of the parse tree, p is the maximum number of rules applicable to one tree node, and q is the maximum size of an applicable rule." ></td>
	<td class="line x" title="117:298	For a given rule-set, this algorithm runs in time linear to the length of the input sentence, since p and q are considered grammar constants, and n is proportional to the input length." ></td>
	<td class="line x" title="118:298	The full pseudocode is worked out in Algorithm 1." ></td>
	<td class="line x" title="119:298	A restricted version of this algorithm first appears in compiling for optimal code generation from expression-trees (Aho and Johnson, 1976)." ></td>
	<td class="line x" title="120:298	In computational linguistics, the bottom-up version of this algorithm resembles the tree parsing algorithm for TSG by Eisner (2003)." ></td>
	<td class="line x" title="121:298	Similar algorithms have also been proposed for dependency-based translation (Lin, 2004; Ding and Palmer, 2005)." ></td>
	<td class="line x" title="122:298	5.2 Log-linear Model: k-best Search Under the log-linear model, one still prefers to search for the globally best derivation d: d = argmax dD() Pr(d) Pr(C(d))e|C(d)| (10) However, integrating the n-gram model with the translation model in the search is computationally very expensive." ></td>
	<td class="line x" title="123:298	As a standard alternative, rather than aiming at the exact best derivation, we search for top-k derivations under the direct model using Algorithm 1, and then rerank the k-best list with the language model and length penalty." ></td>
	<td class="line x" title="124:298	Like other instances of dynamic programming, Algorithm 1 can be viewed as a hypergraph search problem." ></td>
	<td class="line x" title="125:298	To this end, we use an efficient algorithm by Huang and Chiang (2005, Algorithm 3) that solves the general k-best derivations problem in monotonic hypergraphs." ></td>
	<td class="line x" title="126:298	It consists of a normal forward phase for the 1-best derivation and a recursive backward phase for the 2nd, 3rd, . . ., kth derivations." ></td>
	<td class="line x" title="127:298	Unfortunately, different derivations may have the same yield (a problem called spurious ambiguity), due to multi-level LHS of our rules." ></td>
	<td class="line x" title="128:298	In practice, this results in a very small ratio of unique strings among top-k derivations." ></td>
	<td class="line x" title="129:298	To alleviate this problem, determinization techniques have been proposed by Mohri and Riley (2002) for finite-state automata and extended to tree automata by May and Knight (2006)." ></td>
	<td class="line x" title="130:298	These methods eliminate spurious ambiguity by effectively transforming the grammar into an equivalent deterministic form." ></td>
	<td class="line x" title="131:298	However, this transformation often leads to a blow-up in forest size, which is exponential to the original size in the worst-case." ></td>
	<td class="line x" title="132:298	So instead of determinization, here we present a simple-yet-effective extension to the Algorithm 3 of Huang and Chiang (2005) that guarantees to output unique translated strings:  keep a hash-table of unique strings at each vertex in the hypergraph  when asking for the next-best derivation of a vertex, keep asking until we get a new string, and then add it into the hash-table This method should work in general for any equivalence relation (say, same derived tree) that can be defined on derivations." ></td>
	<td class="line x" title="133:298	6 Experiments Our experiments are on English-to-Chinese translation, the opposite direction to most of the recent work in SMT." ></td>
	<td class="line x" title="134:298	We are not doing the reverse direction at this time partly due to the lack of a sufficiently good parser for Chinese." ></td>
	<td class="line x" title="135:298	6.1 Data Preparation Our training set is a Chinese-English parallel corpus with 1.95M aligned sentences (28.3M words on the English side)." ></td>
	<td class="line oc" title="136:298	We first word-align them by GIZA++, then parse the English side by a variant of Collins (1999) parser, and finally apply the rule-extraction algorithm of Galley et al.(2004)." ></td>
	<td class="line x" title="138:298	The resulting rule set has 24.7M xRs rules." ></td>
	<td class="line x" title="139:298	We also use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a Chinese trigram model with Knesser-Ney smoothing on the Chinese side of the parallel corpus." ></td>
	<td class="line x" title="140:298	Our evaluation data consists of 140 short sentences (< 25 Chinese words) of the Xinhua portion of the NIST 2003 Chinese-to-English evaluation set." ></td>
	<td class="line x" title="141:298	Since we are translating in the other direction, we use the first English reference as the source input and the Chinese as the single reference." ></td>
	<td class="line x" title="142:298	6 Algorithm 1 Top-down Memoized Recursion 1: function TRANSLATE() 2: if cache[] defined then triangleright this sub-tree visited before?" ></td>
	<td class="line x" title="143:298	3: return cache[] 4: best0 5: for rRdo triangleright try each rule r 6: matched, sublistPATTERNMATCH(t(r),) triangleright tree pattern matching 7: if matched then triangleright if matched, sublist contains a list of matched subtrees 8: probPr(r) triangleright the probability of rule r 9: for isublist do 10: pi,siTRANSLATE(i) triangleright recursively solve each sub-problem 11: probprobpi 12: if prob > best then 13: bestprob 14: str[ximapstosi]s(r) triangleright plug in the results 15: cache[]best, str triangleright caching the best solution for future use 16: return cache[] triangleright returns the best string with its prob." ></td>
	<td class="line x" title="144:298	6.2 Initial Results We implemented our system as follows: for each input sentence, we first run Algorithm 1, which returns the 1-best translation and also builds the derivation forest of all translations for this sentence." ></td>
	<td class="line x" title="145:298	Then we extract the top 5000 non-duplicate translated strings from this forest and rescore them with the trigram model and the length penalty." ></td>
	<td class="line x" title="146:298	We compared our system with a state-of-the-art phrase-based system Pharaoh (Koehn, 2004) on the evaluation data." ></td>
	<td class="line x" title="147:298	Since the target language is Chinese, we report character-based BLEU score instead of word-based to ensure our results are independent of Chinese tokenizations (although our language models are word-based)." ></td>
	<td class="line x" title="148:298	The BLEU scores are based on single reference and up to 4-gram precisions (r1n4)." ></td>
	<td class="line x" title="149:298	Feature weights of both systems are tuned on the same data set.3 For Pharaoh, we use the standard minimum error-rate training (Och, 2003); and for our system, since there are only two independent features (as we always fix  = 1), we use a simple grid-based line-optimization along the language-model weight axis." ></td>
	<td class="line x" title="150:298	For a given languagemodel weight , we use binary search to find the best length penalty  that leads to a length-ratio closest 3In this sense, we are only reporting performances on the development set at this point." ></td>
	<td class="line x" title="151:298	We will report results tuned and tested on separate data sets in the final version of this paper." ></td>
	<td class="line x" title="152:298	Table 1: BLEU (r1n4) score results system BLEU Pharaoh 25.5 direct model (1-best) 20.3 log-linear model (rescored 5000-best) 23.8 to 1 against the reference." ></td>
	<td class="line x" title="153:298	The results are summarized in Table 1." ></td>
	<td class="line x" title="154:298	The rescored translations are better than the 1-best results from the direct model, but still slightly worse than Pharaoh." ></td>
	<td class="line x" title="155:298	7 Conclusion and On-going Work This paper presents an adaptation of the classic syntax-directed translation with linguisticallymotivated formalisms for statistical MT. Currently we are doing larger-scale experiments." ></td>
	<td class="line x" title="156:298	We are also investigating more principled algorithms for integrating n-gram language models during the search, rather than k-best rescoring." ></td>
	<td class="line x" title="157:298	Besides, we will extend this work to translating the top k parse trees, instead of committing to the 1-best tree, as parsing errors certainly affect translation quality." ></td>
	<td class="line x" title="158:298	7 References A. V. Aho and S. C. Johnson." ></td>
	<td class="line x" title="159:298	1976." ></td>
	<td class="line x" title="160:298	Optimal code generation for expression trees." ></td>
	<td class="line x" title="161:298	J. ACM, 23(3):488501." ></td>
	<td class="line x" title="162:298	Alfred V. Aho and Jeffrey D. Ullman." ></td>
	<td class="line x" title="163:298	1972." ></td>
	<td class="line x" title="164:298	The Theory of Parsing, Translation, and Compiling, volume I: Parsing." ></td>
	<td class="line x" title="165:298	Prentice Hall, Englewood Cliffs, New Jersey." ></td>
	<td class="line x" title="166:298	Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas." ></td>
	<td class="line x" title="167:298	2000." ></td>
	<td class="line x" title="168:298	Learning dependency translation models as collections of finite state head transducers." ></td>
	<td class="line x" title="169:298	Computational Linguistics, 26(1):4560." ></td>
	<td class="line x" title="170:298	Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer." ></td>
	<td class="line x" title="171:298	1993." ></td>
	<td class="line x" title="172:298	The mathematics of statistical machine translation: Parameter estimation." ></td>
	<td class="line x" title="173:298	Computational Linguistics, 19:263311." ></td>
	<td class="line x" title="174:298	Eugene Charniak." ></td>
	<td class="line x" title="175:298	2000." ></td>
	<td class="line x" title="176:298	A maximum-entropy-inspired parser." ></td>
	<td class="line x" title="177:298	In Proc." ></td>
	<td class="line x" title="178:298	of NAACL, pages 132139." ></td>
	<td class="line x" title="179:298	David Chiang." ></td>
	<td class="line x" title="180:298	2005." ></td>
	<td class="line x" title="181:298	A hierarchical phrase-based model for statistical machine translation." ></td>
	<td class="line x" title="182:298	In Proc." ></td>
	<td class="line x" title="183:298	of the 43rd ACL." ></td>
	<td class="line x" title="184:298	Michael Collins." ></td>
	<td class="line x" title="185:298	1999." ></td>
	<td class="line x" title="186:298	Head-Driven Statistical Models for Natural Language Parsing." ></td>
	<td class="line x" title="187:298	Ph.D. thesis, University of Pennsylvania." ></td>
	<td class="line x" title="188:298	Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein." ></td>
	<td class="line x" title="189:298	2001." ></td>
	<td class="line x" title="190:298	Introduction to Algorithms." ></td>
	<td class="line x" title="191:298	MIT Press, second edition." ></td>
	<td class="line x" title="192:298	Yuan Ding and Martha Palmer." ></td>
	<td class="line x" title="193:298	2005." ></td>
	<td class="line x" title="194:298	Machine translation using probablisitic synchronous dependency insertion grammars." ></td>
	<td class="line x" title="195:298	In Proceedings of the 43rd ACL." ></td>
	<td class="line x" title="196:298	Jason Eisner." ></td>
	<td class="line x" title="197:298	2003." ></td>
	<td class="line x" title="198:298	Learning non-isomorphic tree mappings for machine translation." ></td>
	<td class="line x" title="199:298	In Proceedings of ACL (companion volume), pages 205208." ></td>
	<td class="line x" title="200:298	Heidi J. Fox." ></td>
	<td class="line x" title="201:298	2002." ></td>
	<td class="line x" title="202:298	Phrasal cohesion and statistical machine translation." ></td>
	<td class="line x" title="203:298	In In Proc." ></td>
	<td class="line x" title="204:298	of EMNLP." ></td>
	<td class="line x" title="205:298	Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu." ></td>
	<td class="line x" title="206:298	2004." ></td>
	<td class="line x" title="207:298	Whats in a translation rule?" ></td>
	<td class="line x" title="208:298	In HLTNAACL." ></td>
	<td class="line x" title="209:298	F. Gecseg and M. Steinby." ></td>
	<td class="line x" title="210:298	1984." ></td>
	<td class="line x" title="211:298	Tree Automata." ></td>
	<td class="line x" title="212:298	Akademiai Kiado, Budapest." ></td>
	<td class="line x" title="213:298	Jonathan Graehl and Kevin Knight." ></td>
	<td class="line x" title="214:298	2004." ></td>
	<td class="line x" title="215:298	Training tree transducers." ></td>
	<td class="line x" title="216:298	In HLT-NAACL, pages 105112." ></td>
	<td class="line x" title="217:298	Liang Huang and David Chiang." ></td>
	<td class="line x" title="218:298	2005." ></td>
	<td class="line x" title="219:298	Better k-best Parsing." ></td>
	<td class="line x" title="220:298	In Proceedings of the Nineth International Workshop on Parsing Technologies (IWPT-2005), 9-10 October 2005, Vancouver, Canada." ></td>
	<td class="line x" title="221:298	E. T. Irons." ></td>
	<td class="line x" title="222:298	1961." ></td>
	<td class="line x" title="223:298	A syntax-directed compiler for ALGOL 60." ></td>
	<td class="line x" title="224:298	Comm." ></td>
	<td class="line x" title="225:298	ACM, 4(1):5155." ></td>
	<td class="line x" title="226:298	Aravind Joshi and Yves Schabes." ></td>
	<td class="line x" title="227:298	1997." ></td>
	<td class="line x" title="228:298	Tree-adjoining grammars." ></td>
	<td class="line x" title="229:298	In G. Rozenberg and A. Salomaa, editors, Handbook of Formal Languages, volume 3, pages 69  124." ></td>
	<td class="line x" title="230:298	Springer, Berlin." ></td>
	<td class="line x" title="231:298	Philipp Koehn, Franz Joseph Och, and Daniel Marcu." ></td>
	<td class="line x" title="232:298	2003." ></td>
	<td class="line x" title="233:298	Statistical phrase-based translation." ></td>
	<td class="line x" title="234:298	In Proc." ></td>
	<td class="line x" title="235:298	of HLT-NAACL, pages 127133." ></td>
	<td class="line x" title="236:298	Philipp Koehn." ></td>
	<td class="line x" title="237:298	2004." ></td>
	<td class="line x" title="238:298	Pharaoh: a beam search decoder for phrase-based statistical machine translation models." ></td>
	<td class="line x" title="239:298	In Proc." ></td>
	<td class="line x" title="240:298	of AMTA, pages 115124." ></td>
	<td class="line x" title="241:298	Shankar Kumar and William Byrne." ></td>
	<td class="line x" title="242:298	2003." ></td>
	<td class="line x" title="243:298	A weighted finite state transducer implementation of the alignment template model for statistical machine translation." ></td>
	<td class="line x" title="244:298	In Proc." ></td>
	<td class="line x" title="245:298	of HLT-NAACL, pages 142149." ></td>
	<td class="line x" title="246:298	P. M. Lewis and R. E. Stearns." ></td>
	<td class="line x" title="247:298	1968." ></td>
	<td class="line x" title="248:298	Syntax-directed transduction." ></td>
	<td class="line x" title="249:298	Journal of the ACM, 15(3):465488." ></td>
	<td class="line x" title="250:298	Dekang Lin." ></td>
	<td class="line x" title="251:298	2004." ></td>
	<td class="line x" title="252:298	A path-based transfer model for machine translation." ></td>
	<td class="line x" title="253:298	In Proceedings of the 20th COLING." ></td>
	<td class="line x" title="254:298	Jonathan May and Kevin Knight." ></td>
	<td class="line x" title="255:298	2006." ></td>
	<td class="line x" title="256:298	A better n-best list: Practical determinization of weighted finite tree automata." ></td>
	<td class="line x" title="257:298	Submitted to HLT-NAACL 2006." ></td>
	<td class="line x" title="258:298	Mehryar Mohri and Michael Riley." ></td>
	<td class="line x" title="259:298	2002." ></td>
	<td class="line x" title="260:298	An efficient algorithm for the n-best-strings problem." ></td>
	<td class="line x" title="261:298	In Proceedings of the International Conference on Spoken Language Processing 2002 (ICSLP 02), Denver, Colorado, September." ></td>
	<td class="line x" title="262:298	Franz Josef Och and Hermann Ney." ></td>
	<td class="line x" title="263:298	2002." ></td>
	<td class="line x" title="264:298	Discriminative training and maximum entropy models for statistical machine translation." ></td>
	<td class="line x" title="265:298	In Proc." ></td>
	<td class="line x" title="266:298	of ACL." ></td>
	<td class="line x" title="267:298	F. J. Och and H. Ney." ></td>
	<td class="line x" title="268:298	2004." ></td>
	<td class="line x" title="269:298	The alignment template approach to statistical machine translation." ></td>
	<td class="line x" title="270:298	Computational Linguistics, 30:417449." ></td>
	<td class="line x" title="271:298	Franz Och." ></td>
	<td class="line x" title="272:298	2003." ></td>
	<td class="line x" title="273:298	Minimum error rate training for statistical machine translation." ></td>
	<td class="line x" title="274:298	In Proc." ></td>
	<td class="line x" title="275:298	of ACL." ></td>
	<td class="line x" title="276:298	Chris Quirk, Arul Menezes, and Colin Cherry." ></td>
	<td class="line x" title="277:298	2005." ></td>
	<td class="line x" title="278:298	Dependency treelet translation: Syntactically informed phrasal smt." ></td>
	<td class="line x" title="279:298	In Proceedings of the 43rd ACL." ></td>
	<td class="line x" title="280:298	Stuart Shieber and Yves Schabes." ></td>
	<td class="line x" title="281:298	1990." ></td>
	<td class="line x" title="282:298	Synchronous tree-adjoining grammars." ></td>
	<td class="line x" title="283:298	In Proc." ></td>
	<td class="line x" title="284:298	of COLING, pages 253258." ></td>
	<td class="line x" title="285:298	Andrea Stolcke." ></td>
	<td class="line x" title="286:298	2002." ></td>
	<td class="line x" title="287:298	Srilm: an extensible language modeling toolkit." ></td>
	<td class="line x" title="288:298	In Proc." ></td>
	<td class="line x" title="289:298	of ICSLP." ></td>
	<td class="line x" title="290:298	Dekai Wu." ></td>
	<td class="line x" title="291:298	1997." ></td>
	<td class="line x" title="292:298	Stochastic inversion transduction grammars and bilingual parsing of parallel corpora." ></td>
	<td class="line x" title="293:298	Computational Linguistics, 23(3):377404." ></td>
	<td class="line x" title="294:298	Kenji Yamada and Kevin Knight." ></td>
	<td class="line x" title="295:298	2001." ></td>
	<td class="line x" title="296:298	A syntax-based statistical translation model." ></td>
	<td class="line x" title="297:298	In Proc." ></td>
	<td class="line x" title="298:298	of ACL ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-1512
Computing Translation Units and Quantifying Parallelism in Parallel Dependency Treebanks
Buch-Kromann, Matthias;"></td>
	<td class="line x" title="1:121	Proceedings of the Linguistic Annotation Workshop, pages 6976, Prague, June 2007." ></td>
	<td class="line x" title="2:121	c2007 Association for Computational Linguistics Computing translation units and quantifying parallelism in parallel dependency treebanks Matthias Buch-Kromann ISV Computational Linguistics Group Copenhagen Business School mbk.isv@cbs.dk Abstract The linguistic quality of a parallel treebank depends crucially on the parallelism between the source and target language annotations." ></td>
	<td class="line x" title="3:121	We propose a linguistic notion of translation units and a quantitative measure of parallelism for parallel dependency treebanks, and demonstrate how the proposed translation units and parallelism measure can be used to compute transfer rules, spot annotation errors, and compare different annotation schemes with respect to each other." ></td>
	<td class="line x" title="4:121	The proposal is evaluated on the 100,000 word Copenhagen Danish-English Dependency Treebank." ></td>
	<td class="line x" title="5:121	1 Introduction Parallel treebanks are increasingly seen as a valuable resource for many different tasks, including machine translation, word alignment, translation studies and contrastive linguistics ( Cmejrek et al. , 2004; Cyrus, 2006; Hansen-Schirra et al. , 2006)." ></td>
	<td class="line x" title="6:121	However, the usefulness of a parallel treebank for these purposes is directly correlated with the degree of syntactic parallelism in the treebank." ></td>
	<td class="line x" title="7:121	Some non-parallelism is inevitable because two languages always differ with respect to their syntactic structure." ></td>
	<td class="line x" title="8:121	But nonparallelism can also be the result of differences in the linguistic analyses of the source text and target text, eg, with respect to whether noun phrases are headed by nouns or determiners, whether conjunctions are headed by the first conjunct or the coordinator, whether prepositions are analyzed as heads or adjuncts in prepositional phrases, etc. In this paper, we focus on parallel dependency treebanks that consist of source texts and translations annotated with dependency analyses and word-alignments." ></td>
	<td class="line x" title="9:121	These requirements are directly satisfied by the analytical layer of the Prague Czech-English Dependency Treebank ( Cmejrek et al. , 2004) and by the dependency layer of the Copenhagen Danish-English Dependency Treebank (Buch-Kromann et al. , 2007)." ></td>
	<td class="line x" title="10:121	The requirements are also indirectly satisifed by parallel treebanks with a constituent layer and a word alignment, eg (Han et al. , 2002; Cyrus, 2006; Hansen-Schirra et al. , 2006; Samuelsson and Volk, 2006), since it is possible to transform constituent structures into dependency structures  a procedure used in the CoNLL shared tasks in 2006 and 2007 (Buchholz and Marsi, 2006)." ></td>
	<td class="line x" title="11:121	Finally, it is worth pointing out that the requirements are also met by any corpus equipped with two different dependency annotations since a text is always trivially word-aligned with itself." ></td>
	<td class="line x" title="12:121	The methods proposed in the paper therefore apply to a wide range of parallel treebanks, as well as to comparing two monolingual treebank annotations with each other." ></td>
	<td class="line x" title="13:121	The paper is structured as follows." ></td>
	<td class="line x" title="14:121	In section 2, we define our notions of word alignments and dependencies." ></td>
	<td class="line x" title="15:121	In section 3, we define our notion of translation units and state an algorithm for computing the translation units in a parallel dependency treebank." ></td>
	<td class="line x" title="16:121	Finally, in sections 4, 5 and 6, we demonstrate how translation units can be used to compute transfer rules, quantify parallelism, spot annotation errors, and compare monolingual and bilingual annotation schemes with respect to each other." ></td>
	<td class="line x" title="17:121	69 Complement roles Adjunct roles aobj adjectival object appa parenthetical apposition avobj adverbial object appr restrictive apposition conj conjunct of coordinator coord coordination dobj direct object list unanalyzed sequence expl expletive subject mod modifier iobj indirect object modo dobj-oriented modifier lobj locative-directional obj." ></td>
	<td class="line x" title="18:121	modp parenthetical modifier nobj nominal object modr restrictive modifier numa additive numeral mods subject-oriented mod." ></td>
	<td class="line x" title="19:121	numm multiplicative numeral name additional proper name part verbal particle namef additional first name pobj prepositional object namel additional last name possd possessed in genitives pnct punctuation modifier pred subject/object predicate rel relative clause qobj quotation object title title of person subj subject xpl explification (colon) vobj verbal object Figure 1: The main dependency roles in the dependency framework Discontinuous Grammar." ></td>
	<td class="line x" title="20:121	2 Word alignments and dependencies In our linguistic analyses, we will assume that a word alignment W Wprime encodes a translational correspondence between the word clusters W and Wprime in the source text and target text, ie, the word alignment expresses the human intuition that the subset W of words in the source text corresponds roughly in meaning or function to the subset Wprime of words in the target text." ></td>
	<td class="line x" title="21:121	The translations may contain additions or deletions, ie, W and Wprime may be empty." ></td>
	<td class="line x" title="22:121	We also assume that a dependency edge g rd encodes a complement or adjunct relation between a word g (the governor) and a complement or adjunct phrase headed by the word d (the dependent), where the edge label r specifies the complement or adjunct dependency role.1 As an illustration of how complement and adjunct relations can be encoded by means of dependency roles, the most important dependency roles used in the dependency framework Discontinuous Grammar (Buch-Kromann, 2006) are shown in Figure 1." ></td>
	<td class="line x" title="23:121	Finally, we will assume that the dependencies form a tree (or a forest)." ></td>
	<td class="line x" title="24:121	The tree may be non-projective, ie, it may contain crossing branches (technically, a dependency g rd is projective if 1Following standard dependency theoretic assumptions, we will assume the following differences between complement and adjunct relations: (a) complements are lexically licensed by their governor, whereas adjuncts license their adjunct governor; (b) in the functor-argument structure, complements act as arguments of their governor, whereas adjuncts act as modifiers of their governor; (c) a governor can have several adjuncts with the same adjunct role, whereas no two complements of the same governor can have the same complement role." ></td>
	<td class="line x" title="25:121	X X skal must kun only koncentrere concentrate sig self om about Y Y subj vobjmod dobj pobj nobj X has to concentrate only on Y subj dobj vobj pobjmod nobj Figure 2: Parallel dependency treebank analysis with word alignment and two monolingual dependency analyses." ></td>
	<td class="line x" title="26:121	and only if g is a transitive governor of all the words between g and d)." ></td>
	<td class="line x" title="27:121	Figure 2 shows an example of this kind of analysis, based on the annotation conventions used in Discontinuous Grammar and the associated Copenhagen Danish-English Dependency Treebank (BuchKromann et al. , 2007)." ></td>
	<td class="line x" title="28:121	In the example, word alignments are indicated by lines connecting Danish word clusters with English word clusters, and dependencies are indicated by means of arrows that point from the governor to the dependent, with the dependency role written at the arrow tip." ></td>
	<td class="line x" title="29:121	For example, the Danish word cluster koncentrere sig (concentrate self) has been aligned with the English word concentrate, and the English phrase 70 headed by on is analyzed as a prepositional object of the verb concentrate. In the Danish dependency analysis, the dependency between the adverbial kun (only) and its prepositional governor om (about) is non-projective because om does not dominate the words koncentrere (concentrate) and selv (self)." ></td>
	<td class="line x" title="30:121	Dependency analyses differ from phrase-structure analyses in that phrases are a derived notion: in a dependency tree, each word has a derived phrase that consists of all the words that can be reached from the word by following the arrows." ></td>
	<td class="line x" title="31:121	For example, the English word concentrate heads the phrase concentrate only on Y, and the Danish word om heads the discontinuous phrase kun om Y. If a parallel dependency analysis is well-formed, in a sense to be made clear in the following section, each alignment edge corresponds to what we will call a translation unit." ></td>
	<td class="line x" title="32:121	Intuitively, given an aligment edge W Wprime, we can create the corresponding translation unit by taking the source and target subtrees headed by the words in W and Wprime, deleting all parallel adjuncts of W Wprime, and replacing all remaining parallel dependents of W Wprime with variables x1,,xn and xprime1,,xprimen." ></td>
	<td class="line x" title="33:121	The resulting translation unit will be denoted by T(x1,,xn)Tprime(xprime1,,xprimen), where T and Tprime denote the source and target dependency trees in the translation unit." ></td>
	<td class="line x" title="34:121	For convenience, we will sometimes use vector notation and write T(x)Tprime(xprime) instead of T(x1,,xn)Tprime(xprime1,,xprimen)." ></td>
	<td class="line x" title="35:121	Dependencies are usually defined as relations between words, but by an abuse of terminology, we will say that a word d is a dependent of an alignment edge W Wprime provided d is a dependent of some word in W Wprime and d is not itself contained in W Wprime." ></td>
	<td class="line x" title="36:121	Figure 3 shows the six translation units that can be derived from the parallel dependency analysis in Figure 2, by means of the procedure outlined above." ></td>
	<td class="line x" title="37:121	Each translation unit can be interpreted as a bidirectional translation rules: eg, the second translation unit in Figure 3 can be interpreted as a translation rule stating that a Danish dependency tree with terminals x1 skal x2 can be translated into an English dependency tree with terminals xprime1 has to xprime2 where the English phrases xprime1,xprime2 are translations of the Danish phrases x1,x2, and vice versa." ></td>
	<td class="line x" title="38:121	In the following section, we will go deeper into XX x1x1 skalmust x2x2 koncentrereconcentratesigself x1x1 kunonly omabout x1x1 YY subj vobj dobj pobj nobj X x1 has to x2 concentratex1 only on x1 Y subj dobj vobj pobj nobj Figure 3: The six translation units derived from the parallel dependency analysis in Figure 2." ></td>
	<td class="line x" title="39:121	the definition and interpretation of these rules." ></td>
	<td class="line x" title="40:121	In particular, unlike the essentially context-free translation rules used in frameworks such as (Quirk et al. , 2005; Ding, 2006; Chiang, 2007), we will not assume that the words in the translation rules are ordered, and that the translation rules can only be used in a way that leads to projective dependency trees." ></td>
	<td class="line x" title="41:121	3 Translation units within a simple dependency-based translation model In many parallel treebanks, word alignments and syntactic annotations are created independently of each other, and there is therefore no guarantee that the word or phrase alignments coincide with any meaningful notion of translation units." ></td>
	<td class="line x" title="42:121	To rectify this problem, we need to define a notion of translation units that links the word alignments and the source and target dependency analysis in a meaningful way, and we need to specify a procedure for constructing a meaningful set of word alignments from the actual treebank annotation." ></td>
	<td class="line x" title="43:121	Statistical machine translation models often embody an explicit notion of translation units." ></td>
	<td class="line nc" title="44:121	However, many of these models are not applicable to parallel treebanks because they assume translation units where either the source text, the target text or both are represented as word sequences without any syntactic structure (Galley et al. , 2004; Marcu et al. , 2006; Koehn et al. , 2003)." ></td>
	<td class="line x" title="45:121	Other SMT models assume translation units where the source and target language annotation is based on either contextfree grammar (Yamada and Knight, 2001; Chiang, 2007) or context-free dependency grammar (Quirk et al. , 2005; Ding, 2006)." ></td>
	<td class="line x" title="46:121	However, since non71 projectivity is not directly compatible with contextfree grammar, and parallel dependency treebanks tend to encode non-projective dependencies directly, the context-free SMT models are not directly applicable to parallel dependency treebanks in general." ></td>
	<td class="line x" title="47:121	But the context-free SMT models are an important inspiration for the simple dependency-based translation model and notion of translation units that we will present below." ></td>
	<td class="line x" title="48:121	In our translation model, we will for simplicity assume that both the source dependency analysis and the target dependency analysis are unordered trees, ie, dependency transfer and word ordering are modelled as two separate processes." ></td>
	<td class="line x" title="49:121	In this paper, we only look at the dependency transfer and ignore the word ordering, as well as the probabilistic modelling of the rules for transfer and word ordering." ></td>
	<td class="line x" title="50:121	There are three kinds of translation rules in the model: A. Complement rules have the form T(x)Tprime(xprime) where T(x) is a source dependency tree with variables x = (x1,,xn), Tprime(xprime) is a target dependency tree with variables xprime = (xprime1,,xprimen), the words in T are aligned to the words in Tprime, and the variables xk,xprimek denote parallel source and target subtrees." ></td>
	<td class="line x" title="51:121	The rule states that a source tree T(x) can be transferred into a target tree Tprime(xprime) by transferring the source subtrees in x into the target subtrees in xprime." ></td>
	<td class="line x" title="52:121	B. Adjunct rules have the form (x aT(y)) (xprime aprimeTprime(yprime)) where T(y) is a source dependency tree, Tprime(yprime) is a target dependency tree, and x,xprime are variables that denote parallel adjunct subtrees with adjunct roles a,aprime, respectively." ></td>
	<td class="line x" title="53:121	The rule states that given a translation unit T(y)T(yprime), an a-adjunct of any word in T can be translated into an aprime-adjunct of any word in Tprime.2 C. Addition/deletion rules have the form T(y) (xprime aprimeTprime(yprime)) and (x aT(y))Tprime(yprime) where x,xprime are variables that denote adjunct subtrees, and a,aprime are adjunct relations." ></td>
	<td class="line x" title="54:121	The addition rule states that an adjunct subtree xprime can be introduced into the target tree Tprime in a translation unit T(y)T(yprime) without any corresponding adjunct in the source tree T . Similarly, the deletion rule states that the adjunct subtree 2In the form stated here, adjunct rules obviously overgenerate because they do not place any restrictions on the words in Tprime that the target adjunct can attach to." ></td>
	<td class="line x" title="55:121	In a full-fledged translation model, the adjunct rules must be augmented with a probabilistic model that can keep track of these restrictions." ></td>
	<td class="line x" title="56:121	X X skal must kun only koncentrere concentrate sig self om about Y Y subj mod vobj dobj pobj nobj X has to concentrate only on Y subj dobj vobj pobjmod nobj Figure 4: Parallel dependency analysis that is incompatible with our translation model." ></td>
	<td class="line x" title="57:121	x in the source tree T does not have to correspond to any adjunct in the target tree Tprime.3 The translation model places severe restrictions on the parallel dependency annotations." ></td>
	<td class="line x" title="58:121	For example, the annotation in Figure 4 is incompatible with our proposed translation model with respect to the adjunct only, since only attaches to the verb skal/must in the Danish analysis, but attaches to the preposition on in the English analysis  ie, it does not satisfy a requirement that follows implicitly from the adjunct rule: that corresponding source and target adjunct governors must belong to the same translation unit." ></td>
	<td class="line x" title="59:121	In our example, there are two ways of rectifying the problem: we can (a) correct the dependency analysis as shown in Figure 2, or (b) correct the word alignment as shown in Figure 5." ></td>
	<td class="line x" title="60:121	It can be shown that our translation model translates into the following four requirements on parallel analyses  ie, the requirements are necessary and sufficient for ensuring that the linguistic annotations are compatible with our translation model." ></td>
	<td class="line x" title="61:121	In the following, two words are said to be coaligned if they belong to the same alignment edge." ></td>
	<td class="line x" title="62:121	A dependency edge d rg is called internal if d and g are coaligned, and external otherwise." ></td>
	<td class="line x" title="63:121	A word w is called singular if it fails to be coaligned with at least one word in the other language." ></td>
	<td class="line x" title="64:121	Requirement I. The internal dependencies within a translation unit must form two connected trees." ></td>
	<td class="line x" title="65:121	Ie, 3As with adjunct rules, addition/deletion rules obviously overgenerate, and must be augmented with probabilistic models that keep track of the precise characteristics of the adjunct subtrees that are added to or deleted from the parallel analyses." ></td>
	<td class="line x" title="66:121	72 X X skal must kun only koncentrere concentrate sig self om about Y Y subj mod vobj dobj pobj nobj X has to concentrate only on Y subj dobj vobj pobjmod nobj Figure 5: Making the analysis from Figure 4 compatible with our translation model by changing the alignment edges." ></td>
	<td class="line x" title="67:121	in an alignment W Wprime, the internal dependencies within W must form a connected source tree, and similarly for Wprime." ></td>
	<td class="line x" title="68:121	Requirement II." ></td>
	<td class="line x" title="69:121	The external dependencies between translation units must form an acyclic graph." ></td>
	<td class="line x" title="70:121	Ie, in an alignment W Wprime, no word w W can be coaligned with an external transitive dependent of any word in Wprime, and vice versa." ></td>
	<td class="line x" title="71:121	Requirement III." ></td>
	<td class="line x" title="72:121	Parallel external governors must be aligned to each other." ></td>
	<td class="line x" title="73:121	Ie, if two nodes n,nprime are coaligned with external governor edges n rg and nprime rprimegprime, then g and gprime must be coaligned." ></td>
	<td class="line x" title="74:121	Requirement IV." ></td>
	<td class="line x" title="75:121	The graph contains no singular external complements." ></td>
	<td class="line x" title="76:121	If the source word c is a complement of governor g and c is not coaligned to any target word, then c and g must be coaligned to each other; and similarly for target complements." ></td>
	<td class="line x" title="77:121	A graph that satisfies all four requirements is said to be well-formed with respect to our translation model." ></td>
	<td class="line x" title="78:121	It can be shown that we can always transform an ill-formed graph G into a well-formed graph Gprime by merging alignment edges; Gprime is then called a reduction of G, and a reduction with a minimal number of mergings is called a minimal reduction of G. In a well-formed graph, we will refer to an alignment edge and its associated source and target dependency tree as a translation unit." ></td>
	<td class="line x" title="79:121	It can be shown that minimal reductions can be computed by means of the algorithm shown in Figure 6.4 The body of the for-loop in Figure 6 ensures 4In the algorithm, G is viewed as a directed graph that contains the source and target dependencies, and alignment edges procedure minimal-reduction(graph G) merge each alignment edge in G with itself (ie, ensure int." ></td>
	<td class="line x" title="80:121	connectedness & ext." ></td>
	<td class="line x" title="81:121	acyclicity) for each W Wprime in bottom-up order do merge W Wprime with all of its external singular complements merge all external governors of W Wprime return the modified graph G Figure 6: Algorithm for computing the minimal reduction of a graph G. Requirements III (coaligned external governors) and IV (no singular complements), and the merging operation is designed so that it ensures Requirements I (internal connectedness) and II (acyclicity).5 The ill-formed analysis in Figure 4 has the minimal reduction shown in Figure 2, whereas the analyses in Figure 2 and 5 are well-formed, ie, they are their own minimal reductions." ></td>
	<td class="line x" title="82:121	In the remainder of the paper, we will describe how minimal reductions and translation units can be used for extracting transfer rules, detecting annotation errors, and comparing different annotation schemes with each other." ></td>
	<td class="line x" title="83:121	4 Extracting transfer rules and quantifying parallelism The complement, adjunct, and addition/deletion rules in our simple dependency transfer model can be read off directly from the minimal reductions." ></td>
	<td class="line x" title="84:121	Figure 7 shows the three complement rules induced from Figure 4 via the minimal reduction in Figure 5." ></td>
	<td class="line x" title="85:121	Figure 8 (repeated from Figure 3) shows the six complement rules induced from the alternative analysis in Figure 2." ></td>
	<td class="line x" title="86:121	We have tested the extraction procedure on a large scale by applying it to the 100,000 word Copenhagen Danish-English Dependency Treebank (Buch-Kromann et al. , 2007)." ></td>
	<td class="line x" title="87:121	Figure 9 shows the percentage of translation units with size at least n W Wprime are viewed as short-hands for the set of all bidirectional edges that link two distinct nodes in W Wprime." ></td>
	<td class="line x" title="88:121	5The merging operation performs three steps: (a) replace two alignment edges W1Wprime1 and W2Wprime2 with their union W Wprime where W = W1W2 and Wprime = Wprime1Wprime2; (b) merge W Wprime with the smallest set of nodes that turns W and Wprime into connected dependency trees; (c) merge W Wprime with all nodes on cycles that involve at least one node from W Wprime." ></td>
	<td class="line x" title="89:121	73 XX x1x1 skalmust koncentrereconcentratesigself omabout x2x2 YY subj vobj dobj pobj nobj X x1 has to concentrateon x2 Y subj dobj vobj pobj nobj Figure 7: The three complement rules induced from Figure 4 via the minimal reduction in Figure 5." ></td>
	<td class="line x" title="90:121	XX x1x1 skalmust x2x2 koncentrereconcentratesigself x1x1 kunonly omabout x1x1 YY subj vobj dobj pobj nobj X x1 has to x2 concentratex1 only on x1 Y subj dobj vobj pobj nobj Figure 8: The six complement rules induced from the minimal reduction in Figure 2 (repeated from Figure 3)." ></td>
	<td class="line x" title="91:121	translation unit size n percenttunits with size  n normal scale(solid line) logarithmic scale(dotted line) 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 2 10 20 30 40 50 100% 10% 1% 0.1% 0.01% 0.001% Figure 9: The percentage of translation units in the Copenhagen Danish-English Dependency Treebank with size at least n, plotted on normal and logarithmic scales." ></td>
	<td class="line x" title="92:121	in the parallel treebank, where the size of a translation unit is measured as the number of nodes in the associated complement transfer rule." ></td>
	<td class="line x" title="93:121	The extracted transfer rules are useful for many purposes, including machine translation, lexicography, contrastive linguistics, and translation studies, but describing these applications is outside the scope of this paper." ></td>
	<td class="line x" title="94:121	5 Spotting annotation errors To the human annotator who must check the wordaligned dependency analyses in a parallel dependency treebank, the analyses in Figure 2 and Figure 4 look almost identical." ></td>
	<td class="line x" title="95:121	However, from the induced translation units and the associated complement rules shown above, it would have been immediately obvious to the annotator that the analysis in Figure 2 is significantly better than the analysis in Figure 4." ></td>
	<td class="line x" title="96:121	This suggests that we can increase the quality of the human annotation in parallel treebank projects by designing annotation tools that continuously compute the induced translation units and present them visibly to the human annotator." ></td>
	<td class="line x" title="97:121	From a linguistic point of view, it can be expected that errors in the dependency annotation will often show up as non-parallelism that results in a large induced translation unit." ></td>
	<td class="line x" title="98:121	So in a parallel dependency treebank, we can identify the most egregious examples of non-parallelism errors automatically by computing the induced translation units, and sorting them with respect to their size; the largest translation units will then have a high probability of being the result of annotation errors." ></td>
	<td class="line x" title="99:121	To confirm our linguistic expectation that large translation units are often caused by annotation errors, we have selected a sample of 75 translation units from the Copenhagen Danish-English Dependency Treebank, distributed more or less uniformly with respect to translation unit size in order to ensure that all translation unit sizes are sampled evenly." ></td>
	<td class="line x" title="100:121	We have then hand-checked each translation unit carefully in order to determine whether the translation unit contains any annotation errors or not, giving us a data set of the form (C,N) where N is the size of the translation unit and C indicates whether the translation unit is correct (C = 1) or not (C = 0)." ></td>
	<td class="line x" title="101:121	Figure 10 shows our maximum likelihood estimate of the conditional probability P(C = 1|N = n) that a translation unit with size n is correct.6 From the 6In order to estimate the conditional probability p(n) = P(C = 1|S = n) that a translation unit with size n is correct, we have fitted p(n) to the parametric family p(n) = n by means of conditional maximum likelihood estimation with conditional likelihood L = 75i=1 p(ni)ci(1 p(ni))1ci . The resulting esti74 translation unit size n est." ></td>
	<td class="line x" title="102:121	percentcorrect tunits with size = n normal scale(solid line) logarithmic scale(dotted line) 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 2 10 20 30 40 50 100% 10% 1% 0.1% 0.01% 0.001% Figure 10: The estimated percentage of translation units with size n that are correct, plotted on normal and logarithmic scales." ></td>
	<td class="line x" title="103:121	graph, we see that the correctness rate decreases quickly with n. For example, only 55% of all translation units with size 10 are correct, and only 13% of all translation units with size 20 are correct." ></td>
	<td class="line x" title="104:121	Thus, the statistics confirm that large translation units are often caused by annotation errors in the treebank, so focusing the effort on large translation units can make the postediting more cost-efficient." ></td>
	<td class="line x" title="105:121	This also suggests that when developing algorithms for automatic annotation of parallel dependency treebanks, the algorithms can improve their accuracy by penalizing large translation units." ></td>
	<td class="line x" title="106:121	6 Comparing annotation schemes Translation units can also be used to compare different annotation schemes." ></td>
	<td class="line x" title="107:121	This is relevant in parallel treebank projects where there are several possible annotation schemes for one of the languages  eg, because there is more than one treebank or rule-based parser for that language." ></td>
	<td class="line x" title="108:121	In this situation, we have the freedom of choosing the annotation schemes for the source and target languages so that they maximize the parallelism between the source and target language annotations." ></td>
	<td class="line x" title="109:121	To make an informed choice, we can create a small pilot parallel treebank for each annotation scheme, and compare mates are  = 0.99 and  = 1.77 with confidence value 0.87, ie, if a data set D with the same translation unit sizes is generated randomly from the distribution p(n)= n, then the conditional likelihood of D will be larger than the likelihood of our observed data set in 87% of the cases." ></td>
	<td class="line x" title="110:121	This means that a two-sided test does not reject that the data are generated from the estimated distribution p(n)." ></td>
	<td class="line x" title="111:121	the treebank annotations qualitatively by looking at their induced translation units, and quantitatively by looking at their average translation unit size." ></td>
	<td class="line x" title="112:121	The best choice of annotation schemes is then the combination that leads to the smallest and most sensible translation units." ></td>
	<td class="line x" title="113:121	Since texts are always trivially word-aligned with themselves, the same procedure applies to monolingual corpora where we want to compare two different dependency annotations with each other." ></td>
	<td class="line x" title="114:121	In this setup, structural differences between the two monolingual annotation schemes will show up as large translation units." ></td>
	<td class="line x" title="115:121	While these structural differences between annotation schemes could have been revealed by careful manual inspection, the automatic computation of translation units speeds up the process of identifying the differences." ></td>
	<td class="line x" title="116:121	The method also suggests that the conversion from one annotation scheme to another can be viewed as a machine translation problem  that is, if we can create a machine translation algorithm that learns to translate from one language to another on the basis of a parallel dependency treebank, then this algorithm can also be used to convert from one dependency annotation scheme to another, given a training corpus that has been annotated with both annotation schemes." ></td>
	<td class="line x" title="117:121	7 Conclusion In this paper, we have addressed the problem that the linguistic annotations in parallel treebanks often fail to correspond to meaningful translation units, because of internal incompatibilities between the dependency analyses and the word alignment." ></td>
	<td class="line x" title="118:121	We have defined a meaningful notion of translation units and provided an algorithm for computing these translation units from any parallel dependency treebank." ></td>
	<td class="line x" title="119:121	Finally, we have sketched how our notion of translation units can be used to aid the creation of parallel dependency treebanks by using the translation units as a visual aid for the human annotator, by using translation unit sizes to identify likely annotation errors, and by allowing a quantitative and qualitative comparison of different annotation schemes, both for parallel and monolingual treebanks." ></td>
	<td class="line x" title="120:121	75 8 Acknowledgments The work was supported by two grants from the Danish Research Council for the Humanities." ></td>
	<td class="line x" title="121:121	Thanks to the anonymous reviewers for their helpful comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1136
Extracting Synchronous Grammar Rules From Word-Level Alignments in Linear Time
Zhang, Hao;Gildea, Daniel;Chiang, David;"></td>
	<td class="line x" title="1:191	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 10811088 Manchester, August 2008 Extracting Synchronous Grammar Rules From Word-Level Alignments in Linear Time Hao Zhang and Daniel Gildea Computer Science Department University of Rochester Rochester, NY 14627, USA David Chiang Information Sciences Institute University of Southern California Marina del Rey, CA 90292, USA Abstract We generalize Uno and Yagiuras algorithm for finding all common intervals of two permutations to the setting of two sequences with many-to-many alignment links across the two sides." ></td>
	<td class="line x" title="2:191	We show how to maximally decompose a word-aligned sentence pair in linear time, which can be used to generate all possible phrase pairs or a Synchronous Context-Free Grammar (SCFG) with the simplest rules possible." ></td>
	<td class="line x" title="3:191	We also use the algorithm to precisely analyze the maximum SCFG rule length needed to cover hand-aligned data from various language pairs." ></td>
	<td class="line x" title="4:191	1 Introduction Many recent syntax-based statistical machine translation systems fall into the general formalism of Synchronous Context-Free Grammars (SCFG), where the grammar rules are found by first aligning parallel text at the word level." ></td>
	<td class="line oc" title="5:191	From wordlevel alignments, such systems extract the grammar rules consistent either with the alignments and parse trees for one of languages (Galley et al., 2004), or with the the word-level alignments alone without reference to external syntactic analysis (Chiang, 2005), which is the scenario we address here." ></td>
	<td class="line x" title="6:191	In this paper, we derive an optimal, linear-time algorithm for the problem of decomposing an arbitrary word-level alignment into SCFG rules such that each rule has at least one aligned word and is minimal in the sense that it cannot be further decomposed into smaller rules." ></td>
	<td class="line x" title="7:191	Extracting minimal rules is of interest both because rules with fewer words are more likely to generalize to new data, c2008." ></td>
	<td class="line x" title="8:191	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="9:191	Some rights reserved." ></td>
	<td class="line x" title="10:191	and because rules with lower rank (the number of nonterminals on the right-hand side) can be parsed more efficiently." ></td>
	<td class="line x" title="11:191	This algorithm extends previous work on factoring permutations to the general case of factoring many-to-many alignments." ></td>
	<td class="line x" title="12:191	Given two permutations of n, a common interval is a set of numbers that are consecutive in both." ></td>
	<td class="line x" title="13:191	The breakthrough algorithm of Uno and Yagiura (2000) computes all K common intervals of two length n permutations in O(n + K) time." ></td>
	<td class="line x" title="14:191	This is achieved by designing data structures to index possible boundaries of common intervals as the computation proceeds, so that not all possible pairs of beginning and end points need to be considered." ></td>
	<td class="line x" title="15:191	Landau et al.(2005) and Bui-Xuan et al.(2005) show that all common intervals can be encoded in O(n) space, and adapt Uno and Yagiuras algorithm to produce this compact representation in O(n) time." ></td>
	<td class="line x" title="18:191	Zhang and Gildea (2007) use similar techniques to factorize Synchronous Context Free Grammars in linear time." ></td>
	<td class="line x" title="19:191	These previous algorithms assume that the input is a permutation, but in machine translation it is common to work with word-level alignments that are many-to-many; in general any set of pairs of words, one from each language, is a valid alignment for a given bilingual sentence pair." ></td>
	<td class="line x" title="20:191	In this paper, we consider a generalized concept of common intervals given such an alignment: a common interval is a pair of phrases such that no word pair in the alignment links a word inside the phrase to a word outside the phrase." ></td>
	<td class="line x" title="21:191	Extraction of such phrases is a common feature of state-of-the-art phrase-based and syntax-based machine translation systems (Och and Ney, 2004a; Chiang, 2005)." ></td>
	<td class="line x" title="22:191	We generalize Uno and Yagiuras algorithm to this setting, and demonstrate a linear time algorithm forapairofalignedsequences." ></td>
	<td class="line x" title="23:191	Theoutputisa tree representation of possible phrases, which directly provides a set of minimal synchronous grammar 1081 rules for an SCFG-based machine translation system." ></td>
	<td class="line x" title="24:191	For phrase-based machine translation, one can also read all phrase pairs consistent with the original alignment off of the tree in time linear in the number of such phrases." ></td>
	<td class="line x" title="25:191	2 Alignments and Phrase Pairs Let [x,y] denote the sequence of integers between x and y inclusive, and [x,y) the integers between x and y1 inclusive." ></td>
	<td class="line x" title="26:191	An aligned sequence pair or simply an alignment is a tuple (E,F,A), where E = e1en and F = f1fm are strings, and A is a set of links (x,y), where 1  x  n and 1ym, connecting E and F. For most of this paper, since we are not concerned with the identity of the symbols in E and F, we will assume for simplicity that ei = i and fj = j, so that E = [1,n] and F = [1,m]." ></td>
	<td class="line x" title="27:191	In the context of statistical machine translation (Brown et al., 1993), we may interpretE as an English sentence, F its translation in French, and A a representation of how the words correspond to each other in the two sentences." ></td>
	<td class="line x" title="28:191	A pair of substrings [s,t] E and [u,v] F is a phrase pair (Och and Ney, 2004b) if and only if the subset of links emitted from [s,t] in E is equal to the subset of links emitted from [u,v] in F, and both are nonempty." ></td>
	<td class="line x" title="29:191	Figure 1a shows an example of a many-tomany alignment, where E = [1,6], F = [1,7], and A = {(1,6),(2,5),(2,7),(3,4), (4,1),(4,3),(5,2),(6,1),(6,3)}." ></td>
	<td class="line x" title="30:191	The eight phrase pairs in this alignment are: ([1,1], [6,6]), ([1,2], [5,7]), ([3,3], [4,4]), ([1,3], [4,7]), ([5,5], [2,2]), ([4,6], [1,3]), ([3,6], [1,4]), ([1,6], [1,7])." ></td>
	<td class="line x" title="31:191	In Figure 1b, we show the alignment matrix representation of the given alignment." ></td>
	<td class="line x" title="32:191	By default, the columns correspond to the tokens in E, the rows correspond to the tokens in F, and the black cells in the matrix are the alignment links in A. Using the matrix representation, the phrase pairs can be viewed as submatrices as shown with the blacklined boundary boxes." ></td>
	<td class="line x" title="33:191	Visually, a submatrix represents a phrase pair when it contains at least one alignment link and there are no alignment links directly above, below, or to the right or left of it." ></td>
	<td class="line x" title="34:191	e1 e2 e3 e4 e5 e6 f1 f2 f3 f4 f5 f6 f7 1 1 2 2 3 3 4 4 5 5 6 6 7 (a) (b) Figure 1: An example of (a) a many-to-many alignment and (b) the same alignment as a matrix, with its phrase pairs marked." ></td>
	<td class="line x" title="35:191	2.1 Number of Phrase Pairs In this section, we refine our definition of phrase pairs with the concept of tightness and give an asymptotic upper bound on the total number of such phrase pairs as the two sequences lengths grow." ></td>
	<td class="line x" title="36:191	In the original definition, the permissive many-to-many constraint allows for unaligned tokens in both sequences E and F. If there is an unaligned token adjacent to a phrase pair, then there is also a phrase pair that includes the unaligned token." ></td>
	<td class="line x" title="37:191	We say that a phrase pair ([s,t],[u,v]) is tight if none of es, et, fu and fv is unaligned." ></td>
	<td class="line x" title="38:191	By focusing on tight phrase pairs, we eliminate the non-tight ones that share the same set of alignment links with their tight counterpart." ></td>
	<td class="line x" title="39:191	Given [s,t] in E, let l be the first member of F that any position in [s,t] links to, and let u be the last." ></td>
	<td class="line x" title="40:191	According to the definition of tight phrase pair, [l,u] is the only candidate phrase in F to pair up with [s,t] in E. So, the total number of tight phrase pairs is upper-bounded by the total number of intervals in each sequence, which is O(n2)." ></td>
	<td class="line x" title="41:191	If we do not enforce the tightness constraint, the total number of phrase pairs can grow much faster." ></td>
	<td class="line x" title="42:191	For example, if a sentence contains only a single alignment link between the midpoint of F and the midpoint of E, then there will be O(n2m2) possible phrase pairs, but only a single tight phrase pair." ></td>
	<td class="line x" title="43:191	From now on, term phrase pair always refers to a tight phrase pair." ></td>
	<td class="line x" title="44:191	2.2 Hierarchical Decomposition of Phrase Pairs In this section, we show how to encode all the tight phrase pairs of an alignment in a tree of sizeO(n)." ></td>
	<td class="line x" title="45:191	Lemma 2.1." ></td>
	<td class="line x" title="46:191	When two phrase pairs overlap, the intersection, the differences, and the union of the two are also phrase pairs." ></td>
	<td class="line x" title="47:191	The following picture graphically represents the two possible overlapping structures of two phrase 1082 ([1,6], [1,7]) ([1,3], [4,7]) ([1,2], [5,7]) ([1,1], [6,6]) ([3,3], [4,4]) ([4,6], [1,3]) ([5,5], [2,2]) Figure 2: The normalized decomposition tree of the alignment in Figure 1." ></td>
	<td class="line x" title="48:191	pairs: ([s,t],[u,v]) and ([s,t],[u,v])." ></td>
	<td class="line x" title="49:191	s s t t u u v v s s t t u u v v Let AB and BC be two overlapping English phrases, with B being their overlap." ></td>
	<td class="line x" title="50:191	There are six possible phrases, A, B, C, AB, BC, and ABC, but if we omit BC, the remainder are nested and can be represented compactly by ((AB)C), from which BC can easily be recovered." ></td>
	<td class="line x" title="51:191	If we systematicallyapply this to the whole sentence, we obtain ahierarchicalrepresentationofallthephrasepairs, which we call the normalized decomposition tree." ></td>
	<td class="line x" title="52:191	The normalized decomposition tree for the example is shown in Figure 2." ></td>
	<td class="line x" title="53:191	Bui-Xuan et al.(2005) show that the family of common intervals is weakly partitive, i.e. closed under intersection, difference and union." ></td>
	<td class="line x" title="55:191	This allows the family to be represented as a hierarchical decomposition." ></td>
	<td class="line x" title="56:191	The normalized decomposition focuses on the right strong intervals, those that do not overlap with any others on the right." ></td>
	<td class="line x" title="57:191	Lemma 2.1 shows that the family of phrase pairs is also a weakly partitive family and can be hierarchically decomposed after normalization." ></td>
	<td class="line x" title="58:191	A minor difference is we prefer left strong intervals since our algorithms scan F from left to right." ></td>
	<td class="line x" title="59:191	Another difference is that we binarize a linearly-arranged sequence of non-overlapping phrase pairs instead of grouping them together." ></td>
	<td class="line x" title="60:191	In the following sections, we show how to produce the normalized hierarchical analysis of a given alignment." ></td>
	<td class="line x" title="61:191	3 Shift-Reduce Algorithm Inthissection,wepresentanO(n2+m+|A|)algorithm that is similar in spirit to a shift-reduce algorithm for parsing context-free languages." ></td>
	<td class="line x" title="62:191	This algorithm is not optimal, but its left-to-right bottomup control will form the basis for the improved algorithm in the next section." ></td>
	<td class="line x" title="63:191	First, we can efficiently test whether a span [x,y] is a phrase as follows." ></td>
	<td class="line x" title="64:191	Define a pair of functions l(x,y) and u(x,y) that record the minimum and maximum, respectively,of the positionson the French side that are linked to the positions [x,y]: l(x,y) = min{j|(i,j)A,i[x,y]} u(x,y) = max{j|(i,j)A,i[x,y]} Note thatl(,y) is monotone increasingandu(,y) is monotone decreasing." ></td>
	<td class="line x" title="65:191	Define a step of l(,y) (or u(,y)) to be a maximal interval over which l(,y) (resp., u(,y)) is constant." ></td>
	<td class="line x" title="66:191	We can compute u(x,y) in constant time from its value on smaller spans: u(x,y) = max{u(x,z),u(z + 1,y)} and similarly for l(x,y)." ></td>
	<td class="line x" title="67:191	We define the following functions to count the number of links emitted from prefixes of F and E: Fc(j) =|{(i,j)A|j j}| Ec(i) =|{(i,j)A|i i}| Then the difference Fc(u)Fc(l1) counts the total number of links to positions in [l,u], and Ec(y)Ec(x1) counts the total number of links to positions in [x,y]." ></td>
	<td class="line x" title="68:191	Ec and Fc can be precomputed in O(n + m +|A|) time." ></td>
	<td class="line x" title="69:191	Finally, let f(x,y) = Fc(u(x,y))Fc(l(x,y)1) (Ec(y)Ec(x1)) Note that f is non-negative, but not monotonic in general." ></td>
	<td class="line x" title="70:191	Figure 4 provides a visualization of u, l, and f for the example alignment from Section 2." ></td>
	<td class="line x" title="71:191	This gives us our phrase-pair test: Lemma 3.1." ></td>
	<td class="line x" title="72:191	[x,y] and [l(x,y),u(x,y)] are a phrase pair if and only if f(x,y) = 0." ></td>
	<td class="line x" title="73:191	This test is used in the following shift-reducestyle algorithm: X{1} for y[2,n] from left to right do append y to X for xX from right to left do compute u(x,y) from u(x + 1,y) compute l(x,y) from l(x + 1,y) if f(x,y) = 0 then [x,y] is a phrase 1083 remove [x+ 1,y] from X end if end for end for In the worst case, at each iteration we traverse the entire stack X without a successful reduction, indicating that the worst case time complexity is O(n2)." ></td>
	<td class="line x" title="74:191	4 A Linear Algorithm In this section, we modify the shift-reduce algorithm into a linear-time algorithm that avoids unnecessary reduction attempts." ></td>
	<td class="line x" title="75:191	It is a generalization of Uno and Yagiuras algorithm." ></td>
	<td class="line x" title="76:191	4.1 Motivation Thereasonthatourpreviousalgorithmisquadratic is that for each y, we try every possible combination with the values in X. Uno and Yagiura (2000) point out that in the case of permutations, it is not necessary to examine all spans, because it is possible to delete elements from X so that f(,y) is monotone decreasing on X. This means that all the x  X such that f(x,y) = 0 can always be conveniently found at the end of X. That this can be done safely is guaranteed by the following: Lemma 4.1." ></td>
	<td class="line x" title="77:191	If x1 < x2 < y and f(x1,y) < f(x2,y), then for all y  y, f(x2,y) > 0 (i.e., [x2,y] is not a phrase)." ></td>
	<td class="line x" title="78:191	Let us say that x2 violates monotonicity if x1 is the predecessor of x2 in X and f(x1,y) < f(x2,y)." ></td>
	<td class="line x" title="79:191	Then by Lemma 4.1, we can safely remove x2 from X. Furthermore, Uno and Yagiura (2000) show that we can enforce monotonicity at all times in such a way that the whole algorithm runs in linear time." ></td>
	<td class="line x" title="80:191	This is made possible with a shortcut based on the following: Lemma 4.2." ></td>
	<td class="line x" title="81:191	If x1 < x2 < y and u(x1,y1) > u(x2,y1) but u(x1,y) = u(x2,y), then for all y y, f(x2,y) > 0 (i.e., [x2,y] is not a phrase)." ></td>
	<td class="line x" title="82:191	The same holds mutatis mutandis for l. Let us say that y updates a step [x,y] of u (or l) if u(x,y) > u(x,y  1) (resp., l(x,y) < l(x,y1))." ></td>
	<td class="line x" title="83:191	By Lemma 4.2, if [x1,y1] and [x2,y2] are different steps of u(,y1) (resp., l(,y1)) and y updates both of them, then we can remove from X all x such that x2 x < y. u(, y  1) l(, y  1) u(, y) l(, y) x1 y2 y x2 y1 Figure 3: Illustration of step (3) of the algorithm." ></td>
	<td class="line x" title="84:191	The letters indicate substeps of (3)." ></td>
	<td class="line x" title="85:191	4.2 Generalized algorithm These results generalize to the many-to-many alignment case, although we must introduce a few nuances." ></td>
	<td class="line x" title="86:191	The new algorithm proceeds as follows: Initialize X = {1}." ></td>
	<td class="line x" title="87:191	For y  [2,n] from left to right: 1." ></td>
	<td class="line x" title="88:191	Append y to X. 2." ></td>
	<td class="line x" title="89:191	Update u and l: (a) Traverse the steps of u(,y  1) from right to left and compute u(,y) until we have found the leftmost step [x,y] of u(,y1) that gets updated by y." ></td>
	<td class="line x" title="90:191	(b) Do the same for l. We have computed two values for x; let x1 be the smaller and x2 be the larger." ></td>
	<td class="line x" title="91:191	Similarly, let y1 be the smaller y." ></td>
	<td class="line x" title="92:191	3." ></td>
	<td class="line x" title="93:191	Enforce monotonicity of f(,y) (see Figure 3): (a) The positions left of the smaller x always satisfy monotonicity, so do nothing." ></td>
	<td class="line x" title="94:191	(b) For x  [x1,x2)X while x violates monotonicity, remove x from X." ></td>
	<td class="line x" title="95:191	(c) For x  [x2,y1]X while x violates monotonicity, remove x from X." ></td>
	<td class="line x" title="96:191	(d) The steps right of y1 may or may not violate monotonicity, but we use the stronger Lemma 4.2 to delete all of them (excluding y).1 1In the special case where [x,y] is updated by y to the 1084 y = 1 : 1 1 2 2 3 3 4 4 5 5 6 6 7 u,l x 10 2 1 3 2 4 3 5 4 6 5 6 f x y = 2 : 1 1 2 2 3 3 4 4 5 5 6 6 7 u,l x 10 2 1 3 2 4 3 5 4 6 5 6 f x y = 3 : 1 1 2 2 3 3 4 4 5 5 6 6 7 u,l x 10 2 1 3 2 4 3 5 4 6 5 6 f x y = 4 : 1 1 2 2 3 3 4 4 5 5 6 6 7 u,l x 10 2 1 3 2 4 3 5 4 6 5 6 f x y = 5 : 1 1 2 2 3 3 4 4 5 5 6 6 7 u,l x 10 2 1 3 2 4 3 5 4 6 5 6 f x y = 6 : 1 1 2 2 3 3 4 4 5 5 6 6 7 u,l x 10 2 1 3 2 4 3 5 4 6 5 6 f x Figure 4: The evolution of u(x,y) , l(x,y), and f(x,y) as y goes from 1 to 6 for the example alignment." ></td>
	<td class="line x" title="97:191	Each pair of diagrams shows the state of affairs between steps (3) and (4) of the algorithm." ></td>
	<td class="line x" title="98:191	Light grey boxes are the steps of u, and darker grey boxes are the steps of l. We use solid boxes to plot the values of remaining xs on the list but also show the other values in empty boxes for completeness." ></td>
	<td class="line x" title="99:191	(e) Finally, if y violates monotonicity, remove it from X. 4." ></td>
	<td class="line x" title="100:191	For xX from right to left until f(x,y) > 0, output [x,y] and remove xs successor in X.2 An example of the algorithms execution is shown in Figure 4." ></td>
	<td class="line x" title="101:191	The evolution of u(x,y), l(x,y), and f(x,y) is displayed for increasing y (from 1 to 6)." ></td>
	<td class="line x" title="102:191	We point out the interesting steps." ></td>
	<td class="line x" title="103:191	When y = 2, position 2 is eliminated due to step (3e) of our algorithm to ensure monotonicity of f at the right end, and [1,2] is reduced." ></td>
	<td class="line x" title="104:191	When same value as the step to its left, we can use Lemma 4.2 to delete [x,y] and y as well, bypassing steps (3b),(3c), and (3e)." ></td>
	<td class="line x" title="105:191	2If there are any such x, they must lie to the left of x1." ></td>
	<td class="line x" title="106:191	Therefore a further optimization would be to perform step (4) before step (3), starting with the predecessor of x1." ></td>
	<td class="line x" title="107:191	If a reduction is made, we can jump to step (3e)." ></td>
	<td class="line x" title="108:191	y = 3, two reductions are made: one on [3,3] and the other on [1,3]." ></td>
	<td class="line x" title="109:191	Because of leftmost normalization, position 3 is deleted." ></td>
	<td class="line x" title="110:191	When y = 6, we have x1 = x2 = y1 = 5, so that position 5 is deleted by step (3c) and position 6 is deleted by step (3e)." ></td>
	<td class="line x" title="111:191	4.3 Correctness We have already argued in Section 4.1 that the deletionof elements fromX does not alterthe output of the algorithm." ></td>
	<td class="line x" title="112:191	It remains to show that step (3) guarantees monotonicity: Claim 4.3." ></td>
	<td class="line x" title="113:191	For all y, at the end of step (3), f(,y) is monotone decreasing." ></td>
	<td class="line x" title="114:191	Proof." ></td>
	<td class="line x" title="115:191	By induction on y. For y = 1, the claim is trivially true." ></td>
	<td class="line x" title="116:191	For y > 1, we want to show that for x1,x2 adjacent in X such that x1 < x2, f(x1,y)f(x2,y)." ></td>
	<td class="line x" title="117:191	We consider the five regions of X covered by step (3) (cf.Figure 3), and then 1085 the boundaries between them." ></td>
	<td class="line x" title="119:191	Region (a): x1,x2  [1,x1]." ></td>
	<td class="line x" title="120:191	Since u(xi,y) = u(xi,y1) and l(xi,y) = l(xi,y1), we have: f(xi,y)f(xi,y1) = 0(Ec(y)Ec(y1)) i.e., in this region, f shifts down uniformly from iteration y1 to iteration y. Hence, if f(,y 1) was monotonic, then f(,y) is also monotonic within this region." ></td>
	<td class="line x" title="121:191	Region (b): x1,x2  [x1,x2)." ></td>
	<td class="line x" title="122:191	Since u(x1,y 1) = u(x2,y1) and u(x1,y) = u(x2,y) and similarly for l, we have: f(x1,y)f(x1,y1) = f(x2,y)f(x2,y1) i.e., in this region, f shifts up or down uniformly.3 Hence, if f(,y1) was monotonic, then f(,y) is also monotonic within this region." ></td>
	<td class="line x" title="123:191	Region (c): x1,x2[x2,y1]." ></td>
	<td class="line x" title="124:191	Same as Case 2." ></td>
	<td class="line x" title="125:191	Region (d) and (e): Vacuous (these regions have at most one element)." ></td>
	<td class="line x" title="126:191	The remaining values of x1,x2 are those that straddle the boundaries between regions." ></td>
	<td class="line x" title="127:191	But step (3) of the algorithm deals with each of these boundaries explicitly, deleting elements until f(x1)f(x2)." ></td>
	<td class="line x" title="128:191	Thus f(,y) is monotonic everywhere." ></td>
	<td class="line x" title="129:191	4.4 Implementation and running time X should be implemented in a way that allows linear-time traversal and constant-time deletion; also, u and l must be implemented in a way that allows linear-time traversal of their steps." ></td>
	<td class="line x" title="130:191	Doublylinked lists are appropriate for all three functions." ></td>
	<td class="line x" title="131:191	Claim 4.4." ></td>
	<td class="line x" title="132:191	The above algorithm runs in O(n + m +|A|) time." ></td>
	<td class="line x" title="133:191	We can see that the algorithmruns in linear time if we observe that whenever we traverse a part of X, we delete it, except for a constant amount of workperiteration(thatis,pervalueofy): thesteps traversed in (2) are all deleted in (3d) except four (two for u and two for l); the positions traversed in (3b), (3c), and (4) are all deleted except one." ></td>
	<td class="line x" title="134:191	4.5 SCFG Rule extraction The algorithm of the previous section outputs the normalized decomposition tree depicted in Figure 2." ></td>
	<td class="line x" title="135:191	From this tree, it is straightforwardto obtain 3It can be shown further that in this region, f shifts up or is unchanged." ></td>
	<td class="line x" title="136:191	Therefore any reductions in step (4) must be in region (a)." ></td>
	<td class="line x" title="137:191	AB(1) C(2), C(2) B(1) BD(1) E(2), E(2) D(1) DG(1) e2, f5 G(1) f6 Ge1, f6 Ee3, f4 Ce4 F(1) e6, f1 F(1) f3 F e5, f2 Figure 5: Each node from the normalized decompositiontree of Figure2 is convertedinto an SCFG rule." ></td>
	<td class="line x" title="138:191	a set of maximally-decomposed SCFG rules." ></td>
	<td class="line x" title="139:191	As an example, the tree of Figure 2 produces the rules shown in Figure 5." ></td>
	<td class="line x" title="140:191	We adopt the SCFG notation of Satta and Peserico (2005)." ></td>
	<td class="line x" title="141:191	Each rule has a right-hand side sequence for both languages, separated by a comma." ></td>
	<td class="line x" title="142:191	Superscript indices in the right-hand side of grammar rules such as: AB(1) C(2), C(2) B(1) indicate that the nonterminals with the same index are linked across the two languages, and will eventually be rewritten by the same rule application." ></td>
	<td class="line x" title="143:191	The example above inverts the order of B and C when translating from the source language to the target language." ></td>
	<td class="line x" title="144:191	The SCFG rule extraction proceeds as follows." ></td>
	<td class="line x" title="145:191	Assignanonterminallabeltoeachnodeinthetree." ></td>
	<td class="line x" title="146:191	Then for each node (S,T) in the tree top-down, where S and T are sequences of positions, 1." ></td>
	<td class="line x" title="147:191	For each child (S,T), S and T must be subsequences of S and T, respectively." ></td>
	<td class="line x" title="148:191	Replace their occurrences in S andT with a pair of coindexed nonterminals X, where X is the nonterminal assigned to the child." ></td>
	<td class="line x" title="149:191	2." ></td>
	<td class="line x" title="150:191	For each remaining position i in S, replace i with ei." ></td>
	<td class="line x" title="151:191	3." ></td>
	<td class="line x" title="152:191	For each remaining position j in T, replace j with fj." ></td>
	<td class="line x" title="153:191	4." ></td>
	<td class="line x" title="154:191	Output the rule X  S, T, where X is the nonterminal assigned to the parent." ></td>
	<td class="line x" title="155:191	As an example, consider the node ([4,6],[1,3]) in Figure 2." ></td>
	<td class="line x" title="156:191	After step 1, it becomes (4F(1) 6,1F(1) 3) and after steps 2 and 3, it becomes (e4 F(1) e6,f1 F(1) f3) 1086 0 1 2 3 4 5 6 Hindi/English 52.8 53.5 99.9 99.9 100.0 Chinese/English 51.0 52.4 99.7 99.8 100.0 100.0 100.0 French/English 52.1 53.5 99.9 100.0 100.0 100.0 Romanian/English 50.8 52.6 99.9 99.9 100.0 100.0 Spanish/English 50.7 51.8 99.9 100.0 100.0 100.0 Table 1: Cumulative percentages of rule tokens by number of nonterminals in right-hand side." ></td>
	<td class="line x" title="157:191	A blank indicates that no rules were found with that number of nonterminals." ></td>
	<td class="line x" title="158:191	Finally, step 4 outputs Ce4 F(1) e6, f1 F(1) f3 A few choices are available to the user depending on the application intended for the SCFG extraction." ></td>
	<td class="line x" title="159:191	The above algorithm starts by assigning a nonterminal to each node in the decomposition tree;onecouldassignauniquenonterminaltoeach node, so that the resulting grammar produces exactly the set of sentences given as input." ></td>
	<td class="line x" title="160:191	But for machine translation, one may wish to use a single nonterminal, such that the extracted rules can recombine freely, as in Chiang (2005)." ></td>
	<td class="line x" title="161:191	Unaligned words in either language (an empty row or column in the alignmentmatrix, not present in our example) will be attached as high as possible in our tree." ></td>
	<td class="line x" title="162:191	However, other ways of handling unaligned words are possible given the decomposition tree." ></td>
	<td class="line x" title="163:191	One can produce all SCFG rules consistent with the alignment by, for each unaligned word, looping through possible attachment points in the decomposition tree." ></td>
	<td class="line x" title="164:191	In this case, the number of SCFG rules produced may be exponential in the size of the original input sentence; however, even in this case, the decomposition tree enables a rule extractionalgorithmthatis linearin the output length (the number of SCFG rules)." ></td>
	<td class="line x" title="165:191	4.6 Phrase extraction We briefly discuss the process of extracting all phrase pairs consistent with the original alignment from the normalized decomposition tree." ></td>
	<td class="line x" title="166:191	First of all, every node in the tree gives a valid phrase pair." ></td>
	<td class="line x" title="167:191	Then, in the case of overlapping phrase pairs such as the example in Section 2.1, the decomposition tree will contain a left-branching chain of binary nodes all performing the same permutation." ></td>
	<td class="line x" title="168:191	While traversing the tree, whenever we identify such a chain, let 1,,k be the sequence of all the children of the nodes in the chain." ></td>
	<td class="line x" title="169:191	Then, each of the subsequences{i,,j | 1 < i < j  k}yields a valid phrase pair." ></td>
	<td class="line x" title="170:191	In our example, the root of the tree of Figure 2 and its left child form such a chain, with three children; the subsequence {([3,3],[4,4]),([4,6],[1,3])} yields the phrase ([3,6],[1,4])." ></td>
	<td class="line x" title="171:191	In the case of unaligned words, we can also consider all combinations of their attachments, as discussed for SCFG rule extraction." ></td>
	<td class="line x" title="172:191	5 Experiments on Analyzing Word Alignments One application of our factorization algorithm is analyzing human-annotated word alignments." ></td>
	<td class="line x" title="173:191	Wellington et al.(2006) argue for the necessity of discontinuous spans (i.e., for a formalism beyond Synchronous CFG) in order for synchronous parsing to cover human-annotated word alignment data under the constraint that rules have a rank of no more than two." ></td>
	<td class="line x" title="175:191	In a related study, Zhang and Gildea (2007) analyze the rank of the Synchronous CFG derivation trees needed to parse the same data." ></td>
	<td class="line x" title="176:191	The number of discontinuous spans and the rank determine the complexity of dynamic programming algorithms for synchronous parsing (alignment) or machine translation decoding." ></td>
	<td class="line x" title="177:191	Both studies make simplifying assumptions on the alignment data to avoid dealing with many-tomany word links." ></td>
	<td class="line x" title="178:191	Here, we apply our alignment factorization algorithm directly to the alignments to produce a normalized decomposition tree for eachalignmentandcollectstatisticson the branching factors of the trees." ></td>
	<td class="line x" title="179:191	We use the same alignment data for the five language pairs Chinese-English, RomanianEnglish, Hindi-English, Spanish-English, and French-English as Wellington et al.(2006)." ></td>
	<td class="line x" title="181:191	Table 1 reports the number of rules extracted by the rank, or number of nonterminals on the right-hand side." ></td>
	<td class="line x" title="182:191	Almost all rules are binary, implying both that binary synchronous grammars are adequate for MT, and that our algorithmcan find such grammars." ></td>
	<td class="line x" title="183:191	Table 2 gives similar statistics for the number of terminals in each rule." ></td>
	<td class="line x" title="184:191	The phrases we extract are short enough that they are likely to generalize to new sentences." ></td>
	<td class="line x" title="185:191	The apparent difficulty of 1087 0 1 2 3 4 5 6 7 8 9 10 max Hindi/English 39.6 92.2 97.7 99.5 99.7 99.9 99.9 100.0 7 Chinese/English 39.8 87.2 96.2 99.0 99.7 99.9 100.0 100.0 100.0 100.0 100.0 12 French/English 44.5 89.0 93.4 95.8 97.5 98.4 99.0 99.3 99.6 99.8 100.0 18 Romanian/English 42.9 89.8 96.9 98.9 99.5 99.8 99.9 100.0 100.0 9 Spanish/English 47.5 91.8 97.7 99.4 99.9 99.9 100.0 100.0 100.0 9 Table 2: Cumulative percentages of rule tokens by number of terminals in right-hand side." ></td>
	<td class="line x" title="186:191	A blank indicates that no rules were found with that number of terminals." ></td>
	<td class="line x" title="187:191	the French-English pair is due to the large number of possible alignments in this dataset." ></td>
	<td class="line x" title="188:191	6 Conclusion By extending the algorithm of Uno and Yagiura (2000) from one-to-one mappings to many-tomanymappings,wehaveshownhowtoconstructa hierarchicalrepresentationofallthephrasepairsin a given aligned sentence pair in linear time, which yields a set of minimal SCFG rules." ></td>
	<td class="line x" title="189:191	We have also illustratedhow to apply the algorithm as an analytical tool for aligned bilingual data." ></td>
	<td class="line x" title="190:191	Acknowledgments Thanks to Bob Moore for suggesting the extension to phrase extraction at SSST 2007." ></td>
	<td class="line x" title="191:191	This work was supported in part by NSF grants IIS-0546554 and ITR-0428020, andDARPAgrantHR0011-06-C-0022underBBN Technologies subcontract 9500008412." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1138
Grammar Comparison Study for Translational Equivalence Modeling and Statistical Machine Translation
Zhang, Min;Jiang, Hongfei;Li, Haizhou;Aw, Aiti;Li, Sheng;"></td>
	<td class="line x" title="1:198	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 10971104 Manchester, August 2008 Grammar Comparison Study for Translational Equivalence Modeling and Statistical Machine Translation Min Zhang 1 ,  Hongfei Jiang 2 ,  Haizhou Li 1 ,  Aiti Aw 1   and  Sheng Li 2  1 Institute for Infocomm Research, Singapore 2 Harbin Institute of Technology, China {mzhang, hli, aaiti}@i2r.a-star.edu.sg {hfjiang, lisheng}@mtlab.hit.edu.cn  Abstract This paper presents a general platform, namely synchronous tree sequence substitution grammar (STSSG), for the grammar comparison study in Translational Equivalence Modeling (TEM) and Statistical Machine Translation (SMT)." ></td>
	<td class="line x" title="2:198	Under the STSSG platform, we compare the expressive abilities of various grammars through synchronous parsing and a real translation platform on a variety of Chinese-English bilingual corpora." ></td>
	<td class="line x" title="3:198	Experimental results show that the STSSG is able to better explain the data in parallel corpora than other grammars." ></td>
	<td class="line x" title="4:198	Our study further finds that the complexity of structure divergence is much higher than suggested in literature, which imposes a big challenge to syntactic transformationbased SMT." ></td>
	<td class="line x" title="5:198	1 Introduction Translational equivalence is a mathematical relation that holds between linguistic expressions with the same meaning (Wellington et al., 2006)." ></td>
	<td class="line x" title="6:198	The common explicit representations of this relation are word alignments, phrase alignments and structure alignments between bilingual sentences." ></td>
	<td class="line x" title="7:198	Translational Equivalence Modeling (TEM) is a process to describe and build these alignments using mathematical models." ></td>
	<td class="line x" title="8:198	Thus, the study of TEM is highly relevant to Statistical Machine Translation (SMT)." ></td>
	<td class="line x" title="9:198	Grammar is the most important infrastructure for TEM and SMT since translation models expressive and generative abilities are mainly de  2008." ></td>
	<td class="line x" title="10:198	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/)." ></td>
	<td class="line x" title="11:198	Some rights reserved." ></td>
	<td class="line x" title="12:198	termined by the grammar." ></td>
	<td class="line x" title="13:198	Many grammars, such as finite-state grammars (FSG), bracket/inversion transduction grammars (BTG/ITG) (Wu, 1997), context-free grammar (CFG), tree substitution grammar (TSG) (Comon et al., 2007) and their synchronous versions, have been explored in SMT." ></td>
	<td class="line x" title="14:198	Based on these grammars, a great number of SMT models have been recently proposed, including string-to-string model (Synchronous FSG) (Brown et al., 1993; Koehn et al., 2003), tree-to-string model (TSG-string) (Huang et al., 2006; Liu et al., 2006; Liu et al., 2007), string-totree model (string-CFG/TSG) (Yamada and Knight, 2001; Galley et al., 2006; Marcu et al., 2006), tree-to-tree model (Synchronous CFG/TSG, Data-Oriented Translation) (Chiang, 2005; Cowan et al., 2006; Eisner, 2003; Ding and Palmer, 2005; Zhang et al., 2007; Bod, 2007; Quirk wt al., 2005; Poutsma, 2000; Hearne and Way, 2003) and so on." ></td>
	<td class="line x" title="15:198	Although many achievements have been obtained by these advances, it is still unclear which of these important pursuits is able to best explain human translation data, as each has its advantages and disadvantages." ></td>
	<td class="line x" title="16:198	Therefore, it has great meaning in both theory and practice to do comparison studies among these grammars and SMT models to see which of them are capable of better describing parallel translation data." ></td>
	<td class="line x" title="17:198	This is a fundamental issue worth exploring in multilingual information processing." ></td>
	<td class="line x" title="18:198	However, little effort in previous work has been put in this point." ></td>
	<td class="line x" title="19:198	To address this issue, in this paper we define a general platform, namely synchronous tree sequence substitution grammar (STSSG), for the comparison studies." ></td>
	<td class="line x" title="20:198	The STSSG can be seen as a generalization of Synchronous TSG (STSG) by replacing elementary tree (a single subtree used in STSG) with contiguous tree sequence as the basic translation unit." ></td>
	<td class="line x" title="21:198	As a result, most of previous grammars used in SMT can be interpreted as the reduced versions of the STSSG." ></td>
	<td class="line x" title="22:198	Under the STSSG platform, we compare the expressive 1097 abilities of various grammars and translation models through linguistically-based synchronous parsing and a real translation platform." ></td>
	<td class="line x" title="23:198	By synchronous parsing, we aim to study which grammar can well explain translation data (i.e. translational equivalence alignment) while by the real translation platform, we expect to investigate which model can achieve better translation performance." ></td>
	<td class="line x" title="24:198	In addition, we also measure the impact of various factors in this study, including the genera of corpora (newspaper domain via spoken domain), the accuracy of word alignments and syntax parsing (automatically vs. manually)." ></td>
	<td class="line x" title="25:198	We report our experimental settings, experimental results and our findings in detail in the rest of the paper, which is organized as follows: Section 2 reviews previous work." ></td>
	<td class="line x" title="26:198	Section 3 elaborates the general framework while Section 4 reports the experimental results." ></td>
	<td class="line x" title="27:198	Finally, we conclude our work in Section 5." ></td>
	<td class="line x" title="28:198	2 Previous Work There are only a few of previous work related to the study of translation grammar comparison." ></td>
	<td class="line x" title="29:198	Fox (2002) is the first to look at how well proposed translation models fit actual translation data empirically." ></td>
	<td class="line x" title="30:198	She examined the issue of phrasal cohesion between English and French and discovered that while there is less cohesion than one might desire, there is still a large amount of regularity in the constructions where breakdowns occur." ></td>
	<td class="line x" title="31:198	This suggests that reordering words by phrasal movement is a reasonable strategy (Fox, 2002)." ></td>
	<td class="line x" title="32:198	She has also examined the differences in cohesion between Treebank-style parse trees, trees with flattened verb phrases, and dependency structures." ></td>
	<td class="line x" title="33:198	Their experimental results indicate that the highest degree of cohesion is present in dependency structures." ></td>
	<td class="line oc" title="34:198	Motivated by the same problem raised by Fox (2002), Galley et al.(2004) study what rule can better explain human translation data." ></td>
	<td class="line x" title="36:198	They first propose a theory that gives formal semantics to word-level alignments defined over parallel corpora, and then use the theory to introduce a linear algorithm that is used to derive from wordaligned, parallel corpora the minimal set of syntactically motivated transformation rules to explain human translation data." ></td>
	<td class="line x" title="37:198	Their basic idea is to create transformation rules that condition on larger fragments of tree structure." ></td>
	<td class="line x" title="38:198	Their experimental results suggest that their proposed rules provide a good, realistic indicator of the complexities inherent in translation than SCFG." ></td>
	<td class="line x" title="39:198	Wellington et al.(2006) describes their study of the patterns of translational equivalence exhibited by a variety of bilingual/monolingual bitexts." ></td>
	<td class="line x" title="41:198	They empirically measure the lower bounds on alignment failure rates with and without gaps under the constraints of word alignment alone or with one or both side parse trees." ></td>
	<td class="line x" title="42:198	Their study finds surprisingly many examples of translational equivalence that could not be analyzed using binary-branching structures without discontinuities." ></td>
	<td class="line x" title="43:198	Thus, they claim that the complexity of these patterns in every bitext is higher than suggested in the literature." ></td>
	<td class="line x" title="44:198	In addition, they suggest that the low coverage rates without gaps under the constraints of independently generated monolingual parse trees might be the main reason why syntactic constraints have not yet increased the accuracy of SMT systems." ></td>
	<td class="line x" title="45:198	However, they find that simply allowing a single gap in bilingual phrases or other types of constituent can improve coverage dramatically." ></td>
	<td class="line x" title="46:198	DeNeefe et al.(2007) compares the strengths and weaknesses of a syntax-based MT model with a phrase-based MT model from the viewpoints of translational equivalence extraction methods and coverage." ></td>
	<td class="line x" title="48:198	They find that there are surprising differences in phrasal coverage  neither is merely a superset of the other." ></td>
	<td class="line x" title="49:198	They also investigate the reason why some phrase pairs are not learned by the syntax-based model." ></td>
	<td class="line x" title="50:198	They further propose several solutions and evaluate on the syntax-based extraction techniques in light of phrase pairs captured and translation accuracy." ></td>
	<td class="line x" title="51:198	Finally, significant performance improvement is reported using their solutions." ></td>
	<td class="line x" title="52:198	Different from previous work discussed above, this paper mainly focuses on the expressive ability comparison studies among different grammars and models through synchronous parsing and a real SMT platform." ></td>
	<td class="line nc" title="53:198	Fox (2002), Galley et al (2004) and Wellington et al.(2006) examine TEM only." ></td>
	<td class="line x" title="55:198	DeNeefe et al.(2007) only compares the strengths and weaknesses of a syntax-based MT model with a phrase-based MT model." ></td>
	<td class="line x" title="57:198	3 The General Platform: the STSSG In this section, we first define the STSSG platform in Subsection 3.1, and then explain why it is a general framework that can cover most of previous syntax-based translation grammars and models in Subsection 3.2." ></td>
	<td class="line x" title="58:198	In Subsection 3.3 and 3.4, we discuss the STSSG-based SMT and synchronous parsing, which are used to compare different grammars and translation models." ></td>
	<td class="line x" title="59:198	1098 1 () I Te 1 () J Tf A   Figure 1." ></td>
	<td class="line x" title="60:198	A word-aligned parse tree pairs of a Chinese sentence and its English translation    Figure 2." ></td>
	<td class="line x" title="61:198	Two examples of translation rules 3.1 Definition of the STSSG The STSSG is an extension of the STSG by using tree sequences (rather than elementary trees) as the basic translation unit." ></td>
	<td class="line x" title="62:198	A STSSG is a septet ,,, ,,,tttsssGNNSP=< > , where: z s  and t  are source and target terminal alphabets (POSs or lexical words), respectively, and z sN  and tN are source and target nonterminal alphabets (linguistic phrase tag, i.e. NP/VP), respectively, and z s sSN  and ttSN  are the source and target start symbols (roots of source and target parse trees), and z P is a production rule set." ></td>
	<td class="line x" title="63:198	A grammar rule i r  in the STSSG is an aligned tree sequence pair, < s , t , A   >, where s and t  are tree sequences of source side and target sides, respectively, and A  is the alignments between leaf nodes of two tree sequences." ></td>
	<td class="line x" title="64:198	Here, the key concept of tree sequence refers to an ordered subtree sequence covering a consecutive tree fragment in a complete parse tree." ></td>
	<td class="line x" title="65:198	The leaf nodes of a subtree in a tree sequence can be either non-terminal symbols or terminal symbols." ></td>
	<td class="line x" title="66:198	Fig." ></td>
	<td class="line x" title="67:198	2 shows two STSSG rules extracted from the aligned tree pair shown in Fig." ></td>
	<td class="line x" title="68:198	1, where 1 r is also a STSG rule." ></td>
	<td class="line x" title="69:198	In the STSSG, a translational equivalence is modeled as a tree sequence pair while MT is viewed as a tree sequence substitution process." ></td>
	<td class="line x" title="70:198	From the definition of tree sequence, we can see that a subtree in a tree sequence is a so-called elementary tree used in TSG." ></td>
	<td class="line x" title="71:198	This suggests that SCFG and STSG are only a subset of STSSG and SCFG is a subset of STSG." ></td>
	<td class="line x" title="72:198	The next subsection discusses how to configure the STSSG to implement the other two simplified grammars." ></td>
	<td class="line x" title="73:198	This is the reason why we call the STSSG a general framework for synchronous grammar-based translation modeling." ></td>
	<td class="line x" title="74:198	It is worth noting that, from rule rewriting viewpoint, STSSG can be thought of as a restricted version of synchronous multi-component TAGs (Schuler et al., 2000) although TAG is more powerful than TSG due to the additional operation adjunctions." ></td>
	<td class="line x" title="75:198	The synchronous multicomponent TAG can also rewrite several nonterminals in one step of derivation." ></td>
	<td class="line x" title="76:198	The difference between them is that the rewriting sites (i.e. the substitution nodes) must be contiguous in STSSG." ></td>
	<td class="line x" title="77:198	In addition, STSSG is also related to tree automata (Comon et al., 2007)." ></td>
	<td class="line x" title="78:198	However, the discussion on the theoretical relation and comparison between them is out of the scope of the paper." ></td>
	<td class="line x" title="79:198	In this paper, we focus on the comparison study of SMT grammars using the STSSG platform." ></td>
	<td class="line x" title="80:198	3.2 Rule Extraction and Grammar Configuration All the STSSG mapping rules are extracted from bi-parsed trees." ></td>
	<td class="line x" title="81:198	Our rule extraction algorithm is an extension of that presented at (Chiang, 2005; Liu et al., 2006; Zhang et al., 2007)." ></td>
	<td class="line x" title="82:198	We modify their tree-to-tree/string rule extraction algorithms to extract tree-sequence-to-tree-sequence rules." ></td>
	<td class="line x" title="83:198	Our rules 2  are extracted in two steps:  2  We classify the rules into two categories: initial rules, whose leaf nodes must be terminals, and ab1099 1) Extracting initial rules from bi-parsed trees." ></td>
	<td class="line x" title="84:198	This is rather straightforward." ></td>
	<td class="line x" title="85:198	We first generate all fully lexicalized source and target tree sequences (whose leaf nodes must be lexical words) using a DP algorithm and then iterate over all generated source and target sequence pairs." ></td>
	<td class="line x" title="86:198	If their word alignments are all within the scope of the current tree sequence pair, then the current tree sequence pair is an initial rule." ></td>
	<td class="line x" title="87:198	2) Extracting abstract rules from the extracted initial rules." ></td>
	<td class="line x" title="88:198	The idea behind is that we generate an abstract rule from a big initial rule by removing one or more small initial rules from the big one, where the small ones must be a sub-graph of the big one." ></td>
	<td class="line x" title="89:198	Please refer to (Chiang, 2005; Liu et al., 2006; Zhang et al., 2007) for the implementation details." ></td>
	<td class="line x" title="90:198	As indicated before (Chiang, 2005; Zhang et al., 2007), the above scheme generates a very large number of rules, which not only makes the system too complicated but also introduces too many undesirable ambiguities." ></td>
	<td class="line x" title="91:198	To control the overall model complexity, we introduce the following parameters: 1) The maximal numbers of trees in the source and target tree sequences: s  and t  . 2) The maximal tree heights in the source and target tree sequences: s  and t  . 3) The maximal numbers of non-terminal leaf nodes in the source and target tree sequences: s  and t  . Now let us see how to implement other models in relation to STSSG based the STSSG through configuring the above parameters." ></td>
	<td class="line x" title="92:198	1) STSG-based tree-to-tree model (Zhang et al., 2007; Bod, 2007) when s  = t  =1." ></td>
	<td class="line x" title="93:198	2) SCFG-based tree-to-tree model when s  = t  =1 and s  = t  =2." ></td>
	<td class="line x" title="94:198	3) Phrase-based translation model only (no reordering model) when s  = t  =0 and s  = t  =1." ></td>
	<td class="line x" title="95:198	4) TSG-CFG-based tree-to-string model (Liu et al., 2006) when s  = t  =1, t  =2 and ignore phrase tags in target side." ></td>
	<td class="line x" title="96:198	5) CFG-TSG-based string-to-tree model (Galley et al., 2006) when s  = t  =1and s  =2." ></td>
	<td class="line x" title="97:198	6) TSSG-CFG-based tree-sequence-to-string model (Liu et al., 2007) when t  =2 and ignore phrase tags in target side." ></td>
	<td class="line x" title="98:198	stract rule that having at least one non-terminal leaf node." ></td>
	<td class="line x" title="99:198	From the above definitions, we can see that all of previous related models/grammars can be can be interpreted as the reduced versions of the STSSG." ></td>
	<td class="line x" title="100:198	This is the reason why we use the STSSG as a general platform for our model and grammar comparison studies." ></td>
	<td class="line x" title="101:198	3.3 Model Training and Decoder for SMT We use the tree sequence mapping rules to model the translation process." ></td>
	<td class="line x" title="102:198	Given the source parse tree 1 () J Tf , there are multiple derivations 3  that could lead to the same target tree 1 () I Te , the mapping probability 11 (( )| ( ) IJ rPTe Tf is obtained by summing over the probabilities of all derivations." ></td>
	<td class="line x" title="103:198	The probability of each derivation is given by the product of the probabilities of all the rules () i p r  used in the derivation (here we assume that a rule is applied independently in a derivation)." ></td>
	<td class="line x" title="104:198	11 1 1 (| ) (()|( )                   = ( ) i IJ I J i r rrP ef PTeTf pr   =            (1) The model is implemented under log-linear framework." ></td>
	<td class="line x" title="105:198	We use seven basic features that are analogous to the commonly used features in phrase-based systems (Koehn, 2004): 1) bidirectional rule mapping probabilities; 2) bidirectional lexical translation probabilities; 3) the target language model; 4) the number of rules used and 5) the number of target words." ></td>
	<td class="line x" title="106:198	Besides, we define two new features: 1) the number of lexical words in a rule to control the models preference for lexicalized rules over un-lexicalized rules and 2) the average tree height in a rule to balance the usage of hierarchical rules and more flat rules." ></td>
	<td class="line x" title="107:198	The overall training process is similar to the process in the phrase-based system (koehn et al., 2007): word alignment, rule extraction, feature extraction and probability calculation and feature weight tuning." ></td>
	<td class="line x" title="108:198	Given 1 () J Tf , the decoder is to find the best derivation   that generates < 1 () J Tf , 1 () I Te >." ></td>
	<td class="line x" title="109:198	1 1 11 ,  arg max ( ( ) | ( ))   arg max ( ) I I i IJ e i e r rePTeTf pr   =                (2) By default, same as other SMT decoder, here we use Viterbi derivation in Eq (2) instead of the  3  A derivation is a sequence of tree sequence rules that maps a source parse tree to its target one." ></td>
	<td class="line x" title="110:198	1100 summing probabilities in Eq (3)." ></td>
	<td class="line x" title="111:198	This is to make the decoder speed not too slow." ></td>
	<td class="line x" title="112:198	The decoder is a standard span-based chart parser together with a function for mapping the source derivations to the target ones." ></td>
	<td class="line x" title="113:198	To speed up the decoder, we utilize several thresholds to limit the search beams for each span, such as the number of rules used and the number of hypotheses generated." ></td>
	<td class="line x" title="114:198	3.4 Synchronous Parsing A synchronous parser is an algorithm that can infer the syntactic structure of each component text in a multitext and simultaneously infer the correspondence relation between these structures." ></td>
	<td class="line x" title="115:198	When a parsers input can have fewer dimensions than the parsers grammar, we call it a translator." ></td>
	<td class="line x" title="116:198	When a parsers grammar can have fewer dimensions than the parsers input, we call it a synchronizer (Melamed, 2004)." ></td>
	<td class="line x" title="117:198	Therefore, synchronous parsing and MT are closed to each other." ></td>
	<td class="line x" title="118:198	In this paper, we use synchronous parsing to compare the ability of different grammars in translational equivalence modeling." ></td>
	<td class="line x" title="119:198	Given a bilingual sentence pair 1 J f and 1 I e , the synchronous parser is to find a derivation   that generates < 1 () J Tf , 1 () I Te >." ></td>
	<td class="line x" title="120:198	Our synchronous parser is similar to the synchronous CKY parser presented at (Melamed, 2004)." ></td>
	<td class="line x" title="121:198	The difference is that we implement it based on our STSSG decoder." ></td>
	<td class="line x" title="122:198	Therefore, in nature the parser is a standard synchronous chart parser but constrained by the rules of the STSSG grammar." ></td>
	<td class="line x" title="123:198	In our implementation, we simply use our decoder to simulate the bilingual parser: 1) for each sentence pair, we extract one model; 2) we use the model and the decoder to translate the source sentence of the given sentence pair; 3) if the target sentence is successfully generated by the decoder, then we say the symphonious parsing is successful." ></td>
	<td class="line x" title="124:198	Please note that the synchronous parsing is considered as successful once the last words in the source and target sentences are covered by the decoder even if there is no a complete target parse tree generated (it may be a tree sequence)." ></td>
	<td class="line x" title="125:198	This is because our study only concerns whether all translational equivalences are linked together by the synchronous parser correctly." ></td>
	<td class="line x" title="126:198	4 Experiments 4.1 Experimental Settings Synchronous parsing settings: Our experiments of synchronous parsing are carried on three Chinese-to-English bilingual corpora: the FBIS corpus, the IWSLT 2007 training set and the HIT Corpus." ></td>
	<td class="line x" title="127:198	The FBIS data is a collection of translated newswire documents published by major news agencies from three representative locations: Beijing, Taipei and Hongkong." ></td>
	<td class="line x" title="128:198	The IWSLT data is a multilingual speech corpus on travel domain while the HIT corpus consists of example sentences of a Chinese-English dictionary." ></td>
	<td class="line x" title="129:198	The first two corpora are sentence-aligned while the HIT corpus is a manually bi-parsed corpus with manually annotated word alignments." ></td>
	<td class="line x" title="130:198	We use the three corpora to study whether the models expressive abilities are domain dependent and how the performance of word alignment and parsing affect the ability of translation models." ></td>
	<td class="line x" title="131:198	We selected 2000 sentence pairs from each individual corpus for the comparison study of translational equivalence modeling." ></td>
	<td class="line x" title="132:198	Table 1 gives descriptive statistics of the tree data set." ></td>
	<td class="line x" title="133:198	Chinese English FBIS 48,331 59,788 IWSLT  17,667 18,427 HIT 18,215 20,266  Table 1." ></td>
	<td class="line x" title="134:198	# of words of experimental data for synchronous parsing (there are 2k sentence pairs in each individual corpus)  In the synchronous parsing experiments, we compared three synchronous grammars: SCFG, STSG and STSSG using the STSSG platform." ></td>
	<td class="line x" title="135:198	We use the same settings except the following parameters (please refer to Subsection 3.2 for their definitions): s  = t  =1, s  = t  =2 for SCFG ; s  = t  =1 and s  = t  =6 for STSG; s  = t  = 4 and s  = t  =6 for STSSG." ></td>
	<td class="line x" title="136:198	We iterate over each sentence pair in the three corpora with the following process: 1) to used Stanford parser (Klein and Manning, 2003) to parse bilingual sentences separately, this means that our study is based on the Penn Treebank style grammar." ></td>
	<td class="line x" title="137:198	2) to extract SCFG, STSG and STSSG rules form each sentence pair, respectively; 3) to do synchronous parsing using the exacted rules." ></td>
	<td class="line x" title="138:198	Finally, we can calculate the successful rate of the synchronous parsing on each corpus." ></td>
	<td class="line x" title="139:198	SMT evaluation settings: For the SMT experiments, we trained the translation model on the FBIS corpus (7.2M (Chinese)+9.2M(English) words) and trained a 4-gram language model on 1101 the Xinhua portion of the English Gigaword corpus (181M words) using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998)." ></td>
	<td class="line x" title="140:198	We used these sentences with less than 50 characters from the NIST MT-2002 test set as our development set and the NIST MT-2005 test set as our test set." ></td>
	<td class="line x" title="141:198	We used the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test sets." ></td>
	<td class="line x" title="142:198	The evaluation metric is casesensitive BLEU-4 (Papineni et al., 2002)." ></td>
	<td class="line x" title="143:198	We used GIZA++ and the heuristics grow-diagfinal to generate m-to-n word alignments." ></td>
	<td class="line x" title="144:198	For the MER training, we modified Koehns MER trainer (Koehn, 2004) for our STSSG-based system." ></td>
	<td class="line x" title="145:198	For significance test, we used Zhang et als implementation (Zhang et al, 2004)." ></td>
	<td class="line x" title="146:198	We compared four SMT systems: Moses (Koehn et al., 2007), SCFG-based, STSG-based and STSSGbased tree-to-tree translation models." ></td>
	<td class="line x" title="147:198	For Moses, we used its default settings." ></td>
	<td class="line x" title="148:198	For the others, we implemented them on the STSSG platform by adopting the same settings as used in the synchronous parsing." ></td>
	<td class="line x" title="149:198	We optimized the decoding parameters on the development sets empirically." ></td>
	<td class="line x" title="150:198	4.2 Experimental Results   SCFG STSG STSSG FBIS 7 (0.35%) 143 (7.15%) 388 (19.4%) IWSLT 171 (8.6%) 1179 (58.9%) 1708 (85.4%) HIT 65 (3.23%) 1133 (56.6%) 1532 (76.6%)  Table 2." ></td>
	<td class="line x" title="151:198	Successful rates (numbers inside bracket) of synchronous parsing over 2,000 sentence pairs, where the integers outside bracket are the numbers of successfullyparsed sentence pairs  Table 2 reports the experimental results of synchronous parsing." ></td>
	<td class="line x" title="152:198	It shows that: 1) As an extension of STSG/SCFG, STSSG outperforms STSG and SCFG consistently in the three data sets." ></td>
	<td class="line x" title="153:198	The significant difference suggests that the STSSG is much more effective in modeling translational equivalences and structure divergences." ></td>
	<td class="line x" title="154:198	The reason is simply because the STSSG uses tree sequences as the basic translation unit so that it can model non-syntactic phrase equivalence with structure information and handle structure reordering in a large span." ></td>
	<td class="line x" title="155:198	2) STSG shows much better performance than SCFG." ></td>
	<td class="line x" title="156:198	It is mainly due to that STSG allow multiple level tree nodes operation and reordering in a larger span than SCFG." ></td>
	<td class="line oc" title="157:198	It reconfirms that only allowing sibling nodes reordering as done in SCFG may be inadequate for translational equivalence modeling (Galley et al., 2004) 4 . 3) All the three models on the FBIS corpus show much lower performance than that on the other two corpora." ></td>
	<td class="line x" title="158:198	The main reason, as shown in Table 1, is that the sentences in the FBIS corpus are much longer than that in the other corpus, so their syntactic structures are significantly more complicated than the other two." ></td>
	<td class="line x" title="159:198	In addition, although tree sequences are utilized, STSSG show much lower performance in the FBIS corpus." ></td>
	<td class="line nc" title="160:198	This implies that the complexity of structure divergence between two languages is higher than suggested in literature (Fox, 2002; Galley et al., 2004)." ></td>
	<td class="line x" title="161:198	Therefore, structure divergence is still a big challenge to translational equivalence modeling when using syntactic structure mapping." ></td>
	<td class="line x" title="162:198	4) The HIT corpus does not show better performance than the IWSLT corpus although the HIT corpus is manually annotated with parse trees and word alignments." ></td>
	<td class="line x" title="163:198	In order to study whether high performance word alignment and parsing results can help synchronous parsing, we do several cross validations and report the experimental results in Table 3." ></td>
	<td class="line x" title="164:198	Gold Word Alignment Automatic Word Alignment  Gold Parse 3.2/56.6/76.6 2.9/57.7/80.9  Automatic Parse 3.2/55.6/76.0 2.9/54.2/78.8  Table 3." ></td>
	<td class="line x" title="165:198	Successful rates (SCFG/STSG/ STSSG)(%) with regards to different word alignments and parse trees  on the HIT corpus  Table 3 compares the performance of synchronous parsing on the HIT corpus when using gold and automatic parser and word alignment." ></td>
	<td class="line x" title="166:198	It is surprised that gold word alignments and parse trees do not help and even decrease the performance slightly." ></td>
	<td class="line x" title="167:198	Our analysis further finds that  4  This claim is mainly hold for linguistically-informed SCFG since formal SCFG and BTG already showed much better performance in the formally syntax-based translation framework (Chiang, 2005)." ></td>
	<td class="line x" title="168:198	This is because the formal syntax is learned from phrase translational equivalences directly without relying on any linguistic theory (Chiang, 2005)." ></td>
	<td class="line x" title="169:198	Thus, it may not suffer from the issues of non-isomorphic structure alignment and non-syntactic phrase usage heavily (Wellington et al., 2006)." ></td>
	<td class="line x" title="170:198	1102 more than 90% sentence pairs out of all the sentence pairs that can be successfully bi-parsed are in common in the four experiments." ></td>
	<td class="line x" title="171:198	This suggests that the STSSG/STSG (SCFG achieves too much lower performance) and our rule extraction algorithm are robust in dealing with the errors introduced by the word alignment and parsing programs." ></td>
	<td class="line x" title="172:198	If a parser, for example, makes a systematic error, we expect to learn a rule that can nevertheless be systematically used to model correct translational equivalence." ></td>
	<td class="line x" title="173:198	Our error analysis on the three corpora shows that most of the failures of synchronous parsing are due to the structure divergence (i.e. the nature of nonisomorphic structure mapping) and the long distance dependence in the syntactic structures." ></td>
	<td class="line x" title="174:198	SCFG Moses STSG STSSG BLEU(%) 22.72 23.86 24.71 26.07       Table 3." ></td>
	<td class="line x" title="175:198	Performance comparison of different grammars on FBIS corpus  Table 3 compares different grammars in terms of translation performance." ></td>
	<td class="line x" title="176:198	It shows that: 1) The same as synchronous parsing, the STSSG-based model statistically significantly outperforms (p < 0.01) previous phrase-based and linguistically syntax-based methods." ></td>
	<td class="line x" title="177:198	This empirically verifies the effect of the tree-sequence-based grammar for statistical machine translation." ></td>
	<td class="line x" title="178:198	2) Both STSSG and STSG outperform Moses significantly and STSSG clearly outperforms STSG, which suggest that: z The linguistically motivated structure features are still useful for SMT, which can be captured by the two syntax-based grammars through tree node operations." ></td>
	<td class="line x" title="179:198	z STSSG is much more effective in utilizing linguistic structures than STSG since it uses tree sequence as the basic translation unit." ></td>
	<td class="line x" title="180:198	This enables STSSG not only to handle structure reorderings by tree node operations in a larger span, but also to capture non-syntactic phrases with syntactic information, and hence giving the grammar more expressive power." ></td>
	<td class="line x" title="181:198	3) The linguistic-based SCFG shows much lower performance." ></td>
	<td class="line x" title="182:198	This is largely because SCFG only allows sibling nodes reordering and fails to utilize both non-syntactic phrases and those syntactic phrases that cannot be covered by a single CFG rule." ></td>
	<td class="line x" title="183:198	It thereby suggests that SCFG is less effective in modelling parse tree structure transfer." ></td>
	<td class="line x" title="184:198	The above two experimental results show that STSSG achieves significant improvements over the other two grammars in terms of synchronous parsings successful rate and translation Bleu score." ></td>
	<td class="line x" title="185:198	5 Conclusions Grammar is the fundamental infrastructure in translational equivalence modeling and statistical machine translation since grammar formalizes what kind of rule to be learned from a parallel text." ></td>
	<td class="line x" title="186:198	In this paper, we first present a general platform STSSG and demonstrate that a number of synchronous grammars and SMT models can be easily implemented based on the platform." ></td>
	<td class="line x" title="187:198	We then compare the expressive abilities of different grammars on the platform using synchronous parsing and statistical machine translation." ></td>
	<td class="line x" title="188:198	Our experimental results show that STSSG can better explain the data in parallel corpora than the other two synchronous grammars." ></td>
	<td class="line x" title="189:198	We further finds that, although syntactic structure features are helpful in modeling translational equivalence, the complexity of structure divergence is much higher than suggested in literature, which imposes a big challenge to syntactic transformationbased SMT." ></td>
	<td class="line x" title="190:198	This may explain why traditional syntactic constraints in SMT do not yield much performance improvement over robust phrasesubstitution models." ></td>
	<td class="line x" title="191:198	The fundamental assumption underlying much recent work on syntax-based modeling, which is considered to be one of next technology breakthroughs in SMT, is that translational equivalence can be well modeled by structural transformation." ></td>
	<td class="line oc" title="192:198	However, as discussed in prior arts (Galley et al., 2004) and this paper, linguistically-informed SCFG is an inadequate model for parallel corpora due to its nature that only allowing child-node reorderings." ></td>
	<td class="line x" title="193:198	Although STSG shows much better performance than SCFG, its two major limitations are that it only allows structure distortion operated on a single sub-tree and cannot model non-syntactic phrases." ></td>
	<td class="line x" title="194:198	STSSG extends STSG by using tree sequence as the basic translation unit." ></td>
	<td class="line x" title="195:198	This gives the grammar much more expressive power." ></td>
	<td class="line x" title="196:198	There are many open issues in the syntactic transformation-based SMT due to the divergence nature between bilingual structure mappings." ></td>
	<td class="line x" title="197:198	We find that structural divergences are more serious than suggested in the literature (Fox, 2002; Gallery et al., 2004) or what we expected when sentences are longer." ></td>
	<td class="line x" title="198:198	We will continue to investigate 1103 whether and how parallel corpora can be well modeled by syntactic structure mappings." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D08-1021
Syntactic Constraints on Paraphrases Extracted from Parallel Corpora
Callison-Burch, Chris;"></td>
	<td class="line x" title="1:209	Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 196205, Honolulu, October 2008." ></td>
	<td class="line x" title="2:209	c2008 Association for Computational Linguistics Syntactic Constraints on Paraphrases Extracted from Parallel Corpora Chris Callison-Burch Center for Language and Speech Processing Johns Hopkins University Baltimore, Maryland ccb cs jhu edu Abstract We improve the quality of paraphrases extracted from parallel corpora by requiring that phrases and their paraphrases be the same syntactic type." ></td>
	<td class="line x" title="3:209	This is achieved by parsing the English side of a parallel corpus and altering the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs." ></td>
	<td class="line x" title="4:209	In order to retain broad coverage of non-constituent phrases, complex syntactic labels are introduced." ></td>
	<td class="line x" title="5:209	A manual evaluation indicates a 19% absolute improvement in paraphrase quality over the baseline method." ></td>
	<td class="line x" title="6:209	1 Introduction Paraphrases are alternative ways of expressing the same information." ></td>
	<td class="line x" title="7:209	Being able to identify or generate paraphrases automatically is useful in a wide range of natural language applications." ></td>
	<td class="line x" title="8:209	Recent work has shown how paraphrases can improve question answering through query expansion (Riezler et al., 2007), automatic evaluation of translation and summarization by modeling alternative lexicalization (Kauchak and Barzilay, 2006; Zhou et al., 2006; Owczarzak et al., 2006), and machine translation both by dealing with out of vocabulary words and phrases (Callison-Burch et al., 2006) and by expanding the set of reference translations for minimum error rate training (Madnani et al., 2007)." ></td>
	<td class="line x" title="9:209	While all applications require the preservation of meaning when a phrase is replaced by its paraphrase, some additionally require the resulting sentence to be grammatical." ></td>
	<td class="line x" title="10:209	In this paper we examine the effectiveness of placing syntactic constraints on a commonly used paraphrasing technique that extracts paraphrases from parallel corpora (Bannard and Callison-Burch, 2005)." ></td>
	<td class="line x" title="11:209	The paraphrasing technique employs various aspects of phrase-based statistical machine translation including phrase extraction heuristics to obtain bilingual phrase pairs from word alignments." ></td>
	<td class="line x" title="12:209	English phrases are considered to be potential paraphrases of each other if they share a common foreign language phrase among their translations." ></td>
	<td class="line x" title="13:209	Multiple paraphrases are frequently extracted for each phrase and can be ranked using a paraphrase probability based on phrase translation probabilities." ></td>
	<td class="line x" title="14:209	We find that the quality of the paraphrases that are generated in this fashion improves significantly when they are required to be the same syntactic type as the phrase that they are paraphrasing." ></td>
	<td class="line x" title="15:209	This constraint:  Eliminates a trivial but pervasive error that arises from the interaction of unaligned words with phrase extraction heuristics." ></td>
	<td class="line x" title="16:209	 Refines the results for phrases that can take on different syntactic labels." ></td>
	<td class="line x" title="17:209	 Applies both to phrases which are linguistically coherent and to arbitrary sequences of words." ></td>
	<td class="line x" title="18:209	 Results in much more grammatical output when phrases are replaced with their paraphrases." ></td>
	<td class="line x" title="19:209	A thorough manual evaluation of the refined paraphrasing technique finds a 19% absolute improve196 ment in the number of paraphrases that are judged to be correct." ></td>
	<td class="line x" title="20:209	This paper is structured as follows: Section 2 describes related work in syntactic constraints on phrase-based SMT and work utilizing syntax in paraphrase discovery." ></td>
	<td class="line x" title="21:209	Section 3 details the problems with extracting paraphrases from parallel corpora and our improvements to the technique." ></td>
	<td class="line x" title="22:209	Section 4 describes our experimental design and evaluation methodology." ></td>
	<td class="line x" title="23:209	Section 5 gives the results of our experiments, and Section 6 discusses their implications." ></td>
	<td class="line x" title="24:209	2 Related work A number of research efforts have focused on employing syntactic constraints in statistical machine translation." ></td>
	<td class="line x" title="25:209	Wu (1997) introduced the inversion transduction grammar formalism which treats translation as a process of parallel parsing of the source and target language via a synchronized grammar." ></td>
	<td class="line x" title="26:209	The synchronized grammar places constraints on which words can be aligned across bilingual sentence pairs." ></td>
	<td class="line x" title="27:209	To achieve computational efficiency, the original proposal used only a single non-terminal label rather than a linguistic grammar." ></td>
	<td class="line x" title="28:209	Subsequent work used more articulated parses to improve alignment quality by applying cohesion constraints (Fox, 2002; Lin and Cherry, 2002)." ></td>
	<td class="line x" title="29:209	If two English phrases are in disjoint subtrees in the parse, then the phrasal cohesion constraint prevents them from being aligned to overlapping sequences in the foreign sentence." ></td>
	<td class="line oc" title="30:209	Other recent work has incorporated constituent and dependency subtrees into the translation rules used by phrase-based systems (Galley et al., 2004; Quirk et al., 2005)." ></td>
	<td class="line x" title="31:209	Phrase-based rules have also been replaced with synchronous context free grammars (Chiang, 2005) and with tree fragments (Huang and Knight, 2006)." ></td>
	<td class="line x" title="32:209	A number of techniques for generating paraphrases have employed syntactic information, either in the process of extracting paraphrases from monolingual texts or in the extracted patterns themselves." ></td>
	<td class="line x" title="33:209	Lin and Pantel (2001) derived paraphrases based on the distributional similarity of paths in dependency trees." ></td>
	<td class="line x" title="34:209	Barzilay and McKeown (2001) incorporated part-of-speech information and other morphosyntactic clues into their co-training algorithm." ></td>
	<td class="line x" title="35:209	They extracted paraphrase patterns that incorporate this information." ></td>
	<td class="line x" title="36:209	Ibrahim et al.(2003) generated structural paraphrases capable of capturing longdistance dependencies." ></td>
	<td class="line x" title="38:209	Pang et al.(2003) employed a syntax-based algorithm to align equivalent English sentences by merging corresponding nodes in parse trees and compressing them down into a word lattice." ></td>
	<td class="line x" title="40:209	Perhaps the most closely related work is a recent extension to Bannard and Callison-Burchs paraphrasing method." ></td>
	<td class="line x" title="41:209	Zhao et al.(2008b) extended the method so that it is capable of generating richer paraphrase patterns that include part-of-speech slots, rather than simple lexical and phrasal paraphrases." ></td>
	<td class="line x" title="43:209	For example, they extracted patterns such as consider NN  take NN into consideration." ></td>
	<td class="line x" title="44:209	To accomplish this, Zhao el al. used dependency parses on the English side of the parallel corpus." ></td>
	<td class="line x" title="45:209	Their work differs from the work presented in this paper because their syntactic constraints applied to slots within paraphrase patters, and our constraints apply to the paraphrases themselves." ></td>
	<td class="line x" title="46:209	3 Paraphrasing with parallel corpora Bannard and Callison-Burch (2005) extract paraphrases from bilingual parallel corpora." ></td>
	<td class="line x" title="47:209	They give a probabilistic formation of paraphrasing which naturally falls out of the fact that they use techniques from phrase-based statistical machine translation: e2 = argmax e2:e2negationslash=e1 p(e2|e1) (1) where p(e2|e1) = summationdisplay f p(f|e1)p(e2|f,e1) (2)  summationdisplay f p(f|e1)p(e2|f) (3) Phrase translation probabilities p(f|e1) and p(e2|f) are commonly calculated using maximum likelihood estimation (Koehn et al., 2003): p(f|e) = count(e,f)summationtext f count(e,f) (4) where the counts are collected by enumerating all bilingual phrase pairs that are consistent with the 197 conseguido .opportunitiesequalcreatetofailedhasprojecteuropeanthe oportunidadesdeigualdadlahanoeuropeoproyectoel Figure 1: The interaction of the phrase extraction heuristic with unaligned English words means that the Spanish phrase la igualdad aligns with equal, create equal, and to create equal." ></td>
	<td class="line x" title="48:209	word alignments for sentence pairs in a bilingual parallel corpus." ></td>
	<td class="line x" title="49:209	Various phrase extraction heuristics are possible." ></td>
	<td class="line x" title="50:209	Och and Ney (2004) defined consistent bilingual phrase pairs as follows: BP(fJ1 ,eI1,A) ={(fj+mj ,ei+ni ) : (iprime,jprime)A : jjprime j +miiprime i+n (iprime,jprime)A : jjprime j +miiprime i+n} where fJ1 is a foreign sentence, eI1 is an English sentence and A is a set of word alignment points." ></td>
	<td class="line x" title="51:209	The heuristic allows unaligned words to be included at the boundaries of the source or target language phrases." ></td>
	<td class="line x" title="52:209	For example, when enumerating the consistent phrase pairs for the sentence pair given in Figure 1, la igualdad would align not only to equal, but also to create equal, and to create equal." ></td>
	<td class="line x" title="53:209	In SMT these alternative translations are ranked by the translation probabilities and other feature functions during decoding." ></td>
	<td class="line x" title="54:209	The interaction between the phrase extraction heuristic and unaligned words results in an undesirable effect for paraphrasing." ></td>
	<td class="line x" title="55:209	By Bannard and Callison-Burchs definition, equal, create equal, and to create equal would be considered paraphrases because they are aligned to the same foreign phrase." ></td>
	<td class="line x" title="56:209	Tables 1 and 2 show how suband super-phrases can creep into the paraphrases: equal can be paraphrased as equal rights and create equal can be paraphrased as equal." ></td>
	<td class="line x" title="57:209	Obviously when e2 is substituted for e1 the resulting sentence will generally be ungrammatical." ></td>
	<td class="line x" title="58:209	The first case could result in equal equal rights, and the second would drop the verb." ></td>
	<td class="line x" title="59:209	This problem is pervasive." ></td>
	<td class="line x" title="60:209	To test its extent we attempted to generate paraphrases for 900,000 phrases using Bannard and Callison-Burchs method trained on the Europarl corpora (as described in Section 4)." ></td>
	<td class="line x" title="61:209	It generated a total of 3.7 million paraphrases for equal equal .35 equally .02 same .07 the .02 equality .03 fair .01 equals .02 equal rights .01 Table 1: The baseline methods paraphrases of equal and their probabilities (excluding items with p < .01)." ></td>
	<td class="line x" title="62:209	create equal create equal .42 same .03 equal .06 created .02 to create a .05 conditions .02 create .04 playing .02 to create equality .03 creating .01 Table 2: The baselines paraphrases of createequal." ></td>
	<td class="line x" title="63:209	Most are clearly bad, and the most probable e2 negationslash= e1 is a substring of e1." ></td>
	<td class="line x" title="64:209	400,000 phrases in the list.1 We observed that 34% of the paraphrases (excluding the phrase itself) were superor sub-strings of the original phrase." ></td>
	<td class="line x" title="65:209	The most probable paraphrase was a superor sub-string of the phrase 73% of the time." ></td>
	<td class="line x" title="66:209	There are a number of strategies that might be adopted to alleviate this problem:  Bannard and Callison-Burch (2005) rank their paraphrases with a language model when the paraphrases are substituted into a sentence." ></td>
	<td class="line x" title="67:209	 Bannard and Callison-Burch (2005) sum over multiple parallel corpora C to reduce the problems associated with systematic errors in the 1The remaining 500,000 phrases could not be paraphrased either because e2 negationslash= e1 or because they were not consistently aligned to any foreign phrases." ></td>
	<td class="line x" title="68:209	198 word alignments in one language pair: e2 = argmax e2 summationdisplay cC summationdisplay f p(f|e1)p(e2|f) (5)  We could change the phrase extraction heuristics treatment of unaligned words, or we could attempt to ensure that we have fewer unaligned items in our word alignments." ></td>
	<td class="line x" title="69:209	 The paraphrase criterion could be changed from being e2 negationslash= e1 to specifying that e2 is not subor super-string of e1." ></td>
	<td class="line x" title="70:209	In this paper we adopt a different strategy." ></td>
	<td class="line x" title="71:209	The essence of our strategy is to constrain paraphrases to be the same syntactic type as the phrases that they are paraphrasing." ></td>
	<td class="line x" title="72:209	Syntactic constraints can apply in two places: during phrase extraction and when substituting paraphrases into sentences." ></td>
	<td class="line x" title="73:209	These are described in sections 3.1 and 3.2." ></td>
	<td class="line x" title="74:209	3.1 Syntactic constraints on phrase extraction When we apply syntactic constraints to the phrase extraction heuristic, we change how bilingual phrase pairs are enumerated and how the component probabilities of the paraphrase probability are calculated." ></td>
	<td class="line x" title="75:209	We use the syntactic type s of e1 in a refined version of the paraphrase probability: e2 = argmax e2:e2negationslash=e1s(e2)=s(e1) p(e2|e1,s(e1)) (6) where p(e2|e1,s(e1)) can be approximated as: summationdisplay cC summationtext f p(f|e1,s(e1))p(e2|f,s(e1)) |C| (7) We define a new phrase extraction algorithm that operates on an English parse tree P along with foreign sentence fJ1 , English sentence eI1, and word alignment A. We dub this SBP for syntactic bilingual phrases: SBP(fJ1 ,eI1,A,P) ={(fj+mj ,ei+ni ,s(ei+ni )) : (iprime,jprime)A : jjprime j +miiprime i+n (iprime,jprime)A : jjprime j +miiprime i+n subtree P with label s spanning words (i,i+n)} equal JJ equal .60 similar .02 same .14 equivalent .01 fair .02 ADJP equal .79 the same .01 necessary .02 equal in law .01 similar .02 equivalent .01 identical .02 Table 3: Syntactically constrained paraphrases for equal when it is labeled as an adjective or adjectival phrase." ></td>
	<td class="line x" title="76:209	The SBP phrase extraction algorithm produces tuples containing a foreign phrase, an English phrase and a syntactic label (f,e,s)." ></td>
	<td class="line x" title="77:209	After enumerating these for all phrase pairs in a parallel corpus, we can calculate p(f|e1,s(e1)) and p(e2|f,s(e1)) as: p(f|e1,s(e1)) = count(f,e1,s(e1))summationtext f count(f,e1,s(e1)) p(e2|f,s(e1)) = count(f,e2,s(e1))summationtext e2 count(f,e2,s(e1)) By redefining the probabilities in this way we partition the space of possible paraphrases by their syntactic categories." ></td>
	<td class="line x" title="78:209	In order to enumerate all phrase pairs with their syntactic labels we need to parse the English side of the parallel corpus (but not the foreign side)." ></td>
	<td class="line x" title="79:209	This limits the potential applicability of our refined paraphrasing method to languages which have parsers." ></td>
	<td class="line x" title="80:209	Table 3 gives an example of the refined paraphrases for equal when it occurs as an adjective or adjectival phrase." ></td>
	<td class="line x" title="81:209	Note that most of the paraphrases that were possible under the baseline model (Table 1) are now excluded." ></td>
	<td class="line x" title="82:209	We no longer get the noun equality, the verb equals, the adverb equally, the determier the or the NP equal rights." ></td>
	<td class="line x" title="83:209	The paraphrases seem to be higher quality, especially if one considers their fidelity when they replace the original phrase in the context of some sentence." ></td>
	<td class="line x" title="84:209	We tested the rate of paraphrases that were suband super-strings when we constrain paraphrases based on non-terminal nodes in parse trees." ></td>
	<td class="line x" title="85:209	The percent of the best paraphrases being substrings dropped from 73% to 24%, and the overall percent of paraphrases subsuming or being subsumed by the original phrase dropped from 34% to 12%." ></td>
	<td class="line x" title="86:209	However, the number of phrases for which we were able 199 SBARQ WHADVP WRB How SQ VBP do NP PRP we VP VB create NP JJ equal NNS rights . ? Figure 2: In addition to extracting phrases that are dominated by a node in the parse tree, we also generate labels for non-syntactic constituents." ></td>
	<td class="line x" title="87:209	Three labels are possible for create equal." ></td>
	<td class="line x" title="88:209	to generated paraphrases dropped from 400,000 to 90,000, since we limited ourselves to phrases that were valid syntactic constituents." ></td>
	<td class="line x" title="89:209	The number of unique paraphrases dropped from several million to 800,000." ></td>
	<td class="line x" title="90:209	The fact that we are able to produce paraphrases for a much smaller set of phrases is a downside to using syntactic constraints as we have initially proposed." ></td>
	<td class="line x" title="91:209	It means that we would not be able to generate paraphrases for phrases such as create equal." ></td>
	<td class="line x" title="92:209	Many NLP tasks, such as SMT, which could benefit from paraphrases require broad coverage and may need to paraphrases for phrases which are not syntactic constituents." ></td>
	<td class="line x" title="93:209	Complex syntactic labels To generate paraphrases for a wider set of phrases, we change our phrase extraction heuristic again so that it produces phrase pairs for arbitrary spans in the sentence, including spans that arent syntactic constituents." ></td>
	<td class="line x" title="94:209	We assign every span in a sentence a syntactic label using CCG-style notation (Steedman, 1999), which gives a syntactic role with elements missing on the left and/or right hand sides." ></td>
	<td class="line x" title="95:209	SBP(fJ1 ,eI1,A,P) ={(fj+mj ,ei+ni ,s) : (iprime,jprime)A : jjprime j +miiprime i+n (iprime,jprime)A : jjprime j +miiprime i+n sCCG-labels(ei+ni ,P)} The function CCG-labels describes the set of CCGlabels for the phrase spanning positions i to i+n in create equal VP/(NP/NNS) create equal .92 creating equal .08 VP/(NP/NNS) PP create equal .96 promote equal .03 establish fair .01 VP/(NP/NNS) PP PP create equal .80 creating equal .10 provide equal .06 create genuinely fair .04 VP/(NP/(NP/NN) PP) create equal .83 create a level playing .17 VP/(NP/(NP/NNS) PP) create equal .83 creating equal .17 Table 4: Paraphrases and syntactic labels for the nonconstituent phrase create equal." ></td>
	<td class="line x" title="96:209	a parse tree P. It generates three complex syntactic labels for the non-syntactic constituent phrase create equal in the parse tree given in Figure 2: 1." ></td>
	<td class="line x" title="97:209	VP/(NP/NNS)  This label corresponds to the innermost circle." ></td>
	<td class="line x" title="98:209	It indicates that create equal is a verb phrase missing a noun phrase to its right." ></td>
	<td class="line x" title="99:209	That noun phrase in turn missing a plural noun (NNS) to its right." ></td>
	<td class="line x" title="100:209	2." ></td>
	<td class="line x" title="101:209	SQ\VBP NP/(VP/(NP/NNS))  This label corresponds to the middle circle." ></td>
	<td class="line x" title="102:209	It indicates that create equal is an SQ missing a VBP and a NP to its left, and the complex VP to its right." ></td>
	<td class="line x" title="103:209	3." ></td>
	<td class="line x" title="104:209	SBARQ\WHADVP (SQ\VBP NP/(VP/(NP/NNS)))/." ></td>
	<td class="line x" title="105:209	 This label corresponds to the outermost circle." ></td>
	<td class="line x" title="106:209	It indicates that create equal is an SBARQ missing a WHADVP and the complex SQ to its left, and a punctuation mark to its right." ></td>
	<td class="line x" title="107:209	We can use these complex labels instead of atomic non-terminal symbols to handle non-constituent phrases." ></td>
	<td class="line x" title="108:209	For example, Table 4 shows the paraphrases and syntactic labels that are generated for the non-constituent phrase create equal." ></td>
	<td class="line x" title="109:209	The paraphrases are significantly better than the paraphrases generated for the phrase by the baseline method (refer back to Table 2)." ></td>
	<td class="line x" title="110:209	The labels shown in the figure are a fraction of those that can be derived for the phrase in the parallel corpus." ></td>
	<td class="line x" title="111:209	Each of these corresponds to a different 200 syntactic context, and each has its own set of associated paraphrases." ></td>
	<td class="line x" title="112:209	We increase the number of phrases that are paraphrasable from the 90,000 in our initial definition of SBP to 250,000 when we use complex CCG labels." ></td>
	<td class="line x" title="113:209	The number of unique paraphrases increases from 800,000 to 3.5 million, which is nearly as many paraphrases that were produced by the baseline method for the sample." ></td>
	<td class="line x" title="114:209	3.2 Syntactic constraints when substituting paraphrases into a test sentence In addition to applying syntactic constraints to our phrase extraction algorithm, we can also apply them when we substitute a paraphrase into a sentence." ></td>
	<td class="line x" title="115:209	To do so, we limit the paraphrases to be the same syntactic type as the phrase that it is replacing, based on the syntactic labels that are derived from the phrase tree for a test sentence." ></td>
	<td class="line x" title="116:209	Since each phrase normally has a set of different CCG labels (instead of a single non-termal symbol) we need a way of choosing which label to use when applying the constraint." ></td>
	<td class="line x" title="117:209	There are several different possibilities for choosing among labels." ></td>
	<td class="line x" title="118:209	We could simultaneously choose the best paraphrase and the best label for the phrase in the parse tree of the test sentence: e2 = argmax e2:e2negationslash=e1 argmax sCCG-labels(e1,P) p(e2|e1,s) (8) Alternately, we could average over all of the labels that are generated for the phrase in the parse tree: e2 = argmax e2:e2negationslash=e1 summationdisplay sCCG-labels(e1,P) p(e2|e1,s) (9) The potential drawback of using Equations 8 and 9 is that the CCG labels for a particular sentence significantly reduces the paraphrases that can be used." ></td>
	<td class="line x" title="119:209	For instance, VP/(NP/NNS) is the only label for the paraphrases in Table 4 that is compatible with the parse tree given in Figure 2." ></td>
	<td class="line x" title="120:209	Because the CCG labels for a given sentence are so specific, many times there are no matches." ></td>
	<td class="line x" title="121:209	Therefore we also investigated a looser constraint." ></td>
	<td class="line x" title="122:209	We choose the highest probability paraphrase with any label (i.e. the set of labels extracted from all parse trees in our parallel corpus): e2 = argmax e2:e2negationslash=e1 argmax sT in CCCG-labels(e1,T) p(e2|e1,s)(10) Equation 10 only applies syntactic constraints during phrase extraction and ignores them during substitution." ></td>
	<td class="line x" title="123:209	In our experiments, we evaluate the quality of the paraphrases that are generated using Equations 8, 9 and 10." ></td>
	<td class="line x" title="124:209	We compare their quality against the Bannard and Callison-Burch (2005) baseline." ></td>
	<td class="line x" title="125:209	4 Experimental design We conducted a manual evaluation to evaluate paraphrase quality." ></td>
	<td class="line x" title="126:209	We evaluated whether paraphrases retained the meaning of their original phrases and whether they remained grammatical when they replaced the original phrase in a sentence." ></td>
	<td class="line x" title="127:209	4.1 Training materials Our paraphrase model was trained using the Europarl corpus (Koehn, 2005)." ></td>
	<td class="line x" title="128:209	We used ten parallel corpora between English and (each of) Danish, Dutch, Finnish, French, German, Greek, Italian, Portuguese, Spanish, and Swedish, with approximately 30 million words per language for a total of 315 million English words." ></td>
	<td class="line x" title="129:209	Automatic word alignments were created for these using Giza++ (Och and Ney, 2003)." ></td>
	<td class="line x" title="130:209	The English side of each parallel corpus was parsed using the Bikel parser (Bikel, 2002)." ></td>
	<td class="line x" title="131:209	A total of 1.6 million unique sentences were parsed." ></td>
	<td class="line x" title="132:209	A trigram language model was trained on these English sentences using the SRI language modeling toolkit (Stolcke, 2002)." ></td>
	<td class="line x" title="133:209	The paraphrase model and language model for the Bannard and Callison-Burch (2005) baseline were trained on the same data to ensure a fair comparison." ></td>
	<td class="line x" title="134:209	4.2 Test phrases The test set was the English portion of test sets used in the shared translation task of the ACL2007 Workshop on Statistical Machine Translation (Callison-Burch et al., 2007)." ></td>
	<td class="line x" title="135:209	The test sentences were also parsed with the Bikel parser." ></td>
	<td class="line x" title="136:209	The phrases to be evaluated were selected such that there was an even balance of phrase lengths (from one word long up to five words long), with half of the phrases being valid syntactic constituents and half being arbitrary sequences of words." ></td>
	<td class="line x" title="137:209	410 phrases were selected at random for evaluation." ></td>
	<td class="line x" title="138:209	30 items were excluded from our results subsequent to evaluation on the grounds that they consisted 201 solely of punctuation and stop words like determiners, prepositions and pronouns." ></td>
	<td class="line x" title="139:209	This left a total of 380 unique phrases." ></td>
	<td class="line x" title="140:209	4.3 Experimental conditions We produced paraphrases under the following eight conditions: 1." ></td>
	<td class="line x" title="141:209	Baseline  The paraphrase probability defined by Bannard and Callison-Burch (2005)." ></td>
	<td class="line x" title="142:209	Calculated over multiple parallel corpora as given in Equation 5." ></td>
	<td class="line x" title="143:209	Note that under this condition the best paraphrase is the same for each occurrence of the phrase irrespective of which sentence it occurs in." ></td>
	<td class="line x" title="144:209	2." ></td>
	<td class="line x" title="145:209	Baseline + LM  The paraphrase probability (as above) combined with the language model probability calculated for the sentence with the phrase replaced with the paraphrase." ></td>
	<td class="line x" title="146:209	3." ></td>
	<td class="line x" title="147:209	Extraction Constraints  This condition selected the best paraphrase according to Equation 10." ></td>
	<td class="line x" title="148:209	It chooses the single best paraphrase over all labels." ></td>
	<td class="line x" title="149:209	Conditions 3 and 5 only apply the syntactic constraints at the phrase extraction stage, and do not require that the paraphrase have the same syntactic label as the phrase in the sentence that it is being subtituted into." ></td>
	<td class="line x" title="150:209	4." ></td>
	<td class="line x" title="151:209	Extraction Constraints + LM  As above, but the paraphrases are also ranked with a language model probability." ></td>
	<td class="line x" title="152:209	5." ></td>
	<td class="line x" title="153:209	Substitution Constraints  This condition corresponds to Equation 8, which selects the highest probability paraphrase which matches at least one of the syntactic labels of the phrase in the test sentence." ></td>
	<td class="line x" title="154:209	Conditions 58 apply the syntactic constraints both and the phrase extraction and at the substitution stages." ></td>
	<td class="line x" title="155:209	6." ></td>
	<td class="line x" title="156:209	Syntactic Constraints + LM  As above, but including a language model probability as well." ></td>
	<td class="line x" title="157:209	7." ></td>
	<td class="line x" title="158:209	Averaged Substitution Constraints  This condition corresponds to Equation 9, which averages over all of the syntactic labels for the phrase in the sentence, instead of choosing the single one which maximizes the probability." ></td>
	<td class="line x" title="159:209	MEANING 5 All of the meaning of the original phrase is retained, and nothing is added 4 The meaning of the original phrase is retained, although some additional information may be added but does not transform the meaning 3 The meaning of the original phrase is retained, although some information may be deleted without too great a loss in the meaning 2 Substantial amount of the meaning is different 1 The paraphrase doesnt mean anything close to the original phrase GRAMMAR 5 The sentence with the paraphrase inserted is perfectly grammatical 4 The sentence is grammatical, but might sound slightly awkward 3 The sentence has an agreement error (such as between its subject and verb, or between a plural noun and singular determiner) 2 The sentence has multiple errors or omits words that would be required to make it grammatical 1 The sentence is totally ungrammatical Table 5: Annotators rated paraphrases along two 5-point scales." ></td>
	<td class="line x" title="160:209	8." ></td>
	<td class="line x" title="161:209	Averaged Substitution Constraints + LM  As above, but including a language model probability." ></td>
	<td class="line x" title="162:209	4.4 Manual evaluation We evaluated the paraphrase quality through a substitution test." ></td>
	<td class="line x" title="163:209	We retrieved a number of sentences which contained each test phrase and substituted the phrase with automatically-generated paraphrases." ></td>
	<td class="line x" title="164:209	Annotators judged whether the paraphrases had the same meaning as the original and whether the resulting sentences were grammatical." ></td>
	<td class="line x" title="165:209	They assigned two values to each sentence using the 5-point scales given in Table 5." ></td>
	<td class="line x" title="166:209	We considered an item to have the same meaning if it was assigned a score of 3 or greater, and to be grammatical if it was assigned a score of 4 or 5." ></td>
	<td class="line x" title="167:209	We evaluated several instances of a phrase when it occurred multiple times in the test corpus, since paraphrase quality can vary based on context (Szpektor et al., 2007)." ></td>
	<td class="line x" title="168:209	There were an average of 3.1 instances for each phrase, with a maximum of 6." ></td>
	<td class="line x" title="169:209	There were a total of 1,195 sentences that para202 phrases were substituted into, with a total of 8,422 judgements collected." ></td>
	<td class="line x" title="170:209	Note that 7 different paraphrases were judged on average for every instance." ></td>
	<td class="line x" title="171:209	This is because annotators judged paraphrases for eight conditions, and because we collected judgments for the 5-best paraphrases for many of the conditions." ></td>
	<td class="line x" title="172:209	We measured inter-annotator agreement with the Kappa statistic (Carletta, 1996) using the 1,391 items that two annotators scored in common." ></td>
	<td class="line x" title="173:209	The two annotators assigned the same absolute score 47% of the time." ></td>
	<td class="line x" title="174:209	If we consider chance agreement to be 20% for 5-point scales, then K = 0.33, which is commonly interpreted as fair (Landis and Koch, 1977)." ></td>
	<td class="line x" title="175:209	If we instead measure agreement in terms of how often the annotators both judged an item to be above or below the thresholds that we set, then their rate of agreement was 80%." ></td>
	<td class="line x" title="176:209	In this case chance agreement would be 50%, so K = 0.61, which is substantial." ></td>
	<td class="line x" title="177:209	4.5 Data and code In order to allow other researchers to recreate our results or extend our work, we have prepared the following materials for download2:  The complete set of paraphrases generated for the test set." ></td>
	<td class="line x" title="178:209	This includes the 3.7 million paraphrases generated by the baseline method and the 3.5 million paraphrases generated with syntactic constraints." ></td>
	<td class="line x" title="179:209	 The code that we used to produce these paraphrases and the complete data sets (including all 10 word-aligned parallel corpora along with their English parses), so that researchers can extract paraphrases for new sets of phrases." ></td>
	<td class="line x" title="180:209	 The manual judgments about paraphrase quality." ></td>
	<td class="line x" title="181:209	These may be useful as development material for setting the weights of a log-linear formulation of paraphrasing, as suggested in Zhao et al.(2008a)." ></td>
	<td class="line x" title="183:209	5 Results Table 6 summarizes the results of the manual evaluation." ></td>
	<td class="line x" title="184:209	We can observe a strong trend in the syntactically constrained approaches performing better 2Available from http://cs.jhu.edu/ccb/." ></td>
	<td class="line x" title="185:209	correct correct both meaning grammar correct Baseline .56 .35 .30 Baseline+LM .46 .44 .36 Extraction Constraints .62 .57 .46 Extraction Const+LM .60 .65 .50 Substitution Constraints .60 .60 .50 Substitution Const+LM .61 .68 .54 Avg Substitution Const .62 .61 .51 Avg Substit Const+LM .61 .68 .55 Table 6: The results of the manual evaluation for each of the eight conditions." ></td>
	<td class="line x" title="186:209	Correct meaning is the percent of time that a condition was assigned a 3, 4, or 5, and correct grammar is the percent of time that it was given a 4 or 5, using the scales from Table 5." ></td>
	<td class="line x" title="187:209	than the baseline." ></td>
	<td class="line x" title="188:209	They retain the correct meaning more often (ranging from 4% to up to 15%)." ></td>
	<td class="line x" title="189:209	They are judged to be grammatical far more frequently (up to 26% more often without the language model, and 24% with the language model) . They perform nearly 20% better when both meaning and grammaticality are used as criteria.3 Another trend that can be observed is that incorporating a language model probability tends to result in more grammatical output (a 79% increase), but meaning suffers as a result in some cases." ></td>
	<td class="line x" title="190:209	When the LM is applied there is a drop of 12% in correct meaning for the baseline, but only a slight dip of 12% for the syntactically-constrained phrases." ></td>
	<td class="line x" title="191:209	Note that for the conditions where the paraphrases were required to have the same syntactic type as the phrase in the parse tree, there was a reduction in the number of paraphrases that could be applied." ></td>
	<td class="line x" title="192:209	For the first two conditions, paraphrases were posited for 1194 sentences, conditions 3 and 4 could be applied to 1142 of those sentences, but conditions 58 could only be applied to 876 sentences." ></td>
	<td class="line x" title="193:209	The substitution constraints reduce coverage to 73% of the test sentences." ></td>
	<td class="line x" title="194:209	Given that the extraction constraints have better coverage and nearly identical performance on 3Our results show a significantly lower score for the baseline than reported in Bannard and Callison-Burch (2005)." ></td>
	<td class="line x" title="195:209	This is potentially due to the facts that in this work we evaluated on out-of-domain news commentary data, and we randomly selected phrases." ></td>
	<td class="line x" title="196:209	In the pervious work the test phrases were drawn from WordNet, and they were evaluated solely on in-domain European parliament data." ></td>
	<td class="line x" title="197:209	203 the meaning criterion, they might be more suitable in some circumstances." ></td>
	<td class="line x" title="198:209	6 Conclusion In this paper we have presented a novel refinement to paraphrasing with bilingual parallel corpora." ></td>
	<td class="line x" title="199:209	We illustrated that a significantly higher performance can be achieved by constraining paraphrases to have the same syntactic type as the original phrase." ></td>
	<td class="line x" title="200:209	A thorough manual evaluation found an absolute improvement in quality of 19% using strict criteria about paraphrase accuracy when comparing against a strong baseline." ></td>
	<td class="line x" title="201:209	The syntactically enhanced paraphrases are judged to be grammatically correct over two thirds of the time, as opposed to the baseline method which was grammatically correct under half of the time." ></td>
	<td class="line x" title="202:209	This paper proposed constraints on paraphrases at two stages: when deriving them from parsed parallel corpora and when substituting them into parsed test sentences." ></td>
	<td class="line x" title="203:209	These constraints produce paraphrases that are better than the baseline and which are less commonly affected by problems due to unaligned words." ></td>
	<td class="line x" title="204:209	Furthermore, by introducing complex syntactic labels instead of solely relying on non-terminal symbols in the parse trees, we are able to keep the broad coverage of the baseline method." ></td>
	<td class="line x" title="205:209	Syntactic constraints significantly improve the quality of this paraphrasing method, and their use opens the question about whether analogous constraints can be usefully applied to paraphrases generated from purely monolingual corpora." ></td>
	<td class="line x" title="206:209	Our improvements to the extraction of paraphrases from parallel corpora suggests that it may be usefully applied to other NLP applications, such as generation, which require grammatical output." ></td>
	<td class="line x" title="207:209	Acknowledgments Thanks go to Sally Blatz, Emily Hinchcliff and Michelle Bland for conducting the manual evaluation and to Michelle Bland and Omar Zaidan for proofreading and commenting on a draft of this paper." ></td>
	<td class="line x" title="208:209	This work was supported by the National Science Foundation under Grant No. 0713448." ></td>
	<td class="line x" title="209:209	The views and findings are the authors alone." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D08-1064
Decomposability of Translation Metrics for Improved Evaluation and Efficient Algorithms
Chiang, David;DeNeefe, Steve;Chan, Yee Seng;Ng, Hwee Tou;"></td>
	<td class="line x" title="1:146	Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 610619, Honolulu, October 2008." ></td>
	<td class="line x" title="2:146	c2008 Association for Computational Linguistics Decomposability of Translation Metrics for Improved Evaluation and Efficient Algorithms David Chiang and Steve DeNeefe Information Sciences Institute University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 USA {chiang,sdeneefe}@isi.edu Yee Seng Chan and Hwee Tou Ng Department of Computer Science National University of Singapore Law Link Singapore 117590 {chanys,nght}@comp.nus.edu.sg Abstract B is the de facto standard for evaluation and development of statistical machine translation systems." ></td>
	<td class="line x" title="3:146	We describe three real-world situations involving comparisons between different versions of the same systems where one can obtain improvements in B scores that are questionable or even absurd." ></td>
	<td class="line x" title="4:146	These situations arise because B lacks the property of decomposability, a property which is also computationally convenient for various applications." ></td>
	<td class="line x" title="5:146	We propose a very conservative modification to B and a cross between B and word error rate that address these issues while improving correlation with human judgments." ></td>
	<td class="line x" title="6:146	1 Introduction B (Papineni et al., 2002) was one of the first automatic evaluation metrics for machine translation (MT), and despite being challenged by a number of alternative metrics (Melamed et al., 2003; Banerjee and Lavie, 2005; Snover et al., 2006; Chan and Ng, 2008), it remains the standard in the statistical MTliterature.Callison-Burchetal.(2006)havesubjected B to a searching criticism, with two realworld case studies of significant failures of correlation between B and human adequacy/fluency judgments.Bothcasesinvolvecomparisonsbetween statistical MT systems and other translation methods (human post-editing and a rule-based MT system), and they recommend that the use of B be restrictedtocomparisonsbetweenrelatedsystemsor different versions of the same systems." ></td>
	<td class="line x" title="7:146	In Bs defense, comparisons between different versions of the same system were exactly what B was designed for." ></td>
	<td class="line x" title="8:146	However, we show that even in such situations, difficulties with B can arise." ></td>
	<td class="line x" title="9:146	We illustrate three ways that properties of B can be exploited to yield improvements that are questionable or even absurd." ></td>
	<td class="line x" title="10:146	All of these scenarios arose in actual practice and involve comparisons between different versions of the same statistical MT systems." ></td>
	<td class="line x" title="11:146	They can be traced to the fact that B is not decomposable at the sentence level: that is, it lacks the property that improving a sentence in a test set leads to an increase in overall score, and degrading a sentence leads to a decrease in the overall score." ></td>
	<td class="line x" title="12:146	This property is not only intuitive, but also computationally convenient for various applications such as translation reranking and discriminative training." ></td>
	<td class="line x" title="13:146	We propose a minimal modification to B that reduces its nondecomposability, as well as a cross between B and word error rate (WER) that is decomposable down to the subsentential level (in a sense to be made more precise below)." ></td>
	<td class="line x" title="14:146	Both metrics correct the observed problems and correlate with human judgments better than B. 2 The B metric Let gk(w) be the multiset of all k-grams of a sentence w. We are given a sequence of candidate translations c to be scored against a set of sequences of reference translations, {rj} = r1,,rR: c = c1, c2, c3,,cN r1 = r11, r12, r13, ,r1N  rR = rR1,rR2,rR3,,rRN 610 Then the B score of c is defined to be B(c,{rj}) = 4productdisplay k=1 prk(c,{rj})14  bp(c,{rj}) (1) where1 prk(c,{rj}) = summationtext i vextendsinglevextendsinglevextendsingle vextendsinglegk(ci)  uniontextj gk(rji ) vextendsinglevextendsinglevextendsingle vextendsinglesummationtext i |gk(ci)| (2) is the k-gram precision of c with respect to {rj}, and bp(c,r), known as the brevity penalty, is defined as follows." ></td>
	<td class="line x" title="15:146	Let (x) = exp(1  1/x)." ></td>
	<td class="line x" title="16:146	In the case of a single reference r, bp(c,r) =  parenleftBigg min braceleftBigg 1, summationtext i |ci|summationtext i |ri| bracerightBiggparenrightBigg (3) In the multiple-reference case, the length |ri| is replaced with an effective reference length, which can be calculated in several ways." ></td>
	<td class="line x" title="17:146	 In the original definition (Papineni et al., 2002), it is the length of the reference sentence whose length is closest to the test sentence." ></td>
	<td class="line x" title="18:146	 In the NIST definition, it is the length of the shortest reference sentence." ></td>
	<td class="line x" title="19:146	 A third possibility would be to take the average length of the reference sentences." ></td>
	<td class="line x" title="20:146	The purpose of the brevity penalty is to prevent a system from generating very short but precise translations, and the definition of effective reference length impacts how strong the penalty is. The NIST definition is the most tolerant of short translations and becomes more tolerant with more reference sentences." ></td>
	<td class="line x" title="21:146	The original definition is less tolerant but has the counterintuitive property that decreasing the length of a test sentence can eliminate the brevity penalty." ></td>
	<td class="line x" title="22:146	Using the average reference length seems attractive but has the counterintuitive property that 1We use the following definitions about multisets: if X is a multiset, let #X(a) be the number of times a occurs in X. Then: |X|  summationdisplay a #X(a) #XY(a)  min{#X(a),#Y(a)} #XY(a)  max{#X(a),#Y(a)} an exact match with one of the references may not get a 100% score." ></td>
	<td class="line x" title="23:146	Throughout this paper we use the NIST definition, as it is currently the definition most used in the literature and in evaluations." ></td>
	<td class="line x" title="24:146	The brevity penalty can also be seen as a standin for recall." ></td>
	<td class="line x" title="25:146	The fraction summationtext i |ci|summationtext i |ri| in the definition of the brevity penalty (3) indeed resembles a weak recall score in which every guessed item counts as a match." ></td>
	<td class="line x" title="26:146	However, with recall, the per-sentence score |ci| |ri| would never exceed unity, but with the brevitypenalty, it can." ></td>
	<td class="line x" title="27:146	This means that if a system generates a long translation for one sentence, it can generate a short translation for another sentence without facing a penalty." ></td>
	<td class="line x" title="28:146	This is a serious weakness in the B metric, as we demonstrate below using three scenarios, encountered in actual practice." ></td>
	<td class="line x" title="29:146	3 Exploiting the B metric 3.1 The sign test We are aware of two methods that have been proposed for significance testing with B: bootstrap resampling (Koehn, 2004b; Zhang et al., 2004) and the sign test (Collins et al., 2005)." ></td>
	<td class="line x" title="30:146	In bootstrap resampling, we sample with replacement from the test set to synthesize a large number of test sets, and then we compare the performance of two systems on those synthetic test sets to see whether one is better 95% (or 99%) of the time." ></td>
	<td class="line x" title="31:146	But Collins et al.(2005) note that it is not clear whether the conditions requiredbybootstrapresamplingaremetinthecaseof B, and recommend the sign test instead." ></td>
	<td class="line x" title="33:146	Suppose wewanttodeterminewhetherasetofoutputscfrom a test system is better or worse than a set of baseline outputs b. The sign test requires a function f(bi,ci) that indicates whether ci is a better, worse, or samequality translation relative to bi." ></td>
	<td class="line x" title="34:146	However, because B is not defined on single sentences, Collins et al. use an approximation: for each i, form a composite set of outputs bprime = {b1,,bi1,ci,bi+1,,bN}, and compare the B scores of b and bprime." ></td>
	<td class="line x" title="35:146	The goodness of this approximation depends on to what extent the comparison between b and bprime is dependent only on bi and ci, and independent of the other sentences." ></td>
	<td class="line x" title="36:146	However, B scores are highly context-dependent: for example, if the sentences in b are on average epsilon1 words longer than the reference sentences, then ci can be as short as (N  1)epsilon1 words 611 shorter than ri without incurring the brevity penalty." ></td>
	<td class="line x" title="37:146	Moreover, since the ci are substituted in one at a time, we can do this for all of the ci." ></td>
	<td class="line x" title="38:146	Hence, c could have a disastrously low B score (because of the brevity penalty) yet be found by the sign test to be significantly better than the baseline." ></td>
	<td class="line x" title="39:146	We have encountered this situation in practice: two versions of the same system with B scores of 29.6 (length ratio 1.02) and 29.3 (length ratio 0.97), where the sign test finds the second system to be significantly better than the first (and the first system significantly better than the second)." ></td>
	<td class="line x" title="40:146	Clearly, in orderforasignificancetesttobesensible,itshouldnot contradict the observed scores, and should certainly not contradict itself." ></td>
	<td class="line x" title="41:146	In the rest of this paper, except where indicated, all significance tests are performed using bootstrap resampling." ></td>
	<td class="line x" title="42:146	3.2 Genre-specific training For several years, much statistical MT research has focused on translating newswire documents." ></td>
	<td class="line x" title="43:146	One likely reason is that the DARPA TIDES program used newswire documents for evaluation for several years." ></td>
	<td class="line x" title="44:146	But more recent evaluations have included other genres such as weblogs and conversation." ></td>
	<td class="line x" title="45:146	The conventional wisdom has been that if one uses a single statistical translation system to translate text from several different genres, it may perform poorly, and it is better to use several systems optimized separately for each genre." ></td>
	<td class="line x" title="46:146	However, if our task is to translate documents from multiple known genres, but they are evaluated together, the B metric allows us to use that fact to our advantage." ></td>
	<td class="line x" title="47:146	To understand how, notice that our system has an optimal number of words that it should generate for the entire corpus: too few and it will be penalized by Bs brevity penalty, and too many increases the risk of additional non-matching k-grams." ></td>
	<td class="line x" title="48:146	But these words can be distributed among the sentences (and genres) in any way we like." ></td>
	<td class="line x" title="49:146	Instead of translating sentences from each genre with the best genre-specific systems possible, we can generate longer outputs for the genre we have more confidence in, while generating shorter outputs for the harder genre." ></td>
	<td class="line x" title="50:146	This strategy will have mediocre performance on each individual genre (according to both intuition and B), yet will receive a higher B score on the combined test set than the combined systems optimized for each genre." ></td>
	<td class="line x" title="51:146	In fact, knowing which sentence is in which genre is not even always necessary." ></td>
	<td class="line x" title="52:146	In one recent task, we translated documents from two different genres, without knowing the genre of any given sentence." ></td>
	<td class="line x" title="53:146	The easier genre, newswire, also tended to have shorter reference sentences (relative to the source sentences) than the harder genre, weblogs." ></td>
	<td class="line x" title="54:146	For example, in one dataset, the newswire reference sets had between 1.3 and 1.37 English words per Arabic word, but the weblog reference set had 1.52 English words per Arabic word." ></td>
	<td class="line x" title="55:146	Thus, a system that is uniformly verbose across both genres will apportion more of its output to newswire than to weblogs, serendipitously leading to a higher score." ></td>
	<td class="line x" title="56:146	This phenomenon has subsequently been observed by Och (2008) as well." ></td>
	<td class="line oc" title="57:146	We trained three Arabic-English syntax-based statistical MT systems (Galley et al., 2004; Galley et al., 2006) using max-B training (Och, 2003): one on a newswire development set, one on a weblog development set, and one on a combined development set containing documents from both genres." ></td>
	<td class="line x" title="58:146	We then translated a new mixed-genre test set in two ways: (1) each document with its appropriate genrespecific system, and (2) all documents with the system trained on the combined (mixed-genre) development set." ></td>
	<td class="line x" title="59:146	In Table 3, we report the results of both approaches on the entire test dataset as well as the portion of the test dataset in each genre, for both the genre-specific and mixed-genre trainings." ></td>
	<td class="line x" title="60:146	The genre-specific systems each outperform the mixed system on their own genre as expected, but when the same results are combined, the mixed systemsoutputisafullB pointhigherthanthecombination of the genre-specific systems." ></td>
	<td class="line x" title="61:146	This is because the mixed system produces outputs that have about 1.35 English words per Arabic word on average: longer than the shortest newswire references, but shorter than the weblog references." ></td>
	<td class="line x" title="62:146	The mixed system does worse on each genre but better on the combined test set, whereas, according to intuition, a system that does worse on the two subsets should also do worse on the combined test set." ></td>
	<td class="line x" title="63:146	3.3 Word deletion A third way to take advantage of the B metric is to permit an MT system to delete arbitrary words 612 in the input sentence." ></td>
	<td class="line x" title="64:146	We can do this by introducing new phrases or rules into the system that match words in the input sentence but generate no output; to these rules we attach a feature whose weight is tuned during max-B training." ></td>
	<td class="line x" title="65:146	Such rules have been in use for some time but were only recently discussed by Li et al.(2008)." ></td>
	<td class="line x" title="67:146	When we add word-deletion rules to our MT system, we find that the B increases significantly (Table 6, line 2)." ></td>
	<td class="line x" title="68:146	Figure 1 shows some examples of deletion in Chinese-English translation." ></td>
	<td class="line x" title="69:146	The first sentence has a proper name,<[[/maigesaisai Magsaysay,whichhasbeenmistokenizedintofour tokens." ></td>
	<td class="line x" title="70:146	The baseline system attempts to translate the first two phonetic characters as wheat Georgia, whereas the other system simply deletes them." ></td>
	<td class="line x" title="71:146	On the other hand, the second sentence shows how word deletion can sacrifice adequacy for the sake of fluency, and the third sentence shows that sometimes word deletion removes words that could have been translated well (as seen in the baseline translation)." ></td>
	<td class="line x" title="72:146	Does B reward word deletion fairly?" ></td>
	<td class="line x" title="73:146	We note two reasons why word deletion might be desirable." ></td>
	<td class="line x" title="74:146	First, some function words should truly be deleted: for example, the Chinese particle/de and Chinese measure words often have no counterpart in English (Li et al., 2008)." ></td>
	<td class="line x" title="75:146	Second, even content word deletion might be helpful if it allows a more fluent translation to be assembled from the remnants." ></td>
	<td class="line x" title="76:146	We observe that in the above experiment, word deletion caused the absolute number of k-gram matches, and not just kgram precision, to increase for all 1  k  4." ></td>
	<td class="line x" title="77:146	Human evaluation is needed to conclusively determine whether B rewards deletion fairly." ></td>
	<td class="line x" title="78:146	But to control for these potentially positive effects of deletion, we tested a sentence-deletion system, which is the same as the word-deletion system but constrained to delete all of the words in a sentence or none of them." ></td>
	<td class="line x" title="79:146	This system (Table 6, line 3) deleted 810% of its input and yielded a B score with no significant decrease (p  0.05) from the baseline systems. Given that our model treats sentences independently, so that it cannot move information from one sentence to another, we claim that deletion of nearly 10% of the input is a grave translation deficiency, yet B is insensitive to it." ></td>
	<td class="line x" title="80:146	What does this tell us about word deletion?" ></td>
	<td class="line x" title="81:146	While acknowledging that some word deletions can improve translation quality, we suggest in addition that because word deletion provides a way for the system to translate the test set selectively, a behavior which we have shown that B is insensitive to, part of the score increase due to word deletion is likely an artifact of B. 4 Other metrics Are other metrics susceptible to the same problems as the B metric?" ></td>
	<td class="line x" title="82:146	In this section we examine several other popular metrics for these problems, propose two of our own, and discuss some desirable characteristics for any new MT evaluation metric." ></td>
	<td class="line x" title="83:146	4.1 Previous metrics We ran a suite of other metrics on the above problem cases to see whether they were affected." ></td>
	<td class="line x" title="84:146	In none of these cases did we repeat minimum-error-rate training; all these systems were trained using max-B. The metrics we tested were:  METEOR (Banerjee and Lavie, 2005), version 0.6,usingtheexact,Porter-stemmer,andWordNet synonmy stages, and the optimized parameters  = 0.81,  = 0.83,  = 0.28 as reported in (Lavie and Agarwal, 2007)." ></td>
	<td class="line x" title="85:146	 GTM (Melamed et al., 2003), version 1.4, with default settings, except e = 1.2, following the WMT 2007 shared task (Callison-Burch et al., 2007)." ></td>
	<td class="line x" title="86:146	 MS (Chan and Ng, 2008), more specifically MSn, which skips the dependency relations." ></td>
	<td class="line x" title="87:146	On the sign test (Table 2), all metrics found significant differences consistent with the difference in score between the two systems." ></td>
	<td class="line x" title="88:146	The problem related to genre-specific training does not seem to affect the other metrics (see Table 4), but they still manifest the unintuitive result that genre-specific training is sometimesworsethanmixed-genretraining.Finally, all metrics but GTM disfavored both word deletion and sentence deletion (Table 7)." ></td>
	<td class="line x" title="89:146	4.2 Strict brevity penalty AveryconservativewayofmodifyingtheB metric to combat the effects described above is to im613 (a) source 9]<[[V reference fei xiaotong awarded magsaysay prize baseline fei xiaotong was awarded the wheat georgia xaixai prize delete fei xiaotong was awarded xaixai award (b) source c-/EAp-NqHa reference the center of the yuhua stone bears an image which very much resembles the territory of the people s republic of china . baseline rain huashi center is a big clear images of chinese territory . delete rain is a clear picture of the people s republic of china ." ></td>
	<td class="line x" title="90:146	(c) source :FDRw  reference urban construction becomes new hotspot for foreign investment in qinghai baseline urban construction become new hotspot for foreign investment qinghai delete become new foreign investment hotspot Figure 1: Examples of word deletion." ></td>
	<td class="line x" title="91:146	Underlined Chinese words were deleted in the word-deletion system; underlined English words correspond to deleted Chinese words." ></td>
	<td class="line x" title="92:146	pose a stricter brevity penalty." ></td>
	<td class="line x" title="93:146	In Section 2, we presented the brevity penalty as a stand-in for recall, but noted that unlike recall, the per-sentence score |ci| |ri| can exceed unity." ></td>
	<td class="line x" title="94:146	This suggests the simple fix ofclipping the per-sentence recall scores in a similar fashion to the clipping of precision scores: bp(c,r) =  parenleftBiggsummationtext i min{|ci|,|ri|}summationtext i |ri| parenrightBigg (4) Then if a translation system produces overlong translations for some sentences, it cannot use those translations to license short translations for other sentences." ></td>
	<td class="line x" title="95:146	Call this revised metric B- (for B with strict brevity penalty)." ></td>
	<td class="line x" title="96:146	We can test this revised definition on the problem cases described above." ></td>
	<td class="line x" title="97:146	Table 2 shows that B resolves the inconsistency observed between B and the sign test, using the example test sets fromSection3.1(nomax-B- trainingwasperformed)." ></td>
	<td class="line x" title="98:146	Table 5 shows the new scores of the mixedgenre example from Section 3.2 after max-B- training." ></td>
	<td class="line x" title="99:146	These results fall in line with intuition tuning separately for each genre leads to slightly better scores in all cases." ></td>
	<td class="line x" title="100:146	Finally, Table 8 shows the B- scores for the word-deletion example from Section 3.3, using both max-B training and maxB- training." ></td>
	<td class="line x" title="101:146	We see that B- reduces the benefit of word deletion to an insignificant level on the test set, and severely punishes sentence deletion." ></td>
	<td class="line x" title="102:146	When we retrain using max-B-, the rate of word deletion is reduced and sentence deletion is all but eliminated, and there are no significant differences on the test set." ></td>
	<td class="line x" title="103:146	4.3 4-gram recognition rate All of the problems we have examinedexcept for word deletionare traceable to the fact that B is not a sentence-level metric." ></td>
	<td class="line x" title="104:146	Any metric which is defined as a weighted average of sentence-level scores, where the weights are system-independent, will be immune to these problems." ></td>
	<td class="line x" title="105:146	Note that any metricinvolvingmicro-averagedprecision(inwhich the sentence-level counts of matches and guesses are summed separately before forming their ratio) cannot have this property." ></td>
	<td class="line x" title="106:146	Of the metrics surveyed in the WMT 2007 evaluation-evaluation (CallisonBurch et al., 2007), at least the following metrics have this property: WER (Nieen et al., 2000), TER (Snover et al., 2006), and ParaEval-Recall (Zhou et al., 2006)." ></td>
	<td class="line x" title="107:146	Moreover, this evaluation concern dovetails with a frequent engineering concern, that sentence-level scores are useful at various points in the MT pipeline: for example, minimum Bayes risk decoding (Kumar and Byrne, 2004), selecting oracle translations for discriminative reranking (Liang 614 et al., 2006; Watanabe et al., 2007), and sentenceby-sentence comparisons of outputs during error analysis." ></td>
	<td class="line x" title="108:146	A variation on B is often used for these purposes, in which the k-gram precisions are smoothed by adding one to the numerator and denominator (Lin and Och, 2004); this addresses the problem of a zero k-gram match canceling out the entire score, but it does not address the problems illustrated above." ></td>
	<td class="line x" title="109:146	The remaining issue, word deletion, is more difficult to assess." ></td>
	<td class="line x" title="110:146	It could be argued that part of the gain due to word deletion is caused by B allowing a system to selectively translate those parts of a sentence on which higher precision can be obtained." ></td>
	<td class="line x" title="111:146	It would be difficult indeed to argue that an evaluation metric, in order to be fair, must be decomposable into subsentential scores, and we make no such claim." ></td>
	<td class="line x" title="112:146	However, there is again a dovetailing engineering concern which is quite legitimate." ></td>
	<td class="line x" title="113:146	If one wants to select the minimum-Bayes-risk translation from a lattice (or shared forest) instead of an n-best list (Tromble et al., 2008), or to select an oracle translation from a lattice (Tillmann and Zhang, 2006; Dreyer et al., 2007; Leusch et al., 2008), or to perform discriminative training on all the examples containedinalattice(Taskaretal.,2004),onewould need a metric that can be calculated on the edges of the lattice." ></td>
	<td class="line x" title="114:146	Of the metrics surveyed in the WMT 2007 evaluation-evaluation, only one metric, to our knowledge, has this property: word error rate (Nieen et al., 2000)." ></td>
	<td class="line x" title="115:146	Here, we deal with the related word recognition rate (McCowan et al., 2005), WRR = 1  WER = 1  min I + D+S|r| = max M  I|r| (5) where I is the number of insertions, D of deletions, S of substitutions, and M = |r|  D  S the number of matches." ></td>
	<td class="line x" title="116:146	The dynamic program for WRR can be formulated as a Viterbi search through a finite-state automaton: given a candidate sentence c and a referencesentencer,findthehighest-scoringpathmatching c through the automaton with states 0,,|r|, initial state 0, final state |r|, and the following transitions (a star matches any symbol): For 0  i < |r|: i ri+1:1 i+1 match i epsilon1:0 i+1 deletion i star:0 i+1 substitution For 0  i  |r|: i star:1 i insertion This automaton can be intersected with a typical stack-based phrase-based decoder lattice (Koehn, 2004a) or CKY-style shared forest (Chiang, 2007) in much the same way that a language model can, yielding a polynomial-time algorithm for extracting the best-scoring translation from a lattice or forest (Wagner, 1974)." ></td>
	<td class="line x" title="117:146	Intuitively, the reason for this is that WRR, like most metrics, implicitly constructs a word alignment between c and r and only counts matches between aligned words; but unlike other metrics, this alignment is constrained to be monotone." ></td>
	<td class="line x" title="118:146	We can combine WRR with the idea of k-gram matching in B to yield a new metric, the 4-gram recognition rate: 4-GRR = max summationtext4 k=1 Mk  I  Dsummationtext 4 k=1 |gk(r)| (6) where Mk is the number of k-gram matches,  and  control the penalty for insertions and deletions, and gk is as defined in Section 2." ></td>
	<td class="line x" title="119:146	We presently set  = 1, = 0 by analogy with WRR, but explore other settings below." ></td>
	<td class="line x" title="120:146	To calculate 4-GRR on a whole test set, we sum the numerators and denominators as in micro-averaged recall." ></td>
	<td class="line x" title="121:146	The 4-GRR can also be formulated as a finitestate automaton, with states {(i,m) | 0  i  |r|,0  m  3}, initial state (0,0), final states (|r|,m), and the following transitions: For 0  i < |r|, 0  m  3: (i,m) ri+1:m+1 (i+1,min{m+1,3}) match (i,m) epsilon1: (i+1,0) deletion (i,m) star:0 (i+1,0) substitution 615 Metric Adq Flu Rank Con Avg Sem.roleoverlap 77.4 83.9 80.3 74.1 78.9 ParaEvalrecall 71.2 74.2 76.8 79.8 75.5 METEOR 70.1 71.9 74.5 66.9 70.9 B 68.9 72.1 67.2 60.2 67.1 WER 51.0 54.2 34.5 52.4 48.0 B- 73.9 76.7 73.5 63.4 71.9 4-GRR 72.3 75.5 74.3 64.2 71.6 Table 1: Our new metrics correlate with human judgmentsbetterthanB (case-sensitive).Adq=Adequacy, Flu = Fluency, Con = Constituent, Avg = Average." ></td>
	<td class="line x" title="122:146	For 0  i  |r|, 0  m  3: (i,m) star: (i,0) insertion Therefore 4-GRR can also be calculated efficiently on lattices or shared forests." ></td>
	<td class="line x" title="123:146	We did not attempt max-4-GRR training, but we evaluated the word-deletion test sets obtained by max-B and max-B- training using 4-GRR." ></td>
	<td class="line x" title="124:146	The results are shown in Table 7." ></td>
	<td class="line x" title="125:146	In general, the results are very similar to B- except that 4-GRR sometimes scores word deletion slightly lower than baseline." ></td>
	<td class="line x" title="126:146	5 Correlation with human judgments The shared task of the 2007 Workshop on Statistical Machine Translation (Callison-Burch et al., 2007) was conducted with several aims, one of which was to measure the correlation of several automatic MT evaluation metrics (including B) against human judgments." ></td>
	<td class="line x" title="127:146	The task included two datasets (one drawn from the Europarl corpus and the other from the News Commentary corpus) and across three language pairs (from German, Spanish, and French to English, and back)." ></td>
	<td class="line x" title="128:146	In our experiments, we focus on the tasks where the target language is English." ></td>
	<td class="line x" title="129:146	For human evaluations of the MT submissions, four different criteria were used:  Adequacy: how much of the meaning expressed in the reference translation is also expressed in the hypothesis translation." ></td>
	<td class="line x" title="130:146	 Fluency: how well the translation reads in the target language." ></td>
	<td class="line x" title="131:146	 Rank: each translation is ranked from best to worst, relative to the other translations of the same sentence." ></td>
	<td class="line x" title="132:146	 Constituent: constituents are selected from source-side parse-trees, and human judges are asked to rank their translations." ></td>
	<td class="line x" title="133:146	We scored the workshop shared task submissions with B- and 4-GRR, then converted the raw scores to rankings and calculated the Spearman correlations with the human judgments." ></td>
	<td class="line x" title="134:146	Table 1 shows theresultsalongwithB andthethreemetricsthat achieved higher correlations than B: semantic role overlap (Gimenez and Marquez, 2007), ParaEval recall (Zhou et al., 2006), and METEOR (Banerjee and Lavie, 2005)." ></td>
	<td class="line x" title="135:146	We find that both our proposed metrics correlate with human judgments better than B does." ></td>
	<td class="line x" title="136:146	However, recall the parameters  and  in the definitionof4-GRRthatcontrolthepenaltyforinserted and deleted words." ></td>
	<td class="line x" title="137:146	Experimenting with this parameter reveals that  = 0.9, = 1 yields a correlation of 78.9%." ></td>
	<td class="line x" title="138:146	In other words, a metric that unboundedly rewards spuriously inserted words correlates better with human judgments than a metric that punishes them." ></td>
	<td class="line x" title="139:146	We assume this is because there are not enough data points (systems) in the sample and ask that all these figures be taken with a grain of salt." ></td>
	<td class="line x" title="140:146	As a general remark, it may be beneficial for humancorrelation datasets to include a few straw-man systems that have very short or very long translations." ></td>
	<td class="line x" title="141:146	6 Conclusion We have described three real-world scenarios involving comparisons between different versions of the same statistical MT systems where B gives counterintuitive results." ></td>
	<td class="line x" title="142:146	All these issues center around the issue of decomposability: the sign test fails because substituting translations one sentence at a time can improve the overall score yet substituting them all at once can decrease it; genre-specific training fails because improving the score of two halves of a test set can decrease the overall score; and sentence deletion is not harmful because generating empty translations for selected sentences does not necessarily decrease the overall score." ></td>
	<td class="line x" title="143:146	We proposed a minimal modification to B, calledB-,andshowedthatitamelioratesthese 616 problems." ></td>
	<td class="line x" title="144:146	We also proposed a metric, 4-GRR, that is decomposable at the sentence level and is therefore guaranteed to solve the sign test, genre-specific tuning, and sentence deletion problems; moreoever, it is decomposable at the subsentential level, which has potential implications for evaluating word deletion and promising applications to translation reranking and discriminative training." ></td>
	<td class="line x" title="145:146	Acknowledgments Our thanks go to Daniel Marcu for suggesting modifying the B brevity penalty, and to Jonathan May and Kevin Knight for their insightful comments." ></td>
	<td class="line x" title="146:146	ThisresearchwassupportedinpartbyDARPAgrant HR0011-06-C-0022 under BBN Technologies subcontract 9500008412." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D08-1093
Automatic Prediction of Parser Accuracy
Ravi, Sujith;Knight, Kevin;Soricut, Radu;"></td>
	<td class="line x" title="1:194	Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 887896, Honolulu, October 2008." ></td>
	<td class="line x" title="2:194	c2008 Association for Computational Linguistics Automatic Prediction of Parser Accuracy Sujith Ravi and Kevin Knight University of Southern California Information Sciences Institute Marina del Rey, California 90292 {sravi,knight}@isi.edu Radu Soricut Language Weaver, Inc. 4640 Admiralty Way, Suite 1210 Marina del Rey, California 90292 rsoricut@languageweaver.com Abstract Statistical parsers have become increasingly accurate, to the point where they are useful in many natural language applications." ></td>
	<td class="line x" title="3:194	However, estimating parsing accuracy on a wide variety of domains and genres is still a challenge in the absence of gold-standard parse trees." ></td>
	<td class="line x" title="4:194	In this paper, we propose a technique that automatically takes into account certain characteristics of the domains of interest, and accurately predicts parser performance on data from these new domains." ></td>
	<td class="line x" title="5:194	As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain." ></td>
	<td class="line x" title="6:194	1 Introduction Statistical natural language parsers have recently become more accurate and more widely available." ></td>
	<td class="line oc" title="7:194	As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008)." ></td>
	<td class="line x" title="8:194	These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute." ></td>
	<td class="line x" title="9:194	Ideally, one would want to have available a recipe for precisely answering this question: given a parser and a particular domain of interest, how accurate are the parse trees produced? The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample set from the domain of interest, and consequently use them to compute a PARSEVAL (Black et al., 1991) score that is indicative of the intrinsic performance of the parser." ></td>
	<td class="line x" title="10:194	Given the wide range of domains and genres for which NLP applications are of interest, combined with the high expertise required from human annotators to produce parse tree annotations, this recipe is, albeit precise, too expensive." ></td>
	<td class="line x" title="11:194	The other recipe that is currently used on a large scale is to measure the performance of a parser on existing treebanks, such as WSJ (Marcus et al., 1993), and assume that the accuracy measure will carry over to the domains of interest." ></td>
	<td class="line x" title="12:194	This recipe, albeit cheap, cannot provide any guarantee regarding the performance of a parser on a new domain, and, as experiments in this paper show, can give wrong indications regarding important decisions for the design of NLP systems that use a syntactic parser as an important component." ></td>
	<td class="line x" title="13:194	This paper proposes another method for measuring the performance of a parser on a given domain that is both cheap and effective." ></td>
	<td class="line x" title="14:194	It is a fully automated procedure (no expensive annotation involved) that uses properties of both the domain of interest and the domain on which the parser was trained in order to measure the performance of the parser on the domain of interest." ></td>
	<td class="line x" title="15:194	It is, in essence, a solution to the following prediction problem: Input: (1) a statistical parser and its training data, (2) some chunk of text from a new domain or genre Output: an estimate of the accuracy of the parse trees produced for that chunk of text 887 Accurate estimations for this prediction problem will allow a system designer to make the right decisions for the given domain of interest." ></td>
	<td class="line x" title="16:194	Such decisions include, but are not restricted to, the choice of the parser, the choice of the training data, the choice of how to implement various components such as the treatment of unknown words, etc. Altogether, a correct estimation of the impact of such decisions on the resulting parse trees can guide a system designer in a hill-climbing scenario for which an extrinsic metric (such as the impact on the overall quality of the system) is usually too expensive to be employed often enough." ></td>
	<td class="line x" title="17:194	To provide an example, a machine translation engine that requires parse trees as training data in order to learn syntax-based translation rules (Galley et al., 2006) needs to employ a syntactic parser as soon as the training process starts, but it can take up to hundreds and even thousands of CPU hours (for large training data sets) to train the engine before translations can be produced and measured." ></td>
	<td class="line x" title="18:194	Although a real estimate of the impact of a parser design decision in this scenario can only be gauged from the quality of the translations produced, it is impractical to create such estimates for each design decision." ></td>
	<td class="line x" title="19:194	On the other hand, estimates using the solution proposed in this paper can be obtained fast, before submitting the parser output to a costly training procedure." ></td>
	<td class="line x" title="20:194	2 Related Work and Experimental Framework There have been previous studies which explored the problem of automatically predicting the task difficulty for various NLP applications." ></td>
	<td class="line x" title="21:194	(Albrecht and Hwa, 2007) presented a regression based method for developing automatic evaluation metrics for machine translation systems without directly relying on human reference translations." ></td>
	<td class="line x" title="22:194	(Hoshino and Nakagawa, 2007) built a computer-adaptive system for generating questions to teach English grammar and vocabulary to students, by predicting the difficulty level of a question using various features." ></td>
	<td class="line x" title="23:194	There have been a few studies of English parser accuracy in domains/genres other than WSJ (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006), but in order to make measurements for such studies, it is necessary to have gold-standard parses in the nonWSJ domain of interest." ></td>
	<td class="line x" title="24:194	Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists." ></td>
	<td class="line x" title="25:194	He looked at sentences with 40 words or less." ></td>
	<td class="line x" title="26:194	(Bacchiani et al., 2006) carried out a similar experiment on sentences of all lengths, and (McClosky et al., 2006) report additional results." ></td>
	<td class="line x" title="27:194	The table below shows results from our own measurements of Charniak parser1 (Charniak and Johnson, 2005) accuracy (F-measure on sentences of all lengths), which are consistent with these studies." ></td>
	<td class="line x" title="28:194	For the Brown corpus, the test set was formed from every tenth sentence in the corpus (Gildea, 2001)." ></td>
	<td class="line x" title="29:194	TrainingSet TestSet Sent." ></td>
	<td class="line x" title="30:194	count Charniak accuracy WSJsec." ></td>
	<td class="line x" title="31:194	02-21 WSJsec." ></td>
	<td class="line x" title="32:194	24 1308 90.48 (39,832sent.)" ></td>
	<td class="line x" title="33:194	WSJsec." ></td>
	<td class="line x" title="34:194	23 2343 91.13 Brown-test 2186 86.34 Here we investigate algorithms for predicting the accuracy of a parser P on sentences, chunks of sentences, and whole corpora." ></td>
	<td class="line x" title="35:194	We also investigate and contrast several scenarios for prediction: (1) the predictor looks at the input text only, (2) the predictor looks at the input text and the output parse trees of P, and (3) the predictor looks at the input text, the output parse trees of P, and the outputs of other programs, such as the output parse trees of a different parser Pref used as a reference." ></td>
	<td class="line x" title="36:194	Under none of these scenarios is the predictor allowed to look at goldstandard parses in the new domain/genre." ></td>
	<td class="line x" title="37:194	The intuition behind what we are trying to achieve here can be compared to an analogous tasktrying to assess the performance of a median student from a math class on a given test, without having access to the answer sheet." ></td>
	<td class="line x" title="38:194	Looking at the test only, we could probably tell whether the test looks hard or not, and therefore whether the student will do well or not." ></td>
	<td class="line x" title="39:194	Looking at the students answers will likely give us an even better idea of the performance." ></td>
	<td class="line x" title="40:194	Finally, the answers of a second student with similar proficiency will provide even better clues: if the students agree on every answer, then they probably both did well, but if they disagree frequently, then they (and hence our student) probably did not do as well." ></td>
	<td class="line x" title="41:194	Our first experiments are concerned with validating the idea itself: can a predictor be trained such 1Downloaded from ftp.cs.brown.edu/pub/nlparser/rerankingparserAug06.tar.gz in February, 2007." ></td>
	<td class="line x" title="42:194	888 that it predicts the same F-scores as the ones obtained using gold-trees?" ></td>
	<td class="line x" title="43:194	We first validate this using the WSJ corpus itself, by dividing the WSJ treebank into several sections: 1." ></td>
	<td class="line x" title="44:194	Training (WSJ section 02-21)." ></td>
	<td class="line x" title="45:194	The parser P is trained on this data." ></td>
	<td class="line x" title="46:194	2." ></td>
	<td class="line x" title="47:194	Development (WSJ section 24)." ></td>
	<td class="line x" title="48:194	We use this data for training our predictor." ></td>
	<td class="line x" title="49:194	3." ></td>
	<td class="line x" title="50:194	Test (WSJ section 23)." ></td>
	<td class="line x" title="51:194	We use this data for measuring our predictions." ></td>
	<td class="line x" title="52:194	For each test sentence, we compute (1) the PARSEVAL F-measure score using the test gold standard, and (2) our predicted F-measure." ></td>
	<td class="line x" title="53:194	We report the correlation coefficient (r) between the actual F-scores and our predicted Fscores." ></td>
	<td class="line x" title="54:194	We will also use a root-mean-square error (rms error) metric to compare actual and predicted F-scores." ></td>
	<td class="line x" title="55:194	Section 3 describes the features used by our predictor." ></td>
	<td class="line x" title="56:194	Given these features, as well as actual F-scores computed for the development data, we use supervised learning to set the feature weights." ></td>
	<td class="line x" title="57:194	To this end, we use SVM-Regression2 (Smola and Schoelkopf, 1998) with an RBF kernel, to learn the feature weights and build our predictor system.3 We validate the accuracy of the predictor trained in this fashion on both WSJ (Section 4) and the Brown corpus (Section 5)." ></td>
	<td class="line x" title="58:194	3 Features Used for Predicting Parser Accuracy 3.1 Text-based Features One hypothesis we explore is that (all other things being equal) longer sentences are harder to parse correctly than shorter sentences." ></td>
	<td class="line x" title="59:194	When exposed to the development set, SVM-Regression learns weights to best predict F-scores using the values for this feature corresponding to each sentence in the corpus." ></td>
	<td class="line x" title="60:194	Does the predicted F-score correlate with actual F-score on a sentence by sentence basis?" ></td>
	<td class="line x" title="61:194	There was a positive but weak correlation: 2Weka software (http://www.cs.waikato.ac.nz/ml/weka/) 3We compared a few regression algorithms like SVMRegression (using different kernels and parameter settings) and Multi-Layer Perceptron (neural networks)  we trained the algorithms separately on dev data and picked the one that gave the best cross-validation accuracy (F-measure)." ></td>
	<td class="line x" title="62:194	Featureset dev(r) test(r) Length 0.13 0.19 Another hypothesis is that the parser performance is influenced by the number of UNKNOWN words in the sentence to be parsed, i.e., the number of words in the test sentence that were never seen before in the training set." ></td>
	<td class="line x" title="63:194	Training the predictor with this feature produces a positive correlation, slightly weaker compared to the Length feature." ></td>
	<td class="line x" title="64:194	Featureset dev(r) test(r) UNK 0.11 0.11 Unknown words are not the only ones that can influence the performance of a parser." ></td>
	<td class="line x" title="65:194	Rare words, for which statistical models do not have reliable estimates, are also likely to impact parsing accuracy." ></td>
	<td class="line x" title="66:194	To test this hypothesis, we add a language model perplexitybased (LM-PPL) feature." ></td>
	<td class="line x" title="67:194	We extract the yield of the training trees, on which we train a trigram language model.4 We compute the perplexity of each test sentence with respect to this language model, and use it as feature in our predictor system." ></td>
	<td class="line x" title="68:194	Note that this feature is meant as a refinement of the previous UNK feature, in the sense that perplexity numbers are meant to signal the occurrence of unknown words, as well as rare (from the training data perspective) words." ></td>
	<td class="line x" title="69:194	However, the correlation we observe for this feature is similar to the correlation observed for the UNK feature, which seems to suggest that the smoothing techniques used by the parsers employed in these experiments lead to correct treatment of the rare words." ></td>
	<td class="line x" title="70:194	Featureset dev(r) test(r) LM-PPL 0.11 0.10 We also look at the possibility of automatically detecting certain cue words that are appropriate for our prediction problem." ></td>
	<td class="line x" title="71:194	That is, we want to see if we can detect certain words that have a discriminating power in deciding whether parsing a sentence that contains them is difficult or easy." ></td>
	<td class="line x" title="72:194	To this end, we use a subset of the development data, which contains the 200 best-parsed and 200 worst-parsed sentences (based on F-measure scores)." ></td>
	<td class="line x" title="73:194	For each word in the development dataset, we compute the information gain (IG) (Yang and Pedersen, 1997) score for that word with respect to the best/worst parsed 4We trained using the SRILM language modeling toolkit, with default settings." ></td>
	<td class="line x" title="74:194	889 dataset." ></td>
	<td class="line x" title="75:194	These words are then ranked by their IG scores, and the top 100 words are included as lexical features in our predictor system." ></td>
	<td class="line x" title="76:194	As expected, the correlation on the development set is quite high (given that these lexical cues are extracted from this particular set), but a positive correlation holds for the test set as well." ></td>
	<td class="line x" title="77:194	Featureset dev(r) test(r) lexCount100 0.43 0.18 3.2 Parser Pbased Features Besides exploiting the information present in the input text, we can also inspect the output tree of the parser P for which we are interested in predicting accuracy." ></td>
	<td class="line x" title="78:194	We create a rootSYN feature based on the syntactic category found at the root of the output tree (is it S?, is it FRAG?)." ></td>
	<td class="line x" title="79:194	We also create a puncSYN feature based on the number of words labeled as punctuation tags (based on the intuition that heavy use of punctuation can be indicative of the difficulty of the input sentences), and a labelSYN feature in which we bundled together information regarding the number of internal nodes in the parse tree output that have particular labels (how many nodes are labeled with PP?)." ></td>
	<td class="line x" title="80:194	In our predictor, we use 72 such labelSYN features corresponding to all the syntactic labels found in the parse tree output for the development set." ></td>
	<td class="line x" title="81:194	The test set correlation given by the rootSYN and the labelSYN features are higher than some of the text-based features, whereas the puncSYN feature seems to have little discriminative power." ></td>
	<td class="line x" title="82:194	Featureset dev(r) test(r) rootSYN 0.21 0.17 puncSYN 0.09 0.01 labelSYN 0.33 0.28 3.3 Reference Parser Pref based Features In addition to the text-based features and parser P based features, we can bring in an additional parser Pref whose output is used as a reference against which the output of parser P is measured." ></td>
	<td class="line x" title="83:194	For the reference parser feature, our goal is to measure how similar/different are the results from the two parsers." ></td>
	<td class="line x" title="84:194	We find that if the parses are similar, they are more likely to be right." ></td>
	<td class="line x" title="85:194	In order to compute similarity, we can compare the constituents in the two parse trees from P and Pref , and see how many constituents match." ></td>
	<td class="line x" title="86:194	This is most easily accomplished by considering Pref to be a gold standard (even though it is not necessarily a correct parse) and computing the F-measure score of parser P against Pref . We use this F-measure score as a feature for prediction." ></td>
	<td class="line x" title="87:194	For the experiments presented in this section we use as Pref , the parser from (Bikel, 2002)." ></td>
	<td class="line x" title="88:194	Intuitively, the requirement for choosing parser Pref in conjunction with parser P seems to be that they are different enough to produce non-identical trees when presented with the same input, and at the same time to be accurate enough to produce reliable parse trees." ></td>
	<td class="line x" title="89:194	The choice of P as (Charniak and Johnson, 2005) and Pref as (Bikel, 2002) fits this bill, but many other choices can be made regarding Pref , such as (Klein and Manning, 2003; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008)." ></td>
	<td class="line x" title="90:194	We leave the task of creating features based on the consensus of multiple parsers as future work." ></td>
	<td class="line x" title="91:194	The correlation given by the reference parser based feature Pref on the test set is the highest among all the features we explored." ></td>
	<td class="line x" title="92:194	Featureset dev(r) test(r) Pref 0.40 0.36 3.4 The Aggregated Power of Features The table below lists all the individual features we have described in this section, sorted according to the correlation value obtained on the test set." ></td>
	<td class="line x" title="93:194	Featureset dev(r) test(r) Pref 0.40 0.36 labelSYN 0.33 0.28 lexCount500 0.56 0.23 lexBool500 0.58 0.20 lexCount1000 0.67 0.20 lexBool1000 0.58 0.20 Length 0.13 0.19 lexCount100 0.43 0.18 lexBool100 0.43 0.18 rootSYN 0.21 0.17 UNK 0.11 0.11 LM-PPL 0.11 0.10 puncSYN 0.09 0.01 Note how the lexical features tend to over-fit the development datathe words were specifically chosen for their discriminating power on that particular set." ></td>
	<td class="line x" title="94:194	Hence, adding more lexical features to the predictor system improves the correlation on development (due to over-fitting), but it does not produce consistent improvement on the test set." ></td>
	<td class="line x" title="95:194	However, 890 Method (using 3 features: Length,UNK,Pref ) # of random restarts dev(r) SVMRegression 0.42 1 0.138 5 0.136 MaximumCorrelation 10 0.166 Training(MCT) 25 0.178 100 0.232 1000 0.27 10,000 0.401 Table 1: Comparison of correlation (r) obtained using MCT versus SVM-Regression on development corpus." ></td>
	<td class="line x" title="96:194	there is some indication that the counts of the lexical features are important, and count-based lexical features tend to have similar or better performance compared to their boolean-based counterparts." ></td>
	<td class="line x" title="97:194	Since these features measure different but overlapping pieces of the information available, it is to be expected that some of the feature combinations would provide better correlation that the individual features, but the gains are not strictly additive." ></td>
	<td class="line x" title="98:194	By taking the individual features that provide the best discriminative power, we are able to get a correlation score of 0.42 on the test set." ></td>
	<td class="line x" title="99:194	Featureset dev(r) test(r) Pref + labelSYN+ Length+ lexCount100+ rootSYN+ UNK+ LM-PPL 0.55 0.42 3.5 Optimizing for Maximum Correlation If our goal is to obtain the highest correlations with the F-score measure, is SVM regression the best method?" ></td>
	<td class="line x" title="100:194	Liu and Gildea (2007) recently introduced Maximum Correlation Training (MCT), a search procedure that follows the gradient of the formula for correlation coefficient (r)." ></td>
	<td class="line x" title="101:194	We implemented MCT, but obtained no better results." ></td>
	<td class="line x" title="102:194	Moreover, it required many random re-starts just to obtain results comparable to SVM regression (Table 1)." ></td>
	<td class="line x" title="103:194	4 Predicting Accuracy on Multiple Sentences The results for the scenario presented in Section 3 are encouraging, but other scenarios are also important from a practical perspective." ></td>
	<td class="line x" title="104:194	For instance, we are interested in predicting the performance of a particular parser not on a sentence-by-sentence basis, but for a representative chunk of sentences from the new domain." ></td>
	<td class="line x" title="105:194	In order to predict the F-measure on multiple sentences, we modify our feature set to generate information on a whole chunk of sentences Sentences in chunk(n) WSJ-test(r) WSJ-test (rmserror) 1 0.42 0.098 20 0.61 0.026 50 0.62 0.019 100 0.69 0.015 500 0.79 0.011 Table 2: Performance of predictor on n-sentence chunks from WSJ-test (Correlation and rms error between actual/predicted accuracies)." ></td>
	<td class="line x" title="106:194	rather than a single sentence." ></td>
	<td class="line x" title="107:194	Predicting the correlation at chunk level is, not unexpectedly, an easier problem than predicting correlation at sentence level, as the results in the first two columns of Table 2 show." ></td>
	<td class="line x" title="108:194	For 100-sentence chunks, we also plot the predicted accuracies versus actual accuracies for the WSJ-test set in Figure 1." ></td>
	<td class="line x" title="109:194	This scatterplot brings to light an artifact of using correlation metric (r) for evaluating our predictors performance." ></td>
	<td class="line x" title="110:194	Although our objective is to improve correlation between actual and predicted F-scores, the correlation metric (r) does not tell us directly how well the predictor is doing." ></td>
	<td class="line x" title="111:194	In Figure 1, the system predicts that on an average, most sentence chunks can be parsed with an accuracy of 0.9085 (which is the mean predicted F-score on WSJ-test)." ></td>
	<td class="line x" title="112:194	But the range of predictions from our system [0.89,0.92] is smaller than the actual F-score range [0.86,0.95]." ></td>
	<td class="line x" title="113:194	Hence, even though the correlation scores are high, this does not necessarily mean that our predictions are on target." ></td>
	<td class="line x" title="114:194	An additional metric, root-mean-square (rms) error, which measures the distance between actual and predicted F-measures, can be used to gauge the quality of our predictions." ></td>
	<td class="line x" title="115:194	For a particular chunk-size, lowering the rms error translates into aligning the points of a scatterplot as the one in Figure 1, closer to the x=y line, implying that the predictor is getting better at exactly predicting the F-score values." ></td>
	<td class="line x" title="116:194	The third column in Table 2 shows the rms error for our predictor at different chunk sizes." ></td>
	<td class="line x" title="117:194	The results using this metric also show that the prediction problem becomes easier as the chunk size increases." ></td>
	<td class="line x" title="118:194	Assuming that we have the test set of WSJ section 23, but without the gold-standard trees, how can we get an approximation for the overall accuracy of a parser P on this test set?" ></td>
	<td class="line x" title="119:194	One possibility, which we use here as a baseline, is to compute the F-score on a set for which we do have gold-standard trees." ></td>
	<td class="line x" title="120:194	If we use our development set (WSJ section 891  0.85  0.86  0.87  0.88  0.89  0.9  0.91  0.92  0.93  0.94  0.95  0.85 0.86 0.87 0.88 0.89 0.9 0.91 0.92 0.93 0.94 0.95 Actual Accuracy Predicted Accuracy per-chunk-accuracyx=y line Fitted-line Figure 1: Plot showing Actual vs. Predicted accuracies for WSJ-test (100-sentence chunks)." ></td>
	<td class="line x" title="121:194	Each plot point represents a 100-sentence chunk." ></td>
	<td class="line x" title="122:194	(rms error = 0.015)  0.85  0.86  0.87  0.88  0.89  0.9  0.91  0.92  0.93  0.94  0.95  0.85 0.86 0.87 0.88 0.89 0.9 0.91 0.92 0.93 0.94 0.95 Actual Accuracy Predicted Accuracy per-chunk-accuracyx=y line Figure 2: Plot showing Actual vs. Adjusted Predicted accuracies (shifting with  = 0.757, skewing with  = 1.0) for WSJ-test (100-sentence chunks)." ></td>
	<td class="line x" title="123:194	(rms error = 0.014) System F-measure CharniakF-measureon WSJ-dev (baseline) 90.48(fd) Predictor(featureweightsset withWSJ-dev) 90.85(fp) ActualCharniakaccuracy 91.13(ft) Table 3: Comparing Charniak parser accuracy (from different systems) on entire WSJ-test corpus 24) for this purpose, and (Charniak and Johnson, 2005) as the parser P, the baseline is an F-score of 90.48 (fd), which is the actual Charniak parser accuracy on WSJ section 24." ></td>
	<td class="line x" title="124:194	Instead, if we run our predictor on the test set (a single chunk containing all the sentences in the test set), it predicts an F-score of 90.85 (fp)." ></td>
	<td class="line x" title="125:194	These two predictions are listed as the first two rows in Table 3." ></td>
	<td class="line x" title="126:194	Of course, having the actual gold-standard trees for WSJ section 23 helps us decide which prediction is better: the actual accuracy of the Charniak parser on WSJ section 23 is an F-score of 91.13 (ft), which makes our prediction better than the baseline." ></td>
	<td class="line x" title="127:194	4.1 Shifting Predictions to Match Actual Accuracy We correctly predict (in Table 3) that the WSJ-test is easier to parse than the WSJdev (90.85 > 90.48)." ></td>
	<td class="line x" title="128:194	However, our predictor is too conservativethe WSJ-test is actually even easier to parse (91.13 > 90.85)." ></td>
	<td class="line x" title="129:194	We can fix this by shifting the mean predicted F-score (which is equal to fp) further away from the dev F-measure (fd), and closer to the actual F-measure (ft)." ></td>
	<td class="line x" title="130:194	This is achieved by shifting all the individual predictions by a certain amount as shown below." ></td>
	<td class="line x" title="131:194	Letpbe an individual prediction from our system." ></td>
	<td class="line x" title="132:194	The shifted prediction pprime is given by: pprime = p+(fpfd) (1) We can tune  to make the new mean prediction (fprimep) to be equal to the actual F-measure (ft)." ></td>
	<td class="line x" title="133:194	fprimep = fp +(fpfd) (2)  = ftfpf pfd (3) Using the F-score values from Table 3, we get an  = 0.757 and an exact prediction of 91.13." ></td>
	<td class="line x" title="134:194	Of course, this is because we tune on test, so we need to validate this idea on a new test set to see if it leads to improved predictions (Section 5)." ></td>
	<td class="line x" title="135:194	4.2 Skewing to Widen Prediction Range Our predictor is also too conservative about its distribution (see Figure 1)." ></td>
	<td class="line x" title="136:194	It knows (roughly) which chunks are easier to parse and which are harder, but its range of predictions is lower than the range of actual F-measure scores." ></td>
	<td class="line x" title="137:194	We can skew individual predictions so that sentences predicted to be easy are re-predicted to be even easier (and those that are hard to be even harder)." ></td>
	<td class="line x" title="138:194	For each prediction pprime (from Equation 1), we compute pprimeprime = pprime+(pprimefprimep) (4) We simply set  to 1.0, doubling the distance of each prediction pprime (in Equation 1) from the (adjusted) mean predictionfprimep, to obtain the skewed prediction pprimeprime." ></td>
	<td class="line x" title="139:194	Figure 2 shows how the points representing 100sentence chunks in Figure 1 look after the predictions have been shifted ( = 0.757) and skewed ( = 1.0)." ></td>
	<td class="line x" title="140:194	These two operations have the desired effect of changing the range of predictions from [0.89,0.92] to [0.87,0.94], much closer to the actual 892 Sentences in chunk (n) WSJ-test (rmserror) Brown-test Prediction (rmserror) Brown-test Adjusted Prediction (rmserror) 1 0.098 0.129 0.139 20 0.026 0.039 0.036 50 0.019 0.032 0.029 100 0.015 0.025 0.020 500 0.011 0.038 0.024 Table 4: Performance of predictor on n-sentence chunks from WSJ-test and Brown-test (rms error between actual/predicted accuracies)." ></td>
	<td class="line x" title="141:194	range of [0.86,0.95]." ></td>
	<td class="line x" title="142:194	The points in the new plot (Figure 2) also align closer to the x=y line than in the original graph (Figure 1)." ></td>
	<td class="line x" title="143:194	The rms error also drops from 0.015 to 0.014 (7% relative reduction), showing that the predictions have improved." ></td>
	<td class="line x" title="144:194	Since we use the WSJ-test corpus to tune the parameter values for shifting and skewing, we need to apply our predictor on a different test set to see if we get similar improvements by using these techniques, which we do in the next section." ></td>
	<td class="line x" title="145:194	5 Predicting Accuracy on the Brown Corpus The Brown corpus represents a genuine challenge for our predictor, as it presents us with the opportunity to test the performance of our predictor in an out-of-domain scenario." ></td>
	<td class="line x" title="146:194	Our predictor, trained on WSJ data, is now employed to predict the performance of a WSJ-trained parser P on the Browntest corpus." ></td>
	<td class="line x" title="147:194	As in the previous experiments, we use (Charniak and Johnson, 2005) trained on WSJ sections 02-21 as parser P. The feature weights for our predictor are again trained on section 24 of WSJ, and the shifting and skewing parameters ( = 0.757,  = 1.0) are determined using section 23 of WSJ." ></td>
	<td class="line x" title="148:194	The results on the Brown-test, both the original predictions and after they have been adjusted (shifted/skewed), are shown in Table 4, at different level of chunking." ></td>
	<td class="line x" title="149:194	For chunks of size n > 1, the shifting and skewing techniques help in lowering the rms error." ></td>
	<td class="line x" title="150:194	On 100-sentence chunks from the Brown test, shifting and skewing ( = 0.757,  = 1.0) leads to a 20% relative reduction in the rms error." ></td>
	<td class="line x" title="151:194	In a similar vein with the evaluation done in Section 4, we are interested in estimating the overall accuracy of a WSJ-trained parser P given an out-ofdomain set such as the Brown test set (for which, at least for now, we do not have access to gold-standard System F-measure Baseline1(F-measureon WSJsec." ></td>
	<td class="line x" title="152:194	23) 91.13 Baseline2(F-measureon WSJsec." ></td>
	<td class="line x" title="153:194	24) 90.48 Predictor(base) 88.48 AdjustedPredictor(shiftingusing= 0.757) 86.96 Actualaccuracy 86.34 Table 5: Charniak parser accuracy on entire Brown-test corpus trees)." ></td>
	<td class="line x" title="154:194	If we use (Charniak and Johnson, 2005) as parser P, a cheap and readily-available answer is to approximate the performance using the Charniak parser performance on WSJ section 23, which has an F-score of 91.13." ></td>
	<td class="line x" title="155:194	Another cheap and readilyavailable answer is to take the Charniak parser performance on WSJ section 24 with an F-score of 90.48." ></td>
	<td class="line x" title="156:194	Table 5 lists these baselines, along with the prediction made by our system when using a single chunk containing all the sentences in the Brown test set (both base predictions and adjusted predictions, i.e. shifting using  = 0.757)." ></td>
	<td class="line x" title="157:194	Again, having goldstandard trees for the Brown test set helps us decide which prediction is better." ></td>
	<td class="line x" title="158:194	Our predictions are much closer to the actual Charniak parser performance on the Brown-test set, with the adjusted prediction at 86.96 compared to the actual F-score of 86.34." ></td>
	<td class="line x" title="159:194	6 Ranking Parser Performance One of the main goals for computing F-score figures (either by traditional PARSEVAL evaluation against gold standards or by methods such as the one proposed in this paper) is to compare parsing accuracy when confronted with a choice between various parser deployments." ></td>
	<td class="line x" title="160:194	Not only are there many parsing techniques available (Collins, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008), but recent annotation efforts in providing training material for statistical parsing (LDC, 2005; LDC, 2006a; LDC, 2006b; LDC, 2006c; LDC, 2007) have compounded the difficulty of the choices (Do I parse using parser X?, Do I train parser X using the treebank Y or Z?)." ></td>
	<td class="line x" title="161:194	In this section, we show how our predictor can provide guidance when dealing with some of these choices, namely the choice of the training material to use with a statistical parser, prior to its application in an NLP task." ></td>
	<td class="line x" title="162:194	For the experiments reported in this paper, we use as parser P, our in-house implementation of the Collins parser (Collins, 2003), to which various 893 speed-related enhancements (Goodman, 1997) have been applied." ></td>
	<td class="line x" title="163:194	This choice has been made to better reflect a scenario in which parser P would be used in a data-intensive application such as syntax-driven machine translation, in which the parser must be able to run through hundreds of millions of training words in a timely manner." ></td>
	<td class="line x" title="164:194	We use the more accurate, but slower Charniak parser (Charniak and Johnson, 2005) as the reference parser Pref in our predictor (see Section 3.3)." ></td>
	<td class="line x" title="165:194	In order to predict the Collinsstyle parser behavior on the ranking task, we use the same predictor model (including feature weights and adjustment parameters) that was used for predicting Charniak parser behavior on the Brown corpus (Section 5)." ></td>
	<td class="line x" title="166:194	We compare three training scenarios that make for three different parsers: (1) PWSJ trained on sections 02-21 of WSJ." ></td>
	<td class="line x" title="167:194	(2) PNews trained on the union of the English Chinese Translation Treebank (LDC, 2007) (news stories from Xinhua News Agency translated from Chinese into English) and the English Newswire Translation Treebank (LDC, 2005; LDC, 2006a; LDC, 2006b; LDC, 2006c) (An-Nahar new stories translated from Arabic into English)." ></td>
	<td class="line x" title="168:194	(3) PWSJNews trained on the union of all the above training material." ></td>
	<td class="line x" title="169:194	When comparing the performance of these three parsers on a development set from WSJ (section 0), we get the following F-scores.5 Parser WSJ(sec.0)Accuracy (F-scores) PWSJ 88.25 PNews 83.00 PWSJNews 88.00 Consider now that we are interested in comparing the parsing accuracy of these parsers on a domain completely different from WSJ." ></td>
	<td class="line x" title="170:194	The ranking PWSJ>PWSJNews>PNews, given by the evaluation above, provides some guidance, but is this guidance accurate?" ></td>
	<td class="line x" title="171:194	The intuition here is that the information that we already have about the new domain of interest (which implicitly appears in texts 5Because of tokenization differences between the different treebanks involved in these experiments, we have to adopt a tokenization scheme different from the one used in the Penn Treebank, and therefore the F-scores, albeit in the same range, are not directly comparable with the ones in the parsing literature." ></td>
	<td class="line x" title="172:194	Parser Xinhua News Prediction (F-scores) Xinhua News Accuracy (F-scores) PWSJ 85.1 79.14 PNews 87.0 84.84 PWSJNews 89.4 85.14 Table 6: Performance of predictor on the Xinhua News domain, compared with actual F-scores." ></td>
	<td class="line x" title="173:194	extracted from this domain), can be used to better guide this decision." ></td>
	<td class="line x" title="174:194	Our predictor is able to capitalize on this information, and provide domaininformed guidance for choosing the most accurate parser to use with the new data, which in this case relates to choosing the best training strategy for the parser P. If we consider as our domain of interest, news stories from Xinhua News Agency, then using our predictor on a chunk of 1866 sentences from this domain gives the F-scores shown in the second column of Table 6." ></td>
	<td class="line x" title="175:194	As with the previous experiments, we can compute the actual PARSEVAL F-scores (using goldstandard) for this particular 1866-sentence test set, as it happens to be part of the English Chinese Translation Treebank (LDC, 2007)." ></td>
	<td class="line x" title="176:194	These F-score figures are shown in the third column of Table 6." ></td>
	<td class="line x" title="177:194	As these results show, for this particular domain the correct ranking is PWSJNews>PNews>PWSJ, which is exactly the ranking predicted by our method, without the aid of gold-standard trees." ></td>
	<td class="line x" title="178:194	We observe that even though the system predicts the ranking correctly, the predictions in the Xinhua News domain might not be as accurate in comparison to the predictions on Brown corpus (predicted F-score = 86.96, actual F-score = 86.34)." ></td>
	<td class="line x" title="179:194	One possible reason for this lower accuracy is that we use the same prediction model without optimizing for the particular parser on which we wish to make predictions." ></td>
	<td class="line x" title="180:194	Still, the model was able to make distinctions between multiple parsers for the ranking task correctly, and decide the best parser to use with the given data." ></td>
	<td class="line x" title="181:194	We believe this to be useful in typical NLP applications which use parsing as a component, and where making the right choice between different parsers can affect the end-to-end accuracy of the system." ></td>
	<td class="line x" title="182:194	7 Conclusion The steady advances in statistical parsing over the last years have taken this technology to the point 894 where it is accurate enough to be useful in a variety of natural language applications." ></td>
	<td class="line x" title="183:194	However, due to large variations in the characteristics of the domains for which these applications are developed, estimating parsing accuracy becomes more involved than simply taking for granted accuracy estimates done on a certain well-studied domain, such as WSJ." ></td>
	<td class="line x" title="184:194	As the results in this paper show, it is possible to take into account these variations in the domain characteristics (encoded in our predictor as text-based, syntax-based, and agreement-based features)to make better predictions about the accuracy of certain statistical parsers (and under different training scenarios), instead of relying on accuracy estimates done on a standard domain." ></td>
	<td class="line x" title="185:194	We have provided a mechanism to incorporate these domain variations for making predictions about parsing accuracy, without the costly requirement of creating human annotations for each of the domains of interest." ></td>
	<td class="line x" title="186:194	The experiments shown in the paper were limited to readily available statistical parsers (which are widely deployed in a number of applications), and certain domains/genres (because of ready access to gold-standard data on which we could verify predictions)." ></td>
	<td class="line x" title="187:194	However, the features we use in our predictor are independent of the particular type of parser or domain, and the same technique could be applied for making predictions on other parsers as well." ></td>
	<td class="line x" title="188:194	There are many avenues for future work opened up by the work presented here." ></td>
	<td class="line x" title="189:194	The accuracy of the predictor can be further improved by incorporating more complex syntax-based features and multipleagreement features." ></td>
	<td class="line x" title="190:194	Moreover, rather than predicting an intrinsic metric such as the PARSEVAL Fscore, the metric that the predictor learns to predict can be chosen to better fit the final metric on which an end-to-end system is measured, in the style of (Och, 2003)." ></td>
	<td class="line x" title="191:194	The end-result is a finely-tuned tool for predicting the impact of various parser design decisions on the overall quality of a system." ></td>
	<td class="line x" title="192:194	8 Acknowledgements We wish to acknowledge our colleagues at ISI, who provided useful suggestions and constructive criticism on this work." ></td>
	<td class="line x" title="193:194	We are also grateful to all the reviewers for their detailed comments." ></td>
	<td class="line x" title="194:194	This work was supported in part by NSF grant IIS-0428020." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-1023
Forest-Based Translation
Mi, Haitao;Huang, Liang;Liu, Qun;"></td>
	<td class="line x" title="1:135	Proceedings of ACL-08: HLT, pages 192199, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:135	c2008 Association for Computational Linguistics Forest-Based Translation Haitao Mi Liang Huang Qun Liu Key Lab." ></td>
	<td class="line x" title="3:135	of Intelligent Information Processing Department of Computer & Information Science Institute of Computing Technology University of Pennsylvania Chinese Academy of Sciences Levine Hall, 3330 Walnut Street P.O. Box 2704, Beijing 100190, China Philadelphia, PA 19104, USA {htmi,liuqun}@ict.ac.cn lhuang3@cis.upenn.edu Abstract Among syntax-based translation models, the tree-based approach, which takes as input a parse tree of the source sentence, is a promising direction being faster and simpler than its string-based counterpart." ></td>
	<td class="line x" title="4:135	However, current tree-based systems suffer from a major drawback: they only use the 1-best parse to direct the translation, which potentially introduces translation mistakes due to parsing errors." ></td>
	<td class="line x" title="5:135	We propose a forest-based approach that translates a packed forest of exponentially many parses, which encodes many more alternatives than standard n-best lists." ></td>
	<td class="line x" title="6:135	Large-scale experiments show an absolute improvement of 1.7 BLEU points over the 1-best baseline." ></td>
	<td class="line x" title="7:135	This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time." ></td>
	<td class="line x" title="8:135	1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years." ></td>
	<td class="line oc" title="9:135	Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006)." ></td>
	<td class="line x" title="10:135	Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tree substitution grammar for the latter (Huang et al., 2006)." ></td>
	<td class="line x" title="11:135	However, despite these advantages, current tree-based systems suffer from a major drawback: they only use the 1best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors (Quirk and Corston-Oliver, 2006)." ></td>
	<td class="line x" title="12:135	This situation becomes worse with resource-poor source languages without enough Treebank data to train a high-accuracy parser." ></td>
	<td class="line x" title="13:135	One obvious solution to this problem is to take as input k-best parses, instead of a single tree." ></td>
	<td class="line x" title="14:135	This kbest list postpones some disambiguation to the decoder, which may recover from parsing errors by getting a better translation from a non 1-best parse." ></td>
	<td class="line x" title="15:135	However, a k-best list, with its limited scope, often has too few variations and too many redundancies; for example, a 50-best list typically encodes a combination of 5 or 6 binary ambiguities (since 25 < 50 < 26), and many subtrees are repeated across different parses (Huang, 2008)." ></td>
	<td class="line x" title="16:135	It is thus inefficient either to decode separately with each of these very similar trees." ></td>
	<td class="line x" title="17:135	Longer sentences will also aggravate this situation as the number of parses grows exponentially with the sentence length." ></td>
	<td class="line x" title="18:135	We instead propose a new approach, forest-based translation (Section 3), where the decoder translates a packed forest of exponentially many parses,1 1There has been some confusion in the MT literature regarding the term forest: the word forest in forest-to-string rules 192 VP PP P yu x1:NPB VPB VV juxng AS le x2:NPB held x2 with x1 Figure 1: An example translation rule (r3 in Fig." ></td>
	<td class="line x" title="19:135	2)." ></td>
	<td class="line x" title="20:135	which compactly encodes many more alternatives than k-best parses." ></td>
	<td class="line x" title="21:135	This scheme can be seen as a compromise between the string-based and treebased methods, while combining the advantages of both: decoding is still fast, yet does not commit to a single parse." ></td>
	<td class="line x" title="22:135	Large-scale experiments (Section 4) show an improvement of 1.7 BLEU points over the 1-best baseline, which is also 0.8 points higher than decoding with 30-best trees, and takes even less time thanks to the sharing of common subtrees." ></td>
	<td class="line x" title="23:135	2 Tree-based systems Current tree-based systems perform translation in two separate steps: parsing and decoding." ></td>
	<td class="line x" title="24:135	A parser first parses the source language input into a 1-best tree T, and the decoder then searches for the best derivation (a sequence of translation steps) d that converts source tree T into a target-language string among all possible derivations D: d = argmax dD P(d|T)." ></td>
	<td class="line x" title="25:135	(1) We will now proceed with a running example translating from Chinese to English: (2) A3FC B`ush Bush  yu with/and E9E9 Shalong Sharon1 BQC4 juxng hold DQ le pass." ></td>
	<td class="line x" title="26:135	AQA8 hu`tan talk2 Bush held a talk2 with Sharon1 Figure 2 shows how this process works." ></td>
	<td class="line x" title="27:135	The Chinese sentence (a) is first parsed into tree (b), which will be converted into an English string in 5 steps." ></td>
	<td class="line x" title="28:135	First, at the root node, we apply rule r1 preserving top-level word-order between English and Chinese, (r1) IP(x1:NPB x2:VP)x1 x2 (Liu et al., 2007) was a misnomer which actually refers to a set of several unrelated subtrees over disjoint spans, and should not be confused with the standard concept of packed forest." ></td>
	<td class="line x" title="29:135	(a) B`ush [yu Shalong ]1 [juxng le hu`tan ]2 1-best parser(b) IP NPB NR B`ush VP PP P yu NPB NR Shalong VPB VV juxng AS le NPB NN hu`tan r1 (c) NPB NR B`ush VP PP P yu NPB NR Shalong VPB VV juxng AS le NPB NN hu`tan r2 r3 (d) Bush held NPB NN hu`tan with NPB NR Shalong r4 r5 (e) Bush [held a talk]2 [with Sharon]1 Figure 2: An example derivation of tree-to-string translation." ></td>
	<td class="line x" title="30:135	Shaded regions denote parts of the tree that is pattern-matched with the rule being applied." ></td>
	<td class="line x" title="31:135	which results in two unfinished subtrees in (c)." ></td>
	<td class="line x" title="32:135	Then rule r2 grabs the B`ush subtree and transliterate it (r2) NPB(NR(B`ush))Bush." ></td>
	<td class="line x" title="33:135	Similarly, rule r3 shown in Figure 1 is applied to the VP subtree, which swaps the two NPBs, yielding the situation in (d)." ></td>
	<td class="line x" title="34:135	This rule is particularly interesting since it has multiple levels on the source side, which has more expressive power than synchronous context-free grammars where rules are flat." ></td>
	<td class="line x" title="35:135	193 More formally, a (tree-to-string) translation rule (Huang et al., 2006) is a tuplet,s,, where t is the source-side tree, whose internal nodes are labeled by nonterminal symbols in N, and whose frontier nodes are labeled by source-side terminals in  or variables from a setX ={x1,x2,}; s(X) is the target-side string where  is the target language terminal set; and  is a mapping fromX to nonterminals in N. Each variable xi X occurs exactly once in t and exactly once in s. We denoteRto be the translation rule set." ></td>
	<td class="line x" title="36:135	A similar formalism appears in another form in (Liu et al., 2006)." ></td>
	<td class="line oc" title="37:135	These rules are in the reverse direction of the original string-to-tree transducer rules defined by Galley et al.(2004)." ></td>
	<td class="line x" title="39:135	Finally, from step (d) we apply rules r4 and r5 (r4) NPB(NN(hu`tan))a talk (r5) NPB(NR(Shalong))Sharon which perform phrasal translations for the two remaining subtrees, respectively, and get the Chinese translation in (e)." ></td>
	<td class="line x" title="40:135	3 Forest-based translation We now extend the tree-based idea from the previous section to the case of forest-based translation." ></td>
	<td class="line x" title="41:135	Again, there are two steps, parsing and decoding." ></td>
	<td class="line x" title="42:135	In the former, a (modified) parser will parse the input sentence and output a packed forest (Section 3.1) rather than just the 1-best tree." ></td>
	<td class="line x" title="43:135	Such a forest is usually huge in size, so we use the forest pruning algorithm (Section 3.4) to reduce it to a reasonable size." ></td>
	<td class="line x" title="44:135	The pruned parse forest will then be used to direct the translation." ></td>
	<td class="line x" title="45:135	In the decoding step, we first convert the parse forest into a translation forest using the translation rule set, by similar techniques of pattern-matching from tree-based decoding (Section 3.2)." ></td>
	<td class="line x" title="46:135	Then the decoder searches for the best derivation on the translation forest and outputs the target string (Section 3.3)." ></td>
	<td class="line x" title="47:135	3.1 Parse Forest Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar (Billot and Lang, 1989)." ></td>
	<td class="line x" title="48:135	For example, consider the Chinese sentence in Example (2) above, which has (at least) two readings depending on the part-of-speech of the word yu, which can be either a preposition (P with) or a conjunction (CC and)." ></td>
	<td class="line x" title="49:135	The parse tree for the preposition case is shown in Figure 2(b) as the 1-best parse, while for the conjunction case, the two proper nouns (B`ush and Shalong) are combined to form a coordinated NP NPB0,1 CC1,2 NPB2,3 NP0,3 (*) which functions as the subject of the sentence." ></td>
	<td class="line x" title="50:135	In this case the Chinese sentence is translated into (3)  [Bush and Sharon] held a talk." ></td>
	<td class="line x" title="51:135	Shown in Figure 3(a), these two parse trees can be represented as a single forest by sharing common subtrees such as NPB0,1 and VPB3,6." ></td>
	<td class="line x" title="52:135	Such a forest has a structure of a hypergraph (Klein and Manning, 2001; Huang and Chiang, 2005), where items like NP0,3 are called nodes, and deductive steps like (*) correspond to hyperedges." ></td>
	<td class="line x" title="53:135	More formally, a forest is a pairV,E, where V is the set of nodes, and E the set of hyperedges." ></td>
	<td class="line x" title="54:135	For a given sentence w1:l = w1 wl, each node vV is in the form of Xi,j, which denotes the recognition of nonterminal X spanning the substring from positions i through j (that is, wi+1 wj)." ></td>
	<td class="line x" title="55:135	Each hyperedge e  E is a pair tails(e),head(e), where head(e)  V is the consequent node in the deductive step, and tails(e)V  is the list of antecedent nodes." ></td>
	<td class="line x" title="56:135	For example, the hyperedge for deduction (*) is notated: (NPB0,1, CC1,2, NPB2,3), NP0,3." ></td>
	<td class="line x" title="57:135	There is also a distinguished root node TOP in each forest, denoting the goal item in parsing, which is simply S0,l where S is the start symbol and l is the sentence length." ></td>
	<td class="line x" title="58:135	3.2 Translation Forest Given a parse forest and a translation rule setR, we can generate a translation forest which has a similar hypergraph structure." ></td>
	<td class="line x" title="59:135	Basically, just as the depthfirst traversal procedure in tree-based decoding (Figure 2), we visit in top-down order each node v in the 194 (a) IP0,6 NP0,3 NPB0,1 NR0,1 B`ush CC1,2 yu VP1,6 PP1,3 P1,2 NPB2,3 NR2,3 Shalong VPB3,6 VV3,4 juxng AS4,5 le NPB5,6 NN5,6 hu`tan translation rule setR (b) IP0,6 NP0,3 NPB0,1 CC1,2 VP1,6 PP1,3 P1,2 NPB2,3 VPB3,6 VV3,4 AS4,5 NPB5,6 e5 e2 e6 e4 e3 e1 (c) translation hyperedge translation rule e1 r1 IP(x1:NPB x2:VP)x1 x2 e2 r6 IP(x1:NP x2:VPB)x1 x2 e3 r3 VP(PP(P(yu) x1:NPB) VPB(VV(juxng) AS(le) x2:NPB))held x2 with x1 e4 r7 VP(PP(P(yu) x1:NPB) x2:VPB)x2 with x1 e5 r8 NP(x1:NPB CC(yu) x2:NPB)x1 and x2 e6 r9 VPB(VV(juxng) AS(le) x1:NPB)held x1 Figure 3: (a) the parse forest of the example sentence; solid hyperedges denote the 1-best parse in Figure 2(b) while dashed hyperedges denote the alternative parse due to Deduction (*)." ></td>
	<td class="line x" title="60:135	(b) the corresponding translation forest after applying the translation rules (lexical rules not shown); the derivation shown in bold solid lines (e1 and e3) corresponds to the derivation in Figure 2; the one shown in dashed lines (e2, e5, and e6) uses the alternative parse and corresponds to the translation in Example (3)." ></td>
	<td class="line x" title="61:135	(c) the correspondence between translation hyperedges and translation rules." ></td>
	<td class="line x" title="62:135	parse forest, and try to pattern-match each translation rule r against the local sub-forest under node v. For example, in Figure 3(a), at node VP1,6, two rules r3 and r7 both matches the local subforest, and will thus generate two translation hyperedges e3 and e4 (see Figure 3(b-c))." ></td>
	<td class="line x" title="63:135	More formally, we define a function match(r,v) which attempts to pattern-match rule r at node v in the parse forest, and in case of success, returns a list of descendent nodes of v that are matched to the variables in r, or returns an empty list if the match fails." ></td>
	<td class="line x" title="64:135	Note that this procedure is recursive and may 195 Pseudocode 1 The conversion algorithm." ></td>
	<td class="line x" title="65:135	1: Input: parse forest Hp and rule setR 2: Output: translation forest Ht 3: for each node vVp in top-down order do 4: for each translation rule rRdo 5: vars match(r,v)  variables 6: if vars is not empty then 7: evars,v,s(r) 8: add translation hyperedge e to Ht involve multiple parse hyperedges." ></td>
	<td class="line x" title="66:135	For example, match(r3,VP1,6) = (NPB2,3, NPB5,6), which covers three parse hyperedges, while nodes in gray do not pattern-match any rule (although they are involved in the matching of other nodes, where they match interior nodes of the source-side tree fragments in a rule)." ></td>
	<td class="line x" title="67:135	We can thus construct a translation hyperedge from match(r,v) to v for each node v and rule r. In addition, we also need to keep track of the target string s(r) specified by rule r, which includes target-language terminals and variables." ></td>
	<td class="line x" title="68:135	For example, s(r3) = held x2 with x1." ></td>
	<td class="line x" title="69:135	The subtranslations of the matched variable nodes will be substituted for the variables in s(r) to get a complete translation for node v. So a translation hyperedge e is a tripletails(e),head(e),swhere s is the target string from the rule, for example, e3 =(NPB2,3, NPB5,6),VP1,6,held x2 with x1." ></td>
	<td class="line x" title="70:135	This procedure is summarized in Pseudocode 1." ></td>
	<td class="line x" title="71:135	3.3 Decoding Algorithms The decoder performs two tasks on the translation forest: 1-best search with integrated language model (LM), and k-best search with LM to be used in minimum error rate training." ></td>
	<td class="line x" title="72:135	Both tasks can be done efficiently by forest-based algorithms based on k-best parsing (Huang and Chiang, 2005)." ></td>
	<td class="line x" title="73:135	For 1-best search, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) which approximately intersects the translation forest with the LM." ></td>
	<td class="line x" title="74:135	Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Huang and Chiang (2005) to speed up the computation." ></td>
	<td class="line x" title="75:135	An +LM item of node v has the form (vab), where a and b are the target-language boundary words." ></td>
	<td class="line x" title="76:135	For example, (VP held  Sharon1,6 ) is an +LM item with its translation starting with held and ending with Sharon." ></td>
	<td class="line x" title="77:135	This scheme can be easily extended to work with a general n-gram by storing n1 words at both ends (Chiang, 2007)." ></td>
	<td class="line x" title="78:135	For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives." ></td>
	<td class="line x" title="79:135	However, this time we work on a finer-grained forest, called translation+LM forest, resulting from the intersection of the translation forest and the LM, with its nodes being the +LM items during cube pruning." ></td>
	<td class="line x" title="80:135	Although this new forest is prohibitively large, Algorithm 3 is very efficient with minimal overhead on top of 1-best." ></td>
	<td class="line x" title="81:135	3.4 Forest Pruning Algorithm We use the pruning algorithm of (Jonathan Graehl, p.c.; Huang, 2008) that is very similar to the method based on marginal probability (Charniak and Johnson, 2005), except that it prunes hyperedges as well as nodes." ></td>
	<td class="line x" title="82:135	Basically, we use an Inside-Outside algorithm to compute the Viterbi inside cost (v) and the Viterbi outside cost (v) for each node v, and then compute the merit (e) for each hyperedge: (e) = (head(e))+ summationdisplay uitails(e) (ui) (4) Intuitively, this merit is the cost of the best derivation that traverses e, and the difference (e) = (e) (TOP) can be seen as the distance away from the globally best derivation." ></td>
	<td class="line x" title="83:135	We prune away a hyperedge e if (e) > p for a threshold p. Nodes with all incoming hyperedges pruned are also pruned." ></td>
	<td class="line x" title="84:135	4 Experiments We can extend the simple model in Equation 1 to a log-linear one (Liu et al., 2006; Huang et al., 2006): d = argmax dD P(d|T)0e1|d|Plm(s)2e3|s| (5) where T is the 1-best parse, e1|d| is the penalty term on the number of rules in a derivation, Plm(s) is the language model and e3|s| is the length penalty term 196 on target translation." ></td>
	<td class="line x" title="85:135	The derivation probability conditioned on 1-best tree, P(d | T), should now be replaced by P(d|Hp) where Hp is the parse forest, which decomposes into the product of probabilities of translation rules rd: P(d|Hp) = productdisplay rd P(r) (6) where each P(r) is the product of five probabilities: P(r) = P(t|s)4Plex(t|s)5 P(s|t)6Plex(s|t)7 P(t|Hp) 8." ></td>
	<td class="line x" title="86:135	(7) Here t and s are the source-side tree and targetside string of rule r, respectively, P(t | s) and P(s | t) are the two translation probabilities, and Plex() are the lexical probabilities." ></td>
	<td class="line x" title="87:135	The only extra term in forest-based decoding is P(t | Hp) denoting the source side parsing probability of the current translation rule r in the parse forest, which is the product of probabilities of each parse hyperedge ep covered in the pattern-match of t against Hp (which can be recorded at conversion time): P(t|Hp) = productdisplay epHp, ep covered by t P(ep)." ></td>
	<td class="line x" title="88:135	(8) 4.1 Data preparation Our experiments are on Chinese-to-English translation, and we use the Chinese parser of Xiong et al.(2005) to parse the source side of the bitext." ></td>
	<td class="line x" title="90:135	Following Huang (2008), we modify the parser to output a packed forest for each sentence." ></td>
	<td class="line x" title="91:135	Our training corpus consists of 31,011 sentence pairs with 0.8M Chinese words and 0.9M English words." ></td>
	<td class="line x" title="92:135	We first word-align them by GIZA++ refined by diagand from Koehn et al.(2003), and apply the tree-to-string rule extraction algorithm (Galley et al., 2006; Liu et al., 2006), which resulted in 346K translation rules." ></td>
	<td class="line x" title="94:135	Note that our rule extraction is still done on 1-best parses, while decoding is on k-best parses or packed forests." ></td>
	<td class="line x" title="95:135	We also use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a trigram language model with Kneser-Ney smoothing on the English side of the bitext." ></td>
	<td class="line x" title="96:135	We use the 2002 NIST MT Evaluation test set as our development set (878 sentences) and the 2005 0.230 0.232 0.234 0.236 0.238 0.240 0.242 0.244 0.246 0.248 0.250  0  5  10  15  20  25  30  35 BLEU score average decoding time (secs/sentence) 1-best p=5 p=12 k=10 k=30 k=100 k-best trees forests decoding Figure 4: Comparison of decoding on forests with decoding on k-best trees." ></td>
	<td class="line x" title="97:135	NIST MT Evaluation test set as our test set (1082 sentences), with on average 28.28 and 26.31 words per sentence, respectively." ></td>
	<td class="line x" title="98:135	We evaluate the translation quality using the case-sensitive BLEU-4 metric (Papineni et al., 2002)." ></td>
	<td class="line x" title="99:135	We use the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the systems BLEU score on the dev set." ></td>
	<td class="line x" title="100:135	On dev and test sets, we prune the Chinese parse forests by the forest pruning algorithm in Section 3.4 with a threshold of p = 12, and then convert them into translation forests using the algorithm in Section 3.2." ></td>
	<td class="line x" title="101:135	To increase the coverage of the rule set, we also introduce a default translation hyperedge for each parse hyperedge by monotonically translating each tail node, so that we can always at least get a complete translation in the end." ></td>
	<td class="line x" title="102:135	4.2 Results The BLEU score of the baseline 1-best decoding is 0.2325, which is consistent with the result of 0.2302 in (Liu et al., 2007) on the same training, development and test sets, and with the same rule extraction procedure." ></td>
	<td class="line x" title="103:135	The corresponding BLEU score of Pharaoh (Koehn, 2004) is 0.2182 on this dataset." ></td>
	<td class="line x" title="104:135	Figure 4 compares forest decoding with decoding on k-best trees in terms of speed and quality." ></td>
	<td class="line x" title="105:135	Using more than one parse tree apparently improves the BLEU score, but at the cost of much slower decoding, since each of the top-k trees has to be decoded individually although they share many common subtrees." ></td>
	<td class="line x" title="106:135	Forest decoding, by contrast, is much faster 197  0  5  10  15  20  25  0  10  20  30  40  50  60  70  80  90  100 Percentage of sentences (%) i (rank of the parse tree picked by the decoder) forest decoding 30-best trees Figure 5: Percentage of the i-th best parse tree being picked in decoding." ></td>
	<td class="line x" title="107:135	32% of the distribution for forest decoding is beyond top-100 and is not shown on this plot." ></td>
	<td class="line x" title="108:135	and produces consistently better BLEU scores." ></td>
	<td class="line x" title="109:135	With pruning threshold p = 12, it achieved a BLEU score of 0.2485, which is an absolute improvement of 1.6% points over the 1-best baseline, and is statistically significant using the sign-test of Collins et al.(2005) (p < 0.01)." ></td>
	<td class="line x" title="111:135	We also investigate the question of how often the ith-best parse tree is picked to direct the translation (i = 1,2,), in both k-best and forest decoding schemes." ></td>
	<td class="line x" title="112:135	A packed forest can be roughly viewed as a (virtual)-best list, and we can thus ask how often is a parse beyond top-k used by a forest, which relates to the fundamental limitation of k-best lists." ></td>
	<td class="line x" title="113:135	Figure 5 shows that, the 1-best parse is still preferred 25% of the time among 30-best trees, and 23% of the time by the forest decoder." ></td>
	<td class="line x" title="114:135	These ratios decrease dramatically as i increases, but the forest curve has a much longer tail in large i. Indeed, 40% of the trees preferred by a forest is beyond top-30, 32% is beyond top-100, and even 20% beyond top-1000." ></td>
	<td class="line x" title="115:135	This confirms the fact that we need exponentially large kbest lists with the explosion of alternatives, whereas a forest can encode these information compactly." ></td>
	<td class="line x" title="116:135	4.3 Scaling to large data We also conduct experiments on a larger dataset, which contains 2.2M training sentence pairs." ></td>
	<td class="line x" title="117:135	Besides the trigram language model trained on the English side of these bitext, we also use another trigram model trained on the first 1/3 of the Xinhua portion of Gigaword corpus." ></td>
	<td class="line x" title="118:135	The two LMs have disapproach\ruleset TR TR+BP 1-best tree 0.2666 0.2939 30-best trees 0.2755 0.3084 forest (p = 12) 0.2839 0.3149 Table 1: BLEU score results from training on large data." ></td>
	<td class="line x" title="119:135	tinct weights tuned by minimum error rate training." ></td>
	<td class="line x" title="120:135	The dev and test sets remain the same as above." ></td>
	<td class="line x" title="121:135	Furthermore, we also make use of bilingual phrases to improve the coverage of the ruleset." ></td>
	<td class="line x" title="122:135	Following Liu et al.(2006), we prepare a phrase-table from a phrase-extractor, e.g. Pharaoh, and at decoding time, for each node, we construct on-the-fly flat translation rules from phrases that match the sourceside span of the node." ></td>
	<td class="line x" title="124:135	These phrases are called syntactic phrases which are consistent with syntactic constituents (Chiang, 2005), and have been shown to be helpful in tree-based systems (Galley et al., 2006; Liu et al., 2006)." ></td>
	<td class="line x" title="125:135	The final results are shown in Table 1, where TR denotes translation rule only, and TR+BP denotes the inclusion of bilingual phrases." ></td>
	<td class="line x" title="126:135	The BLEU score of forest decoder with TR is 0.2839, which is a 1.7% points improvement over the 1-best baseline, and this difference is statistically significant (p < 0.01)." ></td>
	<td class="line x" title="127:135	Using bilingual phrases further improves the BLEU score by 3.1% points, which is 2.1% points higher than the respective 1-best baseline." ></td>
	<td class="line x" title="128:135	We suspect this larger improvement is due to the alternative constituents in the forest, which activates many syntactic phrases suppressed by the 1-best parse." ></td>
	<td class="line x" title="129:135	5 Conclusion and future work We have presented a novel forest-based translation approach which uses a packed forest rather than the 1-best parse tree (or k-best parse trees) to direct the translation." ></td>
	<td class="line x" title="130:135	Forest provides a compact data-structure for efficient handling of exponentially many tree structures, and is shown to be a promising direction with state-of-the-art translation results and reasonable decoding speed." ></td>
	<td class="line x" title="131:135	This work can thus be viewed as a compromise between string-based and tree-based paradigms, with a good trade-off between speed and accuarcy." ></td>
	<td class="line x" title="132:135	For future work, we would like to use packed forests not only in decoding, but also for translation rule extraction during training." ></td>
	<td class="line x" title="133:135	198 Acknowledgement Part of this work was done while L. H. was visiting CAS/ICT." ></td>
	<td class="line x" title="134:135	The authors were supported by National Natural Science Foundation of China, Contracts 60736014 and 60573188, and 863 State Key Project No. 2006AA010108 (H. M and Q. L.), and by NSF ITR EIA-0205456 (L. H.)." ></td>
	<td class="line x" title="135:135	We would also like to thank Chris Quirk for inspirations, Yang Liu for help with rule extraction, Mark Johnson for posing the question of virtual -best list, and the anonymous reviewers for suggestions." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W08-0411
Syntax-Driven Learning of Sub-Sentential Translation Equivalents and Translation Rules from Parsed Parallel Corpora
Lavie, Alon;Parlikar, Alok;Ambati, Vamshi;"></td>
	<td class="line x" title="1:219	Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 8795, ACL-08: HLT, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:219	c2008 Association for Computational Linguistics Syntax-driven Learning of Sub-sentential Translation Equivalents and Translation Rules from Parsed Parallel Corpora Alon Lavie alavie@cs.cmu.edu Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA Alok Parlikar aup@cs.cmu.edu Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA Vamshi Ambati vambati@cs.cmu.edu Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA Abstract We describe a multi-step process for automatically learning reliable sub-sentential syntactic phrases that are translation equivalents of each other and syntactic translation rules between two languages." ></td>
	<td class="line x" title="3:219	The input to the process is a corpus of parallel sentences, word-aligned and annotated with phrase-structure parse trees." ></td>
	<td class="line x" title="4:219	We first apply a newly developed algorithm for aligning parse-tree nodes between the two parallel trees." ></td>
	<td class="line x" title="5:219	Next, we extract all aligned sub-sentential syntactic constituents from the parallel sentences, and create a syntax-based phrase-table." ></td>
	<td class="line x" title="6:219	Finally, we treat the node alignmentsastreedecompositionpointsandextract from the corpus all possible synchronous parallel tree fragments." ></td>
	<td class="line x" title="7:219	These are then converted into synchronous context-free rules." ></td>
	<td class="line x" title="8:219	We describetheapproachandanalyzeitsapplication to Chinese-English parallel data." ></td>
	<td class="line x" title="9:219	1 Introduction Phrase-based Statistical MT (PB-SMT) (Koehn et al., 2003) has become the predominant approach to Machine Translation in recent years." ></td>
	<td class="line x" title="10:219	PB-SMT requires broad-coverage databases of phrase-to-phrase translation equivalents." ></td>
	<td class="line x" title="11:219	These are commonly acquired from large volumes of automatically wordaligned sentence-parallel text corpora." ></td>
	<td class="line o" title="12:219	Accurate identification of sub-sentential translation equivalents, however, is a critical process in all data-driven MT approaches, including a variety of data-driven syntax-based approaches that have been developed in recent years." ></td>
	<td class="line oc" title="13:219	(Chiang, 2005) (Imamura et al., 2004) (Galley et al., 2004)." ></td>
	<td class="line x" title="14:219	In this paper, we describe a multi-step process for automatically learning reliable sub-sentential syntactic phrases that are translation equivalents of each other and syntactic translation rules between two languages." ></td>
	<td class="line x" title="15:219	The input to the process is a corpus of parallel sentences, word-aligned and annotated with phrase-structure parse trees for both languages." ></td>
	<td class="line x" title="16:219	Our method consists of three steps." ></td>
	<td class="line x" title="17:219	In the first step, we apply a newly developed algorithm for aligning parse-tree nodes between the two parallel trees." ></td>
	<td class="line x" title="18:219	In the second step, we extract all aligned sub-sentential syntactic constituents from the parallel sentences, and create a syntax-based phrase-table." ></td>
	<td class="line x" title="19:219	Our syntactic phrases come with constituent labels which can guide their syntactic function during decoding." ></td>
	<td class="line x" title="20:219	In the final step, we treat the node alignments as tree decomposition points and extract from the corpus all possible synchronous parallel tree fragments." ></td>
	<td class="line x" title="21:219	These are then converted into synchronous contextfree rules." ></td>
	<td class="line x" title="22:219	Our methods do not depend on any specific properties of the underlying phrase-structure representations or the parsers used, and were designed to be applicable even when these representations are quite different for the two languages." ></td>
	<td class="line x" title="23:219	The approach described is used to acquire the resources for a statistical syntax-based MT approach that we have developed (Stat-XFER), briefly described below." ></td>
	<td class="line x" title="24:219	The resulting resources can, however, be used in any syntax-based data-driven MT approach other than our own." ></td>
	<td class="line x" title="25:219	The focus of this paper is on our syntax-driven process for extracting phrases and rules from data." ></td>
	<td class="line x" title="26:219	We describe the approach and analyze its effectiveness when applied to large-volumes of Chinese-English parallel data." ></td>
	<td class="line x" title="27:219	87 1.1 The Stat-XFER MT Framework Stat-XFER is a search-based syntax-driven framework for building MT systems." ></td>
	<td class="line x" title="28:219	The underlying formalism is based on synchronous context-free grammars." ></td>
	<td class="line x" title="29:219	The synchronous rules can optionally be augmented by unification-style feature constraints." ></td>
	<td class="line x" title="30:219	The synchronous grammars can be acquired automatically from data, but also manually developed by experts." ></td>
	<td class="line x" title="31:219	A simple example transfer-rule (for Chineseto-English) can be seen below: {NP,1062753} NP::NP [DNP NP] -> [NP PP] ( (*score* 0.946640316205534) (X2::Y1) (X1::Y2) ) Each rule has a unique identifier followed by a synchronous rule for both source and target sides." ></td>
	<td class="line x" title="32:219	The alignment of source-to-target constituents is explicitly represented using X indices for the source side, and Y indices for the target side." ></td>
	<td class="line x" title="33:219	Rules can also have lexical items on either side, in which case no alignment information is required for these elements." ></td>
	<td class="line x" title="34:219	Feature constraints can optionally be specified for both source and target elements of the rule." ></td>
	<td class="line x" title="35:219	We do not address the learning of feature constraints in the work described here, and concentrate only on the acquisition of the synchronous CFG rules." ></td>
	<td class="line x" title="36:219	The rules can be modeled statistically and assigned scores, which can then be used as decoding features." ></td>
	<td class="line x" title="37:219	The Stat-XFER framework also includes a fullyimplemented transfer engine that applies the transfer grammar to a source-language input sentence at runtime, and produces collections of scored word and phrase-level translations according to the grammar. These are collected into a lattice data-structure." ></td>
	<td class="line x" title="38:219	Scores are based on a log-linear combination of several features, and a beam-search controls the underlying parsing and transfer process." ></td>
	<td class="line x" title="39:219	A secondstage monotonic decoder is responsible for combining translation fragments into complete translation hypotheses (Lavie, 2008) 2 PFA Algorithm for Node Aligment 2.1 Objectives of the Algorithm Our objective of the first stage of our approach is to detect sub-sentential constituent correspondences in parallel sentences, based on phrase-structure parses for the two corresponding sentences." ></td>
	<td class="line x" title="40:219	Given a pair of parallel sentences and their corresponding parse trees, our goal is to find pairings of nodes in the source and target trees whose yields are translation equivalents of each other." ></td>
	<td class="line x" title="41:219	Our current approach only considers complete constituents and their contigious yields, and will therefore not align discontiguous phrases or partial constituents." ></td>
	<td class="line x" title="42:219	Similar to phrase extraction methods in PB-SMT, we rely on word-level alignments (derived manually or automatically) as indicators for translation equivalence." ></td>
	<td class="line x" title="43:219	The assumption applied is that if two words are aligned with each other, they carry the same meaning and can be treated as translation equivalents." ></td>
	<td class="line x" title="44:219	Constituents are treated as compositional units of meaning and translation equivalence." ></td>
	<td class="line x" title="45:219	2.2 Related Work Aligning nodes in parallel trees has been investigated by a number of previous researchers." ></td>
	<td class="line x" title="46:219	(Samuelsson and Volk, 2007) describe a process for manual alignment of nodes in parallel trees." ></td>
	<td class="line x" title="47:219	This approach is well suited for generating reliable parallel treebanks, but is impractical for accumulating resources from large parallel data." ></td>
	<td class="line x" title="48:219	(Tinsley et al., 2007)usestatisticallexiconsderivedfromautomatic statistical word alignment for aligning nodes in parallel trees." ></td>
	<td class="line x" title="49:219	In our approach, we use the word alignment information directly, which we believe may be more reliable than the statistical lexicon." ></td>
	<td class="line x" title="50:219	(Groves et al., 2004) propose a method of aligning nodes between parallel trees automatically, based on word alignments." ></td>
	<td class="line x" title="51:219	In addition to the word alignment information, their approach uses the constituent labels of nodes in the trees, and the general structure of the tree." ></td>
	<td class="line x" title="52:219	Our approach is more general in the sense that we only consider the word alignments, thereby making the approach applicable to any parser or phrasestructure representation, even ones that are quite different for the two languages involved." ></td>
	<td class="line x" title="53:219	88 2.3 Unaligned Words and Contiguity Word-level alignment of phrase-level translation equivalents often leaves some words unaligned." ></td>
	<td class="line x" title="54:219	For example, some languages have articles, while others do not." ></td>
	<td class="line x" title="55:219	It is thus reasonable to expect that constituent pairs in parallel trees that are good translation equivalents of each other may contain some unaligned words." ></td>
	<td class="line x" title="56:219	Our PFA node-alignment algorithm allows for such constituents to be matched." ></td>
	<td class="line x" title="57:219	Differentlanguageshavedifferentwordorders." ></td>
	<td class="line x" title="58:219	In English, an adjective always comes before a noun, while in French, in most cases, the adjective follows its noun." ></td>
	<td class="line x" title="59:219	Our node alignment algorithm allows aligning of constituents regardless of the word order expressed by the linear precedence relation of their sub-constituents." ></td>
	<td class="line x" title="60:219	As long as one piece of contiguous textdominatedbyanodecoversthesameword-level alignments as the yield of a node in the parallel tree, the two nodes can be aligned." ></td>
	<td class="line x" title="61:219	2.4 Wellformedness constraints Given a pair of word-aligned sentences and their corresponding parse trees S and T, represented as sets of constituent nodes, our PFA node alignment algorithm produces a collection of aligned nodepairs (Si,Tj)." ></td>
	<td class="line x" title="62:219	The underlying assumptions of compositionality in meaning and word-level alignments being indicative of translation equivalence lead directly to the following node alignment wellformedness criteria: 1." ></td>
	<td class="line x" title="63:219	If a node Si is linked to a node Tj, then any node within the subtree of node Si can only be linked to nodes within the subtree of node Tj." ></td>
	<td class="line x" title="64:219	2." ></td>
	<td class="line x" title="65:219	If a node Si is linked to a node Tj, then any node that dominates the node Si can only be linked to nodes that dominate the node Tj." ></td>
	<td class="line x" title="66:219	3." ></td>
	<td class="line x" title="67:219	If a node Si is linked to a node Tj, then the word alignments of the yields of the two constituents must satisfy the following: (a) Every word in the yield of the node Si must be aligned to one or more words in the yield of the node Tj, or it should be unaligned." ></td>
	<td class="line x" title="68:219	(b) Every word in the yield of the node Tj must be aligned to one or more words in the yield of the node Si, or it should be unaligned." ></td>
	<td class="line x" title="69:219	(c) There should be at least one alignment between the yields of nodes Si and Tj." ></td>
	<td class="line x" title="70:219	Thus, the words in the yields can not all be unaligned." ></td>
	<td class="line x" title="71:219	2.5 Arithmetic Representation Our PFA algorithm uses a arithmetic mapping that elegently carries over the constraints characterized by the wellformedness constraints elaborated above." ></td>
	<td class="line x" title="72:219	Thismappingisdesignedtoensurethateachaligned word, which carries a distinct piece of meaning can be uniquely identified, and also inherently reflects the compositional properties of constituent translation equivalence." ></td>
	<td class="line x" title="73:219	This is accomplished by assigning numerical values to the nodes of the two parse trees being aligned, in a bottom-up fashion, starting from the leaf nodes of the trees." ></td>
	<td class="line x" title="74:219	Leaf nodes that correspond to words that are aligned are each assigned a unique prime number." ></td>
	<td class="line x" title="75:219	Unaligned leaf nodes are assigned a value of 1." ></td>
	<td class="line x" title="76:219	Constituent nodes in the parse trees are then assigned a value that is the product of all its sub-constituent nodes." ></td>
	<td class="line x" title="77:219	Because of the arithmetic property that any composite number can be uniquely factored into primes, it should be evident that the value of every constituent node uniquely identifies the aligned words that are covered by its yield." ></td>
	<td class="line x" title="78:219	Consequently, by assigning the sameprimevaluestothealignedwordsofbothtrees, retrieving aligned constituent nodes is as simple as finding the set of nodes in the two trees that carry the same numerical value." ></td>
	<td class="line x" title="79:219	Note that by assigning values of 1 to unaligned words, these unaligned words do not influence the numerical values assigned to constituent nodes, thus reflecting their treatment as dont cares with respect to the translation equivalence of constituent nodes." ></td>
	<td class="line x" title="80:219	2.6 Description of the PFA Algorithm The PFA algorithm uses the concept of composite meaningasprimefactorization, andhencethename (Prime Factorization and Alignments)." ></td>
	<td class="line x" title="81:219	The algorithm assigns values to the leaf nodes, propogates the values up the tree, and then compares the node values across the trees to align the nodes." ></td>
	<td class="line x" title="82:219	As described above, leaf nodes which have word alignments are assigned unique prime numbers, and the 89 Figure 1: Node-Aligned Parallel Sentences same prime is assigned to the corresponding aligned words in the parallel sentences." ></td>
	<td class="line x" title="83:219	Leaf nodes corresponding to unaligned words are assigned the value 1." ></td>
	<td class="line x" title="84:219	The treatment of one-to-many word alignments is a special case." ></td>
	<td class="line x" title="85:219	Such alignments are considered to carry the same meaning, and should thus be assigned the same value." ></td>
	<td class="line x" title="86:219	To accomplish this, if a single word is aligned to multiple words in the other language, we assign the same prime number to all words on the multiple side, and assign the product of these to the single word equivalent." ></td>
	<td class="line x" title="87:219	Another special case is when the parse trees contain unary productions." ></td>
	<td class="line x" title="88:219	In this case, the values of both nodes involved in this production are the same." ></td>
	<td class="line x" title="89:219	Our node alignment algorithm breaks this tie by selecting the node that is lower in the tree (the daughter node of the unary production)." ></td>
	<td class="line x" title="90:219	A similar situation with two nodes being assigned identical values can arise when one or more unaligned words are attached directly to the parent node." ></td>
	<td class="line x" title="91:219	Here too, our algorithm aligns the lower node and leaves the higher node unaligned." ></td>
	<td class="line x" title="92:219	These decisions reflect our desire to be conservative with respect to such ambiguous cases, and their implications on the notion of translational equivalence." ></td>
	<td class="line x" title="93:219	This also provides some robustness against noisy alignments." ></td>
	<td class="line x" title="94:219	It is straightfoward to verify that the PFA algorithm satisfies the wellformedness constraints described above." ></td>
	<td class="line x" title="95:219	Also, since multiplication is commutative, the algorithm is not effected by differing word orders within parallel constituent structures." ></td>
	<td class="line x" title="96:219	The PFA algorithm run on a sample ChineseEnglish parallel sentence is shown in Figure 1." ></td>
	<td class="line x" title="97:219	The value of each node as shown as a part of its label." ></td>
	<td class="line x" title="98:219	The aligned nodes are marked by shapes." ></td>
	<td class="line x" title="99:219	A triangle aligns to a triangle, and squares to squares." ></td>
	<td class="line x" title="100:219	3 Syntax-based Sub-sentential Phrase Extraction The alignment of nodes as described in the previous section allows us to build a comprehensive syntaxbased phrase-to-phrase translation lexicon from a parallel corpus." ></td>
	<td class="line x" title="101:219	To build a syntax-based phrase table, we simply extract all aligned constituent nodes along with their yields and enter them into a database, while accumulating frequency counts." ></td>
	<td class="line x" title="102:219	In addition to the source-to-target phrase correspondences, we record the constituent labels of the aligned constituent nodes on both the source and target sides (which may be different)." ></td>
	<td class="line x" title="103:219	These labels connect the phrases with synatactic transfer rules during decoding." ></td>
	<td class="line x" title="104:219	The set of phrases extracted from the example sentence in Figure 1 is shown in Figure 2." ></td>
	<td class="line x" title="105:219	90 Figure 2: Phrases extracted from Aligned Nodes The process of building syntax-based phrase tables from large corpora of sentence-parallel data is quite similar to the corresponding process in phrasebased SMT systems." ></td>
	<td class="line x" title="106:219	Our phrase correspondences, however, only reflect contiguous and complete constituent correspondences." ></td>
	<td class="line x" title="107:219	We also note that the extractedphrasetablesinbothapproachescanbemodeledstatisticallyinsimilarways." ></td>
	<td class="line x" title="108:219	Similartocommon practice in PB-SMT, we currently use the frequency counts of the phrases to calculate relative likelihood estimates and use these as features in our Stat-XFER decoder." ></td>
	<td class="line x" title="109:219	4 Evaluation of the PFA algorithm The accuracy of our node alignment algorithm depends on both the quality of the word alignments as well as the accuracy of the parse trees." ></td>
	<td class="line x" title="110:219	We performed several experiments to assess the effects of these underlying resources on the accuracy of our approach." ></td>
	<td class="line x" title="111:219	The most accurate condition is when the parallel sentences are manually word-aligned, and when verified correct parse trees are available for both source and target sentences." ></td>
	<td class="line x" title="112:219	Performance is expected to degrade when word alignments are produced using automatic methods, and when correct parse trees are replaced with automatic parser output." ></td>
	<td class="line x" title="113:219	Intheseexperiments, weusedamanuallywordaligned parallel Chinese-English TreeBank consisting of 3342 parallel sentences." ></td>
	<td class="line x" title="114:219	4.1 Manual Constituent Node Alignments We first investigated the accuracy of our approach under the most accurate condition." ></td>
	<td class="line x" title="115:219	We sampled 30 sentences from the Chinese-English treebank corpus." ></td>
	<td class="line x" title="116:219	A bilingual expert from our group then manually aligned the nodes in these trees." ></td>
	<td class="line x" title="117:219	These node Precision Recall F-1 F-0.5 0.8129 0.7325 0.7705 0.7841 Table 1: Accuracy of PFA Node Alignments against Manual Node Alignments alignments were then used as a gold standard." ></td>
	<td class="line x" title="118:219	We then used the accurate parse trees and the manually created word alignments for these sentence pairs, and ran the PFA node algorithm, and compared the resulting node alignments with the gold standard alignments." ></td>
	<td class="line x" title="119:219	The Precision, Recall, F-1 and F-0.5 results are reported in Table 1." ></td>
	<td class="line x" title="120:219	We manually inspected cases where there was a mismatch between the manual and automatic node alignments, and found several trends." ></td>
	<td class="line x" title="121:219	Many of the alignment differences were the result of one-tomany or many-to-many word alignemnts." ></td>
	<td class="line x" title="122:219	For example, in some cases a verb in Chinese was wordaligned to an auxiliary and a head verb on the english side (e.g. have and put)." ></td>
	<td class="line x" title="123:219	The PFA algorithm in this case node-aligns the VP that governs the Chinese verb to the VP that contains both auxiliary and head verbs on the English side." ></td>
	<td class="line x" title="124:219	The gold standard human alignments, however, in some cases, aligned the VP of the Chinese verb to the English VP that governs just the main verb." ></td>
	<td class="line x" title="125:219	Other mismatches were attributed to errors or inconsistencies in the manual word alignment and to the treatment of traces and fillers in the parse trees." ></td>
	<td class="line x" title="126:219	4.2 Effect of Using Automatic Word Alignments We next tested how sensitive the PFA algorithm is to errors in automatic word alignment." ></td>
	<td class="line x" title="127:219	We use the entire 3342 sentences in the parallel treebank for this experiment." ></td>
	<td class="line x" title="128:219	We first ran the algorithm with the correct parse trees and manual word-alignments as input." ></td>
	<td class="line x" title="129:219	We use the resulting node alignments as the gold standard in this case." ></td>
	<td class="line x" title="130:219	We then used GIZA++ to get bidirectional word alignments, and combined them using various strategies." ></td>
	<td class="line x" title="131:219	In this scenario, the trees are high-quality (from the treebank), but the alignments are noisy." ></td>
	<td class="line x" title="132:219	The results obtained are shown in Table 2." ></td>
	<td class="line x" title="133:219	Unsurprisingly, the Union combination method has the best precision but worst recall, while the Intersection combination method has the best recall but worst precision." ></td>
	<td class="line x" title="134:219	The four 91 Comb Method Prec Rec F-1 F-0.5 Intersection 0.6382 0.5395 0.5846 0.6014 Union 0.8114 0.2915 0.4288 0.5087 Sym1 0.7142 0.4534 0.5546 0.5992 Sym2 0.7135 0.4631 0.5616 0.6045 Grow-Diag-Final 0.7777 0.3462 0.4790 0.5493 Grw-Diag-Fin-And 0.6988 0.4700 0.5619 0.6011 Table 2: Manual Trees, Automatic Node Alignments other methods for combining word alignments fall in between." ></td>
	<td class="line x" title="135:219	Three of the four (all except growdiag-final) behave quite similarly." ></td>
	<td class="line x" title="136:219	We generally believethatprecisionissomewhatmoreimportantthan recall for this task, and have thus used the sym2 method (Ortiz-Martnez et al., 2005) (which has the best F-0.5 score) for our translation experiments." ></td>
	<td class="line x" title="137:219	4.3 Effect of Using Automatic Parses We evaluated the effect of parsing errors (as reflected in automatically derived parse trees) on the quality of the node alignments." ></td>
	<td class="line x" title="138:219	We parsed the treebank corpus on both English and Chinese using the Stanford parser, and extracted phrases using manual wordalignments." ></td>
	<td class="line x" title="139:219	Comparedtothephrasesextracted from the manual trees, we obtained a precision of 0.8749, and a recall of 0.7227, that is, an F-0.5 measure of 0.8174." ></td>
	<td class="line x" title="140:219	We then evaluated the most noisy condition that involves both automatic word alignments and automatic parse trees." ></td>
	<td class="line x" title="141:219	We evaluated the phrase extraction with different Viterbi combination strategies." ></td>
	<td class="line x" title="142:219	The sym2 combination gave the best results, with a precision of 0.6251, recall of 0.3566, thus an F-0.5 measure of 0.4996." ></td>
	<td class="line x" title="143:219	5 Synchronous Tree Fragment and CFG Rule Extraction 5.1 Related Work Syntax-based reordering rules can be used as a preprocessingstepforPB-SMT(andotherapproaches), to decrease the word-order and syntactic distortion between the source and target languages (Xia and McCord, 2004)." ></td>
	<td class="line x" title="144:219	A variety of hierarchical and syntax-based models, which are applied during decoding, have also been developed." ></td>
	<td class="line x" title="145:219	Many of these approaches involve automatic learning and extraction of the underlying syntax-based rules from data." ></td>
	<td class="line oc" title="146:219	The underlying formalisms used has been quite broad and include simple formalisms such as ITGs (Wu, 1997), hierarchicalsynchronousrules(Chiang, 2005), string to tree models by (Galley et al., 2004) and (Galley et al., 2006), synchronous CFG models such (Xia and McCord, 2004) (Yamada and Knight, 2001), synchronous Lexical Functional Grammar inspired approaches (Probst et al., 2002) and others." ></td>
	<td class="line o" title="147:219	Most of the previous approaches for acquiring syntactic transfer or reordering rules from parallel corpora use syntactic information from only one side of the parallel corpus, typically the target side." ></td>
	<td class="line x" title="148:219	(Hearne and Way, 2003) describes an approach that uses syntactic information from the source side to derive reordering subtrees, which can then be used within a data-oriented translation (DOT) MT system, similar in framework to (Poutsma, 2000)." ></td>
	<td class="line x" title="149:219	Our work is different from the above in that we use syntactic trees for both source and target sides to infer constituent node alignments, from which we then learn synchronous trees and rules." ></td>
	<td class="line oc" title="150:219	Our process of extraction of rules as synchronous trees and then converting them to synchronous CFG rules is most similar to that of (Galley et al., 2004)." ></td>
	<td class="line x" title="151:219	5.2 Synchronous Tree Fragment Pair Extraction The main concept underlying our syntactic rule extraction process is that we treat the node alignments discovered by the PFA algorithm (described in previous sections) as synchronous tree decomposition points." ></td>
	<td class="line x" title="152:219	This reflects the fact that these nodes denote points in the synchronous parse trees where translation correspondences can be put together compositionally." ></td>
	<td class="line x" title="153:219	Using the aligned nodes as decomposition points, we break apart the synchronous trees into collections of minimal synchronous tree fragments." ></td>
	<td class="line x" title="154:219	Finally, the synchronous fragments are also converted into synchronous context-free rules." ></td>
	<td class="line x" title="155:219	These are then collected into a database of synchronous rules." ></td>
	<td class="line x" title="156:219	Theinputtoourruleextractionprocessconsistsof the parallel parse trees along with their node alignment information." ></td>
	<td class="line x" title="157:219	The constituent nodes in the parallel trees that were aligned by the PFA node alignment algorithm are treated as tree decomposition points." ></td>
	<td class="line x" title="158:219	At each such decomposition point, spliting the two parallel trees results in two partial trees or tree fragments." ></td>
	<td class="line x" title="159:219	One synchronous pair consists of 92 the subtrees that are headed by the aligned nodes where the decomposition took place." ></td>
	<td class="line x" title="160:219	Since the subtrees are rooted at aligned nodes, their yields are translation equivalents of each other." ></td>
	<td class="line x" title="161:219	The other synchronous tree fragment pair consists of the remaining portions of the trees." ></td>
	<td class="line x" title="162:219	The translation equivalence of the complete tree (or subtree) prior to decompositionimpliesthatthesetreefragments(whichexclude the detached subtrees) also correspond to translation equivalents." ></td>
	<td class="line x" title="163:219	The tree fragments that are obtained by decomposing the synchronous trees in this fashion are similar to the Synchronous Tree Insertion Grammar of (Shieber and Schabes, 1990)." ></td>
	<td class="line x" title="164:219	We developed a tree traversal algorithm that decomposes parallel trees into all minimal tree fragments." ></td>
	<td class="line x" title="165:219	Given two synchronous trees and their node alignment decomposition information, our tree fragment extraction algorithm operates by an in-order traversal of the trees top down, starting from the root nodes." ></td>
	<td class="line x" title="166:219	The traversal can be guided by either the source or target parse tree." ></td>
	<td class="line x" title="167:219	Each node in the tree that is marked as an aligned node triggers a decomposition." ></td>
	<td class="line x" title="168:219	The subtree that is rooted at this node is removed from the currently traversed tree." ></td>
	<td class="line x" title="169:219	A copy of the removed subtree is then recursively processed fortop-downdecomposition." ></td>
	<td class="line x" title="170:219	Ifthecurrenttreenode beingexploredisnotanalignednode(andthusisnot adecompositionpoint),thetraversalcontinuesdown the tree, possibly all the way to the leaves of the tree." ></td>
	<td class="line x" title="171:219	Decomposition is performed on the corresponding parallel tree at the same time." ></td>
	<td class="line x" title="172:219	We apply this process on all the aligned constituent nodes (decomposition points) to obtain all possible decomposed synchronous tree fragment pairs from the original parallel parse trees." ></td>
	<td class="line x" title="173:219	This results in a collection of all minimal synchronous subtree fragments." ></td>
	<td class="line x" title="174:219	These synchronous subtree fragments are minimal in the sense that they do not contain any internal aligned nodes." ></td>
	<td class="line x" title="175:219	Another property of the synchronous subtree fragments is that their frontier nodes are either aligned nodes from the original tree or leaf nodes (corresponding to lexical items)." ></td>
	<td class="line x" title="176:219	Figure 3 shows some sample tree fragment pairs that were obtained from the example discussed earlier in Figure 1." ></td>
	<td class="line x" title="177:219	5.3 Synchronous Transfer Rule Creation In the last step, we convert the synchronous tree fragment pairs obtained as described above into synFigure 3: Tree Fragment Pairs Extracted from Aligned Nodes chronous context-free rules." ></td>
	<td class="line x" title="178:219	This creates rules in a format that is compatible with the Stat-XFER formalism that was described in Section 1." ></td>
	<td class="line x" title="179:219	Our system currently does not use the internal tree structure information that is contained in the synchronous tree fragments." ></td>
	<td class="line x" title="180:219	Therefore, onlythesyntacticcategorylabels of the roots of the tree fragments, and the nodes on the fragment frontier are relevant to decoding." ></td>
	<td class="line x" title="181:219	This in essense corresponds to a flattening of the synchronous tree fragment into a synchronous context free style rule." ></td>
	<td class="line x" title="182:219	The flattening of the tree fragments is accomplished by an in-order traversal on each of the tree fragments to produce a string representation." ></td>
	<td class="line x" title="183:219	Frontier nodes in the fragment are either labeled constituentnodesorleafnodesoftheoriginalparsetree." ></td>
	<td class="line x" title="184:219	These form the right-hand sides of the flattened rule." ></td>
	<td class="line x" title="185:219	The positions of the constituent nodes in the output stringarenumberedtokeeptrackofalignmentofthe nodes, which is often non-monotonic due to reordering between the source and target languages." ></td>
	<td class="line x" title="186:219	Finally the root constituent label of the source tree fragment becomes the source-side parent category of the rule, while the root label of the target tree fragment becomes the target side parent category." ></td>
	<td class="line x" title="187:219	Accurate automatic transfer rule learning requires accurate word alignments and parse structures." ></td>
	<td class="line x" title="188:219	Thus, to favor high precision (at the expense of some loss of recall), in our work to date on Chinese and other languages, while we extract syntactic phrases from all available parallel data, we extract 93 rules only from manually word-aligned parsed parallel data." ></td>
	<td class="line x" title="189:219	To compensate for the limited amount of data, we generalize the rules as much as possible." ></td>
	<td class="line x" title="190:219	Elements in the rules that originate from leaf nodes in the parse trees are generalized to their part-ofspeech categories, if the corresponding words were one-to-one aligned in the parallel sentences." ></td>
	<td class="line x" title="191:219	Unaligned words and words that are part of one-tomany alignments are not generalized to the POS level and remain lexicalized in the final rule." ></td>
	<td class="line x" title="192:219	Thephrasetableextractedfromthecorpusandthe rules are scored together to ensure that they are consistent when used in our translation system." ></td>
	<td class="line x" title="193:219	For all Stat-XFER experiments to date, we have used just the source side conditionig with a constant smoothing factor for robustness to noise." ></td>
	<td class="line x" title="194:219	6 Extraction Applied to Chinese-English Parallel Data We used the pipeline of PFA node alignment followed by rule extraction to build resources for a Stat-XFER Chinese-to-English MT system." ></td>
	<td class="line x" title="195:219	The syntax-based phrase table was constructed from two large parallel corpora released by LDC for the DARPA/GALE program." ></td>
	<td class="line x" title="196:219	The parallel sentences for both English and Chinese were parsed using the Stanford parser." ></td>
	<td class="line x" title="197:219	The first corpus consists of about 1.2 million sentence pairs." ></td>
	<td class="line x" title="198:219	Our extraction process applied to this corpus resulted in a syntax-based phrase table of about 9.2 million entries." ></td>
	<td class="line x" title="199:219	The other data source used was a parallel corpus of about 2.6 million sentences, but many of its entries were from a Chinese-English lexicon." ></td>
	<td class="line x" title="200:219	From this corpus, we extracted 8.75 million phrases." ></td>
	<td class="line x" title="201:219	Rule learning was performed on a 10K-sentence parallel corpus that was manually word-aligned, released by LDC for the DARPA/GALE program." ></td>
	<td class="line x" title="202:219	Thismanuallyword-alignedcorpusincludestheparallel Chinese-English treebank of 3,343 sentence pairs." ></td>
	<td class="line x" title="203:219	The treebank sentences come with verified correct parse trees for English and Chinese." ></td>
	<td class="line x" title="204:219	The rest of the 10K corpus was parsed by the Stanford parser." ></td>
	<td class="line x" title="205:219	The complete 10K parallel corpus was node aligned and rules were extracted as described in Section 5." ></td>
	<td class="line x" title="206:219	Figure 3 shows two synchronous tree fragments that were extracted from the example node-aligned sentence pair in Figure 1." ></td>
	<td class="line x" title="207:219	After generalization and flatFigure 4: Rules Extracted from Tree Pairs Table 3: Statistics for Chinese-English Rules tening, we obtain rules such as those shown in Figure 4." ></td>
	<td class="line x" title="208:219	The above process resulted in a collection of almost 100K rules." ></td>
	<td class="line x" title="209:219	Some statistics on this rule set are shown in Table 3." ></td>
	<td class="line x" title="210:219	Analysis of this rule set indicates that only about 4% of these rules were observed more than once in the data." ></td>
	<td class="line x" title="211:219	These include the most general and useful rules for mapping Chinese syntactic structures to their corresponding English structures." ></td>
	<td class="line x" title="212:219	Most of the singleton rules are highly lexicalized." ></td>
	<td class="line x" title="213:219	A large portion of the singleton rules are noisy rules, but many of them are good and useful rules." ></td>
	<td class="line x" title="214:219	Experiments indicate that removing all singleton rules hurts translation performance." ></td>
	<td class="line x" title="215:219	7 Conclusions The process described in this paper provides a fully automated solution for extracting large collection of reliable syntax-based phrase tables and syntactic synchronous transfer rules from large volumes of parsed parallel corpora." ></td>
	<td class="line x" title="216:219	In conjunction with the Stat-XFER syntax-based framework, this provides a fully automated solution for building syntax-based MT systems." ></td>
	<td class="line x" title="217:219	The current performance of this approach still lags behind state-of-the-art phrase-based systemswhentrainedonthesameparalleldatabutis showing encouraging improvements." ></td>
	<td class="line x" title="218:219	Furthermore, the resources extracted by our process can be used by various other syntax-based MT approaches." ></td>
	<td class="line x" title="219:219	94" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1076
Synchronous Tree Adjoining Machine Translation
DeNeefe, Steve;Knight, Kevin;"></td>
	<td class="line x" title="1:194	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 727736, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:194	c 2009 ACL and AFNLP Synchronous Tree Adjoining Machine Translation Steve DeNeefe and Kevin Knight USC Information Sciences Institute 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 USA {sdeneefe,knight}@isi.edu Abstract Tree Adjoining Grammars have well-known advantages, but are typically considered too difficult for practical systems." ></td>
	<td class="line x" title="3:194	We demonstrate that, when done right, adjoining improves translation quality without becoming computationally intractable." ></td>
	<td class="line x" title="4:194	Using adjoining to modeloptionalityallows generaltranslation patterns to be learned without the clutter of endless variations of optional material." ></td>
	<td class="line x" title="5:194	The appropriatemodifiers can later be spliced in as needed." ></td>
	<td class="line x" title="6:194	In this paper, we describe a novel method for learning a type of Synchronous Tree Adjoining Grammar and associated probabilities from aligned tree/string training data." ></td>
	<td class="line x" title="7:194	We introduce a method of converting these grammars to a weakly equivalent tree transducer for decoding." ></td>
	<td class="line x" title="8:194	Finally, we show that adjoining results in an end-to-end improvement of +0.8 BLEU over a baseline statistical syntax-based MTmodelonalarge-scaleArabic/EnglishMT task." ></td>
	<td class="line x" title="9:194	1 Introduction Statistical MT has changed a lot in recent years." ></td>
	<td class="line x" title="10:194	We have seen quick progress from manually crafted linguistic models to empirically learned statistical models, from word-based models to phrase-based models, and from string-based models to tree-based models." ></td>
	<td class="line x" title="11:194	Recently there is a swing back to incorporating more linguistic information again, but this time linguistic insight carefully guides the setup of empirically learned models." ></td>
	<td class="line x" title="12:194	Shieber (2007) recently argued that probabilistic Synchronous Tree Adjoining Grammars (Shieber and Schabes, 1990) have the right combination of properties that satisfy both linguists and empirical MT practitioners." ></td>
	<td class="line x" title="13:194	So far, though, most work in this area has been either more linguistic than statistical (Abeille et al., 1990) or statistically-based, but linguistically light (Nesson et al., 2006)." ></td>
	<td class="line nc" title="14:194	Current tree-based models that integrate linguistics and statistics, such as GHKM (Galley et al., 2004), are not able to generalize well from a single phrase pair." ></td>
	<td class="line n" title="15:194	For example, from the data in Figure 1, GHKM can learn rule (a) to translate nouns with two pre-modifiers, but does not generalize to learn translation rules (b) (d) without the optional adjective or noun modifiers." ></td>
	<td class="line x" title="16:194	Likewise, none of these rules allow extra material to be introduced, e.g. Pakistans national defense minister." ></td>
	<td class="line x" title="17:194	In large enough training data sets, we see many examples of all the common patterns, but the rarer patterns have sparse statistics or poor coverage." ></td>
	<td class="line x" title="18:194	NP JJ national NN defense NN minister wzyr AldfAE AlwTnY (a) NP JJ 1 NN 2 NN 3  NN 3 NN 2 JJ 1 (b) NP NN 1 NN 2  NN 2 NN 1 (c) NP JJ 1 NN 2  NN 2 JJ 1 (d) NP NN 1  NN 1 Figure 1: Rule (a)can be learned from this training example." ></td>
	<td class="line x" title="19:194	Arguably, the more general rules (b) (d) should also be learnable." ></td>
	<td class="line x" title="20:194	To mitigate this problem, the parse trees used as training data for these systems can be binarized (Wang et al., 2007)." ></td>
	<td class="line x" title="21:194	Binarization allows rules with partial constituents to be learned, resulting in more general rules, richer statistics, and better phrasal coverage (DeNeefe et al., 2007), but no principled required vs. optional decision has been made." ></td>
	<td class="line x" title="22:194	This methods key weakness is that binarization always keeps adjacent siblings together, so there is no way to group the head with a required complement if optional information intervenes between the two." ></td>
	<td class="line x" title="23:194	Furthermore, if all kinds of children are considered equally optional, then we have removed important syntactic constraints, which may end up permitting too much freedom." ></td>
	<td class="line x" title="24:194	In addition, spurious alignments may limit the binarization tech727 niques effectiveness." ></td>
	<td class="line x" title="25:194	In this paper, we present a method of learning a type of probabilistic Synchronous Tree Adjoining Grammar (STAG) automatically from a corpus of word-aligned tree/string pairs." ></td>
	<td class="line x" title="26:194	To learn this grammar we use linguistic resources to make the required vs. optional decision." ></td>
	<td class="line x" title="27:194	We then directly model the optionality in the translation rules by learning statistics for the required parts of the rule independently from the optional parts." ></td>
	<td class="line x" title="28:194	We also present a method of converting these rules into a well-studied tree transducer formalism for decoding purposes." ></td>
	<td class="line x" title="29:194	We then show that modeling optionality using adjoining results in a statistically significant BLEU gain over our baseline syntax-based model with no adjoining." ></td>
	<td class="line x" title="30:194	2 Translation Model 2.1 Synchronous Tree Insertion Grammars Tree Adjoining Grammars (TAG), introduced by Joshi et al.(1975) and Joshi (1985), allow insertion of unbounded amounts of material into the structure of an existing tree using an adjunction operation." ></td>
	<td class="line x" title="32:194	Usually they also include a substitution operation, which has a fill in the blank semantics, replacing a substitution leaf node with a tree." ></td>
	<td class="line x" title="33:194	Figure 2 visually demonstrates TAG operations." ></td>
	<td class="line x" title="34:194	Shieber and Schabes (1990) offer a synchronous version of TAG (STAG), allowing the construction of a pair of trees in lockstep fashion using the TAG operations of substitution and adjunction on tree pairs." ></td>
	<td class="line x" title="35:194	To facilitate this synchronous behavior, links between pairs of nodes in each tree pair define the possible sites for substitution and adjunction to happen." ></td>
	<td class="line x" title="36:194	One application of STAG is machine translation (Abeille et al., 1990)." ></td>
	<td class="line x" title="37:194	One negative aspect of TAG is the computational complexity: O(n 6 ) time is required for monolingual parsing (and thus decoding), and STAG requires O(n 12 ) for bilingual parsing (which might be used for training the model directly on bilingual data)." ></td>
	<td class="line x" title="38:194	Tree Insertion Grammars (TIG) are a restricted form of TAG that was introduced (Schabes and Waters, 1995) to keep the same benefits as TAG (adjoining of unbounded material) without the computational complexity TIG parsing is O(n 3 )." ></td>
	<td class="line x" title="39:194	This reduction is due to a limitation on adjoining: auxiliary trees can only introduce tree material to the left or the right of the node adjoined to." ></td>
	<td class="line x" title="40:194	Thus an auxiliary tree can be classified by direction as left or right adjoining." ></td>
	<td class="line x" title="41:194	adjunction NP DT the NP NN NP JJ NP* substitution substitution NN minister JJ defense = NP DT the NP JJ defense NP NN minister Figure 2: TAG grammars use substitution and adjunction operations to construct trees." ></td>
	<td class="line x" title="42:194	Substitution replaces the substitution node (marked with ) with another tree." ></td>
	<td class="line x" title="43:194	Adjunction inserts an auxiliary treea special kind of tree fragment with a foot node (marked with *)into an existing tree at a permitted non-terminal node." ></td>
	<td class="line x" title="44:194	Note that in TAG, adjunctions are permitted at any non-terminal with the same label as the root and foot node of the auxiliary tree, while in STAG adjunctions are restricted to linked sites." ></td>
	<td class="line x" title="45:194	Nesson et al.(2006) introduce a probabilistic, synchronous variant of TIG and demonstrate its use for machine translation, showing results that beat both word-based and phrase-based MT models on a limited-vocabulary, small-scale training and test set." ></td>
	<td class="line x" title="47:194	Training the model uses an O(n 6 ) bilingual parsing algorithm, and decoding is O(n 3 )." ></td>
	<td class="line x" title="48:194	Though this model uses trees in the formal sense, it does not create Penn Treebank (Marcus et al., 1993) style linguistic trees, but uses only one non-terminal label (X) to create those trees using six simple rule structures." ></td>
	<td class="line x" title="49:194	The grammars we use in this paper share some properties in common with those of Nesson et al.(2006) in that they are of the probabilistic, synchronous tree-insertion variety." ></td>
	<td class="line x" title="51:194	All pairs of sites (both adjunction and substitution in our case) are explicitly linked." ></td>
	<td class="line x" title="52:194	Adjunction sites are restricted by direction: at each linked site, the source and target side each specify one allowed direction." ></td>
	<td class="line x" title="53:194	The result isthat each synchronous adjunction site can be classified into one of four direction classes: {LR, LL, RR, RL}." ></td>
	<td class="line x" title="54:194	For example, LR means the source side site only allows left adjoining trees and the target side site only allows right adjoining trees." ></td>
	<td class="line x" title="55:194	There are several important differences between our grammars and the ones of Nesson et al.(2006): Richer, Linguistic Trees: Our grammars have a 728 PennTreebank-style linguistic tree onthe English (target) side, and ahierarchical structure using only a single non-terminal symbol (X) on the source side." ></td>
	<td class="line x" title="56:194	We believe this provides the rich information needed in the target language without over-constraining the model." ></td>
	<td class="line x" title="57:194	Substitution Sites/Non-lexical trees: We use both substitution and adjunction (Nesson et al.(2006) only used adjunction) and do not require all trees to contain lexical items as is commonly done in TIG (Schabes and Waters, 1995)." ></td>
	<td class="line x" title="59:194	Single Adjunction/Multiple Sites: Each nonterminal node in a tree may allow multiple adjunction sites, but every site only allows at most one adjunction, 1 a common assumption for TAG as specified in the Vijay-Shanker (1987) definition." ></td>
	<td class="line x" title="60:194	Here are some examples of automatically learned translation rules with interpretations of how they work: 1." ></td>
	<td class="line x" title="61:194	simple lexical rules for translating words or phrases: IN without  X AlA interpretation: translate the Arabic word AlA as the preposition without 2." ></td>
	<td class="line x" title="62:194	rules with substitution for translating phrases with holes (substitution sites are designated by an arrow and numeric subscript, e.g. NP 1 ): PP PP IN of NP 1  X X 1 interpretation: insert of to turn a noun phrase into a prepositional phrase 3." ></td>
	<td class="line x" title="63:194	simple adjoining rules for inserting optional modifiers (adjoining sites are designated by 1 An adjoined rule may itself have adjoining sites allowing further adjunction." ></td>
	<td class="line x" title="64:194	an alphabetic subscript before or after a nonterminal to indicate direction of adjoining, e.g. a NP): a NP JJ 1 NP*  X X* X a X 1 interpretation: adjoin an adjective before a noun in English but after in Arabic, and allowing further adjoinings in those same directions afterward 4." ></td>
	<td class="line x" title="65:194	rules with multiple adjunction and substitution sites: a S NP 1 b S c VP d VP e VBD 2 NP 3  X a X X 2 X X 1 e,b X d,c X 3 interpretation: translate an Arabic sentence in VSO form into an English sentence in SVO form, with multiple adjoining options 2.2 Generative Story When we use these rules to translate from a foreign sentence f into an English sentence e, we use several models together in a log-linear fashion, but our primary model is a joint model of P(e tree ,f tree ), which is our surrogate for directly modeling P(e|f)." ></td>
	<td class="line x" title="66:194	This can be justified because P(e|f)= P(e,f) P(f) , and P(f) is fixed for a given foreign sentence." ></td>
	<td class="line x" title="67:194	Therefore: argmax e P(e|f) = argmax e P(e,f)  yield(argmax e tree P(e tree ,f tree ))  yield(argmax e tree P(d e tree ,f tree )) where d e tree ,f tree is a derivation tree of rules that generates e tree and f tree . In other words, e, the highest probability translation off,can be approximated by taking the yield of the highest probability tree e tree that is a translation of the highest probability tree of f. This can further be approximated by the highest probability derivation of rules translating between f and e via trees." ></td>
	<td class="line x" title="68:194	Now we define the probability of generating d e tree ,f tree . Starting with an initial symbol pair 729 representing a rule with a single substitution site, 2 TOP,X, a tree pair can be generated by the following steps: 1." ></td>
	<td class="line x" title="69:194	Foreach substitution site s i in the current rule r 1 : (a) Choose with probability P sub (r 2 |label L (s i ),label R (s i )) a rule r 2 having root node labels label L (s i ) and label R (s i ) that match the left and right labels at s i . 2." ></td>
	<td class="line x" title="70:194	For each adjunction site s i,r 1 in the current rule r 1 : (a) Choose with rule-specific probability P ifadj (decision adjoin |s i,r 1 ,r 1 ) choose whether or not to adjoin at the current site s i,r 1 ." ></td>
	<td class="line x" title="71:194	(b) If we are adjoining at site s i,r 1 , choose with probability P adj (r 2 |d,label L (s i,r 1 ),label R (s i,r 1 )) a rule r 2 of direction class d having root node labels label L (s i,r 1 ) and label R (s i,r 1 ) that match the left and right labels at s i,r 1 . 3." ></td>
	<td class="line x" title="72:194	Recursively process each of the added rules For all substitution rules r s , adjoining rules r a , and adjoining sites s i,r , the probability of a derivation tree using these rules is the product of all the probabilities used in this process, i.e.: P deriv = productdisplay r s parenleftbigg P sub (r s |root L (r s ),root R (r s ))  productdisplay s i,r s P ifadj (decision adjoin |s i,r s ,r s ) parenrightbigg  productdisplay r a parenleftbigg P adj (r a |dir(r a ),root L (r a ),root R (r a ))  productdisplay s i,r a P ifadj (decision adjoin |s i,r a ,r a ) parenrightbigg Note that while every new substitution site requires an additional rule to be added, adjunction sites may or may not introduce an additional rule based on the rule-specific P ifadj probability." ></td>
	<td class="line x" title="73:194	This allows adjunction to represent linguistic optionality." ></td>
	<td class="line x" title="74:194	2 Here and in the following, we use site as shorthand for synchronous site pair." ></td>
	<td class="line oc" title="75:194	3 Learning the Model Instead of using bilingual parsing to directly train our model from strings as done by Nesson et al.(2006), we follow the method of Galley et al.(2004) by dividing the training process into steps." ></td>
	<td class="line x" title="78:194	First, we word align the parallel sentences and parse the English (target) side." ></td>
	<td class="line x" title="79:194	Then, wetransform the aligned tree/string training data into derivation trees ofminimal translation rules (Section 3.1)." ></td>
	<td class="line x" title="80:194	Finally, we learn our probability models P sub , P ifadj , and P adj by collecting counts over the derivation trees (Section 3.2)." ></td>
	<td class="line x" title="81:194	This method is quick enough to allow us to scale our learning process to largescale data sets." ></td>
	<td class="line x" title="82:194	3.1 Generating Derivation Trees and Rules There are four steps in transforming the training data into derivation trees and rules, the first two operating only on the English parse tree itself: 3 A. Marking Required vs. Optional." ></td>
	<td class="line x" title="83:194	For each constituent in the English parse tree, wemark children as (H)ead, (R)equired, or (O)ptional elements (see step (a) in Figure 3)." ></td>
	<td class="line x" title="84:194	The choice of head, required, or optional has a large impact on the generality and applicability of our grammar." ></td>
	<td class="line nc" title="85:194	If all children are considered required, the result is the same as the GHKM rules of Galley et al.(2004) and has the same problemlots of low count, syntactically over-constrained rules." ></td>
	<td class="line x" title="87:194	Too many optional children, on the other hand, allows ungrammatical output." ></td>
	<td class="line x" title="88:194	Our proposed model is a linguistically motivated middle ground: we consider the linguistic heads and complements selected by Collins (2003) rules to be required and all other children to be optional." ></td>
	<td class="line x" title="89:194	B. Parse tree to TIG tree." ></td>
	<td class="line x" title="90:194	Next, we restructure the English tree to form a TIG derivation where head and required elements are substitutions, and optional elements are adjunctions (see step (b) in Figure 3)." ></td>
	<td class="line x" title="91:194	To allow for adjoining between siblings under a constituent, we first do a head-out binarization of the tree." ></td>
	<td class="line x" title="92:194	This is followed by excising 4 any children marked as optional and replacing them with an adjunction site, as shown in Figure 4." ></td>
	<td class="line x" title="93:194	Note that we excise a chain of optional children as one site with each optional child 3 Thesefirsttwo stepswere inspired by themethod Chiang (2003) used to automatically extract a TIG from an English parse tree." ></td>
	<td class="line x" title="94:194	4 Excising is the opposite of adjoining: extracting out an auxiliary rule from a tree to form two smaller trees." ></td>
	<td class="line x" title="95:194	730 S ADVP , NP VP ." ></td>
	<td class="line x" title="96:194	(a) = S ADVP O , O NP R VP H . O (b) = S ADVP , NP VP . Figure 3: Parse tree to TIG transformation: (a) mark constituent children with (H)ead, (R)equired, and (O)ptional, then (b) restructure the tree so that head and required elements are substitutions, while optional elements are adjoined (shown with dotted lines)." ></td>
	<td class="line x" title="97:194	NT 1 NT 1 ABC NT 2 XYZ = NT 1 ABC NT 1 NT 1 * NT 2 XYZ NT 1 NT 3 XYZ NT 1 NT 2 DEF NT 1 ABC = NT 1 NT 1 NT 1 NT 3 XYZ NT 1 * NT 2 DEF NT 1 * ABC (a) excising one optional child (XYZ) (b) excising a series of optional children (DEF, then XYZ) Figure 4: Two examples of excising auxiliary trees from a head-out binarized parse tree: (a) excising one optional left branch, (b) excising a chain of optional branches in the same (right) direction into a series of adjunctions." ></td>
	<td class="line x" title="98:194	In both examples, the ABC child is the head, while the other children are optional." ></td>
	<td class="line x" title="99:194	adjoined to the previous child, as in Figure 4(b)." ></td>
	<td class="line x" title="100:194	C. Extracting rules and derivation trees." ></td>
	<td class="line x" title="101:194	We now have a TIG derivation tree, with each elementary tree attached to its parent by a substitution or adjunction link." ></td>
	<td class="line x" title="102:194	We can now extract synchronous rules allowed by the alignments and syntactic constituents." ></td>
	<td class="line oc" title="103:194	This can be done using a method inspired by the rule-extraction approach of Galley et al.(2004), but instead of directly operating on the parse tree we process the English TIG derivation tree." ></td>
	<td class="line x" title="105:194	In bottom-up fashion, we visit each elementary tree in the derivation, allowing a rule rooted at this tree to be extracted if its words or those of its descendants are aligned such that they are the English side of a self-contained parallel phrase (i.e., the foreign text of this phrase is not aligned to English leaves outside of the set of descendants)." ></td>
	<td class="line x" title="106:194	Otherwise, this elementary tree is rejoined with its parent to form a larger elementary tree." ></td>
	<td class="line x" title="107:194	At the end of this process we have a new set of linked elementary trees which make up the English side of the grammar, where each substitution or adjunction link becomes a substitution or adjunction site in the synchronous grammar." ></td>
	<td class="line x" title="108:194	Onthe foreign side westart with the foreign text of the self-contained parallel phrase and replace any parts of this phrase covered by substituted or adjoined children of the English side tree withsubstitution sites or adjunction site markers." ></td>
	<td class="line x" title="109:194	From this, we produce a tree with a simple, regular form by placing all items under a root node labeled X. In the case of more than one foreign word or substitution site, we introduce an intermediate level of X-labeled non-terminals to allow for possible adjunction between elements, otherwise the adjoining sites attach to the single root node." ></td>
	<td class="line x" title="110:194	We attach all foreign-side adjoining sites to be left adjoining, except on the right side of the right-hand child." ></td>
	<td class="line x" title="111:194	It is possible to have the head child tree on the English side not aligned to anything, while the adjoined children are." ></td>
	<td class="line x" title="112:194	This may lead to rules with no foreign non-terminal from which to anchor the adjunctions, so in this case, we attach adjoined child elementary trees starting from the head and moving out until we attach a some child with a nonempty foreign side." ></td>
	<td class="line x" title="113:194	D. Generalizing rules." ></td>
	<td class="line x" title="114:194	We need to clarify what makes one rule distinct from another." ></td>
	<td class="line x" title="115:194	Consider the example in Figure 5, which shows selected rules learned in the case of two different noun phrases." ></td>
	<td class="line x" title="116:194	If the noun phrase consists of just a single noun, we learn rule (a), while if the noun phrase also has an adjective, we learn rules (b) and (c)." ></td>
	<td class="line x" title="117:194	Since adjoining the adjective is optional, we 731 consider rules (a) and (c) to be the same rule, the latter with an adjoining seen, and the former with the same adjoining not seen." ></td>
	<td class="line x" title="118:194	3.2 Statistical Models Once we have the derivation trees and list of rules, we learn our statistical models using maximum likelihood estimation." ></td>
	<td class="line x" title="119:194	By counting and normalizing appropriately over the entire corpus, we can straightforwardly learn the P sub and P adj distributions." ></td>
	<td class="line x" title="120:194	However, recall that in our model P ifadj is a rule-specific probability, which makes it more difficult to estimate accurately." ></td>
	<td class="line x" title="121:194	For common rules, we see plenty of examples of adjoining, while for other rules, we need to learn from only a handful of examples." ></td>
	<td class="line x" title="122:194	Smoothing and generalization are especially important for these low frequency cases." ></td>
	<td class="line x" title="123:194	Two options present themselves for how to estimate adjoining: (a) A joint model of adjoining." ></td>
	<td class="line x" title="124:194	We assume that adjoining decisions are made in combination with each other, and so learn non-zero probabilities only for adjoining combinations seen in data (b) An independent model of adjoining." ></td>
	<td class="line x" title="125:194	We assume adjoining decisions are made independently, and learn a model for each adjoining site separately Option (a) may be sufficient for frequent rules, and will accurately model dependencies between different kinds of adjoining." ></td>
	<td class="line x" title="126:194	However, it does not allow us to generalize to unseen patterns of adjoining." ></td>
	<td class="line x" title="127:194	Consider the low frequency situation depicted in Figure 6, rules (d)-(f)." ></td>
	<td class="line x" title="128:194	We may have seen this rule four times, once with adjoining site a, twice with adjoining sites a and b, and once with a third adjoining site c. The joint model will give a zero probability to unseen patterns of adjoining, e.g. no adjoining at any site or adjoining at site b alone." ></td>
	<td class="line x" title="129:194	Even if weuse a discounting method to give anonzero probability to unseen cases, we still have no way to distinguish one from another." ></td>
	<td class="line x" title="130:194	Option (b) allows us to learn reasonable estimates for these missing cases by separating out adjoining decisions and letting each speak for itself." ></td>
	<td class="line x" title="131:194	To properly learn non-zero probabilities for unseen cases 5 we use add k smoothing (k = 1 2 )." ></td>
	<td class="line x" title="132:194	5 For example, low frequency rules may have always been observed with a single adjoining pattern, and never without adjoining." ></td>
	<td class="line x" title="133:194	A weakness of this approach still remains: adjoining is not a truly independent process, as we observe empirically in the data." ></td>
	<td class="line x" title="134:194	In real data, frequent rules have many different observed adjoining sites (10 or 20 in some cases), many of which represent already infrequent sites in combinations never seen together." ></td>
	<td class="line x" title="135:194	To reduce the number of invalid combinations produced, we only allow adjoinings to be used at the same time if they have occurred together in the training data." ></td>
	<td class="line x" title="136:194	This restriction makes it possible to do less adjoining than observed, but not more." ></td>
	<td class="line x" title="137:194	For the example in Figure 6, in addition to the observed patterns, we would also allow site b to be used alone, and we would allow no adjoinings, but we would not allow combinations of site c with either a or b. Later, we will see that this makes the decoding process more efficient." ></td>
	<td class="line x" title="138:194	Because both option (a) and (b) above have strengths and weaknesses, we also explore a third option which builds upon the strengths of each: (c) A log-linear combination of the joint model and independent model." ></td>
	<td class="line x" title="139:194	Weassume the probability has both a dependent and independent element, and learn the relative weight between them automatically To help smooth this model we add two additional binary features: one indicating adjoining patterns seen in data and one indicating previously unseen patterns." ></td>
	<td class="line x" title="140:194	4 Decoding To translate with these rules, we do a monolingual parse using the foreign side of the rules (constraining the search using non-terminal labels from both sides), while keeping track of the English side string and structure for language modeling purposes." ></td>
	<td class="line x" title="141:194	This produces all valid derivations of rules whose foreign side yield is the input string, from which we simply choose the one with the highest log-linear model score." ></td>
	<td class="line x" title="142:194	Though this process could be done directly using a specialized parsing algorithm, we note that these rules have weakly equivalent counterparts in the Synchronous Tree Substitution Grammar (STSG) and Tree-to-string transducer (xLNTs 6 ) worlds, such that each STIG rule can be translated into one equivalent rule, plus some helper rules to model the adjoin/no-adjoin 6 xLNTsisshorthand forextended linearnon-deleting topdown tree-to-string transducer." ></td>
	<td class="line x" title="143:194	732 Case 1: Case 2: NP NN health AlSHp  (a) NP NN 1  X X 1 NP JJ national NP NN defense AldfAE AlwTnY  (b) NP JJ 1 NP*  X X* X X 1 (c) a NP NN 1  X a X 1 Figure 5: Selected rules learned in two cases." ></td>
	<td class="line x" title="144:194	Rule (a) and (c) are considered the same rule, where (c) has the optional synchronous adjoining site marked with a. From these (limited) examples alone we would infer that adjective adjoining happens half the time, and is positioned before the noun in English, but after the noun in Arabic (thus the positioning of site a)." ></td>
	<td class="line x" title="145:194	(d) a QP b IN 1  a X b X 1 (e) a QP IN 1  a X X 1 (f) c QP IN 1  X c X 1 (seen once) (seen twice) (seen once) Figure 6: For a low frequency rule, we may see only a few different adjoining patterns, but we want to infer more." ></td>
	<td class="line x" title="146:194	decision." ></td>
	<td class="line x" title="147:194	Conversion to a better known and explored formalism allows us to take advantage of existing code and algorithms." ></td>
	<td class="line x" title="148:194	Here we describe the conversion process to xLNTs rules, though conversion to STSG is similar." ></td>
	<td class="line x" title="149:194	Algorithm 1 describes the process of converting one of our automatically learned STIG rules." ></td>
	<td class="line x" title="150:194	On each side of the rule, we traverse the tree in a topdown, left-to-right order, recording words, substitution sites, and adjoining sites in the order encountered (left adjoinings before the nodes children and right adjoinings after)." ></td>
	<td class="line x" title="151:194	We make these words and sites as the children under a single root node." ></td>
	<td class="line x" title="152:194	The substitution sites are given states made up of a combination of their source and target labels as are the roots of non-adjoining rules." ></td>
	<td class="line x" title="153:194	Adjoining sites are labeled with a combination of the rule id and a site id. Adjoining rule roots are labeled with a combination of the source and target root labels and the direction class." ></td>
	<td class="line x" title="154:194	To allow for the adjoining/no-adjoining decision, two helper rules are created for each adjoining site, their root state a combination of the rule and site ids." ></td>
	<td class="line x" title="155:194	One of these rules has only epsilon leaf nodes (representing no adjoining), while the other has leaf nodes and a state that match with the corresponding adjoining rule root (labeled with the sites source and target labels and the direction class)." ></td>
	<td class="line x" title="156:194	For each rule, the algorithm generates one main rule and pairs of helper rules to facilitate adjoining/non-adjoining." ></td>
	<td class="line x" title="157:194	For computational efficiency reasons, our decoder supports neither epsilon rules nor non-binary rules." ></td>
	<td class="line x" title="158:194	So we remove epsilons using an exponential expansion of the rules: combine each main rule with an adjoining or nonadjoining helper rule for each adjunction site, then remove epsilon-only branches." ></td>
	<td class="line x" title="159:194	For k adjunction sites this could possibly results in 2 k rules." ></td>
	<td class="line x" title="160:194	But as discussed previously (at the end of Section 3.2), we only allow subsets of adjoining combinations seen in training data, so this number is substantially lower for large values of k. 5 Experiments All experiments are trained with a subset (171,000 sentences or 4 million words) of the ArabicEnglish training data from the constrained data track of the NIST 2008 MT Evaluation, leaving out LDC2004T18, LDC2007E07, and the UN data." ></td>
	<td class="line x" title="161:194	The training data is aligned using the LEAF technique (Fraser and Marcu, 2007)." ></td>
	<td class="line x" title="162:194	The English side of the training data is parsed with an implementation of Collins Model 2 (Collins, 2003) then head-out binarized." ></td>
	<td class="line x" title="163:194	The tuning data (1,178 sentences) and devtest data (1,298 sentences) are 733 Input: Synchronous TIG rule r with j adjoining sites, S  T, where S and T are trees Output: a weakly equivalent xLNTs rule S prime  t 1 t n , where S prime is a one-level tree, and 2  j helper rules for adjoining Run time: O(|S| + |T|) begin rules  {}, lhs-state concat(q, get-root(S), get-root(T)) site-and-word-list-s get-sites-and-words-in-order(S) site-and-word-list-t get-sites-and-words-in-order(T) if r is adjoining then lhs-state concat(lhs-state, get-adjoin-dir(S), get-adjoin-dir(T)) lhsconstruct-LHS(lhs-state, get-root(S), site-and-word-list-s) rhsconstruct-RHS(add-states(id(r), site-and-word-list-t)) add(rules, lhs rhs) / * main rule * / foreach adjoining site i  1k do lhs-state concat(q, id(r), i), rhs-state concat(q, lhs-root) lhs-root concat(source-label(i), target-label(i), source-dir(i), target-dir(i)) lhs construct-LHS(lhs-state, lhs-root, lhs-root) rhs construct-RHS({(rhs-state, lhs-root)}) rhs-eps construct-RHS(epsilon1) add(rules, {lhs rhs, lhs rhs-eps}) / * helper rules for site i * / return rules end function get-sites-and-words-in-order(node) y  {} if node is substitution site or word then append site or word to y else append left adjoining sites to y in outside-to-inside order foreach child c of node do append result of get-yield(c) to y append right adjoining sites to y in inside-to-outside order return y end function add-states(ruld-id, node-list) foreach substitution or adjunction site s i and in node-list do if s i is substitution site then state = concat(q, source-site-label(s i ), target-site-label(s i )) else state = concat(q, rule-id, i) replace s i with (state, s i ) return modified node-list end Algorithm 1: Conversion from synchronous TIG rules to weakly equivalent xLNTs rules BLEU description DevTest NIST06 (1) baseline: all required (GHKM minimal, head-out binarized parse trees) 48.0 47.0 (2) joint adjoining prob model alone (only observed adjoining patterns) 48.0 46.6 (3) independent adjoining prob model alone (only observed adjoining patterns) 48.1 46.7 (4) independent adjoining prob model alone (with new adjoining patterns) 48.5 47.6 (5) independent model alone + features (adjoining pattern, direction) 48.4 47.7 (6) log-linear combination of joint & independent models + features 48.7 47.8 Table 1: End-to-end MT results show that the best adjoining model using a log-linear combination of joint and independent models (line 6) outperforms the baseline (line 1) by +0.7 and +0.8 BLEU,a statistically significant difference at the 95% confidence level." ></td>
	<td class="line x" title="164:194	734 made up of newswire documents drawn from the NIST MT evaluation data from 2004, 2005, and 2006 (GALE part)." ></td>
	<td class="line x" title="165:194	We use the newswire documents from the NIST part of the 2006 evaluation data (765 sentences) as a held-out test set." ></td>
	<td class="line x" title="166:194	We train our feature weights using max-BLEU (Och, 2003) and decode with a CKY-based decoder that supports language model scoring directly integrated into the search." ></td>
	<td class="line x" title="167:194	In addition to P sub , P adj , and P ifadj , we use several other features in our log-linear model during decoding, including: lexical and phrase-based translation probabilities, a model similar to conditional probability on the trees (P(f tree (rule)|e tree (rule))), a probability model for generating the top tree non-terminal, a 5-gram language model 7 , and target length bonus." ></td>
	<td class="line x" title="168:194	We also have several binary featureslexical rule, rule with missing or spurious content wordsand several binary indicator features for specialized rules: unknown word rules; name, number, and date translation rules; and special fail-safe monotone translation rules in case of parse failures and extremely long sentences." ></td>
	<td class="line o" title="169:194	Table 1 shows the comparison between our baseline model (minimal GHKM on head-out binarized parse trees) and different models of adjoining, measured with case-insensitive, NISTtokenized BLEU (IBM definition)." ></td>
	<td class="line x" title="170:194	The top section (lines 14) compares the joint adjoining probability model to the independent adjoining probability model and seen vs. unseen adjoining combinations." ></td>
	<td class="line x" title="171:194	While the joint model results in a BLEU score at the same level as our baseline (line 2), the independent model (line 4) improves BLEU by +0.5 and +0.6, which are significant differences at the 95% confidence level." ></td>
	<td class="line x" title="172:194	Since with the independent model we introduce both new adjoining patterns and a different probability model for adjoining (each site is independent), we also use the independent model with only previously seen adjoining patterns (line 3)." ></td>
	<td class="line x" title="173:194	The insignificant difference in BLEU between lines 2 and 3 leads us to think that the new adjoining patterns are where the improvement comes from, rather than the independent probability model alone." ></td>
	<td class="line x" title="174:194	We also test several other features and combinations." ></td>
	<td class="line x" title="175:194	First, we add binary features to indicate a new adjoining combination vs. one previously 7 The 5-gram LM was trained on 2 billion words of automatically selected collections taken from the NIST 08 allowable data." ></td>
	<td class="line x" title="176:194	seen in data." ></td>
	<td class="line x" title="177:194	We also add features to indicate the direction class of adjoining to test if there is a systematic bias toward particular directions." ></td>
	<td class="line x" title="178:194	These features cause no significant difference in score (line 5)." ></td>
	<td class="line x" title="179:194	We also add the joint-adjoining probability as a feature, allowing it to be combined in a log-linear fashion with the independent probability (line 6)." ></td>
	<td class="line x" title="180:194	This results in our best BLEU gain: +0.7 and +0.8 over our non-adjoining baseline." ></td>
	<td class="line x" title="181:194	6 Conclusion We have presented a novel method for learning the rules and probabilities for a new statistical, linguistically-informed, syntax-based MT model that allows for adjoining." ></td>
	<td class="line x" title="182:194	We have described a method to translate using this model." ></td>
	<td class="line x" title="183:194	And we have demonstrated that linguistically-motivated adjoining improves the end-to-end MT results." ></td>
	<td class="line x" title="184:194	There are many potential directions for research to proceed." ></td>
	<td class="line x" title="185:194	One possibility is to investigate other methods of making the required vs. optional decision, either using linguistic resources such as COMLEX or automatically learning the distinction using EM (as done for tree binarization by Wang et al.(2007))." ></td>
	<td class="line x" title="187:194	In addition, most ideas presented here are extendable to rules with linguistic trees on both sides (using insights from Lavie et al.(2008))." ></td>
	<td class="line x" title="189:194	Also worth investigating is the direct integration of bilingual dictionaries into the grammar (as suggested by Shieber (2007))." ></td>
	<td class="line x" title="190:194	Lastly, rule composition and different amounts of lexicalization (Galley et al., 2006; Marcu et al., 2006; DeNeefe et al., 2007) or context modeling (Marino et al., 2006) have been successful with other models." ></td>
	<td class="line x" title="191:194	Acknowledgments We thank David Chiang for suggestions about adjoining models, Michael Pust and Jens-Sonke Vockler for developing parts of the experimental framework, and other colleagues at ISI for their helpful input." ></td>
	<td class="line x" title="192:194	We also thank the anonymous reviewers for insightful comments and suggestions." ></td>
	<td class="line x" title="193:194	This research is financially supported under DARPA Contract No." ></td>
	<td class="line x" title="194:194	HR0011-06-C-0022, BBN subcontract 9500008412." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1108
Fast Translation Rule Matching for Syntax-based Statistical Machine Translation
Zhang, Hui;Zhang, Min;Li, Haizhou;Tan, Chew Lim;"></td>
	<td class="line x" title="1:261	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 10371045, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:261	c 2009 ACL and AFNLP Fast Translation Rule Matching for Syntax-based Statistical Machine Translation    Hui Zhang 1, 2    Min Zhang 1    Haizhou Li 1    Chew Lim Tan 2  1 Institute for Infocomm Research 2 National University of Singapore zhangh1982@gmail.com   {mzhang, hli}@i2r.a-star.edu.sg   tancl@comp.nus.edu.sg        Abstract In a linguistically-motivated syntax-based translation system, the entire translation process is normally carried out in two steps, translation rule matching and target sentence decoding using the matched rules." ></td>
	<td class="line x" title="3:261	Both steps are very timeconsuming due to the tremendous number of translation rules, the exhaustive search in translation rule matching and the complex nature of the translation task itself." ></td>
	<td class="line x" title="4:261	In this paper, we propose a hyper-tree-based fast algorithm for translation rule matching." ></td>
	<td class="line x" title="5:261	Experimental results on the NIST MT-2003 Chinese-English translation task show that our algorithm is at least 19 times faster in rule matching and is able to help to save 57% of overall translation time over previous methods when using large fragment translation rules." ></td>
	<td class="line pc" title="6:261	1 Introduction Recently linguistically-motivated syntax-based translation method has achieved great success in statistical machine translation (SMT) (Galley et al., 2004; Liu et al., 2006, 2007; Zhang et al., 2007, 2008a; Mi et al., 2008; Mi and Huang 2008; Zhang et al., 2009)." ></td>
	<td class="line x" title="7:261	It translates a source sentence to its target one in two steps by using structured translation rules." ></td>
	<td class="line x" title="8:261	In the first step, which is called translation rule matching step, all the applicable 1  translation rules are extracted from the entire rule set by matching the source parse tree/forest." ></td>
	<td class="line x" title="9:261	The second step is to decode the source sentence into its target one using the extracted translation rules." ></td>
	<td class="line x" title="10:261	Both of the two steps are very time-consuming due to the exponential number of translation rules and the complex nature of machine translation as  1  Given a source structure (either a parse tree or a parse forest), a translation rule is applicable if and only if the left hand side of the translation rule exactly matches a tree fragment of the given source structure." ></td>
	<td class="line x" title="11:261	an NP-hard search problem (Knight, 1999)." ></td>
	<td class="line x" title="12:261	In the SMT research community, the second step has been well studied and many methods have been proposed to speed up the decoding process, such as node-based or span-based beam search with different pruning strategies (Liu et al., 2006; Zhang et al., 2008a, 2008b) and cube pruning (Huang and Chiang, 2007; Mi et al., 2008)." ></td>
	<td class="line x" title="13:261	However, the first step attracts less attention." ></td>
	<td class="line x" title="14:261	The previous solution to this problem is to do exhaustive searching with heuristics on each tree/forest node or on each source span." ></td>
	<td class="line x" title="15:261	This solution becomes computationally infeasible when it is applied to packed forests with loose pruning threshold or rule sets with large tree fragments of large rule height and width." ></td>
	<td class="line x" title="16:261	This not only overloads the translation process but also compromises the translation performance since as shown in our experiments the large tree fragment rules are also very useful." ></td>
	<td class="line x" title="17:261	To solve the above issue, in this paper, we propose a hyper-tree-based fast algorithm for translation rule matching." ></td>
	<td class="line x" title="18:261	Our solution includes two steps." ></td>
	<td class="line x" title="19:261	In the first step, all the translation rules are re-organized using our proposed hyper-tree structure, which is a compact representation of the entire translation rule set, in order to make the common parts of translation rules shared as much as possible." ></td>
	<td class="line x" title="20:261	This enables the common parts of different translation rules to be visited only once in rule matching." ></td>
	<td class="line x" title="21:261	Please note that the first step can be easily done off-line very fast." ></td>
	<td class="line x" title="22:261	As a result, it does not consume real translation time." ></td>
	<td class="line x" title="23:261	In the second step, we design a recursive algorithm to traverse the hyper-tree structure and the input source forest in a top-down manner to do the rule matching between them." ></td>
	<td class="line x" title="24:261	As we will show later, the hyper-tree structure and the recursive algorithm significantly improve the speed of the rule matching and the entire translation process compared with previous methods." ></td>
	<td class="line x" title="25:261	With the proposed algorithm, we are able to carry out experiments with very loose pruning 1037 thresholds and larger tree fragment rules efficiently." ></td>
	<td class="line x" title="26:261	Experimental results on the NIST MT2003 Chinese-English translation task shows that our algorithm is 19 times faster in rule matching and is able to save 57% of overall translation time over previous methods when using large fragment translation rules with height up to 5." ></td>
	<td class="line x" title="27:261	It also shows that the larger rules with height of up to 5 significantly outperforms the rules with height of up to 3 by around 1 BLEU score." ></td>
	<td class="line x" title="28:261	The rest of this paper is organized as follows." ></td>
	<td class="line x" title="29:261	Section 2 introduces the syntax-based translation system that we are working on." ></td>
	<td class="line x" title="30:261	Section 3 reviews the previous work." ></td>
	<td class="line x" title="31:261	Section 4 explains our solution while section 5 reports the experimental results." ></td>
	<td class="line x" title="32:261	Section 6 concludes the paper." ></td>
	<td class="line x" title="33:261	2 Syntax-based Translation This section briefly introduces the forest/treebased tree-to-string translation model which serves as the translation platform in this paper." ></td>
	<td class="line x" title="34:261	2.1 Tree-to-string model     XNA declaration is related to some regulation  Figure 1." ></td>
	<td class="line x" title="35:261	A tree-to-string translation process." ></td>
	<td class="line oc" title="36:261	The tree-to-string model (Galley et al. 2004; Liu et al. 2006) views the translation as a structure mapping process, which first breaks the source syntax tree into many tree fragments and then maps each tree fragment into its corresponding target translation using translation rules, finally combines these target translations into a complete sentence." ></td>
	<td class="line x" title="37:261	Fig." ></td>
	<td class="line x" title="38:261	1 illustrates this process." ></td>
	<td class="line x" title="39:261	In real translation, the number of possible tree fragment segmentations for a given input tree is exponential in the number of tree nodes." ></td>
	<td class="line x" title="40:261	2.2 Forest-based translation To overcome parse error for SMT, Mi and Huang (2008) propose forest-based translation by using a packed forest instead of a single syntax tree as the translation input." ></td>
	<td class="line x" title="41:261	A packed forest (Tomita 1987; Klein and Manning, 2001; Huang and Chiang, 2005) is a compact representation of many possible parse trees of a sentence, which can be formally described as a triple , where V is the set of non-terminal nodes, E is the set of hyper-edges and S is a sentence represented as an ordered word sequence." ></td>
	<td class="line x" title="42:261	A hyper-edge in a packed forest is a group of edges in a tree which connects a father node to all its children nodes, representing a CFG-based parse rule." ></td>
	<td class="line x" title="43:261	Fig." ></td>
	<td class="line x" title="44:261	2 is a packed forest incorporating two parse trees T1 and T2 of a sentence as shown in Fig." ></td>
	<td class="line x" title="45:261	3 and Fig." ></td>
	<td class="line x" title="46:261	4." ></td>
	<td class="line x" title="47:261	Given a hyper-edge e, let h be its father node, then we say that e is attached to h. A non-terminal node in a packed forest can be represented as label [start, stop], where label is its syntax category and [start, stop] is the range of words it covers." ></td>
	<td class="line x" title="48:261	For example, the node in Fig." ></td>
	<td class="line x" title="49:261	5 pointed by the dark arrow is labelled as NP[3,4], where NP is its label and [3,4] means that it covers the span from the 3 rd  word to the 4 th  word." ></td>
	<td class="line x" title="50:261	In forest-based translation, rule matching is much more complicated than the tree-based one." ></td>
	<td class="line x" title="51:261	Figure 2." ></td>
	<td class="line x" title="52:261	A packed forest  Zhang et al.(2009) reduce the tree sequence problem into tree problem by introducing virtual node and related forest conversion algorithms, so 1038 the algorithm proposed in this paper is also applicable to the tree sequence-based models." ></td>
	<td class="line x" title="54:261	Figure 3." ></td>
	<td class="line x" title="55:261	Tree 1 (T1)      Figure 4." ></td>
	<td class="line x" title="56:261	Tree 2 (T2) 3 Matching Methods in Previous Work In this section, we discuss the two typical rule matching algorithms used in previous work." ></td>
	<td class="line x" title="57:261	3.1 Exhaustive search by tree fragments This method generates all possible tree fragments rooted by each node in the source parse tree or forest, and then matches all the generated tree fragments against the source parts (left hand side) of translation rules to extract the useful rules (Zhang et al., 2008a)." ></td>
	<td class="line x" title="58:261	Figure 5." ></td>
	<td class="line x" title="59:261	Node NP[3,4] in packed forest    Figure 6." ></td>
	<td class="line x" title="60:261	Candidate fragments on NP[3,4] For example, if we want to extract useful rules for node NP[3,4] in Fig 5, we have to generate all the tree fragments rooted at node NP[3,4] as shown in Fig 6, and then query each fragment in the rule set." ></td>
	<td class="line x" title="61:261	Let  be a node in the packed forest,  represents the number of possible tree fragments rooted at node , then we have:   null null null nullnull nullnullnull null nullnull  nullnullnullnullnullnullnullnull  nullnullnullnull nullnull null null nullnull null nullnullnullnullnullnullnullnullnullnull nullnullnullnullnullnullnullnull nullnull null    The above equation shows that the number of tree fragments is exponential to the span size, the height and the number of hyper-edges it covers." ></td>
	<td class="line x" title="62:261	In a real system, one can use heuristics, e.g. the maximum number of nodes and the maximum height of fragment, to limit the number of possible fragments." ></td>
	<td class="line x" title="63:261	However, these heuristics are very subjective and hard to optimize." ></td>
	<td class="line x" title="64:261	In addition, they may filter out some good fragments." ></td>
	<td class="line x" title="65:261	3.2 Exhaustive search by rules This method does not generate any source tree fragments." ></td>
	<td class="line x" title="66:261	Instead, it does top-down recursive matching from each node one-by-one with each translation rule in the rule set (Mi and Huang 2008)." ></td>
	<td class="line x" title="67:261	For example, given a translation rule with its left hand side as shown in Fig." ></td>
	<td class="line x" title="68:261	7, the rule matching between the given rule and the node IP[1,4] in Fig." ></td>
	<td class="line x" title="69:261	2 can be done as follows." ></td>
	<td class="line x" title="70:261	1." ></td>
	<td class="line x" title="71:261	Decompose the left hand side of the translation rule as shown in Fig." ></td>
	<td class="line x" title="72:261	7 into a sequence of hyper-edges in top-down, left-to-right order as follows: IP => NP VP;  NP => NP NP;  NP => NN; NN =>     Figure 7." ></td>
	<td class="line x" title="73:261	The left hand side of a rule  2." ></td>
	<td class="line x" title="74:261	Pattern match these hyper-edges(rule) oneby-one in top-down left-to-right order from node IP[1,4]." ></td>
	<td class="line x" title="75:261	If there is a continuous path in the forest matching all of these hyper-edges in order, then we can say that the rule is useful and matchable 1039 with the tree fragment covered by the continuous path." ></td>
	<td class="line x" title="76:261	The following illustrates the matching steps: 1." ></td>
	<td class="line x" title="77:261	Match hyper-edge IP => NP VP with node IP[1,4]." ></td>
	<td class="line x" title="78:261	There are two hyper-edges in the forest matching it: IP[1,4] => NP[1,1] VP[2,4] and IP[1,4] => NP[1,2] VP [3,4], which generates two candidate paths." ></td>
	<td class="line x" title="79:261	2." ></td>
	<td class="line x" title="80:261	Since hyper-edge NP => NP NP fails to match NP[1,1], the path initiated with IP[1,4] => NP[1,1] VP[2,4] is pruned out." ></td>
	<td class="line x" title="81:261	3." ></td>
	<td class="line x" title="82:261	Since there is a hyper-edge NP[1,2] => NP[1,1] NP[2,2] matching NP => NP NP on NP[1,2], then continue for further matching." ></td>
	<td class="line x" title="83:261	4." ></td>
	<td class="line x" title="84:261	Since NP=>NN on NP[2,2] matches NP[2,2] => NN[2,2], then continue for further matching." ></td>
	<td class="line x" title="85:261	5." ></td>
	<td class="line x" title="86:261	NN=> on NN[2,2] matches NN[2,2] => and it is the last hyper-edge in the input rules." ></td>
	<td class="line x" title="87:261	Finally, there is one continuous path successfully matching the left hand side of the input rule." ></td>
	<td class="line x" title="88:261	This method is able to avoid the exponential problem of the first method as described in the previous subsection." ></td>
	<td class="line x" title="89:261	However, it has to do one-byone pattern matching for each rule on each node." ></td>
	<td class="line x" title="90:261	When the rule set is very large (indeed it is very large in the forest-based model even with a small training set), it becomes very slow, and even much slower than the first method." ></td>
	<td class="line x" title="91:261	4 The Proposed Hyper-tree-based Rule Matching Algorithm In this section, we first explain the motivation why we re-organize the translation rule sets, and then elaborate how to re-organize the translation rules using our proposed hyper-tree structure." ></td>
	<td class="line x" title="92:261	Finally we discuss the top-down rule matching algorithm between forest and hyper-tree." ></td>
	<td class="line x" title="93:261	4.1 Motivation                 Figure 8." ></td>
	<td class="line x" title="94:261	Two rules left hand side   Figure 9." ></td>
	<td class="line x" title="95:261	Common part of the two rules left hand sides in Figure 8  Fig." ></td>
	<td class="line x" title="96:261	9 shows the common part of the left hand sides of two translation rules as shown in Fig." ></td>
	<td class="line x" title="97:261	8." ></td>
	<td class="line x" title="98:261	In previous rule matching algorithm, the common parts are matched as many times as they appear in the rule set, which reduces the rule matching speed significantly." ></td>
	<td class="line x" title="99:261	This motivates us to propose the hyper-tree structure and the rule matching algorithm to make the common parts shared by multiple translation rules to be visited only once in the entire rule matching process." ></td>
	<td class="line x" title="100:261	4.2 Hyper-node, hyper-path and hyper-tree A hyper-tree is a compact representation of a group of tree translation rules with common parts shared." ></td>
	<td class="line x" title="101:261	It consists of a set of hyper-nodes with edges connecting different hyper-nodes into a big tree." ></td>
	<td class="line x" title="102:261	A hyper-tree is constructed from the translation rule sets in two steps: 1) Convert each tree translation rule into a hyper-path; 2) Construct the hyper-tree by incrementally adding each individual hyper-path into the hyper-tree." ></td>
	<td class="line x" title="103:261	A tree rule can be converted into a hyper-path without losing information." ></td>
	<td class="line x" title="104:261	Fig." ></td>
	<td class="line x" title="105:261	10 demonstrates the conversion process: 1) We first fill the rule tree with virtual nodes to make all its leaves have the same depth to the root; 2) We then group all the nodes in the same tree level to form a single hyper-node, where we use a comma as a delimiter to separate the tree nodes with different father nodes; 3) A hyper-path is a set of hyper-nodes linked in a top-down manner." ></td>
	<td class="line x" title="106:261	The commas and virtual nodes  are introduced to help to recover the original tree from the hyperpath." ></td>
	<td class="line x" title="107:261	Given a tree node in a hyper-node, if there are n commas before it, then its father node is the (n+1) th  tree node in the father hyper-node." ></td>
	<td class="line x" title="108:261	If we could find father node for each node in hypernodes, then it is straightforward to recover the original tree from the hyper-path by just adding the edges between original father and children nodes except the virtual node . 1040 After converting each tree rule into a hyperpath, we can organize the entire rule set into a big hyper-tree as shown in Figure 11." ></td>
	<td class="line x" title="109:261	The concept of hyper-path and hyper-tree could be viewed as an extension of the 'prefix merging' ideas for CFG rules (Klein and Manning 2001)." ></td>
	<td class="line x" title="110:261	Figure 10." ></td>
	<td class="line x" title="111:261	Convert tree to hyper-path    Figure 11." ></td>
	<td class="line x" title="112:261	A hyper-tree example  Algorithm 1 shows how to organize the rule set into a big hyper-tree." ></td>
	<td class="line x" title="113:261	The general process is that for each rule we convert it into a hyper-path and then add the hyper-path into a hyper-tree incrementally." ></td>
	<td class="line x" title="114:261	However, there are many different hyper-trees generated given a big rule set." ></td>
	<td class="line x" title="115:261	We then introduce a TOP label as the root node to link all the individual hyper-trees to a single big hypertree." ></td>
	<td class="line x" title="116:261	Algorithm 2 shows the process of adding a hyper-path into a hyper-tree." ></td>
	<td class="line x" title="117:261	Given a hyper-path, we do a top-down matching between the hypertree and the input hyper-path from root hypernode until a leaf hyper-node is reached or there is no matching hyper-node at some level found." ></td>
	<td class="line x" title="118:261	Then we add the remaining unmatchable part of the input hyper-path as the descendants of the last matchable hyper-node." ></td>
	<td class="line x" title="119:261	Please note that in Fig." ></td>
	<td class="line x" title="120:261	10 and Fig." ></td>
	<td class="line x" title="121:261	11, we ignore the target side (right hand side) of translation rules for easy discussion." ></td>
	<td class="line x" title="122:261	Indeed, we can easily represent all the complete translation rules (not only left hand side) in Fig." ></td>
	<td class="line x" title="123:261	11 by simply adding the corresponding rule target sides into each hyper-node as done by line 5 of Algorithm 1." ></td>
	<td class="line x" title="124:261	Any hyper-path from the root to any hypernode (not necessarily be a leaf of the hyper-tree) in a hyper-tree can represent a tree fragment." ></td>
	<td class="line x" title="125:261	As a result, the hyper-tree in Fig." ></td>
	<td class="line x" title="126:261	11 can represent up to 6 candidate tree fragments." ></td>
	<td class="line x" title="127:261	It is easy to understand that the maximum number of tree fragments that a hyper-tree can represent is equal to the number of hyper-nodes in it except the root." ></td>
	<td class="line x" title="128:261	It is worth noting that a hyper-node in a hyper-tree without any target side rule attached means there is no translation rule corresponding to the tree fragment represented by the hyper-path from the root to the current hyper-node." ></td>
	<td class="line x" title="129:261	The compact representation of the rule set by hyper-tree enables a fast algorithm to do translation rule matching." ></td>
	<td class="line x" title="130:261	Algorithm 1." ></td>
	<td class="line x" title="131:261	Compile rule set into hyper-tree Input: rule set Output: hyper-tree  1." ></td>
	<td class="line x" title="132:261	Initialize hyper-tree as a TOP node 2." ></td>
	<td class="line x" title="133:261	for  each rule in rule set  do 3." ></td>
	<td class="line x" title="134:261	Convert the left hand side tree to a hyper-path p 4." ></td>
	<td class="line x" title="135:261	Add hyper-path p into hyper-tree 5." ></td>
	<td class="line x" title="136:261	Add rules right hand side to the leaf hyper-node of a hyper-path in the hyper-tree 6." ></td>
	<td class="line x" title="137:261	end for  Algorithm  2." ></td>
	<td class="line x" title="138:261	Add hyper-path into hyper-tree Input: hyper-path p and hyper-tree t Notation:    h: the height of hyper-path p    p(i) : the hyper-node of ith level (top-down) of p    TN: the hyper-node in hyper-tree Output: updated hyper-tree t  1." ></td>
	<td class="line x" title="139:261	Initialize TN as TOP 2." ></td>
	<td class="line x" title="140:261	for  i := 1 to h  do 3." ></td>
	<td class="line x" title="141:261	if there is a child c of TN has the same label as p(i)               then 4." ></td>
	<td class="line x" title="142:261	TN := c 5." ></td>
	<td class="line x" title="143:261	else 6." ></td>
	<td class="line x" title="144:261	Add a child c to TN, label c as p(i) 7." ></td>
	<td class="line x" title="145:261	TN := c 4.3 Translation rule matching between forest and hyper-tree Given the source parse forest and the translation rules represented in the hyper-tree structure, here we present a fast matching algorithm to extract socalled useful translation rules from the entire rule set in a top-down manner for each node of the forest." ></td>
	<td class="line x" title="146:261	As shown in Algorithm 3, the general process of the matching algorithm is as follows:  1041 Algorithm 3." ></td>
	<td class="line x" title="147:261	Rule matching on one node Input: hyper-tree T, forest F, and node n Notation:       FP: a pair <FNS, TN>, FNS is the frontier nodes of              matched tree fragment,              TN is the hyper-tree node matching it       SFP: the queue of FP Output: Available rules on node n  1." ></td>
	<td class="line x" title="148:261	if there is no child c of TOP having the same label as n    then 2." ></td>
	<td class="line x" title="149:261	Return failure." ></td>
	<td class="line x" title="150:261	3." ></td>
	<td class="line x" title="151:261	else 4." ></td>
	<td class="line x" title="152:261	Initialize FP as <{n},c> and put it into SFP 5." ></td>
	<td class="line x" title="153:261	for each FP in SFP do 6." ></td>
	<td class="line x" title="154:261	SFP  PropagateNextLevel(FP.FNS, FP.TN) 7." ></td>
	<td class="line x" title="155:261	for each FP in SFP do 8." ></td>
	<td class="line x" title="156:261	if the rule set attached to FP.TN is not empty          then 9." ></td>
	<td class="line x" title="157:261	Add FP to result  Algorithm 4." ></td>
	<td class="line x" title="158:261	PropagateNextLevel Input: Frontier node sequence FNS, hyper-tree node TN Notation:            CT: a child node of TN                   the number of node sequence (separated by                   comma, see Fig 11) in CT is equal to the number                   of node in TN." ></td>
	<td class="line x" title="159:261	CT(i) : the ith node sequence in hyper-node CT            FNS(i): the ith node in FNS            TFNS: the temporary set of frontier node sequence            RFNS: the result set of frontier node sequence            FP:  a pair of frontier node sequence                    and hyper-tree node            RFP: the result set of FP Output: RFP  1." ></td>
	<td class="line x" title="160:261	for each child hyper-node CT of TN do 2." ></td>
	<td class="line x" title="161:261	for i:= 1 to the number of node sequence in CT do 3." ></td>
	<td class="line x" title="162:261	empty TFNS 4." ></td>
	<td class="line x" title="163:261	if CT(i) ==  then 5." ></td>
	<td class="line x" title="164:261	Add FNS(i) to TFNS." ></td>
	<td class="line x" title="165:261	6." ></td>
	<td class="line x" title="166:261	else 7." ></td>
	<td class="line x" title="167:261	for each hyper-edge e attached to FNS(i) do 8." ></td>
	<td class="line x" title="168:261	if e.children match CT(i) then 9." ></td>
	<td class="line x" title="169:261	Add e.children to TFNS 10." ></td>
	<td class="line x" title="170:261	if TFNS is empty then 11." ></td>
	<td class="line x" title="171:261	empty RFNS 12." ></td>
	<td class="line x" title="172:261	break 13." ></td>
	<td class="line x" title="173:261	else if i == 1 then 14." ></td>
	<td class="line x" title="174:261	RFNS := TFNS 15." ></td>
	<td class="line x" title="175:261	else 16." ></td>
	<td class="line x" title="176:261	RFNS := RFNS  TFNS 17." ></td>
	<td class="line x" title="177:261	for each FNS in RFNS do 18." ></td>
	<td class="line x" title="178:261	add <FNS, CT > into RFP  1) For each node n of the source forest if no child node of TOP in hyper-tree has the same label with it, it means that no rule matches any tree fragments rooted at the node n (i.e., no useful rules to be used for the node n) (line 1-2) 2) Otherwise, we match the sub-forest starting from the node n against a sub-hyper-tree starting from the matchable child node of TOP layer by layer in a top-down manner." ></td>
	<td class="line x" title="179:261	There may be many possible tree fragments rooted at node n and each of them may have multiple useful translation rules." ></td>
	<td class="line x" title="180:261	In our implementation, we maintain a data structure of FP = <FNS, TN> to record the currently matched tree fragment of forest and its corresponding hyper-tree node in the rule set, where FNS is the frontier node set of the current tree fragment and TN is the hyper-tree node." ></td>
	<td class="line x" title="181:261	The data structure FP is used to help extract useful translation rules and is also used for further matching of larger tree fragments." ></td>
	<td class="line x" title="182:261	Finally, all the FPs for the node n are kept in a queue." ></td>
	<td class="line x" title="183:261	During the search, the queue size is dynamically increased." ></td>
	<td class="line x" title="184:261	The matching algorithm terminates when all the FPs have been visited (line 5-6 and Algorithm 4)." ></td>
	<td class="line x" title="185:261	3) In the final queue, each element (FP) of the queue contains the frontier node sequence of the matched tree fragment and its corresponding hyper-tree node." ></td>
	<td class="line x" title="186:261	If the target side of a rule in the hyper-tree node is not empty, we just output the frontier nodes of the matched tree fragment, its root node n and all the useful translation rules for later translation process." ></td>
	<td class="line x" title="187:261	Algorithm 4 describes the detailed process of how to propagate the matching process down to the next level." ></td>
	<td class="line x" title="188:261	<FNS, TN> is the current level frontier node sequence and hyper-tree node." ></td>
	<td class="line x" title="189:261	Given a child hyper-node CT of TN (line 1), we try to find the group of next level frontier node sequence to match it (line 2-18)." ></td>
	<td class="line x" title="190:261	As shown in Fig 11, a hyper-node consists of a sequence of node sequence with comma as delimiter." ></td>
	<td class="line x" title="191:261	For the i th  node sequence CT(i) in CT, If CT(i) is , that means FNS(i) is a leaf/frontier node in the matched tree fragment and thus no need to propagate to the next level (line 4-5)." ></td>
	<td class="line x" title="192:261	Otherwise, we try each hyperedge e of FNS(i) to see whether its children match CT(i), and put the children of the matched hyperedge into a temp set TFNS (line 7-9)." ></td>
	<td class="line x" title="193:261	If the temp set is empty, that means the current matching fails and no further expansion needs (line 10-12)." ></td>
	<td class="line x" title="194:261	Otherwise, we integrate current matched children into the final group of frontier node sequence (line 1316) by Descartes Product ( )." ></td>
	<td class="line x" title="195:261	Finally, we construct all the <FNS, TN> pair for next level matching (line 17-18)." ></td>
	<td class="line x" title="196:261	It would be interesting to study the time complexity of our Algorithm 3 and 4." ></td>
	<td class="line x" title="197:261	Suppose the maximum number of children of each hyper-node in hyper-tree is N (line 1), the maximum number of node sequence in CT is M (line 2), the maximum number of hyper-edge in each node in packed forest is K (line 7), the maximum number of hyper-edge with same children representation in each node in packed forest is C (i.e. the maximum size of TFNS in line 16, and the maximum complexity of the Descartes Product in line 16 1042 would be C M ), then the time complexity upperbound of Algorithm 4 is O(NM(K+C M ))." ></td>
	<td class="line x" title="198:261	For Algorithm 3, its time complexity is O(RNM(K+C M )), where R is the maximum number of tree fragment matched in each node." ></td>
	<td class="line x" title="199:261	5 Experiment 5.1 Experimental settings We carry out experiment on Chinese-English NIST evaluation tasks." ></td>
	<td class="line x" title="200:261	We use FBIS corpus (250K sentence pairs) as training data with the source side parsed by a modified Charniak parser (Charniak 2000) which can output a packed forest." ></td>
	<td class="line x" title="201:261	The Charniak Parser is trained on CTB5, tuned on 301-325 portion, with F1 score of 80.85% on 271300 portion." ></td>
	<td class="line x" title="202:261	We use GIZA++ (Och and Ney, 2003) to do m-to-n word-alignment and adopt heuristic grow-diag-final-and to do refinement." ></td>
	<td class="line x" title="203:261	A 4-gram language model is trained on Gigaword 3 Xinhua portion by SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing." ></td>
	<td class="line x" title="204:261	We use NIST 2002 as development set and NIST 2003 as test set." ></td>
	<td class="line x" title="205:261	The feature weights are tuned by the modified Koehns MER (Och, 2003, Koehn, 2007) trainer." ></td>
	<td class="line x" title="206:261	We use case-sensitive BLEU-4 (Papineni et al., 2002) to measure the quality of translation result." ></td>
	<td class="line x" title="207:261	Zhang et al. 2004s implementation is used to do significant test." ></td>
	<td class="line x" title="208:261	Following (Mi and Huang 2008), we use viterbi algorithm to prune the forest." ></td>
	<td class="line x" title="209:261	Instead of using a static pruning threshold (Mi and Huang 2008), we set the threshold as the distance of the probabilities of the n th  best tree and the 1 st  best tree." ></td>
	<td class="line x" title="210:261	It means the pruned forest is able to at least keep all the top n best trees." ></td>
	<td class="line x" title="211:261	However, because of the sharing nature of the packed forest, it may still contain a large number of additional trees." ></td>
	<td class="line x" title="212:261	Our statistic shows that when we set the threshold as the 100 th  best tree, the average number of all possible trees in the forest is 1.2*10 5  after pruning." ></td>
	<td class="line x" title="213:261	In our experiments, we compare our algorithm with the two traditional algorithms as discussed in section 3." ></td>
	<td class="line x" title="214:261	For the Exhaustive search by tree algorithm, we use a bottom-up dynamic programming algorithm to generate all the candidate tree fragments rooted at each node." ></td>
	<td class="line x" title="215:261	For the Exhaustive search by rule algorithm, we group all rules with the same left hand side in order to remove the duplicated matching for the same left hand side rules." ></td>
	<td class="line x" title="216:261	All these settings aim for fair comparison." ></td>
	<td class="line x" title="217:261	5.2 Accuracy, speed vs. rule heights We first compare the three algorithms performance by setting the maximum rule height from 1 to 5." ></td>
	<td class="line x" title="218:261	We set the forest pruning threshold to the 100 th  best parse tree." ></td>
	<td class="line x" title="219:261	Table 1 compares the speed of the three algorithms." ></td>
	<td class="line x" title="220:261	It clearly shows that the speed of both of the two traditional algorithms increases dramatically while the speed of our hyper-tree based algorithm is almost linear to the tree height." ></td>
	<td class="line x" title="221:261	In the case of rule height of 5, the hyper-tree algorithm is at least 19 times (9.329/0.486) faster than the two traditional algorithms and saves 8.843(9.329 0.486) seconds in rule matching for each sentence on average, which contributes 57% (8.843/(9.329 + 6.21)) speed improvement to the overall translation." ></td>
	<td class="line x" title="222:261	H Rule Matching D Exhaustive by tree Exhaustive by rule Hypertreebased 1 0.043 0.077 0.083   2.96 2 0.047 0.920 0.173   3.56 3 0.237 9.572 0.358   4.02 4 2.300 48.90 0.450   5.27 5 9.329 90.80 0.486   6.21  Table 1." ></td>
	<td class="line x" title="223:261	Speed in seconds per sentence vs. rule height; H is rule height, D represents the decoding time after rule matching   Height BLEU 1 0.1646 2 0.2498 3 0.2824 4 0.2874 5 0.2925 Moses 0.2625  Table 2." ></td>
	<td class="line x" title="224:261	BLEU vs. rule height  Table 2 reports the BLEU score with different rule heights, where Moses, a state-of-the-art phrase-based SMT system, serves as the baseline system." ></td>
	<td class="line x" title="225:261	It shows the BLEU score consistently improves as the rule height increases." ></td>
	<td class="line x" title="226:261	In addition, one can see that the rules with maximum height of 5 are able to outperform the rules with maximum height of 3 by 1 BLEU score (p<0.05) and significantly outperforms Moses by 3 BLEU score (p<0.01)." ></td>
	<td class="line x" title="227:261	To our knowledge, this is the first time to report the performance of rules up to height of 5 for forest-based translation model." ></td>
	<td class="line x" title="228:261	1043 We also study the distribution of the rules used in the 1-best translation output." ></td>
	<td class="line x" title="229:261	The results are shown in Table 3; we could see something interesting that is as the rule height increases, the total number of rules with that height decreases, while the percentage of partial-lexicalized increases dramatically." ></td>
	<td class="line x" title="230:261	And one thing needs to note is the percentage of partial-lexicalized rules with height of 1 is 0, since there is no partial-lexicalized rule with height of 1 in the rule set (the father node of a word is a pos tag node)." ></td>
	<td class="line x" title="231:261	H Total Rule Type Percentage (%) F P U 1 9814   76.58     0 23.42 2 5289   44.99     46.40 8.60 3 3925   18.39     77.25 4.35 4 1810   7.90      87.68 4.41 5 511    6.46 90.50 3.04  Table 3." ></td>
	<td class="line x" title="232:261	statistics of rules used in the 1-best translation output, F means full-lexicalized, P means partial-lexicalized, U means unlexiclaizd." ></td>
	<td class="line x" title="233:261	5.3 Speed vs. forest pruning threshold This section studies the impact of the forest pruning threshold on the rule matching speed when setting the maximum rule height to 5." ></td>
	<td class="line x" title="234:261	Threshold Rule Matching Exhaustive by tree Exhaustive by rule Hypertreebased 1 1.2 23.66 0.171 10 3.1 36.42 0.234 50 5.7 66.20 0.405 100 9.3 90.80 0.486 200 27.3 104.86 0.598 500 133.6 148.54 0.873  Table 4." ></td>
	<td class="line x" title="235:261	Speed in seconds per sentence vs. forest  pruning threshold  In Table 4, we can see that our hyper-tree based algorithm is the fastest among the three algorithms in all pruning threshold settings and even 150 times faster than both of the two traditional algorithms with threshold of 500 th  best." ></td>
	<td class="line x" title="236:261	Table 5 shows the average number of parse trees embedded in a packed forest with different pruning thresholds per sentence." ></td>
	<td class="line x" title="237:261	We can see that the number of trees increases exponentially when the pruning threshold increases linearly." ></td>
	<td class="line x" title="238:261	When the threshold is 500 th  best, the average number of trees per sentence is 1.49*10 9 . However, even in this extreme case, the hyper-tree based algorithm is still capable of completing rule matching within 1 second." ></td>
	<td class="line x" title="239:261	Threshold Number of Trees 1 1 10 32 50 5922 100 128860 200 2.75*10 6  500 1.49*10 9   Table 5." ></td>
	<td class="line x" title="240:261	Average number of trees in packed forest with different pruning threshold." ></td>
	<td class="line x" title="241:261	5.4 Hyper-tree compression rate As we describe in section 4.2, theoretically the number of tree fragments that a hyper-tree can represent is equal to the number of hyper-nodes in it." ></td>
	<td class="line x" title="242:261	However, in real rule set, there is no guarantee that each tree fragment in the hyper-tree has corresponding translation rules." ></td>
	<td class="line x" title="243:261	To gain insights into how effective the compact representation of the hyper-tree and how many hyper-nodes without translation rules, we define the compression rate as follows." ></td>
	<td class="line x" title="244:261	Table 6 reports the different statistics on the rule sets with different maximum rule heights ranging from 1 to 5." ></td>
	<td class="line x" title="245:261	The reported statistics are the number of rules, the number of unique left hand side (since there may be more than one rules having the same left hand side), the number of hypernodes and the compression rate." ></td>
	<td class="line x" title="246:261	H n_rules n_LHS n_nodes c_rate 1 21588 10779 10779 100% 2 141632 51807 51903 99.8% 3 1.73*10 6  491268 494919 99.2% 4 8.65*10 6  2052731 2083296 98.5% 5 1.89*10 7  3966742 4043824 98.1%  Table 6." ></td>
	<td class="line x" title="247:261	Statistics of rule set and hyper-tree." ></td>
	<td class="line x" title="248:261	H is rule height, n_rules is the number of rules, n_LHS is the number of unique left hand side, n_nodes is the number of hyper-nodes in hypertree and c_rate is the compression rate." ></td>
	<td class="line x" title="249:261	Table 6 shows that in all the five cases, the compression rates of hyper-tree are all more than 1044 98%." ></td>
	<td class="line x" title="250:261	It means that almost all the tree fragments embedded in the hyper-tree have corresponding translation rules." ></td>
	<td class="line x" title="251:261	As a result, we are able to use almost only one hyper-edge (i.e. only the frontier nodes of a tree fragment without any internal nodes) to represent all the rules with the same left hand side." ></td>
	<td class="line x" title="252:261	This suggests that our hyper-tree is particularly effective in representing the tree translation rules compactly." ></td>
	<td class="line x" title="253:261	It also shows that there are a lot of common parts among different translation rules." ></td>
	<td class="line x" title="254:261	All the experiments reported in this section convincingly demonstrate the effectiveness of our proposed hyper-tree representation of translation rules and the hyper-tree-based rule matching algorithm." ></td>
	<td class="line x" title="255:261	6 Conclusion In this paper2, we propose the concept of hypertree for compact rule representation and a hypertree-based fast algorithm for translation rule matching in a forest-based translation system." ></td>
	<td class="line x" title="256:261	We compare our algorithm with two previous widelyused rule matching algorithms." ></td>
	<td class="line x" title="257:261	Experimental results on the NIST Chinese-English MT 2003 evaluation data set show the rules with maximum rule height of 5 outperform those with height 3 by 1.0 BLEU and outperform MOSES by 3.0 BLEU." ></td>
	<td class="line x" title="258:261	In the same test cases, our algorithm is at least 19 times faster than the two traditional algorithms, and contributes 57% speed improvement to the overall translation." ></td>
	<td class="line x" title="259:261	We also show that in a more challenging setting (forest containing 1.49*10 9 trees on average) our algorithm is 150 times faster than the two traditional algorithms." ></td>
	<td class="line x" title="260:261	Finally, we show that the hyper-tree structure has more than 98% compression rate." ></td>
	<td class="line x" title="261:261	It means the compact representation by the hyper-tree is very effective for translation rules." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1127
Bilingually-Constrained (Monolingual) Shift-Reduce Parsing
Huang, Liang;Jiang, Wenbin;Liu, Qun;"></td>
	<td class="line x" title="1:211	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 12221231, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:211	c 2009 ACL and AFNLP Bilingually-Constrained (Monolingual) Shift-Reduce Parsing Liang Huang Google Research 1350 Charleston Rd. Mountain View, CA 94043, USA lianghuang@google.com liang.huang.sh@gmail.com Wenbin Jiang and Qun Liu Key Lab." ></td>
	<td class="line x" title="3:211	of Intelligent Information Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China jiangwenbin@ict.ac.cn Abstract Jointly parsing two languages has been shown to improve accuracies on either or both sides." ></td>
	<td class="line x" title="4:211	However, its search space is much bigger than the monolingual case, forcing existing approaches to employ complicated modeling and crude approximations." ></td>
	<td class="line x" title="5:211	Here we propose a much simpler alternative, bilingually-constrained monolingual parsing, where a source-language parser learns to exploit reorderings as additional observation, but not bothering to build the target-side tree as well." ></td>
	<td class="line x" title="6:211	We show specifically how to enhance a shift-reduce dependency parser with alignment features to resolve shift-reduce conflicts." ></td>
	<td class="line x" title="7:211	Experiments on the bilingual portion of Chinese Treebank show that, with just 3 bilingual features, we can improve parsing accuracies by 0.6% (absolute) for both English and Chinese over a state-of-the-art baseline, with negligible (6%) efficiency overhead, thus much faster than biparsing." ></td>
	<td class="line x" title="8:211	1 Introduction Ambiguity resolution is a central task in Natural Language Processing." ></td>
	<td class="line x" title="9:211	Interestingly, not all languages are ambiguous in the same way." ></td>
	<td class="line x" title="10:211	For example, prepositional phrase (PP) attachment is (notoriously) ambiguous in English (and related European languages), but is strictly unambiguous in Chinese and largely unambiguous Japanese; see (1a) I [ saw Bill ] [ with a telescope ]." ></td>
	<td class="line x" title="11:211	wo [ yong wangyuanjin] [kandao le Bier]." ></td>
	<td class="line x" title="12:211	I used a telescope to see Bill. (1b) I saw [ Bill [ with a telescope ] ]." ></td>
	<td class="line x" title="13:211	wo kandao le [ [ na wangyuanjin ] de Bier]." ></td>
	<td class="line x" title="14:211	I saw Bill who had a telescope at hand. Figure 1: PP-attachment is unambiguous in Chinese, which can help English parsing." ></td>
	<td class="line x" title="15:211	Figure 1 for an example.1 It is thus intuitive to use two languages for better disambiguation, which has been applied not only to this PP-attachment problem (Fossum and Knight, 2008; Schwartz et al., 2003), but also to the more fundamental problem of syntactic parsing which subsumes the former as a subproblem." ></td>
	<td class="line oc" title="16:211	For example, Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints, which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al., 2004; Mi and Huang, 2008)." ></td>
	<td class="line x" title="17:211	However, the search space of joint parsing is inevitably much bigger than the monolingual case, 1Chinese uses word-order to disambiguate the attachment (see below)." ></td>
	<td class="line x" title="18:211	By contrast, Japanese resorts to case-markers and the unambiguity is limited: it works for the V or N attachment ambiguities like in Figure 1 (see (Schwartz et al., 2003)) but not for the N1 or N2 case (Mitch Marcus, p.c.)." ></td>
	<td class="line x" title="19:211	1222 forcing existing approaches to employ complicated modeling and crude approximations." ></td>
	<td class="line x" title="20:211	Joint parsing with a simplest synchronous context-free grammar (Wu, 1997) is O(n6) as opposed to the monolingual O(n3) time." ></td>
	<td class="line oc" title="21:211	To make things worse, languages are non-isomorphic, i.e., there is no 1to-1 mapping between tree nodes, thus in practice one has to use more expressive formalisms such as synchronous tree-substitution grammars (Eisner, 2003; Galley et al., 2004)." ></td>
	<td class="line x" title="22:211	In fact, rather than joint parsing per se, Burkett and Klein (2008) resort to separate monolingual parsing and bilingual reranking over k2 tree pairs, which covers a tiny fraction of the whole space (Huang, 2008)." ></td>
	<td class="line x" title="23:211	We instead propose a much simpler alternative, bilingually-constrained monolingual parsing, where a source-language parser is extended to exploit the reorderings between languages as additional observation, but not bothering to build a tree for the target side simultaneously." ></td>
	<td class="line x" title="24:211	To illustrate the idea, suppose we are parsing the sentence (1) I saw Bill [PP with a telescope ]." ></td>
	<td class="line x" title="25:211	which has 2 parses based on the attachment of PP: (1a) I [ saw Bill ] [PP with a telescope ]." ></td>
	<td class="line x" title="26:211	(1b) I saw [ Bill [PP with a telescope ]]." ></td>
	<td class="line x" title="27:211	Both are possible, but with a Chinese translation the choice becomes clear (see Figure 1), because a Chinese PP always immediately precedes the phrase it is modifying, thus making PP-attachment strictly unambiguous.2 We can thus use Chinese to help parse English, i.e., whenever we have a PPattachment ambiguity, we will consult the Chinese translation (from a bitext), and based on the alignment information, decide where to attach the English PP." ></td>
	<td class="line x" title="28:211	On the other hand, English can help Chinese parsing as well, for example in deciding the scope of relative clauses which is unambiguous in English but ambiguous in Chinese." ></td>
	<td class="line x" title="29:211	This method is much simpler than joint parsing because it remains monolingual in the backbone, with alignment information merely as soft evidence, rather than hard constraints since automatic word alignment is far from perfect." ></td>
	<td class="line x" title="30:211	It is thus 2to be precise, in Fig." ></td>
	<td class="line x" title="31:211	1(b), the English PP is translated into a Chinese relative clause, but nevertheless all phrasal modifiers attach to the immediate right in Mandarin Chinese." ></td>
	<td class="line x" title="32:211	straightforward to implement within a monolingual parsing algorithm." ></td>
	<td class="line x" title="33:211	In this work we choose shift-reduce dependency parsing for its simplicity and efficiency." ></td>
	<td class="line x" title="34:211	Specifically, we make the following contributions:  we develop a baseline shift-reduce dependency parser using the less popular, but classical, arc-standard style (Section 2), and achieve similar state-of-the-art performance with the the dominant but complicated arceager style of Nivre and Scholz (2004);  we propose bilingual features based on wordalignment information to prefer target-side contiguity in resolving shift-reduce conflicts (Section 3);  we verify empirically that shift-reduce conflicts are the major source of errors, and correct shift-reduce decisions strongly correlate with the above bilingual contiguity conditions even with automatic alignments (Section 5.3);  finally, with just three bilingual features, we improve dependency parsing accuracy by 0.6% for both English and Chinese over the state-of-the-art baseline with negligible (6%) efficiency overhead (Section 5.4)." ></td>
	<td class="line x" title="35:211	2 Simpler Shift-Reduce Dependency Parsing with Three Actions The basic idea of classical shift-reduce parsing from compiler theory (Aho and Ullman, 1972) is to perform a left-to-right scan of the input sentence, and at each step, choose one of the two actions: either shift the current word onto the stack, or reduce the top two (or more) items on the stack, replacing them with their combination." ></td>
	<td class="line x" title="36:211	This idea has been applied to constituency parsing, for example in Sagae and Lavie (2006), and we describe below a simple variant for dependency parsing similar to Yamada and Matsumoto (2003) and the arc-standard version of Nivre (2004)." ></td>
	<td class="line x" title="37:211	2.1 The Three Actions Basically, we just need to split the reduce action into two symmetric (sub-)actions, reduceL and reduceR, depending on which one of the two 1223 stack queue arcs previous S wi|Q A shift S|wi Q A previous S|st1|st Q A reduceL S|st Q A{(st,st1)} reduceR S|st1 Q A{(st1,st)} Table 1: Formal description of the three actions." ></td>
	<td class="line x" title="38:211	Note that shift requires non-empty queue while reduce requires at least two elements on the stack." ></td>
	<td class="line x" title="39:211	items becomes the head after reduction." ></td>
	<td class="line x" title="40:211	More formally, we describe a parser configuration by a tupleS,Q,Awhere S is the stack, Q is the queue of remaining words of the input, and A is the set of dependency arcs accumulated so far.3 At each step, we can choose one of the three actions: 1." ></td>
	<td class="line x" title="41:211	shift: move the head of (a non-empty) queue Q onto stack S; 2." ></td>
	<td class="line x" title="42:211	reduceL: combine the top two items on the stack, st and st1 (t  2), and replace them with st (as the head), and add a left arc (st,st1) to A; 3." ></td>
	<td class="line x" title="43:211	reduceR: combine the top two items on the stack, st and st1 (t2), and replace them with st1 (as the head), and add a right arc (st1,st) to A. These actions are summarized in Table 1." ></td>
	<td class="line x" title="44:211	The initial configuration is always ,w1 wn, with empty stack and no arcs, and the final configuration iswj,,Awhere wj is recognized as the root of the whole sentence, and A encodes a spanning tree rooted at wj." ></td>
	<td class="line x" title="45:211	For a sentence of n words, there are exactly 2n1 actions: n shifts and n1 reductions, since every word must be pushed onto stack once, and every word except the root will eventually be popped in a reduction." ></td>
	<td class="line x" title="46:211	The time complexity, as other shift-reduce instances, is clearly O(n)." ></td>
	<td class="line x" title="47:211	2.2 Example of Shift-Reduce Conflict Figure 2 shows the trace of this paradigm on the example sentence." ></td>
	<td class="line x" title="48:211	For the first two configurations 3a configuration is sometimes called a state (Zhang and Clark, 2008), but that term is confusing with the states in shift-reduce LR/LL parsing, which are quite different." ></td>
	<td class="line x" title="49:211	0 I saw Bill with a  1 shift I saw Bill with a  2 shift I saw Bill with a  3 reduceL saw Bill with a  I 4 shift saw Bill with a  I 5a reduceR saw with a  I Bill 5b shift saw Bill with a  I Figure 2: A trace of 3-action shift-reduce on the example sentence." ></td>
	<td class="line x" title="50:211	Shaded words are on stack, while gray words have been popped from stack." ></td>
	<td class="line x" title="51:211	After step (4), the process can take either (5a) or (5b), which correspond to the two attachments (1a) and (1b) in Figure 1, respectively." ></td>
	<td class="line x" title="52:211	(0) and (1), only shift is possible since there are not enough items on the stack for reduction." ></td>
	<td class="line x" title="53:211	At step (3), we perform a reduceL, making word I a modifier of saw; after that the stack contains a single word and we have to shift the next word Bill (step 4)." ></td>
	<td class="line x" title="54:211	Now we face a shift-reduce conflict: we can either combine saw and Bill in a reduceR action (5a), or shift Bill (5b)." ></td>
	<td class="line x" title="55:211	We will use features extracted from the configuration to resolve the conflict." ></td>
	<td class="line x" title="56:211	For example, one such feature could be a bigram stst1, capturing how likely these two words are combined; see Table 2 for the complete list of feature templates we use in this baseline parser." ></td>
	<td class="line x" title="57:211	We argue that this kind of shift-reduce conflicts are the major source of parsing errors, since the other type of conflict, reduce-reduce conflict (i.e., whether left or right) is relatively easier to resolve given the part-of-speech information." ></td>
	<td class="line x" title="58:211	For example, between a noun and an adjective, the former is much more likely to be the head (and so is a verb vs. a preposition or an adverb)." ></td>
	<td class="line x" title="59:211	Shift-reduce resolution, however, is more non-local, and often involves a triple, for example, (saw, Bill, with) for a typical PP-attachment." ></td>
	<td class="line x" title="60:211	On the other hand, if we indeed make a wrong decision, a reduce-reduce mistake just flips the head and the modifier, and often has a more local effect on the shape of the tree, whereas a shift-reduce mistake always leads 1224 Type Features Unigram st T(st) stT(st) st1 T(st1) st1T(st1) wi T(wi) wiT(wi) Bigram stst1 T(st)T(st1) T(st)T(wi) T(st)st1T(st1) stst1T(st1) stT(st)T(st1) stT(st)st1 stT(st)st1T(st1) Trigram T(st)T(wi)T(wi+1) T(st1)T(st)T(wi) T(st2)T(st1)T(st) stT(wi)T(wi+1) T(st1)stT(wi) Modifier T(st1)T(lc(st1))T(st) T(st1)T(rc(st1))T(st) T(st1)T(st)T(lc(st)) T(st1)T(st)T(rc(st)) T(st1)T(lc(st1))st T(st1)T(rc(st1))st T(st1)stT(lc(st)) Table 2: Feature templates of the baseline parser." ></td>
	<td class="line x" title="61:211	st, st1 denote the top and next to top words on the stack; wi and wi+1 denote the current and next words on the queue." ></td>
	<td class="line x" title="62:211	T() denotes the POS tag of a given word, and lc() and rc() represent the leftmost and rightmost child." ></td>
	<td class="line x" title="63:211	Symbol  denotes feature conjunction." ></td>
	<td class="line x" title="64:211	Each of these templates is further conjoined with the 3 actions shift, reduceL, and reduceR." ></td>
	<td class="line x" title="65:211	to vastly incompatible tree shapes with crossing brackets (for example, [saw Bill] vs. [Bill with a telescope])." ></td>
	<td class="line x" title="66:211	We will see in Section 5.3 that this is indeed the case in practice, thus suggesting us to focus on shift-reduce resolution, which we will return to with the help of bilingual constraints in Section 3." ></td>
	<td class="line x" title="67:211	2.3 Comparison with Arc-Eager The three action system was originally described by Yamada and Matsumoto (2003) (although their methods require multiple passes over the input), and then appeared as arc-standard in Nivre (2004), but was argued against in comparison to the four-action arc-eager variant." ></td>
	<td class="line x" title="68:211	Most subsequent works on shift-reduce or transition-based dependency parsing followed arc-eager (Nivre and Scholz, 2004; Zhang and Clark, 2008), which now becomes the dominant style." ></td>
	<td class="line x" title="69:211	But we argue that arc-standard is preferable because: 1." ></td>
	<td class="line x" title="70:211	in the three action arc-standard system, the stack always contains a list of unrelated subtrees recognized so far, with no arcs between any of them, e.g.(Isaw) and (Bill) in step 4 of Figure 2), whereas the four action arceager style can have left or right arrows between items on the stack; 2." ></td>
	<td class="line x" title="72:211	the semantics of the three actions are atomic and disjoint, whereas the semantics of 4 actions are not completely disjoint." ></td>
	<td class="line x" title="73:211	For example, their Left action assumes an implicit Reduce of the left item, and their Right action assumes an implicit Shift." ></td>
	<td class="line x" title="74:211	Furthermore, these two actions have non-trivial preconditions which also causes the next problem (see below)." ></td>
	<td class="line x" title="75:211	We argue that this is rather complicated to implement." ></td>
	<td class="line x" title="76:211	3." ></td>
	<td class="line x" title="77:211	the arc-standard scan always succeeds, since at the end we can always reduce with empty queue, whereas the arc-eager style sometimes goes into deadends where no action can perform (prevented by preconditions, otherwise the result will not be a wellformed tree)." ></td>
	<td class="line x" title="78:211	This becomes parsing failures in practice (Nivre and Scholz, 2004), leaving more than one fragments on stack." ></td>
	<td class="line x" title="79:211	As we will see in Section 5.1, this simpler arc-standard system performs equally well with a state-of-the-art arc-eager system (Zhang and Clark, 2008) on standard English Treebank parsing (which is never shown before)." ></td>
	<td class="line x" title="80:211	We argue that all things being equal, this simpler paradigm should be preferred in practice." ></td>
	<td class="line x" title="81:211	4 2.4 Beam Search Extension We also enhance deterministic shift-reduce parsing with beam search, similar to Zhang and Clark (2008), where k configurations develop in parallel." ></td>
	<td class="line x" title="82:211	Pseudocode 1 illustrates the algorithm, where we keep an agenda V of the current active configurations, and at each step try to extend them by applying one of the three actions." ></td>
	<td class="line x" title="83:211	We then dump the best k new configurations from the buffer back 4On the other hand, there are also arguments for arceager, e.g., incrementality; see (Nivre, 2004; Nivre, 2008)." ></td>
	<td class="line x" title="84:211	1225 Pseudocode 1 beam-search shift-reduce parsing." ></td>
	<td class="line x" title="85:211	1: Input: POS-tagged word sequence w1 wn 2: start ,w1 wn,  initial config: empty stack, no arcs 3: V{start}  initial agenda 4: for step12n1 do 5: BUF  buffer for new configs 6: for each config in agenda V do 7: for act {shift, reduceL, reduceR}do 8: if act is applicable to config then 9: next apply act to config 10: insert next into buffer BUF 11: Vtop k configurations of BUF 12: Output: the tree of the best config in V into the agenda for the next step." ></td>
	<td class="line x" title="86:211	The complexity of this algorithm is O(nk), which subsumes the determinstic mode as a special case (k = 1)." ></td>
	<td class="line x" title="87:211	2.5 Online Training To train the parser we need an oracle or goldstandard action sequence for gold-standard dependency trees." ></td>
	<td class="line x" title="88:211	This oracle turns out to be non-unique for the three-action system (also non-unique for the four-action system), because left dependents of a head can be reduced either before or after all right dependents are reduced." ></td>
	<td class="line x" title="89:211	For example, in Figure 2, I is a left dependent of saw, and can in principle wait until Bill and with are reduced, and then finally combine with saw." ></td>
	<td class="line x" title="90:211	We choose to use the heuristic of shortest stack that always prefers reduceL over shift, which has the effect that all left dependents are first recognized inside-out, followed by all right dependents, also inside-out, which coincides with the head-driven constituency parsing model of Collins (1999)." ></td>
	<td class="line x" title="91:211	We use the popular online learning algorithm of structured perceptron with parameter averaging (Collins, 2002)." ></td>
	<td class="line x" title="92:211	Following Collins and Roark (2004) we also use the early-update strategy, where an update happens whenever the goldstandard action-sequence falls off the beam, with the rest of the sequence neglected." ></td>
	<td class="line x" title="93:211	As a special case, for the deterministic mode, updates always co-occur with the first mistake made." ></td>
	<td class="line x" title="94:211	The intuition behind this strategy is that future mistakes are often caused by previous ones, so with the parser on the wrong track, future actions become irrelevant for learning." ></td>
	<td class="line x" title="95:211	See Section 5.3 for more discussions." ></td>
	<td class="line x" title="96:211	(a) I a58a58a58a58a58a58a58a58a58saw Bill with a telescope . wo yong wangyuanjin kandao le Bier." ></td>
	<td class="line x" title="97:211	c(st1,st) =+; reduce is correct (b) I a58a58a58a58a58a58a58a58a58saw Bill with a telescope . wo kandao le na wangyuanjin de Bier." ></td>
	<td class="line x" title="98:211	c(st1,st) =; reduce is wrong (c) I saw a58a58a58a58a58a58a58a58a58a58a58Bill witha58a58a58aa58a58a58a58a58a58a58a58a58a58telescopea58 . wo kandao le na wangyuanjin de Bier." ></td>
	<td class="line x" title="99:211	cR(st,wi) =+; shift is correct (d) I saw a58a58a58a58a58a58a58a58a58Bill witha58a58a58aa58a58a58a58a58a58a58a58a58a58telescopea58 . wo yong wangyuanjin kandao le Bier." ></td>
	<td class="line x" title="100:211	cR(st,wi) =; shift is wrong Figure 3: Bilingual contiguity features c(st1,st) and cR(st,wi) at step (4) in Fig." ></td>
	<td class="line x" title="101:211	2 (facing a shiftreduce decision)." ></td>
	<td class="line x" title="102:211	Bold words are currently on stack while gray ones have been popped." ></td>
	<td class="line x" title="103:211	Here the stack tops are st = Bill, st1 = saw, and the queue head is wi = with; underlined texts mark the source and target spans being considered, and wavy underlines mark the allowed spans (Tab." ></td>
	<td class="line x" title="104:211	3)." ></td>
	<td class="line x" title="105:211	Red bold alignment links violate contiguity constraints." ></td>
	<td class="line x" title="106:211	3 Soft Bilingual Constraints as Features As suggested in Section 2.2, shift-reduce conflicts are the central problem we need to address here." ></td>
	<td class="line x" title="107:211	Our intuition is, whenever we face a decision whether to combine the stack tops st1 and st or to shift the current word wi, we will consult the other language, where the word-alignment information would hopefully provide a preference, as in the running example of PP-attachment (see Figure 1)." ></td>
	<td class="line x" title="108:211	We now develop this idea into bilingual contiguity features." ></td>
	<td class="line x" title="109:211	1226 3.1 A Pro-Reduce Feature c(st1,st) Informally, if the correct decision is a reduction, then it is likely that the corresponding words of st1 and st on the target-side should also form a contiguous span." ></td>
	<td class="line x" title="110:211	For example, in Figure 3(a), the source span of a reduction is [saw  Bill], which maps onto [kandao . . ." ></td>
	<td class="line x" title="111:211	Bier] on the Chinese side." ></td>
	<td class="line x" title="112:211	This target span is contiguous, because no word within this span is aligned to a source word outside of the source span." ></td>
	<td class="line x" title="113:211	In this case we say feature c(st1,st) =+, which encourages reduce." ></td>
	<td class="line x" title="114:211	However, in Figure 3(b), the source span is still [saw  Bill], but this time maps onto a much longer span on the Chinese side." ></td>
	<td class="line x" title="115:211	This target span is discontiguous, since the Chinese words na and wangyuanjin are alinged to English with and telescope, both of which fall outside of the source span." ></td>
	<td class="line x" title="116:211	In this case we say feature c(st1,st) =, which discourages reduce . 3.2 A Pro-Shift Feature cR(st,wi) Similarly, we can develop another feature cR(st,wi) for the shift action." ></td>
	<td class="line x" title="117:211	In Figure 3(c), when considering shifting with, the source span becomes [Bill  with] which maps to [na  Bier] on the Chinese side." ></td>
	<td class="line x" title="118:211	This target span looks like discontiguous in the above definition with wangyuanjin aligned to telescope, but we tolerate this case for the following reasons." ></td>
	<td class="line x" title="119:211	There is a crucial difference between shift and reduce: in a shift, we do not know yet the subtree spans (unlike in a reduce we are always combining two well-formed subtrees)." ></td>
	<td class="line x" title="120:211	The only thing we are sure of in a shift action is that st and wi will be combined before st1 and st are combined (Aho and Ullman, 1972), so we can tolerate any target word aligned to source word still in the queue, but do not allow any target word aligned to an already recognized source word." ></td>
	<td class="line x" title="121:211	This explains the notational difference between cR(st,wi) and c(st1,st), where subscript R means right contiguity." ></td>
	<td class="line x" title="122:211	As a final example, in Figure 3(d), Chinese word kandao aligns to saw, which is already recognized, and this violates the right contiguity." ></td>
	<td class="line x" title="123:211	So cR(st,wi) =, suggesting that shift is probably wrong." ></td>
	<td class="line x" title="124:211	To be more precise, Table 3 shows the formal definitions of the two features." ></td>
	<td class="line x" title="125:211	We basically source target allowed feature f span sp span tp span ap c(st1,st) [st1st] M(sp) [st1st] cR(st,wi) [stwi] M(sp) [stwn] f = + iff." ></td>
	<td class="line x" title="126:211	M1(M(sp))ap Table 3: Formal definition of bilingual features." ></td>
	<td class="line x" title="127:211	M() is maps a source span to the target language, and M1() is the reverse operation mapping back to the source language." ></td>
	<td class="line x" title="128:211	map a source span sp to its target span M(sp), and check whether its reverse image back onto the source language M1(M(sp)) falls inside the allowed span ap." ></td>
	<td class="line x" title="129:211	For cR(st,wi), the allowed span extends to the right end of the sentence.5 3.3 Variations and Implementation To conclude so far, we have got two alignmentbased features, c(st1,st) correlating with reduce, and cR(st,wi) correlating with shift." ></td>
	<td class="line x" title="130:211	In fact, the conjunction of these two features, c(st1,st)cR(st,wi) is another feature with even stronger discrimination power." ></td>
	<td class="line x" title="131:211	If c(st1,st)cR(st,wi) = + it is strongly recommending reduce, while c(st1,st)cR(st,wi) =+ is a very strong signal for shift." ></td>
	<td class="line x" title="132:211	So in total we got three bilingual feature (templates), which in practice amounts to 24 instances (after cross-product with {,+} and the three actions)." ></td>
	<td class="line x" title="133:211	We show in Section 5.3 that these features do correlate with the correct shift/reduce actions in practice." ></td>
	<td class="line x" title="134:211	The naive implemention of bilingual feature computation would be of O(kn2) complexity in the worse case because when combining the largest spans one has to scan over the whole sentence." ></td>
	<td class="line oc" title="135:211	We envision the use of a clever datastructure would reduce the complexity, but leave this to future work, as the experiments (Table 8) show that 5Our definition implies that we only consider faithful spans to be contiguous (Galley et al., 2004)." ></td>
	<td class="line x" title="136:211	Also note that source spans include all dependents of st and st1." ></td>
	<td class="line x" title="137:211	1227 the parser is only marginally (6%) slower with the new bilingual features." ></td>
	<td class="line x" title="138:211	This is because the extra work, with just 3 bilingual features, is not the bottleneck in practice, since the extraction of the vast amount of other features in Table 2 dominates the computation." ></td>
	<td class="line x" title="139:211	4 Related Work in Grammar Induction Besides those cited in Section 1, there are some other related work on using bilingual constraints for grammar induction (rather than parsing)." ></td>
	<td class="line x" title="140:211	For example, Hwa et al.(2005) use simple heuristics to project English trees to Spanish and Chinese, but get discouraging accuracy results learned from those projected trees." ></td>
	<td class="line x" title="142:211	Following this idea, Ganchev et al.(2009) and Smith and Eisner (2009) use constrained EM and parser adaptation techniques, respectively, to perform more principled projection, and both achieve encouraging results." ></td>
	<td class="line x" title="144:211	Our work, by constrast, never uses bilingual tree pairs not tree projections, and only uses word alignment alone to enhance a monolingual grammar, which learns to prefer target-side contiguity." ></td>
	<td class="line x" title="145:211	5 Experiments 5.1 Baseline Parser We implement our baseline monolingual parser (in C++) based on the shift-reduce algorithm in Section 2, with feature templates from Table 2." ></td>
	<td class="line x" title="146:211	We evaluate its performance on the standard Penn English Treebank (PTB) dependency parsing task, i.e., train on sections 02-21 and test on section 23 with automatically assigned POS tags (at 97.2% accuracy) using a tagger similar to Collins (2002), and using the headrules of Yamada and Matsumoto (2003) for conversion into dependency trees." ></td>
	<td class="line x" title="147:211	We use section 22 as dev set to determine the optimal number of iterations in perceptron training." ></td>
	<td class="line x" title="148:211	Table 4 compares our baseline against the state-of-the-art graph-based (McDonald et al., 2005) and transition-based (Zhang and Clark, 2008) approaches, and confirms that our system performs at the same level with those stateof-the-art, and runs extremely fast in the deterministic mode (k=1), and still quite fast in the beamsearch mode (k=16)." ></td>
	<td class="line x" title="149:211	parser accuracy secs/sent McDonald et al.(2005) 90.7 0.150 Zhang and Clark (2008) 91.4 0.195 our baseline at k=1 90.2 0.009 our baseline at k=16 91.3 0.125 Table 4: Baseline parser performance on standard Penn English Treebank dependency parsing task." ></td>
	<td class="line x" title="151:211	The speed numbers are not exactly comparable since they are reported on different machines." ></td>
	<td class="line x" title="152:211	Training Dev Test CTB Articles 1-270 301-325 271-300 Bilingual Paris 2745 273 290 Table 5: Training, dev, and test sets from bilingual Chinese Treebank `a la Burkett and Klein (2008)." ></td>
	<td class="line x" title="153:211	5.2 Bilingual Data The bilingual data we use is the translated portion of the Penn Chinese Treebank (CTB) (Xue et al., 2002), corresponding to articles 1-325 of PTB, which have English translations with goldstandard parse trees (Bies et al., 2007)." ></td>
	<td class="line x" title="154:211	Table 5 shows the split of this data into training, development, and test subsets according to Burkett and Klein (2008)." ></td>
	<td class="line x" title="155:211	Note that not all sentence pairs could be included, since many of them are not oneto-one aligned at the sentence level." ></td>
	<td class="line x" title="156:211	Our wordalignments are generated from the HMM aligner of Liang et al.(2006) trained on approximately 1.7M sentence pairs (provided to us by David Burkett, p.c.)." ></td>
	<td class="line x" title="158:211	This aligner outputs soft alignments, i.e., posterior probabilities for each source-target word pair." ></td>
	<td class="line x" title="159:211	We use a pruning threshold of 0.535 to remove low-confidence alignment links,6 and use the remaining links as hard alignments; we leave the use of alignment probabilities to future work." ></td>
	<td class="line x" title="160:211	For simplicity reasons, in the following experiments we always supply gold-standard POS tags as part of the input to the parser." ></td>
	<td class="line x" title="161:211	5.3 Testing our Hypotheses Before evaluating our bilingual approach, we need to verify empirically the two assumptions we made about the parser in Sections 2 and 3: 6and also removing notoriously bad links in{the, a, an} {de, le}following Fossum and Knight (2008)." ></td>
	<td class="line x" title="162:211	1228 sh  re re  sh sh-re re-re # 92 98 190 7 % 46.7% 49.7% 96.4% 3.6% Table 6: [Hypothesis 1] Error distribution in the baseline model (k = 1) on English dev set." ></td>
	<td class="line x" title="163:211	sh  re means should shift, but reduced." ></td>
	<td class="line x" title="164:211	Shiftreduce conflicts overwhelmingly dominate." ></td>
	<td class="line x" title="165:211	1." ></td>
	<td class="line x" title="166:211	(monolingual) shift-reduce conflict is the major source of errors while reduce-reduce conflict is a minor issue; 2." ></td>
	<td class="line x" title="167:211	(bilingual) the gold-standard decisions of shift or reduce should correlate with contiguities of c(st1,st), and of cR(st,wi)." ></td>
	<td class="line x" title="168:211	Hypothesis 1 is verified in Table 6, where we count all the first mistakes the baseline parser makes (in the deterministic mode) on the English dev set (273 sentences)." ></td>
	<td class="line x" title="169:211	In shift-reduce parsing, further mistakes are often caused by previous ones, so only the first mistake in each sentence (if there is one) is easily identifiable;7 this is also the argument for early update in applying perceptron learning to these incremental parsing algorithms (Collins and Roark, 2004) (see also Section 2)." ></td>
	<td class="line x" title="170:211	Among the 197 first mistakes (other 76 sentences have perfect output), the vast majority, 190 of them (96.4%), are shift-reduce errors (equally distributed between shift-becomesreduce and reduce-becomes-shift), and only 7 (3.6%) are due to reduce-reduce conflicts.8 These statistics confirm our intuition that shift-reduce decisions are much harder to make during parsing, and contribute to the overwhelming majority of errors, which is studied in the next hypothesis." ></td>
	<td class="line x" title="171:211	Hypothesis 2 is verified in Table 7." ></td>
	<td class="line x" title="172:211	We take the gold-standard shift-reduce sequence on the English dev set, and classify them into the four categories based on bilingual contiguity features: (a) c(st1,st), i.e. whether the top 2 spans on stack is contiguous, and (b) cR(st,wi), i.e. whether the 7to be really precise one can define independent mistakes as those not affected by previous ones, i.e., errors made after the parser recovers from previous mistakes; but this is much more involved and we leave it to future work." ></td>
	<td class="line x" title="173:211	8Note that shift-reduce errors include those due to the non-uniqueness of oracle, i.e., between some reduceL and shift." ></td>
	<td class="line x" title="174:211	Currently we are unable to identify genuine errors that would result in an incorrect parse." ></td>
	<td class="line x" title="175:211	See also Section 2.5." ></td>
	<td class="line x" title="176:211	c(st1,st) cR(st,wi) shift reduce +  172  1,209  + 1,432 > 805 + + 4,430  3,696   525  576 total 6,559 = 6,286 Table 7: [Hyp." ></td>
	<td class="line x" title="177:211	2] Correlation of gold-standard shift/reduce decisions with bilingual contiguity conditions (on English dev set)." ></td>
	<td class="line x" title="178:211	Note there is always one more shift than reduce in each sentence." ></td>
	<td class="line x" title="179:211	stack top is contiguous with the current word wi." ></td>
	<td class="line x" title="180:211	According to discussions in Section 3, when (a) is contiguous and (b) is not, it is a clear signal for reduce (to combine the top two elements on the stack) rather than shift, and is strongly supported by the data (first line: 1209 reduces vs. 172 shifts); and while when (b) is contiguous and (a) is not, it should suggest shift (combining st and wi before st1 and st are combined) rather than reduce, and is mildly supported by the data (second line: 1432 shifts vs. 805 reduces)." ></td>
	<td class="line x" title="181:211	When (a) and (b) are both contiguous or both discontiguous, it should be considered a neutral signal, and is also consistent with the data (next two lines)." ></td>
	<td class="line x" title="182:211	So to conclude, this bilingual hypothesis is empirically justified." ></td>
	<td class="line x" title="183:211	On the other hand, we would like to note that these correlations are done with automatic word alignments (in our case, from the Berkeley aligner) which can be quite noisy." ></td>
	<td class="line x" title="184:211	We suspect (and will finish in the future work) that using manual alignments would result in a better correlation, though for the main parsing results (see below) we can only afford automatic alignments in order for our approach to be widely applicable to any bitext." ></td>
	<td class="line x" title="185:211	5.4 Results We incorporate the three bilingual features (again, with automatic alignments) into the baseline parser, retrain it, and test its performance on the English dev set, with varying beam size." ></td>
	<td class="line x" title="186:211	Table 8 shows that bilingual constraints help more with larger beams, from almost no improvement with the deterministic mode (k=1) to +0.5% better with the largest beam (k=16)." ></td>
	<td class="line x" title="187:211	This could be explained by the fact that beam-search is more robust than the deterministic mode, where in the latter, if our 1229 baseline +bilingual k accuracy time (s) accuracy time (s) 1 84.58 0.011 84.67 0.012 2 85.30 0.025 85.62 0.028 4 85.42 0.040 85.81 0.044 8 85.50 0.081 85.95 0.085 16 85.57 0.158 86.07 0.168 Table 8: Effects of beam size k on efficiency and accuracy (on English dev set)." ></td>
	<td class="line x" title="188:211	Time is average per sentence (in secs)." ></td>
	<td class="line x" title="189:211	Bilingual constraints show more improvement with larger beams, with a fractional efficiency overhead over the baseline." ></td>
	<td class="line x" title="190:211	English Chinese monolingual baseline 86.9 85.7 +bilingual features 87.5 86.3 improvement +0.6 +0.6 signficance level p < 0.05 p < 0.08 Berkeley parser 86.1 87.9 Table 9: Final results of dependency accuracy (%) on the test set (290 sentences, beam size k=16)." ></td>
	<td class="line x" title="191:211	bilingual features misled the parser into a mistake, there is no chance of getting back, while in the former multiple configurations are being pursued in parallel." ></td>
	<td class="line x" title="192:211	In terms of speed, both parsers run proportionally slower with larger beams, as the time complexity is linear to the beam-size." ></td>
	<td class="line x" title="193:211	Computing the bilingual features further slows it down, but only fractionally so (just 1.06 times as slow as the baseline at k=16), which is appealing in practice." ></td>
	<td class="line x" title="194:211	By contrast, Burkett and Klein (2008) reported their approach of monolingual k-best parsing followed by bilingual k2-best reranking to be 3.8 times slower than monolingual parsing." ></td>
	<td class="line x" title="195:211	Our final results on the test set (290 sentences) are summarized in Table 9." ></td>
	<td class="line x" title="196:211	On both English and Chinese, the addition of bilingual features improves dependency arc accuracies by +0.6%, which is mildly significant using the Z-test of Collins et al.(2005)." ></td>
	<td class="line x" title="198:211	We also compare our results against the Berkeley parser (Petrov and Klein, 2007) as a reference system, with the exact same setting (i.e., trained on the bilingual data, and testing using gold-standard POS tags), and the resulting trees are converted into dependency via the same headrules." ></td>
	<td class="line x" title="199:211	We use 5 iterations of split-merge grammar induction as the 6th iteration overfits the small training set." ></td>
	<td class="line x" title="200:211	The result is worse than our baseline on English, but better than our bilingual parser on Chinese." ></td>
	<td class="line x" title="201:211	The discrepancy between English and Chinese is probably due to the fact that our baseline feature templates (Table 2) are engineered on English not Chinese." ></td>
	<td class="line x" title="202:211	6 Conclusion and Future Work We have presented a novel parsing paradigm, bilingually-constrained monolingual parsing, which is much simpler than joint (bi-)parsing, yet still yields mild improvements in parsing accuracy in our preliminary experiments." ></td>
	<td class="line x" title="203:211	Specifically, we showed a simple method of incorporating alignment features as soft evidence on top of a state-of-the-art shift-reduce dependency parser, which helped better resolve shift-reduce conflicts with fractional efficiency overhead." ></td>
	<td class="line x" title="204:211	The fact that we managed to do this with only three alignment features is on one hand encouraging, but on the other hand leaving the bilingual feature space largely unexplored." ></td>
	<td class="line x" title="205:211	So we will engineer more such features, especially with lexicalization and soft alignments (Liang et al., 2006), and study the impact of alignment quality on parsing improvement." ></td>
	<td class="line x" title="206:211	From a linguistics point of view, we would like to see how linguistics distance affects this approach, e.g., we suspect EnglishFrench would not help each other as much as English-Chinese do; and it would be very interesting to see what types of syntactic ambiguities can be resolved across different language pairs." ></td>
	<td class="line x" title="207:211	Furthermore, we believe this bilingual-monolingual approach can easily transfer to shift-reduce constituency parsing (Sagae and Lavie, 2006)." ></td>
	<td class="line x" title="208:211	Acknowledgments We thank the anonymous reviewers for pointing to us references about arc-standard." ></td>
	<td class="line x" title="209:211	We also thank Aravind Joshi and Mitch Marcus for insights on PP attachment, Joakim Nivre for discussions on arc-eager, Yang Liu for suggestion to look at manual alignments, and David A. Smith for sending us his paper." ></td>
	<td class="line x" title="210:211	The second and third authors were supported by National Natural Science Foundation of China, Contracts 60603095 and 60736014, and 863 State Key Project No. 2006AA010108." ></td>
	<td class="line x" title="211:211	1230" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1136
Bayesian Learning of Phrasal Tree-to-String Templates
Liu, Ding;Gildea, Daniel;"></td>
	<td class="line x" title="1:194	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 13081317, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:194	c 2009 ACL and AFNLP Bayesian Learning of Phrasal Tree-to-String Templates Ding Liu and Daniel Gildea Department of Computer Science University of Rochester Rochester, NY 14627 {dliu, gildea}@cs.rochester.edu Abstract We examine the problem of overcoming noisy word-level alignments when learning tree-to-string translation rules." ></td>
	<td class="line x" title="3:194	Our approach introduces new rules, and reestimates rule probabilities using EM." ></td>
	<td class="line x" title="4:194	The major obstacles to this approach are the very reasons that word-alignments are used for rule extraction: the huge space of possible rules, as well as controlling overfitting." ></td>
	<td class="line x" title="5:194	By carefully controlling which portions of the original alignments are reanalyzed, and by using Bayesian inference during re-analysis, we show significant improvement over the baseline rules extracted from word-level alignments." ></td>
	<td class="line x" title="6:194	1 Introduction Non-parametric Bayesian methods have been successfully applied to directly learn phrase pairs from a bilingual corpus with little or no dependence on word alignments (Blunsom et al., 2008; DeNeroetal., 2008)." ></td>
	<td class="line x" title="7:194	Becausesuchapproachesdirectly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (Brown et al., 1993) or the Hidden Markov Model (HMM) (Vogel et al., 1996)." ></td>
	<td class="line x" title="8:194	We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (Liu et al., 2006; Huang et al., 2006; Galley et al., 2006; May and Knight, 2007), as opposed to formally syntactic systems such as Hiero (Chiang, 2007)." ></td>
	<td class="line x" title="9:194	The stumbling block preventing us from taking this approach is the extremely large space of possible TTS templates when no word alignments are given." ></td>
	<td class="line x" title="10:194	Given a sentence pair and syntax tree over one side, there are an exponential number of potential TTS templates and a polynomial number of phrase pairs." ></td>
	<td class="line x" title="11:194	In this paper, we explore methods for restricting the space of possible TTS templates under consideration, while still allowing good templates to emerge directly from the data as much as possible." ></td>
	<td class="line x" title="12:194	We find an improvement in translation accuracy through,first,usingconstraintstolimitthenumber ofnewtemplates,second,usingBayesianmethods to limit which of these new templates are favored when re-analyzing the training data with EM, and, third, experimenting with different renormalization techniques for the EM re-analysis." ></td>
	<td class="line x" title="13:194	We introduce two constraints to limit the number of TTS templates that we extract directly from tree/string pairs without using word alignments." ></td>
	<td class="line x" title="14:194	The first constraint is to limit direct TTS template extraction to the part of the corpus where word alignment tools such as GIZA++ do poorly." ></td>
	<td class="line x" title="15:194	There is no reason not to re-use the good alignments from GIZA++, which holds a very competitive baseline performance." ></td>
	<td class="line x" title="16:194	As already mentioned, the noisy alignments from GIZA++ are likely to cross the boundaries of the tree constituents, which leads to comparatively big TTS templates." ></td>
	<td class="line x" title="17:194	We use this fact as a heuristic to roughly distinguish noisy from good word alignments.1 Here we define big templates as those with more than 8 symbols in their right hand sides (RHSs)." ></td>
	<td class="line x" title="18:194	The word alignments in big templates are considered to be noisy and will be recomposed by extracting smaller TTS templates." ></td>
	<td class="line x" title="19:194	Another reason to do extraction on big templates is that the applicability of big templates to new sentences is very limited due to their size, and the portion of the training data from which they are extracted is effectively wasted." ></td>
	<td class="line x" title="20:194	The second constraint, after choosing the 1Precisely differentiating the noisy/good word alignments is as hard as correctly aligning the words." ></td>
	<td class="line x" title="21:194	1308 extraction site, is to extract the TTS templates all the way down to the leaves of the hosting templates." ></td>
	<td class="line x" title="22:194	This constraint limits the number of possible left hand sides (LHSs) to be equal to the number of tree nodes in the hosting templates." ></td>
	<td class="line x" title="23:194	The entire extraction process can be summarized in 3 steps: 1." ></td>
	<td class="line x" title="24:194	Compute word alignments using GIZA++, and generate the basic TTS templates." ></td>
	<td class="line x" title="25:194	2." ></td>
	<td class="line x" title="26:194	Select big templates from the basic TTS templates in step 1, and extract smaller TTS templates all the way down to the bottom from big templates, without considering the precomputed word alignments." ></td>
	<td class="line x" title="27:194	3." ></td>
	<td class="line x" title="28:194	Combine TTS templates from step 1 and step 2 and estimate their probabilities using Variational Bayes with a Dirichlet Process prior." ></td>
	<td class="line x" title="29:194	In step 2, since there are no constraints from the pre-computed word alignments, we have complete freedom in generating all possible TTS templates to overcome noisy word alignments." ></td>
	<td class="line x" title="30:194	We use variational EM to approximate the inference of our Bayesian model and explore different normalization methods for the TTS templates." ></td>
	<td class="line x" title="31:194	A two-stage normalization is proposed by combining LHSbased normalization with normalization based on the root of the LHS, and is shown to be the best model when used with variational EM." ></td>
	<td class="line x" title="32:194	Galley et al.(2006) recompose the TTS templates by inserting unaligned target words and combining small templates into bigger ones." ></td>
	<td class="line x" title="34:194	The recomposed templates are then re-estimated using the EM algorithm described in Graehl and Knight (2004)." ></td>
	<td class="line x" title="35:194	This approach also generates TTS templates beyond the precomputed word alignments, but the freedom is only granted over unaligned target words, and most of the pre-computed word alignments remain unchanged." ></td>
	<td class="line x" title="36:194	Other prior approaches towards improving TTS templates focusonimprovingthewordalignmentperformance over the classic models such as IBM series models and Hidden Markov Model (HMM), which do not consider the syntactic structure of the aligning languages and produce syntax-violating alignments." ></td>
	<td class="line x" title="37:194	DeNero and Klein (2007) use a syntaxbased distance in an HMM word alignment model to favor syntax-friendly alignments." ></td>
	<td class="line x" title="38:194	Fossum et al.(2008) start from the GIZA++ alignment and incrementally delete bad links based on a discrimS NP VP  NN  AU X issue has    S NP VP  NN  AU X issue has     Figure 1: 5 small TTS templates are extracted based on the correct word alignments (left), but only 1 big TTS template (right) can be extracted when the cross-boundary noisy alignments are added in." ></td>
	<td class="line x" title="40:194	inative model with syntactic features." ></td>
	<td class="line x" title="41:194	This approachcanonlyfindabettersubsetoftheGIZA++ alignmentandrequiresaparallelcorpuswithgoldstandard word alignment for training the discriminative model." ></td>
	<td class="line x" title="42:194	May and Knight (2007) factorize the word alignment into a set of re-orderings represented by the TTS templates and build a hierarchical syntax-based word alignment model." ></td>
	<td class="line x" title="43:194	The problem is that the TTS templates are generated by the word alignments from GIZA++, which limits the potential of the syntactic re-alignment." ></td>
	<td class="line x" title="44:194	As shown by these prior approaches, directly improving the word alignment either falls into the frameworkofmany-to-onealignment, orissubstantially confined by the word alignment it builds upon." ></td>
	<td class="line x" title="45:194	The remainder of the paper focuses on the Bayesian approach to learning TTS templates and is organized as follows: Section 2 describes the procedure for generating the candidate TTS templates; Section 3 describes the inference methods used to learn the TTS templates; Section 4 gives the empirical results, Section 5 discusses the characteristics of the learned TTS templates, and Section 6 presents the conclusion." ></td>
	<td class="line x" title="46:194	2 Extracting Phrasal TTS Templates The Tree-to-String (TTS) template, the most important component of a SSMT system, usually contains three parts: a fragment of a syntax tree in its left hand side (LHS), a sequence of words and variables in its right hand side (RHS), and a probability indicating how likely the template is to be used in translation." ></td>
	<td class="line x" title="47:194	The RHS of a TTS template shows one possible translation and reordering of its LHS." ></td>
	<td class="line x" title="48:194	The variables in a TTS templatearefurthertransformedusingotherTTStemplates, and the recursive process continues until there are no variables left." ></td>
	<td class="line x" title="49:194	There are two ways 1309 S NP VP NP1 PP ADJAUX in isNP3 of NP3NP1 beautiful 2 1 3 4 Figure 2: Examples of valid and invalid templates extracted from a big Template." ></td>
	<td class="line x" title="50:194	Template 1, invalid, doesnt go all the way down to the bottom." ></td>
	<td class="line x" title="51:194	Template 2 is valid." ></td>
	<td class="line x" title="52:194	Template 3, invalid, doesnt have the same set of variables in its LHS/RHS." ></td>
	<td class="line x" title="53:194	Template 4, invalid, is not a phrasal TTS template." ></td>
	<td class="line x" title="54:194	that TTS templates are commonly used in machine translation." ></td>
	<td class="line x" title="55:194	The first is synchronous parsing (Galley et al., 2006; May and Knight, 2007), where TTS templates are used to construct synchronous parse trees for an input sentence, and the translations will be generated once the synchronous trees are built up." ></td>
	<td class="line x" title="56:194	The other way is the TTS transducer (Liu et al., 2006; Huang et al., 2006), where TTS templates are used just as their name indicates: to transform a source parse tree (or forest) into the proper target string." ></td>
	<td class="line x" title="57:194	Since synchronous parsing considers all possible synchronous parse trees of the source sentence, it is less constrained than TTS transducers and hence requires more computational power." ></td>
	<td class="line x" title="58:194	In this paper, weuseaTTStransducertotesttheperformanceof different TTS templates, but our techniques could also be applied to SSMT systems based on synchronous parsing." ></td>
	<td class="line x" title="59:194	2.1 Baseline Approach: TTS Templates Obeying Word Alignment TTS templates are commonly generated by decomposing a pair of aligned source syntax tree and target string into smaller pairs of tree fragments and target string (i.e., the TTS templates)." ></td>
	<td class="line x" title="60:194	TokeepthenumberofTTStemplatestoamanageable scale, only the non-decomposable TTS templates are generated." ></td>
	<td class="line pc" title="61:194	This algorithm is referred to as GHKM (Galley et al., 2004) and is widely used in SSMT systems (Galley et al., 2006; Liu et al., 2006; Huang et al., 2006)." ></td>
	<td class="line o" title="62:194	The word alignment used in GHKM is usually computed independent ofthesyntacticstructure,andasDeNeroandKlein (2007) and May and Knight (2007) have noted, Ch-En En-Ch Union Heuristic 28.6% 33.0% 45.9% 20.1% Table 1: Percentage of corpus used to generate big templates, based on different word alignments 9-12 13-20 21 Ch-En 18.2% 17.4% 64.4% En-Ch 15.9% 20.7% 63.4% Union 9.8% 15.1% 75.1% Heuristic 24.6% 27.9% 47.5% Table 2: In the selected big templates, the distribution of words in the templates of different sizes, which are measured based on the number of symbols in their RHSs is not the best for SSMT systems." ></td>
	<td class="line x" title="63:194	In fact, noisy word alignments cause more damage to a SSMT system than to a phrase based SMT system, because the TTS templates can only be derived from treeconstituents." ></td>
	<td class="line x" title="64:194	Ifsomenoisyalignmentshappen to cross over the boundaries of two constituents, as shown in Figure 2, a much bigger tree fragment will be extracted as a TTS template." ></td>
	<td class="line x" title="65:194	Even though the big TTS templates still carry the original alignment information, they have much less chance of getting matched beyond the syntax tree where they were extracted, as we show in Section 4." ></td>
	<td class="line x" title="66:194	In other words, a few cross-boundary noisy alignmentscoulddisableabigportionofatraining syntax tree, while for a phrase-based SMT system, their effect is limited to the phrases they align." ></td>
	<td class="line x" title="67:194	As a rough measure of how the training corpus is affected by the big templates, we calculated the distribution of target words in big and non-big TTS templates." ></td>
	<td class="line x" title="68:194	The word alignment is computed using GIZA++2 for the selected 73,597 sentence pairs in the FBIS corpus in both directions and then combined using union and heuristic diagonal growing (Koehn et al., 2003)." ></td>
	<td class="line x" title="69:194	Table 1 shows that big templates consume 20.1% to 45.9% of the training corpusdependingondifferenttypesofwordalignments." ></td>
	<td class="line x" title="70:194	The statistics indicate that a significant portion of the training corpus is simply wasted, if the TTS templates are extracted based on word alignments from GIZA++." ></td>
	<td class="line x" title="71:194	On the other hand, it shows the potential for improving an SSMT system if we can efficiently re-use the wasted training corpus." ></td>
	<td class="line x" title="72:194	By further examining the selected big templates, we find that the most common form of big templates is a big skeleton template starting 2GIZA++ is available at http://www.fjoch.com/GIZA++.html 1310 from the root of the source syntax tree, and having many terminals (words) misaligned in the bottom." ></td>
	<td class="line x" title="73:194	Table 2 shows, in the selected big templates, the distribution of words in the templates of different sizes (measured based on the number of symbols in their RHS)." ></td>
	<td class="line x" title="74:194	We can see that based on either type of word alignment, the most common big templates are the TTS templates with more than 20 symbols in their RHSs, which are generally the big skeleton templates." ></td>
	<td class="line x" title="75:194	The advantage of such big skeleton templates is that they usually have good marginal accuracy3 and allow accurate smaller TTS templates to emerge." ></td>
	<td class="line x" title="76:194	2.2 Liberating Phrasal TTS Templates From Noisy Word Alignments To generate better TTS templates, we use a more direct way than modifying the underlying word alignment: extract smaller phrasal TTS templates from the big templates without looking at their pre-computed word alignments." ></td>
	<td class="line x" title="77:194	We define phrasal TTS templates as those with more than one symbol (word or non-terminal) in their LHS." ></td>
	<td class="line x" title="78:194	The reason to consider only phrasal TTS templates is that they are more robust than the wordlevel TTS templates in addressing the complicated word alignments involved in big templates, which are usually not the simple type of one-to-many or many-to-one." ></td>
	<td class="line x" title="79:194	Abandoningthepre-computedword alignments in big templates, an extracted smaller TTS template can have many possible RHSs, as long as the two sides have the same set of variables." ></td>
	<td class="line x" title="80:194	Note that the freedom is only given to the alignments of the words; for the variables in the big templates, we respect the pre-computed word alignments." ></td>
	<td class="line x" title="81:194	To keep the extracted smaller TTS templatestoamanageablescale,thefollowingtwo constraints are applied: 1." ></td>
	<td class="line x" title="82:194	The LHS of extracted TTS templates should go all the way down to the bottom of the LHS of the big templates." ></td>
	<td class="line x" title="83:194	This constraint ensures that at most N LHSs can be extracted from one big Template, where N is the number of tree nodes in the big Templates LHS." ></td>
	<td class="line x" title="84:194	2." ></td>
	<td class="line x" title="85:194	The number of leaves (including both words and variables) in an extracted TTS templates LHS should not exceed 6." ></td>
	<td class="line x" title="86:194	This constraint limitsthesizeoftheextractedTTStemplates." ></td>
	<td class="line x" title="87:194	3Here, marginal accuracy means the correctness of the TTS templates RHS corresponding to its LHS." ></td>
	<td class="line x" title="88:194	( VP ( AUX is ) ( ADJ  beautiful ) )          ( PP ( IN of )  NP3 )          NP3  ( NP NP 1 ( PP ( IN of )  NP3 ) )          NP3   NP1( NP NP 1 ( PP ( IN of )  NP3 ) )          NP3   NP1   Figure 3: All valid templates that can be extracted from the example in Figure 2.1 for all template t do if size(t.rhs) > 8 then for all tree node s in t.lhs do subt = subtree(s, t.lhs); if leaf num(subt)6 then for i=1:size(t.rhs) do for j=i:size(t.rhs) do if valid(subt, i, j) then create template(subt, i, j); Figure 4: Algorithm that liberates smaller TTS Templates from big templates As we show in Section 4, use of bigger TTS templates brings very limited performance gain." ></td>
	<td class="line x" title="89:194	Figure 2.2 describes the template liberating algorithm running in O(NM2), where N denotes the number of tree nodes in the LHS of the input big TemplateandM denotesthelengthoftheRHS." ></td>
	<td class="line x" title="90:194	In the algorithm, function valid returns true if there are the same set of variables in the left/right hand side of an extracted TTS template; subtree(x, y) denotes the sub-tree in y which is rooted at x and goes all the way down to ys bottom." ></td>
	<td class="line x" title="91:194	Figure 2.1 shows valid and invalid TTS templates which can be extracted from an example hosting TTS template." ></td>
	<td class="line x" title="92:194	Note that, in order to keep the example simple, the hosting TTS template only has 4 symbols in its RHS, which does not qualify as a big template according to our definition." ></td>
	<td class="line x" title="93:194	Figure 2.2 shows the complete set of valid TTS templates which can be extracted from the example TTS template." ></td>
	<td class="line x" title="94:194	The subscripts of the non-terminals are used to differentiate identical non-terminals in different positions." ></td>
	<td class="line x" title="95:194	The extraction process blindly releases smaller TTS templates from the big templates, among which only a small fraction are correct TTS templates." ></td>
	<td class="line x" title="96:194	Therefore, we need an inference method to raise the weight of the correct templates and decrease the weight of the noisy templates." ></td>
	<td class="line x" title="97:194	1311 3 Estimating TTS Template Probability The Expectation-Maximization (EM) algorithm (Dempster et al., 1977) can be used to estimate the TTS templates probabilities, given a generative model addressing how a pair of source syntax tree and target string is generated." ></td>
	<td class="line x" title="98:194	There are two commonly used generative models for syntaxbased MT systems, each of which corresponds to a normalization method for the TTS templates." ></td>
	<td class="line x" title="99:194	The LHS-based normalization (LHSN) (Liu et al., 2006; Huang et al., 2006), corresponds to the generative process where the source syntax subtree is first generated, and then the target string is generated given the source syntax subtree." ></td>
	<td class="line x" title="100:194	The other one is normalization based on the root of the LHS (ROOTN) (Galley et al., 2006), corresponding to the generative process where, given the root of the syntax subtree, the LHS syntax subtree and the RHS string are generated simultaneously." ></td>
	<td class="line x" title="101:194	By omitting the decomposition probability in the LHS-based generative model, the two generativemodelssharethesameformulaforcomputing the probability of a training instance: Pr(T,S) = summationdisplay R Pr(T,S,R) = summationdisplay R parenleftBiggproductdisplay tR Pr(t) parenrightBigg where T and S denote the source syntax tree and target string respectively,Rdenotes the decomposition of (T,S), and t denotes the TTS template." ></td>
	<td class="line x" title="102:194	TheexpectedcountsoftheTTStemplatescanthen be efficiently computed using an inside-outsidelike dynamic programming algorithm (May and Knight, 2007)." ></td>
	<td class="line x" title="103:194	LHSN, as shown by Galley et al.(2006), cannot accurately restore the true conditional probabilities of the target sentences given the source sentences in the training corpus." ></td>
	<td class="line x" title="105:194	This indicates that LHSN is not good at predicting unseen sentences orattranslatingnewsentences." ></td>
	<td class="line x" title="106:194	Butthisdeficiency does not affect its ability to estimate the expected counts of the TTS templates, because the posteriors of the TTS templates only depend on the comparative probabilities of the different derivations of a training instance (a pair of tree and string)." ></td>
	<td class="line x" title="107:194	In fact, as we show in Section 4, LHSN is better than ROOTN in liberating smaller TTS templates out of the big templates, since it is less biased to the big templates in the EM training.4 Because the two normalization methods have their 4Based on LHSN, the difference between the probability of a big Template and the product of the probabilities of E-step: for all pair of syntax tree T and target string S do for all TTS Template t do EC(t)+ = P R:tRPr(T,S,R)  P Rprime Pr(T,S,Rprime) ; Increase ; M-step: for all TTS Template t do if it is the last iteration then Pr(t) = EC(t)P tprime:tprime.root=t.rootEC(tprime) ; else Pr(t) = EC(t)P tprime:tprime.lhs=t.lhsEC(tprime) ; Figure 5: EM Algorithm For Estimating TTS Templates own strength and weakness, both of them are used in our EM algorithm: LHSN is used in all EM iterations except the last one to compute the expected counts of the TTS templates, and ROOTN isusedinthelastEMiterationtocomputethefinal probabilities of the TTS templates." ></td>
	<td class="line x" title="108:194	This two-stage normalization method is denoted as MIXN in this paper." ></td>
	<td class="line x" title="109:194	Deterministic Annealing (Rose et al., 1992) is is used in our system to speed up the training process, similar to Goldwater et al.(2006)." ></td>
	<td class="line x" title="111:194	We start from a high temperature and gradually decrease the temperature to 1; we find that the initialhightemperaturecanalsohelpsmalltemplates to survive the initial iterations." ></td>
	<td class="line x" title="112:194	The complete EM framework is sketched in Figure 3, where is the inverse of the specified temperature, and EC denotes the expected count." ></td>
	<td class="line x" title="113:194	3.1 Bayesian Inference with the Dirichlet Process Prior BayesianinferenceplustheDirichletProcess(DP) have been shown to effectively prevent MT models from overfitting the training data (DeNero et al., 2008; Blunsom et al., 2008)." ></td>
	<td class="line x" title="114:194	A similar approach can be applied here for SSMT by considering each TTS template as a cluster, and using DP to adjust the number of TTS templates according to the training data." ></td>
	<td class="line x" title="115:194	Note that even though there is a size limitation on the liberated phrasal TTS templates, standard EM will still tend to overfit the training data by pushing up the probabilities of the big templates from the noisy word alignments." ></td>
	<td class="line x" title="116:194	The complete generative process, integrating the DP prior and the generative models described in its decomposing TTS templates is much less than the one based on ROOTN, thus LHSN gives comparably more expected counts to the smaller TTS templates than ROOTN." ></td>
	<td class="line x" title="117:194	1312 for all TTS Template t do if it is the last iteration then Pr(t) = exp((EC(t)+G0(t)))exp(((P tprime:tprime.root=t.rootEC(tprime))+)) ; else Pr(t) = exp((EC(t)+G0(t)))exp(((P tprime:tprime.lhs=t.lhsEC(tprime))+)) ; Figure 6: M-step of the Variational EM Section 3.1, is given below: r | {r,Gr0}  DP(r,Gr0) t | t.root  t.root (T,S) | {SG,{t},}  SG({t},) where G0 is a base distribution of the TTS templates, t denotes a TTS template, t.root denotes the multinomial distribution over TTS templates with the same root ast,SGdenotes the generative model for a pair of tree and string in Section 3.1, andis a free parameter which adjusts the rate at which new TTS templates are generated." ></td>
	<td class="line x" title="118:194	It is intractable to do exact inference under the Bayesian framework, even with a conjugate prior such as DP." ></td>
	<td class="line x" title="119:194	Two methods are commonly used for approximate inference: Markov chain Monte Carlo (MCMC) (DeNero et al., 2008), and Variational Bayesian (VB) inference (Blunsom et al., 2008)." ></td>
	<td class="line x" title="120:194	In this paper, the latter approach is used because it requires less running time." ></td>
	<td class="line x" title="121:194	The E-step of VB is exactly the same as standard EM, and in the M-step the digamma function  and the base distributionG0 are used to increase the uncertainty of themodel." ></td>
	<td class="line x" title="122:194	SimilartostandardEM,bothLHS-and root-based normalizations are used in the M-step, as shown in Figure 3.1." ></td>
	<td class="line x" title="123:194	For the TTS templates, whicharealsopairsofsubtreesandstrings, anaturalchoiceofG0 isthegenerativemodelsdescribed in Section 3.1." ></td>
	<td class="line x" title="124:194	BecauseG0 estimates the probability of the new TTS templates, the root-based generative model is superior to the LHS-based generative model and used in our approach." ></td>
	<td class="line x" title="125:194	3.2 Initialization Since the EM algorithm only converges to a local minimum, proper initializations are needed to achieve good performance for both standard EM and variational EM." ></td>
	<td class="line x" title="126:194	For the baseline templates derived from word alignments, the initial counts are set to the raw counts in the training corpus." ></td>
	<td class="line x" title="127:194	For the templates blindly extracted from big templates, the raw count of a LHS tree fragment is distributed among their RHSs based on the likelihood of the template, computed by combining for all big template t do for all template g extracted from t do g.count = g.lhs.count = 0; for all template g extracted from t do g.count += w in(g)w out(g, t); g.lhs.count += w in(g)w out(g, t); for all template g extracted from t do g.init += g.countg.lhs.count; Figure 7: Compute the initial counts of the liberated TTS templates the word-based inside/outside scores." ></td>
	<td class="line x" title="128:194	The algorithm is sketched in Figure 3.2, where the inside score w in(g) is the product of the IBM Model 1 scores in both directions, computed based on the words in gs LHS and RHS." ></td>
	<td class="line x" title="129:194	The outside score w out(g, t) is computed similarly, except that the IBM Model 1 scores are computed based on the words in the hosting template ts LHS/RHS excluding the words in gs LHS/RHS." ></td>
	<td class="line x" title="130:194	The initial probabilities of the TTS templates are then computed by normalizing their initial counts using LHSN or ROOTN." ></td>
	<td class="line x" title="131:194	4 Experiments We train an English-to-Chinese translation systemusingtheFBIScorpus, where73,597sentence pairs are selected as the training data, and 500 sentencepairswithnomorethan25wordsontheChinese side are selected for both the development and test data.5 Charniak (2000)s parser, trained on the Penn Treebank, is used to generate the English syntax trees." ></td>
	<td class="line x" title="132:194	Modified Kneser-Ney trigram models are trained using SRILM (Stolcke, 2002) upon the Chinese portion of the training data." ></td>
	<td class="line x" title="133:194	The trigram language model, as well as the TTS templates generated based on different methods, are used in the TTS transducer." ></td>
	<td class="line x" title="134:194	The model weights of the transducer are tuned based on the development set using a grid-based line search, and the translation results are evaluated based on a single Chinese reference6 using BLEU-4 (Papineni et al., 2002)." ></td>
	<td class="line x" title="135:194	Huang et al.(2006) used character-based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source." ></td>
	<td class="line x" title="137:194	5The total 74,597 sentence pairs used in experiments are those in the FBIS corpus whose English part can be parsed using Charniak (2000)s parser." ></td>
	<td class="line x" title="138:194	6BLEU-4 scores based on a single reference are much lower than the ones based on multiple references." ></td>
	<td class="line oc" title="139:194	1313 E2C C2E Union Heuristic w/ Big 13.37 12.66 14.55 14.28 w/o Big 13.20 12.62 14.53 14.21 Table 3: BLEU-4 scores (test set) of systems based on GIZA++ word alignments 5 6 7 8  BLEU-4 14.27 14.42 14.43 14.45 14.55 Table 4: BLEU-4 scores (test set) of the union alignment, using TTS templates up to a certain size, in terms of the number of leaves in their LHSs 4.1 Baseline Systems GHKM (Galley et al., 2004) is used to generate the baseline TTS templates based on the word alignments computed using GIZA++ and different combination methods, including union and the diagonal growing heuristic (Koehn et al., 2003)." ></td>
	<td class="line x" title="140:194	We also tried combining alignments from GIZA++ based on intersection, but it is worse than both single-direction alignments, due to its low coverage of training corpus and the incomplete translations it generates." ></td>
	<td class="line x" title="141:194	The baseline translation results basedonROOTNareshowninTable4.1." ></td>
	<td class="line x" title="142:194	Thefirst two columns in the table show the results of the two single direction alignments." ></td>
	<td class="line x" title="143:194	e2c and c2e denote the many English words to one Chinese word alignment and the many Chinese words to one English word alignment, respectively." ></td>
	<td class="line x" title="144:194	The two rows show the results with and without the big templates, from which we can see that removing the big templates does not affect performance much; this verifies our postulate that the big templates have very little chance of being used in the translation." ></td>
	<td class="line x" title="145:194	Table 4.1, using the union alignments as the representative and measuring a templates size by the number of leaves in its LHS, also demonstrates that using big TTS templates brings very limited performance gain." ></td>
	<td class="line x" title="146:194	The result that the union-based combination outperforms either single direction alignments and even the heuristic-based combination, combined with the statistics of the disabled corpus in Section 2.2, shows that more disabled training corpus actually leads to better performance." ></td>
	<td class="line x" title="147:194	This can be explained by the fact that the union alignments have the largest number of noisy alignments gathered together in the big templates, and thus have the least amount of noisy alignments which lead to small and low-quality TTS templates." ></td>
	<td class="line x" title="148:194	17  17.5  18  18.5  19  19.5  20  20.5  21  1  2  3  4  5  6  7  8  9  10 1.01.01.01.00.90.80.70.50.30.1 iteration temperature parameter  MIXN-EM LHSN-VB LHSN-EM ROOTN-EM ROOTN-VB MIXN-VB Figure 8: BLEU-4 scores (development set) of annealing EM and annealing VB in each iteration." ></td>
	<td class="line o" title="149:194	4.2 Learning Phrasal TTS Templates To test our learning methods, we start with the TTS templates generated based on e2c, c2e, and union alignments using GHKM." ></td>
	<td class="line x" title="150:194	This gives us 0.98M baseline templates." ></td>
	<td class="line x" title="151:194	We use the big templates from the union alignments as the basis and extract 10.92M new phrasal TTS templates, which, for convenience, are denoted by NEWPHR." ></td>
	<td class="line x" title="152:194	Because based on Table 1 and Table 2 the union alignment has the greatest number of alignment links and therefore produces the largest rules, this gives us the greatest flexibility in realigning the input sentences." ></td>
	<td class="line x" title="153:194	The baseline TTS templates as well as NEW-PHR are initialized using the method in Section 3.3 for both annealing EM and annealing VB." ></td>
	<td class="line x" title="154:194	To simplify the experiments, the same Dirichlet Process prior is used for all multinomial distributions of the TTS templates with different roots." ></td>
	<td class="line x" title="155:194	G0 in the Dirichlet prior is computed based on the 1-level TTS templates selected from the baseline TTS templates, so that the big templates are efficiently penalized." ></td>
	<td class="line x" title="156:194	The training algorithms follow the same annealing schedule, where the temperature parameter  is initialized to 0.1, and gradually increased to 1." ></td>
	<td class="line x" title="157:194	We experiment with the two training algorithms, annealing EM and annealing VB, with different normalization methods." ></td>
	<td class="line x" title="158:194	The experimental results based on the development data are shown in Figure 4.2, where the free parameter  of annealing VB is set to 1, 100, and 100 respectively for ROOTN, LHSN, and MIXN." ></td>
	<td class="line x" title="159:194	The results verify that LHSN is worse than ROOTN in predicting the translations, since MIXN outperforms LHSN with both annealing EM and VB." ></td>
	<td class="line x" title="160:194	ROOTN is on par with MIXN and much better 1314 Max Likelihood Annealing EM Annealing VB w/o new-phr with new-phr w/o new-phr with new-phr w/o new-phr with new-phr LHSN 14.05 13.16 14.31 15.33 14.82 16.15 ROOTN 14.50 13.49 14.90 16.06 14.76 16.12 MIXN NA NA 14.82 16.37 14.93 16.84 Table 5: BLEU-4 scores (test set) of different systems." ></td>
	<td class="line x" title="161:194	Initial Template Final Template number new-phr% number new-phr% ROOTN 11.9M 91.8% 408.0K 21.9% LHSN 11.9M 91.8% 557.2K 29.8% MIXN 11.9M 91.8% 500.5K 27.6% Table 6: The total number of templates and the percentage of NEW-PHR, in the beginning and end of annealing VB than LHSN when annealing EM is used; but with annealing VB, it is outperformed by MIXN by a large margin and is even slightly worse than LHSN." ></td>
	<td class="line x" title="162:194	This indicates that ROOTN is not givinglargeexpectedcountstoNEW-PHRandleaves very little space for VB to further improve the results." ></td>
	<td class="line x" title="163:194	For all the normalization methods, annealing VB outperforms annealing EM and maintains a longer ascending path, showing better control of overfitting for the Bayesian models." ></td>
	<td class="line x" title="164:194	Figure 4.2 shows the optimized results of the development set based on annealing VB with different ." ></td>
	<td class="line x" title="165:194	The best performance is achieved as  approaches 1, 100, and 100 for ROOTN, LHSN and MIXN respectively." ></td>
	<td class="line x" title="166:194	The  parameter can be viewed as a weight used to balance the expected counts and the probabilities from G0." ></td>
	<td class="line x" title="167:194	Thus it is reasonable for LHSN and MIXN to have bigger optimal  than ROOTN, since ROOTN gives lower expected counts to NEW-PHR than LHSN and MIXN do." ></td>
	<td class="line x" title="168:194	To see the contribution of the phrasal template extraction in the performance gain, MT experiments are conducted by turning this component on and off." ></td>
	<td class="line x" title="169:194	Results on the test set, obtained by using parameters optimized on the development set, are shown in Table 4.2." ></td>
	<td class="line x" title="170:194	The template counts used in the Max-Likelihood training are the same as the ones used in the initialization of annealing EM and VB." ></td>
	<td class="line x" title="171:194	Results show that for annealing EM and VB, use of NEW-PHR greatly improves performance, while for the Max-Likelihood training, use of NEW-PHR hurts performance." ></td>
	<td class="line x" title="172:194	This is not surprising, because Max-Likelihood training cannot efficiently filter out the noisy phrasal templatesintroducedintheinitialNEW-PHR." ></td>
	<td class="line x" title="173:194	Another observation is that annealing VB does not always outperform annealing EM." ></td>
	<td class="line x" title="174:194	With NEW-PHR  19.8  20  20.2  20.4  20.6  20.8  21  0.1  1  10  100  1000  MIXN ROOTN LHSN Figure 9: BLEU-4 scores (development set) of annealing VB with different ." ></td>
	<td class="line x" title="175:194	turned on, annealing VB shows consistent superiority over annealing EM; while without NEWPHR, it only outperforms annealing EM based on LHSN and MIXN, and the improvement is not as big as when NEW-PHR is turned on." ></td>
	<td class="line x" title="176:194	This indicates that without NEW-PHR, there is less need to use VB to shrink down the size of the template set." ></td>
	<td class="line x" title="177:194	Table 4.2 shows the statistics of the initialtemplatesetincludingNEW-PHRandthefinal TTS template set after annealing VB is conducted, where we can see annealing VB efficiently reduces NEW-PHR to a relatively small size and results in much more compact systems than the systembasedonthebaselinetemplatesfromGIZA++ alignments." ></td>
	<td class="line x" title="178:194	Comparing with the best GIZA++based system union, our best system, utilizing NEW-PHR and the two-stage template normalization, demonstrates the strength of annealing VB by an absolute improvement of 2.29% in BLEU4 score, from 14.55 to 16.84." ></td>
	<td class="line x" title="179:194	This improvement is significant at p < 0.005 based on 2000 iterations of paired bootstrap re-sampling of the test set (Koehn, 2004)." ></td>
	<td class="line x" title="180:194	5 Discussion Our experimental results are obtained based on a relatively small training corpus, the improved performance may be questionable when a larger training corpus is used." ></td>
	<td class="line x" title="181:194	Someone may wonder if the performance gain primarily comes from the 1315 Many-to-one Alignment ( VP ( VB make ) ( NP ( DT a ) ( JJ complete ) ( NN statement ) ) )  ( S ( VP VBG ( NP ( DT the ) ( NN mass ) ( NN line ) ) PP ) ) PP VBG  ( PP ( TO to ) ( NP ( DT the ) ( JJS greatest ) ( NN extent ) ) )   ( PP ( IN of ) ( NP ( JJ peaceful ) ( NNP coexistence ) ) )  Many-to-many Alignment ( VP ( VBN based ) ( PP ( IN on ) ( NP ( JJ actual ) ( NNS needs ) ) ) )    ( PP ( IN into ) ( NP ( NP ( DT the ) ( NNS hands ) ) PP ) )   PP  ( VP ( VBP exercise ) ( NP ( JJ strict ) ( NN self-discipline ) ) )     ( SBAR ( S ( NP ( DT the ) ( VBG aging ) NN ) ( VP ( aux is ) NP ) ) ) NN     NP ( NP NP1 PP ( , , ) ( VP ( VBN centered ) ( PP ( IN around ) NP2 ) ) )  NP2    NP1 PP Allowance of Bad Word Segmentation ( NP ( NP ( NNP japan ) ( POS 's ) ) ( NNP sdf ) ( NNP navy ) )    ( NP ( PDT all ) ( NP ( NNS people ) ( POS 's ) ) ( NNS organizations ) )    Figure 10: Examples of the learned TTS templates reduced out of vocabulary (OOV) ratio." ></td>
	<td class="line x" title="182:194	We examined the OOV ratio of the test set with/without the learned TTS templates, and found the difference was very small." ></td>
	<td class="line x" title="183:194	In fact, our method is designed to learn the phrasal TTS templates, and explictly avoids lexical pairs." ></td>
	<td class="line x" title="184:194	To further understand the characteristics of the learned TTS templates, welistsomerepresentativetemplatesinFigure4.2 classified in 3 groups." ></td>
	<td class="line x" title="185:194	The group Many-to-one Alignment and Many-to-many Alignment show the TTS templates based on complicated word alignments, which are difficult to compute based on the existing word alignment models." ></td>
	<td class="line x" title="186:194	These templates do not have rare English words, whose translation cannot be found outside the big templates." ></td>
	<td class="line x" title="187:194	The difficulty lies in the non-literal translation of the source words, which are unlikely to learnt by soly increasing the size of the training corpus." ></td>
	<td class="line x" title="188:194	One other interesting observation is that our learning method is tolerant to noisy Chinese word segmentation, as shown in group Allowance of Bad Word Segmentation." ></td>
	<td class="line x" title="189:194	6 Conclusion This paper proposes a Bayesian model for extracting the Tree-to-String templates directly from the data." ></td>
	<td class="line x" title="190:194	By limiting the extraction to the big templates from the pre-computed word alignments and applying a set of constraints, we restrict the space of possible TTS templates under consideration, while still allowing new and more accurate templates to emerge from the training data." ></td>
	<td class="line x" title="191:194	The empirical results demonstrate the strength of our approach, which outperforms the GIZA++-based systems by a large margin." ></td>
	<td class="line x" title="192:194	This encourages a move from word-alignment-based systems to systems based on consistent, end-to-end probabilistic modeling." ></td>
	<td class="line x" title="193:194	Because our Bayesian model employs a very simple prior, more sophisticated generative models provide a possible direction for further experimentation." ></td>
	<td class="line x" title="194:194	Acknowledgments This work was supported by NSF grants IIS-0546554 and ITR-0428020." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-1025
11,001 New Features for Statistical Machine Translation
Chiang, David;Knight, Kevin;Wang, Wei;"></td>
	<td class="line x" title="1:173	Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218226, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:173	c 2009 Association for Computational Linguistics 11,001 New Features for Statistical Machine Translation David Chiang and Kevin Knight USC Information Sciences Institute 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 USA Wei Wang Language Weaver, Inc. 4640 Admiralty Way, Suite 1210 Marina del Rey, CA 90292 USA Abstract We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrasebased translation system and our syntax-based translation system." ></td>
	<td class="line x" title="3:173	On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 Bleu and +1.1 Bleu, respectively." ></td>
	<td class="line x" title="4:173	We analyze the impact of the new features and the performance of the learning algorithm." ></td>
	<td class="line x" title="5:173	1 Introduction What linguistic features can improve statistical machine translation (MT)?" ></td>
	<td class="line x" title="6:173	This is a fundamental question for the discipline, particularly as it pertains to improving the best systems we have." ></td>
	<td class="line x" title="7:173	Further:  Do syntax-based translation systems have unique and e ective levers to pull when designing new features?" ></td>
	<td class="line x" title="8:173	Can large numbers of feature weights be learned e ciently and stably on modest amounts of data?" ></td>
	<td class="line x" title="9:173	In this paper, we address these questions by experimenting with a large number of new features." ></td>
	<td class="line x" title="10:173	We add more than 250 features to improve a syntaxbased MT systemalready the highest-scoring single system in the NIST 2008 Chinese-English common-data trackby +1.1 Bleu." ></td>
	<td class="line x" title="11:173	We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 Bleu improvement." ></td>
	<td class="line x" title="12:173	This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies." ></td>
	<td class="line x" title="13:173	Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model." ></td>
	<td class="line x" title="14:173	Thus they widen the advantage that syntaxbased models have over other types of models." ></td>
	<td class="line x" title="15:173	The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003)." ></td>
	<td class="line x" title="16:173	Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks." ></td>
	<td class="line x" title="17:173	2 Related Work The work of Och et al (2004) is perhaps the bestknown study of new features and their impact on translation quality." ></td>
	<td class="line x" title="18:173	However, it had a few shortcomings." ></td>
	<td class="line x" title="19:173	First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008)." ></td>
	<td class="line x" title="20:173	Second, it attempted to incorporate syntax by applying o -the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never designed for." ></td>
	<td class="line x" title="21:173	By contrast, we incorporate features directly into hierarchical and syntaxbased decoders." ></td>
	<td class="line x" title="22:173	A third di culty with Och et al.s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets." ></td>
	<td class="line x" title="23:173	Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scalability: to train many features, we need many train218 ing examples, and to train discriminatively, we need to search through all possible translations of each training example." ></td>
	<td class="line x" title="24:173	Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset." ></td>
	<td class="line x" title="25:173	We follow this approach here." ></td>
	<td class="line x" title="26:173	3 Systems Used 3.1 Hiero Hiero (Chiang, 2005) is a hierarchical, string-tostring translation system." ></td>
	<td class="line x" title="27:173	Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X!X1 de X2;X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997)." ></td>
	<td class="line x" title="28:173	The baseline model includes 12 features whose weights are optimized using MERT." ></td>
	<td class="line x" title="29:173	Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models." ></td>
	<td class="line x" title="30:173	This grammar can be parsed e ciently using cube pruning (Chiang, 2007)." ></td>
	<td class="line x" title="31:173	3.2 Syntax-based system Our syntax-based system transforms source Chinese strings into target English syntax trees." ></td>
	<td class="line x" title="32:173	Following previous work in statistical MT (Brown et al., 1993), we envision a noisy-channel model in which a language model generates English, and then a translation model transforms English trees into Chinese." ></td>
	<td class="line x" title="33:173	We represent the translation model as a tree transducer (Knight and Graehl, 2005)." ></td>
	<td class="line x" title="34:173	It is obtained from bilingual text that has been word-aligned and whose English side has been syntactically parsed." ></td>
	<td class="line oc" title="35:173	From this data, we use the the GHKM minimal-rule extraction algorithm of (Galley et al., 2004) to yield rules like: NP-C(x0:NPB PP(IN(of x1:NPB))$x1 de x0 Though this rule can be used in either direction, here we use it right-to-left (Chinese to English)." ></td>
	<td class="line x" title="36:173	We follow Galley et al.(2006) in allowing unaligned Chinese words to participate in multiple translation rules, and in collecting larger rules composed of minimal rules." ></td>
	<td class="line x" title="38:173	These larger rules have been shown to substantially improve translation accuracy (Galley et al., 2006; DeNeefe et al., 2007)." ></td>
	<td class="line x" title="39:173	We apply Good-Turing discounting to the transducer rule counts and obtain probability estimates: P(rule) = count(rule)count(LHS-root(rule)) When we apply these probabilities to derive an English sentence e and a corresponding Chinese sentence c, we wind up with the joint probability P(e;c)." ></td>
	<td class="line x" title="40:173	The baseline model includes log P(e;c), the two n-gram language models log P(e), and other features for a total of 25." ></td>
	<td class="line x" title="41:173	For example, there is a pair of features to punish rules that drop Chinese content words or introduce spurious English content words." ></td>
	<td class="line x" title="42:173	All features are linearly combined and their weights are optimized using MERT." ></td>
	<td class="line x" title="43:173	For e cient decoding with integrated n-gram language models, all transducer rules must be binarized into rules that contain at most two variables and can be incrementally scored by the language model (Zhang et al., 2006)." ></td>
	<td class="line x" title="44:173	Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences." ></td>
	<td class="line x" title="45:173	We include two other techniques in our baseline." ></td>
	<td class="line x" title="46:173	To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al.(2006)." ></td>
	<td class="line x" title="48:173	3.3 MIRA training We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008)." ></td>
	<td class="line x" title="49:173	Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly:  Select a batch of input sentences f1;:::;fm and decode each fi to obtain a forest of translations." ></td>
	<td class="line x" title="50:173	For each i, select from the forest a set of hypothesis translations ei1;:::;ein, which are the 219 10-best translations according to each of: h(e) w Bleu(e) + h(e) w  Bleu(e) + h(e) w (1)  For each i, select an oracle translation: e = arg max e (Bleu(e) + h(e) w) (2) Let  hij = h(e i ) h(eij)." ></td>
	<td class="line x" title="51:173	For each eij, compute the loss ij = Bleu(e i ) Bleu(eij) (3)  Update w to the value of w0 that minimizes: 1 2kw 0 wk2 + C mX i=1 max 1 j n (ij  hij w0) (4) where C = 0:01." ></td>
	<td class="line x" title="52:173	This minimization is performed by a variant of sequential minimal optimization (Platt, 1998)." ></td>
	<td class="line x" title="53:173	Following Chiang et al.(2008), we calculate the sentence Bleu scores in (1), (2), and (3) in the context of some previous 1-best translations." ></td>
	<td class="line x" title="55:173	We run 20 of these learners in parallel, and when training is finished, the weight vectors from all iterations of all learners are averaged together." ></td>
	<td class="line x" title="56:173	Since the interface between the trainer and the decoder is fairly simplefor each sentence, the decoder sends the trainer a forest, and the trainer returns a weight updateit is easy to use this algorithm with a variety of CKY-based decoders: here, we are using it in conjunction with both the Hiero decoder and our syntax-based decoder." ></td>
	<td class="line x" title="57:173	4 Features In this section, we describe the new features introduced on top of our baseline systems." ></td>
	<td class="line x" title="58:173	Discount features Both of our systems calculate several features based on observed counts of rules in the training data." ></td>
	<td class="line x" title="59:173	Though the syntax-based system uses Good-Turing discounting when computing the P(e;c) feature, we find, as noted above, that it uses quite a few one-count rules, suggesting that their probabilities have been overestimated." ></td>
	<td class="line x" title="60:173	We can directly attack this problem by adding features counti that reward or punish rules seen i times, or features count[i;j] for rules seen between i and j times." ></td>
	<td class="line x" title="61:173	4.1 Target-side features String-to-tree MT o ers some unique levers to pull, in terms of target-side features." ></td>
	<td class="line x" title="62:173	Because the system outputs English trees, we can analyze output trees on the tuning set and design new features to encourage the decoder to produce more grammatical trees." ></td>
	<td class="line x" title="63:173	Rule overlap features While individual rules observed in decoder output are often quite reasonable, two adjacent rules can create problems." ></td>
	<td class="line x" title="64:173	For example, a rule that has a variable of type IN (preposition) needs another rule rooted with IN to fill the position." ></td>
	<td class="line x" title="65:173	If the second rule supplies the wrong preposition, a bad translation results." ></td>
	<td class="line x" title="66:173	The IN node here is an overlap point between rules." ></td>
	<td class="line x" title="67:173	Considering that certain nonterminal symbols may be more reliable overlap points than others, we create a binary feature for each nonterminal." ></td>
	<td class="line x" title="68:173	A rule like: IN(at)$zai will have feature rule-root-IN set to 1 and all other rule-root features set to 0." ></td>
	<td class="line x" title="69:173	Our rule root features range over the original (non-split) nonterminal set; we have 105 in total." ></td>
	<td class="line x" title="70:173	Even though the rule root features are locally attached to individual rulesand therefore cause no additional problems for the decoder searchthey are aimed at problematic rule/rule interactions." ></td>
	<td class="line x" title="71:173	Bad single-level rewrites Sometimes the decoder uses questionable rules, for example: PP(x0:VBN x1:NP-C)$x0 x1 This rule is learned from 62 cases in our training data, where the VBN is almost always the word given." ></td>
	<td class="line x" title="72:173	However, the decoder misuses this rule with other VBNs." ></td>
	<td class="line x" title="73:173	So we can add a feature that penalizes any rule in which a PP dominates a VBN and NP-C." ></td>
	<td class="line x" title="74:173	The feature class bad-rewrite comprises penalties for the following configurations based on our analysis of the tuning set: PP!VBN NP-C PP-BAR!NP-C IN VP!NP-C PP CONJP!RB IN 220 Node count features It is possible that the decoder creates English trees with too many or too few nodes of a particular syntactic category." ></td>
	<td class="line x" title="75:173	For example, there may be an tendency to generate too many determiners or past-tense verbs." ></td>
	<td class="line x" title="76:173	We therefore add a count feature for each of the 109 (non-split) English nonterminal symbols." ></td>
	<td class="line x" title="77:173	For a rule like NPB(NNP(us) NNP(president) x0:NNP) $meiguo zongtong x0 the feature node-count-NPB gets value 1, nodecount-NNP gets value 2, and all others get 0." ></td>
	<td class="line x" title="78:173	Insertion features Among the rules we extract from bilingual corpora are target-language insertion rules, which have a word on the English side, but no words on the source Chinese side." ></td>
	<td class="line x" title="79:173	Sample syntaxbased insertion rules are: NPB(DT(the) x0:NN)$x0 S(x0:NP-C VP(VBZ(is) x1:VP-C))$x0 x1 We notice that our decoder, however, frequently fails to insert words like is and are, which often have no equivalent in the Chinese source." ></td>
	<td class="line x" title="80:173	We also notice that the-insertion rules sometimes have a good e ect, as in the translation in the bloom of youth, but other times have a bad e ect, as in people seek areas of the conspiracy. Each time the decoder uses (or fails to use) an insertion rule, it incurs some risk." ></td>
	<td class="line x" title="81:173	There is no guarantee that the interaction of the rule probabilities and the language model provides the best way to manage this risk." ></td>
	<td class="line x" title="82:173	We therefore provide MIRA with a feature for each of the most common English words appearing in insertion rules, e.g., insert-the and insert-is. There are 35 such features." ></td>
	<td class="line x" title="83:173	4.2 Source-side features We now turn to features that make use of source-side context." ></td>
	<td class="line x" title="84:173	Although these features capture dependencies that cross boundaries between rules, they are still local in the sense that no new states need to be added to the decoder." ></td>
	<td class="line x" title="85:173	This is because the entire source sentence, being fixed, is always available to every feature." ></td>
	<td class="line x" title="86:173	Soft syntactic constraints Neither of our systems uses source-side syntactic information; hence, both could potentially benefit from soft syntactic constraints as described by Marton and Resnik (2008)." ></td>
	<td class="line x" title="87:173	In brief, these features use the output of an independent syntactic parser on the source sentence, rewarding decoder constituents that match syntactic constituents and punishing decoder constituents that cross syntactic constituents." ></td>
	<td class="line x" title="88:173	We use separatelytunable features for each syntactic category." ></td>
	<td class="line x" title="89:173	Structural distortion features Both of our systems have rules with variables that generalize over possible fillers, but neither systems basic model conditions a rule application on the size of a filler, making it di cult to distinguish long-distance reorderings from short-distance reorderings." ></td>
	<td class="line x" title="90:173	To remedy this problem, Chiang et al.(2008) introduce a structural distortion model, which we include in our experiment." ></td>
	<td class="line x" title="92:173	Our syntax-based baseline includes the generative version of this model already." ></td>
	<td class="line x" title="93:173	Word context During rule extraction, we retain word alignments from the training data in the extracted rules." ></td>
	<td class="line x" title="94:173	(If a rule is observed with more than one set of word alignments, we keep only the most frequent one.)" ></td>
	<td class="line x" title="95:173	We then define, for each triple (f;e; f+1), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f; and similarly for triples (f;e; f 1) with f 1 occurring to the left of f. In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>." ></td>
	<td class="line x" title="96:173	These features are somewhat similar to features used by Watanabe et al.(2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al.(2007); here, we are incorporating some of its features directly into the translation model." ></td>
	<td class="line x" title="99:173	5 Experiments For our experiments, we used a 260 million word Chinese/English bitext." ></td>
	<td class="line x" title="100:173	We ran GIZA++ on the entire bitext to produce IBM Model 4 word alignments, and then the link deletion algorithm (Fossum et al., 2008) to yield better-quality alignments." ></td>
	<td class="line x" title="101:173	For 221 System Training Features # Tune Test Hiero MERT baseline 11 35.4 36.1 MIRA syntax, distortion 56 35.9 36.9 syntax, distortion, discount 61 36.6 37.3 all source-side, discount 10990 38.4 37.6 Syntax MERT baseline 25 38.6 39.5 MIRA baseline 25 38.5 39.8 overlap 132 38.7 39.9 node count 136 38.7 40.0 all target-side, discount 283 39.6 40.6 Table 1: Adding new features with MIRA significantly improves translation accuracy." ></td>
	<td class="line x" title="102:173	Scores are case-insensitive IBM Bleu scores." ></td>
	<td class="line x" title="103:173	or  = significantly better than MERT baseline (p < 0:05 or 0:01, respectively)." ></td>
	<td class="line x" title="104:173	the syntax-based system, we ran a reimplementation of the Collins parser (Collins, 1997) on the English half of the bitext to produce parse trees, then restructured and relabeled them as described in Section 3.2." ></td>
	<td class="line x" title="105:173	Syntax-based rule extraction was performed on a 65 million word subset of the training data." ></td>
	<td class="line x" title="106:173	For Hiero, rules with up to two nonterminals were extracted from a 38 million word subset and phrasal rules were extracted from the remainder of the training data." ></td>
	<td class="line x" title="107:173	We trained three 5-gram language models: one on the English half of the bitext, used by both systems, one on one billion words of English, used by the syntax-based system, and one on two billion words of English, used by Hiero." ></td>
	<td class="line x" title="108:173	Modified Kneser-Ney smoothing (Chen and Goodman, 1998) was applied to all language models." ></td>
	<td class="line x" title="109:173	The language models are represented using randomized data structures similar to those of Talbot et al.(2007)." ></td>
	<td class="line x" title="111:173	Our tuning set (2010 sentences) and test set (1994 sentences) were drawn from newswire data from the NIST 2004 and 2005 evaluations and the GALE program (with no overlap at either the segment or document level)." ></td>
	<td class="line x" title="112:173	For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets." ></td>
	<td class="line x" title="113:173	We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both." ></td>
	<td class="line x" title="114:173	We then ran MIRA on the tuning set with 20 parallel learners for Hiero and 73 parallel learners for the syntax-based system." ></td>
	<td class="line x" title="115:173	We chose a stopping iteration based on the Bleu score on the tuning set, and used the averaged feature weights from all iterSyntax-based Hiero count weight count weight 1 +1:28 1 +2:23 2 +0:35 2 +0:77 35  0:73 3 +0:54 610  0:64 4 +0:29 5+  0:02 Table 2: Weights learned for discount features." ></td>
	<td class="line x" title="116:173	Negative weights indicate bonuses; positive weights indicate penalties." ></td>
	<td class="line x" title="117:173	ations of all learners to decode the test set." ></td>
	<td class="line x" title="118:173	The results (Table 1) show significant improvements in both systems (p < 0:01) over already very strong MERT baselines." ></td>
	<td class="line x" title="119:173	Adding the source-side and discount features to Hiero yields a +1.5 Bleu improvement, and adding the target-side syntax and discount features to the syntax-based system yields a +1.1 Bleu improvement." ></td>
	<td class="line x" title="120:173	The results also show that for Hiero, the various classes of features contributed roughly equally; for the syntax-based system, we see that two of the feature classes make small contributions but time constraints unfortunately did not permit isolated testing of all feature classes." ></td>
	<td class="line x" title="121:173	6 Analysis How did the various new features improve the translation quality of our two systems?" ></td>
	<td class="line x" title="122:173	We begin by examining the discount features." ></td>
	<td class="line x" title="123:173	For these features, we used slightly di erent schemes for the two systems, shown in Table 2 with their learned feature weights." ></td>
	<td class="line x" title="124:173	We see in both cases that one-count rules are strongly penalized, as expected." ></td>
	<td class="line x" title="125:173	222 Reward  0:42 a  0:13 are  0:09 at  0:09 on  0:05 was  0:05 from  0:04 s  0:04 by  0:04 is  0:03 it  0:03 its ::: Penalty +0:67 of +0:56 the +0:47 comma +0:13 period +0:11 in +0:08 for +0:06 to +0:05 will +0:04 and +0:02 as +0:02 have ::: Table 3: Weights learned for inserting target English words with rules that lack Chinese words." ></td>
	<td class="line x" title="126:173	6.1 Syntax features Table 3 shows word-insertion feature weights." ></td>
	<td class="line x" title="127:173	The system rewards insertion of forms of be; examples 13 in Figure 1 show typical improved translations that result." ></td>
	<td class="line x" title="128:173	Among determiners, inserting a is rewarded, while inserting the is punished." ></td>
	<td class="line x" title="129:173	This seems to be because the is often part of a fixed phrase, such as the White House, and therefore comes naturally as part of larger phrasal rules." ></td>
	<td class="line x" title="130:173	Inserting the outside these fixed phrases is a risk that the generative model is too inclined to take." ></td>
	<td class="line x" title="131:173	We also note that the system learns to punish unmotivated insertions of commas and periods, which get into our grammar via quirks in the MT training data." ></td>
	<td class="line x" title="132:173	Table 4 shows weights for rule-overlap features." ></td>
	<td class="line x" title="133:173	MIRA punishes the case where rules overlap with an IN (preposition) node." ></td>
	<td class="line x" title="134:173	This makes sense: if a rule has a variable that can be filled by any English preposition, there is a risk that an incorrect preposition will fill it." ></td>
	<td class="line x" title="135:173	On the other hand, splitting at a period is a safe bet, and frees the model to use rules that dig deeper into NP and VP trees when constructing a top-level S. Table 5 shows weights for generated English nonterminals: SBAR-C nodes are rewarded and commas are punished." ></td>
	<td class="line x" title="136:173	The combined e ect of all weights is subtle." ></td>
	<td class="line x" title="137:173	To interpret them further, it helps to look at gross changes in the systems behavior." ></td>
	<td class="line x" title="138:173	For example, a major error in the baseline system is to move X said or X asked from the beginning of the Chinese input to the middle or end of the English transBonus  0:50 period  0:39 VP-C  0:36 VB  0:31 SG-C  0:30 MD  0:26 VBG  0:25 ADJP  0:22 -LRB 0:21 VP-BAR  0:20 NPB-BAR  0:16 FRAG  0:16 PRN  0:15 NPB  0:13 RB  0:12 SBAR-C  0:12 VP-C-BAR  0:11 -RRB::: Penalty +0:93 IN +0:57 NNP +0:44 NN +0:41 DT +0:34 JJ +0:24 right double quote +0:20 VBZ +0:19 NP +0:16 TO +0:15 ADJP-BAR +0:14 PRN-BAR +0:14 NML +0:13 comma +0:12 VBD +0:12 NNPS +0:12 PRP +0:11 SG ::: Table 4: Weights learned for employing rules whose English sides are rooted at particular syntactic categories." ></td>
	<td class="line x" title="139:173	Bonus  0:73 SBAR-C  0:54 VBZ  0:54 IN  0:52 NN  0:51 PP-C  0:47 right double quote  0:39 ADJP  0:34 POS  0:31 ADVP  0:30 RP  0:29 PRT  0:27 SG-C  0:22 S-C  0:21 NNPS  0:21 VP-BAR  0:20 PRP  0:20 NPB-BAR ::: Penalty +1:30 comma +0:80 DT +0:58 PP +0:44 TO +0:33 NNP +0:30 NNS +0:30 NML +0:22 CD +0:18 PRN +0:16 SYM +0:15 ADJP-BAR +0:15 NP +0:15 MD +0:15 HYPH +0:14 PRN-BAR +0:14 NP-C +0:11 ADJP-C ::: Table 5: Weights learned for generating syntactic nodes of various types anywhere in the English translation." ></td>
	<td class="line x" title="140:173	223 lation." ></td>
	<td class="line x" title="141:173	The error occurs with many speaking verbs, and each time, we trace it to a di erent rule." ></td>
	<td class="line x" title="142:173	The problematic rules can even be non-lexical, e.g.: S(x0:NP-C x1:VP x2:; x3:NP-C x4:VP x5::) $x3 x4 x2 x0 x1 x5 It is therefore di cult to come up with a straightforward feature to address the problem." ></td>
	<td class="line x" title="143:173	However, when we apply MIRA with the features already listed, these translation errors all disappear, as demonstrated by examples 45 in Figure 1." ></td>
	<td class="line x" title="144:173	Why does this happen?" ></td>
	<td class="line x" title="145:173	It turns out that in translation hypotheses that move X said or X asked away from the beginning of the sentence, more commas appear, and fewer S-C and SBAR-C nodes appear." ></td>
	<td class="line x" title="146:173	Therefore, the new features work to discourage these hypotheses." ></td>
	<td class="line x" title="147:173	Example 6 shows additionally that commas next to speaking verbs are now correctly deleted." ></td>
	<td class="line x" title="148:173	Examples 78 in Figure 1 show other kinds of unanticipated improvements." ></td>
	<td class="line x" title="149:173	We do not have space for a fuller analysis, but we note that the specific effects we describe above account for only part of the overall Bleu improvement." ></td>
	<td class="line x" title="150:173	6.2 Word context features In Table 6 are shown feature weights learned for the word-context features." ></td>
	<td class="line x" title="151:173	A surprising number of the highest-weighted features have to do with translations of dates and bylines." ></td>
	<td class="line x" title="152:173	Many of the penalties seem to discourage spurious insertion or deletion of frequent words (for, s, said, parentheses, and quotes)." ></td>
	<td class="line x" title="153:173	Finally, we note that several of the features (the thirdand eighth-ranked reward and twelfthranked penalty) shape the translation of shuo said, preferring translations with an overt complementizer that and without a comma." ></td>
	<td class="line x" title="154:173	Thus these features work together to attack a frequent problem that our targetsyntax features also addressed." ></td>
	<td class="line x" title="155:173	Figure 2 shows the performance of Hiero with all of its features on the tuning and test sets over time." ></td>
	<td class="line x" title="156:173	The scores on the tuning set rise rapidly, and the scores on the test set also rise, but much more slowly, and there appears to be slight degradation after the 18th pass through the tuning data." ></td>
	<td class="line x" title="157:173	This seems in line with the finding of Watanabe et al.(2007) that with on the order of 10,000 features, overfitting is possible, but we can still improve accuracy on new data." ></td>
	<td class="line x" title="159:173	35  35.5  36  36.5  37  37.5  38  38.5  0  5  10  15  20  25 BLEU Epoch Tune Test Figure 2: Using over 10,000 word-context features leads to overfitting, but its detrimental e ects are modest." ></td>
	<td class="line x" title="160:173	Scores on the tuning set were obtained from the 1-best output of the online learning algorithm, whereas scores on the test set were obtained using averaged weights." ></td>
	<td class="line x" title="161:173	Early stopping would have given +0.2 Bleu over the results reported in Table 1.1 7 Conclusion We have described a variety of features for statistical machine translation and applied them to syntaxbased and hierarchical systems." ></td>
	<td class="line x" title="162:173	We saw that these features, discriminatively trained using MIRA, led to significant improvements, and took a closer look at the results to see how the new features qualitatively improved translation quality." ></td>
	<td class="line x" title="163:173	We draw three conclusions from this study." ></td>
	<td class="line x" title="164:173	First, we have shown that these new features can improve the performance even of top-scoring MT systems." ></td>
	<td class="line x" title="165:173	Second, these results add to a growing body of evidence that MIRA is preferable to MERT for discriminative training." ></td>
	<td class="line x" title="166:173	When training over 10,000 features on a modest amount of data, we, like Watanabe et al.(2007), did observe overfitting, yet saw improvements on new data." ></td>
	<td class="line x" title="168:173	Third, we have shown that syntax-based machine translation o ers possibilities for features not available in other models, making syntax-based MT and MIRA an especially strong combination for future work." ></td>
	<td class="line x" title="169:173	1It was this iteration, in fact, which was used to derive the combined feature count used in the title of this paper." ></td>
	<td class="line x" title="170:173	224 1 MERT: the united states pending israeli clarification on golan settlement plan MIRA: the united states is waiting for israeli clarification on golan settlement plan 2 MERT: ::: the average life expectancy of only 18 months , canada s minority goverment will ::: MIRA: ::: the average life expectancy of canadas previous minority government is only 18 months ::: 3 MERT: ::: since un inspectors expelled by north korea ::: MIRA: ::: since un inspectors were expelled by north korea ::: 4 MERT: another thing is ::: , ' he said , ' obviously , the first thing we need to do ::: . MIRA: he said : ' obviously , the first thing we need to do ::: , and another thing is ::: . ' 5 MERT: the actual timing ::: reopened in january , yoon said . MIRA: yoon said the issue of the timing ::: 6 MERT: ::: us led coalition forces , said today that the crash ::: MIRA: ::: us led coalition forces said today that a us military ::: 7 MERT: ::: and others will feel the danger . MIRA: ::: and others will not feel the danger . 8 MERT: in residential or public activities within 200 meters of the region , ::: MIRA: within 200 m of residential or public activities area , ::: Figure 1: Improved syntax-based translations due to MIRA-trained weights." ></td>
	<td class="line x" title="171:173	Bonus f e context  1:19 <unk> <unk> f 1 = ri day  1:01 <unk> <unk> f 1 = (  0:84 , that f 1 = shuo say  0:82 yue month <unk> f+1 = <unk>  0:78 ' ' f 1 = <unk>  0:76 ' ' f+1 = <unk>  0:66 <unk> <unk> f+1 = nian year  0:65 , that f+1 = <unk> ::: Penalty f e context +1:12 <unk> ) f+1 = <unk> +0:83 jiang shall be f+1 = <unk> +0:83 zhengfu government the f 1 = <unk> +0:73 <unk> ) f 1 = <unk> +0:73 <unk> ( f+1 = <unk> +0:72 <unk> ) f 1 = ri day +0:70 <unk> ( f 1 = ri day +0:69 <unk> ( f 1 = <unk> +0:66 <unk> for f 1 = <unk> +0:66 <unk> s f 1 = , +0:65 <unk> said f 1 = <unk> +0:60 , , f 1 = shuo say ::: Table 6: Weights learned for word-context features, which fire when English word e is generated aligned to Chinese word f, with Chinese word f 1 to the left or f+1 to the right." ></td>
	<td class="line x" title="172:173	Glosses for Chinese words are not part of features." ></td>
	<td class="line x" title="173:173	225" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-1026
Efficient Parsing for Transducer Grammars
DeNero, John;Bansal, Mohit;Pauls, Adam;Klein, Dan;"></td>
	<td class="line x" title="1:239	Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 227235, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:239	c 2009 Association for Computational Linguistics Efficient Parsing for Transducer Grammars John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein Computer Science Division University of California, Berkeley {denero, mbansal, adpauls, klein}@cs.berkeley.edu Abstract The tree-transducer grammars that arise in current syntactic machine translation systems are large, flat, and highly lexicalized." ></td>
	<td class="line x" title="3:239	We address the problem of parsing efficiently with such grammars in three ways." ></td>
	<td class="line x" title="4:239	First, we present a pair of grammar transformations that admit an efficient cubic-time CKY-style parsing algorithm despite leaving most of the grammar in n-ary form." ></td>
	<td class="line x" title="5:239	Second, we show how the number of intermediate symbols generated by this transformation can be substantially reduced through binarization choices." ></td>
	<td class="line x" title="6:239	Finally, we describe a two-pass coarse-to-fine parsing approach that prunes the search space using predictions from a subset of the original grammar." ></td>
	<td class="line x" title="7:239	In all, parsing time reduces by 81%." ></td>
	<td class="line x" title="8:239	We also describe a coarse-to-fine pruning scheme for forest-based language model reranking that allows a 100-fold increase in beam size while reducing decoding time." ></td>
	<td class="line x" title="9:239	The resulting translations improve by 1.3 BLEU." ></td>
	<td class="line x" title="10:239	1 Introduction Current approaches to syntactic machine translation typically include two statistical models: a syntactic transfer model and an n-gram language model." ></td>
	<td class="line x" title="11:239	Recent innovations have greatly improved the efficiency of language model integration through multipass techniques, such as forest reranking (Huang and Chiang, 2007), local search (Venugopal et al., 2007), and coarse-to-fine pruning (Petrov et al., 2008; Zhang and Gildea, 2008)." ></td>
	<td class="line oc" title="12:239	Meanwhile, translation grammars have grown in complexity from simple inversion transduction grammars (Wu, 1997) to general tree-to-string transducers (Galley et al., 2004) and have increased in size by including more synchronous tree fragments (Galley et al., 2006; Marcuetal.,2006; DeNeefeetal.,2007)." ></td>
	<td class="line x" title="13:239	Asaresult of these trends, the syntactic component of machine translation decoding can now account for a substantial portion of total decoding time." ></td>
	<td class="line x" title="14:239	In this paper, we focus on efficient methods for parsing with very large tree-to-string grammars, which have flat n-ary rules with many adjacent non-terminals, as in Figure 1." ></td>
	<td class="line x" title="15:239	These grammars are sufficiently complex that thepurelysyntacticpassofourmulti-passdecoderis thecompute-timebottleneckundersomeconditions." ></td>
	<td class="line x" title="16:239	Given that parsing is well-studied in the monolingual case, it is worth asking why MT grammars are not simply like those used for syntactic analysis." ></td>
	<td class="line x" title="17:239	There are several good reasons." ></td>
	<td class="line x" title="18:239	The most important is that MT grammars must do both analysis and generation." ></td>
	<td class="line x" title="19:239	To generate, it is natural to memorize larger lexical chunks, and so rules are highly lexicalized." ></td>
	<td class="line x" title="20:239	Second, syntax diverges between languages, and each divergence expands the minimal domain of translation rules, so rules are large and flat." ></td>
	<td class="line x" title="21:239	Finally, we see most rules very few times, so it is challenging to subcategorize non-terminals to the degree done in analytic parsing." ></td>
	<td class="line x" title="22:239	This paper developsencodings, algorithms, andpruningstrategies for such grammars." ></td>
	<td class="line x" title="23:239	We first investigate the qualitative properties of MT grammars, then present a sequence of parsing methods adapted to their broad characteristics." ></td>
	<td class="line x" title="24:239	We give normal forms which are more appropriate than Chomsky normal form, leaving the rules mostly flat." ></td>
	<td class="line x" title="25:239	We then describe a CKY-like algorithm which applies such rules efficiently, working directly over the n-ary forms in cubic time." ></td>
	<td class="line x" title="26:239	We show how thoughtful 227 NNP1 no daba una bofetada a DT2 NN3 verde S ! NNP no daba una bofetada a DT NN verde DT+NN ! DT NN Maria no daba una bofetada a la bruja verde Mary did not slap the green witch S NNP NNDT NNP1 did not slap DT2 green NN3 S ! Lexical normal form (LNF) transformation S ! NNP no daba una bofetada a DT NN verde S\NNP ! no daba una bofetada a DT+NN verde S ! NNP S\NNP Anchored LNF transformation DT+NN ! DT NN 0 22,500 45,000 67,500 90,000 1 2 3 4 5 6 7 8 9 10+ 0 17,500 35,000 52,500 70,000 1 2 3 4 5 6+ Original grammar rules NP ! DT NN NNS S ! NNP no daba una bofetada a DT+NN verde NP ! DT+NN NNS NP ! DT NN+NNSor Type-minimizing binarization Required symbols Sequences to build DT+NNDT NN NNSNNP NP S DT,NN DT,NN,NNS Minimal binary rules for LNF NP ! DT+NN NNS NP ! DT+NN NNS X no la X daba bruja daba VP ! no daba NP ! la bruja  Maria daba  S ! NP   daba S ! NP daba (a) (b) (c) Figure 1: (a) A synchronous transducer rule has coindexed non-terminals on the source and target side." ></td>
	<td class="line x" title="27:239	Internal grammatical structure of the target side has been omitted." ></td>
	<td class="line x" title="28:239	(b) The source-side projection of the rule is a monolingual source-language rule with target-side grammar symbols." ></td>
	<td class="line x" title="29:239	(c) A training sentence pair is annotated with a target-side parse tree and a word alignment, which license this rule to be extracted." ></td>
	<td class="line x" title="30:239	binarization can further increase parsing speed, and we present a new coarse-to-fine scheme that uses rule subsets rather than symbol clustering to build a coarse grammar projection." ></td>
	<td class="line x" title="31:239	These techniques reduce parsing time by 81% in aggregate." ></td>
	<td class="line x" title="32:239	Finally, we demonstrate that we can accelerate forest-based reranking with a language model by pruning with information from the parsing pass." ></td>
	<td class="line x" title="33:239	This approach enables a 100-fold increase in maximum beam size, improving translation quality by 1.3 BLEU while decreasing total decoding time." ></td>
	<td class="line x" title="34:239	2 Tree Transducer Grammars Tree-to-string transducer grammars consist of weighted rules like the one depicted in Figure 1." ></td>
	<td class="line x" title="35:239	Each n-ary rule consists of a root symbol, a sequence of lexical items and non-terminals on the source-side, and a fragment of a syntax tree on the target side." ></td>
	<td class="line x" title="36:239	Each non-terminal on the source side corresponds to a unique one on the target side." ></td>
	<td class="line x" title="37:239	Aligned non-terminals share a grammar symbol derived from a target-side monolingual grammar." ></td>
	<td class="line x" title="38:239	These grammars are learned from word-aligned sentence pairs annotated with target-side phrase structure trees." ></td>
	<td class="line x" title="39:239	Extraction proceeds by using word alignments to find correspondences between targetside constituents and source-side word spans, then discovering transducer rules that match these conNNP1 no daba una bofetada a DT2 NN3 verde S ! NNP no daba una bofetada a DT NN verde DT+NN ! DT NN Maria no daba una bofetada a la bruja verde Mary did not slap the green witch S NNP NNDT NNP1 did not slap DT2 green NN3 S ! Lexical rules cannot contain adjacent non-terminals S ! NNP no daba una bofetada a DT NN verde S\NNP ! no daba una bofetada a DT+NN verde S ! NNP S\NNP Anchored LNF rules are bounded by lexical items DT+NN ! DT NN 0 22,500 45,000 67,500 90,000 1 2 3 4 5 6 7 8 9 10+ 0 17,500 35,000 52,500 70,000 1 2 3 4 5 6+ Original grammar rules are flat and lexical NP ! DT NN NNS S ! NNP no daba una bofetada a DT+NN verde Non-lexical rules are binarized using few symbols Required symbols Sequences to build DT+NNDT NN NNSNNP NP S DT  NN DT  NN  NNS Binary rules for LNF that minimize symbol count NP ! DT+NN NNS NP ! DT+NN NNS X no la X daba bruja daba VP ! no daba NP ! la bruja  Maria daba  S ! NP   daba S ! NP daba (a) (b) (c) 0 22,500 45,000 67,500 90,000 1 2 3 4 5 6 7+ 0 22,500 45,000 67,500 90,000 1 2 3 4 5 6 7 8 9 10+ Right-branching Left-branching Greedy Optimal (ILP) 0 3,000 6,000 9,000 443 1,101 5,871 8,095 0 30,000 60,000 90,000 1 2 3 4 5 6 7+ Figure 2: Transducer grammarsare composed ofvery flat rules." ></td>
	<td class="line x" title="40:239	Above, the histogram shows rule counts for each rule size among the 332,000 rules that apply to an individual 30-word sentence." ></td>
	<td class="line x" title="41:239	The size of a rule is the total number of non-terminals and lexical items in its sourceside yield." ></td>
	<td class="line oc" title="42:239	stituent alignments (Galley et al., 2004)." ></td>
	<td class="line x" title="43:239	Given this correspondence, an array of extraction procedures yields rules that are well-suited to machine translation (Galley et al., 2006; DeNeefe et al., 2007; Marcu et al., 2006)." ></td>
	<td class="line x" title="44:239	Rule weights are estimated by discriminatively combining relative frequency counts and other rule features." ></td>
	<td class="line x" title="45:239	A transducer grammarGcan be projected onto its source language, inducing a monolingual grammar." ></td>
	<td class="line x" title="46:239	Ifweweighteachrulebythemaximumweightofits projecting synchronous rules, then parsing with this projected grammar maximizes the translation model score for a source sentence." ></td>
	<td class="line x" title="47:239	We need not even considerthetargetsideoftransducerrulesuntilintegrating an n-gram language model or other non-local features of the target language." ></td>
	<td class="line x" title="48:239	We conduct experiments with a grammar extracted from 220 million words of Arabic-English bitext, extractingruleswithupto6non-terminals." ></td>
	<td class="line x" title="49:239	A histogram of the size of rules applicable to a typical 30-word sentence appears in Figure 2." ></td>
	<td class="line x" title="50:239	The grammar includes149grammaticalsymbols,anaugmentation of the Penn Treebank symbol set." ></td>
	<td class="line x" title="51:239	To evaluate, we decoded 300 sentences of up to 40 words in length from the NIST05 Arabic-English test set." ></td>
	<td class="line x" title="52:239	3 Efficient Grammar Encodings Monolingual parsing with a source-projected transducer grammar is a natural first pass in multi-pass decoding." ></td>
	<td class="line x" title="53:239	These grammars are qualitatively different from syntactic analysis grammars, such as the lexicalized grammars of Charniak (1997) or the heavily state-split grammars of Petrov et al.(2006)." ></td>
	<td class="line x" title="55:239	228 In this section, we develop an appropriate grammar encoding that enables efficient parsing." ></td>
	<td class="line x" title="56:239	It is problematic to convert these grammars into Chomsky normal form, which CKY requires." ></td>
	<td class="line x" title="57:239	Because transducer rules are very flat and contain specific lexical items, binarization introduces a large numberofintermediategrammarsymbols." ></td>
	<td class="line x" title="58:239	Rulesize and lexicalization affect parsing complexity whether the grammar is binarized explicitly (Zhang et al., 2006) or implicitly binarized using Early-style intermediate symbols (Zollmann et al., 2006)." ></td>
	<td class="line x" title="59:239	Moreover, the resulting binary rules cannot be Markovized to mergesymbols, asinKleinandManning(2003), because each rule is associated with a target-side tree that cannot be abstracted." ></td>
	<td class="line x" title="60:239	We also do not restrict the form of rules in the grammar, a common technique in syntactic machine translation." ></td>
	<td class="line x" title="61:239	For instance, Zollmann et al.(2006) follow Chiang (2005) in disallowing adjacent nonterminals." ></td>
	<td class="line x" title="63:239	Watanabe et al.(2006) limit grammars to Griebach-Normal form." ></td>
	<td class="line x" title="65:239	However, general tree transducer grammars provide excellent translation performance (Galley et al., 2006), and so we focus on parsing with all available rules." ></td>
	<td class="line x" title="66:239	3.1 Lexical Normal Form Sequences of consecutive non-terminals complicate parsing because they require a search over nonterminal boundaries when applied to a sentence span." ></td>
	<td class="line x" title="67:239	We transform the grammar to ensure that all rules containing lexical items (lexical rules) do not contain sequences of non-terminals." ></td>
	<td class="line x" title="68:239	We allow both unary and binary non-lexical rules." ></td>
	<td class="line x" title="69:239	Let L be the set of lexical items and V the set of non-terminal symbols in the original grammar." ></td>
	<td class="line x" title="70:239	Then, lexical normal form (LNF) limits productions to two forms: Non-lexical: X  X1(X2) Lexical: X  (X1)(X2)  = w+(Xiw+) Above, all Xi  V and w+  L+." ></td>
	<td class="line x" title="71:239	Symbols in parentheses are optional." ></td>
	<td class="line x" title="72:239	The nucleus  of lexical rules is a mixed sequence that has lexical items on each end and no adjacent non-terminals." ></td>
	<td class="line x" title="73:239	Converting a grammar into LNF requires two steps." ></td>
	<td class="line x" title="74:239	In the sequence elimination step, for every NNP1 no daba una bofetada a DT2 NN3 verde S ! NNP no daba una bofetada a DT NN verde Maria no daba una bofetada a la bruja verde Mary did not slap the green witch S NNP NNDT NNP1 did not slap DT2 green NN3 S ! LNF replaces non-terminal sequences in lexical rules S ! NNP no daba una bofetada a DT NN verde S\NNP ! no daba una bofetada a DT+NN verde S ! NNP S\NNP Anchored LNF rules are bounded by lexical items 0 22,500 45,000 67,500 90,000 1 2 3 4 5 6 7 8 9 10+ 0 17,500 35,000 52,500 70,000 1 2 3 4 5 6+ Original grammar rules are flat and lexical NP ! DT NN NNS S ! NNP no daba una bofetada a DT+NN verde Non-lexical rules are binarized using few symbols Non-lexical rules before binarization: Equivalent binary rules, minimizing symbol count: X no la X daba bruja daba VP ! no daba NP ! la bruja  Maria daba  S ! NP   daba S ! NP daba (a) (b) (c) 0 22,500 45,000 67,500 90,000 1 2 3 4 5 6 7+ 0 22,500 45,000 67,500 90,000 1 2 3 4 5 6 7 8 9 10+ Right-branching Left-branching Greedy Optimal (ILP) 0 3,000 6,000 9,000 443 1,101 5,871 8,095 0 30,000 60,000 90,000 1 2 3 4 5 6 7+ DT+NN ! DT NN NP ! DT NN NNS DT+NN ! DT NN NP ! DT+NN NNSDT+NN ! DT NN NP ! DT+NN NNSDT+NN ! DT NN Figure 3: We transform the original grammar by first eliminating non-terminal sequences in lexical rules." ></td>
	<td class="line x" title="75:239	Next, we binarize, adding a minimal number of intermediate grammar symbols and binary non-lexical rules." ></td>
	<td class="line x" title="76:239	Finally, anchored LNF further transforms lexical rules to begin and end with lexical items by introducing additional symbols." ></td>
	<td class="line x" title="77:239	lexical rule we replace each sequence of consecutive non-terminalsX1 Xn with the intermediate symbol X1++Xn (abbreviated X1:n) and introduce a non-lexical rule X1++Xn  X1 Xn." ></td>
	<td class="line x" title="78:239	In the binarization step, we introduce further intermediate symbols and rules to binarize all non-lexical rules in the grammar, including those added by sequence elimination." ></td>
	<td class="line x" title="79:239	3.2 Non-terminal Binarization Exactlyhowwebinarizenon-lexicalrulesaffectsthe total number of intermediate symbols introduced by the LNF transformation." ></td>
	<td class="line x" title="80:239	Binarization involves selecting a set of symbols that will allow us to assemble the right-hand side X1 Xn of every non-lexical rule using binary productions." ></td>
	<td class="line x" title="81:239	This symbol set must at least include the left-hand side of every rule in the grammar (lexical and non-lexical), including the intermediate 229 NNP1 no daba una bofetada a DT2 NN3 verde S ! NNP no daba una bofetada a DT NN verde DT+NN ! DT NN Maria no daba una bofetada a la bruja verde Mary did not slap the green witch S NNP NNDT NNP1 did not slap DT2 green NN3 S ! Lexical rules cannot contain adjacent non-terminals S ! NNP no daba una bofetada a DT NN verde S\NNP ! no daba una bofetada a DT+NN verde S ! NNP S\NNP Anchored LNF rules are bounded by lexical items DT+NN ! DT NN 0 22,500 45,000 67,500 90,000 1 2 3 4 5 6 7 8 9 10+ 0 17,500 35,000 52,500 70,000 1 2 3 4 5 6+ Original grammar rules are flat and lexical NP ! DT NN NNS S ! NNP no daba una bofetada a DT+NN verde Non-lexical rules are binarized using few symbols Required symbols Sequences to build DT+NNDT NN NNSNNP NP S DT  NN DT  NN  NNS Binary rules for LNF that minimize symbol count NP ! DT+NN NNS NP ! DT+NN NNS X no la X daba bruja daba VP ! no daba NP ! la bruja  Maria daba  S ! NP   daba S ! NP daba (a) (b) (c) 0 22,500 45,000 67,500 90,000 1 2 3 4 5 6 7+ 0 22,500 45,000 67,500 90,000 1 2 3 4 5 6 7 8 9 10+ Right-branching Left-branching Greedy Optimal (ILP) 0 3,000 6,000 9,000 443 1,101 5,871 8,095 0 30,000 60,000 90,000 1 2 3 4 5 6 7+ Figure 4: The number of non-terminal symbols introduced to the grammar through LNF binarization depends upon the policy for binarizing type sequences." ></td>
	<td class="line x" title="82:239	This experimentshowsresultsfromtransformingagrammarthat has already been filtered for a particular short sentence." ></td>
	<td class="line x" title="83:239	Both the greedy and optimal binarizations use far fewer symbols than naive binarizations." ></td>
	<td class="line x" title="84:239	symbols X1:n introduced by sequence elimination." ></td>
	<td class="line x" title="85:239	To ensure that a symbol sequence X1 Xn can be constructed, we select a split point k and add intermediate types X1:k and Xk+1:n to the grammar." ></td>
	<td class="line x" title="86:239	We must also ensure that the sequences X1 Xk and Xk+1 Xn can be constructed." ></td>
	<td class="line x" title="87:239	As baselines, we used left-branching (where k = 1 always) and right-branching (where k = n1) binarizations." ></td>
	<td class="line x" title="88:239	We also tested a greedy binarization approach, choosing k to minimize the number of grammar symbols introduced." ></td>
	<td class="line x" title="89:239	We first try to selectk such that both X1:k and Xk+1:n are already in the grammar." ></td>
	<td class="line x" title="90:239	If no such k exists, we select k such that one of the intermediate types generated is already used." ></td>
	<td class="line x" title="91:239	If no such k exists again, we choose k = floorleftbig12nfloorrightbig." ></td>
	<td class="line x" title="92:239	This policy only creates new intermediate types when necessary." ></td>
	<td class="line x" title="93:239	Song et al.(2008) propose a similar greedy approachtobinarizationthatusescorpusstatisticsto select common types rather than explicitly reusing types that have already been introduced." ></td>
	<td class="line x" title="95:239	Finally, we computed an optimal binarization that explicitly minimizes the number of symbols in the resulting grammar." ></td>
	<td class="line x" title="96:239	We cast the minimization as an integer linear program (ILP)." ></td>
	<td class="line x" title="97:239	Let V be the set of all base non-terminal symbols in the grammar." ></td>
	<td class="line x" title="98:239	We introduce an indicator variable TY for each symbol Y  V + to indicate that Y is used in the grammar." ></td>
	<td class="line x" title="99:239	Y can be either a base non-terminal symbol Xi or an intermediate symbol X1:n. We also introduce indicators AY,Z for each pairs of symbols, indicating that both Y and Z are used in the grammar." ></td>
	<td class="line x" title="100:239	Let L  V + be the set of left-hand side symbols for all lexical and non-lexical rules already in the grammar. Let R be the set of symbol sequences on the right-hand side of all non-lexical rules." ></td>
	<td class="line x" title="101:239	Then, the ILP takes the form: min summationdisplay YV + TY (1) s.t. TY = 1 Y  L (2) 1  summationdisplay k AX1:k,Xk+1:n X1 Xn  R (3) TX1:n  summationdisplay k AX1:k,Xk+1:n X1:n (4) AY,Z  TY , AY,Z  TZ Y,Z (5) The solution to this ILP indicates which symbols appear in a minimal binarization." ></td>
	<td class="line x" title="102:239	Equation 1 explicitly minimizes the number of symbols." ></td>
	<td class="line x" title="103:239	Equation 2 ensures that all symbols already in the grammar remain in the grammar." ></td>
	<td class="line x" title="104:239	Equation 3 does not require that a symbol represent the entire right-hand side of each non-lexical rule, but does ensure that each right-hand side sequence can be built from two subsequence symbols." ></td>
	<td class="line x" title="105:239	Equation 4 ensures that any included intermediate type can also be built from two subsequence types." ></td>
	<td class="line x" title="106:239	Finally,Equation5ensuresthatifapairisused,each member of the pair is included." ></td>
	<td class="line x" title="107:239	This program can be optimized with an off-the-shelf ILP solver.1 Figure 4 shows the number of intermediate grammar symbols needed for the four binarization policies described above for a short sentence." ></td>
	<td class="line x" title="108:239	Our ILP solver could only find optimal solutions for very short sentences (which have small grammars after relativization)." ></td>
	<td class="line x" title="109:239	Because greedy requires very little time to compute and generates symbol counts that are close to optimal when both can be computed, we use it for our remaining experiments." ></td>
	<td class="line x" title="110:239	3.3 Anchored Lexical Normal Form We also consider a further grammar transformation, anchored lexical normal form (ALNF), in which the yield of lexical rules must begin and end with a lexical item." ></td>
	<td class="line x" title="111:239	As shown in the following section, ALNF improves parsing performance over LNF by shifting work from lexical rule applications to non-lexical 1We used lp solve: http://sourceforge.net/projects/lpsolve." ></td>
	<td class="line x" title="112:239	230 rule applications." ></td>
	<td class="line x" title="113:239	ALNF consists of rules with the following two forms: Non-lexical: X  X1(X2) Lexical: X  w+(Xiw+) To convert a grammar into ALNF, we first transform it into LNF, then introduce additional binary rules that split off non-terminal symbols from the ends of lexical rules, as shown in Figure 3." ></td>
	<td class="line x" title="114:239	4 Efficient CKY Parsing We now describe a CKY-style parsing algorithm for grammars in LNF." ></td>
	<td class="line x" title="115:239	The dynamic program is organized into spans Sij and computes the Viterbi score w(i,j,X) for each edge Sij[X], the weight of the maximumparseoverwordsi+1 toj, rootedatsymbol X. For each Sij, computation proceeds in three phases: binary, lexical, and unary." ></td>
	<td class="line x" title="116:239	4.1 Applying Non-lexical Binary Rules For a span Sij, we first apply the binary non-lexical rulesjustasinstandardCKY,computinganintermediate Viterbi score wb(i,j,X)." ></td>
	<td class="line x" title="117:239	Let r be the weight of rule r. Then, wb(i,j,X) = max r=XX1X2 r j1max k=i+1 w(i,k,X1)w(k,j,X2)." ></td>
	<td class="line x" title="118:239	The quantitiesw(i,k,X1) andw(k,j,X2) will have already been computed by the dynamic program." ></td>
	<td class="line x" title="119:239	The work in this phase is cubic in sentence length." ></td>
	<td class="line x" title="120:239	4.2 Applying Lexical Rules On the other hand, lexical rules in LNF can be applied without binarization, because they only apply to particular spans that contain the appropriate lexicalitems." ></td>
	<td class="line x" title="121:239	ForagivenSij,wefirstcomputeallthelegal mappings of each rule onto the span." ></td>
	<td class="line x" title="122:239	A mapping consists of a correspondence between non-terminals in the rule and subspans of Sij." ></td>
	<td class="line x" title="123:239	In practice, there is typically only one way that a lexical rule in LNF canmapontoaspan, becausemostlexicalitemswill appear only once in the span." ></td>
	<td class="line x" title="124:239	Let m be a legal mapping and r its corresponding rule." ></td>
	<td class="line x" title="125:239	Let S(i)klscript [X] be the edge mapped to the ith nonterminal ofr underm, andr the weight ofr." ></td>
	<td class="line x" title="126:239	Then, wl(i,j,X) = maxm r productdisplay S(i)klscript [X] w(k,lscript,X)." ></td>
	<td class="line x" title="127:239	Again, w(k,lscript,X) will have been computed by the dynamic program." ></td>
	<td class="line x" title="128:239	Assuming only a constant number of mappings per rule per span, the work in this phase is quadratic." ></td>
	<td class="line x" title="129:239	We can then merge wl and wb: w(i,j,X) = max(wl(i,j,X),wb(i,j,X))." ></td>
	<td class="line x" title="130:239	To efficiently compute mappings, we store lexical rules in a trie (or suffix array)  a searchable graph that indexes rules according to their sequence of lexical items and non-terminals." ></td>
	<td class="line x" title="131:239	This data structure has been used similarly to index whole training sentences for efficient retrieval (Lopez, 2007)." ></td>
	<td class="line x" title="132:239	To find all rules that map onto a span, we traverse the trie using depth-first search." ></td>
	<td class="line x" title="133:239	4.3 Applying Unary Rules Unary non-lexical rules are applied after lexical rules and non-lexical binary rules." ></td>
	<td class="line x" title="134:239	w(i,j,X) = max r:r=XX1 rw(i,j,X1)." ></td>
	<td class="line x" title="135:239	While this definition is recursive, we allow only one unary rule application per symbol X at each span to prevent infinite derivations." ></td>
	<td class="line x" title="136:239	This choice does not limit the generality of our algorithm: chains of unaries can always be collapsed via a unary closure." ></td>
	<td class="line x" title="137:239	4.4 Bounding Split Points for Binary Rules Non-lexical binary rules can in principle apply to any span Sij where j i  2, using any split point k such that i < k < j. In practice, however, many rules cannot apply to many (i,k,j) triples because the symbols for their children have not been constructed successfully over the subspans Sik and Skj." ></td>
	<td class="line x" title="138:239	Therefore, the precise looping order over rules and split points can influence computation time." ></td>
	<td class="line x" title="139:239	We found the following nested looping order for the binary phase of processing an edge Sij[X] gave the fastest parsing times for these grammars: 1." ></td>
	<td class="line x" title="140:239	Loop over symbols X1 for the left child 2." ></td>
	<td class="line x" title="141:239	Loop over all rules X  X1X2 containing X1 3." ></td>
	<td class="line x" title="142:239	Loop over split points k : i < k < j 4." ></td>
	<td class="line x" title="143:239	Update wb(i,j,X) as necessary This looping order allows for early stopping via additional bookkeeping in the algorithm." ></td>
	<td class="line x" title="144:239	We track the following statistics as we parse: 231 Grammar Bound checks Parsing time LNF no 264 LNF yes 181 ALNF yes 104 Table 1: Adding bound checks to CKY and transforming the grammar from LNF to anchored LNF reduce parsing time by 61% for 300 sentences of length 40 or less." ></td>
	<td class="line x" title="145:239	No approximations have been applied, so all three scenarios produce no search errors." ></td>
	<td class="line x" title="146:239	Parsing time is in minutes." ></td>
	<td class="line x" title="147:239	minEND(i,X), maxEND(i,X): The minimum and maximum position k for which symbol X was successfully built over Sik." ></td>
	<td class="line x" title="148:239	minSTART(j,X), maxSTART(j,X): The minimum and maximum position k for which symbol X was successfully built over Skj." ></td>
	<td class="line x" title="149:239	We then bound k by mink and maxk in the inner loop using these statistics." ></td>
	<td class="line x" title="150:239	If ever mink > maxk, then the loop is terminated early." ></td>
	<td class="line x" title="151:239	1." ></td>
	<td class="line x" title="152:239	set mink = i+ 1,maxk = j 1 2." ></td>
	<td class="line x" title="153:239	loop over symbols X1 for the left child mink = max(mink,minEND(i,X1)) maxk = min(maxk,maxEND(i,X1)) 3." ></td>
	<td class="line x" title="154:239	loop over rules X  X1X2 mink = max(mink,minSTART(j,X2)) maxk = min(maxk,maxSTART(j,X2)) 4." ></td>
	<td class="line x" title="155:239	loop over split points k : mink  k  maxk 5." ></td>
	<td class="line x" title="156:239	update wb(i,j,X) as necessary In this way, we eliminate unnecessary work by avoiding split points that we know beforehand cannot contribute to wb(i,j,X)." ></td>
	<td class="line x" title="157:239	4.5 Parsing Time Results Table 1 shows the decrease in parsing time from including these bound checks, as well as switching from lexical normal form to anchored LNF." ></td>
	<td class="line x" title="158:239	Using ALNF rather than LNF increases the number of grammar symbols and non-lexical binary rules, but makes parsing more efficient in three ways." ></td>
	<td class="line x" title="159:239	First, it decreases the number of spans for whichalexicalrulehasalegalmapping." ></td>
	<td class="line x" title="160:239	Inthisway, ALNF effectively shifts work from the lexical phase to the binary phase." ></td>
	<td class="line x" title="161:239	Second, ALNF reduces the time spent searching the trie for mappings, because the first transition into the trie must use an edge with a lexical item." ></td>
	<td class="line x" title="162:239	Finally, ALNF improves the frequency that, when a lexical rule matches a span, we have successfully built every edge Sklscript[X] in the mapping for that rule." ></td>
	<td class="line x" title="163:239	This frequency increases from 45% to 96% with ALNF." ></td>
	<td class="line x" title="164:239	5 Coarse-to-Fine Search We now consider two coarse-to-fine approximate search procedures for parsing with these grammars." ></td>
	<td class="line x" title="165:239	Our first approach clusters grammar symbols together during the coarse parsing pass, following work in analytic parsing (Charniak and Caraballo, 1998; Petrov and Klein, 2007)." ></td>
	<td class="line x" title="166:239	We collapse all intermediate non-terminal grammar symbols (e.g., NP) to a single coarse symbol X, while pre-terminal symbols (e.g., NN) are hand-clustered into 7 classes (nouns, verbals, adjectives, punctuation, etc.)." ></td>
	<td class="line x" title="167:239	We then project the rules of the original grammar into this simplified symbol set, weighting each rule of the coarse grammar by the maximum weight of any rule that mapped onto it." ></td>
	<td class="line x" title="168:239	In our second and more successful approach, we select a subset of grammar symbols." ></td>
	<td class="line x" title="169:239	We then include only and all rules that can be built using those symbols." ></td>
	<td class="line x" title="170:239	Because the grammar includes many rules that are compositions of smaller rules, parsing with a subset of the grammar still provides meaningful scores that can be used to prune base grammar symbols while parsing under the full grammar." ></td>
	<td class="line x" title="171:239	5.1 Symbol Selection To compress the grammar, we select a small subset of symbols that allow us to retain as much of the original grammar as possible." ></td>
	<td class="line x" title="172:239	We use a voting scheme to select the symbol subset." ></td>
	<td class="line x" title="173:239	After conversion to LNF (or ALNF), each lexical rule in the original grammar votes for the symbols that are required to build it." ></td>
	<td class="line x" title="174:239	A rule votes as many times as it was observed in the training data to promote frequent rules." ></td>
	<td class="line x" title="175:239	We then select the top nl symbols by vote count and include them in the coarse grammar C. We would also like to retain as many non-lexical rules from the original grammar as possible, but the right-handsideofeachrulecanbebinarizedinmany ways." ></td>
	<td class="line x" title="176:239	We again use voting, but this time each non232 Pruning Minutes Model score BLEU No pruning 104 60,179 44.84 Clustering 79 60,179 44.84 Subsets 50 60,163 44.82 Table 2: Coarse-to-fine pruning speeds up parsing time with minimal effect on either model score or translation quality." ></td>
	<td class="line x" title="177:239	The coarse grammar built using symbol subsets outperforms clustering grammar symbols, reducing parsing time by 52%." ></td>
	<td class="line x" title="178:239	These experiments do not include a language model." ></td>
	<td class="line x" title="179:239	lexical rule votes for its yield, a sequence of symbols." ></td>
	<td class="line x" title="180:239	We select the top nu symbol sequences as the set R of right-hand sides." ></td>
	<td class="line x" title="181:239	Finally, we augment the symbol set of C with intermediate symbols that can construct all sequences in R, using only binary rules." ></td>
	<td class="line x" title="182:239	This step again requires choosing a binarization for each sequence, such that a minimal number of additional symbols is introduced." ></td>
	<td class="line x" title="183:239	We use the greedy approach from Section 3.2." ></td>
	<td class="line x" title="184:239	We then include in C all rules from the original grammar that can be built from the symbols we have chosen." ></td>
	<td class="line x" title="185:239	Surprisingly, we are able to retain 76% of the grammar rules while excluding 92% of the grammar symbols2, which speeds up parsing substantially." ></td>
	<td class="line x" title="186:239	5.2 Max Marginal Thresholding We parse first with the coarse grammar to find the Viterbi derivation score for each edge Sij[X]." ></td>
	<td class="line x" title="187:239	We then perform a Viterbi outside pass over the chart, like a standard outside pass but replacing summationtext with max (Goodman, 1999)." ></td>
	<td class="line x" title="188:239	The product of an edges Viterbi score and its Viterbi outside score gives a max marginal, the score of the maximal parse that uses the edge." ></td>
	<td class="line x" title="189:239	We then prune away regions of the chart that deviate in their coarse max marginal from the global Viterbi score by a fixed margin tuned on a development set." ></td>
	<td class="line x" title="190:239	Table 2 shows that both methods of constructing a coarse grammar are effective in pruning, but selecting symbol subsets outperformed the more typical clustering approach, reducing parsing time by an additional factor of 2." ></td>
	<td class="line x" title="191:239	2We used nl of 500 and nu of 4000 for experiments." ></td>
	<td class="line x" title="192:239	These parameters were tuned on a development set." ></td>
	<td class="line x" title="193:239	6 Language Model Integration Large n-gram language models (LMs) are critical to the performance of machine translation systems." ></td>
	<td class="line x" title="194:239	Recent innovations have managed the complexity of LM integration using multi-pass architectures." ></td>
	<td class="line x" title="195:239	Zhang and Gildea (2008) describes a coarse-to-fine approach that iteratively increases the order of the LM." ></td>
	<td class="line x" title="196:239	Petrov et al.(2008) describes an additional coarse-to-fine hierarchy over language projections." ></td>
	<td class="line x" title="198:239	Both of these approaches integrate LMs via bottomup dynamic programs that employ beam search." ></td>
	<td class="line x" title="199:239	As an alternative, Huang and Chiang (2007) describes a forest-based reranking algorithm called cube growing, which also employs beam search, but focuses computation only where necessary in a top-down pass through a parse forest." ></td>
	<td class="line x" title="200:239	In this section, we show that the coarse-to-fine idea of constraining each pass using marginal predictions of the previous pass also applies effectively to cube growing." ></td>
	<td class="line x" title="201:239	Max marginal predictions from the parse can substantially reduce LM integration time." ></td>
	<td class="line x" title="202:239	6.1 Language Model Forest Reranking Parsing produces a forest of derivations, where each edge in the forest holds its Viterbi (or one-best) derivation under the transducer grammar." ></td>
	<td class="line x" title="203:239	In forest reranking via cube growing, edges in the forest produce k-best lists of derivations that are scored by both the grammar and an n-gram language model." ></td>
	<td class="line x" title="204:239	Using ALNF, each edge must first generate a k-best listofderivationsthatare not scoredbythelanguage model." ></td>
	<td class="line x" title="205:239	These derivations are then flattened to remove the binarization introduced by ALNF, so that the resulting derivations are each rooted by an nary rule r from the original grammar." ></td>
	<td class="line x" title="206:239	The leaves of r correspond to sub-edges in the chart, which are recursively queried for their best language-modelscored derivations." ></td>
	<td class="line x" title="207:239	These sub-derivations are combined by r, and new n-grams at the edges of these derivations are scored by the language model." ></td>
	<td class="line x" title="208:239	The language-model-scored derivations for the edge are placed on a priority queue." ></td>
	<td class="line x" title="209:239	The top of the priority queue is repeatedly removed, and its successors added back on to the queue, until k language-model-scored derivations have been discovered." ></td>
	<td class="line x" title="210:239	These k derivations are then sorted and 233 Pruning Max TM LM Total Inside Outside LM Total strategy beam BLEU score score score time time time time No pruning 20 57.67 58,570 -17,202 41,368 99 0 247 346 CTF parsing 200 58.43 58,495 -16,929 41,556 53 0 186 239 CTF reranking 200 58.63 58,582 -16,998 41,584 98 64 79 241 CTF parse + rerank 2000 58.90 58,602 -16,980 41,622 53 52 148 253 Table 3: Time in minutes and performance for 300 sentences." ></td>
	<td class="line x" title="211:239	We used a trigram language model trained on 220 millionwordsofEnglishtext." ></td>
	<td class="line x" title="212:239	The no pruningbaselineusedafixbeamsizeforforest-basedlanguagemodelreranking." ></td>
	<td class="line x" title="213:239	Coarse-to-fine parsing included a coarse pruning pass using a symbol subset grammar." ></td>
	<td class="line x" title="214:239	Coarse-to-fine reranking used max marginals to constrain the reranking pass." ></td>
	<td class="line x" title="215:239	Coarse-to-fine parse + rerank employed both of these approximations." ></td>
	<td class="line x" title="216:239	supplied to parent edges upon request.3 6.2 Coarse-to-Fine Parsing Even with this efficient reranking algorithm, integrating a language model substantially increased decoding time and memory use." ></td>
	<td class="line x" title="217:239	As a baseline, we reranked using a small fixed-size beam of 20 derivationsateachedge." ></td>
	<td class="line x" title="218:239	Largerbeamsexceededthememory of our hardware." ></td>
	<td class="line x" title="219:239	Results appear in Table 3." ></td>
	<td class="line x" title="220:239	Coarse-to-fineparsingbeforeLMintegrationsubstantially improved language model reranking time." ></td>
	<td class="line x" title="221:239	By pruning the chart with max marginals from the coarse symbol subset grammar from Section 5, we were able to rerank with beams of length 200, leading to a 0.8 BLEU increase and a 31% reduction in total decoding time." ></td>
	<td class="line x" title="222:239	6.3 Coarse-to-Fine Forest Reranking We realized similar performance and speed benefits by instead pruning with max marginals from the full grammar." ></td>
	<td class="line x" title="223:239	We found that LM reranking explored many edges with low max marginals, but used few of them in the final decoder output." ></td>
	<td class="line x" title="224:239	Following the coarse-to-fine paradigm, we restricted the reranker to edges with a max marginal above a fixed threshold." ></td>
	<td class="line x" title="225:239	Furthermore, we varied the beam size of each edge based on the parse." ></td>
	<td class="line x" title="226:239	Let m be the ratio of the max marginal for edge m to the global Viterbi derivation for the sentence." ></td>
	<td class="line x" title="227:239	We used a beam of sizeceilingleftbig k2lnmceilingrightbigfor each edge." ></td>
	<td class="line x" title="228:239	Computing max marginals under the full grammar required an additional outside pass over the full parse forest, adding substantially to parsing time." ></td>
	<td class="line x" title="229:239	3Huang and Chiang (2007) describes the cube growing algorithm in further detail, including the precise form of the successor function for derivations." ></td>
	<td class="line x" title="230:239	However, soft coarse-to-fine pruning based on these max marginals also allowed for beams up to length 200, yielding a 1.0 BLEU increase over the baseline and a 30% reduction in total decoding time." ></td>
	<td class="line x" title="231:239	We also combined the coarse-to-fine parsing approach with this soft coarse-to-fine reranker." ></td>
	<td class="line x" title="232:239	Tiling these approximate search methods allowed another 10-fold increase in beam size, further improving BLEU while only slightly increasing decoding time." ></td>
	<td class="line x" title="233:239	7 Conclusion As translation grammars increase in complexity whileinnovationsdrivedownthecomputationalcost of language model integration, the efficiency of the parsing phase of machine translation decoding is becoming increasingly important." ></td>
	<td class="line x" title="234:239	Our grammar normal form, CKY improvements, and symbol subset coarse-to-fine procedure reduced parsing time for large transducer grammars by 81%." ></td>
	<td class="line x" title="235:239	These techniques also improved forest-based language model reranking." ></td>
	<td class="line x" title="236:239	A full decoding pass without any of our innovations required 511 minutes using only small beams." ></td>
	<td class="line x" title="237:239	Coarse-to-fine pruning in both the parsing and language model passes allowed a 100-fold increase in beam size, giving a performance improvement of 1.3 BLEU while decreasing total decoding time by 50%." ></td>
	<td class="line x" title="238:239	Acknowledgements This work was enabled by the Information Sciences Institute Natural Language Group, primarily through the invaluable assistance of Jens Voeckler, and was supported by the National Science Foundation (NSF) under grant IIS-0643742." ></td>
	<td class="line x" title="239:239	234" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-1058
Streaming for large scale NLP: Language Modeling
Goyal, Amit;Daume III, Hal;Venkatasubramanian, Suresh;"></td>
	<td class="line x" title="1:229	Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 512520, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:229	c 2009 Association for Computational Linguistics Streaming for large scale NLP: Language Modeling Amit Goyal, Hal Daume III, and Suresh Venkatasubramanian University of Utah, School of Computing {amitg,hal,suresh}@cs.utah.edu Abstract In this paper, we explore a streaming algorithm paradigm to handle large amounts of data for NLP problems." ></td>
	<td class="line x" title="3:229	We present an efficient low-memory method for constructing high-order approximate n-gram frequency counts." ></td>
	<td class="line x" title="4:229	The method is based on a deterministic streaming algorithm which efficiently computes approximate frequency counts over a stream of data while employing a small memory footprint." ></td>
	<td class="line x" title="5:229	We show that this method easily scales to billion-word monolingual corpora using a conventional (8 GB RAM) desktop machine." ></td>
	<td class="line x" title="6:229	Statistical machine translation experimental results corroborate that the resulting high-n approximate small language model is as effective as models obtained from other count pruning methods." ></td>
	<td class="line x" title="7:229	1 Introduction In many NLP problems, we are faced with the challenge of dealing with large amounts of data." ></td>
	<td class="line x" title="8:229	Many problems boil down to computing relative frequencies of certain items on this data." ></td>
	<td class="line x" title="9:229	Items can be words, patterns, associations, n-grams, and others." ></td>
	<td class="line oc" title="10:229	Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al., 2005), constructing syntactic rules for SMT (Galley et al., 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies." ></td>
	<td class="line x" title="11:229	We use language modeling as a canonical example of a large-scale task that requires relative frequency estimation." ></td>
	<td class="line x" title="12:229	Computing relative frequencies seems like an easy problem." ></td>
	<td class="line x" title="13:229	However, as corpus sizes grow, it becomes a highly computational expensive task." ></td>
	<td class="line x" title="14:229	Cutoff Size BLEU NIST MET Exact 367.6m 28.73 7.691 56.32 2 229.8m 28.23 7.613 56.03 3 143.6m 28.17 7.571 56.53 5 59.4m 28.33 7.636 56.03 10 18.3m 27.91 7.546 55.64 100 1.1m 28.03 7.607 55.91 200 0.5m 27.62 7.550 55.67 Table 1: Effect of count-based pruning on SMT performance using EAN corpus." ></td>
	<td class="line x" title="15:229	Results are according to BLEU, NIST and METEOR (MET) metrics." ></td>
	<td class="line x" title="16:229	Bold #s are not statistically significant worse than exact model." ></td>
	<td class="line x" title="17:229	Brants et al.(2007) used 1500 machines for a day to compute the relative frequencies of n-grams (summed over all orders from 1 to 5) from 1.8TB of web data." ></td>
	<td class="line x" title="19:229	Their resulting model contained 300 million unique n-grams." ></td>
	<td class="line x" title="20:229	It is not realistic using conventional computing resources to use all the 300 million n-grams for applications like speech recognition, spelling correction, information extraction, and statistical machine translation (SMT)." ></td>
	<td class="line x" title="21:229	Hence, one of the easiest way to reduce the size of this model is to use count-based pruning which discards all n-grams whose count is less than a pre-defined threshold." ></td>
	<td class="line x" title="22:229	Although countbased pruning is quite simple, yet it is effective for machine translation." ></td>
	<td class="line x" title="23:229	As we do not have a copy of the web, we will use a portion of gigaword i.e. EAN (see Section 4.1) to show the effect of count-based pruning on performance of SMT (see Section 5.1)." ></td>
	<td class="line x" title="24:229	Table 1 shows that using a cutoff of 100 produces a model of size 1.1 million n-grams with a Bleu score of 28.03." ></td>
	<td class="line x" title="25:229	If we compare this with an exact model of size 367.6 million n-grams, we see an increase of 0.8 points in Bleu (95% statistical significance level 512  Size BLEU NIST MET Exact 367.6m 28.73 7.691 56.32 1e-10 218.4m 28.64 7.669 56.33 5e-10 171.0m 28.48 7.666 56.38 1e-9 148.0m 28.56 7.646 56.51 5e-9 91.9m 28.27 7.623 56.16 1e-8 69.4m 28.15 7.609 56.19 5e-7 28.5m 28.08 7.595 55.91 Table 2: Effect of entropy-based pruning on SMT performance using EAN corpus." ></td>
	<td class="line x" title="26:229	Results are as in Table 1 is  0.53 Bleu)." ></td>
	<td class="line x" title="27:229	However, we need 300 times bigger model to get such an increase." ></td>
	<td class="line x" title="28:229	Unfortunately, it is not possible to integrate such a big model inside a decoder using normal computation resources." ></td>
	<td class="line x" title="29:229	A better way of reducing the size of n-grams is to use entropy pruning (Stolcke, 1998)." ></td>
	<td class="line x" title="30:229	Table 2 shows the results with entropy pruning with different settings of ." ></td>
	<td class="line x" title="31:229	We see that for three settings of  equal to 1e-10, 5e-10 and 1e-9, we get Bleu scores comparable to the exact model." ></td>
	<td class="line x" title="32:229	However, the size of all these models is not at all small." ></td>
	<td class="line x" title="33:229	The size of smallest model is 25% of the exact model." ></td>
	<td class="line x" title="34:229	Even with this size it is still not feasible to integrate such a big model inside a decoder." ></td>
	<td class="line x" title="35:229	If we take a model of size comparable to count cutoff of 100, i.e., with  = 5e-7, we see both count-based pruning as well as entropy pruning performs the same." ></td>
	<td class="line x" title="36:229	There also have been prior work on maintaining approximate counts for higher-order language models (LMs) ((Talbot and Osborne, 2007a; Talbot and Osborne, 2007b; Talbot and Brants, 2008)) operates under the model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately." ></td>
	<td class="line x" title="37:229	There are two difficulties with scaling all the above approaches as the order of the LM increases." ></td>
	<td class="line x" title="38:229	Firstly, the computation time to build the database of counts increases rapidly." ></td>
	<td class="line x" title="39:229	Secondly, the initial disk storage required to maintain these counts, prior to building the compressed representation is enormous." ></td>
	<td class="line x" title="40:229	The method we propose solves both of these problems." ></td>
	<td class="line x" title="41:229	We do this by making use of the streaming algorithm paradigm (Muthukrishnan, 2005)." ></td>
	<td class="line x" title="42:229	Working under the assumption that multiple-GB models are infeasible, our goal is to instead of estimating a large model and then compressing it, we directly estimate a small model." ></td>
	<td class="line x" title="43:229	We use a deterministic streaming algorithm (Manku and Motwani, 2002) that computes approximate frequency counts of frequently occurring n-grams." ></td>
	<td class="line x" title="44:229	This scheme is considerably more accurate in getting the actual counts as compared to other schemes (Demaine et al., 2002; Karp et al., 2003) that find the set of frequent items without caring about the accuracy of counts." ></td>
	<td class="line x" title="45:229	We use these counts directly as features in an SMT system, and propose a direct way to integrate these features into an SMT decoder." ></td>
	<td class="line x" title="46:229	Experiments show that directly storing approximate counts of frequent 5-grams compared to using count or entropybased pruning counts gives equivalent SMT performance, while dramatically reducing the memory usage and getting rid of pre-computing a large model." ></td>
	<td class="line x" title="47:229	2 Background 2.1 n-gram Language Models Language modeling is based on assigning probabilities to sentences." ></td>
	<td class="line x" title="48:229	It can either compute the probability of an entire sentence or predict the probability of the next word in a sequence." ></td>
	<td class="line x" title="49:229	Let wm1 denote a sequence of words (w1,,wm)." ></td>
	<td class="line x" title="50:229	The probability of estimating word wm depends on previous n-1 words where n denotes the size of n-gram." ></td>
	<td class="line x" title="51:229	This assumption that probability of predicting a current word depends on the previous words is called a Markov assumption, typically estimated by relative frequency: P(wm | wm1mn+1) = C(w m1 mn+1wm) C(wm1mn+1) (1) Eq 1 estimates the n-gram probability by taking the ratio of observed frequency of a particular sequence and the observed frequency of the prefix." ></td>
	<td class="line x" title="52:229	This is precisely the relative frequency estimate we seek." ></td>
	<td class="line x" title="53:229	2.2 Large-scale Language modeling Using higher order LMs to improve the accuracy of SMT is not new." ></td>
	<td class="line x" title="54:229	(Brants et al., 2007; Emami et al., 2007) built 5-gram LMs over web using distributed cluster of machines and queried them via network requests." ></td>
	<td class="line x" title="55:229	Since the use of cluster of machines is not always practical, (Talbot and Osborne, 2007b; Talbot and Osborne, 2007a) showed a randomized data structure called Bloom filter, that can be used to construct space efficient language models 513 for SMT." ></td>
	<td class="line x" title="56:229	(Talbot and Brants, 2008) presented randomized language model based on perfect hashing combined with entropy pruning to achieve further memory reductions." ></td>
	<td class="line x" title="57:229	A problem mentioned in (Talbot and Brants, 2008) is that the algorithm that computes the compressed representation might need to retain the entire database in memory; in their paper, they design strategies to work around this problem." ></td>
	<td class="line x" title="58:229	(Federico and Bertoldi, 2006) also used single machine and fewer bits to store the LM probability by using efficient prefix trees." ></td>
	<td class="line x" title="59:229	(Uszkoreit and Brants, 2008) used partially classbased LMs together with word-based LMs to improve SMT performance despite the large size of the word-based models used." ></td>
	<td class="line x" title="60:229	(Schwenk and Koehn, 2008; Zhang et al., 2006) used higher language models at time of re-ranking rather than integrating directly into the decoder to avoid the overhead of keeping LMs in the main memory since disk lookups are simply too slow." ></td>
	<td class="line x" title="61:229	Now using higher order LMs at time of re-ranking looks like a good option." ></td>
	<td class="line x" title="62:229	However, the target n-best hypothesis list is not diverse enough." ></td>
	<td class="line x" title="63:229	Hence if possible it is always better to integrate LMs directly into the decoder." ></td>
	<td class="line x" title="64:229	2.3 Streaming Consider an algorithm that reads the input from a read-only stream from left to right, with no ability to go back to the input that it has already processed." ></td>
	<td class="line x" title="65:229	This algorithm has working storage that it can use to store parts of the input or other intermediate computations." ></td>
	<td class="line x" title="66:229	However, (and this is a critical constraint), this working storage space is significantly smaller than the input stream length." ></td>
	<td class="line x" title="67:229	For typical algorithms, the storage size is of the order of logk N, where N is the input size and k is some constant." ></td>
	<td class="line x" title="68:229	Stream algorithms were first developed in the early 80s, but gained in popularity in the late 90s as researchers first realized the challenges of dealing with massive data sets." ></td>
	<td class="line x" title="69:229	A good survey of the model and core challenges can be found in (Muthukrishnan, 2005)." ></td>
	<td class="line x" title="70:229	There has been considerable work on the problem of identifying high-frequency items (items with frequency above a threshold), and a detailed review of these methods is beyond the scope of this article." ></td>
	<td class="line x" title="71:229	A new survey by (Cormode and Hadjieleftheriou, 2008) comprehensively reviews the literature." ></td>
	<td class="line x" title="72:229	3 Space-Efficient Approximate Frequency Estimation Prior work on approximate frequency estimation for language models provide a no-false-negative guarantee, ensuring that counts for n-grams in the model are returned exactly, while working to make sure the false-positive rate remains small (Talbot and Osborne, 2007a)." ></td>
	<td class="line x" title="73:229	The notion of approximation we use is different: in our approach, it is the actual count values that will be approximated." ></td>
	<td class="line x" title="74:229	We also exploit the fact that low-frequency n-grams, while constituting the vast majority of the set of unique n-grams, are usually smoothed away and are less likely to influence the language model significantly." ></td>
	<td class="line x" title="75:229	Discarding low-frequency n-grams is particularly important in a stream setting, because it can be shown in general that any algorithm that generates approximate frequency counts for all n-grams requires space linear in the input stream (Alon et al., 1999)." ></td>
	<td class="line x" title="76:229	We employ an algorithm for approximate frequency counting proposed by (Manku and Motwani, 2002) in the context of database management." ></td>
	<td class="line x" title="77:229	Fix parameters s  (0,1), and   (0,1),  s. Our goal is to approximately find all n-grams with frequency at least sN." ></td>
	<td class="line x" title="78:229	For an input stream of n-grams of length N, the algorithm outputs a set of items (and frequencies) and guarantees the following:  All items with frequencies exceeding sN are output (no false negatives)." ></td>
	<td class="line x" title="79:229	 No item with frequency less than (s  )N is output (few false positives)." ></td>
	<td class="line x" title="80:229	 All reported frequencies are less than the true frequencies by at most N (close-to-exact frequencies)." ></td>
	<td class="line x" title="81:229	 The space used by the algorithm is O(1 logN)." ></td>
	<td class="line x" title="82:229	A simple example illustrates these properties." ></td>
	<td class="line x" title="83:229	Let us fix s = 0.01, = 0.001." ></td>
	<td class="line x" title="84:229	Then the algorithm guarantees that all n-grams with frequency at least 1% will be returned, no element with frequency less than 0.9% will be returned, and all frequencies will be no more than 0.1% away from the true frequencies." ></td>
	<td class="line x" title="85:229	The space used by the algorithm is O(logN), which can be compared to the much larger (close to N) space 514 needed to store the initial frequency counts." ></td>
	<td class="line x" title="86:229	In addition, the algorithm runs in linear time by definition, requiring only one pass over the input." ></td>
	<td class="line x" title="87:229	Note that there might be 1 elements with frequency at least N, and so the algorithm uses optimal space (up to a logarithmic factor)." ></td>
	<td class="line x" title="88:229	3.1 The Algorithm We present a high-level overview of the algorithm; for more details, the reader is referred to (Manku and Motwani, 2002)." ></td>
	<td class="line x" title="89:229	The algorithm proceeds by conceptually dividing the stream into epochs, each containing 1/ elements." ></td>
	<td class="line x" title="90:229	Note that there are N epochs." ></td>
	<td class="line x" title="91:229	Each such epoch has an ID, starting from 1." ></td>
	<td class="line x" title="92:229	The algorithm maintains a list of tuples1 of the form (e,f,), where e is an n-gram, f is its reported frequency, and  is the maximum error in the frequency estimation." ></td>
	<td class="line x" title="93:229	While the algorithm reads ngrams associated with the current epoch, it does one of two things: if the new element e is contained in the list of tuples, it merely increments the frequency count f. If not, it creates a new tuple of the form (e,1,T 1), where T is the ID of the current epoch." ></td>
	<td class="line x" title="94:229	After each epoch, the algorithm cleans house by eliminating tuples whose maximum true frequency is small." ></td>
	<td class="line x" title="95:229	Formally, if the epoch that just ended has ID T, then the algorithm deletes all tuples satisfying condition f +   T. Since T  N, this ensures that no low-frequency tuples are retained." ></td>
	<td class="line x" title="96:229	When all elements in the stream have been processed, the algorithm returns all tuples (e,f,) where f  (s)N. In practice, however we do not care about s and return all tuples." ></td>
	<td class="line x" title="97:229	At a high level, the reason the algorithm works is that if an element has high frequency, it shows up more than once each epoch, and so its frequency gets updated enough to stave off elimination." ></td>
	<td class="line x" title="98:229	4 Intrinsic Evaluation We conduct a set of experiments with approximate n-gram counts (stream counts) produced by the stream algorithm." ></td>
	<td class="line x" title="99:229	We define various metrics on which we evaluate the quality of stream counts compared with exact n-gram counts (true counts)." ></td>
	<td class="line x" title="100:229	To 1We use hash tables to store tuples; however smarter data structures like suffix trees could also be used." ></td>
	<td class="line x" title="101:229	Corpus Gzip-MB M-wrds Perplexity EP 63 38 1122.69 afe 417 171 1829.57 apw 1213 540 1872.96 nyt 2104 914 1785.84 xie 320 132 1885.33 Table 3: Corpus Statistics and perplexity of LMs made with each of these corpuses on development set evaluate the quality of stream counts on these metrics, we carry out three experiments." ></td>
	<td class="line x" title="102:229	4.1 Experimental Setup The freely available English side of Europarl (EP) and Gigaword corpus (Graff, 2003) is used for computing n-gram counts." ></td>
	<td class="line x" title="103:229	We only use EP along with two sections of the Gigaword corpus: Agence France Press English Service(afe) and The New York Times Newswire Service (nyt)." ></td>
	<td class="line x" title="104:229	The unigram language models built using these corpuses yield better perplexity scores on the development set (see Section 5.1) compared to The Xinhua News Agency English Service (xie) and Associated Press Worldstream English Service (apw) as shown in Table 3." ></td>
	<td class="line x" title="105:229	The LMs are build using the SRILM language modelling toolkit (Stolcke, 2002) with modified KneserNey discounting and interpolation." ></td>
	<td class="line x" title="106:229	The evaluation of stream counts is done on EP+afe+nyt (EAN) corpus, consisting of 1.1 billion words." ></td>
	<td class="line x" title="107:229	4.2 Description of the metrics To evaluate the quality of counts produced by our stream algorithm four different metrics are used." ></td>
	<td class="line x" title="108:229	The accuracy metric measures the quality of top N stream counts by taking the fraction of top N stream counts that are contained in the top N true counts." ></td>
	<td class="line x" title="109:229	Accuracy = Stream Counts  True CountsTrue Counts Spearmans rank correlation coefficient or Spearmans rho() computes the difference between the ranks of each observation (i.e. n-gram) on two variables (that are top N stream and true counts)." ></td>
	<td class="line x" title="110:229	This measure captures how different the stream count ordering is from the true count ordering." ></td>
	<td class="line x" title="111:229	 = 1 6 summationtextd2 i N(N2 1) 515 di is the difference between the ranks of corresponding elements Xi and Yi; N is the number of elements found in both sets; Xi and Yi in our case denote the stream and true counts." ></td>
	<td class="line x" title="112:229	Mean square error (MSE) quantifies the amount by which a predicted value differs from the true value." ></td>
	<td class="line x" title="113:229	In our case, it estimates how different the stream counts are from the true counts." ></td>
	<td class="line x" title="114:229	MSE = 1N Nsummationdisplay i=1 (truei  predictedi)2 true and predicted denotes values of true and stream counts; N denotes the number of stream counts contained in true counts." ></td>
	<td class="line x" title="115:229	4.3 Varying  experiments In our first experiment, we use accuracy,  and MSE metrics for evaluation." ></td>
	<td class="line x" title="116:229	Here, we compute 5-gram stream counts with different settings of  on the EAN corpus." ></td>
	<td class="line x" title="117:229	 controls the number of stream counts produced by the algorithm." ></td>
	<td class="line x" title="118:229	The results in Table 4 support the theory that decreasing the value of  improves the quality of stream counts." ></td>
	<td class="line x" title="119:229	Also, as expected, the algorithm produces more stream counts with smaller values of ." ></td>
	<td class="line x" title="120:229	The evaluation of stream counts obtained with  = 50e-8 and 20e-8 reveal that the stream counts learned with this large value are more susceptible to errors." ></td>
	<td class="line x" title="121:229	If we look closely at the counts for  = 50e-8, we see that we get at least 30% of the stream counts from 245k true counts." ></td>
	<td class="line x" title="122:229	This number is not significantly worse than the 36% of stream counts obtained from 4,018k true counts for the smallest value of  = 5e-8." ></td>
	<td class="line x" title="123:229	However, if we look at the other two metrics, the ranking correlation  of stream counts compared with true counts on  = 50e-8 and 20e-8 is low compared to other  values." ></td>
	<td class="line x" title="124:229	For the MSE, the error with stream counts on these m values is again high compared to other values." ></td>
	<td class="line x" title="125:229	As we decrease the value of  we continually get better results: decreasing  pushes the stream counts towards the true counts." ></td>
	<td class="line x" title="126:229	However, using a smaller  increases the memory usage." ></td>
	<td class="line x" title="127:229	Looking at the evaluation, it is therefore advisable to use 5-gram stream counts produced with at most  10e-7 for the EAN corpus." ></td>
	<td class="line x" title="128:229	Since it is not possible to compute true 7-grams counts on EAN with available computing resources,  5-gram Acc  MSEproduced 50e-8 245k 0.294 -3.6097 0.4954 20e-8 726k 0.326 -2.6517 0.1155 10e-8 1655k 0.352 -1.9960 0.0368 5e-8 4018k 0.359 -1.7835 0.0114 Table 4: Evaluating quality of 5-gram stream counts for different settings of  on EAN corpus  7-gram Acc  MSEproduced 50e-8 44k 0.509 0.3230 0.0341 20e-8 128k 0.596 0.5459 0.0063 10e-8 246k 0.689 0.7413 0.0018 5e-8 567k 0.810 0.8599 0.0004 Table 5: Evaluating quality of 7-gram stream counts for different settings of  on EP corpus we carry out a similar experiment for 7-grams on EP to verify the results for higher order n-grams 2." ></td>
	<td class="line x" title="129:229	The results in Table 5 tell a story similar to our results for 7-grams." ></td>
	<td class="line x" title="130:229	The size of EP corpus is much smaller than EAN and so we see even better results on each of the metrics with decreasing the value of ." ></td>
	<td class="line x" title="131:229	The overall trend remains the same; here too, setting   10e8 is the most effective strategy." ></td>
	<td class="line x" title="132:229	The fact that these results are consistent across two datasets of different sizes and different n-gram sizes suggests that they will carry over to other tasks." ></td>
	<td class="line x" title="133:229	4.4 Varying top K experiments In the second experiment, we evaluate the quality of the top K (sorted by frequency) 5-gram stream counts." ></td>
	<td class="line x" title="134:229	Here again, we use accuracy,  and MSE for evaluation." ></td>
	<td class="line x" title="135:229	We fix the value of  to 5e-8 and compute 5-gram stream counts on the EAN corpus." ></td>
	<td class="line x" title="136:229	We vary the value of K between 100k and 4,018k (i.e all the n-gram counts produced by the stream algorithm)." ></td>
	<td class="line x" title="137:229	The experimental results in Table 6 support the theory that stream count algorithm computes the exact count of most of the high frequency n-grams." ></td>
	<td class="line x" title="138:229	Looking closer, we see that if we evaluate the algorithm on just the top 100k 5-grams (roughly 5% of all 5-grams produced), we see almost perfect results." ></td>
	<td class="line x" title="139:229	Further, if we take the top 1,000k 5-grams (approximately 25% of all 5-grams) we again see excellent 2Similar evaluation scores are observed for 9-gram stream counts with different values of  on EP corpus." ></td>
	<td class="line x" title="140:229	516 Top K Accuracy  MSE 100k 0.994 0.9994 0.01266 500k 0.934 0.9795 0.0105 1000k 0.723 0.8847 0.0143 2000k 0.504 0.2868 0.0137 4018k 0.359 -1.7835 0.0114 Table 6: Evaluating top K sorted 5-gram stream counts for =5e-8 on EAN corpus performance on all metrics." ></td>
	<td class="line x" title="141:229	The accuracy of the results decrease slightly, but the  and MSE metrics are not decreased that much in comparison." ></td>
	<td class="line x" title="142:229	Performance starts to degrade as we get to 2,000k (over 50% of all 5-grams), a result that is not too surprising." ></td>
	<td class="line x" title="143:229	However, even here we note that the MSE is low, suggesting that the frequencies of stream counts (found in top K true counts) are very close to the true counts." ></td>
	<td class="line x" title="144:229	Thus, we conclude that the quality of the 5-gram stream counts produced for this value of  is quite high (in relation to the true counts)." ></td>
	<td class="line x" title="145:229	As before, we corroborate our results with higher order n-grams." ></td>
	<td class="line x" title="146:229	We evaluate the quality of top K 7gram stream counts on EP.3 Since EP is a smaller corpus, we evaluate the stream counts produced by setting  to 10e-8." ></td>
	<td class="line x" title="147:229	Here we vary the value of K between 10k and 246k (the total number produced by the stream algorithm)." ></td>
	<td class="line x" title="148:229	Results are shown in Table 7." ></td>
	<td class="line x" title="149:229	As we saw earlier with 5-grams, the top 10k (i.e. approximately 5% of all 7-grams) are of very high quality." ></td>
	<td class="line x" title="150:229	Results, and this remains true even when we increase K to 100k." ></td>
	<td class="line x" title="151:229	There is a drop in the accuracy and a slight drop in , while the MSE remains the same." ></td>
	<td class="line x" title="152:229	Taking all counts again shows a significant decrease in both accuracy and  scores, but this does not affect MSE scores significantly." ></td>
	<td class="line x" title="153:229	Hence, the 7-gram stream counts i.e. 246k counts produced by  = 10e-8 are quite accurate when compared to the top 246k true counts." ></td>
	<td class="line x" title="154:229	4.5 Analysis of tradeoff between coverage and space In our third experiment, we investigate whether a large LM can help MT performance." ></td>
	<td class="line x" title="155:229	We evaluate the coverage of stream counts built on the EAN corpus on the test data for SMT experiments (see Sec3Similar evaluation scores are observed for different top K sorted 9-gram stream counts with =10e-8 on EP corpus." ></td>
	<td class="line x" title="156:229	Top K Accuracy  MSE 10k 0.996 0.9997 0.0015 20k 0.989 0.9986 0.0016 50k 0.950 0.9876 0.0016 100k 0.876 0.9493 0.0017 246k 0.689 0.7413 0.0018 Table 7: Evaluating top K sorted 7-gram stream counts for =10e-8 on EP corpus tion 5.1) with different values of m. We compute the recall of each model against 3071 sentences of test data where recall is the fraction of number of n-grams of a dataset found in stream counts." ></td>
	<td class="line x" title="157:229	Recall = Number of n-grams found in stream countsNumber of n-grams in dataset We build unigram, bigram, trigram, 5-gram and 7-gram with four different values of ." ></td>
	<td class="line x" title="158:229	Table 8 contains the gzip size of the count file and the recall of various different stream count n-grams." ></td>
	<td class="line x" title="159:229	As expected, the recall with respect to true counts is maximum for unigrams, bigrams, trigrams and 5-grams." ></td>
	<td class="line x" title="160:229	However the amount of space required to store all true counts in comparison to stream counts is extremely high: we need 4.8GB of compressed space to store all the true counts for 5-grams." ></td>
	<td class="line x" title="161:229	For unigram models, we see that the recall scores are good for all values of ." ></td>
	<td class="line x" title="162:229	If we compare the approximate stream counts produced by largest  (which is worst) to all true counts, we see that the stream counts compressed size is 50 times smaller than the true counts size, and is only three points worse in recall." ></td>
	<td class="line x" title="163:229	Similar trends hold for bigrams, although the loss in recall is higher." ></td>
	<td class="line x" title="164:229	As with unigrams, the loss in recall is more than made up for by the memory savings (a factor of nearly 150)." ></td>
	<td class="line x" title="165:229	For trigrams, we see a 14 point loss in recall for the smallest , but a memory savings of 400 times." ></td>
	<td class="line x" title="166:229	For 5-grams, the best recall value is .020 (1.2k out of 60k 5-gram stream counts are found in the test set)." ></td>
	<td class="line x" title="167:229	However, compared with the true counts we only loss a recall of 0.05 (4.3k out of 60k) points but memory savings of 150 times." ></td>
	<td class="line x" title="168:229	In extrinsic evaluations, we will show that integrating 5-gram stream counts with an SMT system performs slightly worse than the true counts, while dramatically reducing the memory usage." ></td>
	<td class="line x" title="169:229	517 N-gram unigram bigram trigram 5-gram 7-gram  Gzip Recall Gzip Recall Gzip Recall Gzip Recall Gzip RecallMB MB MB MB MB 50e-8 .352 .785 2.3 .459 3.3 .167 1.9 .006 .864 5.6e-5 20e-8 .568 .788 4.5 .494 7.6 .207 5.3 .011 2.7 1.3e-4 10e-8 .824 .791 7.6 .518 15 .237 13 .015 9.7 4.1e-4 5e-8 1.3 .794 13 .536 30 .267 31 .020 43 5.9e-4 all 17 .816 228 .596 1200 .406 4800 .072 NA Table 8: Gzipped space required to store n-gram counts on disk and their coverage on a test set with different m For 7-gram we can not compute the true n-gram counts due to limitations of available computational resources." ></td>
	<td class="line x" title="170:229	The memory requirements with smallest value of  are similar to those of 5-gram, but the recall values are quite small." ></td>
	<td class="line x" title="171:229	For 7-grams, the best recall value is 5.9e-4 which means that stream counts contains only 32 out of 54k 7-grams contained in test set." ></td>
	<td class="line x" title="172:229	The small recall value for 7-grams suggests that these counts may not be that useful in SMT." ></td>
	<td class="line x" title="173:229	We further substantiate our findings in our extrinsic evaluations." ></td>
	<td class="line x" title="174:229	There we show that integrating 7-gram stream counts with an SMT system does not affect its overall performance significantly." ></td>
	<td class="line x" title="175:229	5 Extrinsic Evaluation 5.1 Experimental Setup All the experiments conducted here make use of publicly available resources." ></td>
	<td class="line x" title="176:229	Europarl (EP) corpus French-English section is used as parallel data." ></td>
	<td class="line x" title="177:229	The publicly available Moses4 decoder is used for training and decoding (Koehn and Hoang, 2007)." ></td>
	<td class="line x" title="178:229	The news corpus released for ACL SMT workshop in 2007 consisting of 1057 sentences5 is used as the development set." ></td>
	<td class="line x" title="179:229	Minimum error rate training (MERT) is used on this set to obtain feature weights to optimize translation quality." ></td>
	<td class="line x" title="180:229	The final SMT system performance is evaluated on a uncased test set of 3071 sentences using the BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) scores." ></td>
	<td class="line x" title="181:229	The test set is the union of the 2007 news devtest and 2007 news test data from ACL SMT workshop 2007.6 4http://www.statmt.org/moses/ 5http://www.statmt.org/wmt07/ 6We found that testing on Parliamentary test data was completely insensitive to large n-gram LMs, even when these LMs are exact." ></td>
	<td class="line x" title="182:229	This suggests that for SMT performance, more data 5.2 Integrating stream counts feature into decoder Our method only computes high-frequency n-gram counts; it does not estimate conditional probabilities." ></td>
	<td class="line x" title="183:229	We can either turn these counts into conditional probabilities (by using SRILM) or use the counts directly." ></td>
	<td class="line x" title="184:229	We observed no significant difference in performance between these two approaches." ></td>
	<td class="line x" title="185:229	However, using the counts directly consumes significantly less memory at run-time and is therefore preferable." ></td>
	<td class="line x" title="186:229	Due to space constraints, SRILM results are omitted." ></td>
	<td class="line x" title="187:229	The only remaining open question is: how should we turn the counts into a feature that can be used in an SMT system?" ></td>
	<td class="line x" title="188:229	We considered several alternatives; the most successful was a simple weighted count of n-gram matches of varying size, appropriately backed-off." ></td>
	<td class="line x" title="189:229	Specifically, consider an n-gram model." ></td>
	<td class="line x" title="190:229	For every sequence of words wi,,wi+N1, we obtain a feature score computed recursively according to Eq (2)." ></td>
	<td class="line x" title="191:229	f(wi) = log C(w i) Z  (2) f(wi,,wi+k) = log C(w i,,wi+k) Z  + 12f(wi+1,,wi+k) Here, 12 is the backoff factor and Z is the largest count in the count set (the presence of Z is simply to ensure that these values remain manageable)." ></td>
	<td class="line x" title="192:229	In order to efficiently compute these features, we store the counts in a suffix-tree." ></td>
	<td class="line x" title="193:229	The computation proceeds by first considering wi+N1 alone and then expanding to consider the bigram, then trigram and so on." ></td>
	<td class="line x" title="194:229	The advantage to this order of computation is that the recursive calls can cease whenever a is better only if it comes from the right domain." ></td>
	<td class="line x" title="195:229	518 n-gram() BLEU NIST MET MemGB 3 EP(exact) 25.57 7.300 54.48 2.7 5 EP(exact) 25.79 7.286 54.44 2.9 3 EAN(exact) 27.04 7.428 55.07 4.6 5 EAN(exact) 28.73 7.691 56.32 20.5 4(10e-8) 27.36 7.506 56.19 2.7 4(5e-8) 27.40 7.507 55.90 2.8 5(10e-8) 27.97 7.605 55.52 2.8 5(5e-8) 27.98 7.611 56.07 2.8 7(10e-8) 27.97 7.590 55.88 2.9 7(5e-8) 27.88 7.577 56.01 2.9 9(10e-8) 28.18 7.611 55.95 2.9 9(5e-8) 27.98 7.608 56.08 2.9 Table 9: Evaluating SMT with different LMs on EAN." ></td>
	<td class="line x" title="196:229	Results are according to BLEU, NIST and MET metrics." ></td>
	<td class="line x" title="197:229	Bold #s are not statistically significant worse than exact." ></td>
	<td class="line x" title="198:229	zero count is reached." ></td>
	<td class="line x" title="199:229	(Extending Moses to include this required only about 100 lines of code.)" ></td>
	<td class="line x" title="200:229	5.3 Results Table 9 summarizes SMT results." ></td>
	<td class="line x" title="201:229	We have 4 baseline LMs that are conventional LMs smoothed using modified Kneser-Ney smoothing." ></td>
	<td class="line x" title="202:229	The first two trigram and 5-gram LMs are built on EP corpus and the other two are built on EAN corpus." ></td>
	<td class="line x" title="203:229	Table 9 show that there is not much significant difference in SMT results of 5-gram and trigram LM on EP." ></td>
	<td class="line x" title="204:229	As expected, the trigram built on the large corpus EAN gets an improvement of 1.5 Bleu Score." ></td>
	<td class="line x" title="205:229	However, unlike the EP corpus, building a 5-gram LM on EAN (huge corpus) gets an improvement of 3.2 Bleu Score." ></td>
	<td class="line x" title="206:229	(The 95% statistical significance boundary is about  0.53 Bleu on the test data, 0.077 Nist and 0.16 Meteor according to bootstrap resampling) We see similar gains in Nist and Meteor metrics as shown in Table 9." ></td>
	<td class="line x" title="207:229	We use stream counts computed with two values of , 5e-8 and 10e-8 on EAN corpus." ></td>
	<td class="line x" title="208:229	We use all the stream counts produced by the algorithm." ></td>
	<td class="line x" title="209:229	4, 5, 7 and 9 order n-gram stream counts are computed with these settings of ." ></td>
	<td class="line x" title="210:229	These counts are used along with a trigram LM built on EP to improve SMT performance." ></td>
	<td class="line x" title="211:229	The memory usage (Mem) shown in Table 9 is the full memory size required to run on the test data (including phrase tables)." ></td>
	<td class="line x" title="212:229	Adding 4-gram and 5-gram stream counts as feature helps the most." ></td>
	<td class="line x" title="213:229	The performance gain by using 5-gram stream counts is slightly worse than compared to true 5-gram LM on EAN." ></td>
	<td class="line x" title="214:229	However, using 5-gram stream counts directly is more memory efficient." ></td>
	<td class="line x" title="215:229	Also, the gains for stream counts are exactly the same as we saw for same sized countbased and entropy-based pruning counts in Table 1 and 2 respectively." ></td>
	<td class="line x" title="216:229	Moreover, unlike the pruning methods, our algorithm directly computes a small model, as opposed to compressing a pre-computed large model." ></td>
	<td class="line x" title="217:229	Adding 7-gram and 9-gram does not help significantly, a fact anticipated by the low recall of 7-grambased counts that we saw in Section 4.5." ></td>
	<td class="line x" title="218:229	The results with two different settings of  are largely the same." ></td>
	<td class="line x" title="219:229	This validates our intrinsic evaluation results in Section 4.3 that stream counts learned using   10e-8 are of good quality, and that the quality of the stream counts is high." ></td>
	<td class="line x" title="220:229	6 Conclusion We have proposed an efficient, low-memory method to construct high-order approximate n-gram LMs." ></td>
	<td class="line x" title="221:229	Our method easily scales to billion-word monolingual corpora on conventional (8GB) desktop machines." ></td>
	<td class="line x" title="222:229	We have demonstrated that approximate ngram features could be used as a direct replacement for conventional higher order LMs in SMT with significant reductions in memory usage." ></td>
	<td class="line x" title="223:229	In future, we will be looking into building streaming skip ngrams, and other variants (like cluster n-grams)." ></td>
	<td class="line x" title="224:229	In NLP community, it has been shown that having more data results in better performance (Ravichandran et al., 2005; Brants et al., 2007; Turney, 2008)." ></td>
	<td class="line x" title="225:229	At web scale, we have terabytes of data and that can capture broader knowledge." ></td>
	<td class="line x" title="226:229	Streaming algorithm paradigm provides a memory and space-efficient platform to deal with terabytes of data." ></td>
	<td class="line x" title="227:229	We hope that other NLP applications (where we need to compute relative frequencies) like noun-clustering, constructing syntactic rules for SMT, finding analogies, and others can also benefit from streaming methods." ></td>
	<td class="line x" title="228:229	We also believe that stream counts can be applied to other problems involving higher order LMs such as speech recognition, information extraction, spelling correction and text generation." ></td>
	<td class="line x" title="229:229	519" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1020
Forest-based Tree Sequence to String Translation Model
Zhang, Hui;Zhang, Min;Li, Haizhou;Aw, Aiti;Tan, Chew Lim;"></td>
	<td class="line x" title="1:277	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 172180, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:277	c2009 ACL and AFNLP Forest-based Tree Sequence to String Translation Model  Hui Zhang 1, 2    Min Zhang 1    Haizhou Li 1    Aiti Aw 1    Chew Lim Tan 2  1 Institute for Infocomm Research 2 National University of Singapore zhangh1982@gmail.com   {mzhang, hli, aaiti}@i2r.a-star.edu.sg   tancl@comp.nus.edu.sg     Abstract This paper proposes a forest-based tree sequence to string translation model for syntaxbased statistical machine translation, which automatically learns tree sequence to string translation rules from word-aligned sourceside-parsed bilingual texts." ></td>
	<td class="line x" title="3:277	The proposed model leverages on the strengths of both tree sequence-based and forest-based translation models." ></td>
	<td class="line x" title="4:277	Therefore, it can not only utilize forest structure that compactly encodes exponential number of parse trees but also capture nonsyntactic translation equivalences with linguistically structured information through tree sequence." ></td>
	<td class="line x" title="5:277	This makes our model potentially more robust to parse errors and structure divergence." ></td>
	<td class="line x" title="6:277	Experimental results on the NIST MT-2003 Chinese-English translation task show that our method statistically significantly outperforms the four baseline systems." ></td>
	<td class="line x" title="7:277	1 Introduction Recently syntax-based statistical machine translation (SMT) methods have achieved very promising results and attracted more and more interests in the SMT research community." ></td>
	<td class="line x" title="8:277	Fundamentally, syntax-based SMT views translation as a structural transformation process." ></td>
	<td class="line x" title="9:277	Therefore, structure divergence and parse errors are two of the major issues that may largely compromise the performance of syntax-based SMT (Zhang et al., 2008a; Mi et al., 2008)." ></td>
	<td class="line x" title="10:277	Many solutions have been proposed to address the above two issues." ></td>
	<td class="line x" title="11:277	Among these advances, forest-based modeling (Mi et al., 2008; Mi and Huang, 2008) and tree sequence-based modeling (Liu et al., 2007; Zhang et al., 2008a) are two interesting modeling methods with promising results reported." ></td>
	<td class="line x" title="12:277	Forest-based modeling aims to improve translation accuracy through digging the potential better parses from n-bests (i.e. forest) while tree sequence-based modeling aims to model non-syntactic translations with structured syntactic knowledge." ></td>
	<td class="line x" title="13:277	In nature, the two methods would be complementary to each other since they manage to solve the negative impacts of monolingual parse errors and cross-lingual structure divergence on translation results from different viewpoints." ></td>
	<td class="line x" title="14:277	Therefore, one natural way is to combine the strengths of the two modeling methods for better performance of syntax-based SMT." ></td>
	<td class="line x" title="15:277	However, there are many challenges in combining the two methods into a single model from both theoretical and implementation engineering viewpoints." ></td>
	<td class="line x" title="16:277	In theory, one may worry about whether the advantage of tree sequence has already been covered by forest because forest encodes implicitly a huge number of parse trees and these parse trees may generate many different phrases and structure segmentations given a source sentence." ></td>
	<td class="line x" title="17:277	In system implementation, the exponential combinations of tree sequences with forest structures make the rule extraction and decoding tasks much more complicated than that of the two individual methods." ></td>
	<td class="line x" title="18:277	In this paper, we propose a forest-based tree sequence to string model, which is designed to integrate the strengths of the forest-based and the tree sequence-based modeling methods." ></td>
	<td class="line x" title="19:277	We present our solutions that are able to extract translation rules and decode translation results for our model very efficiently." ></td>
	<td class="line x" title="20:277	A general, configurable platform was designed for our model." ></td>
	<td class="line x" title="21:277	With this platform, we can easily implement our method and many previous syntax-based methods by simple parameter setting." ></td>
	<td class="line x" title="22:277	We evaluate our method on the NIST MT-2003 Chinese-English translation tasks." ></td>
	<td class="line x" title="23:277	Experimental results show that our method significantly outperforms the two individual methods and other baseline methods." ></td>
	<td class="line x" title="24:277	Our study shows that the proposed method is able to effectively combine the strengths of the forest-based and tree sequence-based methods, and thus having great potential to address the issues of parse errors and non-syntactic transla172 tions resulting from structure divergence." ></td>
	<td class="line x" title="25:277	It also indicates that tree sequence and forest play different roles and make contributions to our model in different ways." ></td>
	<td class="line x" title="26:277	The remainder of the paper is organized as follows." ></td>
	<td class="line x" title="27:277	Section 2 describes related work while section 3 defines our translation model." ></td>
	<td class="line x" title="28:277	In section 4 and section 5, the key rule extraction and decoding algorithms are elaborated." ></td>
	<td class="line x" title="29:277	Experimental results are reported in section 6 and the paper is concluded in section 7." ></td>
	<td class="line x" title="30:277	2 Related work As discussed in section 1, two of the major challenges to syntax-based SMT are structure divergence and parse errors." ></td>
	<td class="line x" title="31:277	Many techniques have been proposed to address the structure divergence issue while only fewer studies are reported in addressing the parse errors in the SMT research community." ></td>
	<td class="line x" title="32:277	To address structure divergence issue, many researchers (Eisner, 2003; Zhang et al., 2007) propose using the Synchronous Tree Substitution Grammar (STSG) grammar in syntax-based SMT since the STSG uses larger tree fragment as translation unit." ></td>
	<td class="line x" title="33:277	Although promising results have been reported, STSG only uses one single subtree as translation unit which is still committed to the syntax strictly." ></td>
	<td class="line x" title="34:277	Motivated by the fact that non-syntactic phrases make non-trivial contribution to phrase-based SMT, the tree sequencebased translation model is proposed (Liu et al., 2007; Zhang et al., 2008a) that uses tree sequence as the basic translation unit, rather than using single sub-tree as in the STSG." ></td>
	<td class="line x" title="35:277	Here, a tree sequence refers to a sequence of consecutive sub-trees that are embedded in a full parse tree." ></td>
	<td class="line x" title="36:277	For any given phrase in a sentence, there is at least one tree sequence covering it." ></td>
	<td class="line x" title="37:277	Thus the tree sequence-based model has great potential to address the structure divergence issue by using tree sequence-based non-syntactic translation rules." ></td>
	<td class="line x" title="38:277	Liu et al.(2007) propose the tree sequence concept and design a tree sequence to string translation model." ></td>
	<td class="line x" title="40:277	Zhang et al.(2008a) propose a tree sequence-based tree to tree translation model and Zhang et al.(2008b) demonstrate that the tree sequence-based modelling method can well address the structure divergence issue for syntaxbased SMT." ></td>
	<td class="line x" title="43:277	To overcome the parse errors for SMT, Mi et al.(2008) propose a forest-based translation method that uses forest instead of one best tree as translation input, where a forest is a compact representation of exponentially number of n-best parse trees." ></td>
	<td class="line x" title="45:277	Mi and Huang (2008) propose a forest-based rule extraction algorithm, which learn tree to string rules from source forest and target string." ></td>
	<td class="line x" title="46:277	By using forest in rule extraction and decoding, their methods are able to well address the parse error issue." ></td>
	<td class="line nc" title="47:277	From the above discussion, we can see that traditional tree sequence-based method uses single tree as translation input while the forestbased model uses single sub-tree as the basic translation unit that can only learn tree-to-string (Galley et al. 2004; Liu et al., 2006) rules." ></td>
	<td class="line x" title="48:277	Therefore, the two methods display different strengths, and which would be complementary to each other." ></td>
	<td class="line x" title="49:277	To integrate their strengths, in this paper, we propose a forest-based tree sequence to string translation model." ></td>
	<td class="line x" title="50:277	3 Forest-based tree sequence to string model In this section, we first explain what a packed forest is and then define the concept of the tree sequence in the context of forest followed by the discussion on our proposed model." ></td>
	<td class="line x" title="51:277	3.1 Packed Forest A packed forest (forest in short) is a special kind of hyper-graph (Klein and Manning, 2001; Huang and Chiang, 2005), which is used to represent all derivations (i.e. parse trees) for a given sentence under a context free grammar (CFG)." ></td>
	<td class="line x" title="52:277	A forest F is defined as a triple nullnull,null,nullnull , where null  is non-terminal node set, null  is hyper-edge set and null  is leaf node set (i.e. all sentence words)." ></td>
	<td class="line x" title="53:277	A forest F satisfies the following two conditions:  1) Each node null  in null  should cover a phrase, which is a continuous word sub-sequence in null . 2) Each hyper-edge null  in null  is defined as null null nullnull null null null null null ,nullnull null nullnullnullnullnull,null null nullnullnull , where null null   null null null null  covers a sequence of continuous and non-overlap phrases, null null  is the father node of the children sequence null null null null null null . The phrase covered by null null  is just the sum of all the phrases covered by each child node null null .  We here introduce another concept that is used in our subsequent discussions." ></td>
	<td class="line x" title="54:277	A complete forest CF is a general forest with one additional condition that there is only one root node N in CF, i.e., all nodes except the root N in a CF must have at least one father node." ></td>
	<td class="line x" title="55:277	Fig." ></td>
	<td class="line x" title="56:277	1 is a complete forest while Fig." ></td>
	<td class="line x" title="57:277	7 is a non-complete forest due to the virtual node VV+VV introduced in Fig." ></td>
	<td class="line x" title="58:277	7." ></td>
	<td class="line x" title="59:277	Fig." ></td>
	<td class="line x" title="60:277	2 is a hyperedge (IP => NP VP) of Fig." ></td>
	<td class="line x" title="61:277	1, where NP covers 173 the phrase Xinhuashe, VP covers the phrase shengming youguan guiding and IP covers the entire sentence." ></td>
	<td class="line x" title="62:277	In Fig.1, only root IP has no father node, so it is a complete forest." ></td>
	<td class="line x" title="63:277	The two parse trees T1 and T2 encoded in Fig." ></td>
	<td class="line x" title="64:277	1 are shown separately in Fig." ></td>
	<td class="line x" title="65:277	3 and Fig." ></td>
	<td class="line x" title="66:277	4 1 . Different parse tree represents different derivations and explanations for a given sentence." ></td>
	<td class="line x" title="67:277	For example, for the same input sentence in Fig." ></td>
	<td class="line x" title="68:277	1, T1 interprets it as XNA (Xinhua News Agency) declares some regulations. while T2 interprets it as XNA declaration is related to some regulations.." ></td>
	<td class="line x" title="69:277	Figure 1." ></td>
	<td class="line x" title="70:277	A packed forest for sentence  /Xinhuashe /shengming /youguan  /guiding  Figure 2." ></td>
	<td class="line x" title="71:277	A hyper-edge used in Fig." ></td>
	<td class="line x" title="72:277	1    Figure 3." ></td>
	<td class="line x" title="73:277	Tree 1 (T1)            Figure 4." ></td>
	<td class="line x" title="74:277	Tree 2 (T2) 3.2 Tree sequence in packed forest Similar to the definition of tree sequence used in a single parse tree defined in Liu et al.(2007) and Zhang et al.(2008a), a tree sequence in a forest also refers to an ordered sub-tree sequence that covers a continuous phrase without overlapping." ></td>
	<td class="line x" title="77:277	However, the major difference between  1  Please note that a single tree (as T1 and T2 shown in Fig." ></td>
	<td class="line x" title="78:277	3 and Fig." ></td>
	<td class="line x" title="79:277	4) is represented by edges instead of hyper-edges." ></td>
	<td class="line x" title="80:277	A hyper-edge is a group of edges satisfying the 2 nd  condition as shown in the forest definition." ></td>
	<td class="line x" title="81:277	them lies in that the sub-trees of a tree sequence in forest may belongs to different single parse trees while, in a single parse tree-based model, all the sub-trees in a tree sequence are committed to the same parse tree." ></td>
	<td class="line x" title="82:277	The forest-based tree sequence enables our model to have the potential of exploring additional parse trees that may be wrongly pruned out by the parser and thus are not encoded in the forest." ></td>
	<td class="line x" title="83:277	This is because that a tree sequence in a forest allows its sub-trees coming from different parse trees, where these sub-trees may not be merged finally to form a complete parse tree in the forest." ></td>
	<td class="line x" title="84:277	Take the forest in Fig." ></td>
	<td class="line x" title="85:277	1 as an example, where ((VV shengming) (JJ youguan)) is a tree sequence that all sub-trees appear in T1 while ((VV shengming) (VV youguan)) is a tree sequence whose sub-trees do not belong to any single tree in the forest." ></td>
	<td class="line x" title="86:277	But, indeed the two subtrees (VV shengming) and (VV youguan) can be merged together and further lead to a complete single parse tree which may offer a correct interpretation to the input sentence (as shown in Fig." ></td>
	<td class="line x" title="87:277	5)." ></td>
	<td class="line x" title="88:277	In addition, please note that, on the other hand, more parse trees may introduce more noisy structures." ></td>
	<td class="line x" title="89:277	In this paper, we leave this problem to our model and let the model decide which substructures are noisy features." ></td>
	<td class="line x" title="90:277	Figure 5." ></td>
	<td class="line x" title="91:277	A parse tree that was wrongly pruned out        Figure 6." ></td>
	<td class="line x" title="92:277	A tree sequence to string rule  174 A tree-sequence to string translation rule in a forest is a triple <L, R, A>, where L is the tree sequence in source language, R is the string containing words and variables in target language, and A is the alignment between the leaf nodes of L and R. This definition is similar to that of (Liu et al. 2007, Zhang et al. 2008a) except our treesequence is defined in forest." ></td>
	<td class="line x" title="93:277	The shaded area of Fig." ></td>
	<td class="line x" title="94:277	6 exemplifies a tree sequence to string translation rule in the forest." ></td>
	<td class="line x" title="95:277	3.3 Forest-based tree-sequence to string translation model Given a source forest F and target translation T S  as well as word alignment A, our translation model is formulated as:   Prnull null,null null ,null null null nullnullnull null null null null nullnull null null null null null,null null null null nullnullnull,null null ,nullnull   By the above Eq., translation becomes a tree sequence structure to string mapping issue." ></td>
	<td class="line x" title="96:277	Given the F, T S  and A, there are multiple derivations that could map F to T S  under the constraint A. The mapping probability Prnull null,null null ,null null  in our study is obtained by summing over the probabilities of all derivations  . The probability of each derivation null null  is given as the product of the probabilities of all the rules () i pr  used in the derivation (here we assume that each rule is applied independently in a derivation)." ></td>
	<td class="line x" title="97:277	Our model is implemented under log-linear framework (Och and Ney, 2002)." ></td>
	<td class="line x" title="98:277	We use seven basic features that are analogous to the commonly used features in phrase-based systems (Koehn, 2003): 1) bidirectional rule mapping probabilities, 2) bidirectional lexical rule translation probabilities, 3) target language model, 4) number of rules used and 5) number of target words." ></td>
	<td class="line x" title="99:277	In addition, we define two new features: 1) number of leaf nodes in auxiliary rules (the auxiliary rule will be explained later in this paper) and 2) product of the probabilities of all hyper-edges of the tree sequences in forest." ></td>
	<td class="line oc" title="100:277	4 Training This section discusses how to extract our translation rules given a triple nullnull,null null ,nullnull . As we know, the traditional tree-to-string rules can be easily extracted from nullnull,null null ,nullnull  using the algorithm of Mi and Huang (2008) 2 . We would like  2  Mi and Huang (2008) extend the tree-based rule extraction algorithm (Galley et al., 2004) to forest-based by introducing non-deterministic mechanism." ></td>
	<td class="line x" title="101:277	Their algorithm consists of two steps, minimal rule extraction and composed rule generation." ></td>
	<td class="line x" title="102:277	to leverage on their algorithm in our study." ></td>
	<td class="line x" title="103:277	Unfortunately, their algorithm is not directly applicable to our problem because tree rules have only one root while tree sequence rules have multiple roots." ></td>
	<td class="line x" title="104:277	This makes the tree sequence rule extraction very complex due to its interaction with forest structure." ></td>
	<td class="line x" title="105:277	To address this issue, we introduce the concepts of virtual node and virtual hyperedge to convert a complete parse forest null  to a non-complete forest null  which is designed to encode all the tree sequences that we want." ></td>
	<td class="line x" title="106:277	Therefore, by doing so, the tree sequence rules can be extracted from a forest in the following two steps: 1) Convert the complete parse forest null  into a non-complete forest null  in order to cover those tree sequences that cannot be covered by a single tree node." ></td>
	<td class="line x" title="107:277	2) Employ the forest-based tree rule extraction algorithm (Mi and Huang, 2008) to extract our rules from the non-complete forest." ></td>
	<td class="line x" title="108:277	To facilitate our discussion, here we introduce two notations:  Alignable: A consecutive source phrase is an alignable phrase if and only if it can be aligned with at least one consecutive target phrase under the word-alignment constraint." ></td>
	<td class="line x" title="109:277	The covered source span is called alignable span." ></td>
	<td class="line x" title="110:277	 Node sequence: a sequence of nodes (either leaf or internal nodes) in a forest covering a consecutive span." ></td>
	<td class="line x" title="111:277	Algorithm 1 illustrates the first step of our rule extraction algorithm, which is a CKY-style Dynamic Programming (DP) algorithm to add virtual nodes into forest." ></td>
	<td class="line x" title="112:277	It includes the following steps: 1) We traverse the forest to visit each span in bottom-up fashion (line 1-2), 1.1) for each span [u,v] that is covered by single tree nodes 3 , we put these tree nodes into the set NSS(u,v) and go back to step 1 (line 4-6)." ></td>
	<td class="line x" title="113:277	1.2) otherwise we concatenate the tree sequences of sub-spans to generate the set of tree sequences covering the current larger span (line 8-13)." ></td>
	<td class="line x" title="114:277	Then, we prune the set of node sequences (line 14)." ></td>
	<td class="line x" title="115:277	If this span is alignable, we create virtual father nodes and corresponding virtual hyper-edges to link the node sequences with the virtual father nodes (line 15-20)." ></td>
	<td class="line x" title="116:277	3  Note that in a forest, there would be multiple single tree nodes covering the same span as shown Fig.1." ></td>
	<td class="line x" title="117:277	175 2) Finally we obtain a forest with each alignable span covered by either original tree nodes or the newly-created tree sequence virtual nodes." ></td>
	<td class="line x" title="118:277	Theoretically, there is exponential number of node sequences in a forest." ></td>
	<td class="line x" title="119:277	Take Fig." ></td>
	<td class="line x" title="120:277	7 as an example." ></td>
	<td class="line x" title="121:277	The NSS of span [1,2] only contains NP since it is alignable and covered by the single tree node NP." ></td>
	<td class="line x" title="122:277	However, span [2,3] cannot be covered by any single tree node, so we have to create the NSS of span[2,3] by concatenating the NSSs of span [2,2] and span [3,3]." ></td>
	<td class="line x" title="123:277	Since NSS of span [2,2] contains 4 element {NN, NP, VV, VP} and NSS of span [3, 3] also contains 4 element {VV, VP, JJ, ADJP}, NSS of span [2,3] contains 16=4*4 elements." ></td>
	<td class="line x" title="124:277	To make the NSS manageable, we prune it with the following thresholds:  each node sequence should contain less than n nodes  each node sequence set should contain less than m node sequences  sort node sequences according to their lengths and only keep the k shortest ones Each virtual node is simply labeled by the concatenation of all its childrens labels as shown in Fig." ></td>
	<td class="line x" title="125:277	7." ></td>
	<td class="line x" title="126:277	Algorithm 1." ></td>
	<td class="line x" title="127:277	add virtual nodes into forest Input: packed forest F, alignment A Notation:    L: length of source sentence    NSS(u,v): the set of node sequences covering span [u,v]   VN(ns): virtual father node for node sequence ns." ></td>
	<td class="line x" title="128:277	Output: modified forest F with virtual nodes   1." ></td>
	<td class="line x" title="129:277	for length := 0 to L 1 do 2." ></td>
	<td class="line x" title="130:277	for start := 1 to L length do 3." ></td>
	<td class="line x" title="131:277	stop := start + length 4." ></td>
	<td class="line x" title="132:277	if span[start, stop] covered by tree nodes then 5." ></td>
	<td class="line x" title="133:277	for each node n of span [start, stop] do 6." ></td>
	<td class="line x" title="134:277	add n into NSS(start, stop) 7." ></td>
	<td class="line x" title="135:277	else 8." ></td>
	<td class="line x" title="136:277	for pivot := start to stop 1 9." ></td>
	<td class="line x" title="137:277	for each ns1 in NSS(start, pivot) do 10." ></td>
	<td class="line x" title="138:277	for each ns2 in NSS(pivot+1, stop) do 11." ></td>
	<td class="line x" title="139:277	create nullnull nullnull nullnull1 null nullnull2 12." ></td>
	<td class="line x" title="140:277	if ns is not in NSS(start, stop) then 13." ></td>
	<td class="line x" title="141:277	add ns into NSS(start, stop) 14." ></td>
	<td class="line x" title="142:277	do pruning on NSS(start, stop) 15." ></td>
	<td class="line x" title="143:277	if the span[start, stop] is alignable then 16." ></td>
	<td class="line x" title="144:277	for each ns of NSS(start, stop) do 17." ></td>
	<td class="line x" title="145:277	if node VN(ns) is not in F then 18." ></td>
	<td class="line x" title="146:277	add node VN(ns) into F 19." ></td>
	<td class="line x" title="147:277	add a hyper-edge h into F, 20." ></td>
	<td class="line x" title="148:277	let lhs(h) := VN(ns), rhs(h) := ns  Algorithm 1 outputs a non-complete forest CF with each alignable span covered by either tree nodes or virtual nodes." ></td>
	<td class="line x" title="149:277	Then we can easily extract our rules from the CF using the tree rule extraction algorithm (Mi and Huang, 2008)." ></td>
	<td class="line x" title="150:277	Finally, to calculate rule feature probabilities for our model, we need to calculate the fractional counts (it is a kind of probability defined in Mi and Huang, 2008) of each translation rule in a parse forest." ></td>
	<td class="line x" title="151:277	In the tree case, we can use the inside-outside-based methods (Mi and Huang 2008) to do it." ></td>
	<td class="line x" title="152:277	In the tree sequence case, since the previous method cannot be used directly, we provide another solution by making an independent assumption that each tree in a tree sequence is independent to each other." ></td>
	<td class="line x" title="153:277	With this assumption, the fractional counts of both tree and tree sequence can be calculated as follows:  nullnullnullnull null nullnullnullnullnullnullnullnullnullnull nullnull null nullnullnull null   nullnull null nullnullnullnull null nullnullnull nullnullnullnullnullnullnullnullnullnullnullnull nullnullnull nullnullnullnull nullnullnull nullnullnullnullnullnullnullnull   where nullnullnullnull  is the fractional counts to be calculated for rule r, a frag is either lhs(r) (excluding virtual nodes and virtual hyper-edges) or any tree node in a forest, TOP is the root of the forest, nullnull.null  and nullnull." ></td>
	<td class="line x" title="154:277	) are the outside and inside probabilities of nodes, nullnullnullnullnull.null  returns the root nodes of a tree sequence fragment, nullnullnullnullnullnullnull.null  returns the leaf nodes of a tree sequence fragment, nullnullnullnull  is the hyper-edge probability." ></td>
	<td class="line x" title="155:277	Figure 7." ></td>
	<td class="line x" title="156:277	A virtual node in forest 5 Decoding We benefit from the same strategy as used in our rule extraction algorithm in designing our decoding algorithm, recasting the forest-based tree sequence-to-string decoding problem as a forestbased tree-to-string decoding problem." ></td>
	<td class="line x" title="157:277	Our decoding algorithm consists of four steps: 1) Convert the complete parse forest to a noncomplete one by introducing virtual nodes." ></td>
	<td class="line x" title="158:277	176 2) Convert the non-complete parse forest into a translation forest 4  nullnull  by using the translation rules and the pattern-matching algorithm presented in Mi et al.(2008)." ></td>
	<td class="line x" title="160:277	3) Prune out redundant nodes and add auxiliary hyper-edge into the translation forest for those nodes that have either no child or no father." ></td>
	<td class="line x" title="161:277	By this step, the translation forest nullnull  becomes a complete forest." ></td>
	<td class="line x" title="162:277	4) Decode the translation forest using our translation model and a dynamic search algorithm." ></td>
	<td class="line x" title="163:277	The process of step 1 is similar to Algorithm 1 except no alignment constraint used here." ></td>
	<td class="line x" title="164:277	This may generate a large number of additional virtual nodes; however, all redundant nodes will be filtered out in step 3." ></td>
	<td class="line x" title="165:277	In step 2, we employ the treeto-string pattern match algorithm (Mi et al., 2008) to convert a parse forest to a translation forest." ></td>
	<td class="line x" title="166:277	In step 3, all those nodes not covered by any translation rules are removed." ></td>
	<td class="line x" title="167:277	In addition, please note that the translation forest is already not a complete forest due to the virtual nodes and the pruning of rule-unmatchable nodes." ></td>
	<td class="line x" title="168:277	We, therefore, propose Algorithm 2 to add auxiliary hyper-edges to make the translation forest complete." ></td>
	<td class="line x" title="169:277	In Algorithm 2, we travel the forest in bottomup fashion (line 4-5)." ></td>
	<td class="line x" title="170:277	For each span, we do: 1) generate all the NSS for this span (line 7-12) 2) filter the NSS to a manageable size (line 13) 3) add auxiliary hyper-edges for the current span (line 15-19) if it can be covered by at least one single tree node, otherwise go to step 1 . This is the key step in our Algorithm 2." ></td>
	<td class="line x" title="171:277	For each tree node and each node sequences covering the same span (stored in the current NSS), if the tree node has no children or at least one node in the node sequence has no father, we add an auxiliary hyper-edge to connect the tree node as father node with the node sequence as children." ></td>
	<td class="line x" title="172:277	Since Algorithm 2 is DP-based and traverses the forest in a bottom-up way, all the nodes in a node sequence should already have children node after the lower level process in a small span." ></td>
	<td class="line x" title="173:277	Finally, we re-build the NSS of current span for upper level NSS combination use (line 20-22)." ></td>
	<td class="line x" title="174:277	In Fig." ></td>
	<td class="line x" title="175:277	8, the hyper-edge IP=>NP VV+VV NP is an auxiliary hyper-edge introduced by Algorithm 2." ></td>
	<td class="line x" title="176:277	By Algorithm 2, we convert the translation forest into a complete translation forest." ></td>
	<td class="line x" title="177:277	We then use a bottom-up node-based search  4  The concept of translation forest is proposed in Mi et al.(2008)." ></td>
	<td class="line x" title="179:277	It is a forest that consists of only the hyperedges induced from translation rules." ></td>
	<td class="line x" title="180:277	algorithm to do decoding on the complete translation forest." ></td>
	<td class="line x" title="181:277	We also use Cube Pruning algorithm (Huang and Chiang 2007) to speed up the translation process." ></td>
	<td class="line x" title="182:277	Figure 8." ></td>
	<td class="line x" title="183:277	Auxiliary hyper-edge in a translation forest  Algorithm 2." ></td>
	<td class="line x" title="184:277	add auxiliary hyper-edges into mt forest F Input:  mt forest F Output: complete forest F with auxiliary hyper-edges  1." ></td>
	<td class="line x" title="185:277	for i := 1 to L do 2." ></td>
	<td class="line x" title="186:277	for each node n of span [i, i] do 3." ></td>
	<td class="line x" title="187:277	add n into NSS(i, i) 4." ></td>
	<td class="line x" title="188:277	for length := 1 to L 1 do 5." ></td>
	<td class="line x" title="189:277	for start := 1 to L length do 6." ></td>
	<td class="line x" title="190:277	stop := start + length 7." ></td>
	<td class="line x" title="191:277	for pivot := start to stop-1 do 8." ></td>
	<td class="line x" title="192:277	for each ns1 in NSS (start, pivot) do 9." ></td>
	<td class="line x" title="193:277	for each ns2 in NSS (pivot+1,stop) do 10." ></td>
	<td class="line x" title="194:277	create nullnull nullnull nullnull1 null nullnull2 11." ></td>
	<td class="line x" title="195:277	if ns is not in NSS(start, stop) then 12." ></td>
	<td class="line x" title="196:277	add ns into NSS (start, stop) 13." ></td>
	<td class="line x" title="197:277	do pruning on NSS(start, stop) 14." ></td>
	<td class="line x" title="198:277	if there is tree node cover span [start, stop] then 15." ></td>
	<td class="line x" title="199:277	for each tree node n of span [start,stop] do 16." ></td>
	<td class="line x" title="200:277	for each ns of NSS(start, stop) do 17." ></td>
	<td class="line x" title="201:277	if node n have no children or there is node in ns with no father then 18." ></td>
	<td class="line x" title="202:277	add auxiliary hyper-edge h into F 19." ></td>
	<td class="line x" title="203:277	let lhs(h) := n, rhs(h) := ns 20." ></td>
	<td class="line x" title="204:277	empty NSS(start, stop) 21." ></td>
	<td class="line x" title="205:277	for each node n of span [start, stop] do 22." ></td>
	<td class="line x" title="206:277	add n into NSS(start, stop) 6 Experiment 6.1 Experimental Settings We evaluate our method on Chinese-English translation task." ></td>
	<td class="line x" title="207:277	We use the FBIS corpus as training set, the NIST MT-2002 test set as development (dev) set and the NIST MT-2003 test set as test set." ></td>
	<td class="line x" title="208:277	We train Charniaks parser (Charniak 2000) on CTB5 to do Chinese parsing, and modify it to output packed forest." ></td>
	<td class="line x" title="209:277	We tune the parser on section 301-325 and test it on section 271300." ></td>
	<td class="line x" title="210:277	The F-measure on all sentences is 80.85%." ></td>
	<td class="line x" title="211:277	A 3-gram language model is trained on the Xin177 hua portion of the English Gigaword3 corpus and the target side of the FBIS corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Kenser and Ney, 1995)." ></td>
	<td class="line x" title="212:277	GIZA++ (Och and Ney, 2003) and the heuristics grow-diag-final-and are used to generate m-ton word alignments." ></td>
	<td class="line x" title="213:277	For the MER training (Och, 2003), Koehns MER trainer (Koehn, 2007) is modified for our system." ></td>
	<td class="line x" title="214:277	For significance test, we use Zhang et al.s implementation (Zhang et al, 2004)." ></td>
	<td class="line x" title="215:277	Our evaluation metrics is casesensitive BLEU-4 (Papineni et al., 2002)." ></td>
	<td class="line x" title="216:277	For parse forest pruning (Mi et al., 2008), we utilize the Margin-based pruning algorithm presented in (Huang, 2008)." ></td>
	<td class="line x" title="217:277	Different from Mi et al.(2008) that use a static pruning threshold, our threshold is sentence-depended." ></td>
	<td class="line x" title="219:277	For each sentence, we compute the Margin between the n-th best and the top 1 parse tree, then use the Margin-based pruning algorithm presented in (Huang, 2008) to do pruning." ></td>
	<td class="line x" title="220:277	By doing so, we can guarantee to use at least all the top n best parse trees in the forest." ></td>
	<td class="line x" title="221:277	However, please note that even after pruning there is still exponential number of additional trees embedded in the forest because of the sharing structure of forest." ></td>
	<td class="line x" title="222:277	Other parameters are set as follows: maximum number of roots in a tree sequence is 3, maximum height of a translation rule is 3, maximum number of leaf nodes is 7, maximum number of node sequences on each span is 10, and maximum number of rules extracted from one node is 10000." ></td>
	<td class="line x" title="223:277	6.2 Experimental Results We implement our proposed methods as a general, configurable platform for syntax-based SMT study." ></td>
	<td class="line x" title="224:277	Based on this platform, we are able to easily implement most of the state-of-the-art syntax-based x-to-string SMT methods via simple parameter setting." ></td>
	<td class="line x" title="225:277	For training, we set forest pruning threshold to 1 best for tree-based methods and 100 best for forest-based methods." ></td>
	<td class="line x" title="226:277	For decoding, we set: 1) TT2S: tree-based tree-to-string model by setting the forest pruning threshold to 1 best and the number of sub-trees in a tree sequence to 1." ></td>
	<td class="line x" title="227:277	2) TTS2S: tree-based tree-sequence to string system by setting the forest pruning threshold to 1 best and the maximum number of sub-trees in a tree sequence to 3." ></td>
	<td class="line x" title="228:277	3) FT2S: forest-based tree-to-string system by setting the forest pruning threshold to 500 best, the number of sub-trees in a tree sequence to 1." ></td>
	<td class="line x" title="229:277	4) FTS2S: forest-based tree-sequence to string system by setting the forest pruning threshold to 500 best and the maximum number of sub-trees in a tree sequence to 3." ></td>
	<td class="line x" title="230:277	Model BLEU(%) Moses 25.68 TT2S 26.08 TTS2S 26.95 FT2S 27.66 FTS2S 28.83  Table 1." ></td>
	<td class="line x" title="231:277	Performance Comparison  We use the first three syntax-based systems (TT2S, TTS2S, FT2S) and Moses (Koehn et al., 2007), the state-of-the-art phrase-based system, as our baseline systems." ></td>
	<td class="line x" title="232:277	Table 1 compares the performance of the five methods, all of which are fine-tuned." ></td>
	<td class="line x" title="233:277	It shows that: 1) FTS2S significantly outperforms (p<0.05) FT2S." ></td>
	<td class="line x" title="234:277	This shows that tree sequence is very useful to forest-based model." ></td>
	<td class="line x" title="235:277	Although a forest can cover much more phrases than a single tree does, there are still many non-syntactic phrases that cannot be captured by a forest due to structure divergence issue." ></td>
	<td class="line x" title="236:277	On the other hand, tree sequence is a good solution to non-syntactic translation equivalence modeling." ></td>
	<td class="line x" title="237:277	This is mainly because tree sequence rules are only sensitive to word alignment while tree rules, even extracted from a forest (like in FT2S), are also limited by syntax according to grammar parsing rules." ></td>
	<td class="line x" title="238:277	2) FTS2S shows significant performance improvement (p<0.05) over TTS2S due to the contribution of forest." ></td>
	<td class="line x" title="239:277	This is mainly due to the fact that forest can offer very large number of parse trees for rule extraction and decoder." ></td>
	<td class="line x" title="240:277	3) Our model statistically significantly outperforms all the baselines system." ></td>
	<td class="line x" title="241:277	This clearly demonstrates the effectiveness of our proposed model for syntax-based SMT." ></td>
	<td class="line x" title="242:277	It also shows that the forest-based method and tree sequence-based method are complementary to each other and our proposed method is able to effectively integrate their strengths." ></td>
	<td class="line x" title="243:277	4) All the four syntax-based systems show better performance than Moses and three of them significantly outperforms (p<0.05) Moses." ></td>
	<td class="line x" title="244:277	This suggests that syntax is very useful to SMT and translation can be viewed as a structure mapping issue as done in the four syntax-based systems." ></td>
	<td class="line x" title="245:277	Table 2 and Table 3 report the distribution of different kinds of translation rules in our model (training forest pruning threshold is set to 100 best) and in our decoding (decoding forest pruning threshold is set to 500 best) for one best translation generation." ></td>
	<td class="line x" title="246:277	From the two tables, we can find that: 178 Rule Type Tree to String Tree Sequence to String L 4,854,406 20,526,674 P 37,360,684 58,826,261 U 3,297,302 3,775,734 All 45,512,392 83,128,669  Table 2." ></td>
	<td class="line x" title="247:277	# of rules extracted from training corpus." ></td>
	<td class="line x" title="248:277	L means fully lexicalized, P means partially lexicalized, U means unlexicalized." ></td>
	<td class="line x" title="249:277	Rule Type Tree to String Tree Sequence to String L 10,592 1,161 P 7,132 742 U 4,874 278 All 22,598 2,181  Table 3." ></td>
	<td class="line x" title="250:277	# of rules used to generate one-best translation result in testing  1) In Table 2, the number of tree sequence rules is much larger than that of tree rules although our rule extraction algorithm only extracts those tree sequence rules over the spans that tree rules cannot cover." ></td>
	<td class="line x" title="251:277	This suggests that the non-syntactic structure mapping is still a big challenge to syntax-based SMT." ></td>
	<td class="line x" title="252:277	2) Table 3 shows that the tree sequence rules is around 9% of the tree rules when generating the one-best translation." ></td>
	<td class="line x" title="253:277	This suggests that around 9% of translation equivalences in the test set can be better modeled by tree sequence to string rules than by tree to string rules." ></td>
	<td class="line x" title="254:277	The 9% tree sequence rules contribute 1.17 BLEU score improvement (28.83-27.66 in Table 1) to FTS2S over FT2S." ></td>
	<td class="line x" title="255:277	3) In Table 3, the fully-lexicalized rules are the major part (around 60%), followed by the partially-lexicalized (around 35%) and unlexicalized (around 15%)." ></td>
	<td class="line x" title="256:277	However, in Table 2, partially-lexicalized rules extracted from training corpus are the major part (more than 70%)." ></td>
	<td class="line x" title="257:277	This suggests that most partially-lexicalized rules are less effective in our model." ></td>
	<td class="line x" title="258:277	This clearly directs our future work in model optimization." ></td>
	<td class="line x" title="259:277	BLEU (%) N-best \ model FT2S FTS2S 100 Best 27.40 28.61 500 Best  27.66 28.83 2500 Best  27.66 28.96 5000 Best  27.79 28.89  Table 4." ></td>
	<td class="line x" title="260:277	Impact of the forest pruning  Forest pruning is a key step for forest-based method." ></td>
	<td class="line x" title="261:277	Table 4 reports the performance of the two forest-based models using different values of the forest pruning threshold for decoding." ></td>
	<td class="line x" title="262:277	It shows that: 1) FTS2S significantly outperforms (p<0.05) FT2S consistently in all test cases." ></td>
	<td class="line x" title="263:277	This again demonstrates the effectiveness of our proposed model." ></td>
	<td class="line x" title="264:277	Even if in the 5000 Best case, tree sequence is still able to contribute 1.1 BLEU score improvement (28.89-27.79)." ></td>
	<td class="line x" title="265:277	It indicates the advantage of tree sequence cannot be covered by forest even if we utilize a very large forest." ></td>
	<td class="line x" title="266:277	2) The BLEU scores are very similar to each other when we increase the forest pruning threshold." ></td>
	<td class="line x" title="267:277	Moreover, in one case the performance even drops." ></td>
	<td class="line x" title="268:277	This suggests that although more parse trees in a forest can offer more structure information, they may also introduce more noise that may confuse the decoder." ></td>
	<td class="line x" title="269:277	7 Conclusion In this paper, we propose a forest-based treesequence to string translation model to combine the strengths of forest-based methods and treesequence based methods." ></td>
	<td class="line x" title="270:277	This enables our model to have the great potential to address the issues of structure divergence and parse errors for syntax-based SMT." ></td>
	<td class="line x" title="271:277	We convert our forest-based tree sequence rule extraction and decoding issues to tree-based by introducing virtual nodes, virtual hyper-edges and auxiliary rules (hyper-edges)." ></td>
	<td class="line x" title="272:277	In our system implementation, we design a general and configurable platform for our method, based on which we can easily realize many previous syntax-based methods." ></td>
	<td class="line x" title="273:277	Finally, we examine our methods on the FBIS corpus and the NIST MT2003 Chinese-English translation task." ></td>
	<td class="line x" title="274:277	Experimental results show that our model greatly outperforms the four baseline systems." ></td>
	<td class="line x" title="275:277	Our study demonstrates that forest-based method and tree sequence-based method are complementary to each other and our proposed method is able to effectively combine the strengths of the two individual methods for syntax-based SMT." ></td>
	<td class="line x" title="276:277	Acknowledgement We would like to thank Huang Yun for preparing the pictures in this paper; Run Yan for providing the java version modified MERT program and discussion on the details of MOSES; Mi Haitao for his help and discussion on re-implementing the FT2S model; Sun Jun and Xiong Deyi for their valuable suggestions." ></td>
	<td class="line x" title="277:277	179" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1063
Improving Tree-to-Tree Translation with Packed Forests
Liu, Yang;L, Yajuan;Liu, Qun;"></td>
	<td class="line x" title="1:222	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 558566, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:222	c2009 ACL and AFNLP Improving Tree-to-Tree Translation with Packed Forests Yang Liu and Yajuan Lu and Qun Liu Key Laboratory of Intelligent Information Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {yliu,lvyajuan,liuqun}@ict.ac.cn Abstract Current tree-to-tree models suffer from parsing errors as they usually use only 1best parses for rule extraction and decoding." ></td>
	<td class="line x" title="3:222	We instead propose a forest-based tree-to-tree model that uses packed forests." ></td>
	<td class="line x" title="4:222	The model is based on a probabilistic synchronous tree substitution grammar (STSG), which can be learned from aligned forest pairs automatically." ></td>
	<td class="line x" title="5:222	The decoder finds ways of decomposing trees in the source forest into elementary trees using the source projection of STSG while building target forest in parallel." ></td>
	<td class="line x" title="6:222	Comparable to the state-of-the-art phrase-based system Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees." ></td>
	<td class="line x" title="7:222	1 Introduction Approaches to syntax-based statistical machine translation make use of parallel data with syntactic annotations, either in the form of phrase structure trees or dependency trees." ></td>
	<td class="line x" title="8:222	They can be roughly divided into three categories: string-to-tree models (e.g., (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008)), tree-to-string models (e.g., (Liu et al., 2006; Huang et al., 2006)), and tree-totree models (e.g., (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008))." ></td>
	<td class="line x" title="9:222	By modeling the syntax of both source and target languages, tree-to-tree approaches have the potential benefit of providing rules linguistically better motivated." ></td>
	<td class="line x" title="10:222	However, while string-to-tree and tree-to-string models demonstrate promising results in empirical evaluations, tree-to-tree models have still been underachieving." ></td>
	<td class="line x" title="11:222	We believe that tree-to-tree models face two major challenges." ></td>
	<td class="line x" title="12:222	First, tree-to-tree models are more vulnerable to parsing errors." ></td>
	<td class="line x" title="13:222	Obtaining syntactic annotations in quantity usually entails running automatic parsers on a parallel corpus." ></td>
	<td class="line x" title="14:222	As the amount and domain of the data used to train parsers are relatively limited, parsers will inevitably output ill-formed trees when handling real-world text." ></td>
	<td class="line x" title="15:222	Guided by such noisy syntactic information, syntax-based models that rely on 1-best parses are prone to learn noisy translation rules in training phase and produce degenerate translations in decoding phase (Quirk and CorstonOliver, 2006)." ></td>
	<td class="line x" title="16:222	This situation aggravates for treeto-tree models that use syntax on both sides." ></td>
	<td class="line x" title="17:222	Second, tree-to-tree rules provide poorer rule coverage." ></td>
	<td class="line x" title="18:222	As a tree-to-tree rule requires that there must be trees on both sides, tree-to-tree models lose a larger amount of linguistically unmotivated mappings." ></td>
	<td class="line x" title="19:222	Studies reveal that the absence of such non-syntactic mappings will impair translation quality dramatically (Marcu et al., 2006; Liu et al., 2007; DeNeefe et al., 2007; Zhang et al., 2008)." ></td>
	<td class="line x" title="20:222	Compactly encoding exponentially many parses, packed forests prove to be an excellent fit for alleviating the above two problems (Mi et al., 2008; Mi and Huang, 2008)." ></td>
	<td class="line x" title="21:222	In this paper, we propose a forest-based tree-to-tree model." ></td>
	<td class="line x" title="22:222	To learn STSG rules from aligned forest pairs, we introduce a series of notions for identifying minimal tree-to-tree rules." ></td>
	<td class="line x" title="23:222	Our decoder first converts the source forest to a translation forest and then finds the best derivation that has the source yield of one source tree in the forest." ></td>
	<td class="line x" title="24:222	Comparable to Moses, our forest-based tree-to-tree model achieves an absolute improvement of 3.6 BLEU points over conventional tree-based model." ></td>
	<td class="line x" title="25:222	558 IP1 NP2 VP3 PP4 VP-B5 NP-B6 NP-B7 NP-B8 NR9 CC10P11 NR12 VV13 AS14 NN15 bushi yu shalong juxing le huitan Bush held a talk with Sharon NNP16 VBD17 DT18 NN19 IN20 NNP21 NP22 NP23 NP24 NP25 PP26 NP27 VP28 S 29 Figure 1: An aligned packed forest pair." ></td>
	<td class="line x" title="26:222	Each node is assigned a unique identity for reference." ></td>
	<td class="line x" title="27:222	The solid lines denote hyperedges and the dashed lines denote word alignments." ></td>
	<td class="line x" title="28:222	Shaded nodes are frontier nodes." ></td>
	<td class="line x" title="29:222	2 Model Figure 1 shows an aligned forest pair for a Chinese sentence and an English sentence." ></td>
	<td class="line x" title="30:222	The solid lines denote hyperedges and the dashed lines denote word alignments between the two forests." ></td>
	<td class="line x" title="31:222	Each node is assigned a unique identity for reference." ></td>
	<td class="line x" title="32:222	Each hyperedge is associated with a probability, which we omit in Figure 1 for clarity." ></td>
	<td class="line x" title="33:222	In a forest, a node usually has multiple incoming hyperedges." ></td>
	<td class="line x" title="34:222	We use IN(v) to denote the set of incoming hyperedges of node v. For example, the source node IP1 has following two incoming hyperedges: 1 e1 =(NP-B6,VP3),IP1 e2 =(NP2,VP-B5),IP1 1As there are both source and target forests, it might be confusing by just using a span to refer to a node." ></td>
	<td class="line x" title="35:222	In addition, some nodes will often have the same labels and spans." ></td>
	<td class="line x" title="36:222	Therefore, it is more convenient to use an identity for referring to a node." ></td>
	<td class="line x" title="37:222	The notation IP1 denotes the node that has a label of IP and has an identity of 1." ></td>
	<td class="line x" title="38:222	Formally, a packed parse forest is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar." ></td>
	<td class="line x" title="39:222	Huang and Chiang (2005) define a forest as a tupleV,E, v,R, where V is a finite set of nodes, E is a finite set of hyperedges, vV is a distinguished node that denotes the goal item in parsing, and R is the set of weights." ></td>
	<td class="line x" title="40:222	For a given sentence w1:l = w1 wl, each node v V is in the form of Xi,j, which denotes the recognition of non-terminal X spanning the substring from positions i through j (that is, wi+1 wj)." ></td>
	<td class="line x" title="41:222	Each hyperedge e E is a triple e = T(e),h(e),f(e), where h(e)V is its head, T(e)V  is a vector of tail nodes, and f(e) is a weight function from R|T(e)| to R. Our forest-based tree-to-tree model is based on a probabilistic STSG (Eisner, 2003)." ></td>
	<td class="line x" title="42:222	Formally, an STSG can be defined as a quintuple G = Fs,Ft,Ss,St,P, where Fs andFt are the source and target alphabets, respectively, Ss andSt are the source and target start symbols, and  P is a set of production rules." ></td>
	<td class="line x" title="43:222	A rule r is a triplets,tt,that describes the correspondencebetween a source tree ts and a target tree tt." ></td>
	<td class="line x" title="44:222	To integrate packed forests into tree-to-tree translation, we model the process of synchronous generation of a source forest Fs and a target forest Ft using a probabilistic STSG grammar: Pr(Fs,Ft) = summationdisplay TsFs summationdisplay TtFt Pr(Ts,Tt) = summationdisplay TsFs summationdisplay TtFt summationdisplay dD Pr(d) = summationdisplay TsFs summationdisplay TtFt summationdisplay dD productdisplay rd p(r) (1) where Ts is a source tree, Tt is a target tree, D is the set of all possible derivations that transform Ts into Tt, d is one such derivation, and r is a tree-totree rule." ></td>
	<td class="line x" title="45:222	Table 1 shows a derivation of the forest pair in Figure 1." ></td>
	<td class="line x" title="46:222	A derivation is a sequence of tree-to-tree rules." ></td>
	<td class="line x" title="47:222	Note that we use x to represent a nonterminal." ></td>
	<td class="line x" title="48:222	559 (1) IP(x1:NP-B, x2:VP)S(x1:NP, x2:VP) (2) NP-B(x1:NR)NP(x1:NNP) (3) NR(bushi)NNP(Bush) (4) VP(x1:PP, VP-B(x2:VV, AS(le), x3:NP-B))VP(x2:VBD, NP(DT(a), x3:NP), x1:PP) (5) PP(x1:P, x2:NP-B)PP(x1:IN, x2:NP) (6) P(yu)IN(with) (7) NP-B(x1:NR)NP(x1:NP) (8) NR(shalong) NNP(Sharon) (9) VV(juxing) VBD(held) (10) NP-B(x1:NN)NP(x1:NN) (11) NN(huitan) NN(talk) Table 1: A minimal derivation of the forest pair in Figure 1." ></td>
	<td class="line x" title="49:222	id span cspan complement consistent frontier counterparts 1 1-6 1-2, 4-6 1 1 29 2 1-3 1, 5-6 2, 4 0 0 3 2-6 2, 4-6 1 1 1 28 4 2-3 5-6 1-2, 4 1 1 25, 26 5 4-6 2, 4 1, 5-6 1 0 6 1-1 1 2, 4-6 1 1 16, 22 7 3-3 6 1-2, 4-5 1 1 21, 24 8 6-6 4 1-2, 5-6 1 1 19, 23 9 1-1 1 2, 4-6 1 1 16, 22 10 2-2 5 1-2, 4, 6 1 1 20 11 2-2 5 1-2, 4, 6 1 1 20 12 3-3 6 1-2, 4-5 1 1 21, 24 13 4-4 2 1, 4-6 1 1 17 14 5-5 1-2, 4-6 1 0 15 6-6 4 1-2, 5-6 1 1 19, 23 16 1-1 1 2-4, 6 1 1 6, 9 17 2-2 4 1-3, 6 1 1 13 18 3-3 1-4, 6 1 0 19 4-4 6 1-4 1 1 8, 15 20 5-5 2 1, 3-4, 6 1 1 10, 11 21 6-6 3 1-2, 4, 6 1 1 7, 12 22 1-1 1 2-4, 6 1 1 6, 9 23 3-4 6 1-4 1 1 8, 15 24 6-6 3 1-2, 4, 6 1 1 7, 12 25 5-6 2-3 1, 4, 6 1 1 4 26 5-6 2-3 1, 4, 6 1 1 4 27 3-6 2-3, 6 1, 4 0 0 28 2-6 2-4, 6 1 1 1 3 29 1-6 1-4, 6 1 1 1 Table 2: Node attributes of the example forest pair." ></td>
	<td class="line x" title="50:222	3 Rule Extraction Given an aligned forest pair as shown in Figure 1, how to extract all valid tree-to-tree rules that explain its synchronous generation process?" ></td>
	<td class="line oc" title="51:222	By constructing a theory that gives formal semantics to word alignments, Galley et al.(2004) give principled answers to these questions for extracting tree-to-string rules." ></td>
	<td class="line o" title="53:222	Their GHKM procedure draws connections among word alignments, derivations, and rules." ></td>
	<td class="line o" title="54:222	They first identify the tree nodes that subsume tree-string pairs consistent with word alignments and then extract rules from these nodes." ></td>
	<td class="line p" title="55:222	By this means, GHKM proves to be able to extract all valid tree-to-string rules from training instances." ></td>
	<td class="line o" title="56:222	Although originally developed for the tree-to-string case, it is possible to extend GHKM to extract all valid tree-to-tree rules from aligned packed forests." ></td>
	<td class="line o" title="57:222	In this section, we introduce our tree-to-tree rule extraction method adapted from GHKM, which involves four steps: (1) identifying the correspondence between the nodes in forest pairs, (2) identifying minimum rules, (3) inferring composed rules, and (4) estimating rule probabilities." ></td>
	<td class="line x" title="58:222	3.1 Identifying Correspondence Between Nodes To learn tree-to-tree rules, we need to find aligned tree pairs in the forest pairs." ></td>
	<td class="line x" title="59:222	To do this, the starting point is to identify the correspondence between nodes." ></td>
	<td class="line o" title="60:222	We propose a number of attributes for nodes, most of which derive from GHKM, to facilitate the identification." ></td>
	<td class="line x" title="61:222	Definition 1 Given a node v, its span (v) is an index set of the words it covers." ></td>
	<td class="line x" title="62:222	For example, the span of the source node VP-B5 is {4,5,6} as it covers three source words: juxing, le, and huitan." ></td>
	<td class="line x" title="63:222	For convenience, we use{4-6}to denotes a contiguous span {4,5,6}." ></td>
	<td class="line x" title="64:222	Definition 2 Given a node v, its corresponding span (v) is the index set of aligned words on another side." ></td>
	<td class="line x" title="65:222	For example, the corresponding span of the source node VP-B5 is {2,4}, corresponding to the target words held and talk." ></td>
	<td class="line x" title="66:222	Definition 3 Given a node v, its complement span (v) is the union of corresponding spans of nodes that are neither antecedents nor descendants of v. For example, the complement span of the source node VP-B5 is{1,5-6}, corresponding to target words Bush, with, and Sharon." ></td>
	<td class="line x" title="67:222	Definition 4 A node v is said to be consistent with alignment if and only if closure((v))(v) =." ></td>
	<td class="line x" title="68:222	For example, the closure of the corresponding span of the source node VP-B5 is {2-4} and its complement span is {1,5-6}." ></td>
	<td class="line x" title="69:222	As the intersection of the closure and the complement span is an empty set, the source node VP-B5 is consistent with the alignment." ></td>
	<td class="line x" title="70:222	560 PP4 NP-B7 P11 NR12 PP4 P11 NP-B7 PP4 NP-B7 P11 NR12 PP26 IN20 NP24 NNP21 PP4 P11 NP-B7 PP26 IN 20 NP24 (a) (b) (c) (d) Figure 2: (a) A frontier tree; (b) a minimal frontier tree; (c) a frontier tree pair; (d) a minimal frontier tree pair." ></td>
	<td class="line x" title="71:222	All trees are taken from the example forest pair in Figure 1." ></td>
	<td class="line x" title="72:222	Shaded nodes are frontier nodes." ></td>
	<td class="line x" title="73:222	Each node is assigned an identity for reference." ></td>
	<td class="line x" title="74:222	Definition 5 A node v is said to be a frontier node if and only if: 1." ></td>
	<td class="line x" title="75:222	v is consistent; 2." ></td>
	<td class="line x" title="76:222	There exists at least one consistent node v on another side satisfying:  closure((v))(v);  closure((v))(v)." ></td>
	<td class="line x" title="77:222	v is said to be a counterpart of v. We use (v) to denote the set of counterparts of v. A frontier node often has multiple counterparts on another side due to the usage of unary rules in parsers." ></td>
	<td class="line x" title="78:222	For example, the source node NP-B6 has two counterparts on the target side: NNP16 and NP22." ></td>
	<td class="line x" title="79:222	Conversely, the target node NNP16 also has two counterparts counterparts on the source side: NR9 and NP-B6." ></td>
	<td class="line x" title="80:222	The node attributes of the example forest pair are listed in Table 2." ></td>
	<td class="line x" title="81:222	We use identities to refer to nodes." ></td>
	<td class="line x" title="82:222	cspan denotes corresponding span and complement denotes complement span." ></td>
	<td class="line x" title="83:222	In Figure 1, there are 12 frontier nodes (highlighted by shading) on the source side and 12 frontier nodes on the target side." ></td>
	<td class="line o" title="84:222	Note that while a consistent node is equal to a frontier node in GHKM, this is not the case in our method because we have a tree on the target side." ></td>
	<td class="line x" title="85:222	Frontier nodes play a critical role in forest-based rule extraction because they indicate where to cut the forest pairs to obtain treeto-tree rules." ></td>
	<td class="line x" title="86:222	3.2 Identifying Minimum Rules Given the frontier nodes, the next step is to identify aligned tree pairs, from which tree-to-tree rules derive." ></td>
	<td class="line x" title="87:222	Following Galley et al.(2006), we distinguish between minimal and composed rules." ></td>
	<td class="line x" title="89:222	As a composed rule can be decomposed as a sequence of minimal rules, we are particularly interested in how to extract minimal rules." ></td>
	<td class="line x" title="90:222	Also, we introduce a number of notions to help identify minimal rules." ></td>
	<td class="line x" title="91:222	Definition 6 A frontier tree is a subtree in a forest satisfying: 1." ></td>
	<td class="line x" title="92:222	Its root is a frontier node; 2." ></td>
	<td class="line x" title="93:222	If the tree contains only one node, it must be a lexicalized frontier node; 3." ></td>
	<td class="line x" title="94:222	If the tree contains more than one nodes, its leaves are either non-lexicalized frontier nodes or lexicalized non-frontier nodes." ></td>
	<td class="line x" title="95:222	For example, Figure 2(a) shows a frontier tree in which all nodes are frontier nodes." ></td>
	<td class="line x" title="96:222	Definition 7 A minimal frontier tree is a frontier tree such that all nodes other than the root and leaves are non-frontier nodes." ></td>
	<td class="line x" title="97:222	For example, Figure 2(b) shows a minimal frontier tree." ></td>
	<td class="line x" title="98:222	Definition 8 A frontier tree pair is a triple ts,tt,satisfying: 1." ></td>
	<td class="line x" title="99:222	ts is a source frontier tree; 561 2." ></td>
	<td class="line x" title="100:222	tt is a target frontier tree; 3." ></td>
	<td class="line x" title="101:222	The root of ts is a counterpart of that of tt; 4." ></td>
	<td class="line x" title="102:222	There is a one-to-one correspondence  between the frontier leaves of ts and tt." ></td>
	<td class="line x" title="103:222	For example, Figure 2(c) shows a frontier tree pair." ></td>
	<td class="line x" title="104:222	Definition 9 A frontier tree pairts,tt,is said to be a subgraph of another frontier tree pair ts,tt,if and only if: 1." ></td>
	<td class="line x" title="105:222	root(ts) = root(ts); 2." ></td>
	<td class="line x" title="106:222	root(tt) = root(tt); 3." ></td>
	<td class="line x" title="107:222	ts is a subgraph of ts; 4." ></td>
	<td class="line x" title="108:222	tt is a subgraph of tt." ></td>
	<td class="line x" title="109:222	For example, the frontier tree pair shown in Figure 2(d) is a subgraph of that in Figure 2(c)." ></td>
	<td class="line x" title="110:222	Definition 10 A frontier tree pair is said to be minimal if and only if it is not a subgraph of any other frontier tree pair that shares with the same root." ></td>
	<td class="line x" title="111:222	For example, Figure 2(d) shows a minimal frontier tree pair." ></td>
	<td class="line x" title="112:222	Our goal is to find the minimal frontier tree pairs, which correspond to minimal tree-to-tree rules." ></td>
	<td class="line x" title="113:222	For example, the tree pair shown in Figure 2(d) denotes a minimal rule as follows: PP(x1:P,x2:NP-B)PP(x1:IN, x2:NP) Figure 3 shows the algorithm for identifying minimal frontier tree pairs." ></td>
	<td class="line x" title="114:222	The input is a source forest Fs, a target forest Ft, and a source frontier node v (line 1)." ></td>
	<td class="line x" title="115:222	We use a setP to store collected minimal frontier tree pairs (line 2)." ></td>
	<td class="line x" title="116:222	We first call the procedure FINDTREES(Fs,v) to identify a set of frontier trees rooted at v in Fs (line 3)." ></td>
	<td class="line x" title="117:222	For example, for the source frontier node PP4 in Figure 1, we obtain two frontier trees: (PP4(P11)(NP-B7)) (PP4(P11)(NP-B7(NR12))) Then, we try to find the set of corresponding target frontier trees (i.e., Tt)." ></td>
	<td class="line x" title="118:222	For each counterpart v of v (line 5), we call the procedure FINDTREES(Ft,v) to identify a set of frontier trees rooted at v in Ft (line 6)." ></td>
	<td class="line x" title="119:222	For example, the source 1: procedure FINDTREEPAIRS(Fs,Ft,v) 2: P= 3: TsFINDTREES(Fs,v) 4: Tt 5: for v (v) do 6: TtTtFINDTREES(Ft,v) 7: end for 8: forts,ttTsTt do 9: if ts tt then 10: PP{ts,tt,} 11: end if 12: end for 13: forts,tt,P do 14: if ts,tt,P : ts,tt, ts,tt,then 15: PP{ts,tt,} 16: end if 17: end for 18: end procedure Figure 3: Algorithm for identifying minimal frontier tree pairs." ></td>
	<td class="line x" title="120:222	frontier node PP4 has two counterparts on the target side: NP25 and PP26." ></td>
	<td class="line x" title="121:222	There are four target frontier trees rooted at the two nodes: (NP25(IN20)(NP24)) (NP25(IN20)(NP24(NNP21))) (PP26(IN20)(NP24)) (PP26(IN20)(NP24(NNP21))) Therefore, there are 24 = 8 pairs of trees." ></td>
	<td class="line x" title="122:222	We examine each tree pair ts,tt (line 8) to see whether it is a frontier tree pair (line 9) and then update P (line 10)." ></td>
	<td class="line x" title="123:222	In the above example, all the eight tree pairs are frontier tree pairs." ></td>
	<td class="line x" title="124:222	Finally, we keep only minimal frontier tree pairs in P (lines 13-15)." ></td>
	<td class="line x" title="125:222	As a result, we obtain the following two minimal frontier tree pairs for the source frontier node PP4: (PP4(P11)(NP-B7))(NP25(IN20)(NP24)) (PP4(P11)(NP-B7))(PP26(IN20)(NP24)) To maintain a reasonable rule table size, we restrict that the number of nodes in a tree of an STSG rule is no greater than n, which we refer to as maximal node count." ></td>
	<td class="line x" title="126:222	It seems more efficient to let the procedure FINDTREES(F,v) to search for minimal frontier 562 trees rather than frontier trees." ></td>
	<td class="line x" title="127:222	However, a minimal frontier tree pair is not necessarily a pair of minimal frontier trees." ></td>
	<td class="line x" title="128:222	On our Chinese-English corpus, we find that 38% of minimal frontier tree pairs are not pairs of minimal frontier trees." ></td>
	<td class="line x" title="129:222	As a result, we have to first collect all frontier tree pairs and then decide on the minimal ones." ></td>
	<td class="line x" title="130:222	Table 1 shows some minimal rules extracted from the forest pair shown in Figure 1." ></td>
	<td class="line x" title="131:222	3.3 Inferring Composed Rules After minimal rules are learned, composed rules can be obtained by composing two or more minimal rules." ></td>
	<td class="line x" title="132:222	For example, the composition of the second rule and the third rule in Table 1 produces a new rule: NP-B(NR(shalong))NP(NNP(Sharon)) While minimal rules derive from minimal frontier tree pairs, composed rules correspond to nonminimal frontier tree pairs." ></td>
	<td class="line x" title="133:222	3.4 Estimating Rule Probabilities We follow Mi and Huang (2008) to estimate the fractional count of a rule extracted from an aligned forest pair." ></td>
	<td class="line x" title="134:222	Intuitively, the relative frequency of a subtree that occurs in a forest is the sum of all the trees that traverse the subtree divided by the sum of all trees in the forest." ></td>
	<td class="line x" title="135:222	Instead of enumerating all trees explicitly and computing the sum of tree probabilities, we resort to inside and outside probabilities for efficient calculation: c(r) = p(ts)(root(ts)) producttext vleaves(ts) (v) (vs) p(tt)(root(tt)) producttext vleaves(tt) (v) (vt) where c(r) is the fractional count of a rule, ts is the source tree in r, tt is the target tree in r, root() a function that gets tree root, leaves() is a function that gets tree leaves, and (v) and (v) are outside and inside probabilities, respectively." ></td>
	<td class="line x" title="136:222	4 Decoding Given a source packed forest Fs, our decoder finds the target yield of the single best derivation d that has source yield of Ts(d)Fs: e = e parenleftBigg argmax d s.t. Ts(d)Fs p(d) parenrightBigg (2) We extend the model in Eq." ></td>
	<td class="line x" title="137:222	1 to a log-linear model (Och and Ney, 2002) that uses the following eight features: relative frequencies in two directions, lexical weights in two directions, number of rules used, language model score, number of target words produced, and the probability of matched source tree (Mi et al., 2008)." ></td>
	<td class="line x" title="138:222	Given a source parse forest and an STSG grammar G, we first apply the conversion algorithm proposed by Mi et al.(2008) to produce a translation forest." ></td>
	<td class="line x" title="140:222	The translation forest has a similar hypergraph structure." ></td>
	<td class="line x" title="141:222	While the nodes are the same as those of the parse forest, each hyperedge is associated with an STSG rule." ></td>
	<td class="line x" title="142:222	Then, the decoder runs on the translation forest." ></td>
	<td class="line x" title="143:222	We use the cube pruning method (Chiang, 2007) to approximately intersect the translation forest with the language model." ></td>
	<td class="line x" title="144:222	Traversing the translation forest in a bottom-up order, the decoder tries to build target parses at each node." ></td>
	<td class="line x" title="145:222	After the first pass, we use lazy Algorithm 3 (Huang and Chiang, 2005) to generate k-best translations for minimum error rate training." ></td>
	<td class="line x" title="146:222	5 Experiments 5.1 Data Preparation We evaluated our model on Chinese-to-English translation." ></td>
	<td class="line x" title="147:222	The training corpus contains 840K Chinese words and 950K English words." ></td>
	<td class="line x" title="148:222	A trigram language model was trained on the English sentences of the training corpus." ></td>
	<td class="line x" title="149:222	We used the 2002 NIST MT Evaluation test set as our development set, and used the 2005 NIST MT Evaluation test set as our test set." ></td>
	<td class="line x" title="150:222	We evaluated the translation quality using the BLEU metric, as calculated by mteval-v11b.pl with its default setting except that we used case-insensitive matching of n-grams." ></td>
	<td class="line x" title="151:222	To obtain packed forests, we used the Chinese parser (Xiong et al., 2005) modified by Haitao Mi and the English parser (Charniak and Johnson, 2005) modified by Liang Huang to produce entire parse forests." ></td>
	<td class="line x" title="152:222	Then, we ran the Python scripts (Huang, 2008) provided by Liang Huang to output packed forests." ></td>
	<td class="line x" title="153:222	To prune the packed forests, Huang (2008) uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyperedge away from the globally best derivation." ></td>
	<td class="line x" title="154:222	A hyperedge will be pruned away if the difference is greater than a threshold p. Nodes with all incoming hyperedges pruned are also pruned." ></td>
	<td class="line x" title="155:222	The greater the threshold p is, 563 p avg trees # of rules BLEU 0 1 73,614 0.20210.0089 2 238.94 105,214 0.21650.0081 5 5.78106 347,526 0.23360.0078 8 6.59107 573,738 0.23730.0082 10 1.05108 743,211 0.23850.0084 Table 3: Comparison of BLEU scores for treebased and forest-based tree-to-tree models." ></td>
	<td class="line x" title="156:222	0.04 0.05 0.06 0.07 0.08 0.09 0.10  0  1  2  3  4  5  6  7  8  9  10  11 coverage maximal node count p=0 p=2 p=5 p=8 p=10 Figure 4: Coverage of lexicalized STSG rules on bilingual phrases." ></td>
	<td class="line x" title="157:222	the more parses are encoded in a packed forest." ></td>
	<td class="line x" title="158:222	We obtained word alignments of the training data by first running GIZA++ (Och and Ney, 2003) and then applying the refinement rule grow-diagfinal-and (Koehn et al., 2003)." ></td>
	<td class="line x" title="159:222	5.2 Forests Vs. 1-best Trees Table 3 shows the BLEU scores of tree-based and forest-based tree-to-tree models achieved on the test set over different pruning thresholds." ></td>
	<td class="line x" title="160:222	p is the threshold for pruning packed forests, avg trees is the average number of trees encoded in one forest on the test set, and # of rules is the number of STSG rules used on the test set." ></td>
	<td class="line x" title="161:222	We restrict that both source and target trees in a tree-to-tree rule can contain at most 10 nodes (i.e., the maximal node count n = 10)." ></td>
	<td class="line x" title="162:222	The 95% confidence intervals were computed using Zhang s significance tester (Zhang et al., 2004)." ></td>
	<td class="line x" title="163:222	We chose five different pruning thresholds in our experiments: p = 0,2,5,8,10." ></td>
	<td class="line x" title="164:222	The forests pruned by p = 0 contained only 1-best tree per sentence." ></td>
	<td class="line x" title="165:222	With the increase of p, the average number of trees encoded in one forest rose dramatically." ></td>
	<td class="line x" title="166:222	When p was set to 10, there were over 100M parses encoded in one forest on average." ></td>
	<td class="line x" title="167:222	p extraction decoding 0 1.26 6.76 2 2.35 8.52 5 6.34 14.87 8 8.51 19.78 10 10.21 25.81 Table 4: Comparison of rule extraction time (seconds/1000 sentence pairs) and decoding time (second/sentence) Moreover, the more trees are encoded in packed forests, the more rules are made available to forest-based models." ></td>
	<td class="line x" title="168:222	The number of rules when p = 10 was almost 10 times of p = 0." ></td>
	<td class="line x" title="169:222	With the increase of the number of rules used, the BLEU score increased accordingly." ></td>
	<td class="line x" title="170:222	This suggests that packed forests enable tree-to-tree model to learn more useful rules on the training data." ></td>
	<td class="line x" title="171:222	However, when a pack forest encodes over 1M parses per sentence, the improvements are less significant, which echoes the results in (Mi et al., 2008)." ></td>
	<td class="line x" title="172:222	The forest-based tree-to-tree model outperforms the original model that uses 1-best trees dramatically." ></td>
	<td class="line x" title="173:222	The absolute improvement of 3.6 BLEU points (from 0.2021 to 0.2385) is statistically significant at p < 0.01 using the signtest as described by Collins et al.(2005), with 700(+1), 360(-1), and 15(0)." ></td>
	<td class="line x" title="175:222	We also ran Moses (Koehn et al., 2007) with its default setting using the same data and obtained a BLEU score of 0.2366, slightly lower than our best result (i.e., 0.2385)." ></td>
	<td class="line x" title="176:222	But this difference is not statistically significant." ></td>
	<td class="line x" title="177:222	5.3 Effect on Rule Coverage Figure 4 demonstrates the effect of pruning threshold and maximal node count on rule coverage." ></td>
	<td class="line x" title="178:222	We extracted phrase pairs from the training data to investigate how many phrase pairs can be captured by lexicalized tree-to-tree rules that contain only terminals." ></td>
	<td class="line x" title="179:222	We set the maximal length of phrase pairs to 10." ></td>
	<td class="line x" title="180:222	For tree-based tree-to-tree model, the coverage was below 8% even the maximal node count was set to 10." ></td>
	<td class="line x" title="181:222	This suggests that conventional tree-to-tree models lose over 92% linguistically unmotivated mappings due to hard syntactic constraints." ></td>
	<td class="line x" title="182:222	The absence of such nonsyntactic mappings prevents tree-based tree-totree models from achieving comparable results to phrase-based models." ></td>
	<td class="line x" title="183:222	With more parses included 564 0.09 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20  0  1  2  3  4  5  6  7  8  9  10  11 BLEU maximal node count Figure 5: Effect of maximal node count on BLEU scores." ></td>
	<td class="line x" title="184:222	in packed forests, the rule coverage increased accordingly." ></td>
	<td class="line x" title="185:222	When p = 10 and n = 10, the coverage was 9.7%, higher than that of p = 0." ></td>
	<td class="line x" title="186:222	As a result, packed forests enable tree-to-tree models to capture more useful source-target mappings and therefore improve translation quality." ></td>
	<td class="line x" title="187:222	2 5.4 Training and Decoding Time Table 4 gives the rule extraction time (seconds/1000 sentence pairs) and decoding time (second/sentence) with varying pruning thresholds." ></td>
	<td class="line x" title="188:222	We found that the extraction time grew faster than decoding time with the increase of p. One possible reason is that the number of frontier tree pairs (see Figure 3) rose dramatically when more parses were included in packed forests." ></td>
	<td class="line x" title="189:222	5.5 Effect of Maximal Node Count Figure 5 shows the effect of maximal node count on BLEU scores." ></td>
	<td class="line x" title="190:222	With the increase of maximal node count, the BLEU score increased dramatically." ></td>
	<td class="line x" title="191:222	This implies that allowing tree-to-tree rules to capture larger contexts will strengthen the expressive power of tree-to-tree model." ></td>
	<td class="line x" title="192:222	5.6 Results on Larger Data We also conducted an experiment on larger data to further examine the effectiveness of our approach." ></td>
	<td class="line x" title="193:222	We concatenated the small corpus we used above and the FBIS corpus." ></td>
	<td class="line x" title="194:222	After removing the sentences that we failed to obtain forests, 2Note that even we used packed forests, the rule coverage was still very low." ></td>
	<td class="line x" title="195:222	One reason is that we set the maximal phrase length to 10 words, while an STSG rule with 10 nodes in each tree usually cannot subsume 10 words." ></td>
	<td class="line x" title="196:222	the new training corpus contained about 260K sentence pairs with 7.39M Chinese words and 9.41M English words." ></td>
	<td class="line x" title="197:222	We set the forest pruning threshold p = 5." ></td>
	<td class="line x" title="198:222	Moses obtained a BLEU score of 0.3043 and our forest-based tree-to-tree system achieved a BLEU score of 0.3059." ></td>
	<td class="line x" title="199:222	The difference is still not significant statistically." ></td>
	<td class="line x" title="200:222	6 Related Work In machine translation, the concept of packed forest is first used by Huang and Chiang (2007) to characterize the search space of decoding with language models." ></td>
	<td class="line x" title="201:222	The first direct use of packed forest is proposed by Mi et al.(2008)." ></td>
	<td class="line x" title="203:222	They replace 1-best trees with packed forests both in training and decoding and show superior translation quality over the state-of-the-art hierarchical phrasebased system." ></td>
	<td class="line x" title="204:222	We follow the same direction and apply packed forests to tree-to-tree translation." ></td>
	<td class="line x" title="205:222	Zhang et al.(2008) present a tree-to-tree model that uses STSG." ></td>
	<td class="line x" title="207:222	To capture non-syntactic phrases, they apply tree-sequence rules (Liu et al., 2007) to tree-to-tree models." ></td>
	<td class="line x" title="208:222	Their extraction algorithm first identifies initial rules and then obtains abstract rules." ></td>
	<td class="line x" title="209:222	While this method works for 1-best tree pairs, it cannot be applied to packed forest pairs because it is impractical to enumerate all tree pairs over a phrase pair." ></td>
	<td class="line oc" title="210:222	While Galley (2004) describes extracting treeto-string rules from 1-best trees, Mi and Huang et al.(2008) go further by proposing a method for extracting tree-to-string rules from aligned foreststring pairs." ></td>
	<td class="line x" title="212:222	We follow their work and focus on identifying tree-tree pairs in a forest pair, which is more difficult than the tree-to-string case." ></td>
	<td class="line x" title="213:222	7 Conclusion We have shown how to improve tree-to-tree translation with packed forests, which compactly encode exponentially many parses." ></td>
	<td class="line x" title="214:222	To learn STSG rules from aligned forest pairs, we first identify minimal rules and then get composed rules." ></td>
	<td class="line x" title="215:222	The decoder finds the best derivation that have the source yield of one source tree in the forest." ></td>
	<td class="line x" title="216:222	Experiments show that using packed forests in treeto-tree translation results in dramatic improvements over using 1-best trees." ></td>
	<td class="line x" title="217:222	Our system also achieves comparable performance with the stateof-the-art phrase-based system Moses." ></td>
	<td class="line x" title="218:222	565 Acknowledgement The authors were supported by National Natural Science Foundation of China, Contracts 60603095 and 60736014, and 863 State Key Project No. 2006AA010108." ></td>
	<td class="line x" title="219:222	Part of this work was done while Yang Liu was visiting the SMT group led by Stephan Vogel at CMU." ></td>
	<td class="line x" title="220:222	We thank the anonymous reviewers for their insightful comments." ></td>
	<td class="line x" title="221:222	Many thanks go to Liang Huang, Haitao Mi, and Hao Xiong for their invaluable help in producing packed forests." ></td>
	<td class="line x" title="222:222	We are also grateful to Andreas Zollmann, Vamshi Ambati, and Kevin Gimpel for their helpful feedback." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1064
Fast Consensus Decoding over Translation Forests
DeNero, John;Chiang, David;Knight, Kevin;"></td>
	<td class="line x" title="1:221	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 567575, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:221	c2009 ACL and AFNLP Fast Consensus Decoding over Translation Forests John DeNero David Chiang and Kevin Knight Computer Science Division Information Sciences Institute University of California, Berkeley University of Southern California denero@cs.berkeley.edu {chiang, knight}@isi.edu Abstract The minimum Bayes risk (MBR) decoding objectiveimprovesBLEUscoresformachinetranslation output relative to the standard Viterbi objective of maximizing model score." ></td>
	<td class="line x" title="3:221	However, MBRtargetingBLEUisprohibitivelyslowtooptimize over k-best lists for large k. In this paper, we introduce and analyze an alternative to MBR that is equally effective at improving performance, yet is asymptotically faster  running 80 times faster than MBR in experiments with 1000-best lists." ></td>
	<td class="line x" title="4:221	Furthermore, our fast decoding procedure can select output sentences based on distributions over entire forests of translations, in addition to k-best lists." ></td>
	<td class="line x" title="5:221	We evaluate our procedure on translation forests from two large-scale, state-of-the-art hierarchical machine translation systems." ></td>
	<td class="line x" title="6:221	Our forest-based decoding objective consistently outperformsk-best list MBR, giving improvements of up to 1.0 BLEU." ></td>
	<td class="line x" title="7:221	1 Introduction In statistical machine translation, output translations are evaluated by their similarity to human reference translations, where similarity is most often measured by BLEU (Papineni et al., 2002)." ></td>
	<td class="line x" title="8:221	A decoding objective specifies how to derive final translations from a systems underlying statistical model." ></td>
	<td class="line x" title="9:221	The Bayes optimal decoding objective is to minimize risk based on the similarity measure used for evaluation." ></td>
	<td class="line x" title="10:221	The corresponding minimum Bayes risk (MBR) procedure maximizes the expected similarity score of a systems translations relative to the models distribution over possible translations (Kumar and Byrne, 2004)." ></td>
	<td class="line x" title="11:221	Unfortunately, with a non-linear similarity measure like BLEU, we must resort to approximating the expected loss using a k-best list, which accounts for only a tiny fraction of a models full posterior distribution." ></td>
	<td class="line x" title="12:221	In this paper, we introduce a variant of the MBR decoding procedure that applies efficiently to translation forests." ></td>
	<td class="line x" title="13:221	Instead of maximizing expected similarity, we express similarity in terms of features of sentences, and choose translations that are similar to expected feature values." ></td>
	<td class="line x" title="14:221	Our exposition begins with algorithms over kbest lists." ></td>
	<td class="line x" title="15:221	A nave algorithm for finding MBR translationscomputesthesimilaritybetweenevery pair of k sentences, entailing O(k2) comparisons." ></td>
	<td class="line x" title="16:221	We show that if the similarity measure is linear in features of a sentence, then computing expected similarity for all k sentences requires only k similarity evaluations." ></td>
	<td class="line x" title="17:221	Specific instances of this generalalgorithmhaverecentlybeenproposedfortwo linear similarity measures (Tromble et al., 2008; Zhang and Gildea, 2008)." ></td>
	<td class="line x" title="18:221	However, the sentence similarity measures we want to optimize in MT are not linear functions, and so this fast algorithm for MBR does not apply." ></td>
	<td class="line x" title="19:221	For this reason, we propose a new objective that retains the benefits of MBR, but can be optimized efficiently, even for non-linear similarity measures." ></td>
	<td class="line x" title="20:221	In experiments using BLEU over 1000best lists, we found that our objective provided benefits very similar to MBR, only much faster." ></td>
	<td class="line x" title="21:221	This same decoding objective can also be computed efficiently from forest-based expectations." ></td>
	<td class="line x" title="22:221	Translation forests compactly encode distributions over much larger sets of derivations and arise naturally in chart-based decoding for a wide variety of hierarchical translation systems (Chiang, 2007; Galley et al., 2006; Mi et al., 2008; Venugopal et al., 2007)." ></td>
	<td class="line x" title="23:221	The resulting forest-based decoding procedure compares favorably in both complexity and performance to the recently proposed latticebased MBR (Tromble et al., 2008)." ></td>
	<td class="line x" title="24:221	The contributions of this paper include a lineartime algorithm for MBR using linear similarities, a linear-time alternative to MBR using non-linear similarity measures, and a forest-based extension to this procedure for similarities based on n-gram counts." ></td>
	<td class="line x" title="25:221	In experiments, we show that our fast procedure is on average 80 times faster than MBR using 1000-best lists." ></td>
	<td class="line x" title="26:221	We also show that using forests outperforms using k-best lists consistently across language pairs." ></td>
	<td class="line x" title="27:221	Finally, in the first published multi-system experiments on consensus de567 coding for translation, we demonstrate that benefits can differ substantially across systems." ></td>
	<td class="line x" title="28:221	In all, we show improvements of up to 1.0 BLEU from consensus approaches for state-of-the-art largescale hierarchical translation systems." ></td>
	<td class="line x" title="29:221	2 Consensus Decoding Algorithms Let e be a candidate translation for a sentence f, where e may stand for a sentence or its derivation as appropriate." ></td>
	<td class="line x" title="30:221	Modern statistical machine translation systems take as input somef and score each derivation e according to a linear model of features:summationtexti ii(f,e)." ></td>
	<td class="line x" title="31:221	The standard Viterbi decoding objective is to find e = arg maxe (f,e)." ></td>
	<td class="line x" title="32:221	For MBR decoding, we instead leverage a similarity measure S(e;eprime) to choose a translation using the models probability distribution P(e|f), which has support over a set of possible translations E. The Viterbi derivation e is the mode of this distribution." ></td>
	<td class="line x" title="33:221	MBR is meant to choose a translation that will be similar, on expectation, to any possible reference translation." ></td>
	<td class="line x" title="34:221	To this end, MBR chooses e that maximizes expected similarity to the sentences in E under P(e|f):1 e = arg maxe EP(eprime|f)bracketleftbigS(e;eprime)bracketrightbig = arg maxe summationdisplay eprimeE P(eprime|f)S(e;eprime) MBR can also be interpreted as a consensus decoding procedure: it chooses a translation similar to other high-posterior translations." ></td>
	<td class="line x" title="35:221	Minimizing risk has been shown to improve performance for MT (Kumar and Byrne, 2004), as well as other language processing tasks (Goodman, 1996; Goel and Byrne, 2000; Kumar and Byrne, 2002; Titov and Henderson, 2006; Smith and Smith, 2007)." ></td>
	<td class="line x" title="36:221	The distribution P(e|f) can be induced from a translation systems features and weights by exponentiating with base b to form a log-linear model: P(e|f) = b (f,e) summationtext eprimeE b(f,e prime) We follow Ehling et al.(2007) in choosing b using a held-out tuning set." ></td>
	<td class="line x" title="38:221	For algorithms in this section, we assume that E is a k-best list and b has been chosen already, so P(e|f) is fully specified." ></td>
	<td class="line x" title="39:221	1Typically, MBR is defined as arg min eEE[L(e;e prime)] for some loss function L, for example 1  BLEU(e;eprime)." ></td>
	<td class="line x" title="40:221	These definitions are equivalent." ></td>
	<td class="line x" title="41:221	2.1 Minimum Bayes Risk over Sentence Pairs Given any similarity measure S and a k-best list E, the minimum Bayes risk translation can be found by computing the similarity between all pairs of sentences in E, as in Algorithm 1." ></td>
	<td class="line x" title="42:221	Algorithm 1 MBR over Sentence Pairs 1: A 2: for eE do 3: Ae0 4: for eprime E do 5: AeAe + P(eprime|f)S(e;eprime) 6: if Ae > A then A,eAe,e 7: return e We can sometimes exit the inner for loop early, whenever Ae can never become larger than A (Ehling et al., 2007)." ></td>
	<td class="line x" title="43:221	Even with this shortcut, the running time of Algorithm 1 is O(k2n), where n is the maximum sentence length, assuming that S(e;eprime) can be computed in O(n) time." ></td>
	<td class="line x" title="44:221	2.2 Minimum Bayes Risk over Features We now consider the case when S(e;eprime) is a linear function of sentence features." ></td>
	<td class="line x" title="45:221	Let S(e;eprime) be a function of the form summationtextj j(e)j(eprime), where j(eprime) are real-valued features of eprime, and j(e) are sentence-specific weights on those features." ></td>
	<td class="line x" title="46:221	Then, the MBR objective can be re-written as arg maxeE EP(eprime|f)bracketleftbigS(e;eprime)bracketrightbig = arg maxe summationdisplay eprimeE P(eprime|f) summationdisplay j j(e)j(eprime) = arg maxe summationdisplay j j(e) bracketleftBiggsummationdisplay eprimeE P(eprime|f)j(eprime) bracketrightBigg = arg maxe summationdisplay j j(e)EP(eprime|f)bracketleftbigj(eprime)bracketrightbig." ></td>
	<td class="line x" title="47:221	(1) Equation 1 implies that we can find MBR translations by first computing all feature expectations, then applying S only once for each e. Algorithm 2 proceduralizes this idea: lines 1-4 compute feature expectations, and lines 5-11 find the translation with highest S relative to those expectations." ></td>
	<td class="line x" title="48:221	The time complexity is O(kn), assuming the number of non-zero features (eprime) and weights (e) grow linearly in sentence length n and all features and weights can be computed in constant time." ></td>
	<td class="line x" title="49:221	568 Algorithm 2 MBR over Features 1: [0 for jJ] 2: for eprime E do 3: for jJ such that j(eprime)negationslash= 0 do 4: j  j + P(eprime|f)j(eprime) 5: A 6: for eE do 7: Ae0 8: for jJ such that j(e)negationslash= 0 do 9: AeAe + j(e) j 10: if Ae > A then A,eAe,e 11: return e An example of a linear similarity measure is bag-of-words precision, which can be written as: U(e;eprime) = summationdisplay tT1 (e,t) |e| (e prime,t) where T1 is the set of unigrams in the language, and (e,t) is an indicator function that equals 1 if t appears in e and 0 otherwise." ></td>
	<td class="line x" title="50:221	Figure 1 compares Algorithms 1 and 2 using U(e;eprime)." ></td>
	<td class="line x" title="51:221	Other linear functions have been explored for MBR, including Taylor approximations to the logarithm of BLEU (Tromble et al., 2008) and counts of matching constituents (Zhang and Gildea, 2008), which are discussed further in Section 3.3." ></td>
	<td class="line x" title="52:221	2.3 Fast Consensus Decoding using Non-Linear Similarity Measures Most similarity measures of interest for machine translation are not linear, and so Algorithm 2 does not apply." ></td>
	<td class="line x" title="53:221	Computing MBR even with simple non-linear measures such as BLEU, NIST or bagof-words F1 seems to require O(k2) computation time." ></td>
	<td class="line x" title="54:221	However, these measures are all functions of features of eprime." ></td>
	<td class="line x" title="55:221	That is, they can be expressed as S(e;(eprime)) for a feature mapping  : ERn." ></td>
	<td class="line x" title="56:221	For example, we can express BLEU(e;eprime) = exp ' 1  |e prime| |e|   + 14 4X n=1 ln P tTn min(c(e,t),c(e prime,t)) P tTn c(e,t) # In this expression, BLEU(e;eprime) references eprime only via its n-gram count features c(eprime,t).2 2The length penalty  1  |eprime||e|   is also a function of ngram counts: |eprime| = PtT1 c(eprime,t)." ></td>
	<td class="line x" title="57:221	The negative part operator () is equivalent to min(,0)." ></td>
	<td class="line x" title="58:221	Choose a distribution P over a set of translations E MBR over Sentence Pairs Compute pairwise similarityCompute expectations Max expected similarity Max feature similarity 3/31/42/5 1/34/40/5 2/30/45/5 MBR over Features E[(efficient)] = 0.6 E[(forest)] = 0.7 E[(decoding)] = 0.7 E[(for)] = 0.3 E[(rusty)] = 0.3 E[(coating)]= 0.3 E[(a)] = 0.4 E[(fish)] = 0.4 E[(aint)] = 0.4 c 1 c 2 c 3 r 1 r 2 r 3 1 2 3 2 3 I  telescope Yo vi al hombrecon el telescopio I  sawthe  manwith  telescope the  telescope 0.4 saw the man with 0.6 saw the 1.0 man with E[r(manwith)]=0.4+0.6 1.0 50.0 50.2 50.4 50.6 50.8 511,660 513,245 514,830 Total model score for 1000 translations Corpus BLEU 0 22.5 45.0 67.5 90.0 Hiero SBMT 70.2 84.6 56.6 61.4 51.150.5 Viterbi n-gram precision Forest n-gram precision at Viterbi recall Forest n-gram precision for Er(t) ! 1 Forest samples (b!2) Forest samples (b!5) Viterbi translations U(e 2 ;e 1 )= |efficient| |efficient forrusty coating| EU(e 1 ;e prime )=0.3(1+ 1 3 )+0.4 2 3 =0.667 EU(e 2 ;e prime )=0.375 EU(e 3 ;e prime )=0.520 U(e 1 ;E)= 0.6+0.7+0.7 3 =0.667 U(e 2 ;E)=0.375 U(e 3 ;E)=0.520 P(e 1 |f) =0.3;e 1 =efficient forestdecoding P(e 2 |f) =0.3;e 2 =efficient forrusty coating P(e 3 |f) =0.4;e 3 =Afish aintforestdecoding Figure 1: For the linear similarity measure U(e;eprime), which computes unigram precision, the MBR translation can be found by iterating either over sentence pairs (Algorithm 1) or over features (Algorithm 2)." ></td>
	<td class="line x" title="59:221	These two algorithms take the same input (step 1), but diverge in their consensus computations (steps 2 & 3)." ></td>
	<td class="line x" title="60:221	However, they produce identical results for U and any other linear similarity measure." ></td>
	<td class="line x" title="61:221	Following the structure of Equation 1, we can choose a translation e based on the feature expectations of eprime." ></td>
	<td class="line x" title="62:221	In particular, we can choose e = arg maxeES(e;EP(eprime|f)bracketleftbig(eprime)bracketrightbig)." ></td>
	<td class="line x" title="63:221	(2) This objective differs from MBR, but has a similar consensus-building structure." ></td>
	<td class="line x" title="64:221	We have simply moved the expectation inside the similarity function, just as we did in Equation 1." ></td>
	<td class="line x" title="65:221	This new objective can be optimized by Algorithm 3, a procedure that runs in O(kn) time if the count of non-zero features in eprime and the computation time of S(e;(eprime)) are both linear in sentence length n. This fast consensus decoding procedure shares the same structure as linear MBR: first we compute feature expectations, then we choose the sentence that is most similar to those expectations." ></td>
	<td class="line x" title="66:221	In fact, Algorithm 2 is a special case of Algorithm 3." ></td>
	<td class="line x" title="67:221	Lines 7-9 of the former and line 7 of the latter are equivalent for linear S(e;eprime)." ></td>
	<td class="line x" title="68:221	Thus, for any linear similarity measure, Algorithm 3 is an algorithm for minimum Bayes risk decoding." ></td>
	<td class="line x" title="69:221	569 Algorithm 3 Fast Consensus Decoding 1: [0 for jJ] 2: for eprime E do 3: for jJ such that j(eprime)negationslash= 0 do 4: j  j + P(eprime|f)j(eprime) 5: A 6: for eE do 7: AeS(e; ) 8: if Ae > A then A,eAe,e 9: return e As described, Algorithm 3 can use any similarity measure that is defined in terms of realvalued features of eprime." ></td>
	<td class="line x" title="70:221	There are some nuances of this procedure, however." ></td>
	<td class="line x" title="71:221	First, the precise form of S(e;(eprime)) will affect the output, but S(e;E[(eprime)]) is often an input point for which a sentence similarity measure S was not originally defined." ></td>
	<td class="line x" title="72:221	For example, our definition of BLEU above will have integer valued (eprime) for any real sentenceeprime,butE[(eprime)]willnotbeintegervalued." ></td>
	<td class="line x" title="73:221	As a result, we are extending the domain of BLEU beyond its original intent." ></td>
	<td class="line x" title="74:221	One could imagine different feature-based expressions that also produce BLEU scores for real sentences, but produce different values for fractional features." ></td>
	<td class="line x" title="75:221	Some care must be taken to define S(e;(eprime)) to extend naturally from integer-valued to real-valued features." ></td>
	<td class="line x" title="76:221	Second, while any similarity measure can in principle be expressed as S(e;(eprime)) for a sufficiently rich feature space, fast consensus decoding will not apply effectively to all functions." ></td>
	<td class="line x" title="77:221	For instance, we cannot naturally use functions that include alignments or matchings between e and eprime, such as METEOR (Agarwal and Lavie, 2007) and TER (Snover et al., 2006)." ></td>
	<td class="line x" title="78:221	Though these functions can in principle be expressed in terms of features ofeprime (forinstancewithindicatorfeaturesforwhole sentences), fast consensus decoding will only be effective if different sentences share many features, so that the feature expectations effectively capture trends in the underlying distribution." ></td>
	<td class="line x" title="79:221	3 Computing Feature Expectations We now turn our focus to efficiently computing feature expectations, in service of our fast consensus decoding procedure." ></td>
	<td class="line x" title="80:221	Computing feature expectations from k-best lists is trivial, but k-best lists capture very little of the underlying models posterior distribution." ></td>
	<td class="line x" title="81:221	In place of k-best Choose a distribution P over a set of translations E MBR over Sentence Pairs Compute pairwise similarityCompute expectations Max expected similarity Max feature similarity 3/3 1/4 2/5 1/3 4/4 0/5 2/3 0/4 5/5 MBR over Features E[(efficient)] = 0.6 E[(forest)] = 0.7 E[(decoding)] = 0.7 E[(for)] = 0.3 E[(rusty)] = 0.3 E[(coating)] = 0.3 E[(a)] = 0.4 E[(fish)] = 0.4 E[(aint)] = 0.4 c 1 c 2 c 3 r 1 r 2 r 3 1 2 3 2 3 50.0 50.2 50.4 50.6 50.8 511,660 513,245 514,830 Total model score for 1000 translations Corpus BLEU 0 20 40 60 80 Hiero SBMT 56.6 61.4 51.150.5 N-grams from baseline translation N-grams with high expected count Forest samples (b!2) Forest samples (b!5) Viterbi translations U(e 2 ;e 1 )= |efficient| |efficient for rusty coating| EU(e 1 ;e prime )=0.3(1+ 1 3 )+0.4 2 3 =0.667 EU(e 2 ;e prime )=0.375 EU(e 3 ;e prime )=0.520 U(e 1 ;E)= 0.6+0.7+0.7 3 =0.667 U(e 2 ;E)=0.375 U(e 3 ;E)=0.520 P(e 1 |f) =0.3;e 1 =efficient forest decoding P(e 2 |f) =0.3;e 2 =efficient forrusty coating P(e 3 |f) =0.4;e 3 =A fish aintforest decoding I  telescope Yo vi al hombre con el telescopio I  saw the  man with  telescope the  telescope 0.4 saw the man with 0.6 saw the 1.0 man with E[c(e,manwith)] = summationdisplay h P(h|f)  c(h,manwith) =0.4  1+(0.6  1.0) 1Figure 2: This translation forest for a Spanish sentence encodes two English parse trees." ></td>
	<td class="line x" title="82:221	Hyper-edges (boxes) are annotated with normalized transition probabilities, as well as the bigrams produced by each rule application." ></td>
	<td class="line x" title="83:221	The expected count of the bigram man with is the sum of posterior probabilities of the two hyper-edges that produce it." ></td>
	<td class="line x" title="84:221	In this example, we normalized inside scores at all nodes to 1 for clarity." ></td>
	<td class="line x" title="85:221	lists, compact encodings of translation distributions have proven effective for MBR (Zhang and Gildea, 2008; Tromble et al., 2008)." ></td>
	<td class="line x" title="86:221	In this section, we consider BLEU in particular, for which the relevant features (e) are n-gram counts up to length n = 4." ></td>
	<td class="line x" title="87:221	We show how to compute expectations of these counts efficiently from translation forests." ></td>
	<td class="line x" title="88:221	3.1 Translation Forests Translation forests compactly encode an exponential number of output translations for an input sentence, along with their model scores." ></td>
	<td class="line x" title="89:221	Forests arise naturally in chart-based decoding procedures for many hierarchical translation systems (Chiang, 2007)." ></td>
	<td class="line x" title="90:221	Exploiting forests has proven a fruitful avenue of research in both parsing (Huang, 2008) and machine translation (Mi et al., 2008)." ></td>
	<td class="line x" title="91:221	Formally, translation forests are weighted acyclic hyper-graphs." ></td>
	<td class="line x" title="92:221	The nodes are states in the decoding process that include the span (i,j) of the sentence to be translated, the grammar symbol s over that span, and the left and right context words of the translation relevant for computing n-gram language model scores.3 Each hyper-edge h represents the application of a synchronous ruler that combinesnodescorrespondingtonon-terminalsin 3Decoder states can include additional information as well, such as local configurations for dependency language model scoring." ></td>
	<td class="line x" title="93:221	570 r into a node spanning the union of the child spans and perhaps some additional portion of the input sentence covered directly byrs lexical items." ></td>
	<td class="line x" title="94:221	The weight of h is the incremental score contributed to all translations containing the rule application, including translation model features on r and language model features that depend on both r and the English contexts of the child nodes." ></td>
	<td class="line x" title="95:221	Figure 2 depicts a forest." ></td>
	<td class="line x" title="96:221	Eachn-gramthatappearsinatranslationeisassociated with some h in its derivation: the h correspondingtotherulethatproducesthen-gram." ></td>
	<td class="line x" title="97:221	Unigramsareproducedbylexicalrules, whilehigherorder n-grams can be produced either directly by lexical rules, or by combining constituents." ></td>
	<td class="line x" title="98:221	The n-gram language model score of e similarly decomposes over the h in e that produce n-grams." ></td>
	<td class="line x" title="99:221	3.2 Computing Expected N-Gram Counts We can compute expected n-gram counts efficiently from a translation forest by appealing to the linearity of expectations." ></td>
	<td class="line x" title="100:221	Let (e) be a vector of n-gram counts for a sentence e. Then, (e) is the sum of hyper-edge-specific n-gram count vectors (h) for all h in e. Therefore, E[(e)] =summationtext he E[(h)]." ></td>
	<td class="line x" title="101:221	To compute n-gram expectations for a hyperedge, we first compute the posterior probability of each h, conditioned on the input sentence f: P(h|f) = parenleftBiggsummationdisplay e:he b(f,e) parenrightBiggparenleftBiggsummationdisplay e b(f,e) parenrightBigg1 , where e iterates over translations in the forest." ></td>
	<td class="line x" title="102:221	We computethenumeratorusingtheinside-outsidealgorithm, while the denominator is the inside score of the root node." ></td>
	<td class="line x" title="103:221	Note that many possible derivationsoff areprunedfromtheforestduringdecoding, and so this posterior is approximate." ></td>
	<td class="line x" title="104:221	The expected n-gram count vector for a hyperedge is E[(h)] = P(h|f)(h)." ></td>
	<td class="line x" title="105:221	Hence, after computing P(h|f) for every h, we need only sum P(h|f)(h) for all h to compute E[(e)]." ></td>
	<td class="line x" title="106:221	This entire procedure is a linear-time computation in the number of hyper-edges in the forest." ></td>
	<td class="line x" title="107:221	To complete forest-based fast consensus decoding, we then extract a k-best list of unique translations from the forest (Huang et al., 2006) and continue Algorithm 3 from line 5, which chooses the e from the k-best list that maximizes BLEU(e;E[(eprime)])." ></td>
	<td class="line x" title="108:221	3.3 Comparison to Related Work Zhang and Gildea (2008) embed a consensus decodingprocedureintoalargermulti-passdecoding framework." ></td>
	<td class="line x" title="109:221	They focus on inversion transduction grammars,buttheirideasapplytorichermodelsas well." ></td>
	<td class="line x" title="110:221	They propose an MBR decoding objective of maximizing the expected number of matching constituent counts relative to the models distribution." ></td>
	<td class="line x" title="111:221	The corresponding constituent-matching similarity measure can be expressed as a linear function of features of eprime, which are indicators of constituents." ></td>
	<td class="line x" title="112:221	Expectations of constituent indicator featuresarethesameasposteriorconstituentprobabilities, which can be computed from a translationforestusingtheinside-outsidealgorithm." ></td>
	<td class="line x" title="113:221	This forest-based MBR approach improved translation output relative to Viterbi translations." ></td>
	<td class="line x" title="114:221	Tromble et al.(2008) describe a similar approach using MBR with a linear similarity measure." ></td>
	<td class="line x" title="116:221	They derive a first-order Taylor approximation to the logarithm of a slightly modified definition of corpus BLEU4, which is linear in n-gram indicator features (eprime,t) of eprime." ></td>
	<td class="line x" title="117:221	These features are weighted by n-gram counts c(e,t) and constants  that are estimated from held-out data." ></td>
	<td class="line x" title="118:221	The linear similarity measure takes the following form, where Tn is the set of n-grams: G(e;eprime) = 0|e|+ 4summationdisplay n=1 summationdisplay tTn tc(e,t)(eprime,t)." ></td>
	<td class="line x" title="119:221	Using G, Tromble et al.(2008) extend MBR to word lattices, which improves performance over k-best list MBR." ></td>
	<td class="line x" title="121:221	Our approach differs from Tromble et al.(2008) primarily in that we propose decoding with an alternativetoMBRusingBLEU,whiletheypropose decoding with MBR using a linear alternative to BLEU." ></td>
	<td class="line x" title="123:221	The specifics of our approaches also differ in important ways." ></td>
	<td class="line x" title="124:221	First, word lattices are a subclass of forests that have only one source node for each edge (i.e., a graph, rather than a hyper-graph)." ></td>
	<td class="line x" title="125:221	While forests are more general, the techniques for computing posterior edge probabilities in lattices and forests are similar." ></td>
	<td class="line x" title="126:221	One practical difference is that the forests needed for fast consensus decoding are 4The log-BLEU function must be modified slightly to yield a linear Taylor approximation: Tromble et al.(2008) replace the clipped n-gram count with the product of an ngram count and an n-gram indicator function." ></td>
	<td class="line x" title="128:221	571 generated already by the decoder of a syntactic translation system." ></td>
	<td class="line x" title="129:221	Second, rather than use BLEU as a sentencelevel similarity measure directly, Tromble et al.(2008) approximate corpus BLEU with G above." ></td>
	<td class="line x" title="131:221	Theparameters oftheapproximationmustbeestimated on a held-out data set, while our approach requires no such estimation step." ></td>
	<td class="line x" title="132:221	Third, our approach is also simpler computationally." ></td>
	<td class="line x" title="133:221	The features required to compute G are indicators (eprime,t); the features relevant to us are counts c(eprime,t)." ></td>
	<td class="line x" title="134:221	Tromble et al.(2008) compute expected feature values by intersecting the translation lattice with a lattices for each n-gram t. By contrast, expectations of c(eprime,t) can all be computed with a single pass over the forest." ></td>
	<td class="line x" title="136:221	This contrastimpliesacomplexitydifference." ></td>
	<td class="line x" title="137:221	LetH bethe number of hyper-edges in the forest or lattice, and T the number of n-grams that can potentially appear in a translation." ></td>
	<td class="line x" title="138:221	Computing indicator expectations seems to require O(HT) time because of automata intersections." ></td>
	<td class="line x" title="139:221	Computing count expectations requires O(H) time, because only a constant number of n-grams can be produced by each hyper-edge." ></td>
	<td class="line x" title="140:221	Our approaches also differ in the space of translations from which e is chosen." ></td>
	<td class="line x" title="141:221	A linear similarity measure like G allows for efficient search over the lattice or forest, whereas fast consensus decoding restricts this search to a k-best list." ></td>
	<td class="line x" title="142:221	However, Tromble et al.(2008) showed that most of the improvement from lattice-based consensus decoding comes from lattice-based expectations, not search: searching over lattices instead of k-best lists did not change results for two language pairs, and improved a third language pair by 0.3 BLEU." ></td>
	<td class="line x" title="144:221	Thus, we do not consider our use of k-best lists to be a substantial liability of our approach." ></td>
	<td class="line x" title="145:221	Fast consensus decoding is also similar in character to the concurrently developed variational decoding approach of Li et al.(2009)." ></td>
	<td class="line x" title="147:221	Using BLEU, both approaches choose outputs that match expected n-gram counts from forests, though differ in the details." ></td>
	<td class="line x" title="148:221	It is possible to define a similarity measure under which the two approaches are equivalent.5 5For example, decoding under a variational approximation to the models posterior that decomposes over bigram probabilities is equivalent to fast consensus decoding with the similarity measure B(e;eprime) = QtT2 h c(eprime,t) c(eprime,h(t)) ic(e,t) , where h(t) is the unigram prefix of bigram t. 4 Experimental Results We evaluate these consensus decoding techniques on two different full-scale state-of-the-art hierarchical machine translation systems." ></td>
	<td class="line x" title="149:221	Both systems were trained for 2008 GALE evaluations, in which they outperformed a phrase-based system trained on identical data." ></td>
	<td class="line x" title="150:221	4.1 Hiero: a Hierarchical MT Pipeline Hiero is a hierarchical system that expresses its translation model as a synchronous context-free grammar (Chiang, 2007)." ></td>
	<td class="line x" title="151:221	No explicit syntactic information appears in the core model." ></td>
	<td class="line x" title="152:221	A phrase discovery procedure over word-aligned sentence pairs provides rule frequency counts, which are normalized to estimate features on rules." ></td>
	<td class="line x" title="153:221	The grammar rules of Hiero all share a single non-terminal symbol X, and have at most two non-terminals and six total items (non-terminals and lexical items), for example: my X2 s X1X1 de mi X2 Weextractedthegrammarfromtrainingdatausing standard parameters." ></td>
	<td class="line x" title="154:221	Rules were allowed to span at most 15 words in the training data." ></td>
	<td class="line x" title="155:221	The log-linear model weights were trained using MIRA, a margin-based optimization procedure that accommodates many features (Crammer and Singer, 2003; Chiang et al., 2008)." ></td>
	<td class="line x" title="156:221	In addition to standard rule frequency features, we included the distortion and syntactic features described in Chiang et al.(2008)." ></td>
	<td class="line x" title="158:221	4.2 SBMT: a Syntax-Based MT Pipeline SBMT is a string-to-tree translation system with rich target-side syntactic information encoded in the translation model." ></td>
	<td class="line oc" title="159:221	The synchronous grammar rules are extracted from word aligned sentence pairs where the target sentence is annotated with a syntactic parse (Galley et al., 2004)." ></td>
	<td class="line x" title="160:221	Rules map source-side strings to target-side parse tree fragments, and non-terminal symbols correspond to target-side grammatical categories: (NP (NP (PRP$ my) NN2 (POS s)) NNS1) NNS1 de mi NN2 We extracted the grammar via an array of criteria (Galley et al., 2006; DeNeefe et al., 2007; Marcu et al., 2006)." ></td>
	<td class="line x" title="161:221	The model was trained using minimum error rate training for Arabic (Och, 2003) and MIRA for Chinese (Chiang et al., 2008)." ></td>
	<td class="line x" title="162:221	572 Arabic-English Objective Hiero SBMT Min." ></td>
	<td class="line x" title="163:221	Bayes Risk (Alg 1) 2h 47m 12h 42m Fast Consensus (Alg 3) 5m 49s 5m 22s Speed Ratio 29 142 Chinese-English Objective Hiero SBMT Min." ></td>
	<td class="line x" title="164:221	Bayes Risk (Alg 1) 10h 24m 3h 52m Fast Consensus (Alg 3) 4m 52s 6m 32s Speed Ratio 128 36 Table 1: Fast consensus decoding is orders of magnitude faster than MBR when using BLEU as a similarity measure." ></td>
	<td class="line x" title="165:221	Times only include reranking, not k-best list extraction." ></td>
	<td class="line x" title="166:221	4.3 Data Conditions We evaluated on both Chinese-English and Arabic-English translation tasks." ></td>
	<td class="line x" title="167:221	Both ArabicEnglish systems were trained on 220 million words of word-aligned parallel text." ></td>
	<td class="line x" title="168:221	For the Chinese-English experiments, we used 260 million words of word-aligned parallel text; the hierarchical system used all of this data, and the syntax-based system used a 65-million word subset." ></td>
	<td class="line x" title="169:221	All four systems used two language models: one trained from the combined English sides of both parallel texts, and another, larger, language model trained on 2 billion words of English text (1 billion for Chinese-English SBMT)." ></td>
	<td class="line x" title="170:221	All systems were tuned on held-out data (1994 sentences for Arabic-English, 2010 sentences for Chinese-English) and tested on another dataset (2118 sentences for Arabic-English, 1994 sentences for Chinese-English)." ></td>
	<td class="line x" title="171:221	These datasets were drawn from the NIST 2004 and 2005 evaluation data, plus some additional data from the GALE program." ></td>
	<td class="line x" title="172:221	There was no overlap at the segment or document level between the tuning and test sets." ></td>
	<td class="line x" title="173:221	We tuned b, the base of the log-linear model, to optimize consensus decoding performance." ></td>
	<td class="line x" title="174:221	Interestingly, we found that tuning b on the same dataset used for tuningwas as effective as tuning b on an additional held-out dataset." ></td>
	<td class="line x" title="175:221	4.4 Results over K-Best Lists Taking expectations over 1000-best lists6 and using BLEU7 as a similarity measure, both MBR 6We ensured that k-best lists contained no duplicates." ></td>
	<td class="line x" title="176:221	7Topreventzerosimilarityscores, wealsousedastandard smoothed version of BLEU that added 1 to the numerator and denominator of all n-gram precisions." ></td>
	<td class="line x" title="177:221	Performance results Arabic-English Expectations Similarity Hiero SBMT Baseline 52.0 53.9 104-best BLEU 52.2 53.9 Forest BLEU 53.0 54.0 Forest Linear G 52.3 54.0 Chinese-English Expectations Similarity Hiero SBMT Baseline 37.8 40.6 104-best BLEU 38.0 40.7 Forest BLEU 38.2 40.8 Forest Linear G 38.1 40.8 Table 2: Translation performance improves when computing expected sentences from translation forests rather than 104best lists, which in turn improve over Viterbi translations." ></td>
	<td class="line x" title="178:221	We also contrasted forest-based consensus decoding with BLEU and its linear approximation, G. Both similarity measures are effective, but BLEU outperforms G. and our variant provided consistent small gains of 0.00.2 BLEU." ></td>
	<td class="line x" title="179:221	Algorithms 1 and 3 gave the same small BLEU improvements in each data condition up to three significant figures." ></td>
	<td class="line x" title="180:221	The two algorithms differed greatly in speed, as shown in Table 1." ></td>
	<td class="line x" title="181:221	For Algorithm 1, we terminated the computation of E[BLEU(e;eprime)] for each e whenever e could not become the maximal hypothesis." ></td>
	<td class="line x" title="182:221	MBR speed depended on how often this shortcut applied, which varied by language and system." ></td>
	<td class="line x" title="183:221	Despite this optimization, our new Algorithm 3 was an average of 80 times faster across systems and language pairs." ></td>
	<td class="line x" title="184:221	4.5 Results for Forest-Based Decoding Table 2 contrasts Algorithm 3 over 104-best lists and forests." ></td>
	<td class="line x" title="185:221	Computing E[(eprime)] from a translation forest rather than a 104-best list improved Hiero by an additional 0.8 BLEU (1.0 over the baseline)." ></td>
	<td class="line x" title="186:221	Forest-based expectations always outperformed k-best lists, but curiously the magnitude of benefit was not consistent across systems." ></td>
	<td class="line x" title="187:221	We believe the difference is in part due to more aggressive forest pruning within the SBMT decoder." ></td>
	<td class="line x" title="188:221	For forest-based decoding, we compared two similarity measures: BLEU and its linear Taylor approximationGfromsection3.3.8 Table2shows were identical to standard BLEU." ></td>
	<td class="line x" title="189:221	8We did not estimate the  parameters of G ourselves; instead we used the parameters listed in Tromble et al.(2008), which were also estimated for GALE data." ></td>
	<td class="line x" title="191:221	We also approximated E[(eprime,t)] with a clipped expected count 573 Choose a distribution P over a set of translations E MBR over Sentence Pairs Compute pairwise similarityCompute expectations Max expected similarity Max feature similarity 3/3 1/4 2/5 1/3 4/4 0/5 2/3 0/4 5/5 MBR over Features E[(efficient)] = 0.6 E[(forest)] = 0.7 E[(decoding)] = 0.7 E[(for)] = 0.3 E[(rusty)] = 0.3 E[(coating)] = 0.3 E[(a)] = 0.4 E[(fish)] = 0.4 E[(aint)] = 0.4 c 1 c 2 c 3 r 1 r 2 r 3 1 2 3 2 3 50.0 50.2 50.4 50.6 50.8 511,660 513,245 514,830 Total model score for 1000 translations Corpus BLEU 0 20 40 60 80 Hiero SBMT 56.6 61.4 51.150.5 N-grams from baseline translations N-grams with high expected count Forest samples (b!2) Forest samples (b!5) Viterbi translations U(e 2 ;e 1 )= |efficient| |efficient for rusty coating| EU(e 1 ;e prime )=0.3(1+ 1 3 )+0.4 2 3 =0.667 EU(e 2 ;e prime )=0.375 EU(e 3 ;e prime )=0.520 U(e 1 ;E)= 0.6+0.7+0.7 3 =0.667 U(e 2 ;E)=0.375 U(e 3 ;E)=0.520 P(e 1 |f) = 0.3;e 1 =efficient forest decoding P(e 2 |f) = 0.3;e 2 =efficient for rusty coating P(e 3 |f) = 0.4;e 3 = A fish aint forest decoding I  telescope Yo vi al hombre con el telescopio I  saw the  man with  telescope the  telescope 0.4 saw the man with 0.6 saw the 1.0 man with E[c(e,man with)] = summationdisplay h P(h|f)  c(h,man with) =0.4  1 + (0.6  1.0)  1 N-gram Precision Figure 3: N-grams with high expected count are more likely to appear in the reference translation that n-grams in the translation models Viterbi translation, e." ></td>
	<td class="line x" title="192:221	Above, we compare the precision, relative to reference translations, of sets of n-grams chosen in two ways." ></td>
	<td class="line x" title="193:221	The left bar is the precision of the n-grams in e." ></td>
	<td class="line x" title="194:221	The right bar is the precision of n-grams with E[c(e,t)] > ." ></td>
	<td class="line x" title="195:221	To justify this comparison, we chose  so that both methods of choosing n-grams gave the same ngram recall: the fraction of n-grams in reference translations that also appeared in e or had E[c(e,t)] > ." ></td>
	<td class="line x" title="196:221	that both similarities were effective, but BLEU outperformed its linear approximation." ></td>
	<td class="line x" title="197:221	4.6 Analysis Forest-based consensus decoding leverages information about the correct translation from the entire forest." ></td>
	<td class="line x" title="198:221	In particular, consensus decoding with BLEU chooses translations using n-gram count expectations E[c(e,t)]." ></td>
	<td class="line x" title="199:221	Improvements in translation quality should therefore be directly attributable to information in these expected counts." ></td>
	<td class="line x" title="200:221	We endeavored to test the hypothesis that expected n-gram counts under the forest distribution carry more predictive information than the baseline Viterbi derivatione, which is the mode of the distribution." ></td>
	<td class="line x" title="201:221	To this end, we first tested the predictive accuracy of the n-grams proposed by e: the fraction of the n-grams in e that appear in a reference translation." ></td>
	<td class="line x" title="202:221	We compared this n-gram precision to a similar measure of predictive accuracy for expected n-gram counts: the fraction of the n-grams t with E[c(e,t)]   that appear in a reference." ></td>
	<td class="line x" title="203:221	To make these two precisions comparable, we chose  such that the recall of reference n-grams was equal." ></td>
	<td class="line x" title="204:221	Figure 3 shows that computing n-gram expectationswhich sum over translationsimproves the models ability to predict which n-grams will appear in the reference." ></td>
	<td class="line x" title="205:221	min(1,E[c(eprime,t)])." ></td>
	<td class="line x" title="206:221	Assuming an n-gram appears at most once per sentence, these expressions are equivalent, and this assumption holds for most n-grams." ></td>
	<td class="line x" title="207:221	Reference translation: Mubarak said that he received a telephone call from Sharon in which he said he was ready (to resume negotiations) but the Palestinians are hesitant. Baseline translation: Mubarak said he had received a telephone call from Sharon told him he was ready to resume talks with the Palestinians." ></td>
	<td class="line x" title="208:221	Fast forest-based consensus translation: Mubarak said that he had received a telephone call from Sharon told him that he was ready to resume the negotiations) , but the Palestinians are hesitant. Figure 4: Three translations of an example Arabic sentence: its human-generated reference, the translation with the highest model score under Hiero (Viterbi), and the translation chosen by forest-based consensus decoding." ></td>
	<td class="line x" title="209:221	The consensus translation reconstructs content lost in the Viterbi translation." ></td>
	<td class="line x" title="210:221	We attribute gains from fast consensus decoding to this increased predictive accuracy." ></td>
	<td class="line x" title="211:221	Examining the translations chosen by fast consensus decoding, we found that gains in BLEU often arose from improved lexical choice." ></td>
	<td class="line x" title="212:221	However, in our hierarchical systems, consensus decoding did occasionally trigger large reordering." ></td>
	<td class="line x" title="213:221	We also found examples where the translation quality improved by recovering content that was missing from the baseline translation, as in Figure 4." ></td>
	<td class="line x" title="214:221	5 Conclusion We have demonstrated substantial speed increases in k-best consensus decoding through a new procedure inspired by MBR under linear similarity measures." ></td>
	<td class="line x" title="215:221	To further improve this approach, we computed expected n-gram counts from translation forests instead of k-best lists." ></td>
	<td class="line x" title="216:221	Fast consensus decoding using forest-based n-gram expectations and BLEU as a similarity measure yielded consistent improvements over MBR with k-best lists, yet required only simple computations that scale linearly with the size of the translation forest." ></td>
	<td class="line x" title="217:221	The space of similarity measures is large and relatively unexplored, and the feature expectations that can be computed from forests extend beyond n-gram counts." ></td>
	<td class="line x" title="218:221	Therefore, future work may show additional benefits from fast consensus decoding." ></td>
	<td class="line x" title="219:221	Acknowledgements This work was supported under DARPA GALE, Contract No." ></td>
	<td class="line x" title="220:221	HR0011-06-C-0022." ></td>
	<td class="line x" title="221:221	574" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1108
K-Best A* Parsing
Pauls, Adam;Klein, Dan;"></td>
	<td class="line x" title="1:221	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 958966, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:221	c2009 ACL and AFNLP K-Best A Parsing Adam Pauls and Dan Klein Computer Science Division University of California, Berkeley {adpauls,klein}@cs.berkeley.edu Abstract A parsing makes 1-best search efficient by suppressing unlikely 1-best items." ></td>
	<td class="line x" title="3:221	Existing kbest extraction methods can efficiently search for top derivations, but only after an exhaustive 1-best pass." ></td>
	<td class="line x" title="4:221	We present a unified algorithm for k-best A parsing which preserves the efficiency of k-best extraction while giving the speed-ups of A methods." ></td>
	<td class="line x" title="5:221	Our algorithm produces optimalk-best parses under the same conditions required for optimality in a 1-best A parser." ></td>
	<td class="line x" title="6:221	Empirically, optimal k-best lists can be extracted significantly faster than with other approaches, over a range of grammar types." ></td>
	<td class="line x" title="7:221	1 Introduction Many situations call for a parser to return the kbest parses rather than only the 1-best." ></td>
	<td class="line x" title="8:221	Uses for k-best lists include minimum Bayes risk decoding (Goodman, 1998; Kumar and Byrne, 2004), discriminative reranking (Collins, 2000; Charniak and Johnson, 2005), and discriminative training (Och, 2003; McClosky et al., 2006)." ></td>
	<td class="line x" title="9:221	The most efficient known algorithm for k-best parsing (Jimenez and Marzal, 2000; Huang and Chiang, 2005) performs an initial bottom-up dynamic programming pass before extracting thek-best parses." ></td>
	<td class="line x" title="10:221	In that algorithm, the initial pass is, by far, the bottleneck (Huang and Chiang, 2005)." ></td>
	<td class="line x" title="11:221	In this paper, we propose an extension of A parsing which integratesk-best search with an Abased exploration of the 1-best chart." ></td>
	<td class="line x" title="12:221	A parsing can avoid significant amounts of computation by guiding 1-best search with heuristic estimates of parse completion costs, and has been applied successfully in several domains (Klein and Manning, 2002; Klein and Manning, 2003c; Haghighi et al., 2007)." ></td>
	<td class="line x" title="13:221	Our algorithm extends the speedups achieved in the 1-best case to the k-best case and is optimal under the same conditions as a standard A algorithm." ></td>
	<td class="line x" title="14:221	The amount of work done in the k-best phase is no more than the amount of work done by the algorithm of Huang and Chiang (2005)." ></td>
	<td class="line x" title="15:221	Our algorithm is also equivalent to standard A parsing (up to ties) if it is terminated after the 1-best derivation is found." ></td>
	<td class="line x" title="16:221	Finally, our algorithm can be written down in terms of deduction rules, and thus falls into the well-understood view of parsing as weighted deduction (Shieber et al., 1995; Goodman, 1998; Nederhof, 2003)." ></td>
	<td class="line x" title="17:221	In addition to presenting the algorithm, we show experiments in which we extract k-best lists for three different kinds of grammars: the lexicalized grammars of Klein and Manning (2003b), the state-split grammars of Petrov et al.(2006), and the tree transducer grammars of Galley et al.(2006)." ></td>
	<td class="line x" title="20:221	We demonstrate that optimal k-best lists can be extracted significantly faster using our algorithm than with previous methods." ></td>
	<td class="line x" title="21:221	2 A k-Best A Parsing Algorithm We build up to our full algorithm in several stages, beginning with standard 1-best A parsing and making incremental modifications." ></td>
	<td class="line x" title="22:221	2.1 Parsing as Weighted Deduction Our algorithm can be formulated in terms of prioritized weighted deduction rules (Shieber et al., 1995; Nederhof, 2003; Felzenszwalb and McAllester, 2007)." ></td>
	<td class="line x" title="23:221	A prioritized weighted deduction rule has the form 1 : w1,,n : wn p(w1,,wn)0 : g(w1,,wn) where 1,,n are the antecedent items of the deduction rule and 0 is the conclusion item." ></td>
	<td class="line x" title="24:221	A deduction rule states that, given the antecedents 1,,n with weights w1,,wn, the conclusion 0 can be formed with weight g(w1,,wn) and priority p(w1,,wn)." ></td>
	<td class="line x" title="25:221	958 These deduction rules are executed within a generic agenda-driven algorithm, which constructs items in a prioritized fashion." ></td>
	<td class="line x" title="26:221	The algorithm maintains an agenda (a priority queue of unprocessed items), as well as a chart of items already processed." ></td>
	<td class="line x" title="27:221	The fundamental operation of the algorithm is to pop the highest priority item  from the agenda, put it into the chart with its current weight, and form using deduction rules any items which can be built by combining  with items already in the chart." ></td>
	<td class="line x" title="28:221	If new or improved, resulting items are put on the agenda with priority given by p()." ></td>
	<td class="line x" title="29:221	2.2 AParsing The A parsing algorithm of Klein and Manning (2003c) can be formulated in terms of weighted deduction rules (Felzenszwalb and McAllester, 2007)." ></td>
	<td class="line x" title="30:221	We do so here both to introduce notation and to build to our final algorithm." ></td>
	<td class="line x" title="31:221	First, we must formalize some notation." ></td>
	<td class="line x" title="32:221	Assume we have a PCFG1 G and an input sentence s1sn of length n. The grammarG has a set of symbols , including a distinguished goal (root) symbol G. Without loss of generality, we assume Chomsky normal form, so each non-terminal rule r inGhas the formr = AB C with weightwr (the negative log-probability of the rule)." ></td>
	<td class="line x" title="33:221	Edges are labeled spans e = (A,i,j)." ></td>
	<td class="line x" title="34:221	Inside derivations of an edge (A,i,j) are trees rooted at A and spanning si+1sj." ></td>
	<td class="line x" title="35:221	The total weight of the best (minimum) inside derivation for an edge e is called the Viterbi inside score (e)." ></td>
	<td class="line x" title="36:221	The goal of the 1-best A parsing algorithm is to compute the Viterbi inside score of the edge (G,0,n); backpointers allow the reconstruction of a Viterbi parse in the standard way." ></td>
	<td class="line x" title="37:221	The basic A algorithm operates on deduction items I(A,i,j) which represent in a collapsed way the possible inside derivations of edges (A,i,j)." ></td>
	<td class="line x" title="38:221	We call these items inside edge items or simply inside items where clear; a graphical representation of an inside item can be seen in Figure 1(a)." ></td>
	<td class="line x" title="39:221	The space whose items are inside edges is called the edge space." ></td>
	<td class="line x" title="40:221	These inside items are combined using the single IN deduction schema shown in Table 1." ></td>
	<td class="line x" title="41:221	This schema is instantiated for every grammar rule r 1While we present the algorithm specialized to parsing with a PCFG, it generalizes to a wide range of hypergraph search problems as shown in Klein and Manning (2001)." ></td>
	<td class="line x" title="42:221	VP s3 s4 s5 s1 s2 s6 sn VP VBZ NP DT NN s3 s4 s5 VP G (a) (b) (c) VP VBZ1 NP4 DT NN s3 s4 s5 (e) VP6 s3 s4 s5 VBZ NP DT NN (d) Figure 1: Representations of the different types of items used in parsing." ></td>
	<td class="line x" title="43:221	(a) An inside edge item: I(VP,2,5)." ></td>
	<td class="line x" title="44:221	(b) An outside edge item: O(VP,2,5)." ></td>
	<td class="line x" title="45:221	(c) An inside derivation item: D(TVP,2,5) for a tree TVP." ></td>
	<td class="line x" title="46:221	(d) A ranked derivation item: K(VP,2,5,6)." ></td>
	<td class="line x" title="47:221	(e) A modified inside derivation item (with backpointers to ranked items): D(VP,2,5,3,VP  VBZ NP,1,4)." ></td>
	<td class="line x" title="48:221	in G. For IN, the function g() simply sums the weights of the antecedent items and the grammar rule r, while the priority function p() adds a heuristic to this sum." ></td>
	<td class="line x" title="49:221	The heuristic is a bound on the Viterbi outside score (e) of an edge e; see Klein and Manning (2003c) for details." ></td>
	<td class="line x" title="50:221	A good heuristic allows A to reach the goal item I(G,0,n) while constructing few inside items." ></td>
	<td class="line x" title="51:221	If the heuristic is consistent, then A guarantees that whenever an inside item comes off the agenda, its weight is its true Viterbi inside score (Klein and Manning, 2003c)." ></td>
	<td class="line x" title="52:221	In particular, this guarantee implies that the goal item I(G,0,n) will be popped with the score of the 1-best parse of the sentence." ></td>
	<td class="line x" title="53:221	Consistency also implies that items are popped off the agenda in increasing order of bounded Viterbi scores: (e) +h(e) We will refer to this monotonicity as the ordering property of A (Felzenszwalb and McAllester, 2007)." ></td>
	<td class="line x" title="54:221	One final property implied by consistency is admissibility, which states that the heuristic never overestimates the true Viterbi outside score for an edge, i.e. h(e)  (e)." ></td>
	<td class="line x" title="55:221	For the remainder of this paper, we will assume our heuristics are consistent." ></td>
	<td class="line x" title="56:221	2.3 A Naivek-Best AAlgorithm Due to the optimal substructure of 1-best PCFG derivations, a 1-best parser searches over the space of edges; this is the essence of 1-best dynamic programming." ></td>
	<td class="line x" title="57:221	Although most edges can be built 959 Inside Edge Deductions (Used in A and KA) IN: I(B,i,l) : w1 I(C,l,j) : w2 w1+w2+wr+h(A,i,j) I(A,i,j) : w1 +w2 +wr Table 1: The deduction schema (IN) for building inside edge items, using a supplied heuristic." ></td>
	<td class="line x" title="58:221	This schema is sufficient on its own for 1-best A, and it is used in KA." ></td>
	<td class="line x" title="59:221	Here, r is the rule AB C. Inside Derivation Deductions (Used in NAIVE) DERIV: D(TB,i,l) : w1 D(TC,l,j) : w2 w1+w2+wr+h(A,i,j) D parenleftBigg A TB TC ,i,j parenrightBigg : w1 +w2 +wr Table 2: The deduction schema for building derivations, using a supplied heuristic." ></td>
	<td class="line x" title="60:221	TB and TC denote full tree structures rooted at symbols B and C. This schema is the same as the IN deduction schema, but operates on the space of fully specified inside derivations rather than dynamic programming edges." ></td>
	<td class="line x" title="61:221	This schema forms the NAIVE k-best algorithm." ></td>
	<td class="line x" title="62:221	Outside Edge Deductions (Used in KA) OUT-B: I(G,0,n) : w1 w1 O(G,0,n) : 0 OUT-L: O(A,i,j) : w1 I(B,i,l) : w2 I(C,l,j) : w3 w1+w3+wr+w2 O(B,i,l) : w1 +w3 +wr OUT-R: O(A,i,j) : w1 I(B,i,l) : w2 I(C,l,j) : w3 w1+w2+wr+w3 O(C,l,j) : w1 +w2 +wr Table 3: The deduction schemata for building ouside edge items." ></td>
	<td class="line x" title="63:221	The first schema is a base case that constructs an outside item for the goal (G,0,n) from the inside item I(G,0,n)." ></td>
	<td class="line x" title="64:221	The second two schemata build outside items in a top-down fashion." ></td>
	<td class="line x" title="65:221	Note that for outside items, the completion cost is the weight of an inside item rather than a value computed by a heuristic." ></td>
	<td class="line x" title="66:221	Delayed Inside Derivation Deductions (Used in KA) DERIV: D(TB,i,l) : w1 D(TC,l,j) : w2 O(A,i,j) : w3 w1+w2+wr+w3 D parenleftBigg A TB TC ,i,j parenrightBigg : w1 +w2 +wrTable 4: The deduction schema for building derivations, using exact outside scores computed using OUT deduc-tions." ></td>
	<td class="line x" title="67:221	The dependency on the outside item O(A,i,j) delays building derivation items until exact Viterbi outside scores have been computed." ></td>
	<td class="line x" title="68:221	This is the final search space for the KA algorithm." ></td>
	<td class="line x" title="69:221	Ranked Inside Derivation Deductions (Lazy Version of NAIVE) BUILD: K(B,i,l,u) : w1 K(C,l,j,v) : w2 w1+w2+wr+h(A,i,j) D(A,i,j,l,r,u,v) : w1 +w2 +wr RANK: D1(A,i,j,) : w1  Dk(A,i,j,) : wk maxmwm+h(A,i,j) K(A,i,j,k) : maxmwm Table 5: The schemata for simultaneously building and ranking derivations, using a supplied heuristic, for the lazier form of the NAIVE algorithm." ></td>
	<td class="line x" title="70:221	BUILD builds larger derivations from smaller ones." ></td>
	<td class="line x" title="71:221	RANK numbers derivations for each edge." ></td>
	<td class="line x" title="72:221	Note that RANK requires distinct Di, so a rank k RANK rule will first apply (optimally) as soon as the kth-best inside derivation item for a given edge is removed from the queue." ></td>
	<td class="line x" title="73:221	However, it will also still formally apply (suboptimally) for all derivation items dequeued after the kth." ></td>
	<td class="line x" title="74:221	In practice, the RANK schema need not be implemented explicitly  one can simply assign a rank to each inside derivation item when it is removed from the agenda, and directly add the appropriate ranked inside item to the chart." ></td>
	<td class="line x" title="75:221	Delayed Ranked Inside Derivation Deductions (Lazy Version of KA) BUILD:K(B,i,l,u) :w1 K(C,l,j,v) :w2 O(A,i,j) :w3 w1+w2+wr+w3 D(A,i,j,l,r,u,v) : w1 +w2 +wr RANK:D1(A,i,j,) :w1 Dk(A,i,j,) :wk O(A,i,j) :wk+1 maxmwm+wk+1 K(A,i,j,k) : maxmwm Table 6: The deduction schemata for building and ranking derivations, using exact outside scores computed from OUT deductions, used for the lazier form of the KA algorithm." ></td>
	<td class="line x" title="76:221	960 using many derivations, each inside edge item will be popped exactly once during parsing, with a score and backpointers representing its 1-best derivation." ></td>
	<td class="line x" title="77:221	However, k-best lists involve suboptimal derivations." ></td>
	<td class="line x" title="78:221	One way to compute k-best derivations is therefore to abandon optimal substructure and dynamic programming entirely, and to search over the derivation space, the much larger space of fully specified trees." ></td>
	<td class="line x" title="79:221	The items in this space are called inside derivation items, or derivation items where clear, and are of the form D(TA,i,j), specifying an entire tree TA rooted at symbol A and spanning si+1sj (see Figure 1(c))." ></td>
	<td class="line x" title="80:221	Derivation items are combined using the DERIV schema of Table 2." ></td>
	<td class="line x" title="81:221	The goals in this space, representing root parses, are any derivation items rooted at symbol G that span the entire input." ></td>
	<td class="line x" title="82:221	In this expanded search space, each distinct parse has its own derivation item, derivable only in one way." ></td>
	<td class="line x" title="83:221	If we continue to search long enough, we will pop multiple goal items." ></td>
	<td class="line x" title="84:221	The first k which come off the agenda will be thek-best derivations." ></td>
	<td class="line x" title="85:221	We refer to this approach as NAIVE." ></td>
	<td class="line x" title="86:221	It is very inefficient on its own, but it leads to the full algorithm." ></td>
	<td class="line x" title="87:221	The correctness of thisk-best algorithm follows from the correctness of A parsing." ></td>
	<td class="line x" title="88:221	The derivation space of full trees is simply the edge space of a much larger grammar (see Section 2.5)." ></td>
	<td class="line x" title="89:221	Note that the DERIV schemas priority includes a heuristic just like 1-best A." ></td>
	<td class="line x" title="90:221	Because of the context freedom of the grammar, any consistent heuristic for inside edge items usable in 1-best A is also consistent for inside derivation items (and vice versa)." ></td>
	<td class="line x" title="91:221	In particular, the 1-best Viterbi outside score for an edge is a perfect heuristic for any derivation of that edge." ></td>
	<td class="line x" title="92:221	While correct, NAIVE is massively inefficient." ></td>
	<td class="line x" title="93:221	In comparison with Aparsing overG, where there are O(n2) inside items, the size of the derivation space is exponential in the sentence length." ></td>
	<td class="line x" title="94:221	By the ordering property, we know that NAIVE will process all derivation items d with (d) +h(d)(gk) where gk is the kth-best root parse and () is the inside score of a derivation item (analogous to  for edges).2 Even for reasonable heuristics, this 2The new symbol emphasizes that  scores a specific derivation rather than a minimum over a set of derivations." ></td>
	<td class="line x" title="95:221	number can be very large; see Section 3 for empirical results." ></td>
	<td class="line x" title="96:221	This naive algorithm is, of course, not novel, either in general approach or specific computation." ></td>
	<td class="line x" title="97:221	Earlyk-best parsers functioned by abandoning dynamic programming and performing beam search on derivations (Ratnaparkhi, 1999; Collins, 2000)." ></td>
	<td class="line x" title="98:221	Huang (2005) proposes an extension of Knuths algorithm (Knuth, 1977) to produce k-best lists by searching in the space of derivations, which is essentially this algorithm." ></td>
	<td class="line x" title="99:221	While Huang (2005) makes no explicit mention of a heuristic, it would be easy to incorporate one into their formulation." ></td>
	<td class="line x" title="100:221	2.4 A Newk-Best AParser While NAIVE suffers severe performance degradation for loose heuristics, it is in fact very efficient if h() is perfect, i.e. h(e) = (e)e. In this case, the ordering property of A guarantees that only inside derivation items d satisfying (d) +(d)(gk) will be placed in the chart." ></td>
	<td class="line x" title="101:221	The set of derivation items d satisfying this inequality is exactly the set which appear in the k-best derivations of (G,0,n) (as always, modulo ties)." ></td>
	<td class="line x" title="102:221	We could therefore use NAIVE quite efficiently if we could obtain exact Viterbi outside scores." ></td>
	<td class="line x" title="103:221	One option is to compute outside scores with exhaustive dynamic programming over the original grammar." ></td>
	<td class="line x" title="104:221	In a certain sense, described in greater detail below, this precomputation of exact heuristics is equivalent to the k-best extraction algorithm of Huang and Chiang (2005)." ></td>
	<td class="line x" title="105:221	However, this exhaustive 1-best work is precisely what we want to use A to avoid." ></td>
	<td class="line x" title="106:221	Our algorithm solves this problem by integrating three searches into a single agenda-driven process." ></td>
	<td class="line x" title="107:221	First, an A search in the space of inside edge items with an (imperfect) external heuristic h() finds exact inside scores." ></td>
	<td class="line x" title="108:221	Second, exact outside scores are computed from inside and outside items." ></td>
	<td class="line x" title="109:221	Finally, these exact outside scores guide the search over derivations." ></td>
	<td class="line x" title="110:221	It can be useful to imagine these three operations as operating in phases, but they are all interleaved, progressing in order of their various priorities." ></td>
	<td class="line x" title="111:221	In order to calculate outside scores, we introduce outside items O(A,i,j), which represent best derivations of G  s1si A sj+1sn; see Figure 1(b)." ></td>
	<td class="line x" title="112:221	Where the weights of inside items 961 compute Viterbi inside scores, the weights of outside items compute Viterbi outside scores." ></td>
	<td class="line x" title="113:221	Table 3 shows deduction schemata for building outside items." ></td>
	<td class="line x" title="114:221	These schemata are adapted from the schemata used in the general hierarchical A algorithm of Felzenszwalb and McAllester (2007)." ></td>
	<td class="line x" title="115:221	In that work, it is shown that such schemata maintain the property that the weight of an outside item is the true Viterbi outside score when it is removed from the agenda." ></td>
	<td class="line x" title="116:221	They also show that outside items o follow an ordering property, namely that they are processed in increasing order of (o) +(o) This quantity is the score of the best root derivation which includes the edge corresponding to o. Felzenszwalb and McAllester (2007) also show that both inside and outside items can be processed on the same queue and the ordering property holds jointly for both types of items." ></td>
	<td class="line x" title="117:221	If we delay the construction of a derivation item until its corresponding outside item has been popped, then we can gain the benefits of using an exact heuristic h() in the naive algorithm." ></td>
	<td class="line x" title="118:221	We realize this delay by modifying the DERIV deduction schema as shown in Table 4 to trigger on and prioritize with the appropriate outside scores." ></td>
	<td class="line x" title="119:221	We now have our final algorithm, which we call KA." ></td>
	<td class="line x" title="120:221	It is the union of the IN, OUT, and new delayed DERIV deduction schemata." ></td>
	<td class="line x" title="121:221	In words, our algorithm functions as follows: we initialize the agenda with I(si,i1,i) and D(si,i1,i) for i = 1n." ></td>
	<td class="line x" title="122:221	We compute inside scores in standard A fashion using the IN deduction rule, using any heuristic we might provide to 1-best A." ></td>
	<td class="line x" title="123:221	Once the inside item I(G,0,n) is found, we automatically begin to compute outside scores via the OUT deduction rules." ></td>
	<td class="line x" title="124:221	Once O(si,i1,i) is found, we can begin to also search in the space of derivation items, using the perfect heuristics given by the just-computed outside scores." ></td>
	<td class="line x" title="125:221	Note, however, that all computation is done with a single agenda, so the processing of all three types of items is interleaved, with the k-best search possibly terminating without a full inside computation." ></td>
	<td class="line x" title="126:221	As with NAIVE, the algorithm terminates when ak-th goal derivation is dequeued." ></td>
	<td class="line x" title="127:221	2.5 Correctness We prove the correctness of this algorithm by a reduction to the hierarchical A (HA) algorithm of Felzenszwalb and McAllester (2007)." ></td>
	<td class="line x" title="128:221	The input to HA is a target grammarGm and a list of grammarsG0Gm1 in whichGt1 is a relaxed projection ofGt for all t = 1m." ></td>
	<td class="line x" title="129:221	A grammarGt1 is a projection ofGt if there exists some onto function pit : tmapstot1 defined for all symbols inGt." ></td>
	<td class="line x" title="130:221	We use At1 to represent pit(At)." ></td>
	<td class="line x" title="131:221	A projection is relaxed if, for every rule r = At  BtCt with weight wr there is a rule rprime = At1 Bt1Ct1 inGt1 with weight wrprimewr." ></td>
	<td class="line x" title="132:221	We assume that our external heuristic function h() is constructed by parsing our input sentence with a relaxed projection of our target grammar." ></td>
	<td class="line x" title="133:221	This assumption, though often true anyway, is to allow proof by reduction to Felzenszwalb and McAllester (2007).3 We construct an instance of HAas follows: Let G0 be the relaxed projection which computes the heuristic." ></td>
	<td class="line x" title="134:221	LetG1 be the input grammarG, and let G2, the target grammar of our HAinstance, be the grammar of derivations inGformed by expanding each symbol A in G to all possible inside derivationsTA rooted atA." ></td>
	<td class="line x" title="135:221	The rules inG2 have the form TA TB TC with weight given by the weight of the rule AB C. By construction, G1 is a relaxed projection of G2; by assumption G0 is a relaxed projection of G1." ></td>
	<td class="line x" title="136:221	The deduction rules that describe KA build the same items as HA with same weights and priorities, and so the guarantees from HA carry over to KA." ></td>
	<td class="line x" title="137:221	We can characterize the amount of work done using the ordering property." ></td>
	<td class="line x" title="138:221	Let gk be the kth-best derivation item for the goal edge g. Our algorithm processes all derivation items d, outside items o, and inside items i satisfying (d) +(d)  (gk) (o) +(o)  (gk) (i) +h(i)  (gk) We have already argued that the set of derivation items satisfying the first inequality is the set of subtrees that appear in the optimal k-best parses, modulo ties." ></td>
	<td class="line x" title="139:221	Similarly, it can be shown that the second inequality is satisfied only for edges that appear in the optimal k-best parses." ></td>
	<td class="line x" title="140:221	The last inequality characterizes the amount of work done in the bottom-up pass." ></td>
	<td class="line x" title="141:221	We compare this to 1-best A, which pops all inside items i satisfying (i) +h(i)(g) = (g1) 3KA is correct for any consistent heuristic but a nonreductive proof is not possible in the present space." ></td>
	<td class="line x" title="142:221	962 Thus, the extra inside items popped in the bottom-up pass duringk-best parsing as compared to 1-best parsing are those items i satisfying (g1)(i) +h(i)(gk) The question of how many items satisfy these inequalities is empirical; we show in our experiments that it is small for reasonable heuristics." ></td>
	<td class="line x" title="143:221	At worst, the bottom-up phase pops all inside items and reduces to exhaustive dynamic programming." ></td>
	<td class="line x" title="144:221	Additionally, it is worth noting that our algorithm is naturally online in that it can be stopped at any k without advance specification." ></td>
	<td class="line x" title="145:221	2.6 Lazy Successor Functions The global ordering property guarantees that we will only dequeue derivation fragments of top parses." ></td>
	<td class="line x" title="146:221	However, we will enqueue all combinations of such items, which is wasteful." ></td>
	<td class="line x" title="147:221	By exploiting a local ordering amongst derivations, we can be more conservative about combination and gain the advantages of a lazy successor function (Huang and Chiang, 2005)." ></td>
	<td class="line x" title="148:221	To do so, we represent inside derivations not by explicitly specifying entire trees, but rather by using ranked backpointers." ></td>
	<td class="line x" title="149:221	In this representation, inside derivations are represented in two ways, shown in Figure 1(d) and (e)." ></td>
	<td class="line x" title="150:221	The first way (d) simply adds a rank u to an edge, giving a tuple (A,i,j,u)." ></td>
	<td class="line x" title="151:221	The corresponding item is the ranked derivation item K(A,i,j,u), which represents the uth-best derivation of A over (i,j)." ></td>
	<td class="line x" title="152:221	The second representation (e) is a backpointer of the form (A,i,j,l,r,u,v), specifying the derivation formed by combining the uth-best derivation of (B,i,l) and the vth-best derivation of (C,l,j) using rule r = AB C. The corresponding items D(A,i,j,l,r,u,v) are the new form of our inside derivation items." ></td>
	<td class="line x" title="153:221	The modified deduction schemata for the NAIVE algorithm over these representations are shown in Table 5." ></td>
	<td class="line x" title="154:221	The BUILD schema produces new inside derivation items from ranked derivation items, while the RANK schema assigns each derivation item a rank; together they function like DERIV." ></td>
	<td class="line x" title="155:221	We can find the k-best list by searching until K(G,0,n,k) is removed from the agenda." ></td>
	<td class="line x" title="156:221	The k-best derivations can then be extracted by following the backpointers for K(G,0,n,1)  K(G,0,n,k)." ></td>
	<td class="line x" title="157:221	The KA algorithm can be modified in the same way, shown in Table 6." ></td>
	<td class="line x" title="158:221	1 5 50 500 Heuristic Derivation items pushed (millions) 5-split4-split3-split2-split1-split0-split NAIVE KA* Figure 2: Number of derivation items enqueued as a function of heuristic." ></td>
	<td class="line x" title="159:221	Heuristics are shown in decreasing order of tightness." ></td>
	<td class="line x" title="160:221	The y-axis is on a log-scale." ></td>
	<td class="line x" title="161:221	The actual laziness is provided by additionally delaying the combination of ranked items." ></td>
	<td class="line x" title="162:221	When an item K(B,i,l,u) is popped off the queue, a naive implementation would loop over items K(C,l,j,v) for all v, C, and j (and similarly for left combinations)." ></td>
	<td class="line x" title="163:221	Fortunately, little looping is actually necessary: there is a partial ordering of derivation items, namely, that D(A,i,j,l,r,u,v) will have a lower computed priority than D(A,i,j,l,r,u 1,v) and D(A,i,j,l,r,u,v  1) (Jimenez and Marzal, 2000)." ></td>
	<td class="line x" title="164:221	So, we can wait until one of the latter two is built before triggering the construction of the former." ></td>
	<td class="line x" title="165:221	This triggering is similar to the lazy frontier used by Huang and Chiang (2005)." ></td>
	<td class="line x" title="166:221	All of our experiments use this lazy representation." ></td>
	<td class="line x" title="167:221	3 Experiments 3.1 State-Split Grammars We performed our first experiments with the grammars of Petrov et al.(2006)." ></td>
	<td class="line x" title="169:221	The training procedure for these grammars produces a hierarchy of increasingly refined grammars through statesplitting." ></td>
	<td class="line x" title="170:221	We followed Pauls and Klein (2009) in computing heuristics for the most refined grammar from outside scores for less-split grammars." ></td>
	<td class="line x" title="171:221	We used the Berkeley Parser4 to learn such grammars from Sections 2-21 of the Penn Treebank (Marcus et al., 1993)." ></td>
	<td class="line x" title="172:221	We trained with 6 split-merge cycles, producing 7 grammars." ></td>
	<td class="line x" title="173:221	We tested these grammars on 100 sentences of length at most 30 of Section 23 of the Treebank." ></td>
	<td class="line x" title="174:221	Our target grammar was in all cases the most split grammar." ></td>
	<td class="line x" title="175:221	4http://berkeleyparser.googlecode.com 963 0 200040006000800010000 0 5000 15000 25000 KA* k Items pushed (millions) K Best Bottom-up Heuristic 0 200040006000800010000 0 5000 15000 25000 EXH k Items pushed (millions) K Best Bottom-up Figure 3: The cost of k-best extraction as a function of k for state-split grammars, for both KA and EXH." ></td>
	<td class="line x" title="176:221	The amount of time spent in the k-best phase is negligible compared to the cost of the bottom-up phase in both cases." ></td>
	<td class="line x" title="177:221	Heuristics computed from projections to successively smaller grammars in the hierarchy form successively looser bounds on the outside scores." ></td>
	<td class="line x" title="178:221	This allows us to examine the performance as a function of the tightness of the heuristic." ></td>
	<td class="line x" title="179:221	We first compared our algorithm KA against the NAIVE algorithm." ></td>
	<td class="line x" title="180:221	We extracted 1000-best lists using each algorithm, with heuristics computed using each of the 6 smaller grammars." ></td>
	<td class="line x" title="181:221	In Figure 2, we evaluate only the k-best extraction phase by plotting the number of derivation items and outside items added to the agenda as a function of the heuristic used, for increasingly loose heuristics." ></td>
	<td class="line x" title="182:221	We follow earlier work (Pauls and Klein, 2009) in using number of edges pushed as the primary, hardware-invariant metric for evaluating performance of our algorithms.5 While KA scales roughly linearly with the looseness of the heuristic, NAIVE degrades very quickly as the heuristics get worse." ></td>
	<td class="line x" title="183:221	For heuristics given by grammars weaker than the 4-split grammar, NAIVE ran out of memory." ></td>
	<td class="line x" title="184:221	Since the bottom-up pass of k-best parsing is the bottleneck, we also examine the time spent in the 1-best phase of k-best parsing." ></td>
	<td class="line x" title="185:221	As a baseline, we compared KA to the approach of Huang and Chiang (2005), which we will call EXH (see below for more explanation) since it requires exhaustive parsing in the bottom-up pass." ></td>
	<td class="line x" title="186:221	We performed the exhaustive parsing needed for EXH in our agenda-based parser to facilitate comparison." ></td>
	<td class="line x" title="187:221	For KA, we included the cost of computing the heuristic, which was done by running our agenda-based parser exhaustively on a smaller grammar to compute outside items; we chose the 5We found that edges pushed was generally well correlated with parsing time." ></td>
	<td class="line x" title="188:221	0 200040006000800010000 0 200 600 1000 KA* k Items pushed (millions) K Best Bottom-up Heuristic Figure 4: The performance of KA for lexicalized grammars." ></td>
	<td class="line x" title="189:221	The performance is dominated by the computation of the heuristic, so that both the bottom-up phase and the k-best phase are barely visible." ></td>
	<td class="line x" title="190:221	3-split grammar for the heuristic since it gives the best overall tradeoff of heuristic and bottom-up parsing time." ></td>
	<td class="line x" title="191:221	We separated the items enqueued into items enqueued while computing the heuristic (not strictly part of the algorithm), inside items (bottom-up), and derivation and outside items (together k-best)." ></td>
	<td class="line x" title="192:221	The results are shown in Figure 3." ></td>
	<td class="line x" title="193:221	The cost of k-best extraction is clearly dwarfed by the the 1-best computation in both cases." ></td>
	<td class="line x" title="194:221	However, KA is significantly faster over the bottom-up computations, even when the cost of computing the heuristic is included." ></td>
	<td class="line x" title="195:221	3.2 Lexicalized Parsing We also experimented with the lexicalized parsing model described in Klein and Manning (2003b)." ></td>
	<td class="line x" title="196:221	This model is constructed as the product of a dependency model and the unlexicalized PCFG model in Klein and Manning (2003a)." ></td>
	<td class="line x" title="197:221	We 964 0 200040006000800010000 0 500 1500 2500 KA* k Items pushed (millions) K Best Bottom-up Heuristic 0 200040006000800010000 0 500 1500 2500 EXH k Items pushed (millions) K Best Bottom-up Figure 5: k-best extraction as a function of k for tree transducer grammars, for both KA and EXH." ></td>
	<td class="line x" title="198:221	constructed these grammars using the Stanford Parser.6 The model was trained on Sections 2-20 of the Penn Treebank and tested on 100 sentences of Section 21 of length at most 30 words." ></td>
	<td class="line x" title="199:221	For this grammar, Klein and Manning (2003b) showed that a very accurate heuristic can be constructed by taking the sum of outside scores computed with the dependency model and the PCFG model individually." ></td>
	<td class="line x" title="200:221	We report performance as a function of k for KA in Figure 4." ></td>
	<td class="line x" title="201:221	Both NAIVE and EXH are impractical on these grammars due to memory limitations." ></td>
	<td class="line x" title="202:221	For KA, computing the heuristic is the bottleneck, after which bottom-up parsing and k-best extraction are very fast." ></td>
	<td class="line oc" title="203:221	3.3 Tree Transducer Grammars Syntactic machine translation (Galley et al., 2004) uses tree transducer grammars to translate sentences." ></td>
	<td class="line x" title="204:221	Transducer rules are synchronous contextfree productions that have both a source and a target side." ></td>
	<td class="line x" title="205:221	We examine the cost of k-best parsing in the source side of such grammars with KA, which can be a first step in translation." ></td>
	<td class="line x" title="206:221	We extracted a grammar from 220 million words of Arabic-English bitext using the approach of Galley et al.(2006), extracting rules with at most 3 non-terminals." ></td>
	<td class="line x" title="208:221	These rules are highly lexicalized." ></td>
	<td class="line x" title="209:221	About 300K rules are applicable for a typical 30-word sentence; we filter the rest." ></td>
	<td class="line x" title="210:221	We tested on 100 sentences of length at most 40 from the NIST05 Arabic-English test set." ></td>
	<td class="line x" title="211:221	We used a simple but effective heuristic for these grammars, similar to the FILTER heuristic suggested in Klein and Manning (2003c)." ></td>
	<td class="line x" title="212:221	We projected the source projection to a smaller grammar by collapsing all non-terminal symbols to X, and 6http://nlp.stanford.edu/software/ also collapsing pre-terminals into related clusters." ></td>
	<td class="line x" title="213:221	For example, we collapsed the tags NN, NNS, NNP, and NNPS to N. This projection reduced the number of grammar symbols from 149 to 36." ></td>
	<td class="line x" title="214:221	Using it as a heuristic for the full grammar suppressed60% of the total items (Figure 5)." ></td>
	<td class="line x" title="215:221	4 Related Work While formulated very differently, one limiting case of our algorithm relates closely to the EXH algorithm of Huang and Chiang (2005)." ></td>
	<td class="line x" title="216:221	In particular, if all inside items are processed before any derivation items, the subsequent number of derivation items and outside items popped by KA is nearly identical to the number popped by EXH in our experiments (both algorithms have the same ordering bounds on which derivation items are popped)." ></td>
	<td class="line x" title="217:221	The only real difference between the algorithms in this limited case is that EXH places k-best items on local priority queues per edge, while KA makes use of one global queue." ></td>
	<td class="line x" title="218:221	Thus, in addition to providing a method for speeding up k-best extraction with A, our algorithm also provides an alternate form of Huang and Chiang (2005)s k-best extraction that can be phrased in a weighted deduction system." ></td>
	<td class="line x" title="219:221	5 Conclusions We have presented KA, an extension of A parsing that allows extraction of optimal k-best parses without the need for an exhaustive 1-best pass." ></td>
	<td class="line x" title="220:221	We have shown in several domains that, with an appropriate heuristic, our algorithm can extract kbest lists in a fraction of the time required by current approaches to k-best extraction, giving the best of both A parsing and efficientk-best extraction, in a unified procedure." ></td>
	<td class="line x" title="221:221	965" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-2306
A Study of Translation Rule Classification for Syntax-based Statistical Machine Translation
Jiang, Hongfei;Li, Sheng;Yang, Muyun;Zhao, Tiejun;"></td>
	<td class="line x" title="1:119	Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 4550, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:119	c 2009 Association for Computational Linguistics A Study of Translation Rule Classification for Syntax-based Statistical Machine Translation Hongfei Jiang, Sheng Li, Muyun Yang and Tiejun Zhao School of Computer Science and Technology Harbin Institute of Technology {hfjiang,lisheng,ymy,tjzhao}@mtlab.hit.edu.cn Abstract Recently, numerous statistical machine translation models which can utilize various kinds of translation rules are proposed." ></td>
	<td class="line x" title="3:119	In these models, not only the conventional syntactic rules but also the non-syntactic rules can be applied." ></td>
	<td class="line x" title="4:119	Even the pure phrase rules are includes in some of these models." ></td>
	<td class="line x" title="5:119	Although the better performances are reported over the conventional phrase model and syntax model, the mixture of diversified rules still leaves much room for study." ></td>
	<td class="line x" title="6:119	In this paper, we present a refined rule classification system." ></td>
	<td class="line x" title="7:119	Based on this classification system, the rules are classified according to different standards, such as lexicalization level and generalization." ></td>
	<td class="line x" title="8:119	Especially, we refresh the concepts of the structure reordering rules and the discontiguous phrase rules." ></td>
	<td class="line x" title="9:119	This novel classification system may supports the SMT research community with some helpful references." ></td>
	<td class="line x" title="10:119	1 Introduction Phrase-based statistical machine translation models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004; Koehn, 2004; Koehn et al., 2007) have achieved significant improvements in translation accuracy over the original IBM word-based model." ></td>
	<td class="line x" title="11:119	However, there are still many limitations in phrase based models." ></td>
	<td class="line x" title="12:119	The most frequently pointed limitation is its inefficacy to modeling the structure reordering and the discontiguous corresponding." ></td>
	<td class="line oc" title="13:119	To overcome these limitations, many syntaxbased SMT models have been proposed (Wu, 1997; Chiang, 2007; Ding et al., 2005; Eisner, 2003; Quirk et al., 2005; Liu et al., 2007; Zhang et al., 2007; Zhang et al., 2008a; Zhang et al., 2008b; Gildea, 2003; Galley et al., 2004; Marcu et al., 2006; Bod, 2007)." ></td>
	<td class="line o" title="14:119	The basic motivation behind syntax-based model is that the syntax information has the potential to model the structure reordering and discontiguous corresponding by the intrinsic structural generalization ability." ></td>
	<td class="line n" title="15:119	Although remarkable progresses have been reported, the strict syntactic constraint (the both sides of the rules should strictly be a subtree of the whole syntax parse) greatly hinders the utilization of the non-syntactic translation equivalents." ></td>
	<td class="line x" title="16:119	To alleviate this constraint, a few works have attempted to make full use of the non-syntactic rules by extending their syntax-based models to more general frameworks." ></td>
	<td class="line x" title="17:119	For example, forest-to-string transformation rules have been integrated into the tree-to-string translation framework by (Liu et al., 2006; Liu et al., 2007)." ></td>
	<td class="line x" title="18:119	Zhang et al.(2008a) made it possible to utilize the non-syntactic rules and even the phrases which are used in phrase based model by advancing a general tree sequence to tree sequence framework based on the tree-to-tree model presented in (Zhang et al., 2007)." ></td>
	<td class="line x" title="20:119	In these models, various kinds of rules can be employed." ></td>
	<td class="line x" title="21:119	For example, as shown in Figure 1 and Figure 2, Figure 1 shows a Chinese-to-English sentence pair with syntax parses on both sides and the word alignments (dotted lines)." ></td>
	<td class="line x" title="22:119	Figure 2 lists some of the rules which can be extracted from the sentence pair in Figure 1 by the system used in (Zhang et al., 2008a)." ></td>
	<td class="line x" title="23:119	These rules includes not only conventional syntax rules but also the tree sequence rules (the multi-headed syntax rules )." ></td>
	<td class="line x" title="24:119	Even the phrase rules are adopted by 45 the system." ></td>
	<td class="line x" title="25:119	Although the better performances are reported over the conventional phrase-based model and syntax-based model, the mixture of diversified rules still leaves much room for study." ></td>
	<td class="line x" title="26:119	Given such a hybrid rule set, we must want to know what kinds of rules can make more important contributions to the overall system performance and what kinds of rules are redundant compared with the others." ></td>
	<td class="line x" title="27:119	From engineering point of view, the developers may concern about which kinds of rules should be preferred and which kinds of rules could be discard without too much decline in translation quality." ></td>
	<td class="line x" title="28:119	However, one of the precondition for the investigations of these issues is what are the rule categories?" ></td>
	<td class="line x" title="29:119	In other words, some comprehensive rule classifications are necessary to make the rule analyses feasible." ></td>
	<td class="line x" title="30:119	The motivation of this paper is to present such a rule classification." ></td>
	<td class="line x" title="31:119	2 Related Works A few researches have made some exploratory investigations towards the effects of different rules by classifying the translation rules into different subcategories (Liu et al., 2007; Zhang et al., 2008a; DeNeefe et al., 2007)." ></td>
	<td class="line x" title="32:119	Liu et al.(2007) differentiated the rules in their tree-to-string model which integrated with forest1-to-string into fully lexicalized rules, non-lexicalized rules and partial lexicalized rules according to the lexicalization levels." ></td>
	<td class="line x" title="34:119	As an extension, Zhang et al.(2008a) proposed two more categories: Structure Reordering Rules (SRR) and Discontiguous Phrase Rules (DPR)." ></td>
	<td class="line x" title="36:119	The SRR stands for the rules which have at least two non-terminal leaf nodes with inverted order in the source and target side." ></td>
	<td class="line x" title="37:119	And DPR refers to the rules having at least one non-terminal leaf node between two terminal leaf nodes." ></td>
	<td class="line x" title="38:119	(DeNeefe et al., 2007) made an illuminating breakdown of the different kinds of rules." ></td>
	<td class="line oc" title="39:119	Firstly, they classify all the GHKM2 rules (Galley et al., 2004; Galley et al., 2006) into two categories: lexical rules and non-lexical rules." ></td>
	<td class="line o" title="40:119	The former are the rules whose source side has no source words." ></td>
	<td class="line o" title="41:119	In other words, a non-lexical rule is a purely ab1A forest means a sub-tree sequence derived from a given parse tree 2One reviewer asked about the acronym GHKM." ></td>
	<td class="line oc" title="42:119	We guess it is an acronym for the authors of (Galley et al., 2004): Michel Galley, Mark Hopkins, Kevin Knight and Daniel Marcu." ></td>
	<td class="line x" title="43:119	 Figure 1: A syntax tree pair example." ></td>
	<td class="line x" title="44:119	Dotted lines stands for the word alignments." ></td>
	<td class="line x" title="45:119	stract rule." ></td>
	<td class="line x" title="46:119	The latter is the complementary set of the former." ></td>
	<td class="line x" title="47:119	And then lexical rules are classified further into phrasal rules and non-phrasal rules." ></td>
	<td class="line x" title="48:119	The phrasal rules refer to the rules whose source side and the yield of the target side contain exactly one contiguous phrase each." ></td>
	<td class="line x" title="49:119	And the one or more nonterminals can be placed on either side of the phrase." ></td>
	<td class="line x" title="50:119	In other words, each phrasal rule can be simulated by the conjunction of two more phrase rules." ></td>
	<td class="line x" title="51:119	(DeNeefe et al., 2007) classifies non-phrasal rules further into structural rules, re-ordering rules, and noncontiguous phrase rules." ></td>
	<td class="line x" title="52:119	However, these categories are not explicitly defined in (DeNeefe et al., 2007) since out of its focus." ></td>
	<td class="line x" title="53:119	Our proposed rule classification is inspired by these works." ></td>
	<td class="line x" title="54:119	3 Rules Classifications Currently, there have been several classifications in SMT research community." ></td>
	<td class="line x" title="55:119	Generally, the rules can be classified into two main groups according to whether syntax information is involved: bilingual phrases (Phrase) and syntax rules (Syntax)." ></td>
	<td class="line x" title="56:119	Further, the syntax rules can be divided into three categories according to the lexicalization levels (Liu et al., 2007; Zhang et al., 2008a): 1) Fully lexicalized (FLex): all leaf nodes in both the source and target sides are lexicons (terminals) 2) Unlexicalized (ULex): all leaf nodes in both the 46       Figure 2: Some rules can be extracted by the system used in (Zhang et al., 2008a) from the sentence pair in Figure 1." ></td>
	<td class="line x" title="57:119	source and target sides are non-lexicons (nonterminals) 3) Partially lexicalized (PLex): otherwise." ></td>
	<td class="line x" title="58:119	In Figure 2, R1-R3 are FLex rules, and R5-R8 are PLex rules." ></td>
	<td class="line x" title="59:119	Following (Zhang et al., 2008b), a syntax rule r can be formalized into a tuple < s,t,AT,ANT > , where s and t are tree sequences of source side and target side respectively, AT is a many-to-many correspondence set which includes the alignments between the terminal leaf nodes from source and target side, and ANT is a one-to-one correspondence set which includes the synchronizing relations between the non-terminal leaf nodes from source and target side." ></td>
	<td class="line x" title="60:119	Then, the syntax rules can also fall into two categories according to whether equipping with generalization capability (Chiang, 2007; Zhang et al., 2008a): 1) Initial rules (Initial): all leaf nodes of this rule are terminals." ></td>
	<td class="line x" title="61:119	2) Abstract rules (Abstract): otherwise, i.e. at least one leaf node is a non-terminal." ></td>
	<td class="line x" title="62:119	A non-terminal leaf node in a rule is named an abstract node since it has the generalization capability." ></td>
	<td class="line x" title="63:119	Comparing these two classifications for syntax rules, we can find that a FLex rule is a initial rule when ULex rules and PLex rules belong to abstract rules." ></td>
	<td class="line x" title="64:119	These classifications are clear and easy for understanding." ></td>
	<td class="line x" title="65:119	However, we argue that they need further refinement for in-depth study." ></td>
	<td class="line x" title="66:119	Specially, more refined differentiations are needed for the abstract rules (ULex rules and PLex rules) since they play important roles for the characteristic capabilities which are deemed to be the advantages over the phrase-based model." ></td>
	<td class="line x" title="67:119	For instance, the potentials to model the structure reordering and the discontiguous correspondence." ></td>
	<td class="line x" title="68:119	The Structure Reordering Rules (SRR) and Discontiguous Phrase Rules (DPR) mentioned by (Zhang et al., 2008a) can be regarded as more in-depth classification of the syntax rules." ></td>
	<td class="line x" title="69:119	In (Zhang et al., 2008a), they are described as follows: Definition 1: The Structure Reordering Rule (SRR) refers to the structure reordering rule that has at least two non-terminal leaf nodes with inverted order in the source and target side." ></td>
	<td class="line x" title="70:119	Definition 2: The Discontiguous Phrase Rule (DPR) refers to the rule having at least one nonterminal leaf node between two lexicalized leaf nodes." ></td>
	<td class="line x" title="71:119	47 Based on these descriptions, R7,R8 in Figure 2 belong to the category of SRR and R6,R7 fall into the category of DPR." ></td>
	<td class="line x" title="72:119	Although these two definitions are easy implemented in practice, we argue that the definition of SRR is not complete." ></td>
	<td class="line x" title="73:119	The reordering rules involving the reordering between content word terminals and non-terminal (such as R5 in Figure 2) also can model the useful structure reorderings." ></td>
	<td class="line x" title="74:119	Moreover, it is not uncommon that a rule demonstrates the reorderings between two non-terminals as well as the reorderings between one non-terminal and one content word terminal." ></td>
	<td class="line x" title="75:119	The reason for our emphasis of content word terminal is that the reorderings between the non-terminals and function word are less meaningful." ></td>
	<td class="line x" title="76:119	One of the theoretical problems with phrase based SMT models is that they can not effectively model the discontiguous translations and numerous attempts have been made on this issue (Simard et al., 2005; Quirk and Menezes, 2006; Wellington et al., 2006; Bod, 2007; Zhang et al., 2007)." ></td>
	<td class="line x" title="77:119	What seems to be lacking, however, is a explicit definition to the discontiguous translation." ></td>
	<td class="line x" title="78:119	The definition of DPR in (Zhang et al., 2008a) is explicit but somewhat rough and not very accurate." ></td>
	<td class="line x" title="79:119	For example, in Figure 3(a), non-terminal node pair ([0,], [0,love] ) is surrounded by lexical terminals." ></td>
	<td class="line x" title="80:119	According to Definition 2, it is a DPR." ></td>
	<td class="line x" title="81:119	However, obviously it is not a discontiguous phrase actually." ></td>
	<td class="line x" title="82:119	This rule can be simulated by conjunctions of three phrases (, I; , love; F,you)." ></td>
	<td class="line x" title="83:119	In contrast, the translation rule in Figure 3(b) is an actual discontiguous phrase rule." ></td>
	<td class="line x" title="84:119	The English correspondences of the Chinese word 1 is dispersed in the English side in which the correspondence of Chinese word  is inserted." ></td>
	<td class="line x" title="85:119	This rule can not be simulated by any conjunctions of the sub phrases." ></td>
	<td class="line x" title="86:119	It must be noted that the discontiguous phrase (1-switch  off) can not be abstracted under the existing synchronous grammar frameworks." ></td>
	<td class="line x" title="87:119	The fundamental reason is that the corresponding parts should be abstracted in the same time and lexicalized in the same time." ></td>
	<td class="line x" title="88:119	In other words, the discontiguous phrase can not be modeled by the permutation between non-terminals (abstract nodes)." ></td>
	<td class="line x" title="89:119	Another point to notice is that our focus in this paper is the ability demonstrated by the abstract rules." ></td>
	<td class="line x" title="90:119	Thus, we do not pay much attentions to the reorderings and discontiguous phrases involved in the  Figure 3: Examples for demonstrating the actual discontiguous phrase." ></td>
	<td class="line x" title="91:119	(a) is a negative example for the definition of DPR in (Zhang et al., 2008a), (b) is a actual discontiguous phrase rule." ></td>
	<td class="line x" title="92:119	2 Figure 4: The rule classifications used in this paper." ></td>
	<td class="line x" title="93:119	(a) shows that the rules can be divided into phrase rules and syntax rules according to whether a rule includes the syntactic information." ></td>
	<td class="line x" title="94:119	(b) illustrates that the syntax rules can be classified into three kinds according to the lexicalization level." ></td>
	<td class="line x" title="95:119	(c) shows that the abstract rules can be classified into more refined sub-categories." ></td>
	<td class="line x" title="96:119	phrase rules (e.g. 1-switch the light off) since they lack the generalization capability." ></td>
	<td class="line x" title="97:119	Therefore, the discontiguous phrase is limited to the relation between non-terminals and terminals." ></td>
	<td class="line x" title="98:119	On the basis of the above analyses, we present a novel classification system for the abstract rules based on the crossings between the leaf node alignment links." ></td>
	<td class="line x" title="99:119	Given an abstract rule r =< s,t,AT,ANT >, it is 1) a Structure Reordering Rule (SRR), if  a link l  ANT is crossed with a link lprime {AT ANT} a) a SRR NT2 rule, if the link lprime  ANT b) a SRR NT-T rule, if the link lprime  AT 2) not a Structure Reordering Rule (N-SRR), otherwise." ></td>
	<td class="line x" title="100:119	48    Figure 5: The patterns to show the characteristics of discontiguous phrase rules." ></td>
	<td class="line x" title="101:119	Note that the intersection of SRR NT2 and SRR NTT is not necessary an empty set, i.e. a rule can be both SRR NT2 and SRR NT-T rule." ></td>
	<td class="line x" title="102:119	The basic characteristic of the discontiguous translation is that the correspondence of one nonterminal NT is inserted among the correspondences of one phrase X. Figure 5 (a) illustrates this situation." ></td>
	<td class="line x" title="103:119	However, this characteristic can not support necessary and sufficient condition." ></td>
	<td class="line x" title="104:119	For example, if the phrase X can be divided like Figure 5 (b), then the rule in Figure 5 (a) is actually a reordering rule rather than a discontiguous phrase rule." ></td>
	<td class="line x" title="105:119	For sufficient condition, we constrain that the phrase X = wi wj need to satisfy the requirement: wi should be connected with wj through word alignment links (A word is connected with itself)." ></td>
	<td class="line x" title="106:119	In Figure 5(c), f1 is connected with f2 when NTprime is inserted between e1 and e2." ></td>
	<td class="line x" title="107:119	Thus, the rule in Figure 5(c) is a discontiguous phrase rule." ></td>
	<td class="line x" title="108:119	Definition 3: Given an abstract rule r =< s,t,AT,ANT >, it is a Discontiguous Phrase iff two links lt1, lt2 from AT and a link lnt from ANT , satisfy: lt1, lt2 are emitted from the same word and lt1 is crossed with lnt when lt2 is not crossed with lnt." ></td>
	<td class="line x" title="109:119	Through Definition 3, we know that the DPR is a sub-set of the SRR NT-T." ></td>
	<td class="line x" title="110:119	4 Conclusions and Future Works In this paper, we present a refined rule classification system." ></td>
	<td class="line x" title="111:119	Based on this classification system, the rules are classified according to different standards, such as lexicalization level and generalization." ></td>
	<td class="line x" title="112:119	Especially, we refresh the concepts of the structure reordering rules and the discontiguous phrase rules." ></td>
	<td class="line x" title="113:119	This novel classification system may supports the SMT research community with some helpful references." ></td>
	<td class="line x" title="114:119	In the future works, aiming to analyze the rule contributions and the redundances issues using the presented rule classification based on some real translation systems, we plan to implement some synchronous grammar based syntax translation models such as the one presented in (Liu et al., 2007) or in (Zhang et al., 2008a)." ></td>
	<td class="line x" title="115:119	Taking such a system as the experimental platform, we can perform comprehensive statistics about distributions of different rule categories." ></td>
	<td class="line x" title="116:119	What is more important, the contribution of each rule category can be evaluated seriatim." ></td>
	<td class="line x" title="117:119	Furthermore, which kinds of rules are preferentially applied in the 1-best decoding can be studied." ></td>
	<td class="line x" title="118:119	All these investigations could reveal very useful information for the optimization of rule extraction and the improvement of the computational models for synchronous grammar based machine translation." ></td>
	<td class="line x" title="119:119	Acknowledgments This work is supported by the Key Program of National Natural Science Foundation of China (60736014), and the Key Project of the National High Technology Research and Development Program of China (2006AA010108)." ></td>
</tr></table>
</div
</body></html>
