<html><body><head><link rel="stylesheet" type="text/css" href="style.css" /><script src="map.js"></script><script src="jquery-1.7.1.min.js"></script></head>
<div class="dstPaperData">
J93-1007 <div class="dstPaperTitle">Retrieving Collocations From Text: Xtract</div><div class="dstPaperAuthors">Smadja, Frank A.;</div>
</div>
<table cellspacing="0" cellpadding="0"><tr>
	<td class="srcData" >Source Paper</td>
	<td class="pp legend" ><input type="checkbox" id="cbIPositive" checked="true"/><label for="cbIPositive">Informal +<label></td>
	<td class="nn legend" ><input type="checkbox" id="cbINegative" checked="true"/><label for="cbINegative">Informal -<label></td>
	<td class="oo legend" ><input type="checkbox" id="cbIObjective" checked="true"/><label for="cbIObjective">Informal Neutral<label></td>
	<td class="ppc legend" ><input type="checkbox" id="cbEPositive" checked="true"/><label for="cbEPositive">Formal +</label></td>
	<td class="nnc legend" ><input type="checkbox" id="cbENegative" checked="true"/><label for="cbENegative">Formal -</label></td>
	<td class="ooc legend" ><input type="checkbox" id="cbEObjective" checked="true"/><label for="cbEObjective">Formal Neutral</label></td>
	<td class="lb"><input type="checkbox" id="cbSentenceBoundary"/><label for="cbSentenceBoundary">Sentence Boundary</label></td>
</tr></table>
<div class="dstPaper">
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="A94-1006
Termight: Identifying And Translating Technical Terminology
Dagan, Ido;Church, Kenneth Ward;"></td>
	<td class="line x" title="1:178	Termight: Identifying and Translating Technical Terminology Ido Dagan* Ken Church AT&T Bell Laboratories 600 Mountain Ave. Murray Hill, NJ 07974, USA dagan~bimacs, cs." ></td>
	<td class="line x" title="2:178	biu." ></td>
	<td class="line x" title="3:178	ac." ></td>
	<td class="line x" title="4:178	il kwc@research, att." ></td>
	<td class="line x" title="5:178	com Abstract We propose a semi-automatic tool, termight, that helps professional translators and terminologists identify technical terms and their translations." ></td>
	<td class="line x" title="6:178	The tool makes use of part-of-speech tagging and wordalignment programs to extract candidate terms and their translations." ></td>
	<td class="line x" title="7:178	Although the extraction programs are far from perfect, it isn't too hard for the user to filter out the wheat from the chaff." ></td>
	<td class="line x" title="8:178	The extraction algorithms emphasize completeness." ></td>
	<td class="line x" title="9:178	Alternative proposals are likely to miss important but infrequent terms/translations." ></td>
	<td class="line x" title="10:178	To reduce the burden on the user during the filtering phase, candidates are presented in a convenient order, along with some useful concordance evidence, in an interface that is designed to minimize keystrokes." ></td>
	<td class="line x" title="11:178	Termight is currently being used by the translators at AT T Business Translation Services (formerly AT&T Language Line Services)." ></td>
	<td class="line x" title="12:178	1 Terminology: An Application for Natural Language Technology The statistical corpus-based renaissance in computational linguistics has produced a number of interesting technologies, including part-of-speech tagging and bilingual word alignment." ></td>
	<td class="line x" title="13:178	Unfortunately, these technologies are still not as widely deployed in practical applications as they might be." ></td>
	<td class="line x" title="14:178	Part-ofspeech taggers are used in a few applications, such as speech synthesis (Sproat et al. , 1992) and question answering (Kupiec, 1993b)." ></td>
	<td class="line x" title="15:178	Word alignment is newer, found only in a few places (Gale and Church, 1991a; Brown et al. , 1993; Dagan et al. , 1993)." ></td>
	<td class="line x" title="16:178	It is used at IBM for estimating parameters of their statistical machine translation prototype (Brown et *Author's current address: Dept. of Mathematics and Computer Science, Bar Ilan University, Ramat Gan 52900, Israel." ></td>
	<td class="line x" title="17:178	al., 1993)." ></td>
	<td class="line x" title="18:178	We suggest that part of speech tagging and word alignment could have an important role in glossary construction for translation." ></td>
	<td class="line x" title="19:178	Glossaries are extremely important for translation." ></td>
	<td class="line x" title="20:178	How would Microsoft, or some other software vendor, want the term 'Character menu' to be translated in their manuals?" ></td>
	<td class="line x" title="21:178	Technical terms are difficult for translators because they are generally not as familiar with the subject domain as either the author of the source text or the reader of the target text." ></td>
	<td class="line x" title="22:178	In many cases, there may be a number of acceptable translations, but it is important for the sake of consistency to standardize on a single one." ></td>
	<td class="line x" title="23:178	It would be unacceptable for a manual to use a variety of synonyms for a particular menu or button." ></td>
	<td class="line x" title="24:178	Customarily, translation houses make extensive job-specific glossaries to ensure consistency and correctness of technical terminology for large jobs." ></td>
	<td class="line x" title="25:178	A glossary is a list of terms and their translations." ></td>
	<td class="line x" title="26:178	1 We will subdivide the task of constructing a glossary into two subtasks: (1) generating a list of terms, and (2) finding the translation equivalents." ></td>
	<td class="line x" title="27:178	The first task will be referred to as the monolingual task and the second as the bilingual task." ></td>
	<td class="line x" title="28:178	How should a glossary be constructed?" ></td>
	<td class="line x" title="29:178	Translation schools teach their students to read as much background material as possible in both the source and target languages, an extremely time-consuming process, as the introduction to Hann's (1992, p. 8) text on technical translation indicates: Contrary to popular opinion, the job of a technical translator has little in common with other linguistic professions, such as literature translation, foreign correspondence or interpreting." ></td>
	<td class="line x" title="30:178	Apart from an expert knowledge of both languages, all that is required for the latter professions is a few general dictionaries, whereas a technical translator needs a whole library of specialized dictionaries, encyclopedias and 1The source and target fields are standard, though many other fields can also be found, e.g., usage notes, part of speech constraints, comments, etc. 34 technical literature in both languages; he is more concerned with the exact meanings of terms than with stylistic considerations and his profession requires certain 'detective' skills as well as linguistic and literary ones." ></td>
	<td class="line x" title="31:178	Beginners in this profession have an especially hard time This book attempts to meet this requirement." ></td>
	<td class="line x" title="32:178	Unfortunately, the academic prescriptions are often too expensive for commercial practice." ></td>
	<td class="line x" title="33:178	Translators need just-in-time glossaries." ></td>
	<td class="line x" title="34:178	They cannot afford to do a lot of background reading and 'detective' work when they are being paid by the word." ></td>
	<td class="line x" title="35:178	They need something more practical." ></td>
	<td class="line x" title="36:178	We propose a tool, termight, that automates some of the more tedious and laborious aspects of terminology research." ></td>
	<td class="line x" title="37:178	The tool relies on part-of-speech tagging and word-alignment technologies to extract candidate terms and translations." ></td>
	<td class="line x" title="38:178	It then sorts the extracted candidates and presents them to the user along with reference concordance lines, supporting efficient construction of glossaries." ></td>
	<td class="line x" title="39:178	The tool is currently being used by the translators at AT&T Business Translation Services (formerly AT~T Language Line Services)." ></td>
	<td class="line x" title="40:178	Termight may prove useful in contexts other than human-based translation." ></td>
	<td class="line x" title="41:178	Primarily, it can support customization of machine translation (MT) lexicons to a new domain." ></td>
	<td class="line x" title="42:178	In fact, the arguments for constructing a job-specific glossary for human-based translation may hold equally well for an MT-based process, emphasizing the need for a productivity tool." ></td>
	<td class="line x" title="43:178	The monolingual component of termigM can be used to construct terminology lists in other applications, such as technical writing, book indexing, hypertext linking, natural language interfaces, text categorization and indexing in digital libraries and information retrieval (Salton, 1988; Cherry, 1990; Harding, 1982; Bourigault, 1992; Damerau, 1993), while the bilingual component can be useful for information retrieval in multilingual text collections (Landauer and Littman, 1990)." ></td>
	<td class="line x" title="44:178	2 Monolingual Task: An Application for Part-of-Speech Tagging Although part-of-speech taggers have been around for a while, there are relatively few practical applications of this technology." ></td>
	<td class="line x" title="45:178	The monolingual task appears to be an excellent candidate." ></td>
	<td class="line x" title="46:178	As has been noticed elsewhere (Bourigault, 1992; Justeson and Katz, 1993), most technical terms can be found by looking for multiword noun phrases that satisfy a rather restricted set of syntactic patterns." ></td>
	<td class="line x" title="47:178	We follow Justeson and Katz (1993) who emphasize the importance of term frequency in selecting good candidate terms." ></td>
	<td class="line x" title="48:178	An expert terminologist can then skim the list of candidates to weed out spurious candidates and cliches." ></td>
	<td class="line x" title="49:178	Very simple procedures of this kind have been remarkably successful." ></td>
	<td class="line x" title="50:178	They can save an enormous amount of time over the current practice of reading the document to be translated, focusing on tables, figures, index, table of contents and so on, and writing down terms that happen to catch the translator's eye." ></td>
	<td class="line x" title="51:178	This current practice is very laborious and runs the risk of missing many important terms." ></td>
	<td class="line x" title="52:178	Termight uses a part of speech tagger (Church, 1988) to identify a list of candidate terms which is then filtered by a manual pass." ></td>
	<td class="line x" title="53:178	We have found, however, that the manual pass dominates the cost of the monolingual task, and consequently, we have tried to design an interactive user interface (see Figure 1) that minimizes the burden on the expert terminologist." ></td>
	<td class="line x" title="54:178	The terminologist is presented with a list of candidate terms, and corrects the list with a minimum number of key strokes." ></td>
	<td class="line x" title="55:178	The interface is designed to make it easy for the expert to pull up evidence from relevant concordance lines to help identify incorrect candidates as well as terms that are missing from the list." ></td>
	<td class="line x" title="56:178	A single key-press copies the current candidate term, or the content of any marked emacs region, into the upper-left screen." ></td>
	<td class="line x" title="57:178	The candidates are sorted so that the better ones are found near the top of the list, and so that related candidates appear near one another." ></td>
	<td class="line x" title="58:178	2.1 Candidate terms and associated concordance lines Candidate terms." ></td>
	<td class="line x" title="59:178	The list of candidate terms contains both multi-word noun phrases and single words." ></td>
	<td class="line x" title="60:178	The multi-word terms match a small set of syntactic patterns defined by regular expressions and are found by searching a version of the document tagged with parts of speech (Church, 1988)." ></td>
	<td class="line x" title="61:178	The set of syntactic patterns is considered as a parameter and can be adopted to a specific domain by the user." ></td>
	<td class="line x" title="62:178	Currently our patterns match only sequences of nouns, which seem to yield the best hit rate in our environment." ></td>
	<td class="line x" title="63:178	Single-word candidates are defined by taking the list of all words that occur in the document and do not appear in a standard stop-list of 'noise' words." ></td>
	<td class="line x" title="64:178	Grouping and sorting of terms." ></td>
	<td class="line x" title="65:178	The list of candidate terms is sorted to group together all noun phrase terms that have the same head word (as in Figure 1), which is simply the last word of the term for our current set of noun phrase patterns." ></td>
	<td class="line x" title="66:178	The order of the groups in the list is determined by decreasing frequency of the head word in the document, which usually correlates with the likelihood that this head word is used in technical terms." ></td>
	<td class="line x" title="67:178	Sorting within groups." ></td>
	<td class="line x" title="68:178	Under each head word the terms are sorted alphabetically according to reversed order of the words." ></td>
	<td class="line x" title="69:178	Sorting in this order reflects the order of modification in simple English noun phrases and groups together terms that denote different modifications of a more general term (see 35 File Edit Buffers Help software settings settings font slze de@ault paper size paper size anchor point decimal point end point hgphenatlon point insertion point software settings specific settings settings change slze ~cnt size default paper size paper size umn size size anchor point decimal point end point hgphenatlon point lllssrtlon point ~neertlon point point 5sttlng Advanced Options Ineertlon point 117709: ourtd in gout Write document." ></td>
	<td class="line x" title="70:178	To move the_ insertion I17894: Choose OK. The dialog box dleappears, and the Ineertlon 66058: Chepter 2 Application Baslce__2." ></td>
	<td class="line x" title="71:178	Place the Ineertlon 122717: Inden\[atlon of a paragraph:_ I. Place the insertion 122811: aragreph bw ualng the Ruler:_ I. Place the insertion 122909: _ To create a hanslng Indent:_l." ></td>
	<td class="line x" title="72:178	Place the insertion i17546: Intlng Inelde the window and_ where the Inaertlon 36195: nd date to ang Hotep6d document:_ Move the insertion 35870: search for text in flotepad:_ 1." ></td>
	<td class="line x" title="73:178	Move the insertion 46478: destination_ document are vlslble._3." ></td>
	<td class="line x" title="74:178	Move the insertion 32436: into which gou want to insert the text Move the insertion 67442: ext llne." ></td>
	<td class="line x" title="75:178	Press the 5tACEBAR to_ move the insertion 44476: card._ If gou are using Write." ></td>
	<td class="line x" title="76:178	move the insertion 67667: first character gou want to select and drag the insertion 35932: tch Case check box_ 3." ></td>
	<td class="line x" title="77:178	To search ?lom the insertion 35957: default setting is Oown." ></td>
	<td class="line x" title="78:178	This searches from the 12092g: insert a manual page break:_ Position the ~olnt." ></td>
	<td class="line x" title="79:178	point to a location in the docume ~olnt moves to the_ selected pa ~olnt at the place you want the Informer ~olnt inside the paragraph you 'want to ~olnt inside the paragraph @ou want to c ~olnt Inelde the paragraph in 'which gou oolnt will move to if gou click a locatl oolnt to the place gou want the time and 0olnt to the place gou want to start the 0olnt to the place gou want the package 0olnt_ to the place gou want the text oolnt one space to the right._ To oolnt to the place gou want the object_ 3olnt to the last_ character gou oolnt to the besinrtln S of the fde." ></td>
	<td class="line x" title="80:178	sele insertion-point to the end of the file,_ insertion point where you want the page break and Figure 1: The monolingual user interface consists of three screens: (1) the input list of candidate terms (upper right), (2) the output list of terms, as constructed by the user (upper left), and (3) the concordance lines associated with the current term, as indicated by the cursor position in screen 1." ></td>
	<td class="line x" title="81:178	Typos are due to OCR errors." ></td>
	<td class="line x" title="82:178	Underscores denote line breaks." ></td>
	<td class="line x" title="83:178	for example the terms default paper size, paper size and size in Figure 1)." ></td>
	<td class="line x" title="84:178	Concordance lines." ></td>
	<td class="line x" title="85:178	To decide whether a candidate term is indeed a term, and to identify multiword terms that are missing from the candidate list, one must view relevant lines of the document." ></td>
	<td class="line x" title="86:178	For this purpose we present a concordance line for each occurrence of a term (a text line centered around the term)." ></td>
	<td class="line x" title="87:178	If, however, a term, tl, (like 'point') is contained in a longer term, $2, (like 'insertion point' or 'decimal point') then occurrences of t2 are not displayed for tl." ></td>
	<td class="line x" title="88:178	This way, the occurrences of a general term (or a head word) are classified into disjoint sets corresponding to more specific terms, leaving only unclassified occurrences under the general term." ></td>
	<td class="line x" title="89:178	In the case of 'point', for example, five specific terms are identified that account for 61 occurrences of 'point', and accordingly, for 61 concordance lines." ></td>
	<td class="line x" title="90:178	Only 20 concordance lines are displayed for the word 'point' itself, and it is easy to identify in them 5 occurrences of the term 'starting point', which is missing from the candidate list (because 'starting' is tagged as a verb)." ></td>
	<td class="line x" title="91:178	To facilitate scanning, concordance lines are sorted so that all occurrences of identical preceding contexts of the head word, like 'starting', are grouped together." ></td>
	<td class="line x" title="92:178	Since all the words of the document, except for stop list words, appear in the candidate list as single-word terms it is guaranteed that every term that was missed by the automatic procedure will appear in the concordance lines." ></td>
	<td class="line x" title="93:178	In summary, our algorithm performs the following steps:  Extract multi-word and single-word candidate terms." ></td>
	<td class="line x" title="94:178	 Group terms by head word and sort groups by head-word frequency." ></td>
	<td class="line x" title="95:178	 Sort terms within each group alphabetically in reverse word order." ></td>
	<td class="line x" title="96:178	 Associate concordance lines with each term." ></td>
	<td class="line x" title="97:178	An occurrence of a multi-word term t is not associated with any other term whose words are found in t.  Sort concordance lines of a term alphabetically according to preceding context." ></td>
	<td class="line x" title="98:178	2.2 Evaluation Using the monolingual component, a terminologist at AT&T Business Translation Services constructs terminology lists at the impressive rate of 150-200 terms per hour." ></td>
	<td class="line x" title="99:178	For example, it took about 10 hours to construct a list of 1700 terms extracted from a 300,000 word document." ></td>
	<td class="line x" title="100:178	The tool has at least doubled the rate of constructing terminology lists, which was previously performed by simpler lexicographic tools." ></td>
	<td class="line x" title="101:178	36 2.3 Comparison with related work Alternative proposals are likely to miss important but infrequent terms/translations such as 'Format Disk dialog box' and 'Label Disk dialog box' which occur just once." ></td>
	<td class="line oc" title="102:178	In particular, mutual information (Church and Hanks, 1990; Wu and Su, 1993) and other statistical methods such as (Smadja, 1993) and frequency-based methods such as (Justeson and Katz, 1993) exclude infrequent phrases because they tend to introduce too much noise." ></td>
	<td class="line x" title="103:178	We have found that frequent head words are likely to generate a number of terms, and are therefore more important for the glossary (a 'productivity' criterion)." ></td>
	<td class="line x" title="104:178	Consider the frequent head word box." ></td>
	<td class="line x" title="105:178	In the Microsoft Windows manual, for example, almost any type of box is a technical term." ></td>
	<td class="line x" title="106:178	By sorting on the frequency of the headword, we have been able to find many infrequent terms, and have not had too much of a problem with noise (at least for common headwords)." ></td>
	<td class="line x" title="107:178	Another characteristic of previous work is that each candidate term is scored independently of other terms." ></td>
	<td class="line x" title="108:178	We score a group of related terms rather than each term at a time." ></td>
	<td class="line x" title="109:178	Future work may enhance our simple head-word frequency score and may take into account additional relationships between terms, including common words in modifying positions." ></td>
	<td class="line x" title="110:178	Termight uses a part-of-speech tagger to identify candidate noun phrases." ></td>
	<td class="line x" title="111:178	Justeson and Katz (1993) only consult a lexicon and consider all the possible parts of speech of a word." ></td>
	<td class="line x" title="112:178	In particular, every word that can be a noun according to the lexicon is considered as a noun in each of its occurrences." ></td>
	<td class="line x" title="113:178	Their method thus yields some incorrect noun phrases that will not be proposed by a tagger, but on the other hand does not miss noun phrases that may be missed due to tagging errors." ></td>
	<td class="line x" title="114:178	3 Bilingual Task: An Application for Word Alignment 3.1 Sentence and word alignment Bilingual alignment methods (Warwick et al. , 1990; Brown et al. , 1991a; Brown et al. , 1993; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Roscheisen, 1993; Simard et al. , 1992; Church, 1993; Kupiec, 1993a; Matsumoto et al. , 1993; Dagan et al. , 1993)." ></td>
	<td class="line o" title="115:178	have been used in statistical machine translation (Brown et al. , 1990), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993; van der Eijk, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990; Smadja, 1992), word-sense disambiguation (Brown et al. , 1991b; Gale et al. , 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990)." ></td>
	<td class="line o" title="116:178	Most alignment work was concerned with alignment at the sentence level." ></td>
	<td class="line x" title="117:178	Algorithms for the more difficult task of word alignment were proposed in (Gale and Church, 1991a; Brown et al. , 1993; Dagan et al. , 1993) and were applied for parameter estimation in the IBM statistical machine translation system (Brown et al. , 1993)." ></td>
	<td class="line x" title="118:178	Previously translated texts provide a major source of information about technical terms." ></td>
	<td class="line x" title="119:178	As Isabelle (1992) argues, 'Existing translations contain more solutions to more translation problems than any other existing resource'." ></td>
	<td class="line x" title="120:178	Even if other resources, such as general technical dictionaries, are available it is important to verify the translation of terms in previously translated documents of the same customer (or domain) to ensure consistency across documents." ></td>
	<td class="line x" title="121:178	Several translation workstations provide sentence alignment and allow the user to search interactively for term translations in aligned archives (e.g.(Ogden and Gonzales, 1993))." ></td>
	<td class="line oc" title="123:178	Some methods use sentence alignment and additional statistics to find candidate translations of terms (Smadja, 1992; van der Eijk, 1993)." ></td>
	<td class="line x" title="124:178	We suggest that word level alignment is better suitable for term translation." ></td>
	<td class="line x" title="125:178	The bilingual component of termight gets as input a list of source terms and a bilingual corpus aligned at the word level." ></td>
	<td class="line x" title="126:178	We have been using the output of word_align, a robust alignment program that proved useful for bilingual concordancing of noisy texts (Dagan et al. , 1993)." ></td>
	<td class="line x" title="127:178	Word_align produces a partial mapping between the words of the two texts, skipping words that cannot be aligned at a given confidence level (see Figure 2)." ></td>
	<td class="line x" title="128:178	3.2 Candidate translations and associated concordance lines For each occurrence of a source term, termight identifies a candidate translation based on the alignment of its words." ></td>
	<td class="line x" title="129:178	The candidate translation is defined as the sequence of words between the first and last target positions that are aligned with any of the words of the source term." ></td>
	<td class="line x" title="130:178	In the example of Figure 2 the candidate translation of Optional Parameters box is zone Parametres optionnels, since zone and optionnels are the first and last French words that are aligned with the words of the English term." ></td>
	<td class="line x" title="131:178	Notice that in this case the candidate translation is correct even though the word Parameters is aligned incorrectly." ></td>
	<td class="line x" title="132:178	In other cases alignment errors may lead to an incorrect candidate translation for a specific occurrence of the term." ></td>
	<td class="line x" title="133:178	It is quite likely, however, that the correct translation, or at least a string that overlaps with it, will be identified in some occurrences of the term." ></td>
	<td class="line x" title="134:178	Termight collects the candidate translations from all occurrences of a source term and sorts them in decreasing frequency order." ></td>
	<td class="line x" title="135:178	The sorted list is presented to the user, followed by bilingual concordances for all occurrences of each candidate translation (see Figure 3)." ></td>
	<td class="line x" title="136:178	The user views the concordances to verify correct candidates or to find translations that are 37 You can type application parameters in the Optional Parameters box." ></td>
	<td class="line x" title="137:178	Vous pouvez tapez les parametres d'une application dans la zone Parametres optionnels." ></td>
	<td class="line x" title="138:178	Figure 2: An example of word_align's output for the English and French versions of the Microsoft Windows manual." ></td>
	<td class="line x" title="139:178	The alignment of Parameters to optionnels is an error." ></td>
	<td class="line x" title="140:178	missing from the candidate list." ></td>
	<td class="line x" title="141:178	The latter task becomes especially easy when a candidate overlaps with the correct translation, directing the attention of the user to the concordance lines of this particular candidate, which are likely to be aligned correctly." ></td>
	<td class="line x" title="142:178	A single key-stroke copies a verified candidate translation, or a translation identified as a marked emacs region in a concordance line, into the appropriate place in the glossary." ></td>
	<td class="line x" title="143:178	3.3 Evaluation We evaluated the bilingual component of termight in translating a glossary of 192 terms found in the English and German versions of a technical manual." ></td>
	<td class="line x" title="144:178	The correct answer was often the first choice (40%) or the second choice (7%) in the candidate list." ></td>
	<td class="line x" title="145:178	For the remaining 53% of the terms, the correct answer was always somewhere in the concordances." ></td>
	<td class="line x" title="146:178	Using the interface, the glossary was translated at a rate of about 100 terms per hour." ></td>
	<td class="line oc" title="147:178	3.4 Related work and issues for future research Smadja (1992) and van der Eijk (1993) describe term translation methods that use bilingual texts that were aligned at the sentence level." ></td>
	<td class="line o" title="148:178	Their methods find likely translations by computing statistics on term cooccurrence within aligned sentences and selecting source-target pairs with statistically significant associations." ></td>
	<td class="line x" title="149:178	We found that explicit word alignments enabled us to identify translations of infrequent terms that would not otherwise meet statistical significance criteria." ></td>
	<td class="line x" title="150:178	If the words of a term occur at least several times in the document (regardless of the term frequency) then word_align is likely to align them correctly and termight will identify the correct translation." ></td>
	<td class="line x" title="151:178	If only some of the words of a term are frequent then termight is likely to identify a translation that overlaps with the correct one, directing the user quickly to correctly aligned concordance lines." ></td>
	<td class="line x" title="152:178	Even if all the words of the term were not Migned by word_align it is still likely that most concordance lines are aligned correctly based on other words in the near context." ></td>
	<td class="line x" title="153:178	Termight motivates future improvements in word alignment quality that will increase recall and precision of the candidate list." ></td>
	<td class="line x" title="154:178	In particular, taking into account local syntactic structures and phrase boundaries will impose more restrictions on alignments of complete terms." ></td>
	<td class="line x" title="155:178	Finally, termight can be extended for verifying translation consistency at the proofreading (editing) step of a translation job, after the document has been translated." ></td>
	<td class="line x" title="156:178	For example, in an English-German document pair the tool identified the translation of the term Controls menu as Menu Steuerung in 4 out of 5 occurrences." ></td>
	<td class="line x" title="157:178	In the fifth occurrence word_align failed to align the term correctly because another translation, Steuermenu, was uniquely used, violating the consistency requirement." ></td>
	<td class="line x" title="158:178	Termight, or a similar tool, can thus be helpful in identifying inconsistent translations." ></td>
	<td class="line x" title="159:178	4 Conclusions We have shown that terminology research provides a good application for robust natural language technology, in particular for part-of-speech tagging and word-alignment algorithms." ></td>
	<td class="line x" title="160:178	Although the output of these algorithms is far from perfect, it is possible to extract from it useful information that is later corrected and augmented by a user." ></td>
	<td class="line x" title="161:178	Our extraction algorithms emphasize completeness, and identify also infrequent candidates that may not meet some of the statistical significance criteria proposed in the literature." ></td>
	<td class="line x" title="162:178	To make the entire process efficient, however, it is necessary to analyze the user's work process and provide interfaces that support it." ></td>
	<td class="line x" title="163:178	In many cases, improving the way information is presented to the user may have a larger effect on productivity than improvements in the underlying natural language technology." ></td>
	<td class="line x" title="164:178	In particular, we have found the following to be very effective:  Grouping linguistically related terms, making it easier to judge their validity." ></td>
	<td class="line x" title="165:178	 Sorting candidates such that the better ones are found near the top of the list." ></td>
	<td class="line x" title="166:178	With this sorting one's time is efficiently spent by simply going down the list as far as time limitations permit." ></td>
	<td class="line x" title="167:178	 Providing quick access to relevant concordance lines to help identify incorrect candidates as well as terms or translations that are missing from the candidate list." ></td>
	<td class="line x" title="168:178	38 Character menu menu Caracteres 4 2 Itallque 2 3 No translations available 1 menu Caracteres 4 121163: Formatting characters 135073: ~orme des caracteres The commands on the Character menu control how you ?ormat the Lee commandes du menu Caracteree permettent de determiner la pr 121294: 135188: : Chooee the stgle gou want to use ?rom the Character menu, Tgpe gout tcxt." ></td>
	<td class="line x" title="169:178	The CholsIssez le stgle voulu dans le menu Caracteres." ></td>
	<td class="line x" title="170:178	Tapez votre texte." ></td>
	<td class="line x" title="171:178	Le texte S: Card menu T: menu Fiche S: Character menu T: menu Caracteres S: Control menu T: S: Disk menu T: Figure 3: The bilingual user interface consists of two screens." ></td>
	<td class="line x" title="172:178	The lower screen contains the constructed glossary." ></td>
	<td class="line x" title="173:178	The upper screen presents the current term, candidate translations with their frequencies and a bilingual concordance for each candidate." ></td>
	<td class="line x" title="174:178	Typos are due to OCR errors." ></td>
	<td class="line x" title="175:178	 Minimizing the number of required key-strokes." ></td>
	<td class="line x" title="176:178	As the need for efficient knowledge acquisition tools becomes widely recognized, we hope that this experience with termight will be found useful for other text-related systems as well." ></td>
	<td class="line x" title="177:178	Acknowledgements We would like to thank Pat Callow from AT&T Buiseness Translation Services (formerly AT&T Language Line Services) for her indispensable role in designing and testing termight." ></td>
	<td class="line x" title="178:178	We would also like to thank Bala Satish and Jon Helfman for their part in the project." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C94-1074
A 'Not-So-Shallow' Parser For Collocational Analysis
Basili, Roberto;Pazienza, Maria Teresa;Velardi, Paola;"></td>
	<td class="line x" title="1:270	A 'not-so-shallow' parser for collocational analysis Basili R.(*), M.T. Pazienza (*), P. Velardi () (*) Dipartimento Ingegneria Elettronica, Universit,~ di Roma,Tor Vergata \[rbas, pazienza}@tovvxl, ccd.utow:m, it: () Istituto di Informatica, Universitk di Ancona vela@anvax2, cinec~." ></td>
	<td class="line x" title="2:270	J_t Abstract." ></td>
	<td class="line x" title="3:270	Collocational analysis is the basis of many studies on lexical acquisition." ></td>
	<td class="line x" title="4:270	Collocations are extracted from corpora using more or less shallow processing techniques, that span from purely statistical methods to partial parsers." ></td>
	<td class="line x" title="5:270	Our point is that, despite one of tile objectives of collocational analysis is to acquire high-coverage lexical data at low human cost, this is often not the case." ></td>
	<td class="line x" title="6:270	Human work is in fact required for the initial training of most statistically based methods." ></td>
	<td class="line x" title="7:270	A more serious problem is that shallow processing techniques produce a noise that is not acceptable for a fully automated system." ></td>
	<td class="line x" title="8:270	We propose in this paper a not-so-shallow parsing strategy that reliably detects binary and ternary relations among words." ></td>
	<td class="line x" title="9:270	We show that adding more syntactic knowledge to the." ></td>
	<td class="line x" title="10:270	recipe significantly improves the recall and precision of tile detected collocations, regardless of any subsequent statistical computation, while still nleeting the cornputational requi,'ements of corpus parsers." ></td>
	<td class="line x" title="11:270	1." ></td>
	<td class="line x" title="12:270	Week methods for the analysis of collocations In the past few years there has been a flourishing of interest in the study of word collocations." ></td>
	<td class="line x" title="13:270	A common method to extract collocations is using windowing techniques for the extraction of word associations." ></td>
	<td class="line x" title="14:270	In (Zernik 1990; Calzolari and Bindi 1990; Smadja 1989; Church and Hanks 1990) associations are detected in a 5 window." ></td>
	<td class="line x" title="15:270	A wider window ( tO0 words) is used in (Gale et al. 1992)." ></td>
	<td class="line x" title="16:270	Windowing techniques are also used in (Jelinek et al, 1990), where it is proposed a trigram model to automatically derive, and refine, context-free rules of the grammar (Fujisaki et al, 1991)." ></td>
	<td class="line x" title="17:270	Windowing techniques weekly model tile locality of language as well as other lexical information." ></td>
	<td class="line x" title="18:270	The reliability of the acquired information depends upon tile window size." ></td>
	<td class="line x" title="19:270	A small window fails to detect many important word relations, while enlarging tile window affects the tractability of tile statistical model (especially for markovian n-gram models)." ></td>
	<td class="line x" title="20:270	Finally, window-based collocations provide limited information when dealing with a variety of lexical phenomena." ></td>
	<td class="line x" title="21:270	For example, the simple observation of word cooccurrences is not a suitable marker of lexical subcategorization." ></td>
	<td class="line x" title="22:270	Another popular al;proach is usinl,r a partial parser, augmented with statistical parameters." ></td>
	<td class="line x" title="23:270	Tile reciprocal contribution of syntax and statistics has been outlined in (Zemik 119911) to have an important role for automatic lexicaI acquisition." ></td>
	<td class="line x" title="24:270	The syntactic relations are usually derived by preq)rocessing the target corpus with a part-of-speech tagger or with a simplified parser." ></td>
	<td class="line x" title="25:270	Syntactic markers are applied to elementary links among words or to more structurecl contexts." ></td>
	<td class="line x" title="26:270	The pa,'tial character of the different parsers described in literature makes it possible to process large corpora at a 'reasonable' computational effort." ></td>
	<td class="line x" title="27:270	Most syntax-based statistical approaches use deterministic parsing, derived from Marcus' work on PARSIF'AI." ></td>
	<td class="line x" title="28:270	parser (Marcus, 1980)." ></td>
	<td class="line x" title="29:270	I'ARS1FAL is a deterministic parser with lookahead cat)abilities, that enables partial analyses." ></td>
	<td class="line x" title="30:270	One of the PARSIFAL emanations, the Fidditch parser by I lindle, is used in (Flindle 1990) to detect subject-w~rb-object (SVO) triples." ></td>
	<td class="line x" title="31:270	SVO triples are allowed to be incomplete, i.e. the subject or the object can be missing." ></td>
	<td class="line x" title="32:270	Noisy data (i.e. words that are neither syntactically nor semantically,elated) are reduced by the use of statistical measures, sucl-t as the Itllltllal information (Church et al, 1991), as defined in information tlmory." ></td>
	<td class="line x" title="33:270	The Fidditch parser requires a lexicon including informatkm about base word fornls atld syntactic constraints (e.g,." ></td>
	<td class="line x" title="34:270	tile complement structure of verbs)." ></td>
	<td class="line x" title="35:270	Non-trivial preliminary work is tllus necessary in tuning the lexicon for the different domains and sublanguages." ></td>
	<td class="line x" title="36:270	A second problem with the Fidditch parser is poor performances: tilt_' recall and precision at detecting word collocations are declared to be as low as 50%, I-iowever it is unclear if this value applies only to SVO triples, and how it has been derived." ></td>
	<td class="line x" title="37:270	The recall is low because tile Fidditch parser, as other partial parsers (Sekine et al, 1992; Resnik and Hearst, i993), only detect links between adjacent or near-adjacent words." ></td>
	<td class="line x" title="38:270	Thougll a 50'/,, precision and recall might be 447 reasonable for human assisted tasks, like in lexicography, supervised translation, etc. , it is not 'fair enough' if collocational analysis must serve a fully automated system." ></td>
	<td class="line x" title="39:270	In fact, corpus linguistics became a popular research field because of the claim that shallow techniques could overcome the lexical coverage bottleneck of traditional NLP techniques." ></td>
	<td class="line x" title="40:270	Among the applications of collocational analysis for lexical acquisition are: the derivation of syntactic disambiguation cues (Basili et al. 1991, 1993a; Hindle and Rooths 1991,1993; Sekine 1992) (Bogges et al. 1992), sense preference (Yarowski 1992), acquisition of selectional restrictions (Basili et al. 1992b, 1993b; Utsuro et al. 1993), lexical preference in generation (Smadjia 1991), word clustering (Pereira 1993; Hindle 1990; Basili et al. 1993c), etc. In the majority of these papers, even though the (precedent or subsequent) statistical processing reduces the number of accidental associations, very large corpora (10,000,000 words) are necessary to obtain reliable data on a 'large enough' number of words." ></td>
	<td class="line x" title="41:270	In addition, most papers produce a performance evaluation of their methods but do not provide a measure of the coverage, i.e. the percentage of cases for which their method actually provides a (right or wrong) solution." ></td>
	<td class="line x" title="42:270	It is quite common that results are discussed only for 10-20 cases." ></td>
	<td class="line x" title="43:270	In our previous papers, we used semantic tagging to further reduce the noise and gain evidence of recurrent phenomena even with small corpora." ></td>
	<td class="line x" title="44:270	However, no accurate or shallow method can resume valid information that has been lost in previous steps (i.e. in extracting collocations)." ></td>
	<td class="line x" title="45:270	We believe that a higher precision and recall of the input collocational data is desirable to ensure a good coverage to the whatever lexical learning algorithm." ></td>
	<td class="line x" title="46:270	In this paper we describe a not-so-shallow, multi-step, parsing strategy that allows it to detect long distance syntactic relations while keeping the temporal complexity compatible with the computational requirements of largescale parsers." ></td>
	<td class="line x" title="47:270	We demonstrate that a bit more syntax can be added to the recipe, with a significant improvement over existing partial parsers." ></td>
	<td class="line x" title="48:270	We do not discuss of any subsequent processing (statistically or/and knowledge based) that may be applied to further improve the quality of collocational data, since this is outside the scope of this presentation." ></td>
	<td class="line x" title="49:270	The interested reader may refer to our previous works on the matter." ></td>
	<td class="line x" title="50:270	2." ></td>
	<td class="line x" title="51:270	A 'not-so-shallow' parsing technique Our syntactic analyzer (hereafter SSA) extracts partial syntactic structures from corpora." ></td>
	<td class="line x" title="52:270	The analyzer, based on discontinuous grammar (Dahl,1989), is able to detect binary and ternary syntactic relations among words, that we call elementary slmtactic lil~k,~ (esl), The framework of discontinuous grammars has several advantages: it allows a simple notation, and exhibits portability among different logic programming styles." ></td>
	<td class="line x" title="53:270	The presence of skip rules makes it possible to detect long distance dependencies between co-occurring words." ></td>
	<td class="line x" title="54:270	This is particularly important in many texts, for the presence of long coordinate constructions, nested clauses, lists, parenthesised clauses." ></td>
	<td class="line x" title="55:270	The partial parsing strategy described hereafter requires in input few more than a morphologic lexicon (section 2.1)." ></td>
	<td class="line x" title="56:270	Post morphologic processing, as described in section 2.2, is not strictly required, though obviously it increases the reliability of the detected word relations." ></td>
	<td class="line x" title="57:270	The lexicon used is purely morphologic, unlike for the Fidditch parser, neither it requires training, like in n-gram based models." ></td>
	<td class="line x" title="58:270	This means that the shallow analyzer is portable by minimum changes over different domains." ></td>
	<td class="line x" title="59:270	This is not the case with the deterministic partial parsing used in similar works." ></td>
	<td class="line x" title="60:270	Furthermore the grammar rules are easy to tune to different linguistic subdomains." ></td>
	<td class="line x" title="61:270	The analyzer enables the detection of different types of syntactic links among words: noun-verb, verbnoun, noun-preposition-noun, etc. This information is richer than just SVO triples, in that phrase structures are partitioned in more granular units." ></td>
	<td class="line x" title="62:270	The parsing method has been implemented for different corpora, which exhibit very different linguistic styles: a corpus of commercial activities (CD), in telegraphic style, a legal domain (LD) on taxation norms and lows, and remote sensing (RSD) abstracts." ></td>
	<td class="line x" title="63:270	The latter is in English, while the former two are in Italian." ></td>
	<td class="line x" title="64:270	The English application is rather less developed (a smaller morphologic lexicon, no postmorphology, etc.), however it is useful here to demonstrate that the approach is language independent." ></td>
	<td class="line x" title="65:270	In this paper we use many examples from the RSD." ></td>
	<td class="line x" title="66:270	2.1 Morphology The morphologic analyzer (Marziali, 1992) derives from the work on a generative approach to the Italian morphology (Russo, 1987), first used in DANTE, a NLP system for analysis of short narrative texts in the financial domain (Antonacci et al. 1989)." ></td>
	<td class="line x" title="67:270	Tile analyzer includes over 7000 elementary lemmata (stems without affixes, e.g. flex is the elementary lemma for de448 flex, in-flex, re-fiex) anti has been experimented since now on economic, financial, commercial and legal domains." ></td>
	<td class="line x" title="68:270	Elementary lemmata cover much more than 70(}0 words, since many words have an affix." ></td>
	<td class="line x" title="69:270	An entry in the lexicon is as follows: lexicon(len~na, stem, ending_class, syntactic feature) where l emma iS the elementary lemma (e.g. ancora for ancor-aggio (anchor-age)), stem is the lemma without ending (ancor), ending_class iS one over about 60 types of inflections." ></td>
	<td class="line x" title="70:270	For example, ancora belongs to the class ec cosa, since it inflects like the word cosa (thinq,)." ></td>
	<td class="line x" title="71:270	The Italian morphologic lexicon and grammars are fully general." ></td>
	<td class="line x" title="72:270	This means that the analyzer has a tendency to overgenerate." ></td>
	<td class="line x" title="73:270	For example, the word agente (agent, in the sense of dealer), is interpreted as a i~.oun and as the present participle of the verb agire (to act), though this type of inflected form is never found in both Italian domains." ></td>
	<td class="line x" title="74:270	This problem is less evident in English, that is less inflected." ></td>
	<td class="line x" title="75:270	Overgeneration is a common problem with grammar based approaches to morphology, as opposed to part of speech (pos) taggers." ></td>
	<td class="line x" title="76:270	On the other side, pos taggers need manual work for corpus training every since a new domain is to be analyzed." ></td>
	<td class="line x" title="77:270	To quantitatively evaluate the phenomenon of overgeneration, we conskfered a test set of 25 sentences in the LD, including about 800 words." ></td>
	<td class="line x" title="78:270	Of these 800, there were 546 different nouns, adjectives anti verbs (i.e. potentially ambiguous words)." ></td>
	<td class="line x" title="79:270	The analyzer provided 631 interpretations of the 546 words." ></td>
	<td class="line x" title="80:270	There were 76 ambiguous words." ></td>
	<td class="line x" title="81:270	The overall estimated ambiguity is 76/546:0,139, while the overgeneration ratio is better evaluated by: O = \[631 (546-76)\]/76=161/76:2,11 2.2." ></td>
	<td class="line x" title="82:270	Post morphological processing The purpose of this module is to analyse compound expressions and numbers, such as compound verbs, dates, numeric expressions, and super!atives." ></td>
	<td class="line x" title="83:270	Ad-hoc context free grammar have been defined." ></td>
	<td class="line x" title="84:270	Post morphological processing includes also simple (but generally valid) heuristic rules to reduce certain types of ambiguity." ></td>
	<td class="line x" title="85:270	'Ihere are two group of such rules: (i) Rules to disambiguate ambiguous nounadjective (N/Agg) interpretations (e.g. acid) (ii) Rules to disambiguate ambiguous verb-noun (V/N) interpretations (e.g. study) One example of heuristics for N/Agg is: If N/Agg is neither preceded nor followed by a noun, or N/Agg, before a verb is reached, Then it is a noun." ></td>
	<td class="line x" title="86:270	Ex: and sulphuric ~ was detected Though examples are in English, post morphology has not been developed for the English language at the time we are w,'iting." ></td>
	<td class="line x" title="87:270	After post-morphologic analysis, the 546 nouns, verbs anti adjectives produced only 562 interpretations." ></td>
	<td class="line x" title="88:270	The new overgeneration ratio is then O':(562-(546-76))/76=92/76=1,2 The estimated efficacy of the postrnorphology, is 161/92=1,75, about 50%.'eduction of the initial ambiguity." ></td>
	<td class="line x" title="89:270	2.3." ></td>
	<td class="line x" title="90:270	The parser The SSA syntactic analysis is a rewriting procedure of a single sentence into a set of ~!_1 ~meme~!-y_~y~ i\]jg_jin!~ (esl)." ></td>
	<td class="line x" title="91:270	The SSA is based on a discontinuous grammar, described more formally in (Basili et al. 1992a)." ></td>
	<td class="line x" title="92:270	In tiffs section we provide a qualitative clescription of the rules by which esl's are generated." ></td>
	<td class="line x" title="93:270	Examples of esl's generated by the parser are: N_V (the subject-verb relation), V N (the direct object_verb relation), N P N (noun preposition noun), V P N (verb preposition noun), N_Adj (adjective noun), N N (conq)ound) etc. Overall, we identify over 20 different esl's." ></td>
	<td class="line x" title="94:270	There is a discontinuous grammar rule for each esl." ></td>
	<td class="line x" title="95:270	A description of a rule used to derive N P N links is in Figure 1." ></td>
	<td class="line x" title="96:270	This description applies by straightforward modifications to any other esl type (though some esl rules include a concordance test)." ></td>
	<td class="line x" title="97:270	As remfirked at the beginning of this section, skip rules are the key to extract long distance syntactic relations and to approximate the behaviour of a full parser." ></td>
	<td class="line x" title="98:270	The first predicate LOOK RIGItT of Figure 1 skips over the string X until it finds a preposition (prep(w2))." ></td>
	<td class="line x" title="99:270	The second LOOK_RIG\[ IT skips over Y until it finds a noun (noun(w3))." ></td>
	<td class="line x" title="100:270	Given an initial string NL_segment, BACKTRACK force the system to analyse all the possible solutions of the predicate LOOKRIGHT (i.e. one-step rigth skips) to derive all the N P N groups, headed by the first norm (i.e. wl)." ></td>
	<td class="line x" title="101:270	For example, given the string: low concentrations of acetone and ethyl alchool in acqueous solutions the following N_PN are generated: concentration of acetone, concentration of alchool, concentration in solution, acetone in 449 solution, alchooI in solution, all of which are syntactically correct." ></td>
	<td class="line x" title="102:270	SSA rule( NL segment, N_P_N) BEGIN P, EPIZd~T IFNL_segment is EMPTY 'IIIEN F2KrI'; ELSE BEGIN NL segment=(wl Rest)." ></td>
	<td class="line x" title="103:270	IF (noun(wl) ) THFM BEGIN LOOK_RIGIIT(X, w2, Rest, New_Rest); %Rest=(X w2 NewRest) IF (TEST_ON(X) AND prep(w2) ) 'IIIEN BEG I N LOOK RIGIIT( Y, w2, New_Rest, _); %New_Rest--(Y w3 _) IF ( TEST ON(Y) AND noun(w3) ) 'llIEN ASSERT(esl(N_P_N, wl, w2, w3)); BACKTRACK; END; BACKTRACK; END POPwl FROM Nb_segment; END END." ></td>
	<td class="line x" title="104:270	Figure 1: A description of an N P N rule An uncontrolled application of skip rules would however produce unacceptable noise." ></td>
	<td class="line x" title="105:270	The TEST_ON0 are ad hoc heuristic rules that avoid uncontrolled skips." ></td>
	<td class="line x" title="106:270	For example, TEST2.ON(X) in Figure 1 verifies that the string X does notinclude a verb." ></td>
	<td class="line x" title="107:270	Hence, in the sentence:  the atmospheric code contpared favourably with results  the N P_N(code,with,results) is ~ generated." ></td>
	<td class="line x" title="108:270	In general, there is one-two different heuristic rule for each esl rule." ></td>
	<td class="line x" title="109:270	Heuristic rules are designed to take efficient decisions by exploiting purely syntactic constraints." ></td>
	<td class="line x" title="110:270	Such constraints are simple and require a minimum computational effort (essentialy, unification among simple structures)." ></td>
	<td class="line x" title="111:270	In some case, a lower recall is tolerated to avoid overgeneration." ></td>
	<td class="line x" title="112:270	For example, the second TEST ON(Y) rule of Figure 1 verifies that no more than two prepositions are skipped in the string Y. This rule stems from the observation that words located more than three prepositions apart, are rarely semantically related, though a full syntactic parser would eventually detect a relation." ></td>
	<td class="line x" title="113:270	Hence, in the NL segment: 1% accuracy on the night side of the Earth with stars down to visual magnitude tree the triple (accuracy, to, tree) is la_(gt generated, though syntactically correct." ></td>
	<td class="line x" title="114:270	The derivation of esl's is enabled for non adjacent word by virtue of skip rules." ></td>
	<td class="line x" title="115:270	However, interesting information can be lost in presence of more complex phenomena as nested relative clauses or coordination of phrase structures." ></td>
	<td class="line x" title="116:270	To cope with these phenomena, a post syntactic processor has been developed to extract links stemming from coordination among previously detected links." ></td>
	<td class="line x" title="117:270	This processing significantly increases the set of collected esl, and the quality of the derived lexical information." ></td>
	<td class="line x" title="118:270	The contribution of this post syntactic processing device depends heavily on the structure of incoming sentences." ></td>
	<td class="line x" title="119:270	In this phase, simple unification .mechanisms are used, rather than heuristics." ></td>
	<td class="line x" title="120:270	3." ></td>
	<td class="line x" title="121:270	Performance evaluation Recall and Precision M,'my algorithms evaluate their recall and precision against a human reference performer." ></td>
	<td class="line x" title="122:270	This pose many problems, like finding a 'fair' test material, using a large number of judges to render the evaluation less subjective, and finally interpreting the results." ></td>
	<td class="line oc" title="123:270	One example of the 450 latter problem is the following: in (Smadja 1993) the nature of a syntactic link between two associated words is detected a posteriori." ></td>
	<td class="line o" title="124:270	The performance of the system, called XTRACT, we evaluated by letting human judges compare their choice against that of the system." ></td>
	<td class="line x" title="125:270	The reported performances are about 80% precision, 90% recall." ></td>
	<td class="line o" title="126:270	One such evaluation experiment is, in our view, questionable, since both the human judges and XTRACT make a decision outside the context of a sentence." ></td>
	<td class="line o" title="127:270	The interpretation of the results then does not take into account how much XTRACT succeeds in identifying syntactic relations as they actually occurred in the test suite." ></td>
	<td class="line x" title="128:270	Another problem is that, a human judge ntay consider not correct a syntactic association on the ground of semantic knowledge 1." ></td>
	<td class="line x" title="129:270	Instead, the performance of a syntactic parser should be evaluated only on a syntactic ground." ></td>
	<td class="line x" title="130:270	We define the linguistic performance of SSA as its ability to approximate the generation of the full set of elementary syntactic links derivable by a complete grammar of the domain." ></td>
	<td class="line x" title="131:270	Given the set I2 of all syntactically valid esl and the set m of esl derived applying SSA, the precision of the system can be defined as the ratio cardinality(f2 m co) / cardinality(Q), while its recall can be expressed by: cardinality(co n ~2) / cardinality(~}), Global evaluations of the precision and recall are estimated by the mean values over the whole corpora." ></td>
	<td class="line x" title="132:270	We designed for testing purposes a full attribute grammar of the Italian legal language, and we selected 150 sentences for which the full grammar was proved correct." ></td>
	<td class="line x" title="133:270	For each parsed sentence, a program automatically computes the esrs globally identified (without repetitions) by the parse trees of each sentence, and compares them with those generated by SSA for the same sentence." ></td>
	<td class="line n" title="134:270	The following Table gives a measure of ~erformance: Esl_type N P N V P N RECALL 69.1 ~Yo' N_V 55 % 67.5 % PRECISION 81.8 % 56 % V_N 86.6 % 59 % 60.5 % To fully appreciate these results, we must consider, first, that the evaluation is on a purely syntactic ground (many collocations detected by 1 It is tmclear whether Smadja considered Otis problem in his evaluation experiment the full grammar and not detected by the SSA are in fact semantically wrong), second, that the domain is particularly complex." ></td>
	<td class="line x" title="135:270	There is an average of 23 trees per sentences in the test set." ></td>
	<td class="line x" title="136:270	In particvlar, the low performances of N_V groups (i.e. the subject relation) is influenced by the very frequent (almost 80'}'0) presence of nested relatives (ex: The income that was perceived during 1988i)is included) and inversions (ex: si considerano esenti da tassei redditi=*it is considered tax-free the income)." ></td>
	<td class="line x" title="137:270	No partial parser could cope with these entangled structttres." ></td>
	<td class="line x" title="138:270	One interesting aspect is that these results seem very stable for the domain." ></td>
	<td class="line x" title="139:270	In fact, incrementally adding new groups of sentences, the perfoemance values do not change significantly." ></td>
	<td class="line x" title="140:270	l'or completeness, we also evaluated the English grammar." ></td>
	<td class="line x" title="141:270	In this case, the evaluation was carried entirely by hand, since no full grammar of English was available to automatically derive the complete set of esl's." ></td>
	<td class="line x" title="142:270	F'irst, a test set of 10 remote sensing abstracts (about 1400 words, 67 sentences) was selected at random." ></td>
	<td class="line x" title="143:270	The results are the following: Esl_type RECALL N _ N 78 % V_N." ></td>
	<td class="line x" title="144:270	81% N_p_N 94 % V pN 87 % N_ V 75 % PRECISION 67 % 58 % 54 '~/o 42 % 57 % Here the recall is rather high, since sentences have a much simple structure." ></td>
	<td class="line x" title="145:270	However, there are many valid long distance pp attachments that for example most existing partial parses would not detect." ></td>
	<td class="line x" title="146:270	The precision is lower because the English parser does not have post morphokGy as yet." ></td>
	<td class="line x" title="147:270	One major source of error at detecting N V pairs are, as expected, comIxmnds." ></td>
	<td class="line x" title="148:270	The most important factors that influence the time complexity are: the number N of sentences (words) of the corpus and the number k of different discontinuous rules (about 20, as we said)." ></td>
	<td class="line x" title="149:270	The global rewriting procedure of SSA depends on the length n of the incoming text segment according to the following expression: *t i=l where e(x) is the cost of the application of a grammar rule, as for in Figure 1, to a segment of 4.51 length x. e(x) is easily seen to depend on: 1." ></td>
	<td class="line x" title="150:270	Predicates that test the syntactic category of a word (e.g. noun(w1)), whose cost is equal to that of a simple unification procedure i.e. 't; 2." ></td>
	<td class="line x" title="151:270	TEST ON predicates, whose cost is not greater than '~*n, where n is the substring length." ></td>
	<td class="line x" title="152:270	We can thus say that the expression e(x) of the complexity of SSA syntactic rules verifies the following inequality: e(n) <3r+ 2'rn = O(n) Hence, the global cost is: N n ~ ke(n i) <~_~3'ck + 2'rk(n -i) = i=1 i=1 = 2'rkn(n + 1) +3'~kn = O(n 2) A significant information is that the processing time needed on a Sun Sparc station by the full grammar to parse the test set of 150 sentences is 6 hours, while SSA takes only 10 minutes." ></td>
	<td class="line x" title="153:270	Portability and scalability These two aspects are obviously related." ></td>
	<td class="line x" title="154:270	The question is: How much, in terms of time and resources, is needed to switch to a different domain, or to update a given domain?" ></td>
	<td class="line x" title="155:270	Since we developed three entirely different applications, we can provide some reliable estimate of these parameters." ></td>
	<td class="line x" title="156:270	The estimate of course is strongly dependent upon the specific system we implemented, however we will frame our evaluation in a way that broadly applies to any system that uses similar techniques." ></td>
	<td class="line x" title="157:270	Morphology: Our experience when switching front the commercial to the legal domain was that, when running the analyzer over the new corpus, about 30,000 words could not be analyzed." ></td>
	<td class="line x" title="158:270	This required the insertion of about 1,500 new elementary lemmata." ></td>
	<td class="line x" title="159:270	Accounting for a new word requires entering the stem without affixes, the elementary lemma of the word and the ending class (see section 2.1)." ></td>
	<td class="line x" title="160:270	Entering a new word takes about 5-10 minutes when the linguist is provided with some onqine help, for example a list of ending classes, browsing and testing facilities, etc. With these facilities, updating the lexicon is a relatively easy job, that does not require a specialized linguist to be performed." ></td>
	<td class="line x" title="161:270	Clearly, when implementing several applications, the global updating effort tends to zero." ></td>
	<td class="line x" title="162:270	This is not the case for statistically based part of speech taggers, that require always a fixed effort to train on a new corpus." ></td>
	<td class="line x" title="163:270	On the long run, it seems that grammar based approaches to morphology have an advantage over pos taggers, in terms of portability." ></td>
	<td class="line x" title="164:270	Our experience is that adding a new rule takes about one-two man days." ></td>
	<td class="line x" title="165:270	First, one must detect the linguistic pattern that is not accounted for in the grammar, and verify whether it can be reasonably accounted for, given the intrinsic limitations of the parsing mechanism adopted." ></td>
	<td class="line x" title="166:270	If the linguist decides that, indeed, adding a new rule is necessary and feasible, he/she implements the rule and test its effects." ></td>
	<td class="line x" title="167:270	Grammar modifications are required to: * Select the esl types of interests; * Define the heuristic rules (TEST ON), as discussed in Section 2.3." ></td>
	<td class="line x" title="168:270	One positive aspect of SSA is that its complexity is O(k) with respect to the number k of grammar rules." ></td>
	<td class="line x" title="169:270	Hence adding new rules does not affect the complexity class of the method." ></td>
	<td class="line x" title="170:270	In summary, portability is an essential feature of SSA." ></td>
	<td class="line x" title="171:270	While other parsers need a non trivial effort to be tuned on clifferent linguistic domains, we need only minimal adjustment to ensure the required coverage of the morphologic lexicon." ></td>
	<td class="line x" title="172:270	However, the activity of lexical extension is needed with every approach." ></td>
	<td class="line x" title="173:270	Portability is also guarantied by the modularity of the apl)roach." ></td>
	<td class="line x" title="174:270	4." ></td>
	<td class="line x" title="175:270	Conclusions." ></td>
	<td class="line x" title="176:270	Shallow methods for corpus analysis claim to have several desirable features, such as limited manual work and high coverage." ></td>
	<td class="line x" title="177:270	Our point is that this is not entirely true." ></td>
	<td class="line x" title="178:270	Fully statistical methods require initial training over the corpus to estimate parameters, and this is not trivial." ></td>
	<td class="line x" title="179:270	Most of all, the effort is exactly the same every since the domain changes." ></td>
	<td class="line x" title="180:270	In addition, a lot of noisy data are collected unless some shallow level of linguistic analysis is added to increase performance." ></td>
	<td class="line x" title="181:270	But even then, reliable data are collected only for a fragment of the corpus." ></td>
	<td class="line x" title="182:270	And what about high coverage?" ></td>
	<td class="line x" title="183:270	On tl'te other side, we wouldn't be here, had traditional NLP techniques had any chance to become truly scalable." ></td>
	<td class="line x" title="184:270	This paper showed, if not else, that a bit more syntax can be added to the recipe, while still meeting important requirements, such as computational complexity and portability." ></td>
	<td class="line x" title="185:270	In media stat virtus: ql'ds could be the moral of this paper, and in general of our research on lexical acquisition." ></td>
	<td class="line x" title="186:270	Of course, we don't know where exactly the perfect balance is, we just seek for a better balance." ></td>
	<td class="line x" title="187:270	452 References." ></td>
	<td class="line x" title="188:270	(Antonacci 1989), F. Antonacci, M.T. l'azienza, M. Russo, P. Velardi, (1989), A Logic based system for text analysis and lexical knowledge acquisitio,l, in Data and Knowledge Engineering, vol 4." ></td>
	<td class="line x" title="189:270	(Basili et al. 1991), R. Basili, M. T. Pazienza, P. Velardi, (1991), Using word association for syntactic disambiguation, in Trends in Artificial Intelligence, E. Ardizzone et al. , Eds., I.NAI n. 549, Springer-Verlag." ></td>
	<td class="line x" title="191:270	(Basili et al. 1992 a) R. Basili, M. T. Pazieuza, P. Velardi, (19921, A shallow Syntax to extract word associations from corpora', in Literary and Linguistic Computiug, vol." ></td>
	<td class="line x" title="192:270	2." ></td>
	<td class="line x" title="193:270	(Basili et al. 1992 b) R. Basili, M. T. Pazienza, P. Velardi, (1992), Computational Lexicons: the neat examples and the odd exemplars, Prec." ></td>
	<td class="line x" title="194:270	of 3rd." ></td>
	<td class="line x" title="195:270	Conf." ></td>
	<td class="line x" title="196:270	on Applied NLP." ></td>
	<td class="line x" title="197:270	(Basili et al.1993a), Basili, R. , M.T. Pazienza, P. Velardi, (19931." ></td>
	<td class="line x" title="198:270	Semi-automatic extraction of linguistic information for syntactic disambiguation, Applied Artificial Intelligence, vol." ></td>
	<td class="line x" title="199:270	4, 1993." ></td>
	<td class="line x" title="200:270	(Basill et al.1993b), Basili, R. , M.T. Pazienza, P. Velardi, (19931." ></td>
	<td class="line x" title="201:270	What can be learned from raw texts ?, Journal of Machine Translation, 8:147-173." ></td>
	<td class="line x" title="202:270	(Basili et a1.1993c), Basili, R. , M.T. Pazienza, P. Velardi, (1993)." ></td>
	<td class="line x" title="203:270	llierarchical clustering of verbs, ACLSIGLEX Workshop on Lexical Acquisition, Columbus Ohio, June." ></td>
	<td class="line x" title="204:270	(Bogges,1991), L. Bogges, R. Agarwal, R. Davis, I)isambiguation of prepositional phrases iu automatically labelled technical text (1991)." ></td>
	<td class="line x" title="205:270	Prec." ></td>
	<td class="line x" title="206:270	of AAAI 1991 (Church and llanks, 19901, K. Church and P. llauks, Word association norm, mutnal information and lexicography, Computational Linguistics, vol." ></td>
	<td class="line x" title="207:270	16, n.1, 1990 (Church et al, 1991), Church, Gale, flanks and Ilindlc, Using statistics in lexicaI analysis, (19911." ></td>
	<td class="line x" title="208:270	Lexical Acquisition, U. Zernik Ed., l.awrence Erlbaum Ass., Publ., 115-164." ></td>
	<td class="line x" title="212:270	(Calzolari and Bindi,1990) N.Calzolari and R. Bindi, Acquisition of lexical informatiou from Corpora, (19901, Prec." ></td>
	<td class="line x" title="213:270	of COLING 90." ></td>
	<td class="line x" title="214:270	(Dahl, 1989), Dahl,V. , 'Discontinous grammars', (1989)." ></td>
	<td class="line x" title="215:270	Computational Intelligence, n. 5, 161-179." ></td>
	<td class="line x" title="216:270	(Fujsaki et a1.,1991) Fujisaki T. , F. Jelinek, J. Cooke, E. Black, T. Nishino, A probabilistic parsing method for sentence disambiguation, (19911." ></td>
	<td class="line x" title="218:270	Cu,'reut trends in Parsing Technology, M. Tomita Ed., Kluwer Ac." ></td>
	<td class="line x" title="220:270	Publ., 1991." ></td>
	<td class="line x" title="221:270	(Ilindle and Rooths,1991) D. llindle, M. l~,ooths, Structural Ambiguity and Lexical P, elatious (1991)." ></td>
	<td class="line x" title="222:270	Prec." ></td>
	<td class="line x" title="223:270	of ACL 1991 (ltindle, 19901, D. llindle, Nouu Classification form predicate-argument structure (199tl)." ></td>
	<td class="line x" title="224:270	Prec." ></td>
	<td class="line x" title="225:270	of AC1." ></td>
	<td class="line x" title="226:270	1990 (Ilindle and Rooths,1991) D. Ilindle, M. Rooths, Structural Ambiguity and Lexical Relations (1991)." ></td>
	<td class="line x" title="227:270	Prec." ></td>
	<td class="line x" title="228:270	of ACL 1991 (llindle and Rooths, 1993) 11." ></td>
	<td class="line x" title="229:270	llindle and M. Rooths, Structural ambiguity and lexical relations (1993)." ></td>
	<td class="line x" title="230:270	(2ompntational Linguistics, vol." ></td>
	<td class="line x" title="231:270	19, n. 1, 1993 (Gale et al, 1992), I!stimating the upper and lower bounds on the performance of word-sense disambiguation progrt, ms, (1992)." ></td>
	<td class="line x" title="232:270	l:'roc, of ACL 1992 (Jelinek et al. , 1990) F. Jelinek, J.l)." ></td>
	<td class="line x" title="233:270	l.al'ferty, F,.I Mecer, Basic methods of probabilistic context f,'ee grammars, (19901." ></td>
	<td class="line x" title="234:270	Research Report R( 216374 IBM YorkTown lleights NY 10598, 1990." ></td>
	<td class="line x" title="235:270	(Marcus, 1980), M. Max'cns, A Theory of Syntactic recoguitiou for Natural I.anguage, MIT Press, 1980 (Marziali,19921, Marziali, A. , 'Robust Methods fro." ></td>
	<td class="line x" title="236:270	parsiug large-scale text archives, I)issertation, Facolth di lugegneria, Univerith 'La Sapieuza' I>,oma, a.a. 1992 ." ></td>
	<td class="line x" title="237:270	(Percira et at." ></td>
	<td class="line x" title="238:270	'1993) F.Pereira, N. Tishby, L. Lee, (19931." ></td>
	<td class="line x" title="239:270	'Distributional Clustering of English Words', in Prec." ></td>
	<td class="line x" title="240:270	of ACI, 93 Columbus, Ohio, June, 1993." ></td>
	<td class="line x" title="241:270	(Rnsso, 1987), M. Russo, 'A generative grammar approach for the morphologic and morphosyntactic analysis of the Italian langnage' (1987)." ></td>
	<td class="line x" title="242:270	3rd." ></td>
	<td class="line x" title="243:270	Conf." ></td>
	<td class="line x" title="244:270	of the F.urol~ean Chapter of the ACI,, Copenhaghen, April 1-3 1987." ></td>
	<td class="line x" title="245:270	(Sekiae et al, 1992) Automatic learuiug for semantic collocations, (19921." ></td>
	<td class="line x" title="246:270	Prec." ></td>
	<td class="line x" title="247:270	of 3rd." ></td>
	<td class="line x" title="248:270	ANLI', 1992 (Smadja,1989), F. Smadja, 'Lexical cooccurences: the missing link', (1989)." ></td>
	<td class="line x" title="249:270	Literary and 1,iuguistic Computing, vol.4, n.3, 1989." ></td>
	<td class="line x" title="250:270	(Smadja,1991), F. Smadja, From N-Grams to collocations: au evaluation of XTRACT, (1991)." ></td>
	<td class="line x" title="251:270	Prec." ></td>
	<td class="line x" title="252:270	of ACI, 199l (Smadja,1990), F. Smadja, K. McKeou, Automatically extracting and rcsprescnting collocations for langultge generation, (1990)." ></td>
	<td class="line x" title="253:270	Prec." ></td>
	<td class="line xc" title="254:270	of ACL 1990 (Smadja, 1993), F. Smadja, Retrieving collocations fi'cma text: XTRACT, (1993)." ></td>
	<td class="line x" title="255:270	Computatioual Linguistics, w~l 19, u.l, 1993 (l(esnik and llearst, 1993) P. Resnik, M. llearst, Structural Ambiguity and Conceptual Relations, (1993)." ></td>
	<td class="line x" title="256:270	pt'oc, of the workshop on Very l,arge Corl)ra, Columbus, June 1993 (Iltsuro et a1.,.1993), T. Utsnro, Y. Matsumoto, M. Nagao, verbal case frame acqnisition from bilingual corlx)ra, (1993)." ></td>
	<td class="line x" title="258:270	Prec." ></td>
	<td class="line x" title="259:270	of IJCAI 1993 (Yarowski, 1992) Yarowsky 1)., 'Word-Sense Disambiguation Using Statistical Models of Roger's Categories Trained on Large Corpora', (1992)." ></td>
	<td class="line x" title="261:270	Prec." ></td>
	<td class="line x" title="262:270	of COI,ING-92, Nantes, Aug. 23-28." ></td>
	<td class="line x" title="263:270	(Zernik,1990), tl." ></td>
	<td class="line x" title="264:270	Zernik, P. Jacobs, Tagging for l.earning: Collecting Thematic relations from Corpus (1990)." ></td>
	<td class="line x" title="265:270	Prec." ></td>
	<td class="line x" title="266:270	of COL1NG 1990 (Zeruik,1991), U. Zernik, Ed." ></td>
	<td class="line x" title="267:270	'Lexical Acquisition: Fxploiting on-line resources to build a lexicon', (1991)." ></td>
	<td class="line x" title="268:270	Lawrence Erlbatun Publ., 1991." ></td>
	<td class="line x" title="270:270	453" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C94-1091
Classifier Assignment By Corpus-Based Approach
Sornlertlamvanich, Virach;Pantachat, Wantanee;Meknavin, Surapant;"></td>
	<td class="line x" title="1:156	CLASSIFIER ASSIGNMENT BY CORPUS-BASED APPROACH Virach Sornlertlamvanich Wantanee Pantachat Surapant Meknavin Linguistics and Knowledge Science Laboratory National Electronics and Computer Technology Center National Science and Technology Development Agency Ministry of Science Technology and Environment 22nd Gypsum Metropolitan Tower, 539/2 Sriayudbya Rd. , Bangkok 10400, Thailand { virach,wantanee,surapan } @nwg.nectee.or.th Abstract This paper presents an algorithm for selecting an appropriate classifier word for a noun." ></td>
	<td class="line x" title="2:156	In Thai language, it frequently happens that there is fluctuation in the choice of classifier for a given concrete noun, both from the point of view of the whole speech community and individual speakers." ></td>
	<td class="line x" title="3:156	Basically, there is no exact rule for classifier selection." ></td>
	<td class="line x" title="4:156	As far as we can do in the rule~based approach is to give a default rule to pick up a corresponding classifier of each noun." ></td>
	<td class="line x" title="5:156	Registration of classifier for each noun is limited to the type of unit classifier because other types,are open due to the meaning of representation." ></td>
	<td class="line oc" title="6:156	We propose a corpus-based method (Biber,1993; Nagao,1993; Smadja,1993) which generates Noun Classifier Associations (NCA) to overcome the problems in classifier assignment and semantic construction of noun phrase." ></td>
	<td class="line x" title="7:156	The NCA is created statistically from a large corpus and recomposed under concept hierarchy constraints and frequency of occurrences." ></td>
	<td class="line x" title="8:156	Keywords: Thai language, classifier, corpus-based method, Noun Classifier Associations (NCA) 1." ></td>
	<td class="line x" title="9:156	Introduction A classifier has a significant use in Thai language tbr construction of noun or verb to express quantity, determination, pronoun, etc. By far the most common use of classifiers, however, is in enumerations, where the classifiers follow numerals and precede demonstratives (Noss,1964)." ></td>
	<td class="line x" title="10:156	Not all types of classifier have a relationship with noun or verb as a unit classifier does." ></td>
	<td class="line x" title="11:156	A unit classifier is any classifier which has a special relationship with one or more concrete nouns." ></td>
	<td class="line x" title="12:156	For example, to enumerate members of the class of /rya/ 'boats', tile unit classifier/lain/ is selected as in the phrase below: /rya nung lain/ boat one <boat> 'one boat'." ></td>
	<td class="line x" title="13:156	Other than tile unit classifier, there are collective classifier, metric classifier, frequency classifier and verbal classifier." ></td>
	<td class="line x" title="14:156	A collective classifier is,any classifier which shows general group or set of mass nouns, un a~ ~ /nok soong lung/ 'two flocks of bird'." ></td>
	<td class="line x" title="15:156	A metric classifier is any classifier which occurs in enumerations that modify predicates as well as nouns, v lh l~1~,u~/nam saam kaew/ 'three glasses of water'." ></td>
	<td class="line x" title="16:156	A frequency classifier is any classifier which is used to express the frequency of event that occurs, ~u ~ ~mJ /bin sii roob/ 'fly four rounds'." ></td>
	<td class="line x" title="17:156	A verbal classifier is any classifier which is derived from a verb and usually used in construction with mass nouns, n~z~q~a #a ~ 11')11 /kradaad haa muan/ 'five rolls of paper'." ></td>
	<td class="line x" title="18:156	The unit classifier has a special relationship with concrete noun." ></td>
	<td class="line x" title="19:156	The member of this class of classifier is closed for each noun." ></td>
	<td class="line x" title="20:156	Most of the unit classifiers m'e used with a great many concrete nouns of very different meaning, but few are restricted to a single noun." ></td>
	<td class="line x" title="21:156	Except for the unit classifier, the members of classifier for a noun or predicate are open." ></td>
	<td class="line x" title="22:156	Especially for the metric classifier, the number of classifiers for numeral expression of distance, size, weight, container and value is large." ></td>
	<td class="line x" title="23:156	The use of classifier in Thai is not limited to the nunmral expression but is extended to other expressions such as ordinal, determination, relative pronoun, pronoun, etc. The detail of each classifier phrase is described in the next section." ></td>
	<td class="line x" title="24:156	In many existing natural language processing systems, tile list of available classifiers lk3r each noun is attached to a lexicon base." ></td>
	<td class="line x" title="25:156	Rules for classifier selection from the list can somehow provide the 556 dcfault value but does not guarantee thc appropriateness, tlowever, the problems on classifier phrase construction still remain unsolved." ></td>
	<td class="line x" title="26:156	To overcome the problems of using classifiers, we propose a method of classifier phrase extracting fl'om a large corpus." ></td>
	<td class="line x" title="27:156	As a result, NounClassifier Associations (dcscribcd in Section 3) is statistically created to define the relationship between a noun and a classifier in a classifier phrase." ></td>
	<td class="line x" title="28:156	With the li'equency of tile occurrence of a classifier in a classifier phrase, we can propose the most apl)rot)riate use of a classilier." ></td>
	<td class="line x" title="29:156	Furthermore, we introduce a hierarchy of semantic class for tile induction of a classifier class when they are employed to construct with nouns which belong to the same class of meaning." ></td>
	<td class="line x" title="30:156	Section 3 and Section 4 (lescribc the generation and the imlflcmentation of the NCA, respectively." ></td>
	<td class="line x" title="31:156	2." ></td>
	<td class="line x" title="32:156	The roles of classilier in Thai hmguage in Thai language, we use classifiers ill wuious situations." ></td>
	<td class="line x" title="33:156	The classilier plays atu important role ill COllStrtlciiou with tlnUll to express ordinal, pronoun, for instance." ></td>
	<td class="line x" title="34:156	The classifier phrase is syntactically geneutted according to a specific pattenL Fig." ></td>
	<td class="line x" title="35:156	2.1 showt; the position of a classifier in each pattern, where N stands lot noun, NCNM stands for cardinal nnnlher, CI, stallds for classifier, DET stands for determiner, VATF stands for attributive verh, Rt'iL M stands for relative marker, ITR." ></td>
	<td class="line x" title="36:156	M stands for Interrogative iilarkcr, DONM stands foi ordinal liu/tlt:llil, DDAC statMs fin definite demonshativc Study on tile use of classilira' in each expression inemioned above, we can conclu(le that tile types of classifier are not restricted tt) any kinds of expression, 'to consider tile Selnantic representatioll of each exprcssioit, it happens that tilt: unit classifier is not wgarded its a conceptual refit in all expressions except i~l pattern 6, hut the other types are." ></td>
	<td class="line x" title="37:156	(see examples in a. and b)." ></td>
	<td class="line x" title="38:156	a) 1J7~'~ I'irll ~lU,l ~tl!" ></td>
	<td class="line x" title="39:156	/prachachon 2 khon/ (IJnit-Cl. ,) people 2 <people'~ '2 peot~le' /prachachon 2 Mum/ (Collecfive-CI,) penple 2 <gr(mp> '2 groups el'people' We ellcolmtered to gcnerate tile alWopdate classifier tel noun or verb ill a semantic representation." ></td>
	<td class="line x" title="40:156	'file classifier assignment for non-conceptual representation alld the classifier selection of o\[le to nunly conceptual representation arc over handleable by the rule-based approach." ></td>
	<td class="line x" title="41:156	The propose on classifier assignment using the corpus-based method is another approach." ></td>
	<td class="line x" title="42:156	Based on the collocation of noun and classifier of each pattern shown in Fig." ></td>
	<td class="line x" title="43:156	2.1, we decided to construct the Noun Classifier Association table (see Section 3)." ></td>
	<td class="line x" title="44:156	A stocMstic method combined with the concept hierarchy is proposed as a strategy in making the NCA table." ></td>
	<td class="line x" title="45:156	The table composes of the information about nonn-classifier collocation, statistic occurrences and the representative classifier for each semantic class in the concept hierarchy." ></td>
	<td class="line x" title="46:156	3." ></td>
	<td class="line x" title="47:156	Extraction of Noun-Classifier Collocation 1,1 this section, wc describe tile algorithm used for extraction of Noun Classifier Associations (NCA) from a large corpus." ></td>
	<td class="line x" title="48:156	We used a 40 megabyte Thai coq)us collected from wu'ious areas to create tile table." ></td>
	<td class="line x" title="49:156	The algorithm is as follows: Step 1: Word segmentation." ></td>
	<td class="line x" title="50:156	Input: A corpus." ></td>
	<td class="line x" title="51:156	Output: The wordosegmented corpus." ></td>
	<td class="line x" title="52:156	hi text processing, we often need word boundary information lot several puqmses." ></td>
	<td class="line x" title="53:156	Because Thai has no explicit raarke, to separate words from one another, we have to prcprocess the corpus with word segmentation program." ></td>
	<td class="line x" title="54:156	We used the program developed by Sornlcrthmwanich (1993) with post-editing to correct fault segmcntation." ></td>
	<td class="line x" title="55:156	The program employs heuristic rules of longest malching and least word count incoq)orated with character combining rules for Thai words." ></td>
	<td class="line x" title="56:156	Though tile accuracy of the word segmentation does not reach 100%, but it is high enough (more than 95%) to reduce the post-~iting time." ></td>
	<td class="line x" title="57:156	Step 2: Tagging." ></td>
	<td class="line x" title="58:156	Input: Output of step 1." ></td>
	<td class="line x" title="59:156	Output: The corpus of which each word is tagged with a part of speech and a semantic class." ></td>
	<td class="line x" title="60:156	The word-segmented corpus is then processed with a stochastic paWof.-st)eed, tagger." ></td>
	<td class="line x" title="61:156	Each word w together with its part of speech is then used to reUieve the semantic class of tile word fiom a dictionary." ></td>
	<td class="line x" title="62:156	The result yields a data structure of (w,p,s), where p denotes the pm-t of speech of w and s denotes the semantic chtss of w. For example, the data structure of the word fihf~mA hlakrian/'student' is (ffnt~ou, NCMN, person), where NCMN stml(ls for common noun and t)crson rel)rescnts ffntTml in file class of person." ></td>
	<td class="line x" title="63:156	Step 3: Producing cnncordances." ></td>
	<td class="line x" title="64:156	hq)ut: Output of step 2, a given classifier el." ></td>
	<td class="line x" title="65:156	Output: All the fragnlents containing cl. 557 Expressions 1." ></td>
	<td class="line x" title="66:156	Enumeration 2." ></td>
	<td class="line x" title="67:156	Ordinal 3." ></td>
	<td class="line x" title="68:156	Determination -Definite demonstration -Indefinite demonstration -Referential 4." ></td>
	<td class="line x" title="69:156	Attributive 5." ></td>
	<td class="line x" title="70:156	Noun modifier 6." ></td>
	<td class="line x" title="71:156	Prononn -Relative pronoun -Interrogative pronoun -Ordinal pronoun -Pronoun Patterns N/V-NCNM-CL N-CL-/tii/-NCNM a) N-CL-DET a) N-CL-DET b) N-DET-CL a) N-CL-DET N-CL-VA'Iq' CL-N a) CL-REL_M b) CL-ITR_M c) CL-DONM d) CL-DDAC Samples /nakrian 3 khon/ (N) (N) (CL) student 3 <student> 'three students' /kaew bai thii4/ (N) (CL) (N) glass <glass> 4th 'the fourth gl~Lss I a)/raw chop kruangkhidlek kruang nii/ (N) (CL) (DEW) we like calculator <calculator> this 'we like this c~dculator' a)/phukhawfung khon nung sadaeng (N) (CL) (DEW) participant <participant> one express khwamhen nai thiiprachum/ opinion in conference 'A participant expressed his opinion in the conference'." ></td>
	<td class="line x" title="72:156	b)/sunak bang tua/ (N) (DET) (CL) dog some <dog> 'some dogs' a)/kamakan kana nii thukkhon (N) (CL) (DET) committee <group> this everyone chuua w~m ja thamngan samret/ believe that will work success 'It is this committee that everyone believed its mission would be success'." ></td>
	<td class="line x" title="73:156	/dinsoo theng san/ (N) (CL) (VAT'I') pencil <shape> short ~ncil' /kana naktongtiew/ (CL) (N) group tourist of tourist' a)/nakbanchii khon thii thamngan (N) (CL) (REL-M) (V) accountant who work thii borisat nii/ at company this 'the accountant who works at this company' b) /sing nail (CL) (nR-M) <thing> which 'which one' c) /tua raek/ (CL) (DONM) one first 'the first one' d) /khon nil chop hia mak/ (CI,) (DDAC) the one like beer very 'The one likes b~much' Fig." ></td>
	<td class="line x" title="74:156	2.1 Classification of classifier expressions table 558 (em=n~7;4nq's 111, ~q~'4= 2, 11) (a~rl'~tlnql111, n~/.l_2, 5) (~,=rl'~lJnq's 111, ~.t 1, 6) (Brl 13111, ~'1 1,9) ('#,n_13111, fJ,L2, 4) (~ri 13111,~'q 1, 10) (~,fi_13111,tt~q 2,3) (~nn~=~an 13111, ~'L1, 7) (mL11t, ~1, 67) (RILl I I, f1~I_2, I) (lq~q'I." ></td>
	<td class="line x" title="75:156	1 1 I, ~'l'd, I, 1 7) (tlrlq'~~111,l~qu 1,9) ('n!aql 111, ~q#_2, 1) (~.\]'a.,lq~ 111, ~q~.t 1, 6) (~11 13114, ~r1_1, 12) (t~a 13114, NB 1,3) (Lte,ltl.I 13114, ~n_I, 8) (~tiFm_l 3114." ></td>
	<td class="line x" title="77:156	~n_l, 9) ('\[~1.13111,~3 1, 7) ('~11q_13111, ~3_1, 13) (!4~ 13111,~q_1,5) (~q~l.13111, titan 1, 3) Fig." ></td>
	<td class="line x" title="78:156	3.1 Table of Noun Classificr Associations (NCA) Concrete (1) Subject (11) Concrete place (12) Concrete thing (13) Person (111) Organization (112) .,." ></td>
	<td class="line x" title="79:156	Nature thing (131),,." ></td>
	<td class="line x" title="80:156	living thing (1311)  Animal (13111) .,, Plant (13113) Fruit (13114) Fig." ></td>
	<td class="line x" title="81:156	3.2 Concept hierarchy Instead of picking up the data sentence by sentence, we extracted a fragment of data arouud the el, because there is no explicit marker to indicate sentence boundaries." ></td>
	<td class="line x" title="82:156	We used the range of -10 to +2 words around the cl in our experiments which appeared to cover most of co-occurrence patterns." ></td>
	<td class="line x" title="83:156	Step 4: PaRern naatching Input: Output of step 3." ></td>
	<td class="line x" title="84:156	Output: A list of nouns-classifiers with frequency intormatiou of co-occurrences." ></td>
	<td class="line x" title="85:156	In this step, the tagged corpus is matched with each pattern of classifier occurrences shown below: No -NCNM-CL (Enumeration) N-CL~/tii/-NCNM (Ordinal expression) N-CL-DET (Referential expression) N-DET-CL (Indefinite demonstration expression) N-CL-VA'IT (Attribute noun phrase) CL-N (Noun modifier) N-CL-{~/tii/, ~/sung/, 'ht/n,'fi/, } (Relative/Interrogative pronoun) where N denotes noun, CL denotes classifier, NCNM denotes c~u'dinal number, DET denotes determiner, A,." ></td>
	<td class="line x" title="86:156	.4 VATF denotes attributive verb, ~l/tu/, ~ /sung/ and '\[u /nai/ are specific Thai words, A-B denotes a consecutive pair of A and 1t, aud A--B denotes a possibly separated pair." ></td>
	<td class="line x" title="87:156	Actually, A--B can be 559 separated by several arbitrary words but in our experiments we considered only possible separations by a relative pronoun phrase having no more than 5 words." ></td>
	<td class="line x" title="88:156	This is to limit the search space of general cases to a manageable size with some loss of generality." ></td>
	<td class="line x" title="89:156	The pattern matching process was carried out one by one with each pattern." ></td>
	<td class="line x" title="90:156	For each pattern of AB-C, the matching of B-C pair was simple and was performed at first." ></td>
	<td class="line x" title="91:156	Next, the matching of a pair A-B was done by: 1." ></td>
	<td class="line x" title="92:156	searching for the nearest A from B. If found, mark AI." ></td>
	<td class="line x" title="93:156	2." ></td>
	<td class="line x" title="94:156	from B within a span of five, searching for the nearest relative pronoun." ></td>
	<td class="line x" title="95:156	If found, mark pl then go to 3." ></td>
	<td class="line x" title="96:156	Otherwise, match A1." ></td>
	<td class="line x" title="97:156	3." ></td>
	<td class="line x" title="98:156	further searching for the nearest A from p 1." ></td>
	<td class="line x" title="99:156	If found, mark A2." ></td>
	<td class="line x" title="100:156	If A2 is farther from B than A1, match A2." ></td>
	<td class="line x" title="101:156	Otherwise, match A I. At the end of these steps, we obtained a list of nouns Ni along with the frequency of w in the corpus for each matching pattern (see Fig." ></td>
	<td class="line x" title="102:156	3.1 for sample ouqmts)." ></td>
	<td class="line x" title="103:156	Each entry is of the form (W_N1, CLN2, Freq) where W denotes a noun, N1 denotes a number representing semantic class of W, CL denotes the associated classifier, N2 is a number indicating whether CL is a unit or collective classifier (1 for unit, 2 for collective) and Freq denotes the frequency of cooccurrence between W and CL. The semantic class is shown in Fig." ></td>
	<td class="line x" title="104:156	3.2." ></td>
	<td class="line x" title="105:156	Step 5: Determine representative classifier Input: A list of noun-classifier with frequency information of co-occurrence." ></td>
	<td class="line x" title="106:156	Output: Representative classifier of each noun and each semantic class of nouns." ></td>
	<td class="line x" title="107:156	As it can be observed in Fig." ></td>
	<td class="line x" title="108:156	3.1, each noun may be used with several possible classifiers." ></td>
	<td class="line x" title="109:156	In language generation process." ></td>
	<td class="line x" title="110:156	However, we have to select only one of them." ></td>
	<td class="line x" title="111:156	For each noun we select the classifier with the greatest value of co-occurrence frequency to be the representative classifier for both representative unit classifier and representative collective classifier." ></td>
	<td class="line x" title="112:156	Tile classifier in Fig." ></td>
	<td class="line x" title="113:156	3.1, for example, will have ~__1 as the representative unit classifier and have n~ 2 as the representative collective one for the noun sm~nr~unq'~ 111." ></td>
	<td class="line x" title="114:156	Collective classifiers are used instead of unit classifiers when the notion of 'group' is required." ></td>
	<td class="line x" title="115:156	We also find the representative classifier for each semantic class of nouns in the same manner." ></td>
	<td class="line x" title="116:156	For each semantic class of nouns (grouped by the semantic class attached with each noun), the classifier with the greatest value of co-occurrence frequency is selected to be the representative." ></td>
	<td class="line x" title="117:156	The classifier is used to handle the assignment of classifier to noun which does not exist in the trained corpus." ></td>
	<td class="line x" title="118:156	For example, the representative unit classifiers for each semantic class extracted by the pattern (N-NCNM-CL) are shown in Fig." ></td>
	<td class="line x" title="119:156	3.3." ></td>
	<td class="line x" title="120:156	4. Classifier Resolution The associations as produced in the previous section are useful for determining a proper classifier for a given noun." ></td>
	<td class="line x" title="121:156	For a noun occurring ill the corpus, alternative determination is accomplished in a straightforward manner by using its associated representative classifier which occurs in the corpus more frequently than any other classifiers." ></td>
	<td class="line x" title="122:156	In the other case where the given noun does not exist in tile corpus, the determination is done by using the representative classifier of its class in the concept hierarchy." ></td>
	<td class="line x" title="123:156	Some examples of classifier determining are listed below." ></td>
	<td class="line x" title="124:156	(1) and (3) show the case of nouns appearing in the corpus, while (2) and (4) show a different scenario." ></td>
	<td class="line x" title="125:156	In (2), the unit classifier of/appem/ is obtained by using the representative unit classifier of its class 'fruit' which is ~n_~ /luuld according to Fig." ></td>
	<td class="line x" title="126:156	3.3." ></td>
	<td class="line x" title="127:156	Similarly, in (4), the collective classifier of /gangkerd is determined by the representative collective classifier of its class 'animal' which is ~2 /fuung\]." ></td>
	<td class="line x" title="128:156	Semantic class Unit classifier Collective classifier animal ~'L 1 ~.~_2 human ~u I ~m~2 plant ~u_l fruit nnl Fig." ></td>
	<td class="line x" title="129:156	3.3 NCA for representative classifier 560 Unit classifier /nakrian kon tit sit/ student <sttident> number four v (2) mnlfi'ka ~\]n lint /appern luuk nail apple <apple> which Collective classifier (3) ~ul:n~m~ ~ut.~ un /kanagammagarn kana nan/ committee . group that (4),m,~u ~ ~Tu /gangken fuung nan/ magpie group that Linguistics, Vol." ></td>
	<td class="line x" title="130:156	19, No.3, Set)tember 1993." ></td>
	<td class="line x" title="131:156	\[2\] Nagao, Makato." ></td>
	<td class="line x" title="132:156	(1993)." ></td>
	<td class="line x" title="133:156	'Machine Translation: What Have We to Do'." ></td>
	<td class="line x" title="134:156	Proceedings of MT Summit IV, June 20-22, 1993, Kobe, Japan." ></td>
	<td class="line x" title="135:156	\[3\] Noss, Richard B." ></td>
	<td class="line x" title="136:156	(1964)." ></td>
	<td class="line x" title="137:156	Thai Reference Grammar, U.S. Oovermnent Printing Office, Washington, DC." ></td>
	<td class="line x" title="138:156	\[4\] Smadja, Frank." ></td>
	<td class="line x" title="139:156	(1993)." ></td>
	<td class="line x" title="140:156	'Retrieving Collocations fi'om Text: Xtract'." ></td>
	<td class="line x" title="141:156	Computational Linguistics, Vol." ></td>
	<td class="line x" title="142:156	19, No.l, March 1993." ></td>
	<td class="line x" title="143:156	\[51 Sornlerthmwanich, Virach." ></td>
	<td class="line x" title="144:156	(1993), 'Word Segmen= ration for Thai in Machine Translation System', Machine Translation, National Electronics and Computer Teclmology Center, (in Thai)." ></td>
	<td class="line x" title="145:156	5." ></td>
	<td class="line x" title="146:156	Conclusion 'File proposed approach is a significantly new method to manipulate the classifier phrase in Thai language." ></td>
	<td class="line x" title="147:156	The fact that the expression of some syntactic constituents needs a specific classifier to be constnmted with and the selection of classifier lot each noun or noun phrase depends on tile traditional use and the senmntic class." ></td>
	<td class="line x" title="148:156	The corpus-based approach is quite suitable for detecting the traditional use and searching for the most appropriate one wlmn it does not exist in the corpus yet." ></td>
	<td class="line x" title="149:156	Concept hierarchy of noun provides another path for searching when the NCA does not cover the noun in question." ></td>
	<td class="line x" title="150:156	In the future, this NCA will be included in the generation process of Machine Translation to solve the classifier assignment, and incoqmrated in the analysis process to produce a proper syntactic and semantic structure." ></td>
	<td class="line x" title="151:156	The classifier will then be a key for pattern disambiguation when it is fixed to one of the patterns illustrated in Fig." ></td>
	<td class="line x" title="152:156	2.1." ></td>
	<td class="line x" title="153:156	Acknowledgement We wish to thank the National Electronics and Computer Technology Center (NECTEC) and Center of tile International Cooperation for Computerization (CICC) who provide facilities and a large corpus base for the rescarchl References \[ 1 \] Biber, I)ouglas." ></td>
	<td class="line x" title="154:156	(I 993)." ></td>
	<td class="line x" title="155:156	'Co-occurrence Patterns aulong Collocations: A 'Fool for Corpt, s-Based Lexical Knowledge Acquisition'." ></td>
	<td class="line x" title="156:156	Comlmtational 561 Corpus-based NLP" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C94-1096
An IBM-PC Environment For Chinese Corpus Analysis
Luk, Robert Wing Pong;"></td>
	<td class="line x" title="1:115	AN IBM-PC ENVIRONMENT FOR CHINESE CORPUS ANALYSIS Robert Wing Pong Luk Department of Chinese, Translation and Linguistics, City Polytechnic of Hong Kong Email: CTRWPL92@CPtlKVX.BITNET ABSTRACT This paper describes a set of computer programs for Chinese corpus analysis." ></td>
	<td class="line x" title="2:115	These programs include (1) extraction of different characters, bigrams and words; (2) word segmentation based on bigram, maximal-matching and the combined technique; (3) identification of special terms; (4) Chinese concordancing; (5) compiling collocation statistics and (6) evaluation utilities." ></td>
	<td class="line x" title="3:115	These programs run on the IBMPC and batch programs co-ordinate the use of these programs." ></td>
	<td class="line x" title="4:115	L INTRODUCTION Corpus analysis utilities are developed and widely available for English." ></td>
	<td class="line x" title="5:115	For example, tbe Oxford Concordance program is available for over 10 kinds of mainframe computer (Hockey and Martin, 1987) and the Longman mini-concordancer (Trible and Joncs, 1990) is available for the sales." ></td>
	<td class="line oc" title="6:115	Further enhancement of these utilities include compiling collocation statistics (Smadja, 1993) and semi-automatic gloassary construction (Tong, 1993)." ></td>
	<td class="line x" title="7:115	Current research has focused on bilingual corpora (Gale and Clmrch, 1993) with the alignment of parallel-text becomeing an important technical problem." ></td>
	<td class="line x" title="8:115	However, there has been little development of corpus analysis tools for Chinese." ></td>
	<td class="line x" title="9:115	Since using Chinese fiJr compnters has only become more generally availablein the last ten years, analysis utilities for Chinese are not widely." ></td>
	<td class="line x" title="10:115	Although no integrated environment is available for Chinese corpus analysis, many specific analysis programs have been reported in the literature (Kit et al. , 1989; Tong et al. , 1993; Chang and Chen, 1993; Zhou et al. , 1993)." ></td>
	<td class="line x" title="11:115	A Chinese concordance program a,~d a clmractcr-list extraction program are freely available from a Singapore (FTP) network site (Guo and Liu, 1992)." ></td>
	<td class="line x" title="12:115	tlowever, the programs run in the SUN workstations while many users, particularly non-computing experts, interact with an IBM-PC in Chinese, rather than a SUN workstation." ></td>
	<td class="line x" title="13:115	Tile rapid adwmce of microcomputers has mitigated many storage and processing speed problcms." ></td>
	<td class="line x" title="14:115	As for storage, the hard disk capacity can reach as high as 340M bytes which is adequate in comparison with tile demand for a corpus (8M bytes from the PH corpus) and a dictionary (10M bytes)." ></td>
	<td class="line x" title="15:115	Using a 486 processor, tile processing speed is acceptable if the user expect data to be analyzed over-night, similar to submilting a batch job to a mainframe computer." ></td>
	<td class="line x" title="16:115	For example; the utililies we are developing ranked around 42,000 words in a few minutes and produced about one-hundred lines of keyword-in-context in a few seconds for a 4 million character Chinese corpus." ></td>
	<td class="line x" title="17:115	This paper describes our effort to develop corpus analysis programs for Chinese." ></td>
	<td class="line x" title="18:115	Tile programs are written in Turbo C++, implemented on an IBM-PC (486) with a 120M byte hard disk." ></td>
	<td class="line x" title="19:115	The programs are divided into several types: a. format conversion program (norm.exe, phseg.exe, wform.exe) b. extraction of characters, bigrams and words (exsega.exe, exsegmi.exe,bigram.exe, miana.exe, worddh.exe, wral'~ka.exe, wlranka.cxe) c. word segmentation programs (bisegl.exe, whash.exc, bimaxn.exe) d. concordaucing programs (kwic.exe, kwicw.exe) e. collocation statistics programs (cxtract.exe, cxtractw.exe, cxstat.exe, cxstatw.exe) f. gcncral (evaluation) programs (wcomp.exe, scgperf.exe) To run these analysis utilities, a Chinese computing environment called Eten nmst be set up; otherwise Chinese characters cannot be displayed or entered." ></td>
	<td class="line x" title="20:115	Since there are many different Chinese characlers (i.e. 13,000) compared with Western languages, cach Chincse charactcr is specified by two bytes instead of one." ></td>
	<td class="line x" title="21:115	llowcver, many docmuent includes both single-byte characters and two-byte Chinese characters." ></td>
	<td class="line x" title="22:115	Tiros, tile conversion prograln, norlll.exe, is used to convcrt all the single--byte characters (i.e. AZ,a z :,;, {,~L#,$,%,^,&, *, (,),-,+,=,l,\,/,<,>,', {, }, 1,1, 09, ~, <space>, _ and ') i,~to their corresponding twobyte equivalent, for simplicity." ></td>
	<td class="line x" title="23:115	For example, the-singlebyte character 'a' is converted to '~ ' (2-byte)." ></td>
	<td class="line x" title="24:115	This program also changes tile docmnent iulo a clause or phrase format, using the -e option, where a new line is inserted after a punctuation mark (e.g. comma or fidl stop): --Jt,/kPq'q~-I ~l~\]-I~Jt, ll ' q,~ ~b~ ~/ff ~'~,;'IIM/~L~I:H~i~j~ ~-~e,) l, Figure 1 : Exlraet of file llong Kong Basic l~aw in clause format." ></td>
	<td class="line x" title="25:115	584 If the text are segmented into words by space or '/' markers, it is possible to change or delete these markers using the -s option." ></td>
	<td class="line x" title="26:115	Once, the document is cotwerted into two-byte format using norm.axe, the other utilities can be used." ></td>
	<td class="line x" title="27:115	Batch programs can be written to use these utilities." ></td>
	<td class="line x" title="28:115	For example, tile following batch program extracts different characters, performs bigram segmentation, extracts different words and obtain only the top 10% of the extracted words for compiling key-word in contexts and collocation statistics." ></td>
	<td class="line x" title="29:115	norm -t %1 -o carp.trap -e 2 -s 5 /' 1-byte to 2-byte; phrase format; delete space */ exsegs -t carp.trap -w 2 -b 0 /* extract different charaotsrs and bigrams =1 bigram-m 10 1' sort bigram and extract top 10% */ bisagl /' segment using the top 10% bigrams '/ wotddh 1= extract different words =/ wranka-m 10 1' sort and extract top 10% words '1 kwic -t carp.trap -k words.cut > kwie.lst \[* concordancing on the top 10% words '/ cxtract 1' extract different characters from contexts '/ oxstat /' compile collocation statistics '1 IL EXTRACTION PROGRAMS The extraction progranls assume that the text is not segmented." ></td>
	<td class="line x" title="30:115	Thus, norm.axe should be used to remove markers fi'om the seglnented text." ></td>
	<td class="line x" title="31:115	The programs, exsega.exe and exsegmi.cxe, extract different characters and their co-occurring characters, stored in cfreq.tmp (Fig 2) and bifile/mifile.tnlp, respectively." ></td>
	<td class="line x" title="32:115	The first program obtains the co-occurrence frequencies while the second obtains the inutnal infornmtion." ></td>
	<td class="line x" title="33:115	By default, tbe programs do not count punctuation but this can be override using the -a option, The different characters can be supplemented with information about their frequencies, pcrce,ltagcs and clunnlative percentages if the -w option is set to 2." ></td>
	<td class="line x" title="34:115	~~d 909 3.529 3.5 1 I~J/ 905 3.513 7.0 2 ~'-.f/ 789 3.063 10.1 '3 i'12/ 647 2.512 12.6 4 ~1~/ 645 2.504 15.1 5 ~J:/ 630 2.446 17.6 6 Figure 2: Part of tile extracted single characters from the I long Kong Basic latw." ></td>
	<td class="line x" title="35:115	The characters are ranked by their frequencies." ></td>
	<td class="line x" title="36:115	The first number is the fi'equency, followed by the percentage, cumulative percentage and rank number." ></td>
	<td class="line x" title="37:115	By default, all tltc different ch:lracters are stored." ></td>
	<td class="line x" title="38:115	However, sometimes only tile most frequently or infrequently occurring characters are interesting candidates for filrther investigation (e.g. concordaucing)." ></td>
	<td class="line oc" title="39:115	The user can select characters by their frequencies (i.e. -f and -g options), the top or bottom N% (i.e. -m and -n options), their ranks (i.e. -r and -s options) and by their frequencies above two standard deviations phlS the mean (Smadja, 1993) (i.e. -z option)." ></td>
	<td class="line x" title="40:115	By default, the extracted bigrams have frequencies above unity but this can be override using the -b option." ></td>
	<td class="line x" title="41:115	The bigrams stored can be sorted according to their frequencies or their mutual information in descending order using bigram.exe and miens.axe, respectively." ></td>
	<td class="line x" title="42:115	The sorted bigratns are stored in bifile.rnk or mifile.rnk." ></td>
	<td class="line x" title="43:115	The user can select different bigrams using options available for exseg programs (i.e. -f, -g, -m, -n, -r, -s and -z options)." ></td>
	<td class="line x" title="44:115	Both programs give the frequency distribution of the bigram frequencies and the log of their freqnencies." ></td>
	<td class="line x" title="45:115	The selected bigrams will become usefid for detecting componnd nouns or word segmentation (Zhang et el., 1992)." ></td>
	<td class="line x" title="47:115	Given the text is segtnented by '/' markers (space markers can be converted using norm.exe), worddh.exe can extract all the different words from the text and compute word frequencies." ></td>
	<td class="line x" title="48:115	The program extracted 42,613 words from the PIt corpus." ></td>
	<td class="line x" title="49:115	There is no limit to the number of different words that it can extract but it needs some disk space to hold temporary files." ></td>
	<td class="line x" title="50:115	The extracted words are stored in words.lst and they are sorted in descending frequencies using wranka.exe, hi addition, wlranka.exe sorts the extractcd words firstly by word length and secondly by their frequencies." ></td>
	<td class="line x" title="51:115	This is particularly usefid to examine compound notms, technical terms and translated words as they tend 10 be long." ></td>
	<td class="line x" title="52:115	Furthermore, the segmentation program, whash.exe, needs the words to be order by their length." ></td>
	<td class="line x" title="53:115	111." ></td>
	<td class="line x" title="54:115	WORI) SEGMENTATION PROGRAMS Unlike English, Chinese words are not delimited by any tentative markers like spaces although Chinese clanses are easily identified (Fig 1)." ></td>
	<td class="line x" title="55:115	Many segmentation programs were proposed (Chiang et el, 1993; Fan and Tsai, 1988)." ></td>
	<td class="line x" title="56:115	We have re-implemented the n~axinml-matchillg technique (Kit et al, 1989) using a word list, L, because it is simple to program and achieved one of the best segmentation performance (I2% error rate)." ></td>
	<td class="line x" title="57:115	However, the segmentation accuracy is degraded significantly (to 15% error rate in (Luk, 1993)) when the text has many compotmd notms and technical terms since the accuracy depends on the coverage of L. A word segmentation program using bigrams as well as combining bigrams and maximalmatching was subsequently developed." ></td>
	<td class="line x" title="58:115	The basic idea of tnaximal-nlalching is to match the input clause from left-to-right with entries in the given word list, L. If there is more than one matches, the longest entry is selected." ></td>
	<td class="line x" title="59:115	The process iterates with the remaining clause at the end with the clause matched with the longest entry." ></td>
	<td class="line x" title="60:115	Apart from luaxilnal-matching, 585 whash.exe divides and output the text in the clause format (Fig 2)." ></td>
	<td class="line x" title="61:115	The file that holds the word list can be specified using the -b option and the text using the -t option." ></td>
	<td class="line x" title="62:115	Tile word list should rank tile words, firstly, by their length in descending order (use wlranka) and, secondly, by their .frequencies." ></td>
	<td class="line x" title="63:115	Usually, the segmented clauses are displayed on tile screen for visual inspection after which the ou'tput can be redirected using the > option (MS DOS 5.0 option)." ></td>
	<td class="line x" title="64:115	The current whash.exe program can hold around 20,000 Chinese words in the main memory for segmentation but this is not large enough for a general Chinese dictionary (Fu, 1987) which has about 54,000 entries." ></td>
	<td class="line x" title="65:115	The bigram technique does not need any dictionary for segmentation." ></td>
	<td class="line x" title="66:115	This technique needs a set of bigrams extracted, from the text or from a general corpus." ></td>
	<td class="line x" title="67:115	Typically, tile top 10% of tile bigrams are captured and ranked according to their co-occurrence frequencies (CF) or mutual information (MI)." ></td>
	<td class="line x" title="68:115	This is due to the fac that if tile distributions of CF and MI are normal, then the top 10% corresponds to the 10% significance level." ></td>
	<td class="line x" title="69:115	The distribution of MI lypically does appear normal bnt not for CF." ></td>
	<td class="line x" title="70:115	The top N% bigranls are stored ill either bifile.cut or mifile.cut, The bigram segmentation program, bisegl.exe, loads the bigrams using the -b option." ></td>
	<td class="line x" title="71:115	A segmentation marker is placed between two characters in the text if the bigram of these two adjacent characters does not appear in bifile.cut or mifile.cut." ></td>
	<td class="line x" title="72:115	This segmentation is the same as performing nearest-neighbour clustering of substrings (l,nk, 1993)." ></td>
	<td class="line x" title="73:115	The program detected many non-words depending on N. However, the number of non-words are significantly reduced if we restrict to examining only the top N% (say 10) of the frequently occurring words." ></td>
	<td class="line x" title="74:115	Both maximal-umtching and bigranl techniques were combined, in order to detect words not in the word list and reduce tile amount of non-words detected (Luk, 1993)." ></td>
	<td class="line x" title="75:115	Maximal-matching is carried out first and the bigram technique is used to combine consecutive singlecharacter words in the segmented text since words not in L are usually segmented into smaller ones by maximalmatching." ></td>
	<td class="line x" title="76:115	The test data shows that the combined technique reduced tbe error rate by 33'/o and detected 33% of the desired words not in L. The combined techinque is written as a batch program as follow: whesh -b wordlst.txt-t text > text.trap /' maximal-match with existing word list '/ bimaxn -t text.trap /' combine single-character words from segmented text '/ worddh 1' extract words from segmented text '/ wlrenka -t words.lst /' rank words by their lengths ~/ whash -b wordl.rnk -t text > text.res /' maximal-match with identified words '/ IV." ></td>
	<td class="line x" title="77:115	CONCORDANCE PROGRAMS We modified tile concordance program by Guo and Lin (1992) since tile program assumed that the main nlenlory can hold the entire corpus or text." ></td>
	<td class="line x" title="78:115	Instead, the modified program loads a portiou called a page into the main memory and performs matching to find the appropriate contexts." ></td>
	<td class="line x" title="79:115	The page size can be changed using file -p option but we fouud that tile program operates well at -p 10000 (which is the default size)." ></td>
	<td class="line x" title="80:115	The modified programs, kwic.exe and kwicw.exe, can process files of size just over 2G bytes which is much bigger than the hard disk." ></td>
	<td class="line x" title="81:115	38 l,~Jt-)~utll~l,:~l~, ~l~</il:~>~JJ;~t'~Tth~3 Figure 3: The keyword-ln-context (kwlc) fi~m~at produced by kwie.exe." ></td>
	<td class="line x" title="82:115	Note that the line ntm'tbers are on the left-most posilioes and the keyword is delimited by '<' and '>'." ></td>
	<td class="line x" title="83:115	A keyword file mnst be specified using the -k option and each keyword sltould be terminated by '/'." ></td>
	<td class="line x" title="84:115	The nunlber of characters in tile left and right contexts can be spccified in bytes, using file -1 and -r options respectively." ></td>
	<td class="line x" title="85:115	If-n 0 is specified then lille numbers will appear on the left." ></td>
	<td class="line x" title="86:115	There are additional options for indexing in the original concordance programs but these options are not important in tile current implementation." ></td>
	<td class="line x" title="87:115	Tbe kwicw.exe deals with segmented text." ></td>
	<td class="line x" title="88:115	tlere, the -1 and -r options specify the number of words in the left and right contexts." ></td>
	<td class="line x" title="89:115	The length of each context (approx." ></td>
	<td class="line x" title="90:115	1000 characters allocated) can hold 20 words assuming that each word has 24 characters." ></td>
	<td class="line x" title="91:115	V. COMPILING COLLOCATION STATISTICS Collocation statistics (Fig 4) refers to tile frequencies of each different words or characters at different positions in the contexts of a keyword." ></td>
	<td class="line x" title="92:115	These frequencies are usefid for detecting significant collocation in English but these frequencies are tedious and error prone to conlpile by hand." ></td>
	<td class="line x" title="93:115	We have also written programs lo compile these statistics lbr Chinese but factorial analysis (l\]iber, 1993) still rem,'fins Io be implenlcnlcd." ></td>
	<td class="line x" title="94:115	Chinese concordancing is carricd out first to extract the relevant contexts." ></td>
	<td class="line x" title="95:115	The output of concordancing shonld be storcd in kwic.lst." ></td>
	<td class="line x" title="96:115	Theu, cxtractl.exe will extract all the different words in the context, using an FSM to decode the kwie format." ></td>
	<td class="line x" title="97:115	The program sorts these words according to their fieqncncy of occurrence in the context." ></td>
	<td class="line x" title="98:115	The different words are stored in cxtract.crk and the user can select candidates using options as in exsega.exe Next, cxstat.exe compile the frequencies of these different words at differeut positions in the contexts." ></td>
	<td class="line x" title="99:115	The statistics are stored in cxtract.sla." ></td>
	<td class="line x" title="100:115	For segmented text, kw~cw.exe, cxtractw.exe and cxsiaiw.exe are used instead." ></td>
	<td class="line x" title="101:115	586 kov =lgJ/ <1~>/ \[ 711 0 0 0 0 O< 71> 0 0 0 0 0,fill I 501 5 6 9 2 O< O> 1 4 10 8 5 ~'/ \[ 241 a 7 1 2 1< o> 0 2 1 1 2 ~/ \[181 o o 2 1 I< O> 1 0 1 I 2 Figure 4: Collimation statistics." ></td>
	<td class="line x" title="102:115	'rile dilli~rent words in the contexts are displayed on the left and flJe square brackets show tile frequency of occurrence in the context of the keyword." ></td>
	<td class="line x" title="103:115	The mlgle brackets indicate tile position orthe key,.vord." ></td>
	<td class="line oc" title="104:115	Unlike Smadja (1993), the ke~vord rnay be part of a Chinese word." ></td>
	<td class="line x" title="105:115	Thus, the program can compile statistics about different prefixes, suffixes or stems of a Chinese word." ></td>
	<td class="line x" title="106:115	This is particularly interesting for itwestigatiog translated terms and compound nouns." ></td>
	<td class="line x" title="107:115	VL EVALUATION PROGRAMS Two progranls were written to meastlre the performance of word segmentation and word identification." ></td>
	<td class="line x" title="108:115	For segmentation, segperf.exe examines two identical texts that were segmented by different mettmds." ></td>
	<td class="line x" title="109:115	The program shows the amount of segmentation error, the number of clauses, the mmlber of clause th.'lt are segmented correctly and the amount of overor under-segmentation." ></td>
	<td class="line x" title="110:115	Files of the segntented texts are specified by the -a and -m options." ></td>
	<td class="line x" title="111:115	The user can inspect parallel clauses to examine individual differences in segmentation by setting the -d (diagnostic) option to 1." ></td>
	<td class="line x" title="112:115	For word identification, wcomp.exe compares two sets of different word lists and determines the antount of word overlap." ></td>
	<td class="line x" title="113:115	The program shows the distribution of word overlap for different length of words." ></td>
	<td class="line x" title="114:115	This is important since long words tend to be compound nouns thal are not in a general dictionary." ></td>
	<td class="line x" title="115:115	Using the -i and -j options, the program saves words that overlap and words that do not overlap, respectively." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C94-2202
Lexical Functions And Machine Translation
Heylen, Dirk;Maxwell, Kerry G.;Verhagen, Marc;"></td>
	<td class="line x" title="1:143	LEXICAL FUNCTIONS AND MACHINE TRANSLATION Dirk Heylen, Kerry G. Maxwell and Marc Verhagen OTS, Trans 10, 3512 JK Utrecht, Netherlands CLMT Group, Essex University, Colchester, Essex CO4 3SQ, England email: heylen@let.ruu.nl, kerry@essex.ac.uk, verhm@essex.ac.uk This paper discusses the lexicographical concept of lexical functions (Mel'~uk and Zolkovsky, 1984) and their potential exploitation in the development of a machine translation lexicon designed to handle collocations." ></td>
	<td class="line x" title="2:143	We show how lexical functions can be thought to reflect cross-linguistic meaning concepts for collocational structures and their translational equivalents, and therefore suggest themselves as some kind of languageindependent semantic primitives from which translation strategies can be developed." ></td>
	<td class="line x" title="3:143	~ 1 Description of the Problem Collocations present specific problems in translation, both in human and automatic contexts." ></td>
	<td class="line x" title="4:143	If we take the construction heavy smoker in English and attempt to translate it into French and German, we find that a literal translation of heavy yields the wrong result, since the concept expressed by the adjective (something like ' excess:i_ve ' ) is translated by grand (large) in French and stark (strong) in German." ></td>
	<td class="line x" title="5:143	We observe then that in some sense the adjectives stark, grand and heavy are equivalent in the collocational context, but that this is of course not typically the case in otber contexts, ef grande boite, starke Schachtel and heavy box, where the adjectives could hardly be viewed as equiwdent." ></td>
	<td class="line x" title="6:143	It seems then that adjectives which are not literal translations of one another may share meaning properties specifically in the collocational context." ></td>
	<td class="line x" title="7:143	How then can we specify this special equivalence in the machine translation dictionary?" ></td>
	<td class="line x" title="8:143	The answer seems to lie in addressing the concept which underlies the union of adjective and noun in these three cases, i.e., intensification, and hence establish a single meaning representation tbr the adjectives which can be viewed as an interlingual pivot for translation." ></td>
	<td class="line x" title="9:143	Collocations have been studied by computational linguists in different contexts." ></td>
	<td class="line oc" title="10:143	For instance, there is a substantial body of papers on the extraction of 'frequently co-occurring words' from corpora using statistical methods (e.g. , (Choueka et al. , 1983), (Church and Hanks, 1989), (Smadja, 1993) to list only a few)." ></td>
	<td class="line o" title="11:143	These authors focus on techniques for providing material that can be used in other processing tasks such as x The research rcpmlcd in this paper was undmtaken as the project 'Collocations and the Lexicalisation of Semantic Operations' (ET10/75)." ></td>
	<td class="line x" title="12:143	Financial contributions weir by the Commission of the European Community, Association Suissetra (Geneva) and Oxford University Press." ></td>
	<td class="line x" title="13:143	word sense disambiguation, information retrieval, natural language generation and so on." ></td>
	<td class="line x" title="14:143	Also, the use of collocations in different applications has been discussed by various authors ((McRoy, 1992), (Pnstejovsky et al. , 1992), (Smadja and McKeown, 1990) etc.)." ></td>
	<td class="line x" title="15:143	However, collocations are not only considered usefnl, but also a problem both in certain applications (e.g. generation, (Nirenburg et al. , 1988), machine translation, (Heid and Raab, 1989)) and fiom a more theoretical point of view (e.g.(Abeill6 and Schabes, 1989), (Krenn and Erbach, to appear))." ></td>
	<td class="line x" title="17:143	We have been concerned with investigating the lexical.\['unctions (IJTs) of Mel'0,uk (Mel'6uk and Zolkovsky, 1984) as a candidate interllngual device for tbe translation of adjectival and verbal collocates." ></td>
	<td class="line x" title="18:143	Our work is related to research by (Heid and Raab, /989)." ></td>
	<td class="line x" title="19:143	In some respects it is an extension of some of their suggestions." ></td>
	<td class="line x" title="20:143	Our work differs fi'om theirs in scope and also in the exploration of wtrious other directions." ></td>
	<td class="line x" title="21:143	2 Representation The use we make of lexical functions as interlingual representations, does not respect their original Mel'~.ukian interpretation." ></td>
	<td class="line x" title="22:143	Furthermore, we have transferred them from their context in the Meaning-Text Theory to a different theoretical setting." ></td>
	<td class="line x" title="23:143	We have embedded the concept in an HPSG-like grammar theory?" ></td>
	<td class="line x" title="24:143	In this section we review this operation." ></td>
	<td class="line x" title="25:143	First we consider the features of Mel'~:nk's treatment that we have wanted to preserve." ></td>
	<td class="line x" title="26:143	Next we show how they have been imported into the HPSG fi'amework." ></td>
	<td class="line x" title="27:143	2.1 Collocations and LFs In Mel'~nk's Explanatory Combinatory Dictionary (ECD, see (Mel'~uk et al. , 1984)), expressions such as uneJerme intention, une rdsistance acharnde, un argument de poids, un bruit it~fernal and donner une lefon, faire un pas, commetre un crime are described in the lexical combinatorics zone." ></td>
	<td class="line x" title="28:143	These 'expressions plus ou moins fig6es' will be called 'collocations'." ></td>
	<td class="line x" title="29:143	They are considered to consist of two parts -the base and the collocate." ></td>
	<td class="line x" title="30:143	In the examples above, the nouns are the bases and the adjectives and the verbs are the collocates." ></td>
	<td class="line x" title="31:143	The idea that all adjective collocates and all the verb 2Head Drivt~n Phrase SlltlCItlrc granllllar, see (Pollard and Sag, 1987), (Pollard and Sag, to appear)." ></td>
	<td class="line x" title="32:143	For another treatment,:ff collo cations in HPSG, see (Krenn and ltrbach, to appear)." ></td>
	<td class="line x" title="33:143	1240 collocates share an important meaning component --roughly paraphrasable as intense and do respectively -and the fact that the adjectives and verbs are not interchangeable but are restricted with this meaning to the accompanying nouns, is coded in the dictionary using lcxical functions (in this case Magn and Oper)." ></td>
	<td class="line x" title="34:143	Each article in the ECD describes what is called a 'lexeme': a word in some specflic reading." ></td>
	<td class="line x" title="35:143	In the lexical combinatorics zone, we lind a list of the lexical funclions that are relevant to this particular lexeme." ></td>
	<td class="line x" title="36:143	Each lexical function is followed by one or more lexcrees (the result or value of the function applied to tile head word)." ></td>
	<td class="line x" title="37:143	The idea is that each combination of the argument with one of the values of the function forms a collocation in our terminology." ></td>
	<td class="line x" title="38:143	The argument corresponds to tile base and each value is a collocate." ></td>
	<td class="line x" title="39:143	The |ollowing fcatures of this representation are important to us." ></td>
	<td class="line x" title="40:143	 l,exical functions are used to represent an important syutactico-semantic relation between tile base and the collocate." ></td>
	<td class="line x" title="41:143	 The restricted combinatorial potential of the collocate lexcme is accounted for by listing it at each base with which it can occur." ></td>
	<td class="line x" title="42:143	The secund of these characteristics points out that the collocational restriction is seen as a purely lexical, idiosyncratic one: all collocations are explicitly listed." ></td>
	<td class="line x" title="43:143	One other aspect of collocations which we have to deal with is the relation between the collocate lexeme and its freely occurring counterpart." ></td>
	<td class="line x" title="44:143	Collocate lcxemes often differ in some respects from their literal variants while sharing other properties." ></td>
	<td class="line x" title="45:143	Mel'6uk deals with this by including in the ECD an entry for the free variant and putting tile collocate-specific information in the entry for the base (with the result of the lexical functions)." ></td>
	<td class="line x" title="46:143	The fill entry of the eolh)cate is the result of taking the entry for the free variant and overwriting it with the information provided at the base." ></td>
	<td class="line x" title="47:143	2.2 Collocations in HPSG The three aspects of Mel'6uk's analysis we wanted to encode in HPSG were the following." ></td>
	<td class="line x" title="48:143	Coding the base-collocate relation in the lexicon." ></td>
	<td class="line x" title="49:143	 Choosing the level at which \[cxical functions will be situated." ></td>
	<td class="line x" title="50:143	 Relating the collocate information to the free wniant entry." ></td>
	<td class="line x" title="51:143	We have provided straightforward solutions to these problems." ></td>
	<td class="line x" title="52:143	For tire first problem we have taken over the ECD architecture rather directly, by creating a dedicated 'collocates' field in the entry t)r bases which contains all the relevant collocates." ></td>
	<td class="line x" title="53:143	As far as the second problem is concerned, the obvious place to put lcxical funclions is in the semantic reprcscntation provided by HPSG." ></td>
	<td class="line x" title="54:143	There are wtrious reasons for this." ></td>
	<td class="line x" title="55:143	One is that 13;s arc used in lhe deep syntax level in Mel'6uk's model, a level oriented towards meaning." ></td>
	<td class="line x" title="56:143	Another reason is that this level seems most appropriate to be used in transfer/translation and because we want to use lexical functions in transfer, this is where they should be." ></td>
	<td class="line x" title="57:143	In contrast to the ECD, the meaning of the collocate is represented by the lexical function only." ></td>
	<td class="line x" title="58:143	The following is an example of the entry for criticism with the encoding of strong as a collocate, a We use SEM_IND as all abbreviation for the feature path SEM.CONT.INI)." ></td>
	<td class="line x" title="59:143	PHON criticism REST {cl'iticislll(\[~)} COLLS { SEM_IND VAIl \[~' } REST {Magn(~l~)} Just as in the ECD the base contains a specific zone in which the collocates are listed." ></td>
	<td class="line x" title="60:143	In our case, the feature 'COLLS' has a set of lexical entries as its value." ></td>
	<td class="line x" title="61:143	Each collocate subentry bears the value of the lexical function in its semantics field." ></td>
	<td class="line x" title="62:143	In this representation the lexical function is chosen as the real semantic value of the collocate." ></td>
	<td class="line x" title="63:143	One should read the feature structure as specifying that the semantics of strong (as a collocate) is the predicate Magn(\[~)." ></td>
	<td class="line x" title="64:143	The collocate subentry only provides partial information." ></td>
	<td class="line x" title="65:143	In fitct, it provides only the intbrmation that is specific to the occurrence of strong in its combination with criticism." ></td>
	<td class="line x" title="66:143	In this case only the semantics is given." ></td>
	<td class="line x" title="67:143	We further assume that the lexicon also contains a 'super-entry' which provides all the information that is shared by all the diflerent occurrences of strong." ></td>
	<td class="line x" title="68:143	This entry is where the variable Sstrong points to." ></td>
	<td class="line x" title="69:143	Of course, other architectures that try to avoid redundant specification of information are equally possible." ></td>
	<td class="line x" title="70:143	For instance if one assumes a mechanism of default unification, one can have Sstrong refer to the full entry describing 'strong' in say its ordinary use, and have the values that are particular to the collocational strong overwrite the values provided in the ordinary entry, as in Mel'~uk's proposal." ></td>
	<td class="line x" title="71:143	Collocations, Rules and Principles So far, we have not specified in what way one gets flom the lexical entries for the base and the collocate to the representation of the collocational expression." ></td>
	<td class="line x" title="72:143	ill HPSG, tile descriptions of complex expressions arc constrained by principles." ></td>
	<td class="line x" title="73:143	We will assume that collocations are snbject to the same constraints." ></td>
	<td class="line x" title="74:143	The ordinary rules of combination (combining adjectives and nouns, for instance) thus account for lnost of the properties of the collocational combination." ></td>
	<td class="line x" title="75:143	However, we are still left with the typical 'collocational restriction' which nceds to be accounted for." ></td>
	<td class="line x" title="76:143	We havc therefore addcd a principle which says that constructions that are analysed as collocations (indi cated by tile type COLI.OCATION) are either head-adjunct structure or head-complement structures with specific rcstrietions holding between the head anti the adjunct or aNoticc that hcrc we use a simple VCl'Sion of HPSG based on (Pollard and Sag, 1987) whereas the actual ilnplmncntation was based on (Pollard and Sag, to appear)." ></td>
	<td class="line x" title="77:143	1241 the head and the complement respectively." ></td>
	<td class="line x" title="78:143	Let's consider the former case 4, illustrated by the heavy smoker example, The adjunct daughter will contain the adjective collocate." ></td>
	<td class="line x" title="79:143	In such collocational constructions the collocate adjuncts have to be 'licensed' by the noun or the head daughter." ></td>
	<td class="line x" title="80:143	This is implemented by requiring that the collocates field (C'OI,LS) of the head daughter contains a reference to a lexical entry that is compatible with the adjunct daughter." ></td>
	<td class="line x" title="81:143	In the literal reading of an expression such as heavy smoker, the phrase will not be analysed as a COLL.OCATION and the principle does not apply." ></td>
	<td class="line x" title="82:143	COLLOCATION -~e I H; jA Ii))7 I,)R'IR ~ CELLS {~1~\]} 1 v~OLLOCATE  , > \] COMP_DTRS < \[CELLS 1\[~}\] > 3 Issues in Translation The project has tried to investigate the use of lexical functions as an interlingual device, i.e., one which is shared by the semantic representations of collocations in the language pairs ~." ></td>
	<td class="line x" title="83:143	The typing of a collocation with such aflmction opens up the way to a treatment of collocations inside a given language module and hence to a substantial reduction in the number of collocations explicitly handled in the multilingual transfer dictionary." ></td>
	<td class="line x" title="84:143	The existence of a collocation function is established during analysis." ></td>
	<td class="line x" title="85:143	This infi)rmation is used to generate the correct translation in the target hmguage." ></td>
	<td class="line x" title="86:143	To illustrate, the English analysis modnle might analyse (1) as (2)." ></td>
	<td class="line x" title="87:143	The transfer module maps (2) onto (3) which is then synthesised by the French module to (4)." ></td>
	<td class="line x" title="88:143	(l) heavy smoker-)~ (2) Magn(smoker) -r (3) Magn(fumeur) -4 (4) grand fumeur The exmnple points out that the translation strategy is a mixture of transfer and interlingua." ></td>
	<td class="line x" title="89:143	The bases arc transferred but the representation of the collocate is shared between the source and the target representation." ></td>
	<td class="line x" title="90:143	This treatment of collocations rests, among others, on the assmnptions that there are only a limited number of lexical functions, that lexical functions can be assigned consistently, that all (or a signilicant nmnber ot) collocations realise a lexical function, that lexical functions are not restricted to particular languages~ etc. In the following paragraph we present an outline of the translation process." ></td>
	<td class="line x" title="91:143	Next, we discuss some of the problems which follow flom our approach and we propose some ways to solve them." ></td>
	<td class="line x" title="92:143	4'lb illustrate tile case of huad-conlplenlent structures olle coukl lake some support verb construction (also called ligh!" ></td>
	<td class="line x" title="93:143	verb consh'uction)." ></td>
	<td class="line x" title="94:143	t;For another application of LFs in a muhilingual NLP context see (Held and l/aab, 1989)." ></td>
	<td class="line x" title="95:143	For other Imatlnents of collocations in language generation see (Nirenburg et al. , 1988) and (Smadja and McKeown, 1990)." ></td>
	<td class="line x" title="96:143	1242 31 Lexical Functions as Interlingua it was assumed that the starting point for transfer is the semantic representation of the phrase." ></td>
	<td class="line x" title="97:143	Using a semantic representation as input to transfer implies that we relate semantic values of wm'ds and phrases." ></td>
	<td class="line x" title="98:143	For our purposes this is very satisfying since we will now be using the semantics of collocates instead of their orthography, in other words: we use lcxical flmctions and abstract away fl'om the particular realisation of a collocate in a particular language." ></td>
	<td class="line x" title="99:143	We now state the relation between the semantic representations of the source language and target language." ></td>
	<td class="line x" title="100:143	The semantic relation between the phrase heavy smoker and its French counterpart can be made explicit in the tbllowing bilingual sign: ENISEM-IND \[ VAR ~ \] RUST { smoker(~l-b,Magn(\[~)} FRISEM_INI) \[ VAR \[~ 1 REST {f. ,,,~arI\[-~ M.~/~I)} J Typically, the lexicon will contain a bilingual sign for each possible value of RELN." ></td>
	<td class="line x" title="101:143	Thus, for translating heavy smoker into grandfumeur we will need the obvious entry tot smoker-fumeur plus the entry below: ENISEM-IND \[ VAR \[~ \] REST {M'lgn(~)} The interlingual status of the lexical function is selfevident." ></td>
	<td class="line x" title="102:143	Any occurrence of Magn will be left intact during transfer and it will be the generation component that ultimately assigns a monolingual lexical entry to the LF." ></td>
	<td class="line x" title="103:143	6 3.2 Problems l,exical Functions abstract away from certain nuances in meaning and from different syntactic realizations." ></td>
	<td class="line x" title="104:143	We discuss some of the problems raised by this abstractkm in this section." ></td>
	<td class="line x" title="105:143	Overgenerality An important problem stems fiom the interpretation of LFs implied by their use as an interlinguanamcly that the meaning of the collocate in some ways reduces to the meaning implied by the lexical./unction." ></td>
	<td class="line x" title="106:143	This interpretation is trouble-free if we assume that LFs always deliver unique values; tmlb,'tunately cases to the contrary can be readily observed." ></td>
	<td class="line x" title="107:143	An example attested fiom our corpus was the range of adverbial constructions possible with the verbal head oppose: adamantly, bitterl3; consistently, steadjastly, strongly, vehementl); vigorously, deeply, resolutely, etc. The ftmction Magn is an appropriate descriptor in all cases since each adverb functions as a typical intensitier in this context." ></td>
	<td class="line x" title="108:143	However each adverb also denotes 6p'or more details we refer the reader to (Hcylen, 1993)." ></td>
	<td class="line x" title="109:143	There we also discuss our implementation in Alep, the C.E.C.'s unification." ></td>
	<td class="line x" title="110:143	based glTHillllilr writing environment." ></td>
	<td class="line x" title="111:143	some other nleaning aspect(s)." ></td>
	<td class="line x" title="112:143	'file inlprecisio|l of I,l;s will nlean that we have no means of distinguishing between the vmious intensifiers possible it| tile context of a given keyword, and hence will not have sufticient in forination to choose the most appropriate translation where, correspondingly, nntltiple possibilities exist in tile target language." ></td>
	<td class="line x" title="113:143	All important question here is how dramatic this loss of translation quality really is. It is essentially ill addressing the issue of ovelgenerality that Mel'~:uk introduces suband superscripts to lexical functions, enhancing their precision and making them sensitive to meaning aspects of tile lcxical items over which they operate." ></td>
	<td class="line x" title="114:143	Superscripts are illtended to make the nleaning of tile I,F nlore precise and he|me |nero likely to imply unary inappings between argu|nents and vahlcs, subscripts a|e used to reference a particular semautic COlllpOUellt of a keyword." ></td>
	<td class="line x" title="115:143	The introduclion of such devices into tile account of l,Fs demtmstrates hoth the need tk)r precision and the fact lbat it does seeul necessary to address semantic aspects of lexemes stand| ng it| co-occurrence relatio|ls." ></td>
	<td class="line x" title="116:143	Ill fact it has been asserted by sonm (e.g. , (Anick and Pustciovsky, 1990), (lteid and Raab, 1989)) that collocational systems are systematically predictable from the lexical Selllantics Of nt)tUlS, it) till atteln\]Jt to explore this notion furthel; we have investigated the appr(lach to nolninal semantics known as Qualia structure (Pustejovsky, 1991) and conside|ed how this lnay ct)tnpleu|ent the LF notion to inlprove its descriptive powe| r. alnoDg tile prolnising avenues that occur to tlS are, firstly, tile postulation of I,F subscripts based on the four Qualia roles (assuming thal these are tim lexically hies) relevant aspects of noun selnantics) and, secondly, the application of l,Fs to senlaulic (Qualia) structures rather titan monolithic lexenles; cg: tile I,l; Ibm is used in delivering evahlative qualitiers which are standard expressions of praise or approval." ></td>
	<td class="line x" title="117:143	One could ilnagine application of the ftmctio|| over the Coustitttlivc and Agentive tolcs of file noun lecture, to deliver: Flon(Const : lecture) =intormative Ben(Agent-." ></td>
	<td class="line x" title="118:143	: lecture) = clear In both cases tile idea is that tile precision of tile lexical function is essentially enhanced by appealing to tile semantic facets of ils argunlcnt." ></td>
	<td class="line x" title="119:143	Syntaetic Divergences Allother issue that has lo be raised conccl'lls tile trat|slation el'collocations into noncolh)cational constructions." ></td>
	<td class="line x" title="120:143	It' we are to ulaintain a co||sistent interlingual approach to tile hanslation of these cases, we illUSt cXlelld our l,F-bascd approach accordingly." ></td>
	<td class="line x" title="121:143	We consider o|/e case brielly." ></td>
	<td class="line x" title="122:143	Cross-lingtlistic analysis reveals many cases where nonlinal-based collocational ctmstructs are real|sod as conlpot|nds in Gernmnic hulguages, e.g., hunch of keys sleutelbos." ></td>
	<td class="line x" title="123:143	A possible account of such phenomena nlay be developed fronl the coucept (11: merged I,Fs (Mel'Suk and 7,olkovsky, \]970), Mcrged i,Fs are ill. tended It) be used ill cases where a value lexeme exists )'For i/ COlllllat'i,'-;on belwc/1 aspects of Ou.'l\[ia slrtlCllll'CS alld lcx ical ftmclions see (I lcylcn, to appear)." ></td>
	<td class="line x" title="124:143	which appears to effectively reduce ('|ncrge') an LF meani|lg and its specitied a|gu|nent to a single lcxi-." ></td>
	<td class="line x" title="125:143	calised for|n, rather than projecting a syntagmatic unit." ></td>
	<td class="line x" title="126:143	We could argue that ill cases of compound lbrtnation, exactly tile same process is to lie accounted tbr, siuce the Ctl|nl)ound embodies both the concept mediated by tile LF and its argument lexetne." ></td>
	<td class="line x" title="127:143	We coukl therefore allow compounds to be delivered as values of merged I,F's, eg: //Mult(sleutel)= steutelbos." ></td>
	<td class="line x" title="128:143	These observations are uscful in the MT COl|text if we assmne that we cau effect a |nappiug betweeu merged and unmerged lhSs and thcrefore capture tile correspondence between distinct structural realisations of tile same concept." ></td>
	<td class="line x" title="129:143	One way to ennflate such a Inappi|~g |night be through the use of Mel'~:uk's lexical paraphrasing rules." ></td>
	<td class="line x" title="130:143	For instance, one could conceive ol' a lexical paraphtasiug rule as follows~: W-}Mult(W) e, >//Mull(W)." ></td>
	<td class="line x" title="131:143	If we assurne that ill our |uouolingual English lexicou, we assign tile collocate bunch as tile Mult value of keyword kt?.,, and that accordingly in tile Dutch iexical entry Ior sleutel we instant|ate sleutelbos as the vahle of tile nlerged 1,1;//Mult, then we can use the paraphraslug rule to effect a nlaplfing between tile two 13;'s and hence arrive at an iuterlingual approach to tile trauslation of tile example, despite structural |nisu\]atches, i.e., key + bunch\[ Mult(key)\] sleutel bos\[ llMuit( sleutel ) l l;u~lher examples exist where productive nlorphological processes (e.g. , affixation 'q) lead to tile lexicali sat|tin in one language of concepts that exist as syntagnla|ic constructs ill another." ></td>
	<td class="line x" title="133:143	Again, we suggest tile usc ot'|nerged l,Fs and corresptmding nlappings via lexical paraphrasing rule,; as a possible Iranshttion strategy in these cases, 4 Summary and Conclusions hi ihis paper we have discussed how the lexicographical concept of h,xical./iulctionx, introduced by Mel'~,uk h) describe collocations, can lie used as till intmlingual device in the machine mlnslation of such structures." ></td>
	<td class="line x" title="134:143	We have shown how the essentials of the E(33 analysis can be embedded ill the lexicou and gramnlar of a unit|cation based theory of language." ></td>
	<td class="line x" title="135:143	Our use of lexical functions as an intcrlingua assunles thai the relevant aspects of tile meaning of the colhleate are fully captured by the LK The 1,1: there.fore determ i lies tl m accu racy of )ran s l at| on s, whi chm ay s This is our own |nit|alive it seems lo hc the case as we examine the literal urc lhat neither l,Fs such as Magll, BOll etc (i.c. , those repro sent|rig slandard qualifiers/attributes) nor indeed metxed 13:s tt'aluic in lexical imralflnasing rides, Wc wouhl argue thai cross-linguistic analysis stiggt~Ms Ihal Ihcy should enter this donmin; COlnpotmd lot lnation alld other lypcs of leg|ell|sat|on appear Io bc ICe)liar pal)of rig of II'aUshllioll tR;ross lllally o:~llocatioll,'d COllStrHctS, as WE ilhlslralc here." ></td>
	<td class="line x" title="136:143	9()no could Ihink of an example such as mis-ituerl)ret." ></td>
	<td class="line x" title="137:143	1243 be impoverished due to the generalised nature of basic LFs." ></td>
	<td class="line x" title="138:143	We have suggested some ways in which LFs can be enriched with lexical semantic intbrmation to improve translation quality." ></td>
	<td class="line x" title="139:143	The interlingua level reflects what is semantically common to expressions which form translational equivalents." ></td>
	<td class="line x" title="140:143	It abstracts away from specific syntactic realisations." ></td>
	<td class="line x" title="141:143	Given that collocations may translate as non-collocations, we also have to provide a way to represent these expressions using lexical functions." ></td>
	<td class="line x" title="142:143	We have provided an illustration on how to proceed in one such case." ></td>
	<td class="line x" title="143:143	Acknowledgements We would like to thank the following partners and colleagues: Susan ArmstrongWarwick, Laura Bloksma, Nicoletta Calzolari, R. Lee Humphreys, Simon Murison-Bowie and Andr6 Schenk." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="J94-4003
Word Sense Disambiguation Using A Second Language Monolingual Corpus
Dagan, Ido;Itai, Alon;"></td>
	<td class="line x" title="1:730	Word Sense Disambiguation Using a Second Language Monolingual Corpus Ido Dagan* AT&T Bell Laboratories Alon Itai t Technion--Israel Institute of Technology This paper presents a new approach for resolving lexical ambiguities in one language using statistical data from a monolingual corpus of another language." ></td>
	<td class="line x" title="2:730	This approach exploits the differences between mappings of words to senses in different languages." ></td>
	<td class="line x" title="3:730	The paper concentrates on the problem of target word selection in machine translation, for which the approach is directly applicable." ></td>
	<td class="line x" title="4:730	The presented algorithm identifies syntactic relations between words, using a source language parser, and maps the alternative interpretations of these relations to the target language, using a bilingual lexicon." ></td>
	<td class="line x" title="5:730	The preferred senses are then selected according to statistics on lexical relations in the target language." ></td>
	<td class="line x" title="6:730	The selection is based on a statistical model and on a constraint propagation algorithm, which simultaneously handles all ambiguities in the sentence." ></td>
	<td class="line x" title="7:730	The method was evaluated using three sets of Hebrew and German examples and was found to be very useful for disambiguation." ></td>
	<td class="line x" title="8:730	The paper includes a detailed comparative analysis of statistical sense disambiguation methods." ></td>
	<td class="line x" title="9:730	1." ></td>
	<td class="line x" title="10:730	Introduction The resolution of lexical ambiguities in nonrestricted text is one of the most difficult tasks of natural language processing." ></td>
	<td class="line x" title="11:730	A related task in machine translation, on which we focus in this paper, is target word selection." ></td>
	<td class="line x" title="12:730	This is the task of deciding which target language word is the most appropriate equivalent of a source language word in context." ></td>
	<td class="line x" title="13:730	In addition to the alternatives introduced by the different word senses of the source language word, the target language may specify additional alternatives that differ mainly in their usage." ></td>
	<td class="line x" title="14:730	Traditionally, several linguistic levels were used to deal with this problem: syntactic, semantic, and pragmatic." ></td>
	<td class="line x" title="15:730	Computationally, the syntactic methods are the most affordable, but are of no avail in the frequent situation when the different senses of the word show the same syntactic behavior, having the same part of speech and even the same subcategorization frame." ></td>
	<td class="line x" title="16:730	Substantial application of semantic or pragmatic knowledge about the word and its context requires compiling huge amounts of knowledge, the usefulness of which for practical applications in broad domains has not yet been proven (e.g. , Lenat et al. 1990; Nirenburg et al. 1988; Chodorow, Byrd, and Heidron 1985)." ></td>
	<td class="line x" title="17:730	Moreover, such methods usually do not reflect word usages." ></td>
	<td class="line x" title="18:730	Statistical approaches, which were popular several decades ago, have recently reawakened and were found to be useful for computational linguistics." ></td>
	<td class="line x" title="19:730	Within this framework, a possible (though partial) alternative to using manually constructed * AT&T Bell Laboratories, 600 Mountain Avenue, Murray Hill, NJ 07974, USA." ></td>
	<td class="line x" title="20:730	E-mail: dagan@research.att.com." ></td>
	<td class="line x" title="21:730	The work reported here was done while the author was at the Technion--Israel Institute of Technology." ></td>
	<td class="line x" title="22:730	t Department of Computer Science, Technion--Israel Institute of Technology, Haifa 32000, Israel." ></td>
	<td class="line x" title="23:730	E-maih itai@cs.technion.ac.il." ></td>
	<td class="line x" title="24:730	(~) 1994 Association for Computational Linguistics Computational Linguistics Volume 20, Number 4 knowledge can be found in the use of statistical data on the occurrence of lexical relations in large corpora (e.g. , Grishman, Hirschman, and Nhan 1986)." ></td>
	<td class="line oc" title="25:730	The use of such relations (mainly relations between verbs or nouns and their arguments and modifiers) for various purposes has received growing attention in recent research (Church and Hanks 1990; Zernik and Jacobs 1990; Hindle 1990; Smadja 1993)." ></td>
	<td class="line x" title="26:730	More specifically, two recent works have suggested using statistical data on lexical relations for resolving ambiguity of prepositional phrase attachment (Hindle and Rooth 1991) and pronoun references (Dagan and Itai 1990, 1991)." ></td>
	<td class="line x" title="27:730	Clearly, statistics on lexical relations can also be useful for target word selection." ></td>
	<td class="line x" title="28:730	Consider, for example, the Hebrew sentence extracted from the foreign news section of the daily Ha-Aretz, September 1990 (transcripted to Latin letters): (1) Nose ze mana' mi-shtei ha-mdinot mi-lahtom 'al hoze shalom." ></td>
	<td class="line x" title="29:730	issue this prevented from-two the-countries from-signing on treaty peace \[ This sentence would translate into English as (2) This issue prevented the two countries from signing a peace treaty." ></td>
	<td class="line x" title="30:730	The verb lahtom has four senses: 'sign,' 'seal,' 'finish,' and 'close'." ></td>
	<td class="line x" title="31:730	The noun hoze means both 'contract' and 'treaty,' where the difference is mainly in usage rather than in the meaning (in Hebrew the word h.oze is used for both sub-senses)." ></td>
	<td class="line x" title="32:730	One possible solution is to consult a Hebrew corpus tagged with word senses, from which we would probably learn that the sense 'sign' of lahtom appears more frequently with hoze as its object than all the other senses." ></td>
	<td class="line x" title="33:730	Thus we should prefer that sense." ></td>
	<td class="line x" title="34:730	However, the size of corpora required to identify lexical relations in a broad domain is very large, and therefore it is usually not feasible to have such corpora manually tagged with word senses) The problem of choosing between 'treaty' and 'contract' cannot be solved using only information on Hebrew, because Hebrew does not distinguish between them." ></td>
	<td class="line x" title="35:730	The solution suggested in this paper is to identify the lexical relations in corpora of the target language, instead of the source language." ></td>
	<td class="line x" title="36:730	We consider word combinations and count how often they appear in the same syntactic relation as in the ambiguous sentence." ></td>
	<td class="line x" title="37:730	For the above example, the noun compound 'peace treaty' appeared 49 times in our corpus (see Section 4.3 for details on our corpus), whereas the compound 'peace contract' did not appear at all; the verb-obj combination 'to sign a treaty' appeared 79 times, whereas none of the other three alternatives appeared more than twice." ></td>
	<td class="line x" title="38:730	Thus, we first prefer 'treaty' to 'contract' because of the noun compound 'peace treaty' and then proceed to prefer 'sign' since it appears most frequently having the object 'treaty'." ></td>
	<td class="line x" title="39:730	The order of selection is determined by a constraint propagation algorithm." ></td>
	<td class="line x" title="40:730	In both cases, the correctly selected word is not the most frequent one: 'close' is more frequent in our corpus than 'sign' and 'contract' is more frequent than 'treaty'." ></td>
	<td class="line x" title="41:730	Also, by using a model of statistical confidence, the algorithm avoids a decision in cases in which no alternative is significantly better than the others." ></td>
	<td class="line x" title="42:730	Our approach can be analyzed from two different points of view." ></td>
	<td class="line x" title="43:730	From that of monolingual sense disambiguation, we exploit the fact that the mapping between words and word senses varies significantly among different languages." ></td>
	<td class="line x" title="44:730	This enables 1 Hearst (1991) suggests a sense disambiguation scheme along this line." ></td>
	<td class="line x" title="45:730	See Section 7 for a comparison of several sense disambiguation methods." ></td>
	<td class="line x" title="46:730	564 Ido Dagan and Alon Itai Word Sense Disambiguation US to map an ambiguous construct from one language to another, obtaining representations in which each sense corresponds to a distinct word." ></td>
	<td class="line x" title="47:730	Now it is possible to collect co-occurrence statistics automatically from a corpus of the other language, without requiring manual tagging of senses." ></td>
	<td class="line x" title="48:730	2 From the point of view of machine translation, we suggest that some ambiguity problems are easier to solve at the level of the target language than the source language." ></td>
	<td class="line x" title="49:730	The source language sentences are considered a noisy source for target language sentences, and our task is to devise a target language model that prefers the most reasonable translation." ></td>
	<td class="line x" title="50:730	Machine translation is thus viewed in part as a recognition problem, and the statistical model we use specifically for target word selection may be compared with other language models in recognition tasks (e.g. , Katz 1987; Jelinek 1990, for speech recognition)." ></td>
	<td class="line x" title="51:730	To a limited extent, this view is shared with the statistical machine translation system of Brown et al.(1990), which employs a target language n-gram model (see Section 8 for a comparison with this system)." ></td>
	<td class="line x" title="53:730	In contrast to this view, previous approaches in machine translation typically resolve examples like (1) by stating various constraints in terms of the source language (Nirenburg 1987)." ></td>
	<td class="line x" title="54:730	As explained above, such constraints cannot be acquired automatically and therefore are usually limited in their coverage." ></td>
	<td class="line x" title="55:730	The experiments we conducted clearly show that statistics on lexical relations are very useful for disambiguation." ></td>
	<td class="line x" title="56:730	Most notable is the result for the set of examples of Hebrew to English translation, which was picked randomly from foreign news sections in the Israeli press." ></td>
	<td class="line x" title="57:730	For this set, the statistical model was applicable for 70% of the ambiguous words, and its selection was then correct for 91% of the cases." ></td>
	<td class="line x" title="58:730	We cite also the results of a later experiment (Dagan, Marcus, and Markovitch 1993) that tested a weaker variant of our method on texts in the computer domain, achieving a precision of 85%." ></td>
	<td class="line x" title="59:730	Both results significantly improve upon a naive method that uses only a priori word probabilities." ></td>
	<td class="line x" title="60:730	These results are comparable to recent reports in the literature (see Section 7)." ></td>
	<td class="line x" title="61:730	It should be emphasized, though, that our results were achieved for a realistic simulation of a broad coverage machine translation system, on randomly selected examples." ></td>
	<td class="line x" title="62:730	We therefore believe that our figures reflect the expected performance of the algorithm in a practical implementation." ></td>
	<td class="line x" title="63:730	On the other hand, most other results relate to a small number of words and senses that were determined by the experimenters." ></td>
	<td class="line x" title="64:730	Section 2 of the paper describes the linguistic model we use, employing a syntactic parser and a bilingual lexicon." ></td>
	<td class="line x" title="65:730	Section 3 presents the statistical model, assuming a multinomial model for a single lexical relation and then using a constraint propagation algorithm to account simultaneously for all relations in the sentence." ></td>
	<td class="line x" title="66:730	Section 4 describes the experimental Setting." ></td>
	<td class="line x" title="67:730	Section 5 presents and analyzes the results of the experiment and cites additional results (Dagan, Marcus, and Markovitch 1993)." ></td>
	<td class="line x" title="68:730	In Section 6 we analyze the limitations of the algorithm in different cases and suggest enhancements to improve it." ></td>
	<td class="line x" title="69:730	We also discuss the possibility of adopting the algorithm for monolingual applications." ></td>
	<td class="line x" title="70:730	Finally, in Section 7 we present a comparative analysis of statistical sense disambiguation methods and then conclude in Section 8." ></td>
	<td class="line x" title="71:730	2 A similar observation underlies the use of parallel bilingual corpora for sense disambiguation (Brown et al. 1991; Gale, Church, and Yarowsky 1992)." ></td>
	<td class="line x" title="72:730	As we explain in Section 7, these corpora are a form of a manually tagged corpus and are more difficult to obtain than monolingual corpora." ></td>
	<td class="line x" title="73:730	Erroneously, the preliminary publication of our method (Dagan, Itai, and Schwall 1991) was cited several times as requiring a parallel bilingual corpus, 565 Computational Linguistics Volume 20, Number 4 2." ></td>
	<td class="line x" title="74:730	The Linguistic Model Our approach is first to use a bilingual lexicon to find all possible translations of each lexically ambiguous word in the source sentence and then use statistical information gathered from target language corpora to choose the most appropriate alternative." ></td>
	<td class="line x" title="75:730	To carry out this task we need the following linguistic tools, which are discussed in detail in the following sections: Section 2.1: Parsers for both the source language and the target language." ></td>
	<td class="line x" title="76:730	These parsers should be capable of locating relevant syntactic relations, such as subj-verb, verb-obj, etc. Section 2.2: A bilingual lexicon that lists alternative translations for each source language word." ></td>
	<td class="line x" title="77:730	If a word belongs to several syntactic categories, there should be a separate list for each one." ></td>
	<td class="line x" title="78:730	Section 2.3: A procedure for mapping the source language syntactic relations to those of the target language." ></td>
	<td class="line x" title="79:730	Such tools have been implemented within the framework of many computational linguistic theories." ></td>
	<td class="line x" title="80:730	We have used McCord's implementation of Slot Grammars (McCord 1990, 1991)." ></td>
	<td class="line x" title="81:730	However, our method could have proceeded just as well using other linguistic models." ></td>
	<td class="line x" title="82:730	The linguistic model will be illustrated by the following Hebrew example, taken from the Ha-Aretz daily newspaper from September, 1990 (transcripted to Latin letters): (3) Diplomatim svurim ki hitztarrfuto shell Hon Sun magdila diplomats believe that the joining of Hon Sun increases et ha-sikkuyim l-hassagat hitqaddmut ba-sihot." ></td>
	<td class="line x" title="83:730	the-chances for-achieving progress in the-talks Here, the ambiguous words in translation to English are magdila, hitqaddmut, and sihot." ></td>
	<td class="line x" title="84:730	To facilitate the reading, we give the translation of the sentence into English, and in each case of an ambiguous selection, all the alternatives are listed within curly brackets, the first alternative being the correct one." ></td>
	<td class="line x" title="85:730	(4) Diplomats believe that the joining of Hon Sun {increases I enlarges I magnifies} the chances for achieving {progress I advance I advancement} in the {talks I conversations I calls}." ></td>
	<td class="line x" title="86:730	The following subsections describe in detail the processing steps of the linguistic model." ></td>
	<td class="line x" title="87:730	These include locating the ambiguous words and the relevant syntactic relations among them in the source language sentence, mapping these relations to alternative relations in the target language, and finally, counting occurrences of these alternatives in a target language corpus." ></td>
	<td class="line x" title="88:730	2.1 Locating the Ambiguous Words in the Source Language Our model defines the different 'senses' of a source word to be all its possible translations to the target language, as listed in a bilingual lexicon." ></td>
	<td class="line x" title="89:730	Some translations can be eliminated by the syntactic environment of the word in the source language." ></td>
	<td class="line x" title="90:730	For example, in the following two sentences the word 'consider' should be translated 566 Ido Dagan and Alon Itai Word Sense Disambiguation differently into Hebrew, owing to the different subcategorization frame in each case: (5) I consider him smart." ></td>
	<td class="line x" title="91:730	(6) I consider going to Japan." ></td>
	<td class="line x" title="92:730	In these examples, the different syntactic subcategorization frames determine two different translations to Hebrew (mah.shiv versus shoqel), thus eliminating some of the ambiguity." ></td>
	<td class="line x" title="93:730	Such syntactic rules that allow us to resolve some of the ambiguities may be encoded in the lexicon (e.g. , Golan, Lappin, and Rimon 1988)." ></td>
	<td class="line x" title="94:730	However, many ambiguities cannot be resolved on syntactic grounds." ></td>
	<td class="line x" title="95:730	The purpose of this work is to resolve the remaining ambiguities using lexical co-occurrence preferences, obtained by statistical methods." ></td>
	<td class="line x" title="96:730	2.2 Locating Syntactic Tuples in Source Language Sentences Our basic concept is the syntactic tuple, which denotes a syntactic relation between two or more words." ></td>
	<td class="line x" title="97:730	It is denoted by the name of the syntactic relation followed by a sequence of words that satisfies the relation, appearing in their base form (without morphological inflections)." ></td>
	<td class="line x" title="98:730	For example (subj-verb: man walk) is a syntactic tuple, which occurs in the sentence 'The man walked home'." ></td>
	<td class="line x" title="99:730	We assume that our parser (or an auxilliary program) can locate the syntactic relation corresponding to a given syntactic tuple in a sentence." ></td>
	<td class="line x" title="100:730	The use of the base form of words is justified by the additional assumption that morphological inflections do not affect the probability of syntactic tuples." ></td>
	<td class="line x" title="101:730	This assumption is not entirely accurate, but it has proven practically useful and reduces the number of distinct tuples." ></td>
	<td class="line x" title="102:730	In our experience, the following syntactic relations proved useful for resolving ambiguities:  Relations between a verb and its subject, complements, and adjuncts, including direct and indirect objects, adverbs, and modifying prepositional phrases." ></td>
	<td class="line x" title="103:730	 Relations between a noun and its complements and adjuncts, including adjectives, modifying nouns in noun compounds, and modifying prepositional phrases." ></td>
	<td class="line x" title="104:730	 Relations between adjectives or adverbs and their modifiers." ></td>
	<td class="line x" title="105:730	4 As mentioned earlier, the full list of syntactic relations depends on the syntactic theory of the parser." ></td>
	<td class="line x" title="106:730	Our model is general and does not depend on any particular list." ></td>
	<td class="line x" title="107:730	However, we have found some desired properties in defining the relevant syntactic relations." ></td>
	<td class="line x" title="108:730	One such property is the use of deep, or canonical, relations, as was already identified by Grishman, Hirschman, and Nhan (1986)." ></td>
	<td class="line x" title="109:730	This property was directly available from the ESG parser (McCord 1990, 1991), which identifies the underlying syntactic function in constructs such as passives and relative clauses." ></td>
	<td class="line x" title="110:730	We have also implemented an additional routine, which modified or filtered some of the relations received from the parser." ></td>
	<td class="line x" title="111:730	This postprocessing routine dealt mainly with function words and prepositional phrases to get a set of more informative relations." ></td>
	<td class="line x" title="112:730	For example, it combined the subject and complement of the verb 'be' (as in 'the man is happy') into a single relation." ></td>
	<td class="line x" title="113:730	Likewise, a verb with its preposition and the head noun of a modifying prepositional phrase (as in sit on the chair) were also combined." ></td>
	<td class="line x" title="114:730	The routine was designed to choose relations that impose considerable restrictions on the possible 567 Computational Linguistics Volume 20, Number 4 (or probable) syntactic tuples." ></td>
	<td class="line x" title="115:730	On the other hand, these relations should not be too specific, to allow statistically meaningful samples." ></td>
	<td class="line x" title="116:730	The first step in resolving an ambiguity is to find all the syntactic tuples containing the ambiguous words." ></td>
	<td class="line x" title="117:730	For (3) we get the following syntactic tuples: (7)." ></td>
	<td class="line x" title="118:730	2." ></td>
	<td class="line x" title="119:730	3. 4." ></td>
	<td class="line x" title="120:730	(subj-verb: hitztarrfut higdil) (verb-obj: higdil sikkuy) (verb-obj: hissig hitqaddmut) (noun-pp: hitqaddmut bsih.a) (these tuples translate as joining-increase, increase-chance, achieve-progress, and progressin-talks)." ></td>
	<td class="line x" title="121:730	In using these tuples, we expect to capture lexical constraints that are imposed by syntactic relations." ></td>
	<td class="line x" title="122:730	2.3 Mapping Syntactic Tuples to the Target Language The set of syntactic tuples in the source language sentence is reflected in its translation to the target language." ></td>
	<td class="line x" title="123:730	As a syntactic tuple is defined by both its syntactic relation and the words that appear in it, we need to map both components to the target language." ></td>
	<td class="line x" title="124:730	By definition, every ambiguous source language word maps to several target language words." ></td>
	<td class="line x" title="125:730	We thus get several alternative target language tuples for each source language tuple that involves an ambiguous word." ></td>
	<td class="line x" title="126:730	For example, for tuple 3 in (7) we obtain three alternatives, corresponding to the three different translations of the word hitqaddmut." ></td>
	<td class="line x" title="127:730	For tuple 4 we obtain nine alternative target tuples, since each of the words hitqaddmut and siha maps to three different English words." ></td>
	<td class="line x" title="128:730	The full mapping of the Hebrew tuples in (7) to English tuples appears in Table 1 (the rightmost column should be ignored for the moment)." ></td>
	<td class="line x" title="129:730	Each of the tuple sets (a-d) in this table denotes the alternatives for translating the corresponding Hebrew tuple." ></td>
	<td class="line x" title="130:730	From a theoretical point of view, the mapping of syntactic relations is more problematic." ></td>
	<td class="line x" title="131:730	There need not be a one-to-one mapping from source language relations to target language ones." ></td>
	<td class="line x" title="132:730	In many cases the mapping depends on the words of the syntactic tuple, as seen in the following example of translating from German to English." ></td>
	<td class="line x" title="133:730	(8) Der Tisch gefaellt mir.--I like the table." ></td>
	<td class="line x" title="134:730	In this example the source language subject (Tisch) becomes the direct object (table) in the target, whereas the direct object (mir) in the source language becomes the subject (I) in the target." ></td>
	<td class="line x" title="135:730	Therefore, the German syntactic tuples (9) (subj-verb: Tisch gefaellt) (verb-obj: gefaellt mir) are mapped to the following English syntactic tuples (10) (verb-obj: like table) (subj-verb: I like) (The Hebrew equivalent is similar to the German structure)." ></td>
	<td class="line x" title="136:730	In practice this is less of a problem." ></td>
	<td class="line x" title="137:730	In most cases, the source language relation has a direct equivalent in the target language." ></td>
	<td class="line x" title="138:730	In many other cases, transformation rules can be encoded, either in the lexicon (if they are word dependent) or as syntactic transformations." ></td>
	<td class="line x" title="139:730	These rules are usually available in machine translation systems that 568 Ido Dagan and Alon Itai Word Sense Disambiguation Table 1 The alternative target syntactic tuples with their counts in the target language corpus Source Tuples Target Tuples Counts a." ></td>
	<td class="line x" title="140:730	(subj-verb: hitztarrfut higdil) (subj-verb: joining increase) 0 (subj-verb: joining enlarge) 0 (subj-verb: joining magnify) 0 b. C. d." ></td>
	<td class="line x" title="141:730	(verb-obj: higdil sikkuy) (verb-obj: hissig hitqaddmut) (noun-pp: hitqaddmut bsih.a) (verb-obj: increase chance) 20 (verb-obj: enlarge chance) 0 (verb-obj: magnify chance) 0 (verb-obj: achieve progress) 29 (verb-obj: achieve advance) 5 (verb-obj: achieve advancement) 1 (noun-pp: progress in talk) 7 (noun-pp: progress in conversation) 0 (noun-pp: progress in call) 0 (noun-pp: advance in talk) 2 (noun-pp: advance in conversation) 0 (noun-pp: advance in call) 2 (noun-pp: advancement in talk) 0 (noun-pp: advancement in conversation) 0 (noun-pp: advancement in call) 0 use the transfer method, as this knowledge is required to generate target language structures." ></td>
	<td class="line x" title="142:730	To facilitate further the mapping of syntactic relations and to avoid errors due to fine distinctions between them, we grouped related syntactic relations into a single 'general class' and mapped this class to the target language." ></td>
	<td class="line x" title="143:730	The important classes used were relations between a verb and its arguments and modifiers (counting as one class all objects, indirect objects, complements, and nouns in modifying prepositional phrases) and between a noun and its arguments and modifiers (counting as one class all modifying nouns in compounds and nouns in modifying prepositional phrases)." ></td>
	<td class="line x" title="144:730	The classification enables us to get more statistical data for each class, as it reduces the number of relations." ></td>
	<td class="line x" title="145:730	The success of using this general level of syntactic relations indicates that even a rough mapping of source to target language relations is useful for the statistical model." ></td>
	<td class="line x" title="146:730	2.4 Counting Syntactic Tuples in the Target Language Corpus We now wish to determine the plausibility of each alternative target word being the translation of an ambiguous source word." ></td>
	<td class="line x" title="147:730	In our model, the plausibility of selecting a target word is determined by the plausibility of the tuples that are obtained from it." ></td>
	<td class="line x" title="148:730	The plausibility of alternative target tuples is in turn determined by their relative frequency in the corpus." ></td>
	<td class="line x" title="149:730	Target syntactic tuples are identified in the corpus similarly to source language tuples, i.e., by a target language parser and a companion routine as described in Section 2.1." ></td>
	<td class="line x" title="150:730	The right column of Table 1 shows the counts obtained for the syntactic tuples of our example in the corpora we used." ></td>
	<td class="line x" title="151:730	The table reveals that the tuples containing the correct target word ('talk,' 'progress,' and 'increase') are indeed more frequent." ></td>
	<td class="line x" title="152:730	569 Computational Linguistics Volume 20, Number 4 However, we still need a decision algorithm to analyze the statistical significance of the data and choose the appropriate word accordingly." ></td>
	<td class="line x" title="153:730	3." ></td>
	<td class="line x" title="154:730	The Statistical Model As seen in the previous section, the linguistic model maps each source language syntactic tuple to several alternative target tuples, in which each alternative corresponds to a different selection of target words." ></td>
	<td class="line x" title="155:730	We wish to select the most plausible target language word for each ambiguous source language word, basing our decision on the counts obtained from the target corpus, as illustrated in Table 1." ></td>
	<td class="line x" title="156:730	To that end, we should define a selection algorithm whose outcome depends on all the syntactic tuples in the sentence." ></td>
	<td class="line x" title="157:730	If the data obtained from the corpus do not substantially support any one of the alternatives, the algorithm should notify the translation system that it cannot reach a statistically meaningful decision." ></td>
	<td class="line x" title="158:730	Our algorithm is based on a statistical model." ></td>
	<td class="line x" title="159:730	However, we wish to point out that we do not see the statistical considerations, as expressed in the model, as fully reflecting the linguistic considerations (syntactic, semantic, or pragmatic) that determine the correct translation." ></td>
	<td class="line x" title="160:730	The model reflects only part of the relevant data and in addition makes statistical assumptions that are only partially satisfied." ></td>
	<td class="line x" title="161:730	Therefore, a statistically based model need not make the correct linguistic choices." ></td>
	<td class="line x" title="162:730	The performance of the model can only be empirically evaluated, the statistical considerations serve only as heuristics." ></td>
	<td class="line x" title="163:730	The role of the statistical considerations is therefore to guide us in constructing heuristics that make use of the linguistic data of the sample (the corpus)." ></td>
	<td class="line x" title="164:730	Our experience shows that the statistical methods are indeed very helpful in establishing and comparing useful decision criteria that reflect various linguistic considerations." ></td>
	<td class="line x" title="165:730	3.1 The Probabilistic Model First we discuss decisions based on a single syntactic tuple (as when only one syntactic tuple in the sentence contains an ambiguous word)." ></td>
	<td class="line x" title="166:730	Denote the source language syntactic tuple T and let there be k alternative target tuples for T, denoted by T1, , Tk." ></td>
	<td class="line x" title="167:730	Let the counts obtained for the target tuples be nl,." ></td>
	<td class="line x" title="168:730	., nk." ></td>
	<td class="line x" title="169:730	For notational convenience, we number the tuples by decreasing frequency, i.e., nl ~ y/2 ~ ''' ~ nkSince our goal is to choose for T one of the target tuples Ti, we can consider T a discrete random variable with multinomial distribution, 3 whose possible values are T1,, Tk." ></td>
	<td class="line x" title="170:730	Let Pi be the probability of obtaining Ti, i.e., the probability that Ti is the correct translation for T. We estimate the probabilities Pi by the counts ni in the obvious way, using the maximum likelihood estimator (Agresti 1990, pp." ></td>
	<td class="line x" title="171:730	40-41)." ></td>
	<td class="line x" title="172:730	The estimator \]9i for Pi is Hi /~i -k ' (1) Y~q=l nj The precision of the estimator depends, of course, on the size of the counts in the computation." ></td>
	<td class="line x" title="173:730	We will incorporate this consideration into the decision algorithm by using confidence intervals." ></td>
	<td class="line x" title="174:730	4 3 A variable that can have one of a finite set of values, each of them having a fixed probability." ></td>
	<td class="line x" title="175:730	4 The maximum likelihood estimator is known to give poor estimates when small counts are involved, and there are several methods to improve it (see Church and Gale 1991, for a presentation and discussion of several methods)." ></td>
	<td class="line x" title="176:730	For our needs this is not necessary in most cases, since we are not going to use the estimate itself, but rather a confidence interval for the ratio between two estimations (see below)." ></td>
	<td class="line x" title="177:730	570 Ido Dagan and Alon Itai Word Sense Disambiguation We now have to establish the criterion for choosing the preferred target language syntactic tuple." ></td>
	<td class="line x" title="178:730	The most reasonable assumption is to choose the tuple with the highest estimated probability, that is Tl--the tuple with the largest observed frequency." ></td>
	<td class="line x" title="179:730	According to the model, the probability that T1 is the right choice is estimated as Pl. This criterion should be subject to the condition that the difference between the alternative probabilities is significant." ></td>
	<td class="line x" title="180:730	For example, if/Yl = 0.51 and/52 = 0.49, the expected success rate in choosing T1 is approximately 0.5." ></td>
	<td class="line x" title="181:730	To prevent the system from making a decision in such cases, we need to impose some conditions on the probabilities Pi." ></td>
	<td class="line x" title="182:730	One possible such condition is that \]Jl exceeds a prespecified threshold (or, as we shall describe below, that the threshold requirement be applied to a confidence interval)." ></td>
	<td class="line x" title="183:730	According to the model, this requirement ensures that the success probability of every decision exceeds the threshold." ></td>
	<td class="line x" title="184:730	Even though this method satisfies the probabilistic model, it is vulnerable to noise in the data, which often causes some relatively small counts to be larger than their true value in the sample." ></td>
	<td class="line x" title="185:730	The noise is introduced in part by inaccuracies in the model and in part because of errors during the automatic collection of the statistical data." ></td>
	<td class="line x" title="186:730	Consequently, the estimated value of Pl may be smaller than its true value, because other counts in Equation 1 are too large, thus, preventing Pl from passing the threshold." ></td>
	<td class="line x" title="187:730	To deal with this problem, we have chosen another criterion for significance--the odds ratio." ></td>
	<td class="line x" title="188:730	We choose the alternative T1 only if all the ratios r;2' exceed a prespecified threshold." ></td>
	<td class="line x" title="189:730	Note that 15i/lfij -ni/nj, and since nl _~ n2 _)  ~_ nk, the ratio tYl/lY2 is less than or equal to all the other ratios." ></td>
	<td class="line x" title="190:730	Therefore, it suffices to check the odds ratio only for ill/P2." ></td>
	<td class="line x" title="191:730	This criterion is less sensitive to noise of the above-mentioned type than/)1, since it depends only on the two largest counts." ></td>
	<td class="line x" title="192:730	3.1.1 Underlying Assumptions." ></td>
	<td class="line x" title="193:730	The use of a probabilistic model necessarily introduces several assumptions on the structure of the corresponding linguistic data." ></td>
	<td class="line x" title="194:730	It is important to point out these assumptions, in order to be aware of possible inconsistencies between the model and the linguistic phenomena for which it is used." ></td>
	<td class="line x" title="195:730	The first assumption is introduced by the use of a multinomial model, which presupposes the following: Assumption 1 The events Ti are mutually disjoint." ></td>
	<td class="line x" title="196:730	This assumption is not entirely valid, since sometimes it is possible to translate a source language word to several target language words, such that all the translations are valid." ></td>
	<td class="line x" title="197:730	For example, consider the Hebrew sentence (from the Ha-Aretz daily newspaper, November 27, 1990) whose English translation is (11) The resignation of Thatcher is not {related I connected} to the negotiations with Damascus." ></td>
	<td class="line x" title="198:730	In this sentence (but not in others), the ambiguous word qshura can equally well be translated to either 'related' or 'connected'." ></td>
	<td class="line x" title="199:730	In terms of the probabilistic model, the two corresponding events, i.e., the two alternative English tuples that contain these words, T1 -(verb-comp: relate to negotiation) and T2 = (verb-comp: connect to negotiation) are 571 Computational Linguistics Volume 20, Number 4 both correct, thus the events T1 and T2 both occur (they are not disjoint)." ></td>
	<td class="line x" title="200:730	However, we have to make this assumption, since the counts we have, ni, from which we estimate the probabilities of the Ti values, count actual occurrences of single syntactic tuples." ></td>
	<td class="line x" title="201:730	In other words, we count the number of times that each of Zl and T2 actually occur, not the number of times in which each of them could occur." ></td>
	<td class="line x" title="202:730	Two additional assumptions are introduced by using counts of the occurrences of syntactic tuples of the target language in order to estimate the translation probabilities of source language tuples: Assumption 2 An occurrence of the source language syntactic tuple T can indeed be translated to one of Zl~~ Tk." ></td>
	<td class="line x" title="203:730	Assumption 3 Every occurrence of the target tuple Ti can be the translation of only the source tuple T. Assumption 2 is an assumption on the completeness of the linguistic model." ></td>
	<td class="line x" title="204:730	It is rather reasonable and depends on the completeness of our bilingual lexicon: if the lexicon gives all possible translations of each ambiguous word, then this assumption will hold, since for each syntactic tuple T we will produce all possible translations3 Assumption 3, which may be viewed as a soundness assumption, does not always hold, since a target language word may be the translation of several source language words." ></td>
	<td class="line x" title="205:730	Consider, for example, the Hebrew tuple T = (verb-obj: heh.ziq lul)." ></td>
	<td class="line x" title="206:730	Lul is ambiguous, meaning either a playpen or a chicken pen." ></td>
	<td class="line x" title="207:730	Accordingly, T can be translated to either T1 = (verb-obj: hold playpen) or T2 = (verb-obj: hold pen)." ></td>
	<td class="line x" title="208:730	In the context of 'hold' the first translation is more likely, and we can therefore expect our model to prefer T1." ></td>
	<td class="line x" title="209:730	However, this might not be the case because Assumption 3 is contradicted." ></td>
	<td class="line x" title="210:730	'Pen' can also be the translation of the Hebrew word 'et (the writing instrument), and thus T2 can be the translation of another Hebrew tuple, T' = (verb~bj: heh.ziq 'et)." ></td>
	<td class="line x" title="211:730	This means that when translating T we are counting occurrences of T2 that correspond to both T and T', 'misleading' the selection criterion." ></td>
	<td class="line x" title="212:730	Section 6.3 illustrates another example in which the assumption is not valid, causing the algorithm to fail to select the correct translation." ></td>
	<td class="line x" title="213:730	We must make this assumption since we use only a target language corpus, which is not related to any source language information." ></td>
	<td class="line x" title="214:730	6 Therefore, when seeing an occurrence of the target language word w, we do not know which source language word is appropriate in the current context." ></td>
	<td class="line x" title="215:730	Consequently, we count its occurrence as a translation of all the source language words for which w is a possible translation." ></td>
	<td class="line x" title="216:730	This implies that sometimes we use inaccurate data, which introduce noise into the statistical model (see Section 6.3 for a discussion of an alternative, but expensive, solution, using a bilingual corpus)." ></td>
	<td class="line x" title="217:730	As we shall see, even though the assumption does not always hold, in most cases this noise does not interfere with the decision algorithm." ></td>
	<td class="line x" title="218:730	5 The problem of constructing a bilingual lexicon that is as complete as possible is beyond the scope of this paper." ></td>
	<td class="line x" title="219:730	A promising approach may be to use aligned bilingual corpora, especially for augmenting existing lexicons with domain-specific terminology (Brown et al. 1993; Dagan, Church, and Gale 1993)." ></td>
	<td class="line x" title="220:730	In any case, it seems that any translation system is limited by the completeness of its bilingual lexicon, which makes our assumption a reasonable one." ></td>
	<td class="line x" title="221:730	6 As explained in the introduction, this is a very important advantage of our method over other methods that use bilingual corpora." ></td>
	<td class="line x" title="222:730	572 Ido Dagan and Alon Itai Word Sense Disambiguation 3.2 Statistical Significance of the Decision Another problem we should address is the statistical significance of the data--what confidence do we have that the data indeed reflect the phenomenon." ></td>
	<td class="line x" title="223:730	If the decision is based on small counts, then the difference in the counts might be due to chance." ></td>
	<td class="line x" title="224:730	For example, we should have more confidence in the odds ratio 151/152 = 3 when nl = 30 and //2 = 10 than when nl = 3 and n2 = 1." ></td>
	<td class="line x" title="225:730	Consequently, we shall use a dynamic threshold for 151/152, which is large when the counts are small and decreases as the counts increase." ></td>
	<td class="line x" title="226:730	A common method for determining the statistical significance of estimates is the use of confidence intervals." ></td>
	<td class="line x" title="227:730	Rather than finding a confidence interval for 151/152, we will bound the log odds ratio, ln(151/152)." ></td>
	<td class="line x" title="228:730	Since the variance Of the log odds ratio is independent of the mean, it converges to the normal distribution faster than the odds ratio itself (Agresti 1990)." ></td>
	<td class="line x" title="229:730	We use a one-tailed interval, as we want only to decide whether ln(151/152) is greater than a specific threshold (i.e. , we need only a lower bound for ln(151/152))." ></td>
	<td class="line x" title="230:730	Using this method, for each desired error probability 0 < ~ < 1, we may determine a value B~ and state that with a probability of at least 1 c~ the true value, ln(pl/p2), is greater than B~." ></td>
	<td class="line x" title="231:730	The confidence interval of a random variable X with normal distribution is ZI-~, where ZI-~ is the confidence coefficient, which may be found in statistical tables, and var is the variance." ></td>
	<td class="line x" title="232:730	In our case, the size of the confidence interval is Z1-~/var\[ln~221' In the appendix we approximate the variance by the following \[ ~22\] 1 1 var in 151 ~ __ _}_ --." ></td>
	<td class="line x" title="233:730	//1 //2 The bound we get is thus 151 //1 Since ~ //2 we get ln(P~2 ) >ln(pP-~)-Zl-~V~n~ + 1 //2 ln/P~) ~>ln/n,~-~)-Zl-c,V/n~+ 1 //2 ;o (2) B~(nl,n2) (or B~ when nl and n2 are understood from the context) is defined to be the right-hand side of Equation 2." ></td>
	<td class="line x" title="234:730	The meaning of the inequality is that for every given pair nl~ n2 we know with confidence 1 c~ that In pl ~ B~ (3) P2 or in other words, B,~ is a lower bound for ln(pl/P2) with this confidence level." ></td>
	<td class="line x" title="235:730	To obtain a decision criterion, we choose a threshold 0, for B~, and decide to choose T1 only if B~ > 0." ></td>
	<td class="line x" title="236:730	(4) 573 Computational Linguistics Volume 20, Number 4 If Equation 4 does not hold, the algorithm makes no decision." ></td>
	<td class="line x" title="237:730	The meaning of this criterion is that only if we know with confidence of at least 1 ~ that ln(pl/p2) > O, will we select the most frequent tuple T1 as the appropriate one." ></td>
	<td class="line x" title="238:730	In terms of statistical decision theory, we say that our null hypothesis is that ln(pl/P2) < 0, and we will make a decision only if we can reject this hypothesis with confidence at least 1 ~." ></td>
	<td class="line x" title="239:730	Note that we cannot compute B~ when one of the counts is zero." ></td>
	<td class="line x" title="240:730	In this case we have used the common correction method of adding 0.5 to each of the counts (Agresti 1990, p. 249)." ></td>
	<td class="line x" title="241:730	7 We shall now demonstrate the use of the decision criterion." ></td>
	<td class="line x" title="242:730	In the experiment we conducted we chose the parameters ~ = 0.1, for which Z~ = 1.282, and 0 = 0.2." ></td>
	<td class="line x" title="243:730	Thus, to choose T1 we require that with confidence level of at least 90% the hypothesis should satisfy ln(pl/P2) > 0.2 (i.e. , Pl/P2 >_ e 02 = 1.22)." ></td>
	<td class="line x" title="244:730	For the alternative translations of tuple c in Table 1 we got nl = 29 and n2 = 5." ></td>
	<td class="line x" title="245:730	For these values Be = 1.137." ></td>
	<td class="line x" title="246:730	In this case Equation 4 is satisfied for 0 = 0.2, and the algorithm selects the word 'progress' as the translation of the Hebrew word hitqaddmut." ></td>
	<td class="line x" title="247:730	In another case we had to translate the Hebrew word ro'sh, which can be translated to either 'top' or 'head,' in the sentence whose translation is (12) Sihanuk stood at the {top \] head} of a coalition of underground groups." ></td>
	<td class="line x" title="248:730	The two alternative syntactic tuples were (a) (verb-pp: standat head) 10 (b) (verb-pp: stand at top) 5 For nl = 10 and n2 = 5, we get Be = -0.009 (a negative value means that it is impossible to ensure with a 90% confidence level that Pl > P2)." ></td>
	<td class="line x" title="249:730	Since Be G 0.2, the algorithm will refrain from making a decision in this case." ></td>
	<td class="line x" title="250:730	This abstention reflects the fact that the difference between the counts is not statistically significant, and choosing the first alternative can be wrong in many of the cases (as seen in the five cases that were observed in the corpus)." ></td>
	<td class="line x" title="251:730	As mentioned above, our motivation was to find a criterion that depends on a dynamic threshold for ~1/\]Y2 (or alternatively nl/n2), so that the threshold will be higher when nl and n2 are smaller." ></td>
	<td class="line x" title="252:730	Our criterion indeed satisfies this requirement." ></td>
	<td class="line x" title="253:730	If we substitute B~ in Equation 4, we get the following equivalent criterion: In nl > 0 + Zl_c~/n~ q1 //2 //2 The above inequality clarifies the roles of the two parameters, ~ and 0:0 specifies a lower bound on In(nl/n2), which is independent of the sample size; c~ reflects the statistical significance." ></td>
	<td class="line x" title="254:730	If c~ is decreased (i.e. , we require more confidence), ZI_~ will increase, and therefore, the component dependent on the sample size will increase." ></td>
	<td class="line x" title="255:730	Since this component is in inverse relation to nl and n2, the penalty for decreasing c~ increases when the sample size decreases." ></td>
	<td class="line x" title="256:730	From this analysis we can derive the criterion for choosing the parameters: if we wish to use small counts, then c~ should be small, and 0 depends on the required ratio between nl and n2." ></td>
	<td class="line x" title="257:730	The optimal values of the parameters should be determined empirically and might depend on the corpora and parsers we use." ></td>
	<td class="line x" title="258:730	7 In this case, smoothing methods (Church and Gale 1991) may improve the correction method." ></td>
	<td class="line x" title="259:730	574 Ido Dagan and Alon Itai Word Sense Disambiguation 3.3 Sentences with Several Syntactic Relations In the previous section, we assumed that the source sentence contains only one ambiguous syntactic tuple." ></td>
	<td class="line x" title="260:730	In general there may be several ambiguous words that appear in several tuples." ></td>
	<td class="line x" title="261:730	We should take advantage of the occurrence patterns of all of the tuples to reach a decision." ></td>
	<td class="line x" title="262:730	Since different relations may favor different translations for an ambiguous word, we should devise a strategy for selecting a consistent translation for all words in the sentence." ></td>
	<td class="line x" title="263:730	We have used the following constraint propagation algorithm, which receives as input the list of all source tuples along with their alternative translations to target tuples: . . ." ></td>
	<td class="line x" title="264:730	Compute B~ of each source tuple." ></td>
	<td class="line x" title="265:730	If the largest B~ is less than the threshold, 8, then stop." ></td>
	<td class="line x" title="266:730	Let T be the source tuple for which B~ is maximal." ></td>
	<td class="line x" title="267:730	Select the translation for the ambiguous words (or word) in T according to T1 (the most frequent target alternative for T)." ></td>
	<td class="line x" title="268:730	Remove T from the list of source tuples." ></td>
	<td class="line x" title="269:730	Propagate the constraint: eliminate target tuples that are inconsistent with this decision." ></td>
	<td class="line x" title="270:730	If now some source tuples become unambiguous, remove them from the list of source tuples." ></td>
	<td class="line x" title="271:730	Repeat this procedure for the remaining list of source tuples, until all ambiguities have been resolved, or the maximal B~ is less than 8." ></td>
	<td class="line x" title="272:730	To illustrate the algorithm, we consider Table 1 using the parameters c~ = 0.1 and 0 = 0.2." ></td>
	<td class="line x" title="273:730	The largest value of B~ occurs for the tuple (verb-obj: higdil sikkuy), for which higdil can be translated to ;increase,' 'magnify,' or 'enlarge'." ></td>
	<td class="line x" title="274:730	The first alternative appeared nl = 20 times, and the other alternatives did not appear at all, (n2 = n3 = 0)." ></td>
	<td class="line x" title="275:730	Adding the correction factor and computing B~ yields B~(nl + 0.5~n2 q-0.5) = B,~(20.5, 0.5) = 1.879 > 0.2 = 8." ></td>
	<td class="line x" title="276:730	Therefore, the word 'increase' was chosen as the translation of higdil." ></td>
	<td class="line x" title="277:730	Since this word appears also in the tuple (subj-verb: hitztarrfut higdil), the' target tuples that include alternative translations of higdil were deleted." ></td>
	<td class="line x" title="278:730	Thus (13) (subj-verb: joining enlarge) (subj-verb: joining magnify) were deleted." ></td>
	<td class="line x" title="279:730	This leaves us with only one alternative (subj-verb: joining increase) as a possible translation of this Hebrew tuple, which is therefore removed from the input list." ></td>
	<td class="line x" title="280:730	We now recompute the values of B~ for the remaining tuples." ></td>
	<td class="line x" title="281:730	The maximal value is obtained for the tuple (14) (verb-obj: hissig hitqaddmut) where B~ (29, 5) = 1.137 > 8." ></td>
	<td class="line x" title="282:730	We, therefore, choose the word 'progress' as a translation for hitqaddmut." ></td>
	<td class="line x" title="283:730	Since this word, hitqaddmut, also appears in the tuple (noun-pp: hitqaddmut bsih.a), we delete the Six target tuples that are inconsistent with the selection of 'progress' (those containing the words 'advance' and 'advancement')." ></td>
	<td class="line x" title="284:730	There now remain only three alternative target tuples for hitqaddmut bsih.a. We now recompute the values of B~." ></td>
	<td class="line x" title="285:730	The maximum value is B~ (7.5~ 0.5) = 0.836 > 0 (note that because tuples inconsistent with the previous decisions were eliminated, 575 Computational Linguistics Volume 20, Number 4 n2 dropped from 2 to 0, thus increasing B~)." ></td>
	<td class="line x" title="286:730	Thus, 'talk' is selected as the translation of siha." ></td>
	<td class="line x" title="287:730	Now all the ambiguities have been resolved and the procedure stops." ></td>
	<td class="line x" title="288:730	In the above example all the ambiguities were resolved since in each stage the value of B~ exceeded the threshold 0 = 0.2." ></td>
	<td class="line x" title="289:730	In some cases not all ambiguities are resolved, though the number of ambiguities may decrease." ></td>
	<td class="line x" title="290:730	It should be noted that other methods may be proposed for combining the statistics of several syntactic relations." ></td>
	<td class="line x" title="291:730	For example, it may make sense to multiply estimates of conditional probabilities of tuples in different relations, in a way that is analogous to n-gram language modeling (Jelinek, Mercer, and Roukos 1992)." ></td>
	<td class="line x" title="292:730	However, such an approach will make it harder to take into account the statistical significance of the estimate (a criterion that is missing in standard n-gram models)." ></td>
	<td class="line x" title="293:730	In our set of examples, the constraint propagation method proved to be successful and did not seem to introduce any errors." ></td>
	<td class="line x" title="294:730	Further experimentation, on much larger data sets, is needed to determine which of the two methods (if any) is substantially superior to the other." ></td>
	<td class="line x" title="295:730	4." ></td>
	<td class="line x" title="296:730	The Experiment To evaluate the proposed disambiguation method, we implemented and tested the method on a random set of examples." ></td>
	<td class="line x" title="297:730	The examples consisted of a set of Hebrew paragraphs and a set of German paragraphs." ></td>
	<td class="line x" title="298:730	In both cases the target language was English." ></td>
	<td class="line x" title="299:730	The Hebrew examples consisted of ten paragraphs picked at random from foreign news sections of the Israeli press." ></td>
	<td class="line x" title="300:730	The paragraphs were selected from several news items and articles that appeared in several daily newspapers." ></td>
	<td class="line x" title="301:730	The target language corpus consisted of American newspaper articles, and the Hansard corpus of the proceedings of the Canadian Parliament." ></td>
	<td class="line x" title="302:730	The domain of foreign news articles was chosen to correspond to some of the topics that appear in the English corpus, s The German examples were chosen at random from the German press, without restricting the topic." ></td>
	<td class="line x" title="303:730	9 Since we did not have a translation system from Hebrew or German to English, we simulated the steps such a system would perform." ></td>
	<td class="line x" title="304:730	Hence, the results we report measure the performance of just the target word selection module and not the performance of a complete translation system." ></td>
	<td class="line x" title="305:730	The latter can be expected to be somewhat lower for a real system, depending on the performance of its other components." ></td>
	<td class="line x" title="306:730	Note, however, that since the disambiguation module is highly immune to noise, it might be more useful in a real system: in such a system some of the alternatives would be totally erroneous." ></td>
	<td class="line x" title="307:730	Since the corresponding syntactic tuples would typically not be found in the corpora, they would be eliminated by our module." ></td>
	<td class="line x" title="308:730	The experiment is described in detail in the following subsections." ></td>
	<td class="line x" title="309:730	It provides an example for a thorough evaluation that is carried out without having a complete system available." ></td>
	<td class="line x" title="310:730	We specifically describe the processing of the Hebrew data, which was performed by a professional translator, supervised by the authors." ></td>
	<td class="line x" title="311:730	The German examples were processed very similarly." ></td>
	<td class="line x" title="312:730	4.1 Locating Ambiguous Words To locate ambiguous words, we simulated a bilingual lexicon and syntactic filters of a translation system." ></td>
	<td class="line x" title="313:730	For every source language word, the translator searched all possible 8 The corpus includes many irrelevant topics as well, which introduce noisy data with respect to the given domain." ></td>
	<td class="line x" title="314:730	9 The German examples were prepared by Ulrike Schwall from the IBM Scientific Center, Heidelberg, Germany." ></td>
	<td class="line x" title="315:730	576 Ido Dagan and Alon Itai Word Sense Disambiguation translations using a Hebrew-English dictionary (Alcalay 1990)." ></td>
	<td class="line x" title="316:730	The list of translations proposed by the dictionary was modified according to the following guidelines, to reflect better the lexicon of a practical translation system: . . 3." ></td>
	<td class="line x" title="317:730	. . Eliminate translations that would be ruled out for syntactic reasons, as explained in Section 2.1." ></td>
	<td class="line x" title="318:730	Consider only content words, ignoring function words and proper nouns." ></td>
	<td class="line x" title="319:730	Assume that multi-word terms, such as 'prime minister,' appear in the lexicon as complete terms." ></td>
	<td class="line x" title="320:730	Thus we did not consider each of their constituents separately." ></td>
	<td class="line x" title="321:730	Also, we did not consider source language words that should be translated to a multi-word target phrase." ></td>
	<td class="line x" title="322:730	Eliminate rare and archaic translations that are not expected in the context of foreign affairs in the current press." ></td>
	<td class="line x" title="323:730	The professional translator added translations that were missing in the dictionary." ></td>
	<td class="line x" title="324:730	In addition, each of the remaining target alternatives for each source word was evaluated as to whether it is a suitable translation in the current context." ></td>
	<td class="line x" title="325:730	This evaluation was later used to judge the selections of the algorithm." ></td>
	<td class="line x" title="326:730	If all the alternatives were considered suitable, then the source word was eliminated from the test set, since any decision for it would have been considered successful." ></td>
	<td class="line x" title="327:730	We ended up with 103 Hebrew and 54 German ambiguous words." ></td>
	<td class="line x" title="328:730	For each Hebrew word we had an average of 3.27 alternative translations and an average of 1.44 correct translations." ></td>
	<td class="line x" title="329:730	The average number of translations of a German word was 3.26, and there were 1.33 correct translations." ></td>
	<td class="line x" title="330:730	4.2 Determining the Syntactic Tuples and Mapping Them to English Since we did not have a Hebrew parser, we have simulated the two steps of determining the source syntactic tuples and mapping them to English by reversing the order of these steps, in the following way: First, the sample sentences were translated manually, as literally as possible, into English." ></td>
	<td class="line x" title="331:730	Then, the resulting English sentences were analyzed, using the ESG parser and the postprocessing routine (see Section 2.2), to identify the relevant syntactic tuples." ></td>
	<td class="line x" title="332:730	The tuples were further classified into 'general classes,' as described in Section 2.3." ></td>
	<td class="line x" title="333:730	The use of these general classes, which was intended to facilitate the mapping of syntactic relations from one language to another, also facilitated our simulation method and caused it to produce realistic output." ></td>
	<td class="line x" title="334:730	At the end of the procedure, we had, for each sample sentence, a data structure similar to Table 1 (without the counts)." ></td>
	<td class="line x" title="335:730	4.3 Acquiring the Statistical Data The statistical data were acquired from the following corpora:  Texts from The Washington Post ~0 million words." ></td>
	<td class="line x" title="336:730	 The Hansard corpus of protocols of the Canadian Parliament--85 million words." ></td>
	<td class="line x" title="337:730	 Associated Press news items--24 million words." ></td>
	<td class="line x" title="338:730	577 Computational Linguistics Volume 20, Number 4 However, the effective size of the corpora was only about 25 million words, owing to two filtering criteria." ></td>
	<td class="line x" title="339:730	First, we considered only sentences whose length did not exceed 25 words, since longer sentences required excessive parse time and contained many parsing errors." ></td>
	<td class="line x" title="340:730	Second, even 35% of the shorter sentences failed to parse and had to be eliminated." ></td>
	<td class="line x" title="341:730	The syntactic tuples were located by the ESG parser and the postprocessing routine mentioned earlier." ></td>
	<td class="line x" title="342:730	For the purpose of evaluation, we gathered only the data required for the given test examples." ></td>
	<td class="line x" title="343:730	Within a practical machine translation system, the disambiguation module would require a database containing all the syntactic tuples of the corpus with their frequency counts." ></td>
	<td class="line x" title="344:730	In the current research project we did not have the computing resources necessary for constructing the complete database (the major cost being parsing time)." ></td>
	<td class="line x" title="345:730	However, such resources are not needed in order to evaluate the proposed method." ></td>
	<td class="line x" title="346:730	Since we evaluated the method only on a relatively small number of random sentences, we first constructed the set of all 'relevant' target tuples, i.e., tuples that should be considered for the test sentences." ></td>
	<td class="line x" title="347:730	Then we scanned the entire corpus and extracted only sentences that contain both words from at least one of the relevant tuples." ></td>
	<td class="line x" title="348:730	Only the extracted sentences were parsed, and their counts were recorded in our database." ></td>
	<td class="line x" title="349:730	Even though this database is much smaller than the full database, for the ambiguous words of the test sentences, both databases provide the same information." ></td>
	<td class="line x" title="350:730	Thus, the success rate for the test sentences is the same for both methods, while requiring a considerably smaller amount of resources at the research phase." ></td>
	<td class="line x" title="351:730	The problem with this method is that for every set of sample sentences the entire corpus has to be scanned." ></td>
	<td class="line x" title="352:730	Thus, a practical system would have to preprocess the corpus to construct a database of the entire corpus." ></td>
	<td class="line x" title="353:730	Then, to resolve ambiguities, only this database need be consulted." ></td>
	<td class="line x" title="354:730	After acquiring all the relevant data, the algorithm of Section 3.3 was executed for each of the test sentences." ></td>
	<td class="line x" title="355:730	5." ></td>
	<td class="line x" title="356:730	Evaluation Two measurements, applicability and precision, are used to evaluate the performance of the algorithm." ></td>
	<td class="line x" title="357:730	The applicability (coverage) denotes the proportion of cases for which the model performed a selection, i.e., those cases for which the bound B~ passed the threshold." ></td>
	<td class="line x" title="358:730	The precision denotes the proportion of cases for which the model performed a correct selection out of all the applicable cases." ></td>
	<td class="line x" title="359:730	We compare the precision of our method, which we term TWS (for Target Word Selection), with that of the Word Frequencies procedure, which always selects the most frequent target word." ></td>
	<td class="line x" title="360:730	In other words, the Word Frequencies method prefers the alternative that has the highest a priori probability of appearing in the target language corpus." ></td>
	<td class="line x" title="361:730	This naive 'straw-man' is less sophisticated than other methods suggested in the literature, but it is useful as a common benchmark since it can be easily implemented." ></td>
	<td class="line x" title="362:730	The success rate of the Word Frequencies procedure can serve as a measure for the degree of lexical ambiguity in a given set of examples, and thus different methods can be partly compared by their degree of success relative to this procedure." ></td>
	<td class="line x" title="363:730	Out of the 103 ambiguous Hebrew words, for 33 the bound B~ did not pass the threshold, achieving an applicability of 68%." ></td>
	<td class="line x" title="364:730	The remaining 70 examples were distributed according to Table 2." ></td>
	<td class="line x" title="365:730	Thus the precision of the statistical model was 91% 578 Ido Dagan and Alon Itai Word Sense Disambiguation Table 2 Hebrew-English translation: Comparison of TWS and Word Frequencies methods for the 70 applicable examples Word Frequencies Correct Incorrect Total Correct 42 22 64 TWS Incorrect 2 4 6 Total 44 26 70 (64/70), 1 whereas relying just on Word Frequencies yields 63% (44/70), providing an improvement of 28%." ></td>
	<td class="line x" title="366:730	The table demonstrates that our algorithm corrects 22 erroneous decisions of the Word Frequencies method, but makes only 2 errors that the Word Frequencies method translates correctly." ></td>
	<td class="line x" title="367:730	This implies that with high confidence our method greatly improves the Word Frequencies method." ></td>
	<td class="line x" title="368:730	The number of Hebrew examples is large enough to permit a meaningful analysis of the statistical significance of the results." ></td>
	<td class="line x" title="369:730	By computing confidence intervals for the distribution of proportions, we claim that with 95% confidence our method succeeds in at least 86% of the applicable examples." ></td>
	<td class="line x" title="370:730	This means that though the figure of 91% might be due to a lucky selection of the random examples, there is only a 5% chance that the real figure is less than 86% (for the given domain and corpus)." ></td>
	<td class="line x" title="371:730	The confidence interval was computed as follows: p~f~_Zl_c~f~) 64 f-~4 . 6 -70 1'65V 7-7-07 0'86' where a = 0.05 and the variance is estimated by \]~(1 f))/n. With the same confidence, our method improves the Word Frequencies method by at least 18% (relative to the actual improvement of 28% in the given test set)." ></td>
	<td class="line x" title="372:730	Let Pl be the proportion of cases for which our method succeeds and the Word Frequencies method fails (Pl = 22/70) and P2 be the proportion of cases for which the Word Frequencies method succeeds and ours fails (P2 = 2/70)." ></td>
	<td class="line x" title="373:730	The confidence interval is for the difference of proportions in multinomial distribution and is computed as follows: Pl -P2 (-lYl -P2 -Zl-o~ v/var(151 152) = ~1 -/~2 Zl_c~ --~ V/~t (1 ~t) + ~2(1 ~2) + 2~1~2 22 2 65 1 4/22." ></td>
	<td class="line x" title="374:730	(70-22)+2.(70-2)+2.22.2 =0.18." ></td>
	<td class="line x" title="375:730	70 70 1." ></td>
	<td class="line x" title="376:730	~V 702 Out of the 54 ambiguous German words, for 27 the bound B~ did not pass the threshold (applicability of 50%)." ></td>
	<td class="line x" title="377:730	The remaining 27 examples were distributed according to Table 3." ></td>
	<td class="line x" title="378:730	Thus, the precision of the statistical model was 78% (21/27), whereas 10 An a posteriori observation showed that in three of the six errors the selection of the model was actually acceptable, and the a priori judgment of the human translator was too restrictive." ></td>
	<td class="line x" title="379:730	For example, in one of these cases the statistics selected the expression 'to begin the talks,' whereas the human translator regarded this expression as incorrect and selected 'to start the talks'." ></td>
	<td class="line x" title="380:730	If we consider these cases as correct, then there are only three selection errors, getting 96% precision." ></td>
	<td class="line x" title="381:730	579 Computational Linguistics Volume 20, Number 4 Table 3 German-English translation: Comparison of TWS and Word Frequencies methods for the 27 applicable examples Word Frequencies Correct Incorrect Total Correct 15 6 21 TWS Incorrect 0 6 6 Total 15 12 27 relying just on Word Frequencies yields 56% (15/27)." ></td>
	<td class="line x" title="382:730	Here our method corrected 6 errors of the Word Frequencies method, without causing any new errors." ></td>
	<td class="line x" title="383:730	We attribute the lower success rate for the German examples to the fact that they were not restricted to topics that are well represented in the corpus." ></td>
	<td class="line x" title="384:730	This poor correspondence between the training and testing texts is reflected also by the low precision of the Word Frequencies method." ></td>
	<td class="line x" title="385:730	This means that the a priori probability of the target words, as estimated from the training corpora, provides a very poor prediction of the correct selection in the test examples." ></td>
	<td class="line x" title="386:730	Relative to the a priori probability, the precision of our method is still 22% higher." ></td>
	<td class="line x" title="387:730	5.1 Additional Results Recently, Dagan, Marcus, and Markovitch have implemented a variant of the disambiguation method of the current paper." ></td>
	<td class="line x" title="388:730	This variant was developed for evaluating a method that estimates the probability of word combinations which do not occur in the training corpus (Dagan, Marcus, and Markovitch 1993)." ></td>
	<td class="line x" title="389:730	In this section we quote their results, providing additional evidence for the effectiveness of the TWS method." ></td>
	<td class="line x" title="390:730	The major difference between the TWS method, as presented in this paper, and the variant described by Dagan, Marcus, and Markovitch (1993), which we term TWS ~, is that the latter does not use any parsing for collecting the statistics from the corpus." ></td>
	<td class="line x" title="391:730	Instead, the counts of syntactic tuples are approximated by counting co-occurrences of the given words of the tuple within a short distance in a sentence." ></td>
	<td class="line x" title="392:730	The approximation takes into account the relative order between the words of the tuple, such that occurrences of a certain syntactic relation are approximated only by word co-occurrences that preserve the most frequent word order for that relation (e.g. , an adjective precedes the noun it modifies)." ></td>
	<td class="line x" title="393:730	The TWS ~ method still assumes that the source sentence to be translated is being parsed, in order to identify the words that are syntactically related to an ambiguous word." ></td>
	<td class="line x" title="394:730	This model is therefore relevant for translation systems that use a parser for the source language, but may not have available a robust target language parser." ></td>
	<td class="line x" title="395:730	The corpus used for evaluating the TWS' method consists of articles posted to the USENET news system." ></td>
	<td class="line x" title="396:730	The articles were collected from news groups that discuss computer-related topics." ></td>
	<td class="line x" title="397:730	The length of the corpus is 8,871,125 words (tokens), and the lexicon size (distinct types, at the string level) is 95,559." ></td>
	<td class="line x" title="398:730	The type of text in this corpus is quite noisy, including short and incomplete sentences as well as much irrelevant information, such as person and device names." ></td>
	<td class="line x" title="399:730	The test set used for the experiment consists of 78 Hebrew sentences that were taken out of a book about computers." ></td>
	<td class="line x" title="400:730	These sentences were processed as described in Section 4, obtaining a set of 269 ambiguous Hebrew words." ></td>
	<td class="line x" title="401:730	The average number of alternative translations per ambiguous word in this set is 5.8, and there are 1.35 correct translations." ></td>
	<td class="line x" title="402:730	580 Ido Dagan and Alon Itai Word Sense Disambiguation Table 4 Comparison of TWS' and Word Frequencies methods for the 173 applicable examples Word Frequencies correct incorrect Total correct 120 28 148 TWS' incorrect 3 22 25 Total 123 50 173 Out of the 269 ambiguous Hebrew words, for 96 the bound B~ did not pass the threshold, achieving an applicability of 64.3%." ></td>
	<td class="line x" title="403:730	The remaining 173 examples were distributed according to Table 4." ></td>
	<td class="line x" title="404:730	For the words that are covered by the TWS' method, the Word Frequencies method has a precision of 71.1% (123/173), whereas the TWS' method has a precision of 85.5%(148/173)." ></td>
	<td class="line x" title="405:730	As can be seen in the table, the TWS' method is correct in almost all the cases it disagrees with the Word Frequencies method (28 out of 31)." ></td>
	<td class="line x" title="406:730	The applicability and precision figures in this experiment are somewhat lower than those achieved for the Hebrew set in our original evaluation of the TWS method (Table 2)." ></td>
	<td class="line x" title="407:730	We attribute this to the fact that the original results were achieved using a parsed corpus, which was about 2.5 times larger and of much higher quality than the one used in the second experiment." ></td>
	<td class="line x" title="408:730	Yet, the new results give additional support for the usefulness of the TWS method, even for noisy data provided by a low quality corpus, without any parsing or tagging, u 6." ></td>
	<td class="line x" title="409:730	Analysis and Possible Enhancements In this section we give a detailed analysis of the selections performed by the algorithm and, in particular, analyze the cases when it failed." ></td>
	<td class="line x" title="410:730	The analysis of these modes suggests possible improvements of the model and indicates its limitations." ></td>
	<td class="line x" title="411:730	As described earlier, the algorithm's failure includes either the cases for which the method was not applicable (no selection), or the cases for which it made an incorrect selection." ></td>
	<td class="line x" title="412:730	The following paragraphs list various reasons for both types." ></td>
	<td class="line x" title="413:730	At the end of the section, we discuss the possibility of adapting our approach to monolingual applications." ></td>
	<td class="line x" title="414:730	6.1 Correct Selection In the cases that were treated correctly by our method, such as the examples given in the previous sections, the statistics succeeded in capturing two major types of disambiguating data." ></td>
	<td class="line x" title="415:730	In preferring 'sign-treaty' upon 'seal-treaty' (in Example 1), the statistics reflect the relevant semantic constraint." ></td>
	<td class="line x" title="416:730	In preferring 'peace-treaty' upon 'peacecontract,' the statistics reflect the lexical usage of 'treaty' in English which differs from the usage of 'contract'." ></td>
	<td class="line x" title="417:730	6.2 Inapplicability 6.2.1 Insufficient Data." ></td>
	<td class="line x" title="418:730	This was the reason for nearly all the cases of inapplicability." ></td>
	<td class="line x" title="419:730	In one of our examples, for instance, none of the alternative relations, 'an investigator of corruption' (the correct one) or 'researcher of corruption' (the incorrect one), 11 It should be mentioned that the work of Dagan, Marcus, and Markovitch (1993) includes further results, evaluating an enhancement of the TWS method using their similarity-based estimation method." ></td>
	<td class="line x" title="420:730	This enhancement is beyond the scope of the current paper and is referred to in the next section." ></td>
	<td class="line x" title="421:730	581 Computational Linguistics Volume 20, Number 4 was observed in the parsed corpus." ></td>
	<td class="line x" title="422:730	In this case it is possible to perform the correct selection if we used only statistics about the co-occurrence of 'corruption' with either 'investigator' or 'researcher' in the same local context, without requiring any syntactic relation." ></td>
	<td class="line oc" title="423:730	Statistics on co-occurrence of words in a local context were used recently for monolingual word sense disambiguation (Gale, Church, and Yarowsky 1992b, 1993; Sch6tze 1992, 1993) (see Section 7 for more details and Church and Hanks 1990; Smadja 1993, for other applications of these statistics)." ></td>
	<td class="line o" title="424:730	It is possible to apply these methods using statistics of the target language and thus incorporate them within the framework proposed here for target word selection." ></td>
	<td class="line x" title="425:730	Finding an optimal way of combining the different methods is a subject for further research." ></td>
	<td class="line x" title="426:730	Our intuition, though, as well as some of our initial data, suggests that statistics on word co-occurrence in the local context can substantially increase the applicability of the selection method." ></td>
	<td class="line x" title="427:730	Another way to deal with the lack of statistical data for the specific words in question is to use statistics about similar words." ></td>
	<td class="line x" title="428:730	This is the basis for Sadler's Analogical Semantics (Sadler 1989), which according to his report has not proved effective." ></td>
	<td class="line x" title="429:730	His results may be improved if more sophisticated methods and larger corpora are used to establish similarity between words (such as in Hindle 1990)." ></td>
	<td class="line x" title="430:730	In particular, an enhancement of our disambiguation method, using similarity-based estimation (Dagan, Marcus, and Markovitch 1993), was evaluated recently." ></td>
	<td class="line x" title="431:730	In this evaluation the applicability of the disambiguation method was increased by 15%, with only a slight decrease in the precision." ></td>
	<td class="line x" title="432:730	The increased applicability was achieved by disambiguating additional cases in which statistical data were not available for any of the alternative tuples, whereas data were available for other tuples containing similar words." ></td>
	<td class="line x" title="433:730	6.2.2 Conflicting Data." ></td>
	<td class="line x" title="434:730	In very few cases two alternatives were supported equally by the statistical data, thus preventing a selection." ></td>
	<td class="line x" title="435:730	In such cases, both alternatives are valid at the independent level of the syntactic relation, but may be inappropriate for the specific context." ></td>
	<td class="line x" title="436:730	For instance, the two alternatives of 'to take a job' or 'to take a position' appeared in one of the examples, but since the general context was about the position of a prime minister, only the latter was appropriate." ></td>
	<td class="line x" title="437:730	To resolve such ambiguities, it may be useful to consider also co-occurrences of the ambiguous word with other words in the broader context (e.g. , Gale, Church, and Yarowsky 1993; Yarkowsky 1992)." ></td>
	<td class="line x" title="438:730	For instance, the word 'minister' seems to co-occur in the same context more frequently with 'position' than with 'job'." ></td>
	<td class="line x" title="439:730	In another example both alternatives were appropriate also for the specific context." ></td>
	<td class="line x" title="440:730	This happened with the German verb werfen, which may be translated (among other options) as 'throw,' 'cast,' or 'score'." ></td>
	<td class="line x" title="441:730	In our example, werfen, appeared in the context of 'to throw/cast light,' and these two correct alternatives had equal frequencies in the corpus ('score' was successfully eliminated): In such situations any selection between the alternatives will be appropriate, and therefore, any algorithm that handles conflicting data would work properly." ></td>
	<td class="line x" title="442:730	However, it is difficult to decide automatically when both alternatives are acceptable and when only one of them is. 6.3 Incorrect Selection 6.3.1 Using an Inappropriate Relation." ></td>
	<td class="line x" title="443:730	One of the examples contained the Hebrew word matzav." ></td>
	<td class="line x" title="444:730	This word has several translations, two of which are 'state' and 'position'." ></td>
	<td class="line x" title="445:730	The phrase that contained this word was 'to put an end to the {statelposition } of war'." ></td>
	<td class="line x" title="446:730	The ambiguous word is involved in two syntactic relations, being a complement of 'put' and also modified by 'war'." ></td>
	<td class="line x" title="447:730	The corresponding frequencies were 582 Ido Dagan and Alon Itai Word Sense Disambiguation (15) (verb-comp: put-position) 320 (verb-comp: put-state) 18 (noun-nobj: state-war) 13 (noun-nobj: position-war) 2 The bound of the odds ratio (B~) for the first relation was higher than for the second, and therefore, this relation determined the translation as 'position'." ></td>
	<td class="line x" title="448:730	However, the correct translation should be 'state', as determined by the second relation." ></td>
	<td class="line x" title="449:730	These data suggest that while ordering the relations (or using any other Weighting mechanism) it may be necessary to give different weights to the different types of syntactic relations." ></td>
	<td class="line x" title="450:730	For instance, it seems reasonable that the object of a noun should receive greater weight in selecting the noun's sense than the verb for which this noun serves as a complement." ></td>
	<td class="line x" title="451:730	Further examination of the example suggests another refinement of our method: it turns out that most of the 320 instances of the tuple (verb-comp: put position) include the preposition 'in,' as part of the common phrase 'put in a position'." ></td>
	<td class="line x" title="452:730	Therefore, these instances should not be considered for the current example, which includes the preposition 'to'." ></td>
	<td class="line x" title="453:730	However, the distinction between different prepositions was lost by our program, as a result of using equivalence classes of syntactic tuples (see Section 2.3)." ></td>
	<td class="line x" title="454:730	This suggests that we should not use an equivalence class when there is enough statistical data for specific tuples." ></td>
	<td class="line x" title="455:730	12 6.3.2 Confusing Senses." ></td>
	<td class="line x" title="456:730	In another example, the Hebrew adjective qatann modified the noun sikkuy, which means 'prospect' or 'chance'." ></td>
	<td class="line x" title="457:730	The word qatann has several translations, two of which are 'small' and 'young'." ></td>
	<td class="line x" title="458:730	In this Hebrew word combination, the correct sense of qatann is necessarily 'small'." ></td>
	<td class="line x" title="459:730	However, the relation that was observed in the corpus was 'young prospect,' relating to the human sense of 'prospect' that appeared in sports articles (a promising young person)." ></td>
	<td class="line x" title="460:730	This borrowed sense of 'prospect' is necessarily inappropriate, since in Hebrew it is represented by the equivalent of 'hope' (tiqwa) and not by sikkuy." ></td>
	<td class="line x" title="461:730	The source of this problem is Assumption 3: a target tuple T might be a translation of several source tuples, and while gathering statistics for T, we cannot distinguish between the different sources, since we use only a target language corpus." ></td>
	<td class="line x" title="462:730	A possible solution is to use an aligned bilingual corpus, as suggested by Sadler (1989), Brown et al.(1991), and Gale et al.(1992a)." ></td>
	<td class="line x" title="465:730	In such a corpus the occurrences of the relation 'young prospect' will be aligned to the corresponding occurrences of the Hebrew word tiqwa and will not be used when the Hebrew word sikkuy is involved." ></td>
	<td class="line x" title="466:730	Yet, it should be brought to mind that an aligned corpus is the result of manual translation, which can be viewed as including a manual tagging of the ambiguous words with their equivalent senses in the target language." ></td>
	<td class="line x" title="467:730	This resource is much more expensive and less available than an untagged monolingual corpus, and it seems to be necessary only for relatively rare situations." ></td>
	<td class="line x" title="468:730	Therefore, considering the trade-off between applicability and precision, it seems better to rely on a significantly larger monolingual corpus than on a smaller bilingual corpus." ></td>
	<td class="line x" title="469:730	An optimal method may exploit both types of corpora, in which the somewhat more accurate, but more expensive, data of a bilingual corpus are augmented by the data of a much larger monolingual corpus." ></td>
	<td class="line x" title="470:730	13 12 We thank the anonymous reviewer for suggesting this point." ></td>
	<td class="line x" title="471:730	13 Even though there are large quantities of translated texts, experience has shown that it is much harder to obtain large bilingual corpora than large monolingual corpora." ></td>
	<td class="line x" title="472:730	As mentioned earlier, a bilingual 583 Computational Linguistics Volume 20, Number 4 6.3.3 Lack of Deep Understanding." ></td>
	<td class="line x" title="473:730	By their nature, statistical methods rely on large quantities of shallow information." ></td>
	<td class="line x" title="474:730	Thus, they are doomed to fail when disambiguation can rely only on deep understanding of the text and no other surface cues are available." ></td>
	<td class="line x" title="475:730	This happened in one of the Hebrew examples, in which the two alternatives were either 'emigration law' or 'immigration law' (the Hebrew word hagira is used for both subsenses)." ></td>
	<td class="line x" title="476:730	While the context indicated that the first alternative is correct (emigration from the Soviet Union), the statistics (which were extracted from texts related to North America) preferred the second alternative." ></td>
	<td class="line x" title="477:730	To translate the above phrase, the program would need deep knowledge, to an extent that seems to far exceed the capabilities of current systems." ></td>
	<td class="line x" title="478:730	Fortunately, our results suggest that such cases are quite rare." ></td>
	<td class="line x" title="479:730	6.4 Monolingual Applications The results of our experiments in the context of machine translation suggest the utility of a similar mechanism even for in word sense disambiguation within a single language." ></td>
	<td class="line x" title="480:730	To select the right sense of a word, in a broad coverage application, it is useful to identify lexical relations between word senses." ></td>
	<td class="line x" title="481:730	However, within corpora of a single language it is possible to identify automatically only relations at the word level, which are, of course, not useful for selecting word senses in that language." ></td>
	<td class="line x" title="482:730	This is where other languages can supply the solution, exploiting the fact that the mapping between words and word senses varies significantly between different languages." ></td>
	<td class="line x" title="483:730	For instance, the English words 'sign' and 'seal' (from Example 1 in the introduction) correspond to two distinct senses of the Hebrew word lahtom." ></td>
	<td class="line x" title="484:730	These senses should be distinguished by most applications of Hebrew understanding programs." ></td>
	<td class="line x" title="485:730	To make this distinction, it is possible to perform the same process that is performed for target word selection, by producing all the English alternatives for the lexical relations involving lahtom." ></td>
	<td class="line x" title="486:730	Then the Hebrew sense that corresponds to the most plausible English lexical relations is preferred." ></td>
	<td class="line x" title="487:730	This process requires a bilingual lexicon that maps each Hebrew sense separately into its possible translations, similar to a Hebrew-Hebrew-English lexicon (analogous to the Oxford English-English-Hebrew dictionary of Hornby et al. \[1986\], which lists the senses of an English word, along with the possible Hebrew translations for each of them)." ></td>
	<td class="line x" title="488:730	In some cases, different senses of a Hebrew word map to the same word also in English." ></td>
	<td class="line x" title="489:730	In these cases, the lexical relations of each sense cannot be identified in an English corpus, and a third language is required to distinguish among these senses." ></td>
	<td class="line x" title="490:730	Alternatively, it is possible to combine our method with other disambiguation methods that have been developed in a monolingual context (see the next section)." ></td>
	<td class="line x" title="491:730	As a long-term vision, one can imagine a multilingual corpora-based environment, which exploits the differences between languages to facilitate the acquisition of knowledge about word senses." ></td>
	<td class="line x" title="492:730	7." ></td>
	<td class="line x" title="493:730	Comparative Analysis of Statistical Sense Disambiguation Methods Until recently, word sense disambiguation seemed to be a problem for which there is no satisfactory solution for broad coverage applications." ></td>
	<td class="line x" title="494:730	Recently, several statistical methods have been developed for solving this problem, suggesting the possibility of robust, yet feasible, disambiguation." ></td>
	<td class="line x" title="495:730	In this section we identify and analyze basic aspects of a statistical sense disambiguation method and compare several proposed corpus of moderate size can be valuable when constructing a bilingual lexicon, thus justifying the effort of maintaining such a corpus." ></td>
	<td class="line x" title="496:730	584 Ido Dagan and Alon Itai Word Sense Disambiguation methods (including ours) along these aspects." ></td>
	<td class="line x" title="497:730	TM This analysis may be useful for future research on sense disambiguation, as well as for the development of sense disambiguation modules in practical systems." ></td>
	<td class="line x" title="498:730	The basic aspects that will be reviewed are . 2." ></td>
	<td class="line x" title="499:730	3. 4." ></td>
	<td class="line x" title="500:730	Information sources used by the disambiguation method." ></td>
	<td class="line x" title="501:730	Acquisition of the required information from training texts." ></td>
	<td class="line x" title="502:730	The computational decision model." ></td>
	<td class="line x" title="503:730	Performance evaluation." ></td>
	<td class="line x" title="504:730	The first three aspects deal with the components of a disambiguation method, as would be implemented for a practical application." ></td>
	<td class="line x" title="505:730	The fourth is a methodological issue, which is relevant for developing, testing, and comparing disambiguation methods." ></td>
	<td class="line x" title="506:730	7.1 Information Sources We identify three major types of information that were used in statistical methods for sense disambiguation: . . 3." ></td>
	<td class="line x" title="507:730	Words appearing in the local, syntactically related, context of the ambiguous word." ></td>
	<td class="line x" title="508:730	Words appearing in the global context of the ambiguous word." ></td>
	<td class="line x" title="509:730	Probabilistic syntactic and morphological characteristics of the ambiguous word." ></td>
	<td class="line x" title="510:730	The first type of information is the one used in the current paper, in which words that are syntactically related to an ambiguous word are used to indicate its most probable sense." ></td>
	<td class="line x" title="511:730	Statistical data on the co-occurrence of syntactically related words with each of the alternative senses reflect semantic and lexical preferences and constraints of these senses." ></td>
	<td class="line x" title="512:730	In addition, these statistics may provide information about the topics of discourse that are typical for each sense." ></td>
	<td class="line x" title="513:730	Ideally, the syntactic relations between words should be identified using a syntactic parser, in both the training and the disambiguation phases." ></td>
	<td class="line x" title="514:730	Since robust syntactic parsers are not widely available, and those that exist are not always accurate, it is possible to use various approximations to identify relevant syntactic relations between words." ></td>
	<td class="line x" title="515:730	Hearst (1991) uses a stochastic part of speech tagger and a simple scheme for partial parsing of short phrases." ></td>
	<td class="line x" title="516:730	The structures achieved by this analysis are used to identify approximated syntactic relations between words." ></td>
	<td class="line x" title="517:730	Brown et al.(1991) make even weaker approximations, using only a stochastic part of speech tagger, and defining relations such as 'the first verb to the right' or 'the first noun to the left'." ></td>
	<td class="line x" title="519:730	Finally, Dagan et al.(1993) (see Section 5.1) assume full parsing at the disambiguation phase, but no preprocessing at the training phase, in which a higher level of noise can be accommodated." ></td>
	<td class="line x" title="521:730	A second type of information is provided by words that occur in the global context of the ambiguous word (Gale, Church, and Yarowsky 1992b, 1993; Yarowsky 1992; Sch6tze 1992)." ></td>
	<td class="line x" title="522:730	Gale et al. and Yarowsky use words that appear within 50 words in each 14 The reader is referred to some of these recent papers for thorough surveys of work on sense disambiguation (Hearst 1991; Gale, Church, and Yarowsky 1992a; Yarowsky 1992)." ></td>
	<td class="line x" title="523:730	585 Computational Linguistics Volume 20, Number 4 direction of the ambiguous word." ></td>
	<td class="line x" title="524:730	is Statistical data are stored about the occurrence of words in the context of each sense and are matched against the context in the disambiguated sentence." ></td>
	<td class="line x" title="525:730	Co-occurrence in the global context provides information about typical topics associated with each sense, in which a topic is represented by words that commonly occur in it." ></td>
	<td class="line x" title="526:730	Schiitze (1992, 1993) uses a variant of this type of information, in which contextvectors are maintained for character four-grams, instead of words." ></td>
	<td class="line x" title="527:730	In addition, the context of an occurrence of an ambiguous word is represented by co-occurrence information of a second order, as a set of context vectors (instead of a set of context words)." ></td>
	<td class="line x" title="528:730	Compared with co-occurrence within syntactic relations, information about the global context is less sensitive to fine semantic and lexical distinctions and is less useful when different senses of a word appear in similar contexts." ></td>
	<td class="line x" title="529:730	On the other hand, the global context contains more words and is therefore more likely to provide enough disambiguating information, in cases in which this distinction can be based ~on the topic of discourse." ></td>
	<td class="line x" title="530:730	From a general perspective, these two types of information represent a common trade-off in statistical language processing: the first type is related to a limited amount of deeper, and more precise linguistic information, whereas the second type provides a large amount of shallow information, which can be applied in a more robust manner." ></td>
	<td class="line x" title="531:730	The two sources of information seem to complement each other and may both be combined in future disambiguation methods." ></td>
	<td class="line x" title="532:730	16 Hearst (1991) incorporates a third type of statistical information to distinguish between different senses of nouns (in addition to the first type discussed above)." ></td>
	<td class="line x" title="533:730	For each occurrence of a sense, several syntactic and morphological characteristics are recorded, such as whether the noun modifies or is modified by another word, whether it is capitalized, and whether it is related to certain prepositional phrases." ></td>
	<td class="line x" title="534:730	Then, in the disambiguation phase, a best match is sought between the information recorded for each sense and the syntactic context of the current occurrence of the noun." ></td>
	<td class="line x" title="535:730	This type of information resembles the information that is defined for lexical items in lexicalist approaches for grammars, such as possible subcategorization frames of a word." ></td>
	<td class="line x" title="536:730	The major difference is that Hearst captures probabilistic preferences of senses for such syntactic constructs." ></td>
	<td class="line x" title="537:730	Grammatical formalisms, on the other hand, usually specify only which constructs are possible and at most distinguish between optional and obligatory ones." ></td>
	<td class="line x" title="538:730	Therefore the information recorded in such grammars cannot distinguish between different senses of a word that potentially have the same subcategorization frames, though in practice each sense might have different probabilistic preferences for different syntactic constructs." ></td>
	<td class="line x" title="539:730	It is clear that each of the different types of information provides some information that is not captured by the others." ></td>
	<td class="line x" title="540:730	However, as the acquisition and manipulation of each type of information requires different tools and resources, it is important to assess the relative contribution, and the 'cost effectiveness,' of each of them." ></td>
	<td class="line x" title="541:730	Such comparative evaluations are not available yet, not even for systems that incorporate several types of data (e.g. , McRoy 1992)." ></td>
	<td class="line x" title="542:730	Further research is therefore needed to com15 The size of the context was determined experimentally, based on evaluations of different sizes of context." ></td>
	<td class="line x" title="543:730	This optimization was performed for the Hansard corpus of the proceedings of the Canadian Parliament." ></td>
	<td class="line x" title="544:730	In general, the size of the global context depends on the corpus and typically consists of a homogeneous unit of discourse." ></td>
	<td class="line x" title="545:730	16 See also Gale, Church, and Yarowsky 1992b (pp." ></td>
	<td class="line x" title="546:730	58-59), and Sch~itze, 1992, 1993, for methods of reducing the number of parameters when using global contexts and Dagan, Marcus, and Markovitch 1993, for increasing the applicability of the use of local context, in cases in which there is no direct statistical evidence." ></td>
	<td class="line x" title="547:730	586 Ido Dagan and Alon Itai Word Sense Disambiguation pare the relative importance of different information types and to find optimal ways of combining them." ></td>
	<td class="line x" title="548:730	7.2 Acquisition of Training Information When training a statistical model for sense disambiguation, it is necessary to associate the acquired statistics with word senses." ></td>
	<td class="line x" title="549:730	This seems to require manual tagging of the training corpus with the appropriate sense for each occurrence of an ambiguous word." ></td>
	<td class="line x" title="550:730	A similar approach is being used for stochastic part of speech taggers and probabilistic parsers, relying on the availability of large, manually tagged (or parsed), corpora for training." ></td>
	<td class="line x" title="551:730	However, this approach is less feasible for sense disambiguation, for two reasons." ></td>
	<td class="line x" title="552:730	First, the size of corpora required to acquire sufficient statistics on lexical cooccurrence is usually much larger than that used for acquiring statistics on syntactic constructs or sequences of parts of speech." ></td>
	<td class="line x" title="553:730	Second, lexical co-occurrence patterns, as well as the definition of senses, may vary a great deal across different domains of discourse." ></td>
	<td class="line x" title="554:730	Consequently, it is usually not sufficient to acquire the statistics from one widely available 'balanced' corpus, as is common for syntactic applications." ></td>
	<td class="line x" title="555:730	A sense disambiguation model should be trained on the same type of texts for which it will be applied, thus increasing the cost of manual tagging." ></td>
	<td class="line x" title="556:730	The need to disambiguate a training corpus before acquiring a statistical model for disambiguation is often termed as the circularity problem." ></td>
	<td class="line x" title="557:730	In the following paragraphs we discuss different methods that were proposed to overcome the circularity problem, without exhaustive manual tagging of the training corpus." ></td>
	<td class="line x" title="558:730	In our opinion, this is the most critical issue in developing feasible sense disambiguation methods." ></td>
	<td class="line x" title="559:730	7.2.1 Bootstrapping." ></td>
	<td class="line x" title="560:730	Bootstrapping, which is a general scheme for reducing the amount of manual tagging, was proposed also for sense disambiguation (Hearst 1991)." ></td>
	<td class="line x" title="561:730	The idea is to tag manually an initial set of occurrences for each sense in the lexicon, acquiring initial training statistics from these instances." ></td>
	<td class="line x" title="562:730	Then, using these statistics, the system tries to disambiguate additional occurrences of ambiguous words." ></td>
	<td class="line x" title="563:730	If such an occurrence can be disambiguated automatically with high confidence, the system acquires additional statistics from this occurrence, as if it were tagged by hand." ></td>
	<td class="line x" title="564:730	Hopefully, the system will incrementally acquire all the relevant statistics, demanding just a small amount of manual tagging." ></td>
	<td class="line x" title="565:730	The results of Hearst (1991) show that at least 10 occurrences of each sense have to be tagged by hand, and in most cases 20-30 occurrences are required to get high precision." ></td>
	<td class="line x" title="566:730	These results, which were achieved for a small set of preselected ambiguous words, suggest that the cost of the bootstrapping approach is still very high." ></td>
	<td class="line x" title="567:730	7.2.2 Clustering Occurrences of an Ambiguous Word." ></td>
	<td class="line x" title="568:730	Sch6tze (1992, 1993) proposes a method that can be viewed as an efficient way of manual tagging." ></td>
	<td class="line x" title="569:730	Instead of presenting all occurrences of an ambiguous word to a human, these occurrences are first clustered using automatic clustering algorithms." ></td>
	<td class="line x" title="570:730	17 Then a human is asked to assign one of the senses of the word to each cluster, by observing several members of the cluster." ></td>
	<td class="line x" title="571:730	Each sense is thus represented by one or more clusters." ></td>
	<td class="line x" title="572:730	At the disambiguation phase, a new occurrence of an ambiguous word is matched against the contexts that were recorded for these clusters, selecting the sense of that cluster which provides the best match." ></td>
	<td class="line x" title="573:730	It is interesting to note that the number of occurrences that had to be observed by a human in the experiments of Sch/itze is of the same order as in the bootstrapping 17 Each occurrence is represented as a context vector, and the vectors are then clustered, 587 Computational Linguistics Volume 20, Number 4 approach: 10-20 members of a cluster were observed, with an average of 2.8 clusters per sense." ></td>
	<td class="line x" title="574:730	As both approaches were tested only on a small number of preselected words, further evaluation is necessary to predict the actual cost of their application to broad domains." ></td>
	<td class="line x" title="575:730	The methods described below, on the other hand, rely on resources that were already available on a large scale, and it is therefore possible to estimate the expected cost of their broad application." ></td>
	<td class="line x" title="576:730	7.2.3 Word Classification." ></td>
	<td class="line x" title="577:730	Yarowsky (1992) proposes a method that completely avoids manual tagging of the training corpus." ></td>
	<td class="line x" title="578:730	This is achieved by estimating parameters for classes of words rather than for individual word senses." ></td>
	<td class="line x" title="579:730	In his work, Yarowsky considered the semantic categories defined in Roget's Thesaurus as classes." ></td>
	<td class="line x" title="580:730	He then mapped (manually) each of the senses of an ambiguous word to one or several of the categories under which this word is listed in the thesaurus." ></td>
	<td class="line x" title="581:730	The task of sense disambiguation thus becomes the task of selecting the appropriate category for each occurrence of an ambiguous word." ></td>
	<td class="line x" title="582:730	18 When estimating the parameters of a category/9 any occurrence of a word that belongs to that category is counted as an occurrence of the category." ></td>
	<td class="line x" title="583:730	This means that each occurrence of an ambiguous word is counted as an occurrence of all the categories to which the word belongs and not just the category that corresponds to the specific occurrence." ></td>
	<td class="line x" title="584:730	A substantial amount of noise is introduced by this training method, which is a consequence of the circularity problem." ></td>
	<td class="line x" title="585:730	To avoid the noise, it would be necessary to tag each occurrence of an ambiguous word with the appropriate category." ></td>
	<td class="line x" title="586:730	As explained by Yarowsky, however, this noise can usually be tolerated." ></td>
	<td class="line x" title="587:730	The 'correct' parameters of a certain class are acquired from all its occurrences, whereas the 'incorrect' parameters are distributed through occurrences of many different classes and usually do not produce statistically significant patterns." ></td>
	<td class="line x" title="588:730	To reduce the noise further, Yarowsky uses a system of weights that assigns lower weights to frequent words, since such words may introduce more noise." ></td>
	<td class="line x" title="589:730	2 The word class method thus overcomes the circularity problem by mapping word senses to classes of words." ></td>
	<td class="line x" title="590:730	However, because of this mapping, the method cannot distinguish between senses that belong to the same class, and it also introduces some level of noise." ></td>
	<td class="line x" title="591:730	7.2.4 A Bilingual Corpus." ></td>
	<td class="line x" title="592:730	Brown et al.(1991) were concerned with sense disambiguation for machine translation." ></td>
	<td class="line x" title="594:730	Having a large aligned bilingual corpus available, they noticed that the target word which corresponds to an occurrence of an ambiguous source word can serve as a tag of the appropriate sense." ></td>
	<td class="line x" title="595:730	This kind of tagging provides sense distinctions when different senses of a source word translate to different target words." ></td>
	<td class="line x" title="596:730	For the purpose of translation, these are exactly the cases for which sense distinction is required." ></td>
	<td class="line x" title="597:730	Conceptually, the use of a bilingual corpus does not eliminate (or reduce) manual tagging of the training corpus." ></td>
	<td class="line x" title="598:730	Such a corpus is a result of manual translation, and it is the translator who provides tagging of senses as a side effect of the translation process." ></td>
	<td class="line x" title="599:730	Practically, whenever a bilingual corpus is available, it pro18 In some cases, the Roget index was found to be incomplete, and a missing category had to be added to the list of possibilities for a word." ></td>
	<td class="line x" title="600:730	19 Yarowsky uses statistics on occurrences of specific words in the global context of the category, but the method can be used to collect other types of statistics, such as the co-occurrence of the category with other categories." ></td>
	<td class="line x" title="601:730	20 The method of acquiring parameters from ambiguous occurrences in a corpus, relying on the 'spreading' of noise, can be used in many contexts." ></td>
	<td class="line x" title="602:730	For example, it was used for acquiring statistics for disambiguating prepositional phrase attachments, counting ambiguous occurrences of prepositional phrases as representing both noun-pp and verb-pp constructs (Hindle and Rooth 1991)." ></td>
	<td class="line x" title="603:730	588 Ido Dagan and Alon Itai Word Sense Disambiguation vides a useful source of a sense tagged corpus." ></td>
	<td class="line x" title="604:730	Gale, Church, and Yarowsky (1992a) have also exploited this resource for achieving large amounts of testing and training materials." ></td>
	<td class="line x" title="605:730	7.2.5 A Bilingual Lexicon and a Monolingual Corpus." ></td>
	<td class="line x" title="606:730	The method of the current paper also exploits the fact that different senses of a word are usually mapped to different words in another language." ></td>
	<td class="line x" title="607:730	However, our work shows that the differences between languages enable us to avoid any form of manual tagging of the corpus (including translation)." ></td>
	<td class="line x" title="608:730	This is achieved by a bilingual lexicon that maps a source language word to all its possible equivalents in the target language." ></td>
	<td class="line x" title="609:730	This approach has practical advantages for the purpose of machine translation, in which a bilingual lexicon needs to be constructed in any case, and very large bilingual corpora are not usually available." ></td>
	<td class="line x" title="610:730	From a theoretical point of view, the difference between the two methods can be made clear if we assume that the bilingual lexicon contains exactly all the different translations of a word which occur in a bilingual corpus." ></td>
	<td class="line x" title="611:730	For a given set of senses that need to be disambiguated, our method requires a bilingual corpus of size k, in which each sense occurs at least once, in order to establish its mapping to a target word." ></td>
	<td class="line x" title="612:730	In addition, a larger monolingual corpus, of size n, is required, to provide enough training examples of typical contexts for each sense." ></td>
	<td class="line x" title="613:730	On the other hand, using a bilingual corpus for training the disambiguation model would require a bilingual corpus of size n, which is significantly larger than k. The savings in resources is achieved since the mapping between the languages is done at the level of single words." ></td>
	<td class="line x" title="614:730	The larger amount of information about word combinations, on the other hand, is acquired from an untagged monolingual corpus, after the mapping has been performed." ></td>
	<td class="line x" title="615:730	Our results show that the precision of the selection algorithm is high despite the additional noise which is introduced by mapping single words independently of their context." ></td>
	<td class="line x" title="616:730	As mentioned in Section 6.3, an optimal method may combine the two methods." ></td>
	<td class="line x" title="617:730	In some sense, the use of a bilingual lexicon resembles the use of a thesaurus in Yarowsky's approach." ></td>
	<td class="line x" title="618:730	Both rely on a manually established mapping of senses to other concepts (classes of words or words in another language) and collect information about the target concepts from an untagged corpus." ></td>
	<td class="line x" title="619:730	In both cases, ambiguous words in the corpus introduce some level of noise: counting an occurrence of a word as an occurrence of all the classes to which it belongs, or counting an occurrence of a target word as an occurrence of all the source words to which it may correspond (a smaller amount of noise is introduced in the latter case, as a mapping to target words is much more finely grained than a mapping to Roget's categories)." ></td>
	<td class="line x" title="620:730	Also, both methods can distinguish only between senses that are distinguished by the mappings they use: either senses that belong to different classes, or senses that correspond to different target words." ></td>
	<td class="line x" title="621:730	An interesting difference, though, relates to the feasibility of implementing the two methods for a new domain of texts (in particular technical domains)." ></td>
	<td class="line x" title="622:730	The construction of a bilingual lexicon for a new domain is relatively straightforward and is often carried out for translation purposes." ></td>
	<td class="line x" title="623:730	The construction of an appropriate classification for the words of a new domain is more complex, and furthermore, it is not clear whether it is possible in every domain to construct a classification that is sufficient for the purpose of sense disambiguation." ></td>
	<td class="line x" title="624:730	7.3 The Computational Decision Model Sense disambiguation methods require a decision model that evaluates the relevant statistics." ></td>
	<td class="line x" title="625:730	Sense disambiguation thus resembles many other decision tasks, and not surprisingly, several common decision algorithms were employed in different works." ></td>
	<td class="line x" title="626:730	These include a Bayesian classifier (Gale, Church, and Yarowsky 1993) and a distance 589 Computational Linguistics Volume 20, Number 4 metric between vectors (Schiitze 1993), both inspired from methods in information retrieval; the use of the flip-flop algorithm for ordering possible informants about the preferred sense, trying to maximize the mutual information between the informant and the ambiguous word (Brown et al. 1991); and the use of confidence intervals to establish the degree of confidence in a certain preference, combined with a constraint propagation algorithm (the current paper)." ></td>
	<td class="line x" title="627:730	At the present stage of research on sense disambiguation, it is difficult to judge whether a certain decision algorithm is significantly superior to others." ></td>
	<td class="line x" title="628:730	21 Yet, these decision models can be characterized by several criteria, which clarify the similarities and differences between them." ></td>
	<td class="line x" title="629:730	As will be explained below, many of the differences are correlated with the different information sources employed by these models." ></td>
	<td class="line x" title="630:730	 Combining several informants: The methods described by Brown et al.(1991) and in the current paper combine several informants (i.e. , statistics about several context words) by choosing the informant that seems most indicative for the selection." ></td>
	<td class="line x" title="632:730	The effect of other, less significant, informants is then discarded." ></td>
	<td class="line x" title="633:730	The Bayesian classifier and the vector distance metric combine all informants simultaneously, in a multiplicative or additive manner, possibly assigning a certain weight to each informant." ></td>
	<td class="line x" title="634:730	 Reducing the number of parameters: Since sense disambiguation relies on statistics about lexical co-occurrence, the number of relevant parameters is very high, especially when co-occurrence in the global context is considered." ></td>
	<td class="line x" title="635:730	For this reason, Schiitze uses two compaction methods: First, 5000 'informative' four-grams are used instead of words." ></td>
	<td class="line x" title="636:730	Second, the 5000 dimensions are decomposed to 97 dimensions, using singular value decomposition." ></td>
	<td class="line x" title="637:730	This method reduces the number of parameters significantly, but has the disadvantage that it is impossible to trace the meaning of the entries in the resulting vectors or to associate them directly with the original co-occurrence statistics." ></td>
	<td class="line x" title="638:730	Gale, Church, and Yarowsky (1992b, pp." ></td>
	<td class="line x" title="639:730	58-59) propose another approach and reduce the number of parameters by selecting the most informative context words for each sense." ></td>
	<td class="line x" title="640:730	The selection of context words is based on a theoretically motivated criterion, borrowed from Mosteller and Wallace (1964, pp." ></td>
	<td class="line x" title="641:730	55-56)." ></td>
	<td class="line x" title="642:730	Finally, Yarowsky's method further reduces the number of parameters, as it records co-occurrences between individual words and word classes." ></td>
	<td class="line x" title="643:730	 Statistical significance of the selection: In the current paper, we use confidence intervals to test whether the statistical preference for a certain sense is significant." ></td>
	<td class="line x" title="644:730	In a simple multiplicative preference score, on the other hand, it is not possible to distinguish whether preferences rely on small or large counts." ></td>
	<td class="line x" title="645:730	The method of Gale et al. remedies this problem indirectly (in most cases) by introducing a sophisticated interpolation between the actual counts of the co-occurrence parameters and the frequency counts of the individual words (see Gale, Church, and Yarowsky 1993, for details)." ></td>
	<td class="line x" title="646:730	In Schiitze's method it is not possible to trace the statistical significance of the parameters since they are the result of extensive processing and compaction of the original statistical data." ></td>
	<td class="line x" title="647:730	21 Once the important information sources for sense selection have been identified, it is possible that different decision algorithms would achieve comparable results." ></td>
	<td class="line x" title="648:730	590 Ido Dagan and Alon Itai Word Sense Disambiguation Resolving all ambiguities simultaneously: In the current paper, the selection of a sense for one word affects the selection for another word through a constraint propagation algorithm." ></td>
	<td class="line x" title="649:730	This property is absent in most other methods." ></td>
	<td class="line x" title="650:730	The differences between various disambiguation methods correlate with the difference in information sources, in particular, whether they use local or global context." ></td>
	<td class="line x" title="651:730	When local context is used, only few syntactically related informants may provide reliable information about the selection." ></td>
	<td class="line x" title="652:730	It is therefore reasonable to base the selection on only one, the most informative informant, and it is also important to test the statistical significance of that informant." ></td>
	<td class="line x" title="653:730	The problem of parameter explosion is less severe, and the number of parameters is comparable to that of a bi-gram language model (and even smaller)." ></td>
	<td class="line x" title="654:730	When using the global context, on the other hand, the number of potential parameters is significantly larger, but each of them is usually less informative." ></td>
	<td class="line x" title="655:730	It is therefore important to take into account as many parameters as possible in each ambiguous case, but it is less important to test for detailed statistical significance, or to worry about the mutual effects of sense selections for adjacent words." ></td>
	<td class="line x" title="656:730	7.4 Performance Evaluation In most of the above-mentioned papers, experimental results are reported for a small set of up to 12 preselected words, usually with two or three senses per word." ></td>
	<td class="line x" title="657:730	In the current paper we have evaluated our method using a random set of example sentences, with no a priori selection of the words." ></td>
	<td class="line x" title="658:730	This standard evaluation method, which is commonly used for other natural language processing tasks, provides a direct prediction for the expected success rate of the method when employed in a practical application." ></td>
	<td class="line x" title="659:730	To compare results on different test data, it is useful to compare the precision of the disambiguation method with some a priori figure that reflects the degree of ambiguity in the text." ></td>
	<td class="line x" title="660:730	Reporting the number of senses per example word corresponds to the expected success rate of random selection." ></td>
	<td class="line x" title="661:730	A more informative figure is the success rate of a naive method that always selects the most frequent sense (the Word Frequencies method in our evaluations)." ></td>
	<td class="line x" title="662:730	The success rate of this naive method is higher than that of random selection and thus provides a tighter lower bound for the desired precision of a proposed disambiguation method." ></td>
	<td class="line x" title="663:730	An important practical issue in evaluation is how to get the test examples, which should be tagged with the correct sense." ></td>
	<td class="line x" title="664:730	In most papers (including ours) the tagging of the test data was done by hand, which limits the size of the testing set." ></td>
	<td class="line x" title="665:730	Preparing one test set by hand may still be reasonable, though time consuming." ></td>
	<td class="line x" title="666:730	However, it is useful to have more than one set, such that results will be reported on a new, unseen, set, while another set is used for developing and tuning the system." ></td>
	<td class="line x" title="667:730	One useful source of tagged examples is an aligned bilingual corpus, which can be used for testing any sense disambiguation method, including methods that do not use bilingual material for training." ></td>
	<td class="line x" title="668:730	Gale proposes to use 'pseudo-words' as another practical source of testing examples (Gale, Church, and Yarowsky 1992b) (equivalently, Schfitze \[1992\] uses 'artificial ambiguous words')." ></td>
	<td class="line x" title="669:730	Pseudo-words are constructed artificially as a union of several different words (say, wl, w2, and w3 define three 'senses' of the pseudo-word x)." ></td>
	<td class="line x" title="670:730	The disambiguation method is presented with texts in which all occurrences of wl, w2, and w3 are considered as occurrences of x and should then select the original word (sense) for each occurrence." ></td>
	<td class="line x" title="671:730	Though testing with this method does not provide results for real ambiguities that occur in the text, it can be very useful while develop591 Computational Linguistics Volume 20, Number 4 ing and tuning the method (Gale shows high correlation between the performance of his method on real sense ambiguities and pseudo-words)." ></td>
	<td class="line x" title="672:730	8." ></td>
	<td class="line x" title="673:730	Conclusions The method presented in this paper takes advantage of two linguistic phenomena, both proven to be very useful for sense disambiguation: the different mapping between words and word senses among different languages, and the importance of lexical co-occurrence within syntactic relations." ></td>
	<td class="line x" title="674:730	The first phenomenon provides the solution for the circularity problem in acquiring sense disambiguation data." ></td>
	<td class="line x" title="675:730	Using a bilingual lexicon and a monolingual corpus of the target language, we can acquire statistics on word senses automatically, without manual tagging." ></td>
	<td class="line x" title="676:730	As explained in Section 7, this method has significant practical and theoretical advantages over the use of aligned bilingual corpora." ></td>
	<td class="line x" title="677:730	We pay for these advantages by introducing an additional level of noise, in mapping individual words independently to the other language." ></td>
	<td class="line x" title="678:730	Our results show, however, that the precision of the selection algorithm is high despite this additional noise." ></td>
	<td class="line x" title="679:730	This work also emphasizes the importance of lexical co-occurrence within syntactic relations for the resolution of lexical ambiguity." ></td>
	<td class="line x" title="680:730	Co-occurrences found in a large corpus reflect a huge amount of semantic knowledge, which was traditionally constructed by hand." ></td>
	<td class="line x" title="681:730	Moreover, frequency data for such co-occurrences reflect both linguistic and domain-specific preferences, thus indicating not only what is possible, but also what is probable." ></td>
	<td class="line x" title="682:730	It is important to notice that frequency information on lexical co-occurrence was found to be much more predictive than single word frequency." ></td>
	<td class="line x" title="683:730	In the three experiments we reported, there were 61 cases in which the two types of information contradicted each other, favoring different target words." ></td>
	<td class="line x" title="684:730	In 56 of these cases (92%), it was the most frequent lexical co-occurrence, and not the most frequent word, that predicted the correct translation." ></td>
	<td class="line x" title="685:730	This result may raise relevant hypotheses for psycholinguistic research, which has indicated the relevance of word frequencies to human sense disambiguation (e.g. , Simpson and Burgess 1988)." ></td>
	<td class="line x" title="686:730	We suggest that the high precision achieved in the experiments relies on two characteristics of the ambiguity phenomena, namely the sparseness and redundancy of the disambiguating data." ></td>
	<td class="line x" title="687:730	By sparseness we mean that within the large space of alternative interpretations produced by ambiguous utterances, only a small portion is commonly used." ></td>
	<td class="line x" title="688:730	Therefore, the chance that an inappropriate interpretation is observed in the corpus (in other contexts) is low." ></td>
	<td class="line x" title="689:730	Redundancy relates to the fact that different informants (such as different lexical relations or deep understanding) tend to support rather than contradict one another, and therefore the chance of picking a 'wrong' informant is low." ></td>
	<td class="line x" title="690:730	It is interesting to compare our method with some aspects of the statistical machine translation system of Brown et al.(1990)." ></td>
	<td class="line x" title="692:730	As mentioned in the introduction, this system also incorporates target language statistics in the translation process." ></td>
	<td class="line x" title="693:730	To translate a French sentence, f, they choose the English sentence, e, that maximizes the term Pr(e)  Pr(f I e)." ></td>
	<td class="line x" title="694:730	The first factor in this product, which represents the target language model, may thus affect any aspect of the translation, including target word selection." ></td>
	<td class="line x" title="695:730	It seems, however, that Brown et al. expect that target word selection would be determined mainly by translation probabilities (the second factor in the above term), which should be derived from a bilingual corpus (Brown et al. 1990, p. 79)." ></td>
	<td class="line x" title="696:730	This view is reflected also in their elaborate method for target word selection (Brown et al. 1991), in which better estimates of translation probabilities are achieved as a result of word sense disambiguation." ></td>
	<td class="line x" title="697:730	Our method, on the other hand, incorporates only 592 Ido Dagan and Alon Itai Word Sense Disambiguation target language probabilities and ignores any notion of translation probabilities." ></td>
	<td class="line x" title="698:730	It thus demonstrates a possible trade-off between these two types of probabilities: using more informative statistics of the target language may compensate for the lack of translation probabilities." ></td>
	<td class="line x" title="699:730	For our system, the more informative statistics are achieved by syntactic analysis of both the source and target languages, instead of the simple tri-gram model used by Brown et al. In a broader sense, this can be viewed as a tradeoff between the different components of a translation system: having better analysis and generation models may reduce some burden from the transfer model." ></td>
	<td class="line x" title="700:730	In our opinion, the method proposed in this paper may have immediate practical value, beyond its theoretical aspects." ></td>
	<td class="line x" title="701:730	As we argue below, we believe that the method is feasible for practical machine translation systems and can provide a cost-effective improvement on target word selection methods." ></td>
	<td class="line x" title="702:730	The identification of syntactic relations in the source sentence is available in any machine translation system that uses some form of syntactic parsing." ></td>
	<td class="line x" title="703:730	Trivially, a bilingual lexicon is available." ></td>
	<td class="line x" title="704:730	A parser for the target language becomes common in many systems that offer bidirectional translation capabilities, requiring parsers for several languages (see Miller 1993, for available language pairs in several commercial machine translation systems)." ></td>
	<td class="line x" title="705:730	If a parser for the target language corpus is not available, it is possible to approximate the statistics using word co-occurrence in a window, as was demonstrated by a variant of our method (Dagan, Marcus, and Markovitch 1993) (see Section 5.1)." ></td>
	<td class="line x" title="706:730	In both cases, the statistical model was shown to handle successfully the noise produced in automatic acquisition of the data." ></td>
	<td class="line x" title="707:730	Substantial effort may be required for collecting a sufficiently large target language corpus." ></td>
	<td class="line x" title="708:730	We have not studied the relation between the corpus size and the performance of the algorithm, but it is our impression that a corpus of several hundred thousand words will prove useful for translation in a well-defined domain." ></td>
	<td class="line x" title="709:730	With current availability of texts in electronic form, = a corpus of this size is feasible in many domains." ></td>
	<td class="line x" title="710:730	The effort of assembling this corpus should be compared with the effort of manually coding sense disambiguation information." ></td>
	<td class="line x" title="711:730	Finally, our method was evaluated by simulating realistic machine translation lexicons, on randomly selected examples, and yielded high performance in two different broad domains (foreign news articles and a software manual)." ></td>
	<td class="line x" title="712:730	It is therefore expected that the results reported here will be reproduced in other domains and systems." ></td>
	<td class="line x" title="713:730	To improve the performance of target word selection further, our method may be combined with other sense disambiguation methods." ></td>
	<td class="line x" title="714:730	As discussed in Section 6.2, it is possible to increase the applicability (coverage) of the selection method by considering word co-occurrence in a limited context and/or by using similarity-based methods that reduce the problem of data sparseness." ></td>
	<td class="line x" title="715:730	To a lesser extent, the use of a bilingual corpus may further increase the precision of the selection (see Section 6.3)." ></td>
	<td class="line x" title="716:730	A practical strategy may be to use a bilingual corpus for enriching the bilingual lexicon, while relying mainly on co-occurrence statistics from a larger monolingual corpus for disambiguation." ></td>
	<td class="line x" title="717:730	In a broader context, this paper promotes the combination of statistical and linguistic models in natural language processing." ></td>
	<td class="line x" title="718:730	It provides an example of how a problem can be first defined in detailed linguistic terms, using an implemented linguistic tool (a syntactic parser, in our case)." ></td>
	<td class="line x" title="719:730	Then, having a well-defined linguistic scenario, we apply a suitable statistical model to highly informative linguistic structures." ></td>
	<td class="line x" title="720:730	According to this view, a complex task, such as machine translation, should be first decomposed 22 Optical character recognition can also be used to acquire relevant texts in electronic form." ></td>
	<td class="line x" title="721:730	In this case, it may be necessary to approximate the statistics using word co-occurrence in a window, since parsing noisy output from optical character recognition is difficult." ></td>
	<td class="line x" title="722:730	593 Computational Linguistics Volume 20, Number 4 on a linguistic basis." ></td>
	<td class="line x" title="723:730	Then, appropriate statistical models can be developed for each sub-problem." ></td>
	<td class="line x" title="724:730	We believe that this approach provides a beneficial compromise between two extremes in natural language processing: either using linguistic models that ignore quantitative information, or using statistical models that are linguistically ignorant." ></td>
	<td class="line x" title="725:730	Appendix Approximatingvar\[ln(~)l To approximate var \[In (~)\], we first approximate In (~)by the first order derivatives (the first term of the Taylor series): (\]91) ~__ ln( pI )__ \[~Xl (X1/\]~22 In ~ ~ '}(Pl --,1 ) In pl,p2 q-(\]92--P2) \[~-~21n(X~22)\]pl,p2 : ln(P~2) qfil--p~lpl \]92--P2p2 : ln(P~2)q-\]91 --\]92'pl P2 (5) We use the following equations (see Agresti 1990): var(x+c) = var(x), var(xl x2) = var(xl) + var(x2) 2." ></td>
	<td class="line x" title="726:730	covariance(xl,x2), var(~) -p(1-p), n (c) var(x) va r C2, covariance(fii, l~j ) PiPj n covariance( x l, x2)  r x1 x2 covamance( ~, ~2 ) = clc2 Using (5) we get var\[ln(~22)\] ~ var\[ln(P~221 +l~lp, ~21 = varI~11-~221 \[\]91\] \[lY2\]_2.covariance\[lYl,tY2 \] = var Pll +var P2 \[pl ~22 _ 1 p1(1 -Pl) + 1 p2(1 -P2) +2 PiP2 p2 n p2 n nplP2 1 1 1 1 1 1 +__~ + =--+--." ></td>
	<td class="line x" title="727:730	npl np2 np~l nl~ nl n2 594 Ido Dagan and Alon Itai Word Sense Disambiguation Acknowledgments Special thanks are due to Ulrike Schwall for her fruitful collaboration." ></td>
	<td class="line x" title="728:730	We are grateful to Mori Rimon, Peter Brown, Ayala Cohen, Ulrike Rackow, Herb Leass, and Bill Gale for their help and comments." ></td>
	<td class="line x" title="729:730	We also thank the anonymous reviewers for their detailed comments, which resulted in additional discussions and clarifications." ></td>
	<td class="line x" title="730:730	This research was partially supported by grant number 120-741 of the Israel Council for Research and Development." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W94-0311
Semantic Lexicons: The Cornerstone For Lexical Choice In Natural Language Generation
Viegas, Evelyne;Bouillon, Pierrette;"></td>
	<td class="line x" title="1:168	Semantic Lexicons: the Cornerstone for Lexical Choice in Natural Language Generation Evelyne Viegast Pierrette Bouillon viegas~cs.brandeis.edu pb~divsun.unige.ch tComputer Science Department, Brandeis University, Waltham, MA 02254 USA ISSCO, University of Geneva, 54 route des Acacias, CH-1227 Geneva, Switzerland Abstract In this paper, we address the issue of integrating semantic lexicons into NLG systems and argue that the problem of lexical choice in generation can be approached only by such an integration." ></td>
	<td class="line x" title="2:168	We take the approach of Generative Lexicon Theory (GLT) (Pnstejovsky, 1991, 1994c) which provides a system involving four levels of representation connected by a set of generative devices accounting for a compositional interpretation of words in context." ></td>
	<td class="line x" title="3:168	We are interested in showing that we can reduce the set of collocations listed in the lexicon by introducing the notion of 'semantic collofations' which can be predicted within GLT framework." ></td>
	<td class="line x" title="4:168	We argue that the lack of semantic welldefined calculi in previous approaches, whether linguistic or conceptual, renders them unable to account for semantic collocations." ></td>
	<td class="line x" title="5:168	1 Introduction Whether we talk of monolingual or multilingual generation, it is not surprising that there has been very little focus on the area of lexical choice." ></td>
	<td class="line x" title="6:168	Lexical choice has often been side-stepped, not because it is a daunting issue, but rather because the interest in natural language generation (NLG) first focused on syntactic, morphological and discourse aspects of language." ></td>
	<td class="line x" title="7:168	Semantic accuracy has been therefore sacrificed in the production of fluent grammaticalsentences." ></td>
	<td class="line x" title="8:168	In section 2, we highlight the issue of lexical choice, by arguing that generation systems must integrate lexical semantics and focusing on the treatment of Adjective-noun (Adj-Noun) collocations." ></td>
	<td class="line x" title="9:168	We introduce the notion of 'semantic collocations', which allows us to reduce the set of collocations which are usually listed in lexicons." ></td>
	<td class="line x" title="10:168	In section 3, we present relevant aspects of the Generative Lexicon Theory (GLT), which, we argue, provides a better representation and interpretation of lexical information, enabling us to generate the set of possible semantic collocations in a predictive way without listing them in lexical entries." ></td>
	<td class="line x" title="11:168	GLT is still under development from a theoretical point of view and up to now no generation system (as far as the authors are We would llke to thank Susan Armstrong, Paul Buitelaar, Federica Busa, Dominique Estival, James Pustejovsky, Graham Russell and Scott Waterman for their helpful comments." ></td>
	<td class="line x" title="12:168	aware) has tried to integrate or implement its ideas." ></td>
	<td class="line x" title="13:168	We propose to do so, and are currently studying its theoretical adequacy for generation with special reference to the issue of lexical choice." ></td>
	<td class="line x" title="14:168	In section 4, we show that it is possible to calculate Adj-Noun semantic collocations ( a long book; an easy novel; a fast car) as opposed to the type of collocations where idiosyncrasy seems to be involved ( a large coke vs. a big coke)." ></td>
	<td class="line x" title="15:168	Finally, in section 5, we emphazise the adequacy of a framework such as GLT to generate the possible set of semantic collocations." ></td>
	<td class="line x" title="16:168	2 The Issue of Lexical Choice There is a debate in NLG concerning the place of lexical choice in the generation process." ></td>
	<td class="line x" title="17:168	Should lexical choice take place at the level of the 'planning component' or the 'realization component'?" ></td>
	<td class="line x" title="18:168	Even for generators which do not have a 'traditional' twocomponent architecture, actions are still sequential and lexical choice takes place after some 'planning'." ></td>
	<td class="line x" title="19:168	Lexical choice relates to lexicalization in the sense of not only needing to pick up the right words or expressions but also of needing to 'realize' them or lexicalize them." ></td>
	<td class="line x" title="20:168	We would argue on one hand that lexicalization does not constitute an autonomous module within the process of generation, and on the other hand that lexical choice is not the sole prerogative of either the 'planning' or the 'realization' component." ></td>
	<td class="line x" title="21:168	The reason is that a concept cannot be seen in isolation (the choice of a particular concept will trigger some other related concepts) and when lexicalized, the syntactico-semantics of the lexical item will impose some constraints on the further possible choice of concepts to be lexicalized (thus constraining the set of concepts triggered by the previous one)." ></td>
	<td class="line x" title="22:168	In other words in the process of production a lexical choice can influence a conceptual choice and vice versa." ></td>
	<td class="line x" title="23:168	Thus in terms of NLG this means that lexical choice has some influence at the level of 'planning' and 're91 7th International Generation Workshop  Kennebunkport, Maine  June 21-24, 1994 alization'." ></td>
	<td class="line x" title="24:168	Moreover, if we want to generate in an incremental way, it follows that a strict distinction between these two components can no longer hold, and that we must attempt either to bridge gaps between them (Meteer 1992) or to generate in a partly parallel fashion." ></td>
	<td class="line x" title="25:168	In this paper, we take the view of integrating lexical semantics in the design of the lexicon to be used in an NLG system, in order to perform the right lexicalizations." ></td>
	<td class="line x" title="26:168	We define lexicalization as a complex dynamic process, by which we find the appropriate lexicalized items for utterances, in order to fulfill communicative goals." ></td>
	<td class="line x" title="27:168	In fact, we think that we use a backward and forward process between concepts and lexical items and we believe that it is through incremental (re)lexiealizations(re)conceptuallzations that we perform wellformed linguistic realizations (Viegas, 1993)." ></td>
	<td class="line x" title="28:168	In the following, after a brief overview of the issue of lexical choice, we focus on the treatment of collocations, which poses the problem of complex lexicalizations, and motivates the need of taking into account, in the process of lexicalizing, both several concepts and several lexical items." ></td>
	<td class="line x" title="29:168	2.1 Different Approaches Roughly speaking, the issue of lexical choice has been investigated mainly along two different lines: a conceptual-based approach (mainly in the AI tradition) and a linguistic-based approach." ></td>
	<td class="line x" title="30:168	1 Despite these efforts, lexical choice remains a burning issue." ></td>
	<td class="line x" title="31:168	We agree with McKeown and Swartout (1988) when they say that: ' a truly satisfactory theoretical approach for lexical choice has yet to be developed'." ></td>
	<td class="line x" title="32:168	However, like some leading researchers in generation, we argue that it is of paramount importance to first know the kind of information that should be coded in the lexicon, which means to pay more attention to 'the nature of words' (McDonald, 1988) and to have a 'real knowledge of \[the\] lexical semantics', as was pointed out by Marcus (1987): 'In some important sense, \[the\] systems have no real knowledge of lexical semantics They use fragments of linguistic structure which eventually have words as their frontiers, but have little or no explicit knowledge of what these words mean'." ></td>
	<td class="line x" title="33:168	In this article, we will not give a review of the issue of the lexical choice; it is enough to say that the lexical semantic component for lexical representation is still 1Robin's report (1990) presents a good survey on 'Lexical Choice in NLG'." ></td>
	<td class="line x" title="34:168	See also (Reiter, 1991) and (Nogier and Zock, 1992) for a comprehensive study of the evolution made in the field." ></td>
	<td class="line x" title="35:168	basically unused and that there is a need to tackle that issue if we want to give some new and promising impetus to the study on lexical choice." ></td>
	<td class="line x" title="36:168	2.2 The Treatment of Collocations There is much divergence of opinion on just what the defining criteria for collocations are." ></td>
	<td class="line x" title="37:168	One can minimally define a collocation as the distribution of an object or element in relation to other objects or elements, as dictionaries do; needless to say, apart from remaining vague, at best this does not provide any clue for finding them operationally." ></td>
	<td class="line x" title="38:168	There are three main approaches to the study of collocations, namely, lexicographic, statistical and linguistic: in each of these, the term collocation is used differently." ></td>
	<td class="line x" title="39:168	The traditional approach to collocations has been lexlcographic." ></td>
	<td class="line x" title="40:168	Here dictionaries provide information about what is unpredictable or idiosyncratic." ></td>
	<td class="line x" title="41:168	Benson (1989) synthesizes Hausmann's studies on collocations (Hausmann, 1979), calling expressions such as commit murder, compile a dictionary, inflict a wound, etc. 'fixed combinations, recurrent combinations' or 'collocations'." ></td>
	<td class="line x" title="42:168	In Hausmann's terms a collocation is composed of two elements, a base ('Basis') and a collocate ('Kollokator'); the base is semantically autonomous whereas the collocate cannot be semantically interpreted in isolation." ></td>
	<td class="line x" title="43:168	In other words the set of lexical collocates which can combine with a given base is not predictable and collocations must therefore be listed in dictionaries." ></td>
	<td class="line x" title="44:168	In recent years, there has been a resurgence of statistical approaches applied to the study of natural languages." ></td>
	<td class="line x" title="45:168	Sinclair (1991) states that 'a word which occurs in close proximity to a word under investigation is called a collocate of it  Collocation is the occurrence of two or more words within a short space of each other in a text'." ></td>
	<td class="line oc" title="46:168	The problem is that with such a definition of collocations, even when improved, one identifies not only collocations but freecombining pairs frequently appearing together such as lawyer-client; doctor-hospital, as pointed out by Smadja (1993)." ></td>
	<td class="line x" title="47:168	There has been no real focus on collocations from a linguistic perspective." ></td>
	<td class="line x" title="48:168	The lexicon has been broadly sacrificed by both English-speaking schools and continental European schools." ></td>
	<td class="line x" title="49:168	The scientific agenda of the former has been largely dominated by syntactic issues until recently whereas the latter was more concerned with pragmatic aspects of natural languages." ></td>
	<td class="line x" title="50:168	The focus has been largely on grammatical collocations such as adapt to, aim at, look for." ></td>
	<td class="line x" title="51:168	Lakoff (1970) distinguishes a class of expressions which cannot undergo certain operations, such as nominalization, causativization: the problem is hard; *the hard92 7th International Generation'Workshop  Kennebunkport, Maine  June 21-24, 1994 ness of the problem; *the problem hardened." ></td>
	<td class="line x" title="52:168	Restrictions on the application of certain syntactic operations can help define collocations such as hard problem, for example." ></td>
	<td class="line x" title="53:168	One specific proposal for how to treat collocations in a linguistic model is developed in Mel'~uk's work on lexical functions (Mel'~uk, 1988)." ></td>
	<td class="line x" title="54:168	In this theory, lexicM knowledge is encoded in an entry of the Explanatory Combinatorial Dictlonary, each entry being divided into three zones: the semantic zone (a semantic network representing the meaning of the entry in terms of more primitive words), the syntactic zone (the grammatical properties of the entry) and the lexical combinatorics zone (containing the values of the Lexical Functions (LFs)) ~." ></td>
	<td class="line x" title="55:168	LFs are central to the study of collocations and can be defined as the following : a lexicalfunction F is a correspondence which associates a lexical item L, called the key word of F, with a set of lexical items F(L) the value of F (Mel'~uk, 1988)." ></td>
	<td class="line x" title="56:168	The LF Magn, for example, applies to different categories to deliver collocational values, expressing an intensity: Magn(smoker) = heavy \[smoker\] Magn(opposed)-= strongly/vehemently \[opposed\] Magn(large) = excessively \[large\] The Mel'~ukian approach is very interesting as it provides a model of production well suited for generation with its different strata and also a lot of lexicalsemantic information." ></td>
	<td class="line x" title="57:168	It suffers nevertheless from three main problems (Heylen et al. , 1993)." ></td>
	<td class="line x" title="58:168	First, all the collocational information must be listed in a static way, because the theory does not provide any predictable calculus of the possible expressions which can collocate with each other semantically." ></td>
	<td class="line x" title="59:168	Second, it is sometimes difficult to assign the right lexical functions for newly analyzed lexical items; if we take the example of assigning an LF to an Adj-Noun structure, it involves knowing something about the semantic relation which exists between adjective and noun." ></td>
	<td class="line x" title="60:168	(Bloksma et al. , 1993) state that 'It is precisely this information which in many cases proves extremely difficult to establish, simply because it is just not entirely clear what semantic processes are involved in the union of adjective and noun'." ></td>
	<td class="line x" title="61:168	Finally, sometimes LFs are too general to be useful, as shown in the following examples: Magn t'mp (experience) = lengthy Magn quant (experience) = considerable Magn,on,,q~nc,, (illness) = serious In these cases, superscripts and subscripts are needed to restrict the scope of the LF: they enhance the precision of the LFs, making them sensitive to 2See (Iordanskaja, et al. , 1991) and (Ramos et al. , 1994), concerning the use of MTT and LFs in NLG respectively." ></td>
	<td class="line x" title="62:168	meaning aspects of the lexical items on which they operate, thus constraining overgeneration of multiple values; yet this also shows that the set of LFs described is not sufficient." ></td>
	<td class="line x" title="63:168	By contrast, our general thesis is that there is no single definition for what a collocation is, but rather, collocational behavior emerges from a theory of what the range of connections and relations between lexical items can be." ></td>
	<td class="line x" title="64:168	We claim that much of the allegedly idiosyncratic and language-specific collocation in language is in fact predictable from a sufficiently rich theory of lexical organization." ></td>
	<td class="line x" title="65:168	This is not to say that there is no need for specific lexical encoding of some idioms and phrases, but that there is seldom any attempt made to bridge the gap between conventional semantic selection and the peripheral phenomena of collocations and fixed expressions." ></td>
	<td class="line x" title="66:168	We will make the distinction between the following kinds of combinations: Free-Combining Words: ( a big stick; a wonderful man; there is an old man at the door) Semantic Collocations: ( a fast car; a long book; to start a car) Idiosyncratic Lexical Co-occurrences: (a heavy smoker vs. un grand fumeur (French); un grand/gros mangeur (French) vs. un gran/~gordo comelon (Spanish)) Idioms: (to kick the bucket; take advantage o\])." ></td>
	<td class="line x" title="67:168	Formally, this takes us from purely compositional constructions of 'free-combining words' to the noncompositional structures in idioms." ></td>
	<td class="line x" title="68:168	The vast space between these two extremes can still be explained in terms of compositional principles with mechanisms from GLT such as type coercion and subselection (Pustejovsky, 1991, 1993), as we shall see below." ></td>
	<td class="line x" title="69:168	Idiosyncrasies, of course, should be listed in the lexicon, yet we believe that we can reduce the set of what are conventionally considered idiosyncrasies by differentiating 'true' idiosyncrasies (which cannot be derived or generated) from expressions which, since they are compositional in nature, behave predictably, and which we call semantic collocations." ></td>
	<td class="line x" title="70:168	3 Generative Lexicon Theory The Generative Lexicon Theory (GLT) (Pustejovsky, 1991, 1994c) can be said to take advantage of both linguistic and conceptual approaches, providing a framework which arose from the integration of linguistic studies and of techniques found in AI." ></td>
	<td class="line x" title="71:168	GLT can be briefly characterized as a system which involves four 93 7th International Generation Workshop  Kennebunkport, Maine  June 21-24, 1994 levels of representation which are connected by a set of generative devices accounting for a compositional interpretation of words in context, namely: the argument structure which specifies the predicate argument structure for a word and the conditions under which the variables map to syntactic expressions; the event structure giving the particular event types such as S (state), P (process) or T (transition); the qualia structure distributed among four roles FORM (formal), CONST (constitutive), TELIC and AGENT (Agentive); and the inheritance structure which involves two different kinds of mechanisms:  the fixed inheritance mechanism, which is basically a fixed network of the traditional isa relationship found in AI, enriched with the different roles of the qualia structure;  the projective inheritance mechanism, which can be intuitively characterized as a way of triggering semantically related concepts which define for each role the projective conclusion space (PCS)." ></td>
	<td class="line x" title="72:168	For instance in the PCS of the telic and agentive roles of book we will find at least the following predicates: read, reissue, annotate,  and write, print, bind,  (respectively) 3." ></td>
	<td class="line x" title="73:168	The most important of the generative devices connecting these four levels is a semantic operation called type coercion which 'captures the semantic relatedness between syntactically distinct expressions' (Pustejovsky, 1994a)." ></td>
	<td class="line x" title="74:168	Another notion introduced is that of lexical conceptual paradigms (LCPs), as formalized in (Pustejovsky, 1994b)." ></td>
	<td class="line x" title="75:168	We will say that the aim of an LCP is to capture the conceptual regularities across languages in terms of cognitive invariants, like 'physical-object', 'aperture', 'natural kind' and alternations such as 'container/containee', etc. Moreover, the possible syntactic projections are associated with LCPs." ></td>
	<td class="line x" title="76:168	For instance, one can say 'I left a leaflet in/inside the book at the page I want you to read' as book is an information-phys_obj-container whereas for instance one cannot say 'I put the book in the top of the table' as 'the top of the table' is a surface and not a container 4." ></td>
	<td class="line x" title="77:168	In the following, we will focus on two basic mechanisms of GLT, which allow us to bridge the word usage gap, that is, on a scale of lexical specificity, from freecombining words to idioms." ></td>
	<td class="line x" title="78:168	These are: (1) Reference to the qualia structure: By giving every category the ability to make reference to spe3Thls issue is still unsettled in GLT." ></td>
	<td class="line x" title="79:168	Our point however, being to show how to predict Adj-Noun semantic collocations, our discussion will not suffer from that lack." ></td>
	<td class="line x" title="80:168	4We follow Dubois and Pereita (1993) in their analysis of categorization in relation with cognition." ></td>
	<td class="line x" title="81:168	cific semantic functions, we are encoding the 'semantic basis' of word usage information with a lexical item." ></td>
	<td class="line x" title="82:168	This gives rise to semantic collocations." ></td>
	<td class="line x" title="83:168	(2) Cospecification: This is the basic means of encoding specific usage information in the form of either coherent argument subtypes, or already lexicalized phrases, giving rise to idiosyncrasies and idioms, respectively." ></td>
	<td class="line x" title="84:168	4 Adjectival Semantic Collocations within GLT 4.1 The Semantics of Nominals We illustrate these theoretical notions with some examples for nominals 5, paying particular attention to 'covert-relational nominals ''6, that is, those exhibiting a logical polysemy." ></td>
	<td class="line x" title="85:168	We only present partial entries, which however exhibit semantic information distributed among the qualia, thus allowing the prediction of semantic collocations as will be shown in 4.2." ></td>
	<td class="line x" title="86:168	We give some realizations for beer and writer and discuss their representationsZ: South African Breweries Ltd. , or SAB, the country's largest producer of beer, was hit by a strike at seven of its 11 breweries around the country." ></td>
	<td class="line x" title="87:168	'I am a beer-drinker with a running problem,' one hash lapel button re~tds." ></td>
	<td class="line x" title="88:168	beer ARGSTR = \[ ARGI = X: beverage \] \[ liquid-LCP \] IFORM = beer-liquid(x) | QUALIA = |.TELIC = drink(P,v : individual,x) | \[AGENT= produce(T,w:brewer,x) J Ms. Rifkind is a writer and editor living in New York." ></td>
	<td class="line x" title="89:168	Mr. Ferguson is an editorial writer for Scripps Howard News Service in Washington, D.C. writer ARGSTR = \[ ARGI = x:author'\[ | human-LCP \] J = |FORM = htman(x) L QUALIA L TELIC = write(T,x,v:text) 5For a broader account of the semantic interpretation of nominals, including nominalizations, see Pustejovsky and Anick (1988)." ></td>
	<td class="line x" title="90:168	sWe use 'covert' to differentiate traditional relational nominals (such as/fiend, father, cousin), from the class of nouns . which exhibit a polysemous behaviour (such as book, door, record}." ></td>
	<td class="line x" title="91:168	7We mainly use the approach to typed feature structures as described in Carpenter (1992)." ></td>
	<td class="line x" title="92:168	We cannot develop here the way the information is inherited in the partial lexical entries presented." ></td>
	<td class="line x" title="93:168	94 7th International Generation Workshop  Kennebunkport, Maine  June 21-24, 1994 The argument structure of nouns encodes arguments which are to be taken as logical parameters providing type information for lexical items as discussed in (Pustejovsky, 1994a)." ></td>
	<td class="line x" title="94:168	The predicates 'drink', 'produce', and 'write', are the defaults we find in the qualia of beer and writer respectively." ></td>
	<td class="line x" title="95:168	It is still possible to create the semantic space which these predicates belong to, through the projective inheritance mechanism." ></td>
	<td class="line x" title="96:168	In the cases of covert-relational nominals, exhibiting semantic polysemy, we argue that they have actually well-defined calculi." ></td>
	<td class="line x" title="97:168	If we look at examples (1): (1) a. This book is heavy to carry around." ></td>
	<td class="line x" title="98:168	(physical object) b. I read an angry book." ></td>
	<td class="line x" title="99:168	(text) c. This book is great!" ></td>
	<td class="line x" title="100:168	(text and/or physical object) (la) and (lb) illustrate the polysemy between the physical object and the notion of text, whereas (lc) can either refer to one or both aspects within the same sentence." ></td>
	<td class="line x" title="101:168	Traditional approaches, from transformational grammars to classical Montague grammars, account for this lexical ambiguity by postulating different entries per lexical item." ></td>
	<td class="line x" title="102:168	These fail to capture the core semantics of the lexical items, leaving the complemenlary s senses unrelated." ></td>
	<td class="line x" title="103:168	Following Pustejovsky (1994b) we suggest that covert relational nominals should have a relational structure, thus capturing polysemy within the lexical structure." ></td>
	<td class="line x" title="104:168	For the purpose of clarity We only give a partial representation of book below: 'book ARGSTR = QUALIA = ARG1 = x:text \] ARG2 y:paperj 'lnformat\]on-phys_obj-conta\]ner-LCP' FORM = book.hold(y,x) TELIC = read(T,w:individual,x), publish(T,v:publisher,y) AGENT = write(T,u:writer,x), print(T,z:printer,y) Briefly, this states that book inherits from the relational information-physical_object-container-Lcp, although imposing additional constraints of its own, represented here as the arguments, namely ARG 1 and AR62." ></td>
	<td class="line x" title="105:168	Moreover, we have specified two defaults for the telic and agentive roles, each refering to one aspect of book, either text or physical_object." ></td>
	<td class="line x" title="106:168	The 8Weinreieh (1964) makes the distinction between contrastlvc and complementary ambiguity." ></td>
	<td class="line x" title="107:168	A noun such as record exhibits the former type between the readings written statement o\]\]acts and gramophone record or disc, and the latter between the complementary interpretation of physical object and musical content." ></td>
	<td class="line x" title="108:168	sorts publisher, writer, printer are organized hierarchically with individual as a common super-type." ></td>
	<td class="line x" title="109:168	This nominal representation enables us to capture all the complementary nominal 'polysemous' senses as expressed in the sentences: The writer began his third book (writing), my sister began 'The Bostonians' (reading); the binder finished the books for CUP (binding), etc. The values of these qualia are typed and are accessible to semantic operations in composition with other phrases." ></td>
	<td class="line x" title="110:168	One aspect of nominal representation to be captured with this formalism is the paradigmatic behavior of a lexical item in the syntax, and help understanding the processes involved in lexical selection tasks." ></td>
	<td class="line x" title="111:168	In the next section, we address the issue of selection within the NP, and show the utility of having qualia structure associated with nouns and adjectives for compositional purposes, focusing on semantic collocations." ></td>
	<td class="line x" title="112:168	4.2 Adj-Noun Interpretation Within the approach taken here, adjectives, depending on their types, will be able to modify not only the arguments of the argument structure of the nouns (ARGSTR), but also the arguments inside the agentive and the telic roles." ></td>
	<td class="line x" title="113:168	As the information in the qualia is specific to the noun and as the same adjective can modify different roles, it is possible to deal with the polysemous behavior of adjectives and to provide a generative explanation of semantic collocations." ></td>
	<td class="line x" title="114:168	Very briefly, an adjective selects for a particular type, an event or an object." ></td>
	<td class="line x" title="115:168	When it modifies an object, it selects for a particular semantic type (person, vehicle, information, etc.)." ></td>
	<td class="line x" title="116:168	When it takes an event, it can be restricted to a special type (process, event, transition) or role (agentive or telic)." ></td>
	<td class="line x" title="117:168	If the noun does not have in its argument structure the type required by the adjective, generative mechanisms can exploit the richness of typing of the qualia and generate the required type (Pustejovsky, 1994a), if it is available in the qualia and if common sense knowledge is respected." ></td>
	<td class="line x" title="118:168	In this case, the adjective will only modify one part of the qualia (i.e. of the denotation) of the noun." ></td>
	<td class="line x" title="119:168	Consider, for example, the French adjectives intelligent (clever) and lriste (sad) in (2)." ></td>
	<td class="line x" title="120:168	We give, for each example, the English literal translations (lit." ></td>
	<td class="line x" title="121:168	tr.): (2) a. un homme intelligent/triste; (lit." ></td>
	<td class="line x" title="122:168	tr." ></td>
	<td class="line x" title="123:168	a clever/sad man) b. des yeuz intelligents/tristes; (lit." ></td>
	<td class="line x" title="124:168	tr." ></td>
	<td class="line x" title="125:168	clever/sad eyes) ~ which show the cleverness/sadness of the person in question c. un livre intelligent/triste; (lit." ></td>
	<td class="line x" title="126:168	tr." ></td>
	<td class="line x" title="127:168	clever/sad book) ~ book which shows the cleverness/sadness of the person who writes the book 95 7th International Generation Workshop  Kennebunkport, Maine  June 21-24, 1994 d. un livre intelligent/triste ~ book which causes the *cleverness/sadness of the person who reads it e. un sapin triste; (lit." ></td>
	<td class="line x" title="128:168	tr." ></td>
	<td class="line x" title="129:168	a sad fir-tree) ---~ *fir-tree that causes the sadness of the person who . . ." ></td>
	<td class="line x" title="130:168	f. nne voiture triste; (lit." ></td>
	<td class="line x" title="131:168	tr." ></td>
	<td class="line x" title="132:168	a sad car) ~ *car that causes the sadness of the person who constructs it g. une robe triste; (lit." ></td>
	<td class="line x" title="133:168	tr." ></td>
	<td class="line x" title="134:168	a sad dress) ---~ *that causes the sadness of the person who wears it These adjectives select for an object of type person (as shown in (2a)): triste ARCl \[\] person \[ change.state-LeP 1 QUALIA \[ FORM =,,i. ,~C~ s. \[\] ) J In (2bcd), despite the apparent violation of types, the modification is possible, because the qualia of the noun makes explicit different relations between the type person selected by the adjective (Type-Adj) and the noun (N), as:  (N) is a constitutive element of (Type-Adj) (example (2b))  the telic stipulates that (Type-Adj) uses (N) (example (2d))  the agentive stipulates that (N) is produced by (Type-gdj) (example (2c)) It must be clear that this kind of modification is only possible if the relations are defined in the qualia." ></td>
	<td class="line x" title="135:168	The sentence (2e), for example, is semantically difficult, as the word sapin, as a natural kind, has no telic or agentive roles (independently of particular contexts)." ></td>
	<td class="line x" title="136:168	The modication must also respect very general common sense knowledge: in (2e) and (2g), the readings  a book that causes the cleverness of the person who reads it (2e) and *a dress that causes the sadness of the person who wears it (2g) is blocked by common sense principles, like:  cleverness cannot be communicated, unlike sadness  there must be a direct causal link between the event expressed in the telic/agentive role and the sadness of the individual." ></td>
	<td class="line x" title="137:168	This link does not relate in our societies sadness and wearing a particular dress or building a car." ></td>
	<td class="line x" title="138:168	Take now the case of long." ></td>
	<td class="line x" title="139:168	This adjective, in one of its senses, modifies an event transition, whose it indicates the temporal duration, as shown in the examples (3): (3) a. le long voyage (the long trip) b. nn long livre (a long book) ---~ whose reading/writing is long It will therefore receive the following entry: I long QUALIA I dimensin-LCP \] \[FORM =, ,(~) J (3b) is therefore possible because events are defined in the qualia of the noun livre." ></td>
	<td class="line x" title="140:168	Again, un long sapin has no event reading, because there is no event available in the qualia of the noun sapin." ></td>
	<td class="line x" title="141:168	The adjectives ancient and former are also event submodifiers, distinguished by the role they modify." ></td>
	<td class="line x" title="142:168	Ancient is a relative adjective that submodifies the agentive role of the modified noun: ancient oRM=,,  In this view, ancient stories (in example (3)) are stories which were narrated in the past, so: distant_past (e r) A narrate(e T, z, stories) By contrast, the English adjective former is a property modifier and can only modify the telic role of the noun: former QUALIA \[changestat e-LOP \[FORM = p ,(~) A former architect is a person who performed his job in the past 9, so:." ></td>
	<td class="line x" title="143:168	past(e P) A perform_the_job_of_architect (e P, z) In French, two adjectives with the same meaning past can modify these two roles: ancien and vieuz, which will receive the following feature structure (which does not deal with the absolute sense): vleux QUALIA \[change_st ate-LCP \] LFORM = p-.,(\[~) J 9Let P be any predicate, from the qualla of the noun, and < ei >, a set of ordered events; the semantics associated to past is then the following: past(el) AP(el,x,y) A-~P(e2,x,y) Anow(e2) Ae I < e2." ></td>
	<td class="line x" title="145:168	96 7th International Generation Workshop  Kennebunkport, Maine  June 21-24, 1994 That is not to say that these two adjectives will be ambiguous in context." ></td>
	<td class="line x" title="146:168	We show elsewhere (Bouillon and Viegas, 1994) that the interpretation of the adjective can be influenced by the context or morphological and syntactical constraints as the place of the French adjective, the type of the determiner or the typography (hyphen or quotes)." ></td>
	<td class="line x" title="147:168	Within this approach, semantic collocations can be therefore computed in the same way as other AdjNoun constructions and do not need to be listed in the dictionary." ></td>
	<td class="line x" title="148:168	5 Perspectives for NLG With GLT, we can generate dynamically the set of possible semantic collocations." ></td>
	<td class="line x" title="149:168	This can be done incrementally, as we make available the set of possible choices at run-time, a set which will be constrained by the situational and/or contextual environment." ></td>
	<td class="line x" title="150:168	Suppose that we are generating Adj-Noun constructions from logical forms." ></td>
	<td class="line x" title="151:168	From a structure like the following: By, z, e T livre(y) A lire(e T, x, y) A long(e T) we can generate two sentences: the non-collocational one un livre long h life (lit." ></td>
	<td class="line x" title="152:168	tr." ></td>
	<td class="line x" title="153:168	a book long to read) and the collocational one un long livre (a long book), because the entries of the noun and the adjective in GL specify that this combination is possible." ></td>
	<td class="line x" title="154:168	In contrast, we will not be able to generate from the logical form below une robe triste (a Sad dress) with the meaning of a dress which makes me sad because this NP is blocked by the common sense principles evocated in the previous section." ></td>
	<td class="line x" title="155:168	3y, x, e T, e s robe(y) A porter(e T, x, y) A causer(e T, e S) A triste(e s, x) That is not to say that we can predict generatively all collocations." ></td>
	<td class="line x" title="156:168	Take the examples of Adj-Noun collocations involving grand and gros with nouns denoting activities: (4) a. un grand/gros mangeur (a big eater) b. un grand/gros fraudeur (a big smuggler) c. un *grand/gros client (a big client) d. un grand/*gros fumeur (a heavy smoker) e. un grand/*gros professeur (a great professor) Here, grand and gros are intensifiers of the predicate in the telic." ></td>
	<td class="line x" title="157:168	Un grandfumeur, for example, will receive the following interpretation : Az\[f umeur( x ) . . ." ></td>
	<td class="line x" title="158:168	\[Telic( z ) = AvAeP\[furner(e P, x, v: tabac) A grand(e P) \]\]\] We can predict that gros is intensifier of the quantitative aspect of the predicate while grand will modify both qualitative (4e) and quantitative aspects (4abcd), depending on the salience of these aspects in the predicate (we can assume that a professor is generally judged by the quality of his courses, while a smoker by the quantity of the smoking)." ></td>
	<td class="line x" title="159:168	What we cannot do is to predict which adjective will be used with preference for the quantitative aspects." ></td>
	<td class="line x" title="160:168	To deal with this set of idiosyncratic lexical cooccurrences and idioms, we must take the concept of collocational information a step further, with a theory of cospecification." ></td>
	<td class="line x" title="161:168	This takes advantage of linguistic, statistical and lexicographic approaches (see 2.2), but also adds the dimension of semantic typing, focusing on collocations as they relate to sortal selection." ></td>
	<td class="line x" title="162:168	For instance, the cospecifications associated with the predicates we find in the telic of book, namely read, has encoded sortal pairs, providing the privileged environment (or associations) for that word: 'read COSPECS = cosPEci \[ARol : id.idu \]1 ARG2 = information In the cases of grand fumeur versus gros mangeur, we know that the telic offumeur and mangeur (ruiner and manger) are predicates, denoting activities of type process, on which we can apply a scale (tr~s pen  beaucoup  dnormdment )." ></td>
	<td class="line x" title="163:168	The adjective which will express a point on the scale with a specific noun will be specified in the cospecifications (as below)." ></td>
	<td class="line x" title="164:168	In fact, both grand and gros can generally be understood, with sometimes a clear preference for one of these, depending of the term being modified." ></td>
	<td class="line x" title="165:168	This preference is modelled as a partial ordering ( ) over a type hierarchy < Cospec, E>, encoded in the cospecifications." ></td>
	<td class="line x" title="166:168	*mangeur COSPECS = COSPEC1 = SCALE = gros(e P) \] / COsPEC2 = SCALE= grand(eP)\] k RESTRICT = cospec, C cospecj, i < j 6 Conclusion By working within the framework of GLT (Pustejovsky, 1994c) we can go beyond the 'quarrel' between traditional and non-traditional architecture systems and still generate in an incremental way." ></td>
	<td class="line x" title="167:168	This is due to the richness of the Generative Lexicon which allows for mechanisms to create dynamically on one hand the triggered concepts (by means of the inheritance structures) and on the other hand to make the syntactico-semantic information available in the lin97 7th International Generation Workshop  Kennebunkport, Maine  June 21-24, 1994 guistic environment of words (by means of the argument, event, qualia structures; and the LCPs)." ></td>
	<td class="line x" title="168:168	In this sense we have shown that GLT can be seen as a promising cornerstone for generating the most adequate lexical items." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E95-1003
Criteria For Measuring Term Recognition
Lauriston, Andy;"></td>
	<td class="line x" title="1:150	Criteria for Measuring Term Recognition Andy Lauriston Department of Languages and Linguistics University of Manchester Institute of Science and Technology P.O. Box 88 Manchester M60 1QD United Kingdom andyl@ccl.umist.ac.uk Abstract This paper qualifies what a true termrecognition systems would have to recognize." ></td>
	<td class="line x" title="2:150	The exact bracketing of the maximal termform is then proposed as an achieveable goal upon which current system performance should be measured." ></td>
	<td class="line x" title="3:150	How recall and precision metrics are best adapted for measuring term recognition is suggested." ></td>
	<td class="line x" title="4:150	1 Introduction In recent years, the automatic extraction of terms from running text has become a subject of growing interest." ></td>
	<td class="line x" title="5:150	Practical applications such as dictionary, lexicon and thesaurus construction and maintenance, automatic indexing and machine translation have fuelled this interest." ></td>
	<td class="line x" title="6:150	Given that concerns in automatic term recognition are practical, rather than theoretical, the lack of serious performance measurements in the published literature is surprising." ></td>
	<td class="line x" title="7:150	Accounts of term-recognition systems sometimes consist of a purely descriptive statement of the advantages of a particular approach and make no attempt to measure the pay-off the proposed approach yields (David, 1990)." ></td>
	<td class="line x" title="8:150	Others produce partial figures without any clear statement of how they are derived (Otman, 1991)." ></td>
	<td class="line oc" title="9:150	One of the best efforts to quantify the performance of a term-recognition system (Smadja, 1993) does so only for one processing stage, leaving unassessed the text-to-output performance of the system." ></td>
	<td class="line x" title="10:150	While most automatic term-recognition systems developed to date have been experimental or inhouse ones, a few systems like TermCruncher (Normand, 1993) are now being marketed." ></td>
	<td class="line x" title="11:150	Both the developers and users of such systems would benefit greatly by clearly qualifying what each system aims to achieve, and precisely quantifying how closely the system comes to achieving its stated aim." ></td>
	<td class="line x" title="12:150	Before discussing what a term-recognition system should be expected to recognize and how performance in recognition should be measured, two underlying premises should be made clear." ></td>
	<td class="line x" title="13:150	Firstly, the automatic system is designed to recognize segments of text that, conventionally, have been manually identified by a terminologist, indexer, lexicographer or other trained individual." ></td>
	<td class="line x" title="14:150	Secondly, the performance of automatic term-recognition systems is best measured against human performance for the same task." ></td>
	<td class="line x" title="15:150	These premises mean that for any given application terminological standardization and vocabulary compilation being the focus here it is possible to measure the performance of an automatic term-recognition system, and the best yardstick for doing so is human performance." ></td>
	<td class="line x" title="16:150	Section 2 below draws on the theory of terminology in order to qualify what a true term-recognition system must achieve and what, in the short term, such systems can be expected to achieve." ></td>
	<td class="line x" title="17:150	Section 3 specifies how the established ratios used in information retrieval recall and precision can best be adapted for measuring the recognition of singleand multi-word noun terms." ></td>
	<td class="line x" title="18:150	2 What is to be Recognized?" ></td>
	<td class="line x" title="19:150	Depending upon the meaning given to the expression 'term recognition', it can be viewed as either a rather trivial, low-level processing task or one that is impossible to automate." ></td>
	<td class="line x" title="20:150	A limited form of term recognition has been achieved using current techniques (Pcrron, 1991; Bourigault, 1994; Normand, 1993)." ></td>
	<td class="line x" title="21:150	To appreciate what current limitations are and what would be required to achieve full term recognition, it is useful to draw the distinction between 'term' and 'termform' on the one hand, and 'term recognition' and 'term interpretation' on the other." ></td>
	<td class="line x" title="22:150	2.1 Term vs Termform Particularly in the computing community, there is a tendency to consider 'terms' as strictly formal entities." ></td>
	<td class="line x" title="23:150	Although usage among terminologists varies, a term is generally accepted as being the 'designation of a defined concept in a special language by a linguistic expression' (ISO, 1988)." ></td>
	<td class="line x" title="24:150	A term is hence 17 II Concept II II II II I TERM I II I Termform I f I Figure 1: Term vs Termform the intersection between a conceptual realm (a defined semantic content) and a linguistic realm (an expression or termform) as illustrated in Figure 1." ></td>
	<td class="line x" title="25:150	A term, thus conceived, cannot be polysemous although termforms can, and often d% have several meanings." ></td>
	<td class="line x" title="26:150	As terms precisely defined in information processing, 'virus' and 'Trojan Horse' are unambiguous; as termforms they have other meanings in medicine and Greek mythology respectively." ></td>
	<td class="line x" title="27:150	This view of a term has one very important consequence when discussing term recognition." ></td>
	<td class="line x" title="28:150	Firstly, term recognition cannot be carried out on purely formal grounds." ></td>
	<td class="line x" title="29:150	It requires some level of linguistic anMysis." ></td>
	<td class="line x" title="30:150	Indeed, two term-formation processes do not result in new termforms: conversion and semantic drift 1." ></td>
	<td class="line x" title="31:150	A third term-formation process, compression, can also result in a new meaning being associated with an existing termform 2." ></td>
	<td class="line x" title="32:150	Proper attention to capitalization can generally result in the correct recognition of compressed forms." ></td>
	<td class="line x" title="33:150	Part-of-speech tagging is required to detect new terms formed through conversion." ></td>
	<td class="line x" title="34:150	This is quite feasible using statistical taggers like those of Garside (1987), Church (1988) or Foster (1991) which achieve performance upwards of 97% on unrestricted text." ></td>
	<td class="line x" title="35:150	Terms formed through semantic drift are the wolves in sheep's clothing stealing through terminological pastures." ></td>
	<td class="line x" title="36:150	They are well enough conceMcd to allude at times even the human reader and no automatic term-recognition system has attempted to distinguish such terms, despite the prevalence ofpolysemy in such fields as the social sciences (R.iggs, 1993) and the importance for purposes of terminological standardization that 'deviant' usage be tracked." ></td>
	<td class="line x" title="37:150	Implementing a system to distinguish new 1Conversion occurs when a term is formed by a change in grammatical category." ></td>
	<td class="line x" title="38:150	Verb-to-noun conversion commonly occurs for commands in programming or word processing (e.g. Undelete works if you catch your mistake quickly)." ></td>
	<td class="line x" title="39:150	Semantic drift involves a (sometimes subtle) change in meaning without any change in grammatical category (viz." ></td>
	<td class="line x" title="40:150	'term' as understood in this paper vs the loose ~Jsage of '~etm' to mc~n 'termform')." ></td>
	<td class="line x" title="41:150	2Compression is the shortening of (usually complex) termforms to form acronyms or other initialisms." ></td>
	<td class="line x" title="42:150	Thus PAD can either designate a resistive loss in an electrical circuit or a 'packet assembler-disassembler'." ></td>
	<td class="line x" title="43:150	meanings of established termforms would require analyzing discourse-level clues that an author is assigning a new meaning, and possibly require the application of pragmatic knowledge." ></td>
	<td class="line x" title="44:150	Until such advanced levels of analysis can be practically implemented, 'term recognition' will largely remain 'termform recognition' and the failure to detect new terms in old termforms will remain a qualitative shortcoming of all term-recognition systems." ></td>
	<td class="line x" title="45:150	2.2 Term Recognition vs Term Interpretation The vast majority of terms in published technical dictionaries and terminology standards are nouns." ></td>
	<td class="line x" title="46:150	Furthermore, most terms have a complex termform, i.e. they are comprise~t of more than one word." ></td>
	<td class="line x" title="47:150	Sublanguages create series of complex termforms in which complex forms serve as modifiers (natural language ~ \[natural language\] processing) and/or are themselves modified (applied \[\[natural language\] processing\])." ></td>
	<td class="line x" title="48:150	In special language, complex termforms containing nested termforms, or significant subexpressions (Baudot, 1984), have hundreds of possible syntagmatic structures (Portelance, 1989; Lauriston, 1993)." ></td>
	<td class="line x" title="49:150	The challenge facing developers of term-recognition systems consists in determining the syntactic and conceptual unity that complex nominals must possess in order to achieve termhood 3 Another, and it will be argued far more ambitious, undertaking is term interpretation." ></td>
	<td class="line x" title="50:150	Leonard (1984), Finen (1985) and others have attempted to devise systems that can produce a gloss explicating the semantic relationship that holds between the constituents of complex nominals (e.g. family estate ~ estate owned by a family)." ></td>
	<td class="line x" title="51:150	Such attempts at achieving even limited 'interpretation' result in large sets of possible relationships but fail to account for all compounds." ></td>
	<td class="line x" title="52:150	Furthermore, they have generally been restricted to termforms with two constituents." ></td>
	<td class="line x" title="53:150	For complex termforms with three or more constituents, merely identifying how constituents are nested, i.e., between which constituents there exists a semantic relationship, can be difficult to automate (Sparck-:lones, 1985)." ></td>
	<td class="line x" title="54:150	In most cases, however, term recognition can be achieved without interpreting the meaning of the term and without analyzing the internal structure of complex termforms." ></td>
	<td class="line x" title="55:150	Many term-recognition systems like TERMINO (David, 1990), the noun-phrase detector of LOGOS (Logos, 1987), LEXTER (Bourigault, 1994), etc. , nevertheless attempt to recognize nested termforms." ></td>
	<td class="line x" title="56:150	Encountering 'automatic protection switching equipment', systems adopting this Sin this respect, complex termforms, unlike collocations, must designate definable nodes of the conceptual system of an area of specialized human activity." ></td>
	<td class="line x" title="57:150	Hence general trend may be as strong a collocation as general election, and yet only the latter be considered a term." ></td>
	<td class="line x" title="58:150	18 approach would produce as output several nested termforms (switching equipment, protection switching, protection switching equipment, automatic protection, automatic protection switching) as well as the maximal termform automatic protection switching equipment." ></td>
	<td class="line x" title="59:150	Because such systems list nested termforms in the absence of higher-level analysis, many erroneous 'terms' are generated." ></td>
	<td class="line x" title="60:150	It has been argued previously on pragmatic grounds (Lauriston, 1994) that a safer approach is to detect only the maximal termform." ></td>
	<td class="line x" title="61:150	It could further be said that doing so is theoretically sound." ></td>
	<td class="line x" title="62:150	Nesting termforms is a means by which an author achieves transparency." ></td>
	<td class="line x" title="63:150	Once nested, however, a termform no longer fulfills the naming function." ></td>
	<td class="line x" title="64:150	It serves as a mnemonic device." ></td>
	<td class="line x" title="65:150	In different languages, different nested termforms are sometimes selected to perform this mnemonic function (e.g. on-line credit card checking, for which a documented French equivalent is vdrification de crddit au point de vente, literally 'point-of-sale credit verification')." ></td>
	<td class="line x" title="66:150	Only the maximal termform refers to the designated concept and thus only recognition of the maximal termform constitutes term recognition 4." ></td>
	<td class="line x" title="67:150	Term interpretation may be required, however~ to correctly delimit complex termforms combined by means of conjunctions." ></td>
	<td class="line x" title="68:150	Consider the following three conjunctive expressions taken from telecommunication texts: (1) buffer content and packet delay distributions (2) mean misframe and frame detection times (3) generalized intersymbol-interference and jitterfree modulated signals Even the uninitiated reader would probably be inclined to interpret, correctly, that expression (1) is a combination of two complex termforms: buffer content distribution and packet delay distribution." ></td>
	<td class="line x" title="69:150	Syntax or coarse semantics do nothing, however, to prevent an incorrect reading: buffer content delay distribution and buffer packet delay distribution." ></td>
	<td class="line x" title="70:150	Expression (2) consists of words having the same sequence of grammatical categories as expression (1), but in which this second reading is, in fact, correct: mean misframe detection time and mean frame detection time." ></td>
	<td class="line x" title="71:150	Although rather similar to the first two, conjunctive expression (3) is a single term, sometimes designated by the initialism GIJF." ></td>
	<td class="line x" title="72:150	Complex termforms appearing in conjunctive expressions may thus require term interpretation for proper term recognition, i.e. reconstructing the conjuncts." ></td>
	<td class="line x" title="73:150	If term recognition is to be carried out independently of and prior to term interpretation, as is 'This does not imply that analyzing the internal structure of complex termforms is valueless." ></td>
	<td class="line x" title="74:150	It has the very important, but distinct, value of prodding clues to paradigmatic relationships between terms." ></td>
	<td class="line x" title="75:150	presently feasible, then it can only be properly seen as 'maximal termform recognition' with the meaning of 'maximal termform' extended to include the outermost bracketing of structurally ambiguous conjunctive expressions like the three examples above." ></td>
	<td class="line x" title="76:150	This extension in meaning is not a matter of theoretical soundness but simply of practical necessity." ></td>
	<td class="line x" title="77:150	In summary, current systems recognize termforms but lack mechanisms to detect new terms resulting from several term-formation processes, particularly semantic drift." ></td>
	<td class="line x" title="78:150	Under these circumstances, it is best to admit that 'termform recognition' is the currently feasible objective and to measure performance in achieving it." ></td>
	<td class="line x" title="79:150	Furthermore, since the nested structures of complex termforms perform a mnemonic rather than a naming function, it is theoretically unsound for an automatic term-recognition system to present them as terms." ></td>
	<td class="line x" title="80:150	For purposes of measurement and comparison, 'term recognition' should thus be regarded as 'maximal termform recognition'." ></td>
	<td class="line x" title="81:150	Once this goal has been reliably achieved, the output of a term-recognition system could feed a future 'term interpreter', that would also be required to recognize terms in ambiguous conjunctive expressions." ></td>
	<td class="line x" title="82:150	3 How Can Recognition be Measured?" ></td>
	<td class="line x" title="83:150	Once a consensus has been reached about what is to be recognized, there must be some agreement concerning the way in which performance is to be measured." ></td>
	<td class="line x" title="84:150	Fortunately, established performance measurements used in information retrieval recall and precision can be adapted quite readily for measuring the term-recognition task." ></td>
	<td class="line oc" title="85:150	These measures have, in fact, been used previously in measuring term recognition (Smadja, 1993; Bourigault, 1994; Lauriston, 1994)." ></td>
	<td class="line n" title="86:150	No study, however, adequately discusses how these measurements are applied to term recognition." ></td>
	<td class="line x" title="87:150	3.1 Recall and Precision Traditionally, performance in document retrieval is measured by means of a few simple ratios (Salton, 1989)." ></td>
	<td class="line x" title="88:150	These are based on the premise that any given document in a collection is either pertinent or non-pertinent to a particular user's needs." ></td>
	<td class="line x" title="89:150	There is no scale of relative pertinence." ></td>
	<td class="line x" title="90:150	For a given user query, retrieving a pertinent document constitutes a hit, failing to retrieve a pertinent document constitutes a miss, and retrieving a non-pertinent document constitutes a false hit." ></td>
	<td class="line x" title="91:150	Recall, the ratio of the number of hits to the number of pertinent documents in the collection, measures the effectiveness of retrieval." ></td>
	<td class="line x" title="92:150	Precision, the ratio of the number of hits to the number of retrieved documents, measures the e~iciency of retrieval." ></td>
	<td class="line x" title="93:150	The complement of recall is omission (misses/total pertinent)." ></td>
	<td class="line x" title="94:150	The complement of precision is noise (false hits/total retrieved)." ></td>
	<td class="line x" title="95:150	19 Ideally, recall and precision would equal 1.0, omission and noise 0.0." ></td>
	<td class="line x" title="96:150	Practical document retrieval involves a trade-off between recall and precision." ></td>
	<td class="line x" title="97:150	The performance measurements in document retrieval are quite apparently applicable to term recognition." ></td>
	<td class="line x" title="98:150	The basic premise of a pertinent/nonpertinent dichotomy, which prevails in document retrieval, is probably even better justified for terms than for documents." ></td>
	<td class="line x" title="99:150	Unlike an evaluation of the pertinence of the content of a document, the term/nonterm distinction is based on a relatively simple and cohesive semantic contentS.User judgements of document pertinence would appear to be much more subjective and difficult to quantify." ></td>
	<td class="line x" title="100:150	If all termforms were simple, i.e. single words, and only simple termforms were recognized, then using document retrieval measurements would be perfectly.straightforward." ></td>
	<td class="line x" title="101:150	A manually bracketed term would give rise to a hit or a miss and an automatically recognized word would be a hit or a false hit." ></td>
	<td class="line x" title="102:150	Since complex termforms are prevalent in sublanguage texts, however, further clarification is necessary." ></td>
	<td class="line x" title="103:150	In particular, 'hit' has to be defined more precisely." ></td>
	<td class="line x" title="104:150	Consider the following sentence: The latest committee draft reports progress toward constitutional reform." ></td>
	<td class="line x" title="105:150	A terminologist would probably recognize two terms in this sentence: commiLtee draft and constitutional reform." ></td>
	<td class="line x" title="106:150	The termform of each is complex." ></td>
	<td class="line x" title="107:150	Regardless of whether symbolic or statistical techniques are used, 'hits' of debatable usefulness are apt to be produced by automatic term-recognition systems." ></td>
	<td class="line x" title="108:150	A syntactically based system might have particular difficulty with the three consecutive cases of noun-verb ambiguity draft, reports, progress." ></td>
	<td class="line x" title="109:150	A statistically based system might detect draft reports, since this cooccurrence might well be frequent as a termform elsewhere in the text." ></td>
	<td class="line x" title="110:150	Consequently, the definition of 'hit' needs further qualification." ></td>
	<td class="line x" title="111:150	3.2 Perfect and Imperfect Recognition Two types of hits must be distinguished." ></td>
	<td class="line x" title="112:150	A perfect hit occurs when the boundaries assigned by the term-recognition system coincide with those of a term's maximal termform (\[committee draft\] and \[constitutional reform\] above)." ></td>
	<td class="line x" title="113:150	An imperfect hit occurs when the boundaries assigned do not coincide with those of a term's maximal termform but contain at least one wordform belonging to a term's maximal termform." ></td>
	<td class="line x" title="114:150	A hit is imperfect if bracketing either indudes spurious wordforms (\[latest committee draft\] Sln practice, terminologists have some difficulty agreeing on the exact delimitation of complex termforms." ></td>
	<td class="line x" title="115:150	Still five experienced terminologists scanning a 2,861 word text were found to agree on the identity and boundsties of complex termforms three-quarters of the time (Lauriston, 1993)." ></td>
	<td class="line x" title="116:150	TARGET TERMFORMS misses RECOGNIZED TEKMFOKMS ~ false perfect Jl hits hits II ?<=limperfect hitst=>?" ></td>
	<td class="line x" title="117:150	II recall = hits: perfect (+ imperfect)?" ></td>
	<td class="line x" title="118:150	target termforms precision = hits: perfect + (imperfect)?" ></td>
	<td class="line x" title="119:150	recognized t ermforms Figure 2: Recall, Precision and Imperfect Hits or \[committee draft reports\]), fails to bracket a term constituent (committee \[draft\])or both (committee \[draft reports\])." ></td>
	<td class="line x" title="120:150	Bracketing a segment containing no wordform that is part of a term's maximal termform is, of course, a false hit (\[reports progress\])." ></td>
	<td class="line x" title="121:150	The problematic case is clearly that of an imperfect hit." ></td>
	<td class="line x" title="122:150	In calculating recall and precision, should imperfect hits be grouped with perfect hits, counted as misses, or somehow accounted for separately (Figure 2)?" ></td>
	<td class="line x" title="123:150	How do the perfect recall and precision ratios compare with imperfect recall and precision (including imperfect hits in the numerator) when these performance measurements are applied to real texts?" ></td>
	<td class="line x" title="124:150	Counting imperfectly recognized termforms as hits will obviously lead to higher ratios for recall and precision, but how much higher?" ></td>
	<td class="line x" title="125:150	To answer these questions, a complex-termform recognition algorithm based on weighted syntactic term-formation rules, the details of which are given in Lauriston (1993), was applied to a tagged 2,861 word text." ></td>
	<td class="line x" title="126:150	The weightings were based on the analysis of a 117,000 word corpus containing 11,614 complex termforms as determined by manual bracketing." ></td>
	<td class="line x" title="127:150	The recognition algorithm includes the possibility of weighting of the terminological strength of particular adjectives." ></td>
	<td class="line x" title="128:150	This was carried out to produce the results shown in Figure 3." ></td>
	<td class="line x" title="129:150	Recall and precision, both perfect and imperfect, were plotted as the algorithm's term-recognition threshold was varied." ></td>
	<td class="line x" title="130:150	By choosing a higher threshold, only syntactically stronger links between adjacent words are considered 'terminological links'." ></td>
	<td class="line x" title="131:150	Thus the higher the threshold, the shorter the average complex termform, as weaker modifiers are 20 1.0+ 0.9+ 08+ O7+ 06+ 05+ 04+ 03+ 02+ 01+ 00+ r r r r r r r r r r p r r r p r p r r r p r p r r r p r p Rr Rr p r p r p Rr p Rr p r p r p Rr p Rr p r p r p Rr p Rr p r p r p Rr p Rr p r p r p Rr Pp Rr Pp Rr Pp r p Rr Pp Rr Pp Rr Pp r p Rr Pp Rr Pp Rr Pp r p Rr Pp Rr Pp Rr Pp Rr Pp Rr Pp Rr Pp Rr Pp Rr Pp Rr Pp Rr Pp Rr Pp Rr Pp Rr Pp Rr Pp Rr Pp Rr Pp Rr Pp Rr Pp Rr Pp Rr Pp + + + + 0.05 0.40 0.75 0.95 term-recognition threshold KEY: R perfect recall (perfect hits only) r imperfect recall (imperfect also) P perfect precision (perfect hits only) p imperfect precision (imperfect also) Figure 3: Effect of Imperfect Hits of Performance Ratios stripped from the nucleus." ></td>
	<td class="line x" title="132:150	Lower recall and higher precision can be expected as the threshold rises since only constituents that are surer bets are included in the maximal termform." ></td>
	<td class="line x" title="133:150	This Figure 3 shows that both recall and precision scores are considerably higher when imperfect hits are included in calculating the ratios." ></td>
	<td class="line x" title="134:150	As expected, raising the threshold results in lower recall regardless of whether the ratios are calculated for perfect or imperfect recognition." ></td>
	<td class="line x" title="135:150	There is a marked reduction in perfect recall, however, and only a marginal reduction in imperfect recall." ></td>
	<td class="line x" title="136:150	The precision ratios provide the most interesting point of comparison." ></td>
	<td class="line x" title="137:150	As the threshold is raised, imperfect precision increases just as the principle of recall-precision tradeoff in document retrieval would lead one to expect." ></td>
	<td class="line x" title="138:150	Perfect precision, on the other hand, actually declines slightly." ></td>
	<td class="line x" title="139:150	The difference between perfect and imperfect precision (between the P-bar and p-bar in each group) increases appreciably as the threshold is raised." ></td>
	<td class="line x" title="140:150	This difference is due to the greater number of recognized complex termforms either containing spurious words or only part of the maximal termform." ></td>
	<td class="line x" title="141:150	Two conclusions can be drawn from Figure 3." ></td>
	<td class="line x" title="142:150	Firstly, the recognition algorithm implemented is poor at perfect recognition (perfect recall ~, 0.70; perfect precision ~, 0.40) and only becomes poorer as more stringent rule-weighting is applied." ></td>
	<td class="line x" title="143:150	Secondly, and more importantly for the purpose of this paper, Figure 3 shows that allowing for imperfect bracketing in term recognition makes it possible to obtain artificially high performance ratios for both recall and precision." ></td>
	<td class="line x" title="144:150	Output that recognizes almost all terms but includes spurious words in complex termforms or fails short of recognizing the entire termform leaves a burdensome filtering task for the human user and is next to useless if the 'user' is another level of automatic text processing." ></td>
	<td class="line x" title="145:150	Only the exact bracketing of the maximal termform provides a useful standard for measuring and comparing the performance of term-recognition systems." ></td>
	<td class="line x" title="146:150	4 Conclusion The term-recognition criteria proposed above measuring recall and precision for the exact bracketing of maximal termformsprovide a basic minimum of information needed to assess system performance." ></td>
	<td class="line x" title="147:150	For some applications, it is useful to further specify how these performance ratios differ for the recognition of simple and complex termforms, how they vary for terms resulting from different term-formation processes, what the ratios are for termform types as opposed to tokens, or how well the system recognizes novel termforms not already in a system lexicon or previously encountered in a training corpus." ></td>
	<td class="line x" title="148:150	Precision measurements might usefully state to what extent errors are due to syntactic noise (bracketing crossing syntactic constituents) as distinguished from terminological noise (bracketing including nonclassificatory modifiers or omitting classificatory ones)." ></td>
	<td class="line x" title="149:150	Publishing such performance results for termrecognition systems would not only display their strengths but also expose their weaknesses." ></td>
	<td class="line x" title="150:150	Doing so would ultimately benefit researchers, developers and users of term-recognltion systems." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P95-1007
Corpus Statistics Meet The Noun Compound: Some Empirical Results
Lauer, Mark;"></td>
	<td class="line x" title="1:186	Corpus Statistics Meet the Noun Compound: Some Empirical Results Mark Lauer Microsoft Institute 65 Epping Road, North Ryde NSW 2113 Australia t-marklmicrosoft, com Abstract A variety of statistical methods for noun compound anMysis are implemented and compared." ></td>
	<td class="line x" title="2:186	The results support two main conclusions." ></td>
	<td class="line x" title="3:186	First, the use of conceptual association not only enables a broad coverage, but also improves the accuracy." ></td>
	<td class="line x" title="4:186	Second, an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents, even though the latter is more prevalent in the literature." ></td>
	<td class="line x" title="5:186	1 Background 1.1 Compound Nouns If parsing is taken to be the first step in taming the natural language understanding task, then broad coverage NLP remains a jungle inhabited by wild beasts." ></td>
	<td class="line x" title="6:186	For instance, parsing noun compounds appears to require detailed world knowledge that is unavailable outside a limited domain (Sparek Jones, 1983)." ></td>
	<td class="line x" title="7:186	Yet, far from being an obscure, endangered species, the noun compound is flourishing in modern language." ></td>
	<td class="line x" title="8:186	It has already made five appearances in this paragraph and at least one diachronic study shows a veritable population explosion (Leonard, 1984)." ></td>
	<td class="line x" title="9:186	While substantial work on noun compounds exists in both linguistics (e.g. Levi, 1978; Ryder, 1994) and computational linguistics (Finin, 1980; McDonald, 1982; Isabelle, 1984), techniques suitable for broad coverage parsing remain unavailable." ></td>
	<td class="line x" title="10:186	This paper explores the application of corpus statistics (Charniak, 1993) to noun compound parsing (other computational problems are addressed in Arens el al, 1987; Vanderwende, 1993 and Sproat, 1994)." ></td>
	<td class="line x" title="11:186	The task is illustrated in example 1: Example 1 (a) \[womanN \[aidN workerN\]\] (b) \[\[hydrogenN ionN\] exchangeN\] The parses assigned to these two compounds differ, even though the sequence of parts of speech are identical." ></td>
	<td class="line x" title="12:186	The problem is analogous to the prepositional phrase attachment task explored in Hindle and Rooth (1993)." ></td>
	<td class="line x" title="13:186	The approach they propose involves computing lexical associations from a corpus and using these to select the correct parse." ></td>
	<td class="line x" title="14:186	A similar architecture may be applied to noun compounds." ></td>
	<td class="line x" title="15:186	In the experiments below the accuracy of such a system is measured." ></td>
	<td class="line x" title="16:186	Comparisons are made across five dimensions:  Each of two analysis models are applied: adjacency and dependency." ></td>
	<td class="line x" title="17:186	 Each of a range of training schemes are employed." ></td>
	<td class="line x" title="18:186	 Results are computed with and without tuning factors suggested in the literature." ></td>
	<td class="line x" title="19:186	 Each of two parameterisations are used: associations between words and associations between concepts." ></td>
	<td class="line x" title="20:186	 Results are collected with and without machine tagging of the corpus." ></td>
	<td class="line x" title="21:186	1.2 Training Schemes While Hindle and Rooth (1993) use a partial parser to acquire training data, such machinery appears unnecessary for noun compounds." ></td>
	<td class="line x" title="22:186	Brent (1993) has proposed the use of simple word patterns for the acquisition of verb subcategorisation information." ></td>
	<td class="line x" title="23:186	An analogous approach to compounds is used in Lauer (1994) and constitutes one scheme evaluated below." ></td>
	<td class="line x" title="24:186	While such patterns produce false training examples, the resulting noise often only introduces minor distortions." ></td>
	<td class="line x" title="25:186	A more liberal alternative is the use of a cooccurrence window." ></td>
	<td class="line x" title="26:186	Yarowsky (1992) uses a fixed 100 word window to collect information used for sense disambiguation." ></td>
	<td class="line oc" title="27:186	Similarly, Smadja (1993) uses a six content word window to extract significant collocations." ></td>
	<td class="line x" title="28:186	A range of windowed training schemes are employed below." ></td>
	<td class="line x" title="29:186	Importantly, the use of a window provides a natural means of trading off the amount of data against its quality." ></td>
	<td class="line x" title="30:186	When data sparseness undermines the system accuracy, a wider window may 47 admit a sufficient volume of extra accurate data to outweigh the additional noise." ></td>
	<td class="line x" title="31:186	1.3 Noun Compound Analysis There are at least four existing corpus-based algorithms proposed for syntactically analysing noun compounds." ></td>
	<td class="line x" title="32:186	Only two of these have been subjected to evaluation, and in each case, no comparison to any of the other three was performed." ></td>
	<td class="line x" title="33:186	In fact all authors appear unaware of the other three proposals." ></td>
	<td class="line x" title="34:186	I will therefore briefly describe these algorithms." ></td>
	<td class="line x" title="35:186	Three of the algorithms use what I will call the ADJACENCY MODEL, an analysis procedure that goes back to Marcus (1980, p253)." ></td>
	<td class="line x" title="36:186	Therein, the procedure is stated in terms of calls to an oracle which can determine if a noun compound is acceptable." ></td>
	<td class="line x" title="37:186	It is reproduced here for reference: Given three nouns nl, n2 and nz:  If either \[nl n2\] or In2 n~\] is not semantically acceptable then build the alternative structure;  otherwise, if \[n2 n3\] is semantically preferable to \[nl n2\] then build In2 nz\];  otherwise, build \[nl n2\]." ></td>
	<td class="line x" title="38:186	Only more recently has it been suggested that corpus statistics might provide the oracle, and this idea is the basis of the three algorithms which use the adjacency model." ></td>
	<td class="line x" title="39:186	The simplest of these is reported in Pustejovsky et al (1993)." ></td>
	<td class="line x" title="40:186	Given a three word compound, a search is conducted elsewhere in the corpus for each of the two possible subcomponents." ></td>
	<td class="line x" title="41:186	Whichever is found is then chosen as the more closely bracketed pair." ></td>
	<td class="line x" title="42:186	For example, when backup compiler disk is encountered, the analysis will be: Example 2 (a) \[backupN \[compilerN diskN\]\] when compiler disk appears elsewhere (b) \[\[backupN compilerN\] diskN\] when backup compiler appears elsewhere Since this is proposed merely as a rough heuristic, it is not stated what the outcome is to be if neither or both subcomponents appear." ></td>
	<td class="line x" title="43:186	Nor is there any evaluation of the algorithm." ></td>
	<td class="line x" title="44:186	The proposal of Liberman and Sproat (1992) is more sophisticated and allows for the frequency of the words in the compound." ></td>
	<td class="line x" title="45:186	Their proposal involves comparing the mutual information between the two pairs of adjacent words and bracketing together whichever pair exhibits the highest." ></td>
	<td class="line x" title="46:186	Again, there is no evaluation of the method other than a demonstration that four examples work correctly." ></td>
	<td class="line x" title="47:186	The third proposal based on the adjacency model appears in Resnik (1993) and is rather more complex again." ></td>
	<td class="line x" title="48:186	The SELECTIONAL ASSOCIATION between a predicate and a word is defined based on the contribution of the word to the conditional entropy of the predicate." ></td>
	<td class="line x" title="49:186	The association between each pair of words in the compound is then computed by taking the maximum selectional association from all possible ways of regarding the pair as predicate and argument." ></td>
	<td class="line x" title="50:186	Whilst this association metric is complicated, the decision procedure still follows the outline devised by Marcus (1980) above." ></td>
	<td class="line x" title="51:186	Resnik (1993) used unambiguous noun compounds from the parsed Wall Stree~ Journal (WSJ) corpus to estimate the association ~alues and analysed a test set of around 160 compounds." ></td>
	<td class="line x" title="52:186	After some tuning, the accuracy was about 73%, as compared with a baseline of 64% achieved by always bracketing the first two nouns together." ></td>
	<td class="line x" title="53:186	The fourth algorithm, first described in Lauer (1994), differs in one striking manner from the other three." ></td>
	<td class="line x" title="54:186	It uses what I will call the DEPENDENCY MODEL. This model utilises the following procedure when given three nouns at, n2 and n3:  Determine how acceptable the structures \[nl n2\] and \[nl n3\] are;  if the latter is more acceptable, build \[n2 nz\] first;  otherwise, build In1 rig.\] first." ></td>
	<td class="line x" title="55:186	Figure 1 shows a graphical comparison of the two analysis models." ></td>
	<td class="line x" title="56:186	In Lauer (1994), the degree of acceptability is again provided by statistical measures over a corpus." ></td>
	<td class="line x" title="57:186	The metric used is a mutual information-like measure based on probabilities of modification relationships." ></td>
	<td class="line x" title="58:186	This is derived from the idea that parse trees capture the structure of semantic relationships within a noun compound." ></td>
	<td class="line x" title="59:186	1 The dependency model attempts to choose a parse which makes the resulting relationships as acceptable as possible." ></td>
	<td class="line x" title="60:186	For example, when backup compiler disk is encountered, the analysis will be: Example 3 (a) \[backupN \[compilerN diskN\]\] when backup disk is more acceptable (b) \[\[backupN compilerN\] diskN\] when backup compiler is more acceptable I claim that the dependency model makes more intuitive sense for the following reason." ></td>
	<td class="line x" title="61:186	Consider the compound calcium ion exchange, which is typically left-branching (that is, the first two words are bracketed together)." ></td>
	<td class="line x" title="62:186	There does not seem to be any reason why calcium ion should be any more frequent than ion exchange." ></td>
	<td class="line x" title="63:186	Both are plausible compounds and regardless of the bracketing, ions are the object of an exchange." ></td>
	<td class="line x" title="64:186	Instead, the correct parse depends on whether calcium characterises the ions or mediates the exchange." ></td>
	<td class="line x" title="65:186	Another significant difference between the models is the predictions they make about the proportion 1Lauer and Dras (1994) give a formal construction motivating the algorithm given in Lauer (1994)." ></td>
	<td class="line x" title="66:186	48 L N2 t R Adjacency N3 t Prefer left-branching ig L is more acceptable than R L N1 N2 N3 t t R Dependency Figure 1: Two analysis models and the associations they compare of left and right-branching compounds." ></td>
	<td class="line x" title="67:186	Lauer and Dras (1994) show that under a dependency model, left-branching compounds should occur twice as often as right-branching compounds (that is twothirds of the time)." ></td>
	<td class="line x" title="68:186	In the test set used here and in that of Resnik (1993), the proportion of leftbranching compounds is 67% and 64% respectively." ></td>
	<td class="line x" title="69:186	In contrast, the adjacency model appears to predict a proportion of 50%." ></td>
	<td class="line x" title="70:186	The dependency model has also been proposed by Kobayasi et al (1994) for analysing Japanese noun compounds, apparently independently." ></td>
	<td class="line x" title="71:186	Using a corpus to acquire associations, they bracket sequences of Kanji with lengths four to six (roughly equivalent to two or three words)." ></td>
	<td class="line x" title="72:186	A simple calculation shows that using their own preprocessing hueristics to guess a bracketing provides a higher accuracy on their test set than their statistical model does." ></td>
	<td class="line x" title="73:186	This renders their experiment inconclusive." ></td>
	<td class="line x" title="74:186	2 Method 2.1 Extracting a Test Set A test set of syntactically ambiguous noun compounds was extracted from our 8 million word Grolier's encyclopedia corpus in the following way." ></td>
	<td class="line x" title="75:186	2 Because the corpus is not tagged or parsed, a somewhat conservative strategy of looking for unambiguous sequences of nouns was used." ></td>
	<td class="line x" title="76:186	To distinguish nouns from other words, the University of Pennsylvania morphological analyser (described in Karp et al, 1992) was used to generate the set of words that can only be used as nouns (I shall henceforth call this set AZ)." ></td>
	<td class="line x" title="77:186	All consecutive sequences of these words were extracted, and the three word sequences used to form the test set." ></td>
	<td class="line x" title="78:186	For reasons made clear below, only sequences consisting entirely of words from Roget's thesaurus were retained, giving a total of 308 test triples." ></td>
	<td class="line x" title="79:186	3 These triples were manually analysed using as context the entire article in which they appeared." ></td>
	<td class="line x" title="80:186	In 2We would like to thank Grolier's for permission to use this material for research purposes." ></td>
	<td class="line x" title="81:186	3The 1911 version of Roget's used is available on-line and is in the public domain." ></td>
	<td class="line x" title="82:186	some cases, the sequence was not a noun compound (nouns can appear adjacent to one another across various constituent boundaries) and was marked as an error." ></td>
	<td class="line x" title="83:186	Other compounds exhibited what Hindie and Rooth (1993) have termed SEMANTIC INDETERMINACY where the two possible bracketings cannot be distinguished in the context." ></td>
	<td class="line x" title="84:186	The remaining compounds were assigned either a left-branching or right-branching analysis." ></td>
	<td class="line x" title="85:186	Table 1 shows the number of each kind and an example of each." ></td>
	<td class="line x" title="86:186	Accuracy figures in all the results reported below were computed using only those 244 compounds which received a parse." ></td>
	<td class="line x" title="87:186	2.2 Conceptual Association One problem with applying lexical association to noun compounds is the enormous number of parameters required, one for every possible pair of nouns." ></td>
	<td class="line x" title="88:186	Not only does this require a vast amount of memory space, it creates a severe data sparseness problem since we require at least some data about each parameter." ></td>
	<td class="line x" title="89:186	Resnik and Hearst (1993) coined the term CONCEPTUAL ASSOCIATION to refer to association values computed between groups of words." ></td>
	<td class="line x" title="90:186	By assuming that all words within a group behave similarly, the parameter space can be built in terms of the groups rather than in terms of the words." ></td>
	<td class="line x" title="91:186	In this study, conceptual association is used with groups consisting of all categories from the 1911 version of Roget's thesaurus." ></td>
	<td class="line x" title="92:186	4 Given two thesaurus categories tl and t~, there is a parameter which represents the degree of acceptability of the structure \[nine\] where nl is a noun appearing in tl and n2 appears in t2." ></td>
	<td class="line x" title="93:186	By the assumption that words within a group behave similarly, this is constant given the two categories." ></td>
	<td class="line x" title="94:186	Following Lauer and Dras (1994) we can formally write this parameter as Pr(tl ~ t2) where the event tl ~ t2 denotes the modification of a noun in t2 by a noun in tl." ></td>
	<td class="line x" title="95:186	2.3 Training To ensure that the test set is disjoint from the training data, all occurrences of the test noun compounds have been removed from the training corpus." ></td>
	<td class="line x" title="96:186	4It contains 1043 categories." ></td>
	<td class="line x" title="97:186	49 Type Error Indeterminate Left-branching Right-branching Number 29 35 163 81 Proportion 9% 11% 53% 26% Example In monsoon regions rainfall does not Most advanced aircraft have precision navigation systems." ></td>
	<td class="line x" title="98:186	escaped punishment by the Allied war crimes tribunals." ></td>
	<td class="line x" title="99:186	Ronald Reagan, who won two landslide election victories,  Table 1: Test set distribution Two types of training scheme are explored in this study, both unsupervised." ></td>
	<td class="line x" title="100:186	The first employs a pattern that follows Pustejovsky (1993) in counting the occurrences of subcomponents." ></td>
	<td class="line x" title="101:186	A training instance is any sequence of four words WlW2W3W 4 where wl, w4 ~ .h/and w2, w3 E A/'." ></td>
	<td class="line x" title="102:186	Let county(n1, n2) be the number of times a sequence wlnln2w4 occurs in the training corpus with wl, w4 ~ At'." ></td>
	<td class="line x" title="103:186	The second type uses a window to collect training instances by observing how often a pair of nouns cooccur within some fixed number of words." ></td>
	<td class="line x" title="104:186	In this study, a variety of window sizes are used." ></td>
	<td class="line x" title="105:186	For n > 2, let countn(nl, n2) be the number of times a sequence nlwlwins occurs in the training corpus where i < n 2." ></td>
	<td class="line x" title="106:186	Note that windowed counts are asymmetric." ></td>
	<td class="line x" title="107:186	In the case of a window two words wide, this yields the mutual information metric proposed by Liberman and Sproat (1992)." ></td>
	<td class="line x" title="108:186	Using each of these different training schemes to arrive at appropriate counts it is then possible to estimate the parameters." ></td>
	<td class="line x" title="109:186	Since these are expressed in terms of categories rather than words, it is necessary to combine the counts of words to arrive at estimates." ></td>
	<td class="line x" title="110:186	In all cases the estimates used are: 1 count(wl, w2) Vr(tl --, t2) = ~ ambig(wl) ambig(w2) wlfitl w2qt2 count(wl, w2) where ~ = ~,~j ambig(wl)ambig(w~) w2Et2 Here ambig(w) is the number of categories in which w appears." ></td>
	<td class="line x" title="111:186	It has the effect of dividing the evidence from a training instance across all possible categories for the words." ></td>
	<td class="line x" title="112:186	The normaliser ensures that all parameters for a head noun sum to unity." ></td>
	<td class="line x" title="113:186	2.4 Analysing the Test Set Given the high level descriptions in section 1.3 it remains only to formalise the decision process used to analyse a noun compound." ></td>
	<td class="line x" title="114:186	Each test compound presents a set of possible analyses and the goal is to choose which analysis is most likely." ></td>
	<td class="line x" title="115:186	For three word compounds it suffices to compute the ratio of two probabilities, that of a left-branching analysis and that of a right-branching one." ></td>
	<td class="line x" title="116:186	If this ratio is greater than unity, then the left-branching analysis is chosen. When it is less than unity, a right-branching analysis is chosen." ></td>
	<td class="line x" title="117:186	~ If the ratio is exactly unity, the analyser guesses left-branching, although this is fairly rare for conceptual association as shown by the experimental results below." ></td>
	<td class="line x" title="118:186	For the adjacency model, when the given compound is WlW2W3, we can estimate this ratio as: Ra4i : ~-~t,~cats(~ Pr(tl ---* t2) (1) ~-'~t,ecats(-b Pr(t2 ---* t3) For the dependency model, the ratio is: Rdep = ~-~,,ec~ts(~,) Pr(Q ---* t~) Pr(t~ ---* ta) (2) )-~t,ec~ts(~) Pr(~l ---* t3) Pr(t2 ~ ta) In both cases, we sum over all possible categories for the words in the compound." ></td>
	<td class="line x" title="119:186	Because the dependency model equations have two factors, they are affected more severely by data sparseness." ></td>
	<td class="line x" title="120:186	If the probability estimate for Pr(t2 ~ t3) is zero for all possible categories t2 and t3 then both the numerator and the denominator will be zero." ></td>
	<td class="line x" title="121:186	This will conceal any preference given by the parameters involving Q. In such cases, we observe that the test instance itself provides the information that the event t2 --~ t3 can occur and we recalculate the ratio using Pr(t2 ---* t3) = k for all possible categories t2,t a where k is any non-zero constant." ></td>
	<td class="line x" title="122:186	However, no correction is made to the probability estimates for Pr(tl --~ t2) and Pr(Q --* t3) for unseen cases, thus putting the dependency model on an equal footing with the adjacency model above." ></td>
	<td class="line x" title="123:186	The equations presented above for the dependency model differ from those developed in Lauer and Dras (1994) in one way." ></td>
	<td class="line x" title="124:186	There, an additional weighting factor (of 2.0) is used to favour a left-branching analysis." ></td>
	<td class="line x" title="125:186	This arises because their construction is based on the dependency model which predicts that left-branching analyses should occur twice as often." ></td>
	<td class="line x" title="126:186	Also, the work reported in Lauer and Dras (1994) uses simplistic estimates of the probability of a word given its thesaurus category." ></td>
	<td class="line x" title="127:186	The equations above assume these probabilities are uniformly constant." ></td>
	<td class="line x" title="128:186	Section 3.2 below shows the result of making these two additions to the method." ></td>
	<td class="line x" title="129:186	sit either probability estimate is zero, the other analysis is chosen." ></td>
	<td class="line x" title="130:186	If both are zero the analysis is made as if the ratio were exactly unity." ></td>
	<td class="line x" title="131:186	50 Accuracy (%) 85 80 75 70 65 60 55 50 I I I I I I I I Dependency Model Adjacency Model o Guess Left  Pattern 2 3 4 5 10 50 100 Training scheme (integers denote window widths) Figure 2: Accuracy of dependency and adjacency model for various training schemes 3 Results 3.1 Dependency meets Adjacency Eight different training schemes have been used to estimate the parameters and each set of estimates used to analyse the test set under both the adjacency and the dependency model." ></td>
	<td class="line x" title="132:186	The schemes used are:  the pattern given in section 2.3; and  windowed training schemes with window widths of 2, 3, 4, 5, 10, 50 and 100 words." ></td>
	<td class="line x" title="133:186	The accuracy on the test set for all these experiments is shown in figure 2." ></td>
	<td class="line x" title="134:186	As can be seen, the dependency model is more accurate than the adjacency model." ></td>
	<td class="line x" title="135:186	This is true across the whole spectrum of training schemes." ></td>
	<td class="line x" title="136:186	The proportion of cases in which the procedure was forced to guess, either because no data supported either analysis or because both were equally supported, is quite low." ></td>
	<td class="line x" title="137:186	For the pattern and two-word window training schemes, the guess rate is less than 4% for both models." ></td>
	<td class="line x" title="138:186	In the three-word window training scheme, the guess rates are less than 1%." ></td>
	<td class="line x" title="139:186	For all larger windows, neither model is ever forced to guess." ></td>
	<td class="line x" title="140:186	In the case of the pattern training scheme, the difference between 68.9% for adjacency and 77.5% for dependency is statistically significant at the 5% level (p = 0.0316), demonstrating the superiority of the dependency model, at least for the compounds within Grolier's encyclopedia." ></td>
	<td class="line x" title="141:186	In no case do any of the windowed training schemes outperform the pattern scheme." ></td>
	<td class="line x" title="142:186	It seems that additional instances admitted by the windowed schemes are too noisy to make an improvement." ></td>
	<td class="line x" title="143:186	Initial results from applying these methods to the EMA corpus have been obtained by Wilco ter Stal (1995), and support the conclusion that the dependency model is superior to the adjacency model." ></td>
	<td class="line x" title="144:186	3.2 Tuning Lauer and Dras (1994) suggest two improvements to the method used above." ></td>
	<td class="line x" title="145:186	These are:  a factor favouring left-branching which arises from the formal dependency construction; and  factors allowing for naive estimates of the variation in the probability of categories." ></td>
	<td class="line x" title="146:186	While these changes are motivated by the dependency model, I have also applied them to the adjacency model for comparison." ></td>
	<td class="line x" title="147:186	To implement them, equations 1 and 2 must be modified to incorporate 1 in each term of the sum and the a factor of entire ratio must be multiplied by two." ></td>
	<td class="line x" title="148:186	Five training schemes have been applied with these extensions." ></td>
	<td class="line x" title="149:186	The accuracy results are shown in figure 3." ></td>
	<td class="line x" title="150:186	For comparison, the untuned accuracy figures are shown with dotted lines." ></td>
	<td class="line x" title="151:186	A marked improvement is observed for the adjacency model, while the dependency model is only slightly improved." ></td>
	<td class="line x" title="152:186	3.3 Lexical Association To determine the difference made by conceptual association, the pattern training scheme has been retrained using lexical counts for both the dependency and adjacency model, but only for the words in the test set." ></td>
	<td class="line x" title="153:186	If the same system were to be applied across all of Af (a total of 90,000 nouns), then around 8.1 billion parameters would be required." ></td>
	<td class="line x" title="154:186	Left-branching is favoured by a factor of two as described in the previous section, but no estimates for the category probabilities are used (these being meaningless for the lexical association method)." ></td>
	<td class="line x" title="155:186	Accuracy and guess rates are shown in figure 4." ></td>
	<td class="line x" title="156:186	Conceptual association outperforms lexical association, presumably because of its ability to generalise." ></td>
	<td class="line x" title="157:186	3.4 Using a Tagger One problem with the training methods given in section 2.3 is the restriction of training data to nouns in .Af." ></td>
	<td class="line x" title="158:186	Many nouns, especially common ones, have verbal or adiectival usages that preclude them from being in .Af." ></td>
	<td class="line x" title="159:186	Yet when they occur as nouns, they still provide useful training information that the current system ignores." ></td>
	<td class="line x" title="160:186	To test whether using tagged 51 Accuracy (%) 85 80 75 70 65 60 55 50 I I l I Tuned Dependency  Tuned Adjacency o  . . ." ></td>
	<td class="line x" title="161:186	I I I Pattern 2 3 5 10 Training scheme (integers denote window widths) Figure 3: Accuracy of tuned dependency and adjacency model for various training schemes Accuracy (%) 85 80 75 70 65 60 55 50 I ! Conceptual  Lexical O _ Dependency Adjacency 30 25 2O Guess Rate (%) 15 10 I I Conceptual  Lexical \[\] I I Dependency Adjacency Figure 4: Accuracy and Guess Rates of Lexical and Conceptual Association 52 85 i l Accuracy (%) 80 75 70 65 60 55 50 Tagged Dependency Tagged Adjacency o I I Pattern 3 Training scheme (integers denote window widths) Figure 5: Accuracy using a tagged corpus for various training schemes data would make a difference, the freely available Brill tagger (Brill, 1993) was applied to the corpus." ></td>
	<td class="line x" title="162:186	Since no manually tagged training data is available for our corpus, the tagger's default rules were used (these rules were produced by Brill by training on the Brown corpus)." ></td>
	<td class="line x" title="163:186	This results in rather poor tagging accuracy, so it is quite possible that a manually tagged corpus would produce better results." ></td>
	<td class="line x" title="164:186	Three training schemes have been used and the tuned analysis procedures applied to the test set." ></td>
	<td class="line x" title="165:186	Figure 5 shows the resulting accuracy, with accuracy values from figure 3 displayed with dotted lines." ></td>
	<td class="line x" title="166:186	If anything, admitting additional training data based on the tagger introduces more noise, reducing the accuracy." ></td>
	<td class="line x" title="167:186	However, for the pattern training scheme an improvement was made to the dependency model, producing the highest overall accuracy of 81%." ></td>
	<td class="line x" title="168:186	4 Conclusion The experiments above demonstrate a number of important points." ></td>
	<td class="line x" title="169:186	The most general of these is that even quite crude corpus statistics can provide information about the syntax of compound nouns." ></td>
	<td class="line x" title="170:186	At the very least, this information can be applied in broad coverage parsing to assist in the control of search." ></td>
	<td class="line x" title="171:186	I have also shown that with a corpus of moderate size it is possible to get reasonable results without using a tagger or parser by employing a customised training pattern." ></td>
	<td class="line x" title="172:186	While using windowed co-occurrence did not help here, it is possible that under more data sparse conditions better performance could be achieved by this method." ></td>
	<td class="line x" title="173:186	The significance of the use of conceptual association deserves some mention." ></td>
	<td class="line x" title="174:186	I have argued that without it a broad coverage system would be impossible." ></td>
	<td class="line x" title="175:186	This is in contrast to previous work on conceptual association where it resulted in little improvement on a task which could already be performed." ></td>
	<td class="line x" title="176:186	In this study, not only has the technique proved its worth by supporting generality, but through generalisation of training information it outperforms the equivalent lexical association approach given the same information." ></td>
	<td class="line x" title="177:186	Amongst all the comparisons performed in these experiments one stands out as exhibiting the greatest contrast." ></td>
	<td class="line x" title="178:186	In all experiments the dependency model provides a substantial advantage over the adjacency model, even though the latter is more prevalent in proposals within the literature." ></td>
	<td class="line x" title="179:186	This result is in accordance with the informal reasoning given in section 1.3." ></td>
	<td class="line x" title="180:186	The model also has the further commendation that it predicts correctly the observed proportion of left-branching compounds found in two independently extracted test sets." ></td>
	<td class="line x" title="181:186	In all, the most accurate technique achieved an accuracy of 81% as compared to the 67% achieved by guessing left-branching." ></td>
	<td class="line x" title="182:186	Given the high frequency of occurrence of noun compounds in many texts, this suggests tha; the use of these techniques in probabilistic parsers will result in higher performance in broad coverage natural language processing." ></td>
	<td class="line x" title="183:186	5 Acknowledgements This work has received valuable input from people too numerous to mention." ></td>
	<td class="line x" title="184:186	The most significant contributions have been made by Richard Buckland, Robert Dale and Mark Dras." ></td>
	<td class="line x" title="185:186	I am also indebted to Vance Gledhill, Mike Johnson, Philip Resnik, Richard Sproat, Wilco ter Stal, Lucy Vanderwende and Wayne Wobcke." ></td>
	<td class="line x" title="186:186	Financial support is gratefully ack53 nowledged from the Microsoft Institute and the Australian Government." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P95-1027
A Quantitative Evaluation Of Linguistic Tests For The Automatic Prediction Of Semantic Markedness
Hatzivassiloglou, Vasileios;McKeown, Kathleen R.;"></td>
	<td class="line x" title="1:195	A Quantitative Evaluation of Linguistic Tests for the Automatic Prediction of Semantic Markedness Vasileios Hatzivassiloglou and Kathleen McKeown Department of Computer Science 450 Computer Science Building Columbia University New York, N.Y. 10027 {vh, kathy}~cs, columbia, edu Abstract We present a corpus-based study of methods that have been proposed in the linguistics literature for selecting the semantically unmarked term out of a pair of antonymous adjectives." ></td>
	<td class="line x" title="2:195	Solutions to this problem are applicable to the more general task of selecting the positive term from the pair." ></td>
	<td class="line x" title="3:195	Using automatically collected data, the accuracy and applicability of each method is quantified, and a statistical analysis of the significance of the results is performed." ></td>
	<td class="line x" title="4:195	We show that some simple methods are indeed good indicators for the answer to the problem while other proposed methods fail to perform better than would be attributable to chance." ></td>
	<td class="line x" title="5:195	In addition, one of the simplest methods, text frequency, dominates all others." ></td>
	<td class="line x" title="6:195	We also apply two generic statistical learning methods for combining the indications of the individual methods, and compare their performance to the simple methods." ></td>
	<td class="line x" title="7:195	The most sophisticated complex learning method offers a small, but statistically significant, improvement over the original tests." ></td>
	<td class="line x" title="8:195	1 Introduction The concept of markedness originated in the work of Prague School linguists (Jakobson, 1984a) and refers to relationships between two complementary or antonymous terms which can be distinguished by the presence or absence of a feature (+A versus --A)." ></td>
	<td class="line x" title="9:195	Such an opposition can occur at various linguistic levels." ></td>
	<td class="line x" title="10:195	For example, a markedness contrast can arise at the morphology level, when one of the two words is derived from the other and therefore contains an explicit formal marker such as a prefix; e.g., profitable-unprofitable." ></td>
	<td class="line x" title="11:195	Markedness contrasts also appear at the semantic level in many pairs of gradable antonymous adjectives, especially scalar ones (Levinson, 1983), such as tall-short." ></td>
	<td class="line x" title="12:195	The marked and unmarked elements of such pairs function in different ways." ></td>
	<td class="line x" title="13:195	The unmarked adjective (e.g. , tall) can be used in how-questions to refer to the property described by both adjectives in the pair (e.g. , height), but without any implication about the modified item relative to the norm for the property." ></td>
	<td class="line x" title="14:195	For example, the question How tall is Jack?" ></td>
	<td class="line x" title="15:195	can be answered equally well by four or seven feet." ></td>
	<td class="line x" title="16:195	In contrast, the marked element of the opposition cannot be used generically; when used in a how-question, it implies a presupposition of the speaker regarding the relative position of the modified item on the adjectival scale." ></td>
	<td class="line x" title="17:195	Thus, the corresponding question using the marked term of the opposition (How short is Jack)?" ></td>
	<td class="line x" title="18:195	conveys an implication on the part of the speaker that Jack is indeed short; the distinguishing feature A expresses this presupposition." ></td>
	<td class="line x" title="19:195	While markedness has been described in terms of a distinguishing feature A, its definition does not specify the type of this feature." ></td>
	<td class="line x" title="20:195	Consequently, several different types of features have been employed, which has led into some confusion about the meaning of the term markedness." ></td>
	<td class="line x" title="21:195	Following Lyons (1977), we distinguish between formal markedness where the opposition occurs at the morphology level (i.e. , one of the two terms is derived from the other through inflection or affixation) and semantic markedness where the opposition occurs at the semantic level as in the example above." ></td>
	<td class="line x" title="22:195	When two antonymous terms are also morphologically related, the formally unmarked term is usually also the semantically unmarked one (for example, clear-unclear)." ></td>
	<td class="line x" title="23:195	However, this correlation is not universal; consider the examples unbiased-biased and independent-dependent." ></td>
	<td class="line x" title="24:195	In any case, semantic markedness is the more interesting of the two and the harder to determine, both for humans and computers." ></td>
	<td class="line x" title="25:195	Various tests for determining markedness in general have been proposed by linguists (see Section 3)." ></td>
	<td class="line x" title="26:195	However, although potentially automatic versions of some of these have been successfully applied to the problem at the phonology level (Trubetzkoy, 1939; Greenberg, 1966), little work has been done on the empirical validation or the automatic application of those tests at higher levels (but see (Ku~era, 1982) for an empirical analysis of a proposed markedness test at the syntactic level; some more narrowly focused empirical work has also been done on markedness in second language acquisition)." ></td>
	<td class="line x" title="27:195	In this paper 197 we analyze the performance of several linguistic tests for the selection of the semantically unmarked term out of a pair of gradable antonymous adjectives." ></td>
	<td class="line x" title="28:195	We describe a system that automatically extracts the relevant data for these tests from text corpora and corpora-based databases, and use this system to measure the applicability and accuracy of each method." ></td>
	<td class="line x" title="29:195	We apply statistical tests to determine the significance of the results, and then discuss the performance of complex predictors that combine the answers of the linguistic tests according to two general statistical learning methods, decision trees and loglinear regression models." ></td>
	<td class="line x" title="30:195	2 Motivation The goal of our work is twofold: First, we are interested in providing hard, quantitative evidence on the performance of markedness tests already proposed in the linguistics literature." ></td>
	<td class="line x" title="31:195	Such tests are based on intuitive observations and/or particular theories of semantics, but their accuracy has not been measured on actual data." ></td>
	<td class="line x" title="32:195	The results of our analysis can be used to substantiate theories which are compatible with the empirical evidence, and thus offer insight into the complex linguistic phenomenon of antonymy." ></td>
	<td class="line x" title="33:195	The second purpose of our work is practical applications." ></td>
	<td class="line x" title="34:195	The semantically unmarked term is almost always the positive term of the opposition (Boucher and Osgood, 1969); e.g., high is positive, while low is negative." ></td>
	<td class="line x" title="35:195	Therefore, an automatic method for determining markedness values can also be used to determine the polarity of antonyms." ></td>
	<td class="line x" title="36:195	The work reported in this paper helps clarify which types of data and tests are useful for such a method and which are not." ></td>
	<td class="line x" title="37:195	The need for an automatic corpus-based method for the identification of markedness becomes apparent when we consider the high number of adjectives in unrestricted text and the domain-dependence of markedness values." ></td>
	<td class="line x" title="38:195	In the MRC Psycholinguistic Database (Coltheart, 1981), a large machinereadable annotated word list, 25,547 of the 150,837 entries (16.94%) are classified as adjectives, not including past participles; if we only consider regularly used grammatical categories for each word, the percentage of adjectives rises to 22.97%." ></td>
	<td class="line x" title="39:195	For comparison, nouns (the largest class) account for 51.28% and 57.47% of the words under the two criteria." ></td>
	<td class="line x" title="40:195	In addition, while adjectives tend to have prevalent markedness and polarity values in the language at large, frequently these values are negated in specific domains or contexts." ></td>
	<td class="line x" title="41:195	For example, healthy is in most contexts the unmarked member of the opposition healthy:sick; but in a hospital setting, sickness rather than health is expected, so sick becomes the unmarked term." ></td>
	<td class="line x" title="42:195	The methods we describe are based on the form of the words and their overall statistical properties, and thus cannot predict specific occurfences of markedness reversals." ></td>
	<td class="line x" title="43:195	But they can predict the prevalent markedness value for each adjective in a given domain, something which is impractical to do by hand separately for each domain." ></td>
	<td class="line x" title="44:195	We have built a large system for the automatic, domain-dependent classification of adjectives according to semantic criteria." ></td>
	<td class="line x" title="45:195	The first phase of our system (Hatzivassiloglou and McKeown, 1993) separates adjectives into groups of semantically related ones." ></td>
	<td class="line x" title="46:195	We extract markedness values according to the methods described in this paper and use them in subsequent phases of the system that further analyze these groups and determine their scalar structure." ></td>
	<td class="line x" title="47:195	An automatic method for extracting polarity information would also be useful for the augmentation of lexico-semantic databases such as WordNet (Miller et al. , 1990), particularly when the method accounts for the specificities of the domain sublanguage; an increasing number of NLP systems rely on such databases (e.g. , (Resnik, 1993; Knight and Luk, 1994))." ></td>
	<td class="line oc" title="48:195	Finally, knowledge of polarity can be combined with corpus-based collocation extraction methods (Smadja, 1993) to automatically produce entries for the lexical functions used in MeaningText Theory (Mel'~uk and Pertsov, 1987) for text generation." ></td>
	<td class="line x" title="49:195	For example, knowing that hearty is a positive term enables the assignment of the collocation hearty eater to the lexical function entry MAGS( eater)=-hearty." ></td>
	<td class="line x" title="50:195	1 3 Tests for Semantic Markedness Markedness in general and semantic markedness in particular have received considerable attention in the linguistics literature." ></td>
	<td class="line x" title="51:195	Consequently, several tests for determining markedness have been proposed by linguists." ></td>
	<td class="line x" title="52:195	Most of these tests involve human judgments (Greenberg, 1966; Lyons, 1977; Waugh, 1982; Lehrer, 1985; Ross, 1987; Lakoff, 1987) and are not suitable for computer implementation." ></td>
	<td class="line x" title="53:195	However, some proposed tests refer to comparisons between measurable properties of the words in question and are amenable to full automation." ></td>
	<td class="line x" title="54:195	These tests are: 1." ></td>
	<td class="line x" title="55:195	Text frequency." ></td>
	<td class="line x" title="56:195	Since the unmarked term can appear in more contexts than the marked one, and it has both general and specific senses, it should appear more frequently in text than the marked term (Greenberg, 1966)." ></td>
	<td class="line x" title="57:195	2." ></td>
	<td class="line x" title="58:195	Formal markedness." ></td>
	<td class="line x" title="59:195	A formal markedness relationship (i.e. , a morphology relationship between the two words), whenever it exists, should be an excellent predictor for semantic markedness (Greenberg, 1966; Zwicky, 1978)." ></td>
	<td class="line x" title="60:195	3." ></td>
	<td class="line x" title="61:195	Formal complexity." ></td>
	<td class="line x" title="62:195	Since the unmarked word is the more general one, it should also be morphologically the simpler (Jakobson, 1962; Battistella, 1990)." ></td>
	<td class="line x" title="63:195	The 'economy of language' prin1MAGN stands for magnify." ></td>
	<td class="line x" title="64:195	198 ciple (Zipf, 1949) supports this claim." ></td>
	<td class="line x" title="65:195	Note that this test subsumes test (2)." ></td>
	<td class="line x" title="66:195	4." ></td>
	<td class="line x" title="67:195	Morphological produclivity." ></td>
	<td class="line x" title="68:195	Unmarked words, being more general and frequently used to describe the whole scale, should be freer to combine with other linguistic elements (Winters, 1990; Battistella, 1990)." ></td>
	<td class="line x" title="69:195	5." ></td>
	<td class="line x" title="70:195	Differentialion." ></td>
	<td class="line x" title="71:195	Unmarked terms should exhibit higher differentiation with more subdistinetions (Jakobson, 1984b) (e.g. , the present tense (unmarked) appears in a greater variety of forms than the past), or, equivalently, the marked term should lack some subcategories (Greenberg, 1966)." ></td>
	<td class="line x" title="72:195	The first of the above tests compares the text frequencies of the two words, which are clearly measurable and easily retrievable from a corpus." ></td>
	<td class="line x" title="73:195	We use the one-million word Brown corpus of written American English (Ku~era and Francis, 1967) for this purpose." ></td>
	<td class="line x" title="74:195	The mapping of the remaining tests to quantifiable variables is not as immediate." ></td>
	<td class="line x" title="75:195	We use the length of a word in characters, which is a reasonable indirect index of morphological complexity, for tests (2) and (3)." ></td>
	<td class="line x" title="76:195	This indicator is exact for the case of test (2), since the formally marked word is derived from the unmarked one through the addition of an affix (which for adjectives is always a prefix)." ></td>
	<td class="line x" title="77:195	The number of syllables in a word is another reasonable indicator of morphological complexity that we consider, although it is much harder to compute automatically than word length." ></td>
	<td class="line x" title="78:195	For morphological productivity (test (4)), we measure several variables related to the freedom of the word to receive affixes and to participate in compounds." ></td>
	<td class="line x" title="79:195	Several distinctions exist for the definition of a variable that measures the number of words that are morphologically derived from a given word." ></td>
	<td class="line x" title="80:195	These distinctions involve: Q Whether to consider the number of distinct words in this category (types) or the total frequency of these words (tokens)." ></td>
	<td class="line x" title="81:195	 Whether to separate words derived through affixation from compounds or combine these types of morphological relationships." ></td>
	<td class="line x" title="82:195	 If word types (rather than word frequencies) are measured, we can select to count homographs (words identical in form but with different parts of speech, e.g., light as an adjective and light as a verb) as distinct types or map all homographs of the same word form to the same word type." ></td>
	<td class="line x" title="83:195	Finally, the differentiation test (5) is the one general markedness test that cannot be easily mapped into observable properties of adjectives." ></td>
	<td class="line x" title="84:195	Somewhat arbitrarily, we mapped this test to the number of grammatical categories (parts of speech) that each word can appear under, postulating that the unmarked term should have a higher such number." ></td>
	<td class="line x" title="85:195	The various ways of measuring the quantities compared by the tests discussed above lead to the consideration of 32 variables." ></td>
	<td class="line x" title="86:195	Since some of these variables are closely related and their number is so high that it impedes the task of modeling semantic markedness in terms of them, we combined several of them, keeping 14 variables for the statistical analysis." ></td>
	<td class="line x" title="87:195	4 Data Collection In order to measure the performance of the markedness tests discussed in the previous section, we collected a fairly large sample of pairs of antonymous gradable adjectives that can appear in howquestions." ></td>
	<td class="line x" title="88:195	The Deese antonyms (Deese, 1964) is the prototypical collection of pairs of antonymous adjectives that have been used for similar analyses in the past (Deese, 1964; Justeson and Katz, 1991; Grefenstette, 1992)." ></td>
	<td class="line x" title="89:195	However, this collection contains only 75 adjectives in 40 pairs, some of which cannot be used in our study either because they are primarily adverbials (e.g. , inside-outside) or not gradable (e.g. , alive-dead)." ></td>
	<td class="line x" title="90:195	Unlike previous studies, the nature of the statistical analysis reported in this paper requires a higher number of pairs." ></td>
	<td class="line x" title="91:195	Consequently, we augmented the Deese set with the set of pairs used in the largest manual previous study of markedness in adjective pairs (Lehrer, 1985)." ></td>
	<td class="line x" title="92:195	In addition, we included all gradable adjectives which appear 50 times or more in the Brown corpus and have at least one gradable antonym; the antonyms were not restricted to belong to this set of frequent adjectives." ></td>
	<td class="line x" title="93:195	For each adjective collected according to this last criterion, we included all the antonyms (frequent or not) that were explicitly listed in the Collins COBUILD dictionary (Sinclair, 1987) for each of its senses." ></td>
	<td class="line x" title="94:195	This process gave us a sample of 449 adjectives (both frequent and infrequent ones) in 344 pairs." ></td>
	<td class="line x" title="95:195	2 We separated the pairs on the basis of the how-test into those that contain one semantically unmarked and one marked term and those that contain two marked terms (e.g. , fat-lhin), removing the latter." ></td>
	<td class="line x" title="96:195	For the remaining pairs, we identified the unmarked member, using existing designations (Lehrer, 1985) whenever that was possible; when in doubt, the pair was dropped from further consideration." ></td>
	<td class="line x" title="97:195	We also separated the pairs into two groups according to whether the two adjectives in each pair were morphologically related or not." ></td>
	<td class="line x" title="98:195	This allowed us to study the different behavior of the tests for the two groups separately." ></td>
	<td class="line x" title="99:195	Table 1 shows the results of this crossclassification of the adjective pairs." ></td>
	<td class="line x" title="100:195	Our next step was to measure the variables described in Section 3 which are used in the various 2The collection method is similar to Deese's: He also started from frequent adjectives but used human subjects to elicit antonyms instead of a dictionary." ></td>
	<td class="line x" title="101:195	199 One Both unmarked marked Morphologically 211 54 unrelated Morphologically 68 3 related Total 279 57 Total 265 71 \[\[ 336 Table 1: Cross-classification of adjective pairs according to morphological relationship and markedness status." ></td>
	<td class="line x" title="102:195	tests for semantic markedness." ></td>
	<td class="line x" title="103:195	For these measurements, we used the MRC Psycholinguistic Database (Coltheart, 1981) which contains a variety of measures for 150,837 entries counting different parts of speech or inflected forms as different words (115,331 distinct words)." ></td>
	<td class="line x" title="104:195	We implemented an extractor program to collect the relevant measurements for the adjectives in our sample, namely text frequency, number of syllables, word length, and number of parts of speech." ></td>
	<td class="line x" title="105:195	All this information except the number of syllables can also be automatically extracted from the corpus." ></td>
	<td class="line x" title="106:195	The extractor program also computes information that is not directly stored in the MRC database." ></td>
	<td class="line x" title="107:195	Affixation rules from (Quirk et al. , 1985) are recursively employed to check whether each word in the database can be derived from each adjective, and counts and frequencies of such derived words and compounds are collected." ></td>
	<td class="line x" title="108:195	Overall, 32 measurements are computed for each adjective, and are subsequently combined into the 14 variables used in our study." ></td>
	<td class="line x" title="109:195	Finally, the variables for the pairs are computed as the differences between the corresponding variables for the adjectives in each pair." ></td>
	<td class="line x" title="110:195	The output of this stage is a table, with two strata corresponding to the two groups, and containing measurements on 14 variables for the 279 pairs with a semantically unmarked member." ></td>
	<td class="line x" title="111:195	5 Evaluation of Linguistic Tests For each of the variables, we measured how many pairs in each group it classified correctly." ></td>
	<td class="line x" title="112:195	A positive (negative) value indicates that the first (second) adjective is the unmarked one, except for two variables (word length and number of syllables) where the opposite is true." ></td>
	<td class="line x" title="113:195	When the difference is zero, the variable selects neither the first or second adjective as unmarked." ></td>
	<td class="line x" title="114:195	The percentage of nonzero differences, which correspond to cases where the test actually suggests a choice, is reported as the applicability of the variable." ></td>
	<td class="line x" title="115:195	For the purpose of evaluating the accuracy of the variable, we assign such cases randomly to one of the two possible outcomes in accordance with common practice in classification (Duda and Hart, 1973)." ></td>
	<td class="line x" title="116:195	For each variable and each of the two groups, we also performed a statistical test of the null hypothesis that its true accuracy is 50%, i.e., equal to the expected accuracy of a random binary classifier." ></td>
	<td class="line x" title="117:195	Under the null hypothesis, the number of correct responses follows a binomial distribution with parameter p = 0.5." ></td>
	<td class="line x" title="118:195	Since all obtained measurements of accuracy were higher than 50%, any rejection of the null hypothesis implies that the corresponding test is significantly better than chance." ></td>
	<td class="line x" title="119:195	Table 2 summarizes the values obtained for some of the 14 variables in our data and reveals some surprising facts about their performance." ></td>
	<td class="line x" title="120:195	The frequency of the adjectives is the best predictor in both groups, achieving an overall accuracy of 80.64% with high applicability (98.5-99%)." ></td>
	<td class="line x" title="121:195	This is all the more remarkable in the case of the morphologically related adjectives, where frequency outperforms length of the words; recall that the latter directly encodes the formal markedness relationship, so frequency is able to correctly classify some of the cases where formal and semantic markedness values disagree." ></td>
	<td class="line x" title="122:195	On the other hand, tests based on the 'economy of language' principle, such as word length and number of syllables, perform badly when formal markedness relationships do not exist, with lower applicability and very low accuracy scores." ></td>
	<td class="line x" title="123:195	The same can be said about the test based on the differentiation properties of the words (number of different parts of speech)." ></td>
	<td class="line x" title="124:195	In fact, for these three variables, the hypothesis of random performance cannot be rejected even at the 5% level." ></td>
	<td class="line x" title="125:195	Tests based on the productivity of the words, as measured through affixation and compounding, tend to fall in-between: their accuracy is generally significant, but their applicability is sometimes low, particularly for compounds." ></td>
	<td class="line x" title="126:195	6 Predictions Based on More than One Test While the frequency of the adjectives is the best single predictor, we would expect to gain accuracy by combining the answers of several simple tests." ></td>
	<td class="line x" title="127:195	We consider the problem of determining semantic markedness as a classification problem with two possible outcomes ('the first adjective is unmarked' and 'the second adjective is unmarked')." ></td>
	<td class="line x" title="128:195	To design an appropriate classifier, we employed two general statistical supervised learning methods, which we briefly describe in this section." ></td>
	<td class="line x" title="129:195	Decision trees (Quinlan, 1986) is the first statistical supervised learning paradigm that we explored." ></td>
	<td class="line x" title="130:195	A popular method for the automatic construction of such trees is binary recursive partitioning, which constructs a binary tree in a top-down fashion." ></td>
	<td class="line x" title="131:195	Starting from the root, the variable X which better discriminates among the possible outcomes is selected and a test of the form X < consiant is as200 Test Morphologically Unrelated P-Value Frequency Applicability 99.05% Accuracy 75.36% 8.4.10 -14 Number of syllables 58.29% 55.92% 0.098 Word length 83.41% 52.13% 0.582 Number of 71.09% 56.87% 0.054 homographs Total number of 64.45% 61.14% 0.0015 compounds Unique words derived 95.26% 66.35% 2.3.10 -6 by affixation Total frequency of 82.46% 66.35% 2.3  10 -6 derived words II Morphologically Related Applicability Accuracy P-Value 98.53% 95.59% 100.00% 97.06% 92.65% 95.59% < 10 -16 7.7.10 -14 4.4.10 -16 66.18% 14.71% 98.53% 83.82% 79.41% 60.29% 94.12% 91.18% i.I  i0 -s 0.114 5.8.10 -15 8.2.10 -13 Table 2: Evaluation of simple markedness tests." ></td>
	<td class="line x" title="132:195	The probability of obtaining by chance performance equal to or better than the observed one is listed in the PValue column for each test." ></td>
	<td class="line x" title="133:195	sociated with the root node of the tree." ></td>
	<td class="line x" title="134:195	All training cases for which this test succeeds (fails) belong to the left (right) subtree of the decision tree." ></td>
	<td class="line x" title="135:195	The method proceeds recursively, by selecting a new variable (possibly the same as in the parent node) and a new cutting point for each subtree, until all the cases in one subtree belong to the same category or the data becomes too sparse." ></td>
	<td class="line x" title="136:195	When a node cannot be split further, it is labeled with the locally most probable category." ></td>
	<td class="line x" title="137:195	During prediction, a path is traced from the root of the tree to a leaf, and the category of the leaf is the category reported." ></td>
	<td class="line x" title="138:195	If the tree is left to grow uncontrolled, it will exactly represent the training set (including its peculiarities and random variations), and will not be very useful for prediction on new cases." ></td>
	<td class="line x" title="139:195	Consequently, the growing phase is terminated before the training samples assigned to the leaf nodes are entirely homogeneous." ></td>
	<td class="line x" title="140:195	A technique that improves the quality of the induced tree is to grow a larger than optimal tree and then shrink it by pruning subtrees (Breiman et al. , 1984)." ></td>
	<td class="line x" title="141:195	In order to select the nodes to shrink, we normally need to use new data that has not been used for the construction of the original tree." ></td>
	<td class="line x" title="142:195	In our classifier, we employ a maximum likelihood estimator based on the binomial distribution to select the optimal split at each node." ></td>
	<td class="line x" title="143:195	During the shrinking phase, we optimally regress the probabilities of children nodes to their parent according to a shrinking parameter ~ (Hastie and Pregibon, 1990), instead of pruning entire subtrees." ></td>
	<td class="line x" title="144:195	To select the optimal value for (~, we initially held out a part of the training data." ></td>
	<td class="line x" title="145:195	In a later version of the classifier, we employed cross-validation, separating our training data in 10 equally sized subsets and repeatedly training on 9 of them and validating on the other." ></td>
	<td class="line x" title="146:195	Log-linear regression (Santner and Duffy, 1989) is the second general supervised learning method that we explored." ></td>
	<td class="line x" title="147:195	In classical linear modeling, the response variable y is modeled as y -bTx+e where b is a vector of weights, x is the vector of the values of the predictor variables and e is an error term which is assumed to be normally distributed with zero mean and constant variance, independent of the mean of y. The log-linear regression model generalizes this setting to binomial sampling where the response variable follows a Bernoulli distribution (corresponding to a two-category outcome); note that the variance of the error term is not independent of the mean of y any more." ></td>
	<td class="line x" title="148:195	The resulting generalized linear model (McCullagh and Nelder, 1989) employs a linear predictor y = bTx + e as before, but the response variable y is non-linearly related to through the inverse logit function, eY y __ 1A-e' Note that y E (0, 1); each of the two ends of that interval is associated with one of the possible choices." ></td>
	<td class="line x" title="149:195	We employ the iterative reweighted least squares algorithm (Baker and Nelder, 1989) to approximate the maximum likelihood cstimate of the vector b, but first we explicitly drop the constant term (intercept) and most of the variables." ></td>
	<td class="line x" title="150:195	The intercept is dropped because the prior probabilities of the two outcomes are known to be equal." ></td>
	<td class="line x" title="151:195	3 Several of the variables are dropped to avoid overfitting (Duda and Hart, 1973); otherwise the regression model will use all available variables, unless some of them are linearly dependent." ></td>
	<td class="line x" title="152:195	To identify which variables we should keep in the model, we use the analysis of deviance method with iterative stepwise refinement of the model by iteratively adding or dropping one term if the reduction (increase) in the deviance compares 3The order of the adjectives in the pairs is randomized before training the model, to ensure that both outcomes are equiprobable." ></td>
	<td class="line x" title="153:195	201 12' 10 i  3 40% 50% 60% 70% 80% 90% Accuracy Figure 1: Probability densities for the accuracy of the frequency method (dotted line) and the smoothed log-linear model (solid line) on the morphologically unrelated adjectives." ></td>
	<td class="line x" title="154:195	favorably with the resulting loss (gain) in residual degrees of freedom." ></td>
	<td class="line x" title="155:195	Using a fixed training set, six of the fourteen variables were selected for modeling the morphologically unrelated adjectives." ></td>
	<td class="line x" title="156:195	Frequency was selected as the only component of the model for the morphologically related ones." ></td>
	<td class="line x" title="157:195	We also examined the possibility of replacing some variables in these models by smoothing cubic Bsplines (Wahba, 1990)." ></td>
	<td class="line x" title="158:195	The analysis of deviance for this model indicated that for the morphologically unrelated adjectives, one of the six selected variables should be removed altogether and another should be replaced by a smoothing spline." ></td>
	<td class="line x" title="159:195	7 Evaluation of the Complex Predictors For both decision trees and log-linear regression, we repeatedly partitioned the data in each of the two groups into equally sized training and testing sets, constructed the predictors using the training sets, and evaluated them on the testing sets." ></td>
	<td class="line x" title="160:195	This process was repeated 200 times, giving vectors of estimates for the performance of the various methods." ></td>
	<td class="line x" title="161:195	The simple frequency test was also evaluated in each testing set for comparison purposes." ></td>
	<td class="line x" title="162:195	From these vectors, we estimate the density of the distribution of the scores for each method; Figure 1 gives these densities for the frequency test and the log-linear model with smoothing splines on the most difficult case, the morphologically unrelated adjectives." ></td>
	<td class="line x" title="163:195	Table 3 summarizes the performance of the methods on the two groups of adjective pairs." ></td>
	<td class="line x" title="164:195	4 In order to assess the significance of the differences between 4The applicability of all complex methods was 100% in both groups." ></td>
	<td class="line x" title="165:195	the scores, we performed a nonparametric sign test (Gibbons and Chakraborti, 1992) for each complex predictor against the simple frequency variable." ></td>
	<td class="line x" title="166:195	The test statistic is the number of runs where the score of one predictor is higher than the other's; as is common in statistical practice, ties are broken by assigning half of them to each category." ></td>
	<td class="line x" title="167:195	Under the null hypothesis of equal performance of the two methods that are contrasted, this test statistic follows the binomial distribution with p = 0.5." ></td>
	<td class="line x" title="168:195	Table 3 includes the exact probabilities for obtaining the observed (or more extreme) values of the test statistic." ></td>
	<td class="line x" title="169:195	From the table, we observe that the tree-based methods perform considerably worse than frequency (significant at any conceivable level), even when cross-validation is employed." ></td>
	<td class="line x" title="170:195	Both the standard and smoothed log-linear models outperform the frequency test on the morphologically unrelated adjectives (significant at the 5% and 0.1% levels respectively), while the log-linear model's performance is comparable to the frequency test's on the morphologically related adjectives." ></td>
	<td class="line x" title="171:195	The best predictor overall is the smoothed log-linear model." ></td>
	<td class="line x" title="172:195	5 The above results indicate that the frequency test essentially contains almost all the information that can be extracted collectively from all linguistic tests." ></td>
	<td class="line x" title="173:195	Consequently, even very sophisticated methods for combining the tests can offer only small improvement." ></td>
	<td class="line x" title="174:195	Furthermore, the prominence of one variable can easily lead to overfitting the training data in the remaining variables." ></td>
	<td class="line x" title="175:195	This causes the decision tree models to perform badly." ></td>
	<td class="line x" title="176:195	8 Conclusions and Future Work We have presented a quantitative analysis of the performance of measurable linguistic tests for the selection of the semantically unmarked term out of a pair of antonymous adjectives." ></td>
	<td class="line x" title="177:195	The analysis shows that a simple test, word frequency, outperforms more complicated tests, and also dominates them in terms of information content." ></td>
	<td class="line x" title="178:195	Some of the tests that have been proposed in the linguistics literature, notably tests that are based on the formal complexity and differentiation properties of the words; fail to give any useful information at all, at least with the approximations we used for them (Section 3)." ></td>
	<td class="line x" title="179:195	On the other hand, tests based on morphological productivity are valid, although not as accurate as frequency." ></td>
	<td class="line x" title="180:195	Naturally, the validity of our results depends on the quality of our measurements." ></td>
	<td class="line x" title="181:195	While for most of the variables our measurements are necessarily apsit should be noted here that the independence assumption of the sign test is mildly violated in these repeated runs, since the scores depend on collections of independent samples from a finite population." ></td>
	<td class="line x" title="182:195	This mild dependence will increase somewhat the probabilities under the true null distribution, but we can be confident that probabilities such as 0.08% will remain significant." ></td>
	<td class="line x" title="183:195	202 Morphologically Morphologically Overall Predictor tested unrelated related Accuracy P-Value Accuracy P-Value Accuracy P-Value Frequency 75.87% 97.15% 81.07% Decision tree (no cross-validation) 64.99% 8.2.10 -53 94.40% 1.5.10 -l 72.05% 1.710 TM Decision tree 10-40 75.19% 7.2.10 -47 (cross validated) 69.13% 94.40% 1.510 -l Log-linear model (no smoothing) 76.52% 0.0281 97.17% 1.00 81.55% 0.0228 Log-linear model (with smoothing) 76.82% 0.0008 97.17% 1.00 81.77% 0.0008 Table 3: Evaluation of the complex predictors." ></td>
	<td class="line x" title="184:195	The probability of obtaining by chance a difference in performance relative to the simple frequency test equal to or larger than the observed one is listed in the PValue column for each complex predictor." ></td>
	<td class="line x" title="185:195	proximate, we believe that they are nevertheless of acceptable accuracy since (1) we used a representative corpus; (2) we selected both a large sample of adjective pairs and a large number of frequent adjectives to avoid sparse data problems; (3) the procedure of identifying secondary words for indirect measurements based on morphological productivity operates with high recall and precision; and (4) the mapping of the linguistic tests to comparisons of quantitative variables was in most cases straightforward, and always at least plausible." ></td>
	<td class="line x" title="186:195	The analysis of the linguistic tests and their combinations has also led to a computational method for the determination of semantic markedness." ></td>
	<td class="line x" title="187:195	The method is completely automatic and produces accurate results at 82% of the cases." ></td>
	<td class="line x" title="188:195	We consider this performance reasonably good, especially since no previous automatic method for the task has been proposed." ></td>
	<td class="line x" title="189:195	While we used a fixed set of 449 adjectives for our analysis, the number of adjectives in unrestricted text is much higher, as we noted in Section 2." ></td>
	<td class="line x" title="190:195	This multitude of adjectives, combined with the dependence of semantic markedness on the domain, makes the manual identification of markedness values impractical." ></td>
	<td class="line x" title="191:195	In the future, we plan to expand our analysis to other classes of antonymous words, particularly verbs which are notoriously difficult to analyze semantically (Levin, 1993)." ></td>
	<td class="line x" title="192:195	A similar methodology can be applied to identify unmarked (positive) versus marked (negative) terms in pairs such as agree: dissent." ></td>
	<td class="line x" title="193:195	Acknowledgements This work was supported jointly by the Advanced Research Projects Agency and the Office of Naval Research under contract N00014-89-J-1782, and by the National Science Foundation under contract GER-90-24069." ></td>
	<td class="line x" title="194:195	It was conducted under the auspices of the Columbia University CAT in High Performance Computing and Communications in Healthcare, a New York State Center for Advanced Technology supported by the New York State Science and Technology Foundation." ></td>
	<td class="line x" title="195:195	We wish to thank Judith Klavans, Rebecca Passonneau, and the anonymous reviewers for providing us with useful comments on earlier versions of the paper." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W95-0111
Automatic Suggestion Of Significant Terms For A Predefined Topic
Zhou, Joe F.;Dapkus, Pete;"></td>
	<td class="line x" title="1:189	I Automatic Suggestion of Significant Terms for a Predefined Topic Joe Zhou and Pe~Dapkus LEXIS-NEXIS, a Division of Reed Elsevier, Inc. 9555 Springboro Pike Miamisburg, OH 45342 {joez,peted} @ lexis-nexis.com ABSTRACT This paper presents a preliminary experiment in automatically suggesting significant terms for a predefined topic." ></td>
	<td class="line x" title="2:189	The general method is to compare a topically focused sample created around the predefined topic with a larger and more general base sample." ></td>
	<td class="line x" title="3:189	A set of statistical measures are used to identify significant word units in both samples." ></td>
	<td class="line x" title="4:189	Identification of single word terms is based on the notion of word intervals." ></td>
	<td class="line x" title="5:189	Two-word terms are identified through the computation of mutual information, and the extension of mutual information assists in capturing multi-word terms." ></td>
	<td class="line x" title="6:189	Once significant terms of all these three types are identified, a comparison algorithm is applied to differentiate terms across the two data samples." ></td>
	<td class="line x" title="7:189	If significant changes in the values of certain statistical variables are detected, associated terms will selected as being topic-oriented and included in a suggested list." ></td>
	<td class="line x" title="8:189	To check the quality of the suggested terms, we compare them against terms manually determined by the domain expert." ></td>
	<td class="line x" title="9:189	Though overlaps vary, we find that the automatical suggestion provides more terms that are useful for describing the predefined topic." ></td>
	<td class="line x" title="10:189	1." ></td>
	<td class="line x" title="11:189	INTRODUCTION As we are facing the growing amount of on-line text, the use of text analysis techniques to access information from electronic sources has become more popular and, at the same time, more difficult." ></td>
	<td class="line x" title="12:189	Currently, the effectiveness of such techniques is evaluated not only on how easily they can be applied to text sources to extract information and represent it in a systematic format (Walker 1983), but also on whether they can be applied to large text corpora of several tens of thousand of words." ></td>
	<td class="line x" title="13:189	One of the applications of text analysis is to identify and extract significant terminology from running text." ></td>
	<td class="line x" title="14:189	Choueka (1988), for example, describes an experiment for locating interesting collocational expressions from large textual databases." ></td>
	<td class="line x" title="15:189	A collocational expression, as Choueka defines it, is =sequences of words whose unambiguous meaning cannot be dedved from that of their components'." ></td>
	<td class="line oc" title="16:189	Other representative collocation research can be found in Church and Hanks (1990) and Smadja (1993)." ></td>
	<td class="line o" title="17:189	Though all statistically-based, their definitions of collocations are different from one another." ></td>
	<td class="line x" title="18:189	Unlike Choueka (1988), Church and Hanks (1990) identify as collocations both interrupted and uninterrupted sequences of words." ></td>
	<td class="line pc" title="19:189	Unlike Church and Hanks (1990), Smadja (1993) goes beyond the 'two-word' limitation and deals with 'collocations of arbitrary length'." ></td>
	<td class="line x" title="20:189	131 The primary goal of collocation research is to build a comprehensive lexicographic toolkit, or to assist automatic language generation applications." ></td>
	<td class="line x" title="21:189	Therefore, the focus is on the extraction of all Interesting word pattems without distinction of domain specificity." ></td>
	<td class="line x" title="22:189	Identifying domain-specific terminology is another research effort." ></td>
	<td class="line x" title="23:189	Gierl and Frost (1992) descdbe their approach to extracting terminological knowledge from medical texts." ></td>
	<td class="line x" title="24:189	Following Church and Hanks (1990), they use mutual information to select significant two-word patterns, but, at the same time, a lexical inductive process is incorporated which, as they claim, can improve the collection of domain-specific terms." ></td>
	<td class="line x" title="25:189	Justeson and Katz (1993) introduce an algorithm by which technical terms in running text can be identified." ></td>
	<td class="line x" title="26:189	Prior to the development of their algorithm, they performed a thorough study on the linguistic properties of technical terminology." ></td>
	<td class="line x" title="27:189	They report that, structurally, technical terms make heavy use of noun compounds." ></td>
	<td class="line x" title="28:189	In technical terminology, word constituents are limited to adjectives, nouns and occasionally prepositions." ></td>
	<td class="line x" title="29:189	Verbs, adverbs, or conjunctions are extremely rare." ></td>
	<td class="line x" title="30:189	At the discourse level, technical terms tend to be repetitive." ></td>
	<td class="line x" title="31:189	With these observations in mind, they developed an algorithm which has proved to be effective and domain independent." ></td>
	<td class="line x" title="32:189	In this paper, a preliminary experiment is presented in automatically suggesting significant terms for a predefined topic." ></td>
	<td class="line x" title="33:189	The general method is to compare a topic focused sample based on the predefined topic with a larger and more general base sample." ></td>
	<td class="line x" title="34:189	A set of statistical measures are used to identify significant word units in both samples." ></td>
	<td class="line x" title="35:189	Identification of single word terms is based on the notion of word intervals." ></td>
	<td class="line x" title="36:189	Two-word terms are identified through the computation of mutual information, and an extension of mutual information assists in capturing multi-word terms." ></td>
	<td class="line x" title="37:189	Once significant terms of all these three types are identified, a comparison algorithm is applied to differentiate terms across the two samples." ></td>
	<td class="line x" title="38:189	If significant changes in the values of certain statistical variables are detected, associated terms are selected from the focused sample as being topic-oriented and included in a suggested list." ></td>
	<td class="line x" title="39:189	To check the quality of the suggested terms, we compare them against terms manually determined by a domain expert." ></td>
	<td class="line x" title="40:189	Though the numbers of matches vary, we find that our automatic suggestion process provides more terms (than the manual process) that are useful for describing the predefined topic." ></td>
	<td class="line x" title="41:189	2." ></td>
	<td class="line x" title="42:189	METHODOLOGY 2.1 Manual versus Automatic Term Suggestion TO manually select significant terms for a predefined topic, the domain expert first creates a topic focused sample from one specific source or a combination of sources." ></td>
	<td class="line x" title="43:189	Then, he or she reads the documents, providing a relevance judgment (i.e. a reader-assigned score) to each document." ></td>
	<td class="line x" title="44:189	By carefully examining relevant documents in the focused sample, a list of terms that are deemed to be significant for the definition of the topic is identified." ></td>
	<td class="line x" title="45:189	In many cases, it is possible that the domain expert would introduce some terms based on his or her own professional knowledge about the topic." ></td>
	<td class="line x" title="46:189	These terms may be highly prominent for the topic, yet may not necessarily occur in the focused sample." ></td>
	<td class="line x" title="47:189	132 For automatic suggestion of topical terms, initial attempts were made using the sample documents the domain expert created." ></td>
	<td class="line x" title="48:189	The results were not impressive." ></td>
	<td class="line x" title="49:189	The statistical Information generated from the sample documents was not rich and sufficient enough for any discriminative judgment." ></td>
	<td class="line x" title="50:189	Our experience showed that, to draw terms that are reflective of a given topic, a much larger and more general base sample is required." ></td>
	<td class="line x" title="51:189	Such a base sample should be randomly sampled from the same source as the focused sample and it should contain an array of different topics." ></td>
	<td class="line x" title="52:189	Once the baseline statistics are generated from both data collections, a meaningful comparison could spot terms that occur with unusual frequency in the focused sample." ></td>
	<td class="line x" title="53:189	These terms would constitute good candidates for topically sensitive terminological units (Steier and Belew 1994)." ></td>
	<td class="line x" title="54:189	2.2 Focused Sample and Base Sample For our experiments of automatic term suggestion, we selected a predefined topic called 'European Politics and Business'." ></td>
	<td class="line x" title="55:189	The focused sample was originally created by the domain expert using the 1988 United Press International (UPI)." ></td>
	<td class="line x" title="56:189	Table 1 presents statistical information about this dataset." ></td>
	<td class="line x" title="57:189	After reading each of the relevant documents found in the focused sample, the domain expert manually determined 347 topical terms." ></td>
	<td class="line x" title="58:189	Table 2 provides the statistical breakdown of these terms." ></td>
	<td class="line x" title="59:189	Table 1: Focused and Base Samples Data File Source/Name Size (bytes) Unique Words Focused Sample Sample from 1988 UPI 1,015200 12,065 (5,045') Base Sample Sample from 27,322,598 73,583 (33,114') 1987,1988,1989 UPI * only words which occur more than 3 times were used in the experiments Table 2: Predefined Topic and its Manually Determined Topical Terms Predefined Topic one-word two.word multi.word total terms i European Politics & Business 276 36 35 347 Since the focused sample was drawn from the source of 1988 UPI, the construction of its corresponding base sample was also initiated from the same source of the same year." ></td>
	<td class="line x" title="60:189	Our experiments demonstrated that, in order to obtain a random assortment of topics to be included in the base sample, it may be meaningful to sample documents from the time pedod before and after the focused documents." ></td>
	<td class="line x" title="61:189	Therefore, the final base sample was created by randomly drawing documents from the years of 1987, 1988 and 1989." ></td>
	<td class="line x" title="62:189	The size of this dataset is about 27 times larger than the sample data file (see Table 1 )." ></td>
	<td class="line x" title="63:189	133 Though the ratio between the focused and base samples was arbitrary, in order to generate meaningful statistics, we felt that the base sample should be at least 20 times larger in size than the focused sample." ></td>
	<td class="line x" title="64:189	(For the sake of discussion, hereafter, we may sometimes refer to the focused sample as 'focused' and the base sample as 'base')." ></td>
	<td class="line x" title="65:189	2.3 Experimental Procedure The general method we adopted is as follows." ></td>
	<td class="line x" title="66:189	First, we identified statistically significant terms from both samples." ></td>
	<td class="line x" title="67:189	Next, a comparison algorithm was applied to these two sets of terms to single out those that were common to both samples, yet whose patterns of occurrences differed between these two samples." ></td>
	<td class="line x" title="68:189	Finally, we analyzed and presented this set of terms as content odented candidates for the predefined topic, in this case 'European Politics and Business'." ></td>
	<td class="line x" title="69:189	The terms suggested are split into three categones: single word terms, two-word terms and multi-word terms (or phrases)." ></td>
	<td class="line x" title="70:189	The following three sections descnbe in detail the methods for generating each of the three categories." ></td>
	<td class="line x" title="71:189	2.4 Suggesting Single Word Terms Automatically suggesting single word terms as being topically oriented has been most challenging." ></td>
	<td class="line x" title="72:189	Our experiments indicated that the ffirst order' statistics, probability and entropy alone, are not sufficient for gathering information about the topicality of a word in running text." ></td>
	<td class="line x" title="73:189	The information in both measurements is essentially equivalent since entropy is just the log inverse of probability." ></td>
	<td class="line x" title="74:189	We found that the 'second-order' statistics, such as vadance or standard deviation of term frequencies across documents, provide greater insight into topicality." ></td>
	<td class="line x" title="75:189	We selected the interval between the occurrences of a word as the basis for analysis." ></td>
	<td class="line x" title="76:189	Our intuitions led us to believe that topical single words should appear more frequently and more regularly, i.e. at approximately even intervals, in the focused sample than in the base sample." ></td>
	<td class="line x" title="77:189	The focused sample represents, more or less, a topical sublanguage set while the base sample a general language set." ></td>
	<td class="line x" title="78:189	Unlike probability and entropy statistics which yield average scores for the whole document, the use of interval makes it possible to get an 'instantaneous' measure at any location in the document." ></td>
	<td class="line x" title="79:189	More specifically, an interval can be measured 'instantaneously' at any point in the text between the occurrences of a particular word." ></td>
	<td class="line x" title="80:189	Though using interval alone might still not be sufficient for identifying word topicality, it allowed us to measure the vadance which would help identify words that were always changing in their rate of occurrences." ></td>
	<td class="line x" title="81:189	Thus, three scores were generated for each word: the mean log interval, the standard deviation of the mean log interval, and the normalized standard deviation of the mean log interval." ></td>
	<td class="line x" title="82:189	The use of a log scale for these measurements is to minimize the effect of unduly large variations in words with long mean intervals." ></td>
	<td class="line x" title="83:189	The normalized standard deviation is produced by simply dividing the raw standard deviation by the mean log interval." ></td>
	<td class="line x" title="84:189	In most cases, raw standard deviation is found to be larger for words having long mean intervals." ></td>
	<td class="line x" title="85:189	In order to compare the standard deviations across words of different intervals, we found this normalization process quite useful." ></td>
	<td class="line x" title="86:189	134 i After scores were generated for all the words in both the focused sample and the base sample, score comparisons between the two samples were carried out in two ways: comparing the intervals and comparing the standard deviations." ></td>
	<td class="line x" title="87:189	To compare the intervals, the =base' mean log interval was subtracted from the 'focused' mean log Interval and divided by the raw standard deviation from the base sample." ></td>
	<td class="line x" title="88:189	The result represents the change of mean log intervals." ></td>
	<td class="line x" title="89:189	More explicitly, it yields the number of standard deviations that the 'focused' mean log interval is different from the =base' mean log interval." ></td>
	<td class="line x" title="90:189	The more negative;the value, the more significant the change, and the more prominent the word would appear in the focused sample." ></td>
	<td class="line x" title="91:189	To compare the standard deviations, the normalized =base' standard deviation was subtracted from the normalized 'focused' standard deviation." ></td>
	<td class="line x" title="92:189	The difference symbolizes how the word is distributed in ~e focused sample." ></td>
	<td class="line x" title="93:189	The more negative the value is, the more 'bursty' the word is distributed, and the more likely it is content oriented since 'content words tend to appear in 'bursts' (Church and Mercer 1993)." ></td>
	<td class="line x" title="94:189	If a single word term is found in both data samples and it receives negative scores from both interval and standard deviation comparisons, it would be included in the suggested list as being topical onented." ></td>
	<td class="line x" title="95:189	2.5 Suggesting Two-Word Terms The method for suggesting two-word terms tumed out to be much simpler than that for single word terms though the same techniques are equally applicable." ></td>
	<td class="line x" title="96:189	Here, the traditional mutual information score was used." ></td>
	<td class="line x" title="97:189	As stated in Church, et al.(1991) and elsewhere, the mutual information measurement can be expressed as:." ></td>
	<td class="line x" title="99:189	(:p(WlW2)) l(WlW2) = lg~,p(wl)p(w2) where p(wlw2) is the frequency in the data collection of the two-word compound (wl,w2); and p(wl) and p(w2) the frequency of the word constituents." ></td>
	<td class="line x" title="100:189	The highest mutual information score indicates that the individual probabilities are low while the two words occur together frequently." ></td>
	<td class="line x" title="101:189	Two steps led to our automatic suggestion of topic-oriented two-word terms." ></td>
	<td class="line x" title="102:189	First, the mutual information score was computed for each pair of words that occur in each of the two samples." ></td>
	<td class="line x" title="103:189	To capture topicality, we were only interested in pairs of words with high mutual information scores." ></td>
	<td class="line x" title="104:189	Therefore, any pair which contained =closed class' words, such as determiners, prepositions, auxiliaries, or single letters, digit numbers, or overly common verbs like 'give', 'take', etc. , were excluded." ></td>
	<td class="line x" title="105:189	Such an exclusion not only helped getting pairs of words with high mutual information scores, but also sped up computation significantly." ></td>
	<td class="line x" title="106:189	A threshold value was also set such that if any two-word unit occurred less than 3 times in the sample or received a mutual information score lower than 6.0, it was eliminated and would not participate in the next comparison measurement." ></td>
	<td class="line x" title="107:189	135 With the mutual information scores in hand, a 'delta' score was generated by subtracting the 'base' mutual information score from the ffocused' mutual information score." ></td>
	<td class="line x" title="108:189	Topically, prominent two-word terms normally have lower scores in the focused sample that is 'keyed' to their topic." ></td>
	<td class="line x" title="109:189	This is because the constituent words distribute in wider range of contexts." ></td>
	<td class="line x" title="110:189	The probability of them occurring separately increases relative to the probability of them occurring together (Steier and Belew 1994)." ></td>
	<td class="line x" title="111:189	Therefore, the more negative the 'delta' score, the more topically sensitive the two-word term is. If a two-word term occurs in both data samples and receives a negative 'delta' score, it would be included in the suggested list as being topically onented." ></td>
	<td class="line x" title="112:189	2.6 Suggesting Multi-Word Terms When automatically suggesting content two-word terms, we looked at the mutual information scores for adjacent words." ></td>
	<td class="line x" title="113:189	For multi-word terms, the mutual information score was calculated for non-adjacent words." ></td>
	<td class="line x" title="114:189	Our intuitions led us to believe that if there is a significant statistical linkage, i.e. a high mutual information score, between such a pair of words, it is highly possible that they belong to a larger linguistic component." ></td>
	<td class="line x" title="115:189	Our first step was to compute mutual information scores for a word unit separated by a distance of two (i.e. having one unspecified word separating them)." ></td>
	<td class="line x" title="116:189	Two cdteda apply when selecting 'interesting' word units." ></td>
	<td class="line x" title="117:189	Their mutual information score must be 10 or greater." ></td>
	<td class="line x" title="118:189	Following the observations by Steier and Belew (Steier and Belew1994), we only selected pairs which received lower mutual information score in the focused sample than in the base sample." ></td>
	<td class="line x" title="119:189	Once an 'interesting' word unit of distance two was selected, a concordance was built of all sentences containing that word unit." ></td>
	<td class="line x" title="120:189	These sentences were compared for matching text." ></td>
	<td class="line x" title="121:189	If a stdng of text was found to include that word unit and, at the same time, occur most frequently in the concordance, its leading and trailing 'closed-set' words (if any) were chopped off." ></td>
	<td class="line x" title="122:189	The remaining text stdng was presented as a suggested multi-word term." ></td>
	<td class="line x" title="123:189	3." ></td>
	<td class="line x" title="124:189	RESULTS and DISCUSSION 3.1 Suggested Single Terms The focused sample drawn from the 1988 UPI data contains 12,065 unique words." ></td>
	<td class="line x" title="125:189	Among them, 5,045 are frequent enough (occurring 3 times or more) to calculate statistics for our experiments (refer to Table 1)." ></td>
	<td class="line x" title="126:189	The comparison algorithm identified 2,010 suggested terms based on the fact that they received negative scores for both 'change of mean log interval' and 'distribution burstiness' comparisons." ></td>
	<td class="line x" title="127:189	These negative scores indicate that these single word terms have shorter intervals and more regular occurrences in the focused sample." ></td>
	<td class="line x" title="128:189	We compared the suggested list against the single word terms manually selected by the domain expert." ></td>
	<td class="line x" title="129:189	The results are summarized in Table 3." ></td>
	<td class="line x" title="130:189	136 Table 3: Statistics of the Suggested Single Word Terms suggested 2,010 Comparison of Suggested and Manual Terms total manual 276 not possible* 129 no statistics* 91 possible* 56 hits 42 percent included 75% * not possible: terms not existing In the focused sample * no statbtics: terms which have less than 3 occurrences in the focused sample * possible: targeted terms Of the 276 topical single terms determined by the domain expert, 129 terms do not exist in the focused sample." ></td>
	<td class="line x" title="131:189	As explained earlier, these are the terms intellectually introduced by the domain expert." ></td>
	<td class="line x" title="132:189	Almost half of these terms are geographical names in Europe, such as albania, albertville, andorra, barcelona, belarus, belorus, bosnia, byelorussia, chancellors, comecon, cp, croatia, erm, eurocurrency, eurofed, europeanization, europeanwide, europeenne, europewide, gaullist, gaullists, gilbraltar, greenland, guemsey, kazakhstan, kirghizia, kirgizia, kyrgystan, kzakhstan, labour, liechtenstein, moldavia, moldova, monaco, nc, nib, nicosia, nuuk, pentagonale, reunify, reykjavik, salzburg, sicily, slovenia, svalbard, tadzhikistan, tajikistan, tajikstan, tirana, tirane, tories, torshavn, turkmenia, turkmenistan, uk, ussr, uzbekistan, vaduz, valletta, weu Of the remaining 147 actually occurring terms, 91 are not frequent enough to be included in our experiments." ></td>
	<td class="line x" title="133:189	They occur in the focused sample two times or less." ></td>
	<td class="line x" title="134:189	Again, some of them are geographical names in Europe." ></td>
	<td class="line x" title="135:189	amsterdam, athens, azerbaljan, bulgaria, estonia, euro, eurodollar, eurodollars, georgia, hamburg, holland, iceland, jersey, latvia, liberals, lithuania, naples, oecd, prague, reunified, rome, russia, serbia, sofia, tory, ukraine, unification These non-existent and under-represented terms left us with a maximum of 56 terms we could catch in the suggested ten'ns list." ></td>
	<td class="line x" title="136:189	Of these, 42 were caught with an accuracy rate of 75% (see Appendix for details)." ></td>
	<td class="line x" title="137:189	Further analysis of the missing 14 terms reveals that they were not found in the suggested list due to the statistical constraints we established for our experiments." ></td>
	<td class="line x" title="138:189	As shown in Table 4, 13 of these terms received negative scores either for 'change of mean log interval' or for 'distribution burstiness', but not for both." ></td>
	<td class="line x" title="139:189	We believe that their inclusion is possible since they represent what we would call 'border-line' suggested terms." ></td>
	<td class="line x" title="140:189	137 Table 4: =Missed' single word terms single.word term dgtl dgt2 dgt3 dgt4 dgt5 dgt6 dgt7 dgt8 portugal 10 13.75 0.26 16.86 0.23 3.83 -0.81 0.03 europeans 23 12.55 0.35 15.64 0.32 4.98 -0.62 0.03 eec 3 15.49 0.39 19.28 0.32 6.21 -0.61 0.06 luxembourg 12 13.49 0.42 17.06 0.36 6.07 -0.59 0.07 !copenhagen 3 15.49 0.47 18.54 0.34 6.23 -0.49 0.14 i ;~ cyprus 6 14.49 0.44 18.28 0.43 7.89 -0.48 0.01 yugoslavia 12 13.49 0.47 15.33 0.37 5.66 -0.32 0.10 finland 10 13.75 0.51 15.52 0.46 7.19 -0.25 0.05 kgb 5 14.75 0.57 16.41 0.44 7.29 .-0.23 0.13 sweden 13 13.38 0.48 14.26 0.44 6.33 -0.14 0.03 turkey 11 13.62 0.53 14.47 0.50 7.25 -0.12 0.03 czechoslovakia 9 13.91 0.09 13.70 0.46 6.29 0.03 -0.36 switzerland 9 13.91 0.21 13.81 0.47 6.48 0.01-0.26 Statistics Measurements (dgt = digit) dgtl: number of occurrences On the focused sample) dgt2: mean log interval (in the focused sample) dgt3: normalized SD of mean log interval (in the focused sample) dgt4: mean log interval (in the base sample) dgtS: normalized SD of mean log interval (in the base sample) dgt6: raw SD of mean log interval (in the base sample) dgt7: ((2nd digit 4th digit) / 6th digit)) dgtS: (3rd digit." ></td>
	<td class="line x" title="141:189	Sth digit) Admittedly, the suggested list with the total of 2,010 terms is a fairly large one." ></td>
	<td class="line x" title="142:189	It obviously contains terms that are not topic oriented." ></td>
	<td class="line x" title="143:189	We followed the observations made by Justeson and Katz (1993) and introduced a =post-editing' process." ></td>
	<td class="line x" title="144:189	As a result, the list was reduced to 886 terms." ></td>
	<td class="line x" title="145:189	Basically, we removed from the original list all the =closed-set' words such as determiners, prepositions, auxiliaries, conjunctions, single letters, etc. , as well as other less semantically laden words such as adverbs and verbs." ></td>
	<td class="line x" title="146:189	3.2 Suggested Two-Word Terms Among 512 =interesting' two-word terms, 170 receive negative =delta' scores." ></td>
	<td class="line x" title="147:189	These 164 terms were presented in our suggested two-word terms (see Appendix for details)." ></td>
	<td class="line x" title="148:189	138 I A total of 36 topical terms were manually determined based on the UPI focused sample." ></td>
	<td class="line x" title="149:189	Of this number, only 26 are actually existent terms, which means that 10 terms were introduced independent of the source material." ></td>
	<td class="line x" title="150:189	Among these 26 terms, 6 were too infrequent to generate meaningful statistics though the mutual information scores are high (see Table 5)." ></td>
	<td class="line x" title="151:189	Five terms, i.e. E C, U K, the Channel, the Continent, and the Wal/failed to participate in statistical screening because they contain 'closed-set' words, i.e. single letters and the determiner the." ></td>
	<td class="line x" title="152:189	Table 5: 'No statistics' two-word terms two-word term digit1 digit2 monte carlo 1 13.61674723 i social democrats 1 9.58432575 coalition govea'nment . 1 7.59034954 supreme soviet 1 5.06985277 J downing street 1 11.75425075 socialist party 2 6.36709503 Statistical measurements digitl: frequency (in the focused sample) digit2: mutual information score Of the remaining catchable15 two-word terms, 8 are included in the suggested list." ></td>
	<td class="line x" title="153:189	Table 6 summarizes the statistics of the suggested two-word terms." ></td>
	<td class="line x" title="154:189	Table 6: Statistics of the Suggested Two-Word Terms Comparison of Suggested and Manual Terms total total not no l percent suggested manual possible* statistics* possible* hits i included \[ 170 36 10 11 15 8 53% * not possible: terms not existing In the focused sample * no statistics: terms which have less than 3 occurrences in the focused sample * possible: targeted terms Further screening revealed that 3 manually selected two-word terms (i.e. cold war, common market, and North Sea) were actually captured in the 512 'interesting' list." ></td>
	<td class="line x" title="155:189	They were not included in the suggested list because they did not receive negative 'delta' scores." ></td>
	<td class="line x" title="156:189	The suggested list fails to include 4 manually selected two-word terms because their mutual information scores go up." ></td>
	<td class="line x" title="157:189	Typically, content oriented two-word terms within the topically related subset of documents are expected to go down." ></td>
	<td class="line x" title="158:189	This might be caused by the individual word probabilities." ></td>
	<td class="line x" title="159:189	To use Steier and Belew's terms (Steier and Belew 1994), these pairs appear more 'opaque', meaning that their constituent words are more probable individually than when they are combined inthe focused sample." ></td>
	<td class="line x" title="160:189	Table 7 lists these 4 two-word terms appearing in both samples." ></td>
	<td class="line x" title="161:189	139 Table 7: 'Missed' two-word terms Sample two-word term frequency MI score 'base' atlantic alliance 11 8.80256520 'focused' atlantic alliance 4 9.36193333 'base' cold war 54 8.04486800 'focused' cold war 11 9.97241419 'base' common market 26 6.86310460 'focused' common market 17 7.84030540 'base' 49 'focused' united kingdom united kingdom 25 7.55353160 7.80705217 Our suggested two-word terms list (see the Appendix) contains quite a number of useful additional terms about the targeted predefined topic 'European Politics and Business'." ></td>
	<td class="line x" title="162:189	The following are some examples: US-European relations/politics: armed forces, diplomatic relations, nuclear missiles, nuclear weapons, trade barriers European Business: bilateral trade, economic reform, market integration, pdvate enterprise, pdvate investment Notable European entities: banca commerciale, berlin wall, bdtish spies, swiss francs, brussels belgium Heads of state: felipe gonzalez, francois mitterrand, mikhail gorbachev 3.3 Suggested Multi-Word Terms A total of 97 multi-word terms were extracted from the focused sample for inclusion in the suggested list (see Appendix)." ></td>
	<td class="line x" title="163:189	Admittedly, some of them are simply sentence fragments instead of real phrases." ></td>
	<td class="line x" title="164:189	Of the 35 multi-word terms manually selected by the domain expert, 26 actually occur in the focused sample." ></td>
	<td class="line x" title="165:189	As with the single word and two-word terms, the other 9 multi-word terms are simply intellectual introductions from the domain expert." ></td>
	<td class="line x" title="166:189	Of the 26 tenns, 22 occur frequently enough to generate meaningful statistics." ></td>
	<td class="line x" title="167:189	Out of these 22 catchable terms, only 5 are included in the suggested list." ></td>
	<td class="line x" title="168:189	Table 8 presents the statistical summary." ></td>
	<td class="line x" title="169:189	140 Table 8: Statistics of the Suggested Multi-Word Terms total suggested 97 Comparison of Suggested and Manual Terms total manual 35 not possible* no statistics* 4 possible*!" ></td>
	<td class="line x" title="170:189	hits 22 5 * not possible: terms not existing in the focused sample * no statistics: terms which have less than 3 occurrences in the focused sample * possible: targeted terms percent included 23% One possible explanation for not being able to match more manual selections is that most of the two-word terms that could have been used to detect these phrases consist of two common words, such as house, lords, fund, system." ></td>
	<td class="line x" title="171:189	These two-word terms typically generate fairly low mutual information scores since the constituent words occur frequently by themselves." ></td>
	<td class="line x" title="172:189	It is important to point out that the suggested list does contain a number of useful multi-word terms that are related to the targeted predefined topic =European Politics and Business'." ></td>
	<td class="line x" title="173:189	For example, US-European relations/politics: short range nuclear missiles, tactical nuclear weapons, conventional arms reduction, multi party system European Business: gross national product, higher interest rates and inflation, Bank of England, North Sea Oil Notable European entities: predominantly Catholic Idsh Republic, three Bdtish hostages, World War II, Roman Catholic Church Heads of state or notable dignitaries: Secretary of State James Baker, Secretary of State George Shultz, French President Francois Mitterrand, West German Chancellor Helmut Kohl, Soviet leader Mikhail Gorbachev, Soviet Foreign Minister Eduard Shevardnadze 141 4." ></td>
	<td class="line x" title="174:189	CONCLUSION This paper presents a preliminary experiment in identifying significant terminological units from running text." ></td>
	<td class="line x" title="175:189	By comparing a focused sample randomly drawn for a predefined topic against a larger and more general base sample, we can automatically suggest topic-oriented terms based on the detection of significant changes in some statistical measurements." ></td>
	<td class="line x" title="176:189	Our experiment on one predefined topic demonstrated that, compared to the manual selection of the topical terms, our suggested lists do contain more useful terms that can be used to descdbe the topic." ></td>
	<td class="line x" title="177:189	We also found that the method is efficient enough for applications to very large textual corpora." ></td>
	<td class="line x" title="178:189	Our next step is to further refine the methods by carrying out more experiments across different topics." ></td>
	<td class="line x" title="179:189	We mentioned a number of times that our methods were developed based on our intuitive assumptions or hypotheses." ></td>
	<td class="line x" title="180:189	More experiments on more topics will prove whether we can obtain positive and consistent results." ></td>
	<td class="line x" title="181:189	Identification of significant terms from running text can be very useful in building intelligent information management systems." ></td>
	<td class="line x" title="182:189	Terms identified are good candidates for key word indexing of electronic sources." ></td>
	<td class="line x" title="183:189	Topic specificity can assist in grouping or clustering on-line documents." ></td>
	<td class="line x" title="184:189	For an information retrieval system, terms identified for a pre-determined subject can be used to develop specialized libraries or files for targeted user groups." ></td>
	<td class="line x" title="185:189	Our experiment demonstrated that the methods described can identify vadous people names, organization enlJties and other proper names." ></td>
	<td class="line x" title="186:189	Those special text tokens are important for constructing text extraction systems." ></td>
	<td class="line x" title="187:189	ACKNOWLEDGEMENTS This research was done while the second author worked at LEXIS-NEXIS dudng the summer of 1994." ></td>
	<td class="line x" title="188:189	The authors would like to thank Dan Pliske, Mark Wasson and Rob Keefer for helpful comments on this paper, and Rita Freese for proofreading the final draft." ></td>
	<td class="line x" title="189:189	The authors were also benefited from numerous conversations with Ken Church at Bell Labs." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C96-1009
Extracting Nested Collocations
Frantzi, Katerina T.;Ananiadou, Sophia;"></td>
	<td class="line x" title="1:183	Extracting Nested Collocations Katerina T. Frantzi and Sophia Ananiadou Dept. of ()omputing Manch(;sl;(;r Metroi)olil;an Univ(;rsity Manchester, MI 5G\]), U.K. {K.Frantzi,S.Ananiadou}(~(toc.mmu.a(:.uk Abstract 'l?his paper 1)rovidcs an at)l)roa(:h to tim semi-aul;onmtic exl;i'action of (:ollocaIJons flom eorl)ora using sl;atisti(:s. The growing availability of lm'ge textual cort)ora, and the in(:reasing number of applications of colloeal;ion extra(:tion, has given risc~ 1;o wu.ious apt)roaches on the I;opi(:." ></td>
	<td class="line x" title="2:183	In l;his palter, we address the probl(;m of 'ne,stcd collocrd, ions; thai, is, those being l)art of longer colloc;ttions." ></td>
	<td class="line x" title="3:183	Most approa(:hes till now, tl'(!al;ed subst;rings of collo(:at;ions as eollocal;ions, only if they apl)eared ffequenl;ly enough 1)y l;hemselves in the cor\[)llS." ></td>
	<td class="line x" title="4:183	'Fhese techniques le\['l; ~r lot; of collocations mmxl;ra(:l;ed, in this 1)ai)er, we i)rol)oSe an algoril;hln for a semi-aul;oma|;ic exl;ra(;l;ion of nesl;ed uninl;errupl;ed anti inl;errul)l;ed collo(:al;iolls, paying parl;icular al;l;(~lll;ion to nested collocat;ion." ></td>
	<td class="line x" title="5:183	1 Introduction Tim increased inl;erest in collocation ext;raetion comes from t;hu faeI; l;hal, t;hey can be used for many NLP at)plical;ions such as machine transla(;ion, maehilw, aids R)r t;ra.nslal,ion, dictionary consl;ru(:i;ion, and secon(1 language learning, t.o mmm a few." ></td>
	<td class="line x" title="6:183	Recently, large scale textual corpora give the potential of working with the real data, (!ither fin' grammar inferring, or for enriching the le.xicon." ></td>
	<td class="line x" title="7:183	These corlms-based at)preaches have also been used for the extract, ion of collocal,ions." ></td>
	<td class="line x" title="8:183	In this t)al)er we are concerned wil;h nested collocations." ></td>
	<td class="line x" title="9:183	Collocations Lhat are subst;rings of oLher longer ones." ></td>
	<td class="line oc" title="10:183	Iegar(ling l;his l;ypu of (:olloeation, the approaches till ilOW could be divi(led inl;o t;wo groups: those thai; do uo(, refer to s'ttbstrings of colloco, l, ions as a l)arti(:ular problem, (Church and lla.nks, t99(); Kim and Cho, 1993; Nagao and Mori, 1994), and those t.hat; do (Kita et al. , t994; Smadja, 1993; lkchara et al. , 1995; Kjelhner, 11994)." ></td>
	<td class="line n" title="11:183	\[towew;r, (well the lal;t, er, deal wiLh only 1)arl; of the probh;m: they l,ry not to extract the mlwanl;cd substrings of collocations." ></td>
	<td class="line x" title="12:183	In favour of this, l;hcy leave a large number of nested colloc.ations unextracted." ></td>
	<td class="line x" title="13:183	ht section 2 collocations arc briefly discussed and the." ></td>
	<td class="line x" title="14:183	l)roblem is determined." ></td>
	<td class="line x" title="15:183	In section 3 our approach to t;he probl0an, 1;he algorithm and an examl)le are given." ></td>
	<td class="line x" title="16:183	In section d the experimeld, S are discussed and t;he Inethod is (;olnpare(t with t, hat proposed by (Kita et a.l., 199d)." ></td>
	<td class="line x" title="17:183	In sectioll 5 I;tlel'e are conlmenl;s on relal;ed work and tinally Section 6 eonl;ains I;he conc, hlsions and 1;he fill;life work." ></td>
	<td class="line x" title="18:183	2 Collocations The Problem Collocations are perwtsive in language: 'letters' are 'deliw:red', 'tea' is 'strong' and not 'powelful', we 'l'mt progrants', aitd so Oll." ></td>
	<td class="line x" title="19:183	Linguists have long been interested in collocations and the detinitions are nuiaerous and varied." ></td>
	<td class="line x" title="20:183	Some researchers include multi-o.leinent eOlnpOuIlds as (;xamples of collocations; some admit only collocations (:onsisl;ing of pairs of words, while others admit only eollo(;ations consisting of a maximum of tive or six words; some emphasize synl, aglnat, ic aspecl;s, others Selnmtl;ic aspects." ></td>
	<td class="line oc" title="21:183	The COlllillOil poini;s regarding collocations appear to be, as (Smadja, 1993) suggestsl: they are m'bil;rary (it is nol; clear why to 'Bill through' means to 'fail'), th('y are domain-dependent ('interest rate', 'stock market'), t;hey are recurrenl; and cohesive lo~xical clusters: the presence of one of the." ></td>
	<td class="line x" title="22:183	collocates strongly Sltggesl;S /,tie rest of the cellocat, ion ('Ulfited' could ilnply 'States' or 'Kingdom')." ></td>
	<td class="line x" title="23:183	the classiiics collocations into i)redicative relations, rigid noun phrases and phrasal telnplatcs." ></td>
	<td class="line x" title="24:183	4l It is not the goal of this paper to provide yet another definition of collocation." ></td>
	<td class="line x" title="25:183	We adopt as a working definition the one by (Sinclair, 1991) Collocation is the occurrence of two or more words within a short space of each other in a text." ></td>
	<td class="line x" title="26:183	Let us recall that collocations are domaindependent." ></td>
	<td class="line x" title="27:183	Sublanguages have remarlmbly high incidences of collocation (Ananiadou and McNaught, 1995)." ></td>
	<td class="line x" title="28:183	(Frawley, 1988) neatly sums up the nature of sublanguage, showing the key contribution of collocation: sublanguage is strongly lexically based sublanguage texts focus on content lexical selection is syntactified in sublanguages collocation plays a major role in sublanguage sublanguages demonstrate elaborate lexical cohesion." ></td>
	<td class="line x" title="29:183	The particular structures found in sublanguage texts reflect very closely the structuring of a sublanguage's associated conceptual domain." ></td>
	<td class="line x" title="30:183	It is the particular syntactified combinations of words that reveal this structure." ></td>
	<td class="line x" title="31:183	Since we work with sublanguages we can use 'small' corpora as opposed as if we were working with a general language corpus." ></td>
	<td class="line x" title="32:183	In the Brown Corpus for example, which consists of one million words, there are only 2 occurrences of 'reading material', 2 of 'cups of coffee', 5 of 'for good' and 7 of 'as always', (Kjellmer, 1994)." ></td>
	<td class="line x" title="33:183	We extract uninterrupted and interrupted collocations." ></td>
	<td class="line x" title="34:183	The interrupted are phrasal templates only and not predicatiw~ relations." ></td>
	<td class="line x" title="35:183	We focus on the problem of the extraction of those collocations we call nested collocations." ></td>
	<td class="line x" title="36:183	These collocations are at the same time substrings of other longer collocations." ></td>
	<td class="line x" title="37:183	To make this (:lear, consider the following strings: 'New York Stock Exchange', 'York Stock', 'New York' and 'Stock Exchange'." ></td>
	<td class="line x" title="38:183	Assuine that the first string, being a collocation, is extracted by some method able to extract collocations of length two or more." ></td>
	<td class="line x" title="39:183	Are the other three extracted as well?" ></td>
	<td class="line x" title="40:183	'New York' and 'Stock Exchange' should be extracted, while 'York Stock' should not." ></td>
	<td class="line x" title="41:183	Though the examples here are front domain-specific lexieal collocations, grammatieM ones can be nested as well: 'put down as', 'put down for', 'put down to' and 'put down'." ></td>
	<td class="line oc" title="42:183	(Smadja, 1993; Kits et al. , 1994; Ikehara et al. , 1995), mention about substrings of collocations." ></td>
	<td class="line o" title="43:183	Smadja's Xtract produces only the biggest possible n-grams." ></td>
	<td class="line x" title="44:183	Ikehara et al. , exclude the substrings of the retrieved collocations." ></td>
	<td class="line x" title="45:183	A more precise approach to the problem is provided by (Kits et al. , 1994)." ></td>
	<td class="line x" title="46:183	They extract a substring of a collocation if it, appears a significant amount of times by itself." ></td>
	<td class="line x" title="47:183	The following example illustrates the problem and their N)proach: consider the strings a='in spite' and b='in spite of', with n(a) and n(b) their numbers of oceurrencies in the corpus respectively." ></td>
	<td class="line x" title="48:183	It will always be n(a) > n(b), so whenever b is identified as a collocation, a is too." ></td>
	<td class="line x" title="49:183	Itowever, a should not be extracted as a collocation." ></td>
	<td class="line x" title="50:183	So, they modify the measure of frequency of occurrence to become K(a) = (lal 1)(n(a) n(b)) (1) where a is a word sequence la\[ is the length of a n(a) is the number of occurrencies of a in the curpus." ></td>
	<td class="line x" title="51:183	b is every word sequence that contains a n(b) is the number of occurrencies of b As a result they do not extract the sub-strings of longer collocations unless they appear a significant amount of times by themselves in the corpus." ></td>
	<td class="line x" title="52:183	The problem is not solved." ></td>
	<td class="line x" title="53:183	Table 2 gives the extracted by Cost-Criteria n-grams containing 'Wall Street'." ></td>
	<td class="line x" title="54:183	The corpus consists of 40,000 words of market reports." ></td>
	<td class="line x" title="55:183	Only those n-grants of frequency 3 or more are considered." ></td>
	<td class="line x" title="56:183	It (:an be seen that 'Wall Street' is not extracted as a collocation, though it has a frequency of occurrence of 38." ></td>
	<td class="line x" title="57:183	Table 1: n-grams extracted by Cost-Criteria containing 'Wall Street' c-ckKKA 19 20 22 26 19 38 20 Candidate Collocations Staff' Reporter of The Wall Street Journal Wall Street analysts Reporter of The Wall Street Journal Staff Reporter of The Wall Street of The Wall Street Journal The Wall Street Journal Wall Street Journal Reporter of The Wall Street Wall Street of The Wall Street The Wall Street 42 3 Our approach The Algorithm We, call the extracted strings candidate collocations rather than collocations, since what we accet)t as collo(:ations depends oil tile application." ></td>
	<td class="line x" title="58:183	It is the human judge that will give the tinal de(:ision." ></td>
	<td class="line x" title="59:183	This is tile reason we consider tile method as semi-automatic." ></td>
	<td class="line x" title="60:183	Let us consider the string 'New York Stock Ex(:hange'." ></td>
	<td class="line x" title="61:183	Within this string, that has already been extra(:ted as a candidate collocation, there are two substrings that should/)e extracted, and one that shouhl not." ></td>
	<td class="line x" title="62:183	The issue is how to distinguish when a substring of a (:andidate (:ollo(:ation is a candidate collocation, and when it is not." ></td>
	<td class="line x" title="63:183	Kita et al. assume that the substring is a candidate (:ollocation if it appears by itself (with a relatively high frequency)." ></td>
	<td class="line x" title="64:183	~lb this we add that: the sut)string aI)1)ears in more than one, (:an(li(lat(~' eollo(:ations, eVell if it, (h)es not appear by itself." ></td>
	<td class="line x" title="65:183	'Wall Street', for exalnple, appears 30 times in 6 longer candidate colh)cations, and 8 times by itself." ></td>
	<td class="line x" title="66:183	If we considered only the number of times il; appears by itself, it would get a low value as a candidate collocation." ></td>
	<td class="line x" title="67:183	We have to consider the number of tilnes it apI)ears within hmger candidate collocations." ></td>
	<td class="line x" title="68:183	A second fa(:tor is tit(!" ></td>
	<td class="line x" title="69:183	number of these hmger collocations." ></td>
	<td class="line x" title="70:183	The greater this numt)er is, the better the string is distribute.d, an(l the greater its value as a (:andi(late collocat;ion." ></td>
	<td class="line x" title="71:183	We make the above (:onditions more spe(:iti(: and give the measure for a string being a candidate coll()cation." ></td>
	<td class="line x" title="72:183	The measure is called C-value and the fa(> tors involved are the string's frequency of o(:eurrence in the corpus, its fre(luen(:y of oe(:urrence in longer candidate collocations, the immber of these longer ('andidate (:ollocations and its length." ></td>
	<td class="line x" title="73:183	Regar(ling its length, we (:onsider hmger collocations to t)e 'more important' than shorter appearii~g with the same fi'equency." ></td>
	<td class="line x" title="74:183	More specifically, if \]a\] is the length 2 of the string a, its C-value is analog()us to la I 1." ></td>
	<td class="line x" title="75:183	The 1 is giv(m sin('e the shortest collocations are of length 2, and we want them to be 'of ilnportan(;e' 2-1= 1." ></td>
	<td class="line x" title="76:183	More specifically: 1." ></td>
	<td class="line x" title="77:183	If a has the same hequen('y with a longer candidate (:ollocation that contains a, it is assigne(t C-value(a)=O i.e. is not a collocation." ></td>
	<td class="line x" title="78:183	it is straightforward that in this case a appears in one only hmger candidate collocation." ></td>
	<td class="line x" title="79:183	2We use tit(', same nol;ation with (Kita et al. , 1.994)." ></td>
	<td class="line x" title="80:183	2." ></td>
	<td class="line x" title="81:183	If n(a) is the number of times a appears, and a is not a substring of an already extracted candidate collocation, then a is assigned 3." ></td>
	<td class="line x" title="82:183	If a appears as a substring in one or more collocations (not with the same frequency), then it is assigned (I-I t())." ></td>
	<td class="line x" title="83:183	(3) where t(a) is the total frequency of a in longer candidate collocations and c(a) the number of ttmse candidate collocations." ></td>
	<td class="line x" title="84:183	This is the most complicate ease." ></td>
	<td class="line x" title="85:183	Tit(; ilnportance of the." ></td>
	<td class="line x" title="86:183	Iluinber of occurrences of a string in a longer string is illustrated with the de.nominator of the fraction in Equation 3." ></td>
	<td class="line x" title="87:183	The bigger the nulnber of strings a substring appears in, the smaller the fraction num&~ o\] occu~, the bigger the Cvalue of the string." ></td>
	<td class="line x" title="88:183	The algorithm for the extraction of tile candielate collo(:ations follows: e.xtract the n-grams decide on the lowest frequency of collocations renlove tlle I>granls below this frequency lbr all n-grams a. of lllaxiHlulIl length calculate their C-value= ('u 1)n(a) tbr all substrings b revise t(b) revise c(b) h)r all smaller n-grams a in descending order if (total frequency of a)=(frequency of a in a longer string) a is NOT a collocation else if a appears for the first time else C-v,,1,,.=.(~z) J for all substrings b revise t(b) revise c(b) The above algorithln coinputes the C-value of each string in an incremental way." ></td>
	<td class="line x" title="89:183	That is, for each string a, we 1:(;(;i, a tuple ('n(a), t(a), c(a)} and we revise tt,e t(a) and,:(a) wflues." ></td>
	<td class="line x" title="90:183	For each ngram b, every tin-le it is found ill a longer extracted 43 n-gram a, the vahles t(b) and c(b) are revised: t(1,) = t(b) + (:(b) =,-(t,) + 1." ></td>
	<td class="line x" title="91:183	Ill the, initial stage, n(a) is set to the frequency of a appearing on its own, and t(a) and c(a) are set to 0." ></td>
	<td class="line x" title="92:183	Table, 2: n-grmns e, xtraeted by C-wflue containing 'Wall Street' F Candid~te Colloc;~tions __ 114 19 Staff ReI)orter of 37.34 26 36 22 33 38 31.34 231 6 3 4 20 0 \]9 0 19 0 19 0 20i Tim \V~fll Street Journal Wa.ll Street,hmrnal The \M1 Street Journal \all Strce, t The Wall St, feet \Vail Street ~lm~lysts of The \M1 Street, Journal 12.ei)orter of The Wall Street R,eporter of The Wall St, reel; Journal Staff II,eI)orter of \[File Wall S~reet of The YVa,ll Street An example: Let us calculate the C-value for the string 'Wall Street'." ></td>
	<td class="line x" title="93:183	Table 2 shows all the strings that appear more that twice, and that contain 'Wall Street'." ></td>
	<td class="line x" title="94:183	1." ></td>
	<td class="line x" title="95:183	The analysis starts from the longest string, the 7-gram 'Staff l/.et)orter of The Wall Street Jourrod'." ></td>
	<td class="line x" title="96:183	Its C-value is (:ah:ulated l\[rom Equation 2." ></td>
	<td class="line x" title="97:183	For each substrings eon|;ained in the 7-gram, tile number 11.9 (the l'requen(:y of the 7-gram) is kept, as its (till now) fl'equeney of occurrence in longer,strings." ></td>
	<td class="line x" title="98:183	For each of them, the fact that they have been already l'oun(t in a longer string is kept as well." ></td>
	<td class="line x" title="99:183	Therefbre, t('Wall Street')=19 and c('\gall Street')=l. 2." ></td>
	<td class="line x" title="100:183	We continue with the two 6-grams." ></td>
	<td class="line x" title="101:183	Both of them, 'l~,eporter of The Wall Street Journal' and :'Staff Reporter of The Wall Street' get; C-value=O since they ~q)pear with the same l'requeney as the 7-gram that contains the're." ></td>
	<td class="line x" title="102:183	Therefore, they do not tbrm candidate collocations and they do not change the t('Wall Street') and the c('Wall Street') values." ></td>
	<td class="line x" title="103:183	3." ></td>
	<td class="line x" title="104:183	F/)r the 5-grams, there is one appearing with a l'requency })igger than that of the 7-gram it: is (:()nta,incd in, 'of The Wall Street Jourlml'." ></td>
	<td class="line x" title="105:183	This gets its C-value \[rom Equation 3." ></td>
	<td class="line x" title="106:183	its substrings increase their frequcmey of occurrence ~ (as substrings) by 20 19=1 (20 is the frequency of the 5-gram and 19 the fr0,queney it appeared in longer candidate collocal;ions), and the numt)er of oeeurrence ~s su/)string by 1." ></td>
	<td class="line x" title="107:183	There\[ore, t('Wall Street'')=19+l=20 and c('Wall Street')--1+1--2." ></td>
	<td class="line x" title="108:183	The other 5-gram is not a candidate collocations (it gets C-value=O)." ></td>
	<td class="line x" title="109:183	4." ></td>
	<td class="line x" title="110:183	For tile 4-grams, the 'The Wall Street Journal' occurs in two longer n-grams and therefore gets its C-value from Equation 3." ></td>
	<td class="line x" title="111:183	Froin this string, t('Wall Street')=20+2=22 and c('Wall Street')-2+1=3." ></td>
	<td class="line x" title="112:183	The 'of The Wall Street' is not accepted as a eamtidate collocations since it; apt)ears with the same fl'equeney as the 'of The Wall Street Jom'nal'." ></td>
	<td class="line x" title="113:183	5." ></td>
	<td class="line x" title="114:183	'Wall Street analysts' appears for the first time so it; gets its C-value from Equation 2." ></td>
	<td class="line x" title="115:183	'Wall Street Journal' mnl 'The Wall Street' appearing in longer extracted n-grams get their values from Equation 3." ></td>
	<td class="line x" title="116:183	They make t('Wall Street')=22+3+4+l=30 and c('Wall St, lee t' ) = 3+ \] + 1+ 1 =6." ></td>
	<td class="line x" title="117:183	6. Finally, we evaluate the C-value for 'Wall Street' from Equation 3." ></td>
	<td class="line x" title="118:183	We find C-value('\all Street')=33." ></td>
	<td class="line x" title="119:183	4 ExperimentsComparison The eortms used for the experiments is quite small (40,000 words) and consists of material lotn th(~ Wall Street Journal newswire." ></td>
	<td class="line x" title="120:183	For these experilnents we used n-grams of maxilnuln length 10." ></td>
	<td class="line x" title="121:183	Longer n-grains apt)ea.r once, only (because of the size of the corpus)." ></td>
	<td class="line x" title="122:183	The, maximum length of the n-grams to be extracted is variallle attd depends on the size of the corpus and the application." ></td>
	<td class="line oc" title="123:183	From the extracted n-grams, those with a flequc'ncy of 3 or more were kept (other approaches get rid of n-grams of such low frequencies (Smadja, 1993))." ></td>
	<td class="line x" title="124:183	These n-grams were lbrwarded into the, implementation of our algoril;hm as well as our implementation of the algorithm by (Kita et al. , 1:)94)." ></td>
	<td class="line x" title="125:183	The Cost-Criteria algorithm needs a second threshold (besides tile one for tile frequency of the n-grams): for every n-gram a, K(a) is evaluated, and only those n-grams with this value greater than the' preset threshold will take part to tile rest of the algorithm." ></td>
	<td class="line x" title="126:183	We set this threshold to ;I again for the, same reason as above (the gain we wouhl gel; for precision if we had set a higher threshold would be lost on recall)." ></td>
	<td class="line x" title="127:183	Table 3 shows the candidate c, ollocations with the higher values, extra('Le(l with C-value." ></td>
	<td class="line x" title="128:183	A lot of eandidate e, otlocations extracted may seem unimportant." ></td>
	<td class="line x" title="129:183	This is because t}le algorithm extracts tile word sequences that are fl'equent." ></td>
	<td class="line x" title="130:183	Which of these candidate collocations we should keep depends on the apt)lication." ></td>
	<td class="line x" title="131:183	Brill's t)art-of-speeeh tagger, (Brill, 1992), was used to remove the ngrams that had an article as their last wor(1." ></td>
	<td class="line x" title="132:183	44 'l'a,I)l(~ 3: Exi,raci,('d c~m(tida,t(~ (:olloca, i,ion with Cvakae in (l(~,s(:(m(iin<~ or(l(;r. \[ C:V 2 _F." ></td>
	<td class="line x" title="133:183	I (JandidA te Colloci tion  L84 92 ~Vi\L\]-;STR,10ET,J()UFLNAL 1:14 19 87.6 93 79.6 44 53.2 59 49.5 20 44.75 25 44 48 41.17 44 {17,{/4 26 S6 (i 36 22 33 38 31.34 23 27.8 :~\[ 27 3 27 3 27 30 24 27 24 10 23.3d 27 21,3,1 27 21 10 20 10 20 5 19.67 23 18.5 23 18 18 i 8 6 18 )!" ></td>
	<td class="line x" title="134:183	18 9 {8 21 1'2 17 1'( 17 17 17 17 I I 17 21 1 (iA 19 \] (J I (J 16 4 1 (i 8 l (i 19 15.5 20 15 15 i5 15 \] 5 3 15 3 15 :1 15 3 15 5 15 18 StM\[ Rel)ort(:r o\[ q'tie Wall SI;reei; Journal ( hlil;(xl SI;a,l;es t;\[l(; Unilx!d SI;iti;es i;he Unil;ed {lllll\[ll)(w) to <~lllOlt(~y) \['FO1H I;O (Ill()ll(;y) \[!I'O1H said il; (;h(!" ></td>
	<td class="line x" title="135:183	(;Olnl)~Hly \,VMI ~l;r(?el; J(iurnal <~tillllll)(~l'~ > I;o <~iH()ll(',y~> \[l'Oll\[ (HIOll(~y) }1 VO~/,I' The YVa\[1 ~l;reel;,JOlll'IIiL\[ W;dl Str(!el; The, Wail Sl,l'(x~l, il,,y(!{ir rl'here w(u'e .~Illlllli)(~l') sell|nil (bWs iii t;h(!" ></td>
	<td class="line x" title="136:183	t)(!riod this year There were '~IllllHI)(W~> selling da:ys fii l;lw, period this t;o lie will I)e a.t; l;h(!" ></td>
	<td class="line x" title="137:183	end of I;h(!" ></td>
	<td class="line x" title="138:183	C()lil \[)D.II~/I~ (~o/11 \[)i-I.l'(!( l wil,h < (J() hi ~\il l'\]Nr'l'> I );qr;tI~,ra,\[)tiilll'~ I<;rror <t(X)MMENT> ~ill()ltt!y~' ~l, sii;qt'(~ I)ric(x\[ a,(; .~tilllllhl!r) LO yield White 1 louse t;he I il}tl'\]{(~l; Tot,a I ('ill'S in the \[}nil;ed SI;a,l;es Tim Sri I(hoo Tli( ~, ()nit(!d Sl;a,l;e.<~ N al:iOilil, l Hank hits })(!(!ii said Mr stud t:h;/t I;he (!n(I of of its fi)ui:l,h (lUarl;er I)i:+l,lilOli(| ~ ha,tliro(:|( ~.nlllrlH(~r \]> <C()MM EN'.V> I)il,riLgl'ii~i)hillg \[~\]rl'(il: <(J()MMENT> its well as I, hal; it ill(ir(!" ></td>
	<td class="line x" title="139:183	I, hH.l l ll~,(l been it; is <t\[l()II(!y~> ~LI; I,\[l(!" ></td>
	<td class="line x" title="140:183	(!11(\[ O1!" ></td>
	<td class="line x" title="141:183	<itlltill)(!r~> ill iL ~cclu'il;ies i/lid 14x('ha,nge (}()iliillissi(ili ;i <~tllllli\])(!l'~for Qlilliii})(w~> sl;o(',k ~qflil,,~id(',f~ ix)s(!" ></td>
	<td class="line x" title="142:183	<{illilill)(!r~ I;() ~inotie,v~ l'l'OIII l;h~ti; l;h(!" ></td>
	<td class="line x" title="143:183	\[}nited St,al,es \])(!(',~tll~(~ O\]' Am()ng l,h(l (;xl;ra(;lxxl ii-<~l',~l_illS w(', c}iAl sc(; 1;h(; doniain-si)(x:iti(: (',and|date c, ollocal;ions, mmh a,s 'SI,aff l(;t)orl;er o\[ l;h(; 'vVa,I1 Strc(;l;', 'Na.l;ional lianl' etc. , and those l;ha, t a,i)pe, ar within other colloca,Lions ral;ho, r i;ha,n by 1;h(~,ms(~,lv(~,s, '~;a\]\[ Sla'(x',t .h)urmfl','WM1 Strc(%' etc. Tlw, r(,~ are, howe, vet, t)robl(;ms: |." ></td>
	<td class="line x" title="144:183	W(!" ></td>
	<td class="line x" title="145:183	did tit)l; (:nh:ula, i;e l;tl(', 1)recision or recall ()l' l;\[W, (,'-'val'ttc algoril,hni." ></td>
	<td class="line x" title="146:183	Th(!se cal(:ulai;ions (tepen(1 Oll l;}le (let|nil;ion of ('ol\]o('.~t;ion ant| l;}ley m'(; domain dCl)endenl;." ></td>
	<td class="line x" title="147:183	(l(j(~lhn(;r tU(~liLiOllf4 1,9 ca, l;(> ~>orics o\[ collocation (l(,j(~ilm(;r, 1994))." ></td>
	<td class="line x" title="148:183	2." ></td>
	<td class="line x" title="149:183	As ii; (:a,II l)(; Seell \[rolll '\['al)le 3, one string aI)i)(!a,ring I)ol;h in simdl a,u(t Cal)iLM \](!l;l;ers i~ la'e~tt;d as t:wo (li\[l'ur(~,nt SLl'illt~." ></td>
	<td class="line x" title="150:183	'F}I(!" ></td>
	<td class="line x" title="151:183	l)r()l)l(;in (',au be, parl ially solv(xl if' w(; llS(' ;L c:/moui(:al \[(.)rill." ></td>
	<td class="line x" title="152:183	\[I()w(~,V(!l ', i\[ we wanl; 1;() apply l,he algorii;tnn f()r the el;ra,c,l iolt ol7 domMn-sl)(~(',ili(: (:olloca,tions, (',as(!" ></td>
	<td class="line x" title="153:183	is t)erl;iIV'AlL 3." ></td>
	<td class="line x" title="154:183	'.', iIl sl;rings lilw, '(~.1;.(:.', '0,l; ;ft'." ></td>
	<td class="line x" title="155:183	(~t(:.> is I;~Ll((,~II as a S(HII)O,11C(!" ></td>
	<td class="line x" title="156:183	l)ounda,ry (wen when il; is IlOL 4." ></td>
	<td class="line x" title="157:183	How f,o fill;e,r oul; the exl;ra.clxxl ii-<~i'&i\[is l;hal; a,I'(~ Ii()l; r;l(w;/,nl; i;() l;he at)t)li(:a,l;i}n (for Ihe (',an(lidmc, (:{}ll(}{:ations) wc arc illt(~r~si;(xl in, is anoi,h~,r I)rol)l(!ui." ></td>
	<td class="line x" title="158:183	A(:l,ually, for sonl(, ~ ()\[ l;he (~,xtir;c:i;{!(l t> ~l'//lilS (~:l;O i)(\]>>~ 'lifts l)('.eu', 's~ti(l l;\]la.l;'~ el;(:.), we Calm()1, 1,}|ink ()\[ a,ny al)t)li('~rl;ion l;\]lat tiles(' n-grmns WOIll(\[ l)e. U~e\['ul." ></td>
	<td class="line x" title="159:183	And though some of them ('.ould 1)(!" ></td>
	<td class="line x" title="160:183	tilter(xl oliL l)y a l)a,rl;-o\[Lsptxx:h l;agg(;t', we ca, nnot say I;his for a,l\] l;h(; l;,ype, s of the, %ulw~-ml;e,d' (~,xl;racl;cd ii-~ii-i,t/is. 5 Related -Work Ilesid(~s I,tl(~ work by KiI,~L (!1; al. m(mifion('xl (mrlier, the, re ~r(', oLher inlx~r('stin~ apl)roaches t<) l;he exl;ra('l;ion o\[ collocltl;ions." ></td>
	<td class="line x" title="161:183	((JilOU(!ka, (',L a\].> 1983) 1)r()t)ostx| a Hl(!l,hod, l)ase(1 on the ol)s(;rv(;d \['r(xlUC.tw.y ()f s(xlu(~.nc.i(;s of words, to cxtr~mt unhitcrrupl;(;d collocatioiis, 1)ul; 1;he rcsull;s are d(,~t)(;n(l(;nl; on l;h(~ ~ize of the corl)us." ></td>
	<td class="line x" title="162:183	(Churc, h a n(l \]ia,nks, ;I 990), l)rOl)OSCd 1;he asso(:ial;iOli i'a%i(), ;i, ill(Wt~lll'(~ I)a~(x1 Oll illlli;li~l ill\[ori'Ha1,ion (l<\]ulo~ \]96\]), I,()e~i;hnal;(; word a,~so(',ial;ion 1K)I'IlIS." ></td>
	<td class="line x" title="163:183	They idc, nlify tm, irs o\[ w()rd,~ t, ha, i; ~Lt)l)cm Ix)gel;ller 111()1()()\[I,(~11 t;ilan \])y ch,~illC(l. The coil(icarious til(;y i(ienl;ify ('()uld a\]s() l)e due I;() ~(IIlII-LHI,i(: I'(~tI, SO\[l~." ></td>
	<td class="line x" title="164:183	They Mlow ga,l),~ I)el,wc,(ni the words and I;h('r('fore exlirm',l; inl('rrul)l;c(t wor(t,'-;(~(\]II(HIC(;S, Since l;hey only tie, a,1 wil;h collocations of lengl;h I;W() (l;\[lOll~h iiiltl;lla, l in\[ormaDion (:~LII 1)e (~xlx;ntl(!d \[(il' ILll arl)il;rary llUIill)(!l' ()\[ (!Ve, lllls, (|?~-111o, 1961; X,tcl,\]liec.<~, 1977)), 1;hcy do 1lOl, consider n(;sl;ed colh)(:a,l,ion,~." ></td>
	<td class="line x" title="165:183	(l(im anti (ill(i, 1.9.03), l)rolios(xl IIIIlIAID,1 iufl)rHI:~I;iOtl to ('al(:uliLIX; t, ii(; d('gr(;(; of word a,~so('.ial;ion 45 of compound words." ></td>
	<td class="line x" title="166:183	They extend the measure for three words in a different way than that defined by (Fano, 1961), and no mention is given to how their formulas would be extended for wordsequences of length more that three." ></td>
	<td class="line x" title="167:183	They do not consider nested collocations." ></td>
	<td class="line oc" title="168:183	(Smadja, 1993), extracts uninterrupted as well as interrupted collocations (predicative relations, rigid noun phrases and phrasal templates)." ></td>
	<td class="line p" title="169:183	The system performs very well under two conditions: the corpus must be large, and the collocations we are interested in extracting, must have high frequencies." ></td>
	<td class="line x" title="170:183	(Nagao and Mori, 1994), extract collocations using the tbllowing rule: longer collocations and frequent collocations are more important." ></td>
	<td class="line x" title="171:183	An improvement to this algorithm is that of (Ikehara et al. , 1995)." ></td>
	<td class="line x" title="172:183	They proposed an algorithm for the extraction of uninterrupted as well as interrupted collocations from Japanese corpora." ></td>
	<td class="line x" title="173:183	The extraction involves the following conditions: longer collocations have priority, more frequent collocations have priority, substrings are extracted only if tbund in other places by themselves." ></td>
	<td class="line x" title="174:183	Finally, the Dictionary of English Collocations, (Kjellmer, 1994), includes n-grams appearing even only ()nee." ></td>
	<td class="line x" title="175:183	For each of them its exclusive frequency (number of occurrences the n-gram appeared by itself), its inclusive frequency (number of times it appeared in total) and its relative frequency (the ratio of its ac.tual frequency to its expected frequency), is given." ></td>
	<td class="line x" title="176:183	6 Conclusions and Future Work As collocation identification (either in general language or in sublanguages) finds many applications, the need to automate, as much as possible, that process increases." ></td>
	<td class="line x" title="177:183	Automation is helped by the recent availability of large scale textual corpora." ></td>
	<td class="line x" title="178:183	In this paper we dealt with the extraction of uninterrupted and interrupted collocations focusing on those we call nested collocations (those being substrings of other collocations)." ></td>
	<td class="line x" title="179:183	A inethod tbr their extraction was proposed." ></td>
	<td class="line x" title="180:183	In fllture, we plan to extend our algorithm to include predicative relations." ></td>
	<td class="line x" title="181:183	We are going to incorporate linguistic knowledge to improve the results." ></td>
	<td class="line x" title="182:183	Finally, this algorithm will be applied for term extraction." ></td>
	<td class="line x" title="183:183	7 Acknowledgements We thank our anonymous reviewers for their comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C96-1039
Identification And Classification Of Proper Nouns In Chinese Texts
Chen, Hsin-Hsi;Lee, Jen-Chang;"></td>
	<td class="line x" title="1:433	Identification and Classification of Proper Nouns in Chinese Texts Hsin-Hsi Chen and Jen-Chang Lee Department of Computer Science and Information Engineering National Taiwan University Taipei, TAIWAN, R.O.C. hh chen@csie.ntu, edu.tw Abstract Various strategies are proposed to identify and classify three types of proper nouns in Chinese texts." ></td>
	<td class="line x" title="2:433	Clues from character, sentence and paragraph levels are employed to resolve Chinese personal names." ></td>
	<td class="line x" title="3:433	Character, Syllable and Frequency Conditions are presented to treat transliterated personal names, To deal with organization names, keywords, prefix, word association and parts-of-speech are applied." ></td>
	<td class="line x" title="4:433	For fair evaluation, large scale test data are selected from six sections of a newspaper." ></td>
	<td class="line x" title="5:433	The precision and the recall for these three types are (88.04%, 92.56%), (50.62%, 71.93%) and (61.79%, 54.50%), respectively." ></td>
	<td class="line x" title="6:433	When the former two types are regarded as a category, the performance becomes (81.46%, 91.22%)." ></td>
	<td class="line x" title="7:433	Compared with other approaches, our approach has better performance and our classification is automatic." ></td>
	<td class="line x" title="8:433	1." ></td>
	<td class="line x" title="9:433	Introduction A Chinese sentence is composed of a string of characters without any word boundaries, so that to segment Chinese sentences is indispensable in Chinese language processing (Chen, 1990; Chen, 1994)." ></td>
	<td class="line x" title="10:433	Many word segmentation techniques (Chen & Liu, 1992; Chiang et al. , 1992; Sproat & Shih, 1990) have been developed." ></td>
	<td class="line x" title="11:433	However, the resolution of unknown words, i.e., those words not in the dictionaries, form the bottleneck." ></td>
	<td class="line oc" title="12:433	Some papers (Fung & Wu, 1994; Wang et al. , 1994) based on Smadja's paradigm (1993) learned an aided dictionary from a corpus to reduce the possibility of unknown words." ></td>
	<td class="line x" title="13:433	Chang et al.(1992) proposed a method to extract Chinese personal names from an 11,000-word corpus, and reported 91.87% precision and 80.67% recall." ></td>
	<td class="line x" title="15:433	Wang et al.(1992) recognized unregistered names on the basis of titles and a surname-driven rule." ></td>
	<td class="line x" title="17:433	Linet al.(1993) presented a model to tackle a very restrictive form of unknown words." ></td>
	<td class="line x" title="19:433	Sproat et al.(1994) considered Chinese personal names and transliterations of foreign words." ></td>
	<td class="line x" title="21:433	Their performance was 61.83% precision and 80.99% recall on an 12,000-Chinese-character corpus." ></td>
	<td class="line x" title="22:433	This paper deals with three kinds of proper nouns say, Chinese personal names, transliterated personal names and organization names." ></td>
	<td class="line x" title="23:433	We not only tell if an unknown word is a proper noun, but also assign it a suitable semantic feature." ></td>
	<td class="line x" title="24:433	In other words, '~?~4~ ~' (George Bush) will have a feature of male transliterated personal name when it is identified." ></td>
	<td class="line x" title="25:433	Such a rigid treatment will be helpful for further applications such as anaphora resolution (Chen, 1992), sentence alignment (Chert & Chert, 1994; Chen& Wu, 1995), etc. Section 2 describes the training corpora and the testing corpus we used." ></td>
	<td class="line x" title="26:433	Sections 3, 4 and 5 propose tile identification and classification methods of Chinese personal names, transliterated personal names and organization names, respectively." ></td>
	<td class="line x" title="27:433	Section 6 presents two applications." ></td>
	<td class="line x" title="28:433	Section 7 concludes the remarks." ></td>
	<td class="line x" title="29:433	2." ></td>
	<td class="line x" title="30:433	Training Corpora and Testing Corpus The proposed methods in this paper integrate the rule-based and the statistics-based models, so that training corpora are needed." ></td>
	<td class="line x" title="31:433	To test the performance of language models, a good testing corpus is also necessary." ></td>
	<td class="line x" title="32:433	This section introduces all the corpora that are used in the following sections." ></td>
	<td class="line x" title="33:433	NTU balanced corpus, which follows the standard of LOB corpus (Johansson, 1986), is the first training corpus." ></td>
	<td class="line x" title="34:433	It is segmented by a word segmentation system and is checked manually." ></td>
	<td class="line x" title="35:433	In total, this corpus has 113,647 words and 191,173 characters." ></td>
	<td class="line x" title="36:433	The second training corpus is extracted from three newspaper corpora (China Times, Liberty Times News and United Daily News)." ></td>
	<td class="line x" title="37:433	It is just segmented by a word segmentation system without checking manually." ></td>
	<td class="line x" title="38:433	Although segmentation errors may exist, this corpus is 23.2 times larger than NTU balanced corpus, so that we can get more reliable word association pairs." ></td>
	<td class="line x" title="39:433	The third training corpus is a transliterated personal name corpus." ></td>
	<td class="line x" title="40:433	There are 2,692 transliterated personal names, including 1,414 male's names and 1,278 female's names." ></td>
	<td class="line x" title="41:433	Those transliterated personal names are selected from a book 'English Names For Yon' (Huang, 1992)." ></td>
	<td class="line x" title="42:433	The last training data is a Chinese personal name corpus." ></td>
	<td class="line x" title="43:433	It has 219,738 Chinese personal names and 661,512 characters." ></td>
	<td class="line x" title="44:433	222 Finally, the testing corpus is introduced." ></td>
	<td class="line x" title="45:433	We randomly select six different sections from a newspaper corpus (Liberty Times News)." ></td>
	<td class="line x" title="46:433	The contents are different from the second training corpus." ></td>
	<td class="line x" title="47:433	The following shows the statistics of the testing corpus: (a) the political section There are many items of news about the legislature." ></td>
	<td class="line x" title="48:433	It includes 23,695 words and 36,059 characters." ></td>
	<td class="line x" title="49:433	(b) the social section There are many items of news about police and offenders." ></td>
	<td class="line x" title="50:433	It includes 61,846 words and 90,011 characters." ></td>
	<td class="line x" title="51:433	(c) the entertainment section There are many items of news about TV stars, programs, and so on." ></td>
	<td class="line x" title="52:433	It includes 38,234 words and 55,459 characters." ></td>
	<td class="line x" title="53:433	(d) the international section It contains many items of foreign news and has 19,049 words and 29,331 characters." ></td>
	<td class="line x" title="54:433	(e) the economic section Many items of news about stock market, money, and so on, are recorded." ></td>
	<td class="line x" title="55:433	It includes 39,008 words and 54,124 characters." ></td>
	<td class="line x" title="56:433	(f) the sports section All items of news concern sports." ></td>
	<td class="line x" title="57:433	It includes 36,971 words and 54,124 characters." ></td>
	<td class="line x" title="58:433	Every section has its own characteristics." ></td>
	<td class="line x" title="59:433	In the political section, there are many titles." ></td>
	<td class="line x" title="60:433	In the social section and the entertainment ~ection, there are many Chinese personal names and organization names." ></td>
	<td class="line x" title="61:433	In the international section, transliterated personal names are more than the other two." ></td>
	<td class="line x" title="62:433	In the economic section, stock companies often appear." ></td>
	<td class="line x" title="63:433	In the sports section, there are many Chinese personal names and transliterated personal names." ></td>
	<td class="line x" title="64:433	Because the proper nouns are usually segmented into single characters, they will interfere with one another during identification and classification." ></td>
	<td class="line x" title="65:433	3." ></td>
	<td class="line x" title="66:433	Chinese Personal Names 3.1 Structure of Personal Names Chinese personal names are composed of surnames and names." ></td>
	<td class="line x" title="67:433	Most Chinese surnames are single character and some rare ones are two characters." ></td>
	<td class="line x" title="68:433	The following shows three different types: (a) Single character like '~', }~', '~' and '-9:-'." ></td>
	<td class="line x" title="69:433	(b) Two characters like '~\[~' and' k'(('." ></td>
	<td class="line x" title="70:433	(c) Two surnames together like 'JI,~'." ></td>
	<td class="line x" title="71:433	Most names are two characters and some rare ones are one character." ></td>
	<td class="line x" title="72:433	Theoretically, every character can be considered as names rather than a fixed set." ></td>
	<td class="line x" title="73:433	Thus the length of Chinese personal names ranges from 2 to 6 characters." ></td>
	<td class="line x" title="74:433	3.2 Strategies 3.2.1 Segmentation before Identification Input text has to be segmented roughly beforehand." ></td>
	<td class="line x" title="75:433	This is because many characters have high probabilities to be a Chinese personal name without pre-segmentation." ></td>
	<td class="line x" title="76:433	Consider the example '~!,)~l~,@~l+j (@~l~'." ></td>
	<td class="line x" title="77:433	The character '(i@' has a high score to be a surname." ></td>
	<td class="line x" title="78:433	In this aspect, '~)~' is easy to be a name." ></td>
	<td class="line x" title="79:433	If the input text is not segmented beforehand, it is easy to regard '(q~.~J~' as a Chinese personal name." ></td>
	<td class="line x" title="80:433	On the statistical model, this type of errors is difficult to avoid." ></td>
	<td class="line x" title="81:433	However, it is easy to capture by pre-segmentation." ></td>
	<td class="line x" title="82:433	3.2.2 Variation of a Character How to calculate the score of a candidate is an important issue in this identification system." ></td>
	<td class="line x" title="83:433	The paper (Chang et al. , 1992) proposes the following formula: (I) P(W,GN) = P(GN) * P(WIGN ) This formula has a drawback, i.e., it does not consider the probability of a character to be the other words rather than a surname." ></td>
	<td class="line x" title="84:433	Take the two characters '{,~,~' and 'llil\[' as an example." ></td>
	<td class="line x" title="85:433	The character '{'~j'." ></td>
	<td class="line x" title="86:433	can form '{~', '1',1/~ ', 'l'~/-iiTfi ', and many other words." ></td>
	<td class="line x" title="87:433	On the contrary, the character' I@i' just forms a word '@\[1~', which is a rare word." ></td>
	<td class="line x" title="88:433	The difference shows that the former is easier to be used as the other words than the latter." ></td>
	<td class="line x" title="89:433	The above formula assigns the same score to '@/:-~' and '@\[(-~', when '{'0' and '111~' have the same frequency to be names." ></td>
	<td class="line x" title="90:433	Intuitively, '{~lj.~'-''." ></td>
	<td class="line x" title="91:433	does not look like a name, but 'ItlJU~'." ></td>
	<td class="line x" title="92:433	does." ></td>
	<td class="line x" title="93:433	Thus 'tliI(' should have higher score than '{~l', and the variation of a character should be considered in the formula." ></td>
	<td class="line x" title="94:433	In our model, the variation of characters is learned from NTU balanced corpus." ></td>
	<td class="line x" title="95:433	3.2.3 Baseline Model Equation (2) defines the original formula." ></td>
	<td class="line x" title="96:433	The formula used to calculate P(Ci) is similar to Equation (1)." ></td>
	<td class="line x" title="97:433	When the variation of a character is considered, Equation (3) is formulated." ></td>
	<td class="line x" title="98:433	The variation of a character is measured by the inverse of the frequency of the character to be the other words." ></td>
	<td class="line x" title="99:433	Equation (4) is simplified from Equation (3)." ></td>
	<td class="line x" title="100:433	(2) 1'(C1) x P(C2) x P(C3) 1 l 1 (3) P(CI) x -x t'(C2) x -x P(C3) x -~ -& Cl & C2 & C3 # CI # C2 # C3 (4) -x - -&CI &(72 &C3 where Ci is a character in the input sentence, P(Ci) is the probability of Ci to be a surname or a name, #Ci is the frequency of Ci to be a surname or a name, 223 &Ci is the frequency of Ci to contain in tile other words." ></td>
	<td class="line x" title="101:433	For different types of surnames, different models are adopted." ></td>
	<td class="line x" title="102:433	(a) Single character # CI # C2 # C3 (5) -x -x -> Threxholdl & CI & ('2 & C3 #C1 (6) ---> ThreshoM2 & (71 # (_72 # C3 and -x -> Threshold3 & (72 & (73 (b) Two characters # C2 # (;3 (7) - -> Threshold4 & (72 & C3 (c) Two surnames together #Cll #C12 #C2 #C3 (8) -------&CII &CI2 &C2 &C3 > 'lhreshold5 #Cll #C12 (9) - -> 7hreshoM6 & C11 & C12 # C2 # C3 and ---x --> 7hreshoM7 &C2 &C3 Because the surnames w~th two characters are always surnames, Model (b) neglects the score of surname part." ></td>
	<td class="line x" title="103:433	Models (a) and (c) have two score functions." ></td>
	<td class="line x" title="104:433	It avoids the problem of very high score of surnames." ></td>
	<td class="line x" title="105:433	Consider the string '1~ ;'it( ')j~J ' \['.4:J (J~ @<-'." ></td>
	<td class="line x" title="106:433	Because of the high scores of the characters '1~' and 'S,', '\[~N~} f',' f:, -NI', ' t:,~:J 2' and 'J~ -~' may be identified according to Equation (5)." ></td>
	<td class="line x" title="107:433	Equation (6) screens out the impossible candidates." ></td>
	<td class="line x" title="108:433	The above three models can be extended to single-character names." ></td>
	<td class="line x" title="109:433	When a candidate cannot pass the threshold, its last character is cut off and the remaining part is tried again." ></td>
	<td class="line x" title="110:433	The threshold is different from the original one." ></td>
	<td class="line x" title="111:433	Thresholds are trained from Chinese personal name corpus." ></td>
	<td class="line x" title="112:433	We calculate the score of every Chinese personal name in the corpus using the above formulas." ></td>
	<td class="line x" title="113:433	The scores for each formula are sorted and the one which is less than 99% of the personal names is considered as a threshold for this fornmla." ></td>
	<td class="line x" title="114:433	That is, 99% of the training data can pass the threshold." ></td>
	<td class="line x" title="115:433	3.2.4 Other Clues Text provides many useful clues from three different levels say, character, sentence and paragraph levels." ></td>
	<td class="line x" title="116:433	The baseline model forms the first level, i.e., character level." ></td>
	<td class="line x" title="117:433	The following subsections present other clues." ></td>
	<td class="line x" title="118:433	Of these, gender is also a clue from character level; title, mutual information and punctuation marks come from sentence level; tile paragraph information is recorded in cache." ></td>
	<td class="line x" title="119:433	3.2.4.1 Clue 1: Title The first is title." ></td>
	<td class="line x" title="120:433	Wang et a/." ></td>
	<td class="line x" title="121:433	(1992) propose a model based on titles When a title appears before (after) a candidate, it is probably a personal name." ></td>
	<td class="line x" title="122:433	For example, '~.-~)NI~, ' and ' ~,~ ~)'~ :~, ~." ></td>
	<td class="line x" title="123:433	~\](' However, there are many ~!~,~ \[ J Thus counterexamples, e.g., ' ~'~ ' ; (~ ~'-4 :~'~ ~ ' f'J ' s': Y'.,., we cannot make sure if the characters surrounding a title form a personal name." ></td>
	<td class="line x" title="125:433	Even so, title is still a useful clue." ></td>
	<td class="line x" title="126:433	it can help determine the boundary of a name." ></td>
	<td class="line x" title="127:433	In the example '~.J~J!~'~;~;:i-qj',' ~-IDJ!SJ'~I~, '' is identified incorrectly." ></td>
	<td class="line x" title="128:433	When a title is included in this example, i.e., '~JDJ!~!I~',;I~'~;IJ ', the error does not occur." ></td>
	<td class="line x" title="129:433	In sumnmry, if a title appears, a special bonus is given to the candidate 3.2.4.2 Clue 2: Mutual hfformation Chinese personal names are not always composed of single characters." ></td>
	<td class="line x" title="130:433	For example, the name part of the sentence 'l~i~rl)Jv~Jlai~lc'/;~'r~';jtlfj' is a word." ></td>
	<td class="line x" title="131:433	How to tell out that a word is a content word or a name is indispensable." ></td>
	<td class="line x" title="132:433	Mutual information (Church & Hanks, 1990) provides a measure of word association." ></td>
	<td class="line x" title="133:433	The words surrounding a word candidate are checked." ></td>
	<td class="line x" title="134:433	When there exists a strong relationship, the word candidate has high probability to be a content word." ></td>
	<td class="line x" title="135:433	In the example '1~ ~!J:~l'/ ',~<~Jl~?_~.,.', the two words 'C I!!:' and '~.~ \[tl' have high mutual reformation, so that 'lI~, i 1~' is not a personal name." ></td>
	<td class="line x" title="137:433	Three newspaper corpora (total size is about 2.6 million words) are used to train the word association." ></td>
	<td class="line x" title="138:433	3.2.4.3 Clue 3: Punctuation Marks Personal names usually appear at the head or the tail of a sentence." ></td>
	<td class="line x" title="139:433	A candidate is given an extra bonus when it is found from these two places." ></td>
	<td class="line x" title="140:433	Candidates surrounding the caesura mark, a Chinese-specific punctuation mark, are treated in the similar way." ></td>
	<td class="line x" title="141:433	If some words around this punctuation are personal names, the others are given bonus." ></td>
	<td class="line x" title="142:433	3.2.4.4 Clue 4: Gender There is a special custom in Chinese." ></td>
	<td class="line x" title="143:433	A married woman may mark her husband's surname before her surname." ></td>
	<td class="line x" title="144:433	That forms type 3 personal name mentioned in Section 3.1, Because a surname may be considered as a name, e.g., '7/' in the personal name ~'~'/~Jt~ and in,~,~r/,-, v,." ></td>
	<td class="line x" title="145:433	the candidates with two tile personal name c~ ~,,~,, possible surnames do not always belong to type 3 personal name." ></td>
	<td class="line x" title="146:433	The gender information, i.e., type 3 is always a female, helps us disambiguate the type of personal names." ></td>
	<td class="line x" title="147:433	Some Chinese characters have high score for male and some for female." ></td>
	<td class="line x" title="148:433	The following lists some typical examples: male: ~,,~, ~,', ~t~,-1t(, ~j~, JJ(~, 9~i, JI(, (~l,)~; female: ~i~, J~, I~, ~l l, ~:~, ~;, {}::, '{~L ~Y, )J:, -+-/ 224 We count the frequencies of the characters to be male and female, and compare these two scores." ></td>
	<td class="line x" title="149:433	If the former is larger than the latter, then it is a masculine name." ></td>
	<td class="line x" title="150:433	Otherwise, it is a fenfinine name." ></td>
	<td class="line x" title="151:433	3.2.4.5 Clue 5: Cache A personal name may appear lnore than once in a paragraph This phenomenon is useful durmg identification We use cache to store the identified candidates, and reset cache when next paragraph is considered There are four cases shown below when cache is used: (a) CIC2C3 and C1C2C4 are in the cache, and C I C2 is correct." ></td>
	<td class="line x" title="152:433	(b) CIC2C3 and C1C2C4 are in the cache, and both are correct." ></td>
	<td class="line x" title="153:433	(c) CIC2C3 and C1C2 are in the cache, and C 1 C2C3 is correct." ></td>
	<td class="line x" title="154:433	(d) C1C2C3 and C1C2 are in the cache, and CIC2 is correct." ></td>
	<td class="line x" title="155:433	Here Ci denotes a Chinese character." ></td>
	<td class="line x" title="156:433	It is obvious that case (a) contradicts with case (b)." ></td>
	<td class="line x" title="157:433	Consider the string ' 5J~J!i}'i;~,'J,)-}:jl','lif~:'j: ~l;~'." ></td>
	<td class="line x" title="158:433	A personal,mum '~!" ></td>
	<td class="line x" title="159:433	ID\]J?;\[~, '' is recognized." ></td>
	<td class="line x" title="160:433	When another string '}<-I~lJl})~." ></td>
	<td class="line x" title="161:433	~lt/Jxq':lJI,.';'_~')'l'lili~,t~' is input, '~,~llllfA)~' and 'Tlldx>l ':' are identified Then we find the two strings '/}'i.J~\]}~)~, ' and ' +'iJ~\]J'.~;a:'lY/' are similar." ></td>
	<td class="line x" title="162:433	Here case (a) is correct." ></td>
	<td class="line x" title="163:433	However, case (b) also appears very often in newspapers." ></td>
	<td class="line x" title="164:433	For example, 'l~lL,kT~ ' J~l~J'gfJ~iH~ hi I, ~3L Two personal names, 'li\[\]/k:~'~/ and 'lT\[~\]Kgt'?" ></td>
	<td class="line x" title="165:433	are identified In the examples like '~.~t~\[<gif~ ' ~' and ' ~.~ o ', two candidates '~.~;{~t~, ' and' }'#~' will be identified." ></td>
	<td class="line x" title="166:433	That belon~ to case (d)." ></td>
	<td class="line x" title="167:433	Consider the last examples '11',(,1 i*l 1~,1:<' and '~,(, i~l l~g, rii:j.dz '." ></td>
	<td class="line x" title="168:433	Two candidates '~,({~ i' and '~',(,~' i~l I' will be identified too." ></td>
	<td class="line x" title="169:433	Now, case (c) is adopted." ></td>
	<td class="line x" title="170:433	in our treatment, a weight is assigned to each entry in the cache." ></td>
	<td class="line x" title="171:433	The entry which has clear right boundary has a high weight." ></td>
	<td class="line x" title="172:433	Title and punctuation are chics for boundary." ></td>
	<td class="line x" title="173:433	For those similar pairs which have different weights, the entry having high weight is selected." ></td>
	<td class="line x" title="174:433	If both have high weights, both are chosen." ></td>
	<td class="line x" title="175:433	When both have low weights, the score of the second character of a name part is critical." ></td>
	<td class="line x" title="176:433	It aetermiues if the character is kept or deleted." ></td>
	<td class="line x" title="177:433	3.3 Experiments and Discussions Table l stunmarlzes the identification results of Chinese personal names." ></td>
	<td class="line x" title="178:433	Total Chinese personal names in each section are listed in Cohlnm 2." ></td>
	<td class="line x" title="179:433	Cohmm 3 shows the precision and the recall of the baseline model." ></td>
	<td class="line x" title="180:433	The overall performance is good except for section 4 (the international section) and section 5 (the economic section)." ></td>
	<td class="line x" title="181:433	The remaining colunms demonstrate the change of performance after the clues discussed m Section 3.2 are considered incrementally." ></td>
	<td class="line x" title="182:433	If name part of a candidate is a word, word association is used to measure the relationship between the surrotmding words." ></td>
	<td class="line x" title="183:433	The increase of the precision in Cohmm 4 verifies this idea." ></td>
	<td class="line x" title="184:433	Theoretically, it shottld not decrease the recall." ></td>
	<td class="line x" title="185:433	After checking the result, we find that some unreasonable word association comes from the training corpus." ></td>
	<td class="line x" title="186:433	Recall that it is generated by a rough word segmentation system wlthoul manually-checking." ></td>
	<td class="line x" title="187:433	The next clue is punctuation." ></td>
	<td class="line x" title="188:433	The idea is that the candidates m the beginning or at the end of sentences have larger probabilities to be personal names than they are in other places." ></td>
	<td class="line x" title="189:433	It helps some candidates with lower score to pass the threshold, but it cannot avoid the incorrect candidates to pass the threshold Thus, the performance is dangling." ></td>
	<td class="line x" title="190:433	Then, title is considered The increase of the recall shows that title works well But it decreases the precision too." ></td>
	<td class="line x" title="191:433	From the variation of the performance, we know that cache is powerful." ></td>
	<td class="line x" title="192:433	Both the recall and the precismn increase." ></td>
	<td class="line x" title="193:433	Finally, gender is joined It is used when two successive characters are candidates of surnames." ></td>
	<td class="line x" title="194:433	In other words, it focuses on type 3 personal names." ></td>
	<td class="line x" title="195:433	Almost all type 3 personal names are identified correctly." ></td>
	<td class="line x" title="196:433	Because this type of personal names is rare in the testing newspaper corpus, the variation is not large." ></td>
	<td class="line x" title="197:433	Table 1 shows that our model is good except for section 4 and section 5." ></td>
	<td class="line x" title="198:433	There are many proper nouns in the international section, and ahnost all of them are not included m the dictionary." ></td>
	<td class="line x" title="199:433	All unknown words disturb one another in segmentation." ></td>
	<td class="line x" title="200:433	For example, ' q!ilI~J~0,' is a countiy name." ></td>
	<td class="line x" title="201:433	It is divided into three single characters by our word segmentation system." ></td>
	<td class="line x" title="202:433	From the viewpoint of personal nmne identification, it is easy to regard ' lI,'r~J~ff as a Chinese personal name." ></td>
	<td class="line x" title="203:433	Another source of errors is foreign names." ></td>
	<td class="line x" title="204:433	Some of them are similar to Chinese personal names, e.g., '~'~!Ui}/i:' and ' <t&jE'." ></td>
	<td class="line x" title="205:433	Table 1." ></td>
	<td class="line x" title="206:433	Identification Results of Chinese Personal Names \[l'otal ~asclinc M(Mcl i Wor~tAs soiatio." ></td>
	<td class="line x" title="207:433	+lhmctu ation +Title +Cadre +(;mdcr IIN:,,~~ IP~i~io,, \[Recall IP~oi,lo,,IP.,,~n IPredsionlRecall IPredsionlRecall II'reeisionlRecall scelionl 641 90.54% 91.11% :90.78% 90.64% 89.72% 89.86% 88.84% 90.64% 91.02% 93.29% 91.32% 93.60% scclion2 1628 86.66% 93.74% 86.94% 93.67% 86.76% 93.80% i86.08% 93.86% 93.81% 93.98% 93.99% 94.16', scclion3 666 83.90% 82.13% 83.99% 79.58% 84.01% 81.23% 83.84% 82.58% 86.41% 84.99% 86.26% 84.83% section4 148 54.22% 91.22% 55.14% 90.54% 55.14% 90.54% 55.24% 92.57% 64.09% :95.27% 64.09% 95.27% section5 t76 73.46% 88.07% 74.40% 87.50% 73.91% 86.93% 73.46% 88.07% 74.18% 89.77% 74.18% 89.77% section6 694 83.87% 93.66% 84.1)9% 93.66% 83.83% 94.09% 82.85% 94.67% 84.87% 95.39% 84.87% 95.39% Total 3953 83.79% 90.99% 84.13% 90.41% 83.84% 90.67% 83.19% 91.27% 87.94% 92.46% 88.04% 92.56% 225 The similar problem occurs in the economic section." ></td>
	<td class="line x" title="209:433	There are many company names, and some of them are similar to Chinese personal names." ></td>
	<td class="line x" title="210:433	The company name '~\]~\]i)t' is a typical example." ></td>
	<td class="line x" title="211:433	In summary, there are three major errors." ></td>
	<td class="line x" title="212:433	One is foreign name." ></td>
	<td class="line x" title="213:433	They are identified as proper nouns correctly, but are assigned wrong features." ></td>
	<td class="line x" title="214:433	About 20% of errors belong to this type." ></td>
	<td class="line x" title="215:433	The second type of errors results from the rare surnames, which are not included in the surname table." ></td>
	<td class="line x" title="216:433	Some rare surnames are not real surnames." ></td>
	<td class="line x" title="217:433	They are just artists' stage names." ></td>
	<td class="line x" title="218:433	Near 14% of errors come from this type." ></td>
	<td class="line x" title="219:433	The other errors include place names, organization names, and so on." ></td>
	<td class="line x" title="220:433	4." ></td>
	<td class="line x" title="221:433	Transliterated Personal Names 4.1 Structure of Personal Names Compared with the ideniification of Chinese personal names, the identification of transliterated personal names has the following difficulties: (a) No specific clue like surnames in Chinese personal names to trigger the identification system." ></td>
	<td class="line x" title="222:433	(b) No restriction on the length Of a transliterated personal name." ></td>
	<td class="line x" title="223:433	It may be composed of a single character or more, e.g., '5%', ' ~'i'\]d', '~'i ~Y', '~.f ~':'}~-' and 'd\[if,~ ~\]~1\]' (c) No large scale transliterated personal name corpus." ></td>
	<td class="line x" title="224:433	(d) Ambiguity in classification." ></td>
	<td class="line x" title="225:433	For example, 'J~)~' may denote a city or a former American president." ></td>
	<td class="line x" title="226:433	4.2 Strategies 4.2.1 Basic Idea Almost all foreign names are in transliteration, not in translation." ></td>
	<td class="line x" title="227:433	And the base of transliteration is pronunciation of foreign names." ></td>
	<td class="line x" title="228:433	Pronunciation is composed of syllables and tones." ></td>
	<td class="line x" title="229:433	The major difference of pronunciation between Chinese and English is syllables." ></td>
	<td class="line x" title="230:433	The style of syllabic order is specific in transliteration." ></td>
	<td class="line x" title="231:433	Consider an example." ></td>
	<td class="line x" title="232:433	The transliterated personal name,~\]z~}." ></td>
	<td class="line x" title="233:433	has syllables 'Y ~',7-~zv T-((~'." ></td>
	<td class="line x" title="234:433	Such a syllabic order is rare in Chinese, but is not special for a transliterated string." ></td>
	<td class="line x" title="235:433	In other words, the syllabic orders of transliterated strings and general Chinese strings are not similar." ></td>
	<td class="line x" title="236:433	Besides, a transliterated name consists of a string of single characters after segmentation." ></td>
	<td class="line x" title="237:433	That is, these characters cannot be put together." ></td>
	<td class="line x" title="238:433	However, the unrestrictive length of transliterated names and homophones in Chinese result in the need of very large training corpus." ></td>
	<td class="line x" title="239:433	The following sections show how to modify the basic idea if a large scale corpus is not available." ></td>
	<td class="line x" title="240:433	4.2.2 Character Condition When a foreign name is transliterated, the selection of homophones is restrictive." ></td>
	<td class="line x" title="241:433	Consider an example shown below: Richard Macs ~ll\[~l)~ ~,~,,~ Those strings following English names have the same pronunciations." ></td>
	<td class="line x" title="242:433	The first is usually adopted, and the second is never used." ></td>
	<td class="line x" title="243:433	It shows that the characters used in transliteration are selected from some character set." ></td>
	<td class="line x" title="244:433	In our model, total 483 characters are trained from our transliterated personal name corpus." ></td>
	<td class="line x" title="245:433	They play the similar role of the surnames in the identification of Chinese personal names." ></td>
	<td class="line x" title="246:433	If all the characters in a string belong to this set, i.e., they satisfy character condition, they are regarded as a candidate." ></td>
	<td class="line x" title="247:433	4.2.3 Syllable Condition Because of the unrestrictive length of transliterated names, how to identify their boundary is a problem." ></td>
	<td class="line x" title="248:433	Of course, titles and punctuation used in last section can be adopted too." ></td>
	<td class="line x" title="249:433	But they do not always appear in the text." ></td>
	<td class="line x" title="250:433	Thus another clue should be found." ></td>
	<td class="line x" title="251:433	Syllable order may be a clue." ></td>
	<td class="line x" title="252:433	Those examples like ' ~r~', '~'J,,' and ' ~' which meet the character condition do not look like transliterated names because their pronunciations are not like foreign names." ></td>
	<td class="line x" title="253:433	If there is a large enough transliterated name corpus, the syllable orders can be learned." ></td>
	<td class="line x" title="254:433	However, our transliterated corpus only contain 2692 personal names." ></td>
	<td class="line x" title="255:433	Thus only the first and the last characters are considered." ></td>
	<td class="line x" title="256:433	For each candidate, we check the syllable of the first (the last) character." ></td>
	<td class="line x" title="257:433	If the syllable does not belong to the training corpus, the character is deleted." ></td>
	<td class="line x" title="258:433	The remaining characters are treated in the similar way." ></td>
	<td class="line x" title="259:433	4.2,4 Frequency Condition As mentioned in Section 3.2.3, the frequency of a character to be a part of a personal name is important information." ></td>
	<td class="line x" title="260:433	The concept may be used here." ></td>
	<td class="line x" title="261:433	However, only large scale transliterated personal name corpus can give reliable statistical data." ></td>
	<td class="line x" title="262:433	Based on our small training corpus, the range of the application of the information should be narrowed down." ></td>
	<td class="line x" title="263:433	We only apply it in a candidate of length 2." ></td>
	<td class="line x" title="264:433	This is because it is easy to satisfy the character condition for candidates of the shortest length." ></td>
	<td class="line x" title="265:433	For each candidate which has only two characters, we compute the frequency of these two characters to see if it is larger than a threshold." ></td>
	<td class="line x" title="266:433	If it is not, it is eliminated." ></td>
	<td class="line x" title="267:433	The threshold is determined in the similar way as Section 3.2.3." ></td>
	<td class="line x" title="268:433	4.3 Experiments and Discussions The identification system scans a segnlented sentence from left to right." ></td>
	<td class="line x" title="269:433	It finds the character string that meets the character condition, syllable condition and frequency condition." ></td>
	<td class="line x" title="270:433	Table 2 shows 226 Table 2." ></td>
	<td class="line x" title="271:433	Identification Results of Transliterated Personal Names Total Names System Correct Error Lose Recall Section 1 52 64 34 30 65.38% Section 2 9 88 82 66.67~ Section 3 238 300 180 120 75~3 ~/o Section 4 301 301 230 71 76.4 I% Section 5 34 152 26 126 Section 6 214 300 134 166 62.62/o 610 Total 595 1205 Precision 18 53.13% 3 6.82% 58 60.00% 71 76.41% 8 17.11% 80 44.67% 238 50.62% 848 71.93~' the precision and the recall are both good for sections 3 and 4, i.e., the entertainment and the international sections." ></td>
	<td class="line x" title="272:433	However, sections 2 and 5 (lhe social and the economic sections) have bad precision." ></td>
	<td class="line x" title="273:433	The average recall tells us that the tri,g~ger to the identification system is nsefnl." ></td>
	<td class="line x" title="274:433	The reasons why the recall is not good enough are: some transliterated personal names (e.g. , ',~Oi~'j ~' and '~ D$~') look like Chinese personal names, and the identification of Chinese personal names is done before that of transliterated personal names." ></td>
	<td class="line x" title="275:433	Although they are correctly identified as personal names, they are assigned wrong features." ></td>
	<td class="line x" title="276:433	Similarly, transliterated nouns like popular brands of automobiles ('7\[~\[!~,'i:' and 'l\[~,)l~|~j'), Chinese proper nouns (' ~I\] ~' ' ~.J:x)~' and 't'~ II~') and Chinese personal names ('~ I:~l\]') look like transliterated personal nmnes." ></td>
	<td class="line x" title="277:433	That decreases the precision." ></td>
	<td class="line x" title="278:433	f~esides these types of nouns, boundary errors affect the precision 1oo." ></td>
	<td class="line x" title="279:433	For telling out the error rates from classification, we made another experiment." ></td>
	<td class="line x" title="280:433	If the identified results are not classified, the average precision is 81.46% and the average recall is 91.22%." ></td>
	<td class="line x" title="281:433	5." ></td>
	<td class="line x" title="282:433	Organization Names 5.1 Structures of Organization Names Structures of organization names are more complex than those of personal names." ></td>
	<td class="line x" title="283:433	Some organization names are composed of proper nouns and content r~-ILIII~H,J is made up of words." ></td>
	<td class="line x" title="284:433	For exmnple, '~' '' ' ~:' the place name 'T' ?\[\[; fii' and the content word '~t~',J-'." ></td>
	<td class="line x" title="285:433	A personal nmne can also be combined a content word to form an organization name, e.g., '~t,7~l I~1 ~i~\[ ii~ PJi '." ></td>
	<td class="line x" title="286:433	Some organization names look like personal names, e.g., '\[j\[~)t'." ></td>
	<td class="line x" title="287:433	Some organization names are composed of several related words." ></td>
	<td class="line x" title="288:433	For example, ' 7~ II~ ~ ~ ~ 'b~i, 3,~ ~ ~' contains four words %'~11~', '~'c':', '~' and '3,~'." ></td>
	<td class="line x" title="289:433	Several single-character words can also form an organization name, e.g., ' JWt J~ ii~ ~ '." ></td>
	<td class="line x" title="290:433	Some organization names have nested structures." ></td>
	<td class="line x" title="291:433	Consider the string: '~l'~,~7;~lIfi~ ~~,~ ,i~ /J~\[t'." ></td>
	<td class="line x" title="292:433	The group '~iJ~/J\~\[l' is a part of the committee,x~.~.~q-,, and the committee itself is a part of '~I;~3~i~II~,;5'." ></td>
	<td class="line x" title="293:433	Such complex structures make identification of organization names very difficult." ></td>
	<td class="line x" title="294:433	Basically, a complete organization name can be divided into two parts: name and keyword." ></td>
	<td class="line x" title="295:433	In the '-~' l'i ' ':' i l-I\[~rtl is a name, and '1l~ example i i-Ill I ~f:~f'l, '~' '' /I~J:' is a keyword." ></td>
	<td class="line x" title="296:433	Many words can serve as names, but only some fixed words can be regarded as keywords." ></td>
	<td class="line x" title="297:433	Thus, keyword ix an important clue to identify the organizations." ></td>
	<td class="line x" title="298:433	However, there are still several difficult problems." ></td>
	<td class="line x" title="299:433	First, keyword is usually a common content word." ></td>
	<td class="line x" title="300:433	It is not easy to tell out a keyword and a content word." ></td>
	<td class="line x" title="301:433	Second, a keyword may appear m the abbreviated form." ></td>
	<td class="line x" title="302:433	For exmnple, '.\]~i~i' ix an incomplete keyword of '~:~iill~l~, r\]'." ></td>
	<td class="line x" title="303:433	Third, the keyword may be omitted completely." ></td>
	<td class="line x" title="304:433	For example, '~_~),~' (Acer)." ></td>
	<td class="line x" title="305:433	The following shows two rough classifications, and discusses their feattues." ></td>
	<td class="line x" title="306:433	(l) Complete organization names (a) Structure: This type of organization names is usually composed of proper nouns and keywords." ></td>
	<td class="line x" title="307:433	(b) Length: Some organization names are very long, so it is hard to decide their length." ></td>
	<td class="line x" title="308:433	Fortunately, only some keyword like 'l iil ~ J.~', ' ~ ~', '3,~,~,:~', '~\[\[,~I~', mid so on, have this problem." ></td>
	<td class="line x" title="309:433	(c) Ambiguity: Some organization names with keywords are still mnbiguous." ></td>
	<td class="line x" title="310:433	For exmnple, ' X l' ~(~ ii,~:,' and '1\[~, ~'~'." ></td>
	<td class="line x" title="311:433	They usually denote reading matters, but not organizations." ></td>
	<td class="line x" title="312:433	However, if they are used in some contexts, e.g., '~ l'~f::il;~,~ ~ ~f! J3t\[' and ' l\[~, ~ ~\[~ (l~( ~lf )~ ', they should be interpreted as organizations." ></td>
	<td class="line x" title="313:433	(2) Incomplete organization names (a) Structure: These organization names often omit their keywords." ></td>
	<td class="line x" title="314:433	(b) Ambiguity: The abbreviated organization names may be ambiguous." ></td>
	<td class="line x" title="315:433	For example, '~t!,~($', '~ ~',',~,.,~!~' and '/~'/'  L~ are famous sport teams m Taiwm~ or in U.S.A., however, they are also general content words." ></td>
	<td class="line x" title="317:433	5.2 Strategies This section introduces some strategies used in the identification." ></td>
	<td class="line x" title="318:433	Keyword is a good indicator for an identification system." ></td>
	<td class="line x" title="319:433	It plays the similar role of surnames." ></td>
	<td class="line x" title="320:433	Keyword shows not only the possibility 227 Table 3." ></td>
	<td class="line x" title="321:433	Identification Results of Organization Names System Correct Error I Lose Precision Recall Section 1 596, 512 394 76.95% 66.11% Section 2 650 749 414 55.27% 63.69% Section 3 703 601 391 65.06% 55.62% Section 4 207 207 153 73.91% 73.91% Section 5 347 366 150 Section 6 Total Total Names 1064 3567 711 3146 442 1944 118 202 335 236 210 312 54 54 216 197 269 622 1202 1623 40.98% 62.17% 61.79% 43.23% 41.54% 54.50% of an occurrence of an organization name, but also its right boundary." ></td>
	<td class="line x" title="322:433	For each sentence, we scan it from left to right to find keywords." ></td>
	<td class="line x" title="323:433	Because keyword is a general content word, we need other strategies to tell out its exact meaning." ></td>
	<td class="line x" title="324:433	These strategies also have the capabilities to detect the left boundary if there is an organization name." ></td>
	<td class="line x" title="325:433	Prefix is a good marker for possible left boundary." ></td>
	<td class="line x" title="326:433	For example, '\[,,~I~Z' (National), '~(~qL' (Provincial), '~\]~ qi?" ></td>
	<td class="line x" title="327:433	(Private), and so on." ></td>
	<td class="line x" title="328:433	The name part of an organization may be forlned by single characters or words." ></td>
	<td class="line x" title="329:433	These two cases are discussed as follows." ></td>
	<td class="line x" title="330:433	(a) single characters After segmentation, there nmy be a sequence of single characters preceding a possible keyword The character may exist independently." ></td>
	<td class="line x" title="331:433	That is, it is a single-character word." ></td>
	<td class="line x" title="332:433	In this case, the content word is not a keyword, so that no organization name is found If these characters cannot exist independently, they form the name part of an organization." ></td>
	<td class="line x" title="333:433	The left boundary of the organization is determined by the following rule: We insert a single character to the name part until a word is met." ></td>
	<td class="line x" title="334:433	(b) word(s) Here, a word is composed of at least two characters." ></td>
	<td class="line x" title="335:433	If the word preceding the possible keyword is a place name or a personal name, then the word forms the name part of an organization." ></td>
	<td class="line x" title="336:433	Otherwise, we use word association model to determine the left boundary." ></td>
	<td class="line x" title="337:433	The postulation is: the words to compose a name part usually have strong relationships." ></td>
	<td class="line x" title="338:433	The mutual information mentioned in Section 3.2.4.2 is also used to measure the relationship of two words." ></td>
	<td class="line x" title="339:433	Part of speech is useful to determine the left boundary of an organization." ></td>
	<td class="line x" title="340:433	The categories of verbs are very typical." ></td>
	<td class="line x" title="341:433	The name part of an organization cannot extend beyond a transitive verb." ></td>
	<td class="line x" title="342:433	If a transitive verb precedes a possible keyword, then no organization name is found." ></td>
	<td class="line x" title="343:433	Numeral and classifier are also helpful." ></td>
	<td class="line x" title="344:433	For exan~ple, '~ HJ' (company) in '~fJ' (three companies ) is not a keyword due to the critical parts of speech." ></td>
	<td class="line x" title="345:433	Because a tagger is not involved before identification, the part of speech of a word is determined wholly by lexical probability." ></td>
	<td class="line x" title="346:433	5.3 Experiments and Discussions Table 3 shows the precision and the recall for every section." ></td>
	<td class="line x" title="347:433	Section 4 (The International Section) has better precision and recall than other files." ></td>
	<td class="line x" title="348:433	Most errors result from organization names without keywords, e.g., ',~,=~fijL~' '~+I(6', /\:L~I~:~!~I, JL~J, and so on." ></td>
	<td class="line x" title="349:433	Even keywords appear, e.g., ' \[-riJ~r fJ' and '~r ~ fi: ~-~', there may not always exist organization names." ></td>
	<td class="line x" title="350:433	Besides error candidates and organization names without keywords, error left boundary is also a problem." ></td>
	<td class="line x" title="351:433	Consider the exalnples: '~\[';~\['-'~!~.:),t~' and 'f~-~'." ></td>
	<td class="line x" title="352:433	In the first, '~qS)' should not be included: and in the second, a word ' 3,~'~' is lost." ></td>
	<td class="line x" title="353:433	6." ></td>
	<td class="line x" title="354:433	Applications The senmntic classification of proper nouns is use fill in many applications." ></td>
	<td class="line x" title="355:433	Here, anaphora resolution and sentence aligmnent are presented." ></td>
	<td class="line x" title="356:433	In general, pronoun often refers to the nearest proper noun (Chen, 1992)." ></td>
	<td class="line x" title="357:433	But it is not always true." ></td>
	<td class="line x" title="358:433	The following shows a counter example: The first pronoun '1'1f~' (tie) refers to the personal name '-(~,~'." ></td>
	<td class="line x" title="359:433	It is a normal example." ></td>
	<td class="line x" title="360:433	The second pronoun '~' (he) refers to the same person, but the '~'~' ' ;i;~.~!'." ></td>
	<td class="line x" title="361:433	nearest personal name is  ~j~-~ rather than If we know the gender of every personal name, then it is easy to tell out which person is referred, in the above example, the gender of the Chinese pronouns '~\[~' (he) and '/t\[\[~' (she) is masculine and feminine, respectively; tim persons ':~'17:~ ' and '.1:~'~-~'~'!'~' : ;'a, ~42~1'J\],-5S are nmle and female, respectively." ></td>
	<td class="line x" title="362:433	Therefore, the correct referential relationships can be wellestablished." ></td>
	<td class="line x" title="363:433	In the experiment of the gender assignment, 3/4 of Chinese personal name corpus is regarded as training data, and the renmining l/4 is for testing." ></td>
	<td class="line x" title="364:433	The correct rate is 89%." ></td>
	<td class="line x" title="365:433	Sentence alignment (Chen & Chen, 1994) is important in 228 setup of a bilingual corpus." ></td>
	<td class="line x" title="366:433	Personal name is one of important clues." ></td>
	<td class="line x" title="367:433	Its use in aligning EnglishChinese text is shown in the paper (Chen & Wu, 1995\]." ></td>
	<td class="line x" title="368:433	7, Concluding Remarks This paper proposes various strategies to identify and classify Chinese proper nouns." ></td>
	<td class="line x" title="369:433	The perfornmnce evahmtion criterion is very strict Not only are the proper nouns identified, but also suitable features are assigned." ></td>
	<td class="line x" title="370:433	The perforlnance (precision, recall) for the identification of Chinese personal names, transliterated personal nmnes and organization nmnes is (88.04%, 92.56%), (50.62%, 71.93%) and (61.79%, 54.50%), respectively." ></td>
	<td class="line x" title="371:433	When the criterion is loosed a little, i.e., Chinese personal nmnes and transliterated personal names are regarded as a category, the performance ~s (81.46%, 91.22%)." ></td>
	<td class="line x" title="372:433	Compared with the approaches (Sproat et al. , 1994: Fung & Wu, 1994: Wang et al. , 1994), we deal with more types of proper nouns and we have better performance." ></td>
	<td class="line x" title="373:433	Some difficult problems should be tackled in the flmlre." ></td>
	<td class="line x" title="374:433	Foreign proper nouns may be transformed in part by transliteration and translation." ></td>
	<td class="line x" title="375:433	The example 'George Town' is transformed into ':~'{{~J4~'." ></td>
	<td class="line x" title="376:433	The character 'b~' (town) results in translation and' fq:?(( (George) comes from transliteration." ></td>
	<td class="line x" title="377:433	Tlus problem is interesting and worthy of resolving." ></td>
	<td class="line x" title="378:433	The performance of identification of organization names ms not good enough, especially for those organization names without keywords." ></td>
	<td class="line x" title="379:433	It should be investigated further." ></td>
	<td class="line x" title="380:433	Acknowledgments The research was supported in part by National Science Council, Taipei, Taiwan, R.O.C. under contract NSC83-0408-E002-019, We are also thankful for the anonymous referees' comments, References Chang, J.S. el al." ></td>
	<td class="line x" title="381:433	(1992) 'Large-Corpus-Based Methods for Chinese Personal Name Recognition,' Journal of Chinese InJormation Proeesxing, Vol." ></td>
	<td class="line x" title="382:433	6, No. 3, pp." ></td>
	<td class="line x" title="383:433	7-15." ></td>
	<td class="line x" title="384:433	Chen, H.H." ></td>
	<td class="line x" title="385:433	(1990)'A Logic-Based GovernmentBinding Parser,' t'roceedingx of 13th COLIN(;, Vol." ></td>
	<td class="line x" title="386:433	2, pp." ></td>
	<td class="line x" title="387:433	48-53." ></td>
	<td class="line x" title="388:433	Chen, HH." ></td>
	<td class="line x" title="389:433	(1992) ''The Transfer of Anaphors in Translation,' Literal and Linguisttc Computing, Vol." ></td>
	<td class="line x" title="390:433	7, No. 4, pp." ></td>
	<td class="line x" title="391:433	231-238." ></td>
	<td class="line x" title="392:433	Chen, H.H." ></td>
	<td class="line x" title="393:433	(1994) 'The Contextual Analysis of Chinese Sentences with Punctuation Marks,' Literal and Linguistic Computing, Vol." ></td>
	<td class="line x" title="394:433	9, No. 4, pp." ></td>
	<td class="line x" title="395:433	281-289." ></td>
	<td class="line x" title="396:433	Chen, K.H and Chen, H,H." ></td>
	<td class="line x" title="397:433	(1994) 'A Part-ofSpeech-Based Alignment Algorithm,'' Proceedingx of 15th COIJN(;, pp." ></td>
	<td class="line x" title="398:433	166-17 I. Chen, K.J. and Liu, S.H." ></td>
	<td class="line x" title="399:433	(1992) 'Word Identification for Mandarin Chinese Sentences,' Proc.eedings of 14th ('OLIN(;, pp." ></td>
	<td class="line x" title="400:433	101 107." ></td>
	<td class="line x" title="401:433	Chen, H.H. and Wu, Y.Y." ></td>
	<td class="line x" title="402:433	(1995) 'Aligning Parallel Chinese Texts Using Multiple Clues,' Proceedings of 2nd PA ('LIN(;, pp." ></td>
	<td class="line x" title="403:433	29-48." ></td>
	<td class="line x" title="404:433	Chiang, T.H., et al.(1992) ''Statistical Models fbr Word Segmentation and Unknown Word Resolution,' Proceedingx 0/51h l~O(7,1N(;, pp." ></td>
	<td class="line x" title="406:433	121-146." ></td>
	<td class="line x" title="407:433	Fung, P. and Wu, D." ></td>
	<td class="line x" title="408:433	(1994) 'Statistical Augmentation of a Chinese Machine-Readable Dictionary,' l'roceedings o\[2nd I, VIq, C, pp." ></td>
	<td class="line x" title="409:433	6985." ></td>
	<td class="line x" title="410:433	Jolmnsson, S." ></td>
	<td class="line x" title="411:433	(1986) The 7'agged LOll Corpus: l&er's Manual, Norwegian Computing Centre for the Hunmnities, Bergen." ></td>
	<td class="line x" title="412:433	Huang, Y.J." ></td>
	<td class="line x" title="413:433	(1992) t(nglish Names fi~r You, Learning Publish Company, Taiwan." ></td>
	<td class="line x" title="414:433	Lin, M.Y. Chiang, T.H. and Su, K.Y." ></td>
	<td class="line x" title="415:433	(1993) 'A Preliminary Study on Unknown Word Problem in Chinese Word Segmentation,' l'roceedingx O/'6th I()('IJN(;, Taiwan, pp." ></td>
	<td class="line x" title="416:433	119141." ></td>
	<td class="line x" title="417:433	Smadja, F." ></td>
	<td class="line x" title="418:433	(1993) 'Retrieving Collations from Text: Xtract,' ('omputalional Linguistic.v, Vol." ></td>
	<td class="line x" title="419:433	19, No. 1, pp." ></td>
	<td class="line x" title="420:433	143-177." ></td>
	<td class="line x" title="421:433	Sproat, R. and Shih, C." ></td>
	<td class="line x" title="422:433	(1990) 'A Statistical Method for Finding Word Boundaries m Chinese Text,' Computer l'rocesxing ~?/'('hinexe and Oriental Languages, Vol." ></td>
	<td class="line x" title="423:433	4, No. 4, pp." ></td>
	<td class="line x" title="424:433	316351." ></td>
	<td class="line x" title="425:433	Sproat, R. et al.(1994) 'A Stochastic Finite-State Word-Segmentation Algorithm for Chinese,' l'roceedingx of 32nd Annual Meeting ojA(7,, New Mexico, pp." ></td>
	<td class="line x" title="427:433	66-73." ></td>
	<td class="line x" title="428:433	Wang, L.J. Li, W.C. and Chang, C.H." ></td>
	<td class="line x" title="429:433	(1992) 'Recognizing Unregistered Names for Mandarin Word Identification,' t'roceedings oJ !4th COLIN(l, Nantes, pp." ></td>
	<td class="line x" title="430:433	1239-1243, Wang, M.C. Chen, K.J. and Huang, CR." ></td>
	<td class="line x" title="431:433	(1994) '' The Identification and Classification of Unknown Words in Chinese: A N-Gram Approach,' Proceedings of PA cl,bcol 2, pp." ></td>
	<td class="line x" title="432:433	1731." ></td>
	<td class="line x" title="433:433	229" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C96-1083
Symbolic Word Clustering For Medium-Size Corpora
Habert, Benoit;Naulleau, Elie;Nazarenko, Adeline;"></td>
	<td class="line x" title="1:134	Symbolic word clustering for medium-size corpora Benoit Habert* and Elie Naulleau* ** and Adeline Nazarenko* *Equipe de Linguistique Inform~tique Ecole NorInale Sup&ieure de Fontenay-St Cloud 31 av." ></td>
	<td class="line x" title="2:134	bombart, F-92260 Fontenay-aux-Roses Firstname." ></td>
	<td class="line x" title="3:134	Name@ens-fcl." ></td>
	<td class="line x" title="4:134	fr **Direction des Etudes et Recherches Electricitd de Fra, nce 1, av." ></td>
	<td class="line x" title="5:134	du G ~z de Gaulle, F-92141 Clamart F irstname." ></td>
	<td class="line x" title="6:134	Name@der." ></td>
	<td class="line x" title="7:134	edfgdf, fr Abstract When trying to identify essential concepts and relationships in a medium-size corpus, it is not always possible to rely on statistical methods, as the frequencies are too low." ></td>
	<td class="line x" title="8:134	We present an alternative method, symbolic, based on the simplification of parse trees." ></td>
	<td class="line x" title="9:134	We discuss the resuits on nominal phrases of two technical corpora, analyzed by two different robust parsers used for terminology updating in an industrial company." ></td>
	<td class="line x" title="10:134	We compare our results with Hindle's scores of similarity." ></td>
	<td class="line x" title="11:134	Subjects Clustering, ontology development, robust parsing, knowledge acquisition from corpora, computational terminology 1 Identifying word classes in medium-size corpora In companies with a wide range of activities, such as EDF, the French electricity company, the rapid evolution of technical domains, the huge amount of textual data involved, its variation in length and style imply building or updating numerous terminologies as NLP resources." ></td>
	<td class="line x" title="12:134	In this context, terminology acquisition is defined as a twofold process." ></td>
	<td class="line x" title="13:134	On one hand, a terminologist must identify the essential entities of the domain and their relationships, that is its ontology." ></td>
	<td class="line x" title="14:134	On the other hand, (s)he must relate these entities and relationships to their linguistic realizations, so as to isolate the lexical entries to be considered as certified terms for the domain." ></td>
	<td class="line x" title="15:134	In this paper, we concentrate on the first issue." ></td>
	<td class="line x" title="16:134	Automatic exploration of a sublanguage corpus constitutes a first step towards identifying the semantic classes and relationships which are relevant for this sublanguage." ></td>
	<td class="line oc" title="17:134	In the past five years, important research on the automatic acquisition of word classes based on lexical distribution has been published (Church and Hanks, 1990; Hindle, 1990; Smadja, 1993; Grei~nstette, 1994; Grishman and Sterling, 1994)." ></td>
	<td class="line n" title="18:134	Most of these approaches, however, need large or even very large corpora in order for word classes to be discovered 1 whereas it is often the case that the data to be processed are insufficient to provide reliable lexical intbrmation." ></td>
	<td class="line x" title="19:134	In other words, it is not always possible to resort to statistical methods." ></td>
	<td class="line x" title="20:134	On the other hand, medium size corpora (between 100,000 and 500,000 words: typically a reference manual) are already too complex and too long to rely on reading only, even with concordances." ></td>
	<td class="line x" title="21:134	For this range of corpora, a pure symbolic approach, which recycles and simplifies analyses produced by robust parsers in order to classify words, offers a viable alternative to statistical methods." ></td>
	<td class="line x" title="22:134	We present this approach in section 2." ></td>
	<td class="line x" title="23:134	Section 3 describes the results on two technical corpora with two different robust parsers." ></td>
	<td class="line x" title="24:134	Section 4 compares our results to Itindle's ones (Hindle, 1990)." ></td>
	<td class="line x" title="25:134	2 Simplifying parse trees to classify words 2.1 The need for normalized syntactic contexts As Hindle's work proves it, among others (Grishman and Sterling, 1994; Grefenstette, 1994:), the mere existence of robust syntactic parsers makes it possible to parse large corpora in order to automate the discovery of syntactic patterns in the spirit of Harris's distributional hypothesis." ></td>
	<td class="line x" title="26:134	Itowever, Harris' methodology implies also to simplify and transform each parse tree 2, so as to obtain so-called 'elementary sentences' exhibiting the main conceptual classes for the domain (Sager lIa'or instance, Hindle (Hindle, 1990) needs a six million word corpus in order to extract noun similarities from predicate-argunlent structures." ></td>
	<td class="line x" title="27:134	2Changing passive into active sentences, using a verb instead of a nominalization, and so on." ></td>
	<td class="line x" title="28:134	490 NP\] NPa AP4 I I Nr As I I stenose serre NPo PP2 Pa NP6 I de D9 NPlo le NPll AP12 t NPla AP14 A15 I I I N~ A~r gauche I I tronc eorninun I?igure 1: Parse tree for stenose serre de le hone commun gauche et al. , 1987)." ></td>
	<td class="line x" title="29:134	In order to ~mtomate this normalization, we propose to post-process parse trees so as to emphasize the dependency relationships among the content words and to infer semantic classes." ></td>
	<td class="line x" title="30:134	Our approach can be opposed to the a prior one which consists in building simplified representations while parsing (Basili et al. , 1994; Metzler and Haas, 1989; Smeaton and Sheridan, 19911)." ></td>
	<td class="line x" title="31:134	2.2 R.ecycling the results of robust parsers For the sake of reusability, we chose to add a generic post-processing treatment to the results of robust parsers." ></td>
	<td class="line x" title="32:134	It ilnplies to transduce the trees resulting fl:om different parsers to a common fornlat." ></td>
	<td class="line x" title="33:134	We experimented so t~r two parsers: Aleth(h:am and I,exl;er, which are being used at DEREDI,' for terminology acquisition and updating." ></td>
	<td class="line x" title="34:134	They both analyze corpora of arbitrary length." ></td>
	<td class="line x" title="35:134	AlethGram has been developped winthin the GIIAAL project a. I,EXrI'ER has been developped at DER-EI)F (Bourigault, 1993)." ></td>
	<td class="line x" title="36:134	In this experinlent, we if)cussed on noun phrases, as they are central in most terminologies." ></td>
	<td class="line x" title="37:134	2.3 The simplification algorithm The objective is then to reduce automatically the numerous and complex nominal phrases provided by AlethGram and LEXTEI to elementary trees, 3The Eureka GRAAL project gathers in France (IC1-F, RLI (prime contractor), EDF, Aerospatiale and lenanlt." ></td>
	<td class="line x" title="38:134	which more readily exhibit the flmdamental binary relations, and to classify words with respect to these simplified trees." ></td>
	<td class="line x" title="39:134	For instance, from the parse tree for slenose serve de le tronc eommun gauche 4 (cf.fig." ></td>
	<td class="line x" title="41:134	2, in which non terminal nodes are indexed for reference purposes), the algorithm 5 yields the set of elementary trees of figure 1." ></td>
	<td class="line x" title="42:134	'l'he trees a and c correspond to contiguous words in the original sequence, whereas b and d only appear after modifier removal (see below)." ></td>
	<td class="line x" title="43:134	Two types of simplifications are applied when possible to a given tree: 11.,5'plitting: Each sub-tree immediately dominated by the root is extracted and possibly further simplified." ></td>
	<td class="line x" title="45:134	For instance, removing node NP0 yields two sub-trees: NP\], which is elementary (see below) and PP2, which needs further simplification." ></td>
	<td class="line x" title="46:134	2." ></td>
	<td class="line x" title="47:134	Modifier removal: Within the whole tree, every phrase which represents a modified constituent is replaced by the corresponding non modified constituent." ></td>
	<td class="line x" title="48:134	For example, in NP0, the adjectival modifier scrrc is removed, as well as the determiner and the adjectives 4 Tight stcnosis of left common mainstem." ></td>
	<td class="line x" title="49:134	In both parsers, {,he accents are removed during tile analysis, the lemmas are used instead of inflected fo,'ms. Additionally, fro' simplitication purposes, a contracted word like du is considered as a prepositiondeterminer sequ_enec." ></td>
	<td class="line x" title="50:134	5See (Habet't el; al. , 1.995) for a detailled presental,ion." ></td>
	<td class="line x" title="51:134	The corresponding software, SYCI,AI)E, has been developped by the tirst author." ></td>
	<td class="line x" title="52:134	491 N|)a, NP AP I I N A I I st, chose Serl?e NPv N P,, Nl)~t NP PP ~ I ~ NP AP NP A P N /' Xl' I I I I I I I N A N /I stc~ms~: de N \[ \[ \] I \[ troIlc COIflfft/l\[l ~Tollc.(\]ditch( ~ tronc l)'igm:e 2: I,\]lcmcntary trees for sl.cnose serve de lc tro,m co'mmm~ (l(mchr." ></td>
	<td class="line x" title="53:134	-_coronarien -._gauche -~------___._ a t t ein t e. de~.~ diametre .de --------'-----I~ tr0nc ~ ~,~;tenose de / coronarJ ell /\ / / '~ m rena\],~ / 5~ / ~ presence .de~ /  coronare -. coronarien / \ / '." ></td>
	<td class="line x" title="54:134	I / --_ell.staLe \ coronarJ en L  clrconrzexe / montre_de \ -de artere -\ -~\]roxlmal z -' -' \] k de artere / ~ dJametre de .-.joroximal \ / / ~dr~n \ / / de intervent-riculaire /_ aorti~,e ~ ~, ~' /  /  / '~corortarzen/\ | \ \[ / coronarien --dkffus / \ I \ / / . ~ circonflexe  / \ / k I / lorpzma coronarien existence de \  'ien~ | / / --dia on~\] ~7--k / injec~q '--de artere ~ ~ ~/ g,c   ~ \ c  rJ  --~' non-slgnlr icaEz / ~,." ></td>
	<td class="line x" title="55:134	'~, de carotide4~ -r '~ ~ / ~ \ N mztra ~ ~ eszaue / ~ \ ~7, S / N. ~ (llagnostlC de -d2 cnat;idetdulaJ !e ~ % c;ne arien'-de-'trOnc <rnarien// I ~ ---'." ></td>
	<td class="line x" title="56:134	X -_ / -' ~./ b atheromateux persist  e deI kde artere /.-_severe-/I. ire / / X-~ /severite de_<coronarien / / ~,~ .// / i~ -severeX OOCtUS~ -Cgrl~arlen --ce;;earzen '~'o~ ~'~  / / X -de artere -coronar\] en coronare de artere ' ' --~ de tronc { --severe coronarlen \ | --de tronc -_im3ortant \\severe -~de_ar Sl~tm~-~----___~_coronazzen de artere --'---'!~ a~nte ~ mab~ 1 ~ coronarien diagnostic de -~ frequence de -,1 a~asdemse Figure 3: Example of a strongly connected component ((\]MC corpus) 492 uiodifying I, ro~.c, which l('~ts (.o elenienl;ary I;ree b. W\[i(;li I;\]ie (:lirrolil." ></td>
	<td class="line x" title="57:134	imee is clc'm.~;nlary, t\]io siniplific~lJon process s~ops." ></td>
	<td class="line x" title="58:134	I~efT)re I)rocessing lill(, s(;L ()f oril>;iuM \])aJ:se l;l:ees, OllC llillS\[; dec/a>re 1.|le l;l:ees which li:i/lsi; iv)l; I)e sinll)lilied a.lly fllrl.\]ier." ></td>
	<td class="line x" title="59:134	Ill I:)iis exl)el:inlent;, a.rc (:om~id(:red as e/el\]i(;il\[,itl'y l,he ilOliiinaJ I;l'(;es which exhil)it, a. binary 1;ela.l;\]Oll I)eI;w(;(;l~ l;wo '('oiil;elil'." ></td>
	<td class="line x" title="60:134	words, \[7)r iliSl.a.ii(:e I)ei;weeN /,wo N in &ll N \]) N SCqll011cc." ></td>
	<td class="line x" title="61:134	2.4: lProni (:h:m(:nl;ary ('ont;(:xts I;o word ( '.\].,}It .',4 S ( ~." ></td>
	<td class="line x" title="63:134	S 'l'lle i:esull;iug collo(:~lJons a,e tout, rolled I)y IJie synl.acl;ic felaJ;ionshit)s sl, rll('t;llrillg l, he l)a.r.'q(, t;l'ees, which is liOl." ></td>
	<td class="line x" title="64:134	t;lie case For wi n(low--t)ased a.i)l)roaches ((Jhurch a.nd lla.liks, 199())~ ev(;n wh(;n lJley use i)a.l:l,-oP-Sl)cech la.I)els (Sniasljn, 1993; I)a.ille, 1994)." ></td>
	<td class="line x" title="65:134	Ill l;he ('Xallll)\]( % .qaitr:hc is llOf i:elaJ,ed 1;o s/,<:ltosc', as il." ></td>
	<td class="line x" title="66:134	does iiol~ liiOtti\[3~ this noun." ></td>
	<td class="line x" title="67:134	'l'hc eleiiieii/a.ry l;rces I<~d 1,o oh>sos of syiiDI.C(;ic COllt;C:;',.:Lq." ></td>
	<td class="line x" title="68:134	I'or hlsl;a, ll(:e, frOl\[l {;h(; /ec' corl'(;s|)(:)ll(\[ill~ i;o s/,v/tosc,~{~'#'~'c, t;wo (:lasses o1' (:Oll(,exl;s aye crca.lxxl." ></td>
	<td class="line x" title="69:134	'l'hc Ih'sl, ()tie, <s/cltos,': ~, iu which sl;a.nds Ior t, he lfiw)t, word, conl.a.ins serf+, whereas l.\[i(; second o11(~> N .'7C;P7'C~ (:Oiii;a.illS SC'ItO.S<." ></td>
	<td class="line x" title="70:134	kl; t;he end o1' l;he SilUl)lilic;tl, iou process, I, ll(;s(; classes ha,re I)(:ei~ cOUll)lelied ~licl olJi(;r oiler; (:rea l;e(I. VVe (-laim t,h~l, th(' s(,inant;i(', similaril,y I)elween two lcxical enl, ries is in i)l:Ol)orl;ioli wii;h I;lie mlillt)er of sha, red (:Olll;(:xl,s, \[,hi: insl;mlc(', in ol,, of ore' ((:orl)ora.,,s/,e~tosu,'.ha r(;s 8 conliel,s wit, h l(szom In order I,o get, ~ glohal vision of the similm:il.ies relyi,g on elenient, ary conl.exD;, a. gi'ad)h is C, Olill)lil;c:(\]." ></td>
	<td class="line x" title="72:134	Tim WOl~(ls CO\[lSl;il;llt;,:; l, hc IIO(Ics." ></td>
	<td class="line x" title="73:134	A link corresl~onds 1.o a. Cel:l;&ili lliiliil)er oF shared c.oni;exl;s (a<:c.ordill~ l,O ~t. chosen I.hreshold)." ></td>
	<td class="line x" title="74:134	The edges are labclle, d wiiJi l, he sha.red coiit;cxls." ></td>
	<td class="line x" title="75:134	The sl;l:oiigly colineclx;(I c.oinponeill~.s ~I a.nd t;hc cliques '/ a.l'('~ conil)ul.ed a.s woll, ~s t.hcy ~re l;he tiiosi; t'(;l(; Va.lll; l)a.rl,s oF {tie gra.i)h ~ oil i,opologica.I ~lX)lilidS, '\['lie un(l('.l:lying illl;liil;ion is l;h a,l; a~ COiliieclA;d (:Olll\])Otleli/; I'C'\]itl,(:',S lil:':igjhl)ori/igj words (llollSC\]/ and Savit;ch, \]9!)5) m~d I, hal, the cliques tend l,o isoIal;c,<dmih~i:il;y cla,ssc's." ></td>
	<td class="line x" title="76:134	An ext;rm::t of a connc'ci;ed conll)onenl;, wil;h 3 as a, threshold, a,l)pears in ligu r(; 3." ></td>
	<td class="line x" title="77:134	s'\]'he sub-graphs hi which l.here is ~t 1)aLh I)cl.ween every pair of (lisl>hicl; liO(1CS." ></td>
	<td class="line x" title="78:134	rThc sul><~ra, phs in wlfich l;here is a palJl I)et;wee\]l each lto(le and eve>r?/ olhcr noch: of l;he graph." ></td>
	<td class="line x" title="79:134	3 Results 3.1." ></td>
	<td class="line x" title="80:134	Two corpora We haw; l.esl;ed olir niei.,hod on I.wo i;echnicM niedium-size col:pora The fii:sl; ()li(;> i;he INII-cleaJ: '\[}x:hliOlogjy (}Ol.\])tls (N'I'C) of EI)I', is of a,I)olll;,52,000 words." ></td>
	<td class="line x" title="82:134	'l'he second one, I,he (k)i;<) n~u;y Medicine (JOrl)US (CM(7), is of a.I)ou, ($(), 000 words." ></td>
	<td class="line x" title="83:134	It was buill; for t, he l,;urol)ca.u M I'\]N El,AS t)rojccl." ></td>
	<td class="line x" title="84:134	(Zweigenl)a, ilni, 19)/I) and is used For 1)ilol." ></td>
	<td class="line x" title="85:134	sl,udies in l;erminology exLra.clk)n s. 3.2 A vlsnal lliap of {:OIICOps lUld relationships I@en if iio onl;ology (:~u/ I)c \['ully aJIl;o\]iiat, R:a.ily derived \[;l:Olil a." ></td>
	<td class="line x" title="86:134	('orl)iis (llaJ)erl; ;tll(\[ Na.zarelll,:o, 1996), IJle,gY( JI,A I)1,\] gra.I)hs ('AI.II I)e iise(I I.o I)()oi.slma I) i.he I)ilihting of l, he onl;olog;y o(' a. dolila.iu." ></td>
	<td class="line x" title="87:134	'l'he SY(',I,AI)I,; ii(fl, work gives a <glol>a,I view over t,hc COrl).S which etmhles {m all, ernal;e i)a, ra,digniaJ;ic a, nd sylil;agul~l;ic exl)lora.l, iou of I,he cont;cxl; o\[' a word." ></td>
	<td class="line x" title="88:134	'l'hc gl;al)h (;nat)les 1;o idenl,ify I;lic concel)t,s, I hcir possit)lc t, yl)icaJ I)rOl)erl, ies, a, lld also t, he rcla, l;ionshil)s b(;I;weCll 1;he selecl,cd COilCel)l,s. Tim cliques I)ring ()ill; sitia.ll i)ara.diglllai.ic scl.s of \['orins which, ill a. tirsl, sl.et) > Ca.ll Im iuLer I)relx;d as onl;ologh:M classes rellecl;ing coliCelfl~." ></td>
	<td class="line x" title="89:134	'l'he a.rc lal)Ns l.ticn help Ix) retilie I.llosc chlsses t)y acldiu S sOlile of the Sllrl'Olilidhl~ words whicli axe li()l, pa.i'l; ()\[' t;lie clkluc bul." ></td>
	<td class="line x" title="90:134	which ileverllie-le~s sha,r(; the iiIOSl; siguifica.nl; or SOllle Siiliila.r <Ollbexl,8." ></td>
	<td class="line x" title="91:134	1@0111 the clique {sl,+e~,o,sc, b.<Uos b obsl, r~ml, ion, altcinb:} (of." ></td>
	<td class="line x" title="92:134	fig." ></td>
	<td class="line x" title="93:134	3), one ca, t\] build l lie cla.ss of all'eel;ions which arc Io(:al;ed in l, he I)odv as {Idam.:, occ1.<~7o., s/~.Js<~, Ic,~7o., <:.l<:{li~:ali<.,, ob,~'l, rl,:l, ion, aZl, c'inl, c}." ></td>
	<td class="line x" title="97:134	Siinila.rly, from l, he gt'al)h o\[' I;he (~'M(7 corpus, Oile (:a.li i(leni,i\['y l.ll~ classes of body' ~ii, cs { artcrc, I.'anchc, rcs+sa~l, 'v<'ntri, ~dc, intc',rve, nlriculairc, cft'roli(l~,}, o(' diseases { 'malmli+, arth, crvsclcrose} and oF chirul'gica/ m:ts { l)o*ll,(~gc, rcvasc.ularisatio'n, angioplastic}." ></td>
	<td class="line x" title="98:134	Olic(; l.\]ieS(; (;Oli(:('pts axe identilied, t, hei r i)rolJei;{,i('~S Ca, fl I)C lisl, e(l, t)y i llt;erl)l:el, ing ~ I;he l ld)cls of I lie links, 'l'he al;t;ri hu I~(, of the localizaJJon of l,h(' aJl'ecl;ions is descril)ed l;\]irough three, kinds oF u:lodiliers (lig." ></td>
	<td class="line x" title="99:134	',/):,io,,n,~ (,-, (t<'." ></td>
	<td class="line x" title="100:134	{ a,'Z,.~,'<~ ', t,','<,,u:)),,,;,i,os <' ~,,:lyrics (~ d(; {ca'rotTd<', #tl, crventrTculaTre} aud a(l-,iectivcs rela, l;cd to ;~ q)('(:ific aa'l;ery (~ {coro#utiru, co'ronaricn, diaqonal, ci'lvonfl<~:(;})." ></td>
	<td class="line x" title="101:134	'l'he a l, i;i:iblll;e (legr(:e of (;lie a, fl'ecl, ion is a, lso reveaJed IJlrougjh {~ si,qm/ical, if, n<m-,siqnificati.l; severe, 7m, l)Orl.a.l. , s<;veriZc} . '(41roupe '\['erniinologie el; lnl;elligence Ari;iticielle, I ~ IC--(_I I )171." ></td>
	<td class="line x" title="102:134	| ul;elligcnce A r@icicltc, (7 NI {S 493 etude~ ~evaluatlon a t~.alyse calcul etudeS5 b N ~ essai analyse Figure 4: Polysemy of etude Last, relationships between concepts can be extracted, such as the'part-of' relation between tronc and artere, and segment and artere (fig." ></td>
	<td class="line x" title="103:134	3)." ></td>
	<td class="line x" title="104:134	3.3 Distinguishing word meanings Polysemy and quasi-synonymy often makes the ontological reading of linguistic data difficult." ></td>
	<td class="line x" title="105:134	However, through cliques and edge labels, the SYCLADE structured and documented map of the words helps to capture the word meaning level." ></td>
	<td class="line x" title="106:134	Among a set of connected words where w is similar to wi and wj, cliques bring out coherent subsets where wi and wj are also similar to each other." ></td>
	<td class="line x" title="107:134	We argue that the various cliques in which a word appears represent different axes of similarity and help to identify the different senses of that word." ></td>
	<td class="line x" title="108:134	For instance, in the whole set of words connected to etude (study) in a strongly connected component of the NTC graph (analyse, evaluation, resultat, presentation, principe, calcul, travail), some subsets form cliques with etude." ></td>
	<td class="line x" title="109:134	Two of those cliques (resp." ></td>
	<td class="line x" title="110:134	a and b in fig." ></td>
	<td class="line x" title="111:134	4 threshold of 7) bring out a concrete and a more theoretical use of etude." ></td>
	<td class="line x" title="112:134	The network also enables to distinguish the uses of quasi-synonyms such as eoronaire and coronarien in the CMC corpus." ></td>
	<td class="line x" title="113:134	Even if they are among the most similar adjectives (7 shared contexts) and if they belong to the same clique {coronaire, eoronarien, diagonal, circonflexe}, the fact that eoronarien alone is connected to evaluation adjectives (severe, signifieatif and important) shows that they cannot always substitute to each other." ></td>
	<td class="line x" title="114:134	4 Towards an adequate similarity esfimatation for the building of ontologies The comparison with the similarity score of (Hindle, 1990) shows that SYCLADE similarity indicator is specifically relevant for ontology bootstrap and tuning." ></td>
	<td class="line x" title="115:134	Hindle uses the observed frequencies within a specific syntactic pattern (subject/verb, and verb/object) to derive a cooccu,> rence score which is an estimate of mutual information (Church and Hanks, 1990)." ></td>
	<td class="line x" title="116:134	We adapted this score to noun phrase patterns) However the similarity measures based on cooccurrence scores and nominal phrase patterns are less relevant for an ontological analysis." ></td>
	<td class="line x" title="117:134	The subgraph of the chirurgical acts words, which is easy to identify from the SYCLADE graph (fig." ></td>
	<td class="line x" title="118:134	5a), is split in different parts in the similarity graph (fig." ></td>
	<td class="line x" title="119:134	5b)." ></td>
	<td class="line x" title="120:134	This difference stems from the fact that this cooccurrence score overestimates rare events and underlines the collocations specific to each form." ></td>
	<td class="line x" title="121:134	1 For instance, it appears that the relationship between stenose and lesion, which was central in figure 3, with 8 shared contexts, almost diseappears if one considers the number of shared cooccurrences." ></td>
	<td class="line x" title="122:134	Therefore, similarity measures based on cooccurrences and similarity estimation based on shared contexts must not be used in place of each other." ></td>
	<td class="line x" title="123:134	As opposed to Hindle's lists of similar words which are centered on pivot words whose neighbors are all on the same level, in SYCLADE graphs, a word is represented by its role in a whole syntactic and conceptual network." ></td>
	<td class="line x" title="124:134	The graph enables to distinguish the various meanings of words, a crucial feature in the ontological perspective since the meaning level is closer to the concept level than the word level." ></td>
	<td class="line x" title="125:134	In addition, the results are clear and more easily interpretable than those given by a statistical method, because the reader does not have to supply the explanation as to why and how the words are similar." ></td>
	<td class="line x" title="126:134	The building of an ontology, which is a timeconsuming task and which cannot be achieved automatically, can nevertheless be guided." ></td>
	<td class="line x" title="127:134	The SYCLADE graphs based on shared contexts can facilitate this process." ></td>
	<td class="line x" title="128:134	9For instance, for Na PN2 CoocNi,N~ : log 2 ~~ where f(NIPN2) is the fi'equency of noun N1 occurring with N2 in a noun preposition pattern, f(N1) is the frequency of NI as head of any N1PN,~ sequence and f(N2) the frequency of N2 in modifier/argument position of auy N~PN2 sequence and k is the count of NxPN v elementary trees in the corpus." ></td>
	<td class="line x" title="129:134	COOCNAda and CooeAd~N are similarly defined." ></td>
	<td class="line x" title="130:134	1The various cooccurrence scores retrieve sets of collocations which are sharply different fi'om the contexts shown by SYCLADE connected components." ></td>
	<td class="line x" title="131:134	The coll6cations which get the greatest cooccurrence scores seem to characterize medecine phraseology (facteur (de) risque, milieu hospitalier) but not the coronary diseases as such." ></td>
	<td class="line x" title="132:134	494 pontage angioplastie ~ artere \/ revascul~risation pontage angloplastle I her ed~tC~ionl~y!!: sme pont artere stenose l,'igure 5: Similarity among the chirurgical act family Acknowledgments We ~hank (\]hristian 3aequemin (IRIN), Didier Bourigault, Marie-Luce Herviou, JeanDavid Sta (DER EDF), Marie-tl~51~ne Candito (TAI,ANA) and Sophie Aslanides (ELI) for their remarks on a previous version of this l)aper." ></td>
	<td class="line x" title="133:134	We are very gratefid to Serge Heiden (ELI), who has developed G~aphX (ftp://mycroft." ></td>
	<td class="line x" title="134:134	ens-f c:l fr/pub/graphx/), I;hc graph interactive handling software that enabled us to visualize and handle the SYCLADI,3 graphs." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C96-1089
Learning Bilingual Collocations By Word-Level Sorting
Haruno, Masahiko;Ikehara, Satoru;Yamazaki, Takefumi;"></td>
	<td class="line x" title="1:193	Learning Bilingual Collocations by Word-Level Sorting Masahiko Haruno Satoru Ikehara Takefumi Yamazaki NTT (',ommunicatiolt Sci('nce l,al)s. 1-2:156 Take Yokosuka-Shi Ka.nagawa." ></td>
	<td class="line x" title="2:193	2:18-03,,la,l:)a,n haruno@n'c'ckb, nt-I;, j p ikehara@nttkb, rt'c-I;, j p yamazaki~nttkb, ntt." ></td>
	<td class="line x" title="3:193	j p Abstract This paper I)roposes ;t new tnethod for learning bilingual colloca, tions from sentence-aligned paralM corpora." ></td>
	<td class="line x" title="4:193	Our method COml)ris('s two steps: (1) extracting llseftll word chunks (n-grmns) by word-level sorting and (2) constructing bilingua,l ('ollocations t)y combining the word-(;hunl(s a(-quired iu stag(' (1)." ></td>
	<td class="line x" title="5:193	We apply the method to a very ('hallenging text l)~tir: a stock market 1)ullet;in in Japanese and il;s abstract in En-glish." ></td>
	<td class="line x" title="6:193	I)om;tin sl)ecific collocations are well captured ewm if they were not conta.ined in the dictionaric's of economic tel?IllS." ></td>
	<td class="line x" title="7:193	1 Introduction In the field of machitm translation, there is a, growing interest in corl)llS-I)ased al)l)roa.('hes (Sato and Nagao, 1990; l)a.gan mid (',hutch, 199d; Mat." ></td>
	<td class="line x" title="8:193	sulnoto et M. , 19.93; Kumano m,d ll imka.wa., 199d ; Smadja et al. , 1996)." ></td>
	<td class="line x" title="9:193	The main motiw~.tion behind this is to well ha.ndle, domain specific expressions." ></td>
	<td class="line x" title="10:193	I~ach apl~licatiotl dom~dn has va.rious kinds of collocations ranging from word-level to sentence-level." ></td>
	<td class="line x" title="11:193	'\]'he correct use of these collocations grea.l.ly inlluellcC's the qua.lity ofoutpttt texts." ></td>
	<td class="line x" title="12:193	Ilexa.uso such detaih'd collocations ;~r~'~ <tillicult 1:o hand-conlpile, the automatic extra(:tion of bilingual collocations is needed." ></td>
	<td class="line x" title="13:193	A number of studies haw> aJ.tetnpte(I to extract bilinguaJ collocations from paralM corpora." ></td>
	<td class="line x" title="14:193	These studies c~m be classified into two directions." ></td>
	<td class="line x" title="15:193	One is hased on the full parsing techniques." ></td>
	<td class="line x" title="16:193	(Mat,sumoto et a l. , 1993) I)roposed a. method to find out phrase-lew'l correspondences, while resolving syntactic ambiguities a.t the same time." ></td>
	<td class="line x" title="17:193	Their ninth()(Is determine t)hrase eorresl)ondences I)y using the phrase structures of I,he two hmgua,ges and oxisting bilingual dict.iona.ries." ></td>
	<td class="line x" title="18:193	Unfi)rl.unately tl,'se al)proaches are protnising only for (,he compara-I, ively short sentences l, hat ca, I>e a,a\]yze(I I>y ;t (',I+,:Y type l>arser." ></td>
	<td class="line x" title="19:193	The other direction for extracting bilingual cob local.ions involw+:s statistics." ></td>
	<td class="line x" title="20:193	(Fung, 1995) acquired bilingual word correspondences without sentetlce alignment." ></td>
	<td class="line x" title="21:193	Although these methods ;|re rob/Is| ~HI(I aSSllllle rio illfOl'lll~ttiOll SOltrce~ their outputs are just word-word corresl)otMences." ></td>
	<td class="line x" title="22:193	(Kupiec, 1993; Kumano and }lirakawa, 1!194) extracted noun phr~me (NP) correspondences from aligned parallel corpora." ></td>
	<td class="line x" title="23:193	|n (Kupiec, 1993), Nl's in English and French t;exts are+ first extracted by a. NP recoguizer." ></td>
	<td class="line x" title="24:193	Their correspotldence prol> abilities arc then gradually relined by using an EM-like iteration algorithm." ></td>
	<td class="line x" title="25:193	(t,~uma.no and Ilirakawa, 1994) lirst extracted.Japanese N Ps in the S&III(?" ></td>
	<td class="line x" title="26:193	way, and comhined statistics with a bilingtta.l dictionary tbr MT 1o find out NP (-orrespondences." ></td>
	<td class="line x" title="27:193	Although their apl)ro+tches a.t.ta.ined high accuracy for the+ task considered, the most crucial knowledge for MT is tnorc COml~lex correspOlldelices Sllch ~-LS NI'-VP corres\[,Oll(teltces atHI senl.et,'e-hwe\[ eorrespotldences." ></td>
	<td class="line x" title="28:193	It seems di\[\[icutt I.o extend these statistical lllethods to ~t I)roa.(ler rmtge of collocations because they are specialized to N l's o1: sillglc' words." ></td>
	<td class="line x" title="29:193	(Smmlj~t et al. , 1996) proposed a generM method to extract a I)roader range of colloca.tions." ></td>
	<td class="line oc" title="30:193	They first extract English collocations using the Xtract systetn (Smadja, 1993), and theu look for French coutlterparts." ></td>
	<td class="line o" title="31:193	Their search strategy is an itemtive combina.tion of two elements." ></td>
	<td class="line p" title="32:193	This is ha,sed on the intuitive ide~ tim| 'if a set of words ('onstitutes a collocation, its subset will Mso be correla.ted'." ></td>
	<td class="line n" title="33:193	Although this idea is corre~(:t, the itera|ire combination strategy generates a. mlmber ol + useless expressions." ></td>
	<td class="line o" title="34:193	In fa.ct, Xtract." ></td>
	<td class="line o" title="35:193	employs a. rol)ust l',nglish pa.rser to lilter out the wrong colloca.tions which form more thaal ha.If lhe candidates." ></td>
	<td class="line x" title="36:193	In other hmgua,ges such as Japanese, pa,rser-lmscd prmfi.g cannot be used." ></td>
	<td class="line n" title="37:193	Another drawback of their approa, ch is that only the longesl, n-gram is adopl.ed." ></td>
	<td class="line x" title="38:193	That is, when 'Ja.lmn-US auto trade talks' is ardol)ted as ;/collocation, ',lapall-IlS' cannot bc recognized as a. collocal,ion though it is i.dependently used very often." ></td>
	<td class="line x" title="39:193	In thi,~ pN)er, we propose an alt,ernative method based oil word-lewd sorting." ></td>
	<td class="line x" title="40:193	Our method com525 prises two steps: (1) extracting useful word chunks (n-grams) by word-level sorting and (2) constrncting bilingual collocations by combining the wordchunks acquired at stage (1)." ></td>
	<td class="line x" title="41:193	Given sentencealigned texts in two languages(Haruno and Yamazaki, 1996), the first step detects useful word chunks by sorting and counting all uninterrupted word sequences in sentences." ></td>
	<td class="line x" title="42:193	In this phase, we developed a new technique for extracting only useful chunks." ></td>
	<td class="line x" title="43:193	The second step of the method evaluates the statistical similarity of the word chunks appearing in the corresponding sentences." ></td>
	<td class="line x" title="44:193	Most of the fixed (uninterrupted) collocations are directly extracted from the word chunks." ></td>
	<td class="line x" title="45:193	More flexible (interrupted) collocations are acquired level by level by iteratively combining the chunks." ></td>
	<td class="line x" title="46:193	The proposed method, which uses effective word-level sorting, not only extracts fixed collocations with high precision, but also avoids the combinatorial explosion involved in searching flexible collocations." ></td>
	<td class="line x" title="47:193	In addition, our method is robust and suitable for real-world applications because it only assumes part-of-speech taggers for both languages." ></td>
	<td class="line x" title="48:193	Even if the part-of-speech taggers make errors in word segmentation, the errors can be recovered in the word chunk extraction stage." ></td>
	<td class="line x" title="49:193	2 Two Types of Japanese-English Collocations In this section, we briefly classify the types of Japanese-English collocations by using the material in Table 1 as an example." ></td>
	<td class="line x" title="50:193	These texts were derived from a stock market bulletin written in Japanese and its abstract written in English, which were distributed electrically via a computer network." ></td>
	<td class="line x" title="51:193	In Table 1, (~g-~,~'l-~/Tokyo Forex), (H~I~!IYJ ~\[~n~\]{~ /auto talks between Japan and the U.S)." ></td>
	<td class="line x" title="52:193	and (~k,.'~/ahead of) are Japanese-English collocations whose elements constitute uninterrupted word sequences." ></td>
	<td class="line x" title="53:193	We call hereafter this type of collocation fixed eolloeatlon." ></td>
	<td class="line x" title="54:193	Although fixed collocation seems trivial, more than half of all useful collocations belong to this class." ></td>
	<td class="line x" title="55:193	Thus, it is important to extract fixed collocations with high precision." ></td>
	<td class="line x" title="56:193	In contrast, ( b')t-t~'~ ~ ~1~ ki~?,_ ~ / The U.S. currency was quoted at -~ ) and ( b' )t.~'~ ~ ~l~ ~k_2~ /The dollar stood ~)1 are constructed from interrupted word sequences." ></td>
	<td class="line x" title="57:193	We will call this type of collocation flexible collocation." ></td>
	<td class="line x" title="58:193	From the viewpoint of machine learning, flexible collocations are much more difficult to learn because they involve the combination of elements." ></td>
	<td class="line x" title="59:193	The points when extracting flexible collocations is how the number of combination (candidates) can be reduced." ></td>
	<td class="line x" title="60:193	Our learning method is twofold according to the collocation types." ></td>
	<td class="line x" title="61:193	First, useful uninterrupted 1 ~." ></td>
	<td class="line x" title="62:193	represents any sequence of words." ></td>
	<td class="line x" title="63:193	word chunks are extracted by the word-level sorting method." ></td>
	<td class="line x" title="64:193	To find out fixed collocations, we evaluate stochastic similarity of the chunks." ></td>
	<td class="line x" title="65:193	Next, we iteratively combin the chunks to extract flexible collocations." ></td>
	<td class="line x" title="66:193	3 Extracting Useful Chunks by Word-Level Sorting 3.1 Previous Research With the availability of large corpora and memory devices, there is once again growing interest in extracting n-grams with large values of n." ></td>
	<td class="line x" title="67:193	(Nagao and Mori, 1994) introduced an efficient method for calculating an arbitrary number of n-grams from large corpora." ></td>
	<td class="line x" title="68:193	When the length of a text is I bytes, it occupies l consecutive bytes in memory as depicted in Figure 1." ></td>
	<td class="line x" title="69:193	First, another table of size l is prepared, each field of which represents a pointer to a substring." ></td>
	<td class="line x" title="70:193	A substring pointed to by the (i 1)th entry of the table constitutes a string existing from the ith character to the end of the text string." ></td>
	<td class="line x" title="71:193	Next, to extract common substrings, the pointer table is sorted in alphabetic order." ></td>
	<td class="line x" title="72:193	Two adjacent words in the pointer table are compared and the lengths of coincident prefix parts are counted(Gonnet et al. , 1992)." ></td>
	<td class="line x" title="73:193	For example, when 'auto talks between Japan and the U.S'." ></td>
	<td class="line x" title="74:193	and 'auto talks between Japan and China' are two adjacent words, the nmnber of coincidences is 29 as in 'auto talks between Japan and '." ></td>
	<td class="line x" title="75:193	The n-gram frequency table is constructed by counting the number of pointers which represent the same prefix parts." ></td>
	<td class="line x" title="76:193	Although the method is efficient for large corpora, it involves large volume of fractional and unnecessary expressions." ></td>
	<td class="line x" title="77:193	The reason for this is that the method does not consider the inter-relationships between the extracted strings." ></td>
	<td class="line x" title="78:193	That is, the method generates redundant substrings which are subsumed by longer strings." ></td>
	<td class="line x" title="79:193	text ntr|hg (I oharaoter~: I bytes) la, ol.ter table Figure 1: Nagao's Approach To settle this problem, (Ikehara et al. , 1996) proposed a method to extract only useful strings." ></td>
	<td class="line x" title="80:193	Basically, his methods is based on the longestmatch principle." ></td>
	<td class="line x" title="81:193	When the method extracts a longest n-gram as a chunk, strings subsumed by the chunk are derived only if the shorter string of_ tell appears independently to the longest chunk." ></td>
	<td class="line x" title="82:193	If 'auto talks between Japan and the U.5''." ></td>
	<td class="line x" title="83:193	is extracted as a chunk, 'Japan and the U.S.'is also 526 Tokyo Forex 5 PM: Dollar at 84.21-84.24 yen The dollar stood 0.26 yen lower at 84.21-84.24 at 5 p.m." ></td>
	<td class="line x" title="84:193	Forex market trading was extremely quiet ahead of fnrther auto talks between Japan and the U.S., slated for early dawn Tuesday." ></td>
	<td class="line x" title="85:193	The U.S. currency was quoted at 1.361-1.3863 German marks at 5:15 p.m." ></td>
	<td class="line x" title="86:193	Table 1: Sample of Target Texts extracted because 'Japan and the U.S'." ></td>
	<td class="line x" title="87:193	is used so often independently as in 'Japan and the U.S. agreed '." ></td>
	<td class="line x" title="88:193	However, 'Japan and the' is not extracted because it always appears in the context of 'Japan and the U.S.'." ></td>
	<td class="line x" title="89:193	The method strongly suppresses fractional and unnecessary expressions." ></td>
	<td class="line x" title="90:193	More than 75 % of the strings extracted by Nagao's method are removed with the new method." ></td>
	<td class="line x" title="91:193	3.2 Word-Level Sorting Method al Ipl l I@ i I \[dl@~tlh\] \[ poln~r imbl~ O:~nl ~llm~r Figure 2: Word-Level Sorting Approach The research described in the previous section deals with character-based n-grams, which generate excessive numbers of expressions and requires large memory for the pointer table." ></td>
	<td class="line x" title="92:193	Thus, from a practical point of view, word-based n-grams are preferable in order to further suppress fractional expressions and pointer table use." ></td>
	<td class="line x" title="93:193	In this paper, we extend Ikehara's method to handle word-based n-grams." ></td>
	<td class="line x" title="94:193	First, both Japanese and English texts are part-of-speech (POS) tagged 2 and stored in memory as in Figure 2." ></td>
	<td class="line x" title="95:193	POS tagging is required for two main reasons: (1) There are no explicit word delimiters in Japanese and (2) By using POS information, useless expressions can be removed." ></td>
	<td class="line x" title="96:193	In Figure 2, '@' and '\0' represent the explicit word delimiter and the explicit sentence delimiter, respectively." ></td>
	<td class="line x" title="97:193	Compared to previous research, this data structure has the following advantages." ></td>
	<td class="line x" title="98:193	2We use in this phase the JUMAN morphological analyzing system (Kurohashi et al. , 11994) for tagging Japanese texts and Brill's transformation-based tagget (Brill, 1994) for tagging English texts." ></td>
	<td class="line x" title="99:193	We would like to thank all people concerned for providing us with the tools." ></td>
	<td class="line x" title="100:193	1." ></td>
	<td class="line x" title="101:193	Only heads of each word are recorded in the pointer table." ></td>
	<td class="line x" title="102:193	As depicted in Figure 2, this remarkably reduces memory use because the pointer table also contains other string characteristics as Figure 3." ></td>
	<td class="line x" title="103:193	2. As depicted in Figure 2, only expressions within a sentence are considered by introducing the explicit sentence delimiter '\0'." ></td>
	<td class="line x" title="104:193	3." ></td>
	<td class="line x" title="105:193	Only word-level coincidences are extracted by introducing the explicit word delimiter '@'." ></td>
	<td class="line x" title="106:193	This removes strings arising from a partial match of different words." ></td>
	<td class="line x" title="107:193	For example, the coincident string between 'Japan and China' and 'Japan and Costa Rica' is 'Japan and'in our method, while it is 'Japan and C' in previous methods." ></td>
	<td class="line x" title="108:193	colnol ~ont adopt dance ~4 (15 1-/0 2 I lC)*I 104 string I0 I) 16 I 6 16 J~p. n~-v andc~a  'h m,,ov J, ,.,,<.otc.~,c'o." ></td>
	<td class="line x" title="109:193	~1o." ></td>
	<td class="line x" title="110:193	a ap." ></td>
	<td class="line x" title="111:193	n~-q an d,~ t |~,*~ 1 Js Ju pa t t(~ an,tC,~ t I~ U S J it pit t~ ttnU~, t I~<~ ~ I S Ja p a i ~U ~n (ICa) II~C~O_  / Figure 3: Sorted Pointer Table Next, the pointer table is sorted in alphabetic order as shown in Figure 3." ></td>
	<td class="line x" title="112:193	In this table, sentno, and coincidence represent which senfence the string appeared in and how many characters are shared by the two adjacent strings, respectively." ></td>
	<td class="line x" title="113:193	That is, eoineidenee delineates candidates for usefifl expressions." ></td>
	<td class="line x" title="114:193	Note here that the coincidence between Japan@and@China and Japan@and@Costa Rica is l0 as mentioned above." ></td>
	<td class="line x" title="115:193	Next, in order to remove useless subsumed strings, the pointer table is sorted according to sentno In this stage, adopt is filled with '1' or '0', each of which represents if or not if a string is subsumed by longer word chnnks, respectively." ></td>
	<td class="line x" title="116:193	Sorting by sentno, makes it much easier to check the subsumption of word chunks." ></td>
	<td class="line x" title="117:193	When 527 both 'Japan and the U.S'." ></td>
	<td class="line x" title="118:193	and 'Japan and the' arise from a sentence, the latter is removed because the former subsumes the latter." ></td>
	<td class="line x" title="119:193	Finally, to determine which word-chunks to extract, the pointer table is sorted once again in alphabetic order." ></td>
	<td class="line x" title="120:193	In this stage, we count how many times a string whose adopt is 1 appears in the corpus." ></td>
	<td class="line x" title="121:193	By thresholding the frequency, only usetiff word chunks are extracted." ></td>
	<td class="line x" title="122:193	4 Extracting Bilingual Collocations In this section, we will explain how JapaneseEnglish collocations are constructed from word chnnks extracted in the previous stage." ></td>
	<td class="line x" title="123:193	First, fixed collocations are induced in the following way." ></td>
	<td class="line x" title="124:193	We use the contingency matrix to evaluate the similarity of word-chunk occurrences in both languages." ></td>
	<td class="line x" title="125:193	Consider the contingency matrix, shown Table 2, for Japanese word chunk cjp,~ and English word chunk c~,g. The contingency matrix shows: (a) the number of Japanese-English corresponding sentence pairs in which both Cjp n and ce,~g were found, (b) the number of Japanese-English corresponding sentence pairs in which just c~, v was found, (c) the number of Japanese-English corresponding sentence pairs in which just ejp,~ was fonnd, (d) the mnnber of Japanese-English col responding sentence pairs in which neither chunk was found." ></td>
	<td class="line x" title="126:193	Ceng a b c d Table 2: Contingency Matrix If ejpn and Cen.q are good translations of one another, a should be large, and b and c should bc small." ></td>
	<td class="line x" title="127:193	In contrast, if the two are not good translations of each other, a should be small, mid baud c should be large." ></td>
	<td class="line x" title="128:193	To make this argument more precise, we introduce mutual information ~s follows." ></td>
	<td class="line x" title="129:193	Thresholding the mutual information extracts fixed collocations." ></td>
	<td class="line x" title="130:193	Note that mutual information is reliable in this case because the frequency of each word chunk is thresholded at the word chunk extraction stage." ></td>
	<td class="line x" title="131:193	p,'ob(q,,,,, c~,,.,) = log '(' + ~ + ~ + d) log v,.ob(,:j,,,)v,.ob(~,~,,~) (, + b)(, + c) Next, we sumnmrize how flexible collocations are extracted." ></td>
	<td class="line x" title="133:193	The following is a series of procedures to extract flexible collocations." ></td>
	<td class="line x" title="134:193	1." ></td>
	<td class="line x" title="135:193	For any pair of chunks in a Japanese sentence, compute mutual information." ></td>
	<td class="line x" title="136:193	Con> bine the two chunks of highest mutual information." ></td>
	<td class="line x" title="137:193	Iteratively repeat this procedure and construct a tree level by level." ></td>
	<td class="line x" title="138:193	2." ></td>
	<td class="line x" title="139:193	For any pair of chunks in an English sentence, repeat the operations done in the the Japanese sentence." ></td>
	<td class="line x" title="140:193	3." ></td>
	<td class="line x" title="141:193	Perform node matching between trees of both langnages by using mutual information of Japanese and English word chunks." ></td>
	<td class="line x" title="142:193	tin,~l~ore R Figure 4: Constructing Flexible Collocations The first two steps construct monolingual similarity trees of word chnnks in sentences." ></td>
	<td class="line x" title="143:193	The third step iteratively evalnates the bilingual similarity of word chunk combinations by using the above trees." ></td>
	<td class="line x" title="144:193	Consider the example below, in which the underlined word chunks construct a flexible collocation (~ Yif/~~.~t~,f~t~_~,:x ~g ~, I-iti~'~3: ~_k~-L/~:/~ rose ~ on the oil products spot market in Singapore)." ></td>
	<td class="line x" title="145:193	First, two similarity trees are constructed as shown in Figure 4." ></td>
	<td class="line x" title="146:193	Graph matching is then iteratively attempted by compnting mutual inforlnation fbr groups of word chunks." ></td>
	<td class="line x" title="147:193	In the present implementation, the system combines three word chunks at most." ></td>
	<td class="line x" title="148:193	The technique we use is similar to the parsing-b~sed methods for extracting bilingual collocation(Matsumoto et al. , 1993)." ></td>
	<td class="line x" title="149:193	Our method replaces the parse trees with the similarity trees and thus avoids the combinatorial explosion inherent to the parsing-ba~sed methods." ></td>
	<td class="line x" title="150:193	lia:ample:,,, Naphtha and gas oil rose on the oil products spot market in Singapore 5 Preliminary Evaluation and Discussion We performed a preliminary ewduation of tile proposed method by using 10-days Japanese stock market bulletins and their Fnglish abstracts, each containing 2000 sentences." ></td>
	<td class="line x" title="151:193	The text was first au-tomatically aligned and then hand-checked by a hum~m supervisor." ></td>
	<td class="line x" title="152:193	A sample passage is displayed in TM~Ie 1." ></td>
	<td class="line x" title="153:193	In this experiment, we considered only the word chunks thai; appeared more than 4 times for fixed collocations and more than 6 times for flexible collocations." ></td>
	<td class="line x" title="154:193	Table 4 illustrates the fixed collocations acquired by our method." ></td>
	<td class="line x" title="155:193	Almost all collocat.ions in Table 4 involw~ domain specilic jargon, which 528 ~tmil~Tse English DI(j\](~I-~ ~ I 1\] ~ Tokyo Forex ~ I)olhu' ~tt, ~ yen b',i!--'l-1 ~ '~\[+\]\]~ 9 ~I ~ ~_~k ?c The 1,J.S. clirrency WitS (llloted at were sold ~ dropped as well I I~I~ ~' fJ~'i~(|l b/'e I la,nk of,lapiin injected P~-d~'n Y -~-\]!~ ~' OlllrOll ~ ~lllllil, GlllO Forcsl, fy -Tal)h; 3: Saniples of li'lexible (~ollocations No___ Jttl)O lles(: = ~.k'c,t JAFCO ~-~ -7--~ s q-:~.k b/%'~ ~, 6 t0,j,~,~ ~ ~ 2' ':; Ta) 5~ ~~t~ 14 15 16 17 18 2o 22 ~3 24 2~L_ _ 26 27 28 29 3O 31 3~ aa_~ 34 as 36 _ 37 C B Q s\]z~j ~kA,~ 0 f~ff kt f/19~l Ig} ' t11~ Eiil411sh Tokyo Forex ahead of German mark Japan Associated lCin~nce in contrast remained aidelined watching fear Tokyo Gold future~ (\]in: slow wait-and-ace mood Loco-London gold ~inat mark Convertible bond~ dealers t radin~; volume h~i~h-yieldera Nikkei 300 futuren Aft-opg: ~-cln: con| r~ct ended economic ~timulun packa e~a -cloted t~t futurea cls: bond Int~rket convertible bondn nikkei future~ aft-opg: ~ \[." ></td>
	<td class="line x" title="156:193	disheartened by -'0')~1.~7)~ ~  pet|at|on of %Lo~,9 ~'1~ 4~q: e@;ed up hish-tech sharen wMt-and-~e mood Surnit omo Forestry U.S.-dapttn ~uto t~lks npecul~tive buying of_ Tokyo ~ me.8: ~ intereat rate~ the dapan-U,S. &uto dispute N -oT." ></td>
	<td class="line x" title="157:193	T .J ii i)l-i 11 thG(~ as f~9fll~ *~ 'l',t31~)l 4 a ~Cl 4Al~ --,,m-,4~ 4o /\]'I I a)9'd I) ~--7-~Ltc --~-~--~ 56 57 o~ 0-~: 3 5t 68 70 72 English bond~ and bond futures public funds inttitutional inventors benchmark semiconductor-related ~tocks forei6n inve~tor~ hlgh-tech ~tocks turnover small-lot ~ellinfz r~cord high benchmark low Tokyo Stockn 2nd Sec were weak individual inveatora pretax profit The firnt ~ection of TSI~, the Nikkei ~tock average Tokyo CB~ O qp~ i long-term government bondll were ~rnded at impor terll advanced coverin~ Showa Denko volume w~ hi ta new year'~ hi~rh ruling co~litlon q:i~JJ~Tif~ __ tl~\[:~l~R'~r,i,:}~,~ ,~  Nikkei World Commoditie,: ~ir,J I r/ Sumltomo Special Metals Nikkei 300 futuren Mn~ OSl~ year'~ low ln~ close inched np Ta, ble 4: Siunples of Ficd Collocation,<~ cannot, be const.rueted composit, ionally." ></td>
	<td class="line x" title="158:193	For examphi, No 9 nieans 'Tokyo (~ohl FuLure, m~rkel; ended trading R)r the (lay', but was never written as such." ></td>
	<td class="line x" title="159:193	As well as No. 9, a nuuflml: ofseut;ence-level collocations were also extracl, ed." ></td>
	<td class="line x" title="160:193	No. 9, No. 18, No. 23, No. 2< No. 35, No. 56 and No. 67 a.re t,ypica,l heads of Llle stock markel; report." ></td>
	<td class="line x" title="161:193	These exi)rcssioiis a.pllear eweryda.y in st.ock markel, reports." ></td>
	<td class="line x" title="162:193	IlL is inl, eresl, iil E I4) not, ic(~ lhe variel,y o\[ fixed colh)ca.tions." ></td>
	<td class="line x" title="163:193	They dill'~'r in their consl.rucl.ions; noun phrases, verll phrases, I)rel)osit.iolml phrase<; and sentrnce--level." ></td>
	<td class="line x" title="164:193	All, hough coltventionaJ nleLllotis focus on houri llhrases or |,ry t;o en(:onll/ass all kinds of (-olloca.tions at the sanie time, we beliew' l, ha, t, fixed colloca, tion is au ilnporl,anl, class o\[' colh)cation." ></td>
	<td class="line x" title="165:193	It is useful to iltl,ensively sl,udy fixed collocations because 1,he (:ollocatioll of lilore com-plex structures is (lillic.lt to h'i,', regardle'~,~ of the mf~l,hod used." ></td>
	<td class="line x" title="166:193	'I'MAe 3 exemplifies the flexible colloca.tions we acquired fronl the saint cOrllUS." ></td>
	<td class="line x" title="167:193	No. 1 to No. 4 are typical exprossions in stock nlarkc'l, reports." ></td>
	<td class="line x" title="168:193	These collocation are eXl;l'enlc.ly useful for l,ellll)lal, e-based nlachine /.ra.nsla.tiol~ sysl.enls." ></td>
	<td class="line x" title="169:193	No. 5 is a.n examph~ o1' a useless ('ol\[ocalriOIt." ></td>
	<td class="line x" title="170:193	BOt\]l Olnron a, nd ~unii|,omo Forcst;ry arc cotupap, y names 1,lid, l; co-ocemI'requenl, ly i. sl,ock uia,l'kel, i'el)ort;s, bul, t.he.qc two conlpanics ha,ve uo direct relal;iou." ></td>
	<td class="line x" title="171:193	In fact, nlore I.han half of a.II lh!xibh~ collocations acquired were like No. 5." ></td>
	<td class="line x" title="172:193	To remove useh>ss coJJ()(';tlions, co,stra.inl.s <)n l;ll<' <'haracl.er tyl>eS would I)e useful." ></td>
	<td class="line x" title="173:193	Most useful,lapa/ICSe /lcxiblt' (:ollocai.iOllS coul;;lin al, least one ilira.gamt 3 ch~u-acter." ></td>
	<td class="line x" title="174:193	Thus, 3,I a i)~nese has (,}n'c(~ t,y pe,~ of ch ara~ctcrs ( II ira.ga.na, I(atak;~na., and t<anjO, each of which has dilt't!rcnt a.n.)uttts of i.lbrntalio In ( OllLl,t,qt, Enl-lish ha.s ouly 529 many useless collocations can be removed by imposing this constraint on extracted strings." ></td>
	<td class="line x" title="176:193	It is also interesting to compare our results with a Japanese-English dictionary for economics (Iwatsu, 1990)." ></td>
	<td class="line x" title="177:193	About half of Table 4 and all of Table 3 are not listed in the dictionary." ></td>
	<td class="line x" title="178:193	In particular, no verb-phrase or sentence-level collocations are not covered." ></td>
	<td class="line x" title="179:193	These collocations are more useful for translators than noun phrase collocations, but greatly differ from domain to domain." ></td>
	<td class="line x" title="180:193	Thus, it is difficult in general to hand-compile a dictionary that contains these kinds of collocations." ></td>
	<td class="line x" title="181:193	Because our method automatically extracts these collocations, it will be of significant use in compiling domain specific dictionaries." ></td>
	<td class="line x" title="182:193	Finally, we briefly describe the coverage of the proposed method." ></td>
	<td class="line x" title="183:193	For the corpus examined, 70 % of the fixed collocations and 35 % of the flexible collocations output by the method were correct." ></td>
	<td class="line x" title="184:193	This level of performance was achieved in the face of two problems." ></td>
	<td class="line x" title="185:193	 The English text was not a literal translation." ></td>
	<td class="line x" title="186:193	Parts of Japanese sentence were often omitted and sometimes appeared in a different English sentence." ></td>
	<td class="line x" title="187:193	 The data set was too small." ></td>
	<td class="line x" title="188:193	We are now constructing a larger volume of corpus to address the second problem." ></td>
	<td class="line x" title="189:193	6 Conclusion We have described a new method for learning bilingual collocations from parallel corpora." ></td>
	<td class="line x" title="190:193	Our method consists of two steps: (1) extracting useful word chunks by the word-level sorting technique and (2) constructing bilingual collocations by combining these chunks." ></td>
	<td class="line x" title="191:193	This architecture reflects the fact that fixed collocations play a more crucial role than accepted in previous research." ></td>
	<td class="line x" title="192:193	Our method not only extracts fixed collocations with high precision but also reduces the combinatorial explosion that would be otherwise considered inescapable in extracting flexible collocations." ></td>
	<td class="line x" title="193:193	Although our research is in the preliminary stage and tested with a small number of Japanese stock market bulletins and their English, the experimental results have shown a number of interesting collocations that are not contained in a dictionary of economic terms." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C96-1097
A Statistical Method For Extracting Uninterrupted And Interrupted Collocations From Very Large Corpora
Ikehara, Satoru;Shirai, Satoshi;Uchino, Hajime;"></td>
	<td class="line x" title="1:186	A Statistical Method for Extracting Uninterrupted and Interrupted Collocations from Very Large Corpora Satoru Ikehara, Satoshi Shirai and Hajime Uchino NTT Communication Science Laboratories Take 1-2356, Yokoshuka-shi, Japan (E-mail:{ikehara, shirai, uchino}@nttkb.ntt.jp) Abstract In order to extract rigid expressions with a high frequency of use, new algorithm that can efficiently extract both uninterrupted and interrupted collocations from very large corpora has been proposed." ></td>
	<td class="line x" title="2:186	The statistical method recently proposed for calculating N-gram of m'bitrary N can be applied to the extraction of uninterrupted collocations." ></td>
	<td class="line x" title="3:186	But this method posed problems that so large volumes of fractional and unnecessary expressions are extracted that it was impossible to extract interrupted collocations combining the results." ></td>
	<td class="line x" title="4:186	To solve this problem, this paper proposed a new algorithm that restrains extraction of unnecessary substrings." ></td>
	<td class="line x" title="5:186	This is followed by the proposal of a method that enable to extract interrupted collocations." ></td>
	<td class="line x" title="6:186	The new methods are applied to Japanese newspaper articles involving 8.92 million characters." ></td>
	<td class="line x" title="7:186	In the case of uninterrupted collocations with string length of 2 or mere characters and frequency of appearance 2 or more times, there were 4.4 millions types of expressions (total frequency of 31.2 millions times) extracted by the N-gram method." ></td>
	<td class="line x" title="8:186	In contrast, the new method has reduced this to 0.97 million types (total frequency of 2.6 million times) revealing a substantial reduction in fractional and unnecessary expressions." ></td>
	<td class="line x" title="9:186	In the case of interrupted collocational substring extraction, combining the substring with frequency of 10 times or more extracted by the first method, 6.5 thousand types of pairs of substrings with the total frequency of 21.8 thousands were extracted." ></td>
	<td class="line x" title="10:186	I. Introduction In natural language processing, the importance of large volume corpus has been pointed out together with the need for technology of analyzing these linguistic data." ></td>
	<td class="line x" title="11:186	For example, in machine translation, there are many expressions that are difficult to be translated literally." ></td>
	<td class="line x" title="12:186	Phrase translations or pattern translations based on phrase or pattern dictionaries are considered very useful for the translations of these expressions." ></td>
	<td class="line x" title="13:186	In order to realize these translation, it is required to identify phrases of high frequency and patterns of expressions from the corpora." ></td>
	<td class="line oc" title="14:186	There are many method proposed to extract rigid expressions from corpora such as a method of focusing on the binding strength of two words (Church and Hanks 1990); the distance between words (Smadja and Makeown 1990); and the number of combined words and frequency of appearance (Kita 1993, 1994)." ></td>
	<td class="line x" title="15:186	But it was not easy to identify and extract expressions of arbitrary lengths and high frequency of appearance from very large corpora." ></td>
	<td class="line oc" title="16:186	Thus, conventional methods had to introduce some kinds of restrictions such as the limitation of the kind of chains or the length of chains to be extracted (Smadja 1993, Shinnou and Isahara 1995)." ></td>
	<td class="line x" title="17:186	Recently, a new method which can calculate arbitrary number of n-gram statistics for very large corpora has been proposed (Nagao and Mori 1994)." ></td>
	<td class="line x" title="18:186	This method has made it possible to automatically and quickly extract and tabulate substrings of any length used in source texts." ></td>
	<td class="line x" title="19:186	Unfortunately, in this method, so many fractional substrings that were grammatically and semantically inconsistent were being extracted that it was difficult to extract combi nations of expressions collocated at separate locations (i.e. interrupted collocation) which requires a search of the source text by combining the strings thus extracted." ></td>
	<td class="line x" title="20:186	Thus, the analyses had to be limited into small texts (Colier 1994)." ></td>
	<td class="line x" title="21:186	To overcome this problems, this paper first, proposes a method that can automatically extract and tabulate uninterrupted collocational substrings and without omission from the corpora in the order of substring length and frequency under the condition that fractional substrings are excluded." ></td>
	<td class="line x" title="22:186	Second, using the results of the first method, it also proposes a method that can automatically extract and tabulate interrupted coUocational substrings." ></td>
	<td class="line x" title="23:186	2." ></td>
	<td class="line x" title="24:186	N-gram Method and the Problem Involved (1) Conditions for Collocational Substring extradtion In order to extract uninterrupted collocation without omission and to minimize extraction of fractional substrings, we will introduce the following three conditions." ></td>
	<td class="line x" title="25:186	1st Condition: Substrings can be extracted in the order of the number of matching character (string length)." ></td>
	<td class="line x" title="26:186	2nd Condition: Substrings can be extracted in the order of frequency of use." ></td>
	<td class="line x" title="27:186	3rd Condition: Substrings should be extracted according to the principle of the longest match." ></td>
	<td class="line x" title="28:186	Fig." ></td>
	<td class="line x" title="29:186	1 Substrings to be Extracted Here, 3rd condition means that when a string (for instance a in Fig.l) is extracted from a certain location within the source text, any substring ( B, T ) that is included within the string ( a ) is not subject to extraction." ></td>
	<td class="line x" title="30:186	But should such substring ( 6 ) be located in a separate or overlap 574 position, it is to be extracted." ></td>
	<td class="line x" title="31:186	(2) Conventional Algorithm for N-gram Statistics Before discussing the algorithm which satisfies the previous conditions for uninterrupted collocational substring, let's consider the Nagao and Mori's algorithm propose for N-gram statistics." ></td>
	<td class="line x" title="32:186	\[Statistical Method for N-gram\] Assume that the total number of characters in a source text (corpus) is N. Procedure 1: Preparation of Pointer Table Prepare PT-0 (Pointer Table-O) of N records of SP (Source Pointer), with the values of 0, 1, 2, i,,N-1." ></td>
	<td class="line x" title="33:186	Here, the value i represents the String-word i which is the substring from position i to the last character (N-1 address) in the source text." ></td>
	<td class="line x" title="34:186	Procedure 2: Pointer Table Sorting The records of.PT-0 are sorted in the order of corresponding String-words to obtain SPT-O (Sorted Pointer Table-0)." ></td>
	<td class="line x" title="35:186	Procedure 3: Counting of Matching Characters The characters of String-word i is compared with that of the next String-word i+1 from the beginning." ></td>
	<td class="line x" title="36:186	The number of matched characters are registered in the field of a NMC (Number of Matching Character) in the record i. Procedure E: Extraction of Substrings Comparing the values of NMCs of record i and that of the record i+1 of the SPT from i=1 to i=N-1, substrings are extracted and their frequency are determined* 1." ></td>
	<td class="line x" title="37:186	(3) Problems of N-gram Statistics Nagao and Mori's method obviously fulfills requirements of Conditions 1 and 2, but not Condition 3." ></td>
	<td class="line x" title="38:186	It is expected that the accurate frequency of any substring a is obtained subtracting the frequency by the frequency of the other substring ~ which is included in substring o~ *2." ></td>
	<td class="line x" title="39:186	Unfortunately, this does not satisfy Condition 3." ></td>
	<td class="line x" title="40:186	At the time when extracted substring list has been compiled, information regarding mutual inter-relationship between the extracted substrings within the original text has been lost rendering calculations impossible." ></td>
	<td class="line x" title="41:186	3." ></td>
	<td class="line x" title="42:186	Extraction of Uninterrupted Collocation 3.1 Invaliditafion of Extracted Substfings (1) Co-relations between Extracted Substrings In order to satisfy the requirement of Condition 3, consider the extraction of n-gram substring after extracting m -gram substring." ></td>
	<td class="line x" title="43:186	The problem arises when there is a certain overlap between them as shown in Fig.1." ></td>
	<td class="line x" title="44:186	The Case of Absorbed Relation (Case 1) can be classified into three sub-cases as shown, but regardless of which situation, the m-gram substfing is absorbed in the substring of n-gram and therefore there is no need to extract such a m-gram substring." ></td>
	<td class="line x" title="45:186	Thus, when extracting n-gram strings, there is a need to invalidate the related record of the SPT so that m-gram strings do not become involved in processes to follow." ></td>
	<td class="line x" title="46:186	Fn g,'am \] ., ram  ! , Coincided Beginning < case 1 1 > II1 gram L'2.L~)2.KZ:I \] I ll grain \] K~!Lg.!!!CZ\] Holy Included Coincided Ending < easel2> <casel3 > <Casel> Absorbed relation t~-n gram \]  l :--I11.~ r alll  : \[-rl gram \] preceded by m gram preceded by tl gram <case2-1 > < case22 > <Case2> Overlapped relation Fig." ></td>
	<td class="line x" title="47:186	2 Relationships between Extracted Substrings The Case of Partially Joint Relation (Case 2) can be further classified into two sub-cases." ></td>
	<td class="line x" title="48:186	But in either situation, the m-gram string and n-gram string merely overlapped and therefore they are need to be extracted separately." ></td>
	<td class="line x" title="49:186	(2) Necessity of Validity Check for String-words When one substring is extracted, in order not to extract the absorbed string from the same part of sotlrce text where the substring was already extracted (Case 1), related records of SPT need to be checked if the record is valid or not before extracting the next substring." ></td>
	<td class="line x" title="50:186	For example, the substring of 6 characters in the String -word 3 shown in Fig." ></td>
	<td class="line x" title="51:186	3 was extracted, the substring of String-words 3,4,5,,8 need to be set as invalid for the length equal or less than 6,5,4,.-.,1 characters from the beginning." ></td>
	<td class="line x" title="53:186	Source Address: 1 2 3 4 5 6 7 8 9 10 11    r6 gram Source Text: A B /C D E F G I\[ I J K.   Addres Invalid Range i /-~String-word 4 < 5 ch I D E F G H I K 5 4oh \[E F C ~-{.,i I K 6 < 3 ch IF G H i \] K 7 < 2 ch G HI l K 8 ~ 1 ch \[HI I K Fig.3 Example of Validity check 1 3.2 Extracting Algorithm Here, we propose an algorithm which satisfy Condition 3 as well as Conditions 1 and 2." ></td>
	<td class="line x" title="55:186	< Preparation > Fields of NSC (Number of Significant Characters) and RN (Record Number) are added to SIT-0 (Sorted Pointer Table) used for N-gram statistics." ></td>
	<td class="line x" title="56:186	<Algorithm (See Fig.4)> Procedure 1 thr_ough 3: Same as the N-gram statistics." ></td>
	<td class="line x" title="57:186	Procedure 4: Significant Character Determination The length of substrings to be extracted are decided from NMC and written in the NSC field of SPT0." ></td>
	<td class="line x" title="58:186	Procedure 5: Preparation of Augmented PT After sorting the SPT-0 in the original order, add a VP (Validity Flag) field to obtain an PT1." ></td>
	<td class="line x" title="59:186	* 1 Extraction is conducted based on the relation between the values of consecutive NMC." ></td>
	<td class="line x" title="60:186	Ddetails are in (Nagao and Mori 1994)." ></td>
	<td class="line x" title="61:186	* 2 Recently, combining the frequencies of related substring, calculation was conducted(Kita, etal 1993) to obtain the frequency which satisfy the Condition 3." ></td>
	<td class="line x" title="62:186	But accurate results cannot be obtained by this method." ></td>
	<td class="line x" title="63:186	575 Procedure 6: Validity Determination According to the method shown in 3.1(2), check the validity of the suhstring pointed by the records of the PT-1 in the order of the record number and write the results in the VF field." ></td>
	<td class="line x" title="64:186	Procedure 7: Resorting of PT-1 Re-sort the PT1 in the order of the values of SP fields to obtain a SPT1." ></td>
	<td class="line x" title="65:186	Procedure 8: Extraction and Tabulation By referring to the SPT-1, the strings to be extracted are determined and their frequencies are calculated." ></td>
	<td class="line x" title="66:186	An example of the algorithm is shown in Fig.4." ></td>
	<td class="line x" title="67:186	In this example, the types of substrings extracted by the conventional algorithm amounted to 24 with the total frequency of 72." ></td>
	<td class="line x" title="68:186	In contrast, in the method proposed in this paper, these numbers have reduced to 5 and 10 respectively." ></td>
	<td class="line x" title="69:186	4." ></td>
	<td class="line x" title="70:186	Extraction of Interrupted Collocation 4.1 Conditions for Extraction Here, let's consider combinations of 2 or more uninterrupted collocational substrings in different locations within a single sentence together with a method of determining the frequency of them." ></td>
	<td class="line x" title="71:186	In this case, boundary conditions of sentences and mutual relationship between the extracted substrings need to be considered." ></td>
	<td class="line x" title="72:186	(1) Boundary Conditions of Sentences When considering the collocation of substrings within a sentence, combinations of expressions spread over borders of sentences need to be excluded." ></td>
	<td class="line x" title="73:186	But when a single sentence includes other sentences, the extraction of the combinations in units of sentences poses complications." ></td>
	<td class="line x" title="74:186	To simplify matters, we first assume that the substrings which have any kinds of punctuation mark as a part of them are not extracted in the procedure of uninterrupted collocation extraction." ></td>
	<td class="line x" title="75:186	This can be easily performed by restraining the comparison procedure after finding a punctuation mark in Procedure 3." ></td>
	<td class="line x" title="76:186	Second, we assume that when a left quote character is found within a sentence, all characters are ignored until the right quote character forming a pair with the former character." ></td>
	<td class="line x" title="77:186	(2) Relationships between Extracted Substrings In extraction of interrupted collocations, substrings that are linked to or partially overlap one another are excluded from the scope of extraction." ></td>
	<td class="line x" title="78:186	Let's consider substrings a and ~0 which have been extracted from the same sentence." ></td>
	<td class="line x" title="79:186	The positioning would be one of the three cases shown in Fig.3." ></td>
	<td class="line x" title="80:186	Case (c) in which substring a and ~0 are separate from one another is a case of extracting interrupted collocations, and Cases (a) and Co) are not*3." ></td>
	<td class="line x" title="81:186	(3) Order of Substring Appearance In the case of extracting interrupted collocations, the order of appearance of substrings should be considered." ></td>
	<td class="line x" title="82:186	Hence, collocational substrings are extracted and counted taking notice of the order of the appearance of each substring." ></td>
	<td class="line x" title="83:186	Beginning \[ot  (a) Connected  \[  i End L-, T  Beginning ! V a\] End (b) Overlapped Beginning \[a ~\] \[' B  i End (c) Separated  a, B, 7' : Extracted Substfing Fig.5 Relations between Extracted two Substrings 4.2 Extraction Algorithm \[Preparation\] Sequential number is given to all of the substrings extracted in Chapter 3 in the order of extractions." ></td>
	<td class="line x" title="84:186	These Number are registered in the NES (Number of Extracted Substrings) field of the respective record in SPT1." ></td>
	<td class="line x" title="85:186	Procedure 9: Re-sorting the SPT-1 The SPT1 is sorted in the original order of the values of ST' fields." ></td>
	<td class="line x" title="86:186	Procedure 10: Numbering of the sentences SN(Sentence Number) field is added for entering the sentence number of original sentence to which one's record belongs." ></td>
	<td class="line x" title="87:186	Procedure !1: Table condensation The table obtained is condensed by procedures shown in the following to obtain a SPT-2'." ></td>
	<td class="line x" title="88:186	(1) All fields other than the four, Sentence Numbers, ESN, NSC and RN are deleted." ></td>
	<td class="line x" title="89:186	(2) All records with no values in the ArES field are deleted." ></td>
	<td class="line x" title="90:186	Procedure 12: Extraction of Interrupted Collocation Here, k is the number of substrings which compose interrupted colocational expressions." ></td>
	<td class="line x" title="91:186	Then, all of the combinations of k NESs for every sentence are written down into a file and sorted." ></td>
	<td class="line x" title="92:186	And the number of the same combination of NES are counted." ></td>
	<td class="line x" title="93:186	Thus, the substring list of interrupted collocations can be obtained." ></td>
	<td class="line x" title="94:186	If the sentence number is given to every combination list of NES, the sentences corresi~onding to the extracted interrupted collocation can easily be identified." ></td>
	<td class="line x" title="95:186	The lower part of Fig.4 shows the application of this method for k=2." ></td>
	<td class="line x" title="96:186	In this case, there are possibility of 25 combinations for 5 types of uninterrupted collocational substrings obtained by chapter 3." ></td>
	<td class="line x" title="97:186	Out of these combinations, 7 combinations were extracted as the combinations which collocate twice or more within the same sentence." ></td>
	<td class="line x" title="98:186	And the total frequency of these amount to 14 times." ></td>
	<td class="line x" title="99:186	5." ></td>
	<td class="line x" title="100:186	Experiments 5.1 Uninterrupted Collocational Substrings Applying the proposed method to the newspaper articles of Nikkei Industrial News for three months (8.92 million characters), uninterrupted and interrupted coUocational substrings were extracted." ></td>
	<td class="line x" title="101:186	In this experiments, XEROX *3 In the case of (a), there would be a combination of substrings which is regarded as a interrupted collocation." ></td>
	<td class="line x" title="102:186	However the frequency of such a pair is limitted to 1." ></td>
	<td class="line x" title="103:186	Then there is no need to consider." ></td>
	<td class="line x" title="104:186	576 'ancient' 'ancient' 'of' 'qtrange' 'ciike' . mukasi mukas~ no oKas\]na oKasl \[Source Text\] it-,'la~b it-,'D~bo) 2Sh~btx ~a~bo l~'~fl%b~ la:t~blat 2SD~bt, a~fa~bo <)~eaning) This is a story of cakes ill very old day." ></td>
	<td class="line x" title="105:186	The story of the cake is strange story." ></td>
	<td class="line x" title="106:186	tY\[O-O (Pointer Table) SP String-Words i 1 tb70, b?~ bl 2 ~ b~5'70~ boo : 3 b~;70, bo)~ ~ 4 t270, b ~s~,: 5 fl~boO~s70~bi Proc." ></td>
	<td class="line x" title="107:186	6 bo_)#D~L2: Sortb 7 o)~70, bt~s \[ 8 a3D, big ~s~),; 9 ~ bta)s~, bl :1 0 btg~s~O, bo : 1 1 ~70, bo ~i l:i12 g870~bo ~S70': 13 ~bo ts'D~bi be $S70~ boo : t5 o :~a70~bogL!ci Prec." ></td>
	<td class="line x" title="108:186	\]G *aTe, blit2: (;ounti t7: ~xbo.)l:tlg b\[ N~C 18 bo)llfg bI,-t: ~ 9 ff~lIt2 b~i:~s\[ 20 late bl~::t-S70',: Prec." ></td>
	<td class="line x" title="109:186	z 21 ta: blJ:fS70~ L I l)eter~ 22 b~;t~370~b/g: NSC 23 la~\]o, btg ~ 24 ~70, btats ~: 28 la~lita b o ', 29 Ilat; bo : 30 limbo 311 be 32 Lo." ></td>
	<td class="line x" title="110:186	! 'cake' 'of' :story' . ~s' 'qtrange' 'story' . okasi no I~allaSl na oKaslna ol~anasl O:Substring S1 W( la ta a5 tg a~ o (, ) J (@." ></td>
	<td class="line x" title="111:186	) I (." ></td>
	<td class="line x" title="112:186	@) I (." ></td>
	<td class="line x" title="113:186	) I Prec." ></td>
	<td class="line x" title="114:186	12 (, ) I Write ((5), _(D) I down (@, ) I< (, ~)) I -ZCI (, @)1 (@, )1 (, ) I case of (, ) I k:2 (, @)1 (, )1 (, @)1 (, @)1 (, )1 (@, 0))1 (@, @)1 (@, @)1 Prec." ></td>
	<td class="line x" title="115:186	12 \[ I Sorting SPT 0 (Sorted Pointer 'l'r, ble) I~T | V N N F S M R N SP C C 1 3 0 30 0 2 2 10 2 0 1 1 17 3 1 3 3 29!4 Prec." ></td>
	<td class="line x" title="116:186	5 1 3 3 85 Re-sorting 0 2 2 14 6 0 1 1 24 7 1 5 5 1 8 044 6 9 0 3 3 12 10 0 2 2 20 11 131 412 0 2 0 11 13 0 1 0 19 14 0 0 0 32 15 \[Prec." ></td>
	<td class="line x" title="117:186	6\] 1 3 3 3 i 6 Validity 1 3 2 9 17 Check(VF) 0 2 1 15:18 0 1 0 25 19 1 3 3 27 20 0 2 2 22 01 1 1622 1 1 1 26 2:3 1 5 3 224 0 4:2 7 25 0 31 13 26 0 21 21 27 1!o o 28 29 0 2 0 2330 0 1 1 18 31 0 0 0 31 32 (@, ) (), 0)) \[(,(b)\] (@, (0) (~, @) l~,~'~, IC@,O))I (@, @) ICCD, (4))1 _((--9, (2)) (~,~1 I(, )1 I(@~@)J J I Prec." ></td>
	<td class="line x" title="118:186	12 I Coasting Interrupted~ollocational Former Substring SN' 2 S N VN N N F F S M R N S C C 1(5) 13030 1 0 2 2 10 1 0 1 1 17 1  1 3 3 29 1  1 3 3 8 1 02214 1 0 1 1 2d 1@155 1 1 0 414 6 I 0 3~3' 12 1 0 22 20 1 ~ 1 3 1 4 1 02011 1 0 1 0 1 9 1 0 0 0 32 2(2) 1 3 3 3 2132 9 2 021 15 2 0 i 0 2@133 2 022 2 0 1 1,@))I 2 1 1 1 ~)1 2@i 5 3,)/ 2 o 42 @~ 2 o 3 1 )1 2 021 ~{~: @1 2 0 1 0 (,) 2 @ 1 3 o 2 020 2 0 1 1 2 000 Pairs of substring SPT 2* s N N N E S S P S C : Proc 11 1  3 1 Condense 1 @ 3 4 I  3 5 3 i6 2 ~);3 17 2@320 2 @ 5 <4) 3 I  : .< !  String~ 8P word Latter Substring a}ld Frequency i i ~i,'D, bi i 2 3 4 5: 6: 7 7 Prec." ></td>
	<td class="line x" title="119:186	g ~70' bi Sorting i0 b tg ~s i *sD, b~ 13 #/~L,o : Proc." ></td>
	<td class="line x" title="120:186	lO 14 (_,o ~S! Numbering i5 o ~d70~', for SN I6 $a~, bl 17 1/~ be.); 18 b o)l~t ! 25 19: 0)t~g: 27 20 latabl 22 2 t tall1: 16 22 blI~Si 26 2:3 I~t~)~: 2 2~ ~70~b! 7 25 ;)~b~: l 3 26 br~i 5 28 a~I~t~i 2 8 29 I;lt~b: 2 3 3o ta bo \[ 18 3t bo 3 1  i j _. <Sentence List>  kJ Sentence list for I./7 each pair of I interrupted / collocation_ _ C4)lta::D ' Ca)~i U:me ::::e SPT 1 ' rOposed Metliod!N-gram Method gram ''~'r eqnency \'ttrequency SU~strn'~ Subs!rhtg 5gram (1)~70, U~2:~2 ffs~o, b tS ~is 2 __~:~ z .L/_Z.L_: dgram,;O~ ba ~3 2 3gram ~70>U 2 ~D,U d = : ~  70, b'ta: 2 bg~a 2 @t~ts b 2 {arab 2 tYDSU 20D, b 2 Id : :,i, ~ ', ;,:, ~ 4 2 gram  ! :\] 70, b 6 < i = b  2 boo 2 }-: i }~ 7'g 7i3 2 I t<2 \[1 2 r :-:: l:t ?2." ></td>
	<td class="line x" title="121:186	2 77 _Z ~'J' 70' 2 5 :  ~ t~ 4 F  o~ 2 : I,'t: 3 10 Total 72 S P : Source Pointer a N : Record Number N M C : Number of matched Characters N S C : Number of Significant Characters V F : Validity flag N E S : Number of Extractc, d Substring S N : Sentence Number Fig." ></td>
	<td class="line x" title="122:186	4 Example of Uninterrupted and Interrupted Colh)cational Substring Extraction 577 ARGOSS 5270 (OS4.1.3) was used." ></td>
	<td class="line x" title="123:186	The memory capacity were 48 MB." ></td>
	<td class="line x" title="124:186	(1) Characteristics of Extracted Substring From the view point of the length and frequency, the number of extracted substrings are compared with those of the N-gram method and summarized in Table 1 and Table 2." ></td>
	<td class="line x" title="125:186	Some examples of extracted substrings are shown in Table 3." ></td>
	<td class="line x" title="126:186	And the examples of substrings with high frequency are also shown in Table 4." ></td>
	<td class="line x" title="127:186	Table 1." ></td>
	<td class="line x" title="128:186	Length and Number of Extracted Substrings t p Proposed Metlgod N-gram Statistics a: Extract b: Total c:Extmct d: Total (;ran Substring Frequency Substring Frequency 2 ~ 970,203 2.613,704 4,374,141 31,178,897 5~ 591,901 1,476,922 2,960,487 10,808,458 10~ 52,214 114,270 673,601 1,550,817 20~ 1,792 3,692 177,298 359,810 Ratio a/c b/d 22.2% 8.38% 20.0%13.7% 7." ></td>
	<td class="line x" title="129:186	75% 7." ></td>
	<td class="line x" title="130:186	37% 1.01% 1.03% Table2." ></td>
	<td class="line x" title="131:186	Frequency and Number of Extracted Substrings mp." ></td>
	<td class="line x" title="132:186	Proposed Method a:Extract b: Total Freq.\ Substring Frequency 2~ 970,203 2,613,704 5~ 67,321 551,441 10~ 12,351 217,934 20~ 2,288 92,804 50~ 285 37,850 100 ~ 76 24,167 200 ~ 20 16,771 N-gram Statistics c:Extmct d: Total Substring Frequency 4,377,087 39,588,291 882,217 31,288,701 372,291 28,050,199 169,375 25,871,964 62,991 22,209,875 30,316 19,961,961 14,363 17,759,432 Ratio a/c b/d 22.2 % B. 60% 7." ></td>
	<td class="line x" title="133:186	63% 1.76% 3." ></td>
	<td class="line x" title="134:186	32% 0." ></td>
	<td class="line x" title="135:186	78% 1.35% 0." ></td>
	<td class="line x" title="136:186	36% 0." ></td>
	<td class="line x" title="137:186	45% 0." ></td>
	<td class="line x" title="138:186	17% 0." ></td>
	<td class="line x" title="139:186	25Z 0." ></td>
	<td class="line x" title="140:186	12% 0." ></td>
	<td class="line x" title="141:186	14% 0." ></td>
	<td class="line x" title="142:186	07% From these results, the following observations can be obtained." ></td>
	<td class="line x" title="143:186	@ Compared with the N-gram method, most of fractional substring has been deleted, and the types m~d the number of the extracted substrings have highly reduced." ></td>
	<td class="line x" title="144:186	For example, in the extraction of substrings with the Table 3 Examples of Extracted Substrings (in the order gram Proposed Method b~Ct,~7~ (436), ~'J'~t~N~')g (277), C 0)?~), (158), (make it that ~ ), (EC), (for this purpose), 5 gram dJ'~r~'~ (141), ~1~ (141), ~/Y~-;Z'(133), (market share), (consider that ~ ), (motors), &~b\]c_(130), C~l<~,~b(126), c,~, (112), (enphasized that ~ ), (on the contrary), (subsequently ~ ), \[ 190,925 types Total 499,653 times \] (to be ~ ing), (second), 10 gram ~C&~Cf319~-9#2(19), 8 2~-~,/$>~'Y~(17), (it seems to do ~ ), (82 Japan shop), b-Cb~za ~b~ (16), 7 -2-'2 b >--)L H~N (14) (wonder if ~ do ~ ), (Washington 19 ), \[ 21,155 types Total 47,336 times\] length of 2 or more and the frequency of 2 times or more, the substring type reduced to 22.2 % and total frequency of them reduced to 8.38 %." ></td>
	<td class="line x" title="145:186	This effect increases as the increase of substring length." ></td>
	<td class="line x" title="146:186	In the case of substrings of 20 or more characters, these number reduced to 1%." ></td>
	<td class="line x" title="147:186	@ Most of substrings extracted by the proposed method forms expressions as syntactic or semantic units and there are few fractional substrings." ></td>
	<td class="line x" title="148:186	(2) Processing Time It took about 40 hours to make SPT-O*4." ></td>
	<td class="line x" title="149:186	But successive processes were performed very quickly (within one hour)." ></td>
	<td class="line x" title="150:186	5.2 Interrupted Collocational Substrings (1) Characteristics of Extracted Substrings Interrupted collocational substrings were extracted for every two substrings which had appeared 10 or more times in the source text*5." ></td>
	<td class="line x" title="151:186	The results are shown in Table 5." ></td>
	<td class="line x" title="152:186	And, examples of substrings with high frequency and with much characters in total are shown in Table 6." ></td>
	<td class="line x" title="153:186	Table 5 Number of Extracted Pairs of Substrings ~----___~_ Results Frequency -~--~ or more times _ 5 or more times 10 or more times 20 or more times No." ></td>
	<td class="line x" title="154:186	of Pair of Substrings 6,544 941 237 61 Total Frequency of Pairs 21,829 9,057 4,556 2,291 From these results, it can also be seen that expressions typical to newspapers have been extracted." ></td>
	<td class="line x" title="155:186	Thus, using the output results, we can easily obtain interrupted collocational expressions as well as uninterrupted ones." ></td>
	<td class="line x" title="156:186	of frequency) (cf)." ></td>
	<td class="line x" title="157:186	  :Fractional substfing N-gram Statistics 7~J:o~C(,~3(3710), ~Cb~'~, (2827), l<&~ &, (2753), (became to be ~ ), (be ~ ing but ~ ), (according to ~ ), \[<O (/~ ~ (2721), ~ ~2~2 b ~ ~ (2334), ;5 C & IVf~ (2286), (speaking about ~ ), (be done),   tv-~2o~b~ (2079), & lj~(~ (1997), ~ t t2@\]~ (1849), , (explain that ~ ), (57 fiscal year), \[ 748,172 types Total 3,793,077 times \] b It & C ;5 ~." ></td>
	<td class="line x" title="158:186	J: ~ &, (273), ~ 7'b IV." ></td>
	<td class="line x" title="159:186	b 7~< & C ~5 ~V. & (223), (from what ~ do),  , t~)J 5 7~ \[:." ></td>
	<td class="line x" title="160:186	b #_ & C 7) IV." ></td>
	<td class="line x" title="161:186	(223), t~ t:." ></td>
	<td class="line x" title="162:186	b ?c_ & C 7~ t< & ~ (222), ~V. b?." ></td>
	<td class="line x" title="163:186	A Y__ 7) tV.ck~ A (222),,~_~NItg~'}~:i~N~ (208), (according to that ~ was), (second research party), \[ 132,865 types Total 345,232 times \] Examples of Substrings (frequency > 200 ) Table 4 Examples of Substrings with High Frequency &b~9(586), &~\]t(512), &b~1,~5(436), ~#_(325) ~35~(324), ~{(315) (to say that), (said that), (set as), (again), (is that), (photogralihy), bT'j~b, (302), &~o 7~< (283), N~ (281), (~i~ (278), ~J'NJL, N# (277), bT)~b (274), N-f > b (269), (but), (said that), (Tokyo), (Price), (EC), (however), (Point), ~&~& (264), ~-}'~,~ (259), ~fc, (236), C~t2(220), ~_T<8) (204), ?<1~, (20I) \[ (one word), (sell term) (mere over) (this is) (for this sake), (yet) *4 Indirect sorting is conducted." ></td>
	<td class="line x" title="164:186	When this process is excuted within a memory by the computer which has a compare instruction with indirect adressing for arbitrary length of fields, sorting time will be extremely shortened." ></td>
	<td class="line x" title="165:186	*5 It is expected that when the frequency of each substring is small, the frequency of their cooccurence is further,small." ></td>
	<td class="line x" title="166:186	578 Table 6 Pairs of Substrings with High Frequency Collocations of Compound Nouns 4fllif~ ~ ~'~l#JlJl(257), qZ'~-g)I/~ :E-,~'--X'(ll7) (price ~ sell time), (General ~ Motors) (Summit ~ ) (EC ~ the European Community) 4 ~)> ~ ~.x,/~5'4 i~lgb'/-' (80) (lran ~ Japan Oil Industry) ~, ~& {Z~do~L3~)~&){~_~'cT~ (9), ~oJ~?tl,gc~,P~&)~#_ (9), (did ~ but said that), (In the answer to ~ said ~ ) (we talke that ~ ), (the contents is such that ~) Collocations of Sentence Patterns (moreover the minister said that), (doing ~ said ~) (the contents is ~ and so), (did ~ also about ~ ) b ~l~l~b ~ (5 bb ~(,1), ::)~ 0 ~:'~'o (4), ~t~ bx~l~ b~ (4) (as if ~ looks -~), (ilamely ~ is ~ ), (either ~ or (2) Processing Time In the case of interrupted collocational substring extraction, processing time depend highly on the number of components of substrings." ></td>
	<td class="line x" title="167:186	In this experiment, the turnaround time was 1 or 2 hours where components of collocations to be extracted was limited to the substrings with the frequency of 10 or more times." ></td>
	<td class="line x" title="168:186	6." ></td>
	<td class="line x" title="169:186	Conclusion The methods of automatically identifying and extracting uninterrupted and interrupted collocations from very large corpora has been proposed." ></td>
	<td class="line x" title="170:186	First, from the view point of collocational expression extraction, the problems of Nagao and Moffs algorithm for calculating arbitrary length of N-gram has been pointed out." ></td>
	<td class="line x" title="171:186	And, under the condition that fractional substrings are restrained to be extract, a new method of automatically extracting and tabulating all of the uninterrupted collocational substrings has been proposed." ></td>
	<td class="line x" title="172:186	Next, using these results, a method for automatically extracting interrupted collocational substrings has been proposed." ></td>
	<td class="line x" title="173:186	In this method, combinations of uninterrupted collocational substrings which collocate at different positions within a sentence are extracted and counted." ></td>
	<td class="line x" title="174:186	The method was applied to newspaper articles involving some 8.92 million characters." ></td>
	<td class="line x" title="175:186	The results for uninterrupted collocations were compared with that of Ngram statistics." ></td>
	<td class="line x" title="176:186	In the case of substring extraction with 2 or more characters, conventional method yielded substring of 4.4 millions types and the total frequency of them amount to 31.2 millions." ></td>
	<td class="line x" title="177:186	In contrast, the method proposed in this paper extracted 0.97 millions types of substrings and a total frequency of them has reduced to 2.6 millions." ></td>
	<td class="line x" title="178:186	In the case of interrupted collocational substring extraction, combining the substring with frequency of 10 times or more extracted by the first method, 6.5 thousand types of pairs of substrinks with the total frequency of 21.8 thousands were extracted." ></td>
	<td class="line x" title="179:186	From these results, it can be said that, viewed from the point of extraction of collocational expressions (as units of syntactic and semantic expressions), substrings obtained by conventional methods include a voluminous amount of fractional substrings." ></td>
	<td class="line x" title="180:186	In contrast, the method proposed in this paper reduces many of such fractional substrings and condensed into a group of substrings that can be regarded as units of expression." ></td>
	<td class="line x" title="181:186	As a result, it has been made possible to easily calculate interrupted collocations and together with phrase templates and other basic data regarding sentence structure." ></td>
	<td class="line x" title="182:186	This paper used Japanese character chains to examine the algorithm." ></td>
	<td class="line x" title="183:186	Yet this algorithm can be applied to arbitrary symbol chains." ></td>
	<td class="line x" title="184:186	Various types of applications are possible, such as word chains, syntactic element chains obtained from results of morphological analysis or semantic attribute chains which consist of each word being converted to semantic attributes." ></td>
	<td class="line x" title="185:186	As shown in this paper, applications for Japanese character chains still involve output of some amount of fractional stings." ></td>
	<td class="line x" title="186:186	But when applications to word chains or syntactic element strings are concerued, further restriction of unnecessary elements are anticipated." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C96-2100
Good Bigrams
Johansson, Christer;"></td>
	<td class="line x" title="1:136	Good Bigrams Christer Johansson Dept. of Linguistics at Lund University Helgonabacken 12 223 62 Lund, Sweden email: Christer.Johansson @ ling.lu, se Abstract A desired property of a measure of connective strength in bigrams is that the measure should be insensitive to corpus size." ></td>
	<td class="line x" title="2:136	This paper investigates the stability of three different measures over text genres and expansion of the corpus." ></td>
	<td class="line x" title="3:136	The measures are (1) the commonly used mutual information, (2) the difference in mutual information, and (3) raw occurrence." ></td>
	<td class="line x" title="4:136	Mutual information is further compared to using knowledge about genres to remove overlap between genres." ></td>
	<td class="line x" title="5:136	This last approach considers the difference between two products of the same process (human text-generation) constrained by different genres." ></td>
	<td class="line x" title="6:136	The cancellation of overlap seems to provide the most specific word pairs for each genre." ></td>
	<td class="line x" title="7:136	1 Introduction Statistical methods have been used to find cohesion between local items of language (such as phonemes, morphemes, or words)." ></td>
	<td class="line x" title="8:136	Early work (Stolz, 1965; Zellig, 1955) was inspired by the advances in information science (Shannon, 1951; Shannon & Weaver, 1963)." ></td>
	<td class="line x" title="9:136	The research benefited from the possibility to store huge amounts of information in computer systems, and the optimism could be overwhelming when the problems were simplified and thought mostly restricted by the size of the corpus." ></td>
	<td class="line x" title="10:136	In this paper the stability of some bigram measures will be investigated." ></td>
	<td class="line x" title="11:136	Bigrams are items (i.e. word forms) that occur frequently together in a specific order." ></td>
	<td class="line x" title="12:136	The meanings of bigrams are not discussed since there is no meaning outside of a context." ></td>
	<td class="line x" title="13:136	Co-occurrence is still interesting because bigrams occur non-randomly, sometimes to such an extent that we discern some structure beyond co-occurrence." ></td>
	<td class="line x" title="14:136	The reason why it should be so is probably that part of the use of words is reflected by the company that words keep." ></td>
	<td class="line x" title="15:136	Researchers (Church & Hanks, 1990; Kita & al. , 1994, inter al)." ></td>
	<td class="line x" title="16:136	have noted that mutual information tends to be insensitive to high fi'equency patterns, and unstable for low frequency patterns." ></td>
	<td class="line x" title="17:136	Johansson (1994) compared another measure, the difference in mutual information (Ag), of collocational strength with mutual information (g)." ></td>
	<td class="line x" title="18:136	That measure ranked high frequency bigrams higher than other bigrams if the order was consistent, whereas mutual information tended to pick out combinations of low frequency items." ></td>
	<td class="line x" title="19:136	Since low frequency items carry more specific information such bigrams give an illusion of semantic content." ></td>
	<td class="line x" title="20:136	It is usually this semantic illusion that we are interested in, but what says that 'of the' or 'in a' are worse bigrams than 'wooden spades' or 'various pretexts'." ></td>
	<td class="line x" title="21:136	Johansson proposed the test of finding some of the characters in the children's story 'Alice in Wonderland', and showed that a 'new' measure was to some degree 'better' than mutual information." ></td>
	<td class="line x" title="22:136	Unfortunately, some of that result was based on the fact that mutual information is very sensitive to low frequency items." ></td>
	<td class="line x" title="23:136	2 Definitions 2.1 Mutual information In the following p(x) will denote the observed probability as defined by p(x)=F(x)/N where F(x) is the frequency of occurrence of x, and N is the number of observed cases." ></td>
	<td class="line x" title="24:136	N is, in the calculations, equal to the corpus size in words." ></td>
	<td class="line x" title="25:136	Given this, the mutual information ratio (Church & Hanks, 1990; Church & Mercer, 1993; Steier & Belew, 1991) is expressed by Formula 1." ></td>
	<td class="line x" title="26:136	(Church & Hanks refer to this measure as the association ratio tbr technical reasons)." ></td>
	<td class="line x" title="27:136	592 \]./ --lOg2(RWT)))(~2 ) = (N *Occ(\[wl,w2l)~ Formula 1: The mutual information ratio The instability of statistical measures seems to be a problem in statistical bigralns." ></td>
	<td class="line x" title="28:136	Especially low frequency counts cause instability." ></td>
	<td class="line x" title="29:136	To avoid this use the rule of thumb that a bigram must occur more than four times (cf.Church & Hanks, 1990:p.24) to be considered as a candidate/br an interesting bigram." ></td>
	<td class="line x" title="31:136	2.2 The difference in mutual information: temporal co-occurrence A reasonable way of using the temporal orde~ ring in word pairs is to consider the opposite ordering of the word pair as negative evidence against the present order." ></td>
	<td class="line x" title="32:136	A reasonable measure would be to use the difference in mutual information between the two orderings, hereafter Ag." ></td>
	<td class="line x" title="33:136	The size of the corpus cancels out and Ag can be calculated by a ratio between frequencies." ></td>
	<td class="line x" title="34:136	This is intuitively correct for a comparison between apples and pears, i.e. you can say that apples (wl w2) occur twice as often as pears (w2 w l) in my fruit bowl (corpus)." ></td>
	<td class="line x" title="35:136	(p is the probability in the fixed corpus (fiN) which is different fi'om the probability in the language." ></td>
	<td class="line x" title="36:136	It is impossible to have a fixed corpus that equals the language since language does not have a fixed number of words or word patterns)." ></td>
	<td class="line x" title="37:136	2.2.1 Handling zero negative evidence In the case that the reversed ordering of a word pair has not been observed in the corpus, the measure becomes undefined." ></td>
	<td class="line x" title="38:136	To relieve this the frequency t is multiplied by a constant (10), and the frequency of the reversed ordering is set to 1." ></td>
	<td class="line x" title="39:136	Subtracting 9 from that value does not add anything to the measure for a single occurrence (log(10-9)=0)." ></td>
	<td class="line x" title="40:136	Other ways of handling zero-frequencies are evaluated in (Gale & Church, 1994), e.g. the Good-Turing method." ></td>
	<td class="line x" title="41:136	Relative frequencies of non-observed word pairs are hard to estimate." ></td>
	<td class="line x" title="42:136	For example, the frequencies of frequencies (X) and frequency (Y) used in the 1 1 will use 'frequency' as equivalent to 'occurrence' in the sample corpus." ></td>
	<td class="line x" title="43:136	Good-Turing method are linearly dependent in a log-log scale, i.e., there is an infinite frequency of non-observed items (which is another way of saying that we cannot expect the unexpected)." ></td>
	<td class="line x" title="44:136	A\].l =~ \] (Occ(\[wl,w2\])~ og 2 :it occ(\[w2,w \])>o log 2 (1 0* Oc'c(\[ Wl, W 2 \]) 9) 'if OCC(\[W2,Wl\]): 0 Formula 2: Handling zero frequencies 3 Illustration The difference between the two measures are perhaps best illustrated with some concrete examples." ></td>
	<td class="line x" title="45:136	In a previous paper (Johansson, 1.994) 'Alice's adventures in Wonderhmd' (AIW) was used as an experimental corpus to compare phrase finding for ~t, and a new measure -A~t. A critique against that corpus is that the corpus is very small." ></td>
	<td class="line x" title="46:136	'Through the Looking Glass' and 'The Hunting of the Snark' extend that corpus to about 63 000 words of which 26 831 occurred more than 4 times." ></td>
	<td class="line x" title="47:136	With the criterion that an interesting bigram occurs more than 4 times 1970 bigram candidates were found in this larger corpus." ></td>
	<td class="line x" title="48:136	Effect of Effect of della nm,.~, -,,,, 215 1883 34 7 48 202 33 204 29 174 160 29 47 136 28 1400 -9 -519 931 932!" ></td>
	<td class="line x" title="49:136	190 bigraln cheshire cat hui~ lookingzlass march hare mock tnrtle red king reA queen the dormouse white king white knight white queen white rabbit \[n the previous table the effect is measured by the number of steps a bigram is moved up compared to a sorted frequency list." ></td>
	<td class="line x" title="50:136	The effect of mutual information under these conditions is higher than the proposed measure for finding most characters in A1W, except for some names defined by definite article + noun, and common adjective + noun." ></td>
	<td class="line x" title="51:136	593 4 Material 6 Results In the rest of this paper, the corpus is the SUSANNE corpus (Sampson, 1994)." ></td>
	<td class="line x" title="52:136	This corpus consists of an extensively tagged and annotated subset from the Brown Corpus of American English." ></td>
	<td class="line x" title="53:136	The corpus is fairly small, but provides information on grammatical roles on the word and phrase level." ></td>
	<td class="line x" title="54:136	This makes the SUSANNE corpus suitable for further research." ></td>
	<td class="line x" title="55:136	The SUSANNE corpus is divided into 4 (approximately equally large) genre subcategones: 'A: press reportage G: belles lettres, biography, memoirs J: learned (mainly scientific and technical) writing N: adventure and Western fiction' (Sampson, 1994:p. 1.74) Each genre has approximately 20,000 unique word pairs 2." ></td>
	<td class="line x" title="56:136	The four genres will be used as one factor in the comparison between different measures." ></td>
	<td class="line x" title="57:136	The question is whether the genre interacts with the ability of the different measures to discover bigrams." ></td>
	<td class="line x" title="58:136	In category A 439 unique bigrams (occurring more than 4 times) were found, in G 486, in J 598, N 620, and 2573 for the used corpus 3." ></td>
	<td class="line x" title="59:136	5 Method The highest ranking bigralns according to the measure are sampled at 5 different levels: the 10, 50, 100, 200 and 400 top collocations." ></td>
	<td class="line x" title="60:136	Samples are sorted and compared for overlap by the UNIX command 'comm -12 SAMPLE1 SAMPLE2 I wc -1', and the percentage of overlap was calculated from the size of the sample." ></td>
	<td class="line x" title="61:136	Stability of bigrams was tested by three different overlaps." ></td>
	<td class="line x" title="62:136	1) The overlap between samples from genres, and samples for the entire corpus for the same measure." ></td>
	<td class="line x" title="63:136	2) The overlap between different measures at the five different levels for the different genres and the entire corpus." ></td>
	<td class="line x" title="64:136	3) The overlap between different genres." ></td>
	<td class="line x" title="65:136	2(A 21198 unique / 29969 total / 5332 unique words; G 22248 / 31006 / 6048; J 19039 / 29484 / 4676; N 20902 / 31959 / 4876; all 74126 / 12242\[ / 13458) 3The last small part of each genre was excluded fi'om the start for future purposes." ></td>
	<td class="line x" title="66:136	6.1 Mutual Information The average overlap between genres and the corpus showed that the J sample was much more stabile than the other genres 4." ></td>
	<td class="line x" title="67:136	The J genre would be the genre that information retrieval applications would be most interested in." ></td>
	<td class="line x" title="68:136	The ranking of the genres according to the stability of the overlap is: JANG." ></td>
	<td class="line x" title="69:136	The highest collocations are most stabile for J, where the other genres show less specificity (i.e. equal or growing percentages as the overlap grows)." ></td>
	<td class="line x" title="70:136	10 150 \[100 1200 \]400 \[mean 20 22 30 27.5 21.5 24.2 0 6 10 14.5 16.7 9.4 60 62 48 36.5 31 47.5 !10 6 7 15 22 12.0 A G J N 6.2 Delta Mutual Information Delta mutual information shows little effect of genre, and sample size." ></td>
	<td class="line x" title="71:136	Growing sample size predicts less overlap." ></td>
	<td class="line x" title="72:136	The ranking of genres is: GANJ." ></td>
	<td class="line x" title="73:136	Delta mutual information seems to rank the less specific genres high." ></td>
	<td class="line x" title="74:136	10 150 I lO0 1200 1400 Imear 70 64 53 47.5 44.2 55.7 60 58 54 58.5 51.5 56.4 60 54 48 43 39.2 48.8 50 52 49 51 45.5 49.5 A G J N A factorial ANOVA on measure and genre shows that there is a significant effect (p<0.001) of measure (Ag or g), genre and interaction between measures." ></td>
	<td class="line x" title="75:136	F(measure, 1df)=136.2, F(genre, 3df)=9.8, F(measure, genre, 1, 3)=15.4, p <0.001." ></td>
	<td class="line x" title="76:136	These two measures are significantly different." ></td>
	<td class="line x" title="77:136	6.3 Occurrence The results for the samples are similar to a m The overlap is generally higher for occurrence than Ag, but the ranking of genres is the same: GANJ." ></td>
	<td class="line x" title="78:136	An ANOVA on measure (Ag and occurrence) and genre show less significant effect on measure, and no significant effect of genre, or interaction (these measures behave in the same direction)." ></td>
	<td class="line x" title="79:136	4In preliminary investigations the J genre was the least stabile genre for mutual information." ></td>
	<td class="line x" title="80:136	This was 'corrected' by the demand that candidate bigrams should occur more than 4 times." ></td>
	<td class="line x" title="81:136	594 10 150 I i00 1200 1400 Imeanl 60 70 65 60.5 51 61.3 60 70 69 65.5 61 65.1 70 62 53 48.5 43.5 55.4 70 64 57 54.5 54.2 59.9 F(measure, ldf) = ll.l p<0.02, F(genre, 3df) = 2.7, p>0.05, F(measure, genre, 1, 3) = 0.218, p>0.8." ></td>
	<td class="line x" title="82:136	Occurrence is significantly more stabile than the other measure, but there is only a small difference of genres (occurrence and Ag react in a similar way to genre -i.e. on high occurrence)." ></td>
	<td class="line x" title="83:136	6.4 Comparison between measures The overlap between measures is calculated for all combinations of measures." ></td>
	<td class="line x" title="84:136	At the higher levels a high overlap can be expected since there is little possibility to fall out (e.g. in A 400 out of 439 is 91% of the sample)." ></td>
	<td class="line x" title="85:136	The results from this test indicate that the overlap between D (Ag) and F (occurrence) is significantly and consistently higher than between the other combinations (especially for the entire corpus)." ></td>
	<td class="line x" title="86:136	10 50 100 200 400 Genre Test mean overlap 0 6 22 44.5 93.2 A(439) 0 6 16 37.5 91.0 A 90 64 74 78.0 91.2 A 0 18 23 45.5 86.0 ~__ 0 14 20 43.0 82.0 G 80 76 78 77.5 84.0 G 0 8 13 34.0 72.2 J(598) 0 4 1 l 28.5 64.0 J 60 84 78 72.5 75.5 J 0 8 22 33.5 70.5 N(620) 0 6 20 28.0 63.7 N 40 68 71 67.0 72.5 N 0 0 1 7.0 15.7 a11(2573) 0 0 1 4.0 13.0 all 40 54 58 58.0 59.5 all M=D 33.1 M=F 30.1 D=F 79.4 M=D 34.5 IM=F 31.8 D=F 79." ></td>
	<td class="line x" title="87:136	I M=D 25.4 M:FI 21.5 D=F 74.0 M=D 26.8 M=F 23.5 D:F 63.7 M=D 4.7 M=F 3.6 D=-F 53.9 6.5 Overlap between genres To estimate the overlap of the genres the number of common bigrams between two genres were found and compared to the size of the smallest genre." ></td>
	<td class="line x" title="88:136	The results indicate an average overlap between the genres of 10%." ></td>
	<td class="line x" title="89:136	Overlap A G N of genres (% of smallest genre) A G J N --i 11.0 -9.4 11.0 10.0 12.0 7.5 6.6 Reduction of the bigrams The bigrams that are rated high by the measures (especially mutual information) are mixed between two different types of bigrams: (1) bigrams with high internal cohesion between low frequency items that may be associated with a specific interpretation (e.g. 'carbon tetrachloride' or 'cheshire cat'), (2) bigrams with high internal cohesion with usually high frequency of both items that may be associated with a 'syntactical' interpretation (e.g. 'in the')." ></td>
	<td class="line x" title="90:136	To separate type l from type 2 some information about the overlap of genres might be used." ></td>
	<td class="line x" title="91:136	The type 2 bigrams are typically found in most genres, whereas type 1 bigrams are specific to a text." ></td>
	<td class="line x" title="92:136	The results above indicate that we can use the genres with least overlap to filter out common bigrams (i.e. A use J, G use J, J use N, N use J)." ></td>
	<td class="line x" title="93:136	In the following table the effect of the genre (column 2) is shown by the number of 'surviving' bigrams from the candidate bigrams (column 1)." ></td>
	<td class="line x" title="94:136	The third column shows the effect of removing the bigrams that occur (more than 4 times) in both directions after common bigrams have been removed (first parenthesis shows actual removed, second shows those that would have been removed (i.e. those bigrams with both orderings in the candidate set)." ></td>
	<td class="line x" title="95:136	The fourth column shows the effect of removing bigrams that contains words that occur more than 4 times in the rest of the corpus (i.e. in A G N for J) after the bigrams have been formed." ></td>
	<td class="line x" title="96:136	The reason for filtering after forming bigrams is that words that are filtered out later work as place holders, and prevent some bigrams to form." ></td>
	<td class="line x" title="97:136	The reduction is most notable for removing bigrams that contain common words between genres: genre G and N contain few good candidates of collocations type 1." ></td>
	<td class="line x" title="98:136	Cand." ></td>
	<td class="line x" title="99:136	Genre Word order filter Freq." ></td>
	<td class="line x" title="100:136	words 439 216 179 (-63) (-80) 12 486 159 119 (-40) (-127) 1 .~8-\] 355 277 (-78)(-131) 37 620 /395 291 (-104)(-159) 0 A G J N 595 The following bigrams survived the harshest condition of removing bigrams containing words of other genres." ></td>
	<td class="line x" title="101:136	(Genre J, later ordered by mutual information)." ></td>
	<td class="line x" title="102:136	Some good candidates were (of course) removed, e.g. 'black body', 'per cent', 'united states'." ></td>
	<td class="line x" title="103:136	12.2 poynting robertson 9.1 pulmonary vein 11.8 indirect coombs 8.9 active agent 11.6 burning arcs 8.9 bronchial artery l 1.4 anionic binding 8.9 liquid phase 11.1 binding capacity 8.8 pulmonary artery 11.0 starting buffer 8.6 anode holder 10.7 antenna beam 8.3 solar radiation 10.6 wave lengths 8.2 reaction tubes 10.3 wave length 8.0 quadric surface 10.1 multiple secant 7.8 brightness temperature 10.0 carbon tetrachloride 7.8 mass flow 9.9 bronchial arteries 7.7 gas phase 9.9 heat transfer 7.7 surface cleaning 9.9 ideal gas 7.1 reaction cells 9.8 agglutinin activity 7.1 surface active 9.5 hydrogen atoms 6.7 artery puhnonary 9.4 multiple secants 5.0 anode surface 9.3 antibody activity 4.7 surface temperature 9.1 particle size In the A genre (News) the following 12 bigrams survived: 12.5 anne m:undel 12.0 rhode island 10.0 grand jury 9.9 rule charter 9.2 austin texas 8.9 sunday sales 8.9 sales tax 8.9 payroll tax 8.2 fulton county 8.0 lbotball league 7.5 kennedy administration 7.3 tax bill Genres G and N contain few candidates for collocations (among the 'best' ones in N were 'gray eyes', 'picked up', 'help me' and 'stared at' which are quite telling about the prototypical western story: 'The gray eyes stared at the villain who picked up his knife, while the girl cried 'help me''." ></td>
	<td class="line x" title="104:136	7 Other approaches The temporal dependencies of an ordered collocation \[wordl, word2\] has been seen as a problem since the theory of mutual information assumes the frequencies of word pairs to be symmetric (i.e. , f(\[wl, w2\]) and f(\[w2, w 1\]) to be equal)." ></td>
	<td class="line x" title="105:136	Delta mutual information relies on this difference in temporal ordering." ></td>
	<td class="line x" title="106:136	'\[\] f(x, y) encodes linear precedence." ></td>
	<td class="line x" title="107:136	\[\] Although we could fix this problem by redefining f(x, y) to be symmetric (by averaging the matrix with its transpose), we have decided not to do so, since order information appears to be very interesting'." ></td>
	<td class="line x" title="108:136	(Church & Hanks, 1990:p.24) Merkel, Nilsson, & Ahrenberg (1994) have constructed a system that uses frequency of recurrent segments to determine long phrases." ></td>
	<td class="line x" title="109:136	In their approach they have to chunk the text into contiguous segments." ></td>
	<td class="line x" title="110:136	Significant frequency counts are achieved through the use of a very large corpus, and/or a corpus specialised for a specific task." ></td>
	<td class="line x" title="111:136	They report that it was possible for them to divide a large corpus into smaller sub-sections with little loss." ></td>
	<td class="line oc" title="112:136	Smadja (1993)finds significant bigrams using an estimate of z-score (deviation from an expected mean)." ></td>
	<td class="line n" title="113:136	Smadja's method seems to require very large corpora, since the method needs to estimate a reliable measure of the variance of the frequencies with which words co-occur." ></td>
	<td class="line n" title="114:136	This makes the method dependent on the corpus size." ></td>
	<td class="line o" title="115:136	Smadja reports the use of a corpus of size 10 million words." ></td>
	<td class="line n" title="116:136	'More precisely, the statistical methods we use do not seem to be effective on low frequency words (fewer than 100 occurrences)'." ></td>
	<td class="line oc" title="117:136	(Smadja, 1993:p.168) Kita & al." ></td>
	<td class="line x" title="118:136	(1994) proposed another measure of collocational strength that was based on the notion of a reduction in 'processing cost' if a frequent chunk of text can be processed as one chunk." ></td>
	<td class="line x" title="119:136	Cost reduction tended to extract conventional 'predicate phrase patterns', e.g., 'is that so' and 'thank you very much'." ></td>
	<td class="line x" title="120:136	Steier & Belew (1991) discuss the 'exporting' of phrases into a general vocabulary, where a word pair with high mutual information within a topic tends to have lower mutual information within the collection, and vice versa." ></td>
	<td class="line x" title="121:136	They relate a higher mutual information within a topic than in the collection to a lower value of discrimination." ></td>
	<td class="line x" title="122:136	Church & Gale (1995) have found it useful to compare the distribution of terms across documents." ></td>
	<td class="line x" title="123:136	They showed that a distribution different from what could be expected by a (random) Poisson process indicates interesting terms." ></td>
	<td class="line x" title="124:136	This approach is similar to the use of one genre to find interesting items in 596 another." ></td>
	<td class="line x" title="125:136	However, removal of the overlap needs some knowledge about the genres -apart from checking explicitly for a genre with least overlap." ></td>
	<td class="line x" title="126:136	Cancelling overlap has the advantage that it can cancel out similar underlying causes, while it exaggerates the underlying causes that differ between genres." ></td>
	<td class="line x" title="127:136	Some questions remain: at which level should overlap be formed?" ></td>
	<td class="line x" title="128:136	overlap in words or in bigrams; how many repetitions does it take for a word or bigram to 'belong' to a genre?" ></td>
	<td class="line x" title="129:136	8 Conclusion The question is 'what is gained by using a measure?'." ></td>
	<td class="line x" title="130:136	Mutual infornmtion tends to find combinations of words that are highly co-ordinated with each other, but these bigrams show both interesting bigrams (e.g. 'cheshire cat') and conventional (and uninteresting for keywords) bigrams (e.g. 'in a')." ></td>
	<td class="line x" title="131:136	The stability of interesting bigrams is improved by demanding candidate bigrams to occur more than a fixed number of times." ></td>
	<td class="line x" title="132:136	In this paper it has been shown that genre matters, and can be used to extract items that differ between genres." ></td>
	<td class="line x" title="133:136	Instead of balancing one big corpus, the analysis of one corpus might benefit from finding out how it is different from another corpus." ></td>
	<td class="line x" title="134:136	The bigrams that were formed by using different genres as filters showed interesting characteristics." ></td>
	<td class="line x" title="135:136	However, if we are to deal with larger amounts of data it might be unrealistic to compare differences directly between two large genres without the exclusion of terms that occur by chance." ></td>
	<td class="line x" title="136:136	The method that could be recommended from the results presented in this study is to triangulate a sample by the difference to other gcnres that we have some recta-knowledge about (i.e. we know that Western Fiction and Scientific Writing, at least on the surface, have little vocabulary in common)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W96-0103
Hierarchical Clustering Of Words And Application To NLP Tasks
Ushioda, Akira;"></td>
	<td class="line x" title="1:316	Hierarchical Clustering of Words and Application to NLP Tasks Akira Ushioda* Fujitsu Laboratories Ltd. Kawasaki, Japan email: ushioda@flab, fuj su." ></td>
	<td class="line x" title="2:316	co. jp Abstract This paper describes a data-driven method for hierarchical clustering of words and clustering of multiword compounds." ></td>
	<td class="line x" title="3:316	A large vocabulary of English words (70,000 words) is clustered bottom-up, with respect to corpora ranging in size from 5 million to 50 million words, using mutual information as an objective function." ></td>
	<td class="line x" title="4:316	The resulting hierarchical clusters of words are then naturally transformed to a bit-string representation of (i.e. word bits for) all the words in the vocabulary." ></td>
	<td class="line x" title="5:316	Evaluation of the word bits is carried out through the measurement of the error rate of the ATR Decision-Tree Part-Of-Speech Tagger." ></td>
	<td class="line x" title="6:316	The same clustering technique is then applied to the classification of multiword compounds." ></td>
	<td class="line x" title="7:316	In order to avoid the explosion of the number of compounds to be handled, compounds in a small subclass are bundled and treated as a single compound." ></td>
	<td class="line x" title="8:316	Another merit of this approach is that we can avoid the data sparseness problem which is ubiquitous in corpus statistics." ></td>
	<td class="line x" title="9:316	The quality of one of the obtained compound classes is examined and compared to a conventional approach." ></td>
	<td class="line x" title="10:316	1 Introduction One of the fundamental issues concerning corpus-based NLP is that we can never expect to know from the training data all the necessary quantitative information for the words that might occur in the test data if the vocabulary is large enough to cope with a real world domain." ></td>
	<td class="line x" title="11:316	In view of the effectiveness of class-based n-gram language models against the data sparseness problem (Kneser and Ney 1993, Ueberla 1995), it is expected that classes of words are also useful for NLP tasks in such a way that statistics on classes are used whenever statistics on individual words are unavailable or unreliable." ></td>
	<td class="line x" title="12:316	An ideal type of clusters for NLP is the one which guarantees mutual substitutability, in terms of both syntactic and semantic soundness, among words in the same class (Harris 1951, Brill and Marcus 1992)." ></td>
	<td class="line x" title="13:316	Take, for example, the following sentences." ></td>
	<td class="line x" title="14:316	(a) He went to the house by car." ></td>
	<td class="line x" title="15:316	(b) He went to the apartment by bus." ></td>
	<td class="line x" title="16:316	(c) He went to the ? by ?." ></td>
	<td class="line x" title="17:316	(d) He went to the house by the sea." ></td>
	<td class="line x" title="18:316	Suppose that we want to parse sentences using a statistical parser and that sentences (a) and (b) appeared in the training and test data, respectively." ></td>
	<td class="line x" title="19:316	Since (a) is in the training data, we know that the prepositional phrase by car is attached to the main verb went, not to the noun phrase the house." ></td>
	<td class="line x" title="20:316	Sentence (b) is quite similar to (a) in meaning, and identical to (a) in sentence structure." ></td>
	<td class="line x" title="21:316	Now if the words apartment and bus are unknown to the parsing system *A part of this work is done when the author was at ATR Interpreting Telecommunications Research Laboratories, Kyoto, Japan." ></td>
	<td class="line x" title="22:316	28 (i.e. never occurred in the training data), then sentence (b) must look to the system very much like (c), and it will be very hard for the parsing system to tell the difference in sentence structure between (c) and (d)." ></td>
	<td class="line x" title="23:316	However, if the system has access to a predefined set of classes of words, and if car and bus are in the same class, and house and apartme.nt are in another class, it will not be hard for the system to detect the similarity between (a) and (b) and assign the correct sentence structure to (b) without confusing it with (d)." ></td>
	<td class="line x" title="24:316	The same argument holds for an example-based machine translation system." ></td>
	<td class="line x" title="25:316	In that case, an appropriate translation of (b) is expected to be derived with an example translation of (a) if the system has an access to the classes of words." ></td>
	<td class="line x" title="26:316	Therefore, it is desirable that we build clustering of the vocabulary in terms of mutual substitutability." ></td>
	<td class="line x" title="27:316	Furthermore, clustering is much more useful if the clusters are of variable granularity." ></td>
	<td class="line x" title="28:316	Suppose, for example, that we have two sets of clusters, one is finer than the other, and that word-1 and word-2 are in different finer classes." ></td>
	<td class="line x" title="29:316	With finer clusters alone, the amount of information on the association of the two words that the system can obtain from the clusters is minimal." ></td>
	<td class="line x" title="30:316	However, if the system has a capability of falling back and checking if they belong to the same coarser class, and if that is the case, then the system can take advantage of the class information for the two words." ></td>
	<td class="line x" title="31:316	When we extend this notion of two-level word clustering to many levels, we will have a tree representation of all the words in the vocabulary in which the root node represents the whole vocabulary and a leaf node represents a word in the vocabulary." ></td>
	<td class="line x" title="32:316	Also, any set of nodes in the tree constitutes a partition (or clustering) of the vocabulary if there exists one and only one node in the set along the path from the root node to each leaf node." ></td>
	<td class="line x" title="33:316	In the following sections, we will first describe a method of creating a binary tree representation of the vocabulary and present results of evaluating and comparing the quality of the clusters obtained from texts of very different sizes." ></td>
	<td class="line x" title="34:316	Then we will extend the paradigm of clustering from word-based clustering to compound-based clustering." ></td>
	<td class="line x" title="35:316	In the above examples we looked only at the mutual substitutability of words; however, a lot of information can also be gained if we look at the substitutability of word compounds for either other word compounds or single words." ></td>
	<td class="line x" title="36:316	We will introduce the notion of compound-classes, propose a method for constructing them, and present results of our approach." ></td>
	<td class="line x" title="37:316	2 Hierarchical Clustering of Words Several algorithms have been proposed for automatically clustering words based on a large corpus (Jardino and Adda 91, Brown et al. 1992, Kneser and Ney 1993, Martin et al. 1995, Ueberla 1995)." ></td>
	<td class="line x" title="38:316	They are classified into two types." ></td>
	<td class="line x" title="39:316	One type is based on shuffling words from class to class starting from some initial set of classes." ></td>
	<td class="line x" title="40:316	The other type repeats merging classes starting from a set of singleton classes (which contain only one word)." ></td>
	<td class="line x" title="41:316	Both types are driven by some objective function, in most cases by perplexity or average mutual information." ></td>
	<td class="line x" title="42:316	The merit of the second type for the purpose of constructing hierarchical clustering is that we can easily convert the history of the merging process to a tree-structured representation of the vocabulary." ></td>
	<td class="line x" title="43:316	On the other hand, the second type is prone to being trapped by a local minimum." ></td>
	<td class="line x" title="44:316	The first type is more robust to the local minimum problem, but the quality of classes greatly depends on the initial set of classes, and finding an initial set of good quality is itself a very difficult problem." ></td>
	<td class="line x" title="45:316	Moreover, the first approach only provides a means of partitioning the vocabulary and it doesn't provide a way of constructing a hierarchical clustering of words." ></td>
	<td class="line x" title="46:316	In this paper we adopt the merging approach and propose an improved method of constructing hierarchical clustering." ></td>
	<td class="line x" title="47:316	An attempt is also made to combine the two types of clustering and some results will be shown." ></td>
	<td class="line x" title="48:316	The combination is realized by the construction of clusters using the merging method followed by the reshuffling of words from class to class." ></td>
	<td class="line x" title="49:316	Our word bits construction algorithm (Ushioda 1996) is a modification and an extension 29 of the mutual information (MI) clustering algorithm proposed by Brown et al.(1992)." ></td>
	<td class="line x" title="51:316	The reader is referred to (Ushioda 1996) and (Brown et al. 1992) for details of MI clustering, but we will first briefly summarize the MI clustering and then describe our hierarchical clustering algorithm." ></td>
	<td class="line x" title="52:316	2.1 Mutual Information Clustering Algorithm Suppose we have a text of T words, a vocabulary of V words, and a partition 7r of the vocabulary which is a function from the vocabulary V to the set C of classes of words in the vocabulary." ></td>
	<td class="line x" title="53:316	Brown et al. showed that the likelihood L(Tr) of a bigram class model generating the text is given by the following formula." ></td>
	<td class="line x" title="54:316	L(r) --H -4I (1) Here H is the entropy of the 1-gram word distribution, and I is the average mutual information (AMI) of adjacent classes in the text and is given by equation 2." ></td>
	<td class="line x" title="55:316	F (elc2) I= ~ Pr(clc2)log Pr(cl)Pr(c2) (2) Cl ~C2 Since H is independent of r, the partition that maximizes the AMI also maximizes the likelihood L(r) of the text." ></td>
	<td class="line x" title="56:316	Therefore, we can use the AMI as an objective function for the construction of classes of words." ></td>
	<td class="line x" title="57:316	The mutual information clustering method employs a bottum-up merging procedure." ></td>
	<td class="line x" title="58:316	In the initial stage, each word is assigned to its own distinct class." ></td>
	<td class="line x" title="59:316	We then merge two classes if the merging of them induces minimum AMI reduction among all pairs of classes, and we repeat the merging step until the number of the classes is reduced to the predefined number C. Time complexity of this basic algorithm is O(V s) when implemented straightforwardly, but it can be reduced to O(V 3) by storing the result of all the trial merges at the previous merging step." ></td>
	<td class="line x" title="60:316	Even with the O(V 3) algorithm, however, the calculation is not practical for a large vocabulary of order 104 or higher." ></td>
	<td class="line x" title="61:316	Brown et al. proposed the following method, which we also adopted." ></td>
	<td class="line x" title="62:316	We first make V singleton classes out of the V words in the vocabulary and arrange the classes in the descending order of frequency, then define the merging region as the first C + 1 positions in the sequence of classes." ></td>
	<td class="line x" title="63:316	So initially the C + 1 most frequent words are in the merging region." ></td>
	<td class="line x" title="64:316	Then do the following." ></td>
	<td class="line x" title="65:316	I. Merge the pair of classes in the merging region merging of which induces minimum AMI reduction among all the pairs in the merging region." ></td>
	<td class="line x" title="66:316	2." ></td>
	<td class="line x" title="67:316	Put the class in the (C + 2) nd position into the merging region and shift each class after the (C + 2) nd position to its left." ></td>
	<td class="line x" title="68:316	3." ></td>
	<td class="line x" title="69:316	Repeat I. and 2." ></td>
	<td class="line x" title="70:316	until C classes remain." ></td>
	<td class="line x" title="71:316	With this algorithm, the time complexity becomes O(C2V) which is practical for a workstation with V in the order of 100,00O and C up to 1,000." ></td>
	<td class="line x" title="72:316	2.2 Word Bits Construction Algorithm The simplest way to construct a tree-structured representation of words is to construct a dendrogram as a byproduct of the merging process, that is, to keep track of the order of merging and make a binary tree out of the record." ></td>
	<td class="line x" title="73:316	A simple example with a five word vocabulary is shown in Figure 1." ></td>
	<td class="line x" title="74:316	If we apply this method to the above O(C2V) algorithm straightforwardly, however, we obtain for each class an extremely unbalanced, almost left branching subtree." ></td>
	<td class="line x" title="75:316	The 30 Merging History: Merge(A, B -> A) Merge(C, D -> C) Merge(C, E -> C) Merge(A, C -> A) Merge(X,Y->Z) reads 'merge X and Y and name the new class as Z' Dendrogram f F Figure 1: Dendrogram Construction reason is that after classes in the merging region are grown to a certain size, it is much less expensive, in terms of AMI, to merge a singleton class with lower frequency into a higher frequency class than merging two higher frequency classes with substantial sizes." ></td>
	<td class="line x" title="76:316	A new approach we adopted incorporates the following steps." ></td>
	<td class="line x" title="77:316	1." ></td>
	<td class="line x" title="78:316	MI-clustering: Make C classes using the mutual information clustering algorithm with the merging region constraint mentioned in (2.1)." ></td>
	<td class="line x" title="79:316	2." ></td>
	<td class="line x" title="80:316	Outer-clustering: Replace all words in the text with their class token 1 and execute binary merging without the merging region constraint until all the classes are merged into a single class." ></td>
	<td class="line x" title="81:316	Make a dendrogram out of this process." ></td>
	<td class="line x" title="82:316	This dendrogram, Droot, constitutes the upper part of the final tree." ></td>
	<td class="line x" title="83:316	3." ></td>
	<td class="line x" title="84:316	Inner-clustering: Let {C(1), C(2),  , C(C)} be the set of the classes obtained at step 1." ></td>
	<td class="line x" title="85:316	For each i (1 < i < C) do the following." ></td>
	<td class="line x" title="86:316	(a) Replace all words in the text except those in C(i) with their class token." ></td>
	<td class="line x" title="87:316	Define a new vocabulary V' = V1 U V2, where V1 = {all the words in C(i)}, V2 = {C1,C2,  ,Ci-l,Ci+l,Cc}, and Cj is a token for C(j) for 1 < j _< C. Assign each element in V' to its own class and execute binary merging with a merging constraint such that only those classes which only contain elements of V1 can be merged." ></td>
	<td class="line x" title="88:316	This can be done by ordering elements of V' with elements of V1 in the first Ivll positions and keep merging with a merging region whose width is \]Vll initially and decreases by one with each merging step." ></td>
	<td class="line x" title="89:316	(b) Repeat merging until all the elements in V1 are put in a single class." ></td>
	<td class="line x" title="90:316	Make a dendrogram Dsub out of the merging process for each class." ></td>
	<td class="line x" title="91:316	This dendrogram constitutes a subtree for each class with a leaf node representing each word in the class." ></td>
	<td class="line x" title="92:316	4 Combine the dendrograms by substituting each leaf node of Droot with the corresponding D sub . This algorithm produces a balanced binary tree representation of words in which those words which are close in meaning or syntactic feature come close in position." ></td>
	<td class="line x" title="93:316	Figure 2 shows an example of Dsu b for one class out of 500 classes constructed using this algorithm with a vocabulary of the 70,000 most frequently occurring words in the Wall Street Journal Corpus." ></td>
	<td class="line x" title="94:316	Finally, by tracing the path from the root node to a leaf node and assigning a bit to each branch with zero or one representing a left or right branch, respectively, we can assign a bit-string (word bits) to each word in the vocabulary." ></td>
	<td class="line x" title="95:316	1In the actual implementation, we only have to work on the bigram table instead of the whole text." ></td>
	<td class="line x" title="96:316	31 I I I Figure 2: Sample Subtree for One Class 3 Word Clustering Experiments We performed experiments using plain texts from six years of the Wall Street Journal Corpus to create clusters and word bits." ></td>
	<td class="line x" title="97:316	The sizes of the texts are 5 million words (MW), 10MW, 20MW, and 50MW." ></td>
	<td class="line x" title="98:316	The vocabulary is selected as the 70,000 most frequently occurring words in the entire corpus." ></td>
	<td class="line x" title="99:316	We set the number C of classes to 500." ></td>
	<td class="line x" title="100:316	The obtained hierarchical clusters are evaluated via the error rate of the ATR Decision-Tree Part-Of-Speech Tagger." ></td>
	<td class="line x" title="101:316	Then as an attempt to combine the two types of clustering methods discussed in Section 2, we performed an experiment for incorporating a word-reshuffling process into the word bits construction process." ></td>
	<td class="line x" title="102:316	3.1 Decision-Tree Part-Of-Speech Tagging The ATR Decision-Tree Part-Of-Speech Tagger is an integrated module of the ATR DecisionTree Parser which is based on SPATTER (Magerman 1994)." ></td>
	<td class="line x" title="103:316	The tagger employs a set of 441 syntactic tags, which is one order of magnitude larger than that of the University of Pennsylvania Treebank Project." ></td>
	<td class="line x" title="104:316	Training texts, test texts, and held-out texts are all sequences of word-tag pairs." ></td>
	<td class="line x" title="105:316	In the training phase, a set of events are extracted from the training texts." ></td>
	<td class="line x" title="106:316	An event is a set of feature-value pairs or question-answer pairs." ></td>
	<td class="line x" title="107:316	A feature can be any attribute of the context in which the current word word(O) appears; it is conveniently expressed as a question." ></td>
	<td class="line x" title="108:316	Tagging is performed left to right." ></td>
	<td class="line x" title="109:316	Figure 3 shows an example of an event with a current word like." ></td>
	<td class="line x" title="110:316	The last pair in the event is a special item which shows the answer, i.e., the correct tag of the current word." ></td>
	<td class="line x" title="111:316	The first two lines show questions about identity of words around the current word and tags for previous words." ></td>
	<td class="line x" title="112:316	These questions are called basic questions." ></td>
	<td class="line x" title="113:316	The second type of questions, word bits questions, are on clusters and word bits such as is the current word in Class 295?" ></td>
	<td class="line x" title="114:316	or what is the 29th bit of the previous word's word bits?." ></td>
	<td class="line x" title="115:316	The third type of questions are called linguist's questions and these are compiled by an expert grammarian." ></td>
	<td class="line x" title="116:316	Such questions could concern membership relations of words or sets of words, or morphological features of words." ></td>
	<td class="line x" title="117:316	Out of the set of events, a decision tree is constructed." ></td>
	<td class="line x" title="118:316	The root node of the decision tree represents the set of all the events with each event containing the correct tag for the corresponding word." ></td>
	<td class="line x" title="119:316	Probability distribution of tags for the root node can be obtained by calculating relative frequencies of tags in the set." ></td>
	<td class="line x" title="120:316	By asking a value of a specific feature on each event in the set, the set can be split into N subsets where N is the number of possible values for the feature." ></td>
	<td class="line x" title="121:316	We can then calculate conditional probability distribution of tags for 32 Event128: { (word(0), 'like' ) (word(-1), 'flies' ) (word(-2), 'time' } (word(l), 'a~' ) (word(2), 'arrow' ) (tag(-1), 'Verb-3rd-Sg-type3' ) (tag(-2), 'Noun-Sg-typel4' )  (Basic Questions) (Inclass?(word(0), Class295), 'yes' ) (WordBits(Word(-1), 29), '1' ) (\]sMember?(word(-2), Set('and', 'or', 'nor' )), 'no' ) (Tag, 'Prep-typeS' ) } (WordBits Questions) (IsPrefix?(Word(0), 'anti'), 'no' ) (Linguist's Questions) Figure 3: Example of an event tl L. tim g, \[-q 26 24 22 20 18 16 14 0 \[\] WSJ Text  ATR Corpus t i 60 Clustering Text Size (Million Words) Figure 4: Tagging Error Rate each subset, conditioned on the feature value." ></td>
	<td class="line x" title="122:316	After computing for each feature the entropy reduction incurred by splitting the set, we choose the best feature which yields maximum entropy reduction." ></td>
	<td class="line x" title="123:316	By repeating this step and dividing the sets into their subsets we can construct a decision tree whose leaf nodes contain conditional probability distributions of tags." ></td>
	<td class="line x" title="124:316	The obtained probability distributions are then smoothed using the held-out data." ></td>
	<td class="line x" title="125:316	The reader is referred to (Magerman 1994) for the details of smoothing." ></td>
	<td class="line x" title="126:316	In the test phase the system looks up conditional probability distributions of tags for each word in the test text and chooses the most probable tag sequences using beam search." ></td>
	<td class="line x" title="127:316	We used WSJ texts and the ATR corpus for the tagging experiment." ></td>
	<td class="line x" title="128:316	The WSJ texts are re-tagged manually using the ATR syntactic tag set." ></td>
	<td class="line x" title="129:316	The ATR corpus is a comprehensive sampling of Written American English, displaying language use in a very wide range of styles and settings, and compiled from many different domains (Black et al. 1996)." ></td>
	<td class="line x" title="130:316	Since the ATR corpus is still in the process of development, the size of the texts we have at hand for this experiment is rather minimal considering the large size of the tag set." ></td>
	<td class="line x" title="131:316	Table 1 shows the sizes of texts used for the experiment." ></td>
	<td class="line x" title="132:316	Figure 4 shows the tagging error rates plotted against various clustering text sizes." ></td>
	<td class="line x" title="133:316	Out of the three types of questions, basic questions and word bits 33 Text Size (words) Training Test Held-Out WSJ Text 75,139 5,831 6,534 ATR Text 76,132 23,163 6,680 Table 1: Texts for Tagging Experiments o iN 28 2d 24 2~ 2C 18 16 14 WSJ Text  Word,Bit @ LingQuest & Word\]3its m I II 12  0 10 20 30 40 50 60 Clustering Text Size (Million Words) Figure 5: Comparison of WordBits with LingQuest & WordBits questions are used in this experiment." ></td>
	<td class="line x" title="134:316	To see the effect of introducing word bits information into the tagger, we performed a separate experiment in which a randomly generated bit-string is assigned to each word 2 and basic questions and word bits questions are used." ></td>
	<td class="line x" title="135:316	The results are plotted at zero clustering text size." ></td>
	<td class="line x" title="136:316	For both WSJ texts and ATR corpus, the tagging error rate dropped by more than 30% when using word bits information extracted from the 5MW text, and increasing the clustering text size further decreases the error rate." ></td>
	<td class="line x" title="137:316	At 50MW, the error rate drops by 43%." ></td>
	<td class="line x" title="138:316	This shows the improvement of the quality of clusters with increasing size of the clustering text." ></td>
	<td class="line x" title="139:316	Overall high error rates are attributed to the very large tag set and the small training set." ></td>
	<td class="line x" title="140:316	One notable point in this result is that introducing word bits constructed from WSJ texts is as effective for tagging ATtt texts as it is for tagging WS3 texts even though these texts are from very different domains." ></td>
	<td class="line x" title="141:316	To that extent, the obtained hierarchical clusters are considered to be portable across domains." ></td>
	<td class="line x" title="142:316	Figure 5 contrasts the tagging results using only word bits against the results with both word bits and linguistic questions 3 for the WS3 text." ></td>
	<td class="line x" title="143:316	The zero clustering text size again corresponds ~Since a distinctive bit-string is assigned to each word, the tagger also uses the bit-string as an ID number for each word in the process." ></td>
	<td class="line x" title="144:316	In this control experiment bit-strings are assigned in a random way, but no two words are assigned the same word bits." ></td>
	<td class="line x" title="145:316	Random word bits are expected to give no class information to the tagger except for the identity of words." ></td>
	<td class="line x" title="146:316	3The linguistic questions we used here are still in the initial stage of development and are by no means 34 WSJ Text 22q 20 16 I 14i 12 10 0 60 l \[\]  Word.Bits @  LingQuest &WordBits,i i i I i I J r i i 10 20 30 40 50 Clustering Text Size (Million Words) Figure 6: Effects of Reshuffling for Tagging to a randomly generated bit-string." ></td>
	<td class="line x" title="147:316	Introduction of linguistic questions is shown to significantly reduce the error rates for the WSJ corpus." ></td>
	<td class="line x" title="148:316	Note that the dependency of the error rates on the clustering text size is quite similar in the two cases." ></td>
	<td class="line x" title="149:316	This indicates the effectiveness of combining automatically created word bits and hand-crafted linguistic questions in the same platform, i.e., as features." ></td>
	<td class="line x" title="150:316	In Figure 5 the tagging error rates seem to be approaching saturation after the clustering text size of 50MW." ></td>
	<td class="line x" title="151:316	However, whether no further improvement can be obtained by using texts of greater size is still an unsolved question." ></td>
	<td class="line x" title="152:316	3.2 Reshuffling One way to improve the quality of word bits is to introduce a reshuffling process just after step 1 (MI-clustering) of the word bits construction process (cf. 2.2)." ></td>
	<td class="line x" title="154:316	The reshuffling process we adopted is quite simple." ></td>
	<td class="line x" title="155:316	1." ></td>
	<td class="line x" title="156:316	Pick a word from the vocabulary." ></td>
	<td class="line x" title="157:316	Move the word from its current class to another class if that movement increases the AMI most among all the possible movements." ></td>
	<td class="line x" title="158:316	2." ></td>
	<td class="line x" title="159:316	Repeat step 1 starting from the most frequent word through the least frequent word." ></td>
	<td class="line x" title="160:316	This constitutes one round of reshuffling." ></td>
	<td class="line x" title="161:316	After several rounds of reshuffling, the word bits construction process is resumed from step 2 (Outer-clustering)." ></td>
	<td class="line x" title="162:316	Figure 6 shows the tagging error rates with word bits obtained by zero, two and five rounds of reshuffling 4 with a 23MW text." ></td>
	<td class="line x" title="163:316	Tagging results presented in Figure 5 are also shown as a reference." ></td>
	<td class="line x" title="164:316	Although the vocabulary used in this experiment is slightly different from the other comprehensive." ></td>
	<td class="line x" title="165:316	4The vocabulary used for the reshuffling experiment shown in Figure 6 is the one used for a preliminary experiment and its size is 63850." ></td>
	<td class="line x" title="166:316	35 experiments, we can clearly see the effect of reshuffling for both the word-bits-only case and the case with word bits and linguistic questions." ></td>
	<td class="line x" title="167:316	After five rounds of reshuffling, the tagging error rates become much smaller than the error rates using the 50MW clustering text with no reshuffling." ></td>
	<td class="line x" title="168:316	It is yet to be determined if the effect of reshuffling increases with increasing amount of clustering text." ></td>
	<td class="line x" title="169:316	4 From Word Clustering to Compound Clustering We showed in section 3 that the clusters we obtained are useful for Part-Of-Speech tagging." ></td>
	<td class="line x" title="170:316	However, the clusters we have worked on so far have all been clusters of words, and the PartOf-Speech tagging task has been limited to individual words." ></td>
	<td class="line x" title="171:316	For many other NLP tasks, however, similarities among phrases or multiword compounds are more important than those among individual words." ></td>
	<td class="line x" title="172:316	Let's turn back to the motivation of the clustering work discussed in the Introduction." ></td>
	<td class="line x" title="173:316	Consider the following sentences." ></td>
	<td class="line x" title="174:316	(e) The music sent Mary to sleep." ></td>
	<td class="line x" title="175:316	(f) The music sent Professor Frederic K. Thompson to sleep." ></td>
	<td class="line x" title="176:316	Suppose that we want to translate sentence (f) to some language by an example-based machine translation system with example data including sentence (e) and its translation." ></td>
	<td class="line x" title="177:316	In this case, what the system has to detect is that both 'Mary' and 'Professor Frederic K. Thompson' represent a human." ></td>
	<td class="line x" title="178:316	The similarity between 'Mary' and 'Frederic' as being first names doesn't help in this case." ></td>
	<td class="line x" title="179:316	Similarly, the detection of a correspondence between 'CBS Inc'." ></td>
	<td class="line x" title="180:316	and 'American Telephone & Telegraph Co'." ></td>
	<td class="line x" title="181:316	might be necessary in another case." ></td>
	<td class="line x" title="182:316	This observation leads us to construct classes o.f compounds rather than classes of just words." ></td>
	<td class="line x" title="183:316	Individual words can also be in the same class as multiword compounds, but we will generically call such a class a class of compounds in this paper." ></td>
	<td class="line nc" title="184:316	While several methods have been proposed to automatically extract compounds (Smadja 1993, Suet al. 1994), we know of no successful attempt to automatically make classes of compounds." ></td>
	<td class="line x" title="185:316	The obvious problem we face when we construct classes of compounds is that the possible number of compounds is too large if we try to handle them individually." ></td>
	<td class="line x" title="186:316	However, if we represent compounds by a series of word-classes 5 instead of a series of words, we can constrain the explosion of the number of compounds." ></td>
	<td class="line x" title="187:316	One way of looking at this approach is to bundle quite similar compounds in a small subclass and treat them as a single compound." ></td>
	<td class="line x" title="188:316	For example, in the experiment described in Section 3, it was found that some word class, say WC129, contains almost exclusively first names, and another class, say WC246, contains almost exclusively family names." ></td>
	<td class="line x" title="189:316	Then the chain of classes 'WC129_WC246' represents one pattern of human names, or one group of two-word compounds representing human names." ></td>
	<td class="line x" title="190:316	There are of course many other patterns, or class chains, of different lengths which represent human names." ></td>
	<td class="line x" title="191:316	Therefore, our aim is to collect all the different class chains which are syntactically and semantically similar and put them in one compound-class." ></td>
	<td class="line x" title="192:316	In the following subsection, we describe one approach to this goal which is completely automatic." ></td>
	<td class="line x" title="193:316	4.1 Compound Clustering Method Our compound clustering method consists of the following three steps." ></td>
	<td class="line x" title="194:316	1." ></td>
	<td class="line x" title="195:316	Identification of Class Chains First, we replace each word in a large text with its word-class." ></td>
	<td class="line x" title="196:316	We then use mutual information as a measure of 'stickiness' of two classes, and identify which class pair 5We use the term word-class for a class of words to make a clear distinction from a compound.class." ></td>
	<td class="line x" title="197:316	36 . . should be chained." ></td>
	<td class="line x" title="198:316	Let MI(C1,C2) be mutual information of adjacent classes C1 and C2 in the text." ></td>
	<td class="line x" title="199:316	Then we form chain 'C1_C2' if Pr(ClC2) > *TH * (3) MI(C1, C2) = log Pr(cl)Pr(c2) for some threshold *TH*." ></td>
	<td class="line x" title="200:316	If it is found, in the series of three classes 'C1 C2 C3' in the text, that (C1,C2) forms a chain and (C2,C3) also forms a chain, then we simply form one large chain C1_C2_C3." ></td>
	<td class="line x" title="201:316	In a similar way we form a chain of maximum length for any series of classes in the text." ></td>
	<td class="line x" title="202:316	Construction of Reduced Text and New Vocabulary Each class chain identified is then replaced in the text with a token which represents the chain." ></td>
	<td class="line x" title="203:316	We call such a token a class chain token." ></td>
	<td class="line x" title="204:316	After the scan through the text with this replacement operation of a class chain with its token, the text is represented by a series of word-classes and class chain tokens." ></td>
	<td class="line x" title="205:316	The word classes remaining in the text are the ones which don't form a chain in their context." ></td>
	<td class="line x" title="206:316	Those word classes are then converted back to their corresponding words in the text 6 The resulting text is the same as the original text except that a multiword compound which matches one of the extracted class chains is represented by a class chain token." ></td>
	<td class="line x" title="207:316	We call this text the reduced text." ></td>
	<td class="line x" title="208:316	Out of the reduced text, a new vocabulary is created as a set of words and tokens whose frequency in the reduced text is more than or equal to some threshold." ></td>
	<td class="line x" title="209:316	Compound Clustering We conduct MI-clustering (step 1 of the word bits construction process) using the reduced text and the new vocabulary." ></td>
	<td class="line x" title="210:316	The classes we obtained, which we call compound-classes, contain words and class chain tokens." ></td>
	<td class="line x" title="211:316	Each class chain token in a compound-class is then expanded." ></td>
	<td class="line x" title="212:316	This means that all the multiword compounds that are represented by the class chain token in the text are put into the compound-class." ></td>
	<td class="line x" title="213:316	After expanding all the tokens, the tokens are removed from the compound-classes." ></td>
	<td class="line x" title="214:316	This results in compound-classes containing words and multiword compounds." ></td>
	<td class="line x" title="215:316	It is also possible to construct hierarchical clustering of compounds if we follow all the steps in the word bits construction process after this step." ></td>
	<td class="line x" title="216:316	4.2 Compound Clustering Experiment We used plain texts from two years (1987 and 1988) of Wall Street Journal Corpus to create compound clusters." ></td>
	<td class="line x" title="217:316	The total volume amounts to about 40 MW of text." ></td>
	<td class="line x" title="218:316	The word-classes used in this experiment are taken from the result of MI clustering with the 50MW text followed by five rounds of reshuffling." ></td>
	<td class="line x" title="219:316	The quality of the compound clusters depends on the threshold *TH* in equation 3." ></td>
	<td class="line x" title="220:316	We used *TH*=3 following 'a very rough rule of thumb' used for word-based mutual information in (Church and Hanks, 1990)." ></td>
	<td class="line x" title="221:316	Out of the 40MW text, 7621 distinct class chains and 287,656 distinct multiword compounds are extracted." ></td>
	<td class="line x" title="222:316	To construct a new vocabulary, we selected the words and tokens whose frequency in the reduced text is more than four." ></td>
	<td class="line x" title="223:316	The size of the new vocabulary is 60589 and it contains 4685 class chain tokens." ></td>
	<td class="line x" title="224:316	Some of the compound-classes that were obtained are shown in Figure 7." ></td>
	<td class="line x" title="225:316	The compounds are listed in descending order of frequency in each class, and the lists are truncated at an arbitrary point." ></td>
	<td class="line x" title="226:316	6The conversion of a word-class to a word is not a one-to-one mapping, but with the context in the text the conversion is unique." ></td>
	<td class="line x" title="227:316	In the actual implementation, the text is represented by a series of (word, word-class) pairs and no conversion is actually carried out." ></td>
	<td class="line x" title="228:316	3'7 Figure 7: Examples of Compound Classes COMPOUND CLASS 171: President_Reagan Mr._Reagan Mr._Bush Mr._Dukakis Judge_Bork Ronald_Reagan George_Bush Michael_Dukakis Treasury_Secretary_J ames.B aker Mr._Holmes Vice_President_George_Bush Gov._Dukakis Gen._Noriega Mrs._Thatcher someone_who Mrs._Aquino Mr._Roh Gen._Secord Mr._Lawson Adm._Poindexter anyone_who MrDole Lt._Col._North Jimmy_Carter Sen._Dole Mr._Mulroney Mr._Quayle Sen._Bentsen Mr._Chirac Mr._Gephardt Mr._Marcos Vice_President_Bush Sen._Quayle Mr._Carter Mr._Chun Prime_Minister_Margaret_Thatcher Judge_Greene MrBrady President_Carter President_Chun Judge_Kennedy Sen._Proxmire Robert_Bork Rep._Rostenkowski Mr._Kohl Robert." ></td>
	<td class="line x" title="229:316	Holmes Judge_Pollack MrKemp Prime_Minister_Yasuhiro_Nakasone Mr._Kennedy President_Aquino COMPOUND CLASS 179: General_Motors_Corp." ></td>
	<td class="line x" title="230:316	Drexel_Burnham_LambertInc." ></td>
	<td class="line x" title="231:316	Ford.Motor_Co." ></td>
	<td class="line x" title="232:316	InternationalBusiness_Machines_Corp." ></td>
	<td class="line x" title="233:316	General_Electric_Co." ></td>
	<td class="line x" title="234:316	Shearsoniehman_Brothers_Inc." ></td>
	<td class="line x" title="235:316	Chrysler_Corp." ></td>
	<td class="line x" title="236:316	First.Boston_Corp." ></td>
	<td class="line x" title="237:316	Merrill_Lynch_&_Co." ></td>
	<td class="line x" title="238:316	Morgan_Stanley_&_Co." ></td>
	<td class="line x" title="239:316	Shearson_Lehman_Hut tonInc." ></td>
	<td class="line x" title="240:316	News_Corp." ></td>
	<td class="line x" title="241:316	American_Telephone_&_Telegraph_Co." ></td>
	<td class="line x" title="242:316	PaineWebbet_Inc." ></td>
	<td class="line x" title="243:316	PrudentialB ache_SecuritiesAnc." ></td>
	<td class="line x" title="244:316	TexacoAnc." ></td>
	<td class="line x" title="245:316	McDonnell_Douglas_Corp." ></td>
	<td class="line x" title="246:316	Dean_Witter_ReynoldsAnc." ></td>
	<td class="line x" title="247:316	TimeInc." ></td>
	<td class="line x" title="248:316	AMR_Corp." ></td>
	<td class="line x" title="249:316	CB SAnc." ></td>
	<td class="line x" title="250:316	American." ></td>
	<td class="line x" title="251:316	Express_Co." ></td>
	<td class="line x" title="252:316	Campeau_Corp." ></td>
	<td class="line x" title="253:316	BankAmerica_Corp." ></td>
	<td class="line x" title="254:316	Du_Pont_Co." ></td>
	<td class="line x" title="255:316	Allegis_Corp." ></td>
	<td class="line x" title="256:316	General.Dynamics_Corp." ></td>
	<td class="line x" title="257:316	Digital_Equipment_Corp." ></td>
	<td class="line x" title="258:316	Kohlb erg_Kravis_Roberts_&_Co." ></td>
	<td class="line x" title="259:316	Exxon_Corp." ></td>
	<td class="line x" title="260:316	Chase_Manhattan_Corp." ></td>
	<td class="line x" title="261:316	USX_Corp." ></td>
	<td class="line x" title="262:316	Nikko_Securities_Co." ></td>
	<td class="line x" title="263:316	Lockheed_Corp." ></td>
	<td class="line x" title="264:316	COMPOUND CLASS 221: common_stock preferred_stock cash_flow bank_debt long-term_debt foreign_debt subordinated_debt senior_debt balance_sheet short-term_debt balance_sheets cost_overruns corporate_debt debt_load convertible_preferred_stock international_debt debt_outstanding Class_B_stock debt_ratings cumulative_preferred_stock corporateAOUs current_delivery preference_stock ozoneAayer buffer_stock unsecured_debt convertible_preferred external_debt debt_offering current_contract blood_clots Class_B_common cumulative_convertible_preferred_stock corporate_governance Class_B_common_stock unsold_balance secured_debt debt.issue cumulative_preferred municipal_debt convertible_exchangeable_preferred_stock cash.hoard debt.rating 65-day_supply cash_balance senior_subordinated_debt senior_secured_debt COMPOUND CLASS 256: Fed SEC Reagan_administration IRS Pentagon Justice." ></td>
	<td class="line x" title="265:316	Department Navy Commerce.Department FDA Army FCC FDIC Federal.Reserve_Board State_Department Bundesbank EPA FAA IMF Labor_Department Agriculture_Department FBI NASD Defense." ></td>
	<td class="line x" title="266:316	Department Federal_Home." ></td>
	<td class="line x" title="267:316	Loan_Bank_Board Britishgovernment NRC Finance.Ministry Japanese.government FTC UAW Kremlin PRI Transportation._Department PLO Federal_Trade_Commission CFTC Canadian_government NSC GAO Teamsters Carter_Hawley INS GSA Environmental_ProtectionAgency ANC Labor_Party AFL-CIO FASB NFL Federal_Aviation_Administration ACLU 38 Compound-class-171 consists of names with title many of which are politicians' names." ></td>
	<td class="line x" title="268:316	Compound-class-179 contains multiword company names." ></td>
	<td class="line x" title="269:316	Compound-class-221 consists of multiword compound nouns from several specific semantic domains including money, surgery and natural environment, but most of the frequent compounds are money-related terms." ></td>
	<td class="line x" title="270:316	Compound-class-256 is worth special attention in the sense that although single words and multiword compounds are mixed almost evenly, most of the single words are abbreviations of organizations, mostly public organizations, and the multiword compounds also ahnost exclusively represent public organizations." ></td>
	<td class="line x" title="271:316	Another point to note here is that the pattern of case is not uniform in this list." ></td>
	<td class="line x" title="272:316	Although both 'Defense Department' and 'British government' represent political organizations, the former consists of only capitalized words and the latter doesn't." ></td>
	<td class="line x" title="273:316	In order to measure the performance of this compound clustering method, a consistency check is performed for one class." ></td>
	<td class="line x" title="274:316	The objective is to check what proportion of the identified members of the class actually deserves to be included in the class." ></td>
	<td class="line x" title="275:316	Because this kind of judgement is very difficult in general, we must choose a class whose membership is quite clear to identify." ></td>
	<td class="line x" title="276:316	By this criterion we chose compound-class-179 because it is quite easy to decide if some compound is a correct company name or not." ></td>
	<td class="line x" title="277:316	From the 40MW text, we randomly chose 3000 occurrences of multiword compounds which are members of compound-class-179." ></td>
	<td class="line x" title="278:316	By manual anMysis, it was found that 133 identified compounds were wrong." ></td>
	<td class="line x" title="279:316	The precision is therefore 95.6 %." ></td>
	<td class="line x" title="280:316	Most of the errors are due to the truncation of correct company names." ></td>
	<td class="line x" title="281:316	For example, from the string 'North American Philips Corp.', only 'Philips Corp'." ></td>
	<td class="line x" title="282:316	was extracted." ></td>
	<td class="line x" title="283:316	Although 'Philips Corp'." ></td>
	<td class="line x" title="284:316	is itself a correct company name, we treated this instance as an error because our judgement was occurrence-based." ></td>
	<td class="line x" title="285:316	There was only one instance where a compound irrelevant to company names was extracted (a person name)." ></td>
	<td class="line x" title="286:316	For a control experiment which we will describe shortly, all the incorrect compounds are corrected by hand and a standard file is created which contains all the correct 2999 occurrences of company names." ></td>
	<td class="line x" title="287:316	One merit of the current approach is that the identification of a compound-class is carried out in time linear with the text size." ></td>
	<td class="line x" title="288:316	Therefore, by associating a word with its word-class as a feature in the lexicon, and by storing class chain patterns and their membership to compound classes, we can carry out a real time identification of the compound-classes without actually storing the compounds in the lexicon." ></td>
	<td class="line x" title="289:316	As a control experiment to the above experiment, we conducted word-based compound extraction and compared the result with the above result." ></td>
	<td class="line x" title="290:316	Instead of mutual information of adjacent classes, mutual information of adjacent words are calculated for all the bigrams in the text." ></td>
	<td class="line x" title="291:316	Then using various MI threshold values, words are chained in a similar way as described in Section 4.1, and compounds are identified." ></td>
	<td class="line x" title="292:316	We then evaluated how many of the occurrences of company names in the standard file are identified in the word-based compound extraction experiment." ></td>
	<td class="line x" title="293:316	We varied the MI threshold values from 1.0 to 6.0 with a step of 0.5, but the precision of the word-based approach with respect to the standard file was always below 50 %." ></td>
	<td class="line x" title="294:316	The main reason of the superiority of the class-based approach against the word-based one is associated with the data sparseness problem." ></td>
	<td class="line x" title="295:316	Most of the previously proposed methods to extract compounds or to measure word association using mutual information (MI) either ignore or penalize items with low co-occurrence counts (Church and Hanks 1990, Su, Wu and Chang 1994), because MI becomes unstable when the co-occurrence counts are very small." ></td>
	<td class="line x" title="296:316	Take for example a class chain 'WC129_WC246' discussed above." ></td>
	<td class="line x" title="297:316	Figure 8 shows some examples of compounds matching the pattern 'WC129_WC246' in the 40MW text." ></td>
	<td class="line x" title="298:316	Each column shows, from left to right, word-based MI for the word bigram (WORD-1,WORD-2), co-occurrence frequency of the word bigram, the first word, the second word, class-based MI for the class bigram (CLASS-I, CLASS-2), co-occurrence frequency of the class bigram, the word-class of WORD-l, and the word-class of WORD-2." ></td>
	<td class="line x" title="299:316	Note that the numbers for class-based entries are 39 Figure 8: Examples of Compounds for Names WORD-MI BI-COUNT WORD-1 WORD-2 CLASS-MI CL-BI-COUNT CLASS-I CLASS-2 16.104915 1 Takako Doi 3.941235 52087 129 246 15.881772 3 Mandy Patinkin 3.941235 52087 129 246 14.783159 3 Hideo Sakamaki 3.941235 52087 129 246 12.280086 10 Curt Bradbury 3.941235 52087 129 246 11.048669 3 Matthew Kennelly 3.941235 52087 129 246 9.358209 1 Marsha Gardner 3.941235 52087 129 246 7.994606 7 Ralph Whitehead 3.941235 52087 129 246 5.073718 1 George Hartman 3.941235 52087 129 246 4.328457 1 Daniel Owen 3.941235 52087 129 246 3.914939 3 Charles Walker 3.941235 52087 129 246 3.319351 1 Robert Fischer 3.941235 52087 129 246 2.939145 1 Robert Lucas 3.941235 52087 129 246 2.236354 2 Edward Baker 3.941235 52087 129 246 1.119861 1 Robert Shultz 3.941235 52087 129 246 1.072005 1 Robert Hall 3.941235 52087 129 246 1.069133 1 George Jackson 3.941235 52087 129 246 0.771154 1 Richard Baker 3.941235 52087 129 246 0.218531 1 John Jackson 3.941235 52087 129 246 the same for all the compounds because we collected compounds with the same class chain." ></td>
	<td class="line x" title="300:316	Although all the compounds are compounds of a first name and a family name, the wordbased MI varies considerably." ></td>
	<td class="line x" title="301:316	This is because frequencies of first names and family names vary considerably while frequencies of pairs of first names and family names in the list are very small." ></td>
	<td class="line x" title="302:316	For example, 'John' and 'Jackson' are very common first and second names, but the name 'John Jackson' appeared only once in the text." ></td>
	<td class="line x" title="303:316	Therefore the word-based MI becomes very small." ></td>
	<td class="line x" title="304:316	On the other hand, because 'Takako' and 'Doi' were very rare names in WSJ news articles in 1987 and 1988, the MI becomes very high even though 'Takako Doi' appeared only once in the text." ></td>
	<td class="line x" title="305:316	In contrast, the class-based MI is very stable because the co-occurrence frequency of the two classes is as high as 52087." ></td>
	<td class="line x" title="306:316	When we examined frequencies of all the compounds in the text that match 'WC129_WC246', it turned out that more than 80 % of the compounds appeared less than five times in the text." ></td>
	<td class="line x" title="307:316	This shows how the data sparseness problem is critical for the purpose of compound extraction." ></td>
	<td class="line x" title="308:316	5 Conclusion We presented an algorithm for hierarchical clustering of words, and conducted a clustering experiment using large texts ranging in size from 5MW to 50MW." ></td>
	<td class="line x" title="309:316	High quality of the obtained clusters is confirmed by the effect of introducing word bits into the ATR Decision-Tree Part-OfSpeech Tagger." ></td>
	<td class="line x" title="310:316	The hierarchical clusters obtained from WSJ texts are also shown to be useful for tagging ATR texts which are from quite different domains than WSJ texts." ></td>
	<td class="line x" title="311:316	The wordclasses thus obtained are then used to identify and cluster multiword compounds." ></td>
	<td class="line x" title="312:316	It is shown that by using statistics on classes instead of on words, the data sparseness problem is avoided and the reliability of mutual information is increased." ></td>
	<td class="line x" title="313:316	As a result, class-based compounds identification and extraction becomes more reliable than word-based methods." ></td>
	<td class="line x" title="314:316	This approach 40 also provides a way of automatically clustering compounds, which has rarely been attempted." ></td>
	<td class="line x" title="315:316	Acknowledgements We thank John Lafferty and Christopher Manning for their helpful comments, suggestions and discussion with us." ></td>
	<td class="line x" title="316:316	Special thanks are to Eric Visser for reviewing a draft of this paper." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W96-0304
Using Lexical Semantic Techniques To Classify Free-Responses
Burstein, Jill;Kaplan, Randy M.;Wolff, Susanne;Lu, Chi;"></td>
	<td class="line x" title="1:225	Using Lexical Semantic Techniques to Classify Free-Responses Jill Burstein Educational Testing Service 11R Princeton, New Jersey 08541 jburstein@ets.org Randy Kaplan Educational Testing Service 17R Princeton, New Jersey 08541 rkaplan @ets.org Susanne Wolff Educational Testing Service 17R Princeton, New Jersey 08541 swolff@ets.org Chi Lu Educational Testing Service 17R Princeton, New Jersey 08541 clu@ets.org Abstract This paper discusses a case study in which lexical semantic techniques were used to implement a prototype scoring system for short-answer, free-responses to test questions." ></td>
	<td class="line x" title="2:225	Scoring, as it is discussed in this paper, is a kind of clasgification problem." ></td>
	<td class="line x" title="3:225	Responses are automatically scored by being assigned appropriate classifications." ></td>
	<td class="line x" title="4:225	The ultimate goal is to develop a scoring system which can reliably analyze response content." ></td>
	<td class="line x" title="5:225	For this study, a domain-specific, concept-based lexicon, and a concept grammar were built to represent the response set, using 200 of 378 responses from the original data set." ></td>
	<td class="line x" title="6:225	The lexicon is built, from individual words, and 2-word and 3-word terms from the training data." ></td>
	<td class="line x" title="7:225	The lexicon is best characterized by Bergler's (1995) layered lexicon." ></td>
	<td class="line x" title="8:225	Concept grammar rules are built by mapping concepts from the lexicon onto the concept-structure patterns present in a set of training responses." ></td>
	<td class="line x" title="9:225	Previous attempts to score these responses using lexically-based statistical techniques and structure-independent content grammars were not reliable (Burstein and Kaplan (1995))." ></td>
	<td class="line x" title="10:225	The results discussed in this paper illustrate the reliability of the lexical semantic methods used in the study." ></td>
	<td class="line x" title="11:225	20 ! 1." ></td>
	<td class="line x" title="12:225	Introduction There is a movement in testing to augment the conventional multiple-choice items (i.e. , test questions) with short-answer free-response items." ></td>
	<td class="line x" title="13:225	Due to the large volume of tests administered yearly by Educational Testing Service (ETS), hand-scoring of these tests with these types of items is costly and time-consuming for practical testing programs." ></td>
	<td class="line x" title="14:225	ETS is currently working on natural language understanding systems which could be used for computer-assisted scoring of short-answer freeresponses (see Kaplan and Bennett (1994) and Burstein and Kaplan (1995))) The overall goal of our current research is to develop a scoring system that can handle short-answer free-response items." ></td>
	<td class="line x" title="15:225	Such a scoring system has to be able to identify the relevant content of a response and assign it to an appropriate content category." ></td>
	<td class="line x" title="16:225	Another consideration in the development of a scoring system is that the data sets that are available to us are relatively small, and the responses in these data sets lack lexico-~syntactic patterning." ></td>
	<td class="line x" title="17:225	The items which we work with are either experimental, or have been administered as paper-and-pencil exams." ></td>
	<td class="line x" title="18:225	In the former case, there is a limited subject pool, and in the latter case, we rely on what has been put into electronic form." ></td>
	<td class="line x" title="19:225	The response sets typically range from 300-700 responses which we have to use for training and testing." ></td>
	<td class="line x" title="20:225	This is quite a different scenario from natural language understanding systems which can be designed using large corpora from full text sources, such as the AP News and the Wall Street Journal." ></td>
	<td class="line x" title="21:225	This paper discusses a case study that examined how lexical semantic techniques could be used to build scoring systems, based on small data sets." ></td>
	<td class="line x" title="22:225	Previous attempts to classify these responses using lexically-based statistical techniques and structure-independent content grammars were not reliable (Burstein and Kaplan (1995))." ></td>
	<td class="line x" title="23:225	The results of this case study illustrate the reliability of lexical semantic methods." ></td>
	<td class="line x" title="24:225	For this study, a concept-based lexicon and a concept grammar were built to represent a response set." ></td>
	<td class="line x" title="25:225	The lexicon can best be characterized by Bergler's (1995) layered lexicon in that the list of lexical entry words and terms can remain constant, while the features associated with each entry are modular, so that they can be replaced as necessary." ></td>
	<td class="line x" title="26:225	Concepts in the concept grammars were linked to the lexicon." ></td>
	<td class="line x" title="27:225	In this paper, concepts are superordinate terms which contain one or more subordinate, metonymic terms." ></td>
	<td class="line x" title="28:225	A prototype was implemented to test our hypothesis that a lexical semantics approach to scoring would yield accurate results." ></td>
	<td class="line x" title="29:225	2." ></td>
	<td class="line x" title="30:225	Test Item Types, Response Sets, and Lexical Semantics 2.1 Test Item Types and Response Sets Our previous research with regard to language use in test items revealed that different test items use domain-specific language (Kaplan and Bennett (1994))." ></td>
	<td class="line x" title="31:225	Lexicons restricted to dictionary knowledge of words are not sufficient for interpreting the meaning of responses for unique items." ></td>
	<td class="line x" title="32:225	Concept knowledge bases built from an individual data set of examinee responses can be useful for representing domain-specific language." ></td>
	<td class="line x" title="33:225	To illustrate the use of such knowledge bases in the development of scoring systems, linguistic information from the response set of an inferencing item ~In this paper, a response refers to an examinees 15 20 word answer to an item which can be either in the form of a complete sentence or sentence fragment." ></td>
	<td class="line x" title="34:225	21 will be discussed." ></td>
	<td class="line x" title="35:225	For this item type, examinees are reliant on real-world knowledge with regard to item topic, and responses are based on an examinees own ability to draw inferences." ></td>
	<td class="line x" title="36:225	Responses do not appear show typical features of sublanguage in that there are no domain-specific structures, and the vocabulary is not as restricted." ></td>
	<td class="line nc" title="37:225	Therefore, sublanguage techniques such as Sager (1981) and Smadja (1993) do not work." ></td>
	<td class="line x" title="38:225	In situations where lexico-syntactic patterning is deficient, a lexicon with specified metonymic relations can be developed to yield accurate scoring of response content." ></td>
	<td class="line x" title="39:225	We define metonyms as words which can be used in place of one another when they have a domain-specific relation (Gerstl (1991)) 2.2 Using Lexical Semantics for Response Representation Our goal in building a scoring system for free-responses is to be able to classify individual responses by content, as well as to determine when responses have duplicate meaning (i.e. , one response is the paraphrase of another response)." ></td>
	<td class="line x" title="40:225	In previous research, we used a concept-based approach similar to the one described in this study." ></td>
	<td class="line x" title="41:225	The difference between the previous system and our current prototype is that in the previous system, concepts were not represented with regard to structure, and the lexicon was domain-independent." ></td>
	<td class="line x" title="42:225	The underspecification of concept-structure relationships, and the lack of a domain-specific lexicon degraded the performance of that system (Kaplan and Bennett (1994)." ></td>
	<td class="line x" title="43:225	A second lexically-based, statistical approach performed poorly for the same reasons described above." ></td>
	<td class="line x" title="44:225	The second approach looked at similarity measures between responses based on lexical overlap." ></td>
	<td class="line x" title="45:225	Again, structure was not considered, and the lexicon was domain-independent which contributed to the system's poor performance (Burstein and Kaplan (1995))." ></td>
	<td class="line x" title="46:225	Any system we build must have the ability to analyze the concept-structure patterning in a response, so that response content can be recognized for sconng purposes." ></td>
	<td class="line x" title="47:225	Given our small data set, our assumption was that a lexical semantic approach which employed domain-specific language and concept grammars with concept-structure patterns would facilitate reliable scoring." ></td>
	<td class="line x" title="48:225	Our hypothesis is that this type of representation would denote the content of a response based on its lexical meanings and their relationship to the syntactic structure of the response." ></td>
	<td class="line x" title="49:225	It would appear that Jackendoff's (1983) Lexical Conceptual Structure (LCS) representation may be applicable to our problem." ></td>
	<td class="line x" title="50:225	These structures are considered to be conceptual universals and have been successfully used by Dorr, et al (1995) and Holland (1994) in natural language understanding tasks." ></td>
	<td class="line x" title="51:225	Holland points out, however, that LCSs cannot represent domain knowledge, nor can they handle the interpretation of negation and quantification, all of which are necessary in our scoring systems." ></td>
	<td class="line x" title="52:225	Holland also states that LCSs could not represent a near-match between the two sentences, The person bought a vehicle, and The man bought a car." ></td>
	<td class="line x" title="53:225	As is discussed later in the paper, our scoring systems must be able to deal with such near-match responses." ></td>
	<td class="line x" title="54:225	Based on the above-mentioned limitations of LCSs, the use of such representation for scoring systems does not seem compatible with our response classification problem." ></td>
	<td class="line x" title="55:225	22 3." ></td>
	<td class="line x" title="56:225	The Formulating-Hypotheses Item 2 Responses from the Formulating-Hypotheses item (F-H) were used in this study." ></td>
	<td class="line x" title="57:225	F-H is an experimental inferencing item in which an examinee is presented with a short passage (about 30 words) in which a hypothetical situation is described, and s/he composes up to 15 hypotheses that could explain Why the situation exists." ></td>
	<td class="line x" title="58:225	Examinee responses do not have to be in complete sentences, and can be up tO 15 words in length." ></td>
	<td class="line x" title="59:225	For example, an item referred to as the police item describes a situation in which the number of police being killed has reduced over a 20-year period." ></td>
	<td class="line x" title="60:225	The examinee is theasked to give reasons as to why this might have occurred." ></td>
	<td class="line x" title="61:225	Sample responses are illustrated in (1)." ></td>
	<td class="line x" title="62:225	(1) Sample correct responses to the police item a. Better cadet training programs b. Police wear bullet-proof vests c. Better economic circumstances mean less crime." ></td>
	<td class="line x" title="63:225	d. Advanced medical technology has made it possible to save more lives." ></td>
	<td class="line x" title="64:225	e. Crooks now have a decreased ability to purchase guns." ></td>
	<td class="line x" title="65:225	3.1 Required Scoring Tasks for F-H Our task is to create a system which will score the data using the same criteria used in hand-scoring." ></td>
	<td class="line x" title="66:225	In the hand-scoring process, test developers (i.e. , the individuals who create and score exams) create a multiple-category rubric, that is, a scoring key, in which each category is associated with a set of correct or incorrect responses." ></td>
	<td class="line x" title="67:225	A multiple-category rubric must be created to capture any possible response duplication that could occur in the examinees multiple response file." ></td>
	<td class="line x" title="68:225	For instance, if an examinee had two responses, Better trained police, and Cops are more highly trained, the scoring system must identify these two responses as duplicates which should not both count toward the final score." ></td>
	<td class="line x" title="69:225	Another reason for multiple-category assignment is to be able to provide content-relevant explanations as to why a response was scored a certain way." ></td>
	<td class="line x" title="70:225	Our current prototype was designed to classify responses according to a set of training responses which had been hand-scored by test developers in a multiple-category rubric they had developed." ></td>
	<td class="line x" title="71:225	For the police data set, there were 47 categories associated with a set of 200 training responses." ></td>
	<td class="line x" title="72:225	Each rubric category had between 1 and 10 responses." ></td>
	<td class="line x" title="73:225	3.2." ></td>
	<td class="line x" title="74:225	Characterization of police training data The training set responses have insufficient lexico-syntactic overlap to rely on lexical co-occurrence and frequencies to yield content information." ></td>
	<td class="line x" title="75:225	For instance, police and better occur frequently, but in varying structures, such as in the responses, Police officers were better trained, and Police receiving better training to avoid getting killed in the line of duty." ></td>
	<td class="line x" title="76:225	These two responses must be classified in 2Test items in this paper are copyrighted by Educational Testing Service (ETS)." ></td>
	<td class="line x" title="77:225	No further reproduction is permitted without written permission of ETS." ></td>
	<td class="line x" title="78:225	23 separate categories: (a) Better police training, general, and (b) Types of self-defense~safety techniques, respectively." ></td>
	<td class="line x" title="79:225	Metonyms within content categories had to be manually classified, since such relations were often not derivable from real-world knowledge bases." ></td>
	<td class="line x" title="80:225	For instance, in the training responses, A recent push in safety training has paid off for modern day police, and 'Officers now better combat trained, ' the terms safety training with combat trained, needed to be related." ></td>
	<td class="line x" title="81:225	Test developers had categorized both responses under the Trained for self-defense~safety category." ></td>
	<td class="line x" title="82:225	Safety training and combat train were terms related to a type of training with regard to personal safety." ></td>
	<td class="line x" title="83:225	The terms had to be identified as metonyms in order to classify the responses accurately." ></td>
	<td class="line x" title="84:225	4." ></td>
	<td class="line x" title="85:225	Strategy for Representing Police Responses As previously mentioned, there was insufficient lexico-syntactic patterning to use a contextual word use method, and domain-specific word use could not be derived from real-world knowledge sources." ></td>
	<td class="line x" title="86:225	Therefore, we developed a domain-specific concept lexicon based on a set of 200 training responses over all categories." ></td>
	<td class="line x" title="87:225	Each single, relevant word or 2-3 word term was linked to a concept entry." ></td>
	<td class="line x" title="88:225	Small concept grammars were developed for individual rubric categories." ></td>
	<td class="line x" title="89:225	These grammars were based on the conceptual-structural representations identified in the training response set." ></td>
	<td class="line x" title="90:225	As much as possible, it was important that the rules represented the relationship between multiple concepts within a phrasal constituent." ></td>
	<td class="line x" title="91:225	The phrasal constituent itself, that is, whether it was an NP or a VP did not seem relevant." ></td>
	<td class="line x" title="92:225	It was only meaningful that a constituent relationship occurred." ></td>
	<td class="line x" title="93:225	Without this structural information, the concepts could occur in any position in a response, and automatic category assignment would not be reliable (Burstein and Kaplan (1995))." ></td>
	<td class="line x" title="94:225	The procedure used to identify conceptual and syntactic information, retrieves concepts within specific phrasal and clausal categories." ></td>
	<td class="line x" title="95:225	Once a response was processed, and concept tags were assigned, all phrasal and clausal categories were collapsed into a general phrasal category, XP, for the scoring process, as illustrated in (4), below." ></td>
	<td class="line x" title="96:225	There were some cases, however, where we had no choice but to include some single concepts, due to the limited lexico-syntactic patterning in the data." ></td>
	<td class="line x" title="97:225	4.1." ></td>
	<td class="line x" title="98:225	The Scoring Lexicon for the Police Item What we term the scoring lexicon can best be illustrated by Bergler's (1995) layered lexicon." ></td>
	<td class="line x" title="99:225	The underlying idea in Bergler's approach is that the lexicon has several layers which are modular, and new layers can be plugged in for different texts." ></td>
	<td class="line x" title="100:225	In this way, lexical entries can be linked appropriately to text-specific information." ></td>
	<td class="line x" title="101:225	In the layered lexicon approach, words are linked to definitions within some hierarchy." ></td>
	<td class="line x" title="102:225	Bergler's approach also has a meta-lexical layer which maps from syntactic patterns to semantic interpretation that does not affect the lexicon itself." ></td>
	<td class="line x" title="103:225	By comparison, our scoring lexicon, contains a list of base word forms (i.e. , concepts)." ></td>
	<td class="line x" title="104:225	3 The definitions associated with these concepts were typically metonyms that were specific to the domain of the item." ></td>
	<td class="line x" title="105:225	These metonym definitions were subordinate to the words they defined." ></td>
	<td class="line x" title="106:225	In the spirit of the layered lexicon, the definitions associated with the superordinate concepts are modular, and can be changed given new domains." ></td>
	<td class="line x" title="107:225	3Suffixation was removed so that part of speech did not interfere with conceptual generalizability." ></td>
	<td class="line x" title="108:225	24 For this study, metonyms for each concept were chosen from the entire set of single words over the whole training set, and specialized 2-word and 3-word terms (i.e. , domain-specific and domainindependent idioms) which were found in the training data." ></td>
	<td class="line x" title="109:225	The lexicon developed for this study was based on the training data from all rubric categories." ></td>
	<td class="line x" title="110:225	In (2), below, a sample from the lexicon is given." ></td>
	<td class="line x" title="111:225	Our concept grammars, described in Section 4.2, are in the spirit of Bergler's notion of a meta-lexical layer that provides a mapping between the syntax and semantics of individual responses." ></td>
	<td class="line x" title="112:225	In our lexicon, concepts are preceded by #." ></td>
	<td class="line x" title="113:225	Metonyms follow the concepts in a list." ></td>
	<td class="line x" title="114:225	Lexical entries not preceded by # are relevant words from the set of training responses, which are metonyms of concepts." ></td>
	<td class="line x" title="115:225	These entries will contain a pointer to a concept, indicated by '% <concept>'." ></td>
	<td class="line x" title="116:225	A sample of the lexicon is illustrated below." ></td>
	<td class="line x" title="117:225	(2) Sample from the Police Item Lexicon #BE'ITER \[ better good advance improve increase efficient modem well increase \] ADVANCE \[ %better \] 4.2 Concept Grammar Rules for the Police Item The concept grammar rule templates for mapping and classifying responses were built from the 172 training set responses in 32 categories." ></td>
	<td class="line x" title="118:225	4 The training data was parsed using the parser in Microsoft's Natural Language Processing Tool (see MS-NLP(1996) for a description of this tool)." ></td>
	<td class="line x" title="119:225	For this study, suffixes were removed by hand from the parsed data." ></td>
	<td class="line x" title="120:225	Based on the syntactic parses of these responses and the lexicon, a small concept grammar was manually built for each category which characterized responses by concepts and relevant structural information." ></td>
	<td class="line x" title="121:225	The phrasal constituents were unspecified." ></td>
	<td class="line x" title="122:225	Sample concept grammar rules are illustrated in (3)." ></td>
	<td class="line x" title="123:225	(3) Sample Concept Grammar Rules for Types of self-defense/safety a. XP: \[POLICE\],XP: \[BETTER,TRAIN\],XP: \[SAFETY\] b. XP:\[TRAIN \],XP: \[POLICE,SAFETY\], XP:\[BE'VI'ER,SAFETY\] c. XP: \[POLICE,BE'VrER,TRAIN\], XP:\[SAFETY,DANGER,SITUATION\] d. XP:\[SPECIALIST\],XP:\[TRAIN SAFETY\] 4.3 Processing Responses for Category Assignment Responses were, parsed, and then input into the phrasal node extraction program." ></td>
	<td class="line x" title="124:225	The program extracted words and terms in Noun Phrases (NP), Verb Phrases (VP), Prepositional Phrases (PP), Infinitive Clauses (INFCL), Subordinate Clauses (SUBCL), Adjective Phrases (ADJP) and Adverb Phrases (ADVP)." ></td>
	<td class="line x" title="125:225	All phrasal and clausal constituent nodes were then collapsed into a generalized 4Some categories were not considered in this study due to insufficient data." ></td>
	<td class="line x" title="126:225	25 representation, XP." ></td>
	<td class="line x" title="127:225	All single XPs and combinations of XPs were matched against the concept grammars for each content category to locate rule matches." ></td>
	<td class="line x" title="128:225	This procedure is illustrated below." ></td>
	<td class="line x" title="129:225	(4) a. Input: Cops are better trained in self-defense b. Tag Phrasal Nodes of Parsed Response: \[Cops=POLICE\]NP \[better=BETTER,trained=TRAIN\]VP \[self-defense=SAFETY\]PP c. Collapse Phrasal Nodes: XP: \[Cops=POLICE\] XP: \[better=BETTER,trained=TRAIN\] XP: \[self-defense=SAFETY\] d. Match Tagged Nodes to Concept Grammar Rules: XP: \[POLICE\], XP:\[BETTER,TRAIN\],XP:\[SAFETY\] 4.4 Does Manual Preprocessing of the Data Outweigh the Benefits of Automated Scoring?" ></td>
	<td class="line x" title="130:225	Since the preprocessing of this response data is done by hand, the total person-time must be considered in relation to how long it would take test developers to hand score a data set in a real-world application." ></td>
	<td class="line x" title="131:225	We must address the issue of whether or not a computer-based method would be efficient with regard to time and cost of scoring." ></td>
	<td class="line x" title="132:225	In this study, the manual creation of the lexicon and the concept grammar rules for this data set took two people approximately one week, or 40 hours." ></td>
	<td class="line x" title="133:225	Currently, we are developing a program to automate the generation of the concept grammars." ></td>
	<td class="line x" title="134:225	We expect that once this program is in place, our preprocessing time will be cut in half." ></td>
	<td class="line x" title="135:225	So, we estimate that it would take one person approximately 8 -10 hours to create the lexicon, and another 8 10 hours to do the preprocessing and post-processing required in conjunction with the automatic rule generation process currently being developed." ></td>
	<td class="line x" title="136:225	The F-H item is currently only a pilot item for the Graduate Record Examination (GRE), which administers approximately 28,000 examinees, yearly." ></td>
	<td class="line x" title="137:225	For the F-H item, each examinee can give up to 15 responses." ></td>
	<td class="line x" title="138:225	So, the maximum number of responses for this item over the year would be approximately 420,000." ></td>
	<td class="line x" title="139:225	Each examinee's response set would then typically be scored by two human graders." ></td>
	<td class="line x" title="140:225	It is difficult to estimate how long the manual scoring process would take in hours, but, presumably, it would take longer than the approximately 40 hours it took to build the lexicon and concept grammars." ></td>
	<td class="line x" title="141:225	Certainly, it would take longer than the 20 hours estimated, once the automatic rule generator is implemented." ></td>
	<td class="line x" title="142:225	Therefore, assuming that the accuracy of this method could be improved satisfactorily, automated scoring would appear to be a viable cost-saving and time-saving option." ></td>
	<td class="line x" title="143:225	26 ! 5.1 Initial Results One hundred and seventy-two responses were used for training." ></td>
	<td class="line x" title="144:225	These responses were used to build the lexicon and the concept grammar rules." ></td>
	<td class="line x" title="145:225	An additional, independent set of 206 test responses from 32 content categories was run through our prototype." ></td>
	<td class="line x" title="146:225	The following were the results." ></td>
	<td class="line x" title="147:225	Table 1: Results of Automatic Scoring of Responses Response Set Total Set of Responses (Training Set + Test Set) Test Set Only Coverage 92% (347/378) 87% (180/206) Accuracy 90% (313/347) 81% (146/180) 5.2 Error Accountability Most of the errors made in classifying the data can be accounted for by four error types: (a) lexical gap, (b) human grader misclassification, (c) concept-structure problem, (d) cross-classification." ></td>
	<td class="line x" title="148:225	The lexical gap error characterizes cases in which a response could not be classified because it was missing a concept tag, and, therefore, did not match a rule in the grammar." ></td>
	<td class="line x" title="149:225	In reviewing the lexical gap errors, we found that the words not recognized by the system were metonyms that did not exist in the training, and were not identified as synonyms in any of our available thesaurus or on-line dictionary sources." ></td>
	<td class="line x" title="150:225	For instance, in the response, 'Police are better skilled,' the phrase better skilled, should be equated to better trained, but this could not be done based on the training responses, or dictionary sources." ></td>
	<td class="line x" title="151:225	Forty percent of the errors were lexical gap errors." ></td>
	<td class="line x" title="152:225	The second problem was human grader misclassification which accounted for ! percent of the errors." ></td>
	<td class="line x" title="153:225	In these cases, it was clear that responses had been inadvertently misclassified, so the system either misclassified the response, also." ></td>
	<td class="line x" title="154:225	For example, the response, Officers are better trained and more experienced so they can avoid dangerous situations, was misclassified in Better trained police, general." ></td>
	<td class="line x" title="155:225	It is almost identical to most of the responses in the category Better intervention~crook counseling." ></td>
	<td class="line x" title="156:225	Our." ></td>
	<td class="line x" title="157:225	system, therefore, classified the response in Better intervention~crook counseling." ></td>
	<td class="line x" title="158:225	Concept-structure problems made up 30 percent of the errors." ></td>
	<td class="line x" title="159:225	These were cases in which a response could not be classified because its concept-structural patterning was different from all the concept grammar rules for all content categories." ></td>
	<td class="line x" title="160:225	The fourth error type accounted for 17 percent of the cases in which there was significant conceptual similarity between two categories, such that categofial cross-classification occurred." ></td>
	<td class="line x" title="161:225	5.3 Additional Results Using an Augmented Lexicon As discussed above, 40 percent of the errors could be accounted for by lexical gaps." ></td>
	<td class="line x" title="162:225	We hypothesized that our results would improve if more metonyms of existing concepts were added to the lexicon." ></td>
	<td class="line x" title="163:225	Therefore, we augmented the lexicon with metonyms that could be accessed from the 27 test data." ></td>
	<td class="line x" title="164:225	We reran the scoring program, using the augmented lexicon on the same set of data." ></td>
	<td class="line x" title="165:225	The results of this run were the following." ></td>
	<td class="line x" title="166:225	Table 2: Results from Automatic Scoring Using an Augmented Lexicon Response Set Total Set of Responses (Training Set + Test Set) Test Set Only Coverage 96% (364/378) Accuracy 96% (341/364) 93% (193/206) 93% (178/193) The improvement which occurred by augmenting the lexicon further supports our procedure for classifying responses." ></td>
	<td class="line x" title="167:225	Based on these results, we plan to explore ways to augment the lexicon without consulting the test set." ></td>
	<td class="line x" title="168:225	Furthermore, we will use the augmented lexicon from this second experiment to score a set of 1200 new test data." ></td>
	<td class="line x" title="169:225	5 6." ></td>
	<td class="line x" title="170:225	Conclusion Our results are encouraging and support the hypothesis that a lexical semantic approach can be usefully integrated into a system for scoring the free-response item described in this paper." ></td>
	<td class="line x" title="171:225	Essentially, the results show that given a small set of data which is partitioned into several meaning classifications, core meaning can be identified by concept-structure patterns." ></td>
	<td class="line x" title="172:225	It is crucial that a domain-specific lexicon is created to represent the concepts in the response set." ></td>
	<td class="line x" title="173:225	Therefore, the concepts in the lexicon must denote metonyms which can be derived from the training set." ></td>
	<td class="line x" title="174:225	Relevant synonyms of the metonyms can be added to expand the lexicon using dictionary and thesaurus sources." ></td>
	<td class="line x" title="175:225	Using a layered lexicon approach (Bergler (1995)) allows the words in the lexicon to be maintained, while the part of the entry denoting domain-specific meaning is modular and can be replaced." ></td>
	<td class="line x" title="176:225	The results of this case study illustrate that it is necessary to analyze content of responses based on the mapping between domain-specific concepts and the syntactic structure of a response." ></td>
	<td class="line x" title="177:225	As mentioned earlier in the paper, previous systems did not score responses accurately due to an inability to reliably capture response paraphrases." ></td>
	<td class="line x" title="178:225	These systems did not use structure or domainspecific lexicons in trying to analyze response content." ></td>
	<td class="line x" title="179:225	The results show that the largest number of erroneous classifications occurred due to lexical gaps." ></td>
	<td class="line x" title="180:225	Our second set of results shows that developing new methods to augment the lexicon would improve performance significantly." ></td>
	<td class="line x" title="181:225	In future experiments, we plan to score an independent set of response data from the same item, using the augmented lexicon, to test the generalizability of our prototype." ></td>
	<td class="line x" title="182:225	We realize that the results presented in this case study represent a relatively small data set." ></td>
	<td class="line x" title="183:225	These results are encouraging, however, with regard to using a lexical semantics approach for automatic content identification on small data sets." ></td>
	<td class="line x" title="184:225	5We did not use these 1200 test data in the initial study, since the set of 1200 has not been scored by test developers, so we could not measure agreement with regard to human scoring decisions." ></td>
	<td class="line x" title="185:225	However, we believe that by using the augmented lexicon, and our concept grammars to automatically score the 1200 independent data, we can get a 28 I References Bergler, Sabine; (1995)." ></td>
	<td class="line x" title="186:225	From Lexical Semantics to Text Analysis." ></td>
	<td class="line x" title="187:225	In Patrick Saint-Dizier and Evelyne Viegas (edso), Computational Lexical Semantics, Cambridge University Press,New York, NY." ></td>
	<td class="line x" title="188:225	Burstein, Jill C. and Randy M. Kaplan." ></td>
	<td class="line x" title="189:225	(1995)." ></td>
	<td class="line x" title="190:225	On the Application of Context to Natural Language Processing Applied to the Analysis of Test Responses." ></td>
	<td class="line x" title="191:225	Proceedings from the Workshop on Context in Natural Language Processing, IJCAI, Montreal, Canada." ></td>
	<td class="line x" title="192:225	Dorr, Bonnie, James Hendler, Scott Blanksteen and Bonnie Migdoloff." ></td>
	<td class="line x" title="193:225	(1995)." ></td>
	<td class="line x" title="194:225	On Beyond Syntax: Use of Lexical Conceptual Structure for Intelligent Tutoring." ></td>
	<td class="line x" title="195:225	In V. Melissa Holland, Jonathan Kaplan and Michelle Sams (Eds), Intelligent Language Tutors, Lawrence Erlbaum Publishers, Mahwah, NJ." ></td>
	<td class="line x" title="196:225	Gerstl, P." ></td>
	<td class="line x" title="197:225	(1991)." ></td>
	<td class="line x" title="198:225	A Model for the Interaction of Lexical and Non-Lexical Knowledge in the Determination of Word Meaning." ></td>
	<td class="line x" title="199:225	In J. Pustejovsky and S. Bergler (Eds), Lexical Semantics and Knowledge Representation, Springer-Verlag, New York, NY." ></td>
	<td class="line x" title="200:225	Holland, V. Me!issa." ></td>
	<td class="line x" title="201:225	(1994)." ></td>
	<td class="line x" title="202:225	Intelligent Tutors for Foreign Languages: How Parsers and Lexical Semantics Can Help Learners and Assess Learning." ></td>
	<td class="line x" title="203:225	In Randy M. Kaplan and Jill Burstein (Eds), Proceedings of the Educational Testing Service Conference on Natural Language Processing and Technology in Assessment and Education, Educational Testing Service, Princeton, NJ." ></td>
	<td class="line x" title="204:225	Jackendoff, R.S." ></td>
	<td class="line x" title="205:225	(1993)." ></td>
	<td class="line x" title="206:225	Semantics and Cognition." ></td>
	<td class="line x" title="207:225	MIT Press, Cambridge, MA." ></td>
	<td class="line x" title="208:225	Kaplan, Randy M. and Randy E. Bennett." ></td>
	<td class="line x" title="209:225	(1994)." ></td>
	<td class="line x" title="210:225	Using the Free-Response Scoring Tool To Automatically Score the Formulating-Hypothesis Item." ></td>
	<td class="line x" title="211:225	(RR-94-08)." ></td>
	<td class="line x" title="212:225	Princeton, NJ:Educational Testing Service." ></td>
	<td class="line x" title="213:225	MS-NLP." ></td>
	<td class="line x" title="214:225	(1996)." ></td>
	<td class="line x" title="215:225	http://research.microsoft.com/research/nlp." ></td>
	<td class="line x" title="216:225	Microsoft Corporation." ></td>
	<td class="line x" title="217:225	Redmond, WA." ></td>
	<td class="line x" title="218:225	Sager, N." ></td>
	<td class="line x" title="219:225	(1981)." ></td>
	<td class="line x" title="220:225	Natural Language Information Processing: A computer grammar of English and its applications, Addison-Wesley, Reading, MA." ></td>
	<td class="line x" title="221:225	Smadja, Frank." ></td>
	<td class="line x" title="222:225	(1993)." ></td>
	<td class="line x" title="223:225	Retrieving Collocations from Text: Xtract." ></td>
	<td class="line x" title="224:225	Computational Linguistics." ></td>
	<td class="line x" title="225:225	19(1), 143-177 ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="A97-1026
An Automatic Scoring System For Advanced Placement Biology Essays
Burstein, Jill;Wolff, Susanne;Lu, Chi;Kaplan, Randy M.;"></td>
	<td class="line x" title="1:198	An Automatic Scoring System For Advanced Placement Biology Essays Jill Burstein, Susanne Wolff, Chi Lu Educational Testing Service MS-11R Princeton, NJ 08541 e-mail: fourstein@ets, org Randy M. Kaplan Advanced Technology PECO Energy Philadelphia, PA Abstract This paper describes a prceXype for automatically scoring College Board Advanced Placement (AP) Biology essays." ></td>
	<td class="line x" title="2:198	I. The scoring technique used in this study was based on a previous method used to score sentence-length responses (Burstein, et al, 1996)." ></td>
	<td class="line x" title="3:198	One hundred training essays were used to build an example-based lexicon and concept granunars." ></td>
	<td class="line x" title="4:198	The prototype accesses information from the lexicon and concept grammars to score essays by assigning a classification of Excellent or Poor based on the number of points assigned during scoring." ></td>
	<td class="line x" title="5:198	Final computer-based essay scores are based on the system's recognition of conceptual information in the essays." ></td>
	<td class="line x" title="6:198	Conceptual analysis in essays is essential to provide a classification based on the essay content." ></td>
	<td class="line x" title="7:198	In addition, computergenerated information about essay content can be used to produce diagnostic feedback." ></td>
	<td class="line x" title="8:198	The set of essays used in this study had been scored by human raters." ></td>
	<td class="line x" title="9:198	The results reported in the paper show 94% agreement on exact or adjacent scores between human rater scores and computer-hased scores for 105 test essays." ></td>
	<td class="line x" title="10:198	The methods underlying this application could be used in a number of applications involving rapid semantic analysis of textual materials, especially with regard to scientific or other technical text." ></td>
	<td class="line x" title="11:198	INTRODUCTION To replace the conventional multiple questions on standardized examinations, choice ~Test items in this paper are copyrighted by Educational Testing Service (ETS)." ></td>
	<td class="line x" title="12:198	No further reproduction is permitted without written permission of ETS." ></td>
	<td class="line x" title="13:198	174 Educational Testing Service (ETS) is currently developing computer-based scoring tools for automatic scoring of natural language constructedresponses responses that are written, such as a short-answer or an essay." ></td>
	<td class="line x" title="14:198	The purpose of this work is to develop computer-based methods for scoring so that computer-administered natural language constructed-response items can be used on standardized tests and scored efficiently with regard to time and cost." ></td>
	<td class="line x" title="15:198	Until recently, ETS's automated scoring efforts were primarily devoted to the development of computer programs used to score short-answer constructed-responses of up to 15 words (Burstein and Kaplan, 1995 and Burstein et al. , 1996)." ></td>
	<td class="line x" title="16:198	In this study a classification of Excellent or Poor was automatically assigned to an AP Biology essay." ></td>
	<td class="line x" title="17:198	Our initial goal in this study was to develop a prototype scoring system that could reliably assign a classification of Excellent to a set of AP Biology essays." ></td>
	<td class="line x" title="18:198	For the evaluation of the scoring method, a small sample of Poor essays were also scored to compare the results." ></td>
	<td class="line x" title="19:198	2 Human rater scoring of AP Biology essays is based on a highly constrained scoring key, called a rubric, that specifies the criteria human raters use to assign scores to essays." ></td>
	<td class="line x" title="20:198	Accordingly, for the test question studied here, the criteria for point 2 The Poor classification is not an official AP classification." ></td>
	<td class="line x" title="21:198	It was used in this study to distinguish the Excellent essays with scores of 9 and l0 from essays with lower end scores in the 0 3 range." ></td>
	<td class="line x" title="22:198	assignment are highly constrained." ></td>
	<td class="line x" title="23:198	Essentially, the essay can be treated as a sequence of short-answer responses." ></td>
	<td class="line x" title="24:198	Given our preliminary successes with test questions that elicit multiple responses from examinees, similar scoring methods were applied for scoring AP Biology essay." ></td>
	<td class="line x" title="25:198	The results show 87% agreement for exact scores between human rater and computer scores, and 94% agreement for exact or adjacent scores between human rater and computer scores." ></td>
	<td class="line x" title="26:198	This work is also applicable for other types of assessment as well, such as for employee training courses in corporate and government settings." ></td>
	<td class="line x" title="27:198	Since the methods discussed in this paper describe techniques for analysis of semantic information in text, presumably this application could be extended to public informational settings, in which people might key in 'requests for information' in a number of domains." ></td>
	<td class="line x" title="28:198	In particular, these methods could be successfully applied to the analysis of natural language responses for highly constrained domains, such as exist in scientific or technical fields." ></td>
	<td class="line x" title="29:198	SYSTEM TRAINING One hundred Excellent essays from the original 200 essays were selected to train the scoring system." ></td>
	<td class="line x" title="30:198	The original 200 essays were divided into a training set and test set, selected arbitrarily from the lowest examinee identification number." ></td>
	<td class="line x" title="31:198	Only 85 of the original 100 in the test set were included in the study due to illegibility, or use of diagrams instead of text to respond to the question." ></td>
	<td class="line x" title="32:198	For convenience during training, and later, for scoring, essays were divided up by section, as specified in the scoring guide (see Figure 1), and stored in directories by essay section." ></td>
	<td class="line x" title="33:198	Specifically, the Part A's of the essays were stored in a separate directory, as were Part B's, and Part C's." ></td>
	<td class="line x" title="34:198	Examinees typically partitioned the essay into sections that corresponded to the scoring guide." ></td>
	<td class="line x" title="35:198	System training involved the following steps that are discussed in subsequent sections: a) manual lexicon development, b) automatic generation of concept-structure representation (CSR), c) manual creation of a computer-based rubric, d) manual CSR 'fine-tuning', e) automatic rule generation, and f) evaluation of training process." ></td>
	<td class="line x" title="36:198	Lexicon Development Example-based approaches to lexicon development have been shown to effectively exemplify word meaning within a domain (Richardson, et al. , 1993, and Tsutsumi 1992)." ></td>
	<td class="line x" title="37:198	It has been further pointed out by Wilks, et al, 1992, that word senses can be effectively captured on the basis of textual material, The lexic, on dwlopcd for this study used an example-based approach to compile a list of lexical items that characterized the content vocabulary used in the domain of the test question (i.e. , gel electrophoresis)." ></td>
	<td class="line x" title="38:198	The lexicon is composed of words and terms from the relevant vocabulary of the essays used for training." ></td>
	<td class="line x" title="39:198	To build the lexicon, all words and terms considered to contribute to the core meaning of each relevant sentence in an essay, were included in the lexicon." ></td>
	<td class="line x" title="40:198	The decision with regard to whether or not a sentence was relevant was based on information provided in the scoring guide (in Figure 1)." ></td>
	<td class="line x" title="41:198	For instance, in the sentence, 'Smaller DNA fragments mave faster than larger ones.', the terms Smaller, DNA, fragments, move, faster, larger are considered to be the most meaningful terms in the sentence." ></td>
	<td class="line x" title="42:198	This is based on the criteria for a correct response for the Rate/Size category in the scoring guide." ></td>
	<td class="line x" title="43:198	Each lexical entry contained a superordinate concept and an associated list of metonyms." ></td>
	<td class="line x" title="44:198	Metonyms are words or terms which are acceptable substitutions for a given word or term (Gerstl, 1991)." ></td>
	<td class="line x" title="45:198	Metonyms for concepts in the domain of this test question were selected from the example responses in the training data This paradigm was used to identify word similarity in the domain of the essays." ></td>
	<td class="line x" title="46:198	For instance, the scoring program needed to recognize that sentences, such as Smaller DNA fragments move faster than larger ones and The smaller segments of DNA will travel more quickly than the bi~.~er ones, contain alternate words with similar meanings in the test question domain." ></td>
	<td class="line x" title="47:198	To determine alternate words with similar meanings, metonyms for words, such as fragments and move were established in the 175 lexicon so that the system could identify which words had similar meanings in the test item domain." ></td>
	<td class="line x" title="48:198	The example lexical entries in (1) illustrate that the words fragment and segment are metonyms in this domain, as well as the words move and travel." ></td>
	<td class="line x" title="49:198	In (1), FRAGMENT and MOVE are the higher level lexical concepts." ></td>
	<td class="line x" title="50:198	The associated metonymsfor FRAGMENT and MOVE are in adjacent lists illustrated in (1)." ></td>
	<td class="line x" title="51:198	(1)." ></td>
	<td class="line x" title="52:198	Sample Lexical Entries wouM be digested only once, leaving 2 pieces.', and 'The DNA fragment wouM only have 2 segments,' the phrases DATA segment and DNA fragment are paraphrases of each other, and 2 pieces and 2 segments are paraphrases of each other." ></td>
	<td class="line x" title="53:198	These sentences are represented by the CSR in (2a) and in (2b)." ></td>
	<td class="line x" title="54:198	(2)a. NP: \[DNA,FRAGMENT\] NP: \[TWO,FRAGMENT\] FRAGMENT \[fragment particle segment\] MOVE \[ move travel pass pull repel attract\] In the final version of the CSR, phrasal constituents are reduced to a general XP node, as is illustrated in Concept-Structure Representations (CSR) Obviously, no two essays will be identical, and it is unlikely that two sentences in two different essays will be worded exactly alike." ></td>
	<td class="line x" title="55:198	Therefore, scoring systems must be able to recognize paraphrased information in sentences across essay responses To identify paraphrased information in sentences, the scoring system must be able to identify similar words in consistent syntactic patterns." ></td>
	<td class="line x" title="56:198	As, Montemagni and Vanderwende (1993) have also pointed out, structural patterns are more desirable than string patterns for capturing semantic information from text." ></td>
	<td class="line x" title="57:198	We have implemented a concept-extraction program for preprocessing of essay data that outputs conceptual information as it exists in the structure of a sentence." ></td>
	<td class="line x" title="58:198	The program reads in a parse tree generated by MicrosoR's Natural Language Processing Tools (MSNLP) for each sentence in an essay) The program substitutes words in the parse tree with superordinate concepts from the lexicon, and extracts the phrasal nodes containing these concepts." ></td>
	<td class="line x" title="59:198	(Words in the phrasal node which do not match a lexical concept are not included in the set of extracted phrasal nodes)." ></td>
	<td class="line x" title="60:198	The resulting structures are CSRs." ></td>
	<td class="line x" title="61:198	Each CSR represents a sentence according to conceptual content and phra~l constituent structure." ></td>
	<td class="line x" title="62:198	CSRs characterize paraphrased information in sentences." ></td>
	<td class="line x" title="63:198	For example, in the sentences 'The DNA segment (2)bXP: \[DNA,FRAGMENT\] XP: ITWO,FRAGMENTI Since phrasal category does not have to be specified, the use of a generalized XP node minimizes the number of required lexical entries, as well as the number of concept grammar rules needed for the scoring process." ></td>
	<td class="line x" title="64:198	The Computer Rubric Recall that a rubric is a scoring key." ></td>
	<td class="line x" title="65:198	Rubric categories are the criteria that determine a correct response." ></td>
	<td class="line x" title="66:198	A computer-based rubric was manually created for the purpose of classifying sentences in essays by rubric category during the automated scoring process." ></td>
	<td class="line x" title="67:198	Computer rubric categories are created for the bulleted categories listed in the human rater scoring guide illustrated in Figure 1." ></td>
	<td class="line x" title="68:198	3 See http://research.microsoR.com/research/nlp for information on MS-NLP." ></td>
	<td class="line x" title="69:198	176 Part A. Explain how the principles of gel electrophoresis allow for the separation of DNA fragments (4 point maximum)." ></td>
	<td class="line x" title="70:198	 Electricity  Elechical potential  Charge  Negatively charged fragments  Rate/Size  Smaller fragments move faster  Calibration." ></td>
	<td class="line x" title="71:198	DNA's used as markers/standards  Resolution  Concentration of gel  Apparatus  Use of wells, gel material Past B. Describe the results you would expect from electrophoretic separation of fragments from the following treatments of the DNA segment shown in the question." ></td>
	<td class="line x" title="72:198	(4 point maximum)." ></td>
	<td class="line x" title="73:198	 Treatment I  Describe 4 bands/fragments  Treatment IIDescribe 2 bands/l~agments  Treatment lllDescribe 5 bands/fragments  Treatment IVDescribe 1 band/fragment Part CI." ></td>
	<td class="line x" title="74:198	The mechanism of action ofr~'ietion enz3anes." ></td>
	<td class="line x" title="75:198	(4 point maximum)  Recognition  Binding of enzyme to target sequence  CuRing." ></td>
	<td class="line x" title="76:198	Enzyme cuts at every location  Alternate  Point about enzyme cutting at specific location  Detail Point." ></td>
	<td class="line x" title="77:198	May generate sticky ends Part C2: The different resultsif a mutation occurred at the recognition site for enzyme Y.  Change in I  1 band/fragment  Change in III4 bands/fragments  Alternate  Y no longer recognized and cut  Detail Point  Y site might become an X site Figure 1: Scoring Guide Excerpt Accordingly, the computer-rubric categories were the following." ></td>
	<td class="line x" title="78:198	For Part A, the categories were Electricity, Charge, Rate~size, Calibration, Resolution, and Apparatus." ></td>
	<td class="line x" title="79:198	For Part B the categories were, Treatment I, Treatment 2, Treatment 3, and Treatment IV." ></td>
	<td class="line x" title="80:198	For Part C1, the categories were: Recognition, Cutting, Alternate, and Detail Point." ></td>
	<td class="line x" title="81:198	For Part C2, the categories were Change in l, Change in II, Alternate, and Detail Point." ></td>
	<td class="line x" title="82:198	Each computer-rubric category exists as an electronic file and contains the related concept grammar rules used during the scoring process." ></td>
	<td class="line x" title="83:198	The concept grammar rules are described later in the paper." ></td>
	<td class="line x" title="84:198	Fine-Tuning CSRs CSRs were generated for all sentences in an essay." ></td>
	<td class="line x" title="85:198	During training, the CSRs of relevant sentences from the training set were placed into computerrubric category files." ></td>
	<td class="line x" title="86:198	Relevant sentences in essays were sentences identified in the scoring guide as containing information relevant to a rubric category." ></td>
	<td class="line x" title="87:198	For example, the representation for the sentence, 'The DNA fragment would only have 2 segments,' was placed in the computer rubric category file for Treatment II." ></td>
	<td class="line x" title="88:198	Typically, CSRs are generated with extraneous concepts that do not contribute to the core meaning of the response." ></td>
	<td class="line x" title="89:198	For the purpose of concept grammar rule generation, each CSR from the training data must contain only concepts which denote the core meaning of the sentence." ></td>
	<td class="line x" title="90:198	Extraneous concepts had to be removed before the rule generation process, so that the conceptstructure information in the concept grammar rules would be precise." ></td>
	<td class="line x" title="91:198	The process of removing extraneous concepts from the CSRs is currently done manually." ></td>
	<td class="line x" title="92:198	For this study, all concepts in the CSR that were considered to be extraneous to the core meaning of the sentence were removed by hand." ></td>
	<td class="line x" title="93:198	For example, in the sentence, The DNA segment would be digested only once, leaving 2 pieces, the CSR in (3) was generated." ></td>
	<td class="line x" title="94:198	For Treatment \]I, the scoring guide indicates that if the sentence makes a reference to 2 fragments that it should receive one point." ></td>
	<td class="line x" title="95:198	(The word, piece, is a metonym for the concept, fragment, so these two words may be used interchangably)." ></td>
	<td class="line x" title="96:198	The CSR in (3) was generated by the concept-extraction program." ></td>
	<td class="line x" title="97:198	The CSR in (4) (in which XP:\[DNA,FRAGMENT\] was removed) illustrates the fine-tuned version of the CSR in (3)." ></td>
	<td class="line x" title="98:198	The CSR in (4) was then used for the rule generation process, described in the next section." ></td>
	<td class="line x" title="99:198	177 (3) XP:\[DNA,FRAGMENT\] XP:\[TWO,FRAGMENT\] (4) XP:\[TWO,FRAGMENT\] Concept Grammar Rule Generation At this point in the process, each computer rubric categow is an electronic file which contains finetuned, CSRs." ></td>
	<td class="line x" title="100:198	The CSRs in the computer rubric categories exemplify the information required to receive credit for a sentence in a response." ></td>
	<td class="line x" title="101:198	We have developed a program that automatically generates rules from CSP.s by generating permutations of each CSR The example rules in (5) were generated from the CSR in (4)." ></td>
	<td class="line x" title="102:198	The rules in (5) were used during automated scoring (described in the following section)." ></td>
	<td class="line x" title="103:198	which looks for matches between CSRs and/or subsets of CSRs, and concept grammar rules in rubric categories associated with each essay part." ></td>
	<td class="line x" title="104:198	Recall that CSRs often have extraneous concepts that do not contribute to the core meaning of the sentence." ></td>
	<td class="line x" title="105:198	Therefore, the scoring program looks for matches between concept grammar rules and subsets of CSRs, if no direct match can be found for the complete set of concepts in a CSR." ></td>
	<td class="line x" title="106:198	The scoring program assigns points to an essay as rule matches are found, according to the scoring guide (see Figure 1)." ></td>
	<td class="line x" title="107:198	A total number of points is assigned to the essay after the program has looked at all sentences in an essay." ></td>
	<td class="line x" title="108:198	Essays receiving a total of at least 9 points are classified as Excellent, essays with 3 points or less are classified as Poor, and essays with 4 8 points are classified as 'Not Excellent'." ></td>
	<td class="line x" title="109:198	The example output in Appendix 1 illustrates matches found between sentences in the essay and the rubric rules from an Excellent essay." ></td>
	<td class="line x" title="110:198	(5)a. XP:\[TWO, FRAGMENT\] b. XP:\[FRAGMENT,TWO\] The trade-off for generating rules automatically in this manner is rule overgeneration, but this does not appear to be problematic for the automated scoring process." ></td>
	<td class="line x" title="111:198	Automated rule generation is significantly faster and more accurate than writing the rules by hand." ></td>
	<td class="line x" title="112:198	We estimate that it would have taken two people about two weeks of full-time work to manually create the rules." ></td>
	<td class="line x" title="113:198	Inevitably, there would have been typographical errors and other kinds of 'human error'." ></td>
	<td class="line x" title="114:198	It takes approximately 3 minutes to automatically generate the rules." ></td>
	<td class="line x" title="115:198	AUTOMATED SCORING The 85 remaining Excellent test essays and a set of 20 Poor essays used in this study were scored." ></td>
	<td class="line x" title="116:198	First, all sentences in Parts A, B and C of each essay were parsed using MSNLP." ></td>
	<td class="line x" title="117:198	Next, inflectional suffixes were automatically removed from the words in the parsed sentences, since inflectional suffixed forms are not included in the lexicon." ></td>
	<td class="line x" title="118:198	CSRs were automatically generated for all sentences in each essay." ></td>
	<td class="line x" title="119:198	For each part of the essay, the scoring program uses a searching algorithm RESULTS Table 1 shows the results of using the automatic scoring prototype to score 85 Excellent test essays, and 20 Poor test essays." ></td>
	<td class="line x" title="120:198	Coverage (Cov) illustrates how many essays were assigned a score." ></td>
	<td class="line x" title="121:198	Accuracy (Acc) indicates percentage of agreement between the computer-based score and the human rater score." ></td>
	<td class="line x" title="122:198	Accuracy within 1 (w/i 1) or 2 points (w/i 2) shows the amount of agreement between the computer scores and human raters scores, within 1 or 2 points of human rater scores, respectively." ></td>
	<td class="line x" title="123:198	For Excellent essays computer-based scores would be 1 or 2 points below the 9 point minimum, and for Poor essays, they would be 1 or 2 points above the 3 point maximum." ></td>
	<td class="line x" title="124:198	Data Set Excellent Poor Total Cov Ace 100% 89% 100% 75% 100% 87% Acc w/i 1 Acc w/i 2 95% 100% 90% 95% 94% 96% Table 1: Results of Automatic Scoring Prototype 178 ERROR ANALYSIS An error analysis of the data indicated the following two error categories that reflected a methodological problem: a) Lexicon Deficiency and b) Concept Grammar Rule Deficiency." ></td>
	<td class="line x" title="125:198	These error categories are discussed briefly below." ></td>
	<td class="line x" title="126:198	Both error types could be resolved in future research." ></td>
	<td class="line x" title="127:198	Scoring errors can be linked to data entry errors, morphological stripping errors, parser errors, and erroneous rules generated due to misinterpretations of the scoring guide." ></td>
	<td class="line x" title="128:198	These errors, however, are peripheral to the underlying methods applied in this study." ></td>
	<td class="line x" title="129:198	Lexical Deficiency Recall that the lexicon in this study was built from relevant vocabulary in the set of 100 training essays." ></td>
	<td class="line x" title="130:198	Therefore, vocabulary which occurs in the test data, but not in the training data was ignored during the process of concept-extraction." ></td>
	<td class="line x" title="131:198	This yielded incomplete CSRs, and degraded scoring resulted." ></td>
	<td class="line x" title="132:198	For instance, while the core concept of the commonly occurring phrase one band is more often than not expressed as one band, or one fragment, other equivalent expressions existed in the test data some of which did not occur in the training data." ></td>
	<td class="line x" title="133:198	From our 185 essays we extracted possible substitutions of the term one fragment." ></td>
	<td class="line x" title="134:198	These are: one spot, one band, one inclusive line, one probe, one group, one bond, one segment, one length of nucleotides, one marking, one strand, one solid clump, in one piece, one bar, one mass, one stripe, one bar, and one blot." ></td>
	<td class="line x" title="135:198	An even larger sample of essays could contain more alternate word or phrase substitutions than those are listed here." ></td>
	<td class="line x" title="136:198	Perhaps, increased coverage for the test data can be achieved ff additional standard dictionary sources are used to create a lexicon, in conjunction with the example based method used in this study (Richardson et al. , 1993)." ></td>
	<td class="line x" title="137:198	Corpus-based techniques using domain-specific texts (e.g. , Biology textbooks) might also be helpful (Church and Hanks, 1990)." ></td>
	<td class="line x" title="138:198	Concept Grammar Rule Deficiency In our error analysis, we found cases in which information in a test essay was expressed in a novel way that is not represented in the set of concept grammar rules." ></td>
	<td class="line x" title="139:198	In these cases, essay scores were degraded." ></td>
	<td class="line x" title="140:198	For example, the sentence, 'The action of this mutation would nullify the effect of the site, so the enzyme Y would not affect the site of the mutation." ></td>
	<td class="line x" title="141:198	' is expressed uniquely, as compared to its paraphrases in the training set." ></td>
	<td class="line x" title="142:198	This response says in a somewhat roundabout way that due to the mutation, the enzyme will not recognize the site and will not cut the DNA at this point." ></td>
	<td class="line x" title="143:198	No rule was found to match the CSR generated for this test response." ></td>
	<td class="line x" title="144:198	SUMMARY AND CONCLUSIONS This prototype scoring system for AP Biology essays successfully scored the Excellent and Poor essays with 87% exact agreement with human grader scores." ></td>
	<td class="line x" title="145:198	For the same set of essays, there was 94% agreement between the computer scores and human rater scores for exact or adjacent scores." ></td>
	<td class="line x" title="146:198	The preprocessing steps required for automated scoring are mostly automated." ></td>
	<td class="line oc" title="147:198	Manual processes, such as lexicon development could be automated in the future using standard contextbased, word distribution methods (Smadja, 1993), or other corpus-based techniques." ></td>
	<td class="line x" title="148:198	The error analysis from this study suggests that dictionarybased methods, combined with our current example-based approach, might effectively help to expand the lexicon)." ></td>
	<td class="line x" title="149:198	Such methods could broaden the lexicon and reduce the dependencies on training data vocabulary." ></td>
	<td class="line x" title="150:198	The automation of the fine-tuned CSRs will require more research." ></td>
	<td class="line x" title="151:198	A fully automated process would be optimal with regard to time and cost savings." ></td>
	<td class="line x" title="152:198	Work at the discourse level will have to be done to deal with more sophisticated responses which are currently treated as falling outside of the norm." ></td>
	<td class="line x" title="153:198	Perhaps the most attractive feature of this system in a testing environment is that it is defensible." ></td>
	<td class="line x" title="154:198	The representation used in the system denotes the content of essay responses based on lexical meanings and their relationship to syntactic structure." ></td>
	<td class="line x" title="155:198	The computer-based scores reflect the computer-based analysis of the response content, and how it compares to the scoring guide developed by human experts." ></td>
	<td class="line x" title="156:198	Information generated by the system which denotes response 179 content can be used to generate useful diagnostic feedback to examinees." ></td>
	<td class="line x" title="157:198	Since our methods explicitly analyze the content of text, these or similar methods could be applied in a variety of testing, training or information retrieval tasks." ></td>
	<td class="line x" title="158:198	For instance, these natural language processing techniques could be used for World Wide Web-based queries, especially with regard to scientific subject matter or other material producing constrained natural language text." ></td>
	<td class="line x" title="159:198	Richardson, Stephen D. , Lucy Vandervende, and William Dolan." ></td>
	<td class="line x" title="160:198	(1993)." ></td>
	<td class="line x" title="161:198	Combining Dictionary-Based and Example-Based Methods for Natural Language Analysis." ></td>
	<td class="line x" title="162:198	(MSR-TR-93-08)." ></td>
	<td class="line x" title="163:198	Redmond, WA:Microsofl Corporation." ></td>
	<td class="line xc" title="164:198	Smadja,Frank.(1993)." ></td>
	<td class="line x" title="165:198	Retrieving Collocations fromText:Xtract." ></td>
	<td class="line x" title="166:198	Computational Linguistics." ></td>
	<td class="line x" title="167:198	19(1), 143-177." ></td>
	<td class="line x" title="168:198	ACKNOWLEDGMENTS We are grateful to the College Board for support of this project." ></td>
	<td class="line x" title="169:198	We are thankful to Altamese Jackenthal for her contributions to this project." ></td>
	<td class="line x" title="170:198	We are also grateful to Mary Dee Harris and two anonymous reviewers for helpful comments and suggestions on earlier versions of this paper." ></td>
	<td class="line x" title="171:198	180 Appendix 1: Sample Rule Matches for a Scored Essay Part A: 'The cleaved DNA is then placed in a gel electrophoresis box that has a positlve and a negative end to it'." ></td>
	<td class="line x" title="172:198	Rubric category : CHARGE Rubric Rule:XP: \[DNA\],XP: \[NEGATIVE\] 'The lonEer, heavier bands would move the least and the smaller lighter bands would move the most and farther from the starting point'." ></td>
	<td class="line x" title="173:198	Rubric category: RATE/SIZE Rubric Rule:XP: \[LARGE SIZE\],XP:\[MOVE,LESS\] Part B: 'If the DNA was digested with only enzyme X then there would be 4_ separate bands that would develop'." ></td>
	<td class="line x" title="174:198	Rubric category :Treatment I Rubric Rule:XP:\[FOUR\] 'If the DNA was digested only with enzyme Y then two \[raRments or RFLP's would be visible'." ></td>
	<td class="line x" title="175:198	Rubric Category: Treatment II Rubric Rule:XP:\[TWO, FRAGMENT\] 'If the DNA was digested with both the X and the Y enzyme then there would be 5 RFLP's of 400 base pairs, 500 base pairs, 1,200 base pairs, 1,300 b.p and 1,500 b.p'." ></td>
	<td class="line x" title="176:198	Rubric category : Treatment III ~ubric Rule: XP:\[FIVE,FRAGMENT\] 'If the DNA was undigested then we would find no t~'LP's and, as a result, there would be no bandin that would occur'." ></td>
	<td class="line x" title="177:198	Rubric category: Treatment IV Rubric Rule:XP:\[NOT,FRAGMENT\] Parts CI m~d C2 'Restriction enzymes are types of proteins which recognize certain recognition sites along the DNA sequence and cleave the DNA at that end'." ></td>
	<td class="line x" title="178:198	Rubric category RECOGNITION Rule:XP:\[CUT, DNA\] 'Therefore, there would be no cut at that location and no RbT_~ produced at the Y recognition site'." ></td>
	<td class="line x" title="179:198	Rubric Category Rule:XP:\[NOT\],XP:\[CUT\],XP:\[SITE\] References Burstein, Jill C. , Randy M. Kaplan, Susanne Wolff and Chi Lu." ></td>
	<td class="line x" title="180:198	(1996)." ></td>
	<td class="line x" title="181:198	Using Lexical Semantic Techniques to Classify Free Responses." ></td>
	<td class="line x" title="182:198	Proceedings from the SIGLEX96 Workshop, ACL, University of California, Santa Cruz." ></td>
	<td class="line x" title="183:198	Tsutsumi,T." ></td>
	<td class="line x" title="184:198	(1992) Word Sense Disambiguation by Examples." ></td>
	<td class="line x" title="185:198	In K. Jensen, G. Heidorn and S. Richardson (Eds), Natural Language Processing: the PLNLP Approach, Kluwer Academic Publishers, Boston, MA." ></td>
	<td class="line x" title="186:198	Wilks, Y. , D. Fass, C. Guo, J. McDonald, T. Plate, and B. Slator." ></td>
	<td class="line x" title="187:198	(1992)." ></td>
	<td class="line x" title="188:198	Providing Machine Tractable Dictionary Tools." ></td>
	<td class="line x" title="189:198	In J. Pustejovsky (Ed), Semantics and the Lexicon, Kluwer Academic Publishers, Boston, MA." ></td>
	<td class="line x" title="190:198	Gerstl, P." ></td>
	<td class="line x" title="191:198	(1991)." ></td>
	<td class="line x" title="192:198	A Model for the Interaction of Lexical and Non-Lexical Knowledge in the Determination of Word Meaning." ></td>
	<td class="line x" title="193:198	In J. Pustejovsky and S. Bergler (Eds), Lexical Semantics and Knowledge Representation, Springer-Verlag, New York, NY." ></td>
	<td class="line x" title="194:198	Church, K and P. Hanks." ></td>
	<td class="line x" title="195:198	Word Association Norms, Mutual Information and Lexicography." ></td>
	<td class="line x" title="196:198	Computational Linguistics, 16(1), 22-29." ></td>
	<td class="line x" title="197:198	Montemagni, Simonetta and Lucy Vanderwende (1993)." ></td>
	<td class="line x" title="198:198	'Structural Patterns versus String Patterns for Extracting Semantic Information from Dictionaries,' In K. Jensen, G. Heidorn and S. Richardson (Eds), Natural Language Processing: the PLNLP Approach, Kluwer Academic Publishers, Boston, MA." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="A97-1045
Construction And Visualization Of Key Term Hierarchies
Zhou, Joe F.;Tanner, Troy;"></td>
	<td class="line x" title="1:100	Construction and Visualization of Key Term Hierarchies Joe Zhou and Troy Tanner LEXIS-NEXIS, a Division of Reed Elsevier 9555 Springboro Pike Miamisburg, OH 45342 {joez, tit} @ lexis-nexis.com Abstract This paper presents a prototype system for key term manipulation and visualization in a real-world commercial environment." ></td>
	<td class="line x" title="2:100	The system consists of two components." ></td>
	<td class="line x" title="3:100	A preprocessor generates a set of key terms from a text dataset which represents a specific topic." ></td>
	<td class="line x" title="4:100	The generated key terms are organized in a hierarchical structure and fed into a graphic user interface (GUI)." ></td>
	<td class="line x" title="5:100	The friendly and interactive GUI toolkit allows the user to visualize the key terms in context and explore the content of the original dataset." ></td>
	<td class="line x" title="6:100	1." ></td>
	<td class="line x" title="7:100	INTRODUCTION As the amount of on-line text grows at an exponential rate, developing useful text analysis techniques and tools to access information content from various electronic sources is becoming increasingly important." ></td>
	<td class="line x" title="8:100	In this paper we present an applied research prototype system that intends to accomplish two major tasks." ></td>
	<td class="line x" title="9:100	First, a set of key terms, ranging from single word terms to four word terms, are automatically generated and organized in a hierarchical structure out of a text dataset which represents a specific topic." ></td>
	<td class="line x" title="10:100	Second, a graphic user interface (GUI) is established that provides the domain expert or the user with an interactive environment to visualize the key term hierarchy in the context of the original dataset." ></td>
	<td class="line x" title="11:100	2." ></td>
	<td class="line x" title="12:100	SYSTEM DESCRIPTION The ultimate goal of this prototype system is to offer an automated toolkit which allows the domain expert or the user to visualize and examine key terms in a large information collection." ></td>
	<td class="line x" title="13:100	Such a toolkit has proven to be useful in a number of real applications." ></td>
	<td class="line x" title="14:100	For example, it has helped us reduce the time and manual effort needed to develop and maintain our on-line document indexing and classification schemes." ></td>
	<td class="line x" title="15:100	The system consists of two components: a preprocessing component for the automatic construction of key terms and the front-end component for userguided graphic interface." ></td>
	<td class="line x" title="16:100	2.1 Automatic Generation of Key Terms Automatically identifying meaningful terms from naturally running texts has been an important task for information technologists." ></td>
	<td class="line x" title="17:100	It is widely believed that a set of good terms can be used to express the content of the document." ></td>
	<td class="line x" title="18:100	By capturing a set of good terms, for example, relevant documents can be searched and retrieved from a large document collection." ></td>
	<td class="line x" title="19:100	Though what constitutes a good term still remains to be answered, we know that a good term can be a word stem, a single word, a multiple word term (a phrase), or simply a syntactic unit." ></td>
	<td class="line x" title="20:100	Various existing and workable term extraction tools are either statistically driven, or linguistically oriented, or some hybrid of the two." ></td>
	<td class="line x" title="21:100	They all target frequently co-occurring words in running text." ></td>
	<td class="line x" title="22:100	The earlier work of Choueka (1988) proposed a pure frequency approach in which only quantitative selection criteria were established and applied." ></td>
	<td class="line x" title="23:100	Church and Hanks (1990) introduced a statistical measurement called mutual information for extracting strongly associated or collocated words." ></td>
	<td class="line pc" title="24:100	Tools like Xtract (Smadja 1993) were based on the work of Church and others, but made a step forward by incorporating various statistical measurements like z-score and variance of distribution, as well as shallow linguistic techniques like part-of-speech tagging and lemmatization of input data and partial parsing of raw output." ></td>
	<td class="line x" title="25:100	Exemplary linguistic approaches can be found in the work by Str-zalkowsky (1993) where a fast and accurate syntactic parser is the prerequisite for the selection of significant phrasal terms." ></td>
	<td class="line x" title="26:100	Different applications aim at different types of key terms." ></td>
	<td class="line x" title="27:100	For the purpose of generating key terms for our prototype system, we have adopted a =learn data from data' approach." ></td>
	<td class="line x" title="28:100	The novelty of this 307 approach lies in the automatic comparison of two sample datasets, a topic focused dataset based on a predefined topic and a larger and more general base dataset." ></td>
	<td class="line x" title="29:100	The focused dataset is created by the domain expert either through a submission of an on-line search or through a compilation of documents from a specific source." ></td>
	<td class="line x" title="30:100	The construction of the corresponding base dataset is performed by pulling documents out of a number of sources, such as news wires, newspapers, magazines and legal databases." ></td>
	<td class="line x" title="31:100	The intention is to make the resulted corpora cover a much greater variety of topics or domain subjects than the focused dataset." ></td>
	<td class="line x" title="32:100	To identify interesting word patterns in both samples a set of statistical measures are applied." ></td>
	<td class="line x" title="33:100	The identification of single word terms is based on the variation of a t-test." ></td>
	<td class="line x" title="34:100	Two-word terms are captured through the computation of mutual information (Church et al. 1991), and an extension of mutual information assists in extracting three-word and four-word terms." ></td>
	<td class="line x" title="35:100	Once the significant terms of these four types are identified, a comparison algorithm is applied to differentiate terms across the two samples." ></td>
	<td class="line x" title="36:100	If significant changes in the values of certain statistical variables are detected, associated terms are selected from the focused sample and included in the final generated lists." ></td>
	<td class="line x" title="37:100	(For a complete description of the algorithm and preliminary experiments, please refer to Zhou and Dapkus 1995)." ></td>
	<td class="line x" title="38:100	2.2 Graphic User Interface (GUI) We view our prototype system as a means to achieve information visualization." ></td>
	<td class="line x" title="39:100	Analogous to scientific visualization that allows scientists to make sense out of intellectually large data collections, information visualization aims at organizing large information spaces so that information technologists can visualize what is out there and how various parts are related to each other (Robertson et al. 1991)." ></td>
	<td class="line x" title="40:100	The guiding principle for building the GUI component of our prototype system is to automate the manual process of capturing information content out of large document collections." ></td>
	<td class="line x" title="41:100	2.2.1 General Presentation The design of the GUI component relies on a number of well understood elements which include a suggestive graphic design and a direct manipulation metaphor to achieve an easy-to-learn user interface." ></td>
	<td class="line x" title="42:100	The layout of the graphic design is intended to facilitate the quick comprehension of the displayed information." ></td>
	<td class="line x" title="43:100	The GUI component is divided into two main areas, one for interacting with key terms structures and one for browsing targeted document collections." ></td>
	<td class="line x" title="44:100	The following descriptions should be viewed together with the appropriate figures of the GUI component." ></td>
	<td class="line x" title="45:100	Figure 1, attached at the end of the paper, represents the overall GUI picture." ></td>
	<td class="line x" title="46:100	Figures 2 and 3 capture the area where the interaction with the key term structures occurs." ></td>
	<td class="line x" title="47:100	Figures 4 and 5 present the area for document browsing and key terms selection." ></td>
	<td class="line x" title="48:100	The topic illustrated in the figures is the legal topic =Medical Malpractice'." ></td>
	<td class="line x" title="49:100	2.2.2 Term Access Mechanism The left area of the GUI component (see figures 2 and 3) is devoted to selecting, retrieving and operating on the key terms generated by the preprocessing component of the prototype system." ></td>
	<td class="line x" title="50:100	As can be seen, the key terms, ranging from single word terms to four word terms, are organized in a tree structure." ></td>
	<td class="line x" title="51:100	The tree is a two dimensional visualization of the term hierarchy." ></td>
	<td class="line x" title="52:100	Single word terms are represented as root nodes and multiple word terms can be positioned uniformly below the parent node in the term hierarchy." ></td>
	<td class="line x" title="53:100	The goal of the visualization is to present the key term lists in such a way that a high percentage of the hierarchy is visible with minimal scrolling." ></td>
	<td class="line x" title="54:100	Figure 2 308 The user interaction is structured around term retrieval and navigation as the top level user interactions." ></td>
	<td class="line x" title="55:100	The retrieval of the key terms is treated as an iterative process in which the user may select single world terms from the term hierarchy and navigate to multiple word terms accordingly." ></td>
	<td class="line x" title="56:100	The user begins term navigation by selecting from a list of available topics." ></td>
	<td class="line x" title="57:100	In this case, the legal topic 'Medical Malpractice' (i.e. , medmal3) is selected (see figure 2)." ></td>
	<td class="line x" title="58:100	Often data structures are organized linearly by some metric." ></td>
	<td class="line x" title="59:100	Frequency of key term usage is the metric used to organize and partition the term hierarchy in an ascending numerical order." ></td>
	<td class="line x" title="60:100	The partitioning is necessary as it is difficult to accommodate the large ratio of the term hierarchy on the screen." ></td>
	<td class="line x" title="61:100	Currently, each partition contains 100 root nodes (or folders), representing single word terms." ></td>
	<td class="line x" title="62:100	Once a partition has been selected, the corresponding document collection is loaded into the document browser." ></td>
	<td class="line x" title="63:100	The browser provides the user with the ability to quickly navigate through the document collection to locate relevant key terms." ></td>
	<td class="line x" title="64:100	example, when =malpractice' is selected as the root key term, a list of multiple word terms will be displayed including multiple key terms such as 'medical malpractice', 'malpractice cases', 'medical malpractice action', 'medical malpractice claims', 'limitations for medical malpractice', etc.(see figure 3) Functionality to shrink and collapse subtrees is also in place." ></td>
	<td class="line x" title="66:100	When a term is selected from the tree, a corresponding term lookup is conducted on the document collection to locate the selected term within the currently displayed document." ></td>
	<td class="line x" title="67:100	Documents representing the four highest frequencies for the selected term will be displayed first." ></td>
	<td class="line x" title="68:100	Upon location the selected term is always highlighted within the document browser." ></td>
	<td class="line x" title="69:100	2.2.3 Document Browsing Mechanism The right area of the GUI component (see figures 4 and 5) is occupied by the document browser." ></td>
	<td class="line x" title="70:100	The design of the document browser is intended to provide an easy-to-learn interface for the management and manipulation of the document collection." ></td>
	<td class="line x" title="71:100	There are three subwindows: the document identifier window, the document window and the navigation window." ></td>
	<td class="line x" title="72:100	The document identifier window identifies the document that is currently displayed in the document window." ></td>
	<td class="line x" title="73:100	It shows the document id and the total frequency of the selected key term in the document collection." ></td>
	<td class="line x" title="74:100	The document window provides a view of the content of the targeted document (see figure 4)." ></td>
	<td class="line x" title="75:100	Figure 3 The primary interaction with the key term hierarchy is accomplished by direct manipulation of the tree visualization." ></td>
	<td class="line x" title="76:100	The user can select individual nodes in the tree structure by pointing and clicking the corresponding folders." ></td>
	<td class="line x" title="77:100	When selecting nodes with children, the tree will expand, resulting in the display of multiple word terms of the root key term." ></td>
	<td class="line x" title="78:100	For Figure 4 309 The user can move through the document by making use of the scroll bar, document buttons in the navigation window, or by dragging the mouse up and down while depressing the middle mouse button." ></td>
	<td class="line x" title="79:100	The user can copy relevant key terms to a holding area by selecting 'Edit' from the menubar." ></td>
	<td class="line x" title="80:100	The user is presented with a popup dialog for importing the selected key terms (see figure 5)." ></td>
	<td class="line x" title="81:100	The navigation window enables the user to navigate through the documents to view the selected key terms in context." ></td>
	<td class="line x" title="82:100	In addition, the user is provialed with information regarding term frequencies and term relevance ranking scores." ></td>
	<td class="line x" title="83:100	Figure 5 2.2.4 Implementation The GUI component described above is implemented using the C++ programing language and the OSF Motif graphical user interface toolkit." ></td>
	<td class="line x" title="84:100	The user interface consists of a small set of classes that play various roles in the overall architecture." ></td>
	<td class="line x" title="85:100	The two major objects of the user interface interaction model are the ListTree and the Document Store objects." ></td>
	<td class="line x" title="86:100	ListTree is the primary class for implementing the tree visualization." ></td>
	<td class="line x" title="87:100	Operations for growing, shrinking and manipulating the tree visualization have been implemented." ></td>
	<td class="line x" title="88:100	Document Store provides the interface to document collections." ></td>
	<td class="line x" title="89:100	In particular, a document store provides operations to create, modify and navigate document collections." ></td>
	<td class="line x" title="90:100	3." ></td>
	<td class="line x" title="91:100	RESULTS OF USABILITY TESTING The prototype system, despite its prototype mode, has proven to be useful and applicable in the commercial business environment." ></td>
	<td class="line x" title="92:100	Since the system is in place, we have conducted a series of usability testing within our company." ></td>
	<td class="line x" title="93:100	The preliminary results indicate that the system can provide internal specialized library developers, as well as subject indexing domain experts with an ideal automated toolkit to select and examine significant terms from a sample dataset." ></td>
	<td class="line x" title="94:100	A number of general topics have been tested for developing specialized libraries for our on-line search system." ></td>
	<td class="line x" title="95:100	These include four legal topics =State Tax ~, =Medical Malpractice', =Uniform Commercial Code', and =Energy ~, and three news topics =Campaign', =Legislature', and =Executives'." ></td>
	<td class="line x" title="96:100	Specific subject indexing topics that have been tested are =Advertising Expenditure', =lntranet', =Job interview' and =Mutual fund'." ></td>
	<td class="line x" title="97:100	Two sets of questionnaires were filled out by the domain experts who participated in the usability testing." ></td>
	<td class="line x" title="98:100	The overall ranking for the prototype system falls between 'somewhat useful' to =very useful', depending on the topics." ></td>
	<td class="line x" title="99:100	They pointed out that the system is particularly helpful when dealing with a completely new or unfamiliar topic." ></td>
	<td class="line x" title="100:100	It helps spot significant terms which would normally be missed and objectively examine the significance level of certain fuzzy and ambiguous terms." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="A97-1050
Semi-Automatic Acquisition Of Domain-Specific Translation Lexicons
Resnik, Philip;Melamed, I. Dan;"></td>
	<td class="line x" title="1:291	Semi-Automatic Acquisition of Domain-Specific Translation Lexicons Philip Resnik Dept. of Linguistics and UMIACS University of Maryland College Park, MD 20742 USA resnik~umiacs, umd." ></td>
	<td class="line x" title="2:291	edu I. Dan Melamed Dept. of Computer and Information Science University of Pennsylvania Philadelphia, PA 19104 USA melamedunagi, cis." ></td>
	<td class="line x" title="3:291	upenn, edu Abstract We investigate the utility of an algorithm for translation lexicon acquisition (SABLE), used previously on a very large corpus to acquire general translation lexicons, when that algorithm is applied to a much smaller corpus to produce candidates for domain-specific translation lexicons." ></td>
	<td class="line x" title="4:291	1 Introduction Reliable translation lexicons are useful in many applications, such as cross-language text retrieval." ></td>
	<td class="line x" title="5:291	Although general purpose machine readable bilingual dictionaries are sometimes available, and although some methods for acquiring translation lexicons automatically from large corpora have been proposed, less attention has been paid to the problem of acquiring bilingual terminology specific to a domain, especially given domain-specific parallel corpora of only limited size." ></td>
	<td class="line x" title="6:291	In this paper, we investigate the utility of an algorithm for translation lexicon acquisition (Melamed, 1997), used previously on a very large corpus to acquire general translation lexicons, when that algorithm is applied to a much smaller corpus to produce candidates for domain-specific translation lexicons." ></td>
	<td class="line x" title="7:291	The goal is to produce material suitable for postprocessing in a lexicon acquisition process like the following: 1." ></td>
	<td class="line x" title="8:291	Run the automatic lexicon acquisition algorithm on a domain-specific parallel corpus." ></td>
	<td class="line x" title="9:291	2." ></td>
	<td class="line x" title="10:291	Automatically filter out 'general usage' entries that already appear in a machine readable dictionary (MRD) or other general usage lexical resources." ></td>
	<td class="line x" title="11:291	3." ></td>
	<td class="line x" title="12:291	Manually filter out incorrect or irrelevant entries from the remaining list." ></td>
	<td class="line x" title="13:291	Our aim, therefore, is to achieve sufficient recall and precision to make this process -in particular the time and manual effort required in Step 3 -a viable alternative to manual creation of translation lexicons without automated assistance." ></td>
	<td class="line x" title="14:291	The literature on cross-lingual text retrieval (CLTR) includes work that is closely related to this research, in that recent approaches emphasize the use of dictionaryand corpus-based techniques for translating queries from a source language into the language of the document collection (Oard, 1997)." ></td>
	<td class="line x" title="15:291	Davis and Dunning (1995), for example, generate target-language queries using a corpus-based technique that is similar in several respects to the work described here." ></td>
	<td class="line x" title="16:291	However, the approach does not attempt to distinguish domain-specific from general usage term pairs, and it involves no manual intervention." ></td>
	<td class="line x" title="17:291	The work reported here, focusing on semiautomating the process of acquiring translation lexicons specific to a domain, can be viewed as providing bilingual dictionary entries for CLTR methods like that used by Davis in later work (Davis, 1996), in which dictionary-based generation of an ambiguous target language query is followed by corpus-based disambiguation of that query." ></td>
	<td class="line oc" title="18:291	Turning to the literature on bilingual terminology identification per se, although monolingual terminology extraction is a problem that has been previously explored, often with respect to identifying relevant multi-word terms (e.g.(Daille, 1996; Smadja, 1993)), less prior work exists for bilingual acquisition of domain-specific translations." ></td>
	<td class="line x" title="20:291	Termight (Dagun and Church, 1994) is one method for analyzing parallel corpora to discover translations in technical terminology; Dagan and Church report accuracy of 40% given an English/German technical manual, and observe that even this relatively low accuracy permits the successful application of the system in a translation bureau, when used in conjunction with an appropriate user interface." ></td>
	<td class="line x" title="21:291	The Champollion system (Smadja, McKeown, and Hatzivassiloglou, 1996) moves toward higher accuracy (around 73%) and considerably greater flexibility in the handling of multi-word translations, though the algorithm has been applied primarily to very large corpora such as the Hansards (3-9 million words; Smadja et al. observe that the method has difficulty handling low-frequency cases), and no 340 attempt is made to distinguish corpus-dependent translations from general ones." ></td>
	<td class="line x" title="22:291	Daille et al.(1994) report on a study in which a small (200,000 word) corpus was used as the basis for extracting bilingual terminology, using a combination of syntactic patterns for identifying simple twoword terms monolingually, and a statistical measure for selecting related terms across languages." ></td>
	<td class="line x" title="24:291	Using a manually constructed reference list, they report 70% precision." ></td>
	<td class="line x" title="25:291	The SABLE system (Melamed, 1996b) makes no attempt to handle collocations, but for single-word to single-word translations it offers a very accurate method for acquiring high quality translation lexicons from very large parallel corpora: Melamed reports 90+% precision at 90+% recall, when evaluated on sets of Hansards data of 6-7 million words." ></td>
	<td class="line x" title="26:291	Previous work with SABLE does not attempt to address the question of domain-specific vs. general translations." ></td>
	<td class="line x" title="27:291	This paper applies the SABLE system to a much smaller (approximately 400,000 word) corpus in a technical domain, and assesses its potential contribution to the semi-automatic acquisition process outlined above, very much in the spirit of Dagan and Church (1994) and Daille et al.(1994), but beginning with a higher accuracy starting point and focusing on mono-word terms." ></td>
	<td class="line x" title="29:291	In the remainder of the paper we briefly outline translation lexicon acquisition in the SABLE system, describe its application to a corpus of technical documentation, and provide a quantitative assessment of its performance." ></td>
	<td class="line x" title="30:291	2 SABLE SABLE (Scalable Architecture for Bilingual LExicography) is a turn-key system for producing clean broad-coverage translation lexicons from raw, unaligned parallel texts (bitexts)." ></td>
	<td class="line x" title="31:291	Its design is modular and minimizes the need for language-specific components, with no dependence on genre or word order similarity, nor sentence boundaries or other 'anchors' in the input." ></td>
	<td class="line x" title="32:291	SABLE was designed with the following features in mind:  Independence from linguistic resources: SABLE does not rely on any language-specific resources other than tokenizers and a heuristic for identifying word pairs that are mutual translations, though users can easily reconfigure the system to take advantage of such resources as languagespecific stemmers, part-of-speech taggers, and stop lists when they are available." ></td>
	<td class="line x" title="33:291	 Black box functionality: Automatic acquisition of translation lexicons requires only that the user provide the input bitexts and identify the two languages involved." ></td>
	<td class="line x" title="34:291	Robustness: The system performs well even in the face of omissions or inversions in translations." ></td>
	<td class="line x" title="35:291	* Scalability: SABLE has been used successfully on input bitexts larger than 130MB." ></td>
	<td class="line x" title="36:291	* Portability: SABLE was initially implemented for French/English, then ported to Spanish/English and to Korean/English." ></td>
	<td class="line x" title="37:291	The porting process has been standardized and documented (Melamed, 1996c)." ></td>
	<td class="line x" title="38:291	The following is a brief description of SABLE's main components." ></td>
	<td class="line x" title="39:291	A more detailed description of the entire system is available in (Melamed, 1997)." ></td>
	<td class="line x" title="40:291	2.1 Mapping Bitext Correspondence After both halves of the input bitext(s) have been tokenized, SABLE invokes the Smooth Injective Map Recognizer (SIMR) algorithm (Melamed, 1996a) and related components to produce a bitext map." ></td>
	<td class="line x" title="41:291	A bitext map is an injective partial function between the character positions in the two halves of the bitext." ></td>
	<td class="line x" title="42:291	Each point of correspondence (x,y) in the bitext map indicates that the word centered around character position x in the first half of the bitext is a translation of the word centered around character position y in the second half." ></td>
	<td class="line x" title="43:291	SIMR produces bitext maps a few points at a time, by interleaving a point generation phase and a point selection phase." ></td>
	<td class="line x" title="44:291	SIMR is equipped with several 'plug-in' matching heuristic modules which are based on cognates (Davis et al. , 1995; Simard et al. , 1992; Melamed, 1995) and/or 'seed' translation lexicons (Chen, 1993)." ></td>
	<td class="line x" title="45:291	Correspondence points are generated using a subset of these matching heuristics; the particular subset depends on the language pair and the available resources." ></td>
	<td class="line x" title="46:291	The matching heuristics all work at the word level, which is a happy medium between larger text units like sentences and smaller text units like character n-grams." ></td>
	<td class="line x" title="47:291	Algorithms that map bitext correspondence at the phrase or sentences level are limited in their applicability to bitexts that have easily recognizable phrase or sentence boundaries, and Church (1993) reports that such bitexts are far more rare than one might expect." ></td>
	<td class="line x" title="48:291	Moreover, even when these larger text units can be found, their size imposes an upper bound on the resolution of the bitext map." ></td>
	<td class="line x" title="49:291	On the other end of the spectrum, character-based bitext mapping algorithms (Church, 1993; Davis et al. , 1995) are limited to language pairs where cognates are common; in addition, they may easily be misled by superficial differences in formatting and page layout and must sacrifice precision to be computationally tractable." ></td>
	<td class="line x" title="50:291	SIMR filters candidate points of correspondence using a geometric pattern recognition algorithm." ></td>
	<td class="line x" title="51:291	The recognized patterns may contain non-monotonic sequences of points of correspondence, to account for 341 (  point of correspondenceJ // } -~, -.'  '~.-'' fJ character position in bitoxt half A Figure 1: Word token pairs whose co-ordinates lie between the dashed boundaries count as coocctlr'rence8." ></td>
	<td class="line x" title="52:291	word order differences between languages." ></td>
	<td class="line x" title="53:291	The filtering algorithm can be efficiently interleaved with the point generation algorithm so that SIMR runs in linear time and space with respect to the size of the input bitext." ></td>
	<td class="line x" title="54:291	2.2 Translation Lexicon Extraction Since bitext maps can represent crossing correspondences, they are more general than 'alignments' (Melamed, 1996a)." ></td>
	<td class="line x" title="55:291	For the same reason, bitext maps allow a more general definition of token cooccurrence." ></td>
	<td class="line x" title="56:291	Early efforts at extracting translation lexicons from bitexts deemed two tokens to co-occur if they occurred in aligned sentence pairs (Gale and Church, 1991)." ></td>
	<td class="line x" title="57:291	SABLE counts two tokens as cooccurring if their point of correspondence lies within a short distance 8 of the interpolated bitext map in the bitext space, as illustrated in Figure 1." ></td>
	<td class="line x" title="58:291	To ensure that interpolation is well-defined, minimal sets of non-monotonic points of correspondence are replaced by the lower left and upper right corners of their minimum enclosing rectangles (MERs)." ></td>
	<td class="line x" title="59:291	SABLE uses token co-occurrence statistics to induce an initial translation lexicon, using the method described in (Melamed, 1995)." ></td>
	<td class="line x" title="60:291	The iterative filtering module then alternates between estimating the most likely translations among word tokens in the bitext and estimating the most likely translations between word types." ></td>
	<td class="line x" title="61:291	This re-estimation paradigm was pioneered by Brown et al.(1993)." ></td>
	<td class="line x" title="63:291	However, their models were not designed for human inspection, and though some have tried, it is not clear how to extract translation lexicons from their models (Wu and Xia, 1995)." ></td>
	<td class="line x" title="64:291	In contrast, SABLE automatically constructs an explicit translation lexicon, the lexicon consisting 1oooooo lOOOO := \]000 \[ ~, 3rd plateau :3 ~ ~ ~ 2nd plateau loo ~  1st plateau 10 ~  1| i i i i 0 2000 4000 6000 8000 i 1oooo Entry Number 12000 Figure 2: Translation lexicon entries proposed by SABLE exhibit plateaus of likelihood." ></td>
	<td class="line x" title="65:291	of word type pairs that are not filtered out during the re-estimation cycle." ></td>
	<td class="line x" title="66:291	Neither of the translation lexicon construction modules pay any attention to word order, so they work equally well for language pairs with different word order." ></td>
	<td class="line x" title="67:291	2.3 Thresholding Translation lexicon recall can be automatically computed with respect to the input bitext (Melamed, 1996b), so SABLE users have the option of specifying the recall they desire in the output." ></td>
	<td class="line x" title="68:291	As always, there is a tradeoff between recall and precision; by default, SABLE will choose a likelihood threshold that is known to produce reasonably high precision." ></td>
	<td class="line x" title="69:291	3 Evaluation in a Technical Domain 3.1 Materials Evaluated The SABLE system was run on a corpus comprising parallel versions of Sun Microsystems documentation ('Answerbooks') in French (219,158 words) and English (191,162 words)." ></td>
	<td class="line x" title="70:291	As Melamed (1996b) observes, SABLE's output groups naturally according to 'plateaus' of likelihood (see Figure 2)." ></td>
	<td class="line x" title="71:291	The translation lexicon obtained by running SABLE on the Answerbooks contained 6663 French-English content-word entries on the 2nd plateau or higher, including 5464 on the 3rd plateau or higher." ></td>
	<td class="line x" title="72:291	Table 1 shows a sample of 20 entries selected at random from the Answerbook corpus output on the 3rd plateau and higher." ></td>
	<td class="line x" title="73:291	Exact matches, such as cpio/cpio or clock/clock, comprised roughly 18% of the system's output." ></td>
	<td class="line x" title="74:291	In order to eliminate likely general usage entries from the initial translation lexicon, we automatically filtered out all entries that appeared in a French-English machine-readable dictionary (MRD) (Cousin, Allain, and Love, 1991)." ></td>
	<td class="line x" title="75:291	4071 entries remained on or above the 2nd likelihood plateau, including 3135 on the 3rd likelihood plateau or higher." ></td>
	<td class="line x" title="76:291	342 French English constantes multi-fen~trage risque extensions exemple relhch6 rw-r requs pr6aiable cpio sont defaults fn alphab6tique activ4e machine mettre connect6s bernard superutilisateur constants windows may extensions such released 17 received first cpio will defaults fn alphabetically activates workstation turns connected spunky root Table 1: Random sample of SABLE output on software manuals." ></td>
	<td class="line x" title="77:291	In previous experiments on the Hansard corpus of Canadian parliamentary proceedings, SABLE had uncovered valid general usage entries that were not present in the Collins MRD (e.g. pointillds/dotted)." ></td>
	<td class="line x" title="78:291	Since entries obtained from the Hansard corpus are unlikely to include relevant technical terms, we decided to test the efficacy of a second filtering step, deleting all entries that had also been obtained by running SABLE on the Hansards." ></td>
	<td class="line x" title="79:291	On the 2nd plateau or higher, 3030 entries passed both the Collins and the Hansard filters; 2224 remained on or above the 3rd plateau." ></td>
	<td class="line x" title="80:291	Thus in total, we evaluated four lexicons derived from all combinations of two independent variables: cutoff (after the 2nd plateau vs. after the 3rd plateau) and Hansards filter (with filter vs. without)." ></td>
	<td class="line x" title="81:291	Evaluations were performed on a random sample of 100 entries from each lexicon variation, interleaving the four samples to obscure any possible regularities." ></td>
	<td class="line x" title="82:291	Thus from the evaluator's perspective the task appeared to involve a single sample of 400 translation lexicon entries." ></td>
	<td class="line x" title="83:291	3.2 Evaluation Procedure Our assessment of the system was designed to reasonably approximate the post-processing that would be done in order to use this system for acquisition of translation lexicons in a real-world setting, which would necessarily involve subjective judgments." ></td>
	<td class="line x" title="84:291	We hired six fluent speakers of both French and English at the University of Maryland; they were briefed on the general nature of the task, and given a data sheet containing the 400 candidate entries (pairs containing one French word and one English word) and a 'multiple choice' style format for the annotations, along with the following instructions." ></td>
	<td class="line x" title="85:291	1." ></td>
	<td class="line x" title="86:291	If the pair clearly cannot be of help in constructing a glossary, circle 'Invalid' and go on to the next pair." ></td>
	<td class="line x" title="87:291	2." ></td>
	<td class="line x" title="88:291	If the pair can be of help in constructing a glossary, choose one of the following: 1 V: The two words are of the 'plain vanilla' type you might find in a bilingual dictionary." ></td>
	<td class="line x" title="89:291	P: The pair is a case where a word changes its part of speech during translation." ></td>
	<td class="line x" title="90:291	For example, 'to have protection' in English is often translated as %tre prot6g6' in Canadian parliamentary proceedings, so for that domain the pair protection/prot6g6 would be marked P. I: The pair is a case where a direct translation is incomplete because the computer program only looked at single words." ></td>
	<td class="line x" title="91:291	For example, if French 'imm6diatement' were paired with English 'right', you could select I because the pair is almost certainly the computer's best but incomplete attempt to be pairing 'imm4diatement' with 'right away'." ></td>
	<td class="line x" title="92:291	3." ></td>
	<td class="line x" title="93:291	Then choose one or both of the following:  Specific." ></td>
	<td class="line x" title="94:291	Leaving aside the relationship between the two words (your choice of P, V, or I), the word pair would be of use in constructing a technical glossary." ></td>
	<td class="line x" title="95:291	 General." ></td>
	<td class="line x" title="96:291	Leaving aside the relationship between the two words (your choice of P, V, or I), the word pair would be of use in constructing a general usage glossary." ></td>
	<td class="line x" title="97:291	Notice that a word pair could make sense in both." ></td>
	<td class="line x" title="98:291	For example, 'corbeille/wastebasket' makes sense in the computer domain (in many popular graphical interfaces there is a wastebasket icon that is used for deleting files), but also in more general usage." ></td>
	<td class="line x" title="99:291	So in this case you could in fact decide to choose both 'Specific' and 'General'." ></td>
	<td class="line x" title="100:291	If you can't choose either 'Specific' or 'General', chances are that you should reconsider whether or not to mark this word pair 'Invalid'." ></td>
	<td class="line x" title="101:291	i Since part-of-speech tagging was used in the version of SABLE that produced the candidates in this experiment, entries presented to the annotator also included a minimal form of part-of-speech information, e.g. distinguishing nouns from verbs." ></td>
	<td class="line x" title="102:291	The annotator was informed that these annotations were the computer's best attempt to identify the part-of-speech for the words; it was suggested that they could be used as a hint as to why that word pair had been proposed, if so desired, and otherwise ignored." ></td>
	<td class="line x" title="103:291	343 4." ></td>
	<td class="line x" title="104:291	If you're completely at a loss to decide whether or not the word pair is valid, just put a slash through the number of the example (the number at the beginning of the line) and go on to the next pair." ></td>
	<td class="line x" title="105:291	Annotators also had the option of working electronically rather than on hardcopy." ></td>
	<td class="line x" title="106:291	The assessment questionnaire was designed to elicit information primarily of two kinds." ></td>
	<td class="line x" title="107:291	First, we were concerned with the overall accuracy of the method; that is, its ability to produce reasonable candidate entries whether they be general or domain specific." ></td>
	<td class="line x" title="108:291	The 'Invalid' category captures the system's mistakes on this dimension." ></td>
	<td class="line x" title="109:291	We also explicitly annotated candidates that might be useful in constructing a translation lexicon, but possibly require further elaboration." ></td>
	<td class="line x" title="110:291	The V category captures cases that require minimal or no additional effort, and the P category covers cases where some additional work might need to be done to accommodate the part-of-speech divergence, depending on the application." ></td>
	<td class="line x" title="111:291	The I category captures cases where the correspondence that has been identified may not apply directly at the single-world level, but nonetheless does capture potentially useful information." ></td>
	<td class="line x" title="112:291	Daille et al.(1994) also note the existence of 'incomplete' cases in their results, but collapse them together with 'wrong' pairings." ></td>
	<td class="line x" title="114:291	Second, we were concerned with domain specificity." ></td>
	<td class="line x" title="115:291	Ultimately we intend to measure this in an objective, quantitative way by comparing term usage across corpora; however, for this study we relied on human judgments." ></td>
	<td class="line x" title="116:291	3.3 Use of Context Melamed (1996b) suggests that evaluation of translation lexicons requires that judges have access to bilingual concordances showing the contexts in which proposed word pairs appear; however, out-ofcontext judgments would be easier to obtain in both experimental and real-world settings." ></td>
	<td class="line x" title="117:291	In a preliminary evaluation, we had three annotators (one professional French/English translator and two graduate students at the University of Pennsylvania) perform a version of the annotation task just described: they annotated a set of entries containing the output of an earlier version of the SABLE system (one that used aligned sub-sentence fragments to define term co-occurrence; cf.Section 2.2)." ></td>
	<td class="line x" title="119:291	No bilingual concordances were made available to them." ></td>
	<td class="line x" title="120:291	Analysis of the system's performance in this pilot study, however, as well as annotator comments in a post-study questionnaire, confirmed that context is quite important." ></td>
	<td class="line x" title="121:291	In order to quantify its im: portance, we asked one of the pilot annotators to repeat the evaluation on the same items, this time giving her access to context in the form of the bilingual concordances for each term pair." ></td>
	<td class="line x" title="122:291	These concordances contained up to the first ten instances of that pair as used in context." ></td>
	<td class="line x" title="123:291	For example, given the pair d@lacez/drag, one instance in that pair's bilingual concordance would be: Maintenez SELECT enfoncd et d~placez le dossier vers l' espace de travail . Press SELECT and drag the folder onto the workspace background . The instructions for the in-context evaluation specify that the annotator should look at the context for every word pair, pointing out that 'word pairs may be used in unexpected ways in technical text and words you would not normally expect to be related sometimes turn out to be related in a technical context'." ></td>
	<td class="line x" title="124:291	Although we have data from only one annotator, Table 2 shows the clear differences between the two results." ></td>
	<td class="line x" title="125:291	2 In light of the results of the pilot study, therefore, our six annotators were given access to bilingual concordances for the entries they were judging and instructed in their use as just described." ></td>
	<td class="line x" title="126:291	4 Results 4.1 Group Annotations A 'group annotation' was obtained for each candidate translation lexicon entry based on agreement of at least three of the six annotators." ></td>
	<td class="line x" title="127:291	'Tie scores' or the absence of a 3-of-6 plurality were treated as the absence of an annotation." ></td>
	<td class="line x" title="128:291	For example, if an entry was annotated as 'Invalid' by two annotators, marked as category V and Specific by two annotators, and marked as category P, Specific, and General by the other two annotators, then the group annotation would contain an 'unclassified valid type' (since four annotators chose a valid type, but there was no agreement by at least three on the specific subclasification) and a 'Specific' annotation (agreed on by four annotators)." ></td>
	<td class="line x" title="129:291	All summary statistics are reported in terms of the group annotation." ></td>
	<td class="line x" title="130:291	4.2 Precision SABLE's precision on the Answerbooks bitext is summarized in Figure 3." ></td>
	<td class="line x" title="131:291	3 Each of the percentages being derived from a random sample of 100 observations, we can compute confidence intervals under a normality assumption; if we assume that the observations are independent, then 95% confidence intervals are narrower than one twentieth of a percentage point for all the statistics computed." ></td>
	<td class="line x" title="132:291	The results show that up to 89% of the translation lexicon entries produced by SABLE on or above the 2Again, this sample of data was produced by an older and less accurate version of SABLE, and therefore the percentages should only be analyzed relative to each other, not as absolute measures of performance." ></td>
	<td class="line x" title="133:291	3The exact numbers gladly provided on request." ></td>
	<td class="line x" title="134:291	344 ~1~1~1 ~'l ~'i~ Entries Domain-Specific Only Out-of-Context 39.519.2515.5\[ 57.75 29.75 In-Context 46.75\[\[ 15 13 69.5 38 General Only Usage I Bth 23.5 1 23.25 3.5 Table 2: Effect of in-context vs. out-of-context evaluation." ></td>
	<td class="line x" title="135:291	All numbers are in ~o. n = 400." ></td>
	<td class="line x" title="136:291	100 75 E o~ 50 '5 o~ 25 V ~ P ~ I r---\] Unclassified valid type I ::z:::: :::::::: iiiiiiii with no with no Hansard Hansard Hansard Hansard filter filter filter filter 3rd plateau cutoff 2nd plateau cutoff Figure 3: Summary of filtered translation lexicon validily statistics." ></td>
	<td class="line x" title="137:291	3rd likelihood plateau 'can be of help in constructing a glossary'." ></td>
	<td class="line x" title="138:291	Up to 56% can be considered useful essentially as-is (the V category alone)." ></td>
	<td class="line x" title="139:291	Including all entries on the 2nd plateau or higher provides better coverage, but reduces the fraction of useful entries to 81%." ></td>
	<td class="line x" title="140:291	The fraction of entries that are useful as-is remains roughly the same, at 55%." ></td>
	<td class="line x" title="141:291	At both recall levels, the extra Hansards-based filter had a detrimental effect on precision." ></td>
	<td class="line x" title="142:291	Note that these figures are based on translation lexicons from which many valid general usage entries have been filtered out (see Section 3)." ></td>
	<td class="line x" title="143:291	We can compute SABLE's precision on unfiltered translation lexicons for this corpus by assuming that entries appearing in the Collins MRD are all correct." ></td>
	<td class="line x" title="144:291	4 However, these are not the real figures of interest here, because we are mainly concerned in this study with the acquisition of domain-specific translation lexicons." ></td>
	<td class="line x" title="145:291	4.3 Recall Following Melamed (1996b), we adopt the following approach to measuring recall: the upper bound is defined by the number of different words in the bitext." ></td>
	<td class="line x" title="146:291	Thus, perfect recall implies at least one entry containing each word in the corpus." ></td>
	<td class="line x" title="147:291	This is a much more conservative metric than that used by Daille et al.(1994), who report recall with respect to a relatively 4Result: 88.4% precision at 37.0% recall or 93.7% precision at 30.4% recall." ></td>
	<td class="line x" title="149:291	small, manually constructed reference set." ></td>
	<td class="line x" title="150:291	Although we do not expect to achieve perfect recall on this criterion after general usage entries have been filtered out, the number is useful insofar as it provides a sense of how recall for this corpus correlates with precision." ></td>
	<td class="line x" title="151:291	We have no reason to expect this correlation to change across domain-specific and general lexicon entries." ></td>
	<td class="line x" title="152:291	For the unfiltered translation lexicons, recall on the 3rd likelihood plateau and above was 30.4%." ></td>
	<td class="line x" title="153:291	When all entries on and above the 2nd plateau were considered, recall improved to 37.0%." ></td>
	<td class="line x" title="154:291	100 75 Q_ E,o 50 o 25 domain-specific only \[~ both 1---\] general only with no with no Hansard Hansard Hansard Hansard filter filter filter filter 3rd plateau cutoff 2nd plateau cutoff Figure 4: Summary of filtered translation lexicon domain-specificity statistics." ></td>
	<td class="line x" title="155:291	Hansards Filter?" ></td>
	<td class="line x" title="156:291	% Plateau Domain Cutoff Specific % General Usage Yes 3rd 82 37 No 3rd 71 53 Yes 2nd 66 27 No 2nd 81 47 % Both 35 35 22 47 Table 3: Domain-specificity of filtered translation lexicon entries." ></td>
	<td class="line x" title="157:291	4.4 Domain Specificity Figure 4 demonstrates the effectiveness of the MRDand corpus-based filters, with details in Table 3." ></td>
	<td class="line x" title="158:291	If we assume that translation pairs in the Collins MRD are not specific to our chosen domain, then domainspecific translation lexicon entries constituted only 345 ~--\]A1 A2 A3 A4 A5 ~ 0.70 0.44 0.59 0.82 0.90 0.82 I 0.62 0.67 0.72 0.74 0.55 0.73 0.28 0.19 0.50 0.00 0.00 0.56 0.67 0.69 0.68 0.74 0.61 0.81 Table 4: Infer-annotator agreement." ></td>
	<td class="line x" title="159:291	49% of SABLE's unfiltered output on or above the 2nd plateau and 41% on or above the 3rd plateau." ></td>
	<td class="line x" title="160:291	The MRD filter increased this ratio to 81% and 71%, respectively." ></td>
	<td class="line x" title="161:291	As noted in Section 4.2, the second filter, based on the Hansard bitext, reduced the overall accuracy of the translation lexicons." ></td>
	<td class="line x" title="162:291	Its effects on the proportion of domain-specific entries was mixed: an 11% increase for the entries more likely to be correct, but a 15% decrease overall." ></td>
	<td class="line x" title="163:291	The corpus-based filter is certainly useful in the absence of an MRD." ></td>
	<td class="line x" title="164:291	However, our results suggest that combining filters does not always help, and more research is needed to investigate optimal filter combination strategies." ></td>
	<td class="line x" title="165:291	4.5 Consistency of Annotations In order to assess the consistency of annotation, we follow Carletta (1996) in using Cohen's ~, a chancecorrected measure of inter-rater agreement." ></td>
	<td class="line x" title="166:291	The statistic was developed to distinguish among levels of agreement such as 'almost perfect, substantial, moderate, fair, slight, poor' (Agresti, 1992), and Carletta suggests that as a rule of thumb in the behavioral sciences, values of g greater than .8 indicate good replicability, with values between .67 and .8 allowing tentative conclusions to be drawn." ></td>
	<td class="line x" title="167:291	For each such comparison, four values of ~ were computed: ~:1: agreement on the evaluation of whether or not a pair should be immediately rejected or retained; ~2: agreement, for the retained pairs, on the type V, P, or I assigned to the pair; ~a: agreement, for the retained pairs, on whether to classify the pair as being useful for constructing a domain-specific glossary; g4: agreement, for the retained pairs, on whether to classify the pair as being useful for constructing a general usage glossary." ></td>
	<td class="line x" title="168:291	In each case, the computation of the agreement statistic took into account those cases, if any, where the annotator could not arrive at a decision for this case and opted simply to throw it out." ></td>
	<td class="line x" title="169:291	Resulting values for inter-rater reliability are shown in Table 4; the six annotators are identified as A1, A2,  A6; and each value of ~ reflects the comparison between that annotator and the group annotation." ></td>
	<td class="line x" title="170:291	With the exception of ~3, these values of n indicate that the reliability of the judgments is generally reasonable, albeit not entirely beyond debate." ></td>
	<td class="line x" title="171:291	The outlandish values for ~3, despite high rates of absolute agreement on that dimension of annotation, are explained by the fact that the ~ statistic is known to be highly problematic as a measure of inter-rater reliability when one of the categories that can be chosen is overwhelmingly likely (Grove et al. , 1981; Spitznagel and Helzer, 1985)." ></td>
	<td class="line x" title="172:291	Intuitively this is not surprising: we designed the experiment to yield a predominance of domain-specific terms, by means of the MRD and Hansards filters." ></td>
	<td class="line x" title="173:291	Our having succeeded, there is a very high probability that the 'Specific' annotation will be selected by any two annotators, because it appears so very frequently; as a result the actual agreement rate for that annotation doesn't actually look all that different from what one would get by chance, and so the ~ values are low." ></td>
	<td class="line x" title="174:291	The values of ~3 for annotators 4 and 5 emphasize quite clearly that ~ is measuring not the level of absolute agreement, hut the distinguishability of that level of agreement from chance." ></td>
	<td class="line x" title="175:291	5 Conclusion In this paper, we have investigated the application of SABLE, a turn-key translation lexicon construction system for non-technical users, to the problem of identifying domain-specific word translations given domain-specific corpora of limited size." ></td>
	<td class="line x" title="176:291	Evaluated on a very small (400,000 word) corpus, the system shows real promise as a method of processing small domain-specific corpora in order to propose candidate single-word translations: once likely general usage terms are automatically filtered out, the system obtains precision up to 89% at levels of recall very conservatively estimated in the range of 30-40% on domain-specific terms." ></td>
	<td class="line x" title="177:291	Of the proposed entries not immediately suitable for inclusion in a translation lexicon, many represent part-of-speech divergences (of the protect/protdgg variety) and a smaller number incomplete entries (of the immddiatement/right variety) that would nonetheless be helpful if used as the basis for a bilingual concordance search -for example, a search for French segments containing immddiatemeut in the vicinity of English segments containing right would most likely yield up the obvious correspondence between immgdiatement and right away." ></td>
	<td class="line x" title="178:291	Going beyond single-word correspondences, however, is a priority for future work." ></td>
	<td class="line x" title="179:291	6 Acknowledgments The authors wish to acknowledge the support of Sun Microsystems Laboratories, particularly the assistance of Gary Adams, Cookie Callahan, and Bob Kuhns, as well as useful input from Bonnie Dorr, Ralph Grishman, Marti Hearst, Doug Oard, and three anonymous reviewers." ></td>
	<td class="line x" title="180:291	Melamed also acknowledges grants ARPA N00014-90-J-1863 and ARPA N6600194C 6043." ></td>
	<td class="line x" title="181:291	346 References Alan Agresti." ></td>
	<td class="line x" title="182:291	1992." ></td>
	<td class="line x" title="183:291	Modeling patterns of agreement and disagreement." ></td>
	<td class="line x" title="184:291	Statistical methods in medical research, 1:201-218." ></td>
	<td class="line x" title="185:291	P. F. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer." ></td>
	<td class="line x" title="186:291	1993." ></td>
	<td class="line x" title="187:291	'The Mathematics of Statistical Machine Translation: Parameter Estimation'." ></td>
	<td class="line x" title="188:291	Computational Linguistics 19:2." ></td>
	<td class="line x" title="189:291	Jean Carletta." ></td>
	<td class="line x" title="190:291	1996." ></td>
	<td class="line x" title="191:291	Assessing agreement on classification tasks: the Kappa statistic." ></td>
	<td class="line x" title="192:291	Computational Linguistics, 22(2):249-254, June." ></td>
	<td class="line x" title="193:291	S. Chen." ></td>
	<td class="line x" title="194:291	1993." ></td>
	<td class="line x" title="195:291	'Aligning Sentences in Bilingual Corpora Using Lexical Information'." ></td>
	<td class="line x" title="196:291	Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, Columbus, OH." ></td>
	<td class="line x" title="197:291	K. W. Church." ></td>
	<td class="line x" title="198:291	1993." ></td>
	<td class="line x" title="199:291	'Char_align: A Program for Aligning Parallel Texts at the Character Level'." ></td>
	<td class="line x" title="200:291	Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, Columbus, OH." ></td>
	<td class="line x" title="201:291	P. H. Cousin, L. Sinclair, J. F. Allain, and C. E. Love." ></td>
	<td class="line x" title="202:291	1991." ></td>
	<td class="line x" title="203:291	The Collins Paperback French Dictionary." ></td>
	<td class="line x" title="204:291	Harper Collins Publishers, Glasgow." ></td>
	<td class="line x" title="205:291	Ido Dagan and Ken W. Church." ></td>
	<td class="line x" title="206:291	1994." ></td>
	<td class="line x" title="207:291	TERMIGHT: Identifying and translating technical terminology." ></td>
	<td class="line x" title="208:291	In Proceedings of the Fourth ACL Conference on Applied Natural Language Processing (13-15 October 1994, Stuttgart)." ></td>
	<td class="line x" title="209:291	Association for Computational Linguistics, October." ></td>
	<td class="line x" title="210:291	I. Dagan, K. Church, and W. Gale." ></td>
	<td class="line x" title="211:291	1993." ></td>
	<td class="line x" title="212:291	'Robust Word Alignment for Machine Aided Translation'." ></td>
	<td class="line x" title="213:291	Proceedings of the Workshop on Very Large Corpora: Academic and Industrial Perspectives, available from the ACL." ></td>
	<td class="line x" title="214:291	Beatrice Daille." ></td>
	<td class="line x" title="215:291	1994." ></td>
	<td class="line x" title="216:291	Combined approach for terminology extraction: lexical statistics and linguistic filtering." ></td>
	<td class="line x" title="217:291	Ph.D. thesis, University Paris 7." ></td>
	<td class="line x" title="218:291	Beatrice Daille." ></td>
	<td class="line x" title="219:291	1996." ></td>
	<td class="line x" title="220:291	Study and implementation of combined techniques for automatic extraction of terminology." ></td>
	<td class="line x" title="221:291	In Judith Klavans and Philip Resnik, editors, The Balancing Act: Combining Symbolic and Statistical Approaches to Language." ></td>
	<td class="line x" title="222:291	MIT Press." ></td>
	<td class="line x" title="223:291	Mark Davis." ></td>
	<td class="line x" title="224:291	1996." ></td>
	<td class="line x" title="225:291	'New experiments in crosslanguage text retrieval at NMSU's Computing Research Lab'." ></td>
	<td class="line x" title="226:291	Fifth Text Retrieval Conference (TREC-5)." ></td>
	<td class="line x" title="227:291	NIST." ></td>
	<td class="line x" title="228:291	Mark Davis and Ted Dunning." ></td>
	<td class="line x" title="229:291	1995." ></td>
	<td class="line x" title="230:291	'A TREC evaluation of query translation methods for multilingual text retrieval'." ></td>
	<td class="line x" title="231:291	Fourth Text Retrieval Conference (TREC-4)." ></td>
	<td class="line x" title="232:291	NIST." ></td>
	<td class="line x" title="233:291	Mark Davis, Ted Dunning, and William Ogden." ></td>
	<td class="line x" title="234:291	1995." ></td>
	<td class="line x" title="235:291	Text alignment in the real world: improving alignments of noisy translation using common lexical features, string matching strategies, and n-gram comparisons." ></td>
	<td class="line x" title="236:291	In EACL-95." ></td>
	<td class="line x" title="237:291	W. Gale and K. W. Church." ></td>
	<td class="line x" title="238:291	1991." ></td>
	<td class="line x" title="239:291	'Identifying Word Correspondences in Parallel Texts'." ></td>
	<td class="line x" title="240:291	Proceedings of the DARPA SNL Workshop, 1991." ></td>
	<td class="line x" title="241:291	W. Grove, N. Andreasen, P. McDonald-Scott, M. Keller, and R. Shapiro." ></td>
	<td class="line x" title="242:291	1981." ></td>
	<td class="line x" title="243:291	Reliability studies of psychiatric diagnosis." ></td>
	<td class="line x" title="244:291	Archives of General Psychiatry, 38, April." ></td>
	<td class="line x" title="245:291	I. Dan Melamed, 1995." ></td>
	<td class="line x" title="246:291	Automatic evaluation and uniform filter cascades for inducing n-best translation lexicons." ></td>
	<td class="line x" title="247:291	In Proceedings of the Third Workshop on Very Large Corpora, Cambridge, Massachusetts." ></td>
	<td class="line x" title="248:291	I. Dan Melamed." ></td>
	<td class="line x" title="249:291	1996a." ></td>
	<td class="line x" title="250:291	A geometric approach to mapping bitext correspondence." ></td>
	<td class="line x" title="251:291	In Conference on Empirical Methods in Natural Language Processing, Philadelphia, Pennsylvania." ></td>
	<td class="line x" title="252:291	I. Dan Melamed." ></td>
	<td class="line x" title="253:291	1996b." ></td>
	<td class="line x" title="254:291	Automatic construction of clean broad-coverage translation lexicons." ></td>
	<td class="line x" title="255:291	In Proceedings of the 2nd Conference of the Association for Machine Translation in the Americas, Montreal, Canada." ></td>
	<td class="line x" title="256:291	I. Dan Melamed." ></td>
	<td class="line x" title="257:291	1996c." ></td>
	<td class="line x" title="258:291	Porting SIMR to new language pairs." ></td>
	<td class="line x" title="259:291	IRCS Technical Report 96-26." ></td>
	<td class="line x" title="260:291	University of Pennsylvania." ></td>
	<td class="line x" title="261:291	I. Dan Melamed." ></td>
	<td class="line x" title="262:291	1997." ></td>
	<td class="line x" title="263:291	A scalable architecture for bilingual lexicography." ></td>
	<td class="line x" title="264:291	Dept. of Computer and Information Science Technical Report MS-CIS-9701." ></td>
	<td class="line x" title="265:291	University of Pennsylvania." ></td>
	<td class="line x" title="266:291	Douglas W. Oard." ></td>
	<td class="line x" title="267:291	1997." ></td>
	<td class="line x" title="268:291	'Cross-Language Text Retrieval Research in the USA'." ></td>
	<td class="line x" title="269:291	Third DELOS Workshop." ></td>
	<td class="line x" title="270:291	European Research Consortium for Informatics and Mathematics." ></td>
	<td class="line x" title="271:291	March." ></td>
	<td class="line x" title="272:291	M. Simard, G. F. Foster and P. Isabelle." ></td>
	<td class="line x" title="273:291	1992." ></td>
	<td class="line x" title="274:291	'Using Cognates to Align Sentences in Bilingual Corpora'." ></td>
	<td class="line x" title="275:291	In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation, Montreal, Canada." ></td>
	<td class="line x" title="276:291	Frank Smadja." ></td>
	<td class="line x" title="277:291	1993." ></td>
	<td class="line x" title="278:291	Retrieving collocations from text: Xtract." ></td>
	<td class="line x" title="279:291	Computational Linguistics, 19(1):143-177." ></td>
	<td class="line x" title="280:291	Frank Smadja, Kathleen McKeown, and Vasileios Hatzivassiloglou." ></td>
	<td class="line x" title="281:291	1996." ></td>
	<td class="line x" title="282:291	Translating collocations for bilingual lexicons: A statistical approach." ></td>
	<td class="line x" title="283:291	Computational Linguistics, 22(1), March." ></td>
	<td class="line x" title="284:291	E. Spitznagel and J. Helzer." ></td>
	<td class="line x" title="285:291	'A proposed solution to the base rate problem in the kappa statistic'." ></td>
	<td class="line x" title="286:291	Archives of General Psychiatry, 42." ></td>
	<td class="line x" title="287:291	July, 1985." ></td>
	<td class="line x" title="288:291	D. Wu and X. Xia." ></td>
	<td class="line x" title="289:291	1994." ></td>
	<td class="line x" title="290:291	'Learning an English-Chinese Lexicon from a Parallel Corpus'." ></td>
	<td class="line x" title="291:291	Proceedings of the First Conference of the Association for Machine Translation in the Americas, Columbia, MD ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="A97-1054
A Workbench For Finding Structure In Texts
Mikheev, Andrei;Finch, Steven;"></td>
	<td class="line x" title="1:187	A Workbench for Finding Structure in Texts Andrei Mikheev HCRC, Language Technology Group, University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9LW, UK." ></td>
	<td class="line x" title="2:187	Andrei." ></td>
	<td class="line x" title="3:187	Mikheev@ed." ></td>
	<td class="line x" title="4:187	ac." ></td>
	<td class="line x" title="5:187	uk Steven Finch Thomson Technical Labs, 1375 Piccard Drive, Suite 250, Rockville Maryland, 20850 sfinch@thomtech, com Abstract In this paper we report on a set of computational tools with (n)SGML pipeline data flow for uncovering internal structure in natural language texts." ></td>
	<td class="line x" title="6:187	The main idea behind the workbench is the independence of the text representation and text analysis phases." ></td>
	<td class="line x" title="7:187	At the representation phase the text is converted from a sequence of characters to features of interest by means of the annotation tools." ></td>
	<td class="line x" title="8:187	At the analysis phase those features are used by statistics gathering and inference tools for finding significant correlations in the texts." ></td>
	<td class="line x" title="9:187	The analysis tools are independent of particular assumptions about the nature of the feature-set and work on the abstract level of featureelements represented as SGML items." ></td>
	<td class="line x" title="10:187	1 Introduction There is increasing agreement that progress in various areas of language engineering needs large collections of unconstrained language material." ></td>
	<td class="line x" title="11:187	Such corpora are emerging and are proving to be important research tools in areas such as lexicography, text understanding and information extraction, spoken language understanding, the evaluation of parsers, the construction of large-scale lexica, etc. The key idea of corpus oriented language analysis is to collect frequencies of 'interesting' events and then run statistical inferences on the basis of those frequencies." ></td>
	<td class="line oc" title="12:187	For instance, one might be interested in frequencies of co-occurences of a word with other words and phrases (collocations) (Smadja, 1993), or one might be interested in inducing wordclasses from the text by collecting frequencies of the left and right context words for a word in focus (Finch&Chater, 1993)." ></td>
	<td class="line x" title="13:187	Thus, the building blocks of the 'interesting' events might be words, their morpho-syntactic properties (e.g. part-of-speech, suffix, etc.), phrases or their sub-phrases (e.g. headnoun of a noun group), etc. The 'interesting' events usually also specify the relation between those building blocks such as 'the two words should occur next to each other or in the same sentence'." ></td>
	<td class="line x" title="14:187	In this paper we describe a workbench for uncovering that kind of internal structure in natural language texts." ></td>
	<td class="line x" title="15:187	2 Data Level Integration The underlying idea behind our workbench is data level integration of abstract data processing tools by means of structured streams." ></td>
	<td class="line x" title="16:187	The idea of using an open set of modular tools with stream input/output (IO) is akin to the philosophy behind UNIX." ></td>
	<td class="line x" title="17:187	This allows for localization of specific data processing or manipulation tasks so we can use different combinations of the same tools in a pipeline for fulfilling different tasks." ></td>
	<td class="line x" title="18:187	Our architecture, however, imposes an additional constraint on the IO streams: they should have a common syntactic format which is realized as SGML markup (Goldfarb, 1990)." ></td>
	<td class="line x" title="19:187	A detailed comparison of this SGML-oriented architecture with more traditional data-base oriented architectures can be found in (McKelvie et al. , 1997)." ></td>
	<td class="line x" title="20:187	As a markup device an SGML element has a label (L), a pre-specified set of attributes (attr) and can have character data: <L attr=val attr=val>character data</L> SGML elements can also include other elements thus producing tree-like structures." ></td>
	<td class="line x" title="21:187	For instance, a document can comprise sections which consist of a title and a body-text and the body-text can consist of sentences each of which has its number stated as an attribute, has its contents as character data and can include other marked elements such as pre-tokenized phrases and dates as shown in Figure I. Such structures are described in Document Type Definition (DTD) files which are used to check whether an SGML document is syntactically correct, i.e. whether its SGML elements have only pre-specified attributes and include only the right kinds of other SGML elements." ></td>
	<td class="line x" title="22:187	So, for instance, if in the document shown 372 in Figure 1 we had a header element (H) under an S element this would be detected as a violation of the defined structure." ></td>
	<td class="line x" title="23:187	An important property of SGML is that defining a rigorous syntactic format does not set any assumptions on the semantics of the data and it is up to a tool to assign a specific interpretation to a particular SGML item or its attributes." ></td>
	<td class="line x" title="24:187	Thus a tool in our architecture is a piece of software which uses an SGMLhandling Application Programmer Interface (API) for all its data access to corpora and performs some useful task, whether exploiting markup which has previously been added by other tools, or itself adding new markup to the stream(s) and without destroying the previously added markup." ></td>
	<td class="line x" title="25:187	This approach allows us to remain entirely within the SGML paradigm for corpus markup while allowing us to be very general in designing our tools, each of which can be used for many purposes." ></td>
	<td class="line x" title="26:187	Furthermore, through the ability to pipe data through processes, the UNIX operating system itself provides the natural 'glue' for integrating data-level applications together." ></td>
	<td class="line x" title="27:187	The API methodology is very widely used in the software industry to integrate software components to form finished applications, often making use of some glue environment to stick the pieces together (e.g. tcl/tk, Visual Basic, Delphi, etc.)." ></td>
	<td class="line x" title="28:187	However, we choose to integrate our applications at the data level." ></td>
	<td class="line x" title="29:187	Rather than define a set of functions which can be called to perform tasks, we define a set of representations for how information which is typically produced by the tasks is actually represented." ></td>
	<td class="line x" title="30:187	For natural language processing, there are many advantages to the data level integration approach." ></td>
	<td class="line x" title="31:187	Let us take the practical example of a tokenizer." ></td>
	<td class="line x" title="32:187	Rather than provide a set of functions which take strings and return sets of tokens, we define a tokenizer to be something which takes in a SGML stream and returns a SGML stream which has been marked up for tokens." ></td>
	<td class="line x" title="33:187	Firstly, there is no direct tie to the process which actually performed the markup; provided a tokenizer adds a markup around what it tokenizes, it doesn't matter whether it is written in C or LISP, or whether it is based on a FSA or a neural net." ></td>
	<td class="line x" title="34:187	Some tokenization can even be done by hand, and any downline application which uses tokenization is completely functionally isolated from the processes used to perform the tokenization." ></td>
	<td class="line x" title="35:187	Secondly, each part of the process has a well-defined image at the data level, and a data-level semantics." ></td>
	<td class="line x" title="36:187	Thus a tokenizer as part of a complex task has its own semantics, and furthermore its own image in the data." ></td>
	<td class="line x" title="37:187	3 Queries and Views SGML markup (Goldfarb, 1990) represents a document in terms of embedded elements akin to a file structure with directories, subdirectories and files." ></td>
	<td class="line x" title="38:187	Thus in the example in Figure 1, the document comprises a header and a body text and these might require different strategies for processing 1." ></td>
	<td class="line x" title="39:187	The SGML-handling API in our workbench is realized by means of the LT NSL library (Thompson et al. , 1996) which can handle even the most complex document structures (DTDs)." ></td>
	<td class="line x" title="40:187	It allows a tool to read, change or add attribute values and character data to SGML elements and address a particular element in a normalized 2 SGML (NSGML) stream using its partial description by means of nsl-queries." ></td>
	<td class="line x" title="41:187	Consider the sample text shown in Figure 1." ></td>
	<td class="line x" title="42:187	Given that text and its markup, we can refer to the second sentence under a BODY element which is under a DOC element: /DOC/BODY/S\[n=2\]." ></td>
	<td class="line x" title="43:187	This will sequentially give us the second sentences in all BODYs." ></td>
	<td class="line x" title="44:187	If we want to address only the sentence under the first BODY we can specify that in the query: /DOC/BODY\[0\]/S\[n=2\]." ></td>
	<td class="line x" title="45:187	We can use wildcards in the queries." ></td>
	<td class="line x" title="46:187	For instance, the query . */S says 'give me all sentences anywhere in the document' and the wildcard '.*' means ' at any level of embedding'." ></td>
	<td class="line x" title="47:187	Thus we can directly specify which parts of the stream we want to process and which to skip." ></td>
	<td class="line x" title="48:187	Using nsl-queries we can access required SGML elements in a document." ></td>
	<td class="line x" title="49:187	These elements can have, however, quite complex internal structures which we might want to represent in a number of different ways according to a task at hand." ></td>
	<td class="line x" title="50:187	For instance, if we want to count words in the corpus and these words are marked with their parts of speech and base forms such as <W pos=VBD l=look>looked</W> we should be able to specify to a counting program which fields of the element it should consider." ></td>
	<td class="line x" title="51:187	We might be interested in counting only word-tokens themselves and in this case two wordtokens 'looked' will be counted as one regardless whether they were past verbs or participles." ></td>
	<td class="line x" title="52:187	Using the same markup we can specify that the 'pos' attribute should be considered as well, or we can count just parts-of-speech or lemmas." ></td>
	<td class="line x" title="53:187	A special view pattern provides such information for a counting tool." ></td>
	<td class="line x" title="54:187	A view pattern consists of names of the attributes to consider with the symbol # representing the character data field of the element." ></td>
	<td class="line x" title="55:187	For instance:  {#} this view pattern specifies that only the character data field of the element should be considered ('looked');  {#}{pos} this view pattern says that the 1For instance, unlike common texts, headers often have capitalized words which are not proper nouns." ></td>
	<td class="line x" title="56:187	2There are a number of constraints on SGML markup in its normalized version." ></td>
	<td class="line x" title="57:187	373 <DOC> <H>This is a Title</H> <BODY> <S n=l>This is the first sentence with character data only</S> <S n=2>There can be sub-elements such as <W pos=NN>noun groups</W> inside sentences.</S> </BODY> <H>This is another Title</H> <BODY> <S n=l>This is the first sentence of the second section</S> <S n=2>Here is a marked date <D d=l m=11 y=1996>1st October 1996</D> in this sentence.</S> </BODY> </DOC> Figure 1: SGML marked text." ></td>
	<td class="line x" title="58:187	Corpus Viewers CORPORA X 2 test \[ Retrieval ~ Indexing L' I Tools Tools i~i I INDEXING & RETRIEVAL Convert to SGML J Convert I 'q to NSGMI_J I rJ Element Count 2ontingency Table Builder COUNTING TOOLS Logistic Regression I Dendrogram Builder I Tokenizer I I \[ POS Taggerl I N'." ></td>
	<td class="line x" title="59:187	Lemmatizer I I I I I ! I '''  '' ' Chunker I m j i, sgml tr I I I I I   ,,,o I A No To s K ' J iNs SGML Record/Field Converters ( sgdelmarkup ) _2_ INFERENCE TOOLS UNIX UTILITIES Figure 2: Workbench Architecture." ></td>
	<td class="line x" title="60:187	Thick arrows represent NSGML 'fat' data flow and thin arrows normal (record/field) data flow." ></td>
	<td class="line x" title="61:187	374 character data and the value of the 'pos' attribute should be considered ('looked/VBD');  {1} this view pattern says that only the lemmas will be counted ('look'); 4 The Workbench Using the idea of data level integration the workbench described in this paper promotes the idea of independence of the text representation and the text analysis phases." ></td>
	<td class="line x" title="62:187	At the representation phase the text is converted from a sequence of characters to features of interest by means of the annotation tools." ></td>
	<td class="line x" title="63:187	At the analysis phases those features are used by the tools such as statistics gathering and inference tools for finding significant correlations in the texts." ></td>
	<td class="line x" title="64:187	The analysis tools are independent of particular assumptions about the nature of the feature-set and work on the abstract level of feature-elements which are represented as SGML items." ></td>
	<td class="line x" title="65:187	Figure 2 shows the main modules and data flow between them." ></td>
	<td class="line x" title="66:187	At the first phase documents are represented in an SGML format and then converted to the normalized SGML (NSGML) markup." ></td>
	<td class="line x" title="67:187	Unfortunately there is no general way to convert free text into SGML since it is not trivial to recognize the layout of a text; however, there already is a large body of SGML-marked texts such as, for instance, the British National Corpus." ></td>
	<td class="line x" title="68:187	The widely used on WWW format HTML is based on SGML and requires only a limited amount of efforts to be converted to strict SGML." ></td>
	<td class="line x" title="69:187	Other markup formats such as LATEX can be relatively easily converted to SGML using publicly available utilities." ></td>
	<td class="line x" title="70:187	In many cases one can write a perl script to convert a text in a known layout, for example, Penn Treebank into SGML." ></td>
	<td class="line x" title="71:187	In the simplest case one can put all the text of a document as character data under, for instance, a D0C element." ></td>
	<td class="line x" title="72:187	Such conversions are relatively easy to implement and they can be done 'on the fly' (i.e. in a pipe), thus without the need to keep versions of the same corpus in different formats." ></td>
	<td class="line x" title="73:187	The conversion from arbitrary SGML to NSGML is well defined and is done by a special tool (nsgml) 'on the fly'." ></td>
	<td class="line x" title="74:187	The NSGML stream is then sent to the annotation tools which convert the sequence of characters in specified by the nsl-queries parts of the stream into SGML elements." ></td>
	<td class="line x" title="75:187	At the annotation phase the tools mark up the text elements and their features: words, words with their part-of-speech, syntactic groups, pairs of frequently co-occuring words, sentence length or any other features to be modelled." ></td>
	<td class="line x" title="76:187	The annotated text can be used by other tools which rely on the existence of marked features of interest in the text." ></td>
	<td class="line x" title="77:187	For instance, the statistic gathering tools employ standard algorithms for counting frequencies of the events and are not aware of the nature of these events." ></td>
	<td class="line x" title="78:187	They work with SGML elements which represent features we want to account for in the text." ></td>
	<td class="line x" title="79:187	So these tools are called with the specification of which SGML elements to consider, and what should be the relation between those elements." ></td>
	<td class="line x" title="80:187	Thus the same tools can count words and noun-groups, collect contingency tables for a pair of words in the same sentence or for a pair of sentences in the same or different documents." ></td>
	<td class="line x" title="81:187	For instance, for automatic alignment we might be interested in finding frequently co-occuring words in two sentences, one of which is in English and the other one in French." ></td>
	<td class="line x" title="82:187	Then the collected statistics are used with the standard tools for statistical inferences to produce desirable language models." ></td>
	<td class="line x" title="83:187	The important point here is that neither statistics gathering nor inference tools are aware of the nature of the statistics they work with abstract data (SGML elements) and the semantics of the statistical experiments is controlled at the annotation phase where we enrich texts with the features to model." ></td>
	<td class="line x" title="84:187	4.1 Text Annotation Phase At the text annotation phase the text as a sequence of characters is converted into a set of SGML elements which will later be used by other tools." ></td>
	<td class="line x" title="85:187	These elements can be words with their features, phrases, combinations of words, length of sentences, etc. The set of annotation tools is completely open and the only requirement to the integration of a new tool is that it should be able to work with NSGML and pass through the information it is not supposed to change." ></td>
	<td class="line x" title="86:187	An annotation tool takes a specification (nsl-query) of which part of the stream to annotate and all other parts of the stream are passed through without modifications." ></td>
	<td class="line x" title="87:187	Here is the standard set of the annotators provided by our workbench: sgtoken the tokenizer (marks word boundaries)." ></td>
	<td class="line x" title="88:187	Tokenization is at the base of many NLP applications allowing the jump from the level of characters to the level of words, sgtoken is built on a deterministic FSA library similar to the Xerox FSA library, and as such provides the ability to define tokens as regular expressions." ></td>
	<td class="line x" title="89:187	It also provides the ability to define a priority amongst competing token types so one doesn't need to ensure that the token types defined are entirely distinct." ></td>
	<td class="line x" title="90:187	However of greatest interest to us is how it interfaces with the NSGML stream." ></td>
	<td class="line x" title="91:187	The arguments to sgtoken include a specification of the FSA to use for tokenization (there are several pre-defined ones, or the user can define their own in a perl-like regular expression syntax), an nsl-query which syntactically specifies the part of the source stream to process, and specification of the markup to add." ></td>
	<td class="line x" title="92:187	The output of the process is an NSGML data stream which contains all the data of the input stream (in the same order as it appears in the in375 put stream) together with additional markup which tokenizes those parts of the input stream which are specified by the nsl-query parameter." ></td>
	<td class="line x" title="93:187	A call sgtoken -q /DOC/BODY/S <== what to tokenize -s <== use stemdard tokenizer -m W <== markup tokens as W can produce an output like: <W>books</W><W>,</W> <W>for instance</W> This gives rise to a simple data-level semantics -'everything inside a <W> element is a token added by sgtoken'." ></td>
	<td class="line x" title="94:187	However, this semantics is flexible." ></td>
	<td class="line x" title="95:187	For example, if W markup is used for some other purpose, this markup might be changed to T markup." ></td>
	<td class="line x" title="96:187	itpos a POS-tagger (assigns a single part-ofspeech (POS) to a word according to the context it was used)." ></td>
	<td class="line x" title="97:187	This is an HMM tagger akin to that described in (Kupiec, 1992)." ></td>
	<td class="line x" title="98:187	It receives a tokenized NSGML stream and instructions on what is the markup for words (word element label), where to apply tagging (nsl-query), and how to output the assigned information (attribute to assign)." ></td>
	<td class="line x" title="99:187	For instance, we might want to tag only the body-text of a document, and if the tokenizer marked up words as W elements we specify this to the tagger, together with the attribute that is to stand for the part-of-speech in the W element: itpos -q /DOC/BODY/.*/W <== path to words -m pos <== attribute to set with tag resource <== resources spec." ></td>
	<td class="line x" title="100:187	file This call will produce, for instance, <W pos=NNS>books</W><W pos=CM>,</W> <W pos=NNS>pens</W> Here the 'pos' attributes of the word elements 'W' are set by the tagger to pos-tags: NNS plural noun and CM -comma." ></td>
	<td class="line x" title="101:187	We can combine results produced by different taggers in different attributes which is useful for their evaluation." ></td>
	<td class="line x" title="102:187	Itlem the lemmatizer (finds the base form for a word)." ></td>
	<td class="line x" title="103:187	The lemmatizer takes a stream with word elements together with their part-of-speech tags and further enriches the elements assigning a pre-specified attribute with lemmas, such as: <W pos=NNS l=book>books</W><W pos=CM>,</W> <W pos=NNS l=pen>pens</W> ltchtmk syntactic chunker which determines the structural boundaries for syntactic groups such as noun groups and verb groups as, for instance: <NG> <W pos=DT>the</W> <W pos=JJ>good</W> <W pos--NNS l=man>men</W> </NG> The chunker leaves all previously added information in the text and creates a structural element which includes the words of the chunk." ></td>
	<td class="line x" title="104:187	The chunker itself is a combination of a finite state transducer over SGML elements with a grammar for syntactic groups similar in spirit to that of Fidditch (Hindle, 1983)." ></td>
	<td class="line x" title="105:187	This grammar can employ all the fields of the SGML elements." ></td>
	<td class="line x" title="106:187	For instance a rule can say:'If there is an element of type 'W' with character data 'book' and the 'pos' attribute set to 'NN' followed by zero or more elements of type 'W' with the 'pos' attributes set to 'NN' create an 'NG' element and put this sequence under it." ></td>
	<td class="line x" title="107:187	The transducer itself is application independent it rewrites the SGML streams according to a grammar stated in terms of SGML elements." ></td>
	<td class="line x" title="108:187	It was, for instance, applied for the conversion of SGML markup into the LaTex markup." ></td>
	<td class="line x" title="109:187	The presented annotation tools are quite fast: the whole pipeline annotates at a speed of 1500-2000 words per second." ></td>
	<td class="line x" title="110:187	Thus we never store the results of the annotation on disk, but annotate in the pipe shaping the annotation to the task in hand." ></td>
	<td class="line x" title="111:187	Although the tools presented are deterministic and always produce a single annotation there is a way to incorporate multiple analyses into SGML structures using the hyperlinks described in (McKelvie et al. , 1997)." ></td>
	<td class="line x" title="112:187	This initial set of annotation tools serves a wide range of tasks but one can imagine, for instance, a tool which adds the suffix of a word as its attribute <W s=ed>looked</W> or noun group length in words or characters <NG wl=3 cl=10><W> </NG>." ></td>
	<td class="line x" title="113:187	Another point to mention here is that it is quite easy to integrate another tagger or chunker or other tools as long as they obey the NSGML input/output conventions of the workbench or can be encased in a suitable 'wrapper'." ></td>
	<td class="line x" title="114:187	4.2 Counting Tools After the annotation tools have been used to convert the text from a sequence of characters into a sequence of S(3ML elements, the counting tools can count different phenomena in a uniform way." ></td>
	<td class="line x" title="115:187	Like the annotation tools, the counting tools take a specification of which part of the document to count things from (nsl-query) and they also take a specification of the view of an SGML element, i.e. which parts of those elements to consider for comparison." ></td>
	<td class="line x" title="116:187	The element counting program sgcount counts the number of occurrences of certain SGML elements in pre-specified fields of the stream according to a certain view." ></td>
	<td class="line x" title="117:187	For instance, the call sgcount -q /DOC/H/.*/W -v {#}{pos} will count frequencies of words occurring only in the titles of a document at any level of embedding (.*) and considering their character fields and the partof-speech information." ></td>
	<td class="line x" title="118:187	The call 376 sgcount -q /DOC/BODY/.*/W -v {pos} will produce the distribution of parts of speech in the BODY fields of documents." ></td>
	<td class="line x" title="119:187	To count joint events such as co-occurences of a word with other words, there is a tool for building contingency tables sgcontin." ></td>
	<td class="line x" title="120:187	A contingency table 3 records the number of times a joint event happened and the number of times a corresponding single event happened." ></td>
	<td class="line x" title="121:187	For instance, if we are interested in the association between some two words, in the contingency table we will collect the frequency when these two words were seen together and when only one of them was seen." ></td>
	<td class="line x" title="122:187	For instance, the call: sgcontin -q /DOC/BODY/W -v {#} -q2 /DOC/H/W -v2 {#} will give us a table of the associations between words in the body text of a document with the words in the title." ></td>
	<td class="line x" title="123:187	Here, the program takes the query and the view for each element of a joint event." ></td>
	<td class="line x" title="124:187	We can also specify the relative position of elements to each other." ></td>
	<td class="line x" title="125:187	For instance, the call sgcontin -q /DOC/BODY/W -v {#} -q2 {q}/W\[-l\] -v2 {#} will build a contingency for a word with words to the left of it." ></td>
	<td class="line x" title="126:187	Both sgcount and sgcontin are extremely fast they can process a million word corpus in a few minutes, so it is cheap to collect statistics of different sorts from the corpus." ></td>
	<td class="line x" title="127:187	4.3 The Inference Tools The statistics gathered at the counting phase can be used for different kinds of statistical inferences." ></td>
	<td class="line x" title="128:187	For instance, using mutual information (MI) or X 2 test on a contingency table we can find 'sticky' pairs of items." ></td>
	<td class="line x" title="129:187	Thus if we collected a contingency table of words in titles vs. words in body-texts we can infer which words in a title strongly imply certain words in a body text." ></td>
	<td class="line x" title="130:187	Using a contingency table for collecting left and right contexts of words we can run tests on similarity of the context vectors such as Spearman Rank Correlation Coefficient, Manhattan Metric, Euclidean Metric, Divergence, etc, to see which two words have the most similar context vectors and thus behave similarly in the corpus." ></td>
	<td class="line x" title="131:187	Such similarity would imply closeness of those words and the words can be clustered in one class." ></td>
	<td class="line x" title="132:187	The dendrogram tool then can be used to represent clustered words in a hierarchical classification." ></td>
	<td class="line x" title="133:187	There is a wide body of publicly available statistical software (StatXact, Cytel Software, etc) which 3Here we will talk only about two-way contingency tables but our tools can build n-way tables." ></td>
	<td class="line x" title="134:187	can be used with the collected statistics for performing different sorts of statistical inferences and one can chose the test and package according to the task." ></td>
	<td class="line x" title="135:187	4.4 Indexing/Retrieval Tools For fast access to particular locations in an SGML corpus we can index the text by using features of interest." ></td>
	<td class="line x" title="136:187	Again, as in the case with the statistical tools, the indexing tools work on the level of abstract SGML elements and take as arguments which element should be indexed and what are the indexing units." ></td>
	<td class="line x" title="137:187	For instance, we can index documents by word-tokens in their sentences, or by sentence length, or we can index sentences themselves by their tokens, or by tokens together with their parts-ofspeech or by other features (marked by the annotation tools)." ></td>
	<td class="line x" title="138:187	Then we can instantly retrieve only those documents or sentences which possess the set of features specified by the user or another tool." ></td>
	<td class="line x" title="139:187	If, for instance, we index sentences by their words we can collect the collocations for a particular word or a set of words in seconds." ></td>
	<td class="line x" title="140:187	The mkindex program takes an annotated NSGML stream and indexes elements specified by an nsl-query by their sub-elements specified by another nsl-query: mkindex -dq .*/BODY/S <== index all S in BODY -iq .*/W <= by Ws in these Ss -v {#} <= using only character data of W The 'v' specifies the view of the indexing units." ></td>
	<td class="line x" title="141:187	Such call will produce a table of indexing units (words in our case) with references (sequential numbers) to the indexed elements (sentences) they were found in." ></td>
	<td class="line x" title="142:187	For instance: book 23 78 96 584 says that word 'book' was found in the sentences 23 78 96 and 584." ></td>
	<td class="line x" title="143:187	Next we have to relate (hook) the indexed elements (sentences) to the locations on disk." ></td>
	<td class="line x" title="144:187	Note here that for the indexing itself we used sentences with annotations (they included W elements) but for hooking of these sentences to their absolute locations the annotation is not needed if we want to retrieve sentences as character data." ></td>
	<td class="line x" title="145:187	The call: MakeSGMLHook -dq .*/BODY/S filename.sgm finds all sentences in the BODY elements of the file and stores their locations: 0 12344 i 33444 So sentence 0, for instance, starts from the offset 12344 in the file." ></td>
	<td class="line x" title="146:187	To retrieve a sentence with a certain word (or set of words) we look up in which sentences this word was found and then look up the locations of those sentences in the file." ></td>
	<td class="line x" title="147:187	377 In some cases when the corpus to index already has a required annotation there is no need for our annotation tools." ></td>
	<td class="line x" title="148:187	An example of such case is the British National Corpus (BNC)." ></td>
	<td class="line x" title="149:187	The BNC itself is distributed in SGML format with annotation, thus we used the indexing tools directly after the pipeline conversion into NSGML." ></td>
	<td class="line x" title="150:187	4.5 Utilities and Filters One of the attractive features of data-level integration is the availability of a number of very flexible data manipulation and filtering utilities." ></td>
	<td class="line x" title="151:187	For the record/field data format of the UNIX environment, such utilities include grep, sed, count, awk, sort, tr and so on." ></td>
	<td class="line x" title="152:187	These utilities allow flexible and selective data-manipulation applications to be built by 'piping' data through several utility processes." ></td>
	<td class="line x" title="153:187	SGML is a more powerful data representation language than the simple record/field format assumed by many of these UNIX utilities, and consequently new utilities which exploit the additional power of the SGML representation format are required." ></td>
	<td class="line x" title="154:187	We shall briefly describe the sggrep and sgdelmarkup utilities." ></td>
	<td class="line x" title="155:187	sggrep is an NSGML-aware version of grep." ></td>
	<td class="line x" title="156:187	This utility selects parts of the NSGML stream according to whether or not a regular expression appears in character data at a specific place." ></td>
	<td class="line x" title="157:187	As arguments, it takes two queries, the first (context query) tells it what parts of the stream to select or reject, and the second (content query) tells it where to look for the regular expression (within the context of the first)." ></td>
	<td class="line x" title="158:187	Optional flags tell the utility whether to include or omit parts of the stream falling outside the scope of the first query, or whether to reverse the polarity of the decision to include or exclude (the '-v' flag)." ></td>
	<td class="line x" title="159:187	For instance, the call: sggrep -qx .*/SECT\[t=SUBSECTION\] <== context -qt .*/TITLE <== content miocar.+\[ \]+nf <== regular expr." ></td>
	<td class="line x" title="160:187	will produce a stream of those SECT elements whose attribute 't' has the value SUBSECTION and which contain somewhere an element called TITLE with contiguous characters matched by the regular expression 'miocar.+\[ \]+inf' in the character data field." ></td>
	<td class="line x" title="161:187	sgdelmarkup is a utility which converts SGML elements into the record field format adopted by UNIX so the information can be further processed by the standard UNIX utilities such as perl, awk, sed, tr, etc. This tool takes an nsl-query as the specification which elements to convert and the view to the elements as the specification how to convert them." ></td>
	<td class="line x" title="162:187	For instance, the call: sgdelmarkup -q .*/W\[pos=NN \] -v '{#} {i}' will convert all nouns into 'word lemma' format: <W l=book pos=NN>books</W> ==> books book As an example of the combined functionality we can first extract from an NSGML stream elements of interest by means of sggrep, then convert them into the record-field format using sgdelmarkup and then sort them using the standard UNIX utility sort." ></td>
	<td class="line x" title="163:187	5 Putting it all together Here we present a simple example of extracting and clustering the terminology from a medical corpus using the tools from the workbench." ></td>
	<td class="line x" title="164:187	The PDS is a corpus of short Patient Discharge Summaries written by a doctor to another doctor." ></td>
	<td class="line x" title="165:187	Usually such a letter comprises a structured header and a few paragraphs describing the admission, investigations, treatments and the discharge of the patient." ></td>
	<td class="line x" title="166:187	We easily converted thes texts into SGML format with the structure PDSHEADER-BODY writing a few lines of perl script." ></td>
	<td class="line x" title="167:187	We did not keep a separate SGML version of the corpus and converted it 'on the fly'." ></td>
	<td class="line x" title="168:187	Then we applied the annotation as described in section 4.1 thus marking up words with their lemmas and parts-of-speech and noun/verb group boundaries." ></td>
	<td class="line x" title="169:187	Putting in this annotation is computationally cheap and we did it in the pipe rather than storing the annotated text on disk: cat *.pds i nawk-f pds2sgml.awk I nsgml i sgtoken -q '.*/BODY' -m W I itpos -q '.*/BODY/.*/W' -mpos \[ itlem -q '.*/BODY/.*/W' -m 1 l itchunk -q '.*/BODY/S -m NG ng-gram l itchunk -q '.*/BODY/S -m VG vg-gram We annotated only the body-text of the summaries and as the result of the annotation phase we obtained sentence elements 'S' with noun-group ('NG'), verb-group ('VG') and word ('W') elements inside, such as: <S n=l> -sentence N 1 <NG> -start of noun group <W pos=DT>This</W> <W pos=CD>70</W> <W pos=NN>year</W> <W pos=JJ>old</W> <W pos=NN>man</W> </NG> -end of noun group <VG> -start of verb group <W pos=BED l=be>was</W> <W pos=VBN l=admit>admitted</W> </VG> -end of verb group <W pos=IN>from</W>  -other phrases and words <W pos=SENT>." ></td>
	<td class="line x" title="170:187	</W> </S> -end of sentence 1 Following (Justeson&Katz, 1995) we extracted terminological multi-word phrases as frequent multi378 word noun groups." ></td>
	<td class="line x" title="171:187	We collected all noun-groups with their frequencies of occurrences by running: sgdelmarkup -q '.*/NG/W' -v {#} I sgcount -q /PDS/BODY/S/NG -v {#} The sgdelmarkup call substituted all W elements in noun groups with their character data: <NG><W pos=DT>the</W> <W pos=NN>man</W></NG> <NG>the man</NG> and sgcount counted all NGs considering only their character fields." ></td>
	<td class="line x" title="172:187	Here are the most frequent noun groups found in the corpus: 463 cardiac catheterisation 207 Happy Valley Hospital 144 ischaemic heart disease 114 Consultant Cardiologist 111 Isosorbide Mononitrate 108 the right coronary artery Then we clustered the extracted terms by their left and right contexts." ></td>
	<td class="line x" title="173:187	Using the sgcontin tool we collected frequencies of the two words on the left and two words on the right as the context." ></td>
	<td class="line x" title="174:187	We also imposed a constraint that if there is a group rather than a word, we take only the head word: the last word in a noun group or the last verb in a verb group." ></td>
	<td class="line x" title="175:187	As the result of such clustering we obtained four distinctive clusters: body-parts ('the right coronary artery'), patient-conditions ('70% severe stenosis' ), treatments ('coronary bypass operation') and investigations ('cardiac catheterisation')." ></td>
	<td class="line x" title="176:187	Unlike simple word-level clustering this clustering revealed some interesting details about the terms." ></td>
	<td class="line x" title="177:187	For instance, the terms 'left coronary artery' and 'right coronary artery' were clustered together whereas 'occluded coronary artery' was clustered with 'occlusion' and 'stenosis' thus uncovering the fact that it is of the 'patient-condition' type rather than of the 'bodypart'." ></td>
	<td class="line x" title="178:187	A more detailed description of the process for uncovering corpus regularities can be found in (Mikheev&Finch, 1995)." ></td>
	<td class="line x" title="179:187	6 Conclusion In this paper we outlined a workbench for investigating corpus regularities." ></td>
	<td class="line x" title="180:187	The important concept of the workbench is the uniform representation of corpus data by using SGML markup at the corpus annotation phase." ></td>
	<td class="line x" title="181:187	This allows us to use the same statistics gathering and inference tools with different annotations for modelling on different features." ></td>
	<td class="line x" title="182:187	The workbench is completely open to the integration of new tools but imposes SGML requirements on their input/output interface." ></td>
	<td class="line x" title="183:187	The pipeline architecture of our workbench is not particularly suited for nice GUIs but there are publicly available visual pipeline builders which can be used with our tools." ></td>
	<td class="line x" title="184:187	The tools described in this paper and some other tools are available by contacting the authors." ></td>
	<td class="line x" title="185:187	Most of the tools are implemented in c/c++ under the UNIX environment and now we are porting them to the NT platform since it supports the pipes which are essential to our architecture." ></td>
	<td class="line x" title="186:187	7 Acknowledgements This work was carried out in the HCRC Language Technology Group, partly with support from the UK EPSRC project 'Text Tokenisation Tool'." ></td>
	<td class="line x" title="187:187	The HCRC is a UK ESRC funded institution." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P97-1061
Retrieving Collocations By Co-Occurrences And Word Order Constraints
Shimohata, Sayori;Sugio, Toshiyuki;Nagata, Junji;"></td>
	<td class="line x" title="1:169	Retrieving Collocations by Co-occurrences and Word Order Constraints Sayori Shimohata, Toshiyuki Sugio and Junji Nagata Kansai Laboratory, Research &: Development Group Oki Electric Industry Co. , Ltd. Crystal Tower 1-2-27, Shiromi, Chuo-ku, Osaka, 540, Japan { sayori, sugio, nagat a} kansai." ></td>
	<td class="line x" title="2:169	oki." ></td>
	<td class="line x" title="3:169	co. j p Abstract In this paper, we describe a method for automatically retrieving collocations from large text corpora." ></td>
	<td class="line x" title="4:169	This method retrieve collocations in the following stages: 1) extracting strings of characters as units of collocations 2) extracting recurrent combinations of strings in accordance with their word order in a corpus as collocations." ></td>
	<td class="line x" title="5:169	Through the method, various range of collocations, especially domain specific collocations, are retrieved." ></td>
	<td class="line x" title="6:169	The method is practical because it uses plain texts without any information dependent on a language such as lexical knowledge and parts of speech." ></td>
	<td class="line x" title="7:169	1 Introduction A collocation is a recurrent combination of words, ranging from word level to sentence level." ></td>
	<td class="line x" title="8:169	In this paper, we classify collocations into two types according to their structures." ></td>
	<td class="line x" title="9:169	One is an uninterrupted collocation which consists of a sequence of words, the other is an interrupted collocation which consists of words containing one or several gaps filled in by substitutable words or phrases which belong to the same category." ></td>
	<td class="line x" title="10:169	The features of collocations are defined as follows:  collocations are recurrent  collocations consist of one or several lexical units  order of units are rigid in a collocation." ></td>
	<td class="line x" title="11:169	For language processing such as machine translation, a knowledge of domain specific collocations is indispensable because what collocations mean are different from their literal meaning and the usage and meaning of a collocation is totally dependent on each domain." ></td>
	<td class="line x" title="12:169	In addition, new collocations are produced one after another and most of them are technical jargons." ></td>
	<td class="line oc" title="13:169	There has been a growing interest in corpus-based approaches which retrieve collocations from large corpora (Nagao and Mori, 1994), (Ikehara et al. , 1996) (Kupiec, 1993), (Fung, 1995), (Kitamura and Matsumoto, 1996), (Smadja, 1993), (Smadja et al. , 1996), (Haruno et al. , 1996)." ></td>
	<td class="line n" title="14:169	Although these approaches achieved good results for the task considered, most of them aim to extract fixed collocations, mainly noun phrases, and require the information which is dependent on each language such as dictionaries and parts of speech." ></td>
	<td class="line x" title="15:169	From a practical point of view, however, a more robust and flexible approach is desirable." ></td>
	<td class="line x" title="16:169	We propose a method to retrieve interrupted and uninterrupted collocations by the frequencies of co-occurrences and word order constraints from a monolingual corpus." ></td>
	<td class="line x" title="17:169	The method comprises two stages: the first stage extracts sequences of words (or characters) t from a corpus as units of collocations and the second stage extracts recurrent combinations of units and constructs collocations by arranging them in accordance with word order in the corpus." ></td>
	<td class="line x" title="18:169	2 Algorithm 2.1 Extracting units of collocation (Nagao and Mori, 1994) developed a method to calculate the frequencies of strings composed of n characters(a grams)." ></td>
	<td class="line x" title="19:169	Since this method generates all n-character strings appeared in a text, the output contains a lot of fragments and useless expressions." ></td>
	<td class="line x" title="20:169	For example, even if 'local', 'area', and 'network' always appear as the substrings of '% local area network' in a corpus, this method generates redundant strings such as 'a local', 'a local area' and 'area network'." ></td>
	<td class="line x" title="21:169	To filter out the fragments, we measure the distribution of adjacent words preceding and following 1A word is recognized as a minimum unit in such a language as English where writespace is used to delimit words, while a character is recognized as that in such languages as Japanese and Chinese which have no word delimiters." ></td>
	<td class="line x" title="22:169	Although the method described in this paper is applicable to either kinds of languages, we have taken English as an example." ></td>
	<td class="line x" title="23:169	476 the strings using entropy threshold." ></td>
	<td class="line x" title="24:169	This is based on the idea that adjacent words will be widely distributed if the string is meaningful, and they will be localized if the string is a substring of a meaningful string." ></td>
	<td class="line x" title="25:169	Taking the example mentioned above, the words which follow % local area' are practically identified as 'network' because % local area' is a substring of % local area network' in the corpus." ></td>
	<td class="line x" title="26:169	On the contrary, the words which follow % local area network' are hardly identified because 'a local area network' is a unit of expression and innumerable words are possible to follow the string." ></td>
	<td class="line x" title="27:169	It means that the distribution of adjacent words is effective to judge whether the string is an appropriate unit or not." ></td>
	<td class="line x" title="28:169	We introduce entropy value, which is a measure of disorder." ></td>
	<td class="line x" title="29:169	Let the string be str, the adjacent words wlwn, and the frequency of str freq(str)." ></td>
	<td class="line x" title="30:169	The probability of each possible adjacent word p(wi) is then: y~eq(wi) p(wi)freq(str) (1) At that time, the entropy of str H(str) is defined as: 7l H(str) = ~ -p(wi)logp(wi) (2) i=1 H(str) takes the highest value if n = freq(str) and 1 for all and it takes the lowest value 0 p(wi) = -~ wi, if n = 1 and p(wi) = 1." ></td>
	<td class="line x" title="31:169	Calculating the entropy of both sides of the string, we adopt the lower one as the entropy of the string." ></td>
	<td class="line x" title="32:169	Str is accepted only if the following inequation is satisfied: H(str) > Tentropu (3) Fragmental strings such as 'a local' and 'area network' are filtered out with these procedures because their entropy values are expected to be small." ></td>
	<td class="line x" title="33:169	Most of the strings extracted in this stage are meaningful units such as compound words, prepositional phrases, and idiomatic expressions." ></td>
	<td class="line x" title="34:169	These strings are uninterrupted collocations of themselves while they are used in the next stage to construct collocations." ></td>
	<td class="line x" title="35:169	This method is useful for the languages without word delimiters, and for the other languages as well." ></td>
	<td class="line x" title="36:169	2.2 Extracting collocations By the use of each string derived in the previous stage, this stage extracts strings which frequently co-occur with the string and constructs them as a collocation." ></td>
	<td class="line x" title="37:169	It is based on the idea that there is a string which is used to induce a collocation." ></td>
	<td class="line x" title="38:169	We call this string % key string', hereafter." ></td>
	<td class="line x" title="39:169	The followings are the procedures to retrieve a collocation: 1." ></td>
	<td class="line x" title="40:169	Take a key string strk from the strings stri(i = 1n), and retrieve sentences containing strk from the corpus." ></td>
	<td class="line x" title="41:169	2." ></td>
	<td class="line x" title="42:169	Examine how often each possible combinations of str~ and stri co-occurs, and extract stri if the frequency exceeds a given threshold Tire q. 3." ></td>
	<td class="line x" title="43:169	Examine every two strings stri and strj and refine them by the following steps alternately:  Combine stri and strj when they overlap or adjoin each other and the following inequation is satisfied: freq(stri, strj ) freq(stri) > Tratio (4)  Filter out stri if strj subsumes stri and the following inequation is satisfied: freq(strj) freq(srti) >Tratio (5) 4." ></td>
	<td class="line x" title="44:169	Construct a collocation by arranging the strings stri in accordance with the word order in the corpus." ></td>
	<td class="line x" title="45:169	The second step and the third step narrow down the strings to the units of collocation." ></td>
	<td class="line x" title="46:169	Through these steps, only the strings which significantly co-occur with the key string strk are extracted." ></td>
	<td class="line x" title="47:169	The second step eliminates the strings that are not frequent enough." ></td>
	<td class="line x" title="48:169	Consider the example of Figure 1." ></td>
	<td class="line x" title="49:169	This is a list of sentences containing the key string 'Refer to' retrieved and each underlined string corresponds to a string stri." ></td>
	<td class="line x" title="50:169	Assuming the frequency threshold Tlr~q as 2, the strings which co-occur with str~ more than twice are extracted in the second step." ></td>
	<td class="line x" title="51:169	Table 1 shows the result of this step." ></td>
	<td class="line x" title="52:169	Although it is very simple technique, almost all the useless strings are excluded through this step." ></td>
	<td class="line x" title="53:169	stri f req( strk, stri ) the 4 manual 4 for specific instructions 3 on 2 Table 1: Result of the second step The third step reorganizes the strings to be optimum units in the specific context." ></td>
	<td class="line x" title="54:169	This is based on the idea that a longer string is more significant as a unit of collocations if it is frequent enough." ></td>
	<td class="line x" title="55:169	Assuming that the threshold Tra~io is 0.75, first, a string 'manual for specific instructions' is produced as the inequation (4) is satisfied." ></td>
	<td class="line x" title="56:169	Next, 'manual' and 'for specific instructions' are deleted as the inequation (5) is satisfied." ></td>
	<td class="line x" title="57:169	This process is repeated until no string satisfies the inequations." ></td>
	<td class="line x" title="58:169	Table 2 shows a result of this step." ></td>
	<td class="line x" title="59:169	The fourth step constructs a collocation by arranging the strings in accordance with the word order in the sentences retrieved in the first step." ></td>
	<td class="line x" title="60:169	Taking stri in order of frequency, this step determines 477 Refer to the appropriate manual for instructions o_nn Refer t.o. the manual for specific instructions." ></td>
	<td class="line x" title="61:169	Refer to the installation manual for specific instructions fo__r Refer to the manual for specific in'~~-~ffn ~ Figure 1: Sentences containing 'Refer to' l stri f req( strk, stri ) the 4 manual for specific instructions 3 on 2 Table 2: Result of the third step where stri is placed in a collocation." ></td>
	<td class="line x" title="62:169	In this example, the position of 'the' is examined first." ></td>
	<td class="line x" title="63:169	According to the sentences shown in Figure 1, 'the' is always placed next to 'Refer to'." ></td>
	<td class="line x" title="64:169	Then its position is determined to follow 'Refer to'." ></td>
	<td class="line x" title="65:169	Next, the position of 'manual for specific instructions' is examined and it is determined to follow a gap placed after 'Refer to the'." ></td>
	<td class="line x" title="66:169	Finally, the following collocation is produced: 'Refer to the  manual for specific instructions on ' The broken lines in the collocation indicates the gaps where any substitutable words or phrases can be filled in." ></td>
	<td class="line x" title="67:169	In the example, 'appropriate' or 'installation' is filled in the first gap." ></td>
	<td class="line x" title="68:169	Thus, we retrieve an arbitrary length of interrupted or uninterrupted collocation induced by the key string." ></td>
	<td class="line x" title="69:169	This procedure is performed for each string obtained in the previous stage." ></td>
	<td class="line x" title="70:169	By changing the threshold, various levels of collocations are retrieved." ></td>
	<td class="line x" title="71:169	3 Evaluation We performed an experiment for evaluating the algorithm." ></td>
	<td class="line x" title="72:169	The corpus used in the experiment is a computer manual written in English comprising 1,311,522 words (in 120,240 sentences)." ></td>
	<td class="line x" title="73:169	In the first stage of this method, 167,387 strings are produced." ></td>
	<td class="line x" title="74:169	Among them, 650, 1950, 6774 strings are extracted over the entropy threshold 2, 1.5, 1 respectively." ></td>
	<td class="line x" title="75:169	For 650 strings whose entropy is greater than 2, 162 strings (24.9%) are complete sentences, 297 strings (45.7%) are regarded as grammatically appropriate units, and 114 strings (17.5%) are regarded as meaningful units even though they are not grammatical." ></td>
	<td class="line x" title="76:169	This told us that the precision of the first stage is 88.1%." ></td>
	<td class="line x" title="77:169	Table 3 shows top 20 strings in order of entropy value." ></td>
	<td class="line x" title="78:169	They are quite representative of the given domain." ></td>
	<td class="line x" title="79:169	Most of them are technical jargons related to computers and typical expressions used in manual descriptions although they vary in their constructions." ></td>
	<td class="line x" title="80:169	It is interesting to note that the strings which do not belong to the grammatical units also take high entropy value." ></td>
	<td class="line x" title="81:169	Some of them contain punctuation, and some of them terminate in articles." ></td>
	<td class="line x" title="82:169	Punctuation marks and function words in the strings are useful to recognize how the strings are used in a corpus." ></td>
	<td class="line x" title="83:169	Table 4 illustrates how the entropy is changed with the change of string length." ></td>
	<td class="line x" title="84:169	The third column in the table shows the kinds of adjacent words which follow the strings." ></td>
	<td class="line x" title="85:169	The table shows that the ungrammatical strings such as 'For more information on' and 'For more information, refer to' act more cohesively than the grammatical string 'For more information' in the corpus." ></td>
	<td class="line x" title="86:169	Actually, the former strings are more useful to construct collocations in the second stage." ></td>
	<td class="line x" title="87:169	In the second stage, we extracted collocations from 411 key strings retrieved in the first stage (297 grammatical units and 114 meaningful units)." ></td>
	<td class="line x" title="88:169	Necessary thresholds are given by the following set of equations: rI~q ~ x 0.1 ~\]req(str~) Tratio = 0.8 As a result, 269 combinations of units are retrieved as collocations." ></td>
	<td class="line x" title="89:169	Note that collocations are not generated from all the key strings because some of them are uninterrupted collocations in themselves like No. 2 in Table 3." ></td>
	<td class="line x" title="90:169	Evaluation is done by human check and 180 collocations are regarded as meaningful." ></td>
	<td class="line x" title="91:169	The precision is 43.8% when the number of meaningful collocation is divided by the number of the key strings and 66.9% when it is divided by the number of the collocations retrieved in the second stage 2." ></td>
	<td class="line x" title="92:169	Table 5 shows the collocations extracted with the underlined key strings." ></td>
	<td class="line x" title="93:169	The table indicates that arbitrary length of collocations, which are frequently used in computer manuals, are retrieved through the method." ></td>
	<td class="line x" title="94:169	As the method focuses on the cooccurrence of strings, most of the collocations are specific to the given domain." ></td>
	<td class="line x" title="95:169	Common collocations are tend to be ignored because they are not used repeatedly in a single text." ></td>
	<td class="line x" title="96:169	It is not a serious problem, 2Usually the latter ratio is adopted as precision." ></td>
	<td class="line x" title="97:169	478 however, becausecommon collocations are limited in number and we can efficiently obtain them from dictionaries or by human reflection." ></td>
	<td class="line x" title="98:169	No. 7 and 8 in Table 5 are the examples of invalid collocations." ></td>
	<td class="line x" title="99:169	They contain unnecessary strings such as 'to a' and ', the' in them." ></td>
	<td class="line x" title="100:169	The majority of invalid collocations are of this type." ></td>
	<td class="line x" title="101:169	One possible solution is to eliminate unnecessary strings at the second stage." ></td>
	<td class="line x" title="102:169	Most of the unnecessary strings consist of only punctuation marks and function words." ></td>
	<td class="line x" title="103:169	Therefore, by filtering out these strings, invalid collocations produced by the method should be reduced." ></td>
	<td class="line x" title="104:169	Figure 2 summarizes the result of the evaluation." ></td>
	<td class="line x" title="105:169	In the experiment, 573 strings are retrieved as appropriate units of collocations and 180 combinations of units are retrieved as appropriate collocations." ></td>
	<td class="line x" title="106:169	Precision is 88.1% in the first stage, and 66.9% in the second stage." ></td>
	<td class="line x" title="107:169	1st stage 2nd stage CS= 162(24.9%) GU=297(45.7%) MU=114(17.5%) F=77(11.9%) MC=180(43.8%) F=89(21.7%) NC= 142(34.5%) CS: complete sentences GU: grammatical units MU: meaningful units MC: meaningful collocations F: fragments NC: not captured Figure 2: Summary of evaluation Although evaluation of retrieval systems is usually performed with precision and recall, we cannot examine recall rate in the experiment." ></td>
	<td class="line x" title="108:169	It is difficult to recognize how many collocations are in a corpus because the measure differs largely dependent on the domain or the application considered." ></td>
	<td class="line x" title="109:169	As an alternative way to evaluate the algorithm, we are planning to apply the collocations retrieved to a machine translation system and evaluate how they contribute to the quality of translation." ></td>
	<td class="line oc" title="110:169	4 Related work Algorithms for retrieving collocations has been described (Smadja, 1993) (Haruno et al. , 1996)." ></td>
	<td class="line oc" title="111:169	(Smadja, 1993) proposed a method to retrieve collocations by combining bigrams whose cooccurrences are greater than a given threshold 3." ></td>
	<td class="line o" title="112:169	In their approach, the bigrams are valid only when there are fewer than five words between them." ></td>
	<td class="line o" title="113:169	This is based on the assumption that 'most of the lexical relations involving a word w can be retrieved by examining the neighborhood of w wherever it occurs, within a span of five (-5 and +5 around w) words'." ></td>
	<td class="line n" title="114:169	While the assumption is reasonable for some languages such as English, it cannot be applied to all the languages, especially to the languages without word delimiters." ></td>
	<td class="line x" title="115:169	(Haruno et al. , 1996) constructed collocations by combining a couple of strings 4 of high mutual information iteratively." ></td>
	<td class="line x" title="116:169	But the mutual information is estimated inadequately lower when the cohesiveness between two strings is greatly different." ></td>
	<td class="line x" title="117:169	Take 'in spite (of)', for example." ></td>
	<td class="line x" title="118:169	Despite the fact that 'spite' is frequently used with 'in', mutual information between 'in' and 'spite' is small because 'in' is used in various ways." ></td>
	<td class="line x" title="119:169	Thus, there is the possibility that the method misses significant collocations even though one of the strings have strong cohesiveness." ></td>
	<td class="line x" title="120:169	In contrast to these methods, our method focuses on the distribution of adjacent words (or characters) when retrieving units of collocation and the co-occurrence frequencies and word order between a key string and other strings when retrieving collocations." ></td>
	<td class="line x" title="121:169	Through the method, various kinds of collocations induced by key strings are retrieved regardless of the number of units or the distance between units in a collocation." ></td>
	<td class="line x" title="122:169	Another distinction is that our method does not require any lexical knowledge or language dependent information such as part of speech." ></td>
	<td class="line x" title="123:169	Owing to this, the method have good applicability to many languages." ></td>
	<td class="line x" title="124:169	5 Conclusion In this paper, we described a robust and practical method for retrieving collocations by the cooccurrence of strings and word order constraints." ></td>
	<td class="line x" title="125:169	Through the method, various range of collocations which are frequently used in a specific domain are retrieved automatically." ></td>
	<td class="line x" title="126:169	This method is applicable to various languages because it uses a plain textual corpus and requires only the general information appeared in the corpus." ></td>
	<td class="line x" title="127:169	Although the collocations retrieved by the method are monolingual and they are not available to the machine application for the present, the results will be extensible in various ways." ></td>
	<td class="line x" title="128:169	We plan to compile a knowledge of bilingual collocations by incorporating the method with conventional bilingual approaches." ></td>
	<td class="line x" title="129:169	3This approach is similar to the process of the string refinement described in this paper." ></td>
	<td class="line x" title="130:169	4They call the strings word chunks." ></td>
	<td class="line x" title="131:169	479 No." ></td>
	<td class="line x" title="132:169	str H(str) freq(str) 1 the current functional area 3.8 45 2 Before you install this device : 3.78 44 3 This could introduce data corruption . 3.37 29 4 All rights are reserved . 3.37 29 5 Note that the 2.93 53 6, such as 2.91 87 7 Information on minor numbers is in 2.45 20 8, for example, 2.44 23 9 The default is 2.44 52 10, you can use the 2.26 25 11 to see if the 2.2 24 12 stands for 2.15 30 13 system accounting : 2.14 48 14 These are 2.12 37 15 allocation policy 2.1 21 16 For example, the 2.1 97 17 For more information on 2.1 96 18 permission bits 2.07 26 19 By default, the 2.06 32 20 The syntax for 2.03 57 Table 3: Top 20 strings extracted at the first stage str H(str) n fveq(str) For more 0.13 7 200 For more information 0.33 3 168 For more information, 0.21 4 46 For more information, see 1.03 8 25 For more information, refer to 1.17 6 15 For more information on 2.1 56 96 For more information about 1.69 21 35 Table 4: Strings including 'For more' No." ></td>
	<td class="line x" title="133:169	collocation 1 For more information on  , refer to the  manual." ></td>
	<td class="line x" title="134:169	2 You can use the  to help you." ></td>
	<td class="line x" title="135:169	3 The syntax for  is :  4 output from the execution of commands." ></td>
	<td class="line x" title="136:169	5  , use the  command with the  option 6  have a special meaning in this manual." ></td>
	<td class="line x" title="137:169	7  to a (such as  , and )." ></td>
	<td class="line x" title="138:169	8  if the system or a for a,the Table 5: Examples of collocations extracted at the second stage 480 References Pascale Fung." ></td>
	<td class="line x" title="139:169	1995." ></td>
	<td class="line x" title="140:169	Compiling bilingual lexicon entries from a non-parallel English-Chinese corpus." ></td>
	<td class="line x" title="141:169	In Proceedings of the 3rd Workshop on Very Large Corpora, pages 173-183." ></td>
	<td class="line x" title="142:169	Masahiko Haruno, Satoru Ikehara, and Takefumi Yamazaki." ></td>
	<td class="line x" title="143:169	1996." ></td>
	<td class="line x" title="144:169	Learning Bilingual Collocations by Word-Level Sorting." ></td>
	<td class="line x" title="145:169	In Proceedings o/ the 16th COLING, pages 525-530." ></td>
	<td class="line x" title="146:169	Satoru Ikehara, Satoshi Shirai, and Hajime Uchino." ></td>
	<td class="line x" title="147:169	1996." ></td>
	<td class="line x" title="148:169	A statistical method for extracting uninterrupted and interrupted collocations from very large corpora." ></td>
	<td class="line x" title="149:169	In Proceedings of the 16th COLINC. pages 574-579." ></td>
	<td class="line x" title="150:169	Mihoko Kitamura and Yuji Matsumoto." ></td>
	<td class="line x" title="151:169	1996." ></td>
	<td class="line x" title="152:169	Automatic extraction of word sequence correspondences in parallel corpora." ></td>
	<td class="line x" title="153:169	In Proceedings of the 4th Workshop on Very Large Corpora, pages 7987." ></td>
	<td class="line x" title="154:169	Julian Kupiec." ></td>
	<td class="line x" title="155:169	1993." ></td>
	<td class="line x" title="156:169	An algorithm for finding noun phrase correspondences in bilingual corpora." ></td>
	<td class="line x" title="157:169	In Proceedings of the 31th Annual Meeting of ACL, pages 17-22." ></td>
	<td class="line x" title="158:169	Makoto Nagao and Shinsuke Mori." ></td>
	<td class="line x" title="159:169	1994." ></td>
	<td class="line x" title="160:169	New Method of n-gram statistics for large number of n and automatic extranetion of words and phrases from large text data of Japanese." ></td>
	<td class="line x" title="161:169	In Proceedings of the 15th COLING, pages 611-615." ></td>
	<td class="line x" title="162:169	Frank Smadja." ></td>
	<td class="line x" title="163:169	1993." ></td>
	<td class="line x" title="164:169	Retrieving collocations from text: Xtraet." ></td>
	<td class="line x" title="165:169	In Computational Linguistics, 19(1), pages 143-177." ></td>
	<td class="line x" title="166:169	Frank Smadja, Kathleen MaKeown, and Vasileios Hatzivassiloglou." ></td>
	<td class="line x" title="167:169	1996." ></td>
	<td class="line x" title="168:169	Translating collocations for bilingual lexicons: A statistical approach." ></td>
	<td class="line x" title="169:169	In Computational Linguistics, 22(1), pages 1-38 ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W97-0205
A Lexicon For Underspecified Semantic Tagging
Buitelaar, Paul;"></td>
	<td class="line x" title="1:166	A Lexicon for Underspecified Semantic Tagging Paul Buitelaar Dept. of Computer Science Brandeis University Waltham, MA 02254-9110, USA paulb@cs, brandeis, edu Abstract The paper defends the notion that semantic tagging should be viewed as more than disambiguation between senses." ></td>
	<td class="line x" title="2:166	Instead, semantic tagging should be a first step in the interpretation process by assigning each lexJ.cal item a representation of all of its sy=stematically related senses, from which fuxther semantic processing steps can derive discourse dependent interpretations." ></td>
	<td class="line x" title="3:166	This leads to a new type of semantic lexicon (CoRv.Lzx) that supports underspecified semantic tagging through a design based on systematic polysemous classes and a class-based acquisition of lexical knowledge for specific domains." ></td>
	<td class="line x" title="4:166	1 Underspecified semantic tagging Semantic tagging has mostly been considered as nothing more than disambiguation to be performed along the same lines as part-of-speech tagging: given n lexical items each with m senses apply linguistic heuristics and/or statistical measures to pick the most likely sense for each lexical item (see eg: (Yarowsky, 1Q92) (Stevenson and Wilks, 1997))." ></td>
	<td class="line x" title="5:166	I do not believe this to be the right approach because it blurs the distinction between 'related' (systematic polysemy) and 'unrelated' senses (homonymy : bank bank)." ></td>
	<td class="line x" title="6:166	Although homonyms need to be tagged with a disambiguated sense, this is not necessarily so in the case of systematic polysemy." ></td>
	<td class="line x" title="7:166	There are two reasons for this that I will discuss briefly here." ></td>
	<td class="line x" title="8:166	First, the problem of multiple reference." ></td>
	<td class="line x" title="9:166	Consider this example from the BROWN corpus: \[A long book heavily weighted with milltary technlcalities\]Np, in this edition it is neither so long nor so technical as it was originally." ></td>
	<td class="line x" title="10:166	The discourse marker (it) refers back to an NP that expresses more than one interpretation at the same time." ></td>
	<td class="line x" title="11:166	The head of the NP (book) has a number of systematically related senses that are being expressed simultaneously." ></td>
	<td class="line x" title="12:166	The meaning of book in this sentence cannot be disambiguated between the number of interpretations that are implied: the informational content of the book (military technicalities), its physical appearance (heavily weighted) and the events that are involved in its construction and use (long)." ></td>
	<td class="line x" title="13:166	The example illustrates the fact that disambiguation between related senses is not always possible, which leads to the further question if a discrete distinction between such senses is desirable at all." ></td>
	<td class="line x" title="14:166	A number of researchers have answered this question negatively (see eg: (Pustejovsky, 1995) (Killgariff, 1992))." ></td>
	<td class="line x" title="15:166	Consider these examples from BROWN: (1) fast run-up (of the stock) (2) fast action (by the city government) (3) fast footwork (by Washington) (4) fast weight gaining (5) fast condition (of the track) (6) fast response time (7) fast people (8) fast ball Each use of the adjective 'fast' in these examples has a slightly different interpretation that could be captured in a number of senses, reflecting the different syntactic and semantic patterns." ></td>
	<td class="line x" title="16:166	For instance: 1." ></td>
	<td class="line x" title="17:166	'a fast action' (1, 2, 3, 4) 2." ></td>
	<td class="line x" title="18:166	'a fast state of affairs' (5, 6) 3." ></td>
	<td class="line x" title="19:166	'a fast object' (7, 8) 25 On the other hand all of the interpretations have something in common also, namely the idea of 'speed'." ></td>
	<td class="line x" title="20:166	It seems therefore useful to underspecify the lexical meaning of 'fast' to a representation that captures this primary semantic aspect and gives a general structure for its combination with other lexical items, both locally (in compositional semantics) and globally (in discourse structure)." ></td>
	<td class="line x" title="21:166	Both the multiple reference and the sense enumeration problem show that lexical items mostly have an indefinite number of related but highly discourse dependent interpretations, between which cannot be distinguished by semantic tagging alone." ></td>
	<td class="line x" title="22:166	Instead, semantic tagging should be a first step in the interpretation process by assigning each lexical item a representation of all of its systematically related 'senses'." ></td>
	<td class="line x" title="23:166	Further semantic processing steps derive discourse dependent interpretations from this representation." ></td>
	<td class="line x" title="24:166	Semantic tags are therefore more like pointers to complex knowledge representations, which can be seen as underspecified lexical meanings." ></td>
	<td class="line x" title="25:166	2 CORELEX: A Semantic Lexicon with Systematic Polysemous Classes In this section I describe the structure and content of a lexicon (CORELEX) that builds on the assumptions about lexical semantics and discourse outlined above." ></td>
	<td class="line x" title="26:166	More specifically, it is to be 'structured in such a way that it reflects the lexical semantics of a language in systematic and predictable ways' (Pustejovsky, Boguraev, and Johnston, 1995)." ></td>
	<td class="line x" title="27:166	This assumption is fundamentally different from the design philosophies behind existing lexical semantic resources like WORDNET that do not account for any regularities between senses." ></td>
	<td class="line x" title="28:166	For instance, WORDNET assigns to the noun book the following senses: the content that is being communicated (communicatiofl) and the medium of communication (artifact)." ></td>
	<td class="line x" title="29:166	More accurately, book should be assigned a qualia structure which implies both of these interpretations and connects them to each of the more specific senses that WORDNET assigns: that is, facts, drama and a journal can be part-of the content of a book; a section is part-of both the content and the medium; publication, production and recording are all events in which both the content and the medium aspects of a book can be involved." ></td>
	<td class="line x" title="30:166	An important advantage of the CORELEX approach is more consistency among the assignments of lexical semantic structure." ></td>
	<td class="line x" title="31:166	Consider the senses that WORDNET assigns to door, gate and window: door movable_barrier -,~ artifact entrance ~-~ opening access ~* cognition, knowledge house ~-, ??" ></td>
	<td class="line x" title="32:166	room ~-~ ??" ></td>
	<td class="line x" title="33:166	gate movable_barrier -,~ artifact computer_circult -,~ opening grossAncome -,~ opening window opening -~ opening panel --~ artifact display ~-* cognition, knowledge publication product, production fact dramatic_composltion, dramatic_work record section, subdivision journal Figure I: WORDNET senses for the noun book At the top of the WORDNET hierarchy these seven senses can be reduced to two unrelated 'basic senses': 26 Figure 2: WORDNET senses for the nouns door, window and gate Obviously these are similar words, something which is not expressed in the WORDNET sense assignments." ></td>
	<td class="line x" title="34:166	In the CORELEX approach, these nouns are given the same semantic type, which is underspecifled for any specific 'sense' but assigns them consistently with the same basic lexical semantic structure that expresses the regularities between all of their interpretations." ></td>
	<td class="line x" title="35:166	However, despite its shortcomings WORDNET is a vast resource of lexical semantic knowledge that can m m m mm m \[\] m m n \[\] m m n m m m m m n m m be mined, restructured and extended, which makes it a good starting point for the construction of CORELEX." ></td>
	<td class="line x" title="36:166	The next sections describe how systematic polysem0us classes and underspecified semantic types can be derived from WORDNET." ></td>
	<td class="line x" title="37:166	In this paper I only consider classes of noun,s, but the process described here can also be applied to other parts of speech." ></td>
	<td class="line x" title="38:166	2.1 Systematic polysemous classes We can arrive at classes of systematically polysemous lexical items by investigating which items share the same senses and are thus polysemous in the same way." ></td>
	<td class="line x" title="39:166	This comparison is done at the top levels of the WORDNET hierarchy." ></td>
	<td class="line x" title="40:166	WORDNET does not have an explicit level structure, but for the purpose of this research one can distinguish a set of 32 =basic senses' that partly coincides with, but is not based directly on WORDNET'S list of 26 'top types': act (act), agent (agt), animal (~.m), artifact (art), attribute (air), blunder (bln), cell (cel), chemical (chm), communication (corn), event (evl;), food (rod), form (frm), group_biological (grb), group (grp), group_social (grs), h-m~n (hum), llnear_measure (1me), location (loc), 1ocation_geographical (log), measure (mea), natural_object (nat), phenomenon (p\]m), plant (plt), possession (pos), part (prt), psychological (psy), quantity_definite (qud), quantity_indefinite (qui), relation (re1), space (spc), state (sta), time (tree) Figure 3 shows their distribution among noun stems in the BROWN corpus." ></td>
	<td class="line x" title="41:166	For instance there are 2550 different noun stems (with 49,824 instances) that have each 2 out of the 32 'basic senses' assigned to them in 238 different combinations (a subset of 322 = 1024 possible combinations)." ></td>
	<td class="line x" title="42:166	We now reduce all of WORDNET'S sense assignments to these basic senses." ></td>
	<td class="line x" title="43:166	For instance, the seven different senses that WORDNET assigns to the lexical item book (see Figure I above) can be reduced to the two basic senses: 'art corn'." ></td>
	<td class="line x" title="44:166	We do this for each lexical item and then group them into classes according to their assignments." ></td>
	<td class="line x" title="45:166	From these one can filter out those classes that have only one member because they obviously do not represent a systematically polysemous class." ></td>
	<td class="line x" title="46:166	The lexical items in those classes have a highly idiosyncratic behavior and are most likely homonyms." ></td>
	<td class="line x" title="47:166	This leaves 27 senses comb's stems instances 2 238 2550 49824 3 379 936 35608 4 268 347 22543 5 148 154 15345 6 52 52 5915 7 27 27 5073 8 10 10 3273 9 3 3 1450 I0 1 1 483 11 2 2 959 12 1 1 441 1161 10797 140914 Figure 3: Polysemy of nouns in BROWN a set of 442 polysemous classes, of which Figure 4 gives a selection: act art evt rel act art log act evt nat chin sta com prt frm sta line qud loc psy log pos sta phm pos tel sta click modification reverse berth habitation mooring ascent climb grease ptomaine appendix brickbat index solid vacancy void em fathom fthm inch mil bourn bourne demarcation fairyland rubicon trend vertex barony province accretion usance wastage baronetcy connectedness context efficiency inclusion liquid relationship Figure 4: A selection of polysemous classes Not all of the 442 classes are systematically polysemous." ></td>
	<td class="line x" title="48:166	Consider for example the following classes: Some of these classes are collections of homonyms that are ambigtzotz,s in similar ways, but do not lead to any kind of predictable polysemous behavior, for instance the class 'act anm art' with the lexical items: drill ruff solitaire stud." ></td>
	<td class="line x" title="49:166	Other classes consist of both homonyms and systematically polysemous lexical items like the class act log, which includes caliphate, clearing, emirate, prefecture, repair, wheeling vs. bolivia, charleston, chicago, michigan." ></td>
	<td class="line x" title="50:166	m m act ~nm art act log act plt axt rod loc chmpsy rod hum plt drill ruff solitaire stud bolivia caliphate charleston chicago clearing emirate michigan prefecture repair santiago wheeling chess grapevine rape pike port complex incense mandarin sage swede Figure 5: A selection of ambiguous classes Whereas the first group of nouns express two separated but related meanings (the act of clearing, repair, etc. takes place at a certain location), the second group expresses two meanings that are not related (the charleston dance which was named after the town by the same name)." ></td>
	<td class="line x" title="51:166	The ambiguous classes need to be removed altogether, while the ones with mixed ambiguous and polllsemous lexical items are to be weeded out carefully." ></td>
	<td class="line x" title="52:166	2.2 Underspecified semantic types The next step in the research is to organize the remaining classes into knowledge representations that relate their senses to each other." ></td>
	<td class="line x" title="53:166	These representations are based on Generative Lexicon theory (G), using qualia roles and (dotted) types (Pustejovsky, 19os)." ></td>
	<td class="line x" title="54:166	Qualia roles distinguish different semantic aspects: FORMAL indicates semantic type; CONSTITUTIVE part-whole information; AGENTIVE and TELIC associated events (the first dealing with the origin of the object, the second with its purpose)." ></td>
	<td class="line x" title="55:166	Each role is typed to a specific class of lexical items." ></td>
	<td class="line x" title="56:166	Types are either simple (human, artifact,) or complex (e.g. , information.physical)." ></td>
	<td class="line x" title="57:166	Complex types are called dotted types after the 'dots' that are used as type constructors." ></td>
	<td class="line x" title="58:166	Here I introduce two kinds of dots: Closed clots ''." ></td>
	<td class="line x" title="59:166	connect systematically related types that are always interpreted simultaneonsly." ></td>
	<td class="line x" title="60:166	Open dots 'o' connect systematically related types that are not (normally) interpreted simultaneously." ></td>
	<td class="line x" title="61:166	Both '#*~' and 'aor' denote sets of pairs of objects (a, b), a an object of type ~ and b an object of type ~'." ></td>
	<td class="line x" title="62:166	A condition aRb restricts this set of pairs to only those for which some relation R holds, where R denotes a subset of the Cartesian product of the sets of type ~ objects and type r objects." ></td>
	<td class="line x" title="63:166	The difference between types '#or' and 'cot' is in the nature of the objects they denote." ></td>
	<td class="line x" title="64:166	The type 'aer' denotes sets of pairs of objects where each pair behaves as a complex object in discourse structure." ></td>
	<td class="line x" title="65:166	For instance, the pairs of objects that are introduced by the type informationephysical (book, journal, scoreboard ) are addressed as the complex objects (x:information, y:physical) in discourse." ></td>
	<td class="line x" title="66:166	On the other hand, the type '#or' denotes simply a set of pairs of objects that do not occur together in discourse structure." ></td>
	<td class="line x" title="67:166	For instance, the pairs of objects that are introduced by the type form.artifact (door, gate, window  ) are not (normally) addressed simultaneously in discourse, rather one side of the object is picked out in a particular context." ></td>
	<td class="line x" title="68:166	Nevertheless, the pair as a whole remains active during processing." ></td>
	<td class="line x" title="69:166	The resulting representations can be seen as underspecified lexical meanings and are therefore referred to as underspecified semantic types." ></td>
	<td class="line x" title="70:166	CORELEX currently covers 104 underspecified semantic types." ></td>
	<td class="line x" title="71:166	This section presents a number of examples, for a complete overview see the CORELEX webpage: http://~, ca." ></td>
	<td class="line x" title="72:166	brandeis, edu/'paulb/Cor eLex/corelex, html Closed Dots Consider the underspecified representation for the semantic type actorelation: FORMAL = Q:act.relation CONSTITUTIVE = X:act V Y:relation V Z:act,relation TELIC --P:event (acterelation) A act (R1) A relation(R2,Rs) Figure 6: Representation for type: actorelation The representation introduces a number of objects that are of a certain type." ></td>
	<td class="line x" title="73:166	The FORMAL role introduces an object Q of type actorelation." ></td>
	<td class="line x" title="74:166	The CONSTITUTIVE introduces objects that are in a partwhole relationship with Q. These are either of the same type actorelation or of the simple types act or relation." ></td>
	<td class="line x" title="75:166	The TELIC expresses the event P that can be associated with an object of type acterelation." ></td>
	<td class="line x" title="76:166	For instance, the event of increase as in 'increasing the communication between member states' implies 'increasing' both the act of communicating an object 28 m m m m m m m m m \[\] m mm m m m m m m m m m m mm mm m m m m m RI and the communication relation between two objects R2 and Rs." ></td>
	<td class="line x" title="77:166	All these objects are introduced on the semantic level and correspond to a number of objects that will be realized in syntax." ></td>
	<td class="line x" title="78:166	However, not all semantic objects will be realized in syntax." ></td>
	<td class="line x" title="79:166	(See Section 3.4 for more on the syntax-semantics interface)." ></td>
	<td class="line x" title="80:166	The instances for the type act*relation are given in Figure 7, covering three different systematic polysemous classes." ></td>
	<td class="line x" title="81:166	We could have chosen to include only the instances of the 'act rel' class, but the nouns in the other two classes seem similar enough to describe all of them with the same type." ></td>
	<td class="line x" title="82:166	generative the lexicon should be and if one allows overgeneration of semantic objects." ></td>
	<td class="line x" title="83:166	.nm rod bluepoint capon clam cockle crawdad crawfish crayfish duckling fowl grub hen lamb langouste limpet lobster monkfish mussel octopus panfish partridge pheasant pigeon poultry prawn pullet quail saki scallop scollop shellfish shrimp snail squid whelk whitebait whitefish winkle act evt rel act rel act rel s~a blend competition flux transformation acceleration communication dealings designation discourse gait glide likening negation neologism neology prevention qualifying sharing synchronisation synchronization synchronizing coordination gradation involvement Figure 7: Instances for the type: act*relation Open Dots The type act.relation describes interpretations that can not be separated from each other (the act and relation aspects are intimately connected)." ></td>
	<td class="line x" title="84:166	The following representation for type -nimalofood describes interpretations that can not occur simultaneously but are however related ~." ></td>
	<td class="line x" title="85:166	It therefore uses a 'o' instead of a ''." ></td>
	<td class="line x" title="86:166	as a type constructor: FORMAL = Q:animalofood CONSTITUTIVE = X:an~mal V Y:food TELIC = Pz :act (Rz,'n|mal) V P2 :act (animal,R2) v P3:act(R3,food) Figure 8: Representation for type: animalofood The instances for this type only cover the class ' ~,m rod'." ></td>
	<td class="line x" title="87:166	A case could be made for including also every instance of the class c~-m' because in principal every animal could be eaten." ></td>
	<td class="line x" title="88:166	This is a question of how 1See the literature on animal grinding, for instance (Copestake and Briscoe, 1992) 29 Figure 9: Instances for the type: animalofood 2.3 Homonyms CORELEX is designed around the idea of systematic polysemons classes that exclude homonyms." ></td>
	<td class="line x" title="89:166	Traditionally a lot of research in lexical semantics has been occupied with the problem of ambiguity in homonyms." ></td>
	<td class="line x" title="90:166	Our research shows however that homonyms only make up a fraction of the whole of the lexicon of a language." ></td>
	<td class="line x" title="91:166	Out of the 37,793 noun stems that were derived from WORDNET 1637 are to be viewed as true homonyms because they have two or more unrelated senses, less than 5%." ></td>
	<td class="line x" title="92:166	The remaining 95% are nouns that do have (an indefinite number of) different interpretations, hut all of these are somehow related and should be inferred from a common knowledge representation." ></td>
	<td class="line x" title="93:166	These numbers suggest a stronger emphasis in research on systematic polysemy and less on homonyms, an approach that is advocated here (see also (Killgariff, 1992))." ></td>
	<td class="line x" title="94:166	In CORZLEX homonyms are simply assigned two or more underspecified semantic types, that need to be disambiguated in a traditional way." ></td>
	<td class="line x" title="95:166	There is however an added value also here because each disambiguated type can generate any number of context dependent interpretations." ></td>
	<td class="line x" title="96:166	3 Adapting CORELEx to Domain Specific Corpora The underspectfied semantic type that CORELEX assigns to a noun provides a basic lexical semantic structure that can be seen as the class-wide backbone semantic description on top of which specific information for each lexical item is to be defined." ></td>
	<td class="line x" title="97:166	That is, doors and gates are both artifacts but they have different appearances." ></td>
	<td class="line x" title="98:166	Gates are typically open constructions, whereas doors tend to be solid." ></td>
	<td class="line x" title="99:166	This kind of information however is corpus specific and therefore needs to be adapted specifically to and on the basis of that particular corpus of texts." ></td>
	<td class="line x" title="100:166	This process involves a number of consecutive steps that includes the probabilistic classification of unknown lexical items: 1." ></td>
	<td class="line x" title="101:166	Assignment of underspecified semantic tags to those nouns that are in CORELEX 2." ></td>
	<td class="line x" title="102:166	Running class-sensitive patterns over the (partly) tagged corpus 3." ></td>
	<td class="line x" title="103:166	(a) Constructing a probabilistic classifier from the data obtained in step 2." ></td>
	<td class="line x" title="104:166	(b) Probabilistically tag nouns that are not in CORELEX according to this classifier 4." ></td>
	<td class="line x" title="105:166	Relating the data obtained in step 2." ></td>
	<td class="line x" title="106:166	to one or more qualia roles Step 1." ></td>
	<td class="line x" title="107:166	is trivial, but steps 2." ></td>
	<td class="line x" title="108:166	through 4." ></td>
	<td class="line x" title="109:166	form a complex process of constructing a corpus specific semantic lexicon that is to be used in additional processing for knowledge intensive reasoning steps (i.e. abduction (Hobbs et al. , 1993)) that would solve metaphoric, metonymic and other non-literal use of language." ></td>
	<td class="line x" title="110:166	3.1 Assignment of CORELEX Tags The first step in analyzing a new corpus involves tagging each noun that is in CORELEX with an underspecified semantic tag." ></td>
	<td class="line x" title="111:166	This tag represents the following information: a definition of the type of the noun (FORMAL); a definition of types of possible nouns it can stand in a part-whole relationship with (CONSTITUTIVE); a definition of types of possible verbs it can occur with and their argument structures (AGENTIVE / TELIC)." ></td>
	<td class="line x" title="112:166	CORELEX is implemented as a database of associative arrays, which allows a fast lookup of this information in pattern matching." ></td>
	<td class="line x" title="113:166	3.2 Class-Sensitive Pattern Matching The pattern matcher runs over corpora that are: part-of-speech tagged using a widely used tagger (Brill, 1992); stemmed by using an experimental system that extends the Porter stemmer, a stemming algorithm widely used in information retrieval, with the Celex database on English morphology; (partly) semantically tagged using the CORELEX set of underspecified semantic tags as discussed in the previous section." ></td>
	<td class="line x" title="114:166	There are about 30 different patterns that are arranged around the headnoun of an NP." ></td>
	<td class="line x" title="115:166	They cover 30 the following syntactic constructions that roughly correspond to a VP, an S, an NP and an NP followed by a PP:  verb-headnoun  headnoun-verb  adjective-headnoun  modiflernoun-headnoun  headnoun-preposition-headnoun The patterns assume NP's of the following generic structure 2: PreDet* Det* Num* (Adj INamelNoun)* Noun The heuristics for finding the headnoun is then simply to take the rightmost noun in the NP, which for English is mostly correct." ></td>
	<td class="line x" title="116:166	The verb-headnoun patterns approach that of a true 'verb-obj' analysis by including a normalization of passive constructions as follows: \[Noun Have?" ></td>
	<td class="line x" title="117:166	Be Adv?" ></td>
	<td class="line x" title="118:166	Verb\] =~ \[Verb Noun\] Similarly, the headnoun-verb patterns approach a true 'sub j-verb' analysis." ></td>
	<td class="line x" title="119:166	However, because no deep syntactic analysis is performed, the patterns can only approximate subjects and Objects in this way and I therefore do not refer to these patterns as 'subject-verb' and 'verb-object' respectively." ></td>
	<td class="line x" title="120:166	The pattern matching is class-sensitive in employing the assigned CORELEX tag to determine if the application of this pattern is appropriate." ></td>
	<td class="line x" title="121:166	For instance, one of the headnoun-preposition-headnoun patterns is the following, that is used to detect partwhole (CONSTITUTIVE) relations: PreDet* Det* Num* (Adj \[ Name \[ Noun)* Noun of PreDet* Det* Num* (Adj \[NameJNoun)* Noun Clearly not every syntactic construction that fits this pattern is to be interpreted as the expression of a part-whole relation." ></td>
	<td class="line x" title="122:166	One of the heuristics we therefore use is that the pattern may only apply if both head nouns carry the same CORELEx tag or if the tag of the second head noun subsumes the tag of the first one through a dotted type." ></td>
	<td class="line x" title="123:166	That is, if the second head noun is of a dotted type and the first is of one of its composing types." ></td>
	<td class="line x" title="124:166	For instance, 'paragraph' ~The interpretation of '*' and ''?" ></td>
	<td class="line x" title="125:166	in this section follows that of common usage in regular expressions: 'w indicates 0 or more occurrences; ''?" ></td>
	<td class="line x" title="126:166	indicates 0 or 1 occurrence and 'journal' can be in a part-whole relation to each other because the first is of type information, while the second is of type information*physical." ></td>
	<td class="line x" title="127:166	Similar heuristics can be identified for the application of other patterns." ></td>
	<td class="line x" title="128:166	Recall of the patterns (percentage of nouns that are covered) is on average, among different corpora (wsJ, BROWN, PDGF a corpus we constructed for independent purposes from 1000 medical abstracts in the MEDLINE database on Platelet Derived Growth Factor and DARWIN the complete Origin of Species), about 70% to 80%." ></td>
	<td class="line x" title="129:166	Precision is much harder to measure, but depends both on the accuracy of the output of the part-of-speech tagger and on the accuracy of class-sensitive heuristics." ></td>
	<td class="line x" title="130:166	3.3 Probabilistic Classification The knowledge about the linguistic context of nouns in the corpus that is collected by the pattern matcher is now used to classify unknown nouns." ></td>
	<td class="line x" title="131:166	This involves a similarity measure between the linguistic contexts of classes of nouns that are in CORELEX and the linguistic context of unknown nouns." ></td>
	<td class="line x" title="132:166	For this purpose the pattern matcher keeps two separate arrays, one that collects knowledge only on COrtELEx nouns and the other collecting knowledge on all nouns." ></td>
	<td class="line x" title="133:166	The classifier uses mutual information (MI) scores rather than the raw frequences of the occurring patterns (Church and Hanks, 1990)." ></td>
	<td class="line x" title="134:166	Computing MI scores is by now a standard procedure for measuring the co-occurrence between objects relative to their overall occurrence." ></td>
	<td class="line oc" title="135:166	MI is defined in general as follows: y) I ix y) = log2 P(x) P(y) We can use this definition to derive an estimate of the connectedness between words, in terms of collocations (Smadja, 1993), but also in terms of phrases and grammatical relations (Hindle, 1990)." ></td>
	<td class="line x" title="136:166	For instance the co-occurrence of verbs and the heads of their NP objects iN: size of the corpus, i.e. the number of stems): N Cobj (v n) = log2 /(v) /(n) N N All nouns are now classified by running a similaxity measure over their MI scores and the MI scores of each CoRELEx class." ></td>
	<td class="line x" title="137:166	For this we use the Jaccard measure that compares objects relative to the attributes they share (Grefenstette, 1994)." ></td>
	<td class="line x" title="138:166	In our case the 'attributes' are the different linguistic constructions a noun occurs in: headnoun-verb, adjective-headnoun, modifiernoun-headnoun, etc. The Jaccard measure is defined as the number of attributes shared by two objects divided by the total number of unique attributes shared by both objects: A A+B+C A : attributes shared by both objects B : attributes unique to object 1 C : attributes unique to object 2 The Jaccard scores for each CORELEx class are sorted and the class with the highest score is assigned to the noun." ></td>
	<td class="line x" title="139:166	If the highest score is equal to 0, no class is assigned." ></td>
	<td class="line x" title="140:166	The classification process is evaluated in terms of precision and recall figures, but not directly on the classified unknown nouns, because their precision is hard to measure." ></td>
	<td class="line x" title="141:166	Rather we compute precision and recall on the classification of those nouns that are in CoreLex, because we can check their class automatically." ></td>
	<td class="line x" title="142:166	The assumption then is that the precision and recall figures for the classification of nouns that are known correspond to those that are unknown." ></td>
	<td class="line x" title="143:166	An additional measure of the effectiveness of the classifter is measuring the recall on classification of all nouns, known and unknown." ></td>
	<td class="line x" title="144:166	This number seems to correlate with the size of the corpus, in larger corpora more nouns are being classified, but not necessarily more correctly." ></td>
	<td class="line x" title="145:166	Correct classification rather seems to depend on the homogeneity of the corpus: if it is written in one style, with one theme and so on." ></td>
	<td class="line x" title="146:166	Recall of the classifier (percentage of all nouns that are classified > 0) is on average, among different larger corpora (> 100,000 tokens), about 80% to 90%." ></td>
	<td class="line x" title="147:166	Recall on the nouns in CoRELEx is between 35% and 55%, while precision is between 20% and 40%." ></td>
	<td class="line x" title="148:166	The last number is much better on smaller corpora (70% on average)." ></td>
	<td class="line x" title="149:166	More detailed information about the performance of the classifier, matcher and acquisition tool (see below) can be obtained from (Buitelaar, forthcoming)." ></td>
	<td class="line x" title="150:166	3.4 Lexical Knowledge Acquisition The final step in the process of adapting CORELEx to a specific domain involves the 'translation' of observed syntactic patterns into corresponding semantic ones and generating a semantic lexicon representing that information." ></td>
	<td class="line x" title="151:166	31 There are basically three kinds of semantic patterns that are utilized in a CORELEX lexicon: hyponymy (sub-supertype information) in the FORMAL role, meronymy (part-whole information) in the CONSTITUTIVE role and predicate-argument structure in the TELIC and AGENTIVE roles." ></td>
	<td class="line x" title="152:166	There are no compelling reasons to exclude other kinds of information, but for now we base our basic design on ~, which only includes these three in its definition of qualia structure." ></td>
	<td class="line x" title="153:166	Hyponymic information is acquired through the classification process discussed in Sections 2.2 and 3.3." ></td>
	<td class="line x" title="154:166	Meronymic information is obtained through a translation of various VP and PP patterns into 'has-part' and 'part-of' relations." ></td>
	<td class="line x" title="155:166	Predicate-argument structure finally, is derived from verb-headnoun and headnoun-verb constructions." ></td>
	<td class="line x" title="156:166	The semantic lexicon that is generated in such a way comes in two formats: T2), a Type Description Language based on typed feature-logic (Krieger and Schaefer, 1994a) (Krieger and Schaefer, 1994b) and HTML, the markup language for the World Wide Web." ></td>
	<td class="line x" title="157:166	The first provides a constraintbased formalism that allows CORELEX lexicons to be used stralghtforwardiy in constraint-based grammars." ></td>
	<td class="line x" title="158:166	The second format is used to present a generated semantic lexicon as a semantic index on a World Wide Web document." ></td>
	<td class="line x" title="159:166	We will not elaborate on this further because the subject of semantic indexing is out of the scope of this paper, but we refer to (Pustejovsky et al. , 1997)." ></td>
	<td class="line x" title="160:166	3.5 An Example: The PDGF Lexicon The semantic lexicon we generated for the PDGF corpus covers 1830 noun stems, spread over 81 CORELEX types." ></td>
	<td class="line x" title="161:166	For instance, the noun evidence is of type communication.psychological and the following representation is generated: 4 Conclusion In this paper I discuss the construction of a new type of semantic lexicon that supports underspecifled semantic tagging." ></td>
	<td class="line x" title="162:166	Traditional semantic tagging assumes a number of distinct senses for each lexical item between which the system should choose." ></td>
	<td class="line x" title="163:166	Underspecified semantic tagging however assumes no finite lists of senses, but instead tags each lexical item with a comprehensive knowledge representation from which a specific interpretation can be constructed." ></td>
	<td class="line x" title="164:166	CORZLEx provides such knowledge representations, and as such it is fundamentally different from existing semantic lexicons like WORDNET." ></td>
	<td class="line x" title="165:166	32 'evidence FORMAL '= \[ARG1 = commlmlcation\]\] CLOSED LARG2 psychological J J CONSTITUTIVE -~ I HAS-PAI~ ----TELIC = ip ov,.e \] FIRST L ARG-STRUCT ---~  REST ---.o. FIRST = structure\] 1 REST  Figure 10: Lexical entry for: evidence Additionally, it was shown that CoI~LEx provides for more consistent assignments of lexical semantic structure among classes of lexical items." ></td>
	<td class="line x" title="166:166	Finally, the approach described above allows one to generate domain specific semantic lexicons by enhancing CORELEX lexical entries with corpus based information." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W97-1004
Learning New Compositions From Given Ones
Ji, Donghong;Jun, He;Huang, Changning;"></td>
	<td class="line x" title="1:161	Learning New Compositions from Given Ones Ji Donghong Dept. of Computer Science Tsinghua University Beijing, 100084 P. R. China j dhsl000e, cs." ></td>
	<td class="line x" title="2:161	t snghua, edu." ></td>
	<td class="line x" title="3:161	cn He Jun Dept. of Computer Science Herbin Institute of Technology hj pact 518." ></td>
	<td class="line x" title="4:161	hit." ></td>
	<td class="line x" title="5:161	edu." ></td>
	<td class="line x" title="6:161	cn Huang Changning Dept. of Computer Science Tsinghua University Beijing, 100084 P. R. China hcn@mail.tsinghua.edu.cn Abstract In this paper, we study the problem of 'learning new compositions of words from given ones with a specific syntactic structure, e.g., A-N or V-N structures." ></td>
	<td class="line x" title="7:161	We first cluster words according to the given compositions, then construct a cluster-based compositional frame for each word cluster, which contains both new and given compositions relevant with the words in the cluster." ></td>
	<td class="line x" title="8:161	In contrast to other methods, we don't pre-define the number of clusters, and formalize the problem of clustering words as a non-linear optimization one, in which we specify the environments of words based on word clusters to be determined, rather than their neighboring words." ></td>
	<td class="line x" title="9:161	To solve the problem, we make use of a kind of cooperative evolution strategy to design an evolutionary algorithm." ></td>
	<td class="line x" title="10:161	1 Introduction Word compositions have long been a concern in lexicography(Benson et al. 1986; Miller et al. 1995), and now as a specific kind of lexical knowledge, it has been shown that they have an important role in many areas in natural language processing, e.g., parsing, generation, lexicon building, word sense disambiguation, and information retrieving, etc.(e.g. , Abney 1989, 1990; Benson et al. 1986; Yarowsky 1995; Church and Hanks 1989; Church, Gale, Hans, and Hindle 1989)." ></td>
	<td class="line x" title="11:161	But due to the huge number of words, it is impossible to list all compositions between words by hand in dictionaries." ></td>
	<td class="line x" title="12:161	So an urgent problem occurs: how to automatically acquire word compositions?" ></td>
	<td class="line x" title="13:161	In general, word compositions fall into two categories: free compositions and bound compositions, i.e., collocations." ></td>
	<td class="line x" title="14:161	Free compositions refer to those in which words can be replaced by other similar ones, while in bound compositions, words cannot be replaced freely(Benson 1990)." ></td>
	<td class="line x" title="15:161	Free compositions are predictable, i.e., their reasonableness can be determined according to the syntactic and semantic properties of the words in them." ></td>
	<td class="line oc" title="16:161	While bound compositions are not predictable, i.e., their reasonableness cannot be derived from the syntactic and semantic properties of the words in them(Smadja 1993)." ></td>
	<td class="line oc" title="17:161	Now with the availability of large-scale corpus, automatic acquisition of word compositions, especially word collocations from them have been extensively studied(e.g. , Choueka et al. 1988; Church and Hanks 1989; Smadja 1993)." ></td>
	<td class="line o" title="18:161	The key of their methods is to make use of some statistical means, e.g., frequencies or mutual information, to quantify the compositional strength between words." ></td>
	<td class="line n" title="19:161	These methods are more appropriate for retrieving bound compositions, while less appropriate for retrieving free ones." ></td>
	<td class="line x" title="20:161	This is because in free compositions, words are related with each other in a more loose way, which may result in the invalidity of mutual information and other statistical means in distinguishing reasonable compositions from unreasonable ones." ></td>
	<td class="line x" title="21:161	In this paper, we start from a different point to explore the problem of automatic acquisition of free compositions." ></td>
	<td class="line x" title="22:161	Although we cannot list all free compositions, we can select some typical ones as those specified in some dictionaries(e.g. , Benson 1986; Zhang et al. 1994)." ></td>
	<td class="line x" title="23:161	According to the properties held by free compositions, we can reasonably suppose that selected compositions can provide strong clues for others." ></td>
	<td class="line x" title="24:161	Furthermore we suppose that words can be classified into clusters, with the members in each cluster similar in their compositional ability, which can be characterized as the set of the words able to combined with them to form meaningful phrases." ></td>
	<td class="line x" title="25:161	Thus any given composition, although specifying the relation between two words literally, suggests the relation between two clusters." ></td>
	<td class="line x" title="26:161	So for each word(or clusJi, He and Huang 25 Learning New Compositions Ji Donghong, He Jun and Huang Changning (1997) Learning New Compositions from Given Ones." ></td>
	<td class="line x" title="27:161	In T.M. Ellison (ed)." ></td>
	<td class="line x" title="28:161	CoNLL97: Computational Natural Language Learning, ACL pp 25-32." ></td>
	<td class="line x" title="29:161	(~) 1997 Association for Computational Linguistics ter), there exist some word clusters, the word (or the words in the cluster) can and only can combine with the words in the clusters to form meaningful phrases." ></td>
	<td class="line x" title="30:161	We call the set of these clusters compositional frame of the word (or the cluster)." ></td>
	<td class="line x" title="31:161	A seemingly plausible method to determine compositional frames is to make use of pre-defined semantic classes in some thesauri(e.g. , Miller et al. 1993; Mei et al. 1996)." ></td>
	<td class="line x" title="32:161	The rationale behind the method is to take such an assumption that if one word can be combined with another one to form a meaningful phrase, the words similar to them in meaning can also be combined with each other." ></td>
	<td class="line x" title="33:161	But it has been shown that the similarity between words in meaning doesn't correspond to the similarity in compositional ability(Zhu 1982)." ></td>
	<td class="line x" title="34:161	So adopting semantic classes to construct compositional frames will result in considerable redundancy." ></td>
	<td class="line x" title="35:161	An alternative to semantic class is word cluster based on distributional environment (Brown et al. , 1992), which in general refers to the surrounding words distributed around certain word (e.g. , Hatzivassiloglou et al. , 1993; Pereira et al. , 1993), or the classes of them(Bensch et al. , 1995), or more complex statistical means (Dagan et al. , 1993)." ></td>
	<td class="line x" title="36:161	According to the properties of the clusters in compositional frames, the clusters should be based on the environment, which, however, is narrowed in the given compositions." ></td>
	<td class="line x" title="37:161	Because the given compositions are listed by hand, it is impossible to make use of statistical means to form the environment, the remaining choices are surrounding words or classes of them." ></td>
	<td class="line x" title="38:161	Pereira et a1.(1993) put forward a method to cluster nouns in V-N compositions, taking the verbs which can combine with a noun as its environment." ></td>
	<td class="line x" title="39:161	Although its goal is to deal with the problem of data sparseness, it suffers from the problem itself." ></td>
	<td class="line x" title="40:161	A strategy to alleviate the effects of the problem is to cluster nouns and verbs simultaneously." ></td>
	<td class="line x" title="41:161	But as a result, the problem of word clustering becomes a bootstrapping one, or a non-linear one: the environment is also to be determined." ></td>
	<td class="line x" title="42:161	Bensch et al.(1995) proposed a definite method to deal with the generalized version of the non-linear problem, but it suffers from the problem of local optimization." ></td>
	<td class="line x" title="44:161	In this paper, we focus on A-N compositions in Chinese, and explore the problem of learning new compositions from given ones." ></td>
	<td class="line x" title="45:161	In order to copy with the problem of sparseness, we take adjective clusters as nouns' environment, and take noun clusters as adjectives' environment." ></td>
	<td class="line x" title="46:161	In order to avoid local optimal solutions, we propose a cooperative evolutionary strategy." ></td>
	<td class="line x" title="47:161	The method uses no specific knowledge of A-N structure, and can be applied to other structures." ></td>
	<td class="line x" title="48:161	The remainder of the paper is organized as follows: in section 2, we give a formal description of the problem." ></td>
	<td class="line x" title="49:161	In section 3, we discuss a kind of cooperative evolution strategy to deal with the problem." ></td>
	<td class="line x" title="50:161	In section 4, we explore the problem of parameter estimation." ></td>
	<td class="line x" title="51:161	In section 5, we present our experiments and the results as well as their evaluation." ></td>
	<td class="line x" title="52:161	In section 6, we give some conclusions and discuss future work." ></td>
	<td class="line x" title="53:161	2 Problem Setting Given an adjective set and a noun set, suppose for each noun, some adjectives are listed as its compositional instances." ></td>
	<td class="line x" title="54:161	Our goal is to learn new reasonable compositions from the instances." ></td>
	<td class="line x" title="55:161	To do so, we cluster nouns and adjectives simultaneously and build a compositional frame for each noun." ></td>
	<td class="line x" title="56:161	Suppose A is the set of adjectives, N is the set of nouns, for any a E A, let f(a) C N be the instance set of a, i.e., the set of nouns in N which can be combined with a, and for any n E N, let g(n) C A be the instance set of n, i.e., the set of adjectives in A which can be combined with n. We first give some formal definitions in the following: Definition 1 partition Suppose U is a non-empty finite set, we call < U1, U2, , Uk > a partition of U, if: i) for any Ui, and Uj, i  j, Ui M Uj = ii) U = Ul<t<kUl We call Ui a cluster of U. Suppose U --< A1,A2,,Ap > is a partition of A, V ~< N1,N2,,Nq > is a partition of N, f and g are defined as above, for any N/, let g(N ) = {& : n # ), and for any n, let,f<U,V>(n ) =l {a : 3At,Al E g(Nk),a E Ajl} -g(n) I, where n E Nk." ></td>
	<td class="line x" title="57:161	Intuitively, 5<Uv>(n) is the number of the new instances relevant with n. We define the general learning amount as the following: Definition 2 learning amount hEN Based on the partitions of both nouns and adjectives, we can define the distance between nouns and that between adjectives." ></td>
	<td class="line x" title="58:161	Definition 3 distance between words for anya EA, let fv(a) = {Ni : 1<i < q, Ni M f(a) ~ ~}, for any n E N, let g~= {Ai : 1 < i < p, Ai Mg(n)  ), for any two nouns nl and ha, any two adjectives al and a2, we define the distances between them respectively as the following: Ji, He and Huang 26 Learning New Compositions i) ii) dis~(nl, n2) = 1 gff(nl) V1 gu(n2) gu(nl) U gu(n2) disv(al, a2) = 1 f~-(al) N fv(a2) fV(al) t_J fv(a2) According to the distances between words, we can define the distances between word sets." ></td>
	<td class="line x" title="59:161	Definition 4 distance between word sets Given any two adjective sets X1, X2 C A, any two noun sets Y1, Y2 C N, their distances are: i) disv(Zl, X2) = max {disv(al, a2)} alEXi,a2EX2 ii) max dis~r hi, dis (Yl,Y2) = { v( n2)} nl EYi,n2EY2 Intuitively, the distance between word sets refer to the biggest distance between words respectively in the two sets." ></td>
	<td class="line x" title="60:161	We formalize the problem of clustering nouns and adjectives simultaneously as an optimization problem with some constraints." ></td>
	<td class="line x" title="61:161	(1)To determine a partition U =< A1,A2,,Ap > of A, and a partition V =< N1,N2,,Nq > of N, where p,q > O, which satisfies i) and ii), and minimize ~<e,v>' i) for any al, a2 E Ai, 1 < i < p, disg(al, as) < tl; for Ai and Aj, 1 < i # j < p, disv(Ai,Aj) > tl; ii) for any nl,n2 E Ni,1 < i < q, disg(nl,n2) < t2; for Ni and Ny, 1 _< i  j _< p, disg(Ni, Nj) k t2; Intuitively, the conditions i) and ii) make the distances between words within clusters smaller, and those between different clusters bigger, and to minimize 6 ~ ~_ means to minimize the distances between the words within clusters." ></td>
	<td class="line x" title="62:161	In fact, (U, V) can be seen as an abstraction model over given compositions, and tl, t2 can be seen as its abstraction degree." ></td>
	<td class="line x" title="63:161	Consider the two special case: one is tl = t2 = 0, i.e., the abstract degree is the lowest, when the result is that one noun forms a cluster and on adjective forms a cluster, which means that no new compositions are learned." ></td>
	<td class="line x" title="64:161	The other is tl = t2 = 1, the abstract degree is the highest, when a possible result is that all nouns form a cluster and all adjectives form a cluster, which means that all possible compositions, reasonable or unreasonable, are learned." ></td>
	<td class="line x" title="65:161	So we need estimate appropriate values for the two parameters, in order to make an appropriate abstraction over given compositions, i.e., make the compositional frames contain as many reasonable compositions as possible, and as few unreasonable ones as possible." ></td>
	<td class="line x" title="66:161	3 Cooperative Evolution Since the beginning of evolutionary algorithms, they have been applied in many areas in AI(Davis et al. , 1991; Holland 1994)." ></td>
	<td class="line x" title="67:161	Recently, as a new and powerful learning strategy, cooperative evolution has gained much attention in solving complex non-linear problem." ></td>
	<td class="line x" title="68:161	In this section, we discuss how to deal with the problem (1) based on the strategy." ></td>
	<td class="line x" title="69:161	According to the interaction between adjective clusters and noun clusters, we adopt such a cooperative strategy: after establishing the preliminary solutions, for any preliminary solution, we optimize N's partition based on A's partition, then we optimize A's partition based on N's partition, and so on, until the given conditions are satisfied." ></td>
	<td class="line x" title="70:161	3.1 Preliminary Solutions When determining the preliminary population, we also cluster nouns and adjectives respectively." ></td>
	<td class="line x" title="71:161	However, we see the environment of a noun as the set of all adjectives which occur with it in given compositions, and that of an adjective as the set of all the nouns which occur with it in given compositions." ></td>
	<td class="line x" title="72:161	Compared with (1), the problem is a linear clustering one." ></td>
	<td class="line x" title="73:161	Suppose al,a2 E A, f is defined as above, we define the linear distance between them as (2): (2) dis(a1 a2) -1 I/(ax)nl(a2)l ' \[f(ax)Of(a2)l Similarly, we can define the linear distance between nouns dis(nl,n2) based on g. In contrast, we call the distances in definition 3 non-linear distances." ></td>
	<td class="line x" title="74:161	According to the linear distances between adjectives, we can determine a preliminary partition of N: randomly select an adjective and put it into an empty set X, then scan the other adjectives in A, for any adjective in A X, if its distances from the adjectives in X are all smaller than tl, then put it into X, finally X forms a preliminary cluster." ></td>
	<td class="line x" title="75:161	Similarly, we can build another preliminary cluster in (AX)." ></td>
	<td class="line x" title="76:161	So on, we can get a set of preliminary clusters, which is just a partition of A. According to the different order in which we scan the adjectives, we can get different preliminary partitions of A. Similarly, we can determine the preliminary partitions of N based on the linear distances between nouns." ></td>
	<td class="line x" title="77:161	A partition of A and a partition of N forms a preliminary solution of (1), and all possible preliminary solutions forms the Ji, He and Huang 27 Learning New Compositions population of preliminary solutions, which we also call the population of Oth generation solutions." ></td>
	<td class="line x" title="78:161	3.2 Evolution Operation In general, evolution operation consists of recombination, mutation and selection." ></td>
	<td class="line x" title="79:161	Recombination makes two solutions in a generation combine with each other to form a solution belonging to next generation." ></td>
	<td class="line x" title="80:161	Suppose < U~ i), Vi(')> and < U~ i), V2(') > are two ith generation solutions, where U~ t) and U~ i) are two partitions of A, V?" ></td>
	<td class="line x" title="81:161	i) and V2 (i) are two partitions of N, then < U~ '), V2 (i) > and < U2 (i), V1 (i) > forms two possible (i+l)th generation solutions." ></td>
	<td class="line x" title="82:161	Mutation makes a solution in a generation improve its fitness, and evolve into a new one belonging to next generation." ></td>
	<td class="line x" title="83:161	Suppose < U (i), U (i) > is a ith generation solution, where U (i) =< A1, A2,  , Ap >, V (i) =< N1,N2,,Nq > are partitions of A and N respectively, the mutation is aimed at optimizing V(0 into V (t+l) based on U (t), and makes V (t+l) satisfy the condition ii) in (1), or optimizing U (t) into U(t+l) based on V (0, and makes U (l+1) satisfy the condition i) in (1), then moving words across clusters to minimize d<u,v>' We design three steps for mutation operation: splitting, merging and moving, the former two are intended for the partitions to satisfy the conditions in (1), and the third intended to minimize (f<U,v > . In the following, we take the evolution of V (t+l) as an example to demonstrate the three steps." ></td>
	<td class="line x" title="84:161	Splitting Procedure." ></td>
	<td class="line x" title="85:161	For any Nk, 1 _< k _<, if there exist hi,n2  Nk, such that disv(,+~)(nl,n2 ) _> t2, then splitting Nk into two subsets X and Y. The procedure is given as the following: i) Put nl into X, n2 into Y, ii) Select the noun in (Nk -(X U Y)) whose distance from nl is the smallest, and put it into X, iii) Select the noun in (Nk -(X t_J Y)) whose distance from n2 is the smallest, and put it into Y, iv) Repeat ii) and iii), until X t3 Y = Nk." ></td>
	<td class="line x" title="86:161	For X (or Y), if there exist nl,n2  X (or Y), disv(o >_ t2, then we can make use of the above procedure to split it into more smaller sets." ></td>
	<td class="line x" title="87:161	Obviously, we can split any Nk in V(0 into several subsets which satisfy the condition ii) in (1) by repeating the procedure." ></td>
	<td class="line x" title="88:161	Merging procedure." ></td>
	<td class="line x" title="89:161	If there exist Nj and Nk, where 1 _< j,k _< q, such that disu(~)(Nt,Nk ) < t2, then merging them into a new cluster." ></td>
	<td class="line x" title="90:161	It is easy to prove that U (t) and V(0 will meet the condition i) and ii) in (1) respectively, after splitting and merging procedure." ></td>
	<td class="line x" title="91:161	Moving procedure." ></td>
	<td class="line x" title="92:161	We call moving n from Nj to Nk a word move, where 1 < j  k < q, denoted as (n, Nj, Nk), if the condition (ii) remains satisfied." ></td>
	<td class="line x" title="93:161	The procedure is as the following: i) Select a word move (n, Nj, Na) which minimizes ~<U,V> ' ii) Move n from Nj to Nk, iii) Repeat i) and ii) until there are no word moves which reduce 6<u,v>' After the three steps, U (i) and V (i) evolve into U (i+U and V (i+D respectively." ></td>
	<td class="line x" title="94:161	Selection operation selects the solutions among those in the population of certain generation according to their fitness." ></td>
	<td class="line x" title="95:161	We define the fitness of a solution as its learning amount." ></td>
	<td class="line x" title="96:161	We use Ji to denote the set of i$h generation solutions, H(i, i + 1), as in (3), specifies the similarity between ith generation solutions and (i + 1)th generation solutions." ></td>
	<td class="line x" title="97:161	(3) H(i, i + 1) = min5(u(,+l),v(i+l)) : (U (~+1), V (i+1))  J~+l} min5(u(,),v(,) ) : (U (~), V (i)) E J~) Let t3 be a threshold for H(i, i + 1), the following is the general evolutionary algorithm: Procedure Clustering(A, N, f, g); begin i) Build preliminary solution population I0, ii) Determine 0th generation solution set J0 according to their fitness, iii) Determine/i+1 based on Ji: a) Recombination: if (U~ i), Vff)), (U2 ('), V2 (')) E J,, then (U~ '), V2(')), (U (i), V2 (')) E I~+1, b) Mutation: if (U( ~),V (~)) E J~, then (U (i), V(~+I)), (U (~+D, V (~)) E I~+1, iv) Determine J~+l from Ii+1 according to their fitness, v) If H(i, i + 1) > t3, then exit, otherwise goto iii), end After determining the clusters of adjectives and nouns, we can construct the compositional frame for each noun cluster or each noun." ></td>
	<td class="line x" title="98:161	In fact, for each noun cluster Ni,g(N~) = {Aj : 3n E Ni,Aj Ng(n) 7 ) is just its compositional frame, and for any noun in N/, g(Ni) is also its compositional frame." ></td>
	<td class="line x" title="99:161	Similarly, for each adjective (or adjective cluster), we can also determine its compositional frame." ></td>
	<td class="line x" title="100:161	4 Parameter Estimation The parameters tl and t2 in (1) are the thresholds for the distances between the clusters of A and N reJi, He and Huang 28 Learning New Compositions spectively." ></td>
	<td class="line x" title="101:161	If they are too big, the established frame will contain more unreasonable compositions, on the other hand, if they are too small, many reasonable compositions may not be included in the frame." ></td>
	<td class="line x" title="102:161	Thus, we should determine appropriate values for t~ and t2, which makes the fame contain as many reasonable compositions as possible, meanwhile as few unreasonable ones as possible." ></td>
	<td class="line x" title="103:161	Suppose Fi is the compositional frame of Ni, let F =< F1,F~,,Fq >, for any F~, let AF~ = {a : 3X E F~, a E X}." ></td>
	<td class="line x" title="104:161	Intuitively, AF~ is the set of the adjectives learned as the compositional instances of the noun in Ni." ></td>
	<td class="line x" title="105:161	For any n ~ N~, we use An to denote the set of all the adjectives which in fact can modify n to form a meaningful phrase, we now define deficiency rate and redundancy rate of F. For convenience, we use (iF to represent 5(U, V)." ></td>
	<td class="line x" title="106:161	Definition 5 Deficiency rate o~F El<i<q EneN, \[ A~ ARe \[ Intuitively, aF refers to the ratio between the reasonable compositions which are not learned and all the reasonable ones." ></td>
	<td class="line x" title="107:161	Definition 6 Redundancy rate fir fiR ---El_<i_<q EneNi \] AF~ -An I 5F Intuitively, fie refers to the ratio between unreasonable compositions which are learned and all the learned ones." ></td>
	<td class="line x" title="108:161	So the problem of estimating tl and t2 can be formalized as (5): (5) to find tl and t2, which makes av = 0, and flF=0." ></td>
	<td class="line x" title="109:161	But, (5) may exists no solutions, because its constraints are two strong, on one hand, the sparseness of instances may cause ~F not to get 0 value, even if tl and t~ close to 1, on the other hand, the difference between words may cause fir not to get 0 value, even if tl and t2 close to 0." ></td>
	<td class="line x" title="110:161	So we need to weaken (5)." ></td>
	<td class="line x" title="111:161	In fact, both O~F and flF can be seen as the functions of tl and t2, denoted as o~f(tl,t2) and l~F(tl, tu) respectively." ></td>
	<td class="line x" title="112:161	Given some values for tl and t2, we can compute aF and fiR." ></td>
	<td class="line x" title="113:161	Although there may exist no values (t~,t~) for (tl,t2), such that ! !" ></td>
	<td class="line x" title="114:161	aF(t~,t~) = flF(tx,t2) = 0, but with t~ and t2 increasing, off tends to decrease, while fiE tends to increase." ></td>
	<td class="line x" title="115:161	So we can weaken (5) as (6)." ></td>
	<td class="line x" title="116:161	(6) to find tl and t2, which maximizes (7)." ></td>
	<td class="line x" title="117:161	(7) ~(~l,~)~rl(~',,~'~) ~F(tl, t2) Fi (t' 1, t'2) \[ ~(ta,t:)eF2(t' 1,t~2) ~F(tl, 42)) I r2(t'l, I where rx(t~,t~) = {(tl,t2) : 0 < tl _< t~,0 _< t2 _< t~), r2(t~,t~) = {(tl,t2): t~ < tl < 1,t~ < t2 < 1} Intuitively, if we see the area (\[0, 1\]; \[0, 1\]) as a sample space for tl and t2, Fl(t~,t~) and F2(t~,t~) are its sub-areas." ></td>
	<td class="line x" title="118:161	So the former part of (7) is the ! !" ></td>
	<td class="line x" title="119:161	mean deficiency rate of the points in Fl(tl, tz), and the latter part of (7) is the mean deficiency rate of the points in F2(t~,t~)." ></td>
	<td class="line x" title="120:161	To maximize (7) means to maximize its former part, while to minimize its latter part." ></td>
	<td class="line x" title="121:161	So our weakening (5) into (6) lies in finding a point (t~,t~), such that the mean deficiency rate of the sample points in F2(t~,t~) tends to be very low, rather than finding a point (t~,t~), such that its deficiency rate is 0." ></td>
	<td class="line x" title="122:161	5 Experiment Results and Evaluation We randomly select 30 nouns and 43 adjectives, and retrieve 164 compositions(see Appendix I) between them from Xiandai Hanyu Cihai (Zhang et al. 1994), a word composition dictionary of Chinese." ></td>
	<td class="line x" title="123:161	After checking by hand, we get 342 reasonable compositions (see Appendix I), among which 177 ones are neglected in the dictionary." ></td>
	<td class="line x" title="124:161	So the sufficiency rate (denoted as 7) of these given compositions is 47.9%." ></td>
	<td class="line x" title="125:161	We select 0.95 as the value of t3, and let tl = 0.0, 0.1,0.2,  , 1.0, t2 = 0.0, 0.1, 0.2,  , 1.0 respectively, we get 121 groups of values for O~F and fiR." ></td>
	<td class="line x" title="126:161	Fig.1 and Fig.2 demonstrate the distribution of aF and ~3F respectively." ></td>
	<td class="line x" title="127:161	dcielcney i!iiiii!iiiiiii!i!iiiii!" ></td>
	<td class="line x" title="128:161	4o  ' i i iiiiiiiiiiiiiiiiiiiiiiiiiii ii::  3 t2 tl Figure 1: The distribution of O~F For any given tl, and t2,we found (7) get its biggest value when t I = 0.4 and t2 = 0.4, so we seJi, He and Huang 29 Learning New Compositions rcdundanec  atc(%)  ~:~'::':ili::iii~i~i~i~ ~160-80 .~<.v'.~!!!~i!!!i!!!ii!!!!:':':!:!:!!!!i!!!i!i!i !iiiii!iii!iii!i:i::':':':' D 4.0-50 L00:~:~iii::iiiiiii;;ii;iiiiiiii:: ~  i?" ></td>
	<td class="line x" title="129:161	:~i ~  \[\]Z0-~ 0:iiiiiiiiiiiiiiiii',i',iii',iiiiiii',0 iiiiiiiiiiiiiii ii t2 (L/tO) Figure 2: The distribution of fir ~(%) ~1 ~2 O/F (%) BF(%) 32.5 0.5 0.6 13.2 34.5 47.9 0.4 0.4 15.4 26.4 58.2 0.4 0.4 10.3 15.4 72.5 0.3 0.3 9.5 7.6 Table 1: The relation between 7,~1,t2, aF and fiR." ></td>
	<td class="line x" title="130:161	~(%) ~F(%) e1(%) BF(%) ~2(%) 58.2 11.2 8.3 17.5 10.8 72.5 7.4 4.1 8.7 5.4 Table 2: The relation between 7, mean O~F and mean BF, el and e~ is the mean error." ></td>
	<td class="line x" title="131:161	lect 0.4 as the appropriate value for both tl and t2." ></td>
	<td class="line x" title="132:161	The result is listed in Appendix II." ></td>
	<td class="line x" title="133:161	From Fig.1 and Fig.2, we can see that when tl = 0.4 and t2 = 0.4, both c~F and BF get smaller values." ></td>
	<td class="line x" title="134:161	With the two parameters increasing, aF decreases slowly, while BF increases severely, which demonstrates the fact that the learning of new compositions from the given ones has reached the limit at the point: the other reasonable compositions will be learned at a cost of severely raising the redundancy rate." ></td>
	<td class="line x" title="135:161	From Fig.l, we can see that o~F generally increases as ~1 and t2 increase, this is because that to increase the thresholds of the distances between clusters means to raise the abstract degree of the model, then more reasonable compositions will be learned." ></td>
	<td class="line x" title="136:161	On the other hand, we can see from Fig.2 that when tl _> 0.4, t2 >_ 0.4, fiR roughly increases as ~1 and ~2 increase, but when tz < 0.4, or t2 < 0.4, fir changes in a more confused manner." ></td>
	<td class="line x" title="137:161	This is because that when tl < 0.4, or ~2 < 0.4, it may be the case that much more reasonable compositions and much less unreasonable ones are learned, with tl and t2 increasing, which may result in fiR's reduction, otherwise fir will increase, but when tz >_ 0.4, t2 > 0.4, most reasonable compositions have been learned, so it tend to be the case that more unreasonable compositions will be learned as tl and t2 increase, thus fir increases in a rough way." ></td>
	<td class="line x" title="138:161	To explore the relation between % aF and fiE, we reduce or add the given compositions, then estimate Q and t2, and compute aRE and fiR." ></td>
	<td class="line x" title="139:161	Their correspondence is listed in Table 1." ></td>
	<td class="line x" title="140:161	From Table 1, we can see that as 7 increases, the estimated values for tl and t2 will decrease, and BE will also decrease." ></td>
	<td class="line x" title="141:161	This demonstrates that if given less compositions, we should select bigger values for the two parameters in order to learn as many reasonJi, He and Huang able compositions as possible, however, which will lead to non-expectable increase in fly." ></td>
	<td class="line x" title="142:161	If given more compositions, we only need to select smaller values for the two parameters to learn as many reasonable compositions as possible." ></td>
	<td class="line x" title="143:161	We select other 10 groups of adjectives and nouns, each group contains 20 adjectives and 20 nouns." ></td>
	<td class="line x" title="144:161	Among the 10 groups, 5 groups hold a sufficiency rate about 58.2%, the other 5 groups a sufficiency rate about 72.5%." ></td>
	<td class="line x" title="145:161	We let ~1 -~ 0.4 and t2 = 0.4 for the former 5 groups, and let tl = 0.3 and t2 = 0.3 for the latter 5 groups respectively to further consider the relation between 7, o~F and fiR, with the values for the two parameters fixed." ></td>
	<td class="line x" title="146:161	Table 2 demonstrates that for any given compositions with fixed sufficiency rate, there exist close values for the parameters, which make c~F and fir maintain lower values, and if given enough compositions, the mean errors of O~FF and fie will be lower." ></td>
	<td class="line x" title="147:161	So if given a large number of adjectives and nouns to be clustered, we can extract a small sample to estimate the appropriate values for the two parameters, and then apply them into the original tasks." ></td>
	<td class="line x" title="148:161	6 Conclusions and Future work In this paper, we study the problem of learning new word compositions from given ones by establishing compositional frames between words." ></td>
	<td class="line x" title="149:161	Although we focus on A-N structure of Chinese, the method uses no structure-specific or language-specific knowledge, and can be applied to other syntactic structures, and other languages." ></td>
	<td class="line x" title="150:161	There are three points key to our method." ></td>
	<td class="line x" title="151:161	First, we formalize the problem of clustering adjectives and nouns based on their given compositions as a nonlinear optimization one, in which we take noun clusters as the environment of adjectives, and adjective 30 Learning New Compositions P clusters as the environment of nouns." ></td>
	<td class="line x" title="152:161	Second, we design an evolutionary algorithm to determine its optimal solutions." ></td>
	<td class="line x" title="153:161	Finally, we don't pre-define the number of the clusters, instead it is automatically determined by the algorithm." ></td>
	<td class="line x" title="154:161	Although the effects of the sparseness problem can be alleviated compared with that in traditional methods, it is still the main problem to influence the learning results." ></td>
	<td class="line x" title="155:161	If given enough and typical compositions, the result is very promising." ></td>
	<td class="line x" title="156:161	So important future work is to get as many typical compositions as possible from dictionaries and corpus as the foundation of our algorithms." ></td>
	<td class="line x" title="157:161	At present, we focus on the problem of learning compositional frames from the given compositions with a single syntactic structure." ></td>
	<td class="line x" title="158:161	In future, we may take into consideration several structures to cluster words, and use the clusters to construct more complex frames." ></td>
	<td class="line x" title="159:161	For example, we may consider both A-N and V-N structures in the meantime, and build the frames for them simultaneously." ></td>
	<td class="line x" title="160:161	Now we make use of sample points to estimate appropriate values for the parameters, which seems that we cannot determine very accurate values due to the computational costs with sample points increasing." ></td>
	<td class="line x" title="161:161	Future work includes how to model the sample points and their values using a continuous function, and estimate the parameters based on the function." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C98-1089
Terminological variation, a means of identifying research topics from texts
Ibekwe-SanJuan, Fidelia;"></td>
	<td class="line x" title="1:149	Terminological variation, a means of identifying research topics from texts Fidelia IBEKWE-SANJUAN CRISTAL-GRESEC, Stendhal University, Grenoble France and Dept. of Information & Communication IUT du Havre B.P. 4006 76610 Le Havre France E-mail : fidelia@iut.univ-lehavre.fr Abstract After extracting terms from a corpus of titles and abstracts in English, syntactic variation relations are identified amongst them in order to detect research topics." ></td>
	<td class="line x" title="2:149	Three types of syntactic variations were studied : permutation, expansion and substitution." ></td>
	<td class="line x" title="3:149	These syntactic variations yield other relations of formal and conceptual nature." ></td>
	<td class="line x" title="4:149	Basing on a distinction of the variation relations according to the grammatical function affected in a term head or modifier term variants are first clustered into connected components which are in turn clustered into classes." ></td>
	<td class="line x" title="5:149	These classes relate two or more components through variations involving a change of head word, thus of topic." ></td>
	<td class="line x" title="6:149	The graph obtained reveals the global organisation of research topics in the corpus." ></td>
	<td class="line x" title="7:149	A clustering method has been built to compute such classes of research topics." ></td>
	<td class="line x" title="8:149	Introduction The importance of terms in various natural language tasks such as automatic indexing, computer-aided translation, information retrieval and technology watch need no longer be proved." ></td>
	<td class="line x" title="9:149	Terms are meaningful textual units used for naming concepts or objects in a given field." ></td>
	<td class="line x" title="10:149	Past studies have focused on building term extraction tools : TERMINO (David S. & Plante P. 1991), LEXTER (Bourigault D. 1994), ACABIT (Daille 1994), FASTR (Jacquemin 1995), TERMS (Katz S.M. & Justeson T.S. 1995)." ></td>
	<td class="line x" title="11:149	Here, term extraction and the identification of syntactic variation relations are considered for topic detection." ></td>
	<td class="line x" title="12:149	Variations are changes affecting the structure and the form of a term producing another textual unit close to the initial one e.g. dna amplification and amplification fin~erprintinjz of dna." ></td>
	<td class="line x" title="13:149	Variations can point to terminological evolution and thus to that of the underlying concept." ></td>
	<td class="line x" title="14:149	Topic is used in its grammatical sense, i.e. the head word in a noun phrase." ></td>
	<td class="line x" title="15:149	In the above term, fingerprinting is the topic (head word) and dna amplification its properties (modifiers)." ></td>
	<td class="line x" title="16:149	However, a topic cannot appear by chance in specialised litterature, so this grammatical definition needs to be backed up by empirical evidence such as recurrence of terms sharing the same head word." ></td>
	<td class="line x" title="17:149	We constituted a test corpus of scientific abstracts and titles in English from the field of plant biotechnology making up _--29000 words." ></td>
	<td class="line x" title="18:149	These texts covered publications made over 13 years (1981-1993)." ></td>
	<td class="line x" title="19:149	We focused on three syntactic variation types occurring frequently amongst terms : permutation, substitution and expansion (2)." ></td>
	<td class="line x" title="20:149	Tzoukermann E. Klavans J. and Jacquemin C." ></td>
	<td class="line x" title="21:149	(1997) extracted morpho-syntactic term variants for NLP tasks such as automatic indexing." ></td>
	<td class="line x" title="22:149	They accounted for a wide spectrum of variation producing phenomena like the morpho-syntactic variation involving derivation in tree cutting and trees have been cut down l. We focused for the moment on terms appearing as noun phrases (NP)." ></td>
	<td class="line x" title="23:149	Although term variants can appear as verb phrases (VP), we believe that NP variants reflect more terminological stability thus a real shift in topic (root hair ~ root hair deformation) than their VP counterpart (root hair -+ the root hair appears' deformed)." ></td>
	<td class="line x" title="24:149	Also, our application research topic identification being quite sensitive, requires a careful selection of term variants types depending on their interpretability." ></td>
	<td class="line x" title="25:149	1 Examples taken from Tzoukermann et al.(I 997)." ></td>
	<td class="line x" title="27:149	564 This is to avoid creating relations between terms which could mislead the end-user, typically a technological watcher, in his task." ></td>
	<td class="line x" title="28:149	For instance how do we interpret the relation between concept class and class concept ? Also, our aim is not to extract syntactic variants pet' se but to identify them in order to establish meaningful relations between them." ></td>
	<td class="line x" title="29:149	1 Extracting terms from texts 1.1 Morpho-syntactie features Term extraction is based on their morphosyntactic features." ></td>
	<td class="line x" title="30:149	The morphological composition of NP terms allows for a limited number of categories mostly nouns, adjectives and some prepositions." ></td>
	<td class="line x" title="31:149	Terms can appear under two syntactic structures : compound (the apec~c alfalfa nodulation) or syntagmatic (the .~'pec~i'c nodulation of alfalfa)." ></td>
	<td class="line x" title="32:149	Since terms are used for naming concepts and objects in a given knowledge field, they tend to be relatively short textual units usually between 2-4 words though terms of longer length occur (endogeneous duck hepatitis B virus)." ></td>
	<td class="line x" title="33:149	In this study, we fixed a word limit of 7 not considering determiners and prepositions." ></td>
	<td class="line x" title="34:149	Based on these three features, morphological make-up, syntactic structure and length, clauses are processed in order to extract complex terms rather than atomic ones." ></td>
	<td class="line x" title="35:149	The motivation behind this approach is that complex terms reveal the association of concepts, hence they are more relevant for the application we are considering." ></td>
	<td class="line x" title="36:149	A fine-grained term extraction strategy would isolate the concepts and thus lose the information given by their associations in the corpus." ></td>
	<td class="line x" title="37:149	For this reason, we could not consider the use of an existing term extraction tool and thus had to carry out a manual simulation of the term extraction phase." ></td>
	<td class="line x" title="38:149	NP splitting rules take into account the lexical nature of the constituent words and their raising properties (i.e. derived nouns as opposed to nonderived ones)." ></td>
	<td class="line x" title="39:149	Furthermore, following the empirical approach successfully implemented by Bourigault (1994), we split complex NPs only after a search has been performed in the corpus for occurrences of their sub-segments in unambiguous situations, i.e. when the sub-segments are not included in a larger segment." ></td>
	<td class="line x" title="40:149	This favours the extraction of pre-conceived textual units possibly corresponding to domain terms." ></td>
	<td class="line oc" title="41:149	However morphosyntactic features alone cannot verily the terminological status of the units extracted since they can also select non terms (see Smadja 1993)." ></td>
	<td class="line x" title="42:149	For instance root nodulation is a term in the plant biotechnology field whereas book review also found in the corpus is not, Thus in the first stage, the terms extracted are only plausible candidates which need to be filtered in order to eliminate the most unlikely ones." ></td>
	<td class="line x" title="43:149	This filtering takes advantage of lexical information accessible at our level of analysis to fine-tune the statistical occurrence criterion which used alone, inevitably leads to a massive elimination." ></td>
	<td class="line x" title="44:149	1.2 Splitting complex noun phrases An NP is deemed complex if its morpho-syntactic features do not conform to that specified for terms, e.g. oxygen control of nitrogen fixation gene expression in bradyrhizobium japonicum a title found in our corpus." ></td>
	<td class="line x" title="45:149	Its corresponding syntactic context is : NPl_of_NPz_prepLNP3 where NP is a recognised noun phrase, prep~ refers to the class of preposition not containing of and often found in the morphological composition of terms (for, by, in, from, with)." ></td>
	<td class="line x" title="46:149	Normally, exploiting syntactic information on the raising properties of the head noun (control) and following the distributional approach, the above segment will be split thus : ---> NPl --> NP2 --~ NP3 But this splitting is only performed if no subsegment of the initial one occurred alone in the corpus." ></td>
	<td class="line x" title="47:149	This search yielded nitrogen fixation gene expression and bradyrhizobium japonicum which both occurred more than 6 times in the corpus." ></td>
	<td class="line x" title="48:149	Their existence confirms the relevance of our splitting rule which would have yielded the same result: oxygen control; nitrogen fixation gene expression; bradyrhizobium japonicum Altogether, 4463 candidate terms were extracted from our corpus and subjected to a filtering process which combined lexical and statistical criteria." ></td>
	<td class="line x" title="49:149	The lexical criterion consisted in eliminating terms that contained a determiner other than the that remained after the splitting phase." ></td>
	<td class="line x" title="50:149	Only this determiner can occur in a term as it has the capacity, out of context, to refer to a concept or object in a knowledge field, i.e. the use 565 of the variant the low-line instead of the full term low fertility droughtmaster line 2." ></td>
	<td class="line x" title="51:149	The statistical criterion consisted in eliminating terms starting with the and appearing only once." ></td>
	<td class="line x" title="52:149	These two criteria enabled us to eliminate 30% (1304) candidates and to retain 70% (3159) which we consider to be likely terminological units." ></td>
	<td class="line x" title="53:149	We are aware that this filtering procedure remains approximate and cannot eliminate bad candidates like book review whose morphological and lexical make-up correspond to those of terms." ></td>
	<td class="line x" title="54:149	But we also observe that such bad candidates are naturally filtered out in later stages as they rarely possess variants and thus will not appear as research topics (see 4)." ></td>
	<td class="line x" title="55:149	2 Identifying syntactic variants Given the two syntactic structures under which a term can appear compound or syntagmatic we first pre-processed the terms by transforming those in a syntagmatic structure into their compound version." ></td>
	<td class="line x" title="56:149	This transformation is based on the following noun phrase formation rule for English : DAM1 h p mM2-+ D A mM2 M1 h where D, A and M are respectively strings of determiner, adjective and words whose place can be empty, h is a head noun, m is a word and p is a preposition." ></td>
	<td class="line x" title="57:149	Thus, the compound version of the specific nodulation of alfalfa will give the specific alfalfa nodulation." ></td>
	<td class="line x" title="58:149	This transformation does not modify the original structure under which a term occurred in the corpus." ></td>
	<td class="line x" title="59:149	It only serves to furnish input data to the syntactic variation identification programs." ></td>
	<td class="line x" title="60:149	This transformation which is equivalent to permutation (2.1) is the linguistic relation which once accounted for, reveals the formal nature of the other types of syntactic variations." ></td>
	<td class="line x" title="61:149	Also, it enables us to detect variants in the two syntactic structures thus accounting for syntactic variants such as defined in Tzoukermann et al.(1997)." ></td>
	<td class="line x" title="63:149	In what follows, h and t2 are terms." ></td>
	<td class="line x" title="64:149	2.1 Permutation (Perm) It marks the transformation of a term, from a syntagmatic structure to a compound one : t~=ANMl hpmM2 t2=AmM2NMI h 2 It apparently refers to a breed (line) of cattle." ></td>
	<td class="line x" title="65:149	where tl is really found in the corpus, N is a string of words that is either empty or a noun." ></td>
	<td class="line x" title="66:149	37 terms were concerned by this relation." ></td>
	<td class="line x" title="67:149	Some examples are given in Table 1." ></td>
	<td class="line x" title="68:149	2.2 Substitution (Sub) It marks the replacing of a component word in tj by another word in t2 in terms of equal length." ></td>
	<td class="line x" title="69:149	Only one word can be replaced and at the same position to ensure the interpretability of the relation." ></td>
	<td class="line x" title="70:149	We distinguished between modifier and head substitution." ></td>
	<td class="line x" title="71:149	 Modifier substitution (M-Sub) : t2 is a substitution of tl if and only if : tl-M 1 m M 2 h and t2 = M lm' M 2 h with m' ~ m  Head substitution (H-Sub) : t2 is a substitution of tj if and only if : tl = Mmh and t2 = Mm h' with h'~ h Tzoukermann et al.(1997) considered chemical treatment against disease and disease treatment as substitution variants whereas, in our study, after transformation, they would be a case of leftexpansion (L-Exp)." ></td>
	<td class="line x" title="73:149	Examples of head and modifier substitutions are given in Table 2." ></td>
	<td class="line x" title="74:149	1543 terms shared substitution relations : 1084 in the modifier substitution and 872 in the head substitution." ></td>
	<td class="line x" title="75:149	The same term can occur in both categories." ></td>
	<td class="line x" title="76:149	2.3 Expansion (Exp) Expansion is the generic name designating three elementary operations of word adjunction in an existing term." ></td>
	<td class="line x" title="77:149	Word adjunction can occur in three positions : left, right or within." ></td>
	<td class="line x" title="78:149	Thus we have left expansion, right expansion and insertion respectively." ></td>
	<td class="line x" title="79:149	 Left expansion (L-Exp) : t2 is a left-expansion of t~ if and only if : t~ = M h and t2 = M' m' M h  Right expansion (R-Exp) : t2 is a right-expansion of tz if and only if : t~ --M h and t2 = M h M' h'  Insertion (Ins) : t2 is an insertion of t~ if and only if : t~ =M1 mM2 h t2 = M1 m m' M' M2 h 566 Examples of each sub-type of expansion are given in Table 3." ></td>
	<td class="line x" title="80:149	Some terms combine the two types of expansion left and right expansions (noted LR-Exp), for example root of bragg --~ root exudate of so yabean cultivar bragg." ></td>
	<td class="line x" title="81:149	These complex expansion variants were also identified." ></td>
	<td class="line x" title="82:149	A total of 1014 terms were involved in the expansion variation relations." ></td>
	<td class="line x" title="83:149	Altogether, 82% (2593 out of 3159) terms were involved in the three types of syntactic variations studied showing the importance of the phenomena amongst terms." ></td>
	<td class="line x" title="84:149	Syntagmatic structure Compound structure accession of azolla-anabaena avirulent strain of pscudomonas syringae curling of root hair excision of nodule the specific nodulation of alfalfa azolla-anabaena accession avirulent pseudomonas syringae strain root hair curling / root-hair curling nodule excision the s7~ecific alfalfa nodulation Table 1." ></td>
	<td class="line x" title="85:149	Examples of permutation variants identified in the corpus." ></td>
	<td class="line x" title="86:149	Head substitution variants Modifier substitution variants nodule development regulation alfalfa root hair nodule development arrest curled root hair nodule development consequence lucerne root hair infection thread development characteristic dna fingerprinting infection thread formation conventional dna fingerprinting infection thread initiation complex dna fingerprinting nodulation of soybean mutant enzymatic amplification of dna isolation of soybean mutant amplification of genomic dna property of soybean mutant Table 2." ></td>
	<td class="line x" title="87:149	Some head and modifier substitution variants identified in the corpus." ></td>
	<td class="line x" title="88:149	Left expansion Right expansion Insertion self-licking --~ refractory self-licking stereot37~ic self-licking nitrogenase activity nitrogenase activity of cv." ></td>
	<td class="line x" title="89:149	bragg nitrogenase activity of nitrate nitrogenase activity of nts382 nitrogcnase activity of soyabean blue light --~ blue light-induced expression blue light induction blue lil~ht induction experiment immigrant of eastern countries immigrant children of eastern countries 3 conserved domain --~ conserved central domain conservcd protein domain fast staining of dna---~ fast silver staining of dna Table 3." ></td>
	<td class="line x" title="90:149	Examples of expansions variants identified in the corpus." ></td>
	<td class="line x" title="91:149	The programs identifying syntactic variants were written in the Awk language and implemented on a Sun Sparc workstation." ></td>
	<td class="line x" title="92:149	Syntactic variations possess formal properties such as symmetry and antisymmetry." ></td>
	<td class="line x" title="93:149	Permutation and substitution engender a symmetrical relation between terms, e.g. genomic dna cy template dna." ></td>
	<td class="line x" title="94:149	3 This cxample is fictitious." ></td>
	<td class="line x" title="95:149	567 Expansion engenders an antisymmetrical or order relation between terms, for instance nitrogen fixation<nitrogen fixation gene<nitrogen fixation gene activation." ></td>
	<td class="line x" title="96:149	These two formal properties will form the second level for differentiating variation relations during clustering (see 4)." ></td>
	<td class="line x" title="97:149	3 Conceptual properties of syntactic variations Syntactic variations yield conceptual relations which can reveal the association of concepts represented by the terms." ></td>
	<td class="line x" title="98:149	We observed three conceptual relations : class_of, equivalence, generic/specifitc." ></td>
	<td class="line x" title="99:149	 Class_of Substitution (Sub) engenders a relation between term variants which can be qualified as 'class_off." ></td>
	<td class="line x" title="100:149	Modifier substitution groups properties around the same concept class : template dna, genomic dna, tar_qfg~ dna are properties associated to the class of concept named 'dna'." ></td>
	<td class="line x" title="101:149	Head substitution groups concepts or objects around a class of property : dna .fragment, dna sequence, dna fingerprinting are concepts associated to the class of property named dna." ></td>
	<td class="line x" title="102:149	This relation does not imply a hierarchy amongst terms thus somehow reflecting the symmetrical relation engendered on the formal level." ></td>
	<td class="line x" title="103:149	 Equivalence Permutation engenders a conceptual equivalence between two variants which partially echoes the formal symmetry, e.g. dna fragment -fragment of dna." ></td>
	<td class="line x" title="104:149	 Generic~specific Expansion, all sub-types considered, engenders a generic/specific relation between terms which echoes the antisymmetrical relation observed on the formal level." ></td>
	<td class="line x" title="105:149	Expansion thus introduces a hierarchy amongst terms and allows us to construct paradigms that may correspond to families of concepts or objects (R-Exp, LR-Exp) or families of properties (L-Exp, Ins)." ></td>
	<td class="line x" title="106:149	Jacquemin (1995) reported similar conceptual relations for insertion and coordination variants." ></td>
	<td class="line x" title="107:149	4 Identifying topics organisation We built a novel clustering method Classification by Preferential Clustered Link (CPCI,)  to cluster terms into classes of research topics." ></td>
	<td class="line x" title="108:149	Fir,'.;t we distinguished two categories of variatioi~ relations : those affecting modifier words noted COMP (M-Sub, L-Exp, Ins) and those affecting the head word noted CLAS (H-Sub, LRExp, R-Exp)." ></td>
	<td class="line x" title="109:149	The need to value the variation relations may arise if a type (~,,;ymmetrical or antisymmetrical) is in the minority, q-'o preserve the information it carries, a default vah~e is fixed for this minority type." ></td>
	<td class="line x" title="110:149	The value of the majority type is then calculated as its proportion with regard to the minority type." ></td>
	<td class="line x" title="111:149	In our corpus, Exp (antisymmetrical) relations were in minority compared to Sub (symmetrical relations)." ></td>
	<td class="line x" title="112:149	Their default value was set at 1." ></td>
	<td class="line x" title="113:149	The value of Sub relations was then given by the ratio Exp/Sub where Exp (respectively Sub) is the total number of expansions relations (respectively substitutions) between terms in the corpus." ></td>
	<td class="line x" title="114:149	This valuing of variation relations highlights a type of information that would otherwise be drowned but is not a mandatory condition for the clustering algorithm to work." ></td>
	<td class="line x" title="115:149	COMP relations structure term variants around the same head word thus forming components representing the paradigms in the corpus." ></td>
	<td class="line x" title="116:149	These paradigms typically correspond to isolated topics (see Table 4 hereafter)." ></td>
	<td class="line x" title="117:149	The strength of the link between two components Pi and Pj is given by the sum of the value of variation relations between them." ></td>
	<td class="line x" title="118:149	More formally, we define the COMP relation between terms as : ti COMP tj iff ti and tj share the same head word and if one is the variant of the other." ></td>
	<td class="line x" title="119:149	The transitive closure COMP* of COMP partitions the whole set of terms into components." ></td>
	<td class="line x" title="120:149	These components are not isolated and are linked by transversal CLAS relations implying a change of head word, thus bringing to light the." ></td>
	<td class="line x" title="121:149	as:sociations between research topics in the c~npus." ></td>
	<td class="line x" title="122:149	CLAS relatior)s cluster components basing on the following principle : two components Pi and Pj are clustered if the link between them is stronger than the link between either of them and any other componem Pk which has not been clustered neither with Pi nor with Pj." ></td>
	<td class="line x" title="123:149	We call classification, a partition of terms in such classes." ></td>
	<td class="line x" title="124:149	An efficient algorithm has been implemented in IbekweSanJuan (1997) which seeks growing series of 568 such classifications." ></td>
	<td class="line x" title="125:149	These series represent more or less fine-grained structurings of the corpus." ></td>
	<td class="line x" title="126:149	A more formal description of the CPCL method can be found in Ibekwe-SanJuan (1998)." ></td>
	<td class="line x" title="127:149	Table 4 shows a component and a class." ></td>
	<td class="line x" title="128:149	The component formed around the head word hair reveals the properties (modifiers) associated with this topic but does not tell us anything about its association other topics." ></td>
	<td class="line x" title="129:149	The class on the other hand reveals the association of hair with other topics." ></td>
	<td class="line x" title="130:149	A component \[J A class of terms ~'lifalfa root l~air curled root hair deformed root hair lucerne root hair root hair alfalfa root hair concomitant root hair curling curled root hair deformed root hair hair deIbrmation lucerne root hair occasional hair curling root deformation root hair root hair curling root hair deformation some root hair curling Table 4." ></td>
	<td class="line x" title="131:149	A component and a class." ></td>
	<td class="line x" title="132:149	The graph in Figure 1 hereafter shows the global organisation of classes obtained from the classification of the entire corpus (2593 syntactic term variants)." ></td>
	<td class="line x" title="133:149	External links between classes are given by bold lines for R-Exp and LR-Exp, dotted lines portray head-substitution H-Sub." ></td>
	<td class="line x" title="134:149	Only one term from each class is shown for legibility reasons." ></td>
	<td class="line x" title="135:149	We observe that classes like 17, 19, 18 and 9 have a lot of external links and seem to be at the core of research topics in the corpus." ></td>
	<td class="line x" title="136:149	Classes like 12, 3 and 13 share strong external links with a single class which could indicate privileged thematic relations." ></td>
	<td class="line x" title="137:149	The unique link between class 3 and 19 is explained by the fact that 3 represented an emerging topic 4 at the time the corpus was constituted (1993) : the research done around a new gene type (the klebsiella pneumoniae nifb gene)." ></td>
	<td class="line x" title="138:149	So it was relevant that this class be strongly linked to class 19 without being central." ></td>
	<td class="line x" title="139:149	Also, class 10 represented an emerging topic in 1993 : the research for retrotransposable elements which enables the passing from one gene to another." ></td>
	<td class="line x" title="140:149	Research topics evolution and transformation can be traced through a chronological analysis of clustered term variants (see Ibekwe-SanJuan 1998)." ></td>
	<td class="line x" title="141:149	The results obtained can support scientific and technological watch activities." ></td>
	<td class="line x" title="142:149	Concluding remarks Syntactic variation relations are promising linguistic phenomena for tracking topic evolution in texts." ></td>
	<td class="line x" title="143:149	However, being that clustering is based on syntactic variation relations, tile CPCL method cannot detect topics related through semantic or pragmatic relations." ></td>
	<td class="line x" title="144:149	For instance, the topic depicted by class 8 (glycine max) should have been related to topic 20 (lucerne plant) from a semantic viewpoint." ></td>
	<td class="line x" title="145:149	Their separation was caused by the absence of syntactic variations between the constituent terms." ></td>
	<td class="line x" title="146:149	Such relations can be brought to light only if further knowledge (semantic) is incorporated into the relations used for clustering." ></td>
	<td class="line x" title="147:149	In the future, we will test our clustering method on another corpus of a larger size and extend our study to other variation phenomena as possible topic shifting devices." ></td>
	<td class="line x" title="148:149	4 The interpretations given here are based on an oral communication with a domain information specialist." ></td>
	<td class="line x" title="149:149	569 A cknowledgements Thanks to the reviewers for their constructive comments which I hope, helped improve this paper." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C98-2120
Identifying Syntactic Role of Antecedent in Korean Relative Clause Using Corpus and Thesaurus Information
Li, Hui-Feng;Lee, Jong-Hyeok;Lee, Gary Geunbae;"></td>
	<td class="line x" title="1:157	Identifying Syntactic Role of Antecedent in Korean Relative Clause Using Corpus and Thesaurus Information Hui-Feng Li~ Jong-Hyeok Lee, Geunbae Lee Department of Computer Science and Engineering Pohang University of Science and Technology San 31 Hyoja-dong, Nam-gu, Pohang 790-784, Republic of Korea hflee@madonna.postech.ac.kr, {jhlee, gblee}@postech.ac.kr Abstract This paper describes an approach to identifying the syntactic role of an antecedent in a Korean relative clause, which is essential to structural disambiguation and semantic analysis." ></td>
	<td class="line x" title="2:157	In a learning phase, linguistic knowledge such as conceptual co-occurrence patterns and syntactic role distribution of antecedents is extracted from a large-scale corpus." ></td>
	<td class="line x" title="3:157	Then, in an application phase, the extracted knowledge is applied in determining the correct syntactic role of an antecedent in relative clauses." ></td>
	<td class="line x" title="4:157	Unlike previous research based on co-occurrence patterns at the lexical level, we represent co-occurrence patterns with concept types in a thesaurus." ></td>
	<td class="line x" title="5:157	In an experiment, the proposed method showed a high accuracy rate of 90.4% in resolving ambiguities of syntactic role determination of antecedents." ></td>
	<td class="line x" title="6:157	1 Introduction A relative clause is the one that modifies an antecedent in a sentence." ></td>
	<td class="line x" title="7:157	To determine the syntactic role of the antecedent in a verb argument structure of relative clause is important in parsing and structural disambiguation(Li et al., 1998)." ></td>
	<td class="line x" title="8:157	While applying case frames of a verb for structural disambiguation, identifying the role of antecedent will affect the correctness of structural disambiguation impressively." ></td>
	<td class="line x" title="9:157	In this paper, we will describe a method of identifying the syntactic role of antecedents, which consists of two phases." ></td>
	<td class="line x" title="10:157	First, in the learning phase, conceptual patterns (CPs) and syntactic role distribution of antecedents are extracted from a corpus of 6 million words, the Korean Language Information Base (KLIB)." ></td>
	<td class="line x" title="11:157	The conceptual patterns reflect the possible case restriction of a verb with concept types, while the syntactic role distribution shows the preference of syntactic role of antecedents of a verb." ></td>
	<td class="line x" title="12:157	Second, in the application phase, the syntactic role of an antecedent is decided using CPs and the syntactic role distribution." ></td>
	<td class="line x" title="13:157	In regards to the rest of this paper, Section 2 will review the problems and related work." ></td>
	<td class="line x" title="14:157	Section 3 will describe a statistical approach of conceptual pattern extraction from a large corpus as knowledge for determining syntactic roles." ></td>
	<td class="line x" title="15:157	Section 4 will describe how to identify syntactic roles using conceptual patterns and syntactic role distribution of antecedents in the corpus." ></td>
	<td class="line x" title="16:157	Section 5 will then present an experimental evaluation of the method." ></td>
	<td class="line x" title="17:157	The last section makes a conclusion with some discussion." ></td>
	<td class="line x" title="18:157	The Yale Romanization is used to represent Korean expressions." ></td>
	<td class="line x" title="19:157	2 Problems and Related Work In English, it is possible to recognize the syntactic role of antecedents by their position (trace) in relative clauses and the valency information of verbs." ></td>
	<td class="line x" title="20:157	For example, the syntactic role of an antecedent man can be recognized as subject of the relative clause in a sentence 'He is the man who lives next door' and as object in a sentence 'He is the man whom I met.'" ></td>
	<td class="line x" title="21:157	The relative pronouns such as who, whom, that, whose, and which can also be used in identifying the role of antecedents in relative clauses." ></td>
	<td class="line x" title="22:157	However, it is not a trivial work to identify the syntactic role of antecedents in Korean relative clauses." ></td>
	<td class="line x" title="23:157	Korean is such a head final language that the antecedent comes after the relative clause." ></td>
	<td class="line x" title="24:157	The rest of this section will describe three main characteristics of Korean relative clauses that make it difficult to determine the syntactic role of their antecedents." ></td>
	<td class="line x" title="25:157	The first characteristic is that unlike English, Korean lacks relative words corresponding to English 756 SDT: .  o,   Figure 1: Syntactic dependency tree for (1) relative pronouns." ></td>
	<td class="line x" title="26:157	Instead, an adnominal verb ending follows its verb stem of a relative clause modifying an antecedent." ></td>
	<td class="line x" title="27:157	The adnominal verb ending does not provide any information about the syntactic role of antecedent." ></td>
	<td class="line x" title="28:157	For example, the relative clause kang-cyse hulu(flow in a river) in sentence (1) modifies the antecedent mwul(water), while adnominal verb ending nun provides no clue about the syntactic role of the antecedent mwul (water)." ></td>
	<td class="line x" title="29:157	Figure 1 shows the syntactic dependency tree (SDT) of sentence (1)." ></td>
	<td class="line x" title="30:157	We need to decide the syntactic role of the antecedent mwul(water) in the argument structure of the verb hulu(flow) when applying case frames of the verb for structural disambiguation." ></td>
	<td class="line x" title="31:157	The dependency parser (Lee, 1995) only gives the syntactic relation rood between them, which should be regarded as subject in the relative clause." ></td>
	<td class="line x" title="32:157	(1) nanun kang-eyse hulu-nun mwul-lul poattta." ></td>
	<td class="line x" title="33:157	(I saw water that flowed in a river.)" ></td>
	<td class="line x" title="34:157	As the second characteristic, the syntactic role of an antecedent cannot be determined by word order." ></td>
	<td class="line x" title="35:157	This is because Korean is a relatively free word-order language like Japanese, Russian, or Finnish, and also because some arguments of a verb may be frequently omitted." ></td>
	<td class="line x" title="36:157	In sentence (2), for example, the verb of relative clause nolay-lul pwulless-ten (where \[2\] sang a song \[at the place\]) have two arguments \[I\] and \[place\] omitted." ></td>
	<td class="line x" title="37:157	Thus, the antecedent kos(place) might be identified as subject or adverbial in the relative clause." ></td>
	<td class="line x" title="38:157	arnin~ Partial Parser I J:'2, 22  \] Figure 2: System architecture (2) nolay-lul pwulless-ten kos-ey ha-nun kassta." ></td>
	<td class="line x" title="39:157	(I went to the place where \[I\] sang a song \[at the place\].)" ></td>
	<td class="line x" title="40:157	The third characteristic of Korean relative clauses is that the case particle of an antecedent, that indicates the syntactic role in the relative clause, is omitted during relativization." ></td>
	<td class="line x" title="41:157	In fact, in a relatively free-word order language, the case particles are very important to the syntactic role determination." ></td>
	<td class="line x" title="42:157	Due to lack of syntactic clues, it is very difficult to construct general rules for identifying tile syntactic role of antecendents." ></td>
	<td class="line x" title="43:157	Thus, the corpus-based method has been prefered to tile rule-based one in solving the problem of syntactic role determination in Korean relative clauses." ></td>
	<td class="line x" title="44:157	Yang and Kim (1993) proposed a corpus-based method, where, fbr each noun/verb pair, its word co-occurrence and subcategorization scores are extracted at lexical level." ></td>
	<td class="line x" title="45:157	Park and Kim (1997) described a method of semantic role determination of antecedents using verbal patterns and statistic information from a corpus." ></td>
	<td class="line x" title="46:157	These word co-occurrence patterns are all at lexical-level, so we have to construct a large amount of word co-occurrence patterns and statistical information before applying to a real large-scale problem." ></td>
	<td class="line x" title="47:157	Actually, the system performance mainly relies on the domain of application, the number of word cooccurrence patterns extracted, and the size of corpus." ></td>
	<td class="line x" title="48:157	757 In the following sections, we will describe an approach to acquiring statistical information at conceptual level rather than at lexical level from a corpus using conceptual hierarchy in the Kadokawa thesaurus titled New Synonym Dictionary (Ohno and Hamanishi, 1981), and also describe a method of syntactic role determination using the extracted knowledge." ></td>
	<td class="line x" title="49:157	The system architecture is shown in Figure 2." ></td>
	<td class="line x" title="50:157	3 Extraction of Statistic Information from Corpus First, for each of 100 verbs selected by order of frequency in the KLIB (Korean Language Information Base) corpus of 6 million words, its syntactic relational patterns (SRPs) of the form (Noun, Syntactic relation, Verb) are extracted from the corpus." ></td>
	<td class="line x" title="51:157	Then, the nominal words in the SRPs are substituted with their corresponding concept codes at level 4 of the Kadokawa thesaurus." ></td>
	<td class="line x" title="52:157	A nominal word may have multiple meanings such as C1, C2, ,Ca." ></td>
	<td class="line x" title="53:157	However, since we cannot determine which meaning of the nominal word is used in a SRP, we uni1 formly add n to the frequency of each concept code." ></td>
	<td class="line x" title="54:157	Through this processing, the syntactic relational pattern (SRP) changes into the conceptnal frequency pattern (CFP), ({< C1, fl > ,'~ C2,f2 >,,'~ Cm, fin >},SRj, Vk), where Ci represents a concept code at level four of the Kadokawa thesaurus, fi indicates the frequency of the code Ci, and SRj shows a syntactic relation between these concept codes and verb Vk." ></td>
	<td class="line x" title="55:157	These patterns are then generalized by a concept type filter into more abstract conceptual patterns (CPs), {({C1, C2, , Ca}, SRj, Vk)\[1 _< j _< 5, 1 <_ k _< 100}." ></td>
	<td class="line x" title="56:157	Unlike in CFPs, the concept code in the more generalized CPs may be not only at level four (denoted as L4), but also at level three (L3) and two (L2)." ></td>
	<td class="line x" title="57:157	In addition to the CPs, we also extract the syntactic role distributiion of antecedents." ></td>
	<td class="line x" title="58:157	3.1 Retrieving Syntactic Relational Patterns from Corpus Unlike the conventional parsing problem whose main goal is to completely analyze a whole sentence, the extraction of syntactic relational patterns (SRPs) aims to partially analyze sentences and thus to get the syntactic relations between nominals and verbs." ></td>
	<td class="line x" title="59:157	For this, we designed a partial parser, the analysis result of which is obviously not as precise as that of a full-parser." ></td>
	<td class="line x" title="60:157	However, it can provide much useful information." ></td>
	<td class="line x" title="61:157	For the set of 100 verbs, a total of 282,216 syntactic relational patterns (SRPs) was extracted from tile KLIB corpus." ></td>
	<td class="line x" title="62:157	During the generalization step, the problematic patterns are filtered out." ></td>
	<td class="line x" title="63:157	In Korean, the syntactic relation of nominal words toward a verb is mainly determined by case particles." ></td>
	<td class="line x" title="64:157	During the extraction of SRPs (Ni,SRj, Irk), we only consider the syntactic relation SRjs determined by 5 types of case particles: nominative (-i//ka//kkeyse), accusative (-ul/lul), and three adverbial (-ey//eynun, se/eyse/eysen n, -lo/ulo/ulo un)." ></td>
	<td class="line x" title="65:157	3.2 Conceptual Pattern Extraction 3.2.1 Thesaurus Hierarchy For the purpose of type generalization of norainal words in SRPs, the Kadokawa thesaurus titled New Synonym Dictionary (Ohno and Hamanishi, 1981) is used, which has a four-level hierarchy with about 1,000 semantic classes." ></td>
	<td class="line x" title="66:157	Each class of upper three levels is further divided into 10 subclasses, and is encoded with a unique number." ></td>
	<td class="line x" title="67:157	For example, tile class 'stationary' at level three is encoded with the number 96 and classified into ten subclasses, Figure 3 shows the structure of the Kadokawa thesaurus." ></td>
	<td class="line x" title="68:157	To assign the concept code of Kadokawa thesaurus to Korean words, we take advantage of the existing Japanese-Korean bilingual dictionary (JKBD) that was developed for a Japanese-Korean MT system called COBALTJ/K. The bilingual dictionary contains more than 120,000 words, the meaning of which is encoded with the concept codes that are at level four in tile Kadokawa thesaurus." ></td>
	<td class="line x" title="69:157	Thus, Korean words in the SRPs are automatically assigned their corresponding concept codes of level four through JKBD." ></td>
	<td class="line x" title="70:157	3.2.2 Principle of Generalization We encoded the nouns in SRPs extracted by the parser with concept codes from the Kadokawa thesaurus, and examined histograms of the frequency of concept codes." ></td>
	<td class="line x" title="71:157	We observed that the frequency of codes for different syntactic relations of a verb showed very different distribution shapes." ></td>
	<td class="line x" title="72:157	This means that we could use the distribution of concept codes, together with their frequencies as clues for conceptual pattern ex758 c,mcept I F I I I I I I I I I I 1 \] 3 4 5 $ 7 I I I \[ I I \[ I I il I I I I I I t \[ I I I I nl~ wlvtl* kctq." ></td>
	<td class="line x" title="73:157	MIM #|~4 a~phyd~l:,t~~ Ip~:.dld df~." ></td>
	<td class="line x" title="74:157	flw.d gl.dh bttil rur~* stath~ aark I~ds ~h~4~t~'d*r thcr ~phy ~l 4~olly 1~ ~ di*glu~ m.ry *t el l1 ,3 14 t~ ~t an N tt ~ *1 W1 *J ~4 ~ ~ ~ ~ I 1 \[ I I I I I i I I-\] I I I I I I I I I I o~o est e$2 ~3 o~t e$$ OS~ e67 e~a ~$ 9SO tgl ~1 t~3 ~4 ~ ~s *.t7 ~9 t~ Figure 3: Concept hierarchy of Kadokawa thesaurus traction." ></td>
	<td class="line x" title="75:157	From the histograms of codes of both subject and object relational patterns for the verb ttena-ta (leave), we observed that concept codes about human (codes from 500 to 599) appear most frequently in the role of subject, and (:odes of position (from 100 to 109), codes of place (from 700 to 709) and codes of building (fl'om 940 to 949) appear most often in the role of object." ></td>
	<td class="line x" title="76:157	For each verb Vk, we first analyzed the cooccurrence frequencies fi of concept codes Ci of noun N, and then computed an average frequency fave,~ and standard deviation at around fav~,e, at level /~ (denoted as Le) of the concept hierarchy." ></td>
	<td class="line oc" title="77:157	We then replaced fi with its associated z-score kl,e. kf,e is the strength of code frequency f at Le, and represents the standard deviation above the average of frequency f, ve,t. Referring to Smadja's definition (Smadja, 1993), the standard deviation at at Lt and strength kLt of the code frequencies are defined as shown in formulas 1 and 2." ></td>
	<td class="line x" title="78:157	r~t . 2 ~/Ei--l (fi,t fare,t) *=V U--1 (1) f i,, f aw,e ki~,~,~ (2) where fi,t is the fl'equency of concept code Ci at Lt of Kadokawa thesaurus, fave,t is the average frequency of codes at L~, ne is the number of concept codes at L~." ></td>
	<td class="line x" title="79:157	3.2.3 Code Generalization The standard deviation at at Lt characterizes the shape of the distribution of code frequenLevel Threshold of standard deviation o'0~t Threshold of subj \] obj I advL." ></td>
	<td class="line x" title="80:157	\[ adv2 I adva Strength ko,t L4 2.0 8.0 0.5 0.1 0.9 ko,4-=4.0 L3 6.0 16:0 1.5 2.0 2.0 ko,3=l.0 L2 30.0 50.0 15.0 4.0 i0.0 ko,2=-0,60 Table 1: Thresholds of the filter cies." ></td>
	<td class="line x" title="81:157	If a~ is small, then the shape of tile histogram will tend to be flat, which means that each concept code can be used equally as an argument of a verb with syntactic role SRi." ></td>
	<td class="line x" title="82:157	If at is large, it means that there is one or more codes that tend to be peaks in the histogram, and the corresponding nouns for these concept codes are likely to be used as arguments of a verb." ></td>
	<td class="line x" title="83:157	The filter in our system selects the patterns that have a variation larger than threshold ao,e, and pulls out the concept codes that have a strength of frequency larger than threshold ko,e. If the value of the variation is small, than we (:an assume there is no peak frequency for the nouns." ></td>
	<td class="line x" title="84:157	The patterns that are produced by the filter should represent the concept types of extracted words that appear most frequently as syutactic role SRi with verb Vk." ></td>
	<td class="line x" title="85:157	We later analyzed the distribution of frequency fi in CFPjs to produce an average frequency faw,g and standard deviation atThrough experimentation, we decided the threshold of standard deviation 0,e and strength of frequency ko,e as shown in Table 1." ></td>
	<td class="line x" title="86:157	The lower the value of threshold ko,e is assigned, the more concept codes can be extracted as conceptual patterns flom the CFPs." ></td>
	<td class="line x" title="87:157	We maintained a balance between extracting conceptual codes at low levels of the conceptual hierarchy for the specific usage of concept type and extracting general concept types for enhancing overall system pertbrmance." ></td>
	<td class="line x" title="88:157	These values may be variable in different application." ></td>
	<td class="line x" title="89:157	In Table 2, we enlist the concept types that have more than 5 appearances in the CFP of verb ttena-ta (leave)." ></td>
	<td class="line x" title="90:157	The strength of frequencies for generalization is calculated with formula 2." ></td>
	<td class="line x" title="91:157	1 0.932 kl,4 -2.82513 0.024 759 code code code code code code (freq.)" ></td>
	<td class="line x" title="92:157	(freq.)" ></td>
	<td class="line x" title="93:157	(freq.)" ></td>
	<td class="line x" title="94:157	(freq.)" ></td>
	<td class="line x" title="95:157	(freq.)" ></td>
	<td class="line x" title="96:157	(freq.)" ></td>
	<td class="line x" title="97:157	051(10) 086(7) 117(5) 118(7) 158(5) 160(5) 179(5) 324(5) 410(12) 411(14) 430(16) 436(5) 480(7) 481(8) 482(9) 500(23) 501(31) 503(31) 507(35 508(30) 511(11) 513(8) 514(8) 515(5) 516(5) 519(6) 521(15) 522(19) 523(10) 525(7) 530(5) 535(6) 540(15) 550(7) 572(8) 576(9) 580(7) 581(7) 590(8) 591(5) 595(12) 814(9) 822(5) 828(5) 830(5) 833(7) 941(8) 997(7) 998(6) other(427) * No." ></td>
	<td class="line x" title="98:157	of codes: n4 = 932 * Average freq.: fare,4 = 932/1000 = 0.932 * Standard deviation: c~ t = 2.821530 * 'other' in the table means the total freq." ></td>
	<td class="line x" title="99:157	of nouns less than 5 * The numbers in brackets are the frequencies of code appearance Table 2: Concept types and frequencies in CFP ({< Ci, fi > },subj,ttena-ta) 12 0.932 k12,4 2.82513 3.9176 14 0.932 k14,4 2.82513 4.626 Since the value of k0,4 is set at 4.0, as shown in Table 1, the concept codes with frequencies of more than 13, as the equation for k14,4 shows, are selected as generalized concept types at L4." ></td>
	<td class="line x" title="100:157	After abstraction at L4, the system performs generalization at L3." ></td>
	<td class="line x" title="101:157	It removes selected frequencies, such as frequency 14 of code 411 in Table 2, and sums up the frequencies of the remaining concept codes to form the frequency of higher level group." ></td>
	<td class="line x" title="102:157	For example, the system removes the frequency for code 411 from the group {410(12), 411(14), 412(3), 413(0), 414(0), 415(0), 416(1), 417(0), 418(0), 419(0)}, then sums up the frequencies of the remaining codes for a more abstract code of 41." ></td>
	<td class="line x" title="103:157	The frequency of code 41 then becomes 16." ></td>
	<td class="line x" title="104:157	Through this process, the system performs a generalization at La for the more abstract types of the concept." ></td>
	<td class="line x" title="105:157	The system calculates a~ and strength Kf,~, selects the most promising codes, and stores conceptual patterns ({C1, C2, C3, }, SRj, Vk) as the knowledge source for syntactic role determination in real texts, where concept type Ci is created by the generalization procedure." ></td>
	<td class="line x" title="106:157	After generalization of the CFP patterns for the subject role of the verb ttena-ta (leave), the produced conceptual patterns are: ({411,430, 500, , 06, 11, , 99, 1}, subj, ttena-ta)." ></td>
	<td class="line x" title="107:157	3.3 Syntactic Role Distribution of Antecedents In (Yang et al., 1993), they defined subcategorization score (SS) of a verb considering the verb argument structure in a corpus." ></td>
	<td class="line x" title="108:157	They asserted that the SS of a verb represents how likely a verb might have a specific grammatical complement." ></td>
	<td class="line x" title="109:157	We observed from analyzing the corpus that we cannot infer the syntactic roles of antecedents from subcategorization scores since the syntactic role distribution of verb arguments in a corpus is so different from the syntactic role distribution of antecedents due to the property of free word language." ></td>
	<td class="line x" title="110:157	In Korean, an argument of a verb could be omitted, and so the subcategorization score don't provide possible trend of the role of antecedent in many cases." ></td>
	<td class="line x" title="111:157	For example, 26.8% of arguments of the verb ttenata (leave) are used as subjects, and 54.4% are used as objects, but 74.41% of antecedents of the verb are of subject role, and 6.9% are of object role." ></td>
	<td class="line x" title="112:157	Although the distribution of antecedents is necessary to our task, we cannot automatically retrieve the syntactic role distribution of them from the corpus." ></td>
	<td class="line x" title="113:157	We extracted relative clauses for specific verbs from the corpus, and then counted the number of syntactic roles of the antecedents manually by language trained people." ></td>
	<td class="line x" title="114:157	Since there are about 200 to 500 relative clauses for each verb in the corpus, it is possible to check this information." ></td>
	<td class="line x" title="115:157	This information is represented by relative score RSk(~qRi) of syntactic role SRi for antecedents of verb Vk as is shown bellow and is used in syntactic role determination as described in section 4: RSk(SRi) freqk(SRi) (3) freq(Vk) where freq(Vk) are the frequency of verb Vk of relative clauses, and freqk(SRi) is the frequency of syntactic role SR i of antecedents in relative clauses including verb Vk in the corpus." ></td>
	<td class="line x" title="116:157	4 Identifying Deep Syntactic Relation While determining syntactic relation for antecedents of relative clauses, the system checks the argument structure of the verb in a relative clause first, and then records the empty (or omitted) arguments of the verb in relative 760 2*2 is-a 2*2 is-a 2'1 is-a 4+2 penalty(l.0) 2+3 penahy(0.5) 4+2 penalty(0.5) Figure 4: Conceptual similarity computation Syntactic No." ></td>
	<td class="line x" title="117:157	of Percentage Accuracy relation appearances (%) (%) subject 1,087 object adverb(-ey) adverb(-eyse) adverb(-lo) total 431 12i 19 Ill, 1,772 61.34% 24.32% 6.82% L08% ' 6.44% 100% 90% 92% 89% 92% 89% 90.4% Table 3: The test results of syntactic role deterruination for antecedents clause referring to the verb valency information." ></td>
	<td class="line x" title="118:157	The antecedent that the verb phrase is modifying can be one of these empty arguments." ></td>
	<td class="line x" title="119:157	An antecedent (a noun) usually has one or more meanings, which causes ambiguity in determining the correct syntactic relation between the antecedent and a verb." ></td>
	<td class="line x" title="120:157	We assume that an antecedent has meanings C1, C2, Ca, , C,,, and that CPi is a conceptual pattern ({Pt, P2, , Pro}, SRi, Vk) corresponding to syntactic relation SRi of verb Vk." ></td>
	<td class="line x" title="121:157	The evaluation score SIMi(Np, Vk) of an antecedent N~, that can be syntactic role SRi with verb Vk is defined as formula 4, and conceptual similarity Csim(Cw, Pj) between concept Cw and Pj as formula 5." ></td>
	<td class="line x" title="122:157	SIMi(Np,Vk) = max(Csim(Cw,Pj)) 1 < w < n, 1 < j < m (4) 2  level(MSCA(C~,, Pj)) Csirn(Cw, Pj) = level(Cw) + level(Pj) * ispenalty (5) where MSCA(Cw, Pj) in Csirn(Cw, Pj) represents the most specific common ancestor (MSCA) of concepts Cw and Pj in the Kadokawa concept hierarchy." ></td>
	<td class="line x" title="123:157	Level(Cw) refers to the depth of concept Cw from the root node in the concept hierarchy." ></td>
	<td class="line x" title="124:157	Is_a Penalty is a weight factor reflecting that C~, as a descendant of Pj is preferable to other cases." ></td>
	<td class="line x" title="125:157	Conceptual similarity computation with formula 5 is shown in Figure 4." ></td>
	<td class="line x" title="126:157	Based on these definitions, the syntactic relation SRj between antecedent Np and verb Vk can be calculated as follows: 1." ></td>
	<td class="line x" title="127:157	Let R = {SI~ilSRi is a syntactic relation of an empty (or omitted) argument in the relative clause of Vk, 1 < i < 5}." ></td>
	<td class="line x" title="128:157	2." ></td>
	<td class="line x" title="129:157	For each conceptual pattern CPi of verb Vk of which SRi is in R, and for each concept code Pi in CPi, compute SIMi(Np, Vk)." ></td>
	<td class="line x" title="130:157	3." ></td>
	<td class="line x" title="131:157	Determine the syntactic relation of antecedent Np to SRj on the condition that SIMj(Np, Vk) has the largest value in {SIMi(Np, Vk)ll < i < 5} and SRj in R. If two or more SIMi(Np, Vk) have the same value, decide syntactic role referring to the higher relative score RSk(SRi) of the syntactic role of the verb Vk." ></td>
	<td class="line x" title="132:157	Here, syntactic relation can be one of subj, obj, advl, adv2, and adv3." ></td>
	<td class="line x" title="133:157	The symbols advl, adv2, and adv3 represent adverbs with case particles -ey, -eyse, and -lo, respectively." ></td>
	<td class="line x" title="134:157	5 Experimental Evaluation An informal way to evaluate the correctness of syntactic relation determination is to have an expert examine the test patterns and source sentences that the patterns appears, and give his/her judgment about the correctness of the results produced by the system." ></td>
	<td class="line x" title="135:157	In our experiment, the correctness of syntactic and conceptual relation determination was evaluated manually by humans who were well trained in dependency syntax." ></td>
	<td class="line x" title="136:157	As a test set, we extracted 1,772 sentences that included relative clauses for the 100 verbs from 1.5 million word corpora of integrated Korean information base and test books of primary school." ></td>
	<td class="line x" title="137:157	The distribution of syntactic relation of antecedents among them and the test results were shown in Table 3." ></td>
	<td class="line x" title="138:157	There were 1,087 antecedents (61.34%) that were of subject role." ></td>
	<td class="line x" title="139:157	The baseline accuracy of the problem is 61.34%." ></td>
	<td class="line x" title="140:157	That is, if we always select subject role for antecedents, the accuracy will reach 61.34%." ></td>
	<td class="line x" title="141:157	761 Our system showed 90.4% of accuracy on average in syntactic relation identification, which shows that the conceptual patterns and relative score of syntactic relation produced in the first phase can be a good source for determining the syntactic relation of an antecedent." ></td>
	<td class="line x" title="142:157	Through experiment, we observed several factors that affect the performance of the system." ></td>
	<td class="line x" title="143:157	First, the multiple meanings of a noun will affect the frequency distribution of concept codes." ></td>
	<td class="line x" title="144:157	In our system, we cope with this problem by adjusting the threshold of standard deviation and strength value." ></td>
	<td class="line x" title="145:157	The second problem is the sparseness of corpus domain." ></td>
	<td class="line x" title="146:157	If the corpus for learning is specified as a certain domain, it will greatly increase the validity of conceptual patterns." ></td>
	<td class="line x" title="147:157	If we use a sense tagged corpus in the learning stage, we can achieve high accuracy in syntactic relation determination." ></td>
	<td class="line x" title="148:157	6 Concluding Remarks This paper describes an approach for syntactic role determination between an antecedent and a verb in relative clause for semantic analysis." ></td>
	<td class="line x" title="149:157	This method consists of two phases." ></td>
	<td class="line x" title="150:157	In the first phase, the system extracts conceptual patterns and syntactic role distribution of antecedents from a large corpus." ></td>
	<td class="line x" title="151:157	In the second phase, the system applies the extracted conceptual patterns as knowledge in determining correct syntactic relations for structural disambiguation and semantic analysis in MT system for CG generation." ></td>
	<td class="line x" title="152:157	Unlike previous research that calculates statistical information at a lexical level for every pair of words, which may require a lot of space to store resulting patterns, we represent those co-occurrence patterns with concept types of Kadokawa thesaurus." ></td>
	<td class="line x" title="153:157	The problematic concept types are filtered out by the type generalization procedure." ></td>
	<td class="line x" title="154:157	We used a corpus of 6 million words for conceptual pattern extraction." ></td>
	<td class="line x" title="155:157	Our method can cope with the general scope of texts." ></td>
	<td class="line x" title="156:157	In the experiment evaluation, the proposed method showed a high accuracy rate of 90.4% in identifying the syntactic role of antecedents." ></td>
	<td class="line x" title="157:157	The method described in this paper can be used in resolving syntactic role of antecedents in relative clauses of other free word order languages, and can also be used in generating selectional restrictions of case frames of verbs." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C98-2122
Automatic Retrieval and Clustering of Similar Words
Lin, Dekang;"></td>
	<td class="line x" title="1:108	Automatic Retrieval and Clustering of Similar Words Dekang Lin Department of Computer Science University of Manitoba Winnipeg, Manitoba, Canada R3T 2N2 l indek @ cs.umanitoba.ca Abstract Bootstrapping semantics from text is one of the greatest challenges in natural language learning." ></td>
	<td class="line x" title="2:108	We first define a word similarity measure based on the distributional pattern of words." ></td>
	<td class="line x" title="3:108	The similarity measure allows us to construct a thesaurus using a parsed corpus." ></td>
	<td class="line x" title="4:108	We then present a new evaluation methodology for the automatically constructed thesaurus." ></td>
	<td class="line x" title="5:108	The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is. 1 Introduction The meaning of an unknown word can often be inferred from its context." ></td>
	<td class="line x" title="6:108	Consider the following (slightly modified)example in (Nida, 1975, p. 167): (l) A bottle of tezgiiino is on the table." ></td>
	<td class="line x" title="7:108	Everyone likes tezgiiino." ></td>
	<td class="line x" title="8:108	Tezgaino makes you drunk." ></td>
	<td class="line x" title="9:108	We make tezgiiino out of corn." ></td>
	<td class="line x" title="10:108	The contexts in which the word tezgiiino is used suggest that tezgiiino may be a kind of alcoholic beverage made from corn mash." ></td>
	<td class="line x" title="11:108	Bootstrapping semantics from text is one of the greatest challenges in natural language learning." ></td>
	<td class="line x" title="12:108	It has been argued that similarity plays an important role in word acquisition (Gentner, 1982)." ></td>
	<td class="line x" title="13:108	Identifying similar words is an initial step in learning the definition of a word." ></td>
	<td class="line x" title="14:108	This paper presents a method for making this first step." ></td>
	<td class="line x" title="15:108	For example, given a corpus that includes the sentences in (1), our goal is to be able to infer that tezgiiino is similar to 'beer', 'wine', 'vodka', etc. In addition to the long-term goal of bootstrapping semantics from text, automatic identification of similar words has many immediate applications." ></td>
	<td class="line x" title="16:108	The most obvious one is thesaurus construction." ></td>
	<td class="line x" title="17:108	An automatically created thesaurus offers many advantages over manually constructed thesauri." ></td>
	<td class="line x" title="18:108	Firstly, the terms can be corpusor genre-specific." ></td>
	<td class="line x" title="19:108	Manually constructed general-purpose dictionaries and thesauri include many usages that are very infrequent in a particular corpus or genre of documents." ></td>
	<td class="line x" title="20:108	For example, one of the 8 senses of 'company' in WordNet 1.5 is a 'visitor/visitant', which is a hyponym of 'person'." ></td>
	<td class="line x" title="21:108	This usage of the word is practically never used in newspaper articles." ></td>
	<td class="line x" title="22:108	However, its existance may prevent a co-reference recognizer to rule out the possiblity for personal pronouns to refer to 'company'." ></td>
	<td class="line x" title="23:108	Secondly, certain word usages may be particular to a period of time, which are unlikely to be captured by manually compiled lexicons." ></td>
	<td class="line x" title="24:108	For example, among 274 occurrences of the word 'westerner' in a 45 million word San Jose Mercury corpus, 55% of them refer to hostages." ></td>
	<td class="line x" title="25:108	If one needs to search hostage-related articles, 'westerner' may well be a good search term." ></td>
	<td class="line x" title="26:108	Another application of automatically extracted similar words is to help solve the problem of data sparseness in statistical natural language processing (Dagan et al., 1994; Essen and Steinbiss, 1992)." ></td>
	<td class="line x" title="27:108	When the frequency of a word does not warrant reliable maximum likelihood estimation, its probability can be computed as a weighted sum of the probabilities of words that are similar to it." ></td>
	<td class="line x" title="28:108	It was shown in (Dagan et al., 1997) that a similarity-based smoothing method achieved much better results than backoff smoothing methods in word sense disambiguation." ></td>
	<td class="line x" title="29:108	The remainder of the paper is organized as follows." ></td>
	<td class="line x" title="30:108	The next section is concerned with similarities between words based on their distributional patterns." ></td>
	<td class="line x" title="31:108	The similarity measure can then be used to create a thesaurus." ></td>
	<td class="line x" title="32:108	In Section 3, we evaluate the constructed thesauri by computing the similarity between their entries and entries in manually created thesauri." ></td>
	<td class="line x" title="33:108	Section 4 briefly discuss future work in clustering similar words." ></td>
	<td class="line x" title="34:108	Finally, Section 5 reviews related work and summarize our contributions." ></td>
	<td class="line x" title="35:108	768 2 Word Similarity Our similarity measure is based on a proposal in (Lin, 1997), where the similarity between two objects is defined to be the amount of information contained in the commonality between the objects divided by the amount of information in the descriptions of the objects." ></td>
	<td class="line x" title="36:108	We use a broad-coverage parser (Lin, 1993; Lin, 1994) to extract dependency triples from the text corpus." ></td>
	<td class="line x" title="37:108	A dependency triple consists of two words and the grammatical relationship between them in the input sentence." ></td>
	<td class="line x" title="38:108	For example, the triples extracted from the sentence 'I have a brown dog' are: (2) (have subj I), (I subj-of have), (dog obj-of have), (dog adj-mod brown), (brown adj-mod-of dog), (dog det a), (a det-of dog) We use the notation \]\[w, r, w'l\[ to denote the frequency count of the dependency triple (w, r, w ~) in the parsed corpus." ></td>
	<td class="line x" title="39:108	When w, r, or w ~ is the wild card (,), the frequency counts of all the dependency triples that matches the rest of the pattern are summed up." ></td>
	<td class="line x" title="40:108	For example, Ilcook, obj, *{I is the total occurrences of cook--object relationships in the parsed corpus, and II*, *, *11 is the total number of dependency triples extracted from the parsed corpus." ></td>
	<td class="line x" title="41:108	The description of a word w consists of the frequency counts of all the dependency triples that matches the pattern (w, ,, ,)." ></td>
	<td class="line x" title="42:108	The commonality between two words consists of the dependency triples that appear in the descriptions of both words." ></td>
	<td class="line x" title="43:108	For example, (3) is the the description of the word 'cell'." ></td>
	<td class="line x" title="44:108	(3) Ilcell, Ilcen, Ilcen, Ilcell, Ilcen, Ilcell, Ilcell, Ilcell, Ilcell, Ilcell, Ilcen, Ilcell, Ilcell, subj-of, absorbll=l subj-of, adaptll=l subj-of, behavell=l pobj-of, inl\[=159 pobj-of, inside{{ = 16 pobj-of, intol{--30 nmod-of, abnormality\[\[=3 nmod-of, anemiall=8 nmod-of, architecturell=l obj-of, attackll=6 obj-of, bludgeonl\[=l obj-of, call\[\[=l 1 obj-of, come fromll=3 Ilcell, obj-of, containll=4 Ilcell, obj-of, decoratell=2 Ilcell, nmod, bacteriall=3 Ilcell, nmod, blood vesselll=l IlceU, nmo bodyll=2 Ilcell, nmod, bone marrowll=2 \[\]cell, nmod, burialll=l tlcell, nmod, chameleonll=l Assuming that the frequency counts of the dependency triples are independent of each other, the information contained in the description of a word is the sum of the information contained in each individual frequency count." ></td>
	<td class="line x" title="45:108	To measure the information contained in the statement Ilw, r, w tll=c, we first measure the amount of information in the statement that a randomly selected dependency triple is (w, r, w ~) when we do not know the value of IIw, r,w'lt. We then measure the amount of information in the same statement when we do know the value of \[\[w, r, w ~ IIThe difference between these two amounts is taken to be the information contained in IIw, r, w' II=c, An occurrence of a dependency triple (w, r, w') can be regarded as the co-occurrence of three events: A: a randomly selected word is w; /3: a randomly selected dependency type is r; 6': a randomly selected word is w'." ></td>
	<td class="line x" title="46:108	When the value of IIw, r,w'll is unknown, we assume that A and G' are conditionally independent given B. The probability of A, B and C' cooccurring is estimated by PMLE ( /3 ) PM~.F." ></td>
	<td class="line x" title="47:108	( A I /3 ) PM,* ( C I /3 ) , where PMLE is the maximum likelihood estimation of a probability distribution and PMLE(B) = ll*,*,*ll' PMLE(AIB) = ~, When the value of \[\[w, r, w' \[I is known, we can obtain PMLE(A, .B, C) directly: I~LE(A,B,C) ----llw, r, wll/ll,,,,,lt Let I(w,r,w') denote the amount information contained in llw, r,w'll--c." ></td>
	<td class="line x" title="48:108	Its value can be com769 simHindle(Wl, W2) ---~(r,w)CT(wx)fqT(w2)Are{su~j-of.obj-of} min(I(wl, r, w), I(w2, r, w) ) simHindle~ (Wl, W2) = ~(r,w)eT(wl)nT(w~) min(I(wl, r, w), I(w2, r, w) ) simcosine(Wl, W2) = ' \[T(wl)nT(w2)\[ %/\[T( wl )\[  IT(w2 ) l simDi~e(Wl w2) : 2xlT(~)nT(w2)l ' IT(wx)l+lT(w2)l T(wl)NT(w2) simJacard(Wl, W2) : iT(wl)l+ T(w2)l_lT(wl)nT(w2)l Figure 1: Other Similarity Measures puted as follows: I(w,r,w') = _ Iog(PMLE(B)PMLE(A\[B)PMLE(CIB)) --(-log PMLE (A,/3, C)) = log IIw,r,w ll*,r,*\[ IIw,r,*llx *,r,w'll It is worth noting that I(w,r,w') is equal to the mutual information between w and w' (Hindle, 1990)." ></td>
	<td class="line x" title="49:108	Let T(w) be the set of pairs (r,w') such that \[w,r,w'llxll*,r,*ll log w,r,*llxll*,r,w'll is positive." ></td>
	<td class="line x" title="50:108	We define the similarity sim(wl, w2) between two words wl and w2 as follows: \]~_~(r,w)ET(wl)NT(w2)(I(wl, r, w) -\[I(w2, r, w) ) ~(r,w)CT(wl) I(wl , r, w) + ~~(r,w)CT(w~) I(w2, r, w) We parsed a 64-million-word corpus consisting of the Wall Street Journal (24 million words), San Jose Mercury (21 million words) and AP Newswire (19 million words)." ></td>
	<td class="line x" title="51:108	From the parsed corpus, we extracted 56.5 million dependency triples (8.7 million unique)." ></td>
	<td class="line x" title="52:108	In the parsed corpus, there are 5469 nouns, 2173 verbs, and 2632 adjectives/adverbs that occurred at least 100 times." ></td>
	<td class="line x" title="53:108	We computed the pairwise similarity between all the nouns, all the verbs and all the adjectives/adverbs, using the above similarity measure." ></td>
	<td class="line x" title="54:108	For each word, we created a thesaurus entry which contains the top-N l words that are most similar to it." ></td>
	<td class="line x" title="55:108	2 The thesaurus entry for word w has the following format: w (pos) : wl, sl, w2, s2,, wlv, 8N where pos is a part of speech, wi is a word, si=sim(w, wi) and si's are ordered in descending ~We used N=200 in our experiments 2The resulting thesaurus is available at: http://www.cs.umanitoba.caflindek/sims.htm." ></td>
	<td class="line x" title="56:108	order." ></td>
	<td class="line x" title="57:108	For example, the top-10 words in the noun, verb, and adjective entries for the word 'brief' are shown below: brief(noun): affidavit 0.13, petition 0.05, memorandum 0.05, motion 0.05, lawsuit 0.05, deposition 0.05, slight 0.05, prospectus 0.04, document 0.04 paper 0.04  brief(verb): tell 0.09, urge 0.07, ask 0.07, meet 0.06, appoint 0.06, elect 0.05, name 0.05, empower 0.05, summon 0.05, overrule 0.04  brief (adjective): lengthy 0.13, short 0.12, recent 0.09, prolonged 0.09, long 0.09, extended 0.09, daylong 0.08, scheduled 0.08, stormy 0.07, planned 0.06  Two words are a pair of respective nearest neighbors (RNNs) if each is the other's most similar word." ></td>
	<td class="line x" title="58:108	Our program found 543 pairs of RNN nouns, 212 pairs of RNN verbs and 382 pairs of RNN adjectives/adverbs in the automatically created thesaurus." ></td>
	<td class="line x" title="59:108	Appendix A lists every 10th of the RNNs." ></td>
	<td class="line x" title="60:108	The result looks very strong." ></td>
	<td class="line x" title="61:108	Few pairs of RNNs in Appendix A have clearly better alternatives." ></td>
	<td class="line x" title="62:108	We also constructed several other thesauri using the same corpus, but with the similarity measures in Figure 1." ></td>
	<td class="line x" title="63:108	The measure simHindle is the same as the similarity measure proposed in (Hindie, 1990), except that it does not use dependency triples with negative mutual information." ></td>
	<td class="line x" title="64:108	The measure simHindle r is the same as simHindle except that all types of dependency relationships are used, instead of just subject and object relationships." ></td>
	<td class="line x" title="65:108	The measures simcosine, simdice and simJacard are versions of similarity measures commonly used in information retrieval (Frakes and Baeza-Yates, 1992)." ></td>
	<td class="line x" title="66:108	Unlike sim, simaindle and simHindte~, they only 770 2 log P(c)  simwN(Wl, w2) =lIla, Xc16S(wl)Ac2eS(w2)(maXcEsuper(cl)nsuper(c2) logP(ct)+log P(c2) s 2 R(wl)CIR(w2)l simnoget(Wl, w2) = n(wl) + R(w2) where S(w) is the set of senses of w in the WordNet, super(c) is the set of (possibly indirect) superclasses of concept c in the WordNet, R(w) is the set of words that belong to a same Roget category as w. Figure 2: Word similarity measures based on WordNet and Roget make use of the unique dependency triples and ignore their frequency counts." ></td>
	<td class="line x" title="67:108	3 Evaluation In this section, we present an evaluation of automatically constructed thesauri with two manually compiled thesauri, namely, WordNetl.5 (Miller et al., 1990) and Roget Thesaurus." ></td>
	<td class="line x" title="68:108	We first define two word similarity measures that are based on the structures of WordNet and Roget (Figure 2)." ></td>
	<td class="line x" title="69:108	The similarity measure simwN is based on the proposal in (Lin, 1997)." ></td>
	<td class="line x" title="70:108	The similarity measure simnoaet treats all the words in Roget as features." ></td>
	<td class="line x" title="71:108	A word w possesses the feature f if f and w belong to a same Roget category." ></td>
	<td class="line x" title="72:108	The similarity between two words is then defined as the cosine coefficient of the two feature vectors." ></td>
	<td class="line x" title="73:108	With simwu and simnoget, we transform WordNet and Roget into the same format as the automatically constructed thesauri in the previous section." ></td>
	<td class="line x" title="74:108	We now discuss how to measure the similarity between two thesaurus entries." ></td>
	<td class="line x" title="75:108	Suppose two thesaurus entries for the same word are as follows: w : WI,SI,W2, S2,,WN,SN # # # # W: Wl, S1, 71)2, S2, , W/N, JN Their similarity is defined as: (4) Z N /, 2 N i=l For example, (5) is the entry for 'brief (noun)' in our automatically generated thesaurus and (6) and (7) are corresponding entries in WordNet thesaurus and Roget thesaurus." ></td>
	<td class="line x" title="76:108	(5) brief (noun): affidavit 0.13, petition 0.05, memorandum 0.05, motion 0.05, lawsuit 0.05, deposition 0.05, slight 0.05, prospectus 0.04, document 0.04 paper 0.04." ></td>
	<td class="line x" title="77:108	(6) brief (noun): outline 0.96, instrument 0.84, summary 0.84, affidavit 0.80, deposition 0.80, law 0.77, survey 0.74, sketch 0.74, resume 0.74, argument 0.74." ></td>
	<td class="line x" title="78:108	(7) brief (noun): recital 0.77, saga 0.77, autobiography 0.77, anecdote 0.77, novel 0.77, novelist 0.77, tradition 0.70, historian 0.70, tale 0.64." ></td>
	<td class="line x" title="79:108	According to (4), the similarity between (5) and (6) is 0.297, whereas the similarities between (5) and (7) and between (6) and (7) are 0." ></td>
	<td class="line x" title="80:108	Our evaluation was conducted with 4294 nouns that occurred at least 100 times in the parsed corpus and are found in both WordNetl.5 and the Roget Thesaurus." ></td>
	<td class="line x" title="81:108	Table 1 shows the average similarity between corresponding entries in different thesauri and the standard deviation of the average, which is the standard deviation of the data items divided by the square root of the number of data items." ></td>
	<td class="line x" title="82:108	Since the differences among simcosine, simdice and simy,card are very small, we only included the results for simcosine in Table 1 for the sake of brevity." ></td>
	<td class="line x" title="83:108	It can be seen that sim, Hindler and cosine are significantly more similar to WordNet than Roget is, but are significantly less similar to Roget than WordNet is. The differences between Hindle and Hindler clearly demonstrate that the use of other types of dependencies in addition to subject and object relationships is very beneficial." ></td>
	<td class="line x" title="84:108	The performance of sim, Hindler and cosine are quite close." ></td>
	<td class="line x" title="85:108	To determine whether or not the differences are statistically significant, we computed their differences in similarities to WordNet and Roget thesaurus for each individual entry." ></td>
	<td class="line x" title="86:108	Table 2 shows the average and standard deviation of the average difference." ></td>
	<td class="line x" title="87:108	Since the 95% confidence inter771 Table  Evaluation with WordNet and Roget Roget sim Hindle~ cosine Hindle WordNet average aava 0.178397 0.001636 0.212199 0.001484 0.204179 0.001424 0.199402 0.001352 0.164716 0.001200 Roget average WordNet 0.178397 sim 0.149045 Hindle~ 0.14663 cosine 0.135697 Hindle 0.115489 ~av~ 0.001636 0.001429 0.001383 0.001275 0.001140 vals of all the differences in Table 2 are on the positive side, one can draw the statistical conclusion that simis better than simHindler, which is better than simeosine." ></td>
	<td class="line x" title="88:108	Table 2: Distribution of Differences sim-Hindler sim-cosine Hindler-cosine WordNet average aavg 0.008021 0.000428 0.012798 0.000386 0.004777 0.000561 Roget average aavg sim-Hindler 0.002415 0.000401 sim-cosine 0.013349 0.000375 Hindle~-cosine 0.010933 0.000509 4 Future Work Reliable extraction of similar words from text corpus opens up many possibilities for future work." ></td>
	<td class="line x" title="89:108	For example, one can go a step further by constructing a tree structure among the most similar words so that different senses of a given word can be identified with different subtrees." ></td>
	<td class="line x" title="90:108	Let wl,, Wn be a list of words in descending order of their similarity to a given word w. The similarity tree for w is created as follows:  Initialize the similarity tree to consist of a single node w.  For i=l, 2  n, insert wi as a child of wj such that wj is the most similar one to wi among {w, Wl  wi-1}." ></td>
	<td class="line x" title="91:108	For example, Figure 3 shows the similarity tree for the top-40 most similar words to duty." ></td>
	<td class="line x" title="92:108	The first number behind a word is the similarity of the word to its parent." ></td>
	<td class="line x" title="93:108	The second number is the similarity of the word to the root node of the tree." ></td>
	<td class="line x" title="94:108	duty responsibility 0.21 0.21 role 0.12 0.ii \]__action 0.ii 0.i0 change 0.24 0.08 \]rule 0.16 0.08 Irestriction 0.27 0.08 \] I ban 0.30 0.08 \[ Isanction 0.19 0.08 \]schedule 0.ii 0.07 I regulation 0.37 0.07 challenge 0.13 0.07 l issue 0.13 0.07 Ireason 0.14 0.07 \]matter 0.28 0.07 measure 0.22 0.07 ~ obligation 0.12 0.I0 ~ower 0.17 0.08 l__jurisdiction 0.13 0.08 fright 0.12 0.07 \] control 0.20 0.07 l__ground 0.08 0.07 accountability 0.14 0.08 experience 0.12 0.07 )ost 0.14 0.14 __job 0.17 0.i0 I work 0.17 0." ></td>
	<td class="line x" title="95:108	i0 Itraining 0.11 0.07 ____position 0.25 0.10 task 0.10 0.I0 \[ chore 0.ii 0.07 operation 0.10 0.10 I function 0.i0 0.08 I mission 0.12 0.07 I \[~atrol 0.07 0.07 I staff 0.i0 0.07 __.__penalty 0.09 0.09 I fee 0.17 0.08 \[__tariff 0.13 0.08 \] tax 0.19 0.07 reservist 0.07 0.07 Figure 3: Similarity tree for 'duty' Inspection of sample outputs shows that this algorithm works well." ></td>
	<td class="line x" title="96:108	However, formal evaluation of its accuracy remains to be future work." ></td>
	<td class="line x" title="97:108	5 Related Work and Conclusion There have been many approaches to automatic detection of similar words from text corpora." ></td>
	<td class="line x" title="98:108	Ours is 772 similar to (Grefenstette, 1994; Hindle, 1990; Ruge, 1992) in the use of dependency relationship as the word features, based on which word similarities are computed." ></td>
	<td class="line x" title="99:108	Evaluation of automatically generated lexical resources is a difficult problem." ></td>
	<td class="line x" title="100:108	In (Hindle, 1990), a small set of sample results are presented." ></td>
	<td class="line oc" title="101:108	In (Smadja, 1993), automatically extracted collocations are judged by a lexicographer." ></td>
	<td class="line x" title="102:108	In (Dagan et al., 1993) and (Pereira et al., 1993), clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time." ></td>
	<td class="line x" title="103:108	In (Alshawi and Carter, 1994), the collocations and their associated scores were evaluated indirectly by their use in parse tree selection." ></td>
	<td class="line x" title="104:108	The merits of different measures for association strength are judged by the differences they make in the precision and the recall of the parser outputs." ></td>
	<td class="line x" title="105:108	The main contribution of this paper is a new evaluation methodology for automatically constructed thesaurus." ></td>
	<td class="line x" title="106:108	While previous methods rely on indirect tasks or subjective judgments, our method allows direct and objective comparison between automatically and manually constructed thesauri." ></td>
	<td class="line x" title="107:108	The results show that our automatically created thesaurus is significantly closer to WordNet than Roger Thesaurus is. Our experiments also surpasses previous experiments on automatic thesaurus construction in scale and (possibly) accuracy." ></td>
	<td class="line x" title="108:108	Acknowledgement This research has also been partially supported by NSERC Research Grant OGP121338 and by the Institute for Robotics and Intelligent Systems." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C98-2211
The Computational Lexical Semantics of Syntagmatic Relations
Viegas, Evelyne;Beale, Stephen;Nirenburg, Sergei;"></td>
	<td class="line x" title="1:135	The Computational Lexical Semantics of Syntagmatic Relations Evelyne Viegas, Stephen Beale and Sergei Nirenburg New Mexico State University Computing Research Lab, Las Cruces, NM 88003, USA viegas, sb, sergei@crl, nmsu." ></td>
	<td class="line x" title="2:135	edu Abstract In this paper, we address the issue of syntagmatic expressions from a computational lexical semantic perspective." ></td>
	<td class="line x" title="3:135	From a representational viewpoint, we argue for a hybrid approach combining linguistic and conceptual paradigms, in order to account for the continuum we find in natural languages from free combining words to frozen expressions." ></td>
	<td class="line x" title="4:135	In particular, we focus on the place of lexical and semantic restricted co-occurrences." ></td>
	<td class="line x" title="5:135	From a t)rocessing viewpoint, we show how to generate/analyze syntagmatic expressions by using an efficient constraintbased processor, well fitted for a knowledge-driven approach." ></td>
	<td class="line x" title="6:135	1 Introduction You can take advantage of the chambermaid I is not a collocation one would like to generate in the context of a hotel to mean 'use the services of.'" ></td>
	<td class="line x" title="7:135	This is why collocations should constitute an important part in the design of Machine Translation or Multilingual Generation systems." ></td>
	<td class="line x" title="8:135	In this paper, we address the issue of syntagmatic expressions from a computational lexical semantic perspective." ></td>
	<td class="line x" title="9:135	From a representational viewpoint, we argue for a hybrid approach combining linguistic and conceptual paradigms, in order to account for the continuum we find in natural languages from free combining words to frozen expressions (such as in idioms kick the (proverbial) bucket)." ></td>
	<td class="line x" title="10:135	In particular, we focus on the representation of restricted semantic and lexical co-occurrences, such as heavy smoker and professor  students respectively, that we define later." ></td>
	<td class="line x" title="11:135	From a processing viewpoint, we show how to generate/analyze syntagmatic expressions by using an efficient constraint-based processor, well fitted for a knowledge-driven approach." ></td>
	<td class="line x" title="12:135	In the following, we first compare different approaches to collocations." ></td>
	<td class="line x" title="13:135	Second, we present our approach in terms of representation and processing." ></td>
	<td class="line x" title="14:135	Finally, we show how to facilitate the acquisition of co-occurrences by using 1) the formalisnr of lexical rules (LRs), 2) an 1Lederer, R. 1990." ></td>
	<td class="line x" title="15:135	Anguished English A Laurel Book, Dell Publishing." ></td>
	<td class="line x" title="16:135	inheritance hierarchy of Lexical Semantic Functions (LSFs)." ></td>
	<td class="line x" title="17:135	2 Approaches to Syntagmatic Relations Syntagmatic relations, also known as collocations, are used differently by lexicographers, linguists and statisticians denoting almost similar lint not identical classes of expressions." ></td>
	<td class="line x" title="18:135	The traditional approach to collocations has been lexicographic." ></td>
	<td class="line x" title="19:135	Here dictionaries provide information about what is unpredictable or idiosym cratic." ></td>
	<td class="line x" title="20:135	Benson (1989) synthesizes Hausmann's studies on collocations, calling expressions such as commit murder, compile a dictionary, inflict a wound, etc. 'fixed combinations, recurrent combinations' or 'collocations'." ></td>
	<td class="line x" title="21:135	In Hausmann's terms (1979) a collocation is composed of two elements, a base ('Basis') and a collocate ('Kollokator'); the base is semantically autonomous whereas the collocate cannot be semantically interpreted in isolation." ></td>
	<td class="line x" title="22:135	In other words, the set of lexical collocates which can con> bine with a given basis is not predictable and therefore collocations must be listed in dictionaries." ></td>
	<td class="line x" title="23:135	It is hard to say that there has been a real focus on collocations from a linguistic perspective." ></td>
	<td class="line x" title="24:135	The lexicon has been broadly sacrificed by both Englishspeaking schools and continental European schools." ></td>
	<td class="line x" title="25:135	The scientific agenda of the former has been largely dominated by syntactic issues until recently, whereas the latter was more concerned with pragmatic aspects of natural languages." ></td>
	<td class="line x" title="26:135	The focus has been on grammatical collocations such as adapt to, aim at, look for." ></td>
	<td class="line x" title="27:135	Lakoff (1970) distinguishes a class of expressions which cannot undergo certain operations, such as nominalization, causativization: the prvblem is hard; *the hardness of the problem; *the problem hardened." ></td>
	<td class="line x" title="28:135	The restriction on the application of certain syntactic operations can help define collocations such as hard problem, for example." ></td>
	<td class="line x" title="29:135	Mel'~uk's treatment of collocations will be detailed below." ></td>
	<td class="line x" title="30:135	In recent years, there has been a resurgence of statistical approaches applied to the study of natural languages." ></td>
	<td class="line x" title="31:135	Sinclair (1.991) states that % word 1328 which occurs in close proximity to a word under investigation is called a collocate of it  Collocation is the occurrence of two or more words within a short space of each other in a text'." ></td>
	<td class="line x" title="32:135	The problem is that with such a definition of collocations, even when improved, 2 one identifies not only collocations lint free-combining pairs frequently appearing together such as lawyer-client; doctor-hospital." ></td>
	<td class="line x" title="33:135	However, nowadays, researchers seem to agree that combining statistic with symbolic approaches lead to quantifiable improvements (Klavans and Resnik, 1996)." ></td>
	<td class="line x" title="34:135	The Meaning Text Theory Approach The Meaning Text Theory (MTT) is a generator-oriented lexical grammatical formalism." ></td>
	<td class="line x" title="35:135	Lexical knowledge is encoded in an entry of the Explanatory Combinatorial Dictionary (ECD), each entry being divided into three zones: the semantic zone (a semantic network representing the meaning of the entry in terms of more primitive words), the syntactic zone (the grammatical properties of the entry) and the lexical combinatorics zone (containing the values of the Lexical Functions (LFs) 3)." ></td>
	<td class="line x" title="36:135	LFs are central to the study of collocations: A le~:ical function F is a correspondence which associates a lc~:ical item L, called the key word of F, with a set of lexical items F(L)-thc value of F." ></td>
	<td class="line x" title="37:135	(Mel', uk, 1988) We focus here on syntagmatic LFs describing cooccurrence relations such as pay attention, legitimate complaint; from a distance." ></td>
	<td class="line x" title="38:135	5 Heylen et al.(1993) have worked out some cases which help license a starting point for assigning LFs." ></td>
	<td class="line x" title="40:135	They distinguish four types of syntagmatic LFs:  evaluative qualifier Magn(bleed) = profusely  distributional qualifier Mull(sheep) = flock  co-occ'arrcncc Loc-in((listance)= at a distance  verbal opc~ntor Operl(attention) = pay The MTT approach is very interesting as it provides a model of production well suited for generation with its different strata and also a lot of lexiealsemantic information." ></td>
	<td class="line oc" title="41:135	It seems nevertheless that all 2Church and Ilanks (1989), Smadja (1993) use statistics in their algorithms to extract collocations from texts." ></td>
	<td class="line x" title="42:135	3See (Iordanskaja et al., 1991) and (Ramos et al., 1994) for their use of LFs in MTT and NLG respectively." ></td>
	<td class="line x" title="43:135	4(Ileid, 1989) contrasts tlausman's base and collate to Mel'Olk's keyword and LF values." ></td>
	<td class="line x" title="44:135	5There are about 60 LFs listed said to be universal; the lexicographic approach of Mel'~uk and Z, olkovsky has been applied among other languages to Russian, French, Qerman aml English, the collocational information is listed in a static way." ></td>
	<td class="line x" title="45:135	We believe that one of the main drawbacks of the approach is the lack of any predictable calculi on the possible expressions which can collocate with each other semantically." ></td>
	<td class="line x" title="46:135	3 The Computational Lexical Semantic Approach In order to account for the continuum we find in natural languages, we argue for a continuum perspective, spanning the range fl'oln free-combining words to idioms, with semantic collocations and idiosyncrasies in between as defined in (Viegas and Bouilhm, 1994):  free-combining words (the girl ate candies)  semantic collocations (fast car; long book) 6  idiosyncrasies (large coke; green jealousy)  idioms (to kick the (proverbial) bucket) Formally, we go from a purely compositional approach in 'free-combining words' to a noncompositional approach in idioms, In between, a (semi-)compositional approach is still possible." ></td>
	<td class="line x" title="47:135	(Viegas and Bouillon, 1994) showed that we can reduce the set of what are conventionally considered as idiosyncrasies by difl>rentiating 'true' idiosyncrasies (difficult to derive or calculate) from expressions which have well-defined calculi, being compositional in nature, and that have been called semantic collocations." ></td>
	<td class="line x" title="48:135	In this paper, we further distinguish their idiosyncrasies into:  restricted semantic co-occurrence, where the meaning of the co-occurrence is semicompositional betwee, n the base and the collocate (strong coffee, pay attention, heavy smoker, )  restricted lexical co-occurrence, where the meaning of the collocate is compositional but has a lexieal idiosyncratic behavior (lecture  student; rancid butter; sour milk)." ></td>
	<td class="line x" title="49:135	We provide below examples of restricted semantic co-occurrences in (1), and restricted lexical cooccurrences in (2)." ></td>
	<td class="line x" title="50:135	Restricted semantic co-occurrence The semantics of the combination of the entries is semicompositional." ></td>
	<td class="line x" title="51:135	In other words, there is an entry in the lexicon for the base, (the semantic collocate is encoded inside the base), whereas we cannot directly refer to the sense of the semantic collocate in the lexicon, as it is not part of its senses." ></td>
	<td class="line x" title="52:135	We assign the (:o-occurrence a new semi-compositional sense, 6See (l'uste.jovsky, 1995) for his account of such expressions using a coercion olmrator." ></td>
	<td class="line x" title="53:135	1329 where the sense of the base is composed with a new sense for the collocate." ></td>
	<td class="line x" title="54:135	(la) #O=\[key: rel: (lb) #0=\[key: rel: 'smoker', \[syntagmatic: LSFIntensity \[base: SO, collocate: \[key: 'heavy', gram: \[subCat: Attributive, freq: \[value: 8\]\]\]\]\] 'attention', \[syntagmatic: LSFOper \[base: #0, collocate: \[key: 'pay', gram: \[subCat: SupportVerb, freq: \[value: 5\]\]\]\]\] \] \] In examples (1), the LSFs (LSFIntensity, LSFOper, ) are equivalent (and some identical) to the LFs provided in the ECD." ></td>
	<td class="line x" title="55:135	The notion of LSF is the same as that of LFs." ></td>
	<td class="line x" title="56:135	However, LSFs and LFs are different in two ways: i) conceptually, LSFs are organized into an inheritance hierarchy; ii) formally, they are rules, and produce a new entry composed of two entries, the base with the collocate." ></td>
	<td class="line x" title="57:135	As such, the new composed entry is ready for processing." ></td>
	<td class="line x" title="58:135	These LSFs signal a compositional syntax and a semi-compositional semantics." ></td>
	<td class="line x" title="59:135	For instance, in (la), a heavy smoker is somebody who smokes a lot, and not a 'fat' person." ></td>
	<td class="line x" title="60:135	It has been shown that one cannot code in the lexicon all uses of heavy for heavy smoker, heavy drinker,  Therefore, we do not have in our lexicon for heavy a sense for 'a lot', or a sense for 'strong' to be composed with wine, etc It is well known that such co-occurrences are lexically marked; if we allowed in our lexicons a proliferation of senses, multiplying ambiguities in analysis and choices in generation, then there would be no limit to what could be combined and we could end up generating *heavy coffee with the sense of 'strong' for heavy, in our lexicon." ></td>
	<td class="line x" title="61:135	The left hand-side of the rule LSFIntensity specifies an 'Intensity-Attribute' applied to an event which accepts aspectual features of duration." ></td>
	<td class="line x" title="62:135	In (la), the event is smoke." ></td>
	<td class="line x" title="63:135	The LSFIntensity also provides the syntax-semantic interface, allowing for an Adj-Noun construction to be either predicative (the car is red) or attributive (the red car)." ></td>
	<td class="line x" title="64:135	We need therefore to restrict the co-occurrence to the Attributive use only, as the predicative use is not allowed: (the smoker is heavy) has a literal meaning or figurative, but not collocationah In (lb) again, there is no sense in the dictionary for pay which would mean concentrate." ></td>
	<td class="line x" title="65:135	The rule LSFOper makes the verb a verbal operator." ></td>
	<td class="line x" title="66:135	No further restriction is required." ></td>
	<td class="line x" title="67:135	Restricted lexical co-occurrence The semantics of the combination of the entries is compositional." ></td>
	<td class="line x" title="68:135	In other words, there are entries in the lexicon for the base and the collocate, with the same senses as in the co-occurrence." ></td>
	<td class="line x" title="69:135	Therefore, we can directly refer to the senses of the co-occurring words." ></td>
	<td class="line x" title="70:135	What we are capturing here is a lexical idiosyncrasy or in other words, we specify that we should prefer this particular combination of words." ></td>
	<td class="line x" title="71:135	This is useful for analysis, where it can help disambiguate a sense, and is most relevant for generation; it can be viewed as a preference among the paradigmatic family of the co-occurrence." ></td>
	<td class="line x" title="72:135	(2a) #O=\[key: rel: 'truth', \[syntagmatic: LSFSyn \[base: SO, collocate: \[key: 'plain', sense: adj2, ir: \[comp:no, superl:no\]\]\]\] \] (2b) #0=\[key: rel: 'pupil', \[syntagmatic: LSFSyn \[base: #0, collocate: \[key: 'teacher', sense: n2, freq: \[value: 5\]\]\]\]\] (2c) #0=\[key: rel: 'conference', \[syntagmatic: LSFSyn \[base: #0, collocate: \[key: 'student', sense: nl, freq: \[value: 9\]\]\]\] \] In examples (2), the LSFSyn produces a new entry composed of two or more entries." ></td>
	<td class="line x" title="73:135	As such, tile new entry is ready for processing." ></td>
	<td class="line x" title="74:135	LSFSyn signals a compositional syntax and a compositional semantics, and restricts the use of lexemes to be used in the composition." ></td>
	<td class="line x" title="75:135	We can directly refer to the sense of the collocate, as it is part of the lexicon." ></td>
	<td class="line x" title="76:135	In (2a) the entry for truth specifies one cooccurrence (plain truth), where tile sense of plato here is adj2 (obvious), and not say adj3 (flat)." ></td>
	<td class="line x" title="77:135	The syntagmatic expression inherits all the zones of the entry for 'plain', sense adj2, we only code here the irregularities." ></td>
	<td class="line x" title="78:135	For instance, 'plain' can be used as 'plainer  plainest' in its 'plain' sense in its adj2 entry, but not as such within the lexical cooccurrence '*plainer truth', '*plainest truth', we therefore must block it in the collocate, as expressed in (comp: no, superh no)." ></td>
	<td class="line x" title="79:135	In other words, we will not generate 'plainer/plainest truth'." ></td>
	<td class="line x" title="80:135	Examples (2b) and (2c) illustrate complex entries as there is no direct grammatical dependency between the base and the collocate." ></td>
	<td class="line x" title="81:135	In (2b) for instance, we prefer to associate teacher in the context of a pupil rather than any other element belonging to the pm'adigmatic family of teacher such as professor, instructor." ></td>
	<td class="line x" title="82:135	Formally, there is no difference between the two types of co-occurrences." ></td>
	<td class="line x" title="83:135	In both cases, we specify the base (which is the word described in tile en1330 try itself), tile collocate, the frequency of the cooccurrence in some corpus, and the LSF which links the base with the collocate." ></td>
	<td class="line x" title="84:135	Using the formalism of typed feature structures, both cases are of type Co-occurrence as defined below: Co-occurrence = \[base : Entry, collocate: Entry, freq: Frequency\]; 3.1 Processing of Syntagmatic Relations We utilize an efficient constraint-based control mechanism called Hunter-Gatherer (HG) (Beale, 1997)." ></td>
	<td class="line x" title="85:135	HG allows us to mark certain compositions as t)eing dependent on each other and then forget about them." ></td>
	<td class="line x" title="86:135	Thus, once we have two lexicon entries that we know go together, tIG will ensure that they do." ></td>
	<td class="line x" title="87:135	ttG also gives preference to co-occurring compositions, in analysis, meaning representations constructed using co-occurrences are preferred over those that are not, and, in generation, realizations involving co-occurrences are preferred over equally correct, t)ut non-cooecurring realizations." ></td>
	<td class="line x" title="88:135	7 The real work in processing is making sure that we have the correct two entries to put together." ></td>
	<td class="line x" title="89:135	In restriated semantic co-occurrences, the co-occurrence does not leave the correct sense in the lexicon." ></td>
	<td class="line x" title="90:135	For exmnple, when the phrase heavy smoker is encountered, the lexicon entry foE' heavy would not contain the correct sense." ></td>
	<td class="line x" title="91:135	(la) could be used to create the correct entry." ></td>
	<td class="line x" title="92:135	In (13), the entry for smoker aont, ains the key, or trigger, heavy." ></td>
	<td class="line x" title="93:135	This signals tile analyzer to t)roduee another sense for heavy smoker." ></td>
	<td class="line x" title="94:135	This sense will contain the same syntactic information present in the, 'old' heavy, except for any modifications listed in the 'gram' section (see (la))." ></td>
	<td class="line x" title="95:135	The semantics of the new sense comes directly fi'om the LSF." ></td>
	<td class="line x" title="96:135	Generation works the same, except the trigger is different." ></td>
	<td class="line x" title="97:135	The input to generation will be a SMOKE event along with an Intensity-Attribute." ></td>
	<td class="line x" title="98:135	(la), which would be used to realize the SMOKE event, would trigger LSFIntensify which has the Intensity-Attrihute in the left hand-side, thus confirming the production of heavy." ></td>
	<td class="line x" title="99:135	Restricted lexical co-occurrences are easier in the sense that the correct entry already exists in the lexicon." ></td>
	<td class="line x" title="100:135	The analyzer/generator simply needs to detect the co-occurrence and add the constraint that the corresponding senses be used together." ></td>
	<td class="line x" title="101:135	In examples like (21)), there is no direct grammatical or semantic relationship between the words that co-occur." ></td>
	<td class="line x" title="102:135	Thus, the entire clause, sentence or even text may have to be searched for the co-occurrence." ></td>
	<td class="line x" title="103:135	In practice, we limit such searches to the sentence level." ></td>
	<td class="line x" title="104:135	7The selection of co-occurrences is part of the lexical process, in other words, if there are reasons not to choose a cooccurrence because of the presence of modifiers or because of stylistics reasons, the generator will not generate the cooccurrence." ></td>
	<td class="line x" title="105:135	3.2 Acquisition of Syntagmatic Relations The acquisition of syntagmatic relations is knowledge intensive as it requires human intervention." ></td>
	<td class="line x" title="106:135	In order to minimize this cost we rely on conceptual tools such as lexical rules, on the LSF inheritance hierarchy." ></td>
	<td class="line x" title="107:135	Lexieal Rules in Acquisition The acquisition of restricted semantic co-occurrences can be minimized by detecting rules between different classes of cooccurrences (modulo presence of derived forms in the lexicon with same or subsumed semantics)." ></td>
	<td class="line x" title="108:135	Looking at the following example, A + N <=> V + Adv bitter resentment resent bitterly heavy smoker smoke heavily big eater eat *bigly V + Adv <=> Adv + Adj-ed oppose strongly strongly opposed oblige morally morally obliged we see that after having acquired with human intervention co-occurrences belonging to tile A + N class, we can use lexical rules to deriw; the V + Adv class and also Adv + Adj-ed class." ></td>
	<td class="line x" title="109:135	Lexical rules are a useful conceptual tool to extend a dictionary." ></td>
	<td class="line x" title="110:135	(Viegas et al., 1996) used derivational lexical rules to extend a Spanish lexicon." ></td>
	<td class="line x" title="111:135	We apply their approach to the production of restricted semantic co-occurrences." ></td>
	<td class="line x" title="112:135	Note that eat bi91y will be produced but then rejected, as the form bigly does not exist in a dictionary, The rules overgenerate cooccurrences." ></td>
	<td class="line x" title="113:135	This is a minor problem for analysis than for generation." ></td>
	<td class="line x" title="114:135	To use these derived restricted co-occurrences in generation, the output of the lexical rule processor nmst be checked." ></td>
	<td class="line x" title="115:135	This can be (ton(; in different ways: dictionary check, corpus check and ultimately human check." ></td>
	<td class="line oc" title="116:135	Other classes, such as the ones below (:ate be extracted using lexico-statistical tools, such as in (Smadja, 1993), and then checked by a human." ></td>
	<td class="line x" title="117:135	V+N pay attention, meet an obligation, commit an offence,  N+N dance marathon, marriage ceremony object of derision  LSFs and Inheritance We take advantage of 1) tile semantics encoded in the lexemes, and 2) an inheritance hierarchy of LSFs." ></td>
	<td class="line x" title="118:135	We illustrate brietly this notion of LSF inheritance hierarchy." ></td>
	<td class="line x" title="119:135	For instance, the left hand-side of LSFChangeState specifies that it applies to foods (solid or liquid) which are human processed, and t)roduces the collocates rancid, rancio (Spanish)." ></td>
	<td class="line x" title="120:135	Therefore it, could apply to milk, butter, or wine." ></td>
	<td class="line x" title="121:135	The rule would end up 1331 producing rancid milk, rancid butter, or vino rancio (rancid wine) which is fine in Spanish." ></td>
	<td class="line x" title="122:135	We therefore need to further distinguish LSFChangeState into LSFChangeStateSolid and LSFChangeStateLiquid." ></td>
	<td class="line x" title="123:135	This restricts the application of the rule to produce rancid butter, by going down the hierarchy." ></td>
	<td class="line x" title="124:135	This enables us to factor out information common to several entries, and can be applied to both types of co-occurrences." ></td>
	<td class="line x" title="125:135	We only have to code in the cooccurrence information relevant to the combination, the rest is inherited from its entry in the dictionary." ></td>
	<td class="line x" title="126:135	4 Conclusion In this paper, we built on a continumn perspective, knowledge-based, spanning the range from freecombining words to idioms." ></td>
	<td class="line x" title="127:135	We further distinguished the notion of idiosyncrasies as defined in (Viegas and Bouillon, 1994), into restricted semantic co.occurrenccs and restricted lexical co-occurrences." ></td>
	<td class="line x" title="128:135	We showed that they were formally equivalent, thus facilitating the processing of strictly compositional and semi-compositional expressions." ></td>
	<td class="line x" title="129:135	Moreover, by considering the information in the lexicon as constraints, the linguistic difference between compositionality and semi-eompositionality becomes a virtual difference for Hunter-Gatherer." ></td>
	<td class="line x" title="130:135	We showed ways of minimizing the acquisition costs, by 1) using lexical rules as a way of expanding co-occurrences, 2) taking advantage of the LSF inheritance hierarchy." ></td>
	<td class="line x" title="131:135	The main advantage of our approach over the ECD approach is to use the semantics coded in the lexemes along with the language independent LSF inheritance hierarchy to propagate restricted semantic co-occurrences." ></td>
	<td class="line x" title="132:135	The work presented here is complete concerning representational aspects and processing aspects (analysis and generation): it has been tested on the translations of on-line unrestricted texts." ></td>
	<td class="line x" title="133:135	The large-scale acquisition of restricted co-occurrences is in progress." ></td>
	<td class="line x" title="134:135	5 Acknowledgements This work has been supported in part by DoD under contract number MDA-904-92-C-5189." ></td>
	<td class="line x" title="135:135	We would like to thank Pierrette Bouillon, Ldo Wanner and R(~mi Zajac for helpflfl discussions and the anonymous reviewers for their useful comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="J98-2002
Generalizing Case Frames Using A Thesaurus And The MDL Principle
Li, Hang;Abe, Naoki;"></td>
	<td class="line x" title="1:393	Generalizing Case Frames Using a Thesaurus and the MDL Principle Hang Li* NEC Corporation Naoki Abe* NEC Corporation A new method for automatically acquiring case frame patterns from large corpora is proposed." ></td>
	<td class="line x" title="2:393	In particular, the problem of generalizing values of a case frame slot for a verb is viewed as that of estimating a conditional probability distribution over a partition of words, and a new generalization method based on the Minimum Description Length (MDL) principle is proposed." ></td>
	<td class="line x" title="3:393	In order to assist with efficiency, the proposed method makes use of an existing thesaurus and restricts its attention to those partitions that are present as 'cuts' in the thesaurus tree, thus reducing the generalization problem to that of estimating a 'tree cut model' of the thesaurus tree." ></td>
	<td class="line x" title="4:393	An efficient algorithm is given, which provably obtains the optimal tree cut model for the given frequency data of a case slot, in the sense of MDL." ></td>
	<td class="line x" title="5:393	Case frame patterns obtained by the method were used to resolve PP-attachment ambiguity." ></td>
	<td class="line x" title="6:393	Experimental results indicate that the proposed method improves upon or is at least comparable with existing methods." ></td>
	<td class="line x" title="7:393	1." ></td>
	<td class="line x" title="8:393	Introduction We address the problem of automatically acquiring case frame patterns (selectional patterns, subcategorization patterns) from large corpora." ></td>
	<td class="line x" title="9:393	A satisfactory solution to this problem would have a great impact on various tasks in natural language processing, including the structural disambiguation problem in parsing." ></td>
	<td class="line x" title="10:393	The acquired knowledge would also be helpful for building a lexicon, as it would provide lexicographers with word usage descriptions." ></td>
	<td class="line x" title="11:393	In our view, the problem of acquiring case frame patterns involves the following two issues: (a) acquiring patterns of individual case frame slots; and (b) learning dependencies that may exist between different slots." ></td>
	<td class="line x" title="12:393	In this paper, we confine ourselves to the former issue, and refer the interested reader to Li and Abe (1996), which deals with the latter issue." ></td>
	<td class="line x" title="13:393	The case frame (case slot) pattern acquisition process consists of two phases: extraction of case frame instances from corpus data, and generalization of those instances to case frame patterns." ></td>
	<td class="line x" title="14:393	The generalization step is needed in order to represent the input case frame instances more compactly as well as to judge the (degree of) acceptability of unseen case frame instances." ></td>
	<td class="line pc" title="15:393	For the extraction problem, there have been various methods proposed to date, which are quite adequate (Hindle and Rooth 1991; Grishman and Sterling 1992; Manning 1992; Utsuro, Matsumoto, and Nagao 1992; Brent 1993; Smadja 1993; Grefenstette 1994; Briscoe and Carroll 1997)." ></td>
	<td class="line x" title="16:393	The generalization problem, in contrast, is a more challenging one and has not been solved completely." ></td>
	<td class="line x" title="17:393	A number of methods for generalizing values of a case frame slot for a verb have been * C&C Media Res." ></td>
	<td class="line x" title="18:393	Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan." ></td>
	<td class="line x" title="19:393	E-mail: { |ihang,abe }@ccm.cl.nec.co.jp @ 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 2 proposed." ></td>
	<td class="line x" title="20:393	Some of these methods make use of prior knowledge in the form of an existing thesaurus (Resnik 1993a, 1993b; Framis 1994; Almuallim et al. 1994; Tanaka 1996; Utsuro and Matsumoto 1997), while others do not rely on any prior knowledge (Pereira, Tishby, and Lee 1993; Grishman and Sterling 1994; Tanaka 1994)." ></td>
	<td class="line x" title="21:393	In this paper, we propose a new generalization method, belonging to the first of these two categories, which is both theoretically well-motivated and computationally efficient." ></td>
	<td class="line x" title="22:393	Specifically, we formalize the problem of generalizing values of a case frame slot for a given verb as that of estimating a conditional probability distribution over a partition of words, and propose a new generalization method based on the Minimum Description Length principle (MDL): a principle of data compression and statistical estimation from information theory." ></td>
	<td class="line x" title="23:393	1 In order to assist with efficiency, our method makes use of an existing thesaurus and restricts its attention on those partitions that are present as 'cuts' in the thesaurus tree, thus reducing the generalization problem to that of estimating a 'tree cut model' of the thesaurus tree." ></td>
	<td class="line x" title="24:393	We then give an efficient algorithm that provably obtains the optimal tree cut model for the given frequency data of a case slot, in the sense of MDL." ></td>
	<td class="line x" title="25:393	In order to test the effectiveness of our method, we conducted PP-attachment disambiguation experiments using the case frame patterns obtained by our method." ></td>
	<td class="line x" title="26:393	Our experimental results indicate that the proposed method improves upon or is at least comparable to existing methods." ></td>
	<td class="line x" title="27:393	The remainder of this paper is organized as follows: In Section 2, we formalize the problem of generalizing values of a case frame slot as that of estimating a conditional distribution." ></td>
	<td class="line x" title="28:393	In Section 3, we describe our MDL-based generalization method." ></td>
	<td class="line x" title="29:393	In Section 4, we present our experimental results." ></td>
	<td class="line x" title="30:393	We then give some concluding remarks in Section 5." ></td>
	<td class="line x" title="31:393	2. The Problem 2.1 The Data Sparseness Problem Suppose that the data available to us are of the type shown in Table 1, which are slot values for a given verb (verb,slot_name,slot_value triples) automatically extracted from a corpus using existing techniques." ></td>
	<td class="line x" title="32:393	By counting the frequency of occurrence of each noun at a given slot of a verb, the frequency data shown in Figure 1 can be obtained." ></td>
	<td class="line x" title="33:393	We will refer to this type of data as co-occurrence data." ></td>
	<td class="line x" title="34:393	The problem of generalizing values of a case frame slot for a verb (or, in general, a head) can be viewed as the problem of learning the underlying conditional probability distribution that gives rise to such co-occurrence data." ></td>
	<td class="line x" title="35:393	Such a conditional distribution can be represented by a probability model that specifies the conditional probability P(n I v, r) for each n in the set of nouns.M = {nl, n2  nN}, V in the set of verbs V = {vl, v2  Vv}, and r in the set of slot names T~ = {rl, r2  rR}, satisfying: P(n Iv, r) = 1." ></td>
	<td class="line x" title="36:393	(1) nGM This type of probability model is often referred to as a word-based model." ></td>
	<td class="line x" title="37:393	Since the number of probability parameters in word-based models is large (O(N. V. R)), accurate 1 Recently, MDL and related techniques have become popular in corpus-based natural language processing and other related fields (Ellison 1991, 1992; Cartwright and Brent 1994; Stolcke and Omohundro 1994; Brent, Murthy, and Lundberg 1995; Ristad and Thomas 1995; Brent and Cartwright 1996; Grunwald 1996)." ></td>
	<td class="line x" title="38:393	In this paper, we introduce MDL into the context of case frame pattern acquisition." ></td>
	<td class="line x" title="39:393	218 Li and Abe Generalizing Case Frames Table 1 Example (verb, slot_name, slot_value) triple data." ></td>
	<td class="line x" title="40:393	verb slot_name slot_value fly argl bee fly argl bird fly argl bird fly argl crow fly argl bird fly argl eagle fly argl bee fly argl eagle fly argl bird fly argl crow 'Freq'." ></td>
	<td class="line x" title="41:393	-swallow crow eagle bird bug bee insect Figure 1 Frequency data for the subject slot of verb fly." ></td>
	<td class="line x" title="42:393	estimation of a word-based model is difficult with the data size that is available in practice--a problem usually referred to as the data sparseness problem." ></td>
	<td class="line x" title="43:393	For example, suppose that we employ the maximum-likelihood estimation (or MLE for short) to estimate the probability parameters of a conditional probability distribution, as described above, given the co-occurrence data in Figure 1." ></td>
	<td class="line x" title="44:393	In this case, MLE amounts to estimating the parameters by simply normalizing the frequencies so that they sum to one, giving, for example, the estimated probabilities of 0, 0.2, and 0.4 for swallow, eagle, and bird, respectively (see Figure 2)." ></td>
	<td class="line x" title="45:393	Since in general the number of parameters exceeds the size of data that is typically available, MLE will result in estimating most of the probability parameters to be zero." ></td>
	<td class="line x" title="46:393	To address this problem, Grishman and Sterling (1994) proposed a method of smoothing conditional probabilities using the probability values of similar words, where the similarity between words is judged based on co-occurrence data (see also Dagan, Marcus, and Makovitch \[1992\] and Dagan, Pereira, and Lee \[1994\])." ></td>
	<td class="line x" title="47:393	More specifically, conditional probabilities of words are smoothed by taking the weighted average of those of similar words using the similarity measure as the weights." ></td>
	<td class="line x" title="48:393	The advantage of this approach is that it does not rely on any prior knowledge, but it appears difficult to find a smoothing method that is both efficient and theoretically sound." ></td>
	<td class="line x" title="49:393	As an alternative, a number of authors have proposed the use of class-based 219 Computational Linguistics Volume 24, Number 2 'Prob'." ></td>
	<td class="line x" title="50:393	-0.45 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 0 swallow crow eagle bird bug bee insect Figure 2 Word-based distribution estimated using MLE." ></td>
	<td class="line x" title="51:393	models, which assign (conditional) probability values to (existing) classes of words, rather than individual words." ></td>
	<td class="line x" title="52:393	2.2 Class-based Models An example of the class-based approach is Resnik's method of generalizing values of a case frame slot using a thesaurus and the so-called selectional association measure (Resnik 1993a, 1993b)." ></td>
	<td class="line x" title="53:393	The selectional association, denoted A(C I v, r), is defined as follows: P(CIv, r) (2) A(C I v, F) = P(C I v, F) x log P(C) where C is a class of nouns present in a given thesaurus, v is a verb and r is a slot name, as described earlier." ></td>
	<td class="line x" title="54:393	In generalizing a given noun n to a noun class, this method selects the noun class C having the maximum A(C I v, r), among all super classes of n in a given thesaurus." ></td>
	<td class="line x" title="55:393	This method is based on an interesting intuition, but its interpretation as a method of estimation is not clear." ></td>
	<td class="line x" title="56:393	We propose a class-based generalization method whose performance as a method of estimation is guaranteed to be near optimal." ></td>
	<td class="line x" title="57:393	We define the class-based model as a model that consists of a partition of the set .N' of nouns, and a parameter associated with each member of the partition." ></td>
	<td class="line x" title="58:393	Here, a partition F of .M is any collection of mutually disjoint subsets of iV' that exhaustively cover N. The parameters specify the conditional probability P(C I v, r) for each class (subset) C in that partition, such that P(CIv, r) = 1." ></td>
	<td class="line x" title="59:393	(3) CEF Within a given class C, it is assumed that each noun is generated with equal probability, namely 1 Vn E C: P(n l v, r) = ~ x P(C I v, F)." ></td>
	<td class="line x" title="60:393	(4) Here, we assume that a word belongs to a single class." ></td>
	<td class="line x" title="61:393	In practice, however, many words have sense ambiguity and a word can belong to several different classes, e.g., bird is a member of both BIRD and MEAT." ></td>
	<td class="line x" title="62:393	Thorough treatment of this problem is beyond the scope of the present paper; we simply note that one can employ an existing word-sense disambiguation technique (e.g. ,Yarowsky 1992, 1994) in preprocessing, and use the disambiguated word senses as virtual words in the following 220 Li and Abe Generalizing Case Frames ANIMAL BIRD INSECT swallow crow eagle bird bug bee insect Figure 3 An example thesaurus." ></td>
	<td class="line x" title="63:393	case-pattern acquisition process." ></td>
	<td class="line x" title="64:393	It is also possible to extend our model so that each word probabilistically belongs to several different classes, which would allow us to resolve both structural and word-sense ambiguities at the time of disambiguation." ></td>
	<td class="line x" title="65:393	2 Employing probabilistic membership, however, would make the estimation process significantly more computationally demanding." ></td>
	<td class="line x" title="66:393	We therefore leave this issue as a future topic, and employ a simple heuristic of equally distributing each word occurrence in the data to all of its potential word senses in our experiments." ></td>
	<td class="line x" title="67:393	Since our learning method based on MDL is robust against noise, this should not significantly degrade performance." ></td>
	<td class="line x" title="68:393	2.3 The Tree Cut Model Since the number of partitions for a given set of nouns is extremely large, the problem of selecting the best model from among all possible class-based models is most likely intractable." ></td>
	<td class="line x" title="69:393	In this paper, we reduce the number of possible partitions to consider by using a thesaurus as prior knowledge, following a basic idea of Resnik's (1992)." ></td>
	<td class="line x" title="70:393	In particular, we restrict our attention to those partitions that exist within the thesaurus in the form of a cut." ></td>
	<td class="line x" title="71:393	By thesaurus, we mean a tree in which each leaf node stands for a noun, while each internal node represents a noun class, and domination stands for set inclusion (see Figure 3)." ></td>
	<td class="line x" title="72:393	A cut in a tree is any set of nodes in the tree that defines a partition of the leaf nodes, viewing each node as representing the set of all leaf nodes it dominates." ></td>
	<td class="line x" title="73:393	For example, in the thesaurus of Figure 3, there are five cuts: \[ANIMAL\], \[BIRD, INSECT\], \[BIRD, bug, bee, insect\], \[swallow, crow, eagle, bird, INSECT\], and \[swallow, crow, eagle, bird, bug, bee, insect\]." ></td>
	<td class="line x" title="74:393	The class of tree cut models of a fixed thesaurus tree is then obtained by restricting the partition P in the definition of a class-based model to be those partitions that are present as a cut in that thesaurus tree." ></td>
	<td class="line x" title="75:393	Formally, a tree cut model M can be represented by a pair consisting of a tree cut lP and a probability parameter vector 0 of the same length, that is: V = (r, e) (5) where lP and 0 are: r = \[C1, C2  Ck+l\], e = \[P(C1), P(C2)  P(Ck+l)\] (6) k+l where C1, C2  Ck+l is a cut in the thesaurus tree and ~i=1 P(Ci) = 1 is satisfied." ></td>
	<td class="line x" title="76:393	For simplicity we sometimes write P(Ci), i = 1  (k + 1) for P(Ci \[ v, r)." ></td>
	<td class="line x" title="77:393	If we use MLE for the parameter estimation, we can obtain five tree cut models from the co-occurrence data in Figure 1; Figures 4-6 show three of these." ></td>
	<td class="line x" title="78:393	For example, 2 The model used by Pereira, Tishby, and Lee (1993) is indeed along this direction." ></td>
	<td class="line x" title="79:393	221 Computational Linguistics Volume 24, Number 2 0.45 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 0 -r~-'-------~--~,Prob r-'~-swallow crow eagle bird bug bee insect Figure 4 A tree cut model with \[swallow, crow, eagle, bird, bug, bee, insect\]." ></td>
	<td class="line x" title="80:393	0.45 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 0 'Prob'." ></td>
	<td class="line x" title="81:393	swa'll  ow eagle bi'rd bug bee ins'ect Figure 5 A tree cut model with \[BIRD, bug, bee, insect\]." ></td>
	<td class="line x" title="82:393	~(\[BIRD, bug, bee, insect\], \[0.8,0,0.2,0\]) shown in Figure 5 is one such tree cut model." ></td>
	<td class="line x" title="83:393	Recall that M defines a conditional probability distribution PM(n I v,r) as follows: For any noun that is in the tree cut, such as bee, the probability is given as explicitly specified by the model, i.e., PM(bee I flY, argl) = 0.2." ></td>
	<td class="line x" title="84:393	For any class in the tree cut, the probability is distributed uniformly to all nouns dominated by it." ></td>
	<td class="line x" title="85:393	For example, since there are four nouns that fall under the class BIRD, and swallow is one of them, the probability of swallow is thus given by Pt~(swallow I flY, argl) = 0.8/4 = 0.2." ></td>
	<td class="line x" title="86:393	Note that the probabilities assigned to the nouns under BIRD are smoothed, even if the nouns have different observed frequencies." ></td>
	<td class="line x" title="87:393	We have thus formalized the problem of generalizing values of a case frame slot as that of estimating a model from the class of tree cut models for some fixed thesaurus tree; namely, selecting a model that best explains the data from among the class of tree cut models." ></td>
	<td class="line x" title="88:393	3." ></td>
	<td class="line x" title="89:393	Generalization Method Based On MDL The question now becomes what strategy (criterion) we should employ to select the best tree-cut model." ></td>
	<td class="line x" title="90:393	We adopt the Minimum Description Length principle (Rissanen 1978, 222 Li and Abe Generalizing Case Frames 0.45 0.4 0,35 0,3 0.25 0.2 O,lS 0.1 0,05 0 swallow crow eagle bi'rd Figure 6 A tree cut model with \[BIRD, INSECT\]." ></td>
	<td class="line x" title="91:393	'Prob'." ></td>
	<td class="line x" title="92:393	-Table 2 Number of parameters and KL distance from the empirical distribution for the five tree cut models." ></td>
	<td class="line x" title="93:393	P Number of Parameters KL Distance \[ANIMAL\] \[BIRD, INSECT\] \[BIRD, bug, bee, insect\] \[swallow, crow, eagle, bird, INSECT\] \[swallow, crow, eagle, bird, bug, bee, insect\] 0 0.89 1 0.72 3 0.4 4 0.32 6 0 1983, 1984, 1986, 1989), which has various desirable properties, as will be described later." ></td>
	<td class="line x" title="94:393	3 MDL is a principle of data compression and statistical estimation from information theory, which states that the best probability model for given data is that which requires the least code length in bits for the encoding of the model itself and the given data observed through it." ></td>
	<td class="line x" title="95:393	4 The former is the model description length and the latter the data description length." ></td>
	<td class="line x" title="96:393	In our current problem, it tends to be the case, in general, that a model nearer the root of the thesaurus tree, such as that in Figure 6, is simpler (in terms of the number of parameters), but tends to have a poorer fit to the data." ></td>
	<td class="line x" title="97:393	In contrast, a model nearer the leaves of the thesaurus tree, such as that in Figure 4, is more complex, but tends to have a better fit to the data." ></td>
	<td class="line x" title="98:393	Table 2 shows the number of free parameters and the KL distance from the empirical distribution of the data (namely, the word-based distribution estimated by MLE) shown in Figure 2 for each of the five tree cut models." ></td>
	<td class="line x" title="99:393	5 In the table, one can see that there is a trade-off between the simplicity of a model and the goodness of fit to the data." ></td>
	<td class="line x" title="100:393	In the MDL framework, the model description length is an indicator of model 3 Estimation strategies related to MDL have been independently proposed and studied by various authors (Solomonoff 1964; Wallace and Boulton 1968; Schwarz 1978; Wallace and Freeman 1992)." ></td>
	<td class="line x" title="101:393	4 We refer the interested reader to Quinlan and Rivest (1989) for an introduction to the MDL principle." ></td>
	<td class="line x" title="102:393	5 The KL distance (alsO known as KL-divergence or relative entropy), which is widely used in information theory and statistics, is a measure of distance between two distributions (e.g. , Cover and Thomas 1991)." ></td>
	<td class="line x" title="103:393	It is always normegative and is zero if and only if the two distributions are identical, but is asymmetric and hence not a metric (the usual notion of distance)." ></td>
	<td class="line x" title="104:393	223 Computational Linguistics Volume 24, Number 2 complexity, while the data description length indicates goodness of fit to the data." ></td>
	<td class="line x" title="105:393	The MDL principle stipulates that the model that minimizes the sum total of the description lengths should be the best model (both for data compression and statistical estimation)." ></td>
	<td class="line x" title="106:393	In the remainder of this section, we will describe how we apply MDL to our current problem." ></td>
	<td class="line x" title="107:393	We will then discuss the rationale behind using MDL in our present context." ></td>
	<td class="line x" title="108:393	3.1 Calculating Description Length We first show how the description length for a model is calculated." ></td>
	<td class="line x" title="109:393	We use S to denote a sample (or set of data), which is a multiset of examples, each of which is an occurrence of a noun at a given slot r of a given verb v (i.e. , duplication is allowed)." ></td>
	<td class="line x" title="110:393	We let ISI denote the size of S as a multiset, and n E S indicate the inclusion of n in S as a multiset." ></td>
	<td class="line x" title="111:393	For example, the column labeled slot_value in Table 1 represents a sample S for the subject slot offly, and in this case ISI = 10." ></td>
	<td class="line x" title="112:393	Given a sample S and a tree cut F, we employ MLE to estimate the parameters of the corresponding tree cut model ~,I = (F, 0), where 6 denotes the estimated parameters." ></td>
	<td class="line x" title="113:393	The total description length L(/~,I, S) of the tree cut model/vl and the sample S observed through M is computed as the sum of the model description length L(P), parameter description length L(0 I P), and data description length L(S I F, 6): L(M,S) = L((F,6),S) = L(r) + L(6 I r) +L(Str,6)." ></td>
	<td class="line x" title="114:393	(7) Note that we sometimes refer to L(F) + L(0 I F) as the model description length." ></td>
	<td class="line x" title="115:393	The model description length L(F) is a subjective quantity, which depends on the coding scheme employed." ></td>
	<td class="line x" title="116:393	Here, we choose to assign the same code length to each cut and let: L(F) = log IG\[ (8) where ~ denotes the set of all cuts in the thesaurus tree T. 6 This corresponds to assuming that each tree cut model is equally likely a priori, in the Bayesian interpretation of MDL." ></td>
	<td class="line x" title="117:393	(See Section 3.4)." ></td>
	<td class="line x" title="118:393	The parameter description length L(O I F) is calculated by: k L(0 I r) = ~ x log IsI (9) where ISI denotes the sample size and k denotes the number of free parameters in the tree cut model, i.e., k equals the number of nodes in P minus one." ></td>
	<td class="line x" title="119:393	It is known to be best to use this number of bits to describe probability parameters in order to minimize the expected total description length (Rissanen 1984, 1986)." ></td>
	<td class="line x" title="120:393	An intuitive explanation of this is that the standard deviation of the maximum-likelihood estimator of each parameter is of the order ~, and hence describing each parameter using more than 1 1 log ISI bits would be wasteful for the estimation accuracy possible with log x/~ 2 the given sample size." ></td>
	<td class="line x" title="121:393	Finally, the data description length L(S I F, 0) is calculated by: L(S I r, 0) = ~ log P(n) (10) nES 6 Here and throughout, log denotes the logarithm to the base 2." ></td>
	<td class="line x" title="122:393	For reasons why Equation 8 holds, see, for example, Quinlan and Rivest (1989)." ></td>
	<td class="line x" title="123:393	224 Li and Abe Generalizing Case Frames Table 3 Calculating the description length for the model of Figure 5." ></td>
	<td class="line x" title="124:393	C BIRD bug bee insect f(C) 8 0 2 0 ICI 4 1 1 1 P(C) 0.8 0.0 0.2 0.0 P(n) 0.2 0.0 0.2 0.0 P \[BIRD, bug, bee, insect\] L(0 1 r) (47l) x log 10 = 4.98 L(S I P,~) -(2+4+2+2) x log0.2 = 23.22 where for simplicity we write P(n) for PM(n \[ v, r)." ></td>
	<td class="line x" title="125:393	Recall that P(n) is obtained by MLE, namely, by normalizing the frequencies: 1 P(n) = ~ x P(C) (11) for each C c P and each n E C, where for each C c P: = d(C) (12) ISI wheref(C) denotes the total frequency of nouns in class C in the sample S, and F is a tree cut." ></td>
	<td class="line x" title="126:393	We note that, in fact, the maximum-likelihood estimate is one that minimizes the data description length L(S I F, 0)." ></td>
	<td class="line x" title="127:393	With description length defined in the above manner, we wish to select a model with the minimum description length and output it as the result of generalization." ></td>
	<td class="line x" title="128:393	Since we assume here that every tree cut has an equal L(P), technically we need only calculate and compare L'(/\[d, S) = L(~ I F) + L(S t F, ~) as the description length." ></td>
	<td class="line x" title="129:393	For simplicity, we will sometimes write just L'(F) for L'(7\[/I, S), where I ~ is the tree cut of M, when ~,I and S are clear from context." ></td>
	<td class="line x" title="130:393	The description lengths for the data in Figure 1 using various tree cut models of the thesaurus tree in Figure 3 are shown in Table 4." ></td>
	<td class="line x" title="131:393	(Table 3 shows how the description length is calculated for the model of tree cut \]BIRD, bug, bee, insect\])." ></td>
	<td class="line x" title="132:393	These figures indicate that the model in Figure 6 is the best model, according to MDL." ></td>
	<td class="line x" title="133:393	Thus, given the data in Table 1 as input, the generalization result shown in Table 5 is obtained." ></td>
	<td class="line x" title="134:393	3.2 An Efficient Algorithm In generalizing values of a case flame slot using MDL, we could, in principle, calculate the description length of every possible tree cut model and output a model with the minimum description length as the generalization result, if computation time were of no concern." ></td>
	<td class="line x" title="135:393	But since the number of cuts in a thesaurus tree is exponential in the size of the tree (for example, it is easy to verify that for a complete b-ary tree of depth d it is of the order o(2ba-1)), it is impractical to do so." ></td>
	<td class="line x" title="136:393	Nonetheless, we were able to devise a 225 Computational Linguistics Volume 24, Number 2 Table 4 Description length of the five tree cut models." ></td>
	<td class="line x" title="137:393	r L(~ I r) L(S \] r,~) L'(P) \[ANIMAL\] 0 28.07 28.07 \[BIRD, INSECT\] 1.66 26.39 28.05 \[BIRD, bug, bee, insect\] 4.98 23.22 28.20 \[swallow, crow, eagle, bird, INSECT\] 6.64 22.39 29.03 \[swallow, crow, eagle, bird, bug, bee, insect\] 9.97 19.22 29.19 Table 5 Generalization result." ></td>
	<td class="line x" title="138:393	verb slot~name slot_value probability fly argl BIRD 0.8 fly argl INSECT 0.2 Here we let t denote a thesaurus (sub)tree, root(t) the root of the tree t. Initially t is set to the entire tree." ></td>
	<td class="line x" title="139:393	Also input to the algorithm is a co-occurrence data." ></td>
	<td class="line x" title="140:393	algorithm Find-MDL(t) := cut 1." ></td>
	<td class="line x" title="141:393	if 2." ></td>
	<td class="line x" title="142:393	t is a leaf node 3." ></td>
	<td class="line x" title="143:393	then 4." ></td>
	<td class="line x" title="144:393	retum(\[t\]) 5." ></td>
	<td class="line x" title="145:393	else 6." ></td>
	<td class="line x" title="146:393	For each child tree ti of t ci :=Find-MDL(ti) 7." ></td>
	<td class="line x" title="147:393	c:= append(ci) 8." ></td>
	<td class="line x" title="148:393	if 9." ></td>
	<td class="line x" title="149:393	L'(\[root(t)\]) < L'(c) 10." ></td>
	<td class="line x" title="150:393	then 11." ></td>
	<td class="line x" title="151:393	return(\[root(t)\]) 12." ></td>
	<td class="line x" title="152:393	else 13." ></td>
	<td class="line x" title="153:393	return(c) Figure 7 The algorithm: Find-MDL." ></td>
	<td class="line x" title="154:393	simple and efficient algorithm based on dynamic programming, which is guaranteed to find a model with the minimum description length." ></td>
	<td class="line x" title="155:393	Our algorithm, which we call Find-MDL, recursively finds the optimal MDL model for each child subtree of a given tree and appends all the optimal models of these subtrees and returns the appended models, unless collapsing all the lowerqevel optimal models into a model consisting of a single node (the root node of the given tree) reduces the total description length, in which case it does so." ></td>
	<td class="line x" title="156:393	The details of the algorithm are given in Figure 7." ></td>
	<td class="line x" title="157:393	Note that for simplicity we describe Find-MDL as outputting a tree cut, rather than a complete tree cut model." ></td>
	<td class="line x" title="158:393	Note in the above algorithm that the parameter description length is calculated as 226 Li and Abe Generalizing Case Frames L'(\[ARTIFACT\])=41.09 L'(\[VEHICLE,AIRPLANE\])=40.97 ENTITY L'(\[AIR PLAN E\])=32.27 r,airplane\])=32.72 RO~iNS A a ~ 5A'C'I   7o.2 0.23 BI CT l VEHICLE AIRPLANE swallow crow eagle bird bug ~lS~'~insect car bike jet helicopter airplane f(swallow)=4,f(crow)=4,f(eagle)=4,f(bird)=6,f(bee)=8,f(car)=l,f(jet)=4,f(airplane)=4 Figure 8 An example application of Find-MDL." ></td>
	<td class="line x" title="159:393	log ISI, where k + 1 is the number of nodes in the current cut, both when t is the 2 entire tree and when it is a proper subtree." ></td>
	<td class="line x" title="160:393	This contrasts with the fact that the number of free parameters is k for the former, while it is k + 1 for the latter." ></td>
	<td class="line x" title="161:393	For the purpose of finding a tree cut with the minimum description length, however, this distinction can be ignored (see Appendix A)." ></td>
	<td class="line x" title="162:393	Figure 8 illustrates how the algorithm works (on the co-occurrence data shown at the bottom): In the recursive application of Find-MDL on the subtree rooted at AIRPLANE, the if-clause on line 9 evaluates to true since L'(\[AIRPLANE\]) = 32.27, L'(~et, helicopter, airplane\]) = 32.72, and hence \[AIRPLANE\] is returned." ></td>
	<td class="line x" title="163:393	Then in the call to Find-MDL on the subtree rooted at ARTIFACT, the same if-clause evaluates to false since L'(\[VEHICLE, AIRPLANE\]) = 40.97, L'(\[ARTIFACT\]) = 41.09, and hence \[VEHICLE, AIRPLANE\] is returned." ></td>
	<td class="line x" title="164:393	Concerning the above algorithm, we show that the following proposition holds: Proposition 1 The algorithm Find-MDL terminates in time O(N x ISI), where N denotes the number of leaf nodes in the input thesaurus tree T and ISI denotes the input sample size, and outputs a tree cut model of T with the minimum description length (with respect to the encoding scheme described in Section 3.1)." ></td>
	<td class="line x" title="165:393	Here we will give an intuitive explanation of why the proposition holds, and give the formal proof in Appendix A. The MLE of each node (class) is obtained simply by dividing the frequency of nouns within that class by the total sample size." ></td>
	<td class="line x" title="166:393	Thus, the parameter estimation for each subtree can be done independently from the estimation of the parameters outside the subtree." ></td>
	<td class="line x" title="167:393	The data description length for a subtree thus depends solely on the tree cut within that subtree, and its calculation can be performed independently for each subtree." ></td>
	<td class="line x" title="168:393	As for the parameter description length for a subtree, it depends only on the number of classes in the tree cut within that subtree, and hence can be computed independently as well." ></td>
	<td class="line x" title="169:393	The formal proof proceeds by mathematical induction, which verifies that the optimal model in any (sub)tree is either the model 227 Computational Linguistics Volume 24, Number 2 consisting of the root of the tree or the model obtained by appending the optimal submodels for its child subtrees." ></td>
	<td class="line x" title="170:393	7 3.3 Estimation, Generalization, and MDL When a discrete model (a partition F of the set of nouns W' in our present context) is fixed, and the estimation problem involves only the estimation of probability parameters, the classic maximum-likelihood estimation (MLE) is known to be satisfactory." ></td>
	<td class="line x" title="171:393	In particular, the estimation of a word-based model is one such problem, since the partition is fixed and the size of the partition equals \[.M\[." ></td>
	<td class="line x" title="172:393	Furthermore, for a fixed discrete model, it is known that MLE coincides with MDL: Given data S = {xi : i = 1  m}, MLE estimates parameter P, which maximizes the likelihood with respect to the data; that is: m = arg mpax H P(xi)." ></td>
	<td class="line x" title="173:393	(13) i=1 It is easy to see that P also satisfies: m = arg nun ~ log P(xi)." ></td>
	<td class="line x" title="174:393	(14) i=1 This is nothing but the MDL estimate in this case, since ~i~1 -log P(xi) is the data description length." ></td>
	<td class="line x" title="175:393	When the estimation problem involves model selection, i.e., the choice of a tree cut in the present context, MDUs behavior significantly deviates from that of MLE." ></td>
	<td class="line x" title="176:393	This is because MDL insists on minimizing the sum total of the data description length and the model description length, while MLE is still equivalent to minimizing the data description length only." ></td>
	<td class="line x" title="177:393	So, for our problem of estimating a tree cut model, MDL tends to select a model that is reasonably simple yet fits the data quite well, whereas the model selected by MLE will be a word-based model (or a tree cut model equivalent to the word-based modelS), as it will always manage to fit the data." ></td>
	<td class="line x" title="178:393	In statistical terms, the superiority of MDL as an estimation method is related to the fact we noted earlier that even though MLE can provide the best fit to the given data, the estimation accuracy of the parameters is poor, when applied on a sample of modest size, as there are too many parameters to estimate." ></td>
	<td class="line x" title="179:393	MLE is likely to estimate most parameters to be zero, and thus suffers from the data sparseness problem." ></td>
	<td class="line x" title="180:393	Note in Table 4, that MDL avoids this problem by taking into account the model complexity as well as the fit to the data." ></td>
	<td class="line x" title="181:393	MDL stipulates that the model with the minimum description length should be selected both for data compression and estimation." ></td>
	<td class="line x" title="182:393	This intimate connection between estimation and data compression can also be thought of as that between estimation and generalization, since in order to compress information, generalization is necessary." ></td>
	<td class="line x" title="183:393	In our current problem, this corresponds to the generalization of individual nouns present in case frame instances in the data as classes of nouns present in a given thesaurus." ></td>
	<td class="line x" title="184:393	For example, given the thesaurus in Figure 3 and frequency data in Figure 1, we would 7 The process of finding the MDL model tends to be computationally demanding and is often intractable." ></td>
	<td class="line x" title="185:393	When the model class under consideration is restricted to tree structures, however, dynamic programming is often applicable and the MDL model can be efficiently found." ></td>
	<td class="line x" title="186:393	For example, Rissanen (1995) has devised an algorithm for learning decision trees." ></td>
	<td class="line x" title="187:393	8 Consider, for example, the case when the co-occurrence data is given as f(swallow) = 2,f(crow) = 2,f(eagle) = 2,f(bird) = 2 for the problem in Section 2." ></td>
	<td class="line x" title="188:393	228 Li and Abe Generalizing Case Frames like our system to judge that the class BIRD and the noun bee can be the subject slot of the verb fly." ></td>
	<td class="line x" title="189:393	The problem of deciding whether to stop generalizing at BIRD and bee, or generalizing further to ANIMAL has been addressed by a number of authors (Webster and Marcus 1989; Velardi, Pazienza, and Fasolo 1991; Nomiyama 1992)." ></td>
	<td class="line x" title="190:393	Minimization of the total description length provides a disciplined criterion to do this." ></td>
	<td class="line x" title="191:393	A remarkable fact about MDL is that theoretical findings have indeed verified that MDL, as an estimation strategy, is near optimal in terms of the rate of convergence of its estimated models to the true model as data size increases." ></td>
	<td class="line x" title="192:393	When the true model is included in the class of models considered, the models selected by MDL converge to the true model at the rate of O/~C:~9~_i~!~ where k* is the number of parameters in 2.1Sl J' the true model, and \[S\] the data size, which is near optimal (Barron and Cover 1991; Yamanishi 1992)." ></td>
	<td class="line x" title="193:393	Thus, in the current problem, MDL provides (a) a way of smoothing probability parameters to solve the data sparseness problem, and at the same time, (b) a way of generalizing nouns in the data to noun classes of an appropriate level, both as a corollary to the near optimal estimation of the distribution of the given data." ></td>
	<td class="line x" title="194:393	3.4 The Bayesian Interpretation of MDL and the Choice of Encoding Scheme There is a Bayesian interpretation of MDL: MDL is essentially equivalent to the 'posterior mode' in the Bayesian terminology (Rissanen 1989)." ></td>
	<td class="line x" title="195:393	Given data S and a number of models, the Bayesian estimator (posterior mode) selects a model M that maximizes the posterior probability: = argn~x(P(M)." ></td>
	<td class="line x" title="196:393	P(S I M)) (15) where P(M) denotes the prior probability of the model M and P(S \[ M) the probability of observing the data S given M. Equivalently, M satisfies ~'I = argn~m(logP(M) logP(S I M))." ></td>
	<td class="line x" title="197:393	(16) This is equivalent to the MDL estimate, if we take log P(M) to be the model description length." ></td>
	<td class="line x" title="198:393	Interpreting log P(M) as the model description length translates, in the Bayesian estimation, to assigning larger prior probabilities on simpler models, since it is equivalent to assuming that P(M) = ()t(a), where I(M) is the description length of M." ></td>
	<td class="line x" title="199:393	(Note that if we assign uniform prior probability P(M) to all models M, then (15) becomes equivalent to (13), giving the maximum-likelihood estimate)." ></td>
	<td class="line x" title="200:393	Recall, that in our definition of parameter description length, we assign a shorter parameter description length to a model with a smaller number of parameters k, which admits the above interpretation." ></td>
	<td class="line x" title="201:393	As for the model description length (for tree cuts) we assigned an equal code length to each tree cut, which translates to placing no bias on any cut." ></td>
	<td class="line x" title="202:393	We could have employed a different coding scheme assigning shorter code lengths to cuts nearer the root." ></td>
	<td class="line x" title="203:393	We chose not to do so partly because, for sufficiently large sample sizes, the parameter description length starts dominating the model description length anyway." ></td>
	<td class="line x" title="204:393	Another important property of the definition of description length is that it affects not only the effective prior probabilities on the models, but also the procedure for computing the model minimizing the measure." ></td>
	<td class="line x" title="205:393	Indeed, our definition of model description length was chosen to be compatible with the dynamic programming technique, namely, its calculation is performable locally for each subtree." ></td>
	<td class="line x" title="206:393	For a different choice of coding scheme, it is possible that a simple and efficient MDL algorithm like 229 Computational Linguistics Volume 24, Number 2 Find-MDL may not exist." ></td>
	<td class="line x" title="207:393	We believe that our choice of model description length is derived from a natural encoding scheme with reasonable interpretation as Bayesian prior, and at the same time allows an efficient algorithm for finding a model with the minimum description length." ></td>
	<td class="line x" title="208:393	3.5 The Uniform Distribution Assumption and the Level of Generalization The uniform distribution assumption made in (4), namely that all nouns belonging to a class contained in the tree cut model are assigned the same probability, seems to be rather stringent." ></td>
	<td class="line x" title="209:393	If one were to insist that the model be exactly accurate, then it would seem that the true model would be the word-based model resulting from no generalization at all." ></td>
	<td class="line x" title="210:393	If we allow approximations, however, it is likely that some reasonable tree cut model with the uniform probability assumption will be a good approximation of the true distribution; in fact, a best model for a given data size." ></td>
	<td class="line x" title="211:393	As we remarked earlier, as MDL balances between the fit to the data and the simplicity of the model, one can expect that the model selected by MDL will be a reasonable compromise." ></td>
	<td class="line x" title="212:393	Nonetheless, it is still a shortcoming of our model that it contains an oversimplified assumption, and the problem is especially pressing when rare words are involved." ></td>
	<td class="line x" title="213:393	Rare words may not be observed at a slot of interest in the data simply because they are rare, and not because they are unfit for that particular slot." ></td>
	<td class="line x" title="214:393	9 To see how rare is too rare for our method, consider the following example." ></td>
	<td class="line x" title="215:393	Suppose that the class BIRD contains 10 words, bird, swallow, crow, eagle, parrot, waxwing, etc. Consider co-occurrence data having 8 occurrences of bird, 2 occurrences of swallow, 1 occurrence of crow, 1 occurrence of eagle, and 0 occurrence of all other words, as part of, say, 100 data obtained for the subject slot of verb fly." ></td>
	<td class="line x" title="216:393	For this data set, our method would select the model that generalizes bird, swallow, etc. to the class BIRD, since the sum of the data and parameter description lengths for the BIRD subtree is 76.57 + 3.32 = 79.89 if generalized, and 53.73 + 33.22 = 86.95 if not generalized." ></td>
	<td class="line x" title="217:393	For comparison, consider the data with 10 occurrences of bird, 3 occurrences of swallow and 1 occurrence of crow, and 0 occurrence of all other words, also as part of 100 data for the subject slot of fly." ></td>
	<td class="line x" title="218:393	In this case, our method would select the model that stops generalizing at bird, swallow, eagle, etc. , because the description length for the same subtree now is 86.22 + 3.32 = 89.54 if generalized, and 55.04 + 33.22 = 88.26 if not generalized." ></td>
	<td class="line x" title="219:393	These examples seem to indicate that our MDL-based method would choose to generalize, even when there are relatively large differences in frequencies of words within a class, but knows enough to stop generalizing when the discrepancy in frequencies is especially noticeable (relative to the given sample size)." ></td>
	<td class="line x" title="220:393	4." ></td>
	<td class="line x" title="221:393	Experimental Results 4.1 Experiment 1: A Qualitative Evaluation We applied our generalization method to large corpora and inspected the obtained tree cut models to see if they agreed with human intuition." ></td>
	<td class="line oc" title="222:393	In our experiments, we extracted verbs and their case frame slots (verb, slot_name, slot_value triples) from the tagged texts of the Wall Street Journal corpus (ACL/DCI CD-ROM1) consisting of 126,084 sentences, using existing techniques (specifically, those in Smadja \[1993\]), then 9 There are several possible measures that one could take to address this issue, including the incorporation of absolute frequencies of the words (inside and outside the particular slot in question)." ></td>
	<td class="line x" title="223:393	This is outside the scope of the present paper, and we simply refer the interested reader to one possible approach (Abe and Li 1996)." ></td>
	<td class="line x" title="224:393	230 Li and Abe Generalizing Case Frames Table 6 Example input data (for the direct object slot of eat)." ></td>
	<td class="line x" title="225:393	eat arg2 food 3 eat arg2 lobster 1 eat arg2 seed 1 eat arg2 heart 2 eat arg2 liver 1 eat arg2 plant 1 eat arg2 sandwich 2 eat arg2 crab 1 eat arg2 elephant 1 eat arg2 meal 2 eat arg2 rope 1 eat arg2 seafood 1 eat arg2 amount 2 eat arg2 horse 1 eat arg2 mushroom 1 eat arg2 night 2 eat arg2 bug 1 eat arg2 ketchup 1 eat arg2 lunch 2 eat arg2 bowl 1 eat arg2 sawdust 1 eat arg2 snack 2 eat arg2 month 1 eat arg2 egg 1 eat arg2 jam 2 eat arg2 effect 1 eat arg2 sprout 1 eat arg2 diet 1 eat arg2 debt 1 eat arg2 nail 1 eat arg2 pizza 1 eat arg2 oyster 1 applied our method to generalize the slot_values." ></td>
	<td class="line x" title="226:393	Table 6 shows some example triple data for the direct object slot of the verb eat." ></td>
	<td class="line x" title="227:393	There were some extraction errors present in the data, but we chose not to remove them, because in general there will always be extraction errors and realistic evaluation should leave them in." ></td>
	<td class="line x" title="228:393	When generalizing, we used the noun taxonomy of WordNet (version 1.4) (Miller 1995) as our thesaurus." ></td>
	<td class="line x" title="229:393	The noun taxonomy of WordNet has a structure of directed acyclic graph (DAG), and its nodes stand for a word sense (a concept) and often contain several words having the same word sense." ></td>
	<td class="line x" title="230:393	WordNet thus deviates from our notion of thesaurus--a tree in which each leaf node stands for a noun, each internal node stands for the class of nouns below it, and a noun is uniquely represented by a leaf node--so we took a few measures to deal with this." ></td>
	<td class="line x" title="231:393	First, we modified our algorithm FInd-MDL so that it can be applied to a DAG; now, Find-MDL effectively copies each subgraph having multiple parents (and its associated data) so that the DAG is transformed to a tree structure." ></td>
	<td class="line x" title="232:393	Note that with this modification it is no longer guaranteed that the output model is optimal." ></td>
	<td class="line x" title="233:393	Next, we dealt heuristically with the issue of word-sense ambiguity by equally dividing the observed frequency of a noun between all the nodes containing that noun." ></td>
	<td class="line x" title="234:393	Finally, when an internal node contained nouns actually occurring in the data, we assigned the .frequencies of all the nodes below it to that internal node, and excised the whole subtree (subgraph) below it." ></td>
	<td class="line x" title="235:393	The last of these measures, in effect, defines the 'starting cut' of the thesaurus from which to begin generalizing." ></td>
	<td class="line x" title="236:393	Since (word senses of) nouns that occur in natural language tend to concentrate in the middle of a taxonomy, the starting cut given by this method usually falls around the middle of the thesaurus." ></td>
	<td class="line x" title="237:393	1 Figure 9 shows the starting cut and the resulting cut in WordNet for the direct object slot of eat with respect to the data in Table 6, where // denotes a node in WordNet." ></td>
	<td class="line x" title="238:393	The starting cut consists of nodes/plant/,/food/,etc, which are the highest nodes containing values of the direct object slot of eat." ></td>
	<td class="line x" title="239:393	Since/food/has significantly higher frequencies than its neighbors/solid/and/fluid/, the generalization stops there according to MDL." ></td>
	<td class="line x" title="240:393	In contrast, the nodes under/life_form/have relatively small differences in their frequencies, and thus they are generalized to the node/life_form/." ></td>
	<td class="line x" title="241:393	The same is true of the nodes under /artifact/." ></td>
	<td class="line x" title="242:393	Since /-amount/ has a much 10 Cognitive scientists have observed that concepts in the middle of a taxonomy tend to be more important with respect to learning, recognition, and memory, and their linguistic expressions occur more frequently in natural language--a phenomenon known as basic level primacy." ></td>
	<td class="line x" title="243:393	See Lakoff (1987)." ></td>
	<td class="line x" title="244:393	231 Computational Linguistics Volume 24, Number 2 TOP t~ <abstraction> o.i0 ~-/  ~-~--, -~ <life_form> O. 11 <object> ( <measure,~:ljjafl~it~,amount> <space> <time> /\ / / ',9-089 ' <plant> % <animal>| <substance> {<artifact~ : ; ;, k \', ', I I I ~ ~/ \ ~ 0.39 ~1 ~ t J I I ~ _~,.~ ~ t. resulting cut a a,,,tsohd> <fluid> <foo~>~." ></td>
	<td class="line x" title="245:393	~ ~t~rtinn c.==t I I I I ~ %% / / \ I % I I I \ I I :mushroom> <lobster> <horse> <lobster> <pizza> <rope> Figure 9 An example generalization, result (for the direct object slot of eat)." ></td>
	<td class="line x" title="246:393	higher frequency than its neighbors /time/ and {space), the generalization does not go up higher." ></td>
	<td class="line x" title="247:393	All of these results seem to agree with human intuition, indicating that our method results in an appropriate level of generalization." ></td>
	<td class="line x" title="248:393	Table 7 shows generalization results for the direct object slot of eat and some other arbitrarily selected verbs, where classes are sorted in descending order of their probability values." ></td>
	<td class="line x" title="249:393	(Classes with probabilities less than 0.05 are discarded due to space limitations)." ></td>
	<td class="line x" title="250:393	Table 8 shows the computation time required (on a SPARC 'Ultra 1' work station) to obtain the results shown in Table 7." ></td>
	<td class="line x" title="251:393	(The computation time for loading the WordNet was excluded since it need be done only once)." ></td>
	<td class="line x" title="252:393	Even though the noun taxonomy of WordNet is a large thesaurus containing approximately 50,000 nodes, our method still manages to efficiently generalize case slots using it." ></td>
	<td class="line x" title="253:393	The table also shows the average number of levels generalized for each slot, namely, the average number of links between a node in the starting cut and its ancestor node in the resulting cut." ></td>
	<td class="line x" title="254:393	(For example, the number of levels generalized for/plant-/ is one in Figure 9)." ></td>
	<td class="line x" title="255:393	One can see that a significant amount of generalization is performed by our method--the resulting tree cut is about 5 levels higher than the starting cut, on the average." ></td>
	<td class="line x" title="256:393	4.2 Experiment 2: PP-Attachment Disambiguation Case frame patterns obtained by our method can be used in various tasks in natural language processing." ></td>
	<td class="line x" title="257:393	In this paper, we test its effectiveness in a structural (PPattachment) disambiguation experiment." ></td>
	<td class="line x" title="258:393	Disambiguation Methods." ></td>
	<td class="line x" title="259:393	It has been empirically verified that the use of lexical semantic knowledge is effective in structural disambiguation, such as the PP-attachment problem (Hobbs and Bear 1990; Whittemore, Ferrara, and Brunner 1990)." ></td>
	<td class="line x" title="260:393	There have been 232 Li and Abe Generalizing Case Frames Table 7 Examples of generalization results." ></td>
	<td class="line x" title="261:393	Class Probability Example Words Direct Object of eat (food,nutrient) 0.39 pizza, egg (life_form,organism,being,living_thing) 0.11 lobster, horse /measure,quantity, amount,quantum) 0.10 amount of (artifact,article,artefact) 0.08 as if eat rope Direct Object of buy (object, inanimate-object,physical-object / 0.30 computer, painting (asset) 0.10 stock, share (group,grouping) 0.07 company, bank (legal_document,legal_instrument,official_document  ) 0.05 security, ticket Direct Object of .fly (entity) 0.35 airplane, flag, executive (linear_measure,long_measure) 0.28 mile /group,grouping) 0.08 delegation Direct Object of operate /group,grouping/ 0.13 company, fleet (act,human_action,human_activity) 0.13 flight, operation (structure,construction/ 0.12 center (abstraction) 0.11 service, unit (possession/ 0.06 profit, earnings Table 8 Required computation time and number of generalized levels." ></td>
	<td class="line x" title="262:393	Verb CPU Time (second) Average Number of Generalized Levels eat 1.00 5.2 buy 0.66 4.6 fly 1.11 6.0 operate 0.90 5.0 Average 0.92 5.2 many probabilistic methods proposed in the literature to address the PP-attachment problem using lexical semantic knowledge which, in our view, can be classified into three types." ></td>
	<td class="line x" title="263:393	The first approach (Hindle and Rooth 1991, 1993) takes doubles of the form (verb, prep) and (nounl, prep), like those in Table 9, as training data to acquire semantic knowledge and judges the attachment sites of the prepositional phrases in quadruples of the form (verb, nounl, prep, noun2) e.g., (see, girl, with, telescope)--based on the acquired knowledge." ></td>
	<td class="line x" title="264:393	Hindle and Rooth (1991) proposed the use of the lexical association measure calculated based on such doubles." ></td>
	<td class="line x" title="265:393	More specifically, they estimate P(prep I verb) and P(prep \[ noun1), and calculate the so-called t-score, which is a measure of the statistical significance of the difference between P(prep I verb) and P(prep \[ nounl)." ></td>
	<td class="line x" title="266:393	If the t-score indicates that the former probability is significantly larger, 233 Computational Linguistics Volume 24, Number 2 Table 9 Example input data as doubles." ></td>
	<td class="line x" title="267:393	see in see with girl with man with Table 10 Example input data as triples." ></td>
	<td class="line x" title="268:393	see in park see with telescope girl with scarf see with friend man with hat Table 11 Example input data as quadruples and labels." ></td>
	<td class="line x" title="269:393	see girl in park ADV see man with telescope ADV see girl with scarf ADN then the prepositional phrase is attached to verb, if the latter probability is significantly larger, it is attached to nounl, and otherwise no decision is made." ></td>
	<td class="line x" title="270:393	The second approach (Sekine et al. 1992; Chang, Luo, and Su 1992; Resnik 1993a; Grishman and Sterling 1994; Alshawi and Carter 1994) takes triples (verb, prep, noun2) and (nounl, prep, noun2), like those in Table 10, as training data for acquiring semantic knowledge and performs PP-attachment disambiguation on quadruples." ></td>
	<td class="line x" title="271:393	For example, Resnik (1993a) proposes the use of the selectional association measure calculated based on such triples, as described in Section 2." ></td>
	<td class="line x" title="272:393	More specifically, his method compares maxclassi~noun2 A(Classi \[ verb, prep) and maxclassi~no,m2 A(Classi I nounl,prep) to make disambiguation decisions." ></td>
	<td class="line x" title="273:393	The third approach (Brill and Resnik 1994; Ratnaparkhi, Reynar, and Roukos 1994; Collins and Brooks 1995) receives quadruples (verb, noun1, prep, noun2) and labels indicating which way the PP-attachment goes, like those in Table 11, and learns a disambiguation rule for resolving PP-attachment ambiguities." ></td>
	<td class="line x" title="274:393	For example, Brill and Resnik, (1994) propose a method they call transformation-based error-driven learning (see also Brill \[1995\])." ></td>
	<td class="line x" title="275:393	Their method first learns IF-THEN type rules, where the IF parts represent conditions like (prep is with) and (verb is see), and the THEN parts represent transformations from (attach to verb) to (attach to nounl), or vice versa." ></td>
	<td class="line x" title="276:393	The first rule is always a default decision, and all the other rules indicate transformations (changes of attachment sites) subject to various IF conditions." ></td>
	<td class="line x" title="277:393	We note that, for the disambiguation problem, the first two approaches are basically unsupervised learning methods, in the sense that the training data are merely positive examples for both types of attachments, which could in principle be extracted from pure corpus data with no human intervention." ></td>
	<td class="line x" title="278:393	(For example, one could just use unambiguous sentences)." ></td>
	<td class="line x" title="279:393	The third approach, on the other hand, is a supervised learning method, which requires labeled data prepared by a human being." ></td>
	<td class="line x" title="280:393	234 Li and Abe Generalizing Case Frames Table 12 Number of different types of data." ></td>
	<td class="line x" title="281:393	Training Data Average number of doubles per data set 91218.1 Average number of triples per data set 91218.1 Average number of quadruples per data set 21656.6 Test Data Average number of quadruples per data set 820.4 The generalization method we propose falls into the second category, although it can also be used as a component in a combined scheme with many of the above methods (see Brill and Resnik \[1994\], Alshawi and Carter \[1994\])." ></td>
	<td class="line x" title="282:393	We estimate P(noun2 I verb, prep) and P(noun2 I nount, prep) from training data consisting of triples, and compare them: If the former exceeds the latter (by a certain margin) we attach it to verb, else if the latter exceeds the former (by the same margin) we attach it to noun1." ></td>
	<td class="line x" title="283:393	In our experiments, described below, we compare the performance of our proposed method, which we refer to as MDL, against the methods proposed by Hindle and Rooth (1991), Resnik (1993b), and Brill and Resnik (1994), referred to respectively as LA, SA, and TEL." ></td>
	<td class="line x" title="284:393	Data Set." ></td>
	<td class="line x" title="285:393	We used the bracketed corpus of the Penn Treebank (Wall Street Journal corpus) (Marcus, Santorini, and Marcinkiewicz 1993) as our data." ></td>
	<td class="line x" title="286:393	First we randomly selected one of the 26 directories of the WSJ files as the test data and what remains as the training data." ></td>
	<td class="line x" title="287:393	We repeated this process 10 times and obtained 10 sets of data consisting of different training data and test data." ></td>
	<td class="line x" title="288:393	We used these 10 data sets to conduct cross-validation as described below." ></td>
	<td class="line x" title="289:393	From the test data in each data set, we extracted (verb, noun1, prep, noun2) quadruples using the extraction tool provided by the Penn Treebank called 'tgrep'." ></td>
	<td class="line x" title="290:393	At the same time, we obtained the answer for the PP-attachment site for each quadruple." ></td>
	<td class="line x" title="291:393	We did not double-check if the answers provided in the Penn Treebank were actually correct or not." ></td>
	<td class="line x" title="292:393	Then from the training data of each data set, we extracted (verb, prep) and (noun, prep) doubles, and (verb, prep, noun2) and (nounl,prep, noun2) triples using tools we developed ourselves." ></td>
	<td class="line x" title="293:393	We also extracted quadruples from the training data as before." ></td>
	<td class="line x" title="294:393	We then applied 12 heuristic rules to further preprocess the data, which include (1) changing the inflected form of a word to its stem form, (2) replacing numerals with the word number, (3) replacing integers between 1,900 and 2,999 with the word year, (4) replacing co. , ltd. , etc. with the words company, limited, etc. 11 After preprocessing there still remained some minor errors, which we did not remove further, due to the lack of a good method for doing so automatically." ></td>
	<td class="line x" title="295:393	Table 12 shows the number of different types of data obtained by the above process." ></td>
	<td class="line x" title="296:393	Experimental Procedure." ></td>
	<td class="line x" title="297:393	We first compared the accuracy and coverage for each of the three disambiguation methods based on unsupervised learning: MDL, SA, and LA. 11 The experimental results obtained here are better than those obtained in our preliminary experiment (Li and Abe 1995), in part because we only adopted rule (1) in the past." ></td>
	<td class="line x" title="298:393	235 Computational Linguistics Volume 24, Number 2 0.98 0.96 0.94 0.92 0.9 0.88 0.66 0.84 0.82 0.8 0 '', '''-,., 'E3, '''',  , 'D,." ></td>
	<td class="line x" title="300:393	x*', f I I { 0.2 0.4 0.6 0.8 coverage Figure 10 Accuracy-coverage curves for MDL, SA, and LA. i 'MDL' 'SA' -4-'LA' -~-'LA.t' x  'D,." ></td>
	<td class="line x" title="301:393	'~t 'El For MDL, we generalized noun2 given (verb, prep, noun2) and (nounl,prep, noun2) triples as training data for each data set, using WordNet as the thesaurus in the same manner as in experiment 1." ></td>
	<td class="line x" title="302:393	When disambiguating, we actually compared P(Classl \[ verb, prep) and P(Class2 I noun1, prep), where Class1 and Class2 are classes in the output tree cut models dominating noun2 in place of P(noun2 \] verb, prep) and P(noun2 \] nounl,prep)." ></td>
	<td class="line x" title="303:393	12 We found that doing so gives a slightly better result." ></td>
	<td class="line x" title="304:393	For SA, we employed a somewhat simplified version in which noun2 is generalized given (verb, prep, noun2) and (nounl,prep, noun2) triples using WordNet, and maxcl~ss,~,o,,2 A(Classi I verb, prep) and maxctass,~no,n2 A(Classi l nounl, prep) are compared for disambiguation: If the former exceeds the latter then the prepositional phrase is attached to verb, and otherwise to noun1." ></td>
	<td class="line x" title="305:393	For LA, we estimated P(prep \] verb) and P(prep \] noun1) from the training data of each data set and compared them for disambiguation." ></td>
	<td class="line x" title="306:393	We then evaluated the results achieved by the three methods in terms of accuracy and coverage." ></td>
	<td class="line x" title="307:393	Here, coverage refers to the proportion as a percentage, of the test quadruples on which the disambiguation method could make a decision, and accuracy refers to the proportion of correct decisions among them." ></td>
	<td class="line x" title="308:393	In Figure 10, we plot the accuracy-coverage curves for the three methods." ></td>
	<td class="line x" title="309:393	In plotting these curves, the attachment site is determined by simply seeing if the difference between the appropriate measures for the two alternatives, be it probabilities or selectional association values, exceeds a threshold." ></td>
	<td class="line x" title="310:393	For each method, the threshold was set successively to 0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, and 0.75." ></td>
	<td class="line x" title="311:393	When the difference between the two measures is less than a threshold, we rule that no decision can be made." ></td>
	<td class="line x" title="312:393	These curves were obtained by averaging over the 10 data sets." ></td>
	<td class="line x" title="313:393	12 Recall that a node in WordNet represents a word sense and not a word; noun2 can belong to several classes in the thesaurus." ></td>
	<td class="line x" title="314:393	We thus use maxciassignou,2 (P(Classi \[ verb, prep)) and maxclassi gno,m2 ( P( Classi \[ nounl, prep) ) in place of P( Classl \] verb, prep) and P( Class2 \[ nounl, prep)." ></td>
	<td class="line x" title="315:393	236 Li and Abe Generalizing Case Frames Table 13 Results of PP-attachment disambiguation." ></td>
	<td class="line x" title="316:393	Coverage(%) Accuracy(%) Default 100 56.2 MDL + Default 100 82.2 SA + Default 100 76.7 LA + Default 100 80.7 LA.t + Default 100 78.1 TEL 100 82.4 We also implemented the exact method proposed by Hindle and Rooth (1991), which makes disambiguation judgement using the t-score." ></td>
	<td class="line x" title="317:393	Figure 10 shows the result as LA.t, where the threshold for t-score is set to 1.28 (significance level of 90 percent)." ></td>
	<td class="line x" title="318:393	From Figure 10 we see that with respect to accuracy-coverage curves, MDL outperforms both SA and LA throughout, while SA is better than LA. Next, we tested the method of applying a default rule after applying each method." ></td>
	<td class="line x" title="319:393	That is, attaching (prep, noun2) to verb for the part of the test data for which no decision was made by the method in question." ></td>
	<td class="line x" title="320:393	13 We refer to these combined methods as MDL+Default, SA+Default, LA+Default, and LA.t+Default." ></td>
	<td class="line x" title="321:393	Table 13 shows the results, again averaged over the 10 data sets." ></td>
	<td class="line x" title="322:393	Finally, we used the transformation-based error-driven learning (TEL) to acquire transformation rules for each data set and applied the obtained rules to disambiguate the test data." ></td>
	<td class="line x" title="323:393	The average number of obtained rules for a data set was 2,752.3." ></td>
	<td class="line x" title="324:393	Table 13 shows the disambiguation result averaged over the 10 data sets." ></td>
	<td class="line x" title="325:393	From Table 13, we see that TEL performs the best, edging over the second place MDL+Default by a small margin, and then followed by LA+Default, and SA+Default." ></td>
	<td class="line x" title="326:393	Below we discuss further observations concerning these results." ></td>
	<td class="line x" title="327:393	MDL and SA." ></td>
	<td class="line x" title="328:393	According to our experimental results, the accuracy and coverage of MDL appear to be somewhat better than those of SA." ></td>
	<td class="line x" title="329:393	As Resnik (1993b) pointed ~ P(qv,r) out, the use of selectional association Iu~ ~ seems to be appropriate for cognitive modeling." ></td>
	<td class="line x" title="330:393	Our experiments show, however, that the generalization method currently employed by Resnik has a tendency to overfit the data." ></td>
	<td class="line x" title="331:393	Table 14 shows example generalization results for MDL (with classes with probability less than 0.05 discarded) and SA." ></td>
	<td class="line x" title="332:393	Note that MDL tends to select a tree cut closer to the root of the thesaurus tree." ></td>
	<td class="line x" title="333:393	This is probably the key reason why MDL has a wider coverage than SA for the same degree of accuracy." ></td>
	<td class="line x" title="334:393	One may be concerned that MDL is 'overgeneralizing' here, 14 but as shown in Figure 10, its disambiguation accuracy does not seem to be degraded." ></td>
	<td class="line x" title="335:393	Another problem that must be dealt with concerning SA is how to remove noise (resulting, for example, from erroneous extraction) from the generalization results." ></td>
	<td class="line x" title="336:393	P(Clv,r) Since SA estimates the ratio between two probability values, namely -~y-, the generalization result may be lead astray if one of the estimates of P(C I v, r) and P(C) is unreliable." ></td>
	<td class="line x" title="337:393	For instance, a high estimated value for/drop, bead, pearl / at protect against 13 Interestingly, for the entire data set it is more favorable to attach (prep, noun2) to noun1, but for what remains after applying LA and MDL, it turns out to be more favorable to attach (prep, noun2) to verb." ></td>
	<td class="line x" title="338:393	14 Note that in Experiment 1, there were more data available, and thus the data were more appropriately generalized." ></td>
	<td class="line x" title="339:393	237 Computational Linguistics Volume 24, Number 2 Table 14 Example generalization results for SA and MDL." ></td>
	<td class="line x" title="340:393	Input Verb Preposition Noun Frequency protect against accusation 1 protect against damage 1 protect against decline 1 protect against drop 1 protect against loss 1 protect against resistance 1 protect against squall 1 protect against vagary 1 Generalization Result of MDL Verb Preposition Noun Class Probability protect against (act,human_action,human_activity) 0.212 protect against (phenomenon) 0.170 protect against (psychological_feature) 0.099 protect against (event) 0.097 protect against (abstraction) 0.093 Generalization Result of SA Verb Preposition Noun Class SA protect against protect against protect against protect against protect against protect against protect against protect against protect against protect against protect against protect against protect against protect against protect against (caprice,impulse,vagary, whim) 1.528 (phenomenon) 0.899 (happening,occurrence,natural_event) 0.339 (deterioration,worsening,decline,declination) 0.285 (act,human_action,human_activity) 0.260 (drop,bead,pearl) 0.202 (drop) 0.202 (descent,declivity, fall,decline,downslope) 0.188 (resistor, resistance) 0.130 (underground,resistance) 0.130 {immunity, resistance) O. 124 (resistance, opposition) 0.111 (loss,deprivation) 0.105 (loss) 0.096 (cost,price,terms,damage / 0.052 shown in Table 14 is rather odd, and is because the estimate of P(C) is unreliable (too small)." ></td>
	<td class="line x" title="341:393	This problem apparently costs SA a nonnegligible drop in disambiguation accuracy." ></td>
	<td class="line x" title="342:393	In contrast, MDL does not suffer from this problem since a high estimated probability value is only possible with high frequency, which cannot result just from extraction errors." ></td>
	<td class="line x" title="343:393	Consider, for example, the occurrence of car in the data shown in Figure 8, which has supposedly resulted from an erroneous extraction." ></td>
	<td class="line x" title="344:393	The effect of this datum gets washed away, as the estimated probability for VEHICLE, to which car has been generalized, is negligible." ></td>
	<td class="line x" title="345:393	On the other hand, SA has a merit not shared by MDL, namely its use of the association ratio factors out the effect of absolute frequencies of words, and focuses 238 Li and Abe Generalizing Case Frames Table 15 Some hard examples for LA. Attached to verb Attached to noun1 acquire interest in year buy stock in trade ease restriction on export forecast sale for year make payment on million meet standard for resistance reach agreement in august show interest in session win verdict in winter acquire interest in firm buy stock in index ease restriction on type forecast sale for venture make payment on debt meet standard for car reach agreement in principle show interest in stock win verdict in case on their co-occurrence relation." ></td>
	<td class="line x" title="346:393	Since both MDL and SA have pros and cons, it would be desirable to develop a methodology that combines the merits of the two methods (cf.Abe and Li \[1996\])." ></td>
	<td class="line x" title="348:393	MDL and LA. LA makes its disambiguation decision completely ignoring noun2." ></td>
	<td class="line x" title="349:393	As Resnik (1993b) pointed out, if we hope to improve disambiguation performance by increasing training data, we need a richer model such as those used in MDL and SA." ></td>
	<td class="line x" title="350:393	We found that 8.8% of the quadruples in our entire test data were such that they shared the same verb, prep, noun1 but had different noun2, and their PP-attachment sites go both ways in the same data, i.e., both to verb and to noun1." ></td>
	<td class="line x" title="351:393	Clearly, for these examples, the PP-attachment site cannot be reliably determined without knowing noun2." ></td>
	<td class="line x" title="352:393	Table 15 shows some of these examples." ></td>
	<td class="line x" title="353:393	(We adopted the attachment sites given in the Penn Tree Bank, without correcting apparently wrong judgements)." ></td>
	<td class="line x" title="354:393	MDL and TEL." ></td>
	<td class="line x" title="355:393	We chose TEL as an example of the quadruple approach." ></td>
	<td class="line x" title="356:393	This method was designed specifically for the purpose of resolving PP-attachment ambiguities, and seems to perform slightly better than ours." ></td>
	<td class="line oc" title="357:393	As we remarked earlier, however, the input data required by our method (triples) could be generated automatically from unparsed corpora making use of existing heuristic rules (Brent 1993; Smadja 1993), although for the experiments we report here we used a parsed corpus." ></td>
	<td class="line x" title="358:393	Thus it would seem to be easier to obtain more data in the future for MDL and other methods based on unsupervised learning." ></td>
	<td class="line x" title="359:393	Also note that our method of generalizing values of a case slot can be used for purposes other than disambiguation." ></td>
	<td class="line x" title="360:393	5." ></td>
	<td class="line x" title="361:393	Conclusions We proposed a new method of generalizing case frames." ></td>
	<td class="line x" title="362:393	Our approach of applying MDL to estimate a tree cut model in an existing thesaurus is not limited to just the problem of generalizing values of a case frame slot." ></td>
	<td class="line x" title="363:393	It is potentially useful in other natural language processing tasks, such as the problem of estimating n-gram models (Brown et al. 1992) or the problem of semantic tagging (Cucchiarelli and Velardi 1997)." ></td>
	<td class="line x" title="364:393	We believe that our method has the following merits: (1) it is theoretically sound; (2) it is computationally efficient; (3) it is robust against noise." ></td>
	<td class="line x" title="365:393	Our experimental results indicate that the performance of our method is better than, or at least comparable to, existing methods." ></td>
	<td class="line x" title="366:393	One of the disadvantages of our method is that its performance 239 Computational Linguistics Volume 24, Number 2 depends on the structure of the particular thesaurus used." ></td>
	<td class="line x" title="367:393	This, however, is a problem commonly shared by any generalization method that uses a thesaurus as prior knowledge." ></td>
	<td class="line x" title="368:393	Appendix A: Proof of Proposition 1 Proof For an arbitrary subtree T' of a thesaurus tree T and an arbitrary tree cut model M = (F,0) of T, let MT, = (FT,,0T,) denote the submodel of M that is contained in T'." ></td>
	<td class="line x" title="369:393	Also for any sample S and any subtree T' of T, let ST, denote the subsample of S contained in T'." ></td>
	<td class="line x" title="370:393	(Note that MT = M, ST = S)." ></td>
	<td class="line x" title="371:393	Then define, in general for any submodel MT, and subsample ST,, L(ST, \[ FT,, ~T') to be the data description length of subsample ST, using submodel MT,, L(~T, \[ FT,) to be the parameter description length for the submodel MT,, and L'(MT,,ST,) to be L(ST, I FT',~T') qL(~T, \[ FT,)." ></td>
	<td class="line x" title="372:393	(Note that, when calculating the parameter description length for a submodel, the sample size of the entire sample \]S\] is used)." ></td>
	<td class="line x" title="373:393	First note that for any (sub)tree T, (sub)model MT = (FT, ~T) contained in T, and (sub)sample ST contained in T, and T's child subtrees Ti : i = 1,, k, we have: k L(ST I PT, g) = L(ST, I PT,,g,) (17) i=1 provided that Fz is not a single node (root node of T)." ></td>
	<td class="line x" title="374:393	This follows from the mutual disjointness of the Ti, and the independence of the parameters in the Ti." ></td>
	<td class="line x" title="375:393	We also have, when T is a proper subtree of the thesaurus tree: k L(OT I FT) = ~ L(OT, I FT,)." ></td>
	<td class="line x" title="376:393	(18) i=1 Since the number of free parameters of a model in the entire thesaurus tree equals the number of nodes in the model minus one due to the stochastic condition (that the probability parameters must sum to one), when T equals the entire thesaurus tree, theoretically the parameter description length for a tree cut model of T should be: L(g I rr) = L r) k = L(0r, I rr,) i=1 log Isl (19) where ISI is the size of the entire sample." ></td>
	<td class="line x" title="377:393	Since the second term -~ in (19) is constant once the input sample S is fixed, for the purpose of finding a model with the minimum description length, it is irrelevant." ></td>
	<td class="line x" title="378:393	We will thus use the identity (18) both when T is the entire tree and when it is a proper subtree." ></td>
	<td class="line x" title="379:393	(This allows us to use the same recursive algorithm, Find-MDL, in all cases)." ></td>
	<td class="line x" title="380:393	It follows from (17) and (18) that the minimization of description length can be done essentially independently for each subtree." ></td>
	<td class="line x" title="381:393	Namely, if we let Clmin (MT, ST) denote the minimum description length (as defined by \[17\] and \[18\]) achievable for (sub)model Mr on (sub)sample ST contained in (sub)tree T, \[)s(~) the MLE estimate for node ~\] 240 Li and Abe Generalizing Case Frames using the entire sample S, and root(T) the root node of tree T, then we have: L~nin(MT, ST) min L~nin (MTi, ST i ), k i=1 L'( (\[root(T)\], \[Ps(root(T) )\]), ST) } (20) The rest of the proof proceeds by induction." ></td>
	<td class="line x" title="382:393	First, when T is of a single leaf node, the submodel consisting solely of the node and the MLE of the generation probability for the class represented by T is returned, which is clearly a submodel with minimum description length in the subtree T. Next, inductively assume that Find-MDL(T ~) correctly outputs a (sub)model with the minimum description length for any tree T' of size less than n. Then, given a tree T of size n whose root node has at least two children, say Ti : i = 1  k, for each Ti, Find-MDL(Ti) returns a (sub)model with the minimum description length by the inductive hypothesis." ></td>
	<td class="line x" title="383:393	Then, since (20) holds, whichever way the if-clause on lines 8, 9 of Find-MDL evaluates to, what is returned on line 11 or line 13 will still be a (sub)model with the minimum description length, completing the inductive step." ></td>
	<td class="line x" title="384:393	It is easy to see that the running time of the algorithm is linear in both the number of leaf nodes of the input thesaurus tree and the input sample size." ></td>
	<td class="line x" title="385:393	 Acknowledgments We are grateful to K. Nakamura and T. Fujita of NEC C&C Res." ></td>
	<td class="line x" title="386:393	Labs." ></td>
	<td class="line x" title="387:393	for their constant encouragement." ></td>
	<td class="line x" title="388:393	We thank K. Yaminishi and J. Takeuchi of C&C Res." ></td>
	<td class="line x" title="389:393	Labs." ></td>
	<td class="line x" title="390:393	for their suggestions and comments." ></td>
	<td class="line x" title="391:393	We thank T. Futagami of NIS for his programming efforts." ></td>
	<td class="line x" title="392:393	We also express our special appreciation to the two anonymous reviewers who have provided many valuable comments." ></td>
	<td class="line x" title="393:393	We acknowledge the ACL for providing the ACL/DCI CD-ROM, LDC of the University of Pennsylvania for providing the Penn Treebank corpus data, and Princeton University for providing WordNet, and E. Brill and P. Resnik for providing their PP-attachment disambiguation program." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P99-1029
Using Mutual Information To Resolve Query Translation Ambiguities And Query Term Weighting
Jang, Myung-Gil;Myaeng, Sung Hyon;Park, Se Young;"></td>
	<td class="line x" title="1:153	Using Mutual Information to Resolve Query Translation Ambiguities and Query Term Weighting 1 Myung-Gil Jang, 2 Sung Hyon Myaeng and 1 Se Young Park 1 Dept. of Knowledge Information, Electronics and Telecommunications Research Institute 161 Kajong-Dong, Yusong-Gu, Taejon, Korea 305-350 { mgjang, sypark } @etri.re.kr 2 Dept. of Computer Science, Chungnam National University 220 Gung-Dong, Yusong-Gu, Taejon, Korea 305-764 shmyaeng@cs.chungnam.ac.kr Abstract An easy way of translating queries in one language to the other for cross-language information retrieval (IR) is to use a simple bilingual dictionary." ></td>
	<td class="line x" title="2:153	Because of the generalpurpose nature of such dictionaries, however, this simple method yields a severe translation ambiguity problem." ></td>
	<td class="line x" title="3:153	This paper describes the degree to which this problem arises in Korean-English cross-language IR and suggests a relatively simple yet effective method for disambiguation using mutual information statistics obtained only from the target document collection." ></td>
	<td class="line x" title="4:153	In this method, mutual information is used not only to select the best candidate but also to assign a weight to query terms in the target language." ></td>
	<td class="line x" title="5:153	Our experimental results based on the TREC-6 collection shows that this method can achieve up to 85% of the monolingual retrieval case and 96% of the manual disambiguation case." ></td>
	<td class="line x" title="6:153	Introduction Cross-language information retrieval (IR) enables a user to retrieve documents written in diverse languages using queries expressed in his or her own language." ></td>
	<td class="line x" title="7:153	For cross-language IR, either queries or documents are translated to overcome the language differences." ></td>
	<td class="line x" title="8:153	Although it is possible to apply a high-quality machine translation system for documents as in Oard & Hackett (1997), query translation has emerged as a more popular method because it is much simpler and more economical compared to document translation." ></td>
	<td class="line x" title="9:153	Query translation can be done in one or more of the three approaches: a dictionary-based approach, a thesaurus-based approach, or a corpus-based approach." ></td>
	<td class="line x" title="10:153	There are three problems that a cross-language IR system using a query translation method must solve (Grefenstette, 1998)." ></td>
	<td class="line x" title="11:153	The first problem is to figure out how a term expressed in one language might be written in another." ></td>
	<td class="line x" title="12:153	The second problem is to determine which of the possible translations should be retained." ></td>
	<td class="line x" title="13:153	The third problem is to determine how to properly weight the importance of translation alternatives when more than one is retained." ></td>
	<td class="line x" title="14:153	For cross-language IR between Korean and English, i.e. between Korean queries and English documents, an easy way to handle query, translation is to use a Korean-English machinereadable dictionary (MRD) because such bilingual MRDs are more widely available than other resources such as parallel corpora." ></td>
	<td class="line x" title="15:153	However, it has been known that with a simple use of bilingual dictionaries in other language pairs, retrieval effectiveness can be only 40%60% of that with monolingual retrieval (Ballesteros & Croft, 1997)." ></td>
	<td class="line x" title="16:153	It is obvious that other additional resources need to be used for better performance." ></td>
	<td class="line x" title="17:153	This paper focuses on the last two problems: pruning translations and calculating the weights for translation alternatives." ></td>
	<td class="line x" title="18:153	We first describe the overall query translation process and the extent to which the ambiguity problem arises in Korean-English cross-language IR." ></td>
	<td class="line x" title="19:153	We then propose a relatively simple yet effective method for resolving translation disambiguation using mutual information (MI) (Church and Hanks, 1990) statistics obtained only from the target document collection." ></td>
	<td class="line x" title="20:153	In this method, mutual 223 information is used not only to select the best candidate but also to assign a weight to query terms in the target language." ></td>
	<td class="line x" title="21:153	1 Overall Query Translation Process Our Korean-to-English query translation scheme works in four stages: keyword selection, dictionary-based query translation, bilingual word sense disambiguation, and query term weighting." ></td>
	<td class="line x" title="22:153	Although none of the common resources such as dictionaries, thesauri, and corpora alone is complete enough to produce high quality English queries, we decided to use a bilingual dictionary at the second stage and a target-language corpus for the third and the fourth stages." ></td>
	<td class="line x" title="23:153	Our strategy was to try not to depend on scarce resources to make the approach practical." ></td>
	<td class="line x" title="24:153	Figure 1 shows the four stages of Korean-to-English query translation." ></td>
	<td class="line x" title="25:153	Korean Query Korean-to-English \[ Query Translation Keyword Selection English Query T Query Term I Bilingual Word I Disambiguation \[ Dictionary-Based 1 Query Translation Fig." ></td>
	<td class="line x" title="26:153	1." ></td>
	<td class="line x" title="27:153	Four Stages for Korean-to-English Query Translation." ></td>
	<td class="line x" title="28:153	1.1 Keyword Selection At the first stage, Korean keywords to be fed into the query translation process are extracted from a quasi-natural language query." ></td>
	<td class="line x" title="29:153	This keyword selection is done with a morphological analyzer and a stochastic part-of-speech (POS) tagger for the Korean language (Shin et al. , 1996)." ></td>
	<td class="line x" title="30:153	The role of the tagger is to help select the exact morpheme sequence from the multiple candidate sequences generated by the morphological analysis." ></td>
	<td class="line x" title="31:153	This process of employing a morphological analysis and a tagger is crucial for selecting legitimate query words from the topic statements because Korean is an agglutinative language." ></td>
	<td class="line x" title="32:153	Without the tagger, all the extraneous candidate keywords generated from the morphological analyzer will have to be entered into the translation process, which in and of itself will generate extraneous words, due to one-to-many mapping in the bilingual dictionary." ></td>
	<td class="line x" title="33:153	1.2 Dictionary-Based Query Translation The second stage does the actual query translation based on a dictionary look-up, by applying both word-by-word translation and phrase-level translation." ></td>
	<td class="line oc" title="34:153	For the correct identification of phrases in a Korean query, it would help to identify the lexical relations and produce statistical information on pairs of words in a text corpus as in Smadja (1993)." ></td>
	<td class="line x" title="35:153	Since the bilingual dictionary lacks some words that are essential for a correct interpretation of the Korean query, it is important to identify unknown words such as foreign words and transliterate them into English strings that need to be matched against an English dictionary (Jeong et al. , 1997)." ></td>
	<td class="line x" title="36:153	1.3 Selection of the Correct Translations At the word disambiguation stage, we filter out the extraneous words generated blindly from the dictionary lookup process." ></td>
	<td class="line x" title="37:153	In addition to the POS tagger, we employed a bilingual word disambiguation technique using the cooccurrence information extracted from the collection of target documents." ></td>
	<td class="line x" title="38:153	More specifically, The mutual information statistics between pairs of words were used to determine whether English words from different sets generated by the translation process are 'compatible'." ></td>
	<td class="line x" title="39:153	In a sense, we make use of mutual disambiguation effect among query terms." ></td>
	<td class="line x" title="40:153	More details are described in Section 3." ></td>
	<td class="line x" title="41:153	1.4 Query Term Weighting Finally, we apply our query term weighting technique to produce the final target query." ></td>
	<td class="line x" title="42:153	The term weighting scheme basically reflects the degree of associations between the translated terms, and we give a high or low term weighting value according to the degree of mutual association between query terms." ></td>
	<td class="line x" title="43:153	This is another area where we make use of mutual information obtained from a text corpus." ></td>
	<td class="line x" title="44:153	The result from the four stages is a set of query terms to be used in a 224 vector-space retrieval model." ></td>
	<td class="line x" title="45:153	2 Analysis of Translation Ambiguity Although an easy way to find translations of query terms is to use a bilingual dictionary, this method alone suffers from problems caused by translation ambiguity since there are often oneto-many correspondences in a bilingual dictionary." ></td>
	<td class="line x" title="46:153	For example, in a Korean query consisting of three words, ':Z\]-o--~-5~\]-~7\] _Q_~'(ja-dong-cha gong-gi oh-yum) that means air pollution caused by automobiles, each word can be translated into multiple English words when a Korean-English dictionary is used in a straightforward way." ></td>
	<td class="line x" title="47:153	The first word ':Z\]-o-~-5~\]-' (ja-dong-cha) of the query can be translated into English words with semantically similar but different words like 'motorcar', 'automobile', and 'car'." ></td>
	<td class="line x" title="48:153	The second word '--~-71' (gong-gi), a homonymous word, can be translated into English words with different meanings: 'air', 'atmosphere', 'empty vessel', and 'bowl'." ></td>
	<td class="line x" title="49:153	And the last word '_9--4' (oh-yum) can be translated into two English words, 'pollution' and 'contamination'." ></td>
	<td class="line x" title="50:153	Retaining multiple candidate words can be useful in promoting recall in monolingual IR system, but previous research indicates that failure to disambiguate the meanings of the words can hurt retrieval effectiveness tremendously." ></td>
	<td class="line x" title="51:153	For instance, it is obvious that a phrase like empty vessel would change the meaning of the query entirely." ></td>
	<td class="line x" title="52:153	Even a word like contamination, a synonym of pollution, may end up retrieving unrelated documents due to the slight differences in meaning." ></td>
	<td class="line x" title="53:153	Title Sho~ Long Table 1." ></td>
	<td class="line x" title="54:153	The De~ree of Ambiguities I \[Wrds I Wrd Pairs # in S. # in T. Average # in S. # in T. Average Lan." ></td>
	<td class="line x" title="55:153	Lang." ></td>
	<td class="line x" title="56:153	Ambiguity Lan." ></td>
	<td class="line x" title="57:153	Lang." ></td>
	<td class="line x" title="58:153	Ambiguity 48 158 I 3.29 \[ 29i 3212 8.83 112 447 3.99 1459 16.03 462 1835 3.97 6196 14.65 Table 1 shows the extent to which ambiguity occurs in our query translation when an EnglishKorean dictionary is used blindly after the morphological analysis and tagging." ></td>
	<td class="line x" title="59:153	The three rows, title, short, and long, indicate three different ways of composing queries from the topic statements in the TREC collection." ></td>
	<td class="line x" title="60:153	The left half shows the average number of English words per Korean word for each query, whereas the right half shows the average number of word pairs in English that can be formed from a single word pair in Korean." ></td>
	<td class="line x" title="61:153	The latter indicates that the disambiguation process will have to select one out of more than 9 possible pairs on the average, regardless of which part of the topic statements is used for formal query generation." ></td>
	<td class="line x" title="62:153	3 Query Translation and Mutual Information Our strategy for cross-language IR aims at practicality in that we try not to depend on scarce resources." ></td>
	<td class="line x" title="63:153	Along the same line of reasoning, we opted for a disambiguation approach that requires only a collection of documents in the target language, which is always available in any cross-language IR environment." ></td>
	<td class="line x" title="64:153	Since the goal of disambiguation is to select the best pair among many alternatives as described above, the mutual information statistic is a natural choice in judging the degree to which two words co-occur within a certain text boundary." ></td>
	<td class="line x" title="65:153	It would be reasonable to choose the pair of words that are most strongly associated with each other, thereby eliminating those translations that are not likely to be correct ones." ></td>
	<td class="line x" title="66:153	Mutual information values are calculated based on word co-occurrence statistics and used as a measure to calculate correlation between words." ></td>
	<td class="line x" title="67:153	The mutual information Ml(x,y) is defined as the following formula (Church and Hanks, 1990)." ></td>
	<td class="line x" title="68:153	p(x, y) N fw(X, y ) MI(x, y) = log 2 = log z (1) p(x)p(y) f(x)f(y) Here x and y are words occurring within a window of w words." ></td>
	<td class="line x" title="69:153	The probabilities p(x) and p(y) are estimated by counting the number of observations of x and y in a corpus, f(x) and fly), and normalizing each by N, the size of the corpus." ></td>
	<td class="line x" title="70:153	Joint probabilities, p(x,y), are estimated by counting the number of times, f,(x,y), that x is followed by y in a window of w words and normalizing it by N. In our application of query translation, the joint cooccurrence frequency f,(x,y) has 6-word window size which seems to allow semantic relations of query as well as fixed expressions (idioms such 225 as bread and butter)." ></td>
	<td class="line x" title="71:153	We ensure that the word x be followed by the word y within the same sentence only." ></td>
	<td class="line x" title="72:153	In our query translation scheme, MI values are used to select most likely translations after each Korean query word is translated into one or more English words." ></td>
	<td class="line x" title="73:153	Our use of MI values is based on the assumption that when two words co-occur in the same query, they are likely to cooccur in the same affinity in documents." ></td>
	<td class="line x" title="74:153	Conversely, two words that do not co-occur in the same affinity are not likely to show up in the same query." ></td>
	<td class="line x" title="75:153	In a sense, we are conjecturing mutual information can reveal some degree of semantic association between words." ></td>
	<td class="line x" title="76:153	Table 2 gives some examples of MI values for the alternative word pairs for translated queries of TREC-6 Cross-Language IR Track." ></td>
	<td class="line x" title="77:153	These MI values were extracted from the English text corpus consisting of 1988 1990 AP news, which contains 116,759,540 words." ></td>
	<td class="line x" title="78:153	Table 2." ></td>
	<td class="line x" title="79:153	Exam Word x Word y respiratory ailment teddy bear fossil fuel air pollution research development AIDS spread ivory trade environment protection bear doll region country point interest law terrorism treatment result terrorism government opinion news food life copy price labor information )le of Ml(x, Values fix) fiy) fix,y) I Ml(x,y) 716 1134 74 9.272506 679 7932 262 8.644690 676 13176 333 8.381424 52216 4878 890 6.011214 24278 24213 1317 5.566768 18575 10199 212 4.872597 1885 86608 84 4.095613 7771 13139 36 3.717652 7932 1394 3 3.455646 21093 103833 358 2.948925 30419 51917 107 2.068232 70182 4762 20 1.944089 13432 38055 22 1.614487 4762 193977 29 1.299005 9124 82220 21 1.184332 32222 40625 30 0.984281 6803 90594 10 0.638950 26571 30245 11 0.468861 When Ml(x,y) is large, the word associations are strong and produce credible results for disambiguation of translations." ></td>
	<td class="line x" title="80:153	However, if Ml(x,y) < 0, we can predict that the word x and word y are in complementary distribution." ></td>
	<td class="line x" title="81:153	4 Disambiguation and Weight Calculation We can alleviate the translation ambiguity by discriminating against those word pairs with low MI values." ></td>
	<td class="line x" title="82:153	The word pair with the highest MI value is considered to be the correct one among all the candidates in the two sets." ></td>
	<td class="line x" title="83:153	Since a query is likely to be targeted at a single concept, regardless of how broad or narrow it is, we conjecture that words describing the concept are likely to have a high degree of association." ></td>
	<td class="line x" title="84:153	Although we use the mutual information statistic to measure the association, others such as those used by Ballesteros & Croft (1998) can be considered." ></td>
	<td class="line x" title="85:153	In the example of Section 2, each Korean word has multiple English words due to translation ambiguity." ></td>
	<td class="line x" title="86:153	Figure 2 shows the MI values calculated for the word pairs comprising the translations of the original query." ></td>
	<td class="line x" title="87:153	The words under wl, w2, and w3 are the translations from the three query words, respectively." ></td>
	<td class="line x" title="88:153	The lines indicate that mutual information values are available for the pairs, and the numbers show some of the significant MI values for the corresponding pairs among all the possible pairs." ></td>
	<td class="line x" title="89:153	wl w2 w3 bowl Fig." ></td>
	<td class="line x" title="90:153	2." ></td>
	<td class="line x" title="91:153	An Example of Word Pairs with MI Values Our bilingual word disambiguation and weighting schemes rely on both relative and absolute magnitudes of the MI vales." ></td>
	<td class="line x" title="92:153	The algorithm first looks for the pair with the highest MI value and selects the best candidates before and after the pair by comparing the MI values for the pairs that are connected with the initially chosen pairs." ></td>
	<td class="line x" title="93:153	This process is applied to the words immediately before or after the chosen pair in order to limit the effect of the choice that may be incorrect." ></td>
	<td class="line x" title="94:153	It should be noted that the words not chosen in this process are not used in the translated query unless the MI values are greater than a threshold." ></td>
	<td class="line x" title="95:153	As described below, we assume that the candidates not in the first tier may still be useful if they are strongly associated with the adjacent word selected." ></td>
	<td class="line x" title="96:153	226 For example, the word pair <air, pollution> that has the bold line representing the strongest association in the column is choisen first." ></td>
	<td class="line x" title="97:153	Then the three MI values for the pairs containing air are compared to select the <automobile, air> pair, resulting in <automobile, air, pollution>." ></td>
	<td class="line x" title="98:153	If there were additional columns in the example, the same process would be applied to the rest of the network." ></td>
	<td class="line x" title="99:153	There are three reasons why query term weighting is of some value in addition to the pruning of conceptually unrelated terms." ></td>
	<td class="line x" title="100:153	First, our word selection method is not guaranteed to give the correct translation." ></td>
	<td class="line x" title="101:153	The method would give a reasonable result only when two consecutive query terms are actually used together in many documents, which is a hypothesis yet to be confirmed for its validity." ></td>
	<td class="line x" title="102:153	Second, there may be more than one strong association whose degrees are different from each other by a large magnitude." ></td>
	<td class="line x" title="103:153	Third, seemingly extraneous terms may serve as a recall-enhancing device with a query expansion effect." ></td>
	<td class="line x" title="104:153	The basic idea in our term weighting scheme is to give a large weight to the best candidate and divide the remaining quantity to assign equal weights to the rest of the candidates." ></td>
	<td class="line x" title="105:153	In other words, the weight for the best candidate, W~, is either 1 if it is greater than a threshold value or expressed as follows." ></td>
	<td class="line x" title="106:153	Wb = f(x) 0.5 + 0.5 (2) 0+1 Here x and 0 are a MI value and a threshold, respectively." ></td>
	<td class="line x" title="107:153	The numerator, f(x), gives the smallest integer greater than the MI value so that the resulting weight is the same for all the candidates whose MI values are within a certain interval." ></td>
	<td class="line x" title="108:153	Once the value for W b is calculated, the weight for the rest of the candidates are calculated as follows: Wr _ 1 W h (3) n-1 where n is the number of candidates." ></td>
	<td class="line x" title="109:153	It should be noted that W~ + Z W = 1." ></td>
	<td class="line x" title="110:153	Based on our observation of the calculated MI values, we chose to use 3.0 as the cut-off value in choosing the best candidate and assign a fairly high weight." ></td>
	<td class="line x" title="111:153	The cut-off value was determined purely based on the data we obtained; it can vary based on the new range of MI values when different corpora are used." ></td>
	<td class="line x" title="112:153	In the example of Fig." ></td>
	<td class="line x" title="113:153	2, the word pair candidate between wl and w2 are (motorcar, air), (automobile, air), and (car, air)." ></td>
	<td class="line x" title="114:153	Here because the weight of the word pairs (automobile, air) is W, = 0.83, the word 'automobile' has a relatively higher term weight than the other two words 'motorcar' and 'car'." ></td>
	<td class="line x" title="115:153	Finally the optimal English query set with their term weight, <(motocar,0.085), (automobile, 0.83), (car, 0.085) >, is generated for the translations of wl." ></td>
	<td class="line x" title="116:153	5 Experiments We developed a system for our cross-language IR techniques and conducted some basic experiments using the collection from the CrossLanguage Track of TREC 6." ></td>
	<td class="line x" title="117:153	The 24 English queries are comprised of three fields: titles, descriptions, and narratives." ></td>
	<td class="line x" title="118:153	These English queries were manually translated into Korean queries so that we can pretend as if the Korean queries had been generated by human users for cross-language IR." ></td>
	<td class="line x" title="119:153	In order to compare crosslanguage IR and mono-language IR, we used the Smart 11.0 system developed by Cornell University." ></td>
	<td class="line x" title="120:153	Our goal was to examine the efficacy of the disambiguation and term weighting schemes in our query translation." ></td>
	<td class="line x" title="121:153	We ran our system with three sets of queries, differentiated by the query lengths: 'title' queries with title fields only, 'short' queries with description fields only, and 'long' queries with all the three fields." ></td>
	<td class="line x" title="122:153	The retrieval effectiveness measured with l 1-point average precision was used for comparison against the baseline of monolingual retrieval using the original English query." ></td>
	<td class="line x" title="123:153	Table 3 gives the experimental results from using the four types of query set." ></td>
	<td class="line x" title="124:153	The result from 'Translated Query I' was generated only with the keyword selection and dictionary-based query translation stages." ></td>
	<td class="line x" title="125:153	The result 'Translated Query II' was generated after all the stages of our word disambiguation and query term weighting were done." ></td>
	<td class="line x" title="126:153	And the result from the manually disambiguated query set was generated by manually selecting the best candidate terms from the Translated Query I. 227 Query Sets Original Quer)' Tran." ></td>
	<td class="line x" title="127:153	Query I Tran." ></td>
	<td class="line x" title="128:153	Query II M.Disam." ></td>
	<td class="line x" title="129:153	Query Table 3." ></td>
	<td class="line x" title="130:153	Ex 1 ~erimental Results i Title Short \] Lon~ l lpt." ></td>
	<td class="line x" title="131:153	P C/M(~,:) l lpt." ></td>
	<td class="line x" title="132:153	P C/M('~) \[ l lpt." ></td>
	<td class="line x" title="133:153	P C/M(,~) 0.3251 0.3189 0.2821 0.2290 70.44 0.21443 67.20 0.1587 56.26 0.2675 82.28 0.2698 84.60 0.2232 79.12 0.2779 85.48 0.3002 94.14 0.2433 86.25 The performance of the Translated query set I was about 70%, 67%, and 56% of monolingual retrieval for the three cases, respectively." ></td>
	<td class="line x" title="134:153	The performances of the translated query set II were about 82%, 85%, and 79% of monolingual retrieval for the three cases, respectively." ></td>
	<td class="line x" title="135:153	The performance of the disambiguated queries, 85%, 94%, and 86% of monolingual retrieval for the three cases, respectively, can be treated as the upper limit for the cross-language retrieval." ></td>
	<td class="line x" title="136:153	The reason why they are not 100% is attributed to the several factors." ></td>
	<td class="line x" title="137:153	They are: 1) the inaccuracy of the manual translation of the original English query into the Korean queries, 2) the inaccuracy of the Korean morphological analyzer and the tagger in generating query words, and 3) the inaccuracy in generating candidate terms using the bilingual dictionary." ></td>
	<td class="line x" title="138:153	The difference between Translated Query I and Translated Query II indicates that the Ml-based disambiguation and the term weighting schemes are effective in enhancing the retrieval effectiveness." ></td>
	<td class="line x" title="139:153	In addition, the results show that the use of these query translation schemes is more effective with long queries than with shorter queries." ></td>
	<td class="line x" title="140:153	This is expected because the longer the queries are, the more contextual information can be used for mutual disambiguation." ></td>
	<td class="line x" title="141:153	Conclusion It has been known that query translation using a simple bilingual dictionary leads to a more than 40% drop in retrieval effectiveness due to translation ambiguity." ></td>
	<td class="line x" title="142:153	Our query translation method uses mutual information extracted from the 1988 1990 AP corpus in order to solve the problems of the bilingual word disambiguation and query term weighting." ></td>
	<td class="line x" title="143:153	The experiments using test collection of TREC-6 Cross-Language Track show that the method improves retrieval effectiveness in Korean-to-English crosslanguage IR." ></td>
	<td class="line x" title="144:153	The performance can be up to 85% of the monolingual retrieval case." ></td>
	<td class="line x" title="145:153	We also found that we obtained the largest percent increase with long queries." ></td>
	<td class="line x" title="146:153	While the experimental results are very promising, there are several issues to be explored." ></td>
	<td class="line x" title="147:153	First, we need to test how effectively the method can be applied." ></td>
	<td class="line x" title="148:153	Second, we intend to experiment with other co-occurrence metrics, instead of the mutual information statistic, for possible improvement." ></td>
	<td class="line x" title="149:153	This investigation is motivated by our observation of some counterintuitive MI values." ></td>
	<td class="line x" title="150:153	Third, we also plan on using different algorithms for choosing the terms and calculating the weights." ></td>
	<td class="line x" title="151:153	In addition, we plan to use the pseudo relevance feedback method that has been proven to be effective in monolingual retrieval." ></td>
	<td class="line x" title="152:153	Terms in some top-ranked documents are thrown into the original query with an assumption that at least some, if not all, of the documents are relevant to the original query and that the terms appearing in the documents are useful in representing user's information need." ></td>
	<td class="line x" title="153:153	Here we need to determine a threshold value for the number of top ranked document for our cross-language retrieval situation, let alone other phenomenon." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P99-1041
Automatic Identification Of Non-Compositional Phrases
Lin, Dekang;"></td>
	<td class="line x" title="1:131	Automatic Identification of Non-compositional Phrases Dekang Lin Department of Computer Science University of Manitoba and Winnipeg, Manitoba, Canada, R3T 2N2 lindek@cs.umanitoba.ca UMIACS University of Maryland College Park, Maryland, 20742 lindek@umiacs.umd.edu Abstract Non-compositional expressions present a special challenge to NLP applications." ></td>
	<td class="line x" title="2:131	We present a method for automatic identification of non-compositional expressions using their statistical properties in a text corpus." ></td>
	<td class="line x" title="3:131	Our method is based on the hypothesis that when a phrase is non-composition, its mutual information differs significantly from the mutual informations of phrases obtained by substituting one of the word in the phrase with a similar word." ></td>
	<td class="line x" title="4:131	1 Introduction Non-compositional expressions present a special challenge to NLP applications." ></td>
	<td class="line x" title="5:131	In machine translation, word-for-word translation of non-compositional expressions can result in very misleading (sometimes laughable) translations." ></td>
	<td class="line x" title="6:131	In information retrieval, expansion of words in a non-compositional expression can lead to dramatic decrease in precision without any gain in recall." ></td>
	<td class="line x" title="7:131	Less obviously, non-compositional expressions need to be treated differently than other phrases in many statistical or corpus-based NLP methods." ></td>
	<td class="line x" title="8:131	For example, an underlying assumption in some word sense disambiguation systems, e.g., (Dagan and Itai, 1994; Li et al. , 1995; Lin, 1997), is that if two words occurred in the same context, they are probably similar." ></td>
	<td class="line x" title="9:131	Suppose we want to determine the intended meaning of 'product' in 'hot product'." ></td>
	<td class="line x" title="10:131	We can find other words that are also modified by 'hot' (e.g. , 'hot car') and then choose the meaning of 'product' that is most similar to meanings of these words." ></td>
	<td class="line x" title="11:131	However, this method fails when non-compositional expressions are involved." ></td>
	<td class="line x" title="12:131	For instance, using the same algorithm to determine the meaning of 'line' in 'hot line', the words 'product', 'merchandise', 'car', etc. , would lead the algorithm to choose the 'line of product' sense of 'line'." ></td>
	<td class="line x" title="13:131	We present a method for automatic identification of non-compositional expressions using their statistical properties in a text corpus." ></td>
	<td class="line x" title="14:131	The intuitive idea behind the method is that the metaphorical usage of a non-compositional expression causes it to have a different distributional characteristic than expressions that are similar to its literal meaning." ></td>
	<td class="line x" title="15:131	2 Input Data The input to our algorithm is a collocation database and a thesaurus." ></td>
	<td class="line x" title="16:131	We briefly describe the process of obtaining this input." ></td>
	<td class="line x" title="17:131	More details about the construction of the collocation database and the thesaurus can be found in (Lin, 1998)." ></td>
	<td class="line x" title="18:131	We parsed a 125-million word newspaper corpus with Minipar, 1 a descendent of Principar (Lin, 1993; Lin, 1994), and extracted dependency relationships from the parsed corpus." ></td>
	<td class="line x" title="19:131	A dependency relationship is a triple: (head type modifier), where head and modifier are words in the input sentence and type is the type of the dependency relation." ></td>
	<td class="line x" title="20:131	For example, (la) is an example dependency tree and the set of dependency triples extracted from (la) are shown in (lb)." ></td>
	<td class="line x" title="21:131	compl John married Peter's sister b." ></td>
	<td class="line x" title="22:131	(marry V:subj:N John), (marry V:compl:N sister), (sister N:gen:N Peter) There are about 80 million dependency relationships in the parsed corpus." ></td>
	<td class="line x" title="23:131	The frequency counts of dependency relationships are filtered with the loglikelihood ratio (Dunning, 1993)." ></td>
	<td class="line x" title="24:131	We call a dependency relationship a collocation if its log-likelihood ratio is greater than a threshold (0.5)." ></td>
	<td class="line x" title="25:131	The number of unique collocations in the resulting database 2 is about 11 million." ></td>
	<td class="line x" title="26:131	Using the similarity measure proposed in (Lin, 1998), we constructed a corpus-based thesaurus 3 consisting of 11839 nouns, 3639 verbs and 5658 adjective/adverbs which occurred in the corpus at least 100 times." ></td>
	<td class="line x" title="27:131	3 Mutual Information of a Collocation We define the probability space to consist of all possible collocation triples." ></td>
	<td class="line x" title="28:131	We use LH R M L to denote the 1 available at http://www.cs.umanitoba.ca/-lindek/minipar.htm/ 2available at http://www.cs.umanitob&.ca/-lindek/nlldemo.htm/ 3available at http://www.cs.umanitoba.ca/-lindek/nlldemo.htm/ 317 frequency count of all the collocations that match the pattern (H R M), where H and M are either words or the wild card (*) and R is either a dependency type or the wild card." ></td>
	<td class="line x" title="29:131	For example,  \[marry V:ompl:N sister\[ is the frequency count of (marry V: compl :N sister)." ></td>
	<td class="line x" title="30:131	 \[marry V:compl:~ *1 is the total frequency count of collocations in which the head is marry and the type is V:compl:hi (the verb-object relation)." ></td>
	<td class="line x" title="31:131	 I* * *l is the total frequency count of all collocations extracted from the corpus." ></td>
	<td class="line x" title="32:131	To compute the mutual information in a collocation, we treat a collocation (head type modifier) as the conjunction of three events: A: (* type *) B: (head * *) C: (* * modifier) The mutual information of a collocation is the logarithm of the ratio between the probability of the collocation and the probability of events A, B, and C co-occur if we assume B and C are conditionally independent given A: (2) mutualInfo(head, type, modifier) P(A,B,c) = log P(B\[A)P(C\[A)P(A) \[head type modifier\[ * * *\] = log( \[, type *\[ \[head type *\[ \[* t~Te modifier\[ ) \[* * *\[ \[* type *1 \[*type *1 , \]head type modifier\[x * type * ---log,\]head type * x * type modifier / 4 Mutual Information and Similar Collocations In this section, we use several examples to demonstrate the basic idea behind our algorithm." ></td>
	<td class="line x" title="33:131	Consider the expression 'spill gut'." ></td>
	<td class="line x" title="34:131	Using the automatically constructed thesaurus, we find the following top-10 most similar words to the verb 'spill' and the noun 'gut': spill: leak 0.153, pour 0.127, spew 0.125, dump 0.118, pump 0.098, seep 0.096, burn 0.095, explode 0.094, burst 0.092, spray 0.091; gut: intestine 0.091, instinct 0.089, foresight 0.085, creativity 0.082, heart 0.079, imagination 0.076, stamina 0.074, soul 0.073, liking 0.073, charisma 0.071; The collocation 'spill gut' occurred 13 times in the 125-million-word corpus." ></td>
	<td class="line x" title="35:131	The mutual information of this collocation is 6.24." ></td>
	<td class="line x" title="36:131	Searching the collocation database, we find that it does not contain any collocation in the form (simvspilt V:compl:hl gut) nor (spill V: compl :N simngut), where sirnvsp~u is a verb similar to 'spill' and simng,,~ is a noun similar to 'gut'." ></td>
	<td class="line x" title="37:131	This means that the phrases, such as 'leak gut', 'pour gut', or 'spill intestine', 'spill instinct', either did not appear in the corpus at all, or did not occur frequent enough to pass the log-likelihood ratio test." ></td>
	<td class="line x" title="38:131	The second example is 'red tape'." ></td>
	<td class="line x" title="39:131	The top-10 most similar words to 'red' and 'tape' in our thesaurus are: red: yellow 0.164, purple 0.149, pink 0.146, green 0.136, blue 0.125, white 0.122, color 0.118, orange 0.111, brown 0.101, shade 0.094; tape: videotape 0.196, cassette 0.177, videocassette 0.168, video 0.151, disk 0.129, recording 0.117, disc 0.113, footage 0.111, recorder 0.106, audio 0.106; The following table shows the frequency and mutual information of 'red tape' and word combinations in which one of 'red' or 'tape' is substituted by a similar word: Table 1: red tape mutual verb object freq info red tape 259 5.87 yellow tape 12 3.75 orange tape 2 2.64 black tape 9 1.07 Even though many other similar combinations exist in the collocation database, they have very different frequency counts and mutual information values than 'red tape'." ></td>
	<td class="line x" title="40:131	Finally, consider a compositional phrase: 'economic impact'." ></td>
	<td class="line x" title="41:131	The top-10 most similar words are: economic: financial 0.305, political 0.243, social 0.219, fiscal 0.209, cultural 0.202, budgetary 0.2, technological 0.196, organizational 0.19, ecological 0.189, monetary 0.189; impact: effect 0.227, implication 0.163, consequence 0.156, significance 0.146, repercussion 0.141, fallout 0.141, potential 0.137, ramification 0.129, risk 0.126, influence 0.125; The frequency counts and mutual information values of 'economic impact' and phrases obtained by replacing one of 'economic' and 'impact' with a similar word are in Table 4." ></td>
	<td class="line x" title="42:131	Not only many combinations are found in the corpus, many of them have very similar mutual information values to that of 318 Table 2: economic impact verb economic financial political social budgetary ecological economic economic economic economic economic economic economic economic economic object impact impact impact impact impact impact effect implication consequence significance fallout repercussion potential ramification risk mutual freq info 171 1.85 127 1.72 46 0.50 15 0.94 8 3.20 4 2.59 84 0.70 17 0.80 59 1.88 10 0.84 7 1.66 7 1.84 27 1.24 8 2.19 17 -0.33 nomial distribution can be accurately approximated by a normal distribution (Dunning, 1993)." ></td>
	<td class="line x" title="43:131	Since all the potential non-compositional expressions that we are considering have reasonably large frequency counts, we assume their distributions are normal." ></td>
	<td class="line x" title="44:131	Let Ihead 1;ype modifier I = k and 1." ></td>
	<td class="line x" title="45:131	* .1 = n. The maximum likelihood estimation of the true probability p of the collocation (head type modifier) is /5 = ~." ></td>
	<td class="line x" title="46:131	Even though we do not know what p is, since p is (assumed to be) normally distributed, there is N% chance that it falls within the interval k_.4_ZN _ k.4_z N n,~, n V n n n n where ZN is a constant related to the confidence level N and the last step in the above derivation is due to the fact that k is very small." ></td>
	<td class="line x" title="47:131	Table 3 shows the z~ values for a sample set of confidence intervals." ></td>
	<td class="line x" title="48:131	'economic impact'." ></td>
	<td class="line x" title="49:131	In fact, the difference of mutual information values appear to be more important to the phrasal similarity than the similarity of individual words." ></td>
	<td class="line x" title="50:131	For example, the phrases 'economic fallout' and 'economic repercussion' are intuitively more similar to 'economic impact' than 'economic implication' or 'economic significance', even though 'implication' and 'significance' have higher similarity values to 'impact' than 'fallout' and 'repercussion' do." ></td>
	<td class="line x" title="51:131	These examples suggest that one possible way to separate compositional phrases and noncompositional ones is to check the existence and mutual information values of phrases obtained by substituting one of the words with a similar word." ></td>
	<td class="line x" title="52:131	A phrase is probably non-compositional if such substitutions are not found in the collocation database or their mutual information values are significantly different from that of the phrase." ></td>
	<td class="line x" title="53:131	5 Algorithm In order to implement the idea of separating noncompositional phrases from compositional ones with mutual information, we must use a criterion to determine whether or not the mutual information values of two collocations are significantly different." ></td>
	<td class="line x" title="54:131	Although one could simply use a predetermined threshold for this purpose, the threshold value will be totally arbitrary, b-hrthermore, such a threshold does not take into account the fact that with different frequency counts, we have different levels confidence in the mutual information values." ></td>
	<td class="line x" title="55:131	We propose a more principled approach." ></td>
	<td class="line x" title="56:131	The frequency count of a collocation is a random variable with binomial distribution." ></td>
	<td class="line x" title="57:131	When the frequency count is reasonably large (e.g. , greater than 5), a biTable 3: Sample ZN values IN% 150% 80% 90% 95% 98% 99% I zg 0.67 1.28 1.64 1.96 2.33 2.58 We further assume that the estimations of P(A), P(B\]A) and P(CIA ) in (2) are accurate." ></td>
	<td class="line x" title="58:131	The confidence interval for the true probability gives rise to a confidence interval for the true mutual information (mutual information computed using the true probabilities instead of estimations)." ></td>
	<td class="line x" title="59:131	The upper and lower bounds of this interval are obtained by substituting k with k+z~v'-g and k-z~vff in (2)." ></td>
	<td class="line x" title="60:131	Since our conn n n fidence of p falling between k+,~v~ is N%, we can I% have N% confidence that the true mutual information is within the upper and lower bound." ></td>
	<td class="line x" title="61:131	We use the following condition to determine whether or not a collocation is compositional: (3) A collocation a is non-compositional if there does not exist another collocation/3 such that (a) j3 is obtained by substituting the head or the modifier in a with a similar word and (b) there is an overlap between the 95% confidence interval of the mutual information values of a and f~." ></td>
	<td class="line x" title="62:131	For example, the following table shows the frequency count, mutual information (computed with the most likelihood estimation) and the lower and upper bounds of the 95% confidence interval of the true mutual information: freq." ></td>
	<td class="line x" title="63:131	mutual lower upper verb-object count info bound bound make difference 1489 2.928 2.876 2.978 make change 1779 2.194 2.146 2.239 319 Since the intervals are disjoint, the two collocations are considered to have significantly different mutual information values." ></td>
	<td class="line x" title="64:131	6 Evaluation There is not yet a well-established methodology for evaluating automatically acquired lexical knowledge." ></td>
	<td class="line x" title="65:131	One possibility is to compare the automatically identified relationships with relationships listed in a manually compiled dictionary." ></td>
	<td class="line x" title="66:131	For example, (Lin, 1998) compared automatically created thesaurus with the WordNet (Miller et al. , 1990) and Roget's Thesaurus." ></td>
	<td class="line x" title="67:131	However, since the lexicon used in our parser is based on the WordNet, the phrasal words in WordNet are treated as a single word." ></td>
	<td class="line x" title="68:131	For example, 'take advantage of' is treated as a transitive verb by the parser." ></td>
	<td class="line x" title="69:131	As a result, the extracted non-compositional phrases do not usually overlap with phrasal entries in the WordNet." ></td>
	<td class="line x" title="70:131	Therefore, we conducted the evaluation by manually examining sample results." ></td>
	<td class="line x" title="71:131	This method was also used to evaluate automatically identified hyponyms (Hearst, 1998), word similarity (Richardson, 1997), and translations of collocations (Smadja et al. , 1996)." ></td>
	<td class="line x" title="72:131	Our evaluation sample consists of 5 most frequent open class words in the our parsed corpus: {have, company, make, do, take} and 5 words whose frequencies are ranked from 2000 to 2004: {path, lock, resort, column, gulf}." ></td>
	<td class="line x" title="73:131	We examined three types of dependency relationships: object-verb, noun-noun, and adjective-noun." ></td>
	<td class="line x" title="74:131	A total of 216 collocations were extracted, shown in Appendix A. We compared the collocations in Appendix A with the entries for the above 10 words in the NTC's English Idioms Dictionary (henceforth NTC-EID) (Spears and Kirkpatrick, 1993), which contains approximately 6000 definitions of idioms." ></td>
	<td class="line x" title="75:131	For our evaluation purposes, we selected the idioms in NTC-EID that satisfy both of the following two conditions: (4) a. the head word of the idiom is one of the above 10 words." ></td>
	<td class="line x" title="76:131	b. there is a verb-object, noun-noun, or adjective-noun relationship in the idiom and the modifier in the phrase is not a variable." ></td>
	<td class="line x" title="77:131	For example, 'take a stab at something' is included in the evaluation, whereas 'take something at face value' is not." ></td>
	<td class="line x" title="78:131	There are 249 such idioms in NTC-EID, 34 of which are also found in Appendix A (they are marked with the '+' sign in Appendix A)." ></td>
	<td class="line x" title="79:131	If we treat the 249 entries in NTC-EID as the gold standard, the precision and recall of the phrases in Appendix A are shown in Table 4, To compare the performance with manually compiled dictionaries, we also compute the precision and recall of the entries in the Longman Dictionary of English Idioms (LDOEI) (Long and Summers, 1979) that satisfy the two conditions in (4)." ></td>
	<td class="line x" title="80:131	It can be seen that the overlap between manually compiled dictionaries are quite low, reflecting the fact that different lexicographers may have quite different opinion about which phrases are non-compositional." ></td>
	<td class="line x" title="81:131	Precision Recall Parser Errors Appendix A 15.7% 13.7% 9.7% LDOEI 39.4% 20.9% N.A. Table 4: Evaluation Results The collocations in Appendix A are classified into three categories." ></td>
	<td class="line x" title="82:131	The ones marked with '+' sign are found in NTC-EID." ></td>
	<td class="line x" title="83:131	The ones marked with 'x' are parsing errors (we retrieved from the parsed corpus all the sentences that contain the collocations in Appendix A and determine which collocations are parser errors)." ></td>
	<td class="line x" title="84:131	The unmarked collocations satisfy the condition (3) but are not found in NTC-EID." ></td>
	<td class="line x" title="85:131	Many of the unmarked collocation are clearly idioms, such as 'take (the) Fifth Amendment' and 'take (its) toll', suggesting that even the most comprehensive dictionaries may have many gaps in their coverage." ></td>
	<td class="line x" title="86:131	The method proposed in this paper can be used to improve the coverage manually created lexical resources." ></td>
	<td class="line x" title="87:131	Most of the parser errors are due to the incompleteness of the lexicon used by the parser." ></td>
	<td class="line x" title="88:131	For example, 'opt' is not listed in the lexicon as a verb." ></td>
	<td class="line x" title="89:131	The lexical analyzer guessed it as a noun, causing the erroneous collocation '(to) do opt'." ></td>
	<td class="line x" title="90:131	The collocation 'trig lock' should be 'trigger lock'." ></td>
	<td class="line x" title="91:131	The lexical analyzer in the parser analyzed 'trigger' as the -er form of the adjective 'trig' (meaning wellgroomed)." ></td>
	<td class="line x" title="92:131	Duplications in the corpus can amplify the effect of a single mistake." ></td>
	<td class="line x" title="93:131	For example, the following disclaimer occurred 212 times in the corpus." ></td>
	<td class="line x" title="94:131	'Annualized average rate of return after expenses for the past 30 days: not a forecast of future returns' The parser analyzed '% forecast of future returns' as \[S \[NP a forecast of future\] \[VP returns\]\]." ></td>
	<td class="line x" title="95:131	As a result, (return V:subj :N forecast) satisfied the condition (3)." ></td>
	<td class="line x" title="96:131	Duplications can also skew the mutual information of correct dependency relationships." ></td>
	<td class="line x" title="97:131	For example, the verb-object relationship between 'take' and 'bride' passed the mutual information filter because there are 4 copies of the article containing this phrase." ></td>
	<td class="line x" title="98:131	If we were able to throw away the duplicates and record only one count of 'take-bride', it would have not pass the mutual information filter (3)." ></td>
	<td class="line x" title="99:131	320 The fact that systematic parser errors tend to pass the mutual information filter is both a curse and a blessing." ></td>
	<td class="line x" title="100:131	On the negative side, there is no obvious way to separate the parser errors from true non-compositional expressions." ></td>
	<td class="line x" title="101:131	On the positive side, the output of the mutual information filter has much higher concentration of parser errors than the database that contains millions of collocations." ></td>
	<td class="line x" title="102:131	By manually sifting through the output, one can construct a list of frequent parser errors, which can then be incorporated into the parser so that it can avoid making these mistakes in the future." ></td>
	<td class="line x" title="103:131	Manually going through the output is not unreasonable, because each non-compositional expression has to be individually dealt with in a lexicon anyway." ></td>
	<td class="line x" title="104:131	To find out the benefit of using the dependency relationships identified by a parser instead of simple co-occurrence relationships between words, we also created a database of the co-occurrence relationship between part-of-speech tagged words." ></td>
	<td class="line x" title="105:131	We aggregated all word pairs that occurred within a 4-word window of each other." ></td>
	<td class="line x" title="106:131	The same algorithm and similarity measure for the dependency database are used to construct a thesaurus using the co-occurrence database." ></td>
	<td class="line x" title="107:131	Appendix B shows all the word pairs that satisfies the condition (3) and that involve one of the 10 words {have, company, make, do, take, path, lock, resort, column, gulf}." ></td>
	<td class="line oc" title="108:131	It is clear that Appendix B contains far fewer true non-compositional phrases than Appendix A. 7 Related Work There have been numerous previous research on extracting collocations from corpus, e.g., (Choueka, 1988) and (Smadja, 1993)." ></td>
	<td class="line n" title="109:131	They do not, however, make a distinction between compositional and noncompositional collocations." ></td>
	<td class="line x" title="110:131	Mutual information has often been used to separate systematic associations from accidental ones." ></td>
	<td class="line x" title="111:131	It was also used to compute the distributional similarity between words CHin dle, 1990; Lin, 1998)." ></td>
	<td class="line x" title="112:131	A method to determine the compositionality of verb-object pairs is proposed in (Tapanainen et al. , 1998)." ></td>
	<td class="line x" title="113:131	The basic idea in there is that 'if an object appears only with one verb (of few verbs) in a large corpus we expect that it has an idiomatic nature' (Tapanainen et al. , 1998, p.1290)." ></td>
	<td class="line x" title="114:131	For each object noun o, (Tapanainen et al. , 1998) computes the distributed frequency DF(o) and rank the non-compositionality of o according to this value." ></td>
	<td class="line x" title="115:131	Using the notation introduced in Section 3, DF(o) is computed as follows: DF(o) = ~ Iv,, v:compl:~, ol a n b i=1 where {vl,v2,,,vn} are verbs in the corpus that took o as the object and where a and b are constants." ></td>
	<td class="line x" title="116:131	The first column in Table 5 lists the top 40 verbobject pairs in (Tapanainen et ai., 1998)." ></td>
	<td class="line x" title="118:131	The 'mi' column show the result of our mutual information filter." ></td>
	<td class="line x" title="119:131	The '+' sign means that the verb-object pair is also consider to be non-compositional according to mutual information filter (3)." ></td>
	<td class="line x" title="120:131	The '-' sign means that the verb-object pair is present in our dependency database, but it does not satisfy condition (3)." ></td>
	<td class="line x" title="121:131	For each '-' marked pairs, the 'similar collocation' column provides a similar collocation with a similar mutual information value (i.e. , the reason why the pair is not consider to be non-compositional)." ></td>
	<td class="line x" title="122:131	The '<>' marked pairs are not found in our collocation database for various reasons." ></td>
	<td class="line x" title="123:131	For example, 'finish seventh' is not found because 'seventh' is normalized as '_NUM', 'have a go' is not found because 'a go' is not an entry in our lexicon, and 'take advantage' is not found because 'take advantage of' is treated as a single lexical item by our parser." ></td>
	<td class="line x" title="124:131	The ~/marks in the 'ntc' column in Table 5 indicate that the corresponding verb-object pairs is an idiom in (Spears and Kirkpatrick, 1993)." ></td>
	<td class="line x" title="125:131	It can be seen that none of the verb-object pairs in Table 5 that are filtered out by condition (3) is listed as an idiom in NTC-EID." ></td>
	<td class="line x" title="126:131	8 Conclusion We have presented a method to identify noncompositional phrases." ></td>
	<td class="line x" title="127:131	The method is based on the assumption that non-compositionai phrases have a significantly different mutual information value than the phrases that are similar to their literal meanings." ></td>
	<td class="line x" title="128:131	Our experiment shows that this hypothesis is generally true." ></td>
	<td class="line x" title="129:131	However, many collocations resulted from systematic parser errors also tend to posses this property." ></td>
	<td class="line x" title="130:131	Acknowledgements The author wishes to thank ACL reviewers for their helpful comments and suggestions." ></td>
	<td class="line x" title="131:131	This research was partly supported by Natural Sciences and Engineering Research Council of Canada grant OGP121338." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P99-1043
Mixed Language Query Disambiguation
Fung, Pascale N.;Xiaohu, Liu;Cheung, Chi-Shun;"></td>
	<td class="line x" title="1:167	Mixed Language Query Disambiguation Pascale FUNG, LIU Xiaohu and CHEUNG Chi Shun HKUST Human Language Technology Center Department of Electrical and Electronic Engineering University of Science and Technology, HKUST Clear Water Bay, Hong Kong {pascale, Ixiaohu, eepercy}@ee, ust." ></td>
	<td class="line x" title="2:167	hk Abstract We propose a mixed language query disambiguation approach by using co-occurrence information from monolingual data only." ></td>
	<td class="line x" title="3:167	A mixed language query consists of words in a primary language and a secondary language." ></td>
	<td class="line x" title="4:167	Our method translates the query into monolingual queries in either language." ></td>
	<td class="line x" title="5:167	Two novel features for disambiguation, namely contextual word voting and 1-best contextual word, are introduced and compared to a baseline feature, the nearest neighbor." ></td>
	<td class="line x" title="6:167	Average query translation accuracy for the two features are 81.37% and 83.72%, compared to the baseline accuracy of 75.50%." ></td>
	<td class="line x" title="7:167	1 Introduction Online information retrieval is now prevalent because of the ubiquitous World Wide Web." ></td>
	<td class="line x" title="8:167	The Web is also a powerful platform for another application--interactive spoken language query systems." ></td>
	<td class="line x" title="9:167	Traditionally, such systems were implemented on stand-alone kiosks." ></td>
	<td class="line x" title="10:167	Now we can easily use the Web as a platform." ></td>
	<td class="line x" title="11:167	Information such as airline schedules, movie reservation, car trading, etc. , can all be included in HTML files, to be accessed by a generic spoken interface to the Web browser (Zue, 1995; DiDio, 1997; Raymond, 1997; Fung et al. , 1998a)." ></td>
	<td class="line x" title="12:167	Our team has built a multilingual spoken language interface to the Web, named SALSA (Fung et al. , 1998b; Fung et al. , 1998a; Ma and Fung, 1998)." ></td>
	<td class="line x" title="13:167	Users can use speech to surf the net via various links as well as issue search commands such as 'Show me the latest movie of Jacky Chan'." ></td>
	<td class="line x" title="14:167	The system recognizes commands and queries in English, Mandarin and Cantonese, as well as mixed language sentences." ></td>
	<td class="line x" title="15:167	Until recently, most of the search engines handle keyword based queries where the user types in a series of strings without syntactic structure." ></td>
	<td class="line x" title="16:167	The choice of key words in this case determines the success rate of the search." ></td>
	<td class="line x" title="17:167	In many situations, the key words are ambiguous." ></td>
	<td class="line x" title="18:167	To resolve ambiguity, query expansion is usually employed to look for additional keywords." ></td>
	<td class="line x" title="19:167	We believe that a more useful search engine should allow the user to input natural language sentences." ></td>
	<td class="line x" title="20:167	Sentence-based queries are useful because (1) they are more natural to the user and (2) more importantly, they provide more contextual information which are important for query understanding." ></td>
	<td class="line x" title="21:167	To date, the few sentence-based search engines do not seem to take advantage of context information in the query, but merely extracting key words from the query sentence (AskJeeves, 1998; ElectricMonk, 1998)." ></td>
	<td class="line x" title="22:167	In addition to the need for better query understanding methods for a large variety of domains, it has also become important to handle queries in different languages." ></td>
	<td class="line x" title="23:167	Crosslanguage information retrieval has emerged as an important area as the amount of nonEnglish material is ever increasing (Oard, 1997; Grefenstette, 1998; Ballesteros and Croft, 1998; Picchi and Peters, 1998; Davis, 1998; Hull and Grefenstette, 1996)." ></td>
	<td class="line x" title="24:167	One of the important tasks of cross-language IR is to translate queries from one language to another." ></td>
	<td class="line x" title="25:167	The original query and the translated query are then used to match documents in both the source and target languages." ></td>
	<td class="line x" title="26:167	Target language documents are either glossed or translated by other systems." ></td>
	<td class="line x" title="27:167	According to (Grefenstette, 1998), three main problems of query translations are: 1." ></td>
	<td class="line x" title="28:167	generating translation candidates, 2." ></td>
	<td class="line x" title="29:167	weighting translation candidates, and 333 3." ></td>
	<td class="line x" title="30:167	pruning translation alternatives for document matching." ></td>
	<td class="line x" title="31:167	In cross-language IR, key word disambiguation is even more critical than in monolingual IR (Ballesteros and Croft, 1998) since the wrong translation can lead to a large amount of garbage documents in the target language, in addition to the garbage documents in the source language." ></td>
	<td class="line x" title="32:167	Once again, we believe that sentencebased queries provide more information than mere key words in cross-language IR." ></td>
	<td class="line x" title="33:167	In both monolingual IR and cross-language IR, the query sentence or key words are assumed to be consistently in one language only." ></td>
	<td class="line x" title="34:167	This makes sense in cases where the user is more likely to be a monolingual person who is looking for information in any language." ></td>
	<td class="line x" title="35:167	It is also easier to implement a monolingual search engine." ></td>
	<td class="line x" title="36:167	However, we suggest that the typical user of a cross-language IR system is likely to be bilingual to some extent." ></td>
	<td class="line x" title="37:167	Most Web users in the world know some English." ></td>
	<td class="line x" title="38:167	In fact, since English still constitutes 88% of the current web pages, speakers of another language would like to find English contents as well as contents in their own language." ></td>
	<td class="line x" title="39:167	Likewise, English speakers might want to find information in another language." ></td>
	<td class="line x" title="40:167	A typical example is a Chinese user looking for the information of an American movie, s/he might not know the Chinese name of that movie." ></td>
	<td class="line x" title="41:167	His/her query for this movie is likely to be in mixed language." ></td>
	<td class="line x" title="42:167	Mixed language query is also prevalent in spoken language." ></td>
	<td class="line x" title="43:167	We have observed this to be a common phenomenon among users of our SALSA system." ></td>
	<td class="line x" title="44:167	The colloquial Hong Kong language is Cantonese with mixed English words." ></td>
	<td class="line x" title="45:167	In general, a mixed language consists of a sentence mostly in the primary language with some words in a secondary language." ></td>
	<td class="line x" title="46:167	We are interested in translating such mixed language queries into monolingual queries unambiguously." ></td>
	<td class="line x" title="47:167	In this paper, we propose a mixed language query disambiguation approach which makes use of the co-occurrence information of words between those in the primary language and those in the secondary language." ></td>
	<td class="line x" title="48:167	We describe the overall methodology in Section 2." ></td>
	<td class="line x" title="49:167	In Sections 2.1-3, we present the solutions to the three disambiguation problems." ></td>
	<td class="line x" title="50:167	In Section 2.3 we present three different discriminative features for disambiguation, ranging from the baseline model (Section 2.3.1), to the voting scheme (Section 2.3.2), and finally the 1-best model (Section 2.3.3)." ></td>
	<td class="line x" title="51:167	We describe our evaluation experiments in Section 3, and present the results in Section 4." ></td>
	<td class="line x" title="52:167	We then conclude in Section 5." ></td>
	<td class="line x" title="53:167	2 Methodology Mixed language query translation is halfway between query translation and query disambiguation in that not all words in the query need to be translated." ></td>
	<td class="line x" title="54:167	There are two ways to use the disambiguated mixed language queries." ></td>
	<td class="line x" title="55:167	In one scenario, all secondary language words are translated unambiguously into the primary language, and the resulting monolingual query is processed by a general IR system." ></td>
	<td class="line x" title="56:167	In another scenario, the primary language words are converted into secondary language and the query is passed to another IR system in the secondary language." ></td>
	<td class="line x" title="57:167	Our methods allows for both general and crosslanguage IR from a mixed language query." ></td>
	<td class="line x" title="58:167	To draw a parallel to the three problems of query translation, we suggest that the three main problems of mixed language disambiguation are: 1." ></td>
	<td class="line x" title="59:167	generating translation candidates in the primary language, 2." ></td>
	<td class="line x" title="60:167	weighting translation candidates, and 3." ></td>
	<td class="line x" title="61:167	pruning translation alternatives for query translation." ></td>
	<td class="line oc" title="62:167	Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al. , 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al. , 1991; Dagan et al. , 1991; Dagan and Itai, 1994; Gale et al. , 1992a; Gale et al. , 1992b; Gale et al. , 1992c; Shiitze, 1992; Gale et al. , 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Ballesteros and Croft, 1998)." ></td>
	<td class="line oc" title="63:167	Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora (Smadja et al. , 1996; Kupiec, 1993; Wu, 1995; Tanaka and Iwasaki, 1996; Fung and Lo, 1998), or monolingual corpora (Smadja, 1993; Fung and Wu, 1994; Liu and Li, 1997; Shiitze, 1992; Yarowsky, 1995)." ></td>
	<td class="line x" title="64:167	As we noted in (Fung and Lo, 1998; Fung, 1998), parallel corpora are rare in most domains." ></td>
	<td class="line x" title="65:167	We want to devise a method that uses only monolingual data in the primary language to train co-occurrence information." ></td>
	<td class="line x" title="66:167	2.1 Translation candidate generation Without loss of generality, we suppose the mixed language sentence consists of the words S = (E1,E2,,C,,En}, where C is the only secondary language word 1." ></td>
	<td class="line x" title="67:167	Since in our method we want to find the co-occurrence information between all Ei and C from a monolingual corpus, we need to translate the latter into the primary language word Ec." ></td>
	<td class="line x" title="68:167	This corresponds to the first problem in query translation--translation candidate generation." ></td>
	<td class="line x" title="69:167	We generate translation candidates of C via an online bilingual dictionary." ></td>
	<td class="line x" title="70:167	All translations of secondary language word C, comprising of multiple senses, are taken together as a set {Eci }." ></td>
	<td class="line x" title="71:167	2.2 Translation candidate weighting Problem two in query translation is to weight all translation candidates for C. In our method, the weights are based on co-occurrence information." ></td>
	<td class="line x" title="72:167	The hypothesis is that the correct translations of C should co-occur frequently with the contextual words Ei and incorrect translation of C should co-occur rarely with the contextual words." ></td>
	<td class="line x" title="73:167	Obviously, other information such as syntactical relationship between words or the part-of-speech tags could be used as weights too." ></td>
	<td class="line x" title="74:167	However, it is difficult to parse and tag a mixed language sentence." ></td>
	<td class="line x" title="75:167	The only information we can use to disambiguate C is the co-occurrence information between its translation candidates { Ec, } and El, E2,." ></td>
	<td class="line x" title="76:167	., En." ></td>
	<td class="line x" title="77:167	Mutual information is a good measure of the co-occurrence relationship between two words (Gale and Church, 1993)." ></td>
	<td class="line x" title="78:167	We first compute the mutual information between any word pair from a monolingual corpus in the primary language 2 1In actual experiments, each sentence can contain multiple secondary language words 2This corpus does not need to be in the same domain as the testing data using the following formula, where E is a word and f (E) is the frequency of word E. MI(Ei, Ej) = log f(Ei, Ej) f(Ei) * f(Sj) (1) Ei and Ej can be either neighboring words or any two words in the sentence." ></td>
	<td class="line x" title="79:167	2.3 Translation candidate pruning The last problem in query translation is selecting the target translation." ></td>
	<td class="line x" title="80:167	In our approach, we need to choose a particular Ec from Ec~." ></td>
	<td class="line x" title="81:167	We call this pruning process translation disambiguation." ></td>
	<td class="line x" title="82:167	We present and compare three unsupervised statistical methods in this paper." ></td>
	<td class="line x" title="83:167	The first baseline method is similar to (Dagan et al. , 1991; Dagan and Itai, 1994; Ballesteros and Croft, 1998; Smadja et al. , 1996), where we use the nearest neighboring word of the secondary language word C as feature for disambiguation." ></td>
	<td class="line x" title="84:167	In the second method, we chQose all contextual words as disambiguating feature." ></td>
	<td class="line x" title="85:167	In the third method, the most discriminative contextual word is selected as feature." ></td>
	<td class="line x" title="86:167	2.3.1 Baseline: single neighboring word as disambiguating feature The first disambiguating feature we present here is similar to the statistical feature in (Dagan et al. , 1991; Smadja et al. , 1996; Dagan and Itai, 1994; Ballesteros and Croft, 1998), namely the co-occurrence with neighboring words." ></td>
	<td class="line x" title="87:167	We do not use any syntactic relationship as in (Dagan and Itai, 1994) because such relationship is not available for mixed-language sentences." ></td>
	<td class="line x" title="88:167	The assumption here is that the most powerful word for disambiguating a word is the one next to it." ></td>
	<td class="line x" title="89:167	Based on mutual information, the primary language target word for C is chosen from the set {Ec~}." ></td>
	<td class="line x" title="90:167	Suppose the nearest neighboring word for C in S is Ey, we select the target word Ecr, such that the mutual information between Ec~ and Ev is maximum." ></td>
	<td class="line x" title="91:167	r = argmaxiMI(Ec,, Ey) (2) Ev is taken to be either the left or the right neighbor of our target word." ></td>
	<td class="line x" title="92:167	This idea is illustrated in Figure 1." ></td>
	<td class="line x" title="93:167	MI1, represented by the solid line, is greater than MI2, 335 66 0 Word in the pr~ I~guagu Q ord in th secondary language Selected translation word MII > MI2 Figure 1: The neighboring word as disambiguating feature represented by the dotted line." ></td>
	<td class="line x" title="94:167	Ey is the neighboring word for C. Since MI1 is greater than MI2, Ecl is selected as the translation of C. 2.3.2 Voting: multiple contextual words as disambiguating feature The baseline method uses only the neighboring word to disambiguate C. Is one or two neighboring word really sufficient for disambiguation?" ></td>
	<td class="line x" title="95:167	The intuition for choosing the nearest neighboring word Ey as the disambiguating feature for C is based on the assumption that they are part of a phrase or collocation term, and that there is only one sense per collocation (Dagan and Itai, 1994; Yarowsky, 1993)." ></td>
	<td class="line x" title="96:167	However, in most cases where C is a single word, there might be some other words which are more useful for disambiguating C. In fact, such long-distance dependency occurs frequently in natural language (Rosenfeld, 1995; Huang et al. , 1993)." ></td>
	<td class="line x" title="97:167	Another reason against using single neighboring word comes from (Gale and Church, 1994) where it is argued that as many as 100,000 context words might be needed to have high disambiguation accuracy." ></td>
	<td class="line x" title="98:167	(Shfitze, 1992; Yarowsky, 1995) all use multiple context words as discriminating features." ></td>
	<td class="line x" title="99:167	We have also demonstrated in our domain translation task that multiple context words are useful (Fung and Lo, 1998; Fung and McKeown, 1997)." ></td>
	<td class="line x" title="100:167	Based on the above arguments, we enlarge the disambiguation window to be the entire sentence instead of only one word to the left or right." ></td>
	<td class="line x" title="101:167	We use all the contextual words in the query sentence." ></td>
	<td class="line x" title="102:167	Each contextual word 'votes' by its mutual information with all translation candidates." ></td>
	<td class="line x" title="103:167	Suppose there are n primary language words in S = E1,E2,,C,,En, as shown in Figure 2, we compute mutual information scores between all Ec~ and all Ej where Eci is one of the translation candidates for C and Ej is one of all n words in S. A mutual information score matrix is shown in Table 1." ></td>
	<td class="line x" title="104:167	whereMIjc~ is the mutual information score between contextual word Ej and translation candidate Eel." ></td>
	<td class="line x" title="105:167	E1 E2 o. Ej En Eel Ec2 MIlcl MIlc2 MI2cl MI2c2 Mljcl Mljc2 MIncl MInc2 oo Ec~  MIlcm  MI2cm  MXjc  Mlncm Table 1: Mutual information between all translation candidates and words in the sentence For each row j in Table 1, the largest scoring MIjci receives a vote." ></td>
	<td class="line x" title="106:167	The rest of the row get zero's." ></td>
	<td class="line x" title="107:167	At the end, we sum up all the one's in each column." ></td>
	<td class="line x" title="108:167	The column i receiving the highest vote is chosen as the one representing the real translation." ></td>
	<td class="line x" title="109:167	m m L~ c 0 0 Selected tramlntion Figure 2: Voting for the best translation To illustrate this idea, Table 2 shows that candidate 2 is the correct translation for C. There are four candidates of C and four contextual words to disambiguate C. E1 0 1 0 0 E2 1 0 0 0 E3 0 0 0 1 E4 0 1 0 0 Table 2: Candidate 2 is the correct translation 2.3.3 1-best contextual word as disambiguating feature In the above voting scheme, a candidate receives either a one vote or a zero vote from all contex336 tual words equally no matter how these words axe related to C. As an example, in the query 'Please show me the latest dianying/movie of Jacky Chan', the and Jacky are considered to be equally important." ></td>
	<td class="line x" title="110:167	We believe however, that if the most powerful word is chosen for disambiguation, we can expect better performance." ></td>
	<td class="line x" title="111:167	This is related to the concept of 'trigger pairs' in (Rosenfeld, 1995) and Singular Value Decomposition in (Shfitze, 1992)." ></td>
	<td class="line x" title="112:167	In (Dagan and Itai, 1994), syntactic relationship is used to find the most powerful 'trigger word'." ></td>
	<td class="line x" title="113:167	Since syntactic relationship is unavailable in a mixed language sentence, we have to use other type of information." ></td>
	<td class="line x" title="114:167	In this method, we want to choose the best trigger word among all contextual words." ></td>
	<td class="line x" title="115:167	Referring again to Table 1, Mljci is the mutual information score between contextual word Ej and translation candidate Ec~." ></td>
	<td class="line x" title="116:167	We compute the disambiguation contribution ratio for each context word Ej." ></td>
	<td class="line x" title="117:167	For each row j in Table 1, the largest MI score Mljc~ and the second largest MI score Mljc~ are chosen to yield the contribution for word Ej, which is the ratio between the two scores Mljc/ Contribution(Ej, Eci) = Mljc~ (3) If the ratio between MIjc/and MIjc~ is close to one, we reason that Ej is not discriminative enough as a feature for disambiguating C. On the other hand, if the ratio between MIie/i and MIie.~ is noticeably greater than one, we can use Ej as the feature to disambiguate {Ec~} with high confidence." ></td>
	<td class="line x" title="118:167	We choose the word Ey with maximum contribution as the disambiguating feature, and select the target word Ecr, whose mutual information score with Ey is the highest, as the translation for C. r = arg max MI(Ey, Ec,) (4) This method is illustrated in Figure 3." ></td>
	<td class="line x" title="119:167	Since E2 is the contextual word with highest contribution score, the candidate Ei is chosen that the mutual information between E2 and Eci is the largest." ></td>
	<td class="line x" title="120:167	3 Evaluation experiments The mutual information between co-occurring words and its contribution weight is obi  ''-." ></td>
	<td class="line x" title="121:167	~iI!j/J / Q Word ia the primary language Word in die seconda~ language Slectcd mutslalion of C Figure 3: The best contextual word as disambiguating feature tained from a monolingual training corpus-Wall Street Journal from 1987-1992." ></td>
	<td class="line x" title="122:167	The training corpus size is about 590MB." ></td>
	<td class="line x" title="123:167	We evaluate our methods for mixed language query disambiguation on an automatically generated mixedlanguage test set." ></td>
	<td class="line x" title="124:167	No bilingual corpus, parallel or comparable, is needed for training." ></td>
	<td class="line x" title="125:167	To evaluate our method, a mixed-language sentence set is generated from the monolingual ATIS corpus." ></td>
	<td class="line x" title="126:167	The primary language is English and the secondary language is chosen to be Chinese." ></td>
	<td class="line x" title="127:167	Some English words in the original sentences are selected randomly and translated into Chinese words manually to produce the testing data." ></td>
	<td class="line x" title="128:167	These axe the mixed language sentences." ></td>
	<td class="line x" title="129:167	500 testing sentences are extracted from the ARPA ATIS corpus." ></td>
	<td class="line x" title="130:167	The ratio of Chinese words in the sentences varies from 10% to 65%." ></td>
	<td class="line x" title="131:167	We carry out three sets of experiments using the three different features we have presented in this paper." ></td>
	<td class="line x" title="132:167	In each experiment, the percentage of primary language words in the sentence is incrementally increased at 5% steps, from 35% to 90%." ></td>
	<td class="line x" title="133:167	We note the accuracy of unambiguous translation at each step." ></td>
	<td class="line x" title="134:167	Note that at the 35% stage, the primary language is in fact Chinese." ></td>
	<td class="line x" title="135:167	4 Evaluation results One advantage of using the artificially generated mixed-language test set is that it becomes very easy to evaluate the performance of the disambiguation/translation algorithm." ></td>
	<td class="line x" title="136:167	We just need to compare the translation output with the original ATIS sentences." ></td>
	<td class="line x" title="137:167	The experimental results are shown in Figure 4." ></td>
	<td class="line x" title="138:167	The horizontal axis represents the percentage of English words in the testing data and the vertical axis represents the translation accuracy." ></td>
	<td class="line x" title="139:167	Translation accuracy is the ratio of the number of secondary language (Chinese) words disambiguated correctly over the number of all 337 secondary language (Chinese) words present in the testing sentences." ></td>
	<td class="line x" title="140:167	The three different curves represent the accuracies obtained from the baseline feature, the voting model, and the 1-best model." ></td>
	<td class="line x" title="141:167	O.85 1 i 0,8 VoOng ~-." ></td>
	<td class="line x" title="142:167	ba~ine .e.m B'' u   i i i i i i ~ia~ of primary l.a~uiita Words Figure 4: 1-best is the most discriminating feature We can see that both voting contextual words and the 1-best contextual words are more powerful discriminant than the baseline neighboring word." ></td>
	<td class="line x" title="143:167	The 1-best feature is most effective for disambiguating secondary language words in a mixed-language sentence." ></td>
	<td class="line x" title="144:167	5 Conclusion and Discussion Mixed-language query occurs very often in both spoken and written form, especially in Asia." ></td>
	<td class="line x" title="145:167	Such queries are usually in complete sentences instead of concatenated word strings because they are closer to the spoken language and more natural for user." ></td>
	<td class="line x" title="146:167	A mixed-language sentence consists of words mostly in a primary language and some in a secondary language." ></td>
	<td class="line x" title="147:167	However, even though mixed-languages are in sentence form, they are difficult to parse and tag because those secondary language words introduce an ambiguity factor." ></td>
	<td class="line x" title="148:167	To understand a query can mean finding the matched document, in the case of Web search, or finding the corresponding semantic classes, in the case of an interactive system." ></td>
	<td class="line x" title="149:167	In order to understand a mixed-language query, we need to translate the secondary language words into primary language unambiguously." ></td>
	<td class="line x" title="150:167	In this paper, we present an approach of mixed,language query disambiguation by using co-occurrence information obtained from a monolingual corpus." ></td>
	<td class="line x" title="151:167	Two new types of disambiguation features are introduced, namely voting contextual words and 1-best contextual word." ></td>
	<td class="line x" title="152:167	These two features are compared to the baseline feature of a single neighboring word." ></td>
	<td class="line x" title="153:167	Assuming the primary language is English and the secondary language Chinese, our experiments on English-Chinese mixed language show that the average translation accuracy for the baseline is 75.50%, for the voting model is 81.37% and for the 1-best model, 83.72%." ></td>
	<td class="line x" title="154:167	The baseline method uses only the neighboring word to disambiguate C. The assumption is that the neighboring word is the most semantic relevant." ></td>
	<td class="line x" title="155:167	This method leaves out an important feature of nature language: long distance dependency." ></td>
	<td class="line x" title="156:167	Experimental results show that it is not sufficient to use only the nearest neighboring word for disambiguation." ></td>
	<td class="line x" title="157:167	The performance of the voting method is better than the baseline because more contextual words are used." ></td>
	<td class="line x" title="158:167	The results are consistent with the idea in (Gale and Church, 1994; Shfitze, 1992; Yarowsky, 1995)." ></td>
	<td class="line x" title="159:167	In our experiments, it is found that 1-best contextual word is even better than multiple contextual words." ></td>
	<td class="line x" title="160:167	This seemingly counterintuitive result leads us to believe that choosing the most discriminative single word is even more powerful than using multiple contextual word equally." ></td>
	<td class="line x" title="161:167	We believe that this is consistent with the idea of using 'trigger pairs' in (Rosenfeld, 1995) and Singular Value Decomposition in (Shiitze, 1992)." ></td>
	<td class="line x" title="162:167	We can conclude that sometimes longdistance contextual words are more discriminant than immediate neighboring words, and that multiple contextual words can contribute to better disambiguation.Our results support our belief that natural sentence-based queries are less ambiguous than keyword based queries." ></td>
	<td class="line x" title="163:167	Our method using multiple disambiguating contextual words can take advantage of syntactic information even when parsing or tagging is not possible, such as in the case of mixed-language queries." ></td>
	<td class="line x" title="164:167	Other advantages of our approach include: (1) the training is unsupervised and no domaindependent data is necessary, (2) neither bilingual corpora or mixed-language corpora is needed for training, and (3) it can generate 338 monolingual queries in both primary and secondary languages, enabling true cross-language IR." ></td>
	<td class="line x" title="165:167	In our future work, we plan to analyze the various 'discriminating words' contained in a mixed language or monolingual query to find out which class of words contribute more to the final disambiguation." ></td>
	<td class="line x" title="166:167	We also want to test the significance of the co-occurrence information of all contextual words between themselves in the disambiguation task." ></td>
	<td class="line x" title="167:167	Finally, we plan to develop a general mixed-language and crosslanguage understanding framework for both document retrieval and interactive tasks." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W99-0610
Retrieving Collocations From Korean Text
Kim, Seonho;Yang, Zooil;Song, Mansuk;Ahn, Jung-Ho;"></td>
	<td class="line x" title="1:277	Retrieving Collocations From Korean Text Seonho Kim, Zooil Yang, Mansuk Song {pobi, zooil, mssong}@december.yonsei.ac.kr Dept. of Computer Science, Yonsei University, Seoul, Korea Jung-Ho Ahn jungho@math.yonsei.ac.kr Dept. of Mathematics, Yonsei University, Seoul, Korea Abstract This paper describes a statistical methodology ibr automatically retrieving collocations from POS tagged Korean text using interrupted bigrams." ></td>
	<td class="line x" title="2:277	The free order of Korean makes it hard to identify collocations." ></td>
	<td class="line x" title="3:277	We devised four statistics, 'frequency', 'randomness', 'condensation', and 'correlation'.to account for the more flexible word order properties of Korean collocations." ></td>
	<td class="line x" title="4:277	We extracted meaningful bigrams using an evaluation ihnction and extended the bigrams to n-gram collocations by generating equivalence sets, a-covers." ></td>
	<td class="line x" title="5:277	We view a modeling problem for n-gram collocations as that for clustering of cohesive words." ></td>
	<td class="line x" title="6:277	1 Introduction There have been many theoretical and applied works related to collocations." ></td>
	<td class="line x" title="7:277	A rapidly growing awfilability of copora has attracted interests m statistical methods for automatically extractmg :o\]loeations from textual corpora." ></td>
	<td class="line x" title="8:277	However, it is not easy to )dentify the central tendencies of collocation distribution and the borderlines of criteria are often fuzzy because the expressions can be of arbitrary lengths in a large variety of forms." ></td>
	<td class="line x" title="9:277	Getting reliable collocation patterns is particularly difficult in Korean which allows arguments to scamble so freely." ></td>
	<td class="line x" title="10:277	This paper presents a statistical method using 'interrupted bigrams' for automatically retrieving ~:ollocations and idiomatic expressions from Korean text." ></td>
	<td class="line x" title="11:277	We suggest several statistics to account for the more flexible word order." ></td>
	<td class="line x" title="12:277	If the distribution of a random sample is unknown, we often try to make inferences about its properties described by suitably defined measures." ></td>
	<td class="line x" title="13:277	For the properties of arbitrary collocation distribution, four measure statistics: 'high frequency', 'condensation', 'randomness', and 'correlation' were devised." ></td>
	<td class="line x" title="14:277	Given a morpheme, our system begins by retrieving the frequency distributions of all bigrams within window and then meaningful bigrams are extracted." ></td>
	<td class="line x" title="15:277	We produce a-covers to extend them into n-gram collocations 1 According to the definition of Kjellmer and Cowie, a fossilized phrase is a sequence, where the occurrence of one word almost predicts the rest of the phrase and one word predicts a very limited number of words in a semi-fossilized phrase (Kjellmer, 1995) (Cowie, 1981)." ></td>
	<td class="line x" title="16:277	However, in both fossilized and semi-fossilized types there is a high degree of cohesion among the members of the phrases (Kjellmer, 1995)." ></td>
	<td class="line x" title="17:277	We consider the cohesions as a-covers that are obtained by applying a fuzzy compatibility relation, which satisfies symmetry and reflexivity, to meaningful bigrams." ></td>
	<td class="line x" title="18:277	Namely, n-gram collocations could be interpreted as equivalent sets of the meaningful bigrams through partitioning." ></td>
	<td class="line x" title="19:277	Here, a-covers mean the clustered sets of the meaningful bigrams." ></td>
	<td class="line x" title="20:277	2 Related Works In determining properties of collocations, most of corpus-based approaches accepted that the words of a collocation have a particular statistical distribution(Cruse, 1986)." ></td>
	<td class="line x" title="21:277	Although previous approaches have shown good results in retrieving collocations and many properties have been identified, they depend heavily on the frequency factor." ></td>
	<td class="line x" title="22:277	(Choueka et al. , 1983) proposed an algorithm for retrieving only uninterrupted collocations, 2 IBigrams and n-grams can be either adjacent morphemes or separated morphems by an arbitrary number of other words." ></td>
	<td class="line x" title="23:277	2In the case of an interrupted collocation, words can be separated by an arbitrary number of words, whereas 71 sin(:e,:hey assumed that a collocation is a se-,lu'n(:e of adjacent words that frequently apl:,(~ar t~)gether." ></td>
	<td class="line x" title="24:277	(Church and Hanks, 1989) delhw:(I ;t collocation as a pair of correlated words :m(i,,se(t mutual information to evaluate such \],~xi(:a,1 (:orrelations of word pairs of length two." ></td>
	<td class="line x" title="25:277	They retrieved interrupted word pairs, as well as,minterrupted word pairs." ></td>
	<td class="line x" title="26:277	(Haruno et al. , 1996),:onstructed collocations by combining adjacent n-grams with high value of mutual information." ></td>
	<td class="line x" title="27:277	(Brei(lt, 1993)'s study was motivated by the fact than mutual information could not give realistic figures to low fl'equencies and used t-score for a significance test for V-N combinations." ></td>
	<td class="line x" title="28:277	Martin noted that a span of 5 words on left nnd right sides captures 95% of significant collo(:ations in English (Martin, 1983)." ></td>
	<td class="line oc" title="29:277	Based on this assumption, (Smadja, 1993) stored all bigrams of words along with their relative position, p (-5 < p _~ 5)." ></td>
	<td class="line o" title="30:277	He evaluated the lexical strength of a word pair using 'Z-score' and the variance of its t)osil;ion distribution using '.spread'." ></td>
	<td class="line o" title="31:277	He defined ~,." ></td>
	<td class="line o" title="32:277	(:()\]location as an arbitary, domain dependent, recurrent, and cohesive lexical cluster." ></td>
	<td class="line x" title="33:277	(Nagao and Mori, 1994) developed an algorithm tbr calculating adjacent n-grams to an ar1)itrary large number of n. However, it was hard to find an efficient n and a lot of fragments were obtained." ></td>
	<td class="line x" title="34:277	In Korean, statistics based on adjacent n-grams is not sufficient to capture various types of collocations." ></td>
	<td class="line x" title="35:277	(Shimohata et al. , 1997) employed entropy value to filter out fragments of the adjacent n-gram model." ></td>
	<td class="line x" title="36:277	They evaluated disorderness with the distribution of adjacent words preceding and following a string." ></td>
	<td class="line x" title="37:277	The strings with a high value of entropy were ac(:epted as collocations." ></td>
	<td class="line x" title="38:277	This disorderness is effi(:ient to eliminate fragments but can not han(lle interrupted collocations." ></td>
	<td class="line x" title="39:277	In general, previous ;studies on collocations have dealt with restricted types and depend on filtering measures in a lexically point of view." ></td>
	<td class="line x" title="40:277	3 Input Format In this section, we discuss an input form relevant to Korean language structure and linguistic contents which would work well on an effimfinterrupted collocation is a sequence of words To ~tvoid confusion of terms, we call a sequence of two words as ~ 'a(ljacent bigram' and a sequence of n words as a  ad?accnt n-gram ~." ></td>
	<td class="line x" title="41:277	72 cient statistics." ></td>
	<td class="line x" title="42:277	Korean is one of agglutinative languages as well as a propositional language." ></td>
	<td class="line x" title="43:277	An elementary node being called as 'eojeol' is generally composed of a content word and function words." ></td>
	<td class="line x" title="44:277	Namely, a word in English corresponds to a couple of morphemes in Korean." ></td>
	<td class="line x" title="45:277	A key feature of Korean is that hmction words, such as propositions, endings, copula, auxiliary verbs, and particles, are highly developed as independent morphemes, while they are represented as word order or inflections in English." ></td>
	<td class="line x" title="46:277	Functional morphemes determine grammatical relations, tense, modal, and aspect." ></td>
	<td class="line x" title="47:277	In Korean, there are lots of multiple function words in a rigid forms." ></td>
	<td class="line x" title="48:277	They can be viewed as collocations." ></td>
	<td class="line x" title="49:277	For this reason, our system is designed at the morphological level." ></td>
	<td class="line x" title="50:277	A set of twelve part of speech tags, { N, J, V, P, D, E, T, O, C, A, S, X } 3 was considered." ></td>
	<td class="line x" title="51:277	Another feature is a free word order." ></td>
	<td class="line x" title="52:277	Since the words of a collocation appear in text with the flexible ways, sufficient samples are required to compute accurate probabilities." ></td>
	<td class="line x" title="53:277	We allow positional information to vary by using an interrupted bigram model." ></td>
	<td class="line x" title="54:277	The basic input can be represented in (1)." ></td>
	<td class="line x" title="55:277	An object k means a pair of morphemes (mi,mk) and mk corresponds to one of all possible morphemes, being able to co-occur with mi." ></td>
	<td class="line x" title="56:277	A variable j indicates the j-th position." ></td>
	<td class="line x" title="57:277	Xij denotes the frequency of mk that occurs at the j-th position before mi." ></td>
	<td class="line x" title="58:277	Xi1 X12 X21 X2~ Xi = . . Xnl Xn2 Given a predicate Xll / X210 XnlO (1) morpheme as a base morpheme, the range of window is from -1 to -10." ></td>
	<td class="line x" title="59:277	This distance constraint is for the characteristic of SOV language." ></td>
	<td class="line x" title="60:277	If a bigram includes an adverb morpheme, a larger window, from -20 to 10 is used because the components often appear widely separated from each other on text." ></td>
	<td class="line x" title="61:277	In other cases, we considered the range from -5 to +5." ></td>
	<td class="line x" title="62:277	This distant constraints are for an efficient statistics." ></td>
	<td class="line x" title="63:277	An input data is transformed to a property matrix, T(Xi) as (2) that is a two dimensional 3'Noun','adJective','Verb','Postposition', 'aDverb', 'Ending','pre-ending', 'cOpular', 'Conjunction', 'Auxiliary verb', 'Suffix', 'etc.'." ></td>
	<td class="line x" title="64:277	Cn~,m,) (o}~1,o~ol) (drink,much) (3}~l,t--t ~ = ) (drink,too) (Ot h l,gt) (~nk,~lcan) (3t,~l,OH J) (drink, everyday) (fl\[hl,~OI) (drink,boil) (OtJ, l,~) (drink,,iot) (Ot~,l,@~l) (drink,t.bgether) (0~1,~ ==~) (drink, a tittle) (OH,~) (drink,take) (OtAI,_~) (drink, a little) syntactic relation VD VD VJ VD W VD VD VD W VD preferring position 1 2 4 3 2 1 3 1 2 3 Figure 1: meaningful bigrams of ~\[z\](drink) by Xtract ~rr~:~y of k object.s, k = 1,2,,n, on four vari~t,|)les, V Frequency ~ VCondensation, V Randomness, ~md Vcorrelatio n. F c R cR T(Xi)= '." ></td>
	<td class="line oc" title="65:277	~F ~c ~R ~cR (2) ~\]~) continue explanations, we begin by mentioning the 'Xtrgct' tool by Smadja (Smadja, 1993)." ></td>
	<td class="line o" title="66:277	Our input form was designed in a similar manner with 'Xtract'." ></td>
	<td class="line o" title="67:277	Smadja assumed that the components of a collocation should appear together in a relatively rigid way because of syntactic constraint." ></td>
	<td class="line x" title="68:277	Namely, a bigram pair (mi, 'rnk), where mk Occurs at one(or several) spe(:ific position around mi, would be a meaningful bigrams for collocations." ></td>
	<td class="line x" title="69:277	The rigid word order is related with the variance of frequency distribution of (mi, ink)." ></td>
	<td class="line o" title="70:277	'Xtract' extracted the pairs whose variances are over a threshold and pulled out the interesting positions of them by standar(lizz~tion of the frequency distributions." ></td>
	<td class="line n" title="71:277	Unfbrtunately, the approach for English has several limitations to work 4 on Korean structure ibr the following 'reasons: 1." ></td>
	<td class="line x" title="72:277	For free order languages such as Korean, words are widely distributed in text, so that positional variance affects the overtiltering of Useful bigrams." ></td>
	<td class="line x" title="73:277	Figure 1 shows that there is no pair which contains randomly distributed morphems such as function words or nouns." ></td>
	<td class="line n" title="74:277	This indicates that very few pairs were produced when 'Xtract' is applied to Korean." ></td>
	<td class="line o" title="75:277	4~We ported Smadja's Xtract tool into a Korean version." ></td>
	<td class="line x" title="76:277	73 2." ></td>
	<td class="line x" title="77:277	Suppose that a meaning bigram, (mi, mk) prefers a position pj." ></td>
	<td class="line x" title="78:277	Then, the number of concordances for condition probability P(mi, rnklpj) would be small, specially in a free order language." ></td>
	<td class="line x" title="79:277	As shown in Table 1, the model produced a lot of long meaningless n-grams when compiling into n-grams." ></td>
	<td class="line o" title="80:277	The precision value of Korean version of Xtract was estimated to be 40.4%." ></td>
	<td class="line x" title="81:277	3." ></td>
	<td class="line x" title="82:277	The eliminated bigrams by the previous stage can appear again in n-gram collocations." ></td>
	<td class="line x" title="83:277	When compiling, the model only keeps the words occupying the position with a probability greater than a given threshold from the concordances of (mi, ink, pj)." ></td>
	<td class="line x" title="84:277	As one might imagine, the first stage could be useless." ></td>
	<td class="line x" title="85:277	As stated above, in Korean, the effect of position on collocations needs to be treated in some complex ways." ></td>
	<td class="line x" title="86:277	Korean collocations can be divided into four types: 'idiom' 5, 'semantic collocation' 6, 'syntactic collocation' 7 and 'morphological collocation' s. Idioms and morphological collocations appear on text in a rigid way and word order but others do in the flexible ways." ></td>
	<td class="line x" title="87:277	From a consideration of these more flexible collocations, we adopt an interrupted bigram model and suggest several statistics that consist with characteristics of Korean." ></td>
	<td class="line x" title="88:277	4 Algorithm This section describes how properties are represented as numerical values and how meaningful objects are retrieved." ></td>
	<td class="line x" title="89:277	In the first stage, we extract meaningful interrupted bigrams based on four properties." ></td>
	<td class="line x" title="90:277	Next, the meaningful bigrams are extended into n-gram collocations using a a-compatibility relation." ></td>
	<td class="line x" title="91:277	It empirically showed that a Weibull distribution (3) provides a close approximation of frequency distribution of bigrams." ></td>
	<td class="line x" title="92:277	F(x)=l-~ -'*~ o<x<~ ~h~ ~>0,~>0 (3) 5Idioms have no ambiguous meaning but requires rigid patterns to preserve the idiomatic meaning." ></td>
	<td class="line x" title="93:277	6The replacement of some components by other words is more free than idioms." ></td>
	<td class="line x" title="94:277	~The combination of words is affected by selectionM restrictions of predicate, noun, or adverb." ></td>
	<td class="line x" title="95:277	sit corresponds to multiple function word and appears on a adjacent word group." ></td>
	<td class="line x" title="96:277	I'r('(l II 12 '2 n-grams  _7_@(everyone)." ></td>
	<td class="line x" title="97:277	Noun-~-(objeet case) ((u,d-))o--I(take)(@l-z,l))(drink) . ." ></td>
	<td class="line x" title="98:277	((,~o\]))(much) (@l-z\]))(drink) . Noun  -7-(two) ct-N(legs) ~\](at(location case)) ~(a little) ~ ~ ~(strain) x-l(stand) o-lx-I(with ~ing) ~(f'riend) N'N (with) 1t 71 .~~\].~-l-(talk over) ~ x-I (~ing) ((~--I))(a little) ~1 ~l (coffee) ~(object case) ((-I-xl))(drink) ~(modifying ending) ~(dream) . .  . . @(two) z\]~l-(hours) ~(object case) ((~Nl))(together) ~(alcohol) .~(object case) ((nl-xl))(drink) ~-(modi(ying ending) .2.(he) . . ." ></td>
	<td class="line x" title="99:277	N-el-(cola) ~(object case) ((~'N'N))(a little) ((-I-zl))(drink) o-I 71-~l(~ing)   I-(I) ~(also) '-tlN(baeklim(location)) ~lx-I(at) ((,lt~))(everyday) ~l~(tears) ~(object case) (@Pxl))(dink) ~,I ~(was ~ing) 71 ~l(beeause)   . . L+(I) ~7\](here) -U-el(specially) zll~(dawn) oJ\](at) ((-N-})(fresh) -~-(modifying ending) -~-(water) Y_(also) ((u\[-q))(drink) . Oc-~. ~(that) -~--~\[-(be unsuitable) . 7~-J-(most) . ~ N~z\](says) ~r---~-(exercise) ~-(well)  . Verb . . Noun .~-(object case) ((uJ-~-))(too) ~.~\]:o\](much) ((-~-z\]))(drink) -G-(modifying ending)   . . Noun Noun Noun ~-otix\](in) ((-~o\]))(boil) ~ (@\]-zi))(drink) .~ Noun   -~-~1-7-." ></td>
	<td class="line x" title="100:277	(even though)." ></td>
	<td class="line o" title="101:277	((~))(not) (@},q))(drink),.~_(and) Noun Noun . Verb Ending Verb . . Table 1: n-grams of =l-zl(drink) by Xtract (freq: freq of sentences) dist eval -2 O -1 O -3 X -3 O -I O -3 X -4 X -2 O -2 X -1 O Thus, there are a lot of pairs with low frequency which interrupt to get reliable statistic." ></td>
	<td class="line x" title="102:277	We clinfinated such pairs using median m that is a. value such that PX > m} > 1/2 to a frequency distribution F. If median is less than 3, we took the value 3 as a median." ></td>
	<td class="line x" title="103:277	Any quantity that depends on not any unknown parameters of population distribution but only the sample is called a statistic." ></td>
	<td class="line x" title="104:277	We regarded four statistics relating to properties of (:ollocations as variables." ></td>
	<td class="line x" title="105:277	Before the further explauation, consider Sm~, a sample space of mi as Table 2 whose cardinality \]Sm~l is n. Let one ob.iect be (mi, mk) and its frequency distribution be ./}k,,.rik2,''',fiklO and,::k+ be ~pl_l like." ></td>
	<td class="line x" title="106:277	Suppose that POS(mi) is J and POS(mk) 's p'." ></td>
	<td class="line x" title="107:277	4.1 Properties The properties which we considered are primarily concerned with the frequency and positional infbrmations of word pairs." ></td>
	<td class="line x" title="108:277	As we have emphasized, the correlation between position and (:ollocation is very complicated in Korean." ></td>
	<td class="line x" title="109:277	According to Breidt, MI or T-score thresholds work satisfactory as a filter for extraction of collocations, but filtered out at least half of the actual collocations (Breidt, 1993)." ></td>
	<td class="line x" title="110:277	Generally, assumed properties could not fully account tbr collocations." ></td>
	<td class="line x" title="111:277	Therefore, in order to reduce a h)ss of infbrmation, the combination of observed vaxiables would be better than filtering." ></td>
	<td class="line x" title="112:277	We delined tbur variables for properties of collocations 1." ></td>
	<td class="line x" title="113:277	Vi: 2." ></td>
	<td class="line x" title="114:277	~: as follows." ></td>
	<td class="line x" title="115:277	According to Benson's definition, a collocation is a recurrent word combination (Benson et al. , 1986)." ></td>
	<td class="line x" title="116:277	We agree with this view that a word pair of high frequency would be served as a collocation." ></td>
	<td class="line x" title="117:277	Vf statistic of an object (mi, ink), is represented as (4)." ></td>
	<td class="line x" title="118:277	Here, standardization demands attention." ></td>
	<td class="line x" title="119:277	The mean and standard deviation are calculated in the 'JP' set which the object belongs to." ></td>
	<td class="line x" title="120:277	Vf = fik+--fijp ffijp ' n El fil+ A++JJP = %' = n, (4) aij P = 1=1 (fd f,jp) Intuitively, two words that prefer specific positions must be related with each other." ></td>
	<td class="line x" title="121:277	We seeked to recapture the idea with the flexibility of word order." ></td>
	<td class="line x" title="122:277	For this, the concept of convergence on each position was employed." ></td>
	<td class="line x" title="123:277	In a free order language, a meaningful pair can occur in text either with two distance or three distance." ></td>
	<td class="line x" title="124:277	Let's consider two input vectors x, (0,1,0,0,0,1,0,0,1,0) and y, (0,0,0,1,1,1,0,0,0,0)." ></td>
	<td class="line x" title="125:277	They have the same variance but y would be more meaningful than x, because y can be interpreted as (0,0,0,0,3,0,0,0,0,0) within the free order framework." ></td>
	<td class="line x" title="126:277	Therefore, a spatial mask 74 :t. 14.: word pair (miami) (mi,m2) (mi,mk) (m~,m,~) total POS pair (J,P) (J,P) (a,P) (J,P) total frequency variable(position) distribution kl+ All k~2 k2+ k=l k:2 fik+ Akl fik2 fin+ finl fin2 f~++lJp under a JP relation  . . fillO    fi210 :  fik~o :  . . finlo fi+ltaP fi+MdP ''' fi+lolJP Table 2: all combinations of mi (1/2,1,1/2) was devised for convergence on each position." ></td>
	<td class="line x" title="127:277	The calculation of condensation value rnikv at p-th position is: 'lH, ikp 4fiki+afika+fika p = 1 f ikv { 1 'q'~ f ikp-lf ikp41 = 2 p = 29 fia 8 ~3fik 9 +4fik 1 o : 4' p= 10 The mik,, is c, omputed by neighborhoods that are locdted in the border of the l)-th position." ></td>
	<td class="line x" title="128:277	The may_ALe_ is likely ''3 *~ kk+ to represent :the condensation of (mi, ink) but it is; still deficient." ></td>
	<td class="line x" title="129:277	Intuitively, (0,1,1,1,0,3,2,0,0,0) would be less condensed than (0,0,3,0,0,3,2,0,0,0)." ></td>
	<td class="line x" title="130:277	Therefi)re, n' was designed for a penalty factor." ></td>
	<td class="line x" title="131:277	Irtikp ~." ></td>
	<td class="line x" title="132:277	= max (5) p=1,2  lo ~n'fia+ ',,' is the number of m, such that fikm 7 ~ 0 ti)r 0 <_ m <_ 10, and it is a reverse proportion to the condensation." ></td>
	<td class="line x" title="133:277	Square root was used tbr preventing the excessive influence p of '/t . We were motivated by the idea that if a pair is randomly distributed in terms of position." ></td>
	<td class="line x" title="134:277	then it Would not be meaningful." ></td>
	<td class="line x" title="135:277	Especially in tim case of flmction words, they are likely to be randomly distributed over a given morpheme but distributions of meanin.gful pairs are not random, as shown in Figure 3." ></td>
	<td class="line x" title="136:277	A typical method for the check of randomness is to measure how far the given distribution is away from a uniform distribution." ></td>
	<td class="line x" title="137:277	In (6), fik means the expected number of (mi, ink) at each position on the a,ssumption that the pair randomly occurs 4." ></td>
	<td class="line x" title="138:277	Vat : at the position." ></td>
	<td class="line x" title="139:277	\]fikv-Tikl 71k can be viewed as an error rate at each position p based on the assumption." ></td>
	<td class="line x" title="140:277	The big difference between the expected number and the actual observed frequency means that the distribution is not random." ></td>
	<td class="line x" title="141:277	One might think that this concept is the same with one of variance." ></td>
	<td class="line x" title="142:277	However, note the denominator." ></td>
	<td class="line x" title="143:277	This calculation is somewhat better than variance which depends on frequency." ></td>
	<td class="line x" title="144:277	= ~ ( fikp -fik )2 v, ." ></td>
	<td class="line x" title="145:277	(6) To become a meaningful bigram, a pair should be syntactically valid." ></td>
	<td class="line x" title="146:277	We viewed that if the frequency distribution of a pair keeps the overall frequency distribution of the POS relation set which the pair belongs to, then the pair would be syntactically valid." ></td>
	<td class="line x" title="147:277	To verify this idea, we depict the overall frequency distributions in some POS relations in Figure 2." ></td>
	<td class="line x" title="148:277	It shows the frequency distributions of pairs which are composed of postposition and predicate morpheme." ></td>
	<td class="line x" title="149:277	It is quite interesting that all objects have the similar form of frequency distribution." ></td>
	<td class="line x" title="150:277	They have sharp peaks at the first and third position." ></td>
	<td class="line x" title="151:277	Clearly, this illustrates that a postposition has a high probability of appearing at the first and third position before a predicate." ></td>
	<td class="line x" title="152:277	We can conclude from this that pairs keeping the overall frequency structure would be syntactically valid." ></td>
	<td class="line x" title="153:277	We used correlation coetticient for the structural similarity." ></td>
	<td class="line x" title="154:277	In the case of a pair rnik, the correlation value between (.fikl, fik2,'', .fiklo) and (./i+LI.lP fi+2lJP,''',.fi+loLJP) is evaluated." ></td>
	<td class="line x" title="155:277	Let x 75 1400 frequency 800 1200 1000 600 400 200 1600  ~ (be deep) --~--.~,~." ></td>
	<td class="line x" title="156:277	(b e new) o}-~." ></td>
	<td class="line x" title="157:277	~;(be beautiful)  :  ~ (rern ain,be left)  ~.---~-e-I (be heard,droD in) ----e--,t~~ ( f o lie w, D o u r into) -~-~ ~ o( x I (drol~/fall/aoart) o)AI (drink)  X-f (s tand ) (wear)  O~ 71 (regard) N (increase,clim b)  F=t t) (head to) 1 2 3 4 5 6 7 8 9 10  position distance) Figure 2: Frequency distributions of pairs with 'JP' or 'VP' POS relation and y be two vectors whose components are mean corrected, xi ~ for x, Yi Y for y. The correlation between two variables is straightforward, if x and y is standardized through dividing each of their elements by the standard deviations, ax and ay, respectively." ></td>
	<td class="line x" title="158:277	Let x* be x/ax and y* be y/ay, then the correlation between x and y, VeT can be represented as follows." ></td>
	<td class="line x" title="159:277	Xt = (fikl, fik2,''', fiklo) Y' = (fi+llJP, fi+2WJP,'', fi+~olJP) (7) xJy * gcr = 10 The ranks of bigrams by four measures is summarized in Figure 3." ></td>
	<td class="line x" title="160:277	It tells that each of the measures comes up with our expectation." ></td>
	<td class="line x" title="161:277	4.2 Evaluation Function in this section, we analyze the correlations of fbur measures we defined and explain how to make an evaluation function for extracting meaningful bigrams." ></td>
	<td class="line x" title="162:277	Table 3 shows the values of correlations which exist in the given measures: V/, V~, Ve, VeT." ></td>
	<td class="line x" title="163:277	This explains that the defined measures have redundant parts." ></td>
	<td class="line x" title="164:277	We can say that if a measure has the high values of V~   Vet V/  vT ,." ></td>
	<td class="line x" title="165:277	1.0 -0.495 1.0 -0.203 0.506 1.0 0.252 -0.278 -0.002 1.0 Table 3: correlations between factors correlations between others, then it has a redundant part to be eliminated." ></td>
	<td class="line x" title="166:277	Since we don't know what factors are effective in determining useful bigram, the concept of weights is more reliable than filtering." ></td>
	<td class="line x" title="167:277	We constructed an evaluation function, which reflects the correlations between the measures." ></td>
	<td class="line x" title="168:277	First of all, we standardized four measures." ></td>
	<td class="line x" title="169:277	Standardization gives an effect on adjustment of value range according to its variability." ></td>
	<td class="line x" title="170:277	The degree of relationship between measurel and measure2 can be obtained by Cmeasurel,measure2 which is {corr(measurel, measure2)} +, where x + = x if x > 0, x + = 0 otherwise." ></td>
	<td class="line x" title="171:277	The evaluation function is concerned with the degrees of relationships of measures." ></td>
	<td class="line x" title="172:277	f(Vf, Vr, Vc, Vcr) = Vf rVr +CoVe + crVcr (8) 76 i Cb,." ></td>
	<td class="line x" title="173:277	: (1 Cv.,vf)(1 aCvA2'v )(1 aCvsvr)   (1 Cv,vr)(1 a-~)(1 a--~) (/'or := (1 -Cv~.~,vf)(1 a vS'v~ )(1 -a'~V~ ~'v~ ) where a = 2 2 (9) Here, the cor/stant a(~ 0.845) is for a compensation coefficient." ></td>
	<td class="line x" title="175:277	The minimum value of Cr, c and c~ is 1/3 respectively, where Cv:,v~ = Cvj.,v~, = Cvf,v~ = 0 and all correlations of ~., i,~:, and Vcr = 1." ></td>
	<td class="line x" title="178:277	On the contray, the maxlmum value of ~, , and cr is 1 respectively, where Cv:,v, = Cvf,v~ = cv:,v~ = 0 and all correlations of Vr, Vc, Vcr = 0." ></td>
	<td class="line x" title="179:277	In other words, as the coefficients ~, ~, and c~ get closer to 1, the correlations between measures reduce." ></td>
	<td class="line x" title="180:277	As shown in (8) and (9), we agree that Vf is a. primaryl factor of collocations." ></td>
	<td class="line x" title="181:277	Each coefficient  indicates how much the property is reflected in evaluation." ></td>
	<td class="line x" title="182:277	For example, in the case of Cr, a-~ z~ is a portion which is related with the property of condensation within randomness." ></td>
	<td class="line x" title="183:277	Therefbre,i 1 a--~ corresponds to the remainder, when subtracting this portion from randomness." ></td>
	<td class="line x" title="184:277	The threshold for evaluation was set by testing." ></td>
	<td class="line x" title="185:277	When the value for threshold was 0.5, good results were obtained but in noun morphems, a high value over 0.9 was required." ></td>
	<td class="line x" title="186:277	The pairs are selected as meaningful bigrams whose values of the evaluation function are greater than the threshold." ></td>
	<td class="line x" title="187:277	4.3 :Extending to n-grams The selected meaningful bigrams from the previous step are extended into n-gram collocations." ></td>
	<td class="line x" title="188:277	At the final step, the longest ones among all (~-~:overs are Obtained as n-gram collocations by eliminating substrings." ></td>
	<td class="line x" title="189:277	Here, n-gram collo~:ations mean interrupted collocations as well as n-character strings." ></td>
	<td class="line x" title="190:277	We regarded Cohesive clusters of the meaningful bigrams as n-gram collocations on the assumption that members in a collocation have a high degree of cohesion (Kjellmer, 1995)." ></td>
	<td class="line x" title="191:277	To find cohesive chisters, a fuzzy compatibility rela.tion R is appl!ed." ></td>
	<td class="line x" title="192:277	R on X x X, where Xis the set of all meaningful bigrams which contain ;,, l)~se morpheme mi, means a cohesive relation a.nd partitions of' set X obtained by R correspond to n-gram collocations." ></td>
	<td class="line x" title="193:277	To say shortly, our problem hasshifted to clustering of a set X. A reason to employ the concept of fuzzy is that equivalence sets defined by the relation may be more desirable." ></td>
	<td class="line x" title="194:277	A fuzzy compatability relation R(X,X) is represented as a matrix by a membership function." ></td>
	<td class="line x" title="195:277	The membership function of a fuzzy set A E X is denoted by #A : X ~ \[0, 1\] and maps elements of a given set X into real numbers in \[0,1\]." ></td>
	<td class="line x" title="196:277	These two membership functions #A were used to define the cohesive relation as follows." ></td>
	<td class="line x" title="197:277	p(x)= I*nyl .:.,~_ \]*nyl I*1 '~'J:lyl D(p(x)\[Ip(y)) = p(x)(log pry) log p~x)) if p(x)_(p(y) ~A(X' Y) ~ O(p(y)\[Ip(x)) : p(y)(log p~x) log p~y)) if p(y)(_p(x) (10),, 2J*nyl (11) /ZA I,x,Y)= T~ T Let Ixl and lYl be the frequency of concordances which contains the bigram pairs x and y, respectively." ></td>
	<td class="line x" title="199:277	IxAyl means how often two pairs x and y co-occur in the same concordances under the distance constraint." ></td>
	<td class="line x" title="200:277	(10) is relative entropy measure and (11) is dice coefficient." ></td>
	<td class="line x" title="201:277	This measures are concerned with a lexical relation for cohesive degrees." ></td>
	<td class="line x" title="202:277	To get equivalence sets, it is very important to identify properties of the relation R we defined." ></td>
	<td class="line x" title="203:277	A relation which is reflexive, symmetric and transitive is called as an equivalence relation or similarity relation." ></td>
	<td class="line x" title="204:277	In our case, the fuzzy cohesive relation, R is certainly reflexive and symmetric." ></td>
	<td class="line x" title="205:277	If R(x, z) > ma, xyEy min\[R(x, y), R(y, z)\] is satisfied for all (x, z) e X 2, then R is transitive." ></td>
	<td class="line x" title="206:277	Generally, transitive closure is used for checking transitivity." ></td>
	<td class="line x" title="207:277	The transitive closure of a relation is defined as the smallest fuzzy relation which is transitive and has the fewest possible members with containing the relation itself." ></td>
	<td class="line x" title="208:277	Given a relation S(X,X), its max-min transitive closure ST(X, X) can be calculated by the following algorithm consisted of three steps: 1." ></td>
	<td class="line x" title="209:277	S I = SU (S o S), o is a max-min composition operator." ></td>
	<td class="line x" title="210:277	2." ></td>
	<td class="line x" title="211:277	If S' # S, make S = S ' and go to Step 1." ></td>
	<td class="line x" title="212:277	3. Stop: S'= ST. If above algorithm terminates after the first iteration when applied to R, R satisfies transitivity." ></td>
	<td class="line x" title="213:277	To verify its transitivity, above alogrithm were employed." ></td>
	<td class="line x" title="214:277	As a result, R did not satisfy transitivity." ></td>
	<td class="line x" title="215:277	It means that an element of X could be77 hmg to multiple (:lasses by R. This proves that the relation R is valid to explain collocations." ></td>
	<td class="line x" title="216:277	A iuzzy binary relation R(X,X) which is reth~xive and symmetric is called as a fuzzy compa.til)i\[ity relation and is usually referred to as ~,." ></td>
	<td class="line x" title="217:277	(lunsi-e(tuivalence relation." ></td>
	<td class="line x" title="218:277	When R is a fuzzy compa, tibility relation, compatibility classes are,l(,.fined in terms of a specified membership degre,'." ></td>
	<td class="line x" title="219:277	An a-compatibility class is a subset A of X. s,mh that it(x, y) > a for all x, y E A and the tnmily consisting of the compatibility classes is called as an a-cover of X to R in terms of a specifi,: membership degree a. An a-cover forms partitions of X and an element of X could belong to multiple a-compatibility classes." ></td>
	<td class="line x" title="220:277	Here, we a.ccepted a-covers at 0.20 a-level in dice and (}.3} in relative entropy." ></td>
	<td class="line x" title="221:277	One might argue why we did not directly apply a\]\] bigrams to this stage with skipping the previous stage." ></td>
	<td class="line x" title="222:277	We hope to deal with the comt)arision in a later paper." ></td>
	<td class="line x" title="223:277	5 Evaluation We performed experiments for evaluation on 328,859 sentences(8.5 million-morphemes) from Yonsei balanced copora." ></td>
	<td class="line x" title="224:277	250 morphemes were selected for a test, such that frequency >_ 150." ></td>
	<td class="line x" title="225:277	The morphemes have 8,064 pairs and 773 were extracted as meaningful bigrams." ></td>
	<td class="line x" title="226:277	In the second stage, 3,490 disjoint a-compatibility classes corresponding to lexicMly cohesive clusters were genera,ted." ></td>
	<td class="line x" title="227:277	698 longest n-gram collocations out of the a-compatibility classes were extracted by eliminating the fragments that can be subsumed in longer classes." ></td>
	<td class="line x" title="228:277	The precision of extracted meaningful bigram was 86.3% and 92% in the case of n-gram collocations." ></td>
	<td class="line x" title="229:277	We could take either o~-covers and the hmgest n-grams as n-gram collocations according to applications." ></td>
	<td class="line x" title="230:277	Since unfortunately, there is no existing database of collocations for evaluation, it is not easy to compute precision values and recall values as well." ></td>
	<td class="line x" title="231:277	We computed the precision values by hand." ></td>
	<td class="line x" title="232:277	As a different approach to Korean collocations, (Lee et al. , 1996) extracted interrupted bigrams using several filtering conditions and at least the 90% of the results were adjacent bigrams of length 1." ></td>
	<td class="line x" title="233:277	By this comparison, we may conclude that our approach is more flexible to deal with Korean word order." ></td>
	<td class="line x" title="234:277	Figure 3 9 displays the changes of rank according to measures we considered." ></td>
	<td class="line x" title="235:277	It shows that in contrast to other models, the properties have been effective in retrieving collocations which contain pairs of morphemes with relatively low frequency." ></td>
	<td class="line x" title="236:277	Since the ranks of bigrams in four measures came up with our expectation, if we could make more adequate evaluation function, the precision would be improved." ></td>
	<td class="line x" title="237:277	Table 4 shows some obtained meaningful bigrams of 'o}.>\] (not)'." ></td>
	<td class="line x" title="238:277	There are a great deal of expressions relating negative sentences in Korean." ></td>
	<td class="line x" title="239:277	The components of them occurs separated in various ways." ></td>
	<td class="line x" title="240:277	When evaluating meaningflfl bigrams, the coetticients for tile evaluation flmction are as follows: Cr ~ 0.432, (: v 0.490, cr ~ 0.371 in the case of 'ol-q(not)'." ></td>
	<td class="line x" title="241:277	This means that the influence of three other measures is 1.284 times more than that of frequency measure in 'JP' POS relation." ></td>
	<td class="line x" title="242:277	We will illustrate all steps with a word, '~'(wear)." ></td>
	<td class="line x" title="243:277	The results of the first stage, meaningful bigrams of '4_!" ></td>
	<td class="line x" title="244:277	'(wear) m are shown in Figure 4." ></td>
	<td class="line x" title="245:277	In the second stage, we calculated membership grades of inputs using dice measure and relative entropy measure." ></td>
	<td class="line x" title="246:277	As Figure 4 shows, dice measure looks unsatisfactory in such cases as the pair '(~(object case), ~o} (much))'." ></td>
	<td class="line x" title="247:277	Although the common frequency, 3 is a relatively high in the aspect of the word with lower frequency, 'Nol'(much), the value of dice is low." ></td>
	<td class="line x" title="248:277	Thus, we also tested relative entropy based on the probability of low frequency." ></td>
	<td class="line x" title="249:277	Two measures produce similar results if all values in the level set of R is considered instead of a specific value of o~, but entropy measure produces more good results." ></td>
	<td class="line x" title="250:277	Figure 4 and 5 show all o~-compatibility classes and the longest n-gram collocations of '~'(wear)." ></td>
	<td class="line x" title="251:277	Through our method, various kind of collocations were extracted." ></td>
	<td class="line x" title="252:277	In Figure 4, the order of components of a oe is by concordances." ></td>
	<td class="line x" title="253:277	6 Conclusion In this paper, we implemented measures which reflect the four properties of collocation respec9The meanings of pairs are not described in detail because the pairs including function words are hard to translate into English." ></td>
	<td class="line x" title="254:277	1The word meaning corresponds to 'put on(wear or take on)' in English, but it uses for shoes or socks." ></td>
	<td class="line x" title="255:277	78 ! mk mr poarelation postnot oo~mo. :: ~o~: : Iof:~.t olf ~t oF q Jp t .oF i.~ JP JOFLf JP .~ =oFq Jp !o~ L-I Jp oll !O~M JP o~1 ~ ~ Io~-t.-I Jp ~Vlllld I~lm Frequency distribution 22 20 17 26 40 48 17 15 6 427 0 0 0 0 0 0 0 0 0 53 0 0 0 0 0 0 0 0 1 11 0 0 1 0 0 0 0 1 0 29 0 2 1 4 5 2 4 4 0 82 1 1 0 0 0 1 0 2 0 27 6 4 3 6 4 9 1 16 2 60 24 31 34 35 38 46 24 48 10 13 0 0 0 0 I 7 1 0 0 0 23 24 25 27 24 55 17 13 1 0 19 30 23 30 15 19 19 58 1 1 1 0 1 0 1 3 0 0 0 0 0 0 2 0 1 3 1 0 0 0 6 7 11 13 5 25 3 3 0 0 1 0 0 6 1 2 1 1 0 0 frec lent freq std 638 4.1 53 -0 12 -0 31 -0 104 0.2 32 -0 111 0.3 303 1.7 9 -0 209 1 215 1 6 -1 7 -1 73 -0 1 2 -0 randl mn ran istd 36.3 0.4 90.0 2.5 74.7 1.9 77.7 2.1 52.9 1.1 61.9 1.4 22.9 -0 1.6 -1 53.0 1.1 4.9 -1 5.1 -1 23.3 -0 20.6 -0 9,6 -1 20.6 -0 .~ Ih~ Bml |llmlm ~ll |llal~llll Ollll ~II~111 I~I~I~ ~llrll~ll~l~ IIII~II II~I~I~ I~1 I1~1 ~lllBml~ nenmNI ~11~1~ nenmil 111~1~ O~1 llitai'aa B~ III Bill ~III~IE nml tll~ll~ evalul eval 4.8 3.3 1.9 1.8 1.3 1.2 0.6 0.6 0.5 0.1 0.0 -0.6 -0.7 -0.7 -0.8 Figure 3: Top 15 bigrams of 'o~'(not) by our algorithm mi POSot  J(white) J ~ ~(shoes) N ~-'~-(l~:~ots) N =m~socks) N ~-.~-~'J(rub~u shoes)  N o11(location) P oll * (location) P 0~1(location) P ~ * (location) P ~*(Iocation) P L *(modifying} p L *(modifying) E L *(modif~Rg} E L,(modifying) E ~ * (modifying) E L~ *(modlfy~ng) E L *(modifying) E L * (mod~fyirlg) E L *(modifying) E ~*(subiec~) E > ~*(subject) P ) |*(subiect) P ~ F*(su~ect) P )~(su~ect) P 2 Hsubject} P 2}*(sul~ect) P  m* (object) P t-(object) P ~' (object) P t-(object) P ~*(objecl) P *(object) P ~-' (object) P -~(object) P ~' (ObJ~:':t) P *(object) P ~*(obj~ct\] P ~--(obl~t) P  = function word m~ POS of m= Relative Y ~-~l-Isneakets) N 4 5 1 0.22 0.06 2IS(leather) N 8 4 1 0.t7 0.17 2 P~(leat her} N 11 4 2 0.27 0.51 -T~boots) N 8 11 1 0.11 0.04 ~(whlte) J 3 4 2 0.57 0.19 -.~--~J~ ( mbl0er s PK.~s ) N 19 3 3 0.27 1.85 .~O~(much) O 19 3 1 0.09 0.62 -!:1 (Y, hit e) J 19 4 2 0.17 0.78 -r~(boots) N 19 11 2 0.13 0.10 o~shocks) N 19 8 3 0.22." ></td>
	<td class="line x" title="256:277	0.32 .~-'~,-J~ (Veer shoes) N 19 3 3 0.27 1.85 ~Ol(much) D 50 3 1 0.04 0.94 -~J,-I(sneaker s) N 50 5 2 0.07 0.92 ~(white) J 50 4 3 0.11 1.89 7 t-~(leather) N 50 4 1 0.04 0.63 AJ~shoes) N 50 8 3 0.10 0.69 ~t-(boots) N 50 11 4 0.13 0.55 ~shoc~.s) N 50 8 5 0.17 1.15 .~.-~--'-'~ ( PJbber shoes) N 50 3 2 0.08 1.88 HI * (location) P 50 19 14 0.41 0.71 oF~_.~-(stfll) D 31 3 1 0.06 0.78 -Jo~l-Isneakers) N 0.36 tl:l (vA'~it e) J 0.51 ~il~shoes) N 0.51 :~.(~oots) N 0.09 ~-(Iocati~) P 0.10 L *(modifying) E 0.22 P~ol(much) D 3.03 ~-~i~sneakers) N 2.01 ~(white) J 2.06 7 P.~.,(leather) N 2.74 ~U~(shoes\] N 1.54 ::F~(boots) N 1.41 ol~(shacks) N 1.79 ~-.~-.~ (rubber shoes) N 3.03 Hl*(Iocation) P 0.68 L *(modifying) E 0,12 ~Hsubiect) P 0.22 All c--covers using dice measure ''; I.(subject ) )-:ltl (whit e) L (m odiNng) ~-~ll.t(sneakers)-i.( object ) ~ (wear)   7 ~subject) L (modifyirlg)-,~.J ~r(sl~S)-~_J (wear).-.2 ~sublect) L (modifying)'-N u~(shoesl-II(object ) --.~ (wear).-." ></td>
	<td class="line x" title="258:277	~gsubject).`~(~cati~n)~(mod~fy~ng)~.~`1(~bject)~Bj~(much)``~(we=)~." ></td>
	<td class="line x" title="259:277	2~(subject)oll(Iocation)ll(object)~Ol(rnuch)~(wear)  .-2~-.~ ~-~-(leather boots)-~(wear)  .-2~-~ -.7-.~,.0eather boots) J(object)~ (wear) .o~-~, ~ ~t~'(Ioather shoes).--~(wear)--." ></td>
	<td class="line x" title="260:277	-2}-~ ~J W(k3ather shoes)J.(object)-~(wear) ~(boots)-~(wear) --.?-~-(boots)t}Cobject) ~(wear)''' L (modifying),2 }-~ -7' '~(feather boots)t(object)-~ (wear) ~ (modifying).-o ~ ~ L~,j(leat her shoes)I(obJect)~(wear)   L (modifying)'-~ (wear)  .L (rnodifying)ol (Iocation)-J(object)N (wear) L (modifying)-ol (Iocation)-~(wear).-." ></td>
	<td class="line x" title="261:277	L." ></td>
	<td class="line x" title="262:277	(modiNng-~(object))~ (wear) ''i(object)'-~(wear)', ~OF(much)~(wear)  ~ ~(shoes)-~ (wear) ~ ~(shoes) J(object).-.~ (wear)-o I-~.L~_ (sNI)-~ (wear) oj~:(shccks)~J (wead   ~(socks)~(Iocation)--." ></td>
	<td class="line x" title="263:277	L (modifying).-.-~-~-(boots)t~(object)~ (wear)  ol (location) ~-~,~.AJ (rul0ber shoes)-~ (wear)ol (location)  L (modl lying ) -.~-~(boots)i(object ) ~ (wear) .-." ></td>
	<td class="line x" title="264:277	 .-oll(Iccation) L-(modlfytng).-.~(shocks)J(object),~(wear).-." ></td>
	<td class="line x" title="265:277	ol (Iocatlon)(object)-P~Ol(much)~ (wear)  N (Iocation)-~ (wear) N(Iocation)O~(shocks)~(wear) .,." ></td>
	<td class="line x" title="266:277	-N (Ioc~tion)--.~'gt(shocks)tl(object).--N (wear)  -.N(Iocation)~(white)-~-~.,J(rubber shoes)-~(wear).-." ></td>
	<td class="line x" title="268:277	 c~(Iocation)~(white)~.P~-C~(rubber shoes).i(object)~ (wear) o~(Iocation)-~(white)L(modifying)~-~~(rubber shoes)tl(object)~(wear) -~-.~t4(sneakers).-~ (wear)   -~-lsneakem)-B (object) .-.~J (wear) .M (whit e) .-.~-~--~ (rubber shces)~ (wear) -~ (vANte)-.-~(wear).-.-.~(v,/nite)~-~-tzl<sneakers)~(wear) ---~ (whir e)  -~-.~-~J-( sneaker s)iJ.(object ) -~ (wear).-.~ (white) L(rnodiNng)~ (wear) --." ></td>
	<td class="line x" title="269:277	Figure 4: Meaningful bigrams and all c~-compatibility classes of '~'(wear) tively and the evaluation function which appropriately combines the measures." ></td>
	<td class="line x" title="270:277	Our approach was primarily focused on the subtle relationshit)s between word positions and collocations in a, free order language." ></td>
	<td class="line x" title="271:277	We extracted meaningful bigrams using an ew~luation flmction and extended them into n-grams by producing a-compatibility classes." ></td>
	<td class="line x" title="272:277	The usefulness of our algorithm were illustrated by examples and tables." ></td>
	<td class="line x" title="273:277	This method covered various range of collocations, which the extracted collocation patterns were case frames, multiple functional words, selectional restrictions, semantic phrases, corn79 (Adverb,'o~Q') (Postposition,'o~') (Noun, 'o~q') (~\]~, otq)(not only) (~-x\], o\]-q)(not only) (~t--~-~1, o~q)(simply ~ not) (~e-~Q, o\]-q)(but ~ not) (~, olM)(never) (~V~I, l'q)(not necessarily) (~, o~q)(too) (~ol, obq)(not also) (~b, obq)(not) (2E, o~ )(not) (~-, o\]-w\] )(not) (~, o\]-q)(not also) (~--, o\]M)(not because) (:~, o~q)(not ~ that) (~x~\], o\]-q)(not important) (~, ol-q)(not intend to) (1t71, o\]-q)(not ~ that) (oil-r, o~q)(not the reason) Table 4: Examples of bigrams for negation expressions collocations(Ionsest collocations) dice ''Y b'':F~-''~'' (Sbootswear) Ybo~l ~~--~,~ (SLMOwear)  ~bOll.-.~~ol ~ (SLOmuch wear) -2t ~~ ~~-~(S-M shoes+O.--wear) relative  ~t--oll--'7-@'-~ ''  71-otlc,,~,Nol ~   ~FL ~ ~t~ ~t---~.l ~'-l'." ></td>
	<td class="line x" title="274:277	~  K\] Same S : a proposition for a subject case 0 : a proposition for a object case L : a proposition for a Ioction case M : for modifying Figure 5: the longest n-gram collocations of '~'(wear) pound nouns, and idioms and it could be applicable to other free order languages." ></td>
	<td class="line x" title="275:277	With the development of recognition of phrases, the input format and related distance between morphemes, the algorithm can be used effectively." ></td>
	<td class="line x" title="276:277	Also linguistic contents for statistical constraints should be reflected in the system." ></td>
	<td class="line x" title="277:277	We have plans to check how this algorithm will work in English and to align bilingual collocations for machine translation." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="A00-1042
Evaluation Of Automatically Identified Index Terms For Browsing Electronic Documents
Wacholder, Nina;Klavans, Judith L.;Evans, David Kirk;"></td>
	<td class="line x" title="1:178	Evaluation of Automatically Identified Index Terms for Browsing Electronic Documents I Nina Wacholder, Judith L. Klavans and David K. Evans Columbia University Department of Computer Science and Center for Research on Information Access 1." ></td>
	<td class="line x" title="2:178	Abstract We present an evaluation of domainindependent natural language tools for use in the identification of significant concepts in documents." ></td>
	<td class="line x" title="3:178	Using qualitative evaluation, we compare three shallow processing methods for extracting index terms, i.e., terms that can be used to model the content of documents." ></td>
	<td class="line x" title="4:178	We focus on two criteria: quality and coverage." ></td>
	<td class="line x" title="5:178	In terms of quality alone, our results show that technical term (TT) extraction \[Justeson and Katz 1995\] receives the highest rating." ></td>
	<td class="line x" title="6:178	However, in terms of a combined quality and coverage metric, the Head Sorting (HS) method, described in \[Wacholder 1998\], outperforms both other methods, keyword (KW) and TT." ></td>
	<td class="line x" title="7:178	2." ></td>
	<td class="line x" title="8:178	Introduction In this paper, we consider the problem of how to evaluate the automatic identification of index terms that have been derived without recourse to lexicons or to other kinds of domain-specific information." ></td>
	<td class="line x" title="9:178	By index terms, we mean natural language expressions that constitute a meaningful representation of a document for humans." ></td>
	<td class="line x" title="10:178	The premise of this research is that if significant topics coherently represent information in a document, these topics can be used as index terms that approximate the content of individual documents in large collections of electronic documents." ></td>
	<td class="line x" title="11:178	We compare three shallow processing methods for identifying index terms:  Keywords (KW) are terms identified by counting frequency of stemmed words in a document; Technical terms (TT) are noun phrases (NPs) or subparts of NPs repeated more than twice in a document \[Justeson and Katz 1995\]; Head sorted terms (HS) are identified by a method in which simplex noun phrases (as defined below) are sorted by head and then ranked in decreasing order of frequency \[Wacholder 1998\]." ></td>
	<td class="line x" title="12:178	The three methods that we evaluated are domain-independent in that they use statistical and/or linguistic properties that apply to any natural language document in any field." ></td>
	<td class="line x" title="13:178	These methods are also corpus-independent, in that the ranking of terms for an individual document is not dependent on properties of the corpus." ></td>
	<td class="line x" title="14:178	2.1 Overview of methods and results Subjects were drawn from two groups: professionals and students." ></td>
	<td class="line x" title="15:178	Professionals included librarians and publishing professionals familiar with both manual and automatic text indexing." ></td>
	<td class="line x" title="16:178	Students included undergraduate and graduate students with a variety of academic interests." ></td>
	<td class="line x" title="17:178	To assess terms, we used a standard qualitative ranking technique." ></td>
	<td class="line x" title="18:178	We presented subjects with an article and a list of terms identified by one of the three methods." ></td>
	<td class="line x" title="19:178	Subjects were asked to answer the following general question: 'Would this term be useful in an electronic index for this article'?" ></td>
	<td class="line x" title="20:178	Terms were rated on a scale of 1 to 5, where 1 indicates a high quality term that should definitely be included in the index and 5 indicates a junk term that definitely should not be included." ></td>
	<td class="line x" title="21:178	For ex1 This research was partly funded by NSF IRI 97-12069, 'Automatic identification of significant topics in domain independent full text documents' and NSF IRI 97-53054, 'Computationally tractable methods for document analysis'." ></td>
	<td class="line x" title="22:178	_'tO~ ample, the phrase court-approved affirmative action plans received an average rating of 1 from the professionals, meaning that it was ranked as useful for the article; the KW affirmative received an average rating of 3.75, meaning that it was less useful; and the KW action received an average ranking of 4.5, meaning that it was not useful." ></td>
	<td class="line x" title="23:178	The goal of our research is to determine which method, or combination of methods, provides the best results." ></td>
	<td class="line x" title="24:178	We measure results in terms of two criteria: quality and coverage." ></td>
	<td class="line x" title="25:178	By quality, we mean that evaluators ranked terms high on the 1 to 5 scale from highest to lowest." ></td>
	<td class="line x" title="26:178	By coverage, we mean the thoroughness with which the terms cover the significant topics in the document." ></td>
	<td class="line x" title="27:178	Our methodology permits us to measure both criteria, as shown in Figure 4." ></td>
	<td class="line x" title="28:178	Our results from both the professionals and students show that TTs are superior with respect to quality; however, there are only a small number of TTs per document, so they do not provide adequate coverage in that they are not fully representative of the document as a whole." ></td>
	<td class="line x" title="29:178	In contrast, KWs provide good coverage but relatively poor quality in that KWs are vague, and not well filtered." ></td>
	<td class="line x" title="30:178	SNPs, which have been sorted using HS and filtered, provide a better balance of quality and coverage." ></td>
	<td class="line x" title="31:178	From our study, we draw the following conclusions:  The KW approach identifies some useful index terms, but they are mixed in with a large number of low-ranked terms." ></td>
	<td class="line x" title="32:178	 The TT approach identifies high quality terms, but with low coverage, i.e., relatively few indexing terms." ></td>
	<td class="line x" title="33:178	 The HS approach achieves a balance between quality and coverage." ></td>
	<td class="line x" title="34:178	3." ></td>
	<td class="line x" title="35:178	Domain-independent metrics for identifying significant topics In order to identify significant topics in a document, a significance measure is needed, i.e., a method for determining which concepts in the document are relatively important for a given task." ></td>
	<td class="line x" title="36:178	The need to determine the importance of a particular concept within a document is motivated by a range of applications, including information retrieval \[Salton 1989\], 303 automatic determination of authorship \[Mosteller and Wallace 1963\], similarity metrics for cross-document clustering \[Hatzivassiloglou et al. 1999\], automatic indexing \[Hodges et al. 1996\] and input to summarization \[Paice 1990\]." ></td>
	<td class="line x" title="37:178	For example, one of the earlier applications using frequency for identifying significant topics in a document was proposed by \[Luhn 1958\] for use in creating automatic abstracts." ></td>
	<td class="line x" title="38:178	For each document, a list of stoplisted stems was created, and ranked by frequency; the most frequent keywords were used to identify significant sentences in the original document." ></td>
	<td class="line x" title="39:178	Luhn's premise was that emphasis, as indicated by repetition of words and collocation is an indicator of significance." ></td>
	<td class="line x" title="40:178	Namely, 'the more often certain words are found in each other's company within a sentence, the more significance may be attributed to each of these words'." ></td>
	<td class="line x" title="41:178	This basic observation, although refined extensively by later summarization techniques (as reviewed in \[Paice 1990\]), relies on the capability of identifying significant concepts." ></td>
	<td class="line x" title="42:178	The standard IR technique known as tf*idf \[Salton 1989\] seeks to identify documents relevant to a particular query by relativizing keyword frequency in a document as compared to frequency in a corpus." ></td>
	<td class="line x" title="43:178	This method can be used to locate at least some important concepts in full text." ></td>
	<td class="line x" title="44:178	Although it has been effective for information retrieval, for other applications, such as human-oriented indexing, this technique is impractical." ></td>
	<td class="line x" title="45:178	Ambiguity of stems (trad might refer to trader or tradition) and of isolated words (state might be a political entity or a mode of being) means that lists of keywords have not usually been used to represent the content of a document to human beings." ></td>
	<td class="line x" title="46:178	Furthermore, humans have a difficult time processing stems and parts of words out of phrasal context." ></td>
	<td class="line x" title="47:178	The technical term (TT) method, another technique for identification of significant terms in text that can be used as index terms was introduced by \[Justeson and Katz 1995\], who developed an algorithm for identifying repeated multi-word phrases such as central processing unit in the computer domain or word sense in the lexical semantic domain." ></td>
	<td class="line x" title="48:178	This algorithm identifies candidate TTs in a corpus by locating NPs consisting of nouns, adjectives, and sometimes prepositional phrases." ></td>
	<td class="line x" title="49:178	TTs are defined as those NPs, or their subparts, which occur above some frequency threshold in a corpus." ></td>
	<td class="line x" title="50:178	However, as \[Boguraev and Kennedy 1998\] observe, the TT technique may not characterize the full content of documents." ></td>
	<td class="line x" title="51:178	Indeed, even in a technical document, TTs do not provide adequate coverage of the NPs in a document that contribute to its content, especially since TTs are by definition multi-word." ></td>
	<td class="line x" title="52:178	A truly domain-general method should apply to both technical and nontechnical documents." ></td>
	<td class="line x" title="53:178	The relevant difference between technical and non-technical documents is that in technical documents, many of the topics which are significant to the document as a whole may be also TTs." ></td>
	<td class="line x" title="54:178	\[Wacholder 1998\] proposed the method of Head Sorting for identifying significant topics that can be used to represent a source document." ></td>
	<td class="line x" title="55:178	HS also uses a frequency measure to provide an approximation of topic significance." ></td>
	<td class="line x" title="56:178	However, instead of counting frequency of stems or repetition of word sequences, this method counts frequency of a relatively easily identified grammatical element, heads of simplex noun phrases (SNPs)." ></td>
	<td class="line x" title="57:178	For common NPs (NPs whose head is a common noun), an SNP is a maximal NP that includes premodifiers such as determiners and possessives but not post-nominal constituents such as prepositions or relativizers." ></td>
	<td class="line x" title="58:178	For example, the well-known book is an SNP but the well-known book on asteroids includes two SNPs, wellknown book and asteroids." ></td>
	<td class="line x" title="59:178	For proper names, an SNP is a name that refers to a single entity." ></td>
	<td class="line x" title="60:178	For example, Museum of the City of New York, the name of an organization, is an SNP even though the organizational name incorporates a city name." ></td>
	<td class="line x" title="61:178	Others, such as \[Church 1988\], have discussed a similar concept, sometimes called simple or base NPs." ></td>
	<td class="line x" title="62:178	The HS approach is based on the assumption that nominal elements can be used to convey the gist of a document." ></td>
	<td class="line x" title="63:178	SNPs, which are semantically and syntactically coherent, appear to be at a good level of detail for content representation of the document." ></td>
	<td class="line x" title="64:178	' 304 SNPs are identified by a system \[Evans 1998; Evans et al. 2000\] which sequentially parses text that has been tagged with part of speech using a finite state machine." ></td>
	<td class="line x" title="65:178	Next, the complete list of SNPs identified in a document is sorted by the head of the phrase, which, at least for English-language common SNPs, is almost always the last word." ></td>
	<td class="line x" title="66:178	The intuitive justification for sorting SNPs by head is based on the fundamental linguistic distinction between head and modifier: in general, a head makes a greater contribution to the syntax and semantics of a phrase than does a modifier." ></td>
	<td class="line x" title="67:178	This linguistic insight can be extended to the document level." ></td>
	<td class="line x" title="68:178	If, as a practical matter, it is necessary to rank the contribution to a whole document made by the sequence of words constituting an NP, the head should be ranked more highly than other words in the phrase." ></td>
	<td class="line x" title="69:178	This distinction is important in linguistic theory; for example, \[Jackendoff 1977\] discusses the relationship of heads and modifiers in phrase structure." ></td>
	<td class="line x" title="70:178	It is also important in NLP, where, for example, \[Strzalkowski 1997\] and \[Evans and Zhai 1996\] have used the distinction between heads and modifiers to add query terms to information retrieval systems." ></td>
	<td class="line x" title="71:178	Powerful corpus processing techniques have been developed to measure deviance from an average occurrence or co-occurrence in the corpus." ></td>
	<td class="line x" title="72:178	In this paper we chose to evaluate methods that depend only on document-internal data, independent of corpus, domain or genre." ></td>
	<td class="line oc" title="73:178	We therefore did not use, for example, tf*idf, the purely statistical technique that is the used by most information retrieval systems, or \[Smadja 1993\], a hybrid statistical and symbolic technique for identifying collocations." ></td>
	<td class="line x" title="74:178	4." ></td>
	<td class="line x" title="75:178	Experimental Method To evaluate techniques, we performed a qualitative user evaluation in which the terms identified by each method were compared for usefulness as index terms." ></td>
	<td class="line x" title="76:178	4.1 Subjects We performed our study with librarians, publishing professionals and undergraduate and graduate students at our university." ></td>
	<td class="line x" title="77:178	29 subjects participated in the study: 7 librarians and publishing professionals and 22 students." ></td>
	<td class="line x" title="78:178	4.2 Data For this experiment, we selected three articles from the 1990 Wall Street Journal contained in the Tipster collection of documents." ></td>
	<td class="line x" title="79:178	The articles were about 500 words in length." ></td>
	<td class="line x" title="80:178	To compare methods, each article was processed three times: 1) with SMART to identify stemmed keywords \[Salton 1989\]; 2) with an implementation of the TT algorithm based on \[Justeson and Katz 1995\]; and 3) with our implementation of the HS method." ></td>
	<td class="line x" title="81:178	Output for one article is shown in Appendix A. Figure 1 shows the articles selected, their length in words and the number of index terms from each method for each article presented to the subjects." ></td>
	<td class="line x" title="82:178	DOC words KW TT HS 415-0109 509 63 4 49 516-0043 594 51 9 54 517-0062 514 52 8 57 Figure 1: Word and term count, by type, per article The number of TTs is much lower than the number of KWs or HSs." ></td>
	<td class="line x" title="83:178	This presented us with a problem: on the one hand, we were concerned about preserving the integrity of the three methods, each of which has their own logic, and at the same time, we were concerned to present lists that were balanced relative to each other." ></td>
	<td class="line x" title="84:178	Toward this end, we made several decisions about presentation of the data: 1." ></td>
	<td class="line x" title="85:178	Threshold: So that no bias would be unintentionally introduced, we presented subjects with all terms output by each method, up to a specified cut-off poinHowever, using lists of equal length for each method would have necessitated either omitting HSs and KWs or changing the definition of TTs." ></td>
	<td class="line x" title="86:178	Therefore we made the following decisions:  For TTs, we included all identified terms;  For HSs, we included all terms whose head occurred more than once in the document; 305." ></td>
	<td class="line x" title="87:178	 For KWs, we included all terms in order of decreasing frequency, up to the point where we observed diminishing quality and where the number of KWs approximated the number of HSs." ></td>
	<td class="line x" title="88:178	Order: For the KW and TT approach, order is not significant." ></td>
	<td class="line x" title="89:178	However, for the HS approach, the grouping together of phrases with common heads is, we claim, one of the advantages of the method." ></td>
	<td class="line x" title="90:178	We therefore alphabetized the KWs and TTs in standard left to right order and alphabetized the HSs by head, e.g., trust account precedes money market fund." ></td>
	<td class="line x" title="91:178	Morphological expansion: The KW approach identifies stems which represent a set of one or more morphological variants of the stem." ></td>
	<td class="line x" title="92:178	Since in some cases the stem is not an English word, we expanded each stem to include the morphological variants that actually occurred in the article." ></td>
	<td class="line x" title="93:178	For example, for the stem reject, we listed rejected and rejecting but did not list rejects, which did not occur in the article." ></td>
	<td class="line x" title="94:178	4.3 Presentation to subjects Each subject was presented with three articles." ></td>
	<td class="line x" title="95:178	For one article, the subject received a head sorted list of HSs; for another article, the subject received a list of technical terms, and for the third article, the subject saw a list of keywords." ></td>
	<td class="line x" title="96:178	No time limit was placed on the task." ></td>
	<td class="line x" title="97:178	5." ></td>
	<td class="line x" title="98:178	Results Our results for the three types of terms, by document, are shown in Figure 2." ></td>
	<td class="line x" title="99:178	Although we asked subjects to rate three articles, some volunteers rated only two." ></td>
	<td class="line x" title="100:178	All results were included." ></td>
	<td class="line x" title="101:178	Doc 900405-0109 900516-0043 3.73 900517-0062 2.98 3.27 Avg of Avgs Figure 2: Average index terms Avg KW rating 3.08 Avg Avg TT HS rating rating 1.45 2.71 2.19 2.71 1.7 3.25 1.79 2.89 ratings of 3 types of 5.1 Quality For the three lists of index terms, TTs received the highest ratings for all three documents--an average of 1.79 on the scale of 1 to 5, with 1 being the best rating." ></td>
	<td class="line x" title="102:178	HS came in second, with an average of 2.89, and KW came in last with an average of 3.27." ></td>
	<td class="line x" title="103:178	It should be noted that averaging the average conceals the fact that the number of TTs is much lower than the other two types of terms, as shown in Figure 1." ></td>
	<td class="line x" title="104:178	Figure 3 (included before Appendix A) shows cumulative rankings of terms by method." ></td>
	<td class="line x" title="105:178	The X axis represents ratings awarded by subjects." ></td>
	<td class="line x" title="106:178	The Y axis reflects the percentage of terms receiving a given rank or better." ></td>
	<td class="line x" title="107:178	All data series must reach 100% since every term has been assigned a rating by the evaluators." ></td>
	<td class="line x" title="108:178	At any given data point, a larger value indicates that a larger percentage of that series' data has that particular rating or better." ></td>
	<td class="line x" title="109:178	For example, 100% of the TTs have a rating of 3 or better; while only about 30% of the terms of the lowest-scoring KW document received a score of 3 or better." ></td>
	<td class="line x" title="110:178	In two out of the three documents, HS terms fall between TTs and KWs." ></td>
	<td class="line x" title="111:178	5.2 Coverage The graph in Figure 3 shows results for quality, not coverage." ></td>
	<td class="line x" title="112:178	In contrast, Figure 4, which shows the total number of terms rated at or below specified rankings, allows us to measure quality and coverage." ></td>
	<td class="line x" title="113:178	(1 is the highest rating; 5 is the lowest)." ></td>
	<td class="line x" title="114:178	This figure shows that the HS method identifies more high quality terms than the TT method does." ></td>
	<td class="line x" title="115:178	~ od HS Number of terms ranked at or better than 2 3 4 5 27 75 124 166 41 96 132 160 15 21 21 21 Figure 4: Running total of terms identified at or below a specified rank TT clearly identifies the highest quality terms: 100% of TTs receive a rating of 2 or better." ></td>
	<td class="line x" title="116:178	However, only 8 TTs received a rating of 2 or better (38% of the total), while 41 HSs re306 ceived a rating of 2 or better (26% of the total)." ></td>
	<td class="line x" title="117:178	This indicates that the TT method misses many high quality terms." ></td>
	<td class="line x" title="118:178	KW, the least discriminating method in terms of quality, also provides better coverage than does TT." ></td>
	<td class="line x" title="119:178	This result is consistent with our observation that TT identifies the highest quality terms, but there are very few of them: an average of 7 per 500 words compared to over 50 for HS and KW." ></td>
	<td class="line x" title="120:178	Therefore there is a need for additional high quality terms." ></td>
	<td class="line x" title="121:178	The list of HSs received a higher average rating than did the list of KWs, as shown in Figure 2." ></td>
	<td class="line x" title="122:178	This is consistent with our expectation that phrases containing more content-bearing modifiers would be perceived as more useful index terms than would single word phrases consisting only of heads." ></td>
	<td class="line x" title="123:178	5.3 Ranking variability The difference in the average ratings for the list of KWs and the list of head-sorted SNPs was less than expected." ></td>
	<td class="line x" title="124:178	The small difference in average ratings for the HS list and the KW list can be explained, at least in part, by two factors: 1) Differences among professionals and students in inter-subject agreement and reliability; 2) A discrepancy in the rating of single word terms across term types." ></td>
	<td class="line x" title="125:178	22 students and 7 professionals participated in the study." ></td>
	<td class="line x" title="126:178	Figure 5 shows differences in the ratings of professionals and of students." ></td>
	<td class="line x" title="127:178	KW HS TT Professionals Students 2.64 3.30 2.3 3.03 1.49 2.1 Figure 5: Average ratings, by term type, of professionals and students When variation in the scores for terms was calculated using standard deviation, the standard deviation for the professionals was 0.78, while for the students it was 1.02." ></td>
	<td class="line x" title="128:178	Because of the relatively low number of professionals, the standard deviation was calculated only over terms that were rated by more than one professional." ></td>
	<td class="line x" title="129:178	A review of the students' results showed that they appeared not to be as careful as the professionals." ></td>
	<td class="line x" title="130:178	For example, the phrase 'Wall Street Journal' was included on the HS list only because it is specified as the document source." ></td>
	<td class="line x" title="131:178	However, four of the eight students assigned this term a high rating (1 or 2); this is puzzling because the document is about asbestos-related disease." ></td>
	<td class="line x" title="132:178	The other four students assigned a 4 or 5 to 'Wall Street Journal', as we expected." ></td>
	<td class="line x" title="133:178	But the average score for this term was 3, due to the anomalous ratings." ></td>
	<td class="line x" title="134:178	We therefore have more confidence in the reliability of the professional ratings, even though there are relatively few of them." ></td>
	<td class="line x" title="135:178	We examined some of the differences in rating for term types." ></td>
	<td class="line x" title="136:178	Single word index terms are rated more highly by professionals when they appear in the context of other single word index terms, but are downrated in the context of phrasal expansions that make the meaning of the one-word term more specific." ></td>
	<td class="line x" title="137:178	The KW list and HS list overlap when the SNP consists only of a single word (the head) or only of a head modified by determiners." ></td>
	<td class="line x" title="138:178	When the same word appears in both lists in identical form, the token in the KW list tends to receive a better rating than the token does when it appears in the HS list, where it is often followed by expansions of the head." ></td>
	<td class="line x" title="139:178	For example, the word exposure received an average rating of 2.2 when it appeared on the KW list, but a rating of only 2.75 on the HS list." ></td>
	<td class="line x" title="140:178	However, the more specific phrase racial quotas, which immediately followed quota on the HS list received a rating of 1." ></td>
	<td class="line x" title="141:178	To better understand these differences, we selected 40 multi-word phrases and examined the average score that the phrase received in the TT and HS lists, and compared it to the average ratings that individual words received in the KW list." ></td>
	<td class="line x" title="142:178	We found that in about half of the cases (21 of 40), the phrase as a whole and the individual words in the phrase received similar scores, as in Example 1 in Figure 6." ></td>
	<td class="line x" title="143:178	In just over one-fourth of the cases (12 of 40), the phrase scored well, but scores from the individual words were rated from good to poor, as in Example 2." ></td>
	<td class="line x" title="144:178	In about one-eighth of the cases (6 of 40), the phrase scored well, but the individual words scored poorly, as in Example 3." ></td>
	<td class="line x" title="145:178	Finally, in only one case, shown in Example 4 of Figure 6, the phrase scored poorly but the individual words scored well." ></td>
	<td class="line x" title="146:178	307 Phrase Supreme Court (1.5) reverse discrimiI nation (1) lymph system employment decisions (2.75) Word 1 Supreme (1) reverse (3.25) lymph (1) employment (1.25) Word 2 Court (1.25) discrimination (3.25) system (5) decisions (1.25) Figure 6: Comparison of scores of phrases and single words This shows that single words in isolation are judged differently than the same word when presented in the context of a larger phrase." ></td>
	<td class="line x" title="147:178	These results have important implications in the design of indexing tools." ></td>
	<td class="line x" title="148:178	6." ></td>
	<td class="line x" title="149:178	Conclusion Our results show that the head sorting technique outperforms two other indexing methods, technical terms and keywords, as measured by balance of quality and coverage." ></td>
	<td class="line x" title="150:178	We have performed a qualitative evaluation of three techniques for identifying significant terms in a document, driven by an indexing task." ></td>
	<td class="line x" title="151:178	Such an applicati;on can be used to create a profile or thumbnail of a document by presenting to users a set of terms which can be considered to be a representation of the content of the document." ></td>
	<td class="line x" title="152:178	We have used human judges to evaluate the effectiveness of each method." ></td>
	<td class="line x" title="153:178	This research is a contribution to the overall evaluation of computational linguistic tools in terms of their usefulness for human-oriented computational applications." ></td>
	<td class="line x" title="154:178	Figure 3: Cumulative ranking of terms, by method 0.9 0.8 ~ 0.7 i . 0.6 0.5 0.4 ~ o.a 0.2 ~ 0.1 0 0.5 1 1.5 2 2.5 3 Rating 3.5 4 4.5 5 308 Appendix A: Terms identified in WSJ900405-0109 HSs amendments Hatch amendment other amendments attempts bias job bias intentional bias bill committee Senate labor Committee court Supreme Court co-workers decisions Supreme Court decisions employment decisions Democrats discrimination reverse discrimination employees women employees employers groups civil-rights groups conservative policy groups Orrin Hatch health discriminatory impact Job-Bias Measure basic employment antidiscrimination law 1866 civil-rights law lawsuits lawmakers legislation comprehensive legislation more modest measure minority/minorities panel plans court-approved affirmative action plans discriminatory seniority plans practices employment practices quotas racial quotas fight/rights equal rights year Keywords action address/addressing adopt/adopted affirmative agree aimed alleged/alleging amend approved attempt/attempts bias bill Bush challenge circumstances civil clears committee court/Court decision Democrats discrimination employment/employers/employees force/Force give/giving GOP groups Hatch health ~gh impact job justify labor/Labor law lawmakers lawsuits legislative/legislation make measure minority/minorities Mr. overturning panel plans policy practices quotas racial rejected/rejecting reverse rights rules/ruling safety Sen./Sens. Senate shown street Supreme; vote/voted 309 women workers year Technical terms discriminatory impact employment practice Senator Hatch Supreme Court References Boguraev, Branimir and Kennedy, Christopher (1998) 'Applications of term identification terminology: domain description and content characterisation', Natural Language Engineering 1(1): 1-28." ></td>
	<td class="line x" title="155:178	Church, Kenneth Ward (1988) 'A stochastic parts program and noun phrase parser for unrestricted text', in Proceedings of the Second Conference on Applied Natural Language Processing, pp." ></td>
	<td class="line x" title="156:178	136-143." ></td>
	<td class="line x" title="157:178	Evans, David A. and Chengxiang Zhai (1996) 'Noun-phrase analysis in unrestricted text for information retrieval', Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pp." ></td>
	<td class="line x" title="158:178	17-24.24-27 June 1996, University of California, Santa Cruz, California, Morgan Kaufmann Publishers." ></td>
	<td class="line x" title="159:178	Evans, David K." ></td>
	<td class="line x" title="160:178	(1998) LinklT Documentation, Columbia University Department of Computer Science Report." ></td>
	<td class="line x" title="161:178	Evans, David K. , Klavans, Judith, and Wacholder, Nina (2000) 'Document processing with LinklT', RIAO Conference, Paris, France, to appear." ></td>
	<td class="line x" title="162:178	Hatzivassiloglou, Vasileios, Judith L. Klavans and Eleazar Eskin (1999) 'Detecting text similarity over short passages: exploring linguistic feature combinations via machine learning', Proceedings of the EMNLP/VLC-99 Joint SIGDAT Conference on Empirical Methods in NLP and Very Large Corpora, June 21-22, 1999, University of Maryland, College Park, MD. Hedges, Julia, Shiyun Yie, Ray Reighart and Lois Boggess (1996) 'An automated system that assists in the generation of document indexes', Natural Language Engineering 2(2): 137-160." ></td>
	<td class="line x" title="163:178	Jackendoff, Ray (1977) X-bar Syntax: A Study of Phrase Structure, MIT Press, Cambridge, MA." ></td>
	<td class="line x" title="164:178	Justeson, John S. and Slava M. Katz (1995) 'Technical terminology: some linguistic properties and an algorithm for identification in text', Natural Language Engineering 1(1):9-27." ></td>
	<td class="line x" title="165:178	Luhn, Hans P." ></td>
	<td class="line x" title="166:178	(1958) 'The automatic creation of literature abstracts', IBM Journal, 159-165." ></td>
	<td class="line x" title="167:178	Mosteller, Frederick and David L. Wallace (1963) 'Inference in an authorship problem', Journal of the American Statistical Association 58(302):275-309." ></td>
	<td class="line x" title="168:178	Available at http://www.jstor.org/." ></td>
	<td class="line x" title="169:178	Paice, Chris D." ></td>
	<td class="line x" title="170:178	(1990) 'Constructing literature abstracts by computer: techniques and prospects'." ></td>
	<td class="line x" title="171:178	Information Processing & Management 26(1): 171-186." ></td>
	<td class="line x" title="172:178	Salton, Gerald (1989) Automatic Text Processing: The Transformation, Analysis and Retrieval of lnformation by Computer." ></td>
	<td class="line x" title="173:178	Addison-Wesley, Reading, MA." ></td>
	<td class="line oc" title="174:178	Smadja, Frank (1993) 'Retrieving collocations from text', Computational Linguistics 19(1):143-177." ></td>
	<td class="line x" title="175:178	Strzalkowski, Thomas (1997) 'Building effective queries in natural language information retrieval', Proceedings of the ANLP, ACL, Washington, DC., pp.299-306." ></td>
	<td class="line x" title="177:178	Wacholder, Nina (1998) 'Simplex NPS sorted by head: a method for identifying significant topics within a document', Proceedings of the Workshop on the Computational Treatment of Nominals, pp.70-79." ></td>
	<td class="line x" title="178:178	COLING ACL '98, Montreal, Canada, August 16, 1998 ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C00-2113
An Empirical Method For Identifying And Translating Technical Terminology
Shimohata, Sayori;"></td>
	<td class="line x" title="1:147	An empirical lnethod for identifying and translating technical terminology Sayori Shimohata Research & Development Group, Oki Electric Industry Co. , Ltd. Crystal Tower 1-2-27 Shirolni, Chuo-ku, Osaka 540-6025 Japan shimohat a245 ~oki." ></td>
	<td class="line x" title="2:147	co.j p Abstract This paper describes a. method for retrieving patterns of words a.nd expressions frequently used in a. specific dom a.in and building a. dictionary for ma.chine translatiou(MT)." ></td>
	<td class="line x" title="3:147	The method uses an untagged text corpus in retrieving word sequences a.nd simplified pa.rt-of-speech ternplates in identifying their synta.ctic ca.tegories." ></td>
	<td class="line x" title="4:147	The pa.per presents eperimenta.l results for a.pplying the words and expressions to a patternbased ma.chine translation system." ></td>
	<td class="line x" title="5:147	1 Introduction Th.ere has been a. continuous interest in corpusbased approa.ches which retrieve words and expressions in connection with a specific domain (we call them technical terms herea.fter)." ></td>
	<td class="line x" title="6:147	They may correspond to syntactic phra.ses or components of syntactic relationships and ha.ve been found useful in various application area.s, including inibrmation etra.ction, text sumlna.riza.tion, and ma.chine tra.nsla.tion." ></td>
	<td class="line x" title="7:147	Am.ong others, a. knowledge of technica\] terminology is indispensa.ble for machine tra.nsla.tion beca.use usage and mea.ning of technica.1 terms a.re often quite different from their literal interpreta.tion." ></td>
	<td class="line x" title="8:147	One a.pproa.ch for identifying technical terminology is a. rule-ba.sed a.pproa.eh which learns l.oca.1 syntactic patterns from a training corpus." ></td>
	<td class="line x" title="9:147	A variety of methods ha.ve been developed within this fra.mework, (Ra.msha.w, 1995) (Arga.mon et al. , 1999) (Ca.rdie and Pierce, 1.999) a.nd achieved good results for the considered ta.sk." ></td>
	<td class="line x" title="10:147	Surprisingly, though, little work ha.s been d.evoted to lea.rning local syntactic pa.tterns besides noun phrases." ></td>
	<td class="line x" title="11:147	Another drawback of this a.pproach is tha.t it requires substa.ntiM training corpora, in many cases with pa.rt-of-speech tags." ></td>
	<td class="line x" title="12:147	An." ></td>
	<td class="line x" title="13:147	alternative approa.ch is a. statistical one which retrieves recurrent word sequences as co\]loca.tiolls (Sma.dja., 1993)(Ha.runo et a.1., 1996)(Shimolla.ta et a.1., :1997)." ></td>
	<td class="line x" title="15:147	This a.pproach is robust and pra.ctical because it uses t)lain text corpora, without a.ny inibrmation dependent on a la.ngua.ge." ></td>
	<td class="line x" title="16:147	Unlike the former N)proa.ch, this a.pproach extra.cts va.rious types of local pa.tterns a.t the same time." ></td>
	<td class="line x" title="17:147	Therefore, post-processing, such as part of speech ta.gging and syntactic category identifica.tion, is necessary when we a.pply them to NLP applica.tions." ></td>
	<td class="line x" title="18:147	This pa.per presents a. method for identifying technicM terms froni a. corpus and a.pl)lying them to a. ma.chine tra.nsla.tion system." ></td>
	<td class="line x" title="19:147	The proposed method retrieves local pa.tterns by utilizing the n-gram statistics a.nd identifies their syntactic categories with." ></td>
	<td class="line x" title="20:147	simple pa.rt-ofspeech teml)la.tes." ></td>
	<td class="line x" title="21:147	We ma.ke 3." ></td>
	<td class="line x" title="22:147	ma.chine trans\]a.tion dictiona.ry from the retrieved patterns and tra.nslate documents in the Sa.lne doma.in a.s the original corpus." ></td>
	<td class="line x" title="23:147	In the next section, we briefly describe a pa.ttern-based machine translation." ></td>
	<td class="line x" title="24:147	The following section explains how th.e proposed method works in detail." ></td>
	<td class="line x" title="25:147	We th.en present experimenta.l results a.nd conclude with a discussion." ></td>
	<td class="line x" title="26:147	2 Pattern-based MT system h pattern-ha.seal MT system uses a set of bilingua.1 pa.tterns(CFG rules) (Abeille et a.l., 1990) (Ta.keda., 1.996) (Shimohata." ></td>
	<td class="line x" title="28:147	et a.l., 1.999)." ></td>
	<td class="line x" title="29:147	In the pa.rsing process, the engine performs a. CFGparsing for a.n input sentence and rewrites trees by a.pplying the source pa.tterns." ></td>
	<td class="line x" title="30:147	3'erminals and non-terminals are processed under the sa.me fra.lnework but lexicalized pa.tterns ha.re priority over symbolized pa.tterns 1 A plausible parse We define a symbolized pattern as a pattern without a. terminal and ~L lexicalizcd pattern as that with more than one terminal, we prepares 1000 symbolized patterns a.nd 130,000 lexicalizcd patterns as a system 782 tree will be selected among possible parse trees by the number of l)atterns applied." ></td>
	<td class="line x" title="31:147	Then the pa.rse tree is tr~msferred into target language by using target patterns which correspond to the source patterns." ></td>
	<td class="line x" title="32:147	Figure 1 shows an example of translation patterns between Fmglish and.lapanese." ></td>
	<td class="line x" title="33:147	Each C1 G rule) has col English pattern(a left-half ',' ' responding aal)anese pattern(a right-half CFG rule)." ></td>
	<td class="line x" title="34:147	Non-terminals are bracketed with index numbers which represents correspondence of non-terminals between the source and target pattern." ></td>
	<td class="line x" title="35:147	S *--\[I:NP\] \[2:VP\] S ~--\[I:NP\] ~(subj) \[2:VP\] NP ~a \[I:NP\] NP *-\[I:NP\] VP ~---\[I:VT\] \[2:NP\] VP '~--\[2:NP\],~(dobj) \[I:VT\] VP +-take \[I:NP\] VP ~---\[I:NP\] ~(dobj)nj-7~('do') VP ~-take a bath VP '*-J~=t:t~('bath') \[5('in'),,'~'7~('enter') V ~-take V ',-'~7~('take') N '--' bath N ~--J:~,~('l)ath') Figure 1: translation l)atterns The pattern ibrmat is simple but highly descriptive." ></td>
	<td class="line x" title="36:147	It can represent complicated linguistic phenomena and even correspondences between the languages with quite different structures, l)'urthermore, a.l\] the knowledge necessary fl)r the translation, whether syntactic or lexical, are compiled in the same pattern tbrmat." ></td>
	<td class="line x" title="37:147	Owing to these fea.tures, we can easily apply the retrieved technical terms to a real MT system." ></td>
	<td class="line x" title="38:147	3; Algorithm 1,'igure 2 shows an outline of the l)roposed nlethod." ></td>
	<td class="line x" title="39:147	The inpu t is an untagged :~nonolingu al corpus, while the output is a dolnain dictionary for machine translation." ></td>
	<td class="line x" title="40:147	The process is con> prised of 3 phases: retrieving local patterns, assigning their syntactic categories with part-ofspeech(POS) templates, and making translation patterns." ></td>
	<td class="line x" title="41:147	The dictionary is used when an MT system translates a text in the same domain as the corpus." ></td>
	<td class="line x" title="42:147	We assume that the input is an English corpus and the dictionary is used for an EnglishJapanese MT system." ></td>
	<td class="line x" title="43:147	In the remainder of this section, we will explain each phase in detail with English and Japanese examples." ></td>
	<td class="line x" title="44:147	dictiona.ry." ></td>
	<td class="line x" title="45:147	3.1 Retrieving local patterns We have ah'eady proposed a method for retrieving word sequences (Shimohata et al. , 1997)." ></td>
	<td class="line x" title="46:147	This method generates all n-character (or nword) strings appearing in a text and tilters out ffagl-nenta.1 strings with the distribution of words adjacent to the strings." ></td>
	<td class="line x" title="47:147	This is based on the idea." ></td>
	<td class="line x" title="48:147	that adjacent words are widely distributed if the string is meaningful, m~d are localized if the string is a substring of a meaningful string." ></td>
	<td class="line x" title="49:147	The method introduces entropy value to measure the word distribution." ></td>
	<td class="line x" title="50:147	Let the string t)e 8tr, the adjacent words Wlw,~, and the frequency of str frcq(.slr)." ></td>
	<td class="line x" title="51:147	The probability of each possible adjacent word p(wi) is then: p(wi)frcq(wi) frcq(str) (\]) At ttla,t time~ the entropy of,~tr H(.qtr) is detined a.s: tl(,t,)." ></td>
	<td class="line x" title="52:147	= (2) i=1 Calculating the entropy of both sides of,qtr, the lower one is used as ll(,tr)." ></td>
	<td class="line x" title="53:147	Then the strings whose entropy is larger than a given threshold are retrieved as local pattexns." ></td>
	<td class="line x" title="54:147	3.2 Identifying syntactic categories Since the strings are just word sequences, the l)rocess gives tllem syntactic categories." ></td>
	<td class="line x" title="55:147	For each str .str~ 1." ></td>
	<td class="line x" title="56:147	assign pa.rt-ofspeech tags tl,  t~." ></td>
	<td class="line x" title="57:147	to the coH\]ponent words Wl,  /vr~ 2." ></td>
	<td class="line x" title="58:147	match tag sequence tl,  t,~ with part-ofspeech templates 7~ 3." ></td>
	<td class="line x" title="59:147	give sir corresponding syntactic category,5'6'i, it' it matches Ti 3.2.1 Assigning part-of-speech tags The process uses a simplified part-of speech set shown in table 1." ></td>
	<td class="line x" title="60:147	l?unction words are assigned as they are, while content words except for adverb are fallen into only one part of speech word." ></td>
	<td class="line x" title="61:147	Four kinds of words 'be', 'do', ''not', and 'to' are assigned to speciM tags be, do, not, and to respectively." ></td>
	<td class="line x" title="62:147	There are several reasons to use the simplitied POS tags: 783 Retrieve local patterns ---* Identify syntactic categories Make translation patterns 5 )n Figure 2: outline POS tag part of speech art adv aux eonj det prep prn punc be do not to word article adverb auxiliary verb conjunction determiner preposition pronoun punctuation . do~ '~Ot' 'to' th.e others Table 1: part-of-speech tags  it may sometimes be difl3cult to identify precise parts of speech in such a local pattern." ></td>
	<td class="line x" title="63:147	 words are often used beyond parts of speech in technical terminology  it is eml)irically found that word sequences retrieved through n-gram statistics have distributional concentration on several syntactic categories." ></td>
	<td class="line x" title="64:147	Theretbre, we think the simplified POS tags are sufficient to identify syntactic categories." ></td>
	<td class="line x" title="65:147	The word sequence w~,  w,~ is represented for a part-of-speech tag sequence tl,  ti." ></td>
	<td class="line x" title="66:147	Figure 3 shows examples of POS tagging." ></td>
	<td class="line x" title="67:147	Italic the fuel tank art word word do this step  do det,prn word punc to oprn the to word art Figure 3: examples of POS tagging lines are given word sequences and bold lines are POS tag sequences." ></td>
	<td class="line x" title="68:147	If a word falls into two or more parts of speech, all possible POSs wi\]\] be assigned like 'this' in the second example." ></td>
	<td class="line x" title="69:147	3.2.2 Matching POS templates The process identifies a syntactic category(SC) of sir by checking if str's tag sequence tl,  tn matches a given POS template 7}." ></td>
	<td class="line x" title="70:147	If they 784 match, str is given a syntactic category,5'Ci corresponding to 5/)." ></td>
	<td class="line x" title="71:147	Table 2 shows examt)les of I)OS teml)la.tes and corresl)onding SCs 2 SC POS template N Nprep VT V-ed V 1,'UNC (.,'0 (wo,.d l (.o,q), (,,)o,,d) (.,'0 (wo,.d) + (pw, I~o)(,,,'0." ></td>
	<td class="line x" title="74:147	(.,u.~ I~.o Iv,',,,) * (.,o,,d) + (.,.t) (~) (wo,.d) + (v,'~v)(.,'0: (.u. I~,o I l,,',,)(,~o,.a) ((.'~ \[ .*, I ~o.j Ida* I *','*, I v,',,)+ If SC is N, delete art and generate: NP '-st,NP +-str If SC is VT, delete (aux\[tolprn) and art and generate: VP (-str \[ 1 :NP\] VP ~-\[I:NP\] ~(dobj) st*' 'ej-~Cdo' ) If SC is v, delete (auxltolprn) generate: V +--sO' V *-str ~('do') 3'M)le 2: POS telnplates ~md corresponding SCs The templaes are described in the l'orm of regula.r expressions(Rl~;) a. The first templ~te in table 2, for exanrple, :m~tches a string whose tag sequence begins with an article, contains 0 or m ore rel)etitions of content word s or conj u n ctions, a.nd ends with a content word." ></td>
	<td class="line x" title="78:147	'the fuel ta,nk' in tigure 3 is applied to this templa.tes aald given a SC 'N'." ></td>
	<td class="line x" title="79:147	3.3 Making translation patterns The process converts the strings into translation l)a.tterns." ></td>
	<td class="line x" title="80:147	The l)roblem here is that we need to generate bilingual translation l)al;terns from monolingua\] strings." ></td>
	<td class="line x" title="81:147	We use heuristic rules on borr0wing word s from foreign \]angu ages 1 l!'igure 4 is an example of conversion rides tbr generating English-Jal)anese translation pa.tterns." ></td>
	<td class="line x" title="82:147	To give an exa.mple, 'to open tile' in figure 3, whose SC is vT, is converted into the following patterns in accorda.nce with the second rule in figure 4." ></td>
	<td class="line x" title="83:147	Figure d: conversion rules for generttting translation l)a.tterns 4 Evaluation VVe have tested our algorithln in building a doma.in dictionary and malting a. translation with it." ></td>
	<td class="line x" title="84:147	A corpus used in the exl)eriment is a COml)uter nlanual comprising 167,023 words (in 22,0d i sentences)." ></td>
	<td class="line x" title="85:147	The corl)us contains 24,7137 n-grooms which appear more than twice." ></td>
	<td class="line x" title="86:147	Among them, 7,6116 strings are extracted over the entropy threshold 1." ></td>
	<td class="line x" title="87:147	Table 3 is a list of top 20 strings (except for single words and function word sequences) retrieved from the test c()rptlS." ></td>
	<td class="line x" title="88:147	These strings a.re c~tego:rized into 1,239 POS patterns." ></td>
	<td class="line x" title="89:147	Table 4 is a. list of to I) 10 POS l)at;terns aim the numl)ers of strings classitied into thenl, hi this experiment, the top 10 POS patterns a.ccount for dg.d % of a.ll 1'OS patterns." ></td>
	<td class="line x" title="90:147	It substantiates the fa.ct that the retrieved strings tend to concentr~te in certa.in POS patterns." ></td>
	<td class="line x" title="91:147	VP ~--open \[I:NP\] VP *--\[I:NP\] :~(dobj) open ~7~('do') 2 Note that tile POS templates are strongly dependent on tile features of n-gram strings." ></td>
	<td class="line x" title="92:147	a,.,, causes tile resulting RP, to match 0 or more repetitions of the preceding IE." ></td>
	<td class="line x" title="94:147	'+' causes the resulting RE to match I or more rel)etitions of the preceding RI!'." ></td>
	<td class="line x" title="95:147	'1:' creates a RE exl)ression that will match either right o,: left of 'l''()' indicates the start and end of ~L group." ></td>
	<td class="line x" title="96:147	4 In Japanese, foreign words, especially in technical terminology, are often used as they are in katakana (tiLe phonetic spelling for foreign words) followed by function words which indicate their parts of speech For example, English verbs are followed by 'suru', a verb wliich means 'do' in English." ></td>
	<td class="line x" title="97:147	frcq POS 1886 553 368 229 160 158 121." ></td>
	<td class="line x" title="98:147	1.08 101 81 Wol:d word word art word art word word word prep word art word word word to word prep art word prep word Table 4: top 10 P()S p~tterns 785 .lI(str) freq(atr) .st,' IIH(,, -) freq(str) str 5.51 4.48 4.4:6 3.92 3.79 3.76 3.67 3.58 3.56 3.55 247 1499 100 106 163 309 297 36 169 180 see also the server click OK . use this function the function the following the file in the Server Manager, using the CGI programs 3.55 3.54 3.46 3.46 3 A4 3.36 3.29 3.23 3.22 3.22 552 209 209 168 172 192 132 213 71 575 the client use tim the user click the the catalog agent the request on page a specified if you want to your server Table 3: top 20 strings In the matching process, we prepared 15 templates and 6 SCs." ></td>
	<td class="line x" title="99:147	Table 5 is a result of SC identification." ></td>
	<td class="line x" title="100:147	2,462 strings(32.3 %) are not lnatched to any templates." ></td>
	<td class="line x" title="101:147	The table indicates that most strings retrieved in this method are identified as N and NP." ></td>
	<td class="line x" title="102:147	It is quite reasonable because the majority of the technical terms are supposed to be nouns and noun phrases." ></td>
	<td class="line x" title="103:147	improved in parsing 104 improved in word selection 467 about the same 160 same 21.2 not imt)roved 57 total 1000 SC number of patterns NP N+prep VP VP+prep VT V 722 200 32 10 177 78 Table 5: result of SC identification The retrieved translation patterns total 1,21.9." ></td>
	<td class="line x" title="104:147	Figure 5 shows an example of translation patterns retrieved by our method." ></td>
	<td class="line x" title="105:147	We, then, converted them to an MT dictionary and made a translation with and without it." ></td>
	<td class="line x" title="106:147	Table 6 summarizes the evaluation results translating randomly selected 1.,000 sentences fi'om the test corpus." ></td>
	<td class="line x" title="108:147	Compared with the translations without the dictionary, the translations with the dictionary improved 571 in parsing and word selection." ></td>
	<td class="line x" title="109:147	Figure 6 illustrates changes in translations." ></td>
	<td class="line x" title="110:147	Each column consists of an input sentence, a translation without the dictionary, and a translation with the dictionary." ></td>
	<td class="line x" title="111:147	Bold English words Table 6: Translation evaluation results correspond to underlined a apanese." ></td>
	<td class="line x" title="112:147	First two examples show improvement in word selection." ></td>
	<td class="line x" title="113:147	The transl ations of' map(verb)' and 'exec' are changed from word-for-word transla.tions to non-translation word sequences." ></td>
	<td class="line x" title="114:147	Although 'to make a map' and 'exective' are not wrong translations, they are irrelevant in the computer manual context." ></td>
	<td class="line x" title="115:147	On the contrary, the domain dictionary reduces confltsion caused by the wrong word selection." ></td>
	<td class="line x" title="116:147	Wrong parsing and incomplete p~rsing are also reduced as shown in the next two examples." ></td>
	<td class="line x" title="117:147	In the third example, 'Next' should be a noun, while it is usually used as an adverb." ></td>
	<td class="line x" title="118:147	The domain dictionary solved the syntactic ambiguity properly because it has exclusive priority over system dictionaries." ></td>
	<td class="line x" title="119:147	In the forth example, 'double-click' is an unknown word which could cause incomplete parsing." ></td>
	<td class="line x" title="120:147	But the phrase was parsed as a verb correctly." ></td>
	<td class="line x" title="121:147	The last one is an wrong example of Japanese verb selection." ></td>
	<td class="line x" title="122:147	That was a main cause of errors and declines." ></td>
	<td class="line x" title="123:147	The reason why the undesirable Japanese verbs were selected is that 786 NP *fiflly-qualified domain name NP ~ text search engine NP ~ access log for\[1 :NP\] VP *-save \[I:NP\] V ~ deallocate NP +-fully-qualified domain name NP ~ text search engine NP ~\[I:NP\] (/)('of') access log VP ~ \[I:NP\] ~(dobj)save '-4-7-o('do '') V ~ deallocate 71'~('do') l!'igure 5: tile retrieved transla.tion patterns Type the URL prefix you want to nmp." ></td>
	<td class="line x" title="124:147	&tgtztaqnap \[.,('perform mal~Zt,'~URL prefix ~'('ff\[C'a2L:kl,~'o The exee tag allows an IITML file to execute an arbitrary progranl on the server; ~('exeetive's lag') \[~+)---z {---~ HTML 7741bJa{fc,~tgJr21 q~.la exec ~ It: IITML 7741bh~ server 0){\]~,-~g'fft:lq~Ja~gt~{~gT~O){~ag ; Type the full name of your server, and then click Next." ></td>
	<td class="line x" title="126:147	.:.,~ It_ @~j~) i~tgtza)+Y--/l--cDB~tg~*jd-Jb-C~,Jv~btg~bXo ~t3tz(T) server O)~tg~@~4-3U~ Next,~ click I~t3~U~o Go to the Control Panel and dot,ble-elick the Services icon." ></td>
	<td class="line x" title="127:147	Cont,ol l'anel .'x~ta2;~t,x, ~5-eJ-a%\[~~tE.('double-') \[~ Services 74n>~ p IJ'yO~j-7~ ('elicld') o Control Panel -'x~'g Services icon,~double-click L,('double-click')td2~b~o Selling additional document directories ~I~\]N0) F'z~)tb'b-~4 D~JbIJ'~N<('put, place') 7'~ :~\]JlllY) document \[Z directory ~gT~('assi~~U_~ Figure 6: example sentences in the test corl)us the method added deta.ult semantic intbrmation to the retrieved nouns and noun phrases." ></td>
	<td class="line x" title="128:147	We hope to overcome it by a. model tha.t cla.ssilies noun pllrases, for example using verb-noun or a,djective-n ou :n relation s. 5 Related work As mentioned in section 1, there are two approaches in corpus-based technica.l term retrievah a rule-based approach and a statistical a~pproach." ></td>
	<td class="line x" title="129:147	Major ditlhre:nces between the two 3,re:  the former uses a tagged corlItls while the latter uses an untagged one." ></td>
	<td class="line x" title="130:147	 the former retrieves words and phrases with a designated syntactic category while the bttter :retrieves that with various syntactic categories at the same time." ></td>
	<td class="line x" title="131:147	Our method uses the latter ~pproa, ch because we think it more practical both in resources and in applications." ></td>
	<td class="line oc" title="132:147	For colnparison~ we refer here to Smadja's method (1993) because this method and the proposed method have much in connnon." ></td>
	<td class="line o" title="133:147	In both cases, technicaJ terms are retrieved from a.n untagged corpus with n-gram statistics and given syntactic categories for NI,P applica.tions." ></td>
	<td class="line x" title="134:147	The methods are diflhrent in that Sma.dja uses a 787 parser for syntactic category identification while we use POS templates." ></td>
	<td class="line x" title="135:147	A parser may add more precise syntactic category than I?OS templates." ></td>
	<td class="line x" title="136:147	However, we consider it not to be critical under the specific condition that the variety of input patterns is very small." ></td>
	<td class="line x" title="137:147	In terms of portability, the proposed method has an advantage." ></td>
	<td class="line x" title="138:147	Actually, adding POS templates is not so time consuming as developing a parser." ></td>
	<td class="line x" title="139:147	We have applied the translation patterns retrieved by this method to a real MT system." ></td>
	<td class="line x" title="140:147	As a result, 57.1." ></td>
	<td class="line x" title="141:147	% of translations were improved with 1,219 translation patterns." ></td>
	<td class="line x" title="142:147	To our knowledge, little work has gone into quantifying its effectiveness to NLP applications." ></td>
	<td class="line x" title="143:147	We recognize that the method leaves room for improvement in making translation patterns." ></td>
	<td class="line x" title="144:147	We, therefore, plan to introduce techniques for finding translational equivalent from bilingual corpora (Me\]amed, 1998) to our method." ></td>
	<td class="line x" title="145:147	6 Conclusion We have presented a method for identifying technical terminology and building a domain dictionary tbr MT. Applying the method to technical manuM in English yielded positive resuits." ></td>
	<td class="line x" title="146:147	We have found that the proposed method would dramatically improve the performance of translation." ></td>
	<td class="line x" title="147:147	In the future work, we plan to investigate the availability of POS patterns which are not categorized into any SCs." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C00-2121
Automatic Extraction Of Semantic Relations From Specialized Corpora
Thanopoulos, Aristomenis;Fakotakis, Nikos D.;Kokkinakis, George K.;"></td>
	<td class="line x" title="1:138	Automatic Extraction of Semantic Relations from Specialized Corpora Aristomenis Thanopoulos, Nikos Fakotakis and George Kokkinakis Wire Communications Laboratory Electrical & Computer Engineering Dept. , University of Patras 26500 Rion, Patras, Greece aristom@wcl, ee." ></td>
	<td class="line x" title="2:138	upatras.gr Abstract In this paper we address the problem of discovering word semantic similarities via statistical processing of text corpora." ></td>
	<td class="line x" title="3:138	We propose a knowledge-poor method that exploits the sentencial context of words for extracting similarity relations between them as well as semantic in nature word clusters." ></td>
	<td class="line x" title="4:138	The approach aims at full portability across domains and languages and therefore is based on minimal resources." ></td>
	<td class="line x" title="5:138	1 Motivation Providing digital computers with the capability to acquire conceptual relations between lexical items by processing real-life text corpora is not only an exciting research activity but also a significant task in the framework of many NLP systems." ></td>
	<td class="line x" title="6:138	Specifically: 1." ></td>
	<td class="line x" title="7:138	State-of-the-art Language Modeling techniques (McMahon and Smith., 1996) require lexical intbrmation about word classes." ></td>
	<td class="line x" title="9:138	2." ></td>
	<td class="line x" title="10:138	Thesauri creation in a (semi-) automatic manner in any domain and language with minimal dependence on specialized tools and resources is very important." ></td>
	<td class="line x" title="11:138	Most thematic domains today in most of the languages lack semantic resources." ></td>
	<td class="line x" title="12:138	Adopting a knowledge-poor corpusbased method not only much less labor is necessary in construction of conceptual structures but also domain-dependent semantic relations are obtained." ></td>
	<td class="line x" title="13:138	New resources can be readily created in new domains or existing thesauri can be enlarged or refined by retraining on larger corpora as soon as they become available." ></td>
	<td class="line x" title="14:138	3." ></td>
	<td class="line x" title="15:138	Many currently implemented, both spoken and written, NLP systems operate in a specific domain and usually utilize a constrained vocabulary related directly to their task domain." ></td>
	<td class="line x" title="16:138	Therefore semantic domain-dependent knowledge can be acquired directly from relevant corpora." ></td>
	<td class="line x" title="17:138	4." ></td>
	<td class="line x" title="18:138	Autonomous computational intelligence should rely mainly on processing of tree flow electronic texts for acquiring new semantic and world knowledge." ></td>
	<td class="line x" title="19:138	The present approach aims at corpus-based automatic extraction of domain-dependent semantic similarity relations between lexical items and the formation of corresponding semantic clusters." ></td>
	<td class="line x" title="20:138	For this purpose, the usage of readily available domain-specific text corpora is imperative." ></td>
	<td class="line x" title="21:138	The guideline of our approach was the adaptation to the special characteristics of this type of corpora (specialization, restricted size) without imposing the need for other domaindependent resources and obtaining portability across languages." ></td>
	<td class="line x" title="22:138	2 Related work Three main approaches have been proposed for the automatic extraction of lexical semantics knowledge: syntax-based, n-gram-based and window-based." ></td>
	<td class="line x" title="23:138	Syntax-based methods (referred also as knowledge-rich in contrast to the others knowledge-poor methods) (Pereira and Thishby, 1992; Grefenstette, 1993; Li and Abe, 1997) represent the words under consideration as vectors containing statistic values of their syntactic properties in relation to a given set of words (e.g. statistics of object syntax relations referring to a set of verbs) and cluster the considered words according to similarity of the corresponding vectors." ></td>
	<td class="line x" title="24:138	Methods that use bigrams (Brown et al. , 1992) or trigrams (Martin et al. , 1998) cluster words considering as a word's context the one or two immediately adjacent words and employ as clustering criteria the minimal loss of average 836 nmtual information and the perplexity improvement respectively." ></td>
	<td class="line x" title="25:138	Such methods are oriented to language modeling and aim primarily at rough but fast clustering of large vocabularies." ></td>
	<td class="line x" title="26:138	Brown et al.(1992) also proposed a window method introducing the concept of 'semantic stickiness' of two words as the relatively frequent close occurrence between them (less than 500 words distance)." ></td>
	<td class="line x" title="28:138	Although this is an efficient and entirely knowledge-poor method tbr extracting both semantic relations and clusters, the extracted relations are not restricted to semantic similarity but extend on thematic roles." ></td>
	<td class="line x" title="29:138	Moreover its applicability to small and specialized corpora is uncertain." ></td>
	<td class="line x" title="30:138	3 A knowledge-poor approach In order to achieve portability we approach the issue from a knowledge-poor perspective." ></td>
	<td class="line x" title="31:138	Syntaxbased methods employ partial parsers which require highly language-dependent resources (morphological/grammatical analysis), and/or properly tagged training corpus in order to detect syntactic relations between sentence constituents." ></td>
	<td class="line x" title="32:138	On the other hand, n-gram methods operate on large corpora and, in order to reduce computational resources, consider as context words only the immediately adjacent ones." ></td>
	<td class="line x" title="33:138	Medium-distance word context is not exploited." ></td>
	<td class="line x" title="34:138	Since large corpora are available only for few domains we aimed at developing a method for processing small or medium sized corpora exploiting the most of contextual information, that is, the tifll sentential context of words." ></td>
	<td class="line x" title="35:138	Our approach was driven by the observation that in domain-constrained corpora, unlike fiction or general journalese, the vocabulary is limited, the syntactic structures are not complex and that medium-distance lexical patterns are frequently used to express similar facts." ></td>
	<td class="line x" title="36:138	Specifically we have developed two different algorithms in respect to the context consideration they employ: Word-based and Pattern-based." ></td>
	<td class="line x" title="37:138	The former acquires word-based contextual data (extended up to sentence boundaries), according to the distributional similarity of which, word similarity relations are extracted." ></td>
	<td class="line x" title="38:138	The latter detects common patterns throughout the corpus that indicate possible word similarities." ></td>
	<td class="line x" title="39:138	For example, consider the sentence fragments: 'while the S&I' index inched up 0.3%'." ></td>
	<td class="line x" title="40:138	'The DAX #Mex inched up O. 70 point to close' Although their syntactic structures are different, the common contextual pattern (appearing beyond immediately adjacent words) indicates a possible similarity between the tokens 'S&P' and 'DAX'." ></td>
	<td class="line x" title="41:138	Word pairs that persistently appear such context similarity throughout the corpus (frequently observed in technical texts) are confidently indicated as semantically similar." ></td>
	<td class="line x" title="42:138	Our method captures such context similarity and extracts a proportionate measure about semantic similarity between lexical items." ></td>
	<td class="line x" title="43:138	Most approaches (Brown et al. , 1992; Li & Abe, 1997) inherently extract semantic knowledge in the abstracted form of semantic clusters." ></td>
	<td class="line x" title="44:138	Our method produces semantic similarity relations as an intermediate (and information-richer) semantics representation formalism, from which cluster hierarchies can be generated." ></td>
	<td class="line x" title="45:138	Of great importance is that soft clustering methods can also be applied to this set of relations and cluster polysemous words to more than one classes." ></td>
	<td class="line x" title="46:138	Stock lnarket-financial news and Modem Greek, were used as domain and language test case respectively, ltowever demonstrative examples taken from the WSJ corpus have been used throughout the paper as well." ></td>
	<td class="line x" title="47:138	4 Context Similarity Estimation The main idea supporting context-based word clustering is that two words that can substitute one another in several different contexts always providing meaningful word sequences are probably semantically similar." ></td>
	<td class="line x" title="48:138	Present n-gram based methods utilize this assumption considering as a context of a focus word only the one or two immediately adjacent parameter words." ></td>
	<td class="line x" title="49:138	In the present work, we consider as word context the whole sentence in which the examined word appears, excluding only the semantically empty (i.e. functional) words such as articles, conjunctions, particles, auxiliaries." ></td>
	<td class="line x" title="50:138	Adopting this word context notion we proceed to tile ibllowing analysis: Let us consider a text corpus Tc with vocabulary Vc and Vs _c Vc the set of words that are of interest in extracting semantic similarity relations between them." ></td>
	<td class="line x" title="51:138	Vx comprises the non-functional words of 837 Vc appearing in Tc with a frequency higher than a threshold (set to 20 in the presented experiments) in order to acquire sufficient data for every focus word." ></td>
	<td class="line x" title="52:138	Let Vr_~ Vc be the set of words that will be used as context parameters." ></td>
	<td class="line x" title="53:138	Ideally, any word appearing at least twice in the corpus could be used as context parameter." ></td>
	<td class="line x" title="54:138	However we specified this word frequency threshold to 10 in order to diminish computational time." ></td>
	<td class="line x" title="55:138	Consider a sentence of Tc: Sm= WI,W2,-,Wj-I,Wj,Wj+I,,Wk We define as sentential context o fw~ in S,,, the set of the pairs of the sentence words which are members of Vp, accompanied by their corresponding distance from wj: Cs.,(wi)= {(i-j,w~),i=lk,(i ~j), Vw~ e VI, } Equation (1): Sentential context of wi in S,,, More formally, Cs,.(wi) can be represented as a binary-valued matrix defined over the set ~t =Sxo) where 8={ -l,l,-2,2,,-Lm,Lm}, Lm being the maximum word distance we regard that carries useful contextual information (for full sentence distance Lm=Lma~-I where L,,,x the maximum sentence length in Tc), and o~ the ordered set Vs: Cs,,, (wi) = {ci,,, (d, w)}, where: de&wE~ cj,,.(d,w)=J'l, w=wj,w i cco, d=i-j (2) 0, otherwise Summing over all corpus sentences we obtain the contextual data matrices for every wj c V s : (w/) : {cj (d, w)} = C,,, (w j) (s) de6, we0) m The word semantic similarity estimation has been reduced to matrices similarity estimation." ></td>
	<td class="line x" title="57:138	The obtained contextual matrices are compared using a weighted Tanimoto measure (Chamiak, 1993) and a word similarity measure S,.(wl,wj) is obtained: ZE\[h(d)' ci(d,w)' cj(d,w)l d w Sm(wi,wj) = ZEmaxci (d,w), cj(d,w)} (4) d w The weight function h(d) defines the desired influence that the distance between words should have to context similarity estimation." ></td>
	<td class="line x" title="58:138	In this experiment we set: h(d)=l/ld I. In order to reduce computational time the denominator was set to ZEci(d,w)+ZZcj(d,w), a modification d w d w that has minimal effect on the final result." ></td>
	<td class="line x" title="59:138	Experimental results of this method (Word-based Context Similarity EstimationWCSE), are shown in Table 1." ></td>
	<td class="line x" title="60:138	Note that, since the Cfc,(wj) matrix is sparse, (l) was used as data storing formula instead of (2), in order to diminish computational cost." ></td>
	<td class="line x" title="61:138	The previously described algorithm is handling all contextual data in a uniform way." ></td>
	<td class="line x" title="62:138	However, study of the results showed that preference should be given to hits derived from many different similar contexts instead of few ones appearing many tilnes." ></td>
	<td class="line x" title="63:138	This would clearly give better results since the latter case may be due to often-used stereotyped expressions or repeated ~3cts." ></td>
	<td class="line x" title="64:138	In order to achieve this we modified (4) to: ZZ\[h(d)' log2 \[ci (d,w)  cj (d,w)~ d w d w d w Indeed the experimental results of this variation (Variant WCSE VWCSE) show a significant improvement (see Table 1 )." ></td>
	<td class="line x" title="65:138	5 Dynamic pattern detection for context similarity estimation In the previously described method the notion of word context is based on independent intrasentential word co-occurrences." ></td>
	<td class="line x" title="66:138	Itowever similarity of contextual patterns is much more reliable word similarity criterion than word-based context similarity." ></td>
	<td class="line x" title="67:138	That is, if the sentential contexts Cs,,,(w~) and Cs,(w/) have at least two common elements, we count this as a much more confident hit regarding the wi and w.i similarity." ></td>
	<td class="line x" title="68:138	A measure expressing the weight of the common pattern is obtained." ></td>
	<td class="line x" title="69:138	Since the patterns under detection vary across languages and domains we need a method that extracts them dynamically, regardless of the text genre, domain or language." ></td>
	<td class="line x" title="70:138	For this purpose we propose an algorithm that performs a sentence-by-sentence comparison along the corpus." ></td>
	<td class="line x" title="71:138	This comparison is based on the 838 cross-correlation concept as it is used in digital signal processing." ></td>
	<td class="line x" title="72:138	A sentence can be considered as a digital signal where every semantic token corresponds to a signal sample." ></td>
	<td class="line x" title="73:138	In order to detect words with common contexts every sentence is checked on matching every other one partially (i.e. matching the semantic category of one or more tokens) on every possible relative position between the two sentences." ></td>
	<td class="line x" title="74:138	Wherever colnmon patterns of semantic tokens are lbund the neighboring respective tokens on the two sentences are stored as candidate semantic relatives." ></td>
	<td class="line x" title="75:138	During this process contextual data are not maintained in memory; instead the detection of a common pattern in both sentences results to the storage of several hits (i.e. candidate silnilar word pairs) or to the increase of their corresponding similarity measure according to the pattern similarity ot'their contexts." ></td>
	<td class="line x" title="76:138	Let Sm and S, be two sentences that undergo the cross-correlation procedure." ></td>
	<td class="line x" title="77:138	If 8~={dx, x=lxl, xl>l }, is the set of word distances that satisfy the equality: ci,,,, (d,, Wy) = c./.,, (d,-, Wy ) = 1, then the pair (wi,wi) is stored as a hit accompanied by the l'ollowing context similarity measure:.v 1 .v 1 Keeping only the first term we obtain the same result as in the WCSE method with weight function h(d) =l/\]d\[." ></td>
	<td class="line x" title="79:138	The second term augments the score in proportion to the cohesion and the size o1' the: detected pattern depending on the position of wi (or, equivalently, wi)." ></td>
	<td class="line x" title="80:138	I)ividing (6) by the total length of S,,, and S." ></td>
	<td class="line x" title="81:138	(i.e. 1~.,,, =L,cI. ,, ) we obtain a normalized measure of the cross-correlation of the two sentences: 1,,,,,, (lv,, w.i ) F ',,,,,, (u,,, ~,j) : (7) L IIIll The total similarity measure is obtained from: (8) m n~n applied throughout the corpus." ></td>
	<td class="line x" title="83:138	In order to reduce search thne and required memory during the whole process a pruning mechanism is applied at regular time intervals to eliminate word pairs with a relatively very low semantic similarity score." ></td>
	<td class="line x" title="84:138	Dividing (8) by the product of the word probabilities P(wi)-P(wi) we obtain the normalized similarity measure FN(Wl,Wj)." ></td>
	<td class="line x" title="85:138	in order to constrain the degradation of our results due to sparse data regarding less frequent words, we multiply (8) by Pc, a data sufficiency measure function of P(wi) and P(wi), obtaining Fu, a more reliable measure, ltere we employed: .r, J' < 1' c(P, 1')= -1~ ' 1~." ></td>
	<td class="line x" title="86:138	./ (9) C 1, otherwise where we used PTI,=30/ITcl, ITcl being the size of the corpus." ></td>
	<td class="line x" title="87:138	Finally, sorting the resulting pairs by Fu and keeping the N-best scoring pairs, we obtain the preponderant semantically related candidates." ></td>
	<td class="line x" title="88:138	6 Preprocessing In order to apply the above described algorithms some preprocessing is necessalT: 1." ></td>
	<td class="line x" title="89:138	A trainable sentence splitter and a rule-based chunker are applied." ></td>
	<td class="line x" title="90:138	Sentence boundaries confine the scope of context while phrase boundaries determine the nmximunl extent of semantic tokens (see below)." ></td>
	<td class="line x" title="91:138	2." ></td>
	<td class="line x" title="92:138	The next step of the preprocessing is what we call 'xemantic lokenizalion'." ></td>
	<td class="line x" title="93:138	We try to reduce context parameters and simultaneously to incerease the volulne of contextual data either by reducing the volume of both the focus and parameter word set or by discaring or merging lexical items resulting in reduction of the distance between semantic tokens." ></td>
	<td class="line x" title="94:138	Words or word sequences are thus classified in common semantic categories employing syntactical, morphological and coltocational intbrmation: a.Functionals (auxiliaries, determiners) are discarded since they do not modit) semantically their head words." ></td>
	<td class="line x" title="95:138	Words of indeterminable semantic content (pronouns, low fi'equency words) are treated as empty tokens." ></td>
	<td class="line x" title="96:138	b.Known domain-independent lexical patterns incorporating arithmetic and temporal 839 expressions (e.g. dates, numbers, amounts, etc)." ></td>
	<td class="line x" title="97:138	are regarded as a single semantic token and tagged accordingly." ></td>
	<td class="line x" title="98:138	Their information content is indifferent to semantic kmowledge acquisition; therefore we preserve only class information." ></td>
	<td class="line x" title="99:138	c.Frequently appearing lexical patterns which represent single semantic entities in the specific domain are treated as a single (albeit composite) 'semantic token'." ></td>
	<td class="line oc" title="100:138	Their detection is based on the following algorithm (cf.Smadja, 1993): 1." ></td>
	<td class="line x" title="102:138	Extract 'significant bigrams' confined inside noun phrases i.e. immediately adjacent words that contain a relatively high amount of nmtual information: I  (w.,w2)= log 2 P(w 'w2) (10) P(w t )P(w 2 ) 2." ></td>
	<td class="line x" title="103:138	Combine significant bigrams together to obtain 'significant n-grams' found in the corpus and confined inside noun phrases as well." ></td>
	<td class="line x" title="104:138	Discard subsumed m-grams (re<n) only if they do not occur indepentently in the corpus." ></td>
	<td class="line x" title="105:138	3." ></td>
	<td class="line x" title="106:138	Tag throughout the corpus the significant ngrams as single semantic tokens, starting from the higher-order ones." ></td>
	<td class="line x" title="107:138	Semantic entities that are lexically represented as sticky word chains may be either standard in the framework of the information extraction task named entities, such as 'Latin America' (location), 'Russian president Boris Yeltsin' (person), 'Tpdne~a Mct~c6ovk%-Qpdmlg' ('Bank of Macedonia and Thrace'; organization) or representations of donminspecific typical events ('Gt6~qm 1 tm'coztKo6 Keq)c0~cdon' = rise of equity capital), abstract concepts ('Dow Jones industrials'), etc. To ensure that the detected 'sticky' phrases actually represent semantic entities, human inspection is necessary for discarding the spurious ones, since repeated word sequences that do not constitute always single semantic entities often appear in specialized texts." ></td>
	<td class="line x" title="108:138	From the above it is apparent that we use the term 'semantic token' to refer to a recognized semantic pattern (e.g. <date>), a rigid word chain (e.g. 'Dow Jones industrials') or a single content word." ></td>
	<td class="line x" title="109:138	The context similarity estimation algorithms were run using vocabularies of focus and parameter words derived from the extracted set of semantic tokens." ></td>
	<td class="line x" title="110:138	7 Incorporating heuristics From the study of the erroneously extracted semantic relations certain systematic errors were detected." ></td>
	<td class="line x" title="111:138	For example, adjectives, adverbs or adjunctive nouns that occur interpolating in otherwise similar contexts lead to the extraction of spurious pairs." ></td>
	<td class="line x" title="112:138	Consider ~br example the phrases: '11 c~6~qml 'nlg ztgl\]g 'Cllg J~v~ixrll~' (= the increase of the benzine price) and '11 c~6~qcn 1 zqg 'cqa\]g ~d0~31mlg zqg \[~cv~iwl' (=the increase of the di,sposal benzine price)." ></td>
	<td class="line x" title="113:138	Every algorithm based on word adjacency data outputs as erroneous hits the pairs {benzine-disposal} and { increase-disposal}." ></td>
	<td class="line x" title="114:138	A rule that was applied to deal with this problem is: If wiGS m and wjGSn have similar contexts, count the pair (wi,w j) as a hit only if wi~wj+3 and wj~wi+1." ></td>
	<td class="line x" title="115:138	Such contextual rules can be applied only using the cross-correlation method for the context similarity estimation (either pattern-based or word-based)." ></td>
	<td class="line x" title="116:138	8 Word Clustering Although the obtained semantically related N-best pair list constitutes already a thesaurus-like and information-rich form of semantic knowledge representation, many NLP applications (e.g. language modeling) require word clusters instead of word relations." ></td>
	<td class="line x" title="117:138	However, since a word similarity measure has been extracted, the formation of clusters is a rather trivial problem, although more complex for 'soft clustering' (i.e. a word can be classified in more than one classes)." ></td>
	<td class="line x" title="118:138	In order to construct word classes we applied the unsupervised agglomerative hard clustering algorithn3 shown in Figure 1 over the set of senmntic relations." ></td>
	<td class="line x" title="119:138	Each distinct lexical item is initially assigned to a cluster and then clusters are merged into larger ones according to the average linkage measure." ></td>
	<td class="line x" title="120:138	Merging of clusters stops when the distance between the more proximate clusters exceeds a threshold proportional to the average distance between words." ></td>
	<td class="line x" title="121:138	Tracking the successive merges we obtain sub-cluster hierarchies, such as the one shown in l<igure 2." ></td>
	<td class="line x" title="122:138	840 Repeat until mJn(AvgDistance (C/,C I ) ) > k  for every cluster C:~ for every cluster Ca@CT calculate AvgDJstance(C;,C\[ ) ::merge C',, Arg Inin (AvgDis tall c'e(C'.i,(\]l)) C,/ 1 Z distance(w i, w j) Ivsl 2 v,,,,.,,,,~,, 1 . Z distance(w,wj) Figure 1 : Unsupervised Itard Clustering Algorithm 9 Experimental Results The reported experinaents have been carried out on a 220.000 words corpus, comprised o1' financial news of 1998, which was constructed in the framework of a currently carried out R&D project for Information Extraction from raw text ~." ></td>
	<td class="line x" title="124:138	The methods and their variations described in sections 4 and 5 for obtaining lexical senmntic relations were tested and their accuracy per nurnber of best hits was measured by hunmn inspection." ></td>
	<td class="line x" title="125:138	The VWCSE method was tested using only the previous and next word as context parameters (N&P method), to sketch a method baseline lbr the particular corpus." ></td>
	<td class="line x" title="126:138	Using a Morphological Analyzer & Part-of Speech tagger to restrict semantic relations only between words of the same Part-of-Speech (PoS) we obtain apparently higher accuracy, though we loose some interesting verb noun pairs referring to the same action or condition, e.g.(mOiOqKc (=increased) and dvogo.q (=increment)." ></td>
	<td class="line x" title="128:138	The results indicate that the norlnalization factors indeed iinprove the accuracy of the methods and that context similarity detection based on dynamic patternmatching yields significantly more reliable results than the word-based method." ></td>
	<td class="line x" title="129:138	This demonstrates the importance of the cross-correlation algorithm, which is the only suitable tbr pattern-based context similarity detection." ></td>
	<td class="line x" title="130:138	Regarding the clustering procedure, a set of 1300 words was clustered to 84 hierarchically structured clusters." ></td>
	<td class="line x" title="131:138	Considering an interested cluster formed (Figure 2) we note that from the 18 lexical entities (words or rigid phrases) that constitute the cluster all but two refer to money 1 Project 'M1TOS' of the Greek General Sec|'etariat lbr Reseach and Technology investment o1' profit." ></td>
	<td class="line x" title="132:138	Froln the vocabulary subject to clustering 4 words belonging to the same class were not detected; therefore accuracy and recall for the specific cluster were found at 88.9% and 80% respectively." ></td>
	<td class="line x" title="133:138	Although comparision with other knowledge-poor methods would be very useful it was not realized, nminly because our method produces semantic relations while other methods produce semantic clusters and our clustering process is not yet elaborated enough to yield quality results." ></td>
	<td class="line x" title="134:138	Lexical ltem English TransL Kox'~o)d0)v outlays/g capilals _ w~ot6tl m l\] ui dit),/~___ cxcx'66ocl~ investments cn~:x'~ocn 1 investment *xpoTpd\[qtct'ro ~ program/g ~kLmv &ltmo\[oo state stocks/g oltoLd,/(0v income bonds/g c.wdwo)v_yptqt\[tat\[ow time notes/g ~q\[tt~ losses K;p& 1 profits KfvmO~:oclc; deposils ~q~u(ov losses/g a(0Lqcmt~ purchases co680w incomes/g ~c~o&t incomes m)vcO, Lfty~ dealings *o6\[t~ctoq contract Figure 2: A derived sample hierarchial cluster of lexieal entities ('/g' denotes genitive case) 841 METHOD N&P WCSE VWCSE CCPM CCPM-N CCPM-N-F CCPM-N-F-Pc PoS & VWCSE CCPM CCPM-N CCPM-N-F CCPM-N-F-Pc Precision (%) per number of best hits 100 1200 1300 1400 64 61 57.7 54.75 72 65.5 61.7 57 81 7O 66 62.5 74 59 54 50.5 89 81.5 70.3 63 90 80.5 72.7 67.25 93 82 71.3 66.75 86 80 75 67.5 86 77.5 68 59.5 93 88 79 74 95 88.5 83 77 97 89 82.7 77 N&P: Context = next and previous word WCSE: every word into the sentence is taken as context parameter evenly Eq.(4) VWCSE: contextual similarity variance is favored Eq.(5) CCPM: Dynamic Pattern-Matching based on Cross-Correlation Eq." ></td>
	<td class="line x" title="135:138	(6)&(8) CCPM-N: normalized by L  (7)&(8) CCPM-N-F: normalized by P(wi)' P(wj) CCPM-N-F-Pc: normalized by Pc, Eq.(9) Table I: Comparative Results and Explanation Memo 10 Conclusion Initiating from the conception of word similarity estimation in terms of context similarity we have proposed an approach with several variations for extracting semantic similarity relations betweenlexical entities by processing word adjacency data obtained from small or medium sized corpora." ></td>
	<td class="line x" title="136:138	The described cross-correlation procedure, offers the possibility to dynamically detect pattern context similarities offering strong evidence for semantic similarity." ></td>
	<td class="line x" title="137:138	The presented algorithm featureslanguage and domain portability and the ability to classify keywords irrespective of their grammatical characteristics." ></td>
	<td class="line x" title="138:138	The implementation of the soft clustering algorithm, the test of the method to a different domain and language and the quantified comparison with other kamwledge-poor methods are quite interesting matters belonging to future work." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="J00-3001
Extracting The Lowest-Frequency Words: Pitfalls And Possibilities
Weeber, Marc;Baayen, R. Harald;Vos, Rein;"></td>
	<td class="line x" title="1:319	Extracting the Lowest-Frequency Words: Pitfalls and Possibilities Marc Weeber* University of Groningen R. Harald Baayen* Max Planck Institute for Psycholinguistics Rein Vos t University of Groningen, University of Maastricht In a medical information extraction system, we use common word association techniques to extract side-effect-related terms." ></td>
	<td class="line x" title="2:319	Many of these terms have a frequency of less than five." ></td>
	<td class="line x" title="3:319	Standard word-association-based applications disregard the lowest-frequency words, and hence disregard useful information." ></td>
	<td class="line x" title="4:319	We therefore devised an extraction system for the full word frequency range." ></td>
	<td class="line x" title="5:319	This system computes the significance of association by the log-likelihood ratio and Fisher's exact test." ></td>
	<td class="line x" title="6:319	The output of the system shows a recurrent, corpus-independent pattern in both recall and the number of significant words." ></td>
	<td class="line x" title="7:319	We will explain these patterns by the statistical behavior of the lowest-frequency words." ></td>
	<td class="line x" title="8:319	We used Dutch verb-particle combinations as a second and independent collocation extraction application to illustrate the generality of the observed phenomena." ></td>
	<td class="line x" title="9:319	We will conclude that a) word-association-based extraction systems can be enhanced by also considering the lowest-frequency words, b) significance levels should not be fixed but adjusted for the optimal window size, c) hapax legomena, words occurring only once, should be disregarded a priori in the statistical analysis, and d) the distribution of the targets to extract should be considered in combination with the extraction method." ></td>
	<td class="line x" title="10:319	1." ></td>
	<td class="line x" title="11:319	Introduction The research reported here arose from an attempt to determine the conditions under which optimal recall and precision are obtained for the extraction of terms related to side effects of drugs in medical abstracts." ></td>
	<td class="line x" title="12:319	We used the standard technique of defining a window around a seed term, side-effect in our case, and selected as potentially relevant terms those words that appeared more often in these windows than expected under chance conditions." ></td>
	<td class="line x" title="13:319	Our original question concerned the extent to which recall and precision are influenced by the size of the window." ></td>
	<td class="line x" title="14:319	It turns out, however, that a preliminary question needs to be answered first, namely, how to gauge the significance of the large effect of the lowest-frequency words on recall, precision, and the number of words extracted as potentially relevant terms." ></td>
	<td class="line x" title="15:319	* Groningen University Institute for Drug Exploration, Department of Social Pharmacy and Pharmacoepidemiology, Ant." ></td>
	<td class="line x" title="16:319	Deusinglaan 1, 9713 AV Groningen, The Netherlands." ></td>
	<td class="line x" title="17:319	E-mail: marc@farm.rug.nl t Faculty of Health Sciences, Department of Health Ethics and Philosophy, P.O. Box 616, 6200 MD Maastricht, The Netherlands." ></td>
	<td class="line x" title="18:319	E-mail: rein.vos@zw.unimaas.nl :~ Max Planck Institute for Psycholinguistics, P.O. Box 310, 6500 AH Nijmegen." ></td>
	<td class="line x" title="19:319	E-mail: baayen@mpi.nl (~) 2000 Association for Computational Linguistics Computational Linguistics Volume 26, Number 3 o.i Z 150 100 50 0 I \]lrll,,,.,, 5 10 15 20 0.15 Z 0.10 Z 0.05 5 10 15 20 Frequency Class Frequency Class (a) (b) Figure 1 Frequency distribution of medical expert word types." ></td>
	<td class="line x" title="21:319	Panel (a) shows the number of side-effect-related word types as judged by a medical expert (Nexpert) as a function of the first 23 frequency classes." ></td>
	<td class="line x" title="22:319	Panel (b) shows the proportion of expert types/total corpus types (Ntotal) for the first 23 frequency classes." ></td>
	<td class="line x" title="23:319	The horizontal dashed line indicates the mean proportion of 0.0619." ></td>
	<td class="line x" title="24:319	It is common practice in information retrieval to discard the lowest-frequency words a priori as nonsignificant (Rijsbergen 1979)." ></td>
	<td class="line oc" title="25:319	In Smadja's collocation algorithm Xtract, the lowest-frequency words are effectively discarded as well (Smadja 1993)." ></td>
	<td class="line x" title="26:319	Church and Hanks (1990) use mutual information to identify collocations, a method they claim is reasonably effective for words with a frequency of not less than five." ></td>
	<td class="line x" title="27:319	A frequency threshold of five seems quite low." ></td>
	<td class="line x" title="28:319	Unfortunately, even this lower frequency threshold of five is too high for the extraction of side-effect-related terms from our medical abstracts." ></td>
	<td class="line x" title="29:319	To see this, consider the left panel of Figure 1, which plots the number of side-effect-related words in our corpus of abstracts as judged by a medical expert, as a function of word-frequency class." ></td>
	<td class="line x" title="30:319	The side-effect-related words with a frequency of less than five account for 295 of a total of 432 expert words (68.3%)." ></td>
	<td class="line x" title="31:319	The right panel of Figure 1 shows that the first 23 word-frequency classes are characterized by, on average, the same proportion of side-effect-related words." ></td>
	<td class="line x" title="32:319	The a priori assumption of Rijsbergen (1979) that the lowest-frequency words are nonsignificant is not warranted for our data, and, we suspect not for many other data sets as well." ></td>
	<td class="line x" title="33:319	The recent literature has seen some discussion of the appropriate statistical methods for analyzing the contingency tables that contain the counts of how a word is distributed inside and outside the windows around a seed term." ></td>
	<td class="line x" title="34:319	Dunning (1993) has called attention to the log-likelihood ratio, G 2, as appropriate for the analysis of such contingency tables, especially when such contingency tables concern very low frequency words." ></td>
	<td class="line x" title="35:319	Pedersen (1996) and Pedersen, Kayaalp, and Bruce (1996) follow up Dunning's suggestion that Fisher's exact test might be even more appropriate for such contingency tables." ></td>
	<td class="line x" title="36:319	We have therefore investigated for the full range of word frequencies whether there is an optimal window size with respect to recall and the number of significant words extracted using both the log-likelihood ratio and Fisher's exact test." ></td>
	<td class="line x" title="37:319	In Section 2, we will show that indeed there seems to be an optimal window size for both statistical tests." ></td>
	<td class="line x" title="38:319	However, a recurrent pattern of local optima calls this conclusion into question." ></td>
	<td class="line x" title="39:319	Upon closer inspection, this recurrent pattern appears at fixed ratios of the number of words inside the window to the number of words outside the window (complement)." ></td>
	<td class="line x" title="40:319	302 Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words In Section 3, we will relate the recurrent patterns of local optima at fixed windowcomplement ratios (henceforth W/C-ratios) to the distributions of the lowest-frequency words over window and complement." ></td>
	<td class="line x" title="41:319	We will call attention to the critical effect of the choice of W/C-ratios on the significance of the lowest-frequency words." ></td>
	<td class="line x" title="42:319	As the improvement in the extraction of side-effect terms from medical abstracts, as gauged by the F-measure, which combines recall and precision (Rijsbergen 1979), is small, we also applied the same approach to the extraction of Dutch verb-particle combinations from a newspaper corpus." ></td>
	<td class="line x" title="43:319	In Section 4, we report substantially better results for this more lexical extraction task, which is subject to the same statistical behavior of the lowest-frequency words." ></td>
	<td class="line x" title="44:319	In the last section, we will discuss the consequences of our findings for the optimization of word-based extraction systems and collocation research with respect to the lowest-frequency words." ></td>
	<td class="line x" title="45:319	2." ></td>
	<td class="line x" title="46:319	An Optimal Window Size for Medical Abstracts?" ></td>
	<td class="line x" title="47:319	The MEDLINE bibliographic database contains a large number of abstracts of scientific journal papers discussing medical and drug-related research." ></td>
	<td class="line x" title="48:319	Typically, abstracts discussing medical drugs mention the side effects of these drugs briefly." ></td>
	<td class="line x" title="49:319	Information on side effects is potentially relevant for finding new applications for existing drugs (Rikken and Vos 1995)." ></td>
	<td class="line x" title="50:319	We are therefore interested in any terms related to the side effects of drugs." ></td>
	<td class="line x" title="51:319	Before proceeding, it may be useful to clarify the way in which the present research differs from standard research on collocations." ></td>
	<td class="line x" title="52:319	In the latter kind of research, there is no a priori knowledge of which combinations of words are true collocations." ></td>
	<td class="line x" title="53:319	Moreover, the most salient collocations generally are found at the top of a list ranked according to measures for surprise or association, such as G 2 or mutual information (Manning and Sch~itze 1999)." ></td>
	<td class="line x" title="54:319	The large numbers of word combinations with significant but low values for these measures are often of less interest." ></td>
	<td class="line x" title="55:319	Low-frequency words are predominant among these kinds of collocations." ></td>
	<td class="line x" title="56:319	In our research, we likewise find many low-frequency terms for side effects with low ranks in medical abstracts." ></td>
	<td class="line x" title="57:319	The relatively well-known side effects that are mentioned frequently can be captured by examining the top ranks in the lists of extracted words." ></td>
	<td class="line x" title="58:319	At the same time, the rarely mentioned side-effect terms are no less important, and in post marketing surveillance the extraction of such side-effect terms may be crucial for the acceptance or rejection of new medicines." ></td>
	<td class="line x" title="59:319	Is reliable automatic extraction of both lowand high-frequency side-effect terms from MEDLINE abstracts feasible?" ></td>
	<td class="line x" title="60:319	To answer this question, we explored the efficacy of a standard collocation-based term extraction method that extracts those words that appear more frequently in the immediate neighborhood of a given seed term than might be expected under chance conditions." ></td>
	<td class="line x" title="61:319	We compiled two corpora on the side effects of the cardiovascular drugs captopril and enalapril from MEDLINE abstracts." ></td>
	<td class="line x" title="62:319	The first corpus contains all abstracts mentioning captopril and the word side." ></td>
	<td class="line x" title="63:319	The second corpus contains all abstracts mentioning captopril and at least one of the compounds side-effect, side effect, side-effects, and side effects." ></td>
	<td class="line x" title="64:319	Thus, the second corpus is a subset of the first." ></td>
	<td class="line x" title="65:319	The first corpus is comprised of 118,675 tokens and 7,678 types; the second corpus 103,603 tokens and 6,582 types." ></td>
	<td class="line x" title="66:319	A medical expert marked 432 of the latter word types as side-effect-related terms." ></td>
	<td class="line x" title="67:319	The left panel of Figure 1 summarizes the head of the frequency distribution of these terms in the larger corpus." ></td>
	<td class="line x" title="68:319	Note that most side-effect-related terms have a frequency lower 303 Computational Linguistics Volume 26, Number 3 Table 1 General 2x2 contingency table." ></td>
	<td class="line x" title="69:319	A = frequency of the target in the window corpus, B = frequency of the target in the complement corpus, W = total number of words in the window, C = total number of words in the complement." ></td>
	<td class="line x" title="70:319	Corpus size N = W + C. window complement frequency of target A B sum frequency of other words W A C B W C A+B W+C-A-B W+C than five." ></td>
	<td class="line x" title="71:319	What we need, then, is an extraction method that is sensitive enough to select such very low frequency terms." ></td>
	<td class="line x" title="72:319	In the collocation-based method studied here, the neighborhood of a given seed term is defined in terms of a window around the seed term." ></td>
	<td class="line x" title="73:319	We constructed windows around all seed terms in the corpus, leading to a window corpus and a complement corpus." ></td>
	<td class="line x" title="74:319	The window corpus contains all words that appear within a given window size of the seed term." ></td>
	<td class="line x" title="75:319	For instance, with a window size of 10, any word appearing from five words before the seed to five words after the seed as well as the seed itself is included in the window corpus." ></td>
	<td class="line x" title="76:319	The word tokens not in the window corpus comprise the complement corpus." ></td>
	<td class="line x" title="77:319	Any type in the window corpus is a potential side-effectrelated term." ></td>
	<td class="line x" title="78:319	For any such target type, we tabulate its distribution in window and complement corpora in a contingency table like Table 1." ></td>
	<td class="line x" title="79:319	Given W and C, we need to know whether the frequency of the target in the window corpus, A, is high enough to warrant extraction." ></td>
	<td class="line x" title="80:319	Typically, given the marginal B and distribution of the contingency table, a target is extracted for which wA--~A > ~-2-~, for which the tabulated distribution is nonhomogeneous according to tests such as G 2 and Fisher's exact test for a given cMevel." ></td>
	<td class="line x" title="81:319	In this approach, the window size is a crucial variable." ></td>
	<td class="line x" title="82:319	At small window sizes, many potentially relevant terms fail to appear in the window corpus." ></td>
	<td class="line x" title="83:319	However, at large window sizes, many irrelevant words are found in the window corpus and may be extracted spuriously." ></td>
	<td class="line x" title="84:319	To see to what extent window size may affect the results of the extraction procedure, consider the solid lines in panels (a) and (b) of Figure 2." ></td>
	<td class="line x" title="85:319	The left panel shows the results for recall when we use the log-likelihood ratio, G 2, the right panel the results for Fisher's exact test." ></td>
	<td class="line x" title="86:319	We define recall as the proportion of the number of side-effect words extracted and the total number of side-effect words available in the window." ></td>
	<td class="line x" title="87:319	For both statistical tests, recall seems to be optimal at window size 2." ></td>
	<td class="line x" title="88:319	However, at this window size, the number of words extracted is very small." ></td>
	<td class="line x" title="89:319	This can be seen in panels (c) and (d)." ></td>
	<td class="line x" title="90:319	Considered jointly, panels (a) and (c) suggest an optimal window size of 24 for our larger corpus (corpus 1), as recall is still high, and the number of significant words is maximal." ></td>
	<td class="line x" title="91:319	When Fisher's exact test is used instead of G 2, panels (b) and (d) suggest 42 as the optimal size." ></td>
	<td class="line x" title="92:319	The dashed lines in panels (a) to (d) show the corresponding results for our smaller corpus (corpus 2)." ></td>
	<td class="line x" title="93:319	Unsurprisingly, the general pattern for this subcorpus is quite similar, although the drops in recall and the number of significant words, Nsig, occur at somewhat smaller window sizes." ></td>
	<td class="line x" title="94:319	Interestingly, we can synchronize the curves for both corpora by plotting recall and the number of significant items, Nsig, against the window-complement ratio (W/C)." ></td>
	<td class="line x" title="95:319	This is shown in panels (e) and (f)." ></td>
	<td class="line x" title="96:319	These panels suggest not an optimal window size 304 Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words 04 f 0.3 i ~ 0.2 ~ i 0 20 40 6(J 80 100 120 24 86 Window Size (a) 6r A '~ 4t / ',il _.~~ ~ \[ .,/ i  0 20 40 60 80 100 120 24 86 Window Size (c) 4oo\[, 200I, 03 i } 0.2 o. h 0 20 40 60 80 100 120 6 42 82 Window Size (b) 3oo r, ~o 0 20 40 60 80 100 120 6 42 82 Window Size (d) 300 I iilI  '1 0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8 0.17 0.62 0.05 0.29 0.58 w/c w/c (e) (t) Figure 2 Results of the word extraction procedure (a = 0.05)." ></td>
	<td class="line x" title="97:319	Solid line = corpus 1, dashed line = corpus 2." ></td>
	<td class="line x" title="98:319	Panel (a) shows the log-likelihood, G 2, recall results as a function of the window size." ></td>
	<td class="line x" title="99:319	Panel (b) shows recall values for Fisher's exact test." ></td>
	<td class="line x" title="100:319	Panel (c) shows the total number of significant words (Nsig) as a function of the window size for G 2." ></td>
	<td class="line x" title="101:319	Panel (d) shows the same as (c) but for Fisher's exact test." ></td>
	<td class="line x" title="102:319	Panel (e), G 2, and (f), Fisher's exact test, also show the total number of significant words, but as a function of the W/C-ratio; the ratio of the number of words in the window corpus to the number of words in the complement corpus." ></td>
	<td class="line x" title="103:319	but an optimal W/C-ratio (0.17 for G 2 and 0.29 for Fisher's exact test)." ></td>
	<td class="line x" title="104:319	Although we now seem to have shown that recall and Nsig depend on the choice of window size, the sudden drops in recall and Nsig and the reoccurrence of such drops at various W/C-ratios is a source of worry, not only for G 2 results, but also for the results based on Fisher's exact test." ></td>
	<td class="line x" title="105:319	A further source of worry is the fact that the two tests diverge considerably with respect to the optimal W/C-ratio." ></td>
	<td class="line x" title="106:319	3." ></td>
	<td class="line x" title="107:319	Contingency Tables and the Lowest-Frequency Words Before we can have any confidence in the optimality of a given W/C-ratio, we should understand why the saw-tooth-shaped patterns of Nsig arise." ></td>
	<td class="line x" title="108:319	Both the log-likelihood ratio (G 2) and Fisher's exact test compute the significance of contingency tables similar to Table 1." ></td>
	<td class="line x" title="109:319	So why is it that the left panels in Figure 2 differ from the right panels?" ></td>
	<td class="line x" title="110:319	G 2 has a 2-distribution as N --* cx~." ></td>
	<td class="line x" title="111:319	This convergence is not guaranteed for low expected frequencies and sparse tables, which renders use of G 2 problematic for our lowest-frequency words in that it may suggest words to be more remarkable than they 305 Computational Linguistics Volume 26, Number 3 Table 2 Contingency tables for hapax legomena, dis legomena, and tris legomena." ></td>
	<td class="line x" title="112:319	W = number of words in window corpus; C = number of words in complement corpus." ></td>
	<td class="line x" title="113:319	Total corpus size: N = W + C." ></td>
	<td class="line x" title="114:319	(a): 1 0 (b): 2 0 (c): 1 1 W-1 C W-2 C W-1 C-1 (d): 3 0 (e): 2 1 (f): 1 2 W-3 C W-2 C-1 W-1 C-2 really are." ></td>
	<td class="line x" title="115:319	Fisher's exact test, on the other hand, does not use an approximation to a probability distribution but computes the exact hypergeometric distribution given the marginal totals of the contingency table." ></td>
	<td class="line x" title="116:319	While Fisher's exact test is suitable for the analysis of sparse tables, it is inherently conservative because it regards the marginal totals not as stochastic variables but as fixed boundary conditions." ></td>
	<td class="line x" title="117:319	Consequently, this test is likely to reject words that are in fact remarkably distributed in the contingency table." ></td>
	<td class="line x" title="118:319	The difference in behavior of the two tests is clearly visible in panels (c) and (d) of Figure 2: the number of significant words (Nsig) according to G 2 is roughly twice as large as that according to Fisher's exact test." ></td>
	<td class="line x" title="119:319	When a hapax legomenon 1, a word with frequency 1, occurs in the window corpus, we use contingency table (a) as shown in Table 2." ></td>
	<td class="line x" title="120:319	For dis legomena, words with a frequency of 2, that appear at least once in the window corpus, we obtain the two contingency tables (b) and (c)." ></td>
	<td class="line x" title="121:319	The interesting contingency tables for tris legomena are tables (d) to (f)." ></td>
	<td class="line x" title="122:319	These six tables are relevant for 63.8% of the side-effect-related terms as judged by our medical expert." ></td>
	<td class="line x" title="123:319	How do changes in the W/C-ratio affect G 2 and Fisher's exact test, when applied to contingency tables (a) to (f)?" ></td>
	<td class="line x" title="124:319	In other words, how does the choice of the window size affect whether a low-frequency word is judged to be a significant term, for fixed A and B (e.g. , A = 1 and B = 0 for a hapax legomenon)?" ></td>
	<td class="line x" title="125:319	First, consider contingency tables with B = 0, for instance tables (a), (b), and (d)." ></td>
	<td class="line x" title="126:319	For small A, (A ~ W, C), it is easily seen (see the appendix) that the critical W/C-ratio based on the log-likelihood ratio is: W 1 C ~/eX/2 1' (1) with X the X 2 value corresponding to a given s-level with 1 degree of freedom." ></td>
	<td class="line x" title="127:319	For A = 1 and c~ -0.05, X = 3.84, the critical W/C-ratio equals 0.1718." ></td>
	<td class="line x" title="128:319	This is exactly the W/C-ratio in panel (e) in Figure 2 at which the first and largest drop in the number of significant words occurs." ></td>
	<td class="line x" title="129:319	Up to this ratio, any hapax legomenon appearing in the window corpus is judged to be a significant term." ></td>
	<td class="line x" title="130:319	For W/C > 0.1718, no hapax legomenon will be extracted." ></td>
	<td class="line x" title="131:319	Fisher's exact test is far more conservative." ></td>
	<td class="line x" title="132:319	For this test, the critical W/C-ratio is 1 The term hapax legomenon (literally 'read once') goes back to classical studies and was originally used to refer to the words used once only in the works of a given author, e.g., Homer." ></td>
	<td class="line x" title="133:319	By analogy, dis legomenon and tris legomenon have come into use to refer to words occurring only twice or three times." ></td>
	<td class="line x" title="134:319	306 Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words Table 3 Critical W/C-ratios where sparse and skewed contingency tables lose significance." ></td>
	<td class="line x" title="135:319	Equations 1 and 2 provide the ratios for the B = 0 cases." ></td>
	<td class="line x" title="136:319	The other ratios are obtained by simulations." ></td>
	<td class="line x" title="137:319	distribution G 2 Fisher A-B cz = 0.05 c~ = 0.01 c~ -0.05 c~ -0.01 1 0 0.1718 0.0375 0.0526 0.0101 1 1 0.0400 0.0092 0.0260 0.0050 2 0 0.6204 0.2348 0.2880 0.1111 1 2 0.0232 0.0053 0.0172 0.0033 2 1 0.1917 0.0824 0.1565 0.0626 3 0 1.1155 0.4938 0.5833 0.2746 hapax legomena dis legomena tris legomena (see the appendix for details): w C 1/-P' (2) where P is the s-level." ></td>
	<td class="line x" title="138:319	For A -1 and P = 0.05, the critical W/C-ratio for a hapax legomenon equals 0.0526." ></td>
	<td class="line x" title="139:319	In panel (f) of Figure 2, we observe the first drop in the number of significant words at precisely this W/C-ratio." ></td>
	<td class="line x" title="140:319	For very small W/C-ratios, any hapax legomenon in the window corpus is also judged to be significant according to Fisher's exact test." ></td>
	<td class="line x" title="141:319	Compared to G 2, Fisher's exact test rejects hapax legomena as significant at much smaller W/C-ratios." ></td>
	<td class="line x" title="142:319	Note that when W/C -0.05/0.95 = 0.0526, i.e., when the window corpus is exactly 1/20 of the total corpus, the probability that a hapax legomenon appears in the window corpus equals 0.05." ></td>
	<td class="line x" title="143:319	Our conclusion is that, with the W/C-ratio as the only determinant of significance, the windowing method is not powerful enough to distinguish between relevant and irrelevant hapax legomena." ></td>
	<td class="line x" title="144:319	In other words, hapax legomena should be removed from consideration a priori." ></td>
	<td class="line x" title="145:319	For dis legomena that appear exclusively in the window corpus, the critical ratios are 0.6204 for G 2, corresponding to the second major drop in panel (e) of Figure 2, and 0.2880 for Fisher's exact test, corresponding to the severe drop following the maximum of Nsig in panel (f)." ></td>
	<td class="line x" title="146:319	The third major drop in this panel corresponds to the critical W/C-ratio for tris legomena occurring three times in the window corpus." ></td>
	<td class="line x" title="147:319	For contingency tables with B > 0; A > B; A, B <~ W, C, critical W/C-ratios are not easy to capture analytically." ></td>
	<td class="line x" title="148:319	We therefore carried out a simulation study for W + C = 100,000." ></td>
	<td class="line x" title="149:319	For fixed A and B and a given s-level, we calculated the critical W/C-ratio by iterative approximation." ></td>
	<td class="line x" title="150:319	Results are summarized in Table 3." ></td>
	<td class="line x" title="151:319	When we highlight these critical ratios in Figure 2 by means of vertical dashed lines, we obtain Figure 3." ></td>
	<td class="line x" title="152:319	Panels (a) to (d) correspond to the curves for corpus 2 in the first four panels of Figure 2." ></td>
	<td class="line x" title="153:319	For the log-likelihood ratio, we observe that both the major and minor drops in recall and the number of significant words (Nsig) occur at the W/Cratios where different distributions of the lowest-frequency words lose significance." ></td>
	<td class="line x" title="154:319	For Fisher's exact test, we observe exactly the same pattern." ></td>
	<td class="line x" title="155:319	Panels (e) and (f) show the number of significant words for a pseudorandomized version of corpus 2 where we used the same tokens but randomized the order of their appearance." ></td>
	<td class="line x" title="156:319	Although the number of significant words is lower, the saw-tooth-shaped pattern with the sudden drops at fixed ratios reemerges." ></td>
	<td class="line x" title="157:319	We conclude that W and C are the prime determinants of both recall and the number of significant words." ></td>
	<td class="line x" title="158:319	At first sight, Fisher's test is clearly preferable to the 307 Computational Linguistics Volume 26, Number 3 1-1 1-0 2-1 3-1 2-0 0.41~ ~ = Fi i ~03 ~ ii i ~ 0.2, 0.0 0.2 0.4 0.6 0.8 W/C (a) 1-1 1-0 2-1 3-1 2-0 600 I 42oo1 0.0 0.2 0.4 0.6 0.8 W/C (c) 1-1 1-0 2-1 3-1 2-0 400 f 300\[ Z 200\[ 100\[ 1-0 2-1 2-0 ~ 0.3\[ 0.2 0.1 3-1 3-0 0.0 0.2 0.4 0.6 0.8 W/C (b) 1-0 2-1 2:0 3-1 3-0 0.0 0.2 0.4 0.6 0.8 W/C (d) 1-0 150\[ Z '~ 100\[ 50\[, 0.0 2-1 2-0 3-1 3-0 0.0 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8 W/C W/C (e) (0 Figure 3 Results of word extraction procedure (a = 0.05) with A-B distributions." ></td>
	<td class="line x" title="159:319	Panels (a), log-likelihood ratio, G 2, and (b), Fisher's exact test, show the recall results of the extraction procedure for corpus 2." ></td>
	<td class="line x" title="160:319	Panels (c) and (d) show the total number of significant words (Nsig), again for G 2 and Fisher's exact test, respectively (see also Figure 2)." ></td>
	<td class="line x" title="161:319	Panels (e) and (f) show the results for a randomized corpus for G 2 and Fisher's exact test." ></td>
	<td class="line x" title="162:319	The numbers above the panels indicate the A-B distribution of the contingency tables in Table 2." ></td>
	<td class="line x" title="163:319	log-likelihood ratio because the extreme saw-tooth-shaped pattern is substantially reduced." ></td>
	<td class="line x" title="164:319	However, the use of Fisher's exact test does not eliminate the effect of the choice of window and complement size on the number of significant words and recall." ></td>
	<td class="line x" title="165:319	At specific W/C-ratios, nonnegligible numbers of words with the lowest frequency of occurrence suddenly lose significance." ></td>
	<td class="line x" title="166:319	Moreover, in our discussion thus far, we have not taken extraction precision into account nor the trade-off between precision and recall." ></td>
	<td class="line x" title="167:319	For the assessment of overall extraction results, we turn to the F-measure (Rijsbergen 1979), a measure that assigns equal weights to precision (P) and recall (R): 2PR F= P+R' (3) Figure 4 plots precision, recall, and F as a function of the W/C-ratio." ></td>
	<td class="line x" title="168:319	The common trade-off between recall and precision is clearly present for the smaller window sizes, with the F-measure providing a kind of average." ></td>
	<td class="line x" title="169:319	Thus far, we have applied a common collocation extraction technique to a semantic association task." ></td>
	<td class="line x" title="170:319	Actual extraction performance is low: F is maximally 0.17." ></td>
	<td class="line x" title="171:319	To gauge 308 Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words 0.4 I  (}.3 k o.2 ~'i' {{7., i:, ' '/  ~-' q,_ 1 i (}.(7 0.2 {}.4 (}.6 0.8 I .{} {7.17 {7,62 u2 (}.3 0.2 {7.1 {}.(7 0.2 0.4 0.6 0.{}5 0.29 0.58 i t,i i k O.8 I.O w/c w/c Figure 4 F, recall, and precision as a function of the W/C-ratio." ></td>
	<td class="line x" title="173:319	Recall (R, dashed line), F (solid line), and precision (P, dotted line) using G 2 (left panel) and Fisher's exact test (right panel) for our second corpus plotted as a function of the W/C-ratio." ></td>
	<td class="line x" title="174:319	whether better results can be obtained with the present techniques, we examined the extraction of Dutch verb-particle combinations." ></td>
	<td class="line x" title="175:319	4." ></td>
	<td class="line x" title="176:319	Extracting Verb-Particle Combinations In English, the particle of verb-particle combinations always follows the verb, as in she rang him up." ></td>
	<td class="line x" title="177:319	In Dutch, the particle can occur either before or after the verb." ></td>
	<td class="line x" title="178:319	When it occurs before the verb, it is separated from the verb by te ('to') and/or one or more auxiliary verbs." ></td>
	<td class="line x" title="179:319	Extracting such particle-verb combinations is relatively straightforward." ></td>
	<td class="line x" title="180:319	However, when the particle follows the verb, it may be separated from the verb by many constituents of arbitrary complexity: Hij zegt de belangrijke afspraak met de programmeur voor vanmiddag af ('he says the important meeting with the programmer for this afternoon off'; i.e., he cancels the meeting)." ></td>
	<td class="line x" title="181:319	How well does our present approach lend itself to the extraction of verb-particle combinations with the particle af ('off') when the particle follows the verb?" ></td>
	<td class="line x" title="182:319	We investigated this question by studying verb-particle combinations with af from a Dutch newspaper corpus of about 4.5 million word tokens." ></td>
	<td class="line x" title="183:319	We extracted by hand all sentences from the corpus that contain af (3,802 sentences, 97,903 tokens) and singled out those sentences in which af belongs to a verb-particle combination in which the verb occurs to the left of the particle (2,202 sentences with 42,825 tokens)." ></td>
	<td class="line x" title="184:319	The targets to extract from the 2,202 sentences are 436 different verb inflections, of which 276 have a frequency of less than five." ></td>
	<td class="line x" title="185:319	Just as the judgments of a medical expert were used in the preceding extraction task to provide a frame of reference for the evaluation of precision and recall, the present lexical extraction task has as its frame of reference the 2,202 sentences that we judged to contain a verb followed at some point to the right by a particle." ></td>
	<td class="line x" title="186:319	How many of the 436 different verb inflections can we extract with our windowing technique, and what is the trade-off between recall and precision?" ></td>
	<td class="line x" title="187:319	To answer this question, we defined windows to the left of the seed term af in the range of positions \[-12, -1\]." ></td>
	<td class="line x" title="188:319	We calculated the W/C-ratio for each window size." ></td>
	<td class="line x" title="189:319	For each word in all windows, we calculated its significance according to G 2 and Fisher's exact test." ></td>
	<td class="line x" title="190:319	Using the 436 target verb inflections as a frame of reference, we computed precision, recall, and F. Panel (a) of Figure 5 plots F as a function of the W/C-ratio." ></td>
	<td class="line x" title="191:319	F reaches a maximum F of 0.31 at W/C = 0.59 for G 2 (the solid line in the figure) and a maximum of 0.27 at W/C = 0.50 for Fisher's exact test (the dashed line)." ></td>
	<td class="line x" title="192:319	These 309 Computational Linguistics Volume 26, Number 3 03\[ / {}2 /// O. _ ~}2 -{}14 {}.6 {}.8 I.{} Z 400 200 //J J J\ 0.2 0.4 0.6 {}T8 I.{} W/C (a} W/{'." ></td>
	<td class="line x" title="193:319	(b) {}.3 0.2 {}." ></td>
	<td class="line x" title="194:319	1 f.~,." ></td>
	<td class="line x" title="195:319	J / 7 / __ L t  J. 0,2 0.4 0.6 410 ~0 Z 200 1}.8 1,11 L   ___  L _  {}.2 0.4 0,6 0.8 I .{} W/C W/C (c) (d) Figure 5 Extraction results for the af corpus." ></td>
	<td class="line x" title="196:319	Panel (a) shows F for G 2 (solid line) and Fisher's exact test (dashed line) as a function of the W/C-ratio." ></td>
	<td class="line x" title="197:319	Panel (b) displays the number of significant words (Nsig) according to both tests." ></td>
	<td class="line x" title="198:319	Panel (c) shows F for G 2 at c~ = 0.05 (solid line) and Fisher's exact test at c~ = 0.1 (dotted line)." ></td>
	<td class="line x" title="199:319	Panel (d) shows Nsig for G 2 at c~ = 0.05 and for Fisher's exact test at c~ -0.1." ></td>
	<td class="line x" title="200:319	results compare favorably with the maximum F of 0.17 obtained for the extraction of side-effect terms from medical abstracts." ></td>
	<td class="line x" title="201:319	Panel (b) of Figure 5 shows the by-this-time familiar saw-tooth-shaped pattern of the number of significant word types as function of the W/C-ratio." ></td>
	<td class="line x" title="202:319	We observe again that Fisher's exact test is more conservative, and in the extraction task, less successful, than G 2." ></td>
	<td class="line x" title="203:319	However, by opting for a more liberal c~-level we can compensate for the conservatism of Fisher's exact test and obtain an F profile that is indistinguishable from that of G 2 as shown in panel (c) for ~ -0.1." ></td>
	<td class="line x" title="204:319	Panel (d) returns to the number of significant terms (Nsig) when Fisher's exact test is used with c~ = 0.1." ></td>
	<td class="line x" title="205:319	Note that the optimal W/C-ratio according to F for G 2 (0.59) still leads to a higher Nsig than the optimal W/C-ratio (0.83) for Fisher's exact test with c~ -0.1." ></td>
	<td class="line x" title="206:319	However, in the case of Fisher's exact test, the precision is much higher than when G 2 is used." ></td>
	<td class="line x" title="207:319	These results suggest that the choice of G 2 or Fisher's exact test should be guided by the desired trade-off between precision and recall." ></td>
	<td class="line x" title="208:319	5." ></td>
	<td class="line x" title="209:319	Discussion The question that originally motivated the present research concerned the determination of the optimal window size for the extraction of side-effect-related words." ></td>
	<td class="line x" title="210:319	Most 310 Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words words that are judged by a medical expert to be related to side effects have frequencies of use that are so low that they fall below the frequency thresholds generally used in standard information extraction techniques." ></td>
	<td class="line x" title="211:319	Is it nevertheless possible to single out such low-frequency terms through optimal window size estimation, especially since the log-likelihood ratio and Fisher's exact test have recently been advanced as suitable techniques even for the analysis of the lowest-frequency ranges?" ></td>
	<td class="line x" title="212:319	Manipulation of the window size revealed a saw-tooth-shaped pattern in the number of significant words (Nsig) that depends not on the window size itself but on the W/C-ratio." ></td>
	<td class="line x" title="213:319	This saw-tooth-shaped pattern arises most prominently when the loglikelihood ratio is used to extract significant words, but it is also clearly visible when Fisher's exact test is used." ></td>
	<td class="line x" title="214:319	This pattern is due to the way in which these tests evaluate surprise as a function of the window size for the lowest-frequency words." ></td>
	<td class="line x" title="215:319	We argue that hapax legomena should be disregarded a priori, while for low-frequency words with frequency greater than 1, only the most extreme distributions over window and complement are reliable in that we are confident that these terms are really related to the seed." ></td>
	<td class="line x" title="216:319	For dis and tris legomena, for instance, all occurrences should in effect be concentrated in the window." ></td>
	<td class="line x" title="217:319	Only then are we confident that there is truly a relationship between the seed and the target." ></td>
	<td class="line x" title="218:319	With these restrictions, the optimum W/C-ratio for our side-effect data is just smaller than 0.2880, using Fisher's exact test, which amounts to an optimal window size of 36." ></td>
	<td class="line x" title="219:319	Of the 295 terms with a frequency of 4 or less that a medical expert judged to be side-effect-related terms, we capture 14, which amounts to 4.8%." ></td>
	<td class="line x" title="220:319	When we exclude the hapax legomena as impossible to extract reliably a priori, we capture 14/122 = 11.5%." ></td>
	<td class="line x" title="221:319	Although the gain in number of significant low-frequency items is small, the success for the low-frequency items is still reasonable when compared to the corresponding success rate of 26/137 = 19.0% for the items with a frequency of 5 or more." ></td>
	<td class="line x" title="222:319	These results suggest that the windowing technique is far from optimal for the extraction of side-effect terms from medical abstracts, irrespective of the frequencies of these terms." ></td>
	<td class="line x" title="223:319	The windowing technique applied to the extraction of Dutch verb-particle combinations led to more encouraging results." ></td>
	<td class="line x" title="224:319	Choosing 0.4625 as the optimal W/C-ratio for the af data, which amounts to accepting dis legomena with a 2-0 distribution, and using a = 0.1 with Fisher's exact test, we obtain an optimal window size of 5." ></td>
	<td class="line x" title="225:319	With this window, we extract 42 of the 139 lowest-frequency words in the 2 to 4 range, i.e., 30.2%." ></td>
	<td class="line x" title="226:319	This compares favorably to the success rate of 60/170 = 35.2% for verbs with a frequency greater than 4." ></td>
	<td class="line x" title="227:319	When we use G 2 instead of Fisher's exact test to obtain improved recall at the cost of lesser precision, we extract 58/139 = 41.7% of the lowestfrequency words in the 2 to 4 range and 64/170 = 37.6% of the higher-frequency words (optimum W/C-ratio 0.6204, corresponding window size of 7)." ></td>
	<td class="line x" title="228:319	For this more lexical extraction task, extraction success rates are comparable for the lower-frequency and the higher-frequency words." ></td>
	<td class="line x" title="229:319	Neglecting the extraction of the lower-frequency words a priori would have led to the loss of nearly half of the words currently extracted." ></td>
	<td class="line x" title="230:319	The difference in the results between the two extraction tasks, side effects in medical abstracts and verb-af combinations in a newspaper corpus, is due to the difference in the distributions of the targets around the seed terms." ></td>
	<td class="line x" title="231:319	Concentrating on the lowestfrequency word tokens, the left panel of Figure 6 shows their distribution for the side-effect corpus." ></td>
	<td class="line x" title="232:319	The right panel shows the corresponding distribution for the af corpus." ></td>
	<td class="line x" title="233:319	The side-effect terms reveal a wide scatter around the seed at position 0." ></td>
	<td class="line x" title="234:319	By contrast, verbs predominantly cluster close to the left of af." ></td>
	<td class="line x" title="235:319	Apparently, the distance between the verb and the particle is more constrained than the distance between sideeffect terms and the seed term." ></td>
	<td class="line x" title="236:319	The optimal window size of 7 (position -7) for G 2 311 Computational Linguistics Volume 26, Number 3 10 i k -300 -200 -100 0 100 200 40 20 -40 -20 Position Position (a) (b), ll,,,,,it,, ,, 20 Figure 6 Frequency distribution of words occurring two to four times." ></td>
	<td class="line x" title="237:319	Panel (a) shows for the side effect corpus how the expert words with a frequency of 2, 3, and 4 are distributed around the seed term." ></td>
	<td class="line x" title="238:319	Panel (b) shows this distribution for the af corpus." ></td>
	<td class="line x" title="239:319	obtained above ties in with the distribution of the lowest-frequency words: 68% of all lowest-frequency tokens are in this window." ></td>
	<td class="line x" title="240:319	For the side-effect corpus, only 31% of all low-frequency tokens are in the optimal window of 36 for Fisher's exact test." ></td>
	<td class="line x" title="241:319	This suggests that the optimal window size must be ascertained on the basis of the distribution of targets around the seed, on the one hand, and by optimizing the statistics, on the other hand." ></td>
	<td class="line x" title="242:319	As an illustration of how the statistics can be optimized, we return to the af data." ></td>
	<td class="line x" title="243:319	When we look at the distribution of the lowest-frequency words in Figure 6, an optimal window size of 8 to the left suggests itself." ></td>
	<td class="line x" title="244:319	This translates into a W/C-ratio of 0.6689." ></td>
	<td class="line x" title="245:319	Given that we want to retain dis legomena with a 2-0 distribution, we proceed to compute the corresponding significance levels for both G 2 and Fisher's exact test by Equations 1 and 2." ></td>
	<td class="line x" title="246:319	The critical X 2 value for G 2 equals 3.65, the critical P for Fisher's exact test is 0.161." ></td>
	<td class="line x" title="247:319	The extraction results for both tests as measured by F are 0.31 and 0.33, respectively." ></td>
	<td class="line x" title="248:319	This procedure allows us to extract 64/139 = 46.0% of the lowfrequency words and 66/170 -~ 38.8% of the high-frequency words using G 2, and 64/139 = 46.0% and 79/170 = 46.7%, respectively, using Fisher's exact test." ></td>
	<td class="line x" title="249:319	Note that this technique is optimal for the extraction of the lowest-frequency words, leading to identical performance for G 2 and Fisher's exact test for these words." ></td>
	<td class="line x" title="250:319	For the higherfrequency words, Fisher's exact test leads to a slightly better recall with the same precision scores (0.31 for both tests)." ></td>
	<td class="line x" title="251:319	While we have observed reasonable results with both G 2 and Fisher's exact test, we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information (MI) measure (Church and Hanks 1990): I(x,y) --log 2 P(x,y) (4) P(x)P(y) In (4), y is the seed term and x a potential target word." ></td>
	<td class="line x" title="252:319	A high MI score for a given target word suggests an association between this target and the seed term." ></td>
	<td class="line x" title="253:319	Or perhaps more precisely, a low MI score suggests a dissociation between target and seed word (Manning and Sch/itze 1999)." ></td>
	<td class="line x" title="254:319	To compute recall, precision, and F, we require a cut-off value." ></td>
	<td class="line x" title="255:319	As there is no theoretically motivated cut-off value, we vary it systematically." ></td>
	<td class="line x" title="256:319	Panel (a) of Figure 7 plots the results for the af corpus." ></td>
	<td class="line x" title="257:319	The x-axis represents the MI 312 Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words f-~k g o (a) / -< ~ 0.I 0.2~-'7~__7 _ .q,-' ',~',  ., 04." ></td>
	<td class="line x" title="258:319	Slgl~l/l~ ~nc (Z3 e/eLe ~9~?~\C (b) Figure 7 Extraction results (F) for the af corpus for mutual information and Fisher's exact test." ></td>
	<td class="line x" title="259:319	Panel (a) shows the F score as a function of both W/C-ratio and mutual information cut-off value." ></td>
	<td class="line x" title="260:319	Panel (b) shows F as a function of W/C-ratio and the significance level c~ used with Fisher's exact test." ></td>
	<td class="line x" title="261:319	cut-off value, the y-axis the W/C-ratio, and the z-axis the F value." ></td>
	<td class="line x" title="262:319	Note that F is rather indifferent to variation in window size and MI cut-off value." ></td>
	<td class="line x" title="263:319	It varies between 0 (at the right-hand edge) to 0.17, with most values around 0.15 (the plateau in the figure)." ></td>
	<td class="line x" title="264:319	Interestingly, the highest possible MI cut-off point equals 4.27: the right-hand edge of the plateau." ></td>
	<td class="line x" title="265:319	In fact, 4.27 is the maximum MI score for this corpus size (42,825) and the frequency of the seed term af (2,206), irrespective of the frequency of the target word, reached when all occurrences of the target word are concentrated in the window (see the appendix for details)." ></td>
	<td class="line x" title="266:319	Consequently, any hapax legomenon appearing in the window will automatically be assigned the maximum value of MI, along with target words with the most extreme W/C distributions (Window-Complement: 2-0, 3-0, 4-0, etc.)." ></td>
	<td class="line x" title="267:319	This has the unfortunate consequence that, with regard to their MI score, truly remarkably distributed target words become indistinguishable from the statistically unremarkable hapax legomena." ></td>
	<td class="line x" title="268:319	Panel (b) of Figure 7 displays the corresponding results when we use Fisher's exact test rather than MI." ></td>
	<td class="line x" title="269:319	Instead of varying the MI cut-off value, we vary the significance level a. Note that the resulting F scores tend to be roughly twice as high as those obtained with MI-based extraction." ></td>
	<td class="line x" title="270:319	As there are a number of very similar local maxima, the choice of window size and significance level should be based on the desired tradeoff between precision and recall given the general distribution of the target words around the seed term." ></td>
	<td class="line x" title="271:319	2 We conclude that, at least for the present word extraction task, Fisher's exact test compares favorably to mutual information (as does G2)." ></td>
	<td class="line x" title="272:319	All the analyses presented thus far are conditional analyses, in the sense that we compiled new corpora from the database of abstracts and from the newspaper corpus containing only relevant abstracts (containing the drug names captopril and enalapril as well as the term side-effect) and relevant sentences (containing the particle af and its verb to its left), respectively." ></td>
	<td class="line x" title="273:319	The size of the complement was always determined with respect to these new conditional corpora, and not with respect to all MEDLINE 2 Note that we manipulate the a-levels in the same way as the MI cut-off values." ></td>
	<td class="line x" title="274:319	In the present technique, the a-level is a parameter that we vary to optimize extraction results for a training data set." ></td>
	<td class="line x" title="275:319	Our use of a should be carefully distinguished from the function of preset a-levels when testing the significance of observed differences in experimentally obtained data." ></td>
	<td class="line x" title="276:319	313 Computational Linguistics Volume 26, Number 3 Table 4 General and specific 2 x 2 contingency tables for low-frequency words." ></td>
	<td class="line x" title="277:319	Table (a) provides the general notation of the counts in a 2 x 2 contingency table." ></td>
	<td class="line x" title="278:319	In table (b), A = frequency of rare words (1, 2, 3  ), W = number of words in window, C = number of words in complement." ></td>
	<td class="line x" title="279:319	Corpus size N = W + C." ></td>
	<td class="line x" title="280:319	(a): 1/11 /'/12 /'/1+ /'/21 /'/22 /'/2+ //+1 //+2 //+q(b): A W-A W 0 A C W+C-A C W+C abstracts or to the complete newspaper corpus." ></td>
	<td class="line x" title="281:319	This raises the question of whether better results might have been obtained if the complete data sets had been used." ></td>
	<td class="line x" title="282:319	In principle, more data might imply more power." ></td>
	<td class="line x" title="283:319	At the same time, more data also entails the risk of more noise." ></td>
	<td class="line x" title="284:319	At least for our af data, enlarging the complement leads to worse performance." ></td>
	<td class="line x" title="285:319	When we allow any sentence that contains af in our analyses, F decreases from 0.31 to 0.23 for G 2." ></td>
	<td class="line x" title="286:319	When we base the analyses on the complete newspaper corpus, F reduces further to 0.19." ></td>
	<td class="line x" title="287:319	The reason for this decrease in performance is probably due to the W/C-ratio being very low for all practical window sizes, i.e., at the very left part of the saw-tooth-shaped pattern characterizing Nsig as a function of W/C. Consequently, any low-frequency word is singled out as a significant item whenever it occurs at least once in the window." ></td>
	<td class="line x" title="288:319	Given the Zipfian structure of word-frequency distributions, a great many spurious low-frequency words are extracted." ></td>
	<td class="line x" title="289:319	As mentioned in the introduction, the received wisdom is that the windowing method is unreliable for events with a frequency of less than 5." ></td>
	<td class="line x" title="290:319	By means of an analysis of the behavior of statistical tests for 2 x 2 contingency tables with sparse data, a method for optimizing the use of these tests has been developed." ></td>
	<td class="line x" title="291:319	We hope that this technique will prove to be useful for domains in which the extraction of low-probability events is crucial." ></td>
	<td class="line x" title="292:319	Appendix Log-Likelihood Ratio For the general contingency table, table (a) in Table 4, the log-likelihood ratio is defined by (Agresti 1990): G 2 = 2 ~_, ~ nijin(nij/mq), i j where rhq = ni+n+j/n++." ></td>
	<td class="line x" title="293:319	When we use the specific contingency table for hapax legomena, table (b) in Table 4, we obtain for a specific G 2 of X the formula: W+C (W-A)(W+C) W+C X/2 = Aln~+(W-A)ln W(W+C-A) +CInw+C-A' = In(WA) w-A InW w + In(W qC) w+C In(W qC A) w+C-A, = In (WA)W-A(w qC) w+C wW(w qC A) w+C-A ' (W A)W-A(w qC) w+C eX/2 wW(w + C A) w+c-A ' 314 Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words We rewrite the latter equation to: eX/2w w (W-A) w-A eX/2wW(w -A) A (W-A) w (W + C) w+c (W + C A) w+c-A' (w + c)W(w + c)c(w + c A?" ></td>
	<td class="line x" title="294:319	(W + C A)W(W + C A) c Because W >> A and therefore W + C >> A, we rewrite the formula above as follows: eX/2 W w W A (w + c)W(w + c)C(w + c) A w w (w + c)W(w + c) c eX/2w A = (W  C) A, ~W = W+ C. So that the ratio is: W 1 C ~-1' When N > 10,000, the error of this equation is smaller than 0.001." ></td>
	<td class="line x" title="295:319	Fisher's Exact Test With Fisher's exact test, the observed marginal totals are used to compute the hypergeometric distribution, which is defined for the general 2 x 2 table, table (a) of Table 4, as (Agresti 1990): ?/2+ ) (n'+)(n+, rill -F/ll n+l The probability of every possible table with given marginal totals adds to 1." ></td>
	<td class="line x" title="296:319	We use Fisher's exact test that sums the hypergeometric probabilities of all tables with probabilities less than or equal to the observed table." ></td>
	<td class="line x" title="297:319	With B -0, table (b) in Table 4 is the only table we are interested in so that the probability P for this contingency table is: P A C-A W-A ) (w + c A)!" ></td>
	<td class="line x" title="298:319	(w + c)~ ' W!C! W~(W + C A)~ (w A)~(W + C)~' W(W1) (W-A + 1)(W-A)!" ></td>
	<td class="line x" title="299:319	(W + C A)!" ></td>
	<td class="line x" title="300:319	(w-a)~ (w+ c)(w+ c1)(w+ c-A + 1)(w+ c-A)!" ></td>
	<td class="line x" title="301:319	315 Computational Linguistics Volume 26, Number 3 Because A = 1,2,3,, W >> A and therefore W + C >> A, we allow ourselves to formulate W!" ></td>
	<td class="line x" title="302:319	= wA(w A)!" ></td>
	<td class="line x" title="303:319	and (W + C)!" ></td>
	<td class="line x" title="304:319	= (W + c)A(w + C A)!." ></td>
	<td class="line x" title="305:319	We therefore rewrite Fisher's exact test as follows: The W/C-ratio is then: wAw (w + P = (W + c)Aw!(w + C)'!" ></td>
	<td class="line x" title="306:319	WA (W + C) W W+C' w C 1~Y-P' When N > 20,000, the error of this equation is smaller than 0.001." ></td>
	<td class="line x" title="307:319	Practical Issues Using Fisher's Exact Test." ></td>
	<td class="line x" title="308:319	We used a network algorithm to compute Fisher's exact test (Mehta and Patel 1986; Clarkson, Fan, and Joe 1993)." ></td>
	<td class="line x" title="309:319	This algorithm is computationally intensive, but since many words have the same table, only a few tables have to be computed and their results can be cached." ></td>
	<td class="line x" title="310:319	It takes an average of 50 seconds to compute one window size in a 100,000 word corpus on a Pentium 133MHz, 48MB Linux machine." ></td>
	<td class="line x" title="311:319	Source code for the algorithm can be found at: http://www, acm." ></td>
	<td class="line x" title="312:319	org/pubs/citations/ j ournals/toms/1986-12-2/p154-meht a/ Mutual Information Given the definition of Mutual Information (Church and Hanks 1990), I(x,y) = log 2 P(x,y) P(x)P(y)' we consider the distribution of a window word according to the contingency table (a) in Table 4." ></td>
	<td class="line x" title="313:319	P(x) is the relative frequency of the target word, P(y) is the relative frequency of the seed term, and P(x,y) is the frequency of the target word in the window." ></td>
	<td class="line x" title="314:319	In terms of the contingency table, we have: /'/11 I(x,y) = log 2 n++ //1+ S ' f/++ 7'/++ where S is the frequency of the seed." ></td>
	<td class="line x" title="315:319	Substituting nn = nl+ n12, we find that /11+ -F/12 I(x,y) = log 2 n++ nl+ S ' //++ //++ 1 = log 2 n++ //1+ S ' n++(nl+ -nu) ' n++ 316 Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words = log2(n++) log2(S) log2(nl+) + log2(nl+ n12)." ></td>
	<td class="line x" title="316:319	For a given corpus and extraction task, corpus size (n++) and the frequency of the seed term S are fixed, so that we can write I(x,y) = C log2(nl+) + log2(nl+ n12)." ></td>
	<td class="line x" title="317:319	As n12 K nl+, I(x,y) reaches its maximum value (C) when n12 = 0, i.e., when all instances of the target word are in the window, irrespective of the frequency of the target." ></td>
	<td class="line x" title="318:319	Acknowledgments We are indebted to three anonymous reviewers whose criticisms have led to substantial improvements." ></td>
	<td class="line x" title="319:319	This study was financially supported by the Dutch National Research Council NWO (PIONIER grant to the third author)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W00-1203
Knowledge Extraction For Identification Of Chinese Organization Names
Chen, Keh-Jiann;Chen, Chao-Jan;"></td>
	<td class="line x" title="1:207	Knowledge Extraction for Identification of Chinese Organization Names Keh-Jiann Chen & Chao-jan Chen kchen@iis.siniea.edu.tw fichard@iis.siniea.edu.tw Institute of Information Science, Academia Siniea, Taipei ABSTRACT In this paper, a knowledge extraction process was proposed to extract the knowledge for identifying Chinese organization names." ></td>
	<td class="line x" title="2:207	The knowledge extraction process utilizes the structure property, statistical property as well as partial linguistic knowledge of the organization names to extract new organizations from domain texts." ></td>
	<td class="line x" title="3:207	The knowledge extraction processes were experimented on large amount of texts retrieved from WWW." ></td>
	<td class="line x" title="4:207	With high standard of threshold values, new organization names can be identified with very high precision." ></td>
	<td class="line x" title="5:207	Therefore the knowledge extraction processes can be carried out automatically to self improve the performance in the future." ></td>
	<td class="line x" title="6:207	1." ></td>
	<td class="line x" title="7:207	INTRODUCTION The occurrences of unknown words cause difficulties in natural language processing." ></td>
	<td class="line x" title="8:207	The word set of a natural language is open-ended." ></td>
	<td class="line x" title="9:207	There is no way of collecting every words of a language, since new words will be created for expressing new concepts, new inventions, newborn babies, new organizations." ></td>
	<td class="line x" title="10:207	Therefore how to identify new words in a text will be the most challenging task for natural language processing." ></td>
	<td class="line x" title="11:207	It is especially true for Chinese." ></td>
	<td class="line x" title="12:207	Each Chinese morpheme (usually a single character) carries meanings and most arc polyscmous." ></td>
	<td class="line x" title="13:207	New words are easily constructed by combining morphemes and their meanings are the semantic composition of morpheme components." ></td>
	<td class="line x" title="14:207	However there are also semantically non-compositional compounds, such as proper names." ></td>
	<td class="line x" title="15:207	In Chinese text, there is no blank to mark word boundaries and no inflectional markers nor capitalization markers to denote the syntactic or semantic types of new words." ></td>
	<td class="line x" title="16:207	Hence the unknown word identification for Chinese became one of the most difficult and demanding research topic." ></td>
	<td class="line x" title="17:207	There are many different types of unknown words and each has different morphsyntactic and morph-scmantic structures." ></td>
	<td class="line x" title="18:207	In principle their syntactic and semantic categories can be determined by their content and contextual information, but there arc many difficult problems have to be solved." ></td>
	<td class="line x" title="19:207	First of all it is not possible to find a uniform representational schema and categorization algorithm to handle different types of unknown words due to their different morphsyntactic structures." ></td>
	<td class="line x" title="20:207	Second, the clues for identifying different type of unknown words are also different." ></td>
	<td class="line x" title="21:207	For instance, identification of names of Chinese people is very much relied on the surnames, which is a limited set of characters." ></td>
	<td class="line x" title="22:207	The statistical methods are commonly used for identifying proper names (Chen & Lee 1996, Chang ct al. 1994, Sun et al. 1994)." ></td>
	<td class="line x" title="23:207	The identification of general compounds is more relied on the morphemes and the semantic relations between morphemes (Chcn & Chcn 2000)." ></td>
	<td class="line x" title="24:207	The third difficulty is the problems of ambiguities, such as structure ambiguities, syntactic ambiguities and semantic ambiguities." ></td>
	<td class="line x" title="25:207	For instances, usually a morpheme character/word has multiple meaning and syntactic categories and may play the roles of common words or proper names." ></td>
	<td class="line x" title="26:207	Therefore the ambiguity resolution became one of the major tasks." ></td>
	<td class="line x" title="27:207	In this paper we focus our attention on the identification of the organization names." ></td>
	<td class="line x" title="28:207	It is considered to be a hard task to identify organization names in comparing with the identification of other types of unknown words, because there are not much morphsyntactic and morph-sernantic clues to indicate an organization name." ></td>
	<td class="line x" title="29:207	There is no significant preference on the selection of morphemes/characters and the semantic of the morphemes, which gives no clue leading toward the identification." ></td>
	<td class="line x" title="30:207	For instance, '~, micro-soW (Microsoft) has the character by character (morpheme by morpheme) translation of 'slightly soR' and there is no marker, such as capitalization, to indicate that it is a proper name." ></td>
	<td class="line x" title="31:207	The only reliable clue is its context information." ></td>
	<td class="line x" title="32:207	However an organization's full names usually occur at its first mention, unless it is a well-known organization." ></td>
	<td class="line x" title="33:207	A full name contains its proper 15 name and organization type, such as '~ '~, Acer Computer-Company'." ></td>
	<td class="line x" title="34:207	The organization types became the major clue of identifying a new organization name." ></td>
	<td class="line x" title="35:207	However abbreviated shorter names usually will be used, such as a) omit part of the organization type, for instances '~ ~, Acer Computer', '~ ~~J, Acer Company', b) omit the organization type totally, for instance '~.~, Acer', or c) the abbreviation, for instance '~, global-electric (Acer-computer)'." ></td>
	<td class="line x" title="36:207	Therefore the task became not only the identification of organization names in different forms but also finding their meaning equivalence classes." ></td>
	<td class="line x" title="37:207	To achieve the above goal, the knowledge of 1) proper names of organizations, 2) different lines of the businesses, and 3) different organization types, should be equipped." ></td>
	<td class="line x" title="38:207	Unfortunately there is no wellprepared knowledge sources containing the above information." ></td>
	<td class="line x" title="39:207	Therefore a knowledge extraction model is proposed to extract the above mentioned knowledge from the dictionary and domain texts." ></td>
	<td class="line x" title="40:207	2." ></td>
	<td class="line x" title="41:207	STRUCTURES OF ORGANIZATION NAMES There is no rigid structure for an organization name as mentioned in the previous section." ></td>
	<td class="line x" title="42:207	Roughly speaking an organization name is composed by two major components." ></td>
	<td class="line x" title="43:207	The first part is the proper name and the second part is the organization type." ></td>
	<td class="line x" title="44:207	The second part contains the major key words lead toward the identification of an organization, since the organization types, such as '~J, company', '~'~' foundation', '/J',~J~ group', '~\[\] enterprise' etc, tells what kind of organizations they are." ></td>
	<td class="line x" title="45:207	If it is a company, to be more informative the line of business usually goes with the key word '~\] company', for instances '~ff~.t~J food company', '~ ~ computer company', ' ~M ~='~ ~ investment consultant company', but in most cases the keyword '~B\] company' will be ignored, such as ~--~( President food)." ></td>
	<td class="line x" title="46:207	Sometimes the line of business and the organization type go together to become a single word, such as,qa~ middle school', 'f,l~ bank', '~: ~ hospital'." ></td>
	<td class="line x" title="47:207	By observing the structure of the organization name, it seems that once a complete list of the organization types is well prepared, then it is not hard to identify the organizations by their Rill names." ></td>
	<td class="line x" title="48:207	The only complication is that abbreviated names occur more frequently than fidl names." ></td>
	<td class="line x" title="49:207	The identifier '.~~\] company' is usually ignored in real text." ></td>
	<td class="line x" title="50:207	The lines of business became the major identifier for a company and many business lines are common words, such as '~1~ food', '~JJ~ computer', 'TJ~0~ cement'." ></td>
	<td class="line x" title="51:207	Therefore it is necessary to make the distinction between a common compounds and a company name, for examples, '~J~J~n health food' vvs.,~m~ President food', '~fgj~ personal computer' vs. '~ ~ Acer computer'." ></td>
	<td class="line x" title="53:207	Although they are twoway ambiguous, usually they have only one preference reading." ></td>
	<td class="line x" title="54:207	In conclusion, the types and the proper names of organizations will be the major clues lead toward the identification of the organizations." ></td>
	<td class="line x" title="55:207	In addition, it is also better to have a list of well known organization names, such that the well known company names, like '~.~i\[ MicrosoR', can be identified immediately." ></td>
	<td class="line x" title="56:207	Most of the knowledge preparation works should be done by oflline approaches." ></td>
	<td class="line x" title="57:207	The prepared knowledge would be utilized to online identification of newly coined organiT~ations." ></td>
	<td class="line x" title="58:207	The equivalent classes of the well-known organizations are also classified by a similarity-based approach." ></td>
	<td class="line x" title="59:207	3." ></td>
	<td class="line x" title="60:207	KNOWLEDGE EXTRACTION There are two knowledge sources." ></td>
	<td class="line x" title="61:207	One is the CKIP Chinese lexicon and another is the Chinese text from WWW." ></td>
	<td class="line x" title="62:207	The lexicon provides a partial list of important organizations and the information extracted from them will be the initial knowledge of the identification system." ></td>
	<td class="line x" title="63:207	The texts from WWW provide ample of new organization names implicitly." ></td>
	<td class="line x" title="64:207	The problem is how to extract some, if not all, of them from the texts." ></td>
	<td class="line x" title="65:207	Once we have a list of organization names." ></td>
	<td class="line x" title="66:207	The proper names for organizations and the organization types will be extracted by analyzing the morphological structures of the organization names." ></td>
	<td class="line x" title="67:207	However an effective morphological analyzer depends upon the availability of the knowledge of the organization types, but the lists of the organization types are not available yet." ></td>
	<td class="line x" title="68:207	As we mentioned before the complete organization names have two parts." ></td>
	<td class="line x" title="69:207	The first part is the proper name and the second part is the organization type." ></td>
	<td class="line x" title="70:207	The number of different proper names is unlimited and on the other hand the number of different organization types is limited." ></td>
	<td class="line x" title="71:207	Th/s property will be utilized to separate the variable parts, i.e. the proper name, and the constant parts, i.e. the organization type, from the organization names." ></td>
	<td class="line x" title="72:207	16 The numbers of organization names in the lexicon is very limited, since only the important organizations in the common domain will be collected." ></td>
	<td class="line x" title="73:207	Therefore the initial knowledge extracted from lexicon is also very limited." ></td>
	<td class="line x" title="74:207	To make the sources of knowledge more adequate, vast amount of new organization names should be extracted from each different domain corpus." ></td>
	<td class="line x" title="75:207	Unfortunately none of the existing corpora had tagged the organization names." ></td>
	<td class="line x" title="76:207	Therefore we are going to design a semi-automatic method to extract the high frequency organization names from text corpora." ></td>
	<td class="line x" title="77:207	The locality of occurrences of keywords in a text will be utilized for keyword extraction." ></td>
	<td class="line x" title="78:207	Once an organization name occun-ing in a text it is very probably reoccurred in the same text." ></td>
	<td class="line oc" title="79:207	The recurrence property had been utilized to extract keywords or key-phrases from text (Chien 1999, Fung 1998, Smadja 1993)." ></td>
	<td class="line x" title="80:207	However not all keywords arc organization names." ></td>
	<td class="line x" title="81:207	The knowledge extracted from the lexicon, i.e. the list of the organization types will be the initial knowledge for identifying organization names." ></td>
	<td class="line x" title="82:207	In addition to the initial knowledge, the structure property of the organization names will be also utilized in classifying extracted keywords into organization names and non-organization names." ></td>
	<td class="line x" title="83:207	The extraction processes will be repeated for extracting new organizations and therefore extracting new organization types." ></td>
	<td class="line x" title="84:207	The more knowledge would have been extracted the more accurate of the organization identification will achieve." ></td>
	<td class="line x" title="85:207	3.1 Morphological Analysis for Organization Names There are 1391 number of words in the CKIP lexicon classified as organizations." ></td>
	<td class="line x" title="86:207	Table I shows some of the examples." ></td>
	<td class="line x" title="87:207	Table I. The samples of organizations from the CKIP dictionary As we observed, the morphological structure of an organization name usually is a compounding of a proper name and a organization type." ></td>
	<td class="line x" title="88:207	The organization type might be a compounding of a line of business and a type, for instances i.~JJ~ ~~J (computer company), ~\]~,~(bank), qb~ (middle school), or simply a line of business, for instances AM(food), ~-~J~(computer), 7~ ~ (cement)." ></td>
	<td class="line x" title="89:207	The proper names are variables, since each organization type may have many different institutions with different names." ></td>
	<td class="line x" title="90:207	The types are constants." ></td>
	<td class="line x" title="91:207	There is a limited number of constants attached with many different proper names to form different organization names." ></td>
	<td class="line x" title="92:207	Therefore to extract the organization types is equivalent to extract the high frequency ending morphemes." ></td>
	<td class="line x" title="93:207	Table 2 shows the top 20 high frequency ending morphemes extracted from the 1391 organization names and in deed they are organization types." ></td>
	<td class="line x" title="94:207	52 ~' 38 ~ 36 ~ 30 ~d: 29 ;/q~t 21 ~ 21 I~ 20 ~ 17 Y~ 16 ~z 16 ~ 16 ~ 15 ~ 15 ~ 15 'l-\]g 13 I~ 12 ~ 12 ~\] 12 ~JJ 11 Table 2." ></td>
	<td class="line x" title="95:207	The top 20 organization types ranked with their occurrence frequencies extracted from the 1391 organization names 3.2 Automatic Extraction of Organization Names A Web spider can extract text from each different domain through WWW." ></td>
	<td class="line x" title="96:207	Then keyword extraction technique is applied on domain texts to retrieve possible keywords." ></td>
	<td class="line x" title="97:207	The kcyword set includes organization names, personal names, general compounds, and also error extraction." ></td>
	<td class="line x" title="98:207	Most of which are not organization names." ></td>
	<td class="line x" title="99:207	It is supposed that the available list of the organization types will be the source of knowledge to identify candidates of organizations." ></td>
	<td class="line x" title="100:207	However such a method only identifies the organizations of the known organization types and provides new proper names only." ></td>
	<td class="line x" title="101:207	It will not identify new types of organizations." ></td>
	<td class="line x" title="102:207	Therefore we use a new method to extract the organization names by using the structure property of organization names." ></td>
	<td class="line x" title="103:207	Extraction Algorithm for Organization Types: Step 1." ></td>
	<td class="line x" title="104:207	Using a Web spider to collect Chinese texts of a fixed domain, such as domain of finance and business, from.WWW." ></td>
	<td class="line x" title="105:207	Step 2." ></td>
	<td class="line o" title="106:207	Extract high frequency keywords in the text (Smadja 93, Chang & Su 97, Chien 99)." ></td>
	<td class="line x" title="107:207	Step 3." ></td>
	<td class="line x" title="108:207	For the keywords of length 3,4, and 5, each keyword is divided into two parts X and Y. X is a candidate of proper name and 17 Y is a candidate of organization type." ></td>
	<td class="line x" title="109:207	The X is the initial two-characters of the keyword and Y is the remained characters." ></td>
	<td class="line x" title="110:207	(Since most proper names of organizations have two characters, we can extract the organization types of the lengths 1, 2 and 3 from three different groups of keywords with lengths 3, 4 and 5 respectively)." ></td>
	<td class="line x" title="111:207	Extract the organization type Y, if for some keywords X+Y, the following conditions hold." ></td>
	<td class="line x" title="112:207	a) X satisfies one of the following cases." ></td>
	<td class="line x" title="113:207	1." ></td>
	<td class="line x" title="114:207	X is not in the lexicon, i.e. X is an unknown word." ></td>
	<td class="line x" title="115:207	2." ></td>
	<td class="line x" title="116:207	X has the categories of Nb or No, i.e. it is a known proper name (bib) or a location name (No)." ></td>
	<td class="line x" title="117:207	b) For each Y, assumed to be the organization type, there must have more than n number of different X, such that X+Y in the extracted keyword list." ></td>
	<td class="line x" title="118:207	In practice, the threshold value n was set to 2." ></td>
	<td class="line x" title="119:207	In general, Chinese company names like most proper names are non-common word (unknown words)." ></td>
	<td class="line x" title="120:207	However sometimes they are place names (No), but rarely they are common nouns, adjectives, or verbs." ></td>
	<td class="line x" title="121:207	Therefore in order to avoid too many false alarms, such as '~.,~,/~ super computer', to be considered as a company name, the condition a) of step 3 is set." ></td>
	<td class="line x" title="123:207	The reason to setup the condition b) is that each organization type Y should have many different organizations which have the same organization type Y, such as '~~ Acer computer', '~,.'~_,~ Leo computer', ' ~ ~ ~ ~ Blue-slcy computer', etc The real implementation shows the different threshold value n gives the different precision and recall for identification." ></td>
	<td class="line x" title="124:207	For the first iteration of knowledge extraction, we suggest to have higher recall rate." ></td>
	<td class="line x" title="125:207	Set the threshold value low and manually select the final list of the organization types." ></td>
	<td class="line x" title="126:207	For the future automatic knowledge extraction, in order to increase the precision of the information extraction higher threshold values are suggested." ></td>
	<td class="line x" title="127:207	4." ></td>
	<td class="line x" title="128:207	E~ERIME~ ~S~TS The knowledge extraction processes for Chinese org~tion names are carried out by different stages." ></td>
	<td class="line x" title="129:207	At the first stage, the words marked with semantic category of organization were accessed from the CKIP dictionary." ></td>
	<td class="line x" title="130:207	There are 1391 word organization types." ></td>
	<td class="line x" title="131:207	As mentioned in section 3.1, a pseudo morphological analysis process was carried out, which try to find the high frequency ending morphemes." ></td>
	<td class="line x" title="132:207	Since the structure of an organization name is a composition of X+Y, where X is a proper name and Y is a organization type." ></td>
	<td class="line x" title="133:207	There are 546 different ending morphemes." ></td>
	<td class="line x" title="134:207	The high frequency ending morphemes are exactly to be the morphemes for common organization types." ></td>
	<td class="line x" title="135:207	Many of them are monosyllabic words and they are polysemous, as shown in Table 1." ></td>
	<td class="line x" title="136:207	For the future identification, the disambiguation process has to be carried out for those polysemous ending morphemes (Chen & Chen 2000)." ></td>
	<td class="line x" title="137:207	The extracted morphemes and list of organizations will be the first collection of the organization types." ></td>
	<td class="line x" title="138:207	At the second stage, we try to extract new organizations names from different domain text." ></td>
	<td class="line x" title="139:207	Each different domain has many new organization types." ></td>
	<td class="line x" title="140:207	For instance in the domain of finance and business, there are many company names, which have completely different word strings for the organization types as in the extracted list by the first stage." ></td>
	<td class="line x" title="141:207	The algorithm shown in the section 3.2 was carried out." ></td>
	<td class="line x" title="142:207	At the step 1, 31787 texts of news of the finance and business domain were extracted from http://www.cnyes.com." ></td>
	<td class="line x" title="143:207	At step 2, 40675 keywords were extracted from the news corpus." ></td>
	<td class="line x" title="144:207	At step 3, organization names were identified and the organization types were extracted." ></td>
	<td class="line x" title="145:207	If the threshold value n -2, 92 types were extracted and among them 83 are correct organization types." ></td>
	<td class="line x" title="146:207	The precision is 90%." ></td>
	<td class="line x" title="147:207	If the threshold was set to 3, only 56 types were extracted and all of them happen to be correct." ></td>
	<td class="line x" title="148:207	The precision increased to 100%, but of course the recall rate dropped." ></td>
	<td class="line x" title="149:207	We don't know the exact recall rate, since there are too many keywords in the training set." ></td>
	<td class="line x" title="150:207	However the recall rate is not important, since the whole knowledge extraction process is a recurrent process." ></td>
	<td class="line x" title="151:207	The knowledge extraction procedures should be repeatedly applied on the different set of text and at each iteration more information will be extracted." ></td>
	<td class="line x" title="152:207	Hence the precision is much more important than the recall." ></td>
	<td class="line x" title="153:207	The knowledge sources for future identification of organizations are the accumulated lists of the organization names, the proper names of organizations and the organization types." ></td>
	<td class="line x" title="154:207	Table 3 contains the extracted organization types while the threshold value n=3." ></td>
	<td class="line x" title="155:207	The organization types are classified by their lengths and sorted by their frequencies of uses." ></td>
	<td class="line x" title="156:207	Table 4 contains the extracted organization types which associated with exactly two different names and the last line shows the error extractions." ></td>
	<td class="line x" title="157:207	Among newly extracted organization types only 23 of them 18 are already in the old list." ></td>
	<td class="line x" title="158:207	_~t~ 81 ~ 74 ~:~ 46 ~1~'~ 41 ~---~." ></td>
	<td class="line x" title="159:207	40 {~j 36 19 ~-~-~ 18 ~-~ 17 ~b~ 16 ~ 15 ~q~." ></td>
	<td class="line x" title="160:207	15 ~-~ 15 ~ 15 ~ 14 ~~ 13 ~ II B~ 10 ~ 9 ~ 9 ~ 8 ~ 7 ~=r.~ 6 ~fl~l~ 6 ~f~6 ~I/,~6 )L~5 ~5 ~5 ~5 Ig-~4 ~32 4 ~_T4 .T.:~3 'fJfiI 3 ~i:3 ~/~3,~3 ~3 ~3 ~3 ~3 ~ 3 ~\[\[~ 7 ~\]~ 6 Table 3." ></td>
	<td class="line x" title="161:207	The extracted organization types associated with the number of different names >=3 Table 4." ></td>
	<td class="line x" title="162:207	The extracted organization types associated with two different names and the last lines show the error exWactions." ></td>
	<td class="line x" title="163:207	4.1 Strategies for On-line Identification of Organization Names The knowledge about organizations extracted from the dictionaries and domain texts will be used to identify organization names at on-line sentence processing." ></td>
	<td class="line x" title="164:207	During the word segmentation process, an organization name is either identified immediately (if it is a known organization name), or it will be segmented into two segments of X+Y or several segments of (xl+x2++xn)+Y, where X is a proper names, Y is the organization type." ></td>
	<td class="line x" title="165:207	When the proper name X is a new word, it will be segmented into shorter segments (xl+x2++xn)." ></td>
	<td class="line x" title="166:207	To simplify the experiment process, we assume the proper names X are either the words of categories Nb (i.e. proper names) or Nc (i.e. the place names) or a two-character unknown word." ></td>
	<td class="line x" title="167:207	For the identification experiment, a corpus extract from a T.V. news (http://www.ttv.com.tw) The patterns of X+Y in the testing corpus were searched." ></td>
	<td class="line x" title="168:207	117 different organizations were identified." ></td>
	<td class="line x" title="169:207	Among thern 56 are known organizations, i.e. they are in the organization name list." ></td>
	<td class="line x" title="170:207	61 of them arc identified by the composition of X+Y and 52 of them are correct." ></td>
	<td class="line x" title="171:207	It counts the precision of 52/61=85% for identifying new names." ></td>
	<td class="line x" title="172:207	The total performance is the precision of 108/I 17=92%." ></td>
	<td class="line x" title="173:207	The knowledge-based approach for identifying organization names seems very promising." ></td>
	<td class="line x" title="174:207	It outperforms the reports of the precision of 61.79% and the recall of 54.50% in (Chen & Lee 1996) and the experiment was carried out under the condition that the knowledge extraction process is in its initial stage." ></td>
	<td class="line x" title="175:207	We expect that performance of the algorithm will become better and better while the knowledge extraction process continuously performs." ></td>
	<td class="line x" title="176:207	4.2 Automatic Extraction of Name Equivalent Classes The abbreviated names are very frequently occurred in the real text especially in the domain of the stock market." ></td>
	<td class="line x" title="177:207	By observing the abbreviation names, the heuristic rules for abbreviating a company name can be concluded as follows." ></td>
	<td class="line x" title="178:207	Abbreviation rule: If the proper name of a company is unique, then take the proper name as its abbreviation name, such as '~, Microsoft'." ></td>
	<td class="line x" title="179:207	Otherwise the abbreviation will be a compound of key-characters from part of its proper name and part of its line of business, such as ~ is the abbreviation of'~ m~l~, China petroleum'." ></td>
	<td class="line x" title="180:207	An experiment was carried out to find the full names of the abbreviations of company names shown in the price table of the Taiwan stock market." ></td>
	<td class="line x" title="181:207	The purposes of this experiment are a) to fred the equivalent classes of company names and b) to have some idea about the recall rate of the current knowledge extraction process." ></td>
	<td class="line x" title="182:207	The matching process between the abbreviations and the extracted organization name lists is as follows." ></td>
	<td class="line x" title="183:207	I. For each abbreviation name matches the organization names in the organization name list." ></td>
	<td class="line x" title="184:207	Find all the organization names containing the abbreviation name." ></td>
	<td class="line x" title="185:207	2." ></td>
	<td class="line x" title="186:207	Rank the matched organization names according to the following criterion." ></td>
	<td class="line x" title="187:207	The first rank: The proper name of the organization name is exactly matched with the abbreviation name." ></td>
	<td class="line x" title="188:207	The second rank: The abbreviation is compounding of key-characters from part of the proper name and part of the line of business of the matched organization names." ></td>
	<td class="line x" title="189:207	If there are many candidates with the same rank, then rank them according to their frequencies occurring in the training corpus." ></td>
	<td class="line x" title="190:207	19 There are 471 abbreviated company names in the price list of the stock market." ></td>
	<td class="line x" title="191:207	302 of them have matched candidates." ></td>
	<td class="line x" title="192:207	Each abbreviation name may match many different organization names." ></td>
	<td class="line x" title="193:207	The recall rate for the top ranked candidate is 282/471=60%." ></td>
	<td class="line x" title="194:207	The precision of the first rank candidate is 282/302=93%." ></td>
	<td class="line x" title="195:207	Table 5 shows some of the results." ></td>
	<td class="line x" title="196:207	Abbr." ></td>
	<td class="line x" title="197:207	Candidates arranged in the order of their ranks ~ ~ 8 ~l.IJ~l~l 3 ~I.L~-T-' 2 TW~ttt ~m~4.tt 11 I01 ~~-~; 2 48 m~ 2 I0 ~rd~;H'f~ 2 ~,f~ I 5 ~J~ll 22 ~K~~ 22 21 6 15 ~:~\]~J~ 2 ~\[~ 2 11 67 ~.JW 29 ~,~,J~ 17 6 ~.~J~gg 2 ~.,~ '~r~ 2 :~,,~~ 2 Table 5." ></td>
	<td class="line x" title="199:207	Some examples of the abbreviations and the matched candidates (the correct answer is highlighted by the boldface characters) 5." ></td>
	<td class="line x" title="200:207	CONCLUSIONS The knowledge extraction process will be continuously carried on in the future." ></td>
	<td class="line x" title="201:207	The accumulated knowledge will be utilized for the on-line unknown word identification as well as for the off-line knowledge extraction." ></td>
	<td class="line x" title="202:207	The proposed knowledge extraction processing model can be generalized to extract other types of linguistics or morphological knowledge, for instances, to extract the transliterate foreign names, to extract the titles of people." ></td>
	<td class="line x" title="203:207	Some of the errors are caused by that the titles of the people are wrongly identified as organization types, since the patterns of people's name followed by their title are commonly occurred in real text." ></td>
	<td class="line x" title="204:207	These patterns are similar to the sU'uctures of organization names." ></td>
	<td class="line x" title="205:207	Such kind of errors can be avoid, if the titles of people are known and in fact the titles of people can be extracted by the same extraction model except that most of people's names have three characters instead of two." ></td>
	<td class="line x" title="206:207	In the future, the knowledge extraction processes will be automatically carried out." ></td>
	<td class="line x" title="207:207	We expect that it will be one of the major building blocks for automatic learning systems for Chinese morphology and sentence processing." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W01-0513
Is Knowledge-Free Induction Of Multiword Unit Dictionary Headwords A Solved Problem?
Schone, Patrick;Jurafsky, Daniel;"></td>
	<td class="line x" title="1:211	Is Knowledge-Free Induction of Multiword Unit Dictionary Headwords a Solved Problem?" ></td>
	<td class="line x" title="2:211	Patrick Schone and Daniel Jurafsky University of Colorado, Boulder CO 80309 {schone, jurafsky}@cs.colorado.edu Abstract We seek a knowledge-free method for inducing multiword units from text corpora for use as machine-readable dictionary headwords." ></td>
	<td class="line x" title="3:211	We provide two major evaluations of nine existing collocation-finders and illustrate the continuing need for improvement." ></td>
	<td class="line x" title="4:211	We use Latent Semantic Analysis to make modest gains in performance, but we show the significant challenges encountered in trying this approach." ></td>
	<td class="line x" title="5:211	1 Introduction A multiword unit (MWU) is a connected collocation: a sequence of neighboring words whose exact and unambiguous meaning or connotation cannot be derived from the meaning or connotation of its components (Choueka, 1988)." ></td>
	<td class="line x" title="6:211	In other words, MWUs are typically non-compositional at some linguistic level." ></td>
	<td class="line x" title="7:211	For example, phonological non-compositionality has been observed (Finke & Weibel, 1997; Gregory, et al, 1999) where words like got [gG3Ct] and to [tu] change phonetically to gotta [gG3CG72G46] when combined." ></td>
	<td class="line x" title="8:211	We have interest in inducing headwords for machine-readable dictionaries (MRDs), so our interest is in semantic rather than phonological non-compositionality." ></td>
	<td class="line x" title="9:211	As an example of semantic non-compositionality, consider compact disk: one could not deduce that it was a music medium by only considering the semantics of compact and disk. MWUs may also be non-substitutable and/or non-modifiable (Manning and Schtze, 1999)." ></td>
	<td class="line x" title="10:211	Nonsubstitutability implies that substituting a word of the MWU with its synonym should no longer convey the same original content: compact disk does not readily imply densely-packed disk. Nonmodifiability, on the other hand, suggests one cannot modify the MWUs structure and still convey the same content: compact disk does not signify disk that is compact. MWU dictionary headwords generally satisfy at least one of these constraints." ></td>
	<td class="line x" title="11:211	For example, a compositional phrase would typically be excluded from a hard-copy dictionary since its constituent words would already be listed." ></td>
	<td class="line x" title="12:211	These strategies allow hard-copy dictionaries to remain compact." ></td>
	<td class="line x" title="13:211	As mentioned, we wish to find MWU headwords for machine-readable dictionaries (MRDs)." ></td>
	<td class="line x" title="14:211	Although space is not an issue in MRDs, we desire to follow the lexicographic practice of reducing redundancy." ></td>
	<td class="line x" title="15:211	As Sproat indicated, 'simply expanding the dictionary to encompass every word one is ever likely to encounter is wrong: it fails to take advantage of regularities' (1992, p. xiii)." ></td>
	<td class="line x" title="16:211	Our goal is to identify an automatic, knowledge-free algorithm that finds all and only those collocations where it is necessary to supply a definition." ></td>
	<td class="line x" title="17:211	Knowledge-free means that the process should proceed without human input (other than, perhaps, indicating whitespace and punctuation)." ></td>
	<td class="line x" title="18:211	This seems like a solved problem." ></td>
	<td class="line x" title="19:211	Many collocation-finders exist, so one might suspect that most could suffice for finding MWU dictionary headwords." ></td>
	<td class="line x" title="20:211	To verify this, we evaluate nine existing collocation-finders to see which best identifies valid headwords." ></td>
	<td class="line x" title="21:211	We evaluate using two completely separate gold standards: (1) WordNet and (2) a compendium of Internet dictionaries." ></td>
	<td class="line x" title="22:211	Although web-based resources are dynamic and have better coverage than WordNet (especially for acronyms and names), we show that WordNet-based scores are comparable to those using Internet MRDs." ></td>
	<td class="line x" title="23:211	Yet the evaluations indicate that significant improvement is still needed in MWU-induction." ></td>
	<td class="line x" title="24:211	As an attempt to improve MWU headword induction, we introduce several algorithms using Latent Semantic Analysis (LSA)." ></td>
	<td class="line x" title="25:211	LSA is a technique which automatically induces semantic relationships between words." ></td>
	<td class="line x" title="26:211	We use LSA to try to eliminate proposed MWUs which are semantically compositional." ></td>
	<td class="line x" title="27:211	Unfortunately, this does not help." ></td>
	<td class="line x" title="28:211	Yet when we use LSA to identify substitutable delimiters." ></td>
	<td class="line x" title="29:211	This suggests that in a language with MWUs, we do show modest performance gains." ></td>
	<td class="line x" title="30:211	whitespace, one might prefer to begin at the word 2 Previous Approaches For decades, researchers have explored various techniques for identifying interesting collocations." ></td>
	<td class="line x" title="31:211	There have essentially been three separate kinds of approaches for accomplishing this task." ></td>
	<td class="line x" title="32:211	These approaches could be broadly classified into (1) segmentation-based, (2) word-based and knowledgedriven, or (3) word-based and probabilistic." ></td>
	<td class="line x" title="33:211	We will illustrate strategies that have been attempted in each of the approaches." ></td>
	<td class="line x" title="34:211	Since we assume knowledge of whitespace, and since many of the first and all of the second categories rely upon human input, we will be most interested in the third category." ></td>
	<td class="line x" title="35:211	2.1 Segmentation-driven Strategies Some researchers view MWU-finding as a natural by-product of segmentation." ></td>
	<td class="line x" title="36:211	One can regard text as a stream of symbols and segmentation as a means of placing delimiters in that stream so as to separate logical groupings of symbols from one another." ></td>
	<td class="line x" title="37:211	A segmentation process may find that a symbol stream should not be delimited even though subcomponents of the stream have been seen elsewhere." ></td>
	<td class="line x" title="38:211	In such cases, these larger units may be MWUs." ></td>
	<td class="line x" title="39:211	The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et." ></td>
	<td class="line x" title="40:211	al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al. , 2000; and many others)." ></td>
	<td class="line x" title="41:211	Such efforts have employed various strategies for segmentation, including the use of hidden Markov models, minimum description length, dictionary-based approaches, probabilistic automata, transformation-based learning, and text compression." ></td>
	<td class="line x" title="42:211	Some of these approaches require significant sources of human knowledge, though others, especially those that follow data compression or HMM schemes, do not." ></td>
	<td class="line x" title="43:211	These approaches could be applied to languages where word delimiters exist (such as in European languages delimited by the space character)." ></td>
	<td class="line x" title="44:211	However, in such languages, it seems more prudent to simply take advantage of delimiters rather than introducing potential errors by trying to find word boundaries while ignoring knowledge of the level and identify appropriate word combinations." ></td>
	<td class="line x" title="45:211	2.2 Word-based, knowledge-driven Strategies Some researchers start with words and propose MWU induction methods that make use of parts of speech, lexicons, syntax or other linguistic structure (Justeson and Katz, 1995; Jacquemin, et al. , 1997; Daille, 1996)." ></td>
	<td class="line x" title="46:211	For example, Justeson and Katz indicated that the patterns NOUN NOUN and ADJ NOUN are very typical of MWUs." ></td>
	<td class="line x" title="47:211	Daille also suggests that in French, technical MWUs follow patterns such as NOUN de NOUN' (1996, p. 50)." ></td>
	<td class="line x" title="48:211	To find word combinations that satisfy such patterns in both of these situations necessitates the use of a lexicon equipped with part of speech tags." ></td>
	<td class="line x" title="49:211	Since we are interested in knowledge-free induction of MWUs, these approaches are less directly related to our work." ></td>
	<td class="line x" title="50:211	Furthermore, we are not really interested in identifying constructs such as general noun phrases as the above rules might generate, but rather, in finding only those collocations that one would typically need to define." ></td>
	<td class="line x" title="51:211	2.3 Word-based, Probabilistic Approaches The third category assumes at most whitespace and punctuation knowledge and attempts to infer MWUs using word combination probabilities." ></td>
	<td class="line x" title="52:211	Table 1 (see next page) shows nine commonly-used probabilistic MWU-induction approaches." ></td>
	<td class="line x" title="53:211	In the table, f and P signify frequency and probability XX of a word X. A variable XY indicates a word bigram and indicates its expected frequency at random." ></td>
	<td class="line x" title="54:211	XY An overbar signifies a variables complement." ></td>
	<td class="line x" title="55:211	For more details, one can consult the original sources as well as Ferreira and Pereira (1999) and Manning and Schtze (1999)." ></td>
	<td class="line x" title="56:211	3 Lexical Access Prior to applying the algorithms, we lemmatize using a weakly-informed tokenizer that knows only that whitespace and punctuation separate words." ></td>
	<td class="line x" title="57:211	Punctuation can either be discarded or treated as words." ></td>
	<td class="line x" title="58:211	Since we are equally interested in finding units like Dr. and U. S., we opt to treat punctuation as words." ></td>
	<td class="line x" title="59:211	Once we tokenize, we use Churchs (1995) suffix array approach to identify word n-grams that occur at least T times (for T=10)." ></td>
	<td class="line x" title="60:211	We then rank-order the P X|Y MI XY M Z Pr Z|Y MI ZY G092log [P X P Y P X P Y ] f Y [P XY P XY ] f XY [P XY P XY ] f XY M iG13X,X} jG13Y,Y} (f ij G09 ij ) 2 ij f XY G09 XY XY (1G09( XY /N)) f XY G09 XY f XY (1G09(f XY /N)) Table 1: Probabilistic Approaches METHOD FORMULA Frequency (Guiliano, 1964) f XY Pointwise Mutual Information (MI) (Fano, 1961; Church and Hanks, 1990) log (P / PP) 2XY XY Selectional Association (Resnik, 1996) Symmetric Conditional Probability (Ferreira and Pereira, 1999) P / PP XY X Y 2 Dice Formula (Dice, 1945) 2 f / (f +f ) XY X Y Log-likelihood (Dunning, 1993; (Daille, 1996)." ></td>
	<td class="line oc" title="61:211	Since we need knowledge-poor Daille, 1996) induction, we cannot use human-suggested filtering Chi-squared (G24 ) 2 (Church and Gale, 1991) Z-Score (Smadja, 1993; Fontenelle, et al. , 1994) Students t-Score (Church and Hanks, 1990) n-gram list in accordance to each probabilistic algorithm." ></td>
	<td class="line x" title="62:211	This task is non-trivial since most algorithms were originally suited for finding twoword collocations." ></td>
	<td class="line x" title="63:211	We must therefore decide how to expand the algorithms to identify general n-grams (say, C=w ww )." ></td>
	<td class="line x" title="64:211	We can either generalize or 12 n approximate." ></td>
	<td class="line x" title="65:211	Since generalizing requires exponential compute time and memory for several of the algorithms, approximation is an attractive alternative." ></td>
	<td class="line x" title="66:211	One approximation redefines X and Y to be, respectively, the word sequences www and 12 i www where i is chosen to maximize P P . i+1 i+2 n, X Y This has a natural interpretation of being the expected probability of concatenating the two most probable substrings in order to form the larger unit." ></td>
	<td class="line x" title="67:211	Since it can be computed rapidly with low memory costs, we use this approximation." ></td>
	<td class="line x" title="68:211	Two additional issues need addressing before evaluation." ></td>
	<td class="line x" title="69:211	The first regards document sourcing." ></td>
	<td class="line x" title="70:211	If an n-gram appears in multiple sources (eg., Congressional Record versus Associated Press), its likelihood of accuracy should increase." ></td>
	<td class="line x" title="72:211	This is particularly true if we are looking for MWU headwords for a general versus specialized dictionary." ></td>
	<td class="line x" title="73:211	Phrases that appear in one source may in fact be general MWUs, but frequently, they are text-specific units." ></td>
	<td class="line x" title="74:211	Hence, precision gained by excluding single-source n-grams may be worth losses in recall." ></td>
	<td class="line x" title="75:211	We will measure this trade-off." ></td>
	<td class="line x" title="76:211	Second, evaluating with punctuation as words and applying no filtering mechanism may unfairly bias against some algorithms." ></td>
	<td class="line x" title="77:211	Preor post-processing of n-grams with a linguistic filter has shown to improve some induction algorithms performance rules as in Section 2.2." ></td>
	<td class="line x" title="78:211	Yet we can filter by pruning n-grams whose beginning or ending word is among the top N most frequent words." ></td>
	<td class="line x" title="79:211	This unfortunately eliminates acronyms like U. S. and phrasal verbs like throw up. However, discarding some words may be worthwhile if the final list of n-grams is richer in terms of MRD headwords." ></td>
	<td class="line x" title="80:211	We therefore evaluate with such an automatic filter, arbitrarily (and without optimization) choosing N=75." ></td>
	<td class="line x" title="81:211	4 Evaluating Performance A natural scoring standard is to select a language and evaluate against headwords from existing dictionaries in that language." ></td>
	<td class="line x" title="82:211	Others have used similar standards (Daille, 1996), but to our knowledge, none to the extent described here." ></td>
	<td class="line x" title="83:211	We evaluate thousands of hypothesized units from an unconstrained corpus." ></td>
	<td class="line x" title="84:211	Furthermore, we use two separate evaluation gold standards: (1) WordNet (Miller, et al, 1990) and (2) a collection of Internet MRDs." ></td>
	<td class="line x" title="85:211	Using two gold standards helps valid MWUs." ></td>
	<td class="line x" title="86:211	It also provides evaluation using both static and dynamic resources." ></td>
	<td class="line x" title="87:211	We choose to evaluate in English due to the wealth of linguistic resources." ></td>
	<td class="line x" title="88:211	Rank ZScore G242SCPDice Mutual Info." ></td>
	<td class="line x" title="89:211	Select Assoc." ></td>
	<td class="line x" title="90:211	Log Like." ></td>
	<td class="line x" title="91:211	TScore Freq 1 Iwo Jima Buenos Aires Buenos Aires Buenos Aires Iwo Jima United States United States United States United States 2 bona fide Iwo Jima Iwo Jima Iwo Jima bona fide House of Representatives Los Angeles Los Angeles Los Angeles 4 Burkina Faso Suu Kyi Suu Kyi Suu Kyi Wounded Knee Los Angeles New York New York New York 8 Satanic Verses Sault Ste Sault Ste Sault Ste Hubble Space Telescope my colleagues Soviet Union my colleagues my colleagues 16 Ku Klux Ku Klux Ku Klux Ku Klux alma mater H . R Social Security High School High School 32 Pledge of Allegiance Pledge of Allegiance Pledge of Allegiance Pledge of Allegiance Coca Cola War II House of Representatives Wednesday * * * * 64 Telephone & amp ; Telegraph Telephone & amp ; Telegraph Telephone & amp ; Telegraph Internal Revenue Planned Parenthood Prime Minister * * * real estate New Jersey 128 Prime Minister Prime Minister Prime Minister Salman Rushdie Sault Ste . Marie both sides At the same time Wall Street term care 256 Lehman Hutton Lehman Hutton Lehman Hutton tongue in cheek o  clock At the same del Mar all over grand jury 512 La Habra La Habra La Habra compensatory and punitive 20th Century Monday night days later 80 percent Great Northern 1024 telephone interview telephone interview telephone interview Food and Agriculture Sheriff s deputies South Dakota County Jail where you 300 million Table 2: Outputs from each algorithm at different sorted ranks The * * and * * * are actual units." ></td>
	<td class="line x" title="92:211	In particular, we use a randomly-selected corpus the first five columns as information-like. consisting of a 6.7 million word subset of the TREC Similarly, since the last four columns share databases (DARPA, 1993-1997)." ></td>
	<td class="line x" title="93:211	properties of the frequency approach, we will refer Table 2 illustrates a sample of rank-ordered output to them as frequency-like. from each of the different algorithms (following the Ones application may dictate which set of cross-source, filtered paradigm described in section algorithms to use." ></td>
	<td class="line x" title="94:211	Our gold standard selection 3)." ></td>
	<td class="line x" title="95:211	Note that algorithms in the first four columns reflects our interest in general word dictionaries, so produce results that are similar to each other as do results we obtain may differ from results we might those in the last four columns." ></td>
	<td class="line x" title="96:211	Although the mutual have obtained using terminology lexicons." ></td>
	<td class="line x" title="97:211	information results seem to be almost in a class of If our gold standard contains K MWUs with their own, they actually are similar overall to the corpus frequencies satisfying threshold (T=10), our first four sets of results; therefore, we will refer to figure of merit (FOM) is given by 1 K M K iG0A1 P i, little or even negative impact." ></td>
	<td class="line x" title="98:211	On the other hand, where P (precision at i) equals i/H, and H is the i i i number of hypothesized MWUs required to find the i correct MWU." ></td>
	<td class="line x" title="99:211	This FOM corresponds to area th under a precision-recall curve." ></td>
	<td class="line x" title="100:211	4.1 WordNet-based Evaluation WordNet has definite advantages as an evaluation resource." ></td>
	<td class="line x" title="101:211	It has in excess of 50,000 MWUs, is freely accessible, widely used, and is in electronic form." ></td>
	<td class="line x" title="102:211	Yet, it obviously cannot contain every MWU." ></td>
	<td class="line x" title="103:211	For instance, our corpus contains 177,331 n-grams (for 2G06nG0610) satisfying TG0710, but WordNet contains only 2610 of these." ></td>
	<td class="line x" title="104:211	It is unclear, therefore, if algorithms are wrong when they propose MWUs that are not in WordNet." ></td>
	<td class="line x" title="105:211	We will assume they are wrong but with a special caveat for proper nouns." ></td>
	<td class="line x" title="106:211	WordNet includes few proper noun MWUs." ></td>
	<td class="line x" title="107:211	Yet several algorithms produce large numbers of proper nouns." ></td>
	<td class="line x" title="108:211	This biases against them." ></td>
	<td class="line x" title="109:211	One could contend that all proper nouns MWUs are valid, but we disagree." ></td>
	<td class="line x" title="110:211	Although such may be MWUs, they are not necessarily MRD headwords; one would not include every proper noun in a dictionary, but rather, those needing definitions." ></td>
	<td class="line x" title="111:211	To overcome this, we will have two scoring modes." ></td>
	<td class="line x" title="112:211	The first, S mode (standing for some) discards any proposed capitalized n-gram whose uncapitalized version is not in WordNet." ></td>
	<td class="line x" title="113:211	The second mode N (for none) disregards all capitalized n-grams." ></td>
	<td class="line x" title="114:211	Table 3 illustrates algorithmic performance as compared to the 2610 MWUs from WordNet." ></td>
	<td class="line x" title="115:211	The first double column illustrates out-of-the-box performance on all 177,331 possible n-grams." ></td>
	<td class="line x" title="116:211	The second double column shows cross-sourcing: only hypothesizing MWUs that appear in at least two separate datasets (124,952 in all), but being evaluated against all of the 2610 valid units." ></td>
	<td class="line x" title="117:211	Double columns 3 and 4 show effects from high-frequency filtering the n-grams of the first and second columns (reporting only 29,716 and 17,720 n-grams) respectively." ></td>
	<td class="line x" title="118:211	As Table 3 suggests, for every condition, the information-like algorithms seem to perform best at identifying valid, general MWU headwords." ></td>
	<td class="line x" title="119:211	Moreover, they are enhanced when cross-sourcing is considered; but since much of their strength comes from identifying proper nouns, filtering has the frequency-like approaches are independent of data source." ></td>
	<td class="line x" title="120:211	They also improve significantly with filtering." ></td>
	<td class="line x" title="121:211	Overall, though, after the algorithms are judged, even the best score of 0.265 is far short of the maximum possible, namely 1.0." ></td>
	<td class="line x" title="122:211	Table 3: WordNet-based scores Prob (1) (2) (3) (4) algoWordNet WordNet WordNet WordNet rithm cross+Filter crosssource source +Filter S N S N SNSN Zscore .222 .146 .263 .193 .220 .129 .265 .173 SCP .221 .145 .262 .192 .220 .129 .265 .173 Chi-sqr .222 .146 .263 .193 .220 .129 .265 .173 Dice .242 .167 .265 .199 .230 .142 .256 .172 MI .191 .122 .245 .169 .185 .111 .233 .151 SA .057 .051 .058 .053 .182 .125 .202 .143 Loglike .049 .050 .068 .064 .118 .095 .177 .129 T-score .050 .051 .050 .052 .150 .109 .160 .118 Freq .035 .037 .034 .037 .144 .105 .152 .112 4.2 Web-based Evaluation Since WordNet is static and cannot report on all of a corpus n-grams, one may expect different performance by using a more all-encompassing, dynamic resource." ></td>
	<td class="line x" title="123:211	The Internet houses dynamic resources which can judge practically every induced n-gram." ></td>
	<td class="line x" title="124:211	With permission and sufficient time, one can repeatedly query websites that host large collections of MRDs and evaluate each n-gram." ></td>
	<td class="line x" title="125:211	Having approval, we queried: (1) onelook.com, (2) acronymfinder.com, and (3) infoplease.com." ></td>
	<td class="line x" title="126:211	The first website interfaces with over 600 electronic dictionaries." ></td>
	<td class="line x" title="127:211	The second is devoted to identifying proper acronyms." ></td>
	<td class="line x" title="128:211	The third focuses on world facts such as historical figures and organization names." ></td>
	<td class="line x" title="129:211	To minimize disruption to websites by reducing the total number of queries needed for evaluation, we use an evaluation approach from the information retrieval community (Sparck-Jones and van Rijsbergen, 1975)." ></td>
	<td class="line x" title="130:211	Each algorithm reports its top 5000 MWU choices and the union of these choices (45192 possible n-grams) is looked up on the Internet." ></td>
	<td class="line x" title="131:211	Valid MWUs identified at any website are assumed to be the only valid units in the data." ></td>
	<td class="line x" title="132:211	{X i } n iG0A1 {X i G0B } n iG0A1 cos(X,Y) G0A XG23Y ||X|| ||Y|| . Algorithms are then evaluated based on this showed how one could compute latent semantic collection." ></td>
	<td class="line x" title="133:211	Although this strategy for evaluation is vectors for any word in a corpus (Schone and not flawless, it is reasonable and makes dynamic Jurafsky, 2000)." ></td>
	<td class="line x" title="134:211	Using the same approach, we evaluation tractable." ></td>
	<td class="line x" title="135:211	Table 4 shows the algorithms compute semantic vectors for every proposed word performance (including proper nouns)." ></td>
	<td class="line x" title="136:211	n-gram C=X X X Since LSA involves word Though Internet dictionaries and WordNet are counts, we can also compute semantic vectors completely separate gold standards, results are surprisingly consistent." ></td>
	<td class="line x" title="137:211	One can conclude that WordNet may safely be used as a gold standard in future MWU headword evaluations." ></td>
	<td class="line x" title="138:211	Also, Table 4 Performance on Internet data Prob (1) (2) (3) (4) algorithm Internet Internet Internet Internet cross+Filter crosssource source +Filter Z-Score .165 .260 .169 .269 SCP .166 .259 .170 .270 Chi-sqr .166 .260 .170 .270 Dice .183 .258 .187 .267 MI .139 .234 .140 .234 SA .027 .033 .107 .194 Log Like .023 .043 .087 .162 T-score .025 .027 .110 .142 Freq .016 .017 .104 .134 one can see that Z-scores, G24, and 2 SCP have virtually identical results and seem to best identify MWU headwords (particularly if proper nouns are desired)." ></td>
	<td class="line x" title="139:211	Yet there is still significant room for improvement." ></td>
	<td class="line x" title="140:211	5 Improvement strategies Can performance be improved?" ></td>
	<td class="line x" title="141:211	Numerous strategies could be explored." ></td>
	<td class="line x" title="142:211	An idea we discuss here tries using induced semantics to rescore the output of the best algorithm (filtered, cross-sourced Zscore) and eliminate semantically compositional or modifiable MWU hypotheses." ></td>
	<td class="line x" title="143:211	Deerwester, et al (1990) introduced Latent Semantic Analysis (LSA) as a computational technique for inducing semantic relationships between words and documents." ></td>
	<td class="line x" title="144:211	It forms highdimensional vectors using word counts and uses singular value decomposition to project those vectors into an optimal k-dimensional, semantic subspace (see Landauer, et al, 1998)." ></td>
	<td class="line x" title="145:211	Following an approach from Schtze (1993), we 12 n." ></td>
	<td class="line x" title="146:211	(denoted by G0D) for Cs subcomponents." ></td>
	<td class="line x" title="147:211	These can either include ( ) or exclude ( ) Cs counts." ></td>
	<td class="line x" title="148:211	We seek to see if induced semantics can help eliminate incorrectly-chosen MWUs." ></td>
	<td class="line x" title="149:211	As will be shown, the effort using semantics in this nature has a very small payoff for the expended cost." ></td>
	<td class="line x" title="150:211	5.1 Non-compositionality Non-compositionality is a key component of valid MWUs, so we may desire to emphasize n-grams that are semantically non-compositional." ></td>
	<td class="line x" title="151:211	Suppose we wanted to determine if C (defined above) were noncompositional." ></td>
	<td class="line x" title="152:211	Then given some meaning function, G0C, C should satisfy an equation like: g( G0C(C), h( G0C(X ),,G0C(X ) ) )G070, (1) 1n where h combines the semantics of Cs subcomponents and g measures semantic differences." ></td>
	<td class="line x" title="153:211	If C were a bigram, then if g(a,b) is defined to be |a-b|, if h(c,d) is the sum of c and d, and if G0C(e) is set to -log P, then equation (1) would e become the pointwise mutual information of the bigram." ></td>
	<td class="line x" title="154:211	If g(a,b) were defined to be (a-b)/b, and if  h(a,b)=ab/N and G0C(X)=f, we essentially get ZX scores." ></td>
	<td class="line x" title="155:211	These formulations suggest that several of the probabilistic algorithms we have seen include non-compositionality measures already." ></td>
	<td class="line x" title="156:211	However, since the probabilistic algorithms rely only on distributional information obtained by considering juxtaposed words, they tend to incorporate a significant amount of non-semantic information such as syntax." ></td>
	<td class="line x" title="157:211	Can semantic-only rescoring help?" ></td>
	<td class="line x" title="158:211	To find out, we must select g, h, and G0C." ></td>
	<td class="line x" title="159:211	Since we want to eliminate MWUs that are compositional, we want hs output to correlate well with C when there is compositionality and correlate poorly otherwise." ></td>
	<td class="line x" title="160:211	Frequently, LSA vectors are correlated using the cosine between them: A large cosine indicates strong correlation, so large values for g(a,b)=1-|cos(a,b)| should signal weak correlation or non-compositionality." ></td>
	<td class="line x" title="161:211	h could G4D n iG0A1 w i a i cos cos(X i,Y) G0A min kG13X i,Y} cos(G0D Xi,G0D Y )G09 k G31 k . represent a weighted vector sum of the components required for this task." ></td>
	<td class="line x" title="162:211	This seems to be a significant semantic vectors with weights (w ) set to either 1.0 component." ></td>
	<td class="line x" title="163:211	Yet there is still another: maybe i or the reciprocal of the words frequencies." ></td>
	<td class="line x" title="164:211	semantic compositionality is not always bad." ></td>
	<td class="line x" title="165:211	Table 5 indicates several results using these Interestingly, this is often the case." ></td>
	<td class="line x" title="166:211	Consider settings." ></td>
	<td class="line x" title="167:211	As the first four rows indicate and as vice_president, organized crime, and desired, non-compositionality is more apparent for Marine_Corps." ></td>
	<td class="line x" title="168:211	Although these are MWUs, one G0D * (i.e. , the vectors derived from excluding Cs X counts) than for G0D . Yet, performance overall is X horrible, particularly considering we are rescoring Z-score output whose score was 0.269." ></td>
	<td class="line x" title="169:211	Rescoring caused five-fold degradation!" ></td>
	<td class="line x" title="170:211	Table 5: Equation 1 settings g(a,b)h(a) (X) w Score on i Internet 1-|cos(a,b)| G0D X 1 0.0517 1/fi 0.0473 G0D * X 1 0.0598 1/fi* 0.0523 |cos(a,b)| G0D X 1 0.174 1/fi 0.169 G0D * X 1 0.131 1/fi* 0.128 What happens if we instead emphasize compositionality?" ></td>
	<td class="line x" title="171:211	Rows 5-8 illustrate the effect: there is a significant recovery in performance." ></td>
	<td class="line x" title="172:211	The most reasonable explanation for this is that if MWUs and their components are strongly correlated, the components may rarely occur except in context with the MWU." ></td>
	<td class="line x" title="173:211	It takes about 20 hours to compute the G0D * for each possible n-gram X combination." ></td>
	<td class="line x" title="174:211	Since the probabilistic algorithms already identify n-grams that share strong distributional properties with their components, it seems imprudent to exhaust resources on this LSAbased strategy for non-compositionality." ></td>
	<td class="line x" title="175:211	These findings warrant some discussion." ></td>
	<td class="line x" title="176:211	Why did non-compositionality fail?" ></td>
	<td class="line x" title="177:211	Certainly there is the possibility that better choices for g, h, and could yield improvements." ></td>
	<td class="line x" title="178:211	We actually spent months trying to find an optimal combination as well as a strategy for coupling LSA-based scores with the Zscores, but without avail." ></td>
	<td class="line x" title="179:211	Another possibility: although LSA can find semantic relationships, it may not make semantic decisions at the level would still expect that the first is related to president, the second relates to crime, and the last relates to Marine." ></td>
	<td class="line x" title="180:211	Similarly, tokens such as Johns_Hopkins and Elvis are anaphors for Johns_Hopkins_University and Elvis_Presley, so they should have similar meanings." ></td>
	<td class="line x" title="181:211	This begs the question: can induced semantics help at all?" ></td>
	<td class="line x" title="182:211	The answer is yes. The key is using LSA where it does best: finding things that are similar  or substitutable." ></td>
	<td class="line x" title="183:211	5.2 Non-substitutivity For every collocation C=X X X X X X, we 1 2 i-1 i+1 ni attempt to find other similar patterns in the data, X X X YX X . If X and Y are semantically 1 2 i-1 i+1 n i related, chances are that C is substitutable." ></td>
	<td class="line x" title="184:211	Since LSA excels at finding semantic correlations, we can compare G0D and G0D to see if C is Xi Y substitutable." ></td>
	<td class="line x" title="185:211	We use our earlier approach (Schone and Jurafsky, 2000) for performing the comparison; namely, for every word W, we compute cos(G0DG0D) w, R for 200 randomly chosen words, R. This allows for computation of a correlaton mean ( ) and standard W deviation (1 ) between W and other words." ></td>
	<td class="line x" title="186:211	As W before, we then compute a normalized cosine score ( ) between words of interest, defined by With this set-up, we now look for substitutivity." ></td>
	<td class="line x" title="187:211	Note that phrases may be substitutable and still be headword if their substitute phrases are themselves MWUs." ></td>
	<td class="line x" title="188:211	For example, dioxide in carbon_dioxide is semantically similar to monoxide in carbon_monoxide." ></td>
	<td class="line x" title="189:211	Moreover, there are other important instances of valid substitutivity: G26 Abbreviations AlG12Albert G3C Al_GoreG12Albert_Gore G26 Morphological similarities RicoG12Rican G3C Puerto_RicoG12Puerto_Rican G26 Taxonomic relationships bachelorG11masterG3C bachelor__s_degreeG11master__s_degree." ></td>
	<td class="line x" title="190:211	Figure 1: Precision-recall curve for rescoring However, guilty and innocent are semantically related, but pleaded_guilty and pleaded_innocent are not MWUs." ></td>
	<td class="line x" title="191:211	We would like to emphasize only ngrams whose substitutes are valid MWUs." ></td>
	<td class="line x" title="192:211	To show how we do this using LSA, suppose we want to rescore a list L whose entries are potential MWUs." ></td>
	<td class="line x" title="193:211	For every entry X in L, we seek out all other entries whose sorted order is less than some maximum value (such as 5000) that have all but one word in common." ></td>
	<td class="line x" title="194:211	For example, suppose X is bachelor__s_degree. The only other entry that matches in all but one word is master__s_degree. If the semantic vectors for bachelor and master have a normalized cosine score greater than a threshold of 2.0, we then say that the two MWUs are in each others substitution set." ></td>
	<td class="line x" title="195:211	To rescore, we assign a new score to each entry in substitution set." ></td>
	<td class="line x" title="196:211	Each element in the substitution set gets the same score." ></td>
	<td class="line x" title="197:211	The score is derived using a combination of the previous Z-scores for each element in the substitution set." ></td>
	<td class="line x" title="198:211	The combining function may be an averaging, or a computation of the median, the maximum, or something else." ></td>
	<td class="line x" title="199:211	The maximum outperforms the average and the median on our data." ></td>
	<td class="line x" title="200:211	By applying in to our data, we observe a small but visible improvement of 1.3% absolute to .282 (see Fig." ></td>
	<td class="line x" title="201:211	1)." ></td>
	<td class="line x" title="202:211	It is also possible that other improvements could be gained using other combining strategies." ></td>
	<td class="line x" title="203:211	6 Conclusions This paper identifies several new results in the area of MWU-finding." ></td>
	<td class="line x" title="204:211	We saw that MWU headword evaluations using WordNet provide similar results to those obtained from far more extensive webbased resources." ></td>
	<td class="line x" title="205:211	Thus, one could safely use WordNet as a gold standard for future evaluations." ></td>
	<td class="line x" title="206:211	We also noted that information-like algorithms, particularly Z-scores, SCP, and G242, seem to perform best at finding MRD headwords regardless of filtering mechanism, but that improvements are still needed." ></td>
	<td class="line x" title="207:211	We proposed two new LSA-based approaches which attempted to address issues of non-compositionality and non-substitutivity." ></td>
	<td class="line x" title="208:211	Apparently, either current algorithms already capture much non-compositionality or LSA-based models of non-compositionality are of little help." ></td>
	<td class="line x" title="209:211	LSA does help somewhat as a model of substitutivity." ></td>
	<td class="line x" title="210:211	However, LSA-based gains are small compared to the effort required to obtain them." ></td>
	<td class="line x" title="211:211	Acknowledgments The authors would like to thank the anonymous reviewers for their comments and insights." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C02-1007
The Computation Of Word Associations: Comparing Syntagmatic And Paradigmatic Approaches
Rapp, Reinhard;"></td>
	<td class="line x" title="1:177	The Computation of Word Associations: Comparing Syntagmatic and Paradigmatic Approaches Reinhard Rapp University of Mainz, FASK D-76711 Germersheim, Germany rapp@mail.fask.uni-mainz.de Abstract It is shown that basic language processes such as the production of free word associations and the generation of synonyms can be simulated using statistical models that analyze the distribution of words in large text corpora." ></td>
	<td class="line x" title="2:177	According to the law of association by contiguity, the acquisition of word associations can be explained by Hebbian learning." ></td>
	<td class="line x" title="3:177	The free word associations as produced by subjects on presentation of single stimulus words can thus be predicted by applying first-order statistics to the frequencies of word co-occurrences as observed in texts." ></td>
	<td class="line x" title="4:177	The generation of synonyms can also be conducted on co-occurrence data but requires second-order statistics." ></td>
	<td class="line x" title="5:177	The reason is that synonyms rarely occur together but appear in similar lexical neighborhoods." ></td>
	<td class="line x" title="6:177	Both approaches are systematically compared and are validated on empirical data." ></td>
	<td class="line x" title="7:177	It turns out that for both tasks the performance of the statistical system is comparable to the performance of human subjects." ></td>
	<td class="line x" title="8:177	1 Introduction According to Ferdinand de Saussure (1916), there are two fundamental types of relations between words that he believes correspond to basic operations of our brain: syntagmatic and paradigmatic associations." ></td>
	<td class="line x" title="9:177	There is a syntagmatic relation between two words if they co-occur in spoken or written language more frequently than expected from chance and if they have different grammatical roles in the sentences in which they occur." ></td>
	<td class="line x" title="10:177	Typical examples are the word pairs coffee drink, sun hot, or teacher school." ></td>
	<td class="line x" title="11:177	The relation between two words is paradigmatic if the two words can substitute for one another in a sentence without affecting the grammaticality or acceptability of the sentence." ></td>
	<td class="line x" title="12:177	Typical examples are synonyms or antonyms like quick fast, or eat drink." ></td>
	<td class="line x" title="13:177	Normally, words with a paradigmatic relation are the same part of speech, whereas words with a syntagmatic relation can but need not be the same part of speech." ></td>
	<td class="line x" title="14:177	In this paper we want to show that the two types of relations as defined by de Saussure are reflected in the statistical distribution of words in large corpora." ></td>
	<td class="line x" title="15:177	We present algorithms that automatically retrieve words with either the syntagmatic or the paradigmatic type of relationship from corpora and perform a quantitative evaluation of our results." ></td>
	<td class="line x" title="16:177	2 Paradigmatic Associations Paradigmatic associations are words with high semantic similarity." ></td>
	<td class="line x" title="17:177	According to Ruge (1992), the semantic similarity of two words can be computed by determining the agreement of their lexical neighborhoods." ></td>
	<td class="line x" title="18:177	For example, the semantic similarity of the words red and blue can be derived from the fact that they both frequently co-occur with words like color, flower, dress, car, dark, bright, beautiful, and so forth." ></td>
	<td class="line x" title="19:177	If for each word in a corpus a cooccurrence vector is determined whose entries are the co-occurrences with all other words in the corpus, then the semantic similarities between words can be computed by conducting simple vector comparisons." ></td>
	<td class="line x" title="20:177	To determine the words most similar to a given word, its co-occurrence vector is compared to the co-occurrence vectors of all other words using one of the standard similarity measures, for example, the cosine coefficient." ></td>
	<td class="line x" title="21:177	Those words that obtain the best values are considered to be most similar." ></td>
	<td class="line x" title="22:177	Practical implementations of algorithms based on this principle have led to excellent results as documented in papers by Ruge (1992), Grefenstette (1994), Agarwal (1995), Landauer & Dumais (1997), Schtze (1997), and Lin (1998)." ></td>
	<td class="line x" title="23:177	2.1 Human Data In this section we relate the results of our version of such an algorithm to similarity estimates obtained by human subjects." ></td>
	<td class="line x" title="24:177	Fortunately, we did not need to conduct our own experiment to obtain the humans similarity estimates." ></td>
	<td class="line x" title="25:177	Instead, such data was kindly provided by Thomas K. Landauer, who had taken it from the synonym portion of the Test of English as a Foreign Language (TOEFL)." ></td>
	<td class="line x" title="26:177	Originally, the data came, along with normative data, from the Educational Testing Service (Landauer & Dumais 1997)." ></td>
	<td class="line x" title="27:177	The TOEFL is an obligatory test for foreign students who would like to study at an American or English university." ></td>
	<td class="line x" title="28:177	The data comprises 80 test items." ></td>
	<td class="line x" title="29:177	Each item consists of a problem word in testing parlance and four alternative words, from which the test taker is asked to choose that with the most similar meaning to the problem word." ></td>
	<td class="line x" title="30:177	For example, given the test sentence Both boats and trains are used for transporting the materials and the four alternative words planes, ships, canoes, and railroads, the subject would be expected to choose the word ships, which is the one most similar to boats." ></td>
	<td class="line x" title="31:177	2.2 Corpus As mentioned above, our method of simulating this kind of behavior is based on regularities in the statistical distribution of words in a corpus." ></td>
	<td class="line x" title="32:177	We chose to use the British National Corpus (BNC), a 100million-word corpus of written and spoken language that was compiled with the intention of providing a representative sample of British English." ></td>
	<td class="line x" title="33:177	Since this corpus is rather large, to save disk space and processing time we decided to remove all function words from the text." ></td>
	<td class="line x" title="34:177	This was done on the basis of a list of approximately 200 English function words." ></td>
	<td class="line x" title="35:177	We also decided to lemmatize the corpus as well as the test data." ></td>
	<td class="line x" title="36:177	This not only reduces the sparse-data problem but also significantly reduces the size of the co-occurrence matrix to be computed." ></td>
	<td class="line x" title="37:177	More details on these two steps of corpus preprocessing can be found in Rapp (1999)." ></td>
	<td class="line x" title="38:177	2.3 Co-occurrence Counting For counting word co-occurrences, as in most other studies a fixed window size is chosen and it is determined how often each pair of words occurs within a text window of this size." ></td>
	<td class="line x" title="39:177	Choosing a window size usually means a trade-off between two parameters: specificity versus the sparse-data problem." ></td>
	<td class="line x" title="40:177	The smaller the window, the stronger the associative relation between the words inside the window, but the more severe the sparse data problem (see figure 1 in section 3.2)." ></td>
	<td class="line x" title="41:177	In our case, with 1 word, the window size looks rather small." ></td>
	<td class="line x" title="42:177	However, this can be justified since we have reduced the effects of the sparse-data problem by using a large corpus and by lemmatizing the corpus." ></td>
	<td class="line x" title="43:177	It also should be noted that a window size of 1 applied after elimination of the function words is comparable to a window size of 2 without elimination of the function words (assuming that roughly every second word is a function word)." ></td>
	<td class="line x" title="44:177	Based on the window size of 1, we computed a co-occurrence matrix of about a million words in the lemmatized BNC." ></td>
	<td class="line x" title="45:177	Although the resulting matrix is extremely large, this was feasible since we used a sparse format that does not store zero entries." ></td>
	<td class="line x" title="46:177	2.4 Computation of Word Similarities To determine the words most similar to a given word, the co-occurrence vector of this word is compared to all other vectors in the matrix and the words are ranked according to the similarity values obtained." ></td>
	<td class="line x" title="47:177	It is expected that the most similar words are ranked first in the sorted list." ></td>
	<td class="line x" title="48:177	For vector comparison, different similarity measures can be considered." ></td>
	<td class="line x" title="49:177	Salton & McGill (1983) proposed a number of measures, such as the cosine coefficient, the Jaccard coefficient, and the Dice coefficient." ></td>
	<td class="line x" title="50:177	For the computation of related terms and synonyms, Ruge (1995) and Landauer & Dumais (1997) used the cosine measure, whereas Grefenstette (1994, p. 48) used a weighted Jaccard measure." ></td>
	<td class="line x" title="51:177	We propose here the city-block metric, which computes the similarity between two vectors X and Y as the sum of the absolute differences of corresponding vector positions:  = = n i ii YXs 1 In a number of experiments we compared it to other similarity measures, such as the cosine measure, the Jaccard measure (standard and binary), the Euclidean distance, and the scalar product, and found that the city-block metric yielded good results (see Rapp, 1999)." ></td>
	<td class="line x" title="52:177	2.5 Results Table 1 shows the top five paradigmatic associations to six stimulus words." ></td>
	<td class="line x" title="53:177	As can be seen from the table, nearly all words listed are of the same part of speech as the stimulus word." ></td>
	<td class="line x" title="54:177	Of course, our definition of the term paradigmatic association as given in the introduction implies this." ></td>
	<td class="line x" title="55:177	However, the simulation system never obtained any information on part of speech, and so it is nevertheless surprising that besides computing term similarities it implicitly seems to be able to cluster parts of speech." ></td>
	<td class="line x" title="56:177	This observation is consistent with other studies (e.g. , Ruge, 1995)." ></td>
	<td class="line x" title="57:177	blue cold fruit green tobacco whiskey red hot food red cigarette whisky green warm flower blue alcohol brandy grey dry fish white coal champagne yellow drink meat yellow import lemonade white cool vegetable grey textile vodka Table 1: Computed paradigmatic associations." ></td>
	<td class="line x" title="58:177	A qualitative inspection of the word lists generated by the system shows that the results are quite satisfactory." ></td>
	<td class="line x" title="59:177	Paradigmatic associations like blue a0 red, cold a0 hot, and tobacco a0 cigarette are intuitively plausible." ></td>
	<td class="line x" title="60:177	However, a quantitative evaluation would be preferable, of course, and for this reason we did a comparison with the results of the human subjects in the TOEFL test." ></td>
	<td class="line x" title="61:177	Remember that the human subjects had to choose the word most similar to a given stimulus word from a list of four alternatives." ></td>
	<td class="line x" title="62:177	In the simulation, we assumed that the system had chosen the correct alternative if the correct word was ranked highest among the four alternatives." ></td>
	<td class="line x" title="63:177	This was the case for 55 of the 80 test items, which gives us an accuracy of 69%." ></td>
	<td class="line x" title="64:177	This accuracy may seem low, but it should be taken into account that the TOEFL tests the language abilities of prospective university students and therefore is rather difficult." ></td>
	<td class="line x" title="65:177	Actually, the performance of the average human test taker was worse than the performance of the system." ></td>
	<td class="line x" title="66:177	The human subjects were only able to solve 51.6 of the test items correctly, which gives an accuracy of 64.5%." ></td>
	<td class="line x" title="67:177	Please note that in the TOEFL, average performance (over several types of tests, with the synonym test being just one of them) admits students to most universities." ></td>
	<td class="line x" title="68:177	On the other hand, by definition, the test takers did not have a native command of English, so the performance of native speakers would be expected to be significantly better." ></td>
	<td class="line x" title="69:177	Another consideration is the fact that our simulation program was not designed to make use of the context of the test word, so it neglected some information that may have been useful for the human subjects." ></td>
	<td class="line x" title="70:177	Nevertheless, the results look encouraging." ></td>
	<td class="line x" title="71:177	Given that our method is rather simple, let us now compare our results to the results obtained with more sophisticated methods." ></td>
	<td class="line x" title="72:177	One of the methods reported in the literature is singular value decomposition (SVD); another is shallow parsing." ></td>
	<td class="line x" title="73:177	SVD, as described by Schtze (1997) and Landauer & Dumais (1997), is a method similar to factor analysis or multi-dimensional scaling that allows a significant reduction of the dimensionality of a matrix with minimum information loss." ></td>
	<td class="line x" title="74:177	Landauer & Dumais (1997) claim that by optimizing the dimensionality of the target matrix the performance of their word similarity predictions was significantly improved." ></td>
	<td class="line x" title="75:177	However, on the TOEFL task mentioned above, after empirically determining the optimal dimensionality of their matrix, they report an accuracy of 64.4%." ></td>
	<td class="line x" title="76:177	This is somewhat worse than our result of 69%, which was achieved without SVD and without optimizing any parameters." ></td>
	<td class="line x" title="77:177	It must be emphasized, however, that the validity of this comparison is questionable, as many parameters of the two models are different, making it unclear which ones are responsible for the difference." ></td>
	<td class="line x" title="78:177	For example, Landauer and Dumais used a smaller corpus (4.7 million words), a larger window size (151 words on average), and a different similarity measure (cosine measure)." ></td>
	<td class="line x" title="79:177	We nevertheless tend to interpret the results of our comparison as evidence for the view that SVD is just another method for smoothing that has its greatest benefits for sparse data." ></td>
	<td class="line x" title="80:177	However, we do not deny the technical value of the method." ></td>
	<td class="line x" title="81:177	The one-time effort of the dimensionality reduction may be well spent in a practical system because all subsequent vector comparisons will be speeded up considerably with shorter vectors." ></td>
	<td class="line x" title="82:177	Let us now compare our results to those obtained using shallow parsing, as previously done by Grefenstette (1993)." ></td>
	<td class="line x" title="83:177	The view here is that the window-based method may work to some extent, but that many of the word co-occurrences in a window are just incidental and add noise to the significant word pairs." ></td>
	<td class="line x" title="84:177	A simple method to reduce this problem could be to introduce a threshold for the minimum number of co-occurrences; a more sophisticated method is the use of a (shallow) parser." ></td>
	<td class="line x" title="85:177	Ruge (1992), who was the first to introduce this method, claims that only head-modifier relations, as known from dependency grammar, should be considered." ></td>
	<td class="line x" title="86:177	For example, if we consider the sentence Peter drives the blue car, then we should not count the co-occurrence of Peter and blue, because blue is neither head nor modifier of Peter." ></td>
	<td class="line x" title="87:177	Ruge developed a shallow parser that is able to determine the headmodifier relations in unrestricted English text with a recall of 85% and a precision of 86% (Ruge, 1995)." ></td>
	<td class="line x" title="88:177	Using this parser she extracted all head-modifier relations from the 100 million words of the British National Corpus." ></td>
	<td class="line x" title="89:177	Thus, the resulting co-occurrence matrix only contained the counts of the head-modifier relations." ></td>
	<td class="line x" title="90:177	The word similarities were computed from this matrix by using the cosine similarity measure." ></td>
	<td class="line x" title="91:177	Using this method, Ruge achieved an accuracy of about 69% in the TOEFL synonym task, which is equivalent to our results." ></td>
	<td class="line x" title="92:177	Again, we need to emphasize that parameters other than the basic methodology could have influenced the result, so we need to be cautious with an interpretation." ></td>
	<td class="line x" title="93:177	However, to us it seems that the view that some of the co-occurrences in corpora should be considered as noise is wrong, or else if there is some noise it obviously cancels out over large corpora." ></td>
	<td class="line x" title="94:177	It would be interesting to know how a system performed that used all co-occurrences except the head-modifier relations." ></td>
	<td class="line x" title="95:177	We tend to assume that such a system would perform worse, so the parser selected the good candidates." ></td>
	<td class="line x" title="96:177	However, the experiment has not been done, so we cannot be sure." ></td>
	<td class="line x" title="97:177	Although the shallow parsing could not improve the results in this case, we nevertheless should point out its virtues: It improves efficiency since it leads to sparser matrices." ></td>
	<td class="line x" title="98:177	It also seems to be able to separate the relevant from the irrelevant co-occurrences." ></td>
	<td class="line x" title="99:177	Third, it may be useful for determining the type of relationship between words (e.g. , synonymy, antonymy, meronymy, hyponymy, etc. , see Berland & Charniak, 1999)." ></td>
	<td class="line x" title="100:177	Although this is not within the scope of this paper, it is very relevant for related tasks, for example, the automatic generation of thesauri." ></td>
	<td class="line x" title="101:177	3 Syntagmatic Associations Syntagmatic associations are words that frequently occur together." ></td>
	<td class="line x" title="102:177	Therefore, an obvious approach to extract them from corpora is to look for word pairs whose co-occurrence is significantly larger than chance." ></td>
	<td class="line x" title="103:177	To test for significance, the standard chisquare test can be used." ></td>
	<td class="line x" title="104:177	However, Dunning (1993) pointed out that for the purpose of corpus statistics, where the sparseness of data is an important issue, it is better to use the log-likelihood ratio." ></td>
	<td class="line x" title="105:177	It would then be assumed that the strongest syntagmatic association to a word would be that other word that gets the highest log-likelihood score." ></td>
	<td class="line x" title="106:177	Please note that this method is computationally far more efficient than the computation of paradigmatic associations." ></td>
	<td class="line x" title="107:177	For the computation of the syntagmatic associations to a stimulus word only the vector of this single word has to be considered, whereas for the computation of paradigmatic associations the vector of the stimulus word has to be compared to the vectors of all other words in the vocabulary." ></td>
	<td class="line x" title="108:177	The computation of syntagmatic associations is said to be of first-order type, whereas the computation of paradigmatic associations is of second-order type." ></td>
	<td class="line oc" title="109:177	Algorithms for the computation of first-order associations have been used in lexicography for the extraction of collocations (Smadja, 1993) and in cognitive psychology for the simulation of associative learning (Wettler & Rapp, 1993)." ></td>
	<td class="line x" title="110:177	3.1 Association Norms As we did with the paradigmatic associations, we would like to compare the results of our simulation to human performance." ></td>
	<td class="line x" title="111:177	However, it is difficult to say what kind of experiment should be conducted to obtain human data." ></td>
	<td class="line x" title="112:177	As with the paradigmatic associations, we decided not to conduct our own experiment but to use the Edinburgh Associative Thesaurus (EAT), a large collection of association norms, as compiled by Kiss et al.(1973)." ></td>
	<td class="line x" title="114:177	Kiss presented lists of stimulus words to human subjects and asked them to write after each word the first word that the stimulus word made them think of." ></td>
	<td class="line x" title="115:177	Table 2 gives some examples of the associations the subjects came up with." ></td>
	<td class="line x" title="116:177	As can be seen from the table, not all of the associations given by the subjects seem to be of syntagmatic type." ></td>
	<td class="line x" title="117:177	For example, the word pairs blue black or cold hot are clearly of paradigmatic type." ></td>
	<td class="line x" title="118:177	This observation is of importance and will be discussed later." ></td>
	<td class="line x" title="119:177	blue cold fruit green tobacco whiskey sky hot apple grass smoke drink black ice juice blue cigarette gin green warm orange red pipe bottle red water salad yellow poach soda white freeze machine field road Scotch Table 2: Some sample associations from the EAT." ></td>
	<td class="line x" title="120:177	3.2 Computation For the computation of the syntagmatic associations we used the same corpus as before, namely the British National Corpus." ></td>
	<td class="line x" title="121:177	In a preliminary experiment we tested if there is a correlation between the occurrence of a stimulus word in the corpus and the occurrence of the most frequent associative response as given by the subjects." ></td>
	<td class="line x" title="122:177	For this purpose, we selected 100 stimulus/response pairs and plotted a bar chart from the co-occurrence data (see figure 1)." ></td>
	<td class="line x" title="123:177	In the bar chart, the x-axis corresponds to the distance of the response word from the stimulus word (measured as the number of words separating them), and the y-axis corresponds to the occurrence frequency of the response word in a particular distance from the stimulus word." ></td>
	<td class="line x" title="124:177	Please note that for the purpose of plotting this bar chart, function words have been taken into account." ></td>
	<td class="line x" title="125:177	Figure 1: Occurrence frequency H of a response word in a particular distance A from the corresponding stimulus word (averaged over 100 stimulus/response pairs)." ></td>
	<td class="line x" title="126:177	As can be seen from the figure, the closer we get to the stimulus word, the more likely it is that we find an occurrence of its strongest associative response." ></td>
	<td class="line x" title="127:177	Exceptions are the positions directly neighboring the stimulus word." ></td>
	<td class="line x" title="128:177	Here it is rather unlikely to find the response word." ></td>
	<td class="line x" title="129:177	This observation can be explained by the fact that content words are most often separated by function words, so that the neighboring positions are occupied by function words." ></td>
	<td class="line x" title="130:177	Now that it has been shown that there is some relationship between human word associations and word co-occurrences, let us briefly introduce our algorithm for extracting word associations from texts." ></td>
	<td class="line x" title="131:177	Based on a window size of 20 words, we first compute the co-occurrence vector for a given stimulus word, thereby eliminating all words with a corpus frequency of less than 101." ></td>
	<td class="line x" title="132:177	We then apply the log-likelihood test to this vector." ></td>
	<td class="line x" title="133:177	According to Lawson & Belica1 the log-likelihood ratio can be computed as follows: Given the word W, for each co-occurring word S, its window frequency A, its residual frequency C in the reference corpus, the residual window size B and the residual corpus size D are stored in a 2 by 2 contingency table." ></td>
	<td class="line x" title="134:177	S S Total W A B A+B W C D C+D Total A+C B+D N Then the log-likelihood statistics are calculated: ))log()()log( )()log()( )log()(log loglogloglog(2 DCDCDB DBCACA BABANN DDCCBBAAG +++ +++ +++ +++= Finally, the vocabulary is ranked according to descending values of G as computed for each word." ></td>
	<td class="line x" title="135:177	The word with the highest value is considered to be the primary associative response." ></td>
	<td class="line x" title="136:177	3.3 Results In table 3 a few sample association lists as predicted by our system are listed." ></td>
	<td class="line x" title="137:177	They can be compared to the human associative responses given in table 2." ></td>
	<td class="line x" title="138:177	The valuation of the predictions has to take into account that association norms are conglomerates of the answers of different subjects that differ considerably from each other." ></td>
	<td class="line x" title="139:177	A satisfactory prediction would be proven if the difference between the pre1 Handout at GLDV Meeting, Frankfurt/Main 1999." ></td>
	<td class="line x" title="140:177	dicted and the observed responses were about equal to the difference between an average subject and the rest of the subjects." ></td>
	<td class="line x" title="141:177	This is actually the case." ></td>
	<td class="line x" title="142:177	For 27 out of the 100 stimulus words the predicted response is equal to the observed primary response." ></td>
	<td class="line x" title="143:177	This compares to an average of 28 primary responses given by a subject in the EAT." ></td>
	<td class="line x" title="144:177	Other evaluation measures lead to similar good results (Wettler & Rapp, 1993; Rapp, 1996)." ></td>
	<td class="line x" title="145:177	blue cold fruit green tobacco whiskey red hot vegetable red advertising drink eyes water juice blue smoke Jesse sky warm fresh yellow ban bottle white weather tree leaves cigarette Irish green winter salad colour alcohol pour Table 3: Results with the co-occurrence-based approach." ></td>
	<td class="line x" title="146:177	We conclude from this that our method seems to be well suited to predict the free word associations as produced by humans." ></td>
	<td class="line x" title="147:177	And as human associations are not only of syntagmatic but also of paradigmatic type, so does the co-occurrence-based method predict both types of associations rather well." ></td>
	<td class="line x" title="148:177	In the ranked lists produced by the system we find a mixture of both types of associations." ></td>
	<td class="line x" title="149:177	However, for a given association there is no indication whether it is of syntagmatic or paradigmatic type." ></td>
	<td class="line x" title="150:177	We suggest a simple method to distinguish the paradigmatic from the syntagmatic associations." ></td>
	<td class="line x" title="151:177	Remember that the 2nd-order approach described in the previous section produced paradigmatic associations only." ></td>
	<td class="line x" title="152:177	So if we simply remove the words produced by the 2nd-order approach from the word lists obtained by the 1st-order approach, then this should give us solely syntagmatic associations." ></td>
	<td class="line x" title="153:177	4 Comparison between Syntagmatic and Paradigmatic Associations Table 4 compares the top five associations to a few stimulus words as produced by the 1st-order and the 2nd-order approach." ></td>
	<td class="line x" title="154:177	In the list, we have printed in bold those 1st-order associations that are not among the top five in the second-order lists." ></td>
	<td class="line x" title="155:177	Further inspections of these words shows that they are all syntagmatic associations." ></td>
	<td class="line x" title="156:177	So the method proposed seems to work in principle." ></td>
	<td class="line x" title="157:177	However, we have not yet conducted a systematic quantitative evaluation." ></td>
	<td class="line x" title="158:177	Conducting a systematic evaluation is not trivial, since the definitions of the terms syntagmatic and paradigmatic as given in the introduction may not be precise enough." ></td>
	<td class="line x" title="159:177	Also, for a high recall, the word lists considered should be much longer than the top five." ></td>
	<td class="line x" title="160:177	However, the further down we go in the ranked lists, the less typical are the associations." ></td>
	<td class="line x" title="161:177	So it is not clear where to automatically set a threshold." ></td>
	<td class="line x" title="162:177	We did not further elaborate on this because for our practical work this issue was of lesser importance." ></td>
	<td class="line x" title="163:177	Although both algorithms are based on word cooccurrences, our impression is that their strengths and weaknesses are rather different." ></td>
	<td class="line x" title="164:177	So we see a good chance of obtaining an improved generator for associations by combining the two methods." ></td>
	<td class="line x" title="165:177	stimulus 1st-order 2nd-order blue red red eyes green sky grey white yellow green white cold hot hot water warm warm dry weather drink winter cool fruit vegetable food juice flower fresh fish tree meat salad vegetable green red red blue blue yellow white leaves yellow colour grey tobacco advertising cigarette smoke alcohol ban coal cigarette import alcohol textile whiskey drink whisky Jesse brandy bottle champagne Irish lemonade pour vodka Table 4: Comparison between 1st-order and 2nd-order associations." ></td>
	<td class="line x" title="166:177	5 Discussion and Conclusion We have described algorithms for the computation of 1st-order and 2nd-order associations." ></td>
	<td class="line x" title="167:177	The results obtained have been compared with the answers of human subjects in the free association task and in the TOEFL synonym test." ></td>
	<td class="line x" title="168:177	It could be shown that the performance of our system is comparable to the performance of the subjects for both tasks." ></td>
	<td class="line x" title="169:177	We observed that there seems to be some relationship between the type of computation performed (1st-order versus 2nd-order) and the terms syntagmatic and paradigmatic as coined by de Saussure." ></td>
	<td class="line x" title="170:177	Whereas the results of the 2nd-order computation are of paradigmatic type exclusively, those of the 1st-order computation are a mixture of both syntagmatic and paradigmatic associations." ></td>
	<td class="line x" title="171:177	Removing the 2nd-order associations from the 1st-order associations leads to solely syntagmatic associations." ></td>
	<td class="line x" title="172:177	We believe that the observed relation between our statistical models and the intuitions of de Saussure are not incidental, and that the striking similarity of the simulation results with the human associations also has a deeper reason." ></td>
	<td class="line x" title="173:177	Our explanation for this is that human associative behavior is governed by the law of association by contiguity, which is well known from psychology (Wettler, Rapp & Ferber, 1993)." ></td>
	<td class="line x" title="174:177	In essence, this means that in the process of learning or generating associations the human mind seems to conduct operations that are equivalent to co-occurrence counting, to performing significance tests, or to computing vector similarities (see also Landauer & Dumais, 1997)." ></td>
	<td class="line x" title="175:177	However, further work is required to find out to what extent other language-related tasks can also be explained statistically." ></td>
	<td class="line x" title="176:177	Acknowledgements This research was supported by the DFG." ></td>
	<td class="line x" title="177:177	I would like to thank Manfred Wettler and Gerda Ruge for many inspiring discussions." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C02-2003
Searching The Web By Voice
Franz, Alexander M.;Milch, Brian;"></td>
	<td class="line x" title="1:138	Searching the Web by Voice Alexander FRANZ Google Inc. 2400 Bayshore Parkway Mountain View, CA 94043 alex@google.com Brian MILCH Computer Science Division University of California at Berkeley Berkeley, CA 94720 milch@cs.berkeley.edu Abstract Spoken queries are a natural medium for searching the Web in settings where typing on a keyboard is not practical." ></td>
	<td class="line x" title="2:138	This paper describes a speech interface to the Google search engine." ></td>
	<td class="line x" title="3:138	We present experiments with various statistical language models, concluding that a unigram model with collocations provides the best combination of broad coverage, predictive power, and real-time performance." ></td>
	<td class="line x" title="4:138	We also report accuracy results of the prototype system." ></td>
	<td class="line x" title="5:138	1 Introduction Web search has a number of properties that make it a particularly difficult speech recognition problem." ></td>
	<td class="line x" title="6:138	First, most queries are very short: typical queries range between one and five or six words, with a median length of two words." ></td>
	<td class="line x" title="7:138	Second, search engine queries use a very large vocabulary." ></td>
	<td class="line x" title="8:138	Even a vocabulary of 100,000 words covers only about 80% of the query traffic." ></td>
	<td class="line x" title="9:138	Third, recognition must be done in close to real time." ></td>
	<td class="line x" title="10:138	By contrast, the systems that achieved good accuracy on the 2000 NIST conversational telephone speech task required from 250 to 820 times real time (Fiscus et al. , 2000)." ></td>
	<td class="line x" title="11:138	In this paper, we describe the language modeling techniques that we used to address these problems in creating a prototype voice search system (setting aside the question of how to browse the search results)." ></td>
	<td class="line x" title="12:138	2 Trade-Offs in Language Modeling A speech recognition system uses a language model to determine the probability of different recognition hypotheses." ></td>
	<td class="line x" title="13:138	For our application, there is a trade-off among three considerations: What fraction of the query traffic is covered by the vocabulary of the language model?" ></td>
	<td class="line x" title="14:138	How much predictive power does the language model provide?" ></td>
	<td class="line x" title="15:138	And what is the observed computational complexity of applying the language model during hypothesis search?" ></td>
	<td class="line x" title="16:138	At one extreme, a language model that simply used a list of the most frequent queries in their entirety would have the lowest coverage, but would provide the best predictive power within the covered queries (have the lowest per-query perplexity), and would be the least computationally expensive." ></td>
	<td class="line x" title="17:138	At the other extreme, (Lau, 1998; Ng, 2000) report on experiments with sub-word n-gram language models, which have very high coverage, but rather low predictive power (high per-query perplexity)." ></td>
	<td class="line x" title="18:138	We experimented with various configurations of back-off word n-gram models (Katz, 1987; Jelinek, 1997)." ></td>
	<td class="line x" title="19:138	In our experience with commercially available speech recognition systems, we found that for a vocabulary size of 100,000 items, unigram models were the only computationally feasible choice, yielding close to real-time performance." ></td>
	<td class="line x" title="20:138	When using the bigram model, the recognizer needed to spend several minutes processing each utterance to achieve accuracy as high as it achieved with the unigram model." ></td>
	<td class="line x" title="21:138	Recognition with a bigram model was unacceptably slow even when we pruned the model by removing bigrams that provided little improvement in perplexity (Stolcke, 1998)." ></td>
	<td class="line x" title="22:138	For this reason, we explored a method to increase the predictive power of the unigram model by adding collocations to its vocabulary." ></td>
	<td class="line x" title="23:138	3 Collocations A collocation is an expression of two or more words that corresponds to some conventional way of saying things (Manning and Schutze, 1999)." ></td>
	<td class="line oc" title="24:138	Sometimes, the notion of collocation is defined in terms of syntax (by possible part-of-speech patterns) or in terms of semantics (requiring collocations to exhibit non-compositional meaning) (Smadja, 1993)." ></td>
	<td class="line x" title="25:138	We adopt an empirical approach and consider any sequence of words that cooccurs more often than chance a potential collocation." ></td>
	<td class="line x" title="26:138	3.1 The Likelihood Ratio We adopted a method for collocation discovery based on the likelihood ratio (Dunning, 1993)." ></td>
	<td class="line x" title="27:138	Suppose we wish to test whether two words DB BD DB BE form a collocation." ></td>
	<td class="line x" title="28:138	Under the independence hypothesis we assume that the probability of observing the second word DB BE is independent of the first word: C8B4DB BE CYDB BD B5 BP C8B4DB BE CYBMDB BD B5." ></td>
	<td class="line x" title="29:138	The alternative is that the two words form a collocation: C8B4DB BE CYDB BD B5 BQ C8B4DB BE CYBMDB BD B5." ></td>
	<td class="line x" title="30:138	The likelihood ratio AL is calculated by dividing the likelihood of observing the data under the hypothesis of independence, C4B4C0 CX B5, by the likelihood of observing the data under the hypothesis that the words form a collocation, C4B4C0 CR B5: AL BP C4B4C0 CX B5 C4B4C0 CR B5 After counting how many times the word DB BE and the sequence DB BD DB BE occur in training data, we derive maximum likelihood estimates for C8B4DB BE CYDB BD B5 and PB4DB BE CYBMDB BD B5, and compute the two likelihoods using the binomial distribution (see (Manning and Schutze, 1999) for details)." ></td>
	<td class="line x" title="31:138	If the likelihood ratio is small, then C0 CR explains the data much better than C0 CX, and so the word sequence is likely to be a collocation." ></td>
	<td class="line x" title="32:138	3.2 Discovering Longer Collocations Two-word collocations can be discovered by carrying out the calculations described above for all frequent two-word sequences, ranking the sequences according to their likelihood ratios, and selecting all sequences with ratios below a threshold." ></td>
	<td class="line x" title="33:138	Collocations are not limited to two words, however." ></td>
	<td class="line x" title="34:138	We have extended Dunnings scheme to discover longer collocations by performing the likelihood ratio tests iteratively." ></td>
	<td class="line x" title="35:138	The algorithm for this is shown below." ></td>
	<td class="line x" title="36:138	1." ></td>
	<td class="line x" title="37:138	Count occurrences of sequences of tokens (initially, words) for lengths of up to D2 tokens." ></td>
	<td class="line x" title="38:138	2." ></td>
	<td class="line x" title="39:138	For each sequence CB BP DB BD BNBMBMBMBNDB D2 of D2 tokens in the training data, let ALB4CBB5 be the greatest likelihood ratio found by considering all possible ways to split the D2-token sequence into two contiguous parts." ></td>
	<td class="line x" title="40:138	3." ></td>
	<td class="line x" title="41:138	Sort the D2-token sequences CB by ALB4CBB5, and designate the C3 D2 sequences with the lowest ALB4CBB5 values as collocations." ></td>
	<td class="line x" title="42:138	4." ></td>
	<td class="line x" title="43:138	Re-tokenize the data by treating each collocation as a single token." ></td>
	<td class="line x" title="44:138	5." ></td>
	<td class="line x" title="45:138	Set D2 BP D2A0BD." ></td>
	<td class="line x" title="46:138	6." ></td>
	<td class="line x" title="47:138	Repeat through D2 BPBE." ></td>
	<td class="line x" title="48:138	The constants C3 D2, which represent the number of desired collocations of length D2, are chosen manually." ></td>
	<td class="line x" title="49:138	This algorithm solves two key problems in discovering longer collocations." ></td>
	<td class="line x" title="50:138	The first problem concerns long word sequences that include shorter collocations." ></td>
	<td class="line x" title="51:138	For example, consider the sequence New York flowers: this sequence does indeed occur together more often than chance, but if we identify New York as a collocation then including New York flowers as an additional collocation provides little additional benefit (as measured by the reduction in per-query perplexity)." ></td>
	<td class="line x" title="52:138	To solve this problem, step 2 in the collocation discovery algorithm considers all D2 A0 BD possible ways to divide a potential collocation of length D2 into two parts." ></td>
	<td class="line x" title="53:138	For the case of New York flowers, this means considering the combinations New York + flowers and New + York flowers." ></td>
	<td class="line x" title="54:138	The likelihood ratio used to decide whether the word sequence should be considered a collocation is the maximum of the ratios for all possible splits." ></td>
	<td class="line x" title="55:138	Since flowers is close to independent from New York, the potential collocation is rejected." ></td>
	<td class="line x" title="56:138	The second problem concerns subsequences of long collocations." ></td>
	<td class="line x" title="57:138	For example, consider the collocation New York City." ></td>
	<td class="line x" title="58:138	New York is a collocation in its own right, but York City is not." ></td>
	<td class="line x" title="59:138	To distinguish between these two cases, we need to note that York City occurs more often than chance, but usually as part of the larger collocation New York City, while New York occurs more often than chance outside the larger collocation as well." ></td>
	<td class="line x" title="60:138	The solution to this problem is to find larger collocations first, and to re-tokenize the data to treat collocations as a single token (step 4 above)." ></td>
	<td class="line x" title="61:138	In this way, after New York City is identified as a collocation, all instances of it are treated as a single token, and do not contribute to the counts for New York or York City." ></td>
	<td class="line x" title="62:138	Since New York occurs outside the larger collocation, it is still correctly identified as a collocation, but York City drops out." ></td>
	<td class="line x" title="63:138	4 Implementing Voice Search 4.1 Training and Test Data To create the various language models for the voice search system, we used training data consisting of 19.8 million query occurrences, with 12.6 million distinct queries." ></td>
	<td class="line x" title="64:138	There were 54.9 million word occurrences, and 3.4 million distinct words." ></td>
	<td class="line x" title="65:138	The evaluation data consisted of 2.5 million query occurrences, with 1.9 million distinct queries." ></td>
	<td class="line x" title="66:138	It included 7.1 million word occurrences, corresponding to 750,000 distinct words." ></td>
	<td class="line x" title="67:138	We used a vocabulary of 100,000 items (depending on the model, the vocabulary included words only, or words and collocations)." ></td>
	<td class="line x" title="68:138	The word with the lowest frequency occurred 31 times." ></td>
	<td class="line x" title="69:138	4.2 Constructing the Language Model The procedure for constructing the language model was as follows: 1." ></td>
	<td class="line x" title="70:138	Obtain queries by extracting a sample from Googles query logs." ></td>
	<td class="line x" title="71:138	2." ></td>
	<td class="line x" title="72:138	Filter out non-English queries by discarding queries that were made from abroad, requested result sets in foreign languages, etc. 3." ></td>
	<td class="line x" title="73:138	Use Googles spelling correction mechanism to correct misspelled queries." ></td>
	<td class="line x" title="74:138	4." ></td>
	<td class="line x" title="75:138	Create lists of collocations as described in Section 3 above." ></td>
	<td class="line x" title="76:138	5." ></td>
	<td class="line x" title="77:138	Create the vocabulary consisting of the most frequent words and collocations." ></td>
	<td class="line x" title="78:138	6." ></td>
	<td class="line x" title="79:138	Use a dictionary and an automatic text-tophonemes tool to obtain phonetic transcriptions for the vocabulary, applying a separate algorithm to special terms (such as acronyms, numerals, URLs, and filenames)." ></td>
	<td class="line x" title="80:138	7." ></td>
	<td class="line x" title="81:138	Estimate n-gram probabilities to create the language model." ></td>
	<td class="line x" title="82:138	4.3 System Architecture Figure 1 presents an overview of the voice search system." ></td>
	<td class="line x" title="83:138	The left-hand side of the diagram represents the off-line steps of creating the statistical language model." ></td>
	<td class="line x" title="84:138	The language model is used with a commercially available speech recognition engine, which supplies the acoustic models and the decoder." ></td>
	<td class="line x" title="85:138	The right-hand side of the diagram represents the run-time flow of a voice query." ></td>
	<td class="line x" title="86:138	The speech recognition engine returns a list of the n-best recognition hypotheses." ></td>
	<td class="line x" title="87:138	A disjunctive query is derived from this n-best list, and the query is issued to the Google search engine." ></td>
	<td class="line x" title="88:138	5 Coverage and Perplexity Results We evaluated the coverage and perplexity of different language models." ></td>
	<td class="line x" title="89:138	In our experiments, we varied the language models along two dimensions: Spelling Correction Filtering and Discovery Collocation Modeling Pronunciation Construction Vocabulary Query Logs Language Model Statistical Acoustic Models Hypothesis List NBest Voice Query Construction Query Google Search Results Voice Search Speech Recognition Engine Figure 1: Voice Search Architecture Context." ></td>
	<td class="line x" title="90:138	We evaluated unigram, bigram, and trigram language models to see the effect of taking more context into account." ></td>
	<td class="line x" title="91:138	Collocations." ></td>
	<td class="line x" title="92:138	We evaluated language models whose vocabulary included only the 100,000 most frequent words, as well as models whose vocabulary included the most frequent words and collocations." ></td>
	<td class="line x" title="93:138	Specifically, we ran the algorithm in Section 3.2 to obtain 5000 three-word collocations, and then 20,000 two-token collocations (which could contain two, four, or six words)." ></td>
	<td class="line x" title="94:138	To obtain the final vocabulary of 100,000 words and collocations, we tokenized the training corpus using a vocabulary with all 25,000 collocations, and then selected the 100,000 most frequent tokens." ></td>
	<td class="line x" title="95:138	Most of the collocations were included in the final vocabulary." ></td>
	<td class="line x" title="96:138	5.1 Query Coverage We say that a vocabulary covers a query when all words (and collocations, if applicable) in the query are in the vocabulary." ></td>
	<td class="line x" title="97:138	Table 1 summarizes the coverage of different-sized vocabularies composed of words, words + collocations, or entire queries." ></td>
	<td class="line x" title="98:138	Words Collocations Queries 25k 62.2% 50.0% 12.4% 50k 72.2% 65.2% 15.3% 75k 76.7% 72.8% 17.1% 100k 79.2% 76.9% 18.4% 200k 83.9% 83.2% 21.5% 300k 85.9% 85.5% 23.2% 400k 87.1% 86.8% 24.3% 500k 87.9% 87.7% 25.2% Table 1: Percent of Query Occurrences Covered At a vocabulary size of 100,000 items, there is only a difference of 2.7% between an all-word vocabulary, and a vocabulary that includes words and collocations." ></td>
	<td class="line x" title="99:138	Thus, using collocations does not result in a large loss of coverage." ></td>
	<td class="line x" title="100:138	5.2 Perplexity Results We compared the perplexity of different models with a 100,000 item vocabulary in two ways: by measuring the per-token perplexity, and by measuring the per-query perplexity." ></td>
	<td class="line x" title="101:138	Per-token perplexity measures how well the language model is able to predict the next word (or collocation), while perquery perplexity measures the contribution of the language model to recognizing the entire query." ></td>
	<td class="line x" title="102:138	To avoid complications related to out-of-vocabulary words, we computed perplexity only on queries covered by the vocabulary (79.2% of the test queries for the all-word vocabulary, and 76.9% for words plus collocations)." ></td>
	<td class="line x" title="103:138	The results are shown in Table 2." ></td>
	<td class="line x" title="104:138	Model Per-token Per-query Word unigram 1614 BFBMBH A2BDBC BDBE Word bigram 409 BDBMBI A2BDBC BDBC Word trigram 340 BJBMBL A2BDBC BL Collocation unigram 2019 BEBMBH A2BDBC BDBD Collocation bigram 763 BKBMBK A2BDBC BL Collocation trigram 696 BIBMBG A2BDBC BL Table 2: Language Model Perplexity These results show that there is a large decrease in perplexity from the unigram model to the bigram model, but there is a much smaller decrease in perplexity in moving to a trigram model." ></td>
	<td class="line x" title="105:138	Furthermore, the per-token perplexity of the unigram model with collocations is about 25% higher than that of the word-based unigram model." ></td>
	<td class="line x" title="106:138	This shows that the distribution of the word plus collocation vocabulary is more random than the distribution of words alone." ></td>
	<td class="line x" title="107:138	The bigram and trigram models exhibit the same effect." ></td>
	<td class="line x" title="108:138	5.3 Per-Query Perplexity Per-query perplexity shows the gains from including collocations in the vocabulary." ></td>
	<td class="line x" title="109:138	Using collocations means that the average number of tokens (words or collocations) per query decreases, which leads to less uncertainty per query, making recognition of entire queries significantly easier." ></td>
	<td class="line x" title="110:138	For the unigram model, collocations lead to a reduction of perquery perplexity by a factor of 14." ></td>
	<td class="line x" title="111:138	We can see that the per-query perplexity of the unigram model with collocations is about halfway between the wordbased unigram and bigram models." ></td>
	<td class="line x" title="112:138	In other words, collocations seem to give us about half the effect of word bigrams." ></td>
	<td class="line x" title="113:138	Similarly, the per-query perplexity of the bigram model with collocations is very close to the perplexity of the word-based trigram model." ></td>
	<td class="line x" title="114:138	Furthermore, moving from a collocation bigram model to a collocation trigram model only yields a small additional per-query perplexity decrease." ></td>
	<td class="line x" title="115:138	6 Recall Evaluation We also evaluated the recall of the voice search system using audio recordings that we collected for this purpose." ></td>
	<td class="line x" title="116:138	Since only unigram models yielded close to real-time performance for the speech recognizer, we limited our attention to comparing unigram models with a vocabulary size of 100,000 items consisting of either words, or words and collocations." ></td>
	<td class="line x" title="117:138	With these unigram models, the recognizer took only 1-2 seconds to process each query." ></td>
	<td class="line x" title="118:138	6.1 Data Collection We collected voice query data using a prototype of the voice search system connected to the phone network." ></td>
	<td class="line x" title="119:138	In total, 18 speakers made 809 voice queries." ></td>
	<td class="line x" title="120:138	The collected raw samples exhibited a variety of problems, such as low volume, loud breath sounds, clicks, distortions, dropouts, initial cut-off, static, hiccups, and other noises." ></td>
	<td class="line x" title="121:138	We set aside all samples with insurmountable problems and speakers with very strong accents." ></td>
	<td class="line x" title="122:138	This left 581 good samples." ></td>
	<td class="line x" title="123:138	These good samples include a variety of speakers, various brands of cell phones as well as desktop phones, and different cell phone carriers." ></td>
	<td class="line x" title="124:138	The average length of the utterances was 2.1 words." ></td>
	<td class="line x" title="125:138	6.2 Recall Results We used the 581 good audio samples from the data collection to evaluate recognition recall, for which we adopted a strict definition: disregarding singular/plural variations of nouns, did the recognizer return the exact transcription of the audio sample as one of the top D2 (1, 5, 10) hypotheses?" ></td>
	<td class="line x" title="126:138	Note that this recall metric incorporates coverage as well as accuracy: if a query contains a word not in the vocabulary, the recognizer cannot possibly recognize it correctly." ></td>
	<td class="line x" title="127:138	The results are shown in Table 3." ></td>
	<td class="line x" title="128:138	Recall Words only Words + Collocations @1 27.5% 43.4% @5 42.3% 56.8% @10 45.8% 60.4% Table 3: Recall Results on 581 Queries These results show that adding collocations to the recognition vocabulary leads to a recall improvement of 14-16 percentage points." ></td>
	<td class="line x" title="129:138	7 Conclusion We have shown that a commercial speech recognition engine, using a unigram language model over words and collocations, can return the correct transcription of a spoken search query among its top 10 hypotheses about 60% of the time." ></td>
	<td class="line x" title="130:138	Because we were not able to use a bigram model without sacrificing real-time performance, including collocations in the language model was crucial for attaining this level of recall." ></td>
	<td class="line x" title="131:138	Still, there is a lot of room for improvement in the recall rate." ></td>
	<td class="line x" title="132:138	One idea is to rescore the recognizers top hypotheses with a bigram or trigram language model in a postprocessing step." ></td>
	<td class="line x" title="133:138	However, there are many cases where the correct transcription is not among the recognizers top 100 hypotheses." ></td>
	<td class="line x" title="134:138	Another approach would be to adapt the acoustic and language models to individual users, but such personalization would require a different system architecture." ></td>
	<td class="line x" title="135:138	We might also improve our language models by training on documents as well as queries (Fujii, 2001)." ></td>
	<td class="line x" title="136:138	The language models described in this paper were trained from typed queries, but queries made by voice in different settings might have quite different characteristics." ></td>
	<td class="line x" title="137:138	For example, our data consisted of keyword queries, but voice search users might prefer to ask questions or make other types of natural language queries (which would actually be easier to model and recognize)." ></td>
	<td class="line x" title="138:138	The voice search system is currently available at labs.google.com; the data from this demonstration system could lead to improved language models in the future." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W02-0909
Acquiring Collocations For Lexical Choice Between Near-Synonyms
Inkpen, Diana Zaiu;Hirst, Graeme;"></td>
	<td class="line x" title="1:310	Acquiring Collocations for Lexical Choice between Near-Synonyms Diana Zaiu Inkpen and Graeme Hirst Department of Computer Science University of Toronto fdianaz,ghg@cs.toronto.edu Abstract We extend a lexical knowledge-base of near-synonym differences with knowledge about their collocational behaviour." ></td>
	<td class="line x" title="2:310	This type of knowledge is useful in the process of lexical choice between nearsynonyms." ></td>
	<td class="line x" title="3:310	We acquire collocations for the near-synonyms of interest from a corpus (only collocations with the appropriate sense and part-of-speech)." ></td>
	<td class="line x" title="4:310	For each word that collocates with a near-synonym we use a differential test to learn whether the word forms a less-preferred collocation or an anti-collocation with other near-synonyms in the same cluster." ></td>
	<td class="line x" title="5:310	For this task we use a much larger corpus (the Web)." ></td>
	<td class="line x" title="6:310	We also look at associations (longer-distance co-occurrences) as a possible source of learning more about nuances that the near-synonyms may carry." ></td>
	<td class="line x" title="7:310	1 Introduction Edmonds and Hirst (2002 to appear) developed a lexical choice process for natural language generation (NLG) or machine translation (MT) that can decide which near-synonyms are most appropriate in a particular situation." ></td>
	<td class="line x" title="8:310	The lexical choice process has to choose between clusters of near-synonyms (to convey the basic meaning), and then to choose between the near-synonyms in each cluster." ></td>
	<td class="line x" title="9:310	To group near-synonyms in clusters we trust lexicographers judgment in dictionaries of synonym differences." ></td>
	<td class="line x" title="10:310	For example task, job, duty, assignment, chore, stint, hitch all refer to a one-time piece of work, but which one to choose depends on the duration of the work, the commitment and the effort involved, etc. In order to convey desired nuances of meaning and to avoid unwanted implications, knowledge about the differences among near-synonyms is necessary." ></td>
	<td class="line x" title="11:310	I-Saurus, a prototype implementation of (Edmonds and Hirst, 2002 to appear), uses a small number of hand-built clusters of near-synonyms." ></td>
	<td class="line x" title="12:310	Our goal is to automatically acquire knowledge about distinctions among near-synonyms from a dictionary of synonym differences and from other sources such as free text, in order to build a new lexical resource, which can be used in lexical choice." ></td>
	<td class="line x" title="13:310	Preliminary results on automatically acquiring a lexical knowledge-base of near-synonym differences were presented in (Inkpen and Hirst, 2001)." ></td>
	<td class="line x" title="14:310	We acquired denotational (implications, suggestions, denotations), attitudinal (favorable, neutral, or pejorative), and stylistic distinctions from Choose the Right Word (Hayakawa, 1994) (hereafter CTRW)1." ></td>
	<td class="line x" title="15:310	We used an unsupervised decision-list algorithm to learn all the words used to express distinctions and then applied information extraction techniques." ></td>
	<td class="line x" title="16:310	Another type of knowledge that can help in the process of choosing between near-synonyms is collocational behaviour, because one must not choose a near-synonym that does not collocate well with the other word choices for the sentence." ></td>
	<td class="line x" title="17:310	I-Saurus does not include such knowledge." ></td>
	<td class="line x" title="18:310	The focus of the work we present in this paper is to add knowledge about collocational behaviour to our lexical knowledge-base of near-synonym differences." ></td>
	<td class="line x" title="19:310	The lexical choice process implemented in I-Saurus gen1We are grateful to HarperCollins Publishers, Inc. for permission to use CTRW in this project." ></td>
	<td class="line x" title="20:310	July 2002, pp." ></td>
	<td class="line x" title="21:310	67-76." ></td>
	<td class="line x" title="22:310	Association for Computational Linguistics." ></td>
	<td class="line x" title="23:310	ACL Special Interest Group on the Lexicon (SIGLEX), Philadelphia, Unsupervised Lexical Acquisition: Proceedings of the Workshop of the erates all the possible sentences with a given meaning, and ranks them according to the degree to which they satisfy a set of preferences given as input (these are the denotational, attitudinal, and stylistic nuances mentioned above)." ></td>
	<td class="line x" title="24:310	We can refine the ranking so that it favors good collocations, and penalizes sentences containing words that do not collocate well." ></td>
	<td class="line x" title="25:310	We acquire collocates of all near-synonyms in CTRW from free text." ></td>
	<td class="line x" title="26:310	We combine several statistical measures, unlike other researchers who rely on only one measure to rank collocations." ></td>
	<td class="line x" title="27:310	Then we acquire knowledge about less-preferred collocations and anti-collocations2." ></td>
	<td class="line x" title="28:310	For example daunting task is a preferred collocation, while daunting job is less preferred (it should not be used in lexical choice unless there is no better alternative), and daunting duty is an anti-collocation (it must not be used in lexical choice)." ></td>
	<td class="line x" title="29:310	Like Church et al.(1991), we use the t-test and mutual information." ></td>
	<td class="line x" title="30:310	Unlike them we use the Web as a corpus for this task, we distinguish three different types of collocations, and we apply sense disambiguation to collocations." ></td>
	<td class="line x" title="31:310	Collocations are defined in different ways by different researchers." ></td>
	<td class="line x" title="32:310	For us collocations consist of consecutive words that appear together much more often than by chance." ></td>
	<td class="line x" title="33:310	We also include words separated by a few non-content words (short-distance co-occurrence in the same sentence)." ></td>
	<td class="line x" title="34:310	We are interested in collocations to be used in lexical choice." ></td>
	<td class="line x" title="35:310	Therefore we need to extract lexical collocations (between open-class words), not grammatical collocations (which could contain closedclass words, for example put on)." ></td>
	<td class="line x" title="36:310	For now, we consider only two-word fixed collocations." ></td>
	<td class="line x" title="37:310	In future work we will consider longer and more flexible collocations." ></td>
	<td class="line x" title="38:310	We are also interested in acquiring words that strongly associate with our near-synonyms, especially words that associate with only one of the nearsynonyms in the cluster." ></td>
	<td class="line x" title="39:310	Using these strong associations, we plan to learn about nuances of nearsynonyms in order to validate and extend our lexical knowledge-base of near-synonym differences." ></td>
	<td class="line x" title="40:310	In our first experiment, described in sections 2 and 3 (with results in section 4, and evaluation in 2This term was introduced by Pearce (2001)." ></td>
	<td class="line x" title="41:310	section 5), we acquire knowledge about the collocational behaviour of the near-synonyms." ></td>
	<td class="line x" title="42:310	In step 1 (section 2), we acquire potential collocations from the British National Corpus (BNC)3, combining several measures." ></td>
	<td class="line x" title="43:310	In section 3 we present: (step2) select collocations for the near-synonyms in CTRW; (step 3) filter out wrongly selected collocations using mutual information on the Web; (step 4) for each cluster we compose new collocations by combining the collocate of one near-synonym with the the other near-synonym, and we apply the differential ttest to classify them into preferred collocations, lesspreferred collocations, and anti-collocations." ></td>
	<td class="line x" title="44:310	Section 6 sketches our second experiment, involving word associations." ></td>
	<td class="line x" title="45:310	The last two sections present related work, and conclusions and future work." ></td>
	<td class="line x" title="46:310	2 Extracting collocations from free text For the first experiment we acquired collocations for near-synonyms from a corpus." ></td>
	<td class="line x" title="47:310	We experimented with 100 million words from the Wall Street Journal (WSJ)." ></td>
	<td class="line x" title="48:310	Some of our near-synonyms appear very few times (10.64% appear fewer than 5 times) and 6.87% of them do not appear at all in WSJ (due to its business domain)." ></td>
	<td class="line x" title="49:310	Therefore we need a more general corpus." ></td>
	<td class="line x" title="50:310	We used the 100 million word BNC." ></td>
	<td class="line x" title="51:310	Only 2.61% of our near-synonyms do not occur; and only 2.63% occur between 1 and 5 times." ></td>
	<td class="line x" title="52:310	Many of the near-synonyms appear in more than one cluster, with different parts-of-speech." ></td>
	<td class="line x" title="53:310	We experimented on extracting collocations from raw text, but we decided to use a part-of-speech tagged corpus because we need to extract only collocations relevant for each cluster of near-synonyms." ></td>
	<td class="line x" title="54:310	The BNC is a good choice of corpus for us because it has been tagged (automatically by the CLAWS tagger)." ></td>
	<td class="line x" title="55:310	We preprocessed the BNC by removing all words tagged as closed-class." ></td>
	<td class="line x" title="56:310	To reduce computation time, we also removed words that are not useful for our purposes, such as proper names (tagged NP0)." ></td>
	<td class="line x" title="57:310	If we keep the proper names, they are likely to be among the highest-ranked collocations." ></td>
	<td class="line x" title="58:310	There are many statistical methods that can be used to identify collocations." ></td>
	<td class="line x" title="59:310	Four general methods are presented by Manning and Schutze (1999)." ></td>
	<td class="line x" title="60:310	The first one, based on frequency of co-occurrence, 3http://www.hcu.ox.ac.uk/BNC/ does not consider the length of the corpus." ></td>
	<td class="line x" title="61:310	Part-ofspeech filtering is needed to obtain useful collocations." ></td>
	<td class="line oc" title="62:310	The second method considers the means and variance of the distance between two words, and can compute flexible collocations (Smadja, 1993)." ></td>
	<td class="line x" title="63:310	The third method is hypothesis testing, which uses statistical tests to decide if the words occur together with probability higher than chance (it tests whether we can reject the null hypothesis that the two words occurred together by chance)." ></td>
	<td class="line x" title="64:310	The fourth method is (pointwise) mutual information, an informationtheoretical measure." ></td>
	<td class="line x" title="65:310	We use Ted Pedersens Bigram Statistics Package4." ></td>
	<td class="line x" title="66:310	BSP is a suite of programs to aid in analyzing bigrams in a corpus (newer versions allow Ngrams)." ></td>
	<td class="line x" title="67:310	The package can compute bigram frequencies and various statistics to measure the degree of association between two words: mutual information (MI), Dice, chi-square (2), log-likelihood (LL), and Fishers exact test." ></td>
	<td class="line x" title="68:310	The BSP tools count for each bigram in a corpus how many times it occurs, and how many times the first word occurs." ></td>
	<td class="line x" title="69:310	We briefly describe the methods we use in our experiments, for the two-word case." ></td>
	<td class="line x" title="70:310	Each bigram xy can be viewed as having two features represented by the binary variables X and Y. The joint frequency distribution of X and Y is described in a contingency table." ></td>
	<td class="line x" title="71:310	Table 1 shows an example for the bigram daunting task." ></td>
	<td class="line x" title="72:310	n11 is the number of times the bigram xy occurs; n12 is the number of times x occurs in bigrams at the left of words other than y; n21 is the number of times y occurs in bigrams after words other that x; and n22 is the number of bigrams containing neither x nor y. In Table 1 the variable X denotes the presence or absence of daunting in the first position of a bigram, and Y denotes the presence or absence of task in the second position of a bigram." ></td>
	<td class="line x" title="73:310	The marginal distributions of X and Y are the row and column totals obtained by summing the joint frequencies: n+1 = n11 + n21, n1+ = n11 + n12, and n++ is the total number of bigrams." ></td>
	<td class="line x" title="74:310	The BSP tool counts for each bigram in a corpus how many times it occurs, how many times the first word occurs at the left of any bigram (n+1), and how many times the second words occurs at the right of 4http://www.d.umn.edu/ tpederse/code.html y :y x n11 = 66 n12 = 54 n1+ = 120 :x n21 = 4628 n22 = 15808937 n2+ = 15813565 n+1 = 4694 n+2 = 15808991 n++ = 15813685 Table 1: Contingency table for daunting task (x = daunting, y = task)." ></td>
	<td class="line x" title="75:310	any bigram (n1+)." ></td>
	<td class="line x" title="76:310	Mutual information, I(x;y), compares the probability of observing words x and word y together (the joint probability) with the probabilities of observing x and y independently (the probability of occurring together by chance) (Church and Hanks, 1991)." ></td>
	<td class="line x" title="77:310	I(x;y) = log2 P(x;y)P(x)P(y) The probabilities can be approximated by: P(x) = n+1=n++, P(y) = n1+=n++, P(x;y) = n11=n++." ></td>
	<td class="line x" title="78:310	Therefore: I(x;y) = log2 n++n11n +1n1+ The Dice coefficient is related to mutual information and it is calculated as: Dice(x;y) = 2P(x;y)P(x)+ P(y) = 2n11n +1 + n1+ The next methods fall under hypothesis testing methods." ></td>
	<td class="line x" title="79:310	Pearsons Chi-square and Loglikelihood ratios measure the divergence of observed (ni j) and expected (mi j) sample counts (i = 1;2, j = 1;2)." ></td>
	<td class="line x" title="80:310	The expected values are for the model that assumes independence (assumes that the null hypothesis is true)." ></td>
	<td class="line x" title="81:310	For each cell in the contingency table, the expected counts are: mi j = ni+n+ jn++ . The measures are calculated as (Pedersen, 1996): 2 = i;j (ni j mi j) 2 mi j LL = 2 i;j log2 n 2i j mi j Log-likelihood ratios (Dunning, 1993) are more appropriate for sparse data than chi-square." ></td>
	<td class="line x" title="82:310	Fishers exact test is a significance test that is considered to be more appropriate for sparse and skewed samples of data than statistics such as the log-likelihood ratio or Pearsons Chi-Square test (Pedersen, 1996)." ></td>
	<td class="line x" title="83:310	Fishers exact test is computed by fixing the marginal totals of a contingency table and then determining the probability of each of the possible tables that could result in those marginal totals." ></td>
	<td class="line x" title="84:310	Therefore it is computationally expensive." ></td>
	<td class="line x" title="85:310	The formula is: P = n1+!n2+!n+1!n+2!n ++!n11!n12!n21!n22!" ></td>
	<td class="line x" title="86:310	Because these five measures rank collocations in different ways (as the results in the Appendix will show), and have different advantages and drawbacks, we decided to combine them in choosing collocations." ></td>
	<td class="line x" title="87:310	We choose as potential collocations for each near-synonym a collocation that is selected by at least two of the measures." ></td>
	<td class="line x" title="88:310	For each measure we need to choose a threshold T, and consider as selected collocations only the T highest-ranked bigrams (where T can differ for each measure)." ></td>
	<td class="line x" title="89:310	By choosing higher thresholds we increase the precision (reduce the chance of accepting wrong collocations)." ></td>
	<td class="line x" title="90:310	By choosing lower thresholds we get better recall." ></td>
	<td class="line x" title="91:310	If we opt for low recall we may not get many collocations for some of the near-synonyms." ></td>
	<td class="line x" title="92:310	Because there is no principled way of choosing these thresholds, we prefer to choose lower thresholds (the first 200,000 collocations selected by each measure, except Fishers measure for which we take all 435,000 collocations ranked 1) and to filter out later (in step 2) the bigrams that are not true collocations, using mutual information on the Web." ></td>
	<td class="line x" title="93:310	3 Differential collocations For each cluster of near-synonyms, we now have the words that occur in preferred collocations with each near-synonym." ></td>
	<td class="line x" title="94:310	We need to check whether these words collocate with the other near-synonyms in the same cluster." ></td>
	<td class="line x" title="95:310	For example, if daunting task is a preferred collocation, we check whether daunting collocates with the other near-synonyms of task." ></td>
	<td class="line x" title="96:310	We use the Web as a corpus for differential collocations." ></td>
	<td class="line x" title="97:310	We dont use the BNC corpus to rank less-preferred and anti-collocations, because their absence in BNC may be due to chance." ></td>
	<td class="line x" title="98:310	We can assume that the Web (the portion retrieved by search engines) is big enough that a negative result can be trusted." ></td>
	<td class="line x" title="99:310	We use an interface to AltaVista search engine to count how often a collocation is found." ></td>
	<td class="line x" title="100:310	(See Table 2 for an example.5) A low number of co-occurrences indicates a less-preferred collocation." ></td>
	<td class="line x" title="101:310	But we also need to consider how frequent the two words in the collocation are." ></td>
	<td class="line x" title="102:310	We use the differential t-test to find collocations that best distinguish between two nearsynonyms (Church et al. , 1991), but we use the Web as a corpus." ></td>
	<td class="line x" title="103:310	Here we dont have part-of-speech tags but this is not a problem because in the previous step we selected collocations with the right part-ofspeech for the near-synonym." ></td>
	<td class="line x" title="104:310	We approximate the number of occurrences of a word on the Web with the number of documents containing the word." ></td>
	<td class="line x" title="105:310	The t-test can also be used in the hypothesis testing method to rank collocations." ></td>
	<td class="line x" title="106:310	It looks at the mean and variance of a sample of measurements, where the null hypothesis is that the sample was drawn from a normal distribution with mean ." ></td>
	<td class="line x" title="107:310	It measures the difference between observed ( x) and expected means, scaled by the variance of the data (s2), which in turn is scaled by the sample size (N)." ></td>
	<td class="line x" title="108:310	t = x q s2 N We are interested in the Differential t-test, which can be used for hypothesis testing of differences." ></td>
	<td class="line x" title="109:310	It compares the means of two normal populations: t = x1 x2q s21 N + s22 N Here the null hypothesis is that the average difference is  = 0.Therefore x  =  = x1 x2." ></td>
	<td class="line x" title="110:310	In the denominator we add the variances of the two populations." ></td>
	<td class="line x" title="111:310	If the collocations of interest are xw and yw (or similarly wx and wy), then we have the approximations x1 = s21 = P(x;w) and x2 = s22 = P(y;w); therefore: t = P(x;w) P(y;w)qP(x;w)+P(y;w) n++ = nxw nywpn xw + nyw If w is a word that collocates with one of the nearsynonyms in a cluster, and x is each of the near5The search was done on 13 March 2002." ></td>
	<td class="line x" title="112:310	synonyms, we can approximate the mutual information relative to w: P(w;x) P(x) = nwx nx where P(w) was dropped because it is the same for various x (we cannot compute if we keep it, because we dont know the total number of bigrams on the Web)." ></td>
	<td class="line x" title="113:310	We use this measure to eliminate collocations wrongly selected in step 1." ></td>
	<td class="line x" title="114:310	We eliminate those with mutual information lower that a threshold." ></td>
	<td class="line x" title="115:310	We describe the way we chose this threshold (Tmi) in section 5." ></td>
	<td class="line x" title="116:310	We are careful not to consider collocations of a near-synonym with a wrong part-of-speech (our collocations are tagged)." ></td>
	<td class="line x" title="117:310	But there is also the case when a near-synonym has more than one major sense." ></td>
	<td class="line x" title="118:310	In this case we are likely to retrieve collocations for senses other than the one required in the cluster." ></td>
	<td class="line x" title="119:310	For example, for the cluster job, task, duty, etc. , the collocation import/N duty/N is likely to be for a different sense of duty (the customs sense)." ></td>
	<td class="line x" title="120:310	Our way of dealing with this is to disambiguate the sense used in each collocations (we assume one sense per collocation), by using a simple Lesk-style method (Lesk, 1986)." ></td>
	<td class="line x" title="121:310	For each collocation, we retrieve instances in the corpus, and collect the content words surrounding the collocations." ></td>
	<td class="line x" title="122:310	This set of words is then intersected with the context of the near-synonym in CTRW (that is the whole entry)." ></td>
	<td class="line x" title="123:310	If the intersection is not empty, it is likely that the collocation and the entry use the near-synonym in the same sense." ></td>
	<td class="line x" title="124:310	If the intersection is empty, we dont keep the collocation." ></td>
	<td class="line x" title="125:310	In step 3, we group the collocations of each nearsynonym with a given collocate in three classes, based on the t-test values of pairwise collocations." ></td>
	<td class="line x" title="126:310	We compute the t-test between each collocation and the collocation with maximum frequency, and the t-test between each collocation and the collocation with minimum frequency (see Table 2 for an example)." ></td>
	<td class="line x" title="127:310	Then, we need to determine a set of thresholds that classify the collocations in the three groups: preferred collocations, less preferred collocations, and anti-collocations." ></td>
	<td class="line x" title="128:310	The procedure we use in this step is detailed in section 5." ></td>
	<td class="line x" title="129:310	x Hits MI t max t min task 63573 0.011662 252.07 job 485 0.000022 249.19 22.02 assignment 297 0.000120 250.30 17.23 chore 96 0.151899 251.50 9.80 duty 23 0.000022 251.93 4.80 stint 0 0 252.07 hitch 0 0 252.07 Table 2: The second column shows the number of hits for the collocation daunting x, where x is one of the near-synonyms in the first column." ></td>
	<td class="line x" title="130:310	The third column shows the mutual information, the fourth column, the differential t-test between the collocation with maximum frequency (daunting task) and daunting x, and the last column, the t-test between daunting x and the collocation with minimum frequency (daunting hitch)." ></td>
	<td class="line x" title="131:310	4 Results We obtained 15,813,685 bigrams." ></td>
	<td class="line x" title="132:310	From these, 1,350,398 were distinct and occurred at least 4 times." ></td>
	<td class="line x" title="133:310	We present some of the top-ranked collocations for each measure in the Appendix." ></td>
	<td class="line x" title="134:310	We present the rank given by each measure (1 is the highest), the value of the measure, the frequency of the collocation, and the frequencies of the words in the collocation." ></td>
	<td class="line x" title="135:310	We selected collocations for all 914 clusters in CTRW (5419 near-synonyms in total)." ></td>
	<td class="line x" title="136:310	An example of collocations extracted for the near-synonym task is: daunting/A task/N -MI 24887 10.8556 -LL 5998 907.96 -X2 16341 122196.8257 -Dice 2766 0.0274 repetitive/A task/N -MI 64110 6.7756 -X2 330563 430.4004 where the numbers are, in order, the rank given by the measure and the value of the measure." ></td>
	<td class="line x" title="137:310	We filtered out the collocations using MI on the Web (step 2), and then we applied the differential t-test (step 3)." ></td>
	<td class="line x" title="138:310	Table 2 shows the values of MI between daunting x and x, where x is one of the near-synonyms of task." ></td>
	<td class="line x" title="139:310	It also shows t-test valNear-synonyms daunting particular tough task p p p job ? p p assignment p p chore ? duty p stint hitch Table 3: Example of results for collocations." ></td>
	<td class="line x" title="140:310	ues between (some) pairs of collocations." ></td>
	<td class="line x" title="141:310	Table 3 presents an example of results for differential collocations, where p marks preferred collocations, ? marks less-preferred collocations, and marks anticollocations." ></td>
	<td class="line x" title="142:310	Before proceeding with step 3, we filtered out the collocations in which the near-synonym is used in a different sense, using the Lesk method explained above." ></td>
	<td class="line x" title="143:310	For example, suspended/V duty/N is kept while customs/N duty/N and import/N duty/N are rejected." ></td>
	<td class="line x" title="144:310	The disambiguation part of our system was run only for a subset of CTRW, because we have yet to evaluate it." ></td>
	<td class="line x" title="145:310	The other parts of our system were run for the whole CTRW." ></td>
	<td class="line x" title="146:310	Their evaluation is described in the next section." ></td>
	<td class="line x" title="147:310	5 Evaluation Our evaluation has two purposes: to get a quantitative measure of the quality of our results, and to choose thresholds in a principled way." ></td>
	<td class="line x" title="148:310	As described in the previous sections, in step 1 we selected potential collocations from BNC (the ones selected by at least two of the five measures)." ></td>
	<td class="line x" title="149:310	Then, we selected collocations for each of the nearsynonyms in CTRW (step 2)." ></td>
	<td class="line x" title="150:310	We need to evaluate the MI filter (step 3), which filters out the bigrams that are not true collocations, based on their mutual information computed on the Web." ></td>
	<td class="line x" title="151:310	We also need to evaluate step 4, the three way classification based on the differential t-test on the Web." ></td>
	<td class="line x" title="152:310	For evaluation purposes we selected three clusters from CTRW, with a total of 24 near-synonyms." ></td>
	<td class="line x" title="153:310	For these, we obtained 916 collocations from BNC according to the method described in section 2." ></td>
	<td class="line x" title="154:310	We had two human judges reviewing these collocations to determine which of them are true collocations and which are not." ></td>
	<td class="line x" title="155:310	We presented the collocations to the judges in random order, and each collocation was presented twice." ></td>
	<td class="line x" title="156:310	The first judge was consistent (judged a collocation in the same way both times it appeared) in 90.4% of the cases." ></td>
	<td class="line x" title="157:310	The second judge was consistent in 88% of the cases." ></td>
	<td class="line x" title="158:310	The agreement between the two judges was 67.5% (computed in a strict way, that is we considered agreement only when the two judges had the same opinion including the cases when they were not consistent)." ></td>
	<td class="line x" title="159:310	The consistency and agreement figures show how difficult the task is for humans." ></td>
	<td class="line x" title="160:310	We used the data annotated by the two judges to build a standard solution, so we can evaluate the results of our MI filter." ></td>
	<td class="line x" title="161:310	In the standard solution a bigram was considered a true collocation if both judges considered it so." ></td>
	<td class="line x" title="162:310	We used the standard solution to evaluate the results of the filtering, for various values of the threshold Tmi." ></td>
	<td class="line x" title="163:310	That is, if a bigram had the value of MI on the Web lower than a threshold Tmi, it was filtered out." ></td>
	<td class="line x" title="164:310	We choose the value of Tmi so that the accuracy of our filtering program is the highest." ></td>
	<td class="line x" title="165:310	By accuracy we mean the number of true collocations (as given by the standard solution) identified by our program over the total number of bigrams we used in the evaluation." ></td>
	<td class="line x" title="166:310	The best accuracy was 70.7% for Tmi = 0.0017." ></td>
	<td class="line x" title="167:310	We used this value of the threshold when running our programs for all CTRW." ></td>
	<td class="line x" title="168:310	As a result of this first part of the evaluation, we can say that after filtering collocations based on MI on the Web, approximately 70.7% of the remaining bigrams are true collocation." ></td>
	<td class="line x" title="169:310	This value is not absolute, because we used a sample of the data for the evaluation." ></td>
	<td class="line x" title="170:310	The 70.7% accuracy is much better than a baseline (approximately 50% for random choice)." ></td>
	<td class="line x" title="171:310	Table 4 summarizes our evaluation results." ></td>
	<td class="line x" title="172:310	Next, we proceeded with evaluating the differential t-test three-way classifier." ></td>
	<td class="line x" title="173:310	For each cluster, for each collocation, new collocations were formed from the collocate and all the near-synonyms in the cluster." ></td>
	<td class="line x" title="174:310	In order to learn the classifier, and to evaluate its results, we had the two judges manually classify a sample data into preferred collocations, lesspreferred collocations, and anti-collocations." ></td>
	<td class="line x" title="175:310	We used 2838 collocations obtained for the same three clusters from 401 collocations (out of the initial 916) that remained after filtering." ></td>
	<td class="line x" title="176:310	We built a standard solution for this task, based on the classifications of Step Baseline Our system Filter (MI on the Web) 50% 70.7% Dif." ></td>
	<td class="line x" title="177:310	t-test classifier 71.4% 84.1% Table 4: Accuracy of our main steps." ></td>
	<td class="line x" title="178:310	both judges." ></td>
	<td class="line x" title="179:310	When the judges agreed, the class was clear." ></td>
	<td class="line x" title="180:310	When they did not agree, we designed simple rules, such as: when one judge chose the class preferred collocation, and the other judge chose the class anti-collocation, the class in the solution was less-preferred collocation." ></td>
	<td class="line x" title="181:310	The agreement between judges was 80%; therefore we are confident that the quality of our standard solution is high." ></td>
	<td class="line x" title="182:310	We used this standard solution as training data to learn a decision tree6 for our three-way classifier." ></td>
	<td class="line x" title="183:310	The features in the decision tree are the t-test between each collocation and the collocation from the same group that has maximum frequency on the Web, and the t-test between the current collocation and the collocation that has minimum frequency (as presented in Table 2)." ></td>
	<td class="line x" title="184:310	We could have set aside a part of the training data as a test set." ></td>
	<td class="line x" title="185:310	Instead, we did 10-fold cross validation to quantify the accuracy on unseen data." ></td>
	<td class="line x" title="186:310	The accuracy on the test set was 84.1% (compared with a baseline that chooses the most frequent class, anti-collocations, and achieves an accuracy of 71.4%)." ></td>
	<td class="line x" title="187:310	We also experimented with including MI as a feature in the decision tree, and with manually choosing thresholds (without a decision tree) for the three-way classification, but the accuracy was lower than 84.1%." ></td>
	<td class="line x" title="188:310	The three-way classifier can fix some of the mistakes of the MI filter." ></td>
	<td class="line x" title="189:310	If a wrong collocation remained after the MI filter, the classifier can classify it in the anti-collocations class." ></td>
	<td class="line x" title="190:310	We can conclude that the collocational knowledge we acquired has acceptable quality." ></td>
	<td class="line x" title="191:310	6 Word Association We performed a second experiment, where we looked for long distance co-occurrences (words that co-occur in a window of size K)." ></td>
	<td class="line x" title="192:310	We call these associations, and they include the lexical collocations we extracted in section 2." ></td>
	<td class="line x" title="193:310	6We used C4.5, http://www.cse.unsw.edu.au/ quinlan We use BSP with the option of looking for bigrams in a window larger than 2." ></td>
	<td class="line x" title="194:310	For example if the window size is 3, and the text is vaccine/N cure/V available/A, the extracted bigrams are vaccine/N cure/V, cure/V available/A, and vaccine/N available/A. We would like to choose a large (4 15) window size; the only problem is the increase in computation time." ></td>
	<td class="line x" title="195:310	We look for associations of a word in the paragraph, not only in the sentence." ></td>
	<td class="line x" title="196:310	Because we look for bigrams, we may get associations that occur to the left or to the right of the word." ></td>
	<td class="line x" title="197:310	This is an indication of strong association." ></td>
	<td class="line x" title="198:310	We obtained associations similar to those presented by Church et al.(1991) for the near-synonyms ship and boat." ></td>
	<td class="line x" title="199:310	Church et al. suggest that a lexicographer looking at these associations can infer that a boat is generally smaller than a ship, because they are found in rivers and lakes, while the ships are found in seas." ></td>
	<td class="line x" title="200:310	Also, boats are used for small jobs (e.g. , fishing, police, pleasure), whereas ships are used for serious business (e.g. , cargo, war)." ></td>
	<td class="line x" title="201:310	Our intention is to use the associations to automatically infer this kind of knowledge and to validate acquired knowledge." ></td>
	<td class="line x" title="202:310	For our purpose we need only very strong associations, and we dont want words that associate with all near-synonyms in a cluster." ></td>
	<td class="line x" title="203:310	Therefore we test for anti-associations using the same method we used in section 3, with the difference that the query asked to AltaVista is: x NEAR y (where x and y are the words of interest)." ></td>
	<td class="line x" title="204:310	Words that dont associate with a near-synonym but associate with all the other near-synonyms in a cluster can tell us something about its nuances of meaning." ></td>
	<td class="line x" title="205:310	For example terrible slip is an antiassociation, while terrible associates with mistake, blunder, error." ></td>
	<td class="line x" title="206:310	This is an indication that slip is a minor error." ></td>
	<td class="line x" title="207:310	Table 5 presents some preliminary results we obtained with K = 4 (on half the BNC and then on the Web), for the differential associations of boat (where p marks preferred associations, ? marks less-preferred associations, and marks antiassociations)." ></td>
	<td class="line x" title="208:310	We used the same thresholds as for our experiment with collocations." ></td>
	<td class="line x" title="209:310	Near-synonyms fishing club rowing boat p p p vessel p craft ? ?" ></td>
	<td class="line x" title="210:310	ship ? ?" ></td>
	<td class="line x" title="211:310	Table 5: Example of results for associations." ></td>
	<td class="line x" title="212:310	7 Related work There has been a lot of work done in extracting collocations for different applications." ></td>
	<td class="line x" title="213:310	We have already mentioned some of the most important contributors." ></td>
	<td class="line x" title="214:310	Like Church et al.(1991), we use the t-test and mutual information, but unlike them we use the Web as a corpus for this task (and a modified form of mutual information), and we distinguish three types of collocations (preferred, less-preferred, and anticollocations)." ></td>
	<td class="line x" title="215:310	We are concerned with extracting collocations for use in lexical choice." ></td>
	<td class="line x" title="216:310	There is a lot of work on using collocations in NLG (but not in the lexical choice sub-component)." ></td>
	<td class="line x" title="217:310	There are two typical approaches: the use of phrasal templates in the form of canned phrases, and the use of automatically extracted collocations for unification-based generation (McKeown and Radev, 2000)." ></td>
	<td class="line x" title="218:310	Statistical NLG systems (such as Nitrogen (Langkilde and Knight, 1998)) make good use of the most frequent words and their collocations." ></td>
	<td class="line x" title="219:310	But such a system cannot choose a less-frequent synonym that may be more appropriate for conveying desired nuances of meaning, if the synonym is not a frequent word." ></td>
	<td class="line x" title="220:310	Finally, there is work related to ours from the point of view of the synonymy relation." ></td>
	<td class="line x" title="221:310	Turney (2001) used mutual information to detect the best answer to questions about synonyms from Test of English as a Foreign Language (TOEFL) and English as a Second Language (ESL)." ></td>
	<td class="line x" title="222:310	Given a problem word (with or without context), and four alternative words, the question is to choose the alternative most similar in meaning with the problem word." ></td>
	<td class="line x" title="223:310	His work is based on the assumption that two synonyms are likely to occur in the same document (on the Web)." ></td>
	<td class="line x" title="224:310	This can be true if the author needs to avoid repeating the same word, but not true when the synonym is of secondary importance in a text." ></td>
	<td class="line x" title="225:310	The alternative that has the highest PMI-IR (pointwise mutual information for information retrieval) with the problem word is selected as the answer." ></td>
	<td class="line x" title="226:310	We used the same measure in section 3  the mutual information between a collocation and a collocate that has the potential to discriminate between nearsynonyms." ></td>
	<td class="line x" title="227:310	Both works use the Web as a corpus, and a search engine to estimate the mutual information scores." ></td>
	<td class="line x" title="228:310	Pearce (2001) improves the quality of retrieved collocations by using synonyms from WordNet (Pearce, 2001)." ></td>
	<td class="line x" title="229:310	A pair of words is considered a collocation if one of the words significantly prefers only one (or several) of the synonyms of the other word." ></td>
	<td class="line x" title="230:310	For example, emotional baggage is a good collocation because baggage and luggage are in the same synset and emotional luggage is not a collocation." ></td>
	<td class="line x" title="231:310	As in our work, three types of collocations are distinguished: words that collocate well; words that tend to not occur together, but if they do the reading is acceptable; and words that must not be used together because the reading will be unnatural (anti-collocations)." ></td>
	<td class="line x" title="232:310	In a similar manner with (Pearce, 2001), in section 3, we dont record collocations in our lexical knowledge-base if they dont help discriminate between near-synonyms." ></td>
	<td class="line x" title="233:310	A difference is that we use more than frequency counts to classify collocations (we use a combination of t-test and MI)." ></td>
	<td class="line x" title="234:310	Our evaluation was partly inspired by Evert and Krenn (2001)." ></td>
	<td class="line x" title="235:310	They collect collocations of the form noun-adjective and verb-prepositional phrase." ></td>
	<td class="line x" title="236:310	They build a solution using two human judges, and use the solution to decide what is the best threshold for taking the N highest-ranked pairs as true collocations." ></td>
	<td class="line x" title="237:310	In their experiment MI behaves worse that other measures (LL, t-test), but in our experiment MI on the Web achieves good results." ></td>
	<td class="line x" title="238:310	8 Conclusions and Future Work We presented an unsupervised method to acquire knowledge about the collocational behaviour of near-synonyms." ></td>
	<td class="line x" title="239:310	Our future work includes improving the way we combine the five measures for ranking collocations, maybe by giving more weight to the collocations selected by the log-likelihood ratio." ></td>
	<td class="line x" title="240:310	We also plan to experiment more with disambiguating the senses of the words in a collocation." ></td>
	<td class="line x" title="241:310	Our long-term goal is to acquire knowledge about near-synonyms from corpora and other sources, by bootstrapping with our initial lexical knowledgebase of near-synonym differences." ></td>
	<td class="line x" title="242:310	This includes validating the knowledge already asserted and learning more distinctions." ></td>
	<td class="line x" title="243:310	Acknowledgments We thank Gerald Penn, Olga Vechtomova, and three anonymous reviewers for their helpful comments on previous drafts of this paper." ></td>
	<td class="line x" title="244:310	We thank Eric Joanis and Tristan Miller for helping with the judging task." ></td>
	<td class="line x" title="245:310	Our work is financially supported by the Natural Sciences and Engineering Research Council of Canada and the University of Toronto." ></td>
	<td class="line x" title="246:310	Appendix The first 10 collocations selected by each measure are presented below." ></td>
	<td class="line x" title="247:310	Note that some of the measures rank many collocations equally at rank 1: MI 358 collocations; LL one collocation; 2 828 collocations; Dice 828 collocations; and Fisher 435,000 collocations (when the measure is computed with a precision of 10 digits  higher precision is recommended, but the computation time becomes a problem)." ></td>
	<td class="line x" title="248:310	The rest of the columns are: the rank assigned by the measure, the value of the measure, the frequency of the collocation in BNC, the frequency of the first word in the first position in bigrams, and the frequency of the second word in the second position in bigrams." ></td>
	<td class="line x" title="249:310	Some of the collocations ranked 1 by MI: source-level/A debugger/N 1 21.9147 4 4 4 prosciutto/N crudo/N 1 21.9147 4 4 4 rumpy/A pumpy/A 1 21.9147 4 4 4 thrushes/N blackbirds/N 1 21.9147 4 4 4 clickity/N clickity/N 1 21.9147 4 4 4 bldsc/N microfilming/V 1 21.9147 4 4 4 chi-square/A variate/N 1 21.9147 4 4 4 long-period/A comets/N 1 21.9147 4 4 4 tranquillizers/N sedatives/N 1 21.9147 4 4 4 one-page/A synopsis/N 1 21.9147 4 4 4 First 10 collocations selected by LL: prime/A minister/N 1 123548 9464 11223 18825 see/V p./N 2 83195 8693 78213 10640 read/V studio/N 3 67537 5020 14172 5895 ref/N no/N 4 62486 3630 3651 4806 video-taped/A report/N 5 52952 3765 3765 15886 secretary/N state/N 6 51277 5016 10187 25912 date/N award/N 7 48794 3627 8826 5614 hon./A friend/N 8 47821 4094 10345 10566 soviet/A union/N 9 44797 3894 8876 12538 report/N follows/V 10 44785 3776 16463 6056 Some of the collocations ranked 1 by 2: lymphokine/V activated/A 1 15813684 5 5 5 config/N sys/N 1 15813684 4 4 4 levator/N depressor/N 1 15813684 5 5 5 nobile/N officium/N 1 15813684 11 11 11 line-printer/N dot-matrix/A 1 15813684 4 4 4 dermatitis/N herpetiformis/N 1 15813684 9 9 9 self-induced/A vomiting/N 1 15813684 5 5 5 horoscopic/A astrology/N 1 15813684 5 5 5 mumbo/N jumbo/N 1 15813684 12 12 12 long-period/A comets/N 1 15813684 4 4 4 Some of the collocations ranked 1 by Dice: clarinets/N bassoons/N 1 1.00 5 5 5 email/N footy/N 1 1.00 4 4 4 tweet/V tweet/V 1 1.00 5 5 5 garage/parking/N vehicular/A 1 1.00 4 4 4 growing/N coca/N 1 1.00 5 5 5 movers/N seconders/N 1 1.00 5 5 5 elliptic/A integrals/N 1 1.00 8 8 8 viscose/N rayon/N 1 1.00 15 15 15 cause-effect/A inversions/N 1 1.00 5 5 5 first-come/A first-served/A 1 1.00 6 6 6 Some of the collocations ranked 1 by Fisher: roman/A artefacts/N 1 1.00 4 3148 108 qualitative/A identity/N 1 1.00 16 336 1932 literacy/N education/N 1 1.00 9 252 20350 disability/N pension/N 1 1.00 6 470 2555 units/N transfused/V 1 1.00 5 2452 12 extension/N exceed/V 1 1.00 9 1177 212 smashed/V smithereens/N 1 1.00 5 194 9 climbing/N frames/N 1 1.00 5 171 275 inclination/N go/V 1 1.00 10 53 51663 trading/N connections/N 1 1.00 6 2162 736 References Kenneth Church and Patrick Hanks." ></td>
	<td class="line x" title="250:310	1991." ></td>
	<td class="line x" title="251:310	Word association norms, mutual information and lexicography." ></td>
	<td class="line x" title="252:310	Computational Linguistics, 16(1):2229." ></td>
	<td class="line x" title="253:310	Kenneth Church, William Gale, Patrick Hanks, and Donald Hindle." ></td>
	<td class="line x" title="254:310	1991." ></td>
	<td class="line x" title="255:310	Using statistics in lexical analysis." ></td>
	<td class="line x" title="256:310	In Uri Zernik, editor, Lexical Acquisition: Using On-line Resources to Build a Lexicon, pages 115164." ></td>
	<td class="line x" title="257:310	Lawrence Erlbaum." ></td>
	<td class="line x" title="258:310	Ted Dunning." ></td>
	<td class="line x" title="259:310	1993." ></td>
	<td class="line x" title="260:310	Accurate methods for statistics of surprise and coincidence." ></td>
	<td class="line x" title="261:310	Computational Linguistics, 19(1):6174." ></td>
	<td class="line x" title="262:310	Philip Edmonds and Graeme Hirst." ></td>
	<td class="line x" title="263:310	2002 (to appear)." ></td>
	<td class="line x" title="264:310	Near-synonymy and lexical choice." ></td>
	<td class="line x" title="265:310	Computational Linguistics, 28(2)." ></td>
	<td class="line x" title="266:310	Stefan Evert and Brigitte Krenn." ></td>
	<td class="line x" title="267:310	2001." ></td>
	<td class="line x" title="268:310	Methods for the qualitative evaluation of lexical association measures." ></td>
	<td class="line x" title="269:310	In Proceedings of the 39th Annual Meeting of the of the Association for Computational Linguistics (ACL2001), Toulouse, France." ></td>
	<td class="line x" title="270:310	S. I. Hayakawa." ></td>
	<td class="line x" title="271:310	1994." ></td>
	<td class="line x" title="272:310	Choose the Right Word." ></td>
	<td class="line x" title="273:310	HarperCollins Publishers." ></td>
	<td class="line x" title="274:310	Diana Zaiu Inkpen and Graeme Hirst." ></td>
	<td class="line x" title="275:310	2001." ></td>
	<td class="line x" title="276:310	Building a lexical knowledge-base of near-synonym differences." ></td>
	<td class="line x" title="277:310	In Proceedings of the Workshop on WordNet and Other Lexical Resources, Second Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL2001), Pittsburgh." ></td>
	<td class="line x" title="278:310	Irene Langkilde and Kevin Knight." ></td>
	<td class="line x" title="279:310	1998." ></td>
	<td class="line x" title="280:310	The practical value of N-grams in generation." ></td>
	<td class="line x" title="281:310	In Proceedings of the International Natural Language Generation Workshop, Niagara-on-the-Lake, Ontario." ></td>
	<td class="line x" title="282:310	Michael Lesk." ></td>
	<td class="line x" title="283:310	1986." ></td>
	<td class="line x" title="284:310	Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone." ></td>
	<td class="line x" title="285:310	In Proceedings of SIGDOC Conference, Toronto." ></td>
	<td class="line x" title="286:310	Christopher Manning and Hinrich Schutze." ></td>
	<td class="line x" title="287:310	1999." ></td>
	<td class="line x" title="288:310	Foundations of Statistical Natural Language Processing." ></td>
	<td class="line x" title="289:310	The MIT Press, Cambridge, Massachusetts." ></td>
	<td class="line x" title="290:310	Kathleen McKeown and Dragomir Radev." ></td>
	<td class="line x" title="291:310	2000." ></td>
	<td class="line x" title="292:310	Collocations." ></td>
	<td class="line x" title="293:310	In R. Dale, H. Moisl, and H. Somers, editors, Handbook of Natural Language Processing." ></td>
	<td class="line x" title="294:310	Marcel Dekker." ></td>
	<td class="line x" title="295:310	Darren Pearce." ></td>
	<td class="line x" title="296:310	2001." ></td>
	<td class="line x" title="297:310	Synonymy in collocation extraction." ></td>
	<td class="line x" title="298:310	In Proceedings of the Workshop on WordNet and Other Lexical Resources, Second meeting of the North American Chapter of the Association for Computational Linguistics, Pittsburgh." ></td>
	<td class="line x" title="299:310	Ted Pedersen." ></td>
	<td class="line x" title="300:310	1996." ></td>
	<td class="line x" title="301:310	Fishing for exactness." ></td>
	<td class="line x" title="302:310	In Proceedings of the South-Central SAS Users Group Conference (SCSUG-96), Austin, Texas." ></td>
	<td class="line x" title="303:310	Frank Smadja." ></td>
	<td class="line x" title="304:310	1993." ></td>
	<td class="line x" title="305:310	Retrieving collocations from text: Xtract." ></td>
	<td class="line x" title="306:310	Computational Linguistics, 19(1):143177." ></td>
	<td class="line x" title="307:310	Peter Turney." ></td>
	<td class="line x" title="308:310	2001." ></td>
	<td class="line x" title="309:310	Mining the web for synonyms: PMIIR versus LSA on TOEFL." ></td>
	<td class="line x" title="310:310	In Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001), pages 491502, Freiburg, Germany ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W02-1606
Word Sense Disambiguation In A Korean-To-Japanese MT System Using Neural Networks
Chung, You-Jin;Kang, Sin-Jae;Moon, Kyonghi;Lee, Jong-Hyeok;"></td>
	<td class="line x" title="1:194	Word Sense Disambiguation in a Korean-to-Japanese MT System Using Neural Networks You-Jin Chung, Sin-Jae Kang, Kyong-Hi Moon, and Jong-Hyeok Lee Div." ></td>
	<td class="line x" title="2:194	of Electrical and Computer Engineering, Pohang University of Science and Technology (POSTECH) and Advanced Information Technology Research Center(AlTrc) San 31, Hyoja-dong, Nam-gu, Pohang, R. of KOREA, 790-784 {prizer,sjkang,khmoon,jhlee}@postech.ac.kr Abstract This paper presents a method to resolve word sense ambiguity in a Korean-to-Japanese machine translation system using neural networks." ></td>
	<td class="line x" title="3:194	The execution of our neural network model is based on the concept codes of a thesaurus." ></td>
	<td class="line x" title="4:194	Most previous word sense disambiguation approaches based on neural networks have limitations due to their huge feature set size." ></td>
	<td class="line x" title="5:194	By contrast, we reduce the number of features of the network to a practical size by using concept codes as features rather than the lexical words themselves." ></td>
	<td class="line x" title="6:194	Introduction Korean-to-Japanese machine translation (MT) employs a direct MT strategy, where a Korean homograph may be translated into a different Japanese equivalent depending on which sense is used in a given context." ></td>
	<td class="line x" title="7:194	Thus, word sense disambiguation (WSD) is essential to the selection of an appropriate Japanese target word." ></td>
	<td class="line x" title="8:194	Much research on word sense disambiguation has revealed that several different types of information can contribute to the resolution of lexical ambiguity." ></td>
	<td class="line x" title="9:194	These include surrounding words (an unordered set of words surrounding a target word), local collocations (a short sequence of words near a target word, taking word order into account), syntactic relations (selectional restrictions), parts of speech, morphological forms, etc (McRoy, 1992, Ng and Zelle, 1997)." ></td>
	<td class="line x" title="10:194	Some researchers use neural networks in their word sense disambiguation systems Because of its strong capability in classification (Waltz et al. , 1985, Gallant, 1991, Leacock et al. , 1993, and Mooney, 1996)." ></td>
	<td class="line x" title="11:194	Since, however, most such methods require a few thousands of features or large amounts of hand-written data for training, it is not clear that the same neural network models will be applicable to real world applications." ></td>
	<td class="line x" title="12:194	We propose a word sense disambiguation method that combines both the neural net-based approach and the work of Li et al (2000), especially focusing on the practicality of the method for application to real world MT systems." ></td>
	<td class="line x" title="13:194	To reduce the number of input features of neural networks to a practical size, we use concept codes of a thesaurus as features." ></td>
	<td class="line x" title="14:194	In this paper, Yale Romanization is used to represent Korean expressions." ></td>
	<td class="line x" title="15:194	1 System Architecture Our neural network method consists of two phases." ></td>
	<td class="line x" title="16:194	The first phase is the construction of the feature set for the neural network; the second phase is the construction and training of the neural network." ></td>
	<td class="line x" title="17:194	(see Figure 1)." ></td>
	<td class="line x" title="18:194	For practical reasons, a reasonably small number of features is essential to the design of a neural network." ></td>
	<td class="line x" title="19:194	To construct a feature set of a reasonable size, we adopt Lis method (2000), based on concept co-occurrence information (CCI)." ></td>
	<td class="line x" title="20:194	CCI are concept codes of words which co-occur with the target word for a specific syntactic relation." ></td>
	<td class="line x" title="21:194	In accordance with Lis method, we automatically extract CCI from a corpus by constructing a Korean sense-tagged corpus." ></td>
	<td class="line x" title="22:194	To accomplish this, we apply a Japanese-to-Korean MT system." ></td>
	<td class="line x" title="23:194	Next, we extract CCI from the constructed corpus through partial parsing and scanning." ></td>
	<td class="line x" title="24:194	To eliminate noise and to reduce the number of CCI, refinement proceesing is applied Japanese Corpus COBALT-J/K (Japanese-to-Korean MT system) Sense Tagged Korean Corpus Partial Parsing & Pattern Scanning Raw CCI CCI Refinement Processing Refined CCI Feature Set Construction Neural Net Construction Feature Set Network Construction Neural Network Network Learning Stored in MT Dictionary Network Parameters Figure 1." ></td>
	<td class="line x" title="25:194	System Architecture noun nature character society institute things 0 1 7 8 9 astrocalenanimal phenonomy dar mena 00 01 06 09 goods drugs food stationary machine 90 91 92 96 99 orgaanisinintesegg sex nism mal ews tine 060 061 066 067 068 069 suppwritingcountbell lies tool book 960 961 962 969           L 1 L 2 L 3 L 4 Figure 2." ></td>
	<td class="line x" title="26:194	Concept hierarchy of the Kadokawa thesaurus to the extracted raw CCI." ></td>
	<td class="line x" title="27:194	After completing refinement processing, we use the remaining CCI as features for the neural network." ></td>
	<td class="line x" title="28:194	The trained network parameters are stored in a Korean-to-Japanese MT dictionary for WSD in translation." ></td>
	<td class="line x" title="29:194	2 Construction of Refined Feature Set 2.1 Automatic Construction of Sense-tagged Corpus For automatic construction of the sense-tagged corpus, we used a Japanese-to-Korean MT system called COBALT-J/K 1." ></td>
	<td class="line x" title="30:194	In the transfer dictionary of COBALT-J/K, nominal and verbal words are annotated with concept codes of the Kadokawa thesaurus (Ohno and Hamanishi, 1981), 1,100 semantic classe Concept nodes in level divided into 10 subclasses." ></td>
	<td class="line x" title="31:194	COBALTtranslations from a J nom codes at level a result, a Korean se 1,060,000 se Japanes Newspaper of Econom 1 Translato pract The quality of the constructed sense-tagged corpus is a critical issue." ></td>
	<td class="line x" title="32:194	To evaluate the quality, we collected 1,658 sample sentences (29,420 eojeols 2 ) from the corpus and checked their precision." ></td>
	<td class="line x" title="33:194	The total number of errors was 789, and included such errors as morphological analysis, sense ambiguity resolution and unknown words." ></td>
	<td class="line x" title="34:194	It corresponds to the accuracy of 97.3% (28,631 / 29,420 eojeols)." ></td>
	<td class="line x" title="35:194	Because almost all Japanese common nouns represented by Chinese characters are monosemous little transfer ambiguity is exhibited in Japanese-to-Korean translation." ></td>
	<td class="line x" title="36:194	In our test, the number of ambiguity resolution errors was 202 and it took only 0.69% of the overall corpus (202 / 29,420 eojeols)." ></td>
	<td class="line x" title="37:194	Considering the fact that the overall accuracy of the constructed corpus exceeds 97% and only a few sense ambiguity resolution errors were which has a 4-level hierarchy of about s, as shown in Figure 2." ></td>
	<td class="line x" title="38:194	L 1, L 2 and L 3 are further We made a slight modification of J/K to enable it to produce Korean apanese text, with all inal words tagged with specific concept L 4 of the Kadokawa thesaurus." ></td>
	<td class="line x" title="39:194	As nse-tagged corpus of ntences can be obtained from the e corpus (Asahi Shinbun, Japanese ics, etc.)." ></td>
	<td class="line x" title="40:194	COBALT-J/K (Collocation-Based Language r from Japanese to Korean) is a high-quality ical MT system developed by POSTECH." ></td>
	<td class="line x" title="41:194	found in the Japanese-to-Korean translation of nouns, we regard the generated sense-tagged corpus as highly reliable." ></td>
	<td class="line x" title="42:194	2.2 Extraction of Raw CCI Unlike English, Korean has almost no syntactic constraints on word order as long as the verb appears in the final position." ></td>
	<td class="line x" title="43:194	The variable word order often results in discontinuous constituents." ></td>
	<td class="line x" title="44:194	Instead of using local collocations by word order, Li et al.(2000) defined 13 patterns of CCI for homographs using syntactically related words in a sentence." ></td>
	<td class="line x" title="46:194	Because we are concerned only with 2 An Eojeol is a Korean syntactic unit consisting of a content word and one or more function words." ></td>
	<td class="line x" title="47:194	Table 2." ></td>
	<td class="line x" title="48:194	Concept codes and frequencies in CFP ({<C i,f i >}, type 2, nwun(eye)) Code Freq." ></td>
	<td class="line x" title="49:194	Code Freq." ></td>
	<td class="line x" title="50:194	Code Freq." ></td>
	<td class="line x" title="51:194	Code Freq." ></td>
	<td class="line x" title="52:194	103 4 107 8 121 7 126 4 143 8 160 5 179 7 277 4 320 8 331 6 416 7 419 12 433 4 501 13 503 10 504 11 505 6 507 12 508 27 513 5 530 6 538 16 552 4 557 7 573 5 709 5 718 5 719 4 733 5 819 4 834 4 966 4 987 9 other * 210  other in the table means the set of concept codes with the frequencies less than 4." ></td>
	<td class="line x" title="53:194	Table 1." ></td>
	<td class="line x" title="54:194	Structure of CCI Patterns CCI type Structure of pattern type 0 unordered co-occurrence words type 1 noun + noun or noun + noun type 2 noun + uy + noun type 3 noun + other particles + noun type 4 noun + lo/ulo + verb type 5 noun + ey + verb type 6 noun + eygey + verb type 7 noun + eyse + verb type 8 noun + ul/lul + verb type 9 noun + i/ka + verb type 10 verb + relativizer + noun noun homographs, we adopt 11 patterns from them excluding verb patterns, as shown in Table 1." ></td>
	<td class="line x" title="55:194	The words in bold indicate the target homograph and the words in italic indicate Korean particles." ></td>
	<td class="line x" title="56:194	For a homograph W, concept frequency patterns (CFPs), i.e., ({<C 1,f 1 >,<C 2,f 2 >,  , <C k, f k >}, type i, W(S i )), are extracted from the sense-tagged training corpus for each CCI type i by partial parsing and pattern scanning, where k is the number of concept codes in type i, f i is the frequency of concept code C i appearing in the corpus, type i is an CCI type i, and W(S i ) is a homograph W with a sense S i . All concepts in CFPs are three-digit concept codes at level L 4 in the Kadokawa thesaurus." ></td>
	<td class="line x" title="57:194	Table 2 demonstrates an example of CFP that can co-occur with the homograph nwun(eye) in the form of the CCI type 2 and their frequencies." ></td>
	<td class="line x" title="58:194	2.3 CCI Refinement Processing The extracted CCI are too numerous and too noisy to be used in a practical system, and must to be further selected." ></td>
	<td class="line x" title="59:194	To eliminate noise and to reduce the number of CCI to a practical size, we apply the refinement processing to the extracted CCI." ></td>
	<td class="line x" title="60:194	CCI refinement processing is composed of 2 processes: concept code discrimination and concept code generalization." ></td>
	<td class="line x" title="61:194	2.3.1 Concept Code Discrimination In the extracted CCI, the same concept code may appear for determining the different meanings of a homograph." ></td>
	<td class="line x" title="62:194	To select the most probable concept codes, which frequently co-occur with the target sense of a homograph, Li defined the discrimination value of a concept code using Shannons entropy (Shannon, 1951)." ></td>
	<td class="line x" title="63:194	A concept code with a small entropy has a large discrimination value." ></td>
	<td class="line x" title="64:194	If the discrimination value of the concept code is larger than a threshold, the concept code is selected as useful information for deciding the word sense." ></td>
	<td class="line x" title="65:194	Otherwise, the concept code is discarded." ></td>
	<td class="line x" title="66:194	2.3.2 Concept Code Generalization After concept discrimination, co-occurring concept codes in each CCI type must be further selected and the code generalized." ></td>
	<td class="line oc" title="67:194	To perform code generalization, Li adopted to Smadjas work (Smadja, 1993) and defined the code strength using a code frequency and a standard deviation in each level of the concept hierarchy." ></td>
	<td class="line x" title="68:194	The generalization filter selects the concept codes with a strength larger than a threshold." ></td>
	<td class="line x" title="69:194	We perform this generalizaion processing on the Kadokawa thesaurus level L 4 and L 3 . After processing, the system stores the refined conceptual patterns ({ type real texts." ></td>
	<td class="line x" title="70:194	These refined CCI are used as input features for the neural network." ></td>
	<td class="line x" title="71:194	specific des explained in Li (2000)." ></td>
	<td class="line x" title="72:194	3 3.1 Because of it the used in our sense clas shown in Figure 3, each n represents a C 1, C 2, C 3, }, i, W(S i )) as a knowledge source for WSD of The more cription of the CCI extraction is Construction of Neural Network Neural Network Architecture s strong capability for classification, multilayer feedforward neural network is sification system." ></td>
	<td class="line x" title="73:194	As ode in the input layer concept code in CCI of a target . . CCI type i 2 CCI type i 1 input CCI type 0 input CCI type 1 input CCI type 8 input CCI type 2 input 74 26 022 078 080 50 696 028 419 38 23 239 323 nwun 1 (snow) nwun 2 (eye) . . ." ></td>
	<td class="line x" title="74:194	Figure 5." ></td>
	<td class="line x" title="75:194	The Resulting Network for nwun word and each node in the output lay represents th num nodes in a hi To determine a good topology for the network, we implemented a 2-layer (no hidden layer) and a 3-layer (with a single hidden layer of 5 nodes) network and compared their performance." ></td>
	<td class="line x" title="76:194	The comparison result is given in Section 5." ></td>
	<td class="line x" title="77:194	Each homograph has a network of its own." ></td>
	<td class="line x" title="78:194	Figure 4 3 demonstrates a construction example of the input layer for the homograph nwun with the sense snow and eye." ></td>
	<td class="line x" title="79:194	The left side is the extracted CCI for each sense after refinement processing." ></td>
	<td class="line x" title="80:194	We construct the input layer for nwun by merely integrating the concept codes in both senses." ></td>
	<td class="line x" title="81:194	The resulting input layer is partitioned into several subgroups depending on their CCI types, i.e., type 0, type 1, type 2 and type 8." ></td>
	<td class="line x" title="82:194	Figure 5 shows the overall network architecture for nwun." ></td>
	<td class="line x" title="83:194	3 for th concept codes for  3.2 Network Learning We selected 875 Korean homographs requring the WSD processing in a Korean-to-Japanese translation." ></td>
	<td class="line x" title="84:194	Among the selected nouns, 736 nouns (about 84%) had two senses and the other 139 nouns had more than 3 senses." ></td>
	<td class="line x" title="85:194	Using the extracted CCI, we constructed neural networks and trained network parameters for each homograph." ></td>
	<td class="line x" title="86:194	The training patterns were also extracted from the previously constructed sense-tagged corpus." ></td>
	<td class="line x" title="87:194	The average number of input features (i.e. input nodes) of the constructed networks was approximately 54.1 and the average number of senses (i.e. output nodes) was about 2.19." ></td>
	<td class="line x" title="88:194	In the case of a 2-layer network, the total number of parameters (synaptic weights) needed to be trained is about 118 (54.12.19) for each homograph." ></td>
	<td class="line x" title="89:194	This means that we merely need storage for 118 floating point numbers (for sy features) for reasonable size to be used in real applications." ></td>
	<td class="line x" title="90:194	 CCI type 0 : {26, 022}  CCI type 1 : {080, 696} nwun 1 (snow) CCI type 0 input CCI type 1 74 26 022 078 080 Refined CCI 4 Our WSD approach is a h co knowledge-based overall WSD sense disa First, we Korean-to-Ja The concept codes in Figure 4 are simplified ones e ease of illustration." ></td>
	<td class="line x" title="91:194	In reality there are 87  CCI type 8 : {38, 239} Total 13 concept codes integrate input CCI type 8 input CCI type 2 input 13 nodes nwun 2 (eye)  CCI type 0 : {74, 078}  CCI type 2 : {50, 028, 419}  CCI type 8 : {23, 323} 50 696 028 419 38 23 239 323 Figure 4." ></td>
	<td class="line x" title="92:194	Construction of Input layer for nwun . . ." ></td>
	<td class="line x" title="93:194	Output (senses of the target word) Inputs Outputs . . Hidden Layers input CCI type i k input . . ." ></td>
	<td class="line x" title="94:194	Figure 3." ></td>
	<td class="line x" title="95:194	Topology of Neural Network er e sense of a target word." ></td>
	<td class="line x" title="96:194	The ber of hidden layers and the number of dden layer are another crucial issue." ></td>
	<td class="line x" title="97:194	nwun." ></td>
	<td class="line x" title="98:194	COBALT-K/ naptic weights) and 54 integers (for input each homograph, which is a Word Sense Disambiguation ybrid method, which mbines the advantage of corpus-based and methods." ></td>
	<td class="line x" title="99:194	Figure 6 shows our algorithm." ></td>
	<td class="line x" title="100:194	For a given homograph, mbiguation is performed as follows." ></td>
	<td class="line x" title="101:194	search a collocation dictionary." ></td>
	<td class="line x" title="102:194	The panese translation system J has an MWTU (Multi-Word {078} CCI type 0 CCI type 0 input CCI type 1 input CCI type 8 input CCI type 2 input CCI type 1 nwunmwul-i katuk-han kunye-uy nwun-ul po-mye input : [078] [274]concept code : [503] [331] target word CCI type : (type 0) (type 0) (type 2) (type 8) CCI type 2 CCI type 8 {none} {503} {331} 078 022 74 26 028 50 696 080 239 38 23 419 323 Input Layer Similarity Calculation {274} (0.000) (0.285) (0.250) (1.000) (0.000) (0.857) (0.000) (0.000) (0.000) (0.285) (0.000) (0.000) (0.250) similarity values Figure 7." ></td>
	<td class="line x" title="103:194	Construction of Input Pattern by Using Concept Similarity Calculation Neural Networks Select the most frequent sense Success Success Answer NO NO NO YES YES YES Selectional Restrictions of the Verb Collocation Dictionary Success Figure 6." ></td>
	<td class="line x" title="104:194	The Proposed WSD Algorithm Translation Units) dictionary, which contains idioms, compound words, collocations, etc. If a collocation of the target word exists in the MWTU dictionary, we simply determine the sense of the target word to the sense found in the dictionary." ></td>
	<td class="line x" title="105:194	This method is based on the idea of one sense per collocation." ></td>
	<td class="line x" title="106:194	Next, we verify the selectional restriction of the verb described in the dictionary." ></td>
	<td class="line x" title="107:194	If we cannot find any matched patterns for selectional restrictions, we apply the neural network approach." ></td>
	<td class="line x" title="108:194	WSD in the neural network stage is performed in the following 3 steps." ></td>
	<td class="line x" title="109:194	Step 1." ></td>
	<td class="line x" title="110:194	Extract CCI from the context of the target word." ></td>
	<td class="line x" title="111:194	The window size of the context is a single sentence." ></td>
	<td class="line x" title="112:194	Consider, for example, the sentence in Figure 7 which has the meaning of Seeing her eyes filled with tears, ." ></td>
	<td class="line x" title="113:194	The target word is the homograph nwun." ></td>
	<td class="line x" title="114:194	We extract its CCI from the sentence by partial parsing and pattern scanning." ></td>
	<td class="line x" title="115:194	In Figure 7, the words nwun and kunye(her) with the concept code 503 have the relation of <noun + uy + noun>, which corresponds to CCI type 2 in Table 1." ></td>
	<td class="line x" title="116:194	There is no syntactic relation between the words nwun and nwunmul(tears) with the concept code 078, so we assign CCI type 0 to the concept code 078." ></td>
	<td class="line x" title="117:194	Similarly, we can obtain all pairs of CCI types and their concept codes appearing in the context." ></td>
	<td class="line x" title="118:194	All the extracted <CCI-type: concept codes> pairs are as follows: {<type 0: 078,274>, <type 2: 503>, <type 8: 331>}." ></td>
	<td class="line x" title="119:194	Step 2." ></td>
	<td class="line x" title="120:194	Obtain the input pattern for the network by calculating concept similarities between the features of the input nodes and the concept code in the extracted <CCI-type: concept codes>." ></td>
	<td class="line x" title="121:194	Concept similarity calculation is performed only between the concept codes with the same CCI-type." ></td>
	<td class="line x" title="122:194	The calculated concept similarity score is assigned to each input node as the input value to the network." ></td>
	<td class="line x" title="123:194	Csim(C i, P j ) in Equation 1 is used to calculate the concept similarity between C i and P j, where MSCA(C i, P j ) is the most specific common ancestor of concept codes C i and P j, and weight is a weighting factor reflecting that C i as a descendant of P j is preferable to other cases." ></td>
	<td class="line x" title="124:194	That is, if C i is a descendant of P j, we set weight to 1." ></td>
	<td class="line x" title="125:194	Otherwise we set weight to 0.5." ></td>
	<td class="line x" title="126:194	weight PlevelClevel PCMSCAlevel PCCsim ji ji ji  +  = )()( )),((2 ),( (1) The similarity values between the target (all 0.000) (0.375) (0.857) (0.667) (0.285) (0.250) (0.250) L 1 L 2 L 3 L 4  C i P 1 P 2 P 3 P 4 P 5 P 5 TOP Figure 8." ></td>
	<td class="line x" title="127:194	Concept Similarity on the Kadokawa Thesaurus Hierarchy concept C i and each P j on the Kadokawa thesaurus hierarchy are shown in Figure 8." ></td>
	<td class="line x" title="128:194	These similarity values are computed using Equation 1." ></td>
	<td class="line x" title="129:194	For example, in CCI-type 0 part calculation, the relation between the concept codes 274 and 26 corresponds to the relation between C i and P 4 in Figure 8." ></td>
	<td class="line x" title="130:194	So we assign the similarity 0.285 to the input node labeled by 26." ></td>
	<td class="line x" title="131:194	As another example, the concept codes 503 and 50 have a relation between C i and P 2 and we obtain the similarity 0.857." ></td>
	<td class="line x" title="132:194	If more than two concept codes exist in one CCI-type, such as <CCI-type 0: 078, 274>, the maximum similarity value among them is assigned to the input node, as in Equation 2." ></td>
	<td class="line x" title="133:194	In Equation 2, C i is the concept code of the input node, and P j is the concept codes in the <CCI-type: concept codes> pair which has the same CCI-type as C i . By adopting this concept similarity calculation, we can achieve a broad coverage of the method." ></td>
	<td class="line x" title="134:194	If we use the exact matching scheme instead of concept similarity, we may obtain only a few concept codes matched with the features." ></td>
	<td class="line x" title="135:194	Consequently, sense disambiguation would fail because of the absence of clues." ></td>
	<td class="line x" title="136:194	Step 3." ></td>
	<td class="line x" title="137:194	Feed the obtained input pattern to the neural network and compute activation strengths for each output node." ></td>
	<td class="line x" title="138:194	Next, select the sense of the node that has a larger activation value than all other output node." ></td>
	<td class="line x" title="139:194	If the activation strength is lower than the threshold, it will be discarded and 5 Experimental Evaluation For an experimental evaluation, 10 ambiguous Korean nouns were selected, along with a total of 500 test sentences in which one homograph appears." ></td>
	<td class="line x" title="140:194	In order to follow the ambiguity distribution described in Section 3.2, we set the number of test nouns with two senses to 8 (80%)." ></td>
	<td class="line x" title="141:194	The test sentences were randomly selected from the KIBS (Korean Information Base System) corpus." ></td>
	<td class="line x" title="142:194	The experimental results are shown in Table 3, where result A is the case when the most frequent sense was taken as the answer." ></td>
	<td class="line x" title="143:194	To compare it with our approach (result C), we also performed the experiment using Lis method (result B)." ></td>
	<td class="line x" title="144:194	For sense disambiguation, Lis method features which are similar to our method." ></td>
	<td class="line x" title="145:194	However, unlike our method, which combines all features by using neural networks, Li considers only one clue at each decision step." ></td>
	<td class="line x" title="146:194	As shown in the table, our approach exceeded Lis )),((max)( ji P i PCCsimCInputVal i = (2) Table 3." ></td>
	<td class="line x" title="147:194	Comparison of WSD Results Precision (%) Word Sense No (A) (B) (C) father & child 33 pwuca rich man 17 66 64 72 liver 37 kancang soy source 13 74 84 74 housework 39 kasa words of song 11 78 68 82 shoes 45 kwutwu word of mouth 5 90 70 92 eye 42 nwun snow 8 84 80 86 container 41 yongki 82 72 88 the network will not make any decisions." ></td>
	<td class="line x" title="148:194	This process is represented in Figure 9." ></td>
	<td class="line x" title="149:194	courage 9 doctor 27 uysa intention 23 54 80 84 district 27 cikwu the earth 23 54 84 92 whole body 39 ones past 6 censin telegraph 5 78 84 80 ones best 27 military strength 13 electric power 7 cenlyek past record 3 54 50 72 Average Precision 71.4 73.6 82.2  (A) : Baseline (B) : Lis method (C) : Proposed method (using a 2-layer NN) nwun 1 (snow) nwun 2 (eye) . . ." ></td>
	<td class="line x" title="150:194	threshold (0.000) (0.285) (0.250) (1.000) (0.000) (0.857) (0.000) (0.000) (0.000) (0.285) (0.000) (0.000) (0.250) Figure 9." ></td>
	<td class="line x" title="151:194	Sense Disambiguation for nwun in most of the results except kancang and censin." ></td>
	<td class="line x" title="152:194	This result shows that word sense disambiguation can be improved by combining several clues together (e.g. neural networks) rather than using them independently (e.g. Lis method)." ></td>
	<td class="line x" title="153:194	The performance for each stage of the proposed method is shown in Table 4." ></td>
	<td class="line x" title="154:194	Symbols COL, VSR, NN and MFS in the table indicate 4 stages of our method in Figure 6, respectively." ></td>
	<td class="line x" title="155:194	In the NN stage, the 3-layer model did not show a performance superior to the 2-layer model because of the lack of training samples." ></td>
	<td class="line x" title="156:194	Since the 2-layer model has fewer parameters to be trained, it is more efficient to generalize for limited training corpora than the 3-layer model." ></td>
	<td class="line x" title="157:194	Conclusion To resolve sense ambiguities in Korean-to-Japanese MT, this paper has proposed a practical word sense disambiguation method using neural networks." ></td>
	<td class="line x" title="158:194	Unlike most previous approaches based on neural networks, we reduce the number of features for the network to a practical size by using concept codes rather than lexical words." ></td>
	<td class="line x" title="159:194	In an experimental evaluation, the proposed WSD model using a 2-layer network achieved an average precision of 82.2% with an improvement over Lis method by 8.6%." ></td>
	<td class="line x" title="160:194	This result is very promising for real world MT systems." ></td>
	<td class="line x" title="161:194	We plan further research to improve precision and to expand our method for verb homograph disambiguation." ></td>
	<td class="line x" title="162:194	Acknowledgements This work was supported by the Korea Science and Engineering Foundation (KOSEF) through the Advanced Information Technology Research Center(AITrc)." ></td>
	<td class="line x" title="163:194	Table 4." ></td>
	<td class="line x" title="164:194	Average Precision and Coverage for Each Stage of thePproposed Method <Case 1 : 2-layer NN> COL VSR NN MFS Avg." ></td>
	<td class="line x" title="165:194	Prec 100.0% 91.2% 86.3% 56.1% Avg." ></td>
	<td class="line x" title="166:194	Cov 3.6% 6.8% 73.2% 16.4% <Case 2 : 3-layer NN> COL VSR NN MFS Avg." ></td>
	<td class="line x" title="167:194	Prec 100.0% 91.2% 87.1% 56.0% Avg." ></td>
	<td class="line x" title="168:194	Cov 3.6% 6.8% 72.5% 17.1% References Gallant S." ></td>
	<td class="line x" title="169:194	(1991) A Practical Approach for Representing Context and for Performing Word Sense Disambiguation Using Neural Networks." ></td>
	<td class="line x" title="170:194	Neural Computation, 3/3, pp." ></td>
	<td class="line x" title="171:194	293-309 Leacock C. , Twell G. and Voorhees E." ></td>
	<td class="line x" title="172:194	(1993) Corpus-based Statistical Sense Resolution." ></td>
	<td class="line x" title="173:194	In Proceedings of the ARPA Human Language Technology Workshop, San Francisco, Morgan Kaufman, pp." ></td>
	<td class="line x" title="174:194	260-265 Li H. F., Heo N. W., Moon K. H., Lee J. H. and Lee G. B." ></td>
	<td class="line x" title="175:194	(2000) Lexical Transfer Ambiguity Resolution Using Automatically-Extracted Concept Co-occurrence Information." ></td>
	<td class="line x" title="176:194	International Journal of Computer Processing of Oriental Languages, 13/1, pp." ></td>
	<td class="line x" title="177:194	53-68 McRoy S." ></td>
	<td class="line x" title="178:194	(1992) Using Multiple Knowledge Sources for Word Sense Discrimination." ></td>
	<td class="line x" title="179:194	Computational Linguistics, 18/1, pp." ></td>
	<td class="line x" title="180:194	1-30 Mooney R." ></td>
	<td class="line x" title="181:194	(1996) Comparative Experiments on Disambiguating Word Senses: An Illustration of the Role of Bias in Machine Learning." ></td>
	<td class="line x" title="182:194	In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Philadelphia, PA, pp." ></td>
	<td class="line x" title="183:194	82-91 Ng, H. T. and Zelle J." ></td>
	<td class="line x" title="184:194	(1997) Corpus-Based Approaches to Semantic Interpretation in Natural Language Processing." ></td>
	<td class="line x" title="185:194	AI Magazine, 18/4, pp." ></td>
	<td class="line x" title="186:194	45-64 Ohno S. and Hamanishi M." ></td>
	<td class="line x" title="187:194	(1981) New Synonym Dictionary." ></td>
	<td class="line x" title="188:194	Kadokawa Shoten, Tokyo Smadja F." ></td>
	<td class="line x" title="189:194	(1993) Retrieving Collocations from Text: Xtract." ></td>
	<td class="line x" title="190:194	Computational Linguistics, 19/1, pp." ></td>
	<td class="line x" title="191:194	143-177 Waltz D. L. and Pollack J." ></td>
	<td class="line x" title="192:194	(1985) Massively Parallel Parsing: A Strongly Interactive Model of Natural Language Interpretation." ></td>
	<td class="line x" title="193:194	Cognitive Science, 9, pp." ></td>
	<td class="line x" title="194:194	51-74" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W02-2001
Extracting The Unextractable: A Case Study On Verb-Particles
Baldwin, Timothy;Villavicencio, Aline;"></td>
	<td class="line x" title="1:169	Extracting the Unextractable: A Case Study on Verb-particles Timothy Baldwin and Aline Villavicencioy  CSLI, Ventura Hall, Stanford University Stanford, CA 94305-4115 USA tbaldwin@csli.stanford.edu y University of Cambridge, Computer Laboratory, William Gates Building JJ Thomson Avenue, Cambridge CB3 OFD, UK Aline.Villavicencio@cl.cam.ac.uk Abstract This paper proposes a series of techniques for extracting English verbparticle constructions from raw text corpora." ></td>
	<td class="line x" title="2:169	We initially propose three basic methods, based on tagger output, chunker output and a chunk grammar, respectively, with the chunk grammar method optionally combining with an attachment resolution module to determine the syntactic structure of verbpreposition pairs in ambiguous constructs." ></td>
	<td class="line x" title="3:169	We then combine the three methods together into a single classifler, and add in a number of extra lexical and frequentistic features, producing a flnal F-score of 0.865 over the WSJ." ></td>
	<td class="line x" title="4:169	1 Introduction There is growing awareness of the pervasiveness and idiosyncrasy of multiword expressions (MWEs), and the need for a robust, structured handling thereof (Sag et al. , 2002; Calzolari et al. , 2002; Copestake et al. , 2002)." ></td>
	<td class="line x" title="5:169	Examples of MWEs are lexically flxed expressions (e.g. ad hoc), idioms (e.g. see double), light verb constructions (e.g. make a mistake) and institutionalised phrases (e.g. kindle excitement)." ></td>
	<td class="line x" title="6:169	MWEs pose a challenge to NLP due to their syntactic and semantic idiosyncrasies, which are often unpredictable from their component parts." ></td>
	<td class="line x" title="7:169	Largescale manual annotation of MWEs is infeasible due to their sheer volume (at least equivalent to the number of simplex words (Jackendofi, 1997)), productivity and domain-speciflcity." ></td>
	<td class="line x" title="8:169	Ideally, therefore, we would like to have some means of automatically extracting MWEs from a given domain or corpus, allowing us to pre-tune our grammar prior to deployment." ></td>
	<td class="line x" title="9:169	It is this task of extraction that we target in this paper." ></td>
	<td class="line x" title="10:169	This research represents a component of the LinGO multiword expression project,1 which is targeted at extracting, adequately handling and representing MWEs of all types." ></td>
	<td class="line x" title="11:169	As a research testbed and target resource to expand/domain-tune, we use the LinGO English Resource Grammar (LinGOERG), a linguistically-precise HPSG-based grammar under development at CSLI (Copestake and Flickinger, 2000; Flickinger, 2000)." ></td>
	<td class="line x" title="12:169	The particular MWE type we target for extraction is the English verb-particle construction." ></td>
	<td class="line x" title="13:169	Verb-particle constructions (\VPCs') consist of a 1http://lingo.stanford.edu/mwe head verb and one or more obligatory particles, in the form of intransitive prepositions (e.g. hand in), adjectives (e.g. cut short) or verbs (e.g. let go) (Villavicencio and Copestake, 2002a; Villavicencio and Copestake, 2002b; Huddleston and Pullum, 2002); for the purposes of this paper, we will focus exclusively on prepositional particles|by far the most common and productive of the three types| and further restrict our attention to single-particle VPCs (i.e. we ignore VPCs such as get along together)." ></td>
	<td class="line x" title="14:169	We deflne VPCs to optionally select for an NP complement, i.e. to occur both transitively (e.g. hand in the paper) and intransitively (e.g. battle on)." ></td>
	<td class="line oc" title="15:169	One aspect of VPCs that makes them dicult to extract (cited in, e.g., Smadja (1993)) is that the verb and particle can be non-contiguous, e.g. hand the paper in and battle right on." ></td>
	<td class="line x" title="16:169	This sets them apart from conventional collocations and terminology (see, e.g., Manning and Schutze (1999) and McKeown and Radev (2000)) in that they cannot be captured effectively using N-grams, due to the variability in the number and type of words potentially interceding between the verb and particle." ></td>
	<td class="line x" title="17:169	We are aiming for an extraction technique which is applicable to any raw text corpus, allowing us to tune grammars to novel domains." ></td>
	<td class="line x" title="18:169	Any linguistic annotation required during the extraction process, therefore, is produced through automatic means, and it is only for reasons of accessibility and comparability with other research that we choose to work over the Wall Street Journal section of the Penn Treebank (Marcus et al. , 1993)." ></td>
	<td class="line x" title="19:169	That is, other than in establishing upper bounds on the performance of the difierent extraction methods, we use only the raw text component of the treebank." ></td>
	<td class="line x" title="20:169	In this paper, we flrst outline distinguishing features of VPCs relevant to the extraction process (x 2)." ></td>
	<td class="line x" title="21:169	We then present and evaluate a number of simple methods for extracting VPCs based on, respectively, POS tagging (x 3), the output of a full text chunk parser (x 4), and a chunk grammar (x 5)." ></td>
	<td class="line x" title="22:169	Finally, we detail enhancements to the basic methods (x 6) and give a brief description of related research (x 7) before concluding the paper (x 8)." ></td>
	<td class="line x" title="23:169	2 Distinguishing Features of VPCs Here, we review a number of features of VPCs pertinent to the extraction task." ></td>
	<td class="line x" title="24:169	First, we describe linguistic qualities that characterise VPCs, and second we analyse the actual occurrence of VPCs in the WSJ." ></td>
	<td class="line x" title="25:169	2.1 Linguistic features Given an arbitrary verbpreposition pair, where the preposition is governed by the verb, a number of analyses are possible." ></td>
	<td class="line x" title="26:169	If the preposition is intransitive, a VPC (either intransitive or transitive) results." ></td>
	<td class="line x" title="27:169	If the preposition is transitive, it must select for an NP, producing either a prepositional verb (e.g. refer to) or a free verbpreposition combination (e.g. put it on the table, climb up the ladder)." ></td>
	<td class="line x" title="28:169	A number of diagnostics can be used to distinguish VPCs from both prepositional verbs and free verb preposition combinations (Huddleston and Pullum, 2002): 1." ></td>
	<td class="line x" title="29:169	transitive VPCs undergo the particle alternation 2." ></td>
	<td class="line x" title="30:169	with transitive VPCs, pronominal objects must be expressed in the \split' conflguration 3." ></td>
	<td class="line x" title="31:169	manner adverbs cannot occur between the verb and particle The flrst two diagnostics are restricted to transitive VPCs, while the third applies to both intransitive and transitive VPCs." ></td>
	<td class="line x" title="32:169	The flrst diagnostic is the canonical test for particlehood, and states that transitive VPCs take two word orders: the joined conflguration whereby the verb and particle are adjacent and the NP complement follows the particle (e.g. hand in the paper), and the split conflguration whereby the NP complement occurs between the verb and particle (e.g. hand the paper in)." ></td>
	<td class="line x" title="33:169	Note that prepositional verbs and free verbpreposition combinations can occur only in the joined conflguration (e.g. refer to the book vs. *refer the book to)." ></td>
	<td class="line x" title="34:169	Therefore, the existence of a verbpreposition pair in the split conflguration is sucient evidence for a VPC analysis." ></td>
	<td class="line x" title="35:169	It is important to realise that compatibility with the particle alternation is a sucient but not necessary condition on verbparticlehood." ></td>
	<td class="line x" title="36:169	That is, a small number of VPCs do not readily occur in the split conflguration, including carry out (a threat) (cf.?carry a threat out)." ></td>
	<td class="line x" title="38:169	The second diagnostic stipulates that pronominal NPs can occur only in the split conflguration (hand it in vs. *hand in it)." ></td>
	<td class="line x" title="39:169	Note also that heavy NPs tend to occur in the joined conflguration, and that various other factors interact to determine which conflguration a given VPC in context will occur in (see, e.g., Gries (2000))." ></td>
	<td class="line x" title="40:169	The third diagnostic states that manner adverbs cannot intercede between the verb and particle (e.g. *hand quickly the paper in)." ></td>
	<td class="line x" title="41:169	Note that this constraint is restricted to manner adverbs, and that there is a small set of adverbs which can pre-modify particles and hence occur between the verb and particle (e.g. well in jump well up)." ></td>
	<td class="line x" title="42:169	2.2 Corpus occurrence In order to get a feel for the relative frequency of VPCs in the corpus targeted for extraction, namely 0 5 10 15 20 25 30 35 40 0 10 20 30 40 50 60 70 VPC types (%) Corpus frequency Figure 1: Frequency distribution of VPCs in the WSJ Tagger correctextracted Prec Rec Ffl=1 Brill 135135 1.000 0.177 0.301 Penn 667800 0.834 0.565 0.673 Table 1: POS-based extraction results the WSJ section of the Penn Treebank, we took a random sample of 200 VPCs from the Alvey Natural Language Tools grammar (Grover et al. , 1993) and did a manual corpus search for each." ></td>
	<td class="line x" title="43:169	In the case that a VPC was found attested in the WSJ, we made a note of the frequency of occurrence as: (a) an intransitive VPC, (b) a transitive VPC in the joined conflguration, and (c) a transitive VPC in the split conflguration." ></td>
	<td class="line x" title="44:169	Of the 200 VPCs, only 62 were attested in the Wall Street Journal corpus (WSJ), at a mean token frequency of 5.1 and median token frequency of 2 (frequencies totalled over all 3 usages)." ></td>
	<td class="line x" title="45:169	Figure 1 indicates the relative proportion of the 62 attested VPC types which occur with the indicated frequencies." ></td>
	<td class="line x" title="46:169	From this, it is apparent that two-thirds of VPCs occur at most three times in the overall corpus, meaning that any extraction method must be able to handle extremely sparse data." ></td>
	<td class="line x" title="47:169	Of the 62 attested VPCs, 29 have intransitive usages and 45 have transitive usages." ></td>
	<td class="line x" title="48:169	Of the 45 attested transitive VPCs, 12 occur in both the joined and split conflgurations and can hence be unambiguously identifled as VPCs based on the flrst diagnostic from above." ></td>
	<td class="line x" title="49:169	For the remaining 33 transitive VPCs, we have only the joined usage, and must flnd some alternate means of ruling out a prepositional verb or free verbpreposition combination analysis." ></td>
	<td class="line x" title="50:169	Note that for the split VPCs, the mean number of words occurring between the verb and particle was 1.6 and the maximum 3." ></td>
	<td class="line x" title="51:169	In the evaluation of the various extraction techniques below, recall is determined relative to this limited set of 62 VPCs attested in the WSJ." ></td>
	<td class="line x" title="52:169	That is, recall is an indication of the proportion of the 62 VPCs contained within the set of extracted VPCs." ></td>
	<td class="line x" title="53:169	3 Method-1: Simple POS-based Extraction One obvious method for extracting VPCs is to run a simple regular expression over the output of a partof-speech (POS) tagger, based on the observation that the Penn Treebank POS tagset, e.g., contains a dedicated particle tag (RP)." ></td>
	<td class="line x" title="54:169	Given that all particles are governed by a verb, extraction consists of simply locating each particle and searching back (to the left of the particle, as particles cannot be passivised or otherwise extraposed) for the head verb of the VPC." ></td>
	<td class="line x" title="55:169	Here and for the subsequent methods, we assume that the maximum word length for NP complements in the split conflguration for transitive VPCs is 5,2 i.e. that an NP \heavier' than this would occur more naturally in the joined conflguration." ></td>
	<td class="line x" title="56:169	We thus discount all particles which are more than 5 words from their governing verb." ></td>
	<td class="line x" title="57:169	Additionally, we extracted a set of 73 canonical particles from the LinGO-ERG, and used this to fllter out extraneous particles in the POS data." ></td>
	<td class="line x" title="58:169	In line with our assumption of raw text to extract over, we use the Brill tagger (Brill, 1995) to automatically tag the WSJ, rather than making use of the manual POS annotation provided in the Penn Treebank." ></td>
	<td class="line x" title="59:169	We further lemmatise the data using morph (Minnen et al. , 2001) and extract VPCs based on the Brill tags." ></td>
	<td class="line x" title="60:169	This produces a total of 135 VPCs, which we evaluate according to the standard metrics of precision (Prec), recall (Rec) and F-score (Ffl=1)." ></td>
	<td class="line x" title="61:169	Note that here and for the remainder of this paper, precision is calculated according to the manual annotation for the combined total of 4,173 VPC candidate types extracted by the various methods described in this paper, whereas recall is relative to the 62 attested VPCs from the Alvey Tools data as described above." ></td>
	<td class="line x" title="62:169	As indicated in the flrst line of Table 1 (\Brill'), the simple POS-based method results in a precision of 1.000, recall of 0.177 and F-score of 0.301." ></td>
	<td class="line x" title="63:169	In order to determine the upper bound on performance for this method, we ran the extraction method over the original tagging from the Penn Treebank." ></td>
	<td class="line x" title="64:169	This resulted in an F-score of 0.774 (\Penn' in Table 1)." ></td>
	<td class="line x" title="65:169	The primary reason for the large disparity between the Brill tagger output and original Penn Treebank annotation is that it is notoriously dicult to difierentiate between particles, prepositions and adverbs (Toutanova and Manning, 2000)." ></td>
	<td class="line x" title="66:169	Over the WSJ, the Brill tagger achieves a modest tag recall of 0.103 for particles, and tag precision of 0.838." ></td>
	<td class="line x" title="67:169	That is, it is highly conservative in allocating particle tags, to the extent that it recognises only two particle types for the whole of the WSJ: out and down." ></td>
	<td class="line oc" title="68:169	4 Method-2: Simple Chunk-based Extraction To overcome the shortcomings of the Brill tagger in identifying particles, we next look to full chunk 2Note, this is the same as the maximum span length of 5 used by Smadja (1993), and above the maximum attested NP length of 3 from our corpus study (see Section 2.2)." ></td>
	<td class="line x" title="69:169	WSJ CoNLL Prec Rec Ffl=1 Prec Rec Ffl=1 0.889 0.911 0.900 0.912 0.925 0.919 Table 2: Chunking performance parsing." ></td>
	<td class="line x" title="70:169	Full chunk parsing involves partitioning up a text into syntactically-cohesive, head-flnal segments (\chunks'), without attempting to resolve inter-chunk dependencies." ></td>
	<td class="line x" title="71:169	In the chunk inventory devised for the CoNLL-2000 test chunking shared task (Tjong Kim Sang and Buchholz, 2000), a dedicated particle chunk type once again exists." ></td>
	<td class="line x" title="72:169	It is therefore possible to adopt an analogous approach to that from Method-1, in identifying particle chunks then working back to locate the verb each particle chunk is associated with." ></td>
	<td class="line x" title="73:169	4.1 Chunk parsing method In order to chunk parse the WSJ, we flrst tagged the full WSJ and Brown corpora using the Brill tagger, and then converted them into chunks based on the original Penn Treebank parse trees, with the aid of the conversion script used in preparing the CoNLL-2000 shared task data.3 We next lemmatised the data using morph (Minnen et al. , 2000), and chunk parsed the WSJ with TiMBL 4.1 (Daelemans et al. , 2001) using the Brown corpus as training data." ></td>
	<td class="line x" title="74:169	TiMBL is a memory-based classiflcation system based on the k-nearest neighbour algorithm, which takes as training data a set of flxed-length feature vectors pre-classifled according to an information fleld." ></td>
	<td class="line x" title="75:169	For each test instance described over the same feature vector, it then returns the \neighbours' at the k-nearest distances to the test instance and classifles the test instance according to the class distribution over those neighbours." ></td>
	<td class="line x" title="76:169	TiMBL provides powerful functionality for determining the relative distance between difierent values of a given feature in the form of MVDM, and also supports weighted voting between neighbours in classifying inputs, e.g. in the form of inverse distance weighting." ></td>
	<td class="line x" title="77:169	We ran TiMBL based on the feature set described in Veenstra and van den Bosch (2000), that is using the 5 word lemmata and POS tags to the left and 3 word lemmata and POS tags to the right of each focus word, along with the POS tag and lemma for the focus word." ></td>
	<td class="line x" title="78:169	We set k to 5, ran MVDM over only the POS tags4 and used inverse distance weighting, but otherwise ran TiMBL with the default settings." ></td>
	<td class="line x" title="79:169	We evaluated the basic TiMBL method over both the full WSJ data, training on the Brown section of the Penn Treebank, and over the original shared task data from CoNLL-2000, the results for which are presented in Table 2." ></td>
	<td class="line x" title="80:169	Note that, similarly to the CoNLL-2000 shared task, precision, recall and 3Note that the gold standard chunk data for the WSJ was used only in evaluation of chunking performance, and to establish upper bounds on the performance of the various extraction methods." ></td>
	<td class="line x" title="81:169	4Based on the results of Veenstra and van den Bosch (2000) and the observation that MVDM is temperamental over sparse data (i.e. word lemmata)." ></td>
	<td class="line x" title="82:169	Chunker correctextracted Prec Rec Ffl=1 TiMBL 695854 0.772 0.548 0.641 Penn 651760 0.857 0.694 0.766 Table 3: Chunk tag-based extraction results F-score are all evaluated at the chunk rather than the word level." ></td>
	<td class="line x" title="83:169	The F-score of 0.919 for the CoNLL2000 data is roughly the median score attained by systems performing in the original task, and slightly higher than the F-score of 0.915 reported by Veenstra and van den Bosch (2000), due to the use of word lemmata rather than surface forms, and also inverse distance weighting." ></td>
	<td class="line x" title="84:169	The reason for the dropofi in performance between the CoNLL data and the full WSJ is due to the CoNLL training and test data coming from a homogeneous data source, namely a subsection of the WSJ, but the Brown corpus being used as the training data in chunking the full extent of the WSJ." ></td>
	<td class="line x" title="85:169	4.2 Extraction method Having chunk-parsed the WSJ in the manner described above, we next set about extracting VPCs by identifying each particle chunk, and searching back for the governing verb." ></td>
	<td class="line x" title="86:169	As for Method-1, we allow a maximum of 5 words to intercede between a particle and its governing verb, and we apply the additional stipulation that the only chunks that can occur between the verb and the particle are: (a) noun chunks, (b) preposition chunks adjoining noun chunks, and (c) adverb chunks found in our closed set of particle pre-modiflers (see x 2.1)." ></td>
	<td class="line x" title="87:169	Additionally, we used the gold standard set of 73 particles to fllter out extraneous particle chunks, as for Method-1 above." ></td>
	<td class="line x" title="88:169	The results for chunk-based extraction are presented in Table 3, evaluated over the chunk parser output (\TiMBL') and also the gold-standard chunk data for the WSJ (\Penn')." ></td>
	<td class="line x" title="89:169	These results are signiflcantly better than those for Method-1 over the Brill output and Penn data, respectively, both in terms of the raw number of VPCs extracted and F-score." ></td>
	<td class="line x" title="90:169	One reason for the relative success of extracting over chunker as compared to tagger output is that our chunker was considerably more successful than the Brill tagger at annotating particles, returning an Fscore of 0.737 over particle chunks (precision=0.786, recall=0.693)." ></td>
	<td class="line x" title="91:169	The stipulations on particle type and what could occur between a verb and particle chunk were crucial in maintaining a high VPC extraction precision, relative to both particle chunk precision and the gold standard extraction precision." ></td>
	<td class="line x" title="92:169	As can be seen from the upper bound on recall (i.e. recall over the gold standard chunk data), however, this method has limited applicability." ></td>
	<td class="line x" title="93:169	5 Method-3: Chunk Grammar-based Extraction The principle weakness of Method-2 was recall, leading us to implement a rule-based chunk sequencer which searches for particles in prepositional and adverbial chunks as well as particle chunks." ></td>
	<td class="line x" title="94:169	In essence, Method correctextracted Prec Rec Ffl=1 Ruleatt 6761119 0.604 0.694 0.646 Timblatt 615823 0.747 0.661 0.702 Pennatt 694927 0.749 0.823 0.784 Rule+att 9513126 0.304 0.823 0.444 Timbl+att 7391049 0.704 0.710 0.707 Penn+att 7501079 0.695 0.871 0.773 Table 4: Chunk grammar-based extraction results we take each verb chunk in turn, and search to the right for a single-word particle, prepositional or adverbial chunk which is contained in the gold standard set of 73 particles." ></td>
	<td class="line x" title="95:169	For each such chunk pair, it then analyses: (a) the chunks which occur between them to ensure that, maximally, an NP and particle pre-modifler adverb chunk are found; (b) the chunks that occur immediately after the particle/preposition/adverb chunk to check for a clause boundary or NP; and (c) the clause context of the verb chunk for possible extraposition of an NP verbal complement, through passivisation or relativisation." ></td>
	<td class="line x" title="96:169	The objective of this analysis is to both determine the valence of the VPC candidate (intransitive or transitive) and identify evidence either supporting or rejecting a VPC analysis." ></td>
	<td class="line x" title="97:169	Evidence for or against a VPC analysis is in the form of congruence with the known linguistic properties of VPCs, as described in Section 2.1." ></td>
	<td class="line x" title="98:169	For example, if a pronominal noun chunk were found to occur immediately after the (possibly) particle chunk (e.g. *see ofi him), a VPC analysis would not be possible." ></td>
	<td class="line x" title="99:169	Alternatively, if a punctuation mark (e.g. a full stop) were found to occur immediately after the \particle' chunk and nothing interceded between the verb and particle chunk, then this would be evidence for an intransitive VPC analysis." ></td>
	<td class="line x" title="100:169	The chunk sequencer is not able to furnish positive or negative evidence for a VPC analysis in all cases." ></td>
	<td class="line x" title="101:169	Indeed, in a high proportion of instances, a noun chunk (=NP) was found to follow the \particle' chunk, leading to ambiguity between analysis as a VPC, prepositional verb or free verbpreposition combination (see Section 2.1), or in the case that an NP occurs between the verb and particle, the \particle' being the head of a PP post-modifying an NP." ></td>
	<td class="line x" title="102:169	As a case in point, the VP hand the paper in here could take any of the following structures: (1) hand [the paper] [in] [here] (transitive VPC hand in with adjunct NP here), (2) hand [the paper] [in here] (transitive prepositional verb hand in or simple transitive verb with PP adjunct), and (3) hand [the paper in here] (simple transitive verb)." ></td>
	<td class="line x" title="103:169	In such cases, we can choose to either (a) avoid committing ourselves to any one analysis, and ignore all such ambiguous cases, or (b) use some means to resolve the attachment ambiguity (i.e. whether the NP is governed by the verb, resulting in a VPC, or the preposition, resulting in a prepositional verb or free verbpreposition combination)." ></td>
	<td class="line x" title="104:169	In the latter case, we use an unsupervised attachment disambiguation method, based on the log-likelihood ratio (\LLR', Dunning (1993))." ></td>
	<td class="line x" title="105:169	That is, we use the chunker output to enumerate all the verbpreposition, preposition noun and verbnoun bigrams in the WSJ data, based on chunk heads rather than strict word bigrams." ></td>
	<td class="line x" title="106:169	We then use frequency data to pre-calculate the LLR for each such type." ></td>
	<td class="line x" title="107:169	In the case that the verb and \particle' are joined (i.e. no NP occurs between them), we simply compare the LLR of the verbnoun and particlenoun pairs, and assume a VPC analysis in the case that the former is strictly larger than the latter." ></td>
	<td class="line x" title="108:169	In the case that the verb and \particle' are split (i.e. we have the chunk sequence VC NC1 PC NC2),5 we calculate three scores: (1) the product of the LLR for (the heads of) VC-PC and VC-NC2 (analysis as VPC, with NC2 as an NP adjunct of the verb); (2) the product of the LLR for NC1-PC and PC-NC2 (transitive verb analysis, with the PP modifying NC1); and (3) the product of the LLR for VC-PC and PC-NC2 (analysis as prepositional verb or free verbpreposition combination)." ></td>
	<td class="line x" title="109:169	Only in the case that the flrst of these scores is strictly greater than the other two, do we favour a (transitive) VPC analysis." ></td>
	<td class="line x" title="110:169	Based on the positive and negative grammatical evidence from above, for both intransitive and transitive VPC analyses, we generate four frequencybased features." ></td>
	<td class="line x" title="111:169	The optional advent of data derived through attachment resolution, again for both intransitive and transitive VPC analyses, provides another two features." ></td>
	<td class="line x" title="112:169	These features can be combined in either of two ways: (1) in a rule-based fashion, where a given verbpreposition pair is extracted out as a VPC only in the case that there is positive and no negative evidence for either an intransitive or transitive VPC analysis (\Rule' in Table 4); and (2) according to a classifler, using TiMBL to train over the auto-chunked Brown data, with the same basic settings as for chunking (with the exception that each feature is numeric and MVDM is not used | results presented as \Timbl' in Table 4)." ></td>
	<td class="line x" title="113:169	We also present upper bound results for the classifler-based method using gold standard chunk data, rather than the chunker output (\Penn')." ></td>
	<td class="line x" title="114:169	For each of these three basic methods, we present results with and without the attachment-resolved data (\att')." ></td>
	<td class="line x" title="115:169	Based on the results in Table 4, the classifler-based method (\Timbl') is superior to not only the rulebased method (\Rule'), but also Method-1 and Method-2." ></td>
	<td class="line x" title="116:169	While the rule-based method degrades signiflcantly when the attachment data is factored in, the classifler-based method remains at the same basic F-score value, undergoing a drop in precision but equivalent gain in recall and gaining more than 120 correct VPCs in the process." ></td>
	<td class="line x" title="117:169	Rule+att returns the highest recall value of all the automatic methods to date at 0.823, at the cost of low precision at 0.304." ></td>
	<td class="line x" title="118:169	This points to the attachment disambiguation method having high recall but low precision." ></td>
	<td class="line x" title="119:169	Timblatt and Pennatt are equivalent in terms 5Here, VC = verb chunk, NC = noun chunk and PC = (intransitive or transitive) preposition chunk." ></td>
	<td class="line x" title="120:169	Method correctextracted Prec Rec Ffl=1 Combine 719953 0.754 0.710 0.731 M2 686778 0.882 0.677 0.766 M3att 684788 0.868 0.694 0.771 M3+att 8711020 0.854 0.823 0.838 Combine 10001164 0.859 0.871 0.865 CombinePenn 9311047 0.889 0.903 0.896 Table 5: Consolidated extraction results of precision, but the Penn data leads to considerably better recall." ></td>
	<td class="line x" title="121:169	6 Improving on the Basic Methods Comparing the results for the three basic methods, it is apparent that Method-1 and Method-2 ofier higher precision while Method-3 ofiers higher recall." ></td>
	<td class="line x" title="122:169	In order to capitalise on the respective strengths of the difierent methods, in this section, we investigate the possibility of combining the outputs of the four methods into a single consolidated classifler." ></td>
	<td class="line x" title="123:169	System combination is achieved by taking the union of all VPC outputs from all systems, and having a vector of frequency-based features for each, based on the outputs of the difierent methods for the VPC in question." ></td>
	<td class="line x" title="124:169	For each of Method-1 and Method-2, a single feature is used describing the total number of occurrences of the given VPC detected by that method." ></td>
	<td class="line x" title="125:169	For Method-3, we retain the 6 features used as input to Timblatt, namely the frequency with which positive and negative evidence was detected and also the frequency of VPCs detected through attachment resolution, for both intransitive and transitive VPCs." ></td>
	<td class="line x" title="126:169	Training data comes from the output of the difierent methods over the Brown corpus, and the chunking data for Method-2 and Method-3 was generated using the WSJ gold standard chunk data as training data, analogously to the method used to chunk parse the WSJ." ></td>
	<td class="line x" title="127:169	The result of this simple combination process is presented in the flrst line of Table 5 (\Combine')." ></td>
	<td class="line x" title="128:169	Encouragingly, we achieved the exact same recall as the best of the simple methods (Timbl+att) at 0.710, and signiflcantly higher F-score than any individual method at 0.731." ></td>
	<td class="line x" title="129:169	Steeled by this initial success, we further augment the feature space with features describing the frequency of occurrence of: (a) the particle in the corpus, and (b) deverbal noun and adjective forms of the VPC in the corpus (e.g. turnaround, dried-up), determined through a simple concatenation operation optionally inserting a hyphen." ></td>
	<td class="line x" title="130:169	The flrst of these is attempted to re ect the fact that high-frequency particles (e.g. up, over) are more productive (i.e. are found in novel VPCs more readily) than lowfrequency particles.6 The deverbal feature is intended to re ect the fact that VPCs have the po6We also experimented with a similar feature describing verb frequency, but found it to either degrade or have no efiect on classifler performance." ></td>
	<td class="line x" title="131:169	tential to undergo deverbalisation whereas prepositional verbs and free verbpreposition combinations do not.7 We additionally added in features describing: (a) the number of letters in the verb lemma, (b) the verb lemma, and (c) the particle lemma." ></td>
	<td class="line x" title="132:169	The flrst feature was intended to capture the informal observation that shorter verbs tend to be more productive than longer verbs (which ofiers one possible explanation for the anomalous call/ring/phone/*telephone up)." ></td>
	<td class="line x" title="133:169	The second and third features are intended to capture this same productivity efiect, but on a individual word-level." ></td>
	<td class="line x" title="134:169	Note that as TiMBL treats all features as fully independent, it is not able to directly pick up on the gold standard verbparticle pairs in the training data to select in the test data." ></td>
	<td class="line x" title="135:169	The expanded set of features was used to reevaluate each of: Method-2 (M2 in Table 5); the classifler version of Method-3 with and without attachment-resolved data (M3ATT); and the simple system combination method (Combine)." ></td>
	<td class="line x" title="136:169	Additionally, we calculated an upper bound for the expanded feature set based on the gold standard data for each of the methods (CombinePenn in Table 5)." ></td>
	<td class="line x" title="137:169	The results for these flve consolidated methods are presented in Table 5." ></td>
	<td class="line x" title="138:169	The addition of the 7 new features leads to an appreciable gain in both precision and recall for all methods, with the system combination method once again proving to be the best performer, at an F-score of 0.865." ></td>
	<td class="line x" title="139:169	The difierential between the system combination method when trained over auto-generated POS and chunk data (Combine) and that trained over gold standard data (CombinePenn) is still tangible, but considerably less than for any of the individual methods." ></td>
	<td class="line x" title="140:169	Importantly, Combine outperforms the gold standard results for each of the individual methods." ></td>
	<td class="line x" title="141:169	Examples of false positives (i.e. verbprepositions misclassifled as VPCs) returned by this flnal system conflguration are flrm away, base on and very ofi." ></td>
	<td class="line x" title="142:169	In Section 1, we made the claim that VPCs are highly productive and domain-speciflc." ></td>
	<td class="line x" title="143:169	We validate this claim by comparing the 1000 VPCs correctly extracted by the Combine method against both the LinGO-ERG and the relatively broad-coverage Alvey Tools VPC inventory." ></td>
	<td class="line x" title="144:169	The 28 March, 2002 version of the LinGO-ERG contains a total of 300 intransitive and transitive VPC types, of which 195 were contained in the 1000 correctly-extracted VPCs." ></td>
	<td class="line x" title="145:169	Feeding the remaining 805 VPCs into the grammar (with a lexical type describing their transitivity) would therefore result in an almost fourfold increase in the total number of VPCs, and increase the chances of the grammar being able to parse WSJ-style text." ></td>
	<td class="line x" title="146:169	The Alvey Tools data contains a total of 2254 VPC types." ></td>
	<td class="line x" title="147:169	Of the 1000 extracted VPCs, 284 or slightly over 28%, were not contained in the Alvey data, with examples including head down, blend together and bid up." ></td>
	<td class="line x" title="148:169	Combining this result with that for the LinGO-ERG, one can 7Note that only a limited number of VPCs can be deverbalised in this manner: of the 62 VPCs attested in the WSJ, only 8 had a deverbal usage." ></td>
	<td class="line x" title="149:169	see that we are not simply extracting information already at our flngertips, but are accessing signiflcant numbers of novel VPC types." ></td>
	<td class="line x" title="150:169	7 Related research There is a moderate amount of research related to the extraction of VPCs, or more generally phrasal verbs, which we brie y describe here." ></td>
	<td class="line oc" title="151:169	One of the earliest attempts at extracting \interrupted collocations' (i.e. non-contiguous collocations, including VPCs), was that of Smadja (1993)." ></td>
	<td class="line o" title="152:169	Smadja based his method on bigrams, but unlike conventional collocation work, described bigrams by way of the triple of hword1,word2,posni, where posn is the number of words occurring between word1 and word2 (up to 4)." ></td>
	<td class="line x" title="153:169	For VPCs, we can reasonably expect from 0 to 4 words to occur between the verb and the particle, leading to 5 distinct variants of the same VPC and no motivated way of selecting between them." ></td>
	<td class="line n" title="154:169	Smadja did not attempt to evaluate his method other than anecdotally, making any comparison with our research impossible." ></td>
	<td class="line x" title="155:169	The work of Blaheta and Johnson (2001) is closer in its objectives to our research, in that it takes a parsed corpus and extracts out multiword verbs (i.e. VPCs and prepositional verbs) through the use of log-linear models." ></td>
	<td class="line x" title="156:169	Once again, direct comparison with our results is dicult, as Blaheta and Johnson output a ranked list of all verbpreposition pairs, and subjectively evaluate the quality of difierent sections of the list." ></td>
	<td class="line x" title="157:169	Additionally, they make no attempt to distinguish VPCs from prepositional verbs." ></td>
	<td class="line x" title="158:169	The method which is perhaps closest to ours is that of Kaalep and Muischnek (2002) in extracting Estonian multiword verbs (which are similar to English VPCs in that the components of the multiword verb can be separated by other words)." ></td>
	<td class="line o" title="159:169	Kaalep and Muischnek apply the \mutual expectation' test over a range of \positioned bigrams', similar to those used by Smadja." ></td>
	<td class="line x" title="160:169	They test their method over three difierent corpora, with results ranging from a precision of 0.21 and recall of 0.86 (F-score=0.34) for the smallest corpus, to a precision of 0.03 and recall of 0.85 (F-score=0.06) for the largest corpus." ></td>
	<td class="line x" title="161:169	That is, high levels of noise are evident in the system output, and the F-score values are well below those achieved by our method for English VPCs." ></td>
	<td class="line x" title="162:169	8 Conclusion In conclusion, this paper has been concerned with the extraction of English verbparticle constructions from raw text corpora." ></td>
	<td class="line x" title="163:169	Three basic methods were proposed, based on tagger output, chunker output and a chunk grammar; the chunk grammar method was optionally combined with attachment resolution to determine the syntactic structure of verbpreposition pairs in ambiguous constructs." ></td>
	<td class="line x" title="164:169	We then experimented with combining the output of the three methods together into a single classifler, and further complemented the feature space with a number of lexical and frequentistic features, culminating in an F-score of 0.865 over the WSJ." ></td>
	<td class="line x" title="165:169	It is relatively simple to adapt the methods described here to output subcategorisation types, rather than a binary judgement on verb particlehood." ></td>
	<td class="line x" title="166:169	This would allow the extracted output to be fed directly into the LinGO-ERG for use in parsing." ></td>
	<td class="line x" title="167:169	We are also interested in extending the method to extract prepositional verbs, many of which appear in the attachment resolution data and are subsequently flltered out by the consolidated classifler." ></td>
	<td class="line x" title="168:169	Acknowledgements This research was supported in part by NSF grant BCS-0094638 and also the Research Collaboration between NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation and CSLI, Stanford University." ></td>
	<td class="line x" title="169:169	We would like to thank Francis Bond, Ann Copestake, Dan Flickinger, Diana McCarthy and the three anonymous reviewers for their valuable input on this research." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W03-1705
A Bottom-Up Merging Algorithm For Chinese Unknown Word Extraction
Ma, Wei-Yun;Chen, Keh-Jiann;"></td>
	<td class="line x" title="1:207	A Bottom-up Merging Algorithm for Chinese Unknown Word Extraction Wei-Yun Ma Institute of Information science, Academia Sinica ma@iis.sinica.edu.tw Keh-Jiann Chen Institute of Information science, Academia Sinica kchen@iis.sinica.edu.tw Abstract Statistical methods for extracting Chinese unknown words usually suffer a problem that superfluous character strings with strong statistical associations are extracted as well." ></td>
	<td class="line x" title="2:207	To solve this problem, this paper proposes to use a set of general morphological rules to broaden the coverage and on the other hand, the rules are appended with different linguistic and statistical constraints to increase the precision of the representation." ></td>
	<td class="line x" title="3:207	To disambiguate rule applications and reduce the complexity of the rule matching, a bottom-up merging algorithm for extraction is proposed, which merges possible morphemes recursively by consulting above the general rules and dynamically decides which rule should be applied first according to the priorities of the rules." ></td>
	<td class="line x" title="4:207	Effects of different priority strategies are compared in our experiment, and experimental results show that the performance of proposed method is very promising." ></td>
	<td class="line x" title="5:207	1 Introduction and Related Work Chinese sentences are strings of characters with no delimiters to mark word boundaries." ></td>
	<td class="line x" title="6:207	Therefore the initial step for Chinese processing is word segmentation." ></td>
	<td class="line x" title="7:207	However, occurrences of unknown words, which do not listed in the dictionary, degraded significantly the performances of most word segmentation methods, so unknown word extraction became a key technology for Chinese segmentation." ></td>
	<td class="line x" title="8:207	For unknown words with more regular morphological structures, such as personal names, morphological rules are commonly used for improving the performance by restricting the structures of extracted words (Chen et." ></td>
	<td class="line x" title="9:207	al 1994, Sun et." ></td>
	<td class="line x" title="10:207	al 1994, Lin et." ></td>
	<td class="line x" title="11:207	al 1994)." ></td>
	<td class="line x" title="12:207	However, it's not possible to list morphological rules for all kinds of unknown words, especially those words with very irregular structures, which have the characteristics of variable lengths and flexible morphological structures, such as proper names, abbreviations etc. Therefore, statistical approaches usually play major roles on irregular unknown word extraction in most previous work (Sproat & Shih 1990, Chiang et." ></td>
	<td class="line x" title="13:207	al 1992, Tung and Lee 1995, Palmer 1997, Chang et." ></td>
	<td class="line x" title="14:207	al 1997, Sun et." ></td>
	<td class="line x" title="15:207	al 1998, Ge et." ></td>
	<td class="line x" title="16:207	al 1999)." ></td>
	<td class="line x" title="17:207	For statistical methods, an important issue is how to resolve competing ambiguous extractions which might include erroneous extractions of phrases or partial phrases." ></td>
	<td class="line x" title="18:207	They might have statistical significance in a corpus as well." ></td>
	<td class="line x" title="19:207	Very frequently superfluous character strings with strong statistic associations are extracted." ></td>
	<td class="line x" title="20:207	These wrong results are usually hard to be filtered out unless deep content and context analyses were performed." ></td>
	<td class="line x" title="21:207	To solve this problem, the idea of unknown word detection procedure prior to extraction is proposed." ></td>
	<td class="line x" title="22:207	Lin et al.(1993) adopt the following strategy: First, they decide whether there is any unknown word within a detected region with fix size in a sentence, and then they extract the unknown word from the region by a statistical method if the previous answer is 'yes'." ></td>
	<td class="line x" title="24:207	A limitation of this method is that it restricts at most one unknown word occurs in the detected region, so that it could not deal with occurrences of consecutive unknown words within a sentence." ></td>
	<td class="line x" title="25:207	Chen & Ma (2002) adopt another strategy: After an initial segmentation process, each monosyllable is decided whether it is a common word or a morpheme of unknown word by a set of syntactic discriminators." ></td>
	<td class="line x" title="26:207	The syntactic discriminators are a set of syntactic patterns containing monosyllabic, words which are learned from a large word segmented corpus, to discriminate between monosyllabic words and morphemes of unknown words." ></td>
	<td class="line x" title="27:207	Then more deep analysis can be carried out at the detected unknown word morphemes to extract unknown words." ></td>
	<td class="line x" title="28:207	In this paper, in order to avoid extractions of superfluous character strings with high frequencies, we proposed to use a set of general rules, which is formulated as a context free grammar rules of composing detected morphemes and their adjacent tokens, to match all kinds of unknown words, for instance which includes the rule of (UW" ></td>
	<td class="line x" title="29:207	UW UW)." ></td>
	<td class="line x" title="30:207	To avoid too much superfluous extractions caused by the over general rules, rules are appended with linguistic or statistical constraints." ></td>
	<td class="line x" title="31:207	To disambiguate between rule applications and reduce the complexity of the rule matching, a bottom-up merging algorithm for extraction is proposed, which merges possible morphemes recursively by consulting above general rules and dynamically decides which rule should be applied first according to the priorities of the rules." ></td>
	<td class="line x" title="32:207	The paper is organized into 7 sections." ></td>
	<td class="line x" title="33:207	In the next section, we provide an overview of our system." ></td>
	<td class="line x" title="34:207	Section 3 briefly introduce unknown word detection process and makes some analysis for helping the derivation of general rules for unknown words." ></td>
	<td class="line x" title="35:207	In section 4, we derive a set of general rules to represent all kinds of unknown words, and then modify it by appending rules constraints and priorities." ></td>
	<td class="line x" title="36:207	In section 5, a bottom-up merging algorithm is presented for unknown word extraction." ></td>
	<td class="line x" title="37:207	In section 6, the evaluation of extraction is presented; we also compare the performances to different priority strategies." ></td>
	<td class="line x" title="38:207	Finally, in section 7, we make the conclusion and propose some future works." ></td>
	<td class="line x" title="39:207	2 System Overview The purpose to our unknown word extraction system is to online extract all types of unknown words from a Chinese text." ></td>
	<td class="line x" title="40:207	Figure 1 illustrates the block diagram of the system proposed in this paper." ></td>
	<td class="line x" title="41:207	Initially, the input sentence is segmented by a conventional word segmentation program." ></td>
	<td class="line x" title="42:207	As a result, each unknown word in the sentence will be segmented into several adjacent tokens (known words or monosyllabic morphemes)." ></td>
	<td class="line x" title="43:207	At unknown word detection stage, every monosyllable is decided whether it is a word or an unknown word morpheme by a set of syntactic discriminators, which are learned from a corpus." ></td>
	<td class="line x" title="44:207	Afterward, a bottom-up merging process applies the general rules to extract unknown word candidates." ></td>
	<td class="line x" title="45:207	Finally, the input text is re-segmented by consulting the system dictionary and the extracted unknown word candidates to get the final segmented result." ></td>
	<td class="line x" title="46:207	Figure 1." ></td>
	<td class="line x" title="47:207	Flowchart of the system (1) a0 a1 a2a4a3 a5a4a6a8a7 if can increase gross profit rate 'if gross profit rate can be increased' (2) after first step word segmentation: a0 a1 a2a4a3 a5 a6 a7 after unknown word detection: a0 a1 a2a4a3 a5 ()?" ></td>
	<td class="line x" title="48:207	a6 ()?" ></td>
	<td class="line x" title="49:207	a7 ()?" ></td>
	<td class="line x" title="50:207	after unknown word extraction: a0 a1 a2a4a3 a5a4a6a4a7 For example, the correct segmentation of (1) is shown, but the unknown word a5a9a6a10a7  is segmented into three monosyllabic words after the first step of word segmentation process as shown in (2)." ></td>
	<td class="line x" title="51:207	The unknown word detection process will mark the sentence as a0 () a1 () a2a8a3 () a5 ()?" ></td>
	<td class="line x" title="52:207	a6 ()?" ></td>
	<td class="line x" title="53:207	a7 (?), where ()?" ></td>
	<td class="line x" title="54:207	denotes the detected monosyllabic unknown word morpheme and () denotes common words." ></td>
	<td class="line x" title="55:207	During extracting process, the rule matching process focuses on the morphemes marked with ()?" ></td>
	<td class="line x" title="56:207	only and tries to combine them with left/right neighbors according to the rules for unknown words." ></td>
	<td class="line x" title="57:207	After that, the unknown word a5a4a6a8a7  is extracted." ></td>
	<td class="line x" title="58:207	During the process, we do not need to take care of other superfluous combinations such as a0 a1  even though they might have strong statistical association or co-occurrence too." ></td>
	<td class="line x" title="59:207	3 Analysis of Unknown Word Detection The unknown word detection method proposed by (Chen & Bai 1998) is applied in our system." ></td>
	<td class="line x" title="60:207	It adopts a corpus-based learning algorithm to derive a set of syntactic discriminators, which are used to distinguish whether a monosyllable is a word or an unknown word morpheme after an initial segmentation process." ></td>
	<td class="line x" title="61:207	If all occurrences of monosyllabic words are considered as morphemes of unknown words, the recall of the detection will be about 99%, but the precision is as low as 13.4%." ></td>
	<td class="line x" title="62:207	The basic idea in (Chen & Bai 1998) is that the complementary problem of unknown word detection is the problem of monosyllabic knownword detection, i.e. to remove the monosyllabic known-words as the candidates of unknown word morphemes." ></td>
	<td class="line x" title="63:207	Chen and Bai (1998) adopt ten types of context rule patterns, as shown in table 1, to generate rule instances from a training corpus." ></td>
	<td class="line x" title="64:207	The generated rule instances were checked for applicability and accuracy." ></td>
	<td class="line x" title="65:207	Each rule contains a key token within curly brackets and its contextual tokens without brackets." ></td>
	<td class="line x" title="66:207	For some rules there may be no contextual dependencies." ></td>
	<td class="line x" title="67:207	The function of each rule means that in a sentence, if a character and its context match the key token and the contextual tokens of the rule respectively, this character is a common word (i.e. not a morpheme of unknown word)." ></td>
	<td class="line x" title="68:207	For instance, the rule {Dfa} Vh says that a character with syntactic category Dfa is a common word, if it follows a word of syntactic category Vh." ></td>
	<td class="line x" title="69:207	Rule type Example ================================= char {a0 } word char a1 {a2 } char word {a3 } a4a6a5 category {T} {category} category {Dfa} Vh category {category} Na {Vcl} char category {a7 } VH category char Na {a8 } category category char Na Dfa {a9 } char category category {a10 } Vh T ================================= Table1." ></td>
	<td class="line x" title="70:207	Rule types and Examples The final rule set contains 45839 rules and were used to detect unknown words in the experiment." ></td>
	<td class="line x" title="71:207	It achieves a detection rate of 96%, and a precision of 60%." ></td>
	<td class="line x" title="72:207	Where detection rate 96% means that for 96% of unknown words in the testing data, at least one of its morphemes are detected as part of unknown word and the precision of 60% means that for 60% of detected monosyllables in the testing data, are actually morphemes." ></td>
	<td class="line x" title="73:207	Although the precision is not high, most of over-detecting errors are isolated, which means there are few situations that two adjacent detected monosyllabic unknown morphemes are both wrong at the mean time." ></td>
	<td class="line x" title="74:207	These operative characteristics are very important for helping the design of general rules for unknown words later." ></td>
	<td class="line x" title="75:207	4 Rules for Unknown Words Although morphological rules work well in regular unknown word extraction, it's difficult to induce morphological rules for irregular unknown words." ></td>
	<td class="line x" title="76:207	In this section, we try to represent a common structure for unknown words from another point of view; an unknown word is regarded as the combination of morphemes which are consecutive morphemes/words in context after segmentation, most of which are monosyllables." ></td>
	<td class="line x" title="77:207	We adopt context free grammar (Chomsky 1956), which is the most commonly used generative grammar for modelling constituent structures, to express our unknown word structure." ></td>
	<td class="line x" title="78:207	4.1 Rule Derivation According to the discussion in section 3, for 96% of unknown words, at least one of its morphemes are detected as part of unknown word, which motivates us to represent the unknown word structure with at least one detected morpheme." ></td>
	<td class="line x" title="79:207	Taking this phenomenon into our consideration, the rules for modeling unknown words and an unknown word example are presented as follows." ></td>
	<td class="line x" title="80:207	UW" ></td>
	<td class="line x" title="81:207	UW UW (1) | ms()?" ></td>
	<td class="line x" title="82:207	ms()?" ></td>
	<td class="line x" title="83:207	(2) | ms()?" ></td>
	<td class="line x" title="84:207	ps() (3) | ms()?" ></td>
	<td class="line x" title="85:207	ms() (4) | ps() ms()?" ></td>
	<td class="line x" title="86:207	(5) | ms() ms()?" ></td>
	<td class="line x" title="87:207	(6) | ms()?" ></td>
	<td class="line x" title="88:207	UW (7) | ms() UW (8) | ps() UW (9) | UW ms()?" ></td>
	<td class="line x" title="89:207	(10) | UW ms() (11) | UW ps() (12) Notes: There is one non-terminal symbol." ></td>
	<td class="line x" title="90:207	UW denotes unknown word and is also the start symbol." ></td>
	<td class="line x" title="91:207	There are three terminal symbols, which includes ms(?), which denotes the detected monosyllabic unknown word morpheme, ms(), which denotes the monosyllable that is not detected as the morpheme, and ps(), which denotes polysyllabic (more than one syllable) known word." ></td>
	<td class="line x" title="92:207	Table 2." ></td>
	<td class="line x" title="93:207	General rules for unknown words Figure 2." ></td>
	<td class="line x" title="94:207	A possible structure for the unknown word  a0a2a1a4a3 (Chen Zhi Ming), which is segmented initially and detected as a0 ()?" ></td>
	<td class="line x" title="95:207	a1 ()?" ></td>
	<td class="line x" title="96:207	a3 (), and a3  was marked incorrectly at detection stage." ></td>
	<td class="line x" title="97:207	There are three kinds of commonly used measures applied to evaluate grammars: 1." ></td>
	<td class="line x" title="98:207	generality (recall), the range of sentences the grammar analyzes correctly; 2." ></td>
	<td class="line x" title="99:207	selectivity (precision), the range of non-sentences it identifies as problematic and 3." ></td>
	<td class="line x" title="100:207	understandability, the simplicity of the grammar itself (Allen 1995)." ></td>
	<td class="line x" title="101:207	For generality, 96% unknown words have this kind of structure, so the grammar has high generality to generate unknown words." ></td>
	<td class="line x" title="102:207	But for selectivity, our rules are over-generation." ></td>
	<td class="line x" title="103:207	Many patterns accepted by the rules are not words." ></td>
	<td class="line x" title="104:207	The main reason is that rules have to include nondetected morphemes for high generality." ></td>
	<td class="line x" title="105:207	Therefore selectivity is sacrificed momentary." ></td>
	<td class="line x" title="106:207	In next section, rules would be constrained by linguistic and textbased statistical constraints to compensate the selectivity of the grammar." ></td>
	<td class="line x" title="107:207	For understandability, you can find each rule in (1)-(12) consists of just two right-hand side symbols." ></td>
	<td class="line x" title="108:207	The reason for using this kind of presentation is that it regards the unknown word structure as a series of combinations of consecutive two morphemes, such that we could simplify the analysis of unknown word structure by only analyzing its combinations of consecutive two morphemes." ></td>
	<td class="line x" title="109:207	4.2 Appending Constraints Since the general rules in table 2 have high generality and low selectivity to model unknown words, we append some constraints to restrict their applications." ></td>
	<td class="line x" title="110:207	However, there are tradeoffs between generality and selectivity: higher selectivity usually results in lower generality." ></td>
	<td class="line x" title="111:207	In order to keep high generality while assigning constraints, we assign different constraints on different rules according to their characteristics, such that it is only degraded generality slightly but selectivity being upgraded significantly." ></td>
	<td class="line x" title="112:207	The rules in table 2 are classified into two kinds: one kind is the rules which both its right-hand side symbols consist of detected morphemes, i.e, (1), (2), (7), and (10), the others are the rules that just one of its right-hand side symbols consists of detected morphemes, i.e, (3), (4), (5), (6), (8), (9), (11), and (12)." ></td>
	<td class="line x" title="113:207	The former is regarded as strong structure since they are considered to have more possibility to compose an unknown word or an unknown word morpheme and the latter is regarded as weak structure, which means they are considered to have less possibility to compose an unknown word or an unknown word morpheme." ></td>
	<td class="line x" title="114:207	The basic idea is to assign more constraint on those rules with weak structure and less constraint on those rules with strong structure." ></td>
	<td class="line x" title="115:207	The constraints we applied include word length, linguistic and statistical constraints." ></td>
	<td class="line x" title="116:207	For statistical constraints, since the target of our system is to extract unknown words from a text, we use textbased statistical measure as the statistical constraint." ></td>
	<td class="line x" title="117:207	It is well known that keywords often reoccur in a document (Church 2000) and very possible the keywords are also unknown words." ></td>
	<td class="line x" title="118:207	Therefore the reoccurrence frequency within a document is adopted as the constraint." ></td>
	<td class="line x" title="119:207	Another useful statistical phenomenon in a document is that a polysyllabic morpheme is very unlikely to be the morphemes of two different unknown words within the same text." ></td>
	<td class="line x" title="120:207	Hence we restrict the rule with polysyllabic symbols by evaluating the conditional probability of polysyllabic symbols." ></td>
	<td class="line x" title="121:207	In addition, syntactic constraints are also utilized here." ></td>
	<td class="line x" title="122:207	For most of unknown word morphemes, their syntactic categories belong to bound, verb, noun, and adjective instead of conjunction, prepositionetc. So we restrict the rule with non-detected symbols by checking whether syntactic categories of its non-detected symbols belong to bound, verb, noun, or adjective." ></td>
	<td class="line x" title="123:207	To avoid unlimited recursive rule application, the length of matched unknown word is restricted unless very strong statistical association do occur between two matched tokens." ></td>
	<td class="line x" title="124:207	The constraints adopted so far are presented in table 3." ></td>
	<td class="line x" title="125:207	Rules might be restricted by multiconstraints." ></td>
	<td class="line x" title="126:207	Freqdocu(LR)>=Threshold (3) (4) (5) (6) (8) (9) (11) (12) Pdocu(L|R)=1 (1) (3) (7) (8) (9) (12) Pdocu(R|L)=1 (1) (5) (9) (10) (11) (12) Category(L) is bound, verb, noun or adjective (5) (6) (8) (9) Category(R) is bound, verb, noun or adjective (3) (4) (11) (12) Notes: L denotes left terminal of right-hand side R denotes right terminal of right-hand side Threshold is a function of Length(LR) and text size." ></td>
	<td class="line x" title="127:207	The basic idea is larger amount of length(LR) or text size matches larger amount of Threshold." ></td>
	<td class="line x" title="128:207	Table 3." ></td>
	<td class="line x" title="129:207	Constraints for general rules 4.3 Priority To scheduling and ranking ambiguous rule matching, each step of rule matching is associated with a measure of priority which is calculated by the association strength of right-hand side symbols." ></td>
	<td class="line x" title="130:207	In our extracting algorithm, the priority measure is used to help extracting process dynamically decide which rule should be derived first." ></td>
	<td class="line x" title="131:207	More detail discussion about ambiguity problem and complete disambiguation process are presented in section 5." ></td>
	<td class="line x" title="132:207	We regard the possibility of a rule application as co-occurrence and association strength of its righthand side symbols within a text." ></td>
	<td class="line x" title="133:207	In other words, a rule has higher priority of application while its right-hand side symbols are strongly associated with each other, or co-occur frequently in the same text." ></td>
	<td class="line oc" title="134:207	There have been many statistical measures which estimate co-occurrence and the degree of association in previous researches, such as mutual information (Church 1990, Sporat 1990), t-score (Church 1991), dice matrix (Smadja 1993, 1996)." ></td>
	<td class="line x" title="135:207	Here, we adopt four well-developed kinds of statistical measures as our priority individually: mutual information (MI), a variant of mutual information (VMI), t-score, and co-occurrence." ></td>
	<td class="line x" title="136:207	The formulas are listed in table 4." ></td>
	<td class="line x" title="137:207	MI mainly focuses on association strength, and VMI and tscore consider both co-occurrence and association strength." ></td>
	<td class="line x" title="138:207	The performances of these four measures are evaluated in our experiments discussed in section 6." ></td>
	<td class="line x" title="139:207	==================================== ),(),( RLfRLoccurenceco = ------------------------------------------------------------)()( ),(log),( RPLP RLPRLMI = ------------------------------------------------------------),(),(),( RLMIRLfRLVMI = ------------------------------------------------------------),( )()(),( ),( RLf N RfLfRLf RLscoret  = Notes: f(L,R) denotes the number of occurrences of L,R in the text; N denotes the number of occurrences of all the tokens in the text; length(*) denotes the length of *." ></td>
	<td class="line x" title="140:207	==================================== Table 4." ></td>
	<td class="line x" title="141:207	Formulas of 4 kinds of priority 5 Unknown Word Extraction 5.1 Ambiguity Even though the general rules are appended with well-designed constraints, ambiguous matchings, such as, overlapping and covering, are still existing." ></td>
	<td class="line x" title="142:207	We take the following instance to illustrate that: a0a2a1a4a3  (La Fa Yeh), a warship name, occurs frequently in the text and is segmented and detected as a0 ()?" ></td>
	<td class="line x" title="143:207	a1 ()?" ></td>
	<td class="line x" title="144:207	a3 (?)." ></td>
	<td class="line x" title="145:207	Although a0a5a1 a3  could be derived as an unknown word ((a0a6a1 ) a3 ) by rule 2 and rule 10, a0a5a1  and a1a4a3  might be also derived as unknown words (a0a6a1 ) and (a1a2a3 ) individually by the rule 2." ></td>
	<td class="line x" title="146:207	Hence there are total three possible ambiguous unknown words and only one is actually correct." ></td>
	<td class="line x" title="147:207	Several approaches on unsupervised segmentation of Chinese words were proposed to solve overlapping ambiguity to determine whether to group xyz as xy z or x yz, where x, y, and z are Chinese characters." ></td>
	<td class="line x" title="148:207	Sproat and Shih (1990) adopt a greedy algorithm: group the pair of adjacent characters with largest mutual information greater than some threshold within a sentence, and the algorithm is applied recursively to the rest of the sentence until no character pair satisfies the threshold." ></td>
	<td class="line x" title="149:207	Sun et al.(1998) use various association measures such as t-score besides mutual information to improve (Sproat & Shih 1990)." ></td>
	<td class="line x" title="151:207	They developed an efficient algorithm to solve overlapping character pair ambiguity." ></td>
	<td class="line x" title="152:207	5.2 Bottom-up Merging Algorithm Following the greedy strategy of (Sproat & Shih 1990), here we present an efficient bottom-up merging algorithm consulting the general rules to extract unknown words." ></td>
	<td class="line x" title="153:207	The basic idea is that for a segmented sentence, if there are many rulematched token pairs which also satisfy the rule constraints, the token pair with the highest rule priority within the sentence is merged first and forms a new token string." ></td>
	<td class="line x" title="154:207	Same procedure is then applied to the updated token string recursively until no token pair satisfied the general rules." ></td>
	<td class="line x" title="155:207	It is illustrated by the following example: ====================================== System environment: Co-occurrence priority is adopted." ></td>
	<td class="line x" title="156:207	Text environment: a0 a1a8a7  (Chen Zhi Qiang), an unknown word, occurs three times." ></td>
	<td class="line x" title="157:207	a9a11a10  (take an electing activity), an unknown word, occurs two times." ></td>
	<td class="line x" title="158:207	a0 a1a12a7a8a9a12a10  (Chen Zhi Qiang took an electing activity), a sentence, occurs one time." ></td>
	<td class="line x" title="159:207	Input: a0 a1a11a7a13a9a12a10 After initial segmentation and detection: a0 ()?" ></td>
	<td class="line x" title="160:207	a1 ()?" ></td>
	<td class="line x" title="161:207	a7 ()?" ></td>
	<td class="line x" title="162:207	a9 ()?" ></td>
	<td class="line x" title="163:207	a10 ()?" ></td>
	<td class="line x" title="164:207	3 3 1 2 priority After first iteration: a0 a1 (uw) a7 ()?" ></td>
	<td class="line x" title="165:207	a9 ()?" ></td>
	<td class="line x" title="166:207	a10 ()?" ></td>
	<td class="line x" title="167:207	3 1 2 priority After second iteration: a0 a1a11a7 (uw) a9 ()?" ></td>
	<td class="line x" title="168:207	a10 ()?" ></td>
	<td class="line x" title="169:207	2 priority After third iteration: a0 a1a11a7 (uw) a9a11a10 (uw) ===================================== Figure 3." ></td>
	<td class="line x" title="170:207	Extraction process of input a0 a1a11a7a14a9a11a10 ." ></td>
	<td class="line x" title="171:207	By the general rules and greedy strategy, besides overlapping character pair ambiguity, the algorithm is able to deal with more complex overlapping and coverage ambiguity, even which result from consecutive unknown words." ></td>
	<td class="line x" title="172:207	In finger 3, input sentence a15a17a16a6a18a20a19a22a21  is derived as the correct two unknown words ((a15a17a16 )a18 ) and (a19 a21 ) by rule (2), rule (10), and rule (2) in turn." ></td>
	<td class="line x" title="173:207	a15 a16a6a18  and a19a23a21  are not further merged." ></td>
	<td class="line x" title="174:207	That is because P(a19a24a21 |a15a25a16a24a18 )<1 violates the constraint of rule (1)." ></td>
	<td class="line x" title="175:207	Same reason explains why a15a23a16a24a18  and a19  do not satisfy rule (10) in the third iteration." ></td>
	<td class="line x" title="176:207	By this simple algorithm, unknown words with unlimited length all have possibilities to be extracted." ></td>
	<td class="line x" title="177:207	Observing the extraction process of a26a25a27 a28 , you can find, in the extraction process, boundaries of unknown words might extend during iteration until no rule could be applied." ></td>
	<td class="line x" title="178:207	6 Experiment In our experiments, a word is considered as an unknown word, if either it is not in the CKIP lexicon or it is not identified by the word segmentation program as foreign word (for instance English) or a number." ></td>
	<td class="line x" title="179:207	The CKIP lexicon contains about 80,000 entries." ></td>
	<td class="line x" title="180:207	6.1 Evaluation Formulas The extraction process is evaluated in terms of precision and recall." ></td>
	<td class="line x" title="181:207	The target of our approach is to extract unknown words from a text, so we define correct extractions as unknown word types correctly extracted in the text." ></td>
	<td class="line x" title="182:207	The precision and recall formulas are listed as follows: idocument in sextractioncorrect ofnumber NCi = idocument in rdsunknown wo extracted ofnumber NEi = idocument in rdsunknown wo totalofnumber NTi =" ></td>
	<td class="line x" title="183:207	= = = == 150 1 i 150 1 i NE NC ratePrecision i i i i" ></td>
	<td class="line x" title="184:207	= = = == 150 1 i 150 1 i NT NC rate Recall i i i i 6.2 Data Sets We use the Sinica balanced corpus version 3.0 as our training set for unknown word detection, which contains 5 million segmented words tagged with pos." ></td>
	<td class="line x" title="185:207	We randomly select 150 documents of Chinese news on the internet as our testing set." ></td>
	<td class="line x" title="186:207	These testing data are segmented by hand according to the segmentation standard for information processing designed by the Academia Sinica (Huang et.al 1997)." ></td>
	<td class="line x" title="187:207	In average, each testing text contains about 300 words and 16.6 unknown word types." ></td>
	<td class="line x" title="188:207	6.3 Results Based on the four priority measures listed in table 4, the bottom-up merging algorithm is applied." ></td>
	<td class="line x" title="189:207	The performances are shown in table 5." ></td>
	<td class="line x" title="190:207	Evaluation Priority Match# Extract# Precision Recall Co-occurrence 1122 1485 76% 45% MI 1112 1506 74% 45% VMI 1125 1499 75% 45% t-score 1125 1494 75% 45% Note: There are total 2498 reference unknown word types Table 5." ></td>
	<td class="line x" title="191:207	Experimental results of the four different priority measures In table 5, comparing co-occurrence and MI, we found that the performance of co-occurrence measure is better than MI on both precision and recall." ></td>
	<td class="line x" title="192:207	The possible reason is that the characteristic of reoccurrence of unknown words is more important than morphological association of unknown words while extracting unknown words from a size-limited text." ></td>
	<td class="line x" title="193:207	That is because sometimes different unknown words consist of the same morpheme in a document, and if we use MI as the priority, these unknown words will have low MI values of their morphemes." ></td>
	<td class="line x" title="194:207	Even though they have higher frequency, they are still easily sacrificed when they are competed with their adjacent unknown word candidates." ></td>
	<td class="line x" title="195:207	This explanation is also proved by the performances of VMI and t-score, which emphasize more importance on co-occurrence in their formulas, are better than the performance of MI." ></td>
	<td class="line x" title="196:207	According to above discussions, we adopt cooccurrence as the priority decision making in our unknown word extraction system." ></td>
	<td class="line x" title="197:207	In our final system, we adopt morphological rules to extract regular type unknown words and the general rules to extract the remaining irregular unknown words and the total performance is a recall of 57% and a precision of 76%." ></td>
	<td class="line x" title="198:207	An old system of using the morphological rules for names of people, compounds with prefix or suffix were tested, without using the general rules, having a recall of 25% and a precision of 80%." ></td>
	<td class="line x" title="199:207	The general rules improve 32% of the recall and without sacrificing too much of precision." ></td>
	<td class="line x" title="200:207	7 Conclusion and Future Work In this research, Chinese word segmentation and unknown word extraction has been integrated into a frame work." ></td>
	<td class="line x" title="201:207	To increase the coverage of the morphological rules, we first derive a set of general rules to represent all kinds of unknown words." ></td>
	<td class="line x" title="202:207	To avoid extracting superfluous character strings, we then append these rules with linguistic and statistical constraints." ></td>
	<td class="line x" title="203:207	We propose an efficient bottom-up merging algorithm by consulting the general rules to extract unknown words and using priority measures to resolve the rule matching ambiguities." ></td>
	<td class="line x" title="204:207	In the experiment, we compare effects of different priority strategies, and experimental results show that the co-occurrence measure performances best." ></td>
	<td class="line x" title="205:207	It is found that the performance of unknown word detection would affect the entire performance significantly." ></td>
	<td class="line x" title="206:207	Although the performance of unknown word detection is not bad, there is still room for improvement." ></td>
	<td class="line x" title="207:207	The possible strategies for improvement in our future work include using contextual semantic relations in detection, and some updated statistical methods, such as support vector machine, maximal entropy and so on, to achieve better performance of unknown word detection." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W03-1717
Learning Verb-Noun Relations To Improve Parsing
Wu, Andi;"></td>
	<td class="line x" title="1:108	Learning Verb-Noun Relations to Improve Parsing Andi Wu Microsoft Research One Microsoft Way, Redmond, WA 98052 andiwu@microsoft.com Abstract The verb-noun sequence in Chinese often creates ambiguities in parsing." ></td>
	<td class="line x" title="2:108	These ambiguities can usually be resolved if we know in advance whether the verb and the noun tend to be in the verb-object relation or the modifier-head relation." ></td>
	<td class="line x" title="3:108	In this paper, we describe a learning procedure whereby such knowledge can be automatically acquired." ></td>
	<td class="line x" title="4:108	Using an existing (imperfect) parser with a chart filter and a tree filter, a large corpus, and the log-likelihood-ratio (LLR) algorithm, we were able to acquire verb-noun pairs which typically occur either in verbobject relations or modifier-head relations." ></td>
	<td class="line x" title="5:108	The learned pairs are then used in the parsing process for disambiguation." ></td>
	<td class="line x" title="6:108	Evaluation shows that the accuracy of the original parser improves significantly with the use of the automatically acquired knowledge." ></td>
	<td class="line x" title="7:108	1 Introduction Computer analysis of natural language sentences is a challenging task largely because of the ambiguities in natural language syntax." ></td>
	<td class="line x" title="8:108	In Chinese, the lack of inflectional morphology often makes the resolution of those ambiguities even more difficult." ></td>
	<td class="line x" title="9:108	One type of ambiguity is found in the verb-noun sequence which can appear in at least two different relations, the verb-object relation and the modifierhead relation, as illustrated in the following phrases." ></td>
	<td class="line x" title="10:108	(1)     dengji shouxu de feiyong register procedure DE expense the expense of the registration procedure (2)     banli shouxu de feiyong handle procedure DE expense the expense of going through the procedure In (1), the verb-noun sequence   is an example of the modifier-head relation while   in (2) is an example of the verb-object relation." ></td>
	<td class="line x" title="11:108	The correct analyses of these two phrases are given in Figure 1 and Figure 2, where RELCL stands for relative clause: Figure 1." ></td>
	<td class="line x" title="12:108	Correct analysis of (1) Figure 2." ></td>
	<td class="line x" title="13:108	Correct analysis of (2) However, with the set of grammar rules that cover the above phrases and without any semantic or collocational knowledge of the words involved, there is nothing to prevent us from the wrong analyses in Figure 3 and Figure 4." ></td>
	<td class="line x" title="14:108	Figure 3." ></td>
	<td class="line x" title="15:108	Wrong analysis of (1) Figure 4." ></td>
	<td class="line x" title="16:108	Wrong analysis of (2) To rule out these wrong parses, we need to know that  is a typical modifier of while  typically takes  as an object." ></td>
	<td class="line x" title="17:108	The question is how to acquire such knowledge automatically." ></td>
	<td class="line x" title="18:108	In the rest of this paper, we will present a learning procedure that learns those relations by processing a large corpus with a chart-filter, a treefilter and an LLR filter." ></td>
	<td class="line oc" title="19:108	The approach is in the spirit of Smadja (1993) on retrieving collocations from text corpora, but is more integrated with parsing." ></td>
	<td class="line x" title="20:108	We will show in the evaluation section how much the learned knowledge can help improve sentence analysis." ></td>
	<td class="line x" title="21:108	2 The Learning Procedure The syntactic ambiguity associated with the verbnoun sequence can be either local or global." ></td>
	<td class="line x" title="22:108	The kind of ambiguity we have observed in (1) and (2) is global in nature, which exists even if this noun phrase is plugged into a larger structure or complete sentence." ></td>
	<td class="line x" title="23:108	There are also local ambiguities where the ambiguity disappears once the verbnoun sequence is put into a broader context." ></td>
	<td class="line x" title="24:108	In the following examples, the sentences in (3) and (4) can only receive the analyses in Figure 5 and Figure 6 respectively." ></td>
	<td class="line x" title="25:108	(3)       zhe shi xin de dengji shouxu this be new DE register procedure This is a new registration procedure. (4)      ni bu bi banli shouxu you not must handle procedure You dont have to go through the procedure. Figure 5." ></td>
	<td class="line x" title="26:108	Parse tree of (3) Figure 6." ></td>
	<td class="line x" title="27:108	Parse tree of (4) In the processing of a large corpus, sentences with global ambiguities only have a random chance of being analyzed correctly, but sentences with local ambiguities can often receive correct analyses." ></td>
	<td class="line x" title="28:108	Although local ambiguities will create some confusion in the parsing process, increase the size of the parsing chart, and slow down processing, they can be resolved in the end unless we run out of resources (in terms of time and space) before the analysis is complete." ></td>
	<td class="line x" title="29:108	Therefore, there should be sufficient number of cases in the corpus where the relationship between the verb and the noun is clear." ></td>
	<td class="line x" title="30:108	An obvious strategy we can adopt here is to learn from the clear cases and use the learned knowledge to help resolve the unclear cases." ></td>
	<td class="line x" title="31:108	If a verb-noun pair appears predominantly in the verb-object relationship or the modifier head relationship throughout the corpus, we should prefer this relationship everywhere else." ></td>
	<td class="line x" title="32:108	A simple way to learn such knowledge is by using a tree-filter to collect all instances of each verb-noun pair in the parse trees of a corpus, counting the number of times they appear in each relationship, and then comparing their frequencies to decide which relationship is the predominant one for a given pair." ></td>
	<td class="line x" title="33:108	Once we have the information that  is typically a modifier of  and  typically takes  as an object, for instance, the sentence in (1) will only receive the analysis in Figure 1 and (2) only the analysis in Figure 2." ></td>
	<td class="line x" title="34:108	However, this only works in idealized situations where the parser is doing an almost perfect job, in which case no learning would be necessary." ></td>
	<td class="line x" title="35:108	In reality, the parse trees are not always reliable and the relations extracted from the parses can contain a fair amount of noise." ></td>
	<td class="line x" title="36:108	It is not hard to imagine that a certain verb-noun pair may occur only a couple of times in the corpus and they are misanalyzed in every instance." ></td>
	<td class="line x" title="37:108	If such noise is not filtered out, the knowledge we acquire will mislead us and minimize the benefit we get from this approach." ></td>
	<td class="line x" title="38:108	An obvious solution to this problem is to ignore all the low frequency pairs and keep the high frequency ones only, as wrong analyses tend to be random." ></td>
	<td class="line x" title="39:108	But the cut-off point is difficult to set if we are only looking at the raw frequencies, whose range is hard to predict." ></td>
	<td class="line x" title="40:108	The cut-off point will be too low for some pairs and too high for others." ></td>
	<td class="line x" title="41:108	We need a normalizing factor to turn the raw frequencies into relative frequencies." ></td>
	<td class="line x" title="42:108	Instead of asking which relation is more frequent for a given pair?, the question should be of all the instances of a given verb-noun pair in the corpus, which relation has a higher percentage of occurrence?." ></td>
	<td class="line x" title="43:108	The normalizing factor should then be the total count of a verb-noun pair in the corpus regardless of the syntactic relations between them." ></td>
	<td class="line x" title="44:108	The normalized frequency of a relation for a given pair is thus the number of times this pair is assigned this relation in the parses divided by this normalizing factor." ></td>
	<td class="line x" title="45:108	For example, if   occurs 10 times in the corpus and is analyzed as verb-object 3 times and modifier-head 7 times, the normalized frequencies for these two relations will be 30% and 70% respectively." ></td>
	<td class="line x" title="46:108	What we have now is actually the probability of a given pair occurring in a given relationship." ></td>
	<td class="line x" title="47:108	This probability may not be very accurate, given the fact that the parse trees are not always correct, but it should a good approximation, assuming that the corpus is large enough and most of the potential ambiguities in the corpus are local rather than global in nature." ></td>
	<td class="line x" title="48:108	But how do we count the number of verb-noun pairs in a corpus?" ></td>
	<td class="line x" title="49:108	A simple bigram count will unjustly favor the modifier-head relation." ></td>
	<td class="line x" title="50:108	While the verb and the noun are usually adjacent when the verb modifies the noun, they can be far apart when the noun is the object of the verb, as illustrated in (5)." ></td>
	<td class="line x" title="51:108	(5)       tamen zhengzai banli qu taiwan canjia they PROG handle go Taiwan participate     dishijiujie guoji jisuan yuyanxue 19 th international compute linguistics    huiyi de shouxu conference DE procedure They are going through the procedures for going to Taipei for the 19 th International Conference on Computational Linguistics. To get a true normalizing factor, we must count all the potential dependencies, both local and longdistance." ></td>
	<td class="line x" title="52:108	This is required also because the treefilter we use to collect pair relations consider both local and long-distance dependencies as well." ></td>
	<td class="line x" title="53:108	Since simple string matching is not able to get the potential long-distance pairs, we resorted to the use of a chart-filter." ></td>
	<td class="line x" title="54:108	As the parser we use is a chart parser, all the potential constituents are stored in the chart, though only a small subset of those will end up in the parse tree." ></td>
	<td class="line x" title="55:108	Among the constituents created in the chart for the sentence in (5), for instance, we are supposed to find [] and [ ] which are adjacent to each other." ></td>
	<td class="line x" title="56:108	The fact that   is the head of the second phrase then makes  adjacent to ." ></td>
	<td class="line x" title="57:108	We will therefore be able to get one count of  followed by  from (5) despite the long span of intervening words between them." ></td>
	<td class="line x" title="58:108	The use of the chart-filter thus enables us to make our normalizing factor more accurate." ></td>
	<td class="line x" title="59:108	The probability of a given verb-noun pair occurring in a given relation is now the total count of this relation in the parse trees throughout the corpus divided by the total count of all the potential relations found in all the charts created during the processing of this corpus." ></td>
	<td class="line x" title="60:108	The cut-off point we finally used is 50%, i.e. a pair+relation will be kept in our knowledge base if the probability obtained this way is more than 50%." ></td>
	<td class="line x" title="61:108	This may seem low, but it is higher than we think considering the fact that verb-object and modifier-head are not the only relations that can hold between a verb and a noun." ></td>
	<td class="line x" title="62:108	In (6), for example,  is not related to in either way in spite of their adjacency." ></td>
	<td class="line x" title="63:108	(6)       tamen qu shanghai banli shouxu suoxu they go Shanghai handle procedure need    de gongzheng cailiao DE notarize material They went to Shanghai to handle the notarized material needed for the procedure. We will still find the   pair in the chart, but it is not expected to appear in either the verb-object relation or modifier-head relation in the parse tree." ></td>
	<td class="line x" title="64:108	Therefore, the baseline probability for any pair+relation might be far below 50% and more than 50% is a good indicator that a given pair does typically occur in a given relation." ></td>
	<td class="line x" title="65:108	We can also choose to keep all the pairs with their probabilities in the knowledge base and let the probabilities be integrated into the probability of the complete parse tree at the time of parse ranking." ></td>
	<td class="line x" title="66:108	The results we obtained from the above procedure are quite clean, in the sense that most of the pairs that are classified into the two types of relations with a probability greater than 50% are correct." ></td>
	<td class="line x" title="67:108	Here are some sample pairs that we learned." ></td>
	<td class="line x" title="68:108	Verb-Object:   test truth   allocate recourses   manage business   offer love   cheat pedestrians Modifier-Head:   testing standard   allocation plan   management mode   offering spirit   cheating behavior However, there are pairs that are correct but not typical enough, especially in the verb-object relations." ></td>
	<td class="line x" title="69:108	Here are some examples:   have meaning   have impact   have color   have function   have effect  These are truly verb-object relations, but we may not want to keep them in our knowledge base for the following reasons." ></td>
	<td class="line x" title="70:108	First of all, the verbs in such cases usually can take a wide range of objects and the strength of association between the verb and the object is weak." ></td>
	<td class="line x" title="71:108	In other words, the objects are not typical." ></td>
	<td class="line x" title="72:108	Secondly, those verbs tend not to occur in the modifier-head relation with a following noun and we gain very little in terms of disambiguation by storing those pairs in the knowledge base." ></td>
	<td class="line x" title="73:108	To prune away those pairs, we used the log-likelihood-ratio algorithm (Dunning, 1993) to compute the degree of association between the verb and the noun in each pair." ></td>
	<td class="line x" title="74:108	Pairs where there is high mutual information between the verb and noun would receive higher scores while pairs where the verb can co-occur with many different nouns would receive lower scores." ></td>
	<td class="line x" title="75:108	Pairs with association scores below a certain threshold were then thrown out." ></td>
	<td class="line x" title="76:108	This not only makes the remaining pairs more typical but helps to clean out more garbage." ></td>
	<td class="line x" title="77:108	The resulting knowledge base therefore has higher quality." ></td>
	<td class="line x" title="78:108	3 Evaluation The knowledge acquired by the method described in the previous section is used in subsequent sentence analysis to prefer those parses where the verb-noun sequence is analyzed in the same way as specified in the knowledge base." ></td>
	<td class="line x" title="79:108	When processing a large corpus, what we typically do is analyzing the corpus twice." ></td>
	<td class="line x" title="80:108	The first pass is the learning phase where we acquire additional knowledge by parsing the corpus." ></td>
	<td class="line x" title="81:108	The knowledge acquired is used in the second pass to get better parses." ></td>
	<td class="line x" title="82:108	This is one example of the general approach of improving parsing by parsing, as described in (Wu et al 2002)." ></td>
	<td class="line x" title="83:108	To find out how much the learned knowledge contributes to the improvement of parsing, we performed a human evaluation." ></td>
	<td class="line x" title="84:108	In the evaluation, we used our existing sentence analyzer (Heidorn 2000, Jensen et al 1993, Wu and Jiang 1998) to process a corpus of 271,690 sentences to learn the verb-noun relations." ></td>
	<td class="line x" title="85:108	We then parsed the same sentences first without the additional knowledge and then with the acquired knowledge." ></td>
	<td class="line x" title="86:108	Comparing the outputs, we found that 16,445 (6%) of the sentences had different analyses in the two passes." ></td>
	<td class="line x" title="87:108	We then randomly selected 500 sentences from those diff sentences and presented them to a linguist from an independent agency who, given two different parses of the same sentence, was asked to pick the parse she judged to be more accurate." ></td>
	<td class="line x" title="88:108	The order in which the parses were presented was randomized so that the evaluator had no idea as to which tree was from the first pass and which one from the second pass." ></td>
	<td class="line x" title="89:108	The linguists judgment showed that, with the additional knowledge that we acquired, 350 (70%) of those sentences parsed better with the additional knowledge, 85 (17%) parsed worse, and 65 (13%) had parses that were equally good or bad." ></td>
	<td class="line x" title="90:108	In other words, the accuracy of sentence analysis improved significantly with the learning procedure discussed in this paper." ></td>
	<td class="line x" title="91:108	Here is an example where the parse became better when the automatically acquired knowledge is used." ></td>
	<td class="line x" title="92:108	Due to space limitation, only the parses of a fraction of the sentence is given here: (7)      yao zunzhao guojia ceshi biaozhun want follow nation testing standard (You) must follow the national testing standards. Because of the fact that is ambiguous between a verb (follow) and a preposition (in accordance with), this sentence fragment got the parse tree in Figure 7 before the learned knowledge was used, where was misanalyzed as the object of: Figure 7: old parse of (7) During the learning process, we acquired  as a typical pair where the two words are in the modifier-head relationship." ></td>
	<td class="line x" title="93:108	Once this pair was added to our knowledge base, we got the correct parse, where  is analyzed as a verb and as a modifier of : Figure 8: New tree of (7) We later inspected the sentences where the parses became worse and found two sources for the regressions." ></td>
	<td class="line x" title="94:108	The main source was of course errors in the learned results, since they had not been manually checked." ></td>
	<td class="line x" title="95:108	The second source was an engineering problem: the use of the acquired knowledge required the use of additional memory and consequently exceeded some system limitations when the sentences were very long." ></td>
	<td class="line x" title="96:108	4 Future work The approach described in this paper can be applied to the learning of many other typical syntactic relations between words." ></td>
	<td class="line x" title="97:108	We have already used it to learn noun-noun pairs where the first noun is a typical modifier of the second noun." ></td>
	<td class="line x" title="98:108	This has helped us to rule out incorrect parses where the two nouns were not put into the same constituent." ></td>
	<td class="line x" title="99:108	Other relations we have been trying to learn include:  Noun-noun pairs where the two nouns are in conjunction (e.g.   bride and bridegroom);  Verb-verb pairs where the two verbs are in conjunction (e.g.   investigate and study);  Adjective-adjective pairs where two adjectives are in conjunction (e.g.   young and beautiful);  Noun-verb pairs where the noun is a typical subject of the verb." ></td>
	<td class="line x" title="100:108	Knowledge of this kind, once acquired, will benefit not only parsing, but other NLP applications as well, such as machine translation and information retrieval." ></td>
	<td class="line x" title="101:108	In terms of parsing, the benefit we get there is similar to what we get in lexicalized statistical parsing where parsing decisions can be based on specific lexical items." ></td>
	<td class="line x" title="102:108	However, the training of a statistical parser requires a tree bank which is expensive to create while our approach does not." ></td>
	<td class="line x" title="103:108	Our approach does require an existing parser, but this parser does not have to be perfect and can be improved as the learning goes on." ></td>
	<td class="line x" title="104:108	Once the parser is reasonably good, what we need is just raw text, which is available in large quantities." ></td>
	<td class="line x" title="105:108	5 Conclusion We have shown in this paper that parsing quality can be improved by using the parser as an automatic learner which acquires new knowledge in the first pass to help analysis in the second pass." ></td>
	<td class="line x" title="106:108	We demonstrated this through the learning of typical verb-object and modifier-head relations." ></td>
	<td class="line x" title="107:108	With the use of a chart-filter, a tree-filter and the LLR algorithm, we are able to acquire such knowledge with high accuracy." ></td>
	<td class="line x" title="108:108	Evaluation shows that the quality of sentence analysis can improve significantly with the help of the automatically acquired knowledge." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W03-1805
A Language Model Approach To Keyphrase Extraction
Tomokiyo, Takashi;Hurst, Matthew;"></td>
	<td class="line x" title="1:154	A Language Model Approach to Keyphrase Extraction Takashi Tomokiyo and Matthew Hurst Applied Research Center Intelliseek, Inc. Pittsburgh, PA 15213 a0 ttomokiyo,mhurst a1 @intelliseek.com Abstract We present a new approach to extracting keyphrases based on statistical language models." ></td>
	<td class="line x" title="2:154	Our approach is to use pointwise KL-divergence between multiple language models for scoring both phraseness and informativeness, which can be unified into a single score to rank extracted phrases." ></td>
	<td class="line x" title="3:154	1 Introduction In many real world deployments of text mining technologies, analysts are required to deal with large collections of documents from unfamiliar domains." ></td>
	<td class="line x" title="4:154	Familiarity with the domain is necessary in order to get full leverage from text analysis tools." ></td>
	<td class="line x" title="5:154	However, browsing data is not an efficient way to get an understanding of the topics and events which are particular to a domain." ></td>
	<td class="line x" title="6:154	For example, an analyst concerned with the area of hybrid cars may harvest messages from online forums." ></td>
	<td class="line x" title="7:154	They may then want to rapidly construct a hierarchy of topics based on the content of these messages." ></td>
	<td class="line x" title="8:154	In addition, in cases where these messages are harvested via a search of some sort, there is a requirement to obtain a rich and effective set of search terms." ></td>
	<td class="line x" title="9:154	The technology described in this paper is an example of a phrase finder capable of delivering a set of indicative phrases given a particular set of documents from a target domain." ></td>
	<td class="line x" title="10:154	In the hybrid car example, the result of this process is a set of phrases like that shown in Figure 1." ></td>
	<td class="line x" title="11:154	1 civic hybrid 2 honda civic hybrid 3 toyota prius 4 electric motor 5 honda civic 6 fuel cell 7 hybrid cars 8 honda insight 9 battery pack 10 sports car 11 civic si 12 hybrid car 13 civic lx 14 focus fcv 15 fuel cells 16 hybrid vehicles 17 tour de sol 18 years ago 19 daily driver 20 jetta tdi 21 mustang gt 22 ford escape 23 steering wheel 24 toyota prius today 25 electric motors 26 gasoline engine 27 internal combustion engine 28 gas engine 29 front wheels 30 key sense wire 31 civic type r 32 test drive 33 street race 34 united states 35 hybrid powertrain 36 rear bumper 37 ford focus 38 detroit auto show 39 parking lot 40 rear wheels Figure 1: Top 40 keyphrases automatically extracted from messages relevant to civic hybrid using our system In order to capture domain-specific terms efficiently in limited time, the extraction result should be ranked with more indicative and good phrase first, as shown in this example." ></td>
	<td class="line x" title="12:154	2 Phraseness and informativeness The word keyphrase implies two features: phraseness and informativeness." ></td>
	<td class="line x" title="13:154	Phraseness is a somewhat abstract notion which describes the degree to which a given word sequence is considered to be a phrase." ></td>
	<td class="line x" title="14:154	In general, phraseness is defined by the user, who has his own criteria for the target application." ></td>
	<td class="line x" title="15:154	For instance, one user might want only noun phrases while another user might be interested only in phrases describing a certain set of products." ></td>
	<td class="line x" title="16:154	Although there is no single definition of the term phrase, in this paper, we focus on collocation or cohesion of consecutive words." ></td>
	<td class="line x" title="17:154	Informativeness refers to how well a phrase captures or illustrates the key ideas in a set of documents." ></td>
	<td class="line x" title="18:154	Because informativeness is defined with respect to background information and new knowledge, users will have different perceptions of informativeness." ></td>
	<td class="line x" title="19:154	In our calculations, we make use of the relationship between foreground and background corpora to formalize the notion of informativeness." ></td>
	<td class="line x" title="20:154	The target document set from which representative keyphrases are extracted is called the foreground corpus." ></td>
	<td class="line x" title="21:154	The document set to which this target set is compared is called the background corpus." ></td>
	<td class="line x" title="22:154	For example, a foreground corpus of the current weeks news would be compared to a background corpus of an entire news article archive to determine that certain phrases, like press conference are typical of news stories in general and do not capture the particulars of current events in the way that national museum of antiquities does." ></td>
	<td class="line x" title="23:154	Other examples of foreground and background corpora include: a web site for a certain company and web data in general; a newsgroup and the whole Usenet archive; and research papers of a certain conference and research papers in general." ></td>
	<td class="line x" title="24:154	In order to get a ranked keyphrase list, we need to combine both phraseness and informativeness into a single score." ></td>
	<td class="line x" title="25:154	A sequence of words can be a good phrase but not an informative one, like the expression in spite of. A word sequence can be informative for a particular domain but not a phrase; toyota, honda, ford is an example of a non-phrase sequence of informative words in a hybrid car domain." ></td>
	<td class="line x" title="26:154	The algorithm we propose for keyphrase finding requires that the keyphrase score well for both phraseness and informativeness." ></td>
	<td class="line oc" title="27:154	3 Related work Word collocation Various collocation metrics have been proposed, including mean and variance (Smadja, 1994), the t-test (Church et al. , 1991), the chi-square test, pointwise mutual information (MI) (Church and Hanks, 1990), and binomial loglikelihood ratio test (BLRT) (Dunning, 1993)." ></td>
	<td class="line x" title="28:154	According to (Manning and Schutze, 1999), BLRT is one of the most stable methods for collocation discovery." ></td>
	<td class="line x" title="29:154	(Pantel and Lin, 2001) reports, however, that BLRT score can be also high for two frequent terms that are rarely adjacent, such as the word pair the the, and uses a hybrid of MI and BLRT." ></td>
	<td class="line x" title="30:154	Keyphrase extraction Damerau (1993) uses the relative frequency ratio between two corpora to extract domain-specific keyphrases." ></td>
	<td class="line x" title="31:154	One problem of using relative frequency is that it tends to assign too high a score for words whose frequency in the background corpus is small (or even zero)." ></td>
	<td class="line x" title="32:154	Some work has been done in extracting keyphrases from technical documents treating keyphrase extraction as a supervised learning problem (Frank et al. , 1999; Turney, 2000)." ></td>
	<td class="line x" title="33:154	The portability of a learned classifier across various unstructured/structured text is not clear, however, and the agreement between classifier and human judges is not high.1 We would like to have the ability to extract keyphrases from a totally new domain of text without building a training corpus." ></td>
	<td class="line x" title="34:154	Combining keyphrase and collocation Yamamoto and Church (2001) compare two metrics, MI and Residual IDF (RIDF), and observed that MI is suitable for finding collocation and RIDF is suitable for finding informative phrases." ></td>
	<td class="line x" title="35:154	They took the intersection of each top 10% of phrases identified by MI and RIDF, but did not extend the approach to combining the two metrics into a unified score." ></td>
	<td class="line x" title="36:154	4 Baseline method based on binomial log-likelihood ratio test We can use various statistics as a measure for phraseness and informativeness." ></td>
	<td class="line x" title="37:154	For our baseline, we have selected the method based on binomial loglikelihood ratio test (BLRT) described in (Dunning, 1993)." ></td>
	<td class="line x" title="38:154	The basic idea of using BLRT for text analysis is to consider a word sequence as a repeated sequence of binary trials comparing each word in a corpus to a target word, and use the likelihood ratio of two hypotheses that (i) two events, observed a0a2a1 times out of a3a4a1 total tokens and a0a6a5 times out of a3a7a5 total tokens respectively, are drawn from different distributions and (ii) from the same distribution." ></td>
	<td class="line x" title="39:154	1e.g. Turney reports 62% good, 18% bad, 20% no opinion from human judges." ></td>
	<td class="line x" title="40:154	The BLRT score is calculated with a0a2a1a4a3a6a5a8a7a10a9a12a11 a1a14a13 a0 a1a14a13 a3 a1a16a15 a7a10a9a12a11 a5a17a13 a0 a5a17a13 a3 a5a18a15 a7a10a9a12a11 a13 a0 a1 a13 a3a4a1 a15 a7a10a9a12a11 a13 a0 a5 a13 a3 a5 a15 (1) where a11a20a19a22a21 a0a6a19a24a23 a3a25a19, a11a26a21a27a9 a0 a1a29a28 a0 a5 a15 a23a30a9 a3a4a1a29a28 a3 a5 a15, and a7a10a9a12a11 a13 a0 a13 a3 a15 a21a31a11a33a32a34a9a36a35a38a37a26a11 a15a40a39a42a41 a32 (2) In the case of calculating the phraseness score of an adjacent word pair (a43 a13a45a44 ), the null hypothesis is that a43 and a44 are independent, which can be expressed as a11a2a9 a44a25a46a43 a15 a21a47a11a29a9 a44a22a46a49a48 a43 a15." ></td>
	<td class="line x" title="41:154	We can use Equation (1) to calculate phraseness by setting: a0 a1a50a21a52a51a53a9a54a43 a13a45a44a55a15a16a13 a3a4a1a56a21a57a51a53a9a54a43 a15a16a13 a0 a5a58a21a52a51a53a9 a48 a43 a13a45a44a59a15 a21a57a51a60a9 a44a59a15 a37a61a51a53a9a54a43 a13a45a44a55a15a16a13 a3 a5 a21a57a51a53a9 a48 a43 a15 a21a57a62a64a63a65a51a53a9a54a66 a15 a37a31a51a53a9a54a43 a15 (3) where a51a60a9a54a43 a15 is the frequency of the word a43 and a51a53a9a54a43 a13a45a44a59a15 is the frequency of a44 following a43 . For calculating informativeness of a word a66, a0 a1a50a21a57a51a68a67a70a69a42a9a54a66 a15a16a13 a3 a1 a21a57a62a64a63a65a51a68a67a70a69a42a9a54a66 a15a16a13 a0 a5 a21a57a51a50a71a72a69a42a9a54a66 a15a16a13 a3 a5a38a21a57a62a64a63a65a51a50a71a72a69a42a9a54a66 a15 (4) where a51a68a67a70a69a42a9a54a66 a15 and a51a68a71a72a69a42a9a54a66 a15 are the frequency of a66 in the foreground and background corpus, respectively." ></td>
	<td class="line x" title="42:154	Combining a phraseness score a73a25a74 and an informativeness score a73a29a19 into a single score value is not a trivial task since the the BLRT scores vary a lot between phraseness and informativeness and also depending on data (c.f. Figure 6 (a))." ></td>
	<td class="line x" title="43:154	One way to combine those scores is to use an exponential model." ></td>
	<td class="line x" title="44:154	We experimented with the following logistic function: a73a75a21 a35 a35a56a28a61a76a78a77a30a79a80a9a36a37a38a81a59a73 a74 a37a83a82a70a73 a19 a28a85a84 a15 (5) whose parameters a81,a82, and a84 are estimated on a heldout data set, given feedback from users (i.e. supervised)." ></td>
	<td class="line x" title="45:154	Figure 2 shows some example phrases extracted with this method from the data set described in Section 6.1, where the parameters, a81, a82, a84, are manually optimized on the test data." ></td>
	<td class="line x" title="46:154	Although it is possible to rank keyphrases using this approach, there are a couple of drawbacks." ></td>
	<td class="line x" title="47:154	1 message news 2 minority report 3 star wars 4 john harkness 5 derek janssen 6 robert frenchu 7 sean ohara 8 box office 9 dawn taylor 10 anthony gaza 11 star trek 12 ancient race 13 scooby doo 14 austin powers 15 home.attbi.com hey 16 sixth sense 17 hey kids 18 gaza man 19 lee harrison 20 years ago 21 julia roberts 22 national guard 23 bourne identity 24 metrotoday www.zap2it.com 25 starweek magazine 26 eric chomko 27 wilner starweek 28 tim gueguen 29 jodie foster 30 johnnie kendricks Figure 2: Keyphrases extracted with BLRT (a=0.0003, b=0.000005, c=8) Necessity of tuning parameters the existence of parameters in the combining function requires human labeling, which is sometimes an expensive task to do, and the robustness of learned weight across domains is unknown." ></td>
	<td class="line x" title="48:154	We would like to have a parameter-free and robust way of combining scores." ></td>
	<td class="line x" title="49:154	Inappropriate symmetry BLRT tests to see if two random variables are independent or not." ></td>
	<td class="line x" title="50:154	This sometimes leads to unwanted phrases getting a high score." ></td>
	<td class="line x" title="51:154	For example, when the background corpus happens to have many occurrences of phrase al jazeera which is an unusual phrase in the foreground corpus, then the phrase still gets high score of informativeness because the distribution is so different." ></td>
	<td class="line x" title="52:154	What we would like to have instead is asymmetric scoring function to test the loss of the action of not taking the target phrase as a keyphrase." ></td>
	<td class="line x" title="53:154	In the next section, we propose a new method trying to address these issues." ></td>
	<td class="line x" title="54:154	5 Proposed method 5.1 Language models and expected loss A language model assigns a probability value to every sequence of words a86a87a21a64a66 a1a88a66 a5a2a89a70a89a70a89a88a66 a39 . The probability a90a60a9a54a86 a15 can be decomposed as a90a60a9a54a86 a15 a21 a39 a91 a19a93a92 a1 a90a60a9a54a66a94a19 a46a66 a1a45a66 a5a2a89a70a89a70a89a88a66a94a19 a41 a1 a15 Assuming a66a38a19 only depends on the previous a95 words, N-gram language models are commonly used." ></td>
	<td class="line x" title="55:154	The following is the trigram language model case." ></td>
	<td class="line x" title="56:154	a90a60a9a54a86 a15 a21 a39 a91 a19a93a92 a1 a90 a9a54a66a94a19 a46a66a94a19 a41 a5 a13 a66a94a19 a41 a1 a15 Here each word only depends on the previous two words." ></td>
	<td class="line x" title="57:154	Please refer to (Jelinek, 1990) and (Chen and Goodman, 1996) for more about N-gram models and associated smoothing methods." ></td>
	<td class="line x" title="58:154	Now suppose we have a foreground corpus and a background corpus and have created a language model for each corpus." ></td>
	<td class="line x" title="59:154	The simplest language model is a unigram model, which assumes each word of a given word sequence is drawn independently." ></td>
	<td class="line x" title="60:154	We denote the unigram model for the foreground corpus as a7 a1 a1fg and for the background corpus as a7 a1 a1bg." ></td>
	<td class="line x" title="61:154	We can also train higher order models a7 a1a3a2 fg and a7 a1a3a2 bg for each corpus, each of which is a a95 -gram model, where a95a61a9a5a4 a35 a15 is the order." ></td>
	<td class="line x" title="62:154	a6a7 phrasenessa7 a8 a9 a37a30a37 informativeness a37a30a37a11a10 a7 a1a12a2 fg a7 a1a3a2 bg a7 a1 a1 fg a7 a1 a1 bg Figure 3: Phraseness and informativeness as loss between language models." ></td>
	<td class="line x" title="63:154	Among those four models, a7 a1 a2fg will be the best model to describe the foreground corpus in the sense that it has the smallest cross-entropy or perplexity value over the corpus." ></td>
	<td class="line x" title="64:154	If we use one of the other three models instead, then we have some inefficiency or loss to describe the corpus." ></td>
	<td class="line x" title="65:154	We expect the amount of loss between using a7 a1a3a2fg and a7 a1 a1fg is related to phraseness and the loss between a7 a1a13a2fg and a7 a1a3a2bg is related to informativeness." ></td>
	<td class="line x" title="66:154	Figure 3 illustrates these relationships." ></td>
	<td class="line x" title="67:154	5.2 Pointwise KL-divergence between models One natural metric to measure the loss between two language models is the Kullback-Leibler (KL) divergence." ></td>
	<td class="line x" title="68:154	The KL divergence (also called relative entropy) between two probability mass function a11a29a9a54a43 a15 and a14a59a9a54a43 a15 is defined as a15 a9a12a11a17a16a18a14 a15 a21a20a19a22a21 a11a29a9a54a43 a15 a1a93a3a6a5 a11a29a9a54a43 a15 a14a59a9a54a43 a15 (6) KL divergence is a measure of the inefficiency of assuming that the distribution is a14 when the true distribution is a11 . (Cover and Thomas, 1991) You can see this by the following relationship: a15 a9a12a11a17a16a18a14 a15 a21 a19a22a21 a11a29a9a54a43 a15 a1a4a3a6a5 a11a29a9a54a43 a15 a37 a19a23a21 a11a29a9a54a43 a15 a1a4a3a6a5 a14a59a9a54a43 a15 a21 a19a22a21 a11a29a9a54a43 a15 a35 a1a4a3a6a5 a14a59a9a54a43 a15 a37a17a24a85a9a26a25 a15 The first term a62 a21 a11a29a9a54a43 a15 a1a27a29a28a31a30a33a32a35a34 a21a37a36 is the cross entropy and the second term a24a85a9a26a25 a15 is the entropy of the random variable a25, which is how much we could compress symbols if we know the true distribution a11 . We define pointwise KL divergence a38a40a39 a9a12a11a41a16a42a14 a15 to be the term inside of the summation of Equation (6): a38 a39 a9a12a11a17a16a18a14 a15a44a43a46a45a48a47a21a27a11a2a9a54a86 a15 a1a4a3a6a5 a11a29a9a54a86 a15 a14a59a9a54a86 a15 (7) Intuitively, this is the contribution of the phrase a86 to the expected loss of the entire distribution." ></td>
	<td class="line x" title="69:154	We can now quantify phraseness and informativeness as follows: Phraseness of a86 is how much we lose information by assuming independence of each word by applying the unigram model, instead of the a95 gram model." ></td>
	<td class="line x" title="70:154	a38 a39 a9a72a7 a1 a2 fg a16a56a7 a1 a1 fga15 (8) Informativeness of a86 is how much we lose information by assuming the phrase is drawn from the background model instead of the foreground model." ></td>
	<td class="line x" title="71:154	a38a49a39 a9a72a7 a1 a2 fg a16a56a7 a1 a2 bga15a16a13 or (9) a38a49a39 a9a72a7 a1 a1 fg a16a56a7 a1 a1 bga15 (10) Combined The following is considered to be a mixture of phraseness and informativeness." ></td>
	<td class="line x" title="72:154	a38a49a39 a9a72a7 a1 a2 fg a16a56a7 a1 a1 bga15 (11) Note that the KL divergence is always nonnegative2, but the pointwise KL divergence can be a negative value." ></td>
	<td class="line x" title="73:154	An example is the phraseness of the bigram the the." ></td>
	<td class="line x" title="74:154	a11a29a9 thea13 thea15 a1a4a3a6a5 a11a29a9 thea13 thea15 a11a29a9 thea15 a11a29a9 thea15a1a0 a2 since a11a2a9 thea13 thea15a4a3 a11a29a9 thea15 a11a29a9 thea15 . Also note that in the case of phraseness of a bigram, the equation looks similar to pointwise mutual information (Church and Hanks, 1990), but they are different." ></td>
	<td class="line x" title="75:154	Their relationship is as follows." ></td>
	<td class="line x" title="76:154	a38a49a39 a9a12a11a29a9a54a43 a13a45a44a55a15 a16a22a11a2a9a54a43 a15 a11a29a9 a44a55a15a45a15 a21a31a11a29a9a54a43 a13a45a44a59a15 a1a4a3a6a5 a11a29a9a54a43 a13a45a44a59a15 a11a2a9a54a43 a15 a11a29a9 a44a55a15 a5 a6a8a7 a9 pointwise MI The pointwise KL divergence does not assign a high score to a rare phrase, whose contribution of loss is small by definition, unlike pointwise mutual information, which is known to have problems (as described in (Manning and Schutze, 1999), e.g.)." ></td>
	<td class="line x" title="77:154	5.3 Combining phraseness and informativeness One way of getting a unified score of phraseness and informativeness is using equation (11)." ></td>
	<td class="line x" title="78:154	We can also calculate phraseness and informativeness separately and then combine them." ></td>
	<td class="line x" title="79:154	We combine the phraseness score a73a25a74 and informativeness score a73a80a19 by simply adding them into a single score a73 . a73a75a21a57a73a20a74 a28a47a73 a19 (12) Intuitively, this can be thought of as the total loss." ></td>
	<td class="line x" title="80:154	We will show some empirical results to justify this scoring in the next section." ></td>
	<td class="line x" title="81:154	6 Experimental results In this section, we show some preliminary experimental results of applying our method on real data." ></td>
	<td class="line x" title="82:154	6.1 Data set We used the 20 newsgroups data set3, which contains 20,000 messages (7.4 million words) between February and June 1993 taken from 20 2from Jensens inequality." ></td>
	<td class="line x" title="83:154	3http://www-2.cs.cmu.edu/afs/cs.cmu.edu/ project/theo-20/www/data/news20.html Usenet newsgroups, as the background data set, and another 20,000 messages (4 million words) between June and September 2002 taken from rec.arts.movies.current-films newsgroup as the foreground data set." ></td>
	<td class="line x" title="84:154	Each messages subject header and the body of the message (including quoted text) is tokenized into lowercase tokens on both data set." ></td>
	<td class="line x" title="85:154	No stemming is applied." ></td>
	<td class="line x" title="86:154	6.2 Finding key-bigrams The first experiment we show is to find key-bigrams, which is the simplest case requiring combination of phraseness and informativeness scores." ></td>
	<td class="line x" title="87:154	Figure 4 outlines the extraction procedure." ></td>
	<td class="line x" title="88:154	a10 Inputs: foreground and background corpus." ></td>
	<td class="line x" title="89:154	1." ></td>
	<td class="line x" title="90:154	create background language model from the background corpus." ></td>
	<td class="line x" title="91:154	2." ></td>
	<td class="line x" title="92:154	count all adjacent word pairs in the foreground corpus, skipping pre-annotated boundaries (such as HTML tag boundaries) and stopwords." ></td>
	<td class="line x" title="93:154	3." ></td>
	<td class="line x" title="94:154	for each pair of words (x,y) in the count, calculate phraseness froma11a13a12a15a14a17a16a19a18a21a20 fg and a11a13a12a15a14a22a20 fga11a17a12a15a18a23a20 fg and informativeness from a11a17a12a15a14a17a16a24a18a21a20 fg and a11a17a12a15a14a17a16a24a18a21a20 bg." ></td>
	<td class="line x" title="95:154	Add the two score values as the unified score." ></td>
	<td class="line x" title="96:154	4." ></td>
	<td class="line x" title="97:154	sort the results by the unified score." ></td>
	<td class="line x" title="98:154	a10 Output: a list of key-bigrams ranked by unified score." ></td>
	<td class="line x" title="99:154	Figure 4: Procedure to find key-bigrams For this experiment we used unsmoothed count for calculating phraseness a11a29a9a54a43 a13a45a44a59a15 a21 a51a53a9a54a43 a13a45a44a59a15 a23 a95, a11a29a9a54a66 a15 a21 a51a53a9a54a66 a15 a23 a95 where a95 a21 a62 a21 a51a53a9a54a43 a15 a21 a62 a21 a25a26 a51a60a9a54a43 a13a45a44a55a15, and used the unigram model for calculating informativeness with Katz smoothing (Chen and Goodman, 1996)4 to handle zero occurrences." ></td>
	<td class="line x" title="100:154	Figure 5 shows the extracted key-bigrams using this method." ></td>
	<td class="line x" title="101:154	Comparing to Figure 2, you can see that those two methods extract almost identical ranked phrases." ></td>
	<td class="line x" title="102:154	Note that we needed to tune three parameters to combine phraseness and informativeness in BLRT, but no parameter tuning was required in this method." ></td>
	<td class="line x" title="103:154	The reason why message news becomes the top phrase in both methods is that it appears frequently enough in message citation headers such 4with cutoff a27a29a28a31a30 1 message news 2 minority report 3 star wars 4 john harkness 5 robert frenchu 6 derek janssen 7 box office 8 sean ohara 9 dawn taylor 10 anthony gaza 11 star trek 12 ancient race 13 home.attbi.com hey 14 scooby doo 15 austin powers 16 hey kids 17 years ago 18 gaza man 19 sixth sense 20 lee harrison 21 julia roberts 22 national guard 23 bourne identity 24 metrotoday www.zap2it.com 25 starweek magazine 26 eric chomko 27 wilner starweek 28 tim gueguen 29 jodie foster 30 kevin filmnutboy Figure 5: Key-bigrams extracted with pointwise KL as John Smith a0 js@foo.coma1 wrote in message news:1pk0a@foo.com, which was not common in the 20 newsgroup dataset.5 A more sophisticated document analysis tool to remove citation headers is required to improve the quality further." ></td>
	<td class="line x" title="104:154	Figure 6 shows the distribution of phraseness and informativeness scores of bigrams extracted using the BLRT and pointwise KL methods." ></td>
	<td class="line x" title="105:154	One can see that there is little correlation between phraseness and informativeness in both ranking methods." ></td>
	<td class="line x" title="106:154	Also note that the range of x and y axis is very different in BLRT, but in the pointwise KL method they are comparable ranges." ></td>
	<td class="line x" title="107:154	That makes combining two scores easy in the pointwise KL approach." ></td>
	<td class="line x" title="108:154	6.3 Ranking n-length phrases The next example is ranking a3 -length phrases." ></td>
	<td class="line x" title="109:154	We applied a phrase extension algorithm based on the APriori algorithm (Agrawal and Srikant, 1994) to the output of the key-bigram finder in the previous example to generate a3 -length candidates whose frequency is greater than 5, then applied a linguistic filter which rejects phrases that do not occur in valid noun-phrase contexts (e.g. following articles or possessives) at least once in the corpus." ></td>
	<td class="line x" title="110:154	We ranked resulting phrases using pointwise KL score, using the same smoothing method as in the bigram case." ></td>
	<td class="line x" title="111:154	Figure 7 shows the result of re-ranking keyphrases extracted from the same movie corpus." ></td>
	<td class="line x" title="112:154	We can see that bigrams and trigrams are interleaved in natural order (although not many long phrases are extracted from the dataset, since longer NP did not occur more than five times)." ></td>
	<td class="line x" title="113:154	Figure 1 was another example of the result of the same pipeline of methods." ></td>
	<td class="line x" title="114:154	5a popular citation pattern in 1993 was In article a0 1pk0a@foo.coma1, js@foo.com (John Smith) writes: One question that might be asked is what if we just sort by frequency?." ></td>
	<td class="line x" title="115:154	If we sort by frequency, blair witch project is 92nd and empire strikes back is 110th on the ranked list." ></td>
	<td class="line x" title="116:154	Since the longer the phrase becomes, the lower the frequency of the phrase is, frequency is not an appropriate method for ranking phrases." ></td>
	<td class="line x" title="117:154	1 minority report 2 box office 3 scooby doo 4 sixth sense 5 national guard 6 bourne identity 7 air national guard 8 united states 9 phantom menace 10 special effects 11 hotel room 12 comic book 13 blair witch project 14 short story 15 real life 16 jude law 17 iron giant 18 bin laden 19 black people 20 opening weekend 21 bad guy 22 country bears 23 mans man 24 long time 25 spoiler space 26 empire strikes back 27 top ten 28 politically correct 29 white people 30 tv show 31 bad guys 32 freddie prinze jr 33 monsters ball 34 good thing 35 evil minions 36 big screen 37 political correctness 38 martial arts 39 supreme court 40 beautiful mind Figure 7: Result of re-ranking output from the phrase extension module 6.4 Revisiting unigram informativeness An alternative approach to calculate informativeness from the foreground LM and the background LM is just to take the ratio of likelihood scores, a11 fga9a54a86 a15 a23 a11 bga9a54a86 a15 . This is a smoothed version of relative frequency ratio which is commonly used to find subject-specific terms (Damerau, 1993)." ></td>
	<td class="line x" title="118:154	Figure 8 compares extracted keywords ranked with pointwise KL and likelihood ratio scores, both of which use the same foreground and background unigram language model." ></td>
	<td class="line x" title="119:154	We used messages retrieved from the query Infiniti G35 as the foreground corpus and the same 20 newsgroup data as the background corpus." ></td>
	<td class="line x" title="120:154	Katz smoothing is applied to both language models." ></td>
	<td class="line x" title="121:154	As we can see, those two methods return very different ranked lists." ></td>
	<td class="line x" title="122:154	We think the pointwise KL returns a set of keywords closer to human judgment." ></td>
	<td class="line x" title="123:154	One example is the word infiniti, which we expected to be one of the informative words since it is the query word." ></td>
	<td class="line x" title="124:154	The pointwise KL score picked the word as the third informative word, but the likelihood score missed it." ></td>
	<td class="line x" title="125:154	Whereas 6mt, picked up by the likelihood ratio, which occurs 37 times in the 0 100000 200000 300000 400000 500000 600000 700000 800000 0 2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 informativeness phraseness blrt -0.001 -0.0005 0 0.0005 0.001 0.0015 0.002 -0.0005 0 0.0005 0.001 0.0015 0.002 informativeness phraseness pointKR (a) BLRT (b) LM-pointKL Figure 6: Phraseness and informativeness score of bigrams extracted with BLRT (a) and pointwise KL divergence between LMs (b)." ></td>
	<td class="line x" title="126:154	point KL likelihood ratio rank freq term freq term 1 1599 g35 1599 g35 2 1145 car 156 330i 3 450 infiniti 117 350z 4 299 coupe 113 doo 5 299 nissan 90 wrx 6 383 bmw 76 is300 7 156 330i 47 willow 8 441 cars 39 rsx 9 248 sedan 37 6mt 10 331 originally 35 scooby 11 201 altima 35 s2000 12 117 350z 33 gt-r 13 113 doo 32 lol 14 235 sport 30 heatwave 15 172 maxima 28 g22 16 90 wrx 26 gtr 17 111 skyline 23 g21 18 76 is300 23 g17 19 186 honda 23 nsx 20 221 engine 22 tl-s Figure 8: Top 20 keywords extracted using pointwise-KL and likelihood ratio (after stopwords removed) from messages retrieved from the query Infiniti G35 foreground corpus and none in the background corpus does not seem to be a good keyword." ></td>
	<td class="line x" title="127:154	The following table shows statistics of those two words:6 token a11 fga9a54a66 a15 a11 bga9a54a66 a15 PKL LR 6mt 1.837E-4 8.705E-8 0.0020 2110 infiniti 2.269E-3 4.475E-6 0.0204 506 Since the likelihood of 6mt with respect to the background LM is so small, the likelihood ratio of the word becomes very large." ></td>
	<td class="line x" title="128:154	But the pointwise KL score discounts the score appropriately by consider6infiniti occurs 34 times in the rec.autos section of the 20 newsgroup data set." ></td>
	<td class="line x" title="129:154	ing that the frequency of the word is low." ></td>
	<td class="line x" title="130:154	Likelihood ratio (or relative frequency ratio) has a tendency to pick up rare words as informative." ></td>
	<td class="line x" title="131:154	Pointwise KL seems more robust in sparse data situations." ></td>
	<td class="line x" title="132:154	One disadvantage of the pointwise KL statistic might be that it also picks up stopwords or punctuation, when there is a significant difference in style of writing, etc. , since these words have significantly high frequency." ></td>
	<td class="line x" title="133:154	But stopwords are easy to define or can be generated automatically from corpora, and we dont consider this to be a significant drawback." ></td>
	<td class="line x" title="134:154	We also expect a better background model and better smoothing mechanism could reduce the necessity of the stopword list." ></td>
	<td class="line x" title="135:154	7 Discussion Necessity of both phraseness and informativeness Although phraseness itself is domain-dependent to some extent (Smadja, 1994), we have shown that there is little correlation between informativeness and phraseness scores." ></td>
	<td class="line x" title="136:154	Combining method One way to calculate a combined score is directly comparing a7 a1 a2fg and a7 a1 a1bg in Figure 3." ></td>
	<td class="line x" title="137:154	We have tried both approaches and got a better result from combining separate phraseness and informativeness scores." ></td>
	<td class="line x" title="138:154	We think this is due to data sparseness of the higher order ngram in the background corpus." ></td>
	<td class="line x" title="139:154	Further investigation is required to make a conclusion." ></td>
	<td class="line x" title="140:154	We have used the simplest method of combining two scores by adding them." ></td>
	<td class="line x" title="141:154	We have also tried harmonic mean and geometric mean but they did not improve the result." ></td>
	<td class="line x" title="142:154	We could also apply linear interpolation to put more weight on one score value, or use an exponential model to combine score, but this will require tuning parameters." ></td>
	<td class="line x" title="143:154	Benefits of using a language model One benefit of using a language model approach is that one can take advantage of various smoothing techniques." ></td>
	<td class="line x" title="144:154	For example, by interpolating with a character-based n-gram model, we can make the LM more robust with respect to spelling errors and variations." ></td>
	<td class="line x" title="145:154	Consider the following variations, which we need to treat as a single entity: al-Qaida, al Qaida, al Qaeda, al Queda, al-Qaeda, al-Qaida, al Qaida (found in online sources)." ></td>
	<td class="line x" title="146:154	Since these are such unique spellings in English, character n-gram is expected to be able to give enough likelihood score to different spellings as well." ></td>
	<td class="line x" title="147:154	It is also easy to incorporate other models such as topic or discourse model, use a cache LM to capture local context, and a class-based LM for the shared concept." ></td>
	<td class="line x" title="148:154	It is also possible to add a phrase length prior probability in the model for better likelihood estimation." ></td>
	<td class="line x" title="149:154	Another useful smoothing technique is linear interpolation of the foreground and background language models, when the foreground and background corpus are disjoint." ></td>
	<td class="line x" title="150:154	8 Conclusion We have explained that phraseness and informativeness should be unified into a single score to return useful ranked keyphrases for analysts." ></td>
	<td class="line x" title="151:154	Our proposed approach calculates both scores based on language models and unified into a single score." ></td>
	<td class="line x" title="152:154	The phrases generated by this method are intuitively very useful, but the results are difficult to evaluate quantitatively." ></td>
	<td class="line x" title="153:154	In future work we would like to further explore evaluation of keyphrases, as well as investigate different smoothing techniques." ></td>
	<td class="line x" title="154:154	Further extensions include developing a phrase boundary segmentation algorithm based on this framework and exploring applicability to other languages." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W03-1806
Multiword Unit Hybrid Extraction
Dias, Gael;"></td>
	<td class="line x" title="1:202	Multiword Unit Hybrid Extraction Gal Dias Centre of Mathematics Beira Interior University Covilh, Portugal ddg@di.ubi.pt Abstract This paper describes an original hybrid system that extracts multiword unit candidates from part-of-speech tagged corpora." ></td>
	<td class="line x" title="2:202	While classical hybrid systems manually define local part-ofspeech patterns that lead to the identification of well-known multiword units (mainly compound nouns), our solution automatically identifies relevant syntactical patterns from the corpus." ></td>
	<td class="line x" title="3:202	Word statistics are then combined with the endogenously acquired linguistic information in order to extract the most relevant sequences of words." ></td>
	<td class="line x" title="4:202	As a result, (1) human intervention is avoided providing total flexibility of use of the system and (2) different multiword units like phrasal verbs, adverbial locutions and prepositional locutions may be identified." ></td>
	<td class="line x" title="5:202	The system has been tested on the Brown Corpus leading to encouraging results." ></td>
	<td class="line x" title="6:202	1 Introduction Multiword units (MWUs) include a large range of linguistic phenomena, such as compound nouns (e.g. interior designer), phrasal verbs (e.g. run through), adverbial locutions (e.g. on purpose), compound determinants (e.g. an amount of), prepositional locutions (e.g. in front of) and institutionalized phrases (e.g. con carne)." ></td>
	<td class="line x" title="7:202	MWUs are frequently used in everyday language, usually to precisely express ideas and concepts that cannot be compressed into a single word." ></td>
	<td class="line x" title="8:202	As a consequence, their identification is a crucial issue for applications that require some degree of semantic processing (e.g. machine translation, summarization, information retrieval)." ></td>
	<td class="line x" title="9:202	In recent years, there has been a growing awareness in the Natural Language Processing (NLP) community of the problems that MWUs pose and the need for their robust handling." ></td>
	<td class="line oc" title="10:202	For that purpose, syntactical (Didier Bourigault, 1993), statistical (Frank Smadja, 1993; Ted Dunning, 1993; Gal Dias, 2002) and hybrid syntaxicostatistical methodologies (Batrice Daille, 1996; JeanPhilippe Goldman et al. 2001) have been proposed." ></td>
	<td class="line x" title="11:202	In this paper, we propose an original hybrid system called HELAS 1 that extracts MWU candidates from part-of-speech tagged corpora." ></td>
	<td class="line x" title="12:202	Unlike classical hybrid systems that manually pre-define local part-of-speech patterns of interest (Batrice Daille, 1996; Jean-Philippe Goldman et al. 2001), our solution automatically identifies relevant syntactical patterns from the corpus." ></td>
	<td class="line x" title="13:202	Word statistics are then combined with the endogenously acquired linguistic information in order to extract the most relevant sequences of words i.e. MWU candidates." ></td>
	<td class="line x" title="14:202	Technically, we conjugate the Mutual Expectation (ME) association measure with the acquisition process called GenLocalMaxs (Gal Dias, 2002) in a five step process." ></td>
	<td class="line x" title="15:202	First, the part-of-speech tagged corpus is divided into two sub-corpora: one containing words and one containing part-of-speech tags." ></td>
	<td class="line x" title="16:202	Each sub-corpus is then segmented into a set of positional ngrams i.e. ordered vectors of textual units." ></td>
	<td class="line x" title="17:202	Third, the ME independently evaluates the degree of cohesiveness of each positional ngram i.e. any positional ngram of words and any positional ngram of part-of-speech tags." ></td>
	<td class="line x" title="18:202	A combination of both MEs is then used to evaluate the global degree of cohesiveness of any sequence of words associated with its respective part-of-speech tag sequence." ></td>
	<td class="line x" title="19:202	Finally, the GenLocalMaxs retrieves all the MWU candidates by evidencing local maxima of association measure values thus avoiding the definition of global thresholds." ></td>
	<td class="line x" title="20:202	The overall architecture can be seen in Figure 1." ></td>
	<td class="line x" title="21:202	Compared to existing hybrid systems, the benefits of HELAS are clear." ></td>
	<td class="line x" title="22:202	By avoiding human intervention in the definition of syntactical patterns, it provides total 1 HELAS stands for Hybrid Extraction of Lexical ASsociations." ></td>
	<td class="line x" title="23:202	flexibility of use." ></td>
	<td class="line x" title="24:202	Indeed, the system can be used for any language without any specific tuning." ></td>
	<td class="line x" title="25:202	HELAS also allows the identification of various MWUs like phrasal verbs, adverbial locutions, compound determinants, prepositional locutions and institutionalized phrases." ></td>
	<td class="line x" title="26:202	Finally, it responds to some extent to the affirmation of Benot Habert and Christian Jacquemin (1993) that claim that existing hybrid systems do not sufficiently tackle the problem of the interdependency between the filtering stage [the definition of syntactical patterns] and the acquisition process [the scoring and the election of relevant sequences of words] as they propose that these two steps should be independent." ></td>
	<td class="line x" title="27:202	Figure 1: Global architecture of HELAS The article is divided into five main sections: (1) we introduce the related work; (2) we present the text corpus segmentation into positional ngrams; (3) we define the Mutual Expectation and a new combined association measure; (4) we propose the GenLocalMaxs algorithm as the acquisition process; Finally, in (5), we present some results over the Brown Corpus." ></td>
	<td class="line x" title="28:202	2 Related Work For the purpose of MWU extraction, syntactical, statistical and hybrid syntaxico-statistical methodologies have been proposed." ></td>
	<td class="line x" title="29:202	On one hand, purely linguistic systems (Didier Bourigault, 1993) propose to extract relevant MWUs by using techniques that analyse specific syntactical structures in the texts." ></td>
	<td class="line x" title="30:202	However, these methodologies suffer from their monolingual basis as the systems require highly specialised linguistic techniques to identify clues that isolate possible MWU candidates." ></td>
	<td class="line oc" title="31:202	On the other hand, purely statistical systems (Frank Smadja, 1993; Ted Dunning, 1993; Gal Dias, 2002) extract discriminating MWUs from text corpora by means of association measure regularities." ></td>
	<td class="line p" title="32:202	As they use plain text corpora and only require the information appearing in texts, such systems are highly flexible and extract relevant units independently from the domain and the language of the input text." ></td>
	<td class="line n" title="33:202	However, these methodologies can only identify textual associations in the context of their usage." ></td>
	<td class="line n" title="34:202	As a consequence, many relevant structures can not be introduced directly into lexical databases as they do not guarantee adequate linguistic structures for that purpose." ></td>
	<td class="line x" title="35:202	Finally, hybrid syntactico-statistical systems (Batrice Daille, 1996; Jean-Philippe Goldman et al. 2001) define co-occurrences of interest in terms of syntactical patterns and statistical regularities." ></td>
	<td class="line x" title="36:202	Thus, such systems reduce the searching space to groups of words that correspond to a priori defined syntactical patterns (e.g. Adj+Noun, Noun+Prep+Noun) and apply statistical scores to identify the most relevant sequences of words." ></td>
	<td class="line x" title="37:202	One major drawback of such systems is that they do not deal with a great proportion of interesting MWUs (e.g. phrasal verbs, prepositional locutions)." ></td>
	<td class="line x" title="38:202	Moreover, they lack flexibility as the syntactical patterns have to be revised whenever the targeted language changes." ></td>
	<td class="line x" title="39:202	In order to overcome these difficulties, we propose an original architecture that combines word statistics with endogenously acquired linguistic information." ></td>
	<td class="line x" title="40:202	We base our study on two assumptions." ></td>
	<td class="line x" title="41:202	On one hand, a great deal of studies in lexicography and terminology assess that most of the MWUs evidence well-known morphosyntactic structures (Gaston Gross, 1996)." ></td>
	<td class="line x" title="42:202	On the other hand, MWUs are recurrent combinations of words." ></td>
	<td class="line x" title="43:202	Indeed, according to Benot Habert and Christian Jacquemin (1993), the MWUs may represent a fifth of the overall surface of a text." ></td>
	<td class="line x" title="44:202	Consequently, it is reasonable to think that the syntactical patterns embodied by the MWUs may be endogenously identified by using statistical scores over texts of part-of-speech tags exactly in the same manner as word dependencies are identified in corpora of words." ></td>
	<td class="line x" title="45:202	So, the global degree of cohesiveness of any sequence of words may be evaluated by a combination of its degree of cohesiveness of words and the degree of cohesiveness of its associated part-of-speech tag sequence (See Figure 1)." ></td>
	<td class="line x" title="46:202	Compared to existing systems, the benefits of our architecture are clear." ></td>
	<td class="line x" title="47:202	By avoiding human intervention in the definition of syntactical patterns, (1) HELAS provides total flexibility of use being independent of the targeted Input Tagged Text Text of Words Text of Tags Word ngrams Tag ngrams ME (word ngram) ME (word ngram)  x ME (tag ngram) 1 ME (tag ngram) GenLocalMaxs MWU candidates language and (2) it allows the identification of various MWUs like phrasal verbs, adverbial locutions, compound determinants, prepositional locutions and institutionalized phrases." ></td>
	<td class="line x" title="48:202	3 Text Segmentation Positional ngrams are nothing more than ordered vectors of textual units which principles are introduced in the next subsection." ></td>
	<td class="line x" title="49:202	3.1 Positional Ngrams The original idea of the positional ngram model (Gal Dias, 2002) comes from the lexicographic evidence that most lexical relations associate words separated by at most five other words (John Sinclair, 1974)." ></td>
	<td class="line x" title="50:202	As a consequence, lexical relations such as MWUs can be continuous or discontinuous sequences of words in a context of at most eleven words (i.e. 5 words to the left of a pivot word, 5 words to the right of the same pivot word and the pivot word itself)." ></td>
	<td class="line x" title="51:202	In general terms, a MWU can be defined as a specific continuous or discontinuous sequence of words in a (2.F+1)-word size window context (i.e. F words to the left of a pivot word, F words to the right of the same pivot word and the pivot word itself)." ></td>
	<td class="line x" title="52:202	This situation is illustrated in Figure 2 for the multiword unit Ngram Statistics that fits in the window context of size 2.3+1=7." ></td>
	<td class="line x" title="53:202	Figure 2: 7-word size window context Thus, any substring (continuous or discontinuous) that fits inside the window context and contains the pivot word is called a positional word ngram." ></td>
	<td class="line x" title="54:202	For instance, [Ngram Statistics] is a positional word ngram as is the discontinuous sequence [Ngram ___ from] where the gap represented by the underline stands for any word occurring between Ngram and from (in this case, Statistics)." ></td>
	<td class="line x" title="55:202	More examples are given in Table 1." ></td>
	<td class="line x" title="56:202	Positional word 2grams Positional word 3grams [Ngram Statistics] [Ngram Statistics from] [Ngram ___ from] [Ngram Statistics ___ Large] [Ngram ___ ___ Large] [Ngram ___ from Large] [to ___ Ngram] [to ___ Ngram ___ from] Table 1: Possible positional ngrams Generically, any positional word ngram may be defined as a vector of words [p 11 u 1 p 12 u 2  p 1n u n ] where u i stands for any word in the positional ngram and p 1i represents the distance that separates words u 1 and u i 2." ></td>
	<td class="line x" title="57:202	Thus, the positional word ngram [Ngram Statisitcs] would be rewritten as [0 Ngram +1 Statistics]." ></td>
	<td class="line x" title="58:202	More examples are given in Table 2." ></td>
	<td class="line x" title="59:202	Positional word ngrams Algebraic notation [Ngram ___ from] [0 Ngram +2 from] [Ngram ___ ___ Large] [0 Ngram +3 Large] [to ___ Ngram] [0 to +2 Ngram] [Ngram Statistics ___ Large] [0 Ngram +1 Statisitcs +3 Large] Table 2: Algebraic Notation However, in a part-of-speech tagged corpus, each word is associated to a unique part-of-speech tag." ></td>
	<td class="line x" title="60:202	As a consequence, each positional word ngram is linked to a corresponding positional tag ngram." ></td>
	<td class="line x" title="61:202	A positional tag ngram is nothing more than an ordered vector of part-of-speech tags exactly in the same way a positional word ngram is an ordered vector of words." ></td>
	<td class="line x" title="62:202	Lets exemplify this situation." ></td>
	<td class="line x" title="63:202	Lets consider the following portion of a part-ofspeech tagged sentence following the Brown tag set: Virtual /JJ Approach /NN to /IN Deriving /VBG Ngram /NN Statistics /NN from /IN Large /JJ Scale /NN Corpus /NN It is clear that the corresponding positional tag ngram of the positional word ngram [0 Ngram +1 Statisitcs] is the vector [0 /NN +1 /NN]." ></td>
	<td class="line x" title="64:202	More examples are in Table 3." ></td>
	<td class="line x" title="65:202	Generically, any positional tag ngram may be defined as a vector of part-of-speech tags [p 11 t 1 p 12 t 2  p 1n t n ] where t i stands for any part-of-speech tag in the positional tag ngram and p 1i represents the distance that separates the part-of-speech tags t 1 and t i . Positional word ngrams Positional tag ngrams [0 Ngram +2 from] [0 /NN +2 /IN] [0 Ngram +3 Large] [0 /NN +3 /JJ] [0 to +2 Ngram] [0 /IN +2 /NN] [0 Ngram +1 Statisitcs +3 Large] [0 /NN +1 /NN +3 /JJ] Table 3: Positional tag ngrams So, any sequence of words, in a part-of-speech tagged corpus, is associated to a positional word ngram and a corresponding positional tag ngram." ></td>
	<td class="line x" title="66:202	In order to introduce the part-of-speech tag factor in any sequence of words of part-of-speech tagged corpus, we present an alternative notation of positional ngrams called positional word-tag ngrams." ></td>
	<td class="line x" title="67:202	In order to represent a sequence of words with its associated part-of-speech tags, a positional ngram may be represented by the following vector of words and part2 By statement, any p ii is equal to zero." ></td>
	<td class="line x" title="68:202	Virtual Approach to Deriving Ngram Statistics from Large Scale pivot F=3 F=3 of-speech tags [p 11 u 1 t 1 p 12 u 2 t 2  p 1n u n t n ] where u i stands for any word in the positional ngram, t i stands for the part-of-speech tag of the word u i and p 1i represents the distance that separates words u 1 and u i . Thus, the positional ngram [Ngram Statistics] can be represented by the vector [0 Ngram /NN +1 Statistics /NN] given the text corpus in section (3.1)." ></td>
	<td class="line x" title="69:202	More examples are given in Table 4." ></td>
	<td class="line x" title="70:202	Positional ngrams Alternative notation [Ngram ___ from] [0 Ngram /NN +2 from /IN] [Ngram ___ ___ Large] [0 Ngram /NN +3 Large /JJ] [to ___ Ngram] [0 to /IN +2 Ngram /NN] Table 4: Alternative Notation This alternative notation will allow us to defining, with elegance, our combined association measure, introduced in the next section." ></td>
	<td class="line x" title="71:202	3.2 Data Preparation So, the first step of our architecture deals with segmenting the input text corpus into positional ngrams." ></td>
	<td class="line x" title="72:202	First, the part-of-speech tagged corpus is divided into two sub-corpora: one sub-corpus of words and one subcorpus of part-of-speech tags." ></td>
	<td class="line x" title="73:202	The word sub-corpus is then segmented into its set of positional word ngrams exactly in the same way the tagged sub-corpus is segmented into its set of positional tag ngrams." ></td>
	<td class="line x" title="74:202	In parallel, each positional word ngram is associated to its corresponding positional tag ngram in order to further evaluate the global degree of cohesiveness of any sequence of words in a part-of-speech tagged corpus." ></td>
	<td class="line x" title="75:202	Our basic idea is to evaluate the degree of cohesiveness of each positional ngram independently (i.e. the positional word ngrams on one side and the positional tag ngrams on the other side) in order to calculate the global degree of cohesiveness of any sequence in the part-ofspeech tagged corpus by combining its respective degrees of cohesiveness i.e. the degree of cohesiveness of its sequence of words and the degree of cohesiveness of its sequence of part-of-speech tags." ></td>
	<td class="line x" title="76:202	In order to evaluate the degree of cohesiveness of any sequence of textual units, we use the association measure called Mutual Expectation." ></td>
	<td class="line x" title="77:202	4 Cohesiveness Evaluation The Mutual Expectation (ME) has been introduced by Gal Dias (2002) and evaluates the degree of cohesiveness that links together all the textual units contained in a positional ngram (n, n  2) based on the concept of Normalized Expectation and relative frequency." ></td>
	<td class="line x" title="78:202	4.1 Normalized Expectation The basic idea of the Normalized Expectation (NE) is to evaluate the cost, in terms of cohesiveness, of the loss of one element in a positional ngram." ></td>
	<td class="line x" title="79:202	Thus, the NE is defined in Equation 1 where the function k()." ></td>
	<td class="line x" title="80:202	returns the frequency of any positional ngram 3 . [ ]( ) []() []()                       + =  = n i kk n k NE 2 n1n ^ i ^ 1i1 11n2n i 2i2 22 n1ni1i1 11 n1ni1i1 11 u p  u p  upup  up  up 1 u p  u p up u p  u p up Equation 1: Normalized Expectation In order to exemplify the NE formula, we present in Equation 2 its development for the given positional ngram [0 A +2 C +3 D +4 E] where each letter may represent a word or a part-of-speech tag." ></td>
	<td class="line x" title="81:202	[]() []() []() []()               + + + = E 2,D 1,C0 E 4,D 3,A0 E 4,C 2,A0 D 3,C 2,A0 4 1 E 4,D 3,C 2,A0 E 4 D, 3,C 2,A0 k k k k k NE Equation 2: Normalized Expectation example However, evaluating the average cost of the loss of an element is not enough to characterize the degree of cohesiveness of a sequence of textual units." ></td>
	<td class="line x" title="82:202	The Mutual Expectation is introduced to solve this insufficiency." ></td>
	<td class="line x" title="83:202	4.2 Mutual Expectation Many applied works in Natural Language Processing have shown that frequency is one of the most relevant statistics to identify relevant textual associations." ></td>
	<td class="line x" title="84:202	For instance, in the context of multiword unit extraction, (John Justeson and Slava Katz, 1995; Batrice Daille, 1996) assess that the comprehension of a multiword unit is an iterative process being necessary that a unit should be pronounced more than one time to make its comprehension possible." ></td>
	<td class="line x" title="85:202	Gel Dias (2002) believes that this phenomenon can be enlarged to part-of-speech tags." ></td>
	<td class="line x" title="86:202	From this assumption, they pose that between two positional ngrams with the same NE, the most frequent positional ngram is more likely to be a relevant sequence." ></td>
	<td class="line x" title="87:202	So, the Mutual Expectation of any positional ngram is defined in Equation 3 based on its NE and its relative frequency embodied by the function p(.)." ></td>
	<td class="line x" title="88:202	3 The '^' corresponds to a convention used in Algebra that consists in writing a '^' on the top of the omitted term of a given succession indexed from 1 to n. []() []()n1ni1i1 11n1ni1i1 11 n1ni1i1 11 u p  u p upu p  u p up u p  u p up NEp ME  = Equation 3: Mutual Expectation We will note that the ME shows interesting properties." ></td>
	<td class="line x" title="89:202	One of them is the fact that it does not sub-evaluate interdependencies when frequent individual textual units are present." ></td>
	<td class="line x" title="90:202	In particular, this allows us to avoid the use of lists of stop words." ></td>
	<td class="line x" title="91:202	Thus, when calculating all the positional ngrams, all the words and part-of-speech tags are used." ></td>
	<td class="line x" title="92:202	This fundamentally participates to the flexibility of use of our system." ></td>
	<td class="line x" title="93:202	As we said earlier, the ME is going to be used to calculate the degree cohesiveness of any positional word ngram and any positional tag ngram." ></td>
	<td class="line x" title="94:202	The way we calculate the global degree of cohesiveness of any sequence of words associated to its part-of-speech tag sequence, based on its two MEs, is discussed in the next subsection." ></td>
	<td class="line x" title="95:202	4.3 Combined Association Measure The drawbacks shown by the statistical methodologies evidence the lack of linguistic information." ></td>
	<td class="line x" title="96:202	Indeed, these methodologies can only identify textual associations in the context of their usage." ></td>
	<td class="line x" title="97:202	As a consequence, many relevant structures can not be introduced directly into lexical databases as they do not guarantee adequate linguistic structures for that purpose." ></td>
	<td class="line x" title="98:202	In this paper, we propose a first attempt to solve this problem without pre-defining syntactical patterns of interest that bias the extraction process." ></td>
	<td class="line x" title="99:202	Our idea is simply to combine the strength existing between words in a sequence and the evidenced interdependencies between its part-of-speech tags." ></td>
	<td class="line x" title="100:202	We could summarize this idea as follows: the more cohesive the words of a sequence and the more cohesive its part-of-speech tags, the more likely the sequence may embody a multiword unit." ></td>
	<td class="line x" title="101:202	This idea can only be supported due to two assumptions." ></td>
	<td class="line x" title="102:202	On one hand, a great deal of studies in lexicography and terminology assess that most of the MWUs evidence well-known morpho-syntactic structures (Gaston Gross, 1996)." ></td>
	<td class="line x" title="103:202	On the other hand, MWUs are recurrent combinations of words capable of representing a fifth of the overall surface of a text (Benot Habert and Christian Jacquemin, 1993)." ></td>
	<td class="line x" title="104:202	Consequently, it is reasonable to think that the syntactical patterns embodied by the MWUs may endogenously be identified by using statistical scores over texts of part-of-speech tags exactly in the same manner as word dependencies are identified in corpora of words." ></td>
	<td class="line x" title="105:202	So, the global degree of cohesiveness of any sequence of words may be evaluated by a combination of its own ME and the ME of its associated partof-speech tag sequence." ></td>
	<td class="line x" title="106:202	The degree of cohesiveness of any positional ngram based on a part-of-speech tagged corpus can then be evaluated by the combined association measure (CAM) defined in Equation 4 where  stands as a parameter that tunes the focus whether on words or on part-of-speech tags." ></td>
	<td class="line x" title="107:202	[ ]( ) []()[]()    = 1 n1ni1i1 11n1ni1i1 11 nn 1nii1i11 11 tp  tp tpu p  u p up tu p  tu p tup MEME CAM Equation 4: Combined Association Measure We will see in the final section of this paper that different values of  lead to fundamentally different sets of multiword unit candidates." ></td>
	<td class="line x" title="108:202	Indeed,  can go from a total focus on part-of-speech tags (i.e. the relevance of a word sequence is based only on the relevance of its partof-speech sequence) to a total focus on words (i.e. the relevance of a word sequence is defined only by its word dependencies)." ></td>
	<td class="line x" title="109:202	Before going to experimentation, we need to introduce the used acquisition process which objective is to extract the MWUs candidates." ></td>
	<td class="line x" title="110:202	5 The Acquisition Process The GenLocalMaxs (Gal Dias, 2002) proposes a flexible and fine-tuned approach for the selection process as it concentrates on the identification of local maxima of association measure values." ></td>
	<td class="line x" title="111:202	Specifically, the GenLocalMaxs elects MWUs from the set of all the valued positional ngrams based on two assumptions." ></td>
	<td class="line x" title="112:202	First, the association measures show that the more cohesive a group of words is, the higher its score will be." ></td>
	<td class="line x" title="113:202	Second, MWUs are localized associated groups of words." ></td>
	<td class="line x" title="114:202	So, we may deduce that a positional word-tag ngram is a MWU if its combined association measure value is higher or equal than the combined association measure values of all its sub-groups of (n-1) words and if it is strictly higher than the combined association measure values of all its super-groups of (n+1) words." ></td>
	<td class="line x" title="115:202	Let cam be the combined association measure, W a positional word-tag ngram,  n-1 the set of all the positional word-tag (n-1)grams contained in W,  n+1 the set of all the positional word-tag (n+1)-grams containing W and sizeof()." ></td>
	<td class="line x" title="116:202	a function that returns the number of words of a positional word-tag ngram." ></td>
	<td class="line x" title="117:202	The GenLocalMaxs is defined as: x  n-1, y  n+1, W is a MWU if (sizeof(W)=2  cam(W) > cam(y) )  (sizeof(W)2  cam(W)  cam(x)  cam(W) > cam(y)) Definition 1: GenLocalMaxs Algorithm Among others, the GenLocalMaxs shows one important property: it does not depend on global thresholds." ></td>
	<td class="line x" title="118:202	A direct implication of this characteristic is the fact that, as no tuning needs to be made in order to acquire the set of all the MWU candidates, the use of the system remains as flexible as possible." ></td>
	<td class="line x" title="119:202	Finally, we show the results obtained by applying HELAS over the Brown Corpus." ></td>
	<td class="line x" title="120:202	6 The Experiments In order to test our architecture, we have conducted a number of experiments with 11 different values of  for a portion of the Brown Corpus containing 249 578 words i.e. 249 578 words plus its 249 578 part-ofspeech tags." ></td>
	<td class="line x" title="121:202	The limited size of our corpus is mainly due to the space complexity of our system." ></td>
	<td class="line x" title="122:202	Indeed, the number of computed positional ngrams is huge even for a small corpus." ></td>
	<td class="line x" title="123:202	For instance, 21 463 192 positional ngrams are computed for this particular corpus for a 7word size window context." ></td>
	<td class="line x" title="124:202	As a consequence, computation is hard." ></td>
	<td class="line x" title="125:202	For this experiment, HELAS has been tested on a personal computer with 128 Mb of RAM, 20 Gb of Hard Disk and an AMD 1.4 Ghz processor under Linux Mandrake 7.2." ></td>
	<td class="line x" title="126:202	On average, each experiment (i.e. for a given  ) took 4 hours and 20 minutes." ></td>
	<td class="line x" title="127:202	Knowing that our system increases proportionally with the size of the corpus, it was unmanageable, for this particular experiment, to test our architecture over a bigger corpus." ></td>
	<td class="line x" title="128:202	Even though, the whole processing stage lasted almost 48 hours 4 . We will divide our experiment into two main parts." ></td>
	<td class="line x" title="129:202	First, we will do a quantitative analysis and then we will lead a qualitative analysis." ></td>
	<td class="line x" title="130:202	All results will only tackle contiguous multiword units although non-contiguous sequences may be extracted." ></td>
	<td class="line x" title="131:202	This decision is due to the lack of space." ></td>
	<td class="line x" title="132:202	6.1 Quantitative Analysis In order to understand, as deeply as possible, the interaction between word cohesiveness and part-of-speech tag cohesiveness, we chose eleven different values for , i.e.   {0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1}, going from total focus on words ( = 1) to total focus on partof-speech tags ( = 0)." ></td>
	<td class="line x" title="133:202	First, we show the number of extracted contiguous MWU candidates by  in table 5." ></td>
	<td class="line x" title="134:202	The total results are not surprising." ></td>
	<td class="line x" title="135:202	Indeed, with  = 0, the focus is exclusively on part-of-speech tags." ></td>
	<td class="line x" title="136:202	It means that any word sequence, with an identified relevant part-of-speech sequence, is extracted independently of the words it contains." ></td>
	<td class="line x" title="137:202	For instance, all the word sequences with the pattern [/JJ /NN] (i.e. Adjective + Noun) may be ex4 We are already working on an efficient implementation of HELAS using suffix-arrays and the concept of masks." ></td>
	<td class="line x" title="138:202	tracted independently of their word dependencies!" ></td>
	<td class="line x" title="139:202	This obviously leads to an important number of extracted sequences." ></td>
	<td class="line x" title="140:202	The inclusion of the word factor, by increasing the value of , progressively leads to a decreasing number of extracted positional ngrams." ></td>
	<td class="line x" title="141:202	In fact, the word sequences with relevant syntactical structures are being filtered out depending on their word statistics." ></td>
	<td class="line x" title="142:202	Finally, with  = 1, the focus is exclusively on words." ></td>
	<td class="line x" title="143:202	The impact of the syntactical structure is null and the positional ngrams are extracted based on their word associations." ></td>
	<td class="line x" title="144:202	In this case, the word sequences do not form classes of morpho-syntactic structures being the reason why less positional ngrams are extracted." ></td>
	<td class="line x" title="145:202	alpha 0 0.1 0.2 0.3 0.4 0.5 2gram 23146 21890 20074 17689 15450 13461 3gram 297 467 567 351 1188 1693 4gram 86 108 127 163 225 326 5gram 79 81 81 82 77 82 6gram 62 57 56 57 56 58 TOTAL 23670 22603 20905 18342 16996 15620 alpha 0.6 0.7 0.8 0.9 1.0 2gram 11531 9950 9114 8650 8465 3gram 2147 2501 2728 2828 2651 4gram 428 557 679 740 484 5gram 93 112 128 161 145 6gram 58 58 60 64 60 TOTAL 14257 13178 12709 12443 11805 Table 5: Number of extracted MWU candidates A deeper analysis of table 5 reveals interesting results." ></td>
	<td class="line x" title="146:202	The smaller the values of , the more positional 2grams are extracted." ></td>
	<td class="line x" title="147:202	This situation is illustrated in Figure 3." ></td>
	<td class="line x" title="148:202	# of ext ract ed ngrams by alpha 0 5000 10 0 0 0 15 0 0 0 20000 25000 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 alpha 2gr am 3gr am 4gr am 5gr am 6gr am Figure 3: Number of extracted MWU candidates Once again these results are not surprising." ></td>
	<td class="line x" title="149:202	The Mutual Expectation tends to give more importance to frequent sequences of textual units." ></td>
	<td class="line x" title="150:202	While it performs reasonably well on word sequences, it tends to over-evaluate the part-of-speech tag sequences." ></td>
	<td class="line x" title="151:202	Indeed, sequences of two part-of-speech tags are much more frequent than other types of sequences and, as a consequence, tend to be over-evaluated in terms of cohesiveness." ></td>
	<td class="line x" title="152:202	As small values of  focus on syntactical structures, it is clear that in this case, small sequences of words are preferred over longer sequences." ></td>
	<td class="line x" title="153:202	By looking at Figure 3 and Table 5, we may think that a great number of extracted sequences are common to each experiment." ></td>
	<td class="line x" title="154:202	However, this is not true." ></td>
	<td class="line x" title="155:202	In order to assess this affirmation, we propose, in Table 6, the summary of the identical ratio." ></td>
	<td class="line x" title="156:202	alphas 0 0.1 0.2 0.3 0.4 0.5 0 14.64 5.74 2.99 1.73 1.17 0.1 9.99 3.77 2.08 1.35 0.2 6.2 2.83 1.69 0.3 4.89 2.36 0.4 5.31 0.5 alphas 0.6 0.7 0.8 0.9 1.0 0 0.83 0.63 0.54 0.49 0.47 0.1 0.93 0.70 0.59 0.54 0.52 0.2 1.11 0.81 0.68 0.61 0.59 0.3 1.42 0.98 0.81 0.72 0.69 0.4 2.34 1.44 1.13 0.97 0.90 0.5 4.77 2.26 1.62 1.33 1.17 0.6 5.06 2.82 2.10 1.73 0.7 7.21 3.99 2.81 0.8 9.45 4.50 0.9 7.71 1.0 Table 6: Identical Ratio The identical ratio calculates, for two values of , the quotient between the number of identical extracted sequences and the number of different extracted sequences." ></td>
	<td class="line x" title="157:202	Thus, the first value of the first row of table 6, represents the identical ratio for  =0 and  =0.1, and means that there are 14.64 times more identical extracted sequences than different sequences between both experiments." ></td>
	<td class="line x" title="158:202	Taking  =0 and  =1, it is interesting to see that there are much more different sequences than identical sequences between both experiments (identical ratio = 0.47)." ></td>
	<td class="line x" title="159:202	In fact, this phenomenon progressively increases as the word factor is being introduced in the combined association measure to reach  =1." ></td>
	<td class="line x" title="160:202	This was somewhat unexpected." ></td>
	<td class="line x" title="161:202	Nevertheless, this situation can be partly decrypted from Figure 3." ></td>
	<td class="line x" title="162:202	Indeed, figure 3 shows that longer sequences are being preferred as  increases." ></td>
	<td class="line x" title="163:202	In fact, what happens is that short syntactically wellfounded sequences are being replaced by longer word sequences that may lack linguistic information." ></td>
	<td class="line x" title="164:202	For instance, the sequence [Blue Mosque] was extracted with  =0, although the longer sequence [the Blue Mosque] was preferred with  =1 as whenever [Blue Mosque] appears in the text, the determinant [the] precedes it." ></td>
	<td class="line x" title="165:202	Finally, a last important result concerns the frequency of the extracted sequences." ></td>
	<td class="line x" title="166:202	Table 7 gives an overview of the situation." ></td>
	<td class="line x" title="167:202	The figures are clear." ></td>
	<td class="line x" title="168:202	Most of the extracted sequences occur only twice in the input text corpus." ></td>
	<td class="line x" title="169:202	This result is rather encouraging as most known extractors need high frequencies in order to decide whether a sequence is a MWU or not." ></td>
	<td class="line x" title="170:202	This situation is mainly due to the GenLocalMaxs algorithm." ></td>
	<td class="line oc" title="171:202	alpha 0 0.1 0.2 0.3 0.4 0.5 Freq=2 13555 13093 12235 11061 10803 10458 Freq=3 4203 3953 3616 3118 2753 2384 Freq=4 1952 1839 1649 1350 1166 960 Freq=5 1091 1019 917 743 608 511 Freq>2 2869 2699 2488 2070 1666 1307 TOTAL 23670 22603 20905 18342 16996 15620 alpha 0.6 0.7 0.8 0.9 1.0 Freq=2 10011 9631 9596 9554 9031 Freq=3 2088 1858 1730 1685 1678 Freq=4 766 617 524 485 468 Freq=5 392 276 232 202 189 Freq>2 1000 796 627 517 439 TOTAL 14257 13178 12709 12443 11805 Table 7: Number of extracted MWUs by frequency 6.2 Qualitative Analysis As many authors assess (Frank Smadja, 1993; John Justeson and Slava Katz, 1995), deciding whether a sequence of words is a multiword unit or not is a tricky problem." ></td>
	<td class="line x" title="172:202	For that purpose, different definitions of multiword unit have been proposed." ></td>
	<td class="line x" title="173:202	One of the most successful attempts can be attributed to Gaston Gross (1996) that classifies multiword units into six groups and provides techniques to determine their belonging." ></td>
	<td class="line x" title="174:202	As a consequence, we intend as multiword unit any compound noun (e.g. interior designer), compound determinant (e.g. an amount of), verbal locution (e.g. run through), adverbial locution (e.g. on purpose), adjectival locution (e.g. dark blue) or prepositional locution (e.g. in front of)." ></td>
	<td class="line x" title="175:202	The analysis of the results has been done intramuros although we are aware that an external independent cross validation would have been more suited." ></td>
	<td class="line x" title="176:202	However, it was not logistically possible to do so and by using Gaston Grosss classification and methodology, we narrow the human error evaluation as much as possible." ></td>
	<td class="line x" title="177:202	Technically, we have randomly extracted and analysed 200 positional 2grams, 200 positional 3grams and 100 positional 4grams for each value of  . For the specific case of positional 5grams and 6grams, all the sequences have been analysed." ></td>
	<td class="line x" title="178:202	Precision results of this analysis are given in table 8 and show that word dependencies and part-of-speech tag dependencies may both play an important role in the identification of relevant sequences." ></td>
	<td class="line x" title="179:202	Indeed, values of  between 0.4 and 0.5 seem to lead to optimum results." ></td>
	<td class="line x" title="180:202	Knowing that most extracted sequences are positional 2grams or positional 3grams, the global precision results approximate the results given by 2grams and 3grams." ></td>
	<td class="line x" title="181:202	In these conditions, the best results are for  =0.5 reaching an average precision of 62 %." ></td>
	<td class="line x" title="182:202	This would mean that word dependencies and part-of-speech tags contribute equally to multiword unit identification." ></td>
	<td class="line x" title="183:202	alpha 0 0.1 0.2 0.3 0.4 0.5 2gram 29 % 22 % 30 % 44 % 53 % 60 % 3gram 52 % 77 % 74 % 73 % 80 % 85 % 4gram 38 % 32 % 32 % 46 % 47 % 41 % 5gram 34 % 28 % 29 % 31 % 33 % 34 % 6gram 29 % 22 % 18 % 24 % 31 % 38 % alpha 0.6 0.7 0.8 0.9 1.0 2gram 45 % 23 % 25 % 18 % 30 % 3gram 43 % 35 % 46 % 51 % 36 % 4gram 41 % 45 % 39 % 44 % 37 % 5gram 27 % 27 % 29 % 38 % 38 % 6gram 32 % 37 % 26 % 29 % 29 % Table 8: Precision in % by alpha A deeper look at the results evidences interesting regularities as shown in figure 4." ></td>
	<td class="line x" title="184:202	Indeed, the curves for 4grams, 5grams and 6grams are reasonably steady along the X axis evidencing low results." ></td>
	<td class="line x" title="185:202	This means, to some extent, that that our system does not seem to be able to tackle successfully multiword units with more than three words." ></td>
	<td class="line x" title="186:202	In fact, neither a total focus on words or on part-of-speech tags seems to change the extraction results." ></td>
	<td class="line x" title="187:202	However, the importance of these results must be weakened as they represent a small proportion of the extracted structures." ></td>
	<td class="line x" title="188:202	Precision by alpha and ngram 0% 20% 40% 60% 80% 100% 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 alpha pr eci si on ( % ) 2gram 3gram 4gram 5gram 6gram Figure 4: Precision by alpha and ngram On the other hand, the curves for 2grams and 3grams show different behaviours." ></td>
	<td class="line x" title="189:202	For the 3gram case, it seems that the syntactical structure plays an important role in the identification process." ></td>
	<td class="line x" title="190:202	Indeed, precision falls down drastically when the focus passes to word dependencies." ></td>
	<td class="line x" title="191:202	This is mainly due to the extraction of recurrent sequences of words that do not embody multiword unit syntactical structures like [been able to] or [can still be]." ></td>
	<td class="line x" title="192:202	As 2grams are concerned, the situation is different." ></td>
	<td class="line x" title="193:202	In fact, it seems that too much focus on either words or part-of-speech tags leads to unsatisfactory results." ></td>
	<td class="line x" title="194:202	Indeed, optimum results are obtained for a balance between both criteria." ></td>
	<td class="line x" title="195:202	This result can be explained by the fact that there exist many recurrent sequences of two words in a corpus." ></td>
	<td class="line x" title="196:202	However, most of them are not multiword units like [of the] or [can be]." ></td>
	<td class="line x" title="197:202	For that reason, only a balanced weight on part-of-speech tag and word dependencies may identify relevant two word sequences." ></td>
	<td class="line x" title="198:202	However, not-so-high precision results show that twoword sequences still remain a tricky problem for our extractor as it is difficult to filter out very frequent patterns that embody meaningless syntactical structures." ></td>
	<td class="line x" title="199:202	7 Conclusion This paper describes an original hybrid system that extracts multiword unit candidates by endogenously identifying relevant syntactical patterns from the corpus and by combining word statistics with the acquired linguistic information." ></td>
	<td class="line x" title="200:202	As a result, by avoiding human intervention in the definition of syntactical patterns, (1) HELAS provides total flexibility of use being independent of the targeted language and (2) it allows the identification of various MWUs like compound nouns, compound determinants, verbal locutions, adverbial locutions, prepositional locutions and adjectival locutions without defining any threshold or using lists of stop words." ></td>
	<td class="line x" title="201:202	The system has been tested on the Brown Corpus leading to encouraging results evidenced by a precision score of 62 % for the best configuration." ></td>
	<td class="line x" title="202:202	The system will soon be available on http://helas.di.ubi.pt." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W03-1807
Extracting Multiword Expressions With A Semantic Tagger
Piao, Scott S. L.;Rayson, Paul;Archer, Dawn;Wilson, Andrew;McEnery, Tony;"></td>
	<td class="line x" title="1:193	Extracting Multiword Expressions with A Semantic Tagger Scott S. L. Piao Dept. of Linguistics and MEL Lancaster University s.piao@lancaster.ac.uk Paul Rayson Computing Department Lancaster University paul@comp.lancs.ac.uk Dawn Archer Dept. of Linguistics and MEL Lancaster University d.archer@lancaster.ac.uk Andrew Wilson Dept. of Linguistics and MEL Lancaster University eiaaw@exchange.lancs.ac.uk Tony McEnery Dept. of Linguistics and MEL Lancaster University amcenery@lancaster.ac.uk Abstract Automatic extraction of multiword expressions (MWE) presents a tough challenge for the NLP community and corpus linguistics." ></td>
	<td class="line x" title="2:193	Although various statistically driven or knowledge-based approaches have been proposed and tested, efficient MWE extraction still remains an unsolved issue." ></td>
	<td class="line x" title="3:193	In this paper, we present our research work in which we tested approaching the MWE issue using a semantic field annotator." ></td>
	<td class="line x" title="4:193	We use an English semantic tagger (USAS) developed at Lancaster University to identify multiword units which depict single semantic concepts." ></td>
	<td class="line x" title="5:193	The Meter Corpus (Gaizauskas et al. , 2001; Clough et al. , 2002) built in Sheffield was used to evaluate our approach." ></td>
	<td class="line x" title="6:193	In our evaluation, this approach extracted a total of 4,195 MWE candidates, of which, after manual checking, 3,792 were accepted as valid MWEs, producing a precision of 90.39% and an estimated recall of 39.38%." ></td>
	<td class="line x" title="7:193	Of the accepted MWEs, 68.22% or 2,587 are low frequency terms, occurring only once or twice in the corpus." ></td>
	<td class="line x" title="8:193	These results show that our approach provides a practical solution to MWE extraction." ></td>
	<td class="line x" title="9:193	1 Introduction 2 Automatic extraction of Multiword expressions (MWE) is an important issue in the NLP community and corpus linguistics." ></td>
	<td class="line x" title="10:193	An efficient tool for MWE extraction can be useful to numerous areas, including terminology extraction, machine translation, bilingual/multilingual MWE alignment, automatic interpretation and generation of language." ></td>
	<td class="line x" title="11:193	A number of approaches have been suggested and tested to address this problem." ></td>
	<td class="line x" title="12:193	However, efficient extraction of MWEs still remains an unsolved issue, to the extent that Sag et al.(2001b) call it a pain in the neck of NLP." ></td>
	<td class="line x" title="14:193	In this paper, we present our work in which we approach the issue of MWE extraction by using a semantic field annotator." ></td>
	<td class="line x" title="15:193	Specifically, we use the UCREL Semantic Analysis System (henceforth USAS), developed at Lancaster University to identify multiword units that depict single semantic concepts, i.e. multiword expressions." ></td>
	<td class="line x" title="16:193	We have drawn from the Meter Corpus (Gaizauskas et al. , 2001; Clough et al. , 2002) a collection of British newspaper reports on court stories to evaluate our approach." ></td>
	<td class="line x" title="17:193	Our experiment shows that it is efficient in identifying MWEs, in particular MWEs of low frequencies." ></td>
	<td class="line x" title="18:193	In the following sections, we describe this approach to MWE extraction and its evaluation." ></td>
	<td class="line oc" title="19:193	Related Works Generally speaking, approaches to MWE extraction proposed so far can be divided into three categories: a) statistical approaches based on frequency and co-occurrence affinity, b) knowledgebased or symbolic approaches using parsers, lexicons and language filters, and c) hybrid approaches combining different methods (Smadja 1993; Dagan and Church 1994; Daille 1995; McEnery et al. 1997; Wu 1997; Wermter et al. 1997; Michiels and Dufour 1998; Merkel and Andersson 2000; Piao and McEnery 2001; Sag et al. 2001a, 2001b; Biber et al. 2003)." ></td>
	<td class="line x" title="20:193	In practice, most statistical approaches use linguistic filters to collect candidate MWEs." ></td>
	<td class="line x" title="21:193	Such approaches include Dagan and Churchs (1994) Termight Tool." ></td>
	<td class="line x" title="22:193	In this tool, they first collect candidate nominal terms with a POS syntactic pattern filter, then use concordances to identify frequently co-occurring multiword units." ></td>
	<td class="line oc" title="23:193	In his Xtract system, Smadja (1993) first extracted significant pairs of words that consistently co-occur within a single syntactic structure using statistical scores called distance, strength and spread, and then examined concordances of the bi-grams to find longer frequent multiword units." ></td>
	<td class="line x" title="24:193	Similarly, Merkel and Andersson (2000) compared frequency-based and entropy based algorithms, each of which was combined with a language filter." ></td>
	<td class="line x" title="25:193	They reported that the entropy-based algorithm produced better results." ></td>
	<td class="line n" title="26:193	One of the main problems facing statistical approaches, however, is that they are unable to deal with low-frequency MWEs." ></td>
	<td class="line x" title="27:193	In fact, the majority of the words in most corpora have low frequencies, occurring only once or twice." ></td>
	<td class="line n" title="28:193	This means that a major part of true multiword expressions are left out by statistical approaches." ></td>
	<td class="line x" title="29:193	Lexical resources and parsers are used to obtain better coverage of the lexicon in MWE extraction." ></td>
	<td class="line x" title="30:193	For example, Wu (1997) used an English-Chinese bilingual parser based on stochastic transduction grammars to identify terms, including multiword expressions." ></td>
	<td class="line x" title="31:193	In their DEFI Project, Michiels and Dufour (1998) used dictionaries to identify English and French multiword expressions and their translations in the other language." ></td>
	<td class="line x" title="32:193	Wehrli (1998) employed a generative grammar framework to identify compounds and idioms in their ITS-2 MT English-French system." ></td>
	<td class="line x" title="33:193	Sag et al.(2001b) introduced Head-driven Phrase Structure Grammar for analyzing MWEs." ></td>
	<td class="line x" title="35:193	Like pure statistical approaches, purely knowledgebased symbolic approaches also face problems." ></td>
	<td class="line x" title="36:193	They are language dependent and not flexible enough to cope with complex structures of MWEs." ></td>
	<td class="line x" title="37:193	As Sag et al.(2001b) suggest, it is important to find the right balance between symbolic and statistical approaches." ></td>
	<td class="line x" title="39:193	In this paper, we propose a new approach to MWEs extraction using semantic field information." ></td>
	<td class="line x" title="40:193	In this approach, multiword units depicting single semantic concepts are recognized using the Lancaster USAS semantic tagger." ></td>
	<td class="line x" title="41:193	We describe that system and the algorithms used for identifying single and multiword units in the following section." ></td>
	<td class="line x" title="42:193	3 Lancaster Semantic tagger The USAS system has been in development at Lancaster University since 1990 1." ></td>
	<td class="line x" title="43:193	Based on POS annotation provided by the CLAWS tagger (Garside and Smith, 1997), USAS assigns a set of semantic tags to each item in running text and then attempts to disambiguate the tags in order to choose the most likely candidate in each context." ></td>
	<td class="line x" title="44:193	Items can be single words or multiword expressions." ></td>
	<td class="line x" title="45:193	The semantic tags indicate semantic fields which group together word senses that are related by virtue of their being connected at some level of generality with the same mental concept." ></td>
	<td class="line x" title="46:193	The groups include not only synonyms and antonyms but also hypernyms and hyponyms." ></td>
	<td class="line x" title="47:193	The initial tagset was loosely based on Tom McArthur's Longman Lexicon of Contemporary English (McArthur, 1981) as this appeared to offer the most appropriate thesaurus type classification of word senses for this kind of analysis." ></td>
	<td class="line x" title="48:193	The tagset has since been considerably revised in the light of practical tagging problems met in the course of the research." ></td>
	<td class="line x" title="49:193	The revised tagset is arranged in a hierarchy with 21 major discourse fields expanding into 232 category labels." ></td>
	<td class="line x" title="50:193	The following list shows the 21 labels at the top level of the hierarchy (for the full tagset, see website: http://www.comp.lancs.ac.uk/ucrel/usas)." ></td>
	<td class="line x" title="51:193	1 This work is continuing to be supported by the Benedict project, EU project IST-2001-34237." ></td>
	<td class="line x" title="52:193	A general and abstract terms B the body and the individual C arts and crafts E emotion F food and farming G government and the public domain H architecture, buildings, houses and the home I money and commerce in industry K entertainment, sports and games L life and living things M movement, location, travel and transport N numbers and measurement O substances, materials, objects and equipment P education Q linguistic actions, states and processes S social actions, states and processes T time W the world and our environment X psychological actions, states and processes Y science and technology Z names and grammatical words Currently, the lexicon contains just over 37,000 words and the template list contains over 16,000 multiword units." ></td>
	<td class="line x" title="53:193	These resources were created manually by extending and expanding dictionaries from the CLAWS tagger with observations from large text corpora." ></td>
	<td class="line x" title="54:193	Generally, only the base form of nouns and verbs are stored in the lexicon and a lemmatisation procedure is used for look-up." ></td>
	<td class="line x" title="55:193	However, the base form is not sufficient in some cases." ></td>
	<td class="line x" title="56:193	Stubbs (1996: 40) observes that meaning is not constant across the inflected forms of a lemma, and Tognini-Bonelli (2001: 92) notes that lemma variants have different senses." ></td>
	<td class="line x" title="57:193	In the USAS lexicon, each entry consists of a word with one POS tag and one or more semantic tags assigned to it." ></td>
	<td class="line x" title="58:193	At present, in cases where a word has more than one syntactic tag, it is duplicated (i.e. each syntactic tag is given a separate entry)." ></td>
	<td class="line x" title="59:193	The semantic tags for each entry in the lexicon are arranged in approximate rank frequency order to assist in manual post editing, and to allow for gross automatic selection of the common tag, subject to weighting by domain of discourse." ></td>
	<td class="line x" title="60:193	In the multi-word-unit list, each template consists of a pattern of words and part-ofspeech tags." ></td>
	<td class="line x" title="61:193	The semantic tags for each template are arranged in rank frequency order in the same way as the lexicon." ></td>
	<td class="line x" title="62:193	Various types of multiword expressions are included: phrasal verbs (e.g. stubbed out), noun phrases (e.g. ski boots), proper names (e.g. United States), true idioms (e.g. life of Riley)." ></td>
	<td class="line x" title="63:193	Figure 1 below shows samples of the actual templates used to identify these MWUs." ></td>
	<td class="line x" title="64:193	Each of these example templates has only one semantic tag associated with it, listed on the right-hand end of the template." ></td>
	<td class="line x" title="65:193	However, the second example (ski boot) combines the clothing (B5) and sports (K5.1) fields into one tag." ></td>
	<td class="line x" title="66:193	The pattern on the left of each template consists of a sequence of words joined to POS tags with the underscore character." ></td>
	<td class="line x" title="67:193	The words and POS fields can include the asterisk wildcard character to allow for inflectional variants and to write more powerful templates with wider coverage." ></td>
	<td class="line x" title="68:193	USAS templates can match discontinuous MWUs, and this is illustrated by the first example, which includes optional intervening POS items marked within curly brackets." ></td>
	<td class="line x" title="69:193	Thus this template can match stubbed out and stubbed the cigarette out." ></td>
	<td class="line x" title="70:193	Np is used to match simple noun phrases identified with a noun-phrase chunker." ></td>
	<td class="line x" title="71:193	stub*_* {Np/P*/R*} out_RP O4.6ski_NN1 boot*_NN* B5/K5.1 United_* States_N* Z2 life_NN1 of_IO Riley_NP1 K1 Figure 1 Sample of USAS multiword templates As in the case of grammatical tagging, the task of semantic tagging subdivides broadly into two phases: Phase I (Tag assignment): attaching a set of potential semantic tags to each lexical unit and Phase II (Tag disambiguation): selecting the contextually appropriate semantic tag from the set provided by Phase I. USAS makes use of seven major techniques or sources of information in phase II." ></td>
	<td class="line x" title="72:193	We will list these only briefly here, since they are described in more detail elsewhere (Garside and Rayson, 1997)." ></td>
	<td class="line x" title="73:193	1." ></td>
	<td class="line x" title="74:193	POS tag." ></td>
	<td class="line x" title="75:193	Some senses can be eliminated by prior POS tagging." ></td>
	<td class="line x" title="76:193	The CLAWS part-of-speech tagger is run prior to semantic tagging." ></td>
	<td class="line x" title="77:193	2." ></td>
	<td class="line x" title="78:193	General likelihood ranking for singleword and MWU tags." ></td>
	<td class="line x" title="79:193	In the lexicon and MWU list senses are ranked in terms of frequency, even though at present such ranking is derived from limited or unverified sources such as frequency-based dictionaries, past tagging experience and intuition." ></td>
	<td class="line x" title="80:193	3." ></td>
	<td class="line x" title="81:193	Overlapping MWU resolution." ></td>
	<td class="line x" title="82:193	Normally, semantic multi-word units take priority over single word tagging, but in some cases a set of templates will produce overlapping candidate taggings for the same set of words." ></td>
	<td class="line x" title="83:193	A set of heuristics is applied to enable the most likely template to be treated as the preferred one for tag assignment." ></td>
	<td class="line x" title="84:193	4." ></td>
	<td class="line x" title="85:193	Domain of discourse." ></td>
	<td class="line x" title="86:193	Knowledge of the current domain or topic of discourse is used to alter rank ordering of semantic tags in the lexicon and template list for a particular domain." ></td>
	<td class="line x" title="87:193	5." ></td>
	<td class="line x" title="88:193	Text-based disambiguation." ></td>
	<td class="line x" title="89:193	It has been claimed (by Gale et al, 1992) on the basis of corpus analysis that to a very large extent a word keeps the same meaning throughout a text." ></td>
	<td class="line x" title="90:193	6." ></td>
	<td class="line x" title="91:193	Contextual rules." ></td>
	<td class="line x" title="92:193	The template mechanism is also used in identifying regular contexts in which a word is constrained to occur in a particular sense." ></td>
	<td class="line x" title="93:193	7." ></td>
	<td class="line x" title="94:193	Local probabilistic disambiguation." ></td>
	<td class="line x" title="95:193	It is generally supposed that the correct semantic tag for a given word is substantially determined by the local surrounding context." ></td>
	<td class="line x" title="96:193	After automatic tag assignment has been carried out, manual post-editing can take place, if desired, to ensure that each word and idiom carries the correct semantic classification." ></td>
	<td class="line x" title="97:193	From these seven disambiguation methods, our main interest in this paper is the third technique of overlapping MWU resolution." ></td>
	<td class="line x" title="98:193	When more than one template match overlaps in a sentence, the following heuristics are applied in sequence: 1." ></td>
	<td class="line x" title="99:193	Prefer longer templates over shorter templates 2." ></td>
	<td class="line x" title="100:193	For templates of the same length, prefer shorter span matches over longer span matches (a longer span indicates more intervening items for discontinuous templates) 3." ></td>
	<td class="line x" title="101:193	If the templates do not apply to the same sequence of words, prefer the one that begins earlier in the sentence 4." ></td>
	<td class="line x" title="102:193	For templates matching the same sequence of words, prefer the one which contains the more fully defined template pattern (with fewer wildcards in the word fields) 5." ></td>
	<td class="line x" title="103:193	Prefer templates with a more fully defined first word in the template 6." ></td>
	<td class="line x" title="104:193	Prefer templates with fewer wildcards in the POS tags These six rules were found to differentiate in all cases of overlapping MWU templates." ></td>
	<td class="line x" title="105:193	Cases which failed to be differentiated indicated that two (or more) templates in our MWU list were in fact identical, apart from the semantic tags and required merging together." ></td>
	<td class="line x" title="106:193	4 Experiment of MWE extraction In order to test our approach of extracting MWEs using semantic information, we first tagged the newspaper part of the METER Corpus with the USAS tagger." ></td>
	<td class="line x" title="107:193	We then collected the multiword units assigned as a single semantic unit." ></td>
	<td class="line x" title="108:193	Finally, we manually checked the results." ></td>
	<td class="line x" title="109:193	The Meter Corpus chosen as the test data is a collection of court reports from the British Press Association (PA) and some leading British newspapers (Gaizauskas 2001; Clough et al. , 2002)." ></td>
	<td class="line x" title="110:193	In our experiment, we used the newspaper part of the corpus containing 774 articles with more than 250,000 words." ></td>
	<td class="line x" title="111:193	It provides a homogeneous corpus (in the sense that the reports come from a restricted domain of court events) and is thus a good source from which to extract domain-specific MWEs." ></td>
	<td class="line x" title="112:193	Another reason for choosing this corpus is that it has not been used in training the USAS system." ></td>
	<td class="line x" title="113:193	As an open test, we assume the results of the experiment should reflect true capability of our approach for real-life applications." ></td>
	<td class="line x" title="114:193	The current USAS tagger may assign multiple possible semantic tags for a term when it fails to disambiguate between them." ></td>
	<td class="line x" title="115:193	As mentioned previously, the first one denotes the most likely semantic field of the term." ></td>
	<td class="line x" title="116:193	Therefore, in our experiment we chose the first tag when such situations arose." ></td>
	<td class="line x" title="117:193	A major problem we faced in our experiment is the definition of a MWE." ></td>
	<td class="line x" title="118:193	Although it has been several years since people started to work on MWE extraction, we found that there is, as yet, no available clear-cut definition for MWEs." ></td>
	<td class="line x" title="119:193	We noticed various possible definitions have been suggested for MWE/MWU." ></td>
	<td class="line oc" title="120:193	For example, Smadja (1993) suggests a basic characteristic of collocations and multiword units is recurrent, domain-dependent and cohesive lexical clusters." ></td>
	<td class="line x" title="121:193	Sag et el." ></td>
	<td class="line x" title="122:193	(2001b) suggest that MWEs can roughly be defined as idiosyncratic interpretations that cross word boundaries (or spaces)." ></td>
	<td class="line x" title="123:193	Biber et al.(2003) describe MWEs as lexical bundles, which they go on to define as combinations of words that can be repeated frequently and tend to be used frequently by many different speakers/writers within a register." ></td>
	<td class="line x" title="125:193	Although it is not difficult to interpret these deifications in theory, things became much more complicated when we undertook our practical checking of the MWE candidates." ></td>
	<td class="line x" title="126:193	Quite often, we experienced disagreement between us about whether or not to accept a MWE candidate as a good one." ></td>
	<td class="line x" title="127:193	In practice, we generally followed Biber et al.s definition, i.e. accept a candidate MWE as a good one if it can repeatedly co-occur in the corpus." ></td>
	<td class="line x" title="128:193	Another difficulty we experienced relates to estimating recall." ></td>
	<td class="line x" title="129:193	Because the MWEs in the METER Corpus are not marked-up, we could not automatically calculate the number of MWEs contained in the corpus." ></td>
	<td class="line x" title="130:193	Consequently, we had to manually estimate this figure." ></td>
	<td class="line x" title="131:193	Obviously it is not practical to manually check though the whole corpus within the limited time allowed." ></td>
	<td class="line x" title="132:193	Therefore, we had to estimate the recall on a sample of the corpus, as will be described in the following section." ></td>
	<td class="line x" title="133:193	5 Evaluation In this section, we analyze the results of the MWE extraction in detail for a full evaluation of our approach to MWE extraction." ></td>
	<td class="line x" title="134:193	Overall, after we processed the test corpus, the USAS tagger extracted 4,195 MWE candidates from the test corpus." ></td>
	<td class="line x" title="135:193	After manually checking through the candidates, we selected 3,792 as good MWEs, resulting in overall precision of 90.39%." ></td>
	<td class="line x" title="136:193	As we explained earlier, due to the difficulty of obtaining the total number of true MWEs in the entire test corpus, we had to estimate recall of the MWE extraction on a sample corpus." ></td>
	<td class="line x" title="137:193	In detail, we first randomly selected fifty texts containing 14,711 words from the test corpus, then manually markedup good MWEs in the sample texts, finally counted the number of the marked-up MWUs." ></td>
	<td class="line x" title="138:193	As a result, 1,511 good MWEs were found in the sample." ></td>
	<td class="line x" title="139:193	Since the number of automatically extracted good MWEs in the sample is 595, the recall on the sample is calculated as follows: Recall=(5951511)100%=39.38%." ></td>
	<td class="line x" title="140:193	Considering the homogenous feature of the test data, we assume this local recall is roughly approximate to the global recall of the test corpus." ></td>
	<td class="line x" title="141:193	To analyze the performance of USAS in respect to the different semantic field categories, we divided candidates according to the assigned semantic tag, and calculated the precision for each of them." ></td>
	<td class="line x" title="142:193	Table 1 lists these precisions, sorting the semantic fields by the number of MWE candidates (refer to section 3 for definitions of the twenty-one main semantic field categories)." ></td>
	<td class="line x" title="143:193	As shown in this table, the USAS semantic tagger obtained precisions between 91.23% to 100.00% for each semantic field except for the field of names and grammatical words denoted by Z. As Z was the biggest field (containing 45.39% of the total MWEs and 43.12% of the accepted MWEs), we examined these MWEs more closely." ></td>
	<td class="line x" title="144:193	We discovered that numerous pairs of words are tagged as person names (Z1) and geographical names (Z2) by mistake, e.g. Blackfriars crown (tagged as Z1), stabbed Constance (tagged as Z2) etc. Semantic field Total MWEs Accepted MWEs Precision Z 1,904 1,635 85.87% T 497 459 92.35% A 351 328 93.44% M 254 241 94.88% N 227 211 92.95% S 180 177 98.33% B 131 128 97.71% G 118 110 93.22% X 114 104 91.23% I 74 72 97.30% Q 67 63 94.03% E 58 53 91.38% H 53 52 98.11% K 48 45 93.75% P 39 37 94.87% O 32 29 90.63% F 24 24 100.00% L 11 11 100.00% Y 6 6 100.00% C 5 5 100.00% W 2 2 100.00% Total 4,195 3,792 90.39% Table 1: Precisions for different semantic categories Another possible factor that affects the performance of the USAS tagger is the length of the MWEs." ></td>
	<td class="line x" title="145:193	To observe the performance of our approach from this perspective, we grouped the MWEs by their lengths, and then checked precision for each of the categories." ></td>
	<td class="line x" title="146:193	Table 2 shows the results (once again, they are sorted in descending order by MWE lengths)." ></td>
	<td class="line x" title="147:193	As we might expect, the number of MWEs decreases as the length increases." ></td>
	<td class="line x" title="148:193	In fact, bi-grams alone constitute 80.52% and 81.88% of the candidate and accepted MWEs respectively." ></td>
	<td class="line x" title="149:193	The precision also showed a generally increasing trend as the MWE length increases, but with a major divergence of trigrams." ></td>
	<td class="line x" title="150:193	One main type of error occurred on trigrams is that those with the structure of CIW(capital-initial word) + conjunction + CIW tend to be tagged as Z2 (geographical name)." ></td>
	<td class="line x" title="151:193	The table shows relatively high precision for longer MWEs, reaching 100% for 6grams." ></td>
	<td class="line x" title="152:193	Because the longest MWEs extracted have six words, no longer MWEs could be examined." ></td>
	<td class="line x" title="153:193	MWE length Total MWEs Accepted MWEs Precision 2 3,378 3,105 91.92% 3 700 575 82.14% 4 95 91 95.44% 5 18 17 94.44% 6 4 4 100.00% Total 4,195 3,792 90.39% Table 2: Precisions for MWEs of different lengths As discussed earlier, purely statistical algorithms of MWE extraction generally filter out candidates of low frequencies." ></td>
	<td class="line x" title="154:193	However, such low-frequency terms in fact form major part of MWEs in most corpora." ></td>
	<td class="line x" title="155:193	In our study, we attempted to investigate the possibility of extracting low frequency MWEs by using semantic field annotation." ></td>
	<td class="line x" title="156:193	We divided MWEs into different frequency groups, then checked precision for each of the categories." ></td>
	<td class="line x" title="157:193	Table 3 shows the results, which are sorted by the candidate MWE frequencies." ></td>
	<td class="line x" title="158:193	As we expected, 69.46% of the candidate MWEs and 68.22% of the accepted MWEs occur in the corpus only once or twice." ></td>
	<td class="line x" title="159:193	This means that, with a frequency filter of Min(f)=3, a purely statistical algorithm would exclude more than half of the candidates from the process." ></td>
	<td class="line x" title="160:193	Freq." ></td>
	<td class="line x" title="161:193	of MWE Total number Accepted MWEs Precision 1 2,164 1,892 87.43% 2 750 695 92.67% 3 4 616 570 92.53% 5 7 357 345 96.64% 8 20 253 238 94.07% 21 117 55 52 94.55% Total 4,195 3,792 90.39% Table 3: Precisions for MWEs with different frequencies Table 3 also displays an interesting relationship between the precisions and the frequencies." ></td>
	<td class="line x" title="162:193	Generally, we would expect better precisions for MWEs of higher frequencies, as higher co-occurrence frequencies are expected to reflect stronger affinity between the words within the MWEs." ></td>
	<td class="line x" title="163:193	By and large, slightly higher precisions were obtained for the latter groups of higher frequencies (57, 8-20 and 21-117) than those for the preceding lower frequency groups, i.e. 94.07%-96.64% versus 87.43%-92.67%." ></td>
	<td class="line x" title="164:193	Nevertheless, for the latter three groups of the higher frequencies (5-7, 8-20 and 21117) the precision did not increase as the frequency increases, as we initially expected." ></td>
	<td class="line x" title="165:193	When we made a closer examination of the error MWEs in this frequency range, we found that some frequent domain-specific terms are misclassified by the USAS tagger." ></td>
	<td class="line x" title="166:193	For example, since the texts in the test corpus are newspaper reports of court stories, many law courts (e.g. Manchester crown court, Norwich crown court) are frequently mentioned throughout the corpus, causing high frequencies of such terms (f=20 and f=31 respectively)." ></td>
	<td class="line x" title="167:193	Unfortunately, the templates used in the USAS tagger did not capture them as complete terms." ></td>
	<td class="line x" title="168:193	Rather, fragments were assigned a Z1 person name tag (e.g. Manchester crown)." ></td>
	<td class="line x" title="169:193	A solution to this type of problem is to improve the multiword unit templates used in the USAS tagger." ></td>
	<td class="line x" title="170:193	Other possible solutions may include incorporating a statistical algorithm to help detect boundaries of complete MWEs." ></td>
	<td class="line x" title="171:193	When we examined the error distribution within the semantic fields more closely, we found that most errors occurred within the Z and T categories (refer to Table 1)." ></td>
	<td class="line x" title="172:193	The errors occurring in these semantic field categories and their sub-divisions make up 76.18% of the total errors (403)." ></td>
	<td class="line x" title="173:193	Table 4 shows the error distribution across 14 sub-divisions (for definitions of these subdivisions, see: website: http://www.comp.lancs.ac.uk/ucrel/usas)." ></td>
	<td class="line x" title="174:193	Notice that the majority of errors are from four semantic sub-categories: Z1, Z2, Z3 and T1.3." ></td>
	<td class="line x" title="175:193	Notice, also, that the first two of these account for 60.55% of the total errors." ></td>
	<td class="line x" title="176:193	This shows that the main cause of the errors in the USAS tool is the algorithm and lexical entries used for identifying names personal and geographical and, to a lesser extent, the algorithm and lexical entries for identifying periods of time." ></td>
	<td class="line x" title="177:193	If these components of the USAS can be improved, a much higher precision can be expected." ></td>
	<td class="line x" title="178:193	In sum, our evaluation shows that our semantic approach to MWE extraction is efficient in identifying MWEs, in particular those of lower frequencies." ></td>
	<td class="line x" title="179:193	In addition, a reasonably wide lexical coverage is obtained, as indicated by the recall of 39.38%, which is important for terminology building." ></td>
	<td class="line x" title="180:193	Our approach provides a practical way for extracting MWEs on a large scale, which we envisage can be useful for both linguistic research and practical NLP applications." ></td>
	<td class="line x" title="181:193	Stag Err." ></td>
	<td class="line x" title="182:193	Stag Err." ></td>
	<td class="line x" title="183:193	Z1:person names 119 T1.1.1:time-past 1 Z2:geog." ></td>
	<td class="line x" title="184:193	names 125 T1.1.2:time-present 1 Z3:other names 16 T1.2:time-momentary 8 Z4:discourse bin 3 T1.3:time-period 23 Z5:gram." ></td>
	<td class="line x" title="185:193	bin 2 T2:time-begin/end 2 Z8:pronouns etc. 2 T3:time-age 1 Z99:unmatched 2 T4:time-early/late 2 Table 4: Errors for some semantic sub-divisions 6 Conclusion In this paper, we have shown that it is a practical way to extract MWEs using semantic field information." ></td>
	<td class="line x" title="186:193	Since MWEs are lexical units carrying single semantic concepts, it is reasonable to consider the issue of MWE extraction as an issue of identifying word bundles depicting single semantic units." ></td>
	<td class="line x" title="187:193	The main difficulty facing such an approach is that very few reliable automatic tools available for identifying lexical semantic units." ></td>
	<td class="line x" title="188:193	However, a semantic field annotator, USAS, has been built in Lancaster University." ></td>
	<td class="line x" title="189:193	Although it was not built aiming to the MWE extraction, we thought it might be very well suited for this purpose." ></td>
	<td class="line x" title="190:193	Our experiment shows that the USAS tagger is indeed an efficient tool for MWE extraction." ></td>
	<td class="line x" title="191:193	Nevertheless, the current semantic tagger does not provide a complete solution to the problem." ></td>
	<td class="line x" title="192:193	During our experiment, we found that not all of the multiword units it collects are valid MWEs." ></td>
	<td class="line x" title="193:193	An efficient algorithm is needed for distinguishing between free word combinations and relatively fixed, closely affiliated word bundles." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C04-1141
Collocation Extraction Based On Modifiability Statistics
Wermter, Joachim;Hahn, Udo;"></td>
	<td class="line x" title="1:152	Collocation Extraction Based on Modifiability Statistics Joachim Wermter Udo Hahn Computerlinguistik, Friedrich-Schiller-Universitat Jena Furstengraben 30, D-07743 Jena, Germany wermter@coling.uni-freiburg.de Abstract We introduce a new, linguistically grounded measure of collocativity based on the property of limited modifiability and test it on German PP-verb combinations." ></td>
	<td class="line x" title="2:152	We show that our measure not only significantly outperforms the standard lexical association measures typically employed for collocation extraction, but also yields a valuable by-product for the creation of collocation databases, viz." ></td>
	<td class="line x" title="3:152	possible structural and lexical attributes." ></td>
	<td class="line x" title="4:152	Our approach is language-, structure-, and domain-independent because it only requires some shallow syntactic analysis (e.g. , a POS-tagger and a phrase chunker)." ></td>
	<td class="line x" title="5:152	1 Introduction Natural language is an open and very flexible communication system." ></td>
	<td class="line x" title="6:152	Syntax, of course, imposes constraints, e.g., on word order or the occurrence of particular phrasal types such as PPs or NPs, and lexical semantics imposes, e.g., selectional constraints on conceptually permitted sorts or types within the context of specific verbs or nouns." ></td>
	<td class="line x" title="7:152	Nevertheless, natural language speakers usually enjoy an enormous degree of freedom to express the content they want to convey in a great variety of linguistic forms." ></td>
	<td class="line x" title="8:152	There is, however, a significant subset of expressions which do not share this rather free combinability, so-called collocations." ></td>
	<td class="line x" title="9:152	From a linguistic perspective, they can be characterized by at least three recurrent and prominent properties (Manning and Schutze, 1999): a0 Non-(or limited) compositionality." ></td>
	<td class="line x" title="10:152	The meaning of a collocation is not a straightforward composition of the meanings of its parts." ></td>
	<td class="line x" title="11:152	For example, the meaning of red tape is completely different from the meaning of its components." ></td>
	<td class="line x" title="12:152	a0 Non-(or limited) substitutability." ></td>
	<td class="line x" title="13:152	The parts of a collocation cannot be substituted by semantically similar words." ></td>
	<td class="line x" title="14:152	Thus, gut in to spill gut cannot be substituted by intestine (see also Lin (1999))." ></td>
	<td class="line x" title="15:152	a0 Non-(or limited) modifiability." ></td>
	<td class="line x" title="16:152	Many collocations cannot be supplemented by additional lexical material." ></td>
	<td class="line x" title="17:152	For example, the noun in to kick the bucket cannot be modified as to kick the a1 holey/plastic/watera2 bucket." ></td>
	<td class="line x" title="18:152	Considering these observations, from a natural language processing perspective, collocations should not enter, e.g., the standard syntax-semantics pipeline so as to prevent compositional semantic readings of expressions for which this is absolutely not desired." ></td>
	<td class="line x" title="19:152	Hence, collocations need to be identified as such and subsequently be blocked, e.g., from compositional semantic interpretation." ></td>
	<td class="line x" title="20:152	In computational linguistics, a wide variety of lexical association measures have been employed for the task of (semi-)automatic collocation identification and extraction." ></td>
	<td class="line x" title="21:152	Almost all of these measures can be grouped into one of the following three categories: a0 frequency-based measures (e.g. , based on absolute and relative co-occurrence frequencies) a0 information-theoretic measures (e.g. , mutual information, entropy) a0 statistical measures (e.g. , chi-square, t-test, log-likelihood, Dices coefficient) The corresponding metrics have been extensively discussed in the literature both in terms of their mathematical properties (Dunning, 1993; Manning and Schutze, 1999) and their suitability for the task of collocation extraction (see Evert and Krenn (2001) and Krenn and Evert (2001) for recent evaluations)." ></td>
	<td class="line x" title="22:152	Typically, they are applied to a set of candidate lexeme pairs which were obtained from preprocessors varying in linguistic sophistication.1 The selected measure then assigns an association score 1On the low end, this may just be a preset numeric window span." ></td>
	<td class="line x" title="23:152	In order to reduce the noise among the candidates, however, more elaborate linguistic processing, such as POS tagging, chunking, or even parsing, is increasingly being applied." ></td>
	<td class="line x" title="24:152	to each candidate pair, which is computed from its joint and marginal frequencies, thus expressing the strength of the hypothesis stating whether it constitutes a collocation or not." ></td>
	<td class="line x" title="25:152	While these association measures have their statistical merits in collocation identification, it is interesting to note that they have relatively little to do with the linguistic properties (such as those mentioned at the beginning) which are typically associated with the notion of collocativity." ></td>
	<td class="line x" title="26:152	Therefore, it may be interesting to investigate whether there is a way to implement a measure which directly incorporates linguistic criteria in the collocation identification task, and even more important, whether such a linguistically rooted approach would fare better in comparison to some of the standard lexical association measures." ></td>
	<td class="line x" title="27:152	In the following study, we will introduce such a linguistic measure for identifying PP-verb collocations in German, which is based on the property of nonor limited modifiability." ></td>
	<td class="line x" title="28:152	To the best of our knowledge, this is the first work to use this kind of linguistic measure to acquire collocations automatically." ></td>
	<td class="line x" title="29:152	By contrasting our method to previous studies which use the standard lexical association measures, we intend to emphasize a more linguistically inspired use of statistics in collocation mining." ></td>
	<td class="line x" title="30:152	Section 2 motivates our definition of the notion of collocation and Section 3 describes our methods, in particular the linguistically grounded collocation extraction algorithm, and the experimental setup derived from it." ></td>
	<td class="line x" title="31:152	In Section 4 we present and discuss the results of our experiments." ></td>
	<td class="line x" title="32:152	2 Kinds of Collocations There have been various approaches to define the notion of collocation." ></td>
	<td class="line x" title="33:152	This is by no means an easy task, especially when it comes to defining the demarcation line between collocations and free word combinations (modulo general syntactic and semantic semantic constraints)." ></td>
	<td class="line x" title="34:152	We favor an approach which draws this line on the semantic layer, viz." ></td>
	<td class="line x" title="35:152	the compositionality between the components of a linguistic expression." ></td>
	<td class="line x" title="36:152	For this purpose, we distinguish between three classes of collocations based on varying degrees of semantic compositionality of the basic lexical entities involved: 1." ></td>
	<td class="line x" title="37:152	Idiomatic Phrases." ></td>
	<td class="line x" title="38:152	In this case, none of the lexical components involved contribute to the overall meaning in a semantically transparent way." ></td>
	<td class="line x" title="39:152	The meaning of the expression is metaphorical or figurative." ></td>
	<td class="line x" title="40:152	For example, the literal meaning of the German PP-verb combination [jemanden] auf die Schippe nehmen is to take [someone] onto the shovel." ></td>
	<td class="line x" title="41:152	Its figurative meaning is to lampoon somebody." ></td>
	<td class="line x" title="42:152	2." ></td>
	<td class="line x" title="43:152	Support Verb Constructions/Narrow Collocations." ></td>
	<td class="line x" title="44:152	This second class contains expressions in which at least one component contributes to the overall meaning in a semantically transparent way and thus constitutes its semantic core." ></td>
	<td class="line x" title="45:152	For example, in the support verb construction zur Verfugung stellen (literal: to put to availabilty; actual: to make available), the noun Verfugung is the semantic core of the expression, whereas the verb only has a support function with some impact on argument structure, causativity or aktionsart." ></td>
	<td class="line x" title="46:152	There are, however, also narrow collocations in which the basic lexical meaning of the verb is the semantic core: For example, in aus eigener Tasche bezahlen (to pay out of ones own pocket) the verb bezahlen is the semantic core." ></td>
	<td class="line x" title="47:152	What unifies these two types is the fact that they function as predicates." ></td>
	<td class="line x" title="48:152	3." ></td>
	<td class="line x" title="49:152	Fixed Phrases." ></td>
	<td class="line x" title="50:152	Here, all basic lexical meanings of the components involved contribute to the overall meaning in a semantically much more transparent way." ></td>
	<td class="line x" title="51:152	Still, they are not as completely compositional as to classify them as free word combinations." ></td>
	<td class="line x" title="52:152	For example, all the basic lexical meanings of the different lexical components in im Koma liegen (literal: to lie in coma; actual: to be comatose) contribute to the overall meaning of the expression." ></td>
	<td class="line x" title="53:152	Still, this is different from a completely compositional free word combination, such as auf der Strasse gehen (to walk on the street)." ></td>
	<td class="line x" title="54:152	Our goal is to consider all three types of collocations as a whole, i.e., we will not distinguish between the three different kinds of collocations." ></td>
	<td class="line x" title="55:152	However, in order to focus our experiments, we will concentrate on a particular surface pattern in which they occur, viz." ></td>
	<td class="line x" title="56:152	PP-verb collocations." ></td>
	<td class="line x" title="57:152	3 Methods and Experiments 3.1 Construction and Statistics of the Testset We used a 114-million-word German-language newspaper corpus extracted from the Web to acquire candidate PP-verb collocations." ></td>
	<td class="line x" title="58:152	The corpus was first processed by means of the TNT partof-speech tagger (Brants, 2000)." ></td>
	<td class="line x" title="59:152	Then we ran a sentence/clause recognizer and an NP/PP chunker, both developed at the Text Knowledge Engineering Lab at Freiburg University, on the POS-tagged corpus." ></td>
	<td class="line x" title="60:152	From the XML-marked-up tree output, PPverb complexes were automatically selected in the following way: Taking a particular PP node as a fixed point, either the preceding or the following sibling V node was taken.2 From such a PPverb combination, we extracted and counted both its various heads, in terms of Preposition-Noun-Verb (PNV) triples, and all its associated supplements, i.e., here in this case any additional lexical material which also occurs in the nominal group of the PP, such as articles, adjectives, adverbs, cardinals, etc.3 The extraction of the associated supplements is essential to the linguistic measure described in subsection 3.3 below." ></td>
	<td class="line x" title="61:152	In order to reduce the amount of candidates for evaluation and to eliminate low-frequency data, we only considered PNV-triples with frequency a0a2a1 a3a5a4." ></td>
	<td class="line x" title="62:152	This was also motivated by the wellknown fact that collocations tend to have a higher co-occurrence frequency than free word combinations.4 Table 1 contains the data for the corresponding frequency distributions." ></td>
	<td class="line x" title="63:152	frequency PP-verb combinations candidate tokens candidate types all 1,663,296 1,159,133 a6a8a7a10a9a12a11 279,350 8,644 Table 1: Frequency distribution for PP-Verb tokens and types for our 114-million-word newspaper corpus 3.2 Classification of the Testset Three human judges manually classified the PPverb candidate types with a0a13a1 a3a5a4 in regard to whether they were a collocation or not." ></td>
	<td class="line x" title="64:152	For this purpose, they used a manual, in which the guidelines included the linguistic properties as described in Section 1 and the three collocation classes identified in Section 2." ></td>
	<td class="line x" title="65:152	Among the 8,644 PP-verb candidate types, 1,180 (13.7%) were identified as true collocations." ></td>
	<td class="line x" title="66:152	The inter-annotator agreement was 94.8% (with a standard deviation of 2.1)." ></td>
	<td class="line x" title="67:152	2The verbs in this study are restricted to main verbs and are reduced to their base form after extraction." ></td>
	<td class="line x" title="68:152	3It should be noted that both heads and associated supplements may of course vary depending on the particular linguistic structure targeted for collocation extraction." ></td>
	<td class="line x" title="69:152	4Cf." ></td>
	<td class="line x" title="70:152	also Evert and Krenn (2001) for empirical evidence justifying the exclusion of low-frequency data." ></td>
	<td class="line x" title="71:152	3.3 The Linguistic Measure The linguistic property around which we built our measure for collocativity is the nonor limited modifiabilty of collocations with additional lexical material (i.e. , supplements)." ></td>
	<td class="line x" title="72:152	The underlying assumption is that a PNV triple is less modifiable (and thus more likely to be a collocation) if it has a lexical supplement which, compared to all others, is particularly characteristic." ></td>
	<td class="line x" title="73:152	We express this assumption in the following way: Let a14 be the number of distinct supplements of a particular PNV triple (a15a17a16a19a18a21a20a23a22a25a24a27a26a29a28a31a30 )." ></td>
	<td class="line x" title="74:152	The probability a32 of a particular supplement a33a35a34a37a36a38a36a40a39, a41a43a42a45a44 a3a47a46 a14a49a48, is described by its frequency scaled by the sum of all supplement frequencies: a32a51a50a52a15a17a16a19a18a21a20a23a22a25a24a27a26a29a28a31a30a54a53a55a38a56a57a26a54a26a59a58a59a60a61a42 (1) a0 a50a52a15a17a16a19a18a21a20a23a22a25a24a27a26a29a28a31a30a54a53a55a38a56a57a26a54a26a59a58a5a60 a62a64a63 a24a23a65a67a66 a0 a50a52a15a17a16a19a18a68a20a23a22a69a24a27a26a29a28a31a30a54a53a55a38a56a57a26a54a26a29a70a71a60 with a62a19a63 a24a72a65a67a66 a0 a50a52a15a17a16a19a18a21a20a23a22a25a24a27a26a29a28a73a30a57a53a55a74a56a12a26a54a26a29a70a71a60a64a42 a0 a50a52a15a17a16a19a18a21a20a23a22a25a24a27a26a29a28a31a30a75a60 . 5 Then the modifiability a76a78a77a80a79 of a PNV triple can be described by its most probable supplement: a76a78a77a80a79a81a50a52a15a17a16a19a18a21a20a23a22a25a24a27a26a29a28a73a30a82a60a84a83a31a42 (2) a85a87a86a89a88a91a90a92a85a87a93 a32a51a50a52a15a17a16a19a18a21a20a23a22a25a24a27a26a29a28a31a30a54a53a55a38a56a57a26a54a26a59a58a59a60 a46 a41a51a42a94a44 a3a47a46 a14a49a48 To define a measure of collocativity a95a96a77a98a97a84a97 for a candidate set, some factor regarding frequency has to be taken into account." ></td>
	<td class="line x" title="75:152	Thus, besides a76a78a77a80a79, we take the relative co-occurrence frequency for a specific PNV triple a32a51a50a52a15a17a16a19a18a99a20a23a22a69a24a27a26a29a28a31a30a82a60 (a100 being the number of candidate types (here, 8,644)) a32a51a50a52a15a17a16a101a18a68a20a23a22a25a24a27a26a29a28a73a30a82a60a84a83a31a42 a0 a50a52a15a17a16a19a18a68a20a23a22a69a24a27a26a29a28a31a30a12a60 a62 a20 a102 a65a67a66 a0 a50a52a15a17a16a19a18a21a20a23a22a25a24a27a26a29a28a73a30a52a103a59a60 (3) and incorporate it as a second factor to a95a104a77a98a97a105a97 : a95a96a77a98a97a84a97a91a50a52a15a17a16a101a18a68a20a23a22a25a24a27a26a29a28a73a30a82a60a105a83a31a42 (4) a76a78a77a80a79a81a50a52a15a17a16a101a18a68a20a23a22a25a24a27a26a29a28a73a30a82a60a64a106a107a32a51a50a52a15a17a16a19a18a21a20a23a22a25a24a27a26a29a28a31a30a75a60 3.4 Methods of Evaluation Standard procedures for evaluating the goodness of collocativity measures usually involve identifying the true positives among the a14 -highest ranked candidates returned by a particular measure." ></td>
	<td class="line x" title="76:152	Because this is rather labor-intensive, a14 is usually small, ranging from 50 to several hundred." ></td>
	<td class="line x" title="77:152	Evert and Krenn 5Note that the zero supplement of the PNV triple, i.e., the one for which no lexical supplements co-occur is also included in this set." ></td>
	<td class="line x" title="78:152	(2001), however, point out the inadequacy of such methods claiming they usually lead to very superficial judgements about the measures to be examined." ></td>
	<td class="line x" title="79:152	In contrast, they suggest examining various a14 highest ranked samples, which allows plotting standard precision and recall graphs for the whole candidate set." ></td>
	<td class="line x" title="80:152	We evaluate the a95a104a77a98a97a105a97 measure against two widely used standard statistical tests (t-test and loglikelihood) and against co-occurrence frequency." ></td>
	<td class="line x" title="81:152	The comparison to the t-test is especially interesting because it was found to achieve the best overall precision scores in other studies (see Evert and Krenn (2001))." ></td>
	<td class="line x" title="82:152	Our baseline is defined by the proportion of true positives (13.7%; see subsection 3.2), which can be described as the likelihood of finding one by blindly picking from the candidate set." ></td>
	<td class="line x" title="83:152	4 Experimental Results and Discussion 4.1 Precision and Recall for Collocation Extraction In the first experiment, we incrementally examined parts of the a14 -highest ranked candidate lists returned by the each of the four measures we considered." ></td>
	<td class="line x" title="84:152	The precision values for various a14 were computed such that for each percent point of the list, the proportion of true positives was compared to the overall number of candidate items returned." ></td>
	<td class="line x" title="85:152	This yields the precision curves in Figure 1 and its associated values at selected list portions in the upper table from Table 2." ></td>
	<td class="line x" title="86:152	0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10 20 30 40 50 60 70 80 90 100 Portion of ranked list (in %) Precision ModifiabilityT-test Frequency Log-likelihood Base Figure 1: Precision for Collocation Extraction First, we observe that all measures outperform the baseline by far and, thus, all are potentially useful measures of collocativity." ></td>
	<td class="line x" title="87:152	Of the statistical measures, log-likelihood (the most complex one) performs the worst, whereas t-test and frequency, almost indistinguishable, share the middle position, with frequency measurements having a very slight edge at six rank points." ></td>
	<td class="line x" title="88:152	This is in contrast to the findings reported by Krenn and Evert (2001), which gave the t-test an edge.6 As can be clearly seen, however, our linguistic modifiability measure substantially outperforms all other measures at all points in the ranked list." ></td>
	<td class="line x" title="89:152	Considering 1% (a14a1a0 a2a4a3 ), its precision value is ten percentage points higher than for t-test and frequency, and even 22 points higher compared to loglikelihood." ></td>
	<td class="line x" title="90:152	Until 50% (a14a5a0a7a6a9a8a4a10a4a10 ) of the ranked list is considered, modifiability maintains a three to five percentage point advantage in precision over ttest and frequency." ></td>
	<td class="line x" title="91:152	In the second half of the list, all curves and associated values start converging towards the baseline." ></td>
	<td class="line x" title="92:152	We also tested the significance of differences for our precision results, both between modifiability and frequency and between modifiability and t-test." ></td>
	<td class="line x" title="93:152	Because in both cases the ranked lists were taken from the same set of candidates, viz." ></td>
	<td class="line x" title="94:152	the 8,644 PPverb candidate types, and hence constitute dependent samples, we applied the McNemar test (Sachs, 1984) for statistical testing." ></td>
	<td class="line x" title="95:152	We selected 100 measure points in the ranked list, one after each increment of one percent, and then used the two-tailed test for a confidence interval of 99%." ></td>
	<td class="line x" title="96:152	Table 3, which lists the number of significant differences for 10, 50 and 100 measure points, shows that almost all of them are significantly different." ></td>
	<td class="line x" title="97:152	# of significance # of signicant differences measure points comparing modifiability with frequency t-test 10 9 9 50 49 49 100 96 97 Table 3: Significance testing of differences using the two-tailed McNemar test at 99% confidence interval The recall curves in Figure 2 and their corresponding values in the lower table from Table 2 measure which proportion of all true positives is identified by a particular measure at a certain part of the ranked list." ></td>
	<td class="line x" title="98:152	In this sense, recall is an even better indicator of a particular measures performance." ></td>
	<td class="line x" title="99:152	Again, the linguistically motivated collocation extraction algorithm outscores all others, even more pronounced than for precision." ></td>
	<td class="line x" title="100:152	When examining 20% (a14a11a0 a3a13a12 a10a4a14a38a60, 30% (a14a15a0 a10a4a16a4a14a4a8 ) and 40% 6The reason why frequency performs even slightly better than t-test may very well have to do with the size of our training corpus (114 million words)." ></td>
	<td class="line x" title="101:152	But this just underlines the fact that large corpora are essential for collocation discovery." ></td>
	<td class="line x" title="102:152	Portion of Precision scores of measure evaluated ranked list considered Modifiablity T-test Frequency Log-likelihood Baseline 1% 0.84 0.74 0.74 0.62 0.14 10% 0.51 0.46 0.45 0.39 0.14 20% 0.39 0.34 0.34 0.30 0.14 30% 0.31 0.27 0.28 0.25 0.14 40% 0.27 0.23 0.24 0.21 0.14 50% 0.24 0.20 0.21 0.18 0.14 60% 0.21 0.18 0.19 0.16 0.14 70% 0.19 0.17 0.17 0.15 0.14 80% 0.17 0.15 0.16 0.15 0.14 90% 0.15 0.14 0.15 0.14 0.14 (a0a2a1 a3a5a4a7a6a9a8a9a8 ) 100% 0.14 0.14 0.14 0.14 0.14 Portion of Recall scores of measure evaluated ranked list considered Modifiablity T-test Frequency Log-likelihood 1% 0.06 0.05 0.05 0.05 10% 0.37 0.33 0.33 0.28 20% 0.58 0.50 0.50 0.44 30% 0.69 0.60 0.61 0.55 40% 0.80 0.69 0.70 0.61 50% 0.87 0.75 0.76 0.66 60% 0.93 0.80 0.83 0.72 70% 0.96 0.85 0.88 0.78 80% 0.98 0.89 0.92 0.85 90% 0.99 0.93 0.96 0.92 100% 1.00 1.00 1.00 1.00 Table 2: Precision and Recall Scores for Collocation Extraction at Major Portions of the Ranked List (a14 a0 a8 a6a9a16 a2 ) of the ranked list, modifiability, respectively, identifies almost 60%, 70% and 80% of all true positives, holding a ten percentage point lead over t-test and frequency at each of these points." ></td>
	<td class="line x" title="103:152	When 50% (a14 a0 a6a9a8a4a10a4a10 ) are considered, this difference reaches eleven and twelve points (compared to frequency and t-test, respectively)." ></td>
	<td class="line x" title="104:152	0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 10 20 30 40 50 60 70 80 90 100 Portion of ranked list (in %) Recall Modifiability T-test Frequency Log-likelihood Figure 2: Recall for Collocation Extraction Even more strikingly, for the identification of 90% of all true positives, modifiability only needs to look at 55% (a14 a0 a6 a12 a16 a6 ) of the ranked list." ></td>
	<td class="line x" title="105:152	Frequency, on the other hand, needs to examine 75% (a14 a0 a3 a6 a2 a8 ) and t-test even 85% (a14 a0 a12 a8 a6 a12 ) of the ranked list to reach this high level of recall." ></td>
	<td class="line x" title="106:152	4.2 Modifiability Revisited The previous subsection showed that a measure for collocation discovery which takes into account the linguistic property of limited modifiability fares significantly better than linguistically not so founded, purely statistical measures." ></td>
	<td class="line x" title="107:152	Although the modifiability property constitutes common wisdom about collocations, it has not yet been empirically evaluated." ></td>
	<td class="line x" title="108:152	Thus, we ran an experiment which took both the PNV triples classified as collocations and the PNV triples classified as non-collocations and counted the numbers of distinct supplements (referred to as a14 in Subsection 3.3)." ></td>
	<td class="line x" title="109:152	From this data, we set up a distribution of collocational and noncollocational PNV triples in which the distributional ranking criterion was the number of distinct supplements (cf.Figure 3)." ></td>
	<td class="line x" title="111:152	PNV Triple NP Supplement Frequency in Griff bekommen den/ART Griff/NN 459 to get under control Griff/NN 2 den/ART gewerkschaftlichen/ADJA Griff/NN 1 den/ART dramatischen/ADJA Griff/NN 1 den/ART erzahlerischen/ADJA Griff/NN 1 unter Druck geraten Druck/NN 560 to get under pressure politischen/ADJA Druck/NN 6 erheblichen/ADJA politischen/ADJA Druck/NN 5 teilweise/ADV lebensgefahrlichen/ADJA Druck/NN 1 wachsenden/ADJA Druck/NN 1 noch/ADV starkeren/ADJA Druck/NN 1 schweren/ADJA Druck/NN 1 Table 4: Collocational PNV Triples with Associated Noun Phrase Supplements 0.001 0.01 0.1 1 10 100 1 10 100 1000 proportion of all PNV-triples (in %) number of distinct supplements of PNV-triples Collocations Non-Collocations Figure 3: Distribution of Supplements for (Non-) Collocations in PNV Triples." ></td>
	<td class="line x" title="112:152	The xand y-axes are log-scaled." ></td>
	<td class="line x" title="113:152	As Figure 3 reveals, not only is the proportion of collocational PNV triples with only one distinct supplement higher (36%) than the proportion for non-collocational ones (20%), but with each additional supplement, the collocational proportion curve declines more steeply than its noncollocational counterpart." ></td>
	<td class="line x" title="114:152	Moreover, the collocational proportion curve already ends with 54 distinct supplements, whereas the non-collocational proportion curve leads up 520 distinct supplements." ></td>
	<td class="line x" title="115:152	Thus, we are able to add some empirical grounding to the widespread textbook assumption about the limited modifiablity of collocations." ></td>
	<td class="line x" title="116:152	Another observation (which is also inherent to our linguistic measure) based on this experiment is that some collocations do possess at least limited modifiability." ></td>
	<td class="line x" title="117:152	Collocation acquisition is, of course, not a goal by itself, but rather aims at creating collocation lexicons for both language processing and generation (Smadja and McKeown, 1990)." ></td>
	<td class="line x" title="118:152	From this perspective, our linguistic modifiabilty measure actually yields quite a valuable by-product for the development of lexicons or collocational knowledge bases: A list of possible structural and lexical modifications associated with a particular collocational entry candidate." ></td>
	<td class="line x" title="119:152	In our case, these modifications refer to the nominal group of the PP." ></td>
	<td class="line x" title="120:152	We illustrate this point in Table 4 with two collocational PNV triples and some of their associated NP supplements plus their frequencies." ></td>
	<td class="line x" title="121:152	As can be seen, both structural and lexical attributes of collocations can thus be obtained." ></td>
	<td class="line x" title="122:152	The structural information comes in the form of part-ofspeech (POS) tags." ></td>
	<td class="line x" title="123:152	From this, possible prenominal POS types and their combinations can be used to describe a collocations structural make-up." ></td>
	<td class="line x" title="124:152	From a lexical viewpoint, the collocation can be described by the lexical semantic word classes used for modification.7 As can be seen in Table 4 under the PNV triple for to get under pressure, the noun Druck (pressure) is often modified by a certain semantic class of adjectives, such as stark (strong), schwer (heavy), erheblich (considerable, grave)." ></td>
	<td class="line x" title="125:152	5 Related Work Although there have been many studies on collocation extraction and mining using only statistical approaches (Church and Hanks, 1990; Ikehara et al. , 1996), there has been much less work on collocation acquisition which takes into account the linguistic properties typically associated with collocations." ></td>
	<td class="line pc" title="126:152	Smadja (1993), which is the classic work on collocation extraction, uses a two-stage filtering model in which, in the first step, n-gram statistics determine possible collocations and, in the second step, these candidates are submitted to a syntactic valida7Of course, lexical material is always at least partially dependent on the domain in question." ></td>
	<td class="line x" title="127:152	In our case, this is the news domain with all its associated subdomains (politics, economics, finance, culture, etc.)." ></td>
	<td class="line x" title="128:152	tion procedure (e.g. , determining verb-object collocations) in order to filter out invalid collocations." ></td>
	<td class="line x" title="129:152	In a single-judge evaluation of 4,000 collocation candidates, the incorporation of linguistic criteria (via tagging and predicate-argument parsing) boosts precision up to a level of 80% and recall to 94%." ></td>
	<td class="line x" title="130:152	These results are, of course, not comparable to ours." ></td>
	<td class="line x" title="131:152	First of all, precision and recall are measured at a fixed point for a fixed unranked candidate list." ></td>
	<td class="line x" title="132:152	In order to obtain more reliable evaluation results, we plot these values continuously on a ranked candidate list." ></td>
	<td class="line x" title="133:152	Secondly, our kind of syntactic preprocessing (which is standard nowadays) allows collocation extraction algorithms to better control the structural types of collocations." ></td>
	<td class="line x" title="134:152	Lin (1998) acquires a lexical dependency database by assembling dependency relationships from a parsed corpus." ></td>
	<td class="line x" title="135:152	An entry in this database is classified as collocation if its log-likelihood value is greater than some threshold." ></td>
	<td class="line x" title="136:152	Using an automatically constructed similarity thesaurus, Lin (1999) then separates compositional from non-compositional collocations by taking into account the second linguistic property described in Section 1, viz." ></td>
	<td class="line x" title="137:152	their nonor limited substitutability." ></td>
	<td class="line x" title="138:152	In particular, he checks the existence and mutual information values of phrases obtained by substituting the words with similar ones, which results in the classification of the phrase as being compositional or noncompositional." ></td>
	<td class="line x" title="139:152	Although this study offers some promising results, its applicability rather falls into the category of fine-classifying an already acquired set of collocations, e.g., according to the criteria described in Section 2, and thus is not really comparable to our work." ></td>
	<td class="line x" title="140:152	Moreover, the linguistic property in his focus is of course a semantic one, whereas ours is purely syntactic in nature." ></td>
	<td class="line x" title="141:152	6 Conclusion We introduced a new, linguistically motivated measure of collocativity based on the property of limited modifiability and tested it on a large corpus with emphasis on German PP-verb combinations." ></td>
	<td class="line x" title="142:152	We showed that our measure not only significantly outperforms the standard lexical association measures typically used for collocation extraction, but also yields a valuable by-product for the creation of collocation databases, viz." ></td>
	<td class="line x" title="143:152	possible structural and lexical attributes of a collocation." ></td>
	<td class="line x" title="144:152	Our measure defines the modifiability property in a linguistically simple way, by e.g. ignoring the internal make-up of lexical supplements associated with a collocation candidate." ></td>
	<td class="line x" title="145:152	Hence, it may be worthwhile to investigate whether a more sophisticated approach, by e.g. taking into account internal POS types and their distribution etc. , would improve our results even more." ></td>
	<td class="line x" title="146:152	We may also consider other linguistic criteria (e.g. , limited substitutability) to further refine our measure and to categorize already identified collocations." ></td>
	<td class="line x" title="147:152	At the methodological level, our approach, although tested on German newspaper language data, is language-, structure-, and domain-independent." ></td>
	<td class="line x" title="148:152	All it requires is some sort of shallow syntactic analysis, e.g., POS tagging and phrase chunking." ></td>
	<td class="line x" title="149:152	Thus, in the future we plan to include other syntactic types of collocations, such as verb-object or verbobject-PP combinations, and also apply our methodology to other languages and domains, such as the biomedical field." ></td>
	<td class="line x" title="150:152	Acknowledgements." ></td>
	<td class="line x" title="151:152	We would like to thank our students, Sabine Demsar, Kristina Meller, and Konrad Feldmeier, for their excellent work as human collocation classifiers." ></td>
	<td class="line x" title="152:152	This work was partly supported by DFG grant KL 640/5-1." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P04-1022
Collocation Translation Acquisition Using Monolingual Corpora
L, Yajuan;Zhou, Ming;"></td>
	<td class="line x" title="1:200	Collocation Translation Acquisition Using Monolingual Corpora Yajuan L Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian District, Beijing, China, 100080 t-yjlv@microsoft.com Ming ZHOU Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian District, Beijing, China, 100080 mingzhou@microsoft.com Abstract Collocation translation is important for machine translation and many other NLP tasks." ></td>
	<td class="line x" title="2:200	Unlike previous methods using bilingual parallel corpora, this paper presents a new method for acquiring collocation translations by making use of monolingual corpora and linguistic knowledge." ></td>
	<td class="line x" title="3:200	First, dependency triples are extracted from Chinese and English corpora with dependency parsers." ></td>
	<td class="line x" title="4:200	Then, a dependency triple translation model is estimated using the EM algorithm based on a dependency correspondence assumption." ></td>
	<td class="line x" title="5:200	The generated triple translation model is used to extract collocation translations from two monolingual corpora." ></td>
	<td class="line x" title="6:200	Experiments show that our approach outperforms the existing monolingual corpus based methods in dependency triple translation and achieves promising results in collocation translation extraction." ></td>
	<td class="line x" title="7:200	1 Introduction A collocation is an arbitrary and recurrent word combination (Benson, 1990)." ></td>
	<td class="line o" title="8:200	Previous work in collocation acquisition varies in the kinds of collocations they detect." ></td>
	<td class="line oc" title="9:200	These range from twoword to multi-word, with or without syntactic structure (Smadja 1993; Lin, 1998; Pearce, 2001; Seretan et al. 2003)." ></td>
	<td class="line x" title="10:200	In this paper, a collocation refers to a recurrent word pair linked with a certain syntactic relation." ></td>
	<td class="line x" title="11:200	For instance, <solve, verb-object, problem> is a collocation with a syntactic relation verb-object." ></td>
	<td class="line x" title="12:200	Translation of collocations is difficult for nonnative speakers." ></td>
	<td class="line x" title="13:200	Many collocation translations are idiosyncratic in the sense that they are unpredictable by syntactic or semantic features." ></td>
	<td class="line x" title="14:200	Consider Chinese to English translation." ></td>
	<td class="line x" title="15:200	The translations of   can be solve or resolve." ></td>
	<td class="line x" title="16:200	The translations of   can be problem or issue." ></td>
	<td class="line x" title="17:200	However, translations of the collocation  ~   as solve~problem or resolve~ issue is preferred over solve~issue or resolve ~problem." ></td>
	<td class="line x" title="18:200	Automatically acquiring these collocation translations will be very useful for machine translation, cross language information retrieval, second language learning and many other NLP applications." ></td>
	<td class="line x" title="19:200	(Smadja et al. , 1996; Gao et al. , 2002; Wu and Zhou, 2003)." ></td>
	<td class="line oc" title="20:200	Some studies have been done for acquiring collocation translations using parallel corpora (Smadja et al, 1996; Kupiec, 1993; Echizen-ya et al. , 2003)." ></td>
	<td class="line x" title="21:200	These works implicitly assume that a bilingual corpus on a large scale can be obtained easily." ></td>
	<td class="line x" title="22:200	However, despite efforts in compiling parallel corpora, sufficient amounts of such corpora are still unavailable." ></td>
	<td class="line x" title="23:200	Instead of heavily relying on bilingual corpora, this paper aims to solve the bottleneck in a different way: to mine bilingual knowledge from structured monolingual corpora, which can be more easily obtained in a large volume." ></td>
	<td class="line x" title="24:200	Our method is based on the observation that despite the great differences between Chinese and English, the main dependency relations tend to have a strong direct correspondence (Zhou et al. , 2001)." ></td>
	<td class="line x" title="25:200	Based on this assumption, a new translation model based on dependency triples is proposed." ></td>
	<td class="line x" title="26:200	The translation probabilities are estimated from two monolingual corpora using the EM algorithm with the help of a bilingual translation dictionary." ></td>
	<td class="line x" title="27:200	Experimental results show that the proposed triple translation model outperforms the other three models in comparison." ></td>
	<td class="line x" title="28:200	The obtained triple translation model is also used for collocation translation extraction." ></td>
	<td class="line x" title="29:200	Evaluation results demonstrate the effectiveness of our method." ></td>
	<td class="line x" title="30:200	The remainder of this paper is organized as follows." ></td>
	<td class="line x" title="31:200	Section 2 provides a brief description on the related work." ></td>
	<td class="line x" title="32:200	Section 3 describes our triple translation model and training algorithm." ></td>
	<td class="line x" title="33:200	Section 4 extracts collocation translations from two independent monolingual corpora." ></td>
	<td class="line x" title="34:200	Section 5 evaluates the proposed method, and the last section draws conclusions and presents the future work." ></td>
	<td class="line x" title="35:200	2 Related work There has been much previous work done on monolingual collocation extraction." ></td>
	<td class="line x" title="36:200	They can in general be classified into two types: window-based and syntax-based methods." ></td>
	<td class="line oc" title="37:200	The former extracts collocations within a fixed window (Church and Hanks 1990; Smadja, 1993)." ></td>
	<td class="line x" title="38:200	The latter extracts collocations which have a syntactic relationship (Lin, 1998; Seretan et al. , 2003)." ></td>
	<td class="line x" title="39:200	The syntax-based method becomes more favorable with recent significant increases in parsing efficiency and accuracy." ></td>
	<td class="line x" title="40:200	Several metrics have been adopted to measure the association strength in collocation extraction." ></td>
	<td class="line x" title="41:200	Thanopoulos et al.(2002) give comparative evaluations on these metrics." ></td>
	<td class="line x" title="43:200	Most previous research in translation knowledge acquisition is based on parallel corpora (Brown et al. , 1993)." ></td>
	<td class="line x" title="44:200	As for collocation translation, Smadja et al.(1996) implement a system to extract collocation translations from a parallel EnglishFrench corpus." ></td>
	<td class="line x" title="46:200	English collocations are first extracted using the Xtract system, then corresponding French translations are sought based on the Dice coefficient." ></td>
	<td class="line x" title="47:200	Echizen-ya et al.(2003) propose a method to extract bilingual collocations using recursive chain-link-type learning." ></td>
	<td class="line x" title="49:200	In addition to collocation translation, there is also some related work in acquiring phrase or term translations from parallel corpus (Kupiec, 1993; Yamamoto and Matsumoto 2000)." ></td>
	<td class="line x" title="50:200	Since large aligned bilingual corpora are hard to obtain, some research has been conducted to exploit translation knowledge from non-parallel corpora." ></td>
	<td class="line x" title="51:200	Their work is mainly on word level." ></td>
	<td class="line x" title="52:200	Koehn and Knight (2000) presents an approach to estimating word translation probabilities using unrelated monolingual corpora with the EM algorithm." ></td>
	<td class="line x" title="53:200	The method exhibits promising results in selecting the right translation among several options provided by bilingual dictionary." ></td>
	<td class="line x" title="54:200	Zhou et al.(2001) proposes a method to simulate translation probability with a cross language similarity score, which is estimated from monolingual corpora based on mutual information." ></td>
	<td class="line x" title="55:200	The method achieves good results in word translation selection." ></td>
	<td class="line x" title="56:200	In addition, (Dagan and Itai, 1994) and (Li, 2002) propose using two monolingual corpora for word sense disambiguation." ></td>
	<td class="line x" title="57:200	(Fung, 1998) uses an IR approach to induce new word translations from comparable corpora." ></td>
	<td class="line x" title="58:200	(Rapp, 1999) and (Koehn and Knight, 2002) extract new word translations from non-parallel corpus." ></td>
	<td class="line x" title="59:200	(Cao and Li, 2002) acquire noun phrase translations by making use of web data." ></td>
	<td class="line x" title="60:200	(Wu and Zhou, 2003) also make full use of large scale monolingual corpora and limited bilingual corpora for synonymous collocation extraction." ></td>
	<td class="line x" title="61:200	3 Training a triple translation model from monolingual corpora In this section, we first describe the dependency correspondence assumption underlying our approach." ></td>
	<td class="line x" title="62:200	Then a dependency triple translation model and the monolingual corpus based training algorithm are proposed." ></td>
	<td class="line x" title="63:200	The obtained triple translation model will be used for collocation translation extraction in next section." ></td>
	<td class="line x" title="64:200	3.1 Dependency correspondence between Chinese and English A dependency triple consists of a head, a dependant, and a dependency relation." ></td>
	<td class="line x" title="65:200	Using a dependency parser, a sentence can be analyzed into dependency triples." ></td>
	<td class="line x" title="66:200	We represent a triple as (w 1,r,w 2 ), where w 1 and w 2 are words and r is the dependency relation." ></td>
	<td class="line x" title="67:200	It means that w 2 has a dependency relation r with w 1." ></td>
	<td class="line x" title="68:200	For example, a triple (overcome, verb-object, difficulty) means that difficulty is the object of the verb overcome." ></td>
	<td class="line x" title="69:200	Among all the dependency relations, we only consider the following three key types that we think, are the most important in text analysis and machine translation: verb-object (VO), nounadj(AN), and verbadv(AV)." ></td>
	<td class="line x" title="70:200	It is our observation that there is a strong correspondence in major dependency relations in the translation between English and Chinese." ></td>
	<td class="line x" title="71:200	For example, an object-verb relation in Chinese (e.g.(, VO,  )) is usually translated into the same verb-object relation in English(e.g.(overcome, VO, difficulty))." ></td>
	<td class="line x" title="73:200	This assumption has been experimentally justified based on a large and balanced bilingual corpus in our previous work (Zhou et al. , 2001)." ></td>
	<td class="line x" title="74:200	We come to the conclusion that more than 80% of the above dependency relations have a one-one mapping between Chinese and English." ></td>
	<td class="line x" title="75:200	We can conclude that there is indeed a very strong correspondence between Chinese and English in the three considered dependency relations." ></td>
	<td class="line x" title="76:200	This fact will be used to estimate triple translation model using two monolingual corpora." ></td>
	<td class="line x" title="77:200	3.2 Triple translation model According to Bayess theorem, given a Chinese triple ),,( 21 crcc ctri =, and the set of its candidate English triple translations ),,( 21 eree etri =, the best English triple ),,( 21 eree etri = is the one that maximizes the Equation (1): )|()(maxarg )(/)|()(maxarg )|(maxarg tritritri e tritritritri e tritri e tri ecpep cpecpep cepe tri tri tri = = = (1) where )( tri ep is usually called the language model and )|( tritri ecp is usually called the translation model." ></td>
	<td class="line x" title="78:200	Language Model The language model )( tri ep is calculated with English triples database." ></td>
	<td class="line x" title="79:200	In order to tackle with the data sparseness problem, we smooth the language model with an interpolation method, as described below." ></td>
	<td class="line x" title="80:200	When the given English triple occurs in the corpus, we can calculate it as in Equation (2)." ></td>
	<td class="line x" title="81:200	N erefreq ep e tri ),,( )( 21 = (2) where ),,( 21 erefreq e represents the frequency of triple tri e . N represents the total counts of all the English triples in the training corpus." ></td>
	<td class="line x" title="82:200	For an English triple ),,( 21 eree etri =, if we assume that two words 1 e and 2 e are conditionally independent given the relation e r, Equation (2) can be rewritten as in (3)(Lin, 1998)." ></td>
	<td class="line x" title="83:200	)|()|()()( 21 eeetri repreprpep = (3) where N rfreq rp e e,*)(*, )( =,,*)(*,,*),( )|( 1 1 e e e rfreq refreq rep =,,*)(*, ),(*, )|( 2 22 e e rfreq erfreq rep = . The wildcard symbol * means it can be any word or relation." ></td>
	<td class="line x" title="84:200	With Equations (2) and (3), we get the interpolated language model as shown in (4)." ></td>
	<td class="line x" title="85:200	)|()|()()1( )( )( 21 eee tri tri repreprp N efreq ep  += (4) where 10 <<  .  is calculated as below: )(1 1 1 tri efreq+ = (5) Translation Model We simplify the translation model according the following two assumptions." ></td>
	<td class="line x" title="86:200	Assumption 1: Given an English triple tri e, and the corresponding Chinese dependency relation c r, 1 c and 2 c are conditionally independent." ></td>
	<td class="line x" title="87:200	We have: )|(),|(),|( )|,,()|( 21 21 trictrictric trictritri erpercpercp ecrcpecp = = (6) Assumption 2: For an English triple tri e, assume that i c only depends on {1,2}) (i  i e, and c r only depends on e r . Equation (6) is rewritten as: )|()|()|( )|(),|(),|()|( 2211 21 ec trietrictrictritri rrpecpecp erpercpercpecp = = (7) Notice that )|( 11 ecp and )|( 22 ecp are translation probabilities within triples, they are different from the unrestricted probabilities such as the ones in IBM models (Brown et al. , 1993)." ></td>
	<td class="line x" title="88:200	We distinguish translation probability between head ( )|( 11 ecp ) and dependant ( )|( 22 ecp )." ></td>
	<td class="line x" title="89:200	In the rest of the paper, we use )|( ecp head and )|( ecp dep to denote the head translation probability and dependant translation probability respectively." ></td>
	<td class="line x" title="90:200	As the correspondence between the same dependency relation across English and Chinese is strong, we simply assume 1)|( = ec rrp for the corresponding e r and c r, and 0)|( = ec rrp for the other cases." ></td>
	<td class="line x" title="91:200	)|( 11 ecp head and )|( 22 ecp dep cannot be estimated directly because there is no triple-aligned corpus available." ></td>
	<td class="line x" title="92:200	Here, we present an approach to estimating these probabilities from two monolingual corpora based on the EM algorithm." ></td>
	<td class="line x" title="93:200	3.3 Estimation of word translation probability using the EM algorithm Chinese and English corpora are first parsed using a dependency parser, and two dependency triple databases are generated." ></td>
	<td class="line x" title="94:200	The candidate English translation set of Chinese triples is generated through a bilingual dictionary and the assumption of strong correspondence of dependency relations." ></td>
	<td class="line x" title="95:200	There is a risk that unrelated triples in Chinese and English can be connected with this method." ></td>
	<td class="line x" title="96:200	However, as the conditions that are used to make the connection are quite strong (i.e. possible word translations in the same triple structure), we believe that this risk, is not very severe." ></td>
	<td class="line x" title="97:200	Then, the expectation maximization (EM) algorithm is introduced to iteratively strengthen the correct connections and weaken the incorrect connections." ></td>
	<td class="line x" title="98:200	EM Algorithm According to section 3.2, the translation probabilities from a Chinese triple tri c to an English triple tri e can be computed using the English triple language model )( tri ep and a translation model from English to Chinese )|( tritri ecp . The English language model can be estimated using Equation (4) and the translation model can be calculated using Equation (7)." ></td>
	<td class="line x" title="99:200	The translation probabilities )|( ecp head and )|( ecp dep are initially set to a uniform distribution as follows:       == otherwise cif ecpecp e edephead,0 )(, 1 )|()|( (8) Where e  represents the translation set of the English word e. Then, the word translation probabilities are estimated iteratively using the EM algorithm." ></td>
	<td class="line x" title="100:200	Figure 1 gives a formal description of the EM algorithm." ></td>
	<td class="line x" title="101:200	Figure 1: EM algorithm The basic idea is that under the restriction of the English triple language model )( tri ep and translation dictionary, we wish to estimate the translation probabilities )|( ecp head and )|( ecp dep that best explain the Chinese triple database as a translation from the English triple database." ></td>
	<td class="line x" title="102:200	In each iteration, the normalized triple translation probabilities are used to update the word translation probabilities." ></td>
	<td class="line x" title="103:200	Intuitively, after finding the most probable translation of the Chinese triple, we can collect counts for the word translation it contains." ></td>
	<td class="line x" title="104:200	Since the English triple language model provides context information for the disambiguation of the Chinese words, only the appropriate occurrences are counted." ></td>
	<td class="line x" title="105:200	Now, with the language model estimated using Equation (4) and the translation probabilities estimated using EM algorithm, we can compute the best triple translation for a given Chinese triple using Equations (1) and (7)." ></td>
	<td class="line x" title="106:200	4 Collocation translation extraction from two monolingual corpora This section describes how to extract collocation translation from independent monolingual corpora." ></td>
	<td class="line x" title="107:200	First, collocations are extracted from a monolingual triples database." ></td>
	<td class="line x" title="108:200	Then, collocation translations are acquired using the triple translation model obtained in section 3." ></td>
	<td class="line x" title="109:200	4.1 Monolingual collocation extraction As introduced in section 2, much work has been done to extract collocations." ></td>
	<td class="line x" title="110:200	Among all the measure metrics, log likelihood ratio (LLR) has proved to give better results (Duning, 1993; Thanopoulos et al. , 2002)." ></td>
	<td class="line x" title="111:200	In this paper, we take LLR as the metric to extract collocations from a dependency triple database." ></td>
	<td class="line x" title="112:200	For a given Chinese triple ),,( 21 crcc ctri =, the LLR score is calculated as follows: NN dcdcdbdb cacababa ddccbbaaLogl log )log()()log()( )log()()log()( loglogloglog + ++++ ++++ +++= (9) where, . ),,,(),(*, ),,,(,*),( ),,,( 212 211 21 cbaNd crcfreqcrfreqc crcfreqrcfreqb crcfreqa cc cc c = = = = N is the total counts of all Chinese triples." ></td>
	<td class="line x" title="113:200	Those triples whose LLR values are larger than a given threshold are taken as a collocation." ></td>
	<td class="line x" title="114:200	This syntax-based collocation has the advantage that it can represent both adjacent and long distance word association." ></td>
	<td class="line x" title="115:200	Here, we only extract the three main types of collocation that have been mentioned in section 3.1." ></td>
	<td class="line x" title="116:200	4.2 Collocation translation extraction For the acquired collocations, we try to extract their translations from the other monolingual Train language model for English triple )( tri ep ; Initialize word translation probabilities )|( ecp head and )|( ecp dep uniformly as in Equation (8); Iterate Set )|( ecscore head and )|( ecscore dep to 0 for all dictionary entries (c,e); for all Chinese triples ),,( 21 crcc ctri = for all candidate English triple translations ),,( 21 eree etri = compute triple translation probability )|( tritri cep by )|()|()|()( 2211 ecdepheadtri rrpecpecpep end for normalize )|( tritri cep, so that their sum is 1; for all triple translation ),,( 21 eree etri = add )|( tritri cep to )|( 11 ecscore head add )|( tritri cep to )|( 22 ecscore dep endfor endfor for all translation pairs (c,e) set )|( ecp head to normalized )|( ecscore head ; set )|( ecp dep to normalized )|( ecscore dep ; endfor enditerate corpus using the triple translation model trained with the method proposed in section 3." ></td>
	<td class="line x" title="117:200	Our objective is to acquire collocation translations as translation knowledge for a machine translation system, so only highly reliable collocation translations are extracted." ></td>
	<td class="line x" title="118:200	Figure 2 describes the algorithm for Chinese-English collocation translation extraction." ></td>
	<td class="line x" title="119:200	It can be seen that the best English triple candidate is extracted as the translation of the given Chinese collocation only if the Chinese collocation is also the best translation candidate of the English triple." ></td>
	<td class="line x" title="120:200	But the English triple is not necessarily a collocation." ></td>
	<td class="line x" title="121:200	English collocation translations can be extracted in a similar way." ></td>
	<td class="line x" title="122:200	Figure 2: Collocation translation extraction 4.3 Implementation of our approach Our English corpus is from Wall Street Journal (1987-1992) and Associated Press (1988-1990), and the Chinese corpus is from Peoples Daily (1980-1998)." ></td>
	<td class="line x" title="123:200	The two corpora are parsed using the NLPWin parser 1 (Heidorn, 2000)." ></td>
	<td class="line x" title="124:200	The statistics for three main types of dependency triples are shown in tables 1 and 2." ></td>
	<td class="line x" title="125:200	Token refers to the total number of triple occurrences and Type refers to the number of unique triples in the corpus." ></td>
	<td class="line x" title="126:200	Statistic for the extracted Chinese collocations and the collocation translations is shown in Table 3." ></td>
	<td class="line x" title="127:200	Class #Type #Token VO 1,579,783 19,168,229 AN 311,560 5,383,200 AV 546,054 9,467,103 Table 1: Chinese dependency triples 1 The NLPWin parser is a rule-based parser developed at Microsoft research, which parses several languages including Chinese and English." ></td>
	<td class="line x" title="128:200	Its output can be a phrase structure parse tree or a logical form which is represented with dependency triples." ></td>
	<td class="line x" title="129:200	Class #Type #Token VO 1,526,747 8,943,903 AN 1,163,440 6,386,097 AV 215,110 1,034,410 Table 2: English dependency triples Class #Type #Translated VO 99,609 28,841 AN 35,951 12,615 AV 46,515 6,176 Table 3: Extracted Chinese collocations and E-C translation pairs The translation dictionaries we used in training and translation are combined from two dictionaries: HITDic and NLPWinDic 2 . The final E-C dictionary contains 126,135 entries, and C-E dictionary contains 91,275 entries." ></td>
	<td class="line x" title="130:200	5 Experiments and evaluation To evaluate the effectiveness of our methods, two experiments have been conducted." ></td>
	<td class="line x" title="131:200	The first one compares our method with three other monolingual corpus based methods in triple translation." ></td>
	<td class="line x" title="132:200	The second one evaluates the accuracy of the acquired collocation translation." ></td>
	<td class="line x" title="133:200	5.1 Dependency triple translation Triple translation experiments are conducted from Chinese to English." ></td>
	<td class="line x" title="134:200	We randomly selected 2000 Chinese triples (whose frequency is larger than 2) from the dependency triple database." ></td>
	<td class="line x" title="135:200	The standard translation answer sets were built manually by three linguistic experts." ></td>
	<td class="line x" title="136:200	For each Chinese triple, its English translation set contain English triples provided by anyone of the three linguists." ></td>
	<td class="line x" title="137:200	Among 2000 candidate triples, there are 101 triples that cant be translated into English triples with same relation." ></td>
	<td class="line x" title="138:200	For example, the Chinese triple (, VO,  ) should be translated into bargain." ></td>
	<td class="line x" title="139:200	The two words in triple cannot be translated separately." ></td>
	<td class="line x" title="140:200	We call this kind of collocation translation no-compositional translations." ></td>
	<td class="line x" title="141:200	Our current model cannot deal with this kind of translation." ></td>
	<td class="line x" title="142:200	In addition, there are also 157 error dependency triples, which result from parsing mistakes." ></td>
	<td class="line x" title="143:200	We filtered out these two kinds of triples and got a standard test set with 1,742 Chinese triples and 4,645 translations in total." ></td>
	<td class="line x" title="144:200	We compare our triple translation model with three other models on the same standard test set with the same translation dictionary." ></td>
	<td class="line x" title="145:200	As the 2 These two dictionaries are built by Harbin Institute of Technology and Microsoft Research respectively." ></td>
	<td class="line x" title="146:200	For each Chinese collocation col c : a. Acquire the best English triple translation tri e using C-E triple translation model: )|()(maxarg tritritri e tri ecpepe tri = b. For the acquired tri e, calculate the best Chinese triple translation tri c using E-C triple translation model: )|()(maxarg tritritri c tri cepcpc tri = c. If col c = tri c, add col c  tri e to collocation translation database." ></td>
	<td class="line x" title="147:200	baseline experiment, Model A selects the highestfrequency translation for each word in triple; Model B selects translation with the maximal target triple probability, as proposed in (Dagan 1994); Model C selects translation using both language model and translation model, but the translation probability is simulated by a similarity score which is estimated from monolingual corpus using mutual information measure (Zhou et al. , 2001)." ></td>
	<td class="line x" title="148:200	And our triple translation model is model D. Suppose ),,( 21 crcc ctri = is the Chinese triple to be translated." ></td>
	<td class="line x" title="149:200	The four compared models can be formally expressed as follows: Model A: ))((maxarg,)),((maxarg( 2 )( 1 )( max 2211 efreqrefreqe cTranse e cTranse  = Model B: ),,(maxarg)(maxarg 21 )( )( max 22 11 erepepe e cTranse cTranse tri e tri   == Model C: )),Sim(),Sim()((maxarg ))|(likelyhood)((maxarg 2211 )( )( max 22 11 ceceep ecepe tri cTranse cTranse tritritri e tri = =   where, ),Sim( ce is similarity score between e and c (Zhou et al. , 2001)." ></td>
	<td class="line x" title="150:200	Model D (our model): ))|()|()|()((maxarg ))|()((maxarg 2211 )( )( max 22 11 ecdepheadtri cTranse cTranse tritritri e rrpecpecpep ecpepe tri   = = Accuracy(%) CoveRage(%) Top 1 Top 3 Oracle (%) Model A 17.21 ---Model B 33.56 53.79 Model C 35.88 57.74 Model D 83.98 36.91 58.58 66.30 Table 4: Translation results comparison The evaluation results on the standard test set are shown in Table 4, where coverage is the percentages of triples which can be translated." ></td>
	<td class="line x" title="151:200	Some triples cant be translated by Model B, C and D because of the lack of dictionary translations or data sparseness in triples." ></td>
	<td class="line x" title="152:200	In fact, the coverage of Model A is 100%." ></td>
	<td class="line x" title="153:200	It was set to the same as others in order to compare accuracy using the same test set." ></td>
	<td class="line x" title="154:200	The oracle score is the upper bound accuracy under the conditions of current translation dictionary and standard test set." ></td>
	<td class="line x" title="155:200	Top N accuracy is defined as the percentage of triples whose selected top N translations include correct translations." ></td>
	<td class="line x" title="156:200	We can see that both Model C and Model D achieve better results than Model B. This shows that the translation model trained from monolingual corpora really helps to improve the performance of translation." ></td>
	<td class="line x" title="157:200	Our model also outperforms Model C, which demonstrates the probabilities trained by our EM algorithm achieve better performance than heuristic similarity scores." ></td>
	<td class="line x" title="158:200	In fact, our evaluation method is very rigorous." ></td>
	<td class="line x" title="159:200	To avoid bias in evaluation, we take human translation results as standard." ></td>
	<td class="line x" title="160:200	The real translation accuracy is reasonably better than the evaluation results." ></td>
	<td class="line x" title="161:200	But as we can see, compared to the oracle score, the current models still have much room for improvement." ></td>
	<td class="line x" title="162:200	And coverage is also not high due to the limitations of the translation dictionary and the sparse data problem." ></td>
	<td class="line x" title="163:200	5.2 Collocation translation extraction 47,632 Chinese collocation translations are extracted with the method proposed in section 4." ></td>
	<td class="line x" title="164:200	We randomly selected 1000 translations for evaluation." ></td>
	<td class="line x" title="165:200	Three linguistic experts tag the acceptability of the translation." ></td>
	<td class="line x" title="166:200	Those translations that are tagged as acceptable by at least two experts are evaluated as correct." ></td>
	<td class="line x" title="167:200	The evaluation results are shown in Table 5." ></td>
	<td class="line x" title="168:200	Total Acceptance Accuracy (%) VO 590 373 63.22 AN 292 199 68.15 AV 118 60 50.85 All 1000 632 63.20 ColTrans 334 241 72.16 Table 5: Extracted collocation translation results We can see that the extracted collocation translations achieve a much better result than triple translation." ></td>
	<td class="line x" title="169:200	The average accuracy is 63.20% and the collocations with relation AN achieve the highest accuracy of 68.15%." ></td>
	<td class="line x" title="170:200	If we only consider those Chinese collocations whose translations are also English collocations, we obtain an even better accuracy of 72.16% as shown in the last row of Table 5." ></td>
	<td class="line x" title="171:200	The results justify our idea that we can acquire reliable translation for collocation by making use of triple translation model in two directions." ></td>
	<td class="line x" title="172:200	These acquired collocation translations are very valuable for translation knowledge building." ></td>
	<td class="line x" title="173:200	Manually crafting collocation translations can be time-consuming and cannot ensure high quality in a consistent way." ></td>
	<td class="line x" title="174:200	Our work will certainly improve the quality and efficiency of collocation translation acquisition." ></td>
	<td class="line x" title="175:200	5.3 Discussion Although our approach achieves promising results, it still has some limitations to be remedied in future work." ></td>
	<td class="line x" title="176:200	(1) Translation dictionary extension Due to the limited coverage of the dictionary, a correct translation may not be stored in the dictionary." ></td>
	<td class="line x" title="177:200	This naturally limits the coverage of triple translations." ></td>
	<td class="line x" title="178:200	Some research has been done to expand translation dictionary using a non-parallel corpus (Rapp, 1999; Keohn and Knight, 2002)." ></td>
	<td class="line x" title="179:200	It can be used to improve our work." ></td>
	<td class="line x" title="180:200	(2) Noise filtering of parsers Since we use parsers to generate dependency triple databases, this inevitably introduces some parsing mistakes." ></td>
	<td class="line x" title="181:200	From our triple translation test data, we can see that 7.85% (157/2000) types of triples are error triples." ></td>
	<td class="line x" title="182:200	These errors will certainly influence the translation probability estimation in the training process." ></td>
	<td class="line x" title="183:200	We need to find an effective way to filter out mistakes and perform necessary automatic correction." ></td>
	<td class="line x" title="184:200	(3) Non-compositional collocation translation." ></td>
	<td class="line x" title="185:200	Our model is based on the dependency correspondence assumption, which assumes that a triples translation is also a triple." ></td>
	<td class="line x" title="186:200	But there are still some collocations that cant be translated word by word." ></td>
	<td class="line x" title="187:200	For example, the Chinese triple (, VO,  ) usually be translated into be effective; the English triple (take, VO, place) usually be translated into  ." ></td>
	<td class="line x" title="188:200	The two words in triple cannot be translated separately." ></td>
	<td class="line x" title="189:200	Our current model cannot deal with this kind of non-compositional collocation translation." ></td>
	<td class="line x" title="190:200	Melamed (1997) and Lin (1999) have done some research on noncompositional phrases discovery." ></td>
	<td class="line x" title="191:200	We will consider taking their work as a complement to our model." ></td>
	<td class="line x" title="192:200	6 Conclusion and future work This paper proposes a novel method to train a triple translation model and extract collocation translations from two independent monolingual corpora." ></td>
	<td class="line x" title="193:200	Evaluation results show that it outperforms the existing monolingual corpus based methods in triple translation, mainly due to the employment of EM algorithm in cross language translation probability estimation." ></td>
	<td class="line x" title="194:200	By making use of the acquired triple translation model in two directions, promising results are achieved in collocation translation extraction." ></td>
	<td class="line x" title="195:200	Our work also demonstrates the possibility of making full use of monolingual resources, such as corpora and parsers for bilingual tasks." ></td>
	<td class="line x" title="196:200	This can help overcome the bottleneck of the lack of a large-scale bilingual corpus." ></td>
	<td class="line x" title="197:200	This approach is also applicable to comparable corpora, which are also easier to access than bilingual corpora." ></td>
	<td class="line x" title="198:200	In future work, we are interested in extending our method to solving the problem of noncompositional collocation translation." ></td>
	<td class="line x" title="199:200	We are also interested in incorporating our triple translation model for sentence level translation." ></td>
	<td class="line x" title="200:200	7 Acknowledgements The authors would like to thank John Chen, Jianfeng Gao and Yunbo Cao for their valuable suggestions and comments on a preliminary draft of this paper." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P04-3019
TANGO: Bilingual Collocational Concordancer
Jian, Jia-Yan;Chang, Yu-Chia;Chang, Jason S.;"></td>
	<td class="line x" title="1:28	TANGO: Bilingual Collocational Concordancer Jia-Yan Jian Department of Computer Science National Tsing Hua University 101, Kuangfu Road, Hsinchu, Taiwan g914339@oz.nthu.edu.tw Yu-Chia Chang Inst." ></td>
	<td class="line x" title="2:28	of Information System and Applictaion National Tsing Hua University 101, Kuangfu Road, Hsinchu, Taiwan u881222@alumni.nthu.e du.tw Jason S. Chang Department of Computer Science National Tsing Hua University 101, Kuangfu Road, Hsinchu, Taiwan jschang@cs.nthu.edu.tw Abstract In this paper, we describe TANGO as a collocational concordancer for looking up collocations." ></td>
	<td class="line x" title="3:28	The system was designed to answer users query of bilingual collocational usage for nouns, verbs and adjectives." ></td>
	<td class="line x" title="4:28	We first obtained collocations from the large monolingual British National Corpus (BNC)." ></td>
	<td class="line x" title="5:28	Subsequently, we identified collocation instances and translation counterparts in the bilingual corpus such as Sinorama Parallel Corpus (SPC) by exploiting the wordalignment technique." ></td>
	<td class="line x" title="6:28	The main goal of the concordancer is to provide the user with a reference tools for correct collocation use so as to assist second language learners to acquire the most eminent characteristic of native-like writing." ></td>
	<td class="line x" title="7:28	1 Introduction Collocations are a phenomenon of word combination occurring together relatively often." ></td>
	<td class="line x" title="8:28	Collocations also reflect the speakers fluency of a language, and serve as a hallmark of near nativelike language capability." ></td>
	<td class="line x" title="9:28	Collocation extraction is critical to a range of studies and applications, including natural language generation, computer assisted language learning, machine translation, lexicography, word sense disambiguation, cross language information retrieval, and so on." ></td>
	<td class="line x" title="10:28	Hanks and Church (1990) proposed using pointwise mutual information to identify collocations in lexicography; however, the method may result in unacceptable collocations for low-count pairs." ></td>
	<td class="line x" title="11:28	The best methods for extracting collocations usually take into consideration both linguistic and statistical constraints." ></td>
	<td class="line oc" title="12:28	Smadja (1993) also detailed techniques for collocation extraction and developed a program called XTRACT, which is capable of computing flexible collocations based on elaborated statistical calculation." ></td>
	<td class="line x" title="13:28	Moreover, log likelihood ratios are regarded as a more effective method to identify collocations especially when the occurrence count is very low (Dunning, 1993)." ></td>
	<td class="line x" title="14:28	Smadjas XTRACT is the pioneering work on extracting collocation types." ></td>
	<td class="line x" title="15:28	XTRACT employed three different statistical measures related to how associated a pair to be collocation type." ></td>
	<td class="line x" title="16:28	It is complicated to set different thresholds for each statistical measure." ></td>
	<td class="line x" title="17:28	We decided to research and develop a new and simple method to extract monolingual collocations." ></td>
	<td class="line x" title="18:28	We also provide a web-based user interface capable of searching those collocations and its usage." ></td>
	<td class="line x" title="19:28	The concordancer supports language learners to acquire the usage of collocation." ></td>
	<td class="line x" title="20:28	In the following section, we give a brief overview of the TANGO concordancer." ></td>
	<td class="line x" title="21:28	2 TANGO TANGO is a concordancer capable of answering users queries on collocation use." ></td>
	<td class="line x" title="22:28	Currently, TANGO supports two text collections: a monolingual corpus (BNC) and a bilingual corpus (SPC)." ></td>
	<td class="line x" title="23:28	The system consists of four main parts: 2.1 Chunk and Clause Information Integrated For CoNLL-2000 shared task, chunking is considered as a process that divides a sentence into syntactically correlated parts of words." ></td>
	<td class="line x" title="24:28	With the benefits of CoNLL training data, we built a chunker that turn sentences into smaller syntactic structure of non-recursive basic phrases to facilitate precise collocation extraction." ></td>
	<td class="line x" title="25:28	It becomes easier to identify the argument-predicate relationship by looking at adjacent chunks." ></td>
	<td class="line x" title="26:28	By doing so, we save time as opposed to n-gram statistics or full parsing." ></td>
	<td class="line x" title="27:28	Take a text in CoNLL2000 for example: The words correlated with the same chunk tag can be further grouped together (see Table 1)." ></td>
	<td class="line x" title="28:28	For instance, with chunk information, we can extract" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W04-0407
Representation And Treatment Of Multiword Expressions In Basque
Alegria, Iaki;Ansa, Olatz;Artola, Xabier;Ezeiza, Nerea;Gojenola, Koldo;Urizar, Ruben;"></td>
	<td class="line x" title="1:167	Representation and Treatment of Multiword Expressions in Basque Iaki Alegria, Olatz Ansa, Xabier Artola Nerea Ezeiza, Koldo Gojenola and Ruben Urizar Ixa Group University of the Basque Country 649 pk E-20.080 Donostia." ></td>
	<td class="line x" title="2:167	Basque Country rubenu@sc.ehu.es Abstract This paper describes the representation of Basque Multiword Lexical Units and the automatic processing of Multiword Expressions." ></td>
	<td class="line x" title="3:167	After discussing and stating which kind of multiword expressions we consider to be processed at the current stage of the work, we present the representation schema of the corresponding lexical units in a generalpurpose lexical database." ></td>
	<td class="line x" title="4:167	Due to its expressive power, the schema can deal not only with fixed expressions but also with morphosyntactically flexible constructions." ></td>
	<td class="line x" title="5:167	It also allows us to lemmatize word combinations as a unit and yet to parse the components individually if necessary." ></td>
	<td class="line x" title="6:167	Moreover, we describe HABIL, a tool for the automatic processing of these expressions, and we give some evaluation results." ></td>
	<td class="line x" title="7:167	This work must be placed in a general framework of written Basque processing tools, which currently ranges from the tokenization and segmentation of single words up to the syntactic tagging of general texts." ></td>
	<td class="line x" title="8:167	1 Introduction 2 Most texts are rich in multiword expressions, which must be necessarily processed if we want any NLP tool to perform accurately." ></td>
	<td class="line x" title="9:167	Jackendoff (1997) estimates that their number in the speakers' lexicon is of the same order of magnitude as the number of single words." ></td>
	<td class="line x" title="10:167	There is no agreement among authors about the definition of the term Multiword Expression." ></td>
	<td class="line x" title="11:167	However, in this article, Multiword Expressions (hereafter MWE) refer to any word combinations ranging from idioms, over proper names, compounds, lexical and grammatical collocations to institutionalized phrases." ></td>
	<td class="line x" title="12:167	MWEs comprise both semantically compositional and non-compositional combinations, and both syntactically regular and idiosyncratic phrases, including complex named entities such as proper nouns, dates and number expressions (see section 2)." ></td>
	<td class="line x" title="13:167	In contrast, Multiword Lexical Units (hereafter MWLU) comprise lexicalized phrases  semantically non-compositional or syntactically idiosyncratic word combinations which are represented and stored in the lexical database of Basque (EDBL)." ></td>
	<td class="line x" title="14:167	The remaining sections are organized as follows." ></td>
	<td class="line x" title="15:167	Section 2 presents the main features of MWEs in Basque, and defines which are currently considered for automatic processing." ></td>
	<td class="line x" title="16:167	Section 3 describes the representation of MWLUs in the lexical database." ></td>
	<td class="line x" title="17:167	Section 4 is devoted to the description and evaluation of the automatic treatment of MWEs by means of HABIL." ></td>
	<td class="line x" title="18:167	Section 5 summarizes future work." ></td>
	<td class="line x" title="19:167	And, finally, section 6 outlines some conclusions." ></td>
	<td class="line x" title="20:167	Multiword Expressions in the processing of real texts in Basque The definition of the term Multiword Expression and the types of such MWEs to be treated in NLP may vary considerably depending on the purposes or 'the depth of processing being undertaken' (Copestake et al. , 2002)." ></td>
	<td class="line x" title="21:167	Multiword itself is a Second ACL Workshop on Multiword Expressions: Integrating Processing, July 2004, pp." ></td>
	<td class="line x" title="22:167	48-55 vague term." ></td>
	<td class="line x" title="23:167	At text level, a word could be defined as 'any string of characters between two blanks' (Fontenelle et al. , 1994)." ></td>
	<td class="line x" title="24:167	This is not applicable to languages as Japanese, which are typically written without spaces." ></td>
	<td class="line x" title="25:167	Besides, a great number of MWEs that in uninflected languages would be multiword, constitute a single typographic unit in agglutinative languages such as Basque (ziurrenik 'most probably', aurrerantzean 'from now on', aurretiaz 'in advance')." ></td>
	<td class="line x" title="26:167	Therefore, we consider them single words and they are included in the lexical database as such (or recognized by means of morphological analysis)." ></td>
	<td class="line x" title="27:167	In our case, when deciding which Basque MWEs to include in the database, we mostly rely on lexicographers' expertise since we consider lexicalized phrases have a top priority for both lemmatizing and syntactic purposes." ></td>
	<td class="line x" title="28:167	So, the MWEs dealt with in the database comprise fixed expressions, which admit no morphosyntactic or internal modification including foreign expressions such as in situ, a priori, strictu sensu, etc., idioms, both decomposable and nondecomposable, and lexicalized compounds." ></td>
	<td class="line x" title="29:167	We also consider light verb constructions when they are syntactically idiosyncratic." ></td>
	<td class="line x" title="30:167	However, currently we do not treat open collocations, proverbs, catch phrases and similes." ></td>
	<td class="line x" title="31:167	Mostly, we don't include proper names in the database either, since complex named entities are given a separate treatment." ></td>
	<td class="line x" title="32:167	Apart from proper nouns, also dates and number expressions are treated separately (see 4.1)." ></td>
	<td class="line x" title="33:167	So far we have described 2,270 MWLUs in our database." ></td>
	<td class="line x" title="34:167	This work has been carried out in two phases." ></td>
	<td class="line x" title="35:167	For the first phase, we made use of the Statistical Corpus of 20 th Century Basque (http://www.euskaracorpusa.net) that contains about 4.7 million words." ></td>
	<td class="line x" title="36:167	As a starting point, we chose the MWLUs that occurred more than 10 times in this manually lemmatized corpus." ></td>
	<td class="line x" title="37:167	This amounted to about 1,300 expressions." ></td>
	<td class="line x" title="38:167	For the second phase, this list has been enlarged using the Hiztegi Batua, a dictionary of standard Basque that the Basque Language Academy updates regularly (http://www2.euskaltzaindia.net/hiztegibatua)." ></td>
	<td class="line x" title="39:167	2.1 3 Main features of lexicalized phrases Many of the lexicalized phrases are semantically non-compositional (or partially compositional), i.e. they can hardly be interpreted in terms of the meaning of their constituents (adarra jo 'to pull someone's leg', literally 'to play the horn')." ></td>
	<td class="line x" title="40:167	Often, a component of these sequences hardly occurs in any other context and it is difficult to assign it a part of speech." ></td>
	<td class="line x" title="41:167	For example, the word noizik is an archaism of modern noiztik 'from when', which occurs just in the expressions noizik behin, noizik behinean, noizik noizera, and noizik behinka all meaning 'once in a while'." ></td>
	<td class="line x" title="42:167	Besides, it is not clear which is the part of speech of the words laprast in laprast egin 'to slip' or dir-dir in dir-dir egin 'to shine'." ></td>
	<td class="line x" title="43:167	From a syntactic point of view, many of these MWEs present an unusual structure." ></td>
	<td class="line x" title="44:167	For example, many complex verbs in Basque are light verb constructions, being the meaning of the compound quite compositional, e.g. lo egin 'to sleep' literally 'to make (a) sleep' or lan egin 'to work' literally 'to make (a) work'." ></td>
	<td class="line x" title="45:167	However, lo egin and lan egin can be considered 'syntactically idiomatic' since the nouns in these expressions, lo and lan, take no determiner, which would be completely ungrammatical for a noun functioning as a regular direct object (*arroz jan nuen 'I ate rice')." ></td>
	<td class="line x" title="46:167	Morphosyntactic flexibility, being significant in this type of constructions in Basque, may vary considerably." ></td>
	<td class="line x" title="47:167	For example in lo egin 'to sleep' the noun lo admits modification (lo asko egin zuen 'he slept very much') and may take the partitive assignment (ez dut lorik egin 'I haven't slept') while the verb egin can be subject to focalization (egin duzu lorik bart?" ></td>
	<td class="line x" title="48:167	'did you sleep at all last night?'); besides, the components of the construction may change positions and some elements and phrases may be placed between them (mendian egin omen zuen lasai lo 'it is said that he slept peacefully in the mountain')." ></td>
	<td class="line x" title="49:167	In contrast, alde egin 'to escape' is morphosyntactically quite rigid." ></td>
	<td class="line x" title="50:167	In all the cases, the verb egin can take any inflection." ></td>
	<td class="line x" title="51:167	For our database, we have worked out a single representation that covers all MWLUs ranging from fixed expressions to these of highest morphosyntactic flexibility." ></td>
	<td class="line x" title="52:167	Representation of MWLUs in the lexical database In this section we explain how MWLUs are represented in EDBL (Aldezabal et al. , 2001), a lexical database oriented to language processing that currently contains more than 80,000 entries, out of which 2,270 are MWLUs." ></td>
	<td class="line x" title="53:167	Among these:  ~69% are always unambiguous." ></td>
	<td class="line x" title="54:167	The average number of Surface Realization Schemas (SRS, see section 3.2) is 1.02." ></td>
	<td class="line x" title="55:167	 ~23% are sometimes unambiguous and have 3.6 SRSs in average, half of them ambiguous." ></td>
	<td class="line x" title="56:167	 ~8% are always ambiguous and have 1.2 SRSs in average." ></td>
	<td class="line x" title="57:167	We want to point out that almost all of the unambiguous MWLUs have only one SRS, their components appearing in contiguous positions and always in the same order." ></td>
	<td class="line x" title="58:167	About half of them are inflected, so, even if we discard the interpretations of the components, there is still some morphosyntactic ambiguity left." ></td>
	<td class="line x" title="59:167	However, the identification of these MWLUs helps in disambiguation, as the input of tagging is more precise." ></td>
	<td class="line x" title="60:167	The description of MWLUs within a generalpurpose lexical database must include, at least, two aspects (see Figure 1): (1) their composition, i.e. which the components of the MWLU are, whether each of them can be inflected or not, and according to which one-word lexical unit (OWLU 1 ) it inflects; and (2), what we call the surface realization, that is, the order in which the components may occur in the text, the mandatory or optional contiguousness of components, and the inflectional restrictions applicable to each one of the components." ></td>
	<td class="line x" title="61:167	3.1 Composition As it has just been said, the description of the composition of MWLUs in EDBL gathers two aspects: on the one side, it depicts which the individual components of a MWLU are; on the other side, it links the inflectable components of a MWLU to the corresponding OWLU according to which each of them inflects." ></td>
	<td class="line x" title="62:167	In Figure 1, we can see that the composed of relationship links every MWLU to up to 9 individual components (MWLU_Components)." ></td>
	<td class="line x" title="63:167	Each component is characterized by the following attributes: 1 We consider OWLUs lexical units with no spaces within its orthographical form; so, we also take hyphenated compounds as OWLUs." ></td>
	<td class="line x" title="64:167	 Component_Position: this indicates the position of the component word-form in the canonical form of the MWLU." ></td>
	<td class="line x" title="65:167	 Component_Form: i.e. the word-form itself as it appears in the canonical form of the MWLU." ></td>
	<td class="line x" title="66:167	 Conveys_Morph_Info?: this is a Boolean value, indicating whether the component inflection conveys the morphological information corresponding to the whole MWLU or not 2." ></td>
	<td class="line x" title="67:167	(0,n) (1,1) (1,1) (1,n) (2,9) (1,1) MWLUs Surface_Realization_Schemas Order_Contiguousness Sureness Inflection_Restrictions MWLU_Components Component_Position Component_Form Conveys_Morph_Info?" ></td>
	<td class="line x" title="68:167	OWLUs corresp." ></td>
	<td class="line x" title="69:167	SR schemas inflects according to composed of Figure 1." ></td>
	<td class="line x" title="70:167	Composition and surface realization of MWLUs." ></td>
	<td class="line x" title="71:167	Moreover, the components of a MWLU are linked to its corresponding OWLU (according to which it inflects)." ></td>
	<td class="line x" title="72:167	This is represented by means of the inflects according to relationship (see Figure 1)." ></td>
	<td class="line x" title="73:167	2 The morphological information that the attribute refers to is the set of morphological features the inflection takes in the current component instance." ></td>
	<td class="line x" title="74:167	These two aspects concerning the composition of a MWLU are physically stored in a single table of the relational database in which EDBL resides." ></td>
	<td class="line x" title="75:167	The columns of the table are the following: Entry, Homograph_Id, Component_ Position, Component_Form, Conveys_ Morph_Info?, OWLU_Entry, and OWLU_ Homograph_Id." ></td>
	<td class="line x" title="76:167	In the example below, the composition of the MWLU begi bistan egon 'to be evident' is described." ></td>
	<td class="line x" title="77:167	Note that one row is used per component: <begi bistan egon, 0, 1, begi, -, begi, 2> <begi bistan egon, 0, 2, bistan, -, bista, 1> <begi bistan egon, 0, 3, egon, +, egon, 1> This expression allows different realizations such as begi bistan dago 'it is evident' (literally 'it is at reach of the eyes'), begi bistan daude 'they are evident', begien bistan egon, 'to be evident', etc. In the table rows above, it can be seen that the last component egon 1 'to be' conveys the morphological information for the whole MWLU (+ in the corresponding column)." ></td>
	<td class="line x" title="78:167	3.2 Surface realization As for surface realization, we have already mentioned that the components of a MWLU can occur in a text either contiguously or dispersed." ></td>
	<td class="line x" title="79:167	Besides, the order of the constituents may be fixed or not, and they may either inflect or occur in an invariable form." ></td>
	<td class="line x" title="80:167	In the case of inflected components, some of them may accept any inflection according to its corresponding OWLU, whilst others may only inflect in a restricted way." ></td>
	<td class="line x" title="81:167	Moreover, some MWLUs are unambiguous and some are not, since it cannot be certainly assured that the very same sequence of words in a text corresponds undoubtedly to a multiword entry in every context." ></td>
	<td class="line x" title="82:167	For example, in the sentence Emilek buruaz baiezko keinu bat egin zuen 'Emile nodded his head' the words bat and egin do not correspond to the complex verb bat egin 'to unite' but to two separate phrases." ></td>
	<td class="line x" title="83:167	According to these features, we use a formal description where different realization patterns may be defined for each MWLU." ></td>
	<td class="line x" title="84:167	The corresp." ></td>
	<td class="line x" title="85:167	SR schemas relationship in Figure 1 links every MWLU to one or more Surface_Realization_Schemas." ></td>
	<td class="line x" title="86:167	Each SRS is characterized by the following attributes:  Order_Contiguousness: an expression that indicates both the order in which the components may appear in the different instances of the MWLU and the contiguousness of these components." ></td>
	<td class="line x" title="87:167	In these expressions the position of the digits indicate the position each component takes in a particular SRS, * indicates that 0 or more words may occur between two components, and ? indicates that at most one single word may appear between two given components of the MWLU." ></td>
	<td class="line x" title="88:167	 Unambiguousness: a Boolean value, indicating whether the particular SRS corresponds to an unambiguous MWLU or not." ></td>
	<td class="line x" title="89:167	It expresses whether the sequence of words matching this SRS must be unambiguously analyzed as an instance of the MWLU or, on the contrary, may be analyzed as separate OWLUs in some contexts." ></td>
	<td class="line x" title="90:167	 Inflection_Restrictions: an expression that indicates the inflection paradigm according to which the MWLU may inflect in this specific SRS." ></td>
	<td class="line x" title="91:167	In these expressions each component of the MWLU is represented by one list component (in the same order as the components of the MWLU appear in its canonical form): % indicates that the whole inflection paradigm of the corresponding inflectable component may occur; the minus sign (-) is used for non-inflectable components (no inflection at all may occur); finally, a logical expression (and, or, and not are allowed) composed of attribute-value pairs is used to express the inflectional restrictions and the morphotactics the component undergoes in this particular SRS of the MWLU (in brackets in the examples below)." ></td>
	<td class="line x" title="92:167	In the examples below, it can be seen that one row is used per SRS." ></td>
	<td class="line x" title="93:167	The columns of the table are the following: Entry, Homograph_Id, Order_Contiguousness, Unambiguousness, and Inflection_Restrictions: <begi bistan egon, 0, 123, +, (((CAS=ABS) and (DEF=-)) or ((CAS=GEN) and (NUM=PL)), -, %)> <begi bistan egon, 0, 312, +, (((CAS=ABS) and (DEF=-)) or ((CAS=GEN) and (NUM=PL)), -, %)> <begi bistan egon, 0, 3?12, +, (((CAS=ABS) and (DEF=-)), -, %)> The first SRS matches occurrences such as begi bistan dago hau ez dela aski 'it is evident that it is not enough' or begien bistan zegoen honela bukatuko genuela 'it was evident that we would end up this way', where the components are contiguous and the analysis as an instance of the MWLU would be unambiguous." ></td>
	<td class="line x" title="94:167	This SRS allows the inflection of the first component as absolutive case (non-definite) or as genitive (plural), and the whole set of inflection morphemes of the third one." ></td>
	<td class="line x" title="95:167	The third SRS matches occurrences such as ez dago horren begi bistan 'it is not so evident', where the components are not contiguous (at most one word is allowed between the third component and the first one) and they occur in a noncanonical order: 3?12." ></td>
	<td class="line x" title="96:167	In this case, the interpretation as an instance of the MWLU would also be unambiguous." ></td>
	<td class="line x" title="97:167	However, this SRS only allows the inflection of the first component as absolutive case (non-definite)." ></td>
	<td class="line x" title="98:167	3.3 4 Different information requirements in lemmatization and syntax processing The first prototype for the treatment of MWEs in Basque HABIL (Ezeiza et al. , 1998; Ezeiza, 2003) was built for lemmatization purposes." ></td>
	<td class="line x" title="99:167	However, we are nowadays involved in the construction of a deep syntactic parser (Aduriz et al. , 2004) and the MWEs seem to need a different treatment." ></td>
	<td class="line x" title="100:167	The fact that many MWEs may be syntactically regular but, above all, that an external element may have a dependency relation with one of the constituents, forces us to analyze the elements independently." ></td>
	<td class="line x" title="101:167	For example, in the verb beldur izan 'to be afraid (of)' an external noun phrase may have a modifiernoun dependency relation with beldur 'fear' as in sugeen beldur naiz 'I'm afraid of snakes'." ></td>
	<td class="line x" title="102:167	In loak hartu 'to fall asleep' there is a subject-verb relation as in loak hartu nau 'I have fallen asleep', literally 'sleep has caught me'; therefore subject-auxiliary verb agreement would fade if both components were analyzed as one." ></td>
	<td class="line x" title="103:167	The MWLU representation we have adopted allows us to lemmatize the word combination as a unit and yet to parse the components individually whenever necessary." ></td>
	<td class="line x" title="104:167	In order to do so, when describing each MWLU, we specify whether the elements in the MWLU must be analyzed separately or not 3 . Treatment of multiword expressions MWEs could be treated at different stages of the language process." ></td>
	<td class="line x" title="105:167	Some approaches treat them at tokenization stage, identifying fixed phrases, such as prepositional phrases or compounds, included in a list (Carmona et al. , 1998; Karlsson et al. , 1995)." ></td>
	<td class="line x" title="106:167	Other approaches rely on morphological analysis to better identify the features of the MWE using finite state technology (Breidt et al. , 1996)." ></td>
	<td class="line x" title="107:167	Finally, there is another approach that identifies them after the tagging process, allowing the correction of some tagging errors (Leech et al. , 1994)." ></td>
	<td class="line x" title="108:167	All of these approaches are based on the use of a closed set of MWLUs that could be included in a list or a database." ></td>
	<td class="line x" title="109:167	However, some groups of MWEs are not subject to be included in a database, because they comprise an open class of expressions." ></td>
	<td class="line x" title="110:167	That is the case of collocations, compounds or named entities." ></td>
	<td class="line oc" title="111:167	The group of collocations and compounds should be delimited using statistical approaches, such as Xtract (Smadja, 1993) or LocalMax (Silva et al. , 1999), so that only the most relevantthose of higher frequency are included in the database." ></td>
	<td class="line x" title="112:167	Named entity recognition task has been solved for a large set of languages." ></td>
	<td class="line x" title="113:167	Most of these works are linked to the Message Understanding Conference (Chinchor, 1997)." ></td>
	<td class="line x" title="114:167	There is a variety of methods that have been used in NE recognition, such as HMM, Maximum Entropy Models, Decision Trees, Boosting and Voted Perceptron (Collins, 2002), Syntactic Structure based approaches and WordNet-based approaches (Magnini et al. , 2002; Arvalo, 2002)." ></td>
	<td class="line x" title="115:167	Most references on NE task might be accessed at http://www.muc.saic.com." ></td>
	<td class="line x" title="116:167	4.1 Processing MWEs with HABIL We have implemented HABIL, a tool for the treatment of multiword expressions (MWE), based 3 Currently we are studying the MWLUs in the lexical database in order to determine which of them deserve to be parsed as separate elements." ></td>
	<td class="line x" title="117:167	We have not defined yet how this will be formally represented in the database." ></td>
	<td class="line x" title="118:167	on the features described in the lexical database." ></td>
	<td class="line x" title="119:167	The most important features of HABIL are the following:  It deals with both contiguous and split MWEs." ></td>
	<td class="line x" title="120:167	 It takes into account all the possible orders of the components (SRS)." ></td>
	<td class="line x" title="121:167	 It checks that inflectional restrictions are complied with." ></td>
	<td class="line x" title="122:167	 It generates morphosyntactic interpretations for the MWE." ></td>
	<td class="line x" title="123:167	This tool has two different components: on the one hand, there is a searching engine that identifies MWEs along the text, and, on the other hand, there is a morphosyntactic processor that assigns the corresponding interpretations to the components of the MWE." ></td>
	<td class="line x" title="124:167	The morphosyntactic processor generates the interpretations for MWEs using category and subcategory information in the lexical database." ></td>
	<td class="line x" title="125:167	When one of the components adds information to the MWE, the processor applies pattern-matching techniques to extract the corresponding morphological features of the analyses of that component, and these features are included in the interpretation of the MWE." ></td>
	<td class="line x" title="126:167	Then, it replaces all the morphosyntactic interpretations of the components of unambiguous MWEs with the MWE interpretations." ></td>
	<td class="line x" title="127:167	When MWEs are ambiguous, the new interpretations are added to the existing ones." ></td>
	<td class="line x" title="128:167	HABIL also identifies and treats dates and numerical expressions." ></td>
	<td class="line x" title="129:167	As they make up an open class, they are not obviously included in the lexical database." ></td>
	<td class="line x" title="130:167	Furthermore, their components are always contiguous, have a very strict structure, and use a closed lexicon." ></td>
	<td class="line x" title="131:167	Thus, it is quite easy to identify them using simple finite state transducers." ></td>
	<td class="line x" title="132:167	For the morphosyntactic treatment of dates and numerical expressions, we use the morphosyntactic component of HABIL." ></td>
	<td class="line x" title="133:167	These expressions may appear inflected and, in this case, the last component adds morphosyntactic features to the MWE." ></td>
	<td class="line x" title="134:167	Finally, as they are unambiguous expressions, the processor discards the interpretations of the components and assigns them all the interpretations of the whole expression." ></td>
	<td class="line x" title="135:167	4.2 Evaluation We performed several experiments using 650 unambiguous, contiguous and ordered MWEs." ></td>
	<td class="line x" title="136:167	We treated a reference corpus of around 36,000 tokens and there were 386 instances of 149 different MWEs." ></td>
	<td class="line x" title="137:167	We also applied this process to a small test corpus of around 7,100 tokens in which there were 87 instances of 45 MWEs." ></td>
	<td class="line x" title="138:167	Taking both corpora into account, there were 473 instances of 167 different MWEs, which amounted to 25% of the expressions considered, and 50% of the instances were ambiguous." ></td>
	<td class="line x" title="139:167	Besides, only 14 dates and 12 numerical expressions were found in the reference corpus, and 18 dates and 9 numerical expressions in the test corpus." ></td>
	<td class="line x" title="140:167	Ambiguity Rate Interpretations per Token Recall wordforms: before after 81.78% 79.83% 3.37 3.30 99.31% 99.31% all tokens: before after 67.47% 65.86% 2.96 2.89 99.43% 99.43% Table 1." ></td>
	<td class="line x" title="141:167	Results of HABIL." ></td>
	<td class="line x" title="142:167	The ambiguity measures of the test corpus are shown in Table 1." ></td>
	<td class="line x" title="143:167	The ambiguity rate of wordforms decreases by 2% and the average ambiguity rate by 1.5% after the processing of MWEs." ></td>
	<td class="line x" title="144:167	It is important to point out that no error is made along the process." ></td>
	<td class="line x" title="145:167	Furthermore, some important MWEs, more specifically, some complex sentence connectors that have highly ambiguous components, are correctly disambiguated." ></td>
	<td class="line x" title="146:167	Bearing in mind the proportion of words treated by HABIL, these results help significantly in improving precision results of tagging and avoiding almost 10% of the errors, as shown in Table 2." ></td>
	<td class="line x" title="147:167	Precision Error before MWE processing 94.96% 5.04% after MWE processing 95.42% 4.58% Table 2." ></td>
	<td class="line x" title="148:167	Tagging results." ></td>
	<td class="line x" title="149:167	5 Future work After confirming the viability of the system and the good results in POS tagging, our main goal is to increase the number of MWLUs in the database, which will improve the identification of MWEs in corpora." ></td>
	<td class="line x" title="150:167	A remaining difficulty that we are facing is the problem of ambiguous split MWEs." ></td>
	<td class="line x" title="151:167	At present, we are creating a disambiguation grammar that will discard or select the multiword interpretations in ambiguous MWLUs." ></td>
	<td class="line x" title="152:167	We are developing similar rules using both the Constraint Grammar formalism and finite state transducers (XFST tools, Kartunnen et al. 1997)." ></td>
	<td class="line x" title="153:167	The very first rules seem to be quite effective." ></td>
	<td class="line x" title="154:167	Soon, we will be assessing the first results, and then we will be able to choose the method that performs best with a lesser effort." ></td>
	<td class="line x" title="155:167	Once we have chosen the best formalism, we intend to develop a comprehensive grammar that will disambiguate as many ambiguous MWLUs as possible." ></td>
	<td class="line x" title="156:167	In addition, we are developing new processes after POS tagging in order to identify complex named entities and terminological units." ></td>
	<td class="line x" title="157:167	These units constitute an open class and so their exhaustive inclusion in a database would not be viable." ></td>
	<td class="line x" title="158:167	6 Conclusion 7 In this paper we have described a whole framework for the representation and treatment of MWEs, which is being currently used at the IXA Research Group to process this kind of expressions in general texts." ></td>
	<td class="line x" title="159:167	Although it has been conceived and so far used for Basque, a highly inflected language, we think that it is general enough to be applied to other languages." ></td>
	<td class="line x" title="160:167	A general representation schema for MWLUs at the lexical level has been proposed." ></td>
	<td class="line x" title="161:167	This schema allows us to state which components a MWLU has and to formally encode all the different surface realizations it can adopt in the text." ></td>
	<td class="line x" title="162:167	The problems that diverse information requirements in lemmatization and syntactic processing can eventually pose have been explained, and a possible solution for the representation of these phenomena has also been outlined." ></td>
	<td class="line x" title="163:167	As for the processing aspects, we have described HABIL, the tool for the treatment of MWEs." ></td>
	<td class="line x" title="164:167	HABIL processes MWEs based on their description in the lexical database, dealing also with some types of open class MWEs." ></td>
	<td class="line x" title="165:167	One of the remaining problems when split and ambiguous MWEs are to be tagged is related with disambiguation procedures using Hidden Markov Models, which are not able to manage different paths with variable lengths." ></td>
	<td class="line x" title="166:167	This problem can be solved using rule-based methods or lattice structures for tagging." ></td>
	<td class="line x" title="167:167	Acknowledgements This research is being partially funded by the European Commission (MEANING project, IST2001-34460) and the Basque Government (Etortek-Hizking, Saiotek-Ihardetsi)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W04-0412
Non-Contiguous Word Sequences For Information Retrieval
Doucet, Antoine;Ahonen-Myka, Helana;"></td>
	<td class="line x" title="1:204	Non-Contiguous Word Sequences for Information Retrieval Antoine Doucet and Helena Ahonen-Myka Department of Computer Science P.O. Box 26 (Teollisuuskatu 23) FIN-00014 University of Helsinki, Finland, Antoine.Doucet@cs.helsinki.fi, Helena.Ahonen-Myka@cs.helsinki.fi Abstract The growing amount of textual information available electronically has increased the need for high performance retrieval." ></td>
	<td class="line x" title="2:204	The use of phrases was long seen as a natural way to improve retrieval performance over the common document models that ignore the sequential aspect of word occurrences in documents, considering them as bags of words." ></td>
	<td class="line x" title="3:204	However, both statistical and syntactical phrases showed disappointing results for large document collections." ></td>
	<td class="line x" title="4:204	In this paper we present a recent type of multi-word expressions in the form of Maximal Frequent Sequences (Ahonen-Myka, 1999)." ></td>
	<td class="line x" title="5:204	Mined phrases rather than statistical or syntactical phrases, their main strengths are to form a very compact index and to account for the sequentiality and adjacency of meaningful word co-occurrences, by allowing for a gap between words." ></td>
	<td class="line x" title="6:204	We introduce a method for using these phrases in information retrieval and present our experiments." ></td>
	<td class="line x" title="7:204	They show a clear improvement over the well-known technique of extracting frequent word pairs." ></td>
	<td class="line x" title="8:204	1 Introduction The constantly growing number of electronic documents increases the need for high performance retrieval, the precision of a system being the percentage of relevant documents among the total number of hits returned to a query." ></td>
	<td class="line x" title="9:204	Most information retrieval systems do not account for word order in a document." ></td>
	<td class="line x" title="10:204	However, we can assume that there must exist a way to account for word order, which permits to improve retrieval performance." ></td>
	<td class="line x" title="11:204	Zhai et al.(1997) mention many problems due the use of single word terms only." ></td>
	<td class="line x" title="13:204	They observe that some word associations have a totally different meaning of the sum of the meanings of the words that compose them (e.g. , hot dog is usually not used to refer to a warm dog !)." ></td>
	<td class="line x" title="14:204	Other lexical units pose similar problems (e.g. , kick the bucket)." ></td>
	<td class="line x" title="15:204	Work on the use of phrases in IR has been carried out for more than 25 years." ></td>
	<td class="line x" title="16:204	Early results were very promising." ></td>
	<td class="line x" title="17:204	However, unexpectedly, the constant growth of test collections caused a drastic fall in the quality of the results." ></td>
	<td class="line x" title="18:204	In 1975, Salton et al.(1975) show an improvement in average precision over 10 recall points between 17% and 39%." ></td>
	<td class="line x" title="20:204	In 1989, Fagan (1989) reiterated the exact same experiments with a 10 Mb collection and obtained improvements from 11% to 20%." ></td>
	<td class="line x" title="21:204	This negative impact of the collection size was lately confirmed by Mitra et al.(1987) over a 655 Mb collection, improving the average precison by only one percent ! Turpin and Moffat (1999) revisited and extended this work to obtain improvements between 4% and 6%." ></td>
	<td class="line x" title="23:204	A conclusion of this related work is that phrases improve results in low levels of recall, but are globally inefficient for the n first ranked documents." ></td>
	<td class="line x" title="24:204	According to Mitra et al.(1987), this low benefit from phrases to the best answers is explained by the fact that phrases promote documents that deal with only one aspect of possibly multi-faceted queries." ></td>
	<td class="line x" title="26:204	For example, a topic of TREC-4 is about problems associated with pension plans, such as fraud, skimming, tapping or raiding." ></td>
	<td class="line x" title="27:204	Several top-ranked documents discuss pension plans, but no related problem." ></td>
	<td class="line x" title="28:204	Mitra et al.(1987) term this problem as one of inadequate query coverage." ></td>
	<td class="line x" title="30:204	In our opinion, this does not contradict the idea that adding document descriptors accounting for word order must permit to improve the performance of IR systems." ></td>
	<td class="line x" title="31:204	But related work shows the need for another way to combine phrase and word term descriptors (Smeaton and Kelledy, 1998) and even more the fact that the phrases currently used to model documents are not well suited for that." ></td>
	<td class="line x" title="32:204	In the next section, we will briefly describe Second ACL Workshop on Multiword Expressions: Integrating Processing, July 2004, pp." ></td>
	<td class="line x" title="33:204	88-95 the vector space model (sometimes quoted as bag of words, for it simply ignores words positions)." ></td>
	<td class="line x" title="34:204	We will then describe the different types of phrases used in related work (section 3)." ></td>
	<td class="line x" title="35:204	In section 4, we define our own phrases (maximal frequent sequences) and explain how they will be better document descriptors than those found in the state of the art." ></td>
	<td class="line x" title="36:204	In section 5, we present a technique to incorporate maximal frequent sequences into document indexing and query processing, so as to properly take advantage of this extra information in an information retrieval framework." ></td>
	<td class="line x" title="37:204	In section 6, we present our experiments and results, before we conclude the paper in section 7." ></td>
	<td class="line x" title="38:204	2 Vector Space Model 2.1 Preprocessing The first step of the process is to clean the data." ></td>
	<td class="line x" title="39:204	A way to do this consists in skipping a set of words that are considered least informative, the stopwords." ></td>
	<td class="line x" title="40:204	We also discarded all words of small size (less than three characters)." ></td>
	<td class="line x" title="41:204	We then reduced each word to its stem using the Porter algorithm (Porter, 1980)." ></td>
	<td class="line x" title="42:204	For example, the words models, modelling and modeled are all stemmed to model." ></td>
	<td class="line x" title="43:204	This technique for reducing words to their root permits to further reduce the number of word terms." ></td>
	<td class="line x" title="44:204	This feature selection phase brings more computational comfort for the next steps since it greatly reduces the size of the document collection representation in the vector space model (the dimension of the vector space)." ></td>
	<td class="line x" title="45:204	2.2 Vector Space Model The set of the distinct remaining word stems W is used to represent the document collection within the vector space model." ></td>
	<td class="line x" title="46:204	Each document is represented by a bardblWbardbl-dimensional vector filled in with a weight standing for the importance of each word token with respect to that document." ></td>
	<td class="line x" title="47:204	To calculate this weight, we use a tfnormalized version of the tfc term-weighted components as described by Salton and Buckley (1988), i.e.: tfidfw = tfw log N nw max(tf) radicalbiggsummationtext wiW parenleftBig tfwi log Nnw i parenrightBig2, where tfw is the term frequency of the word w. N is the total number of documents in the collection and nw the number of documents in which w occurs." ></td>
	<td class="line x" title="48:204	3 The use of phrases in IR There are various ways to incorporate phrases in the document modeling." ></td>
	<td class="line x" title="49:204	The usual technique is to consider phrases as supplementary terms of the vector space, with the same technique as for word terms." ></td>
	<td class="line x" title="50:204	In other words, phrases are thrown into the bag of words." ></td>
	<td class="line x" title="51:204	However, Strzalkowski and Carballo (1996) argue that using a standard weighting scheme is inappropriate for mixed feature sets (such as single words and phrases)." ></td>
	<td class="line x" title="52:204	The weight given to least frequent phrases is considered too low." ></td>
	<td class="line x" title="53:204	Their specificity is nevertheless often crucial in order to determine the relevance of a document (Lahtinen, 2000)." ></td>
	<td class="line x" title="54:204	In weighting the phrases, the interdependency between a phrase and the words that compose it is another difficult issue to account for Strzalkowski et al.(1998)." ></td>
	<td class="line x" title="56:204	There are two main types of phrases: statistical phrases, formed by straight word occurrence counts, and syntactical phrases." ></td>
	<td class="line x" title="57:204	Statistical Phrases." ></td>
	<td class="line x" title="58:204	Mitra et al.(1987) form a statistical phrase for each pair of 2 stemmed adjacent words that occur in at least 25 documents of the TREC-1 collection." ></td>
	<td class="line x" title="60:204	The selected pairs are then sorted in lexicographical order." ></td>
	<td class="line x" title="61:204	In this technique, we see 2 problems." ></td>
	<td class="line x" title="62:204	First, this lexicographical sorting means to ignore crucial information about word pairs: their order of occurrence ! This is equivalent to saying that AB is identical to BA." ></td>
	<td class="line x" title="63:204	Furthermore, no gap is allowed, although it is frequent to represent the same concept by adding at least one word between two others." ></td>
	<td class="line x" title="64:204	For example, this definition of a phrase does not permit to note any similarity between the two text fragments XML document retrieval and XML retrieval." ></td>
	<td class="line x" title="65:204	This model is thus quite far from natural language." ></td>
	<td class="line x" title="66:204	Syntactical Phrases." ></td>
	<td class="line x" title="67:204	The technique presented by Mitra et al.(1987) for extracting syntactical phrases is based on a parts-of-speech analysis (POS) of the document collection." ></td>
	<td class="line x" title="69:204	A set of tag sequence patterns are predefined to be recognized as useful phrases." ></td>
	<td class="line x" title="70:204	All maximal sequences of words accepted by this grammar form the set of syntactical phrases." ></td>
	<td class="line x" title="71:204	For example, a sequence of words tagged as verb, cardinal number, adjective, adjective, noun will constitute a syntactical phrase of size 5." ></td>
	<td class="line x" title="72:204	Every sub-phrase occurring in this same order is also generated, with an unlimited gap (e.g. , the pair verb, noun is also generated)." ></td>
	<td class="line x" title="73:204	This technique offers a sensible representation of natural language." ></td>
	<td class="line x" title="74:204	Unfortunately, to obtain the POS of a whole document collection is very costful." ></td>
	<td class="line x" title="75:204	The index size is another issue, given that all phrases are stored, regardless of their frequency." ></td>
	<td class="line x" title="76:204	In the experiments, the authors indeed admit to creating no index a priori, but instead that the phrases were generated according to each query." ></td>
	<td class="line x" title="77:204	This makes the process tractable, but implies very slow answers from the retrieval system, and quite a long wait for the end user." ></td>
	<td class="line x" title="78:204	On top of computational problems, we see a few further issues." ></td>
	<td class="line x" title="79:204	First, the lack of a minimal frequency threshold to reduce the number of phrases in the index." ></td>
	<td class="line x" title="80:204	This means that unfrequent phrases are taking up most of the space, and have a big influence on the results, whereas their low frequency may simply illustrate an inadequate use or a typographical error." ></td>
	<td class="line x" title="81:204	To allow an illimited gap so as to generate subpairs is dangerous as well: the phrase I like to eat hot dogs will generate the subpair hot dogs, but it will also generate the subpair like dogs, whose semantical meaning is very far from that of the original sentence." ></td>
	<td class="line x" title="82:204	Other types of phrases." ></td>
	<td class="line pc" title="83:204	Many efficient techniques exist to extract multiword expressions, collocations, lexical units and idioms (Church and Hanks, 1989; Smadja, 1993; Dias et al. , 2000; Dias, 2003)." ></td>
	<td class="line n" title="84:204	Unfortunately, very few have been applied to information retrieval with a deep evaluation of the results." ></td>
	<td class="line x" title="85:204	Maximal Frequent Sequences." ></td>
	<td class="line x" title="86:204	We propose Maximal Frequent Sequences (MFS) as a new alternative to account for word ordering in the modeling of textual documents." ></td>
	<td class="line x" title="87:204	One of their strength is the fact that they are extracted if and only if they occur more often than a given frequency threshold, which hopefully permits to avoid storing the numerous least significant phrases." ></td>
	<td class="line x" title="88:204	A gap between words is allowed within the extraction process itself, permitting to deal with a larger variety of language." ></td>
	<td class="line x" title="89:204	4 Maximal Frequent Sequences In our approach, we represent documents by word features within the vector space model, and by Maximal Frequent Sequences, accounting for the sequential aspect of text." ></td>
	<td class="line x" title="90:204	For each of those two representations, a Retrieval Status Value (RSV) is computed." ></td>
	<td class="line x" title="91:204	Those values are later combined to form a single RSV per document." ></td>
	<td class="line x" title="92:204	4.1 Definition and Extraction Technique MFS are sequences of words that are frequent in the document collection and, moreover, that are not contained in any other longer frequent sequence." ></td>
	<td class="line x" title="93:204	Given a frequency threshold , a sequence is considered to be frequent if it appears in at least  documents." ></td>
	<td class="line x" title="94:204	Ahonen-Myka (1999) presents an algorithm combining bottom-up and greedy methods, which permits to extract maximal sequences without considering all their frequent subsequences." ></td>
	<td class="line x" title="95:204	This is a necessity, since maximal frequent sequences in documents may be rather long." ></td>
	<td class="line x" title="96:204	Nevertheless, when we tried to extract the maximal frequent sequences from the collection of documents, their number and the total number of word features in the collection did pose a clear computational problem and did not actually permit to obtain any result." ></td>
	<td class="line x" title="97:204	To bypass this complexity problem, we decomposed the collection of documents into several disjoint subcollections, small enough so that we could efficiently extract the set of maximal frequent sequences of each subcollection." ></td>
	<td class="line x" title="98:204	Joining all the sets of MFS, we obtained an approximate of the maximal frequent sequence set for the full collection." ></td>
	<td class="line x" title="99:204	We conjecture that more consistent subcollections permit to obtain a better approximation." ></td>
	<td class="line x" title="100:204	This is due to the fact that maximal frequent sequences are formed from similar text fragments." ></td>
	<td class="line x" title="101:204	Accordingly, we formed the subcollection by clustering similar documents together using the well-known k-means algorithm (see for example Willett (1988) or Doucet and AhonenMyka (2002))." ></td>
	<td class="line x" title="102:204	4.2 Main Strengths of the Maximal Frequent Sequences The method efficiently extracts all the maximal frequent word sequences from the collection." ></td>
	<td class="line x" title="103:204	From the definitions above, a sequence is said to be maximal if and only if no other frequent sequence contains that sequence." ></td>
	<td class="line x" title="104:204	Furthermore, a gap between words is allowed: in a sentence, the words do not have to appear continuously." ></td>
	<td class="line x" title="105:204	A parameter g tells how many other words two words in a sequence can have between them." ></td>
	<td class="line x" title="106:204	The parameter g usually gets values between 1 and 3." ></td>
	<td class="line x" title="107:204	For instance, if g = 2, a phrase President Bush will be found in both of the following text fragments:President of the United States Bush President George W. Bush Note: Articles, prepositions and small words were pruned away during the preprocessing." ></td>
	<td class="line x" title="108:204	This allowance of gaps between words of a sequence is probably the strongest specificity of the method, compared to most existing methods for extracting text descriptors." ></td>
	<td class="line x" title="109:204	This greatly increases the quality of the phrase, since processing takes the variety of natural language into account." ></td>
	<td class="line x" title="110:204	The other powerful specificity of the technique is the ability to extract maximal frequent sequences of any length." ></td>
	<td class="line x" title="111:204	This permits to obtain a very compact description of documents." ></td>
	<td class="line x" title="112:204	For example, by restricting the length of phrases to 8, the presence, in the document collection, of a frequent phrase of 25 words would result in thousands of phrases representing the same knowledge as this one maximal sequence." ></td>
	<td class="line x" title="113:204	The result of this extraction is that each document of the collection is described by a (possibly empty) set of MFS." ></td>
	<td class="line x" title="114:204	5 Evaluating Documents Once documents and queries are represented within our two models, a way to estimate the relevance of a document with respect to a query remains to be found." ></td>
	<td class="line x" title="115:204	As mentioned earlier, we compute two separate RSV values for the word features vector space model and the MFS model." ></td>
	<td class="line x" title="116:204	In the second step, we aggregate these two RSVs into one single relevance score for each document with respect to a query." ></td>
	<td class="line x" title="117:204	5.1 Word features RSV The vector space model offers a very convenient framework for computing similarities between documents and queries." ></td>
	<td class="line x" title="118:204	Indeed, there exist a number of techniques to compare two vectors, Euclidean distance, Jaccard and cosine similarity being the most frequently used in IR." ></td>
	<td class="line x" title="119:204	We have used cosine similarity because of its computational efficiency." ></td>
	<td class="line x" title="120:204	By normalizing the vectors, which we did in the indexing phase, cosine(d1,d2) indeed simplifies to the vector product (d1 d2)." ></td>
	<td class="line x" title="121:204	5.2 MFS RSV The first step is to create an MFS index for the document collection." ></td>
	<td class="line x" title="122:204	Once a set of maximal frequent sequences has been extracted and each document is attached to the corresponding phrases, as detailed in the previous section, it remains to define the procedure to match a phrase describing a document and a keyphrase (from a query)." ></td>
	<td class="line x" title="123:204	Note that from here onwards, keyphrase denotes a phrase found in a query, and maximal sequence denotes a phrase extracted from a document." ></td>
	<td class="line x" title="124:204	Our approach consists in decomposing keyphrases of the query into pairs." ></td>
	<td class="line x" title="125:204	Each of these pairs is bound to a score representing its quantity of relevance." ></td>
	<td class="line x" title="126:204	Informally speaking, the quantity of relevance of a word pair tells how much it makes a document relevant to include an occurrence of this pair." ></td>
	<td class="line x" title="127:204	This value depends on the specificity of the pair (expressed in terms of inverted document frequency) and modifiers, among which is an adjacency coefficient, reducing the quantity of relevance given to a pair formed by two words that are not adjacent." ></td>
	<td class="line x" title="128:204	5.2.1 Definitions: Let D be a collection of N documents and A1 Am a keyphrase of size m. Let Ai and Aj be 2 words of A1 Am occurring in this order, and n be the number of documents of the collection in which AiAj was found." ></td>
	<td class="line x" title="129:204	We define the quantity of relevance of the pair AiAj to be: Qrel(AiAj) = idf(AiAj,D)adj(AiAj), where idf(AiAj,D) represents the specificity of AiAj in collection D: idf(AiAj,D) = log parenleftbiggN n parenrightbigg, and when decomposing the keyphrase A1 Am into pairs, adj(AiAj) is a score modifier to penalize word pairs AiAj formed from non-adjacent words, and d(Ai,Aj) indicates the number of words appearing between the two words Ai and Aj (d(Ai,Aj) = 0 signifies that Ai and Aj are adjacent): adj(AiAj) =    1, if d(Ai,Aj) = 0 1, 0  1  1, if d(Ai,Aj) = 1 2, 0  2  1 if d(Ai,Aj) = 2  m2, 0  m2  m3, if d(Ai,Aj) = m2 Accordingly, the larger the distance between the two words, the lower a quantity of relevance is attributed to the corresponding pair." ></td>
	<td class="line x" title="130:204	In our runs, we will actually ignore distances higher than 1 (i.e. , (k > 1)  (k = 0))." ></td>
	<td class="line x" title="131:204	5.2.2 Example: For example, ignoring distances above 1, a keyphrase ABCD is decomposed into 5 tuples (pair, adjacency coefficient): (AB, 1), (BC, 1), (CD, 1), (AC, 1), (BD, 1) Let us compare this keyphrase to the documents d1,d2,d3,d4 and d5, described respectively by the frequent sequences AB, AC, AFB, ABC and ACB." ></td>
	<td class="line x" title="132:204	The corresponding quantities of relevance brought by the keyphrase ABCD are shown in table 1." ></td>
	<td class="line x" title="133:204	Note that in practice, we lost the maximality property during the partitionjoin step presented in subsection 4.1." ></td>
	<td class="line x" title="134:204	Hence, there can be a frequent sequence AB together with a frequent sequence ABC, if they were extracted from two different document clusters." ></td>
	<td class="line x" title="135:204	Assuming equal idf values, we observe that the quantities of relevance form a coherent order." ></td>
	<td class="line x" title="136:204	The longest matches rank first, and matches of equal size are untied by adjacency." ></td>
	<td class="line x" title="137:204	Moreover, non-adjacent matches (AC and ABC) are not ignored as in many other phrase representations (Mitra et al. , 1987)." ></td>
	<td class="line x" title="138:204	5.3 Aggregated RSV In practice, some queries do not contain any keyphrase, and some documents do not contain any MFS." ></td>
	<td class="line x" title="139:204	However, there can of course be correct answers to these queries, and those documents must be relevant to some queries." ></td>
	<td class="line x" title="140:204	Also, all documents containing the same matching phrases get the same MFS RSV." ></td>
	<td class="line x" title="141:204	Therefore, it is necessary to find a way to separate them." ></td>
	<td class="line x" title="142:204	The word-based cosine similarity measure is very appropriate for that." ></td>
	<td class="line x" title="143:204	Another natural response would have been to re-decompose the pairs into single words and form document vectors accordingly." ></td>
	<td class="line x" title="144:204	However, this would not be satisfying, because the least frequent words are all missed by the algorithm for MFS extraction." ></td>
	<td class="line x" title="145:204	An even more important category of missed words is that of frequent words that do not frequently co-occur with other words." ></td>
	<td class="line x" title="146:204	The loss would be considerable." ></td>
	<td class="line x" title="147:204	This is the reason to compute another RSV using a basic word-features vector space model." ></td>
	<td class="line x" title="148:204	<Keywords> 'concurrency control' 'semantic transaction management' 'application' 'performance benefit' 'prototype' 'simulation' 'analysis' </Keywords> Figure 1: Topic 47 To combine both RSVs to one single score, we must first make them comparable by mapping them to a common interval." ></td>
	<td class="line x" title="149:204	To do so, we used Max Norm, as presented by Vogt and Cottrell (1998), which permits to bring all positive scores within the range [0,1]: New Score = Old ScoreMax Score Following this normalization step, we aggregate both RSVs using a linear interpolation factor  representing the relative weight of scores obtained with each technique (similarly as in Marx et al.(2002))." ></td>
	<td class="line x" title="151:204	Aggregated Score = RSVWord Features+(1)RSVMFS The evidence of experiments with the INEX 2002 collection showed good results when weighting the single word RSV with the number of distinct word terms in the query (let a be that number), and the MFS RSV with the number of distinct word terms found in keyphrases of the query (let b be that number)." ></td>
	<td class="line x" title="152:204	Thus:  = aa+b For example, in Figure 1 showing topic 47, there are 11 distinct word terms and 7 distinct word terms occurring in keyphrases." ></td>
	<td class="line x" title="153:204	Thus, for this topic, we have  = 1111+7." ></td>
	<td class="line x" title="154:204	6 Experiments and Results We based our experiments on the 494Mb INEX document collection (Initiative for the Evaluation of XML retrieval1)." ></td>
	<td class="line x" title="155:204	INEX was created in 2002 to compensate the lack of an evaluation forum for the XML information retrieval." ></td>
	<td class="line x" title="156:204	This collection consists of 12,107 scientific articles written in English from IEEE journals, combined to a set of queries and corresponding manual assessments." ></td>
	<td class="line x" title="157:204	The specificity of this 1available at http://inex.is.informatik.uniduisburg.de:2003/ Document MFS Corresponding pairs Matches Quantity of relevance d1 AB AB AB idf(AB) d2 ACD AC CD AD AC CD idf(CD) + 1.idf(AC) d3 AFB AF FB AB AB idf(AB) d4 ABC AB BC AC AB BC AC idf(AB) + idf(BC) + 1.idf(AC) d5 ACB AC CB AB AC AB idf(AB) + 1.idf(AC) Table 1: Quantity of relevance stemming from various indexing phrases w.r.t. a keyphrase query ABCD document collection is its rich logical structure into sections, subsections, paragraphs, lists, etc. However, in the present experiments, we ignore this structure and only exploit plain text to return full articles as our candidate retrieval answers." ></td>
	<td class="line x" title="158:204	The manual assessments indeed tell us which candidate answers are relevant and which ones are not." ></td>
	<td class="line x" title="159:204	We use these relevance values to compute precision and recall measures, which permit scoring each set of candidate answers, and equivalently the means by which each set was obtained." ></td>
	<td class="line x" title="160:204	In our experiments, we used average precision over the n first hits as our main reference." ></td>
	<td class="line x" title="161:204	This evaluation measure was first introduced by Raghavan et al.(1989) and was used as the official evaluation measure in the INEX 2002 campaign (Govert et al. , 2003)." ></td>
	<td class="line x" title="163:204	Protocol of the Experiments." ></td>
	<td class="line x" title="164:204	As a baseline, we computed and evaluated a run using only single word terms, as detailed in section 2." ></td>
	<td class="line x" title="165:204	Our goal was to compare our new technique to the state of the art." ></td>
	<td class="line x" title="166:204	Thus we computed one run using our technique (aggregating the MFS RSVs and the single word term RSVs topic-wise, with the weighting scheme mentioned hereabove), and one run by calculating all statistical phrases following the definition of Mitra et al.(1987)." ></td>
	<td class="line x" title="168:204	The only difference is that we did not set a minimal document frequency threshold." ></td>
	<td class="line x" title="169:204	We made this choice from the standpoint that our aim was not to measure efficiency, but the quality of the results." ></td>
	<td class="line x" title="170:204	The corresponding number of features is given in table 2." ></td>
	<td class="line x" title="171:204	We extracted 328,289 MFS of different sizes." ></td>
	<td class="line x" title="172:204	Their splitting forms no more than 674,257 pairs (this number is probably lower because the same pair can be extracted from numerous MFS)." ></td>
	<td class="line x" title="173:204	MFS vs. Statistical Phrases." ></td>
	<td class="line x" title="174:204	For those representations, the average precision for the n first retrieved documents are presented in table 3." ></td>
	<td class="line x" title="175:204	We learn two things from those results." ></td>
	<td class="line x" title="176:204	Number of Features Word terms (Baseline) 156,723 Statitiscal Phrases 4,941,051 MFS 674,257 Table 2: Number of feature terms Weight of the word RSV Words & Stat." ></td>
	<td class="line x" title="177:204	Pairs Topicwise (subsection 5.3)." ></td>
	<td class="line x" title="178:204	0.05825 20% 0.05902 40% 0.05957 60% 0.05843 80% 0.05527 100% 0.05302 Table 4: Average Precision@100 for various linear combinations First, the fact that phrases improve results in lower levels of recall is confirmed, as greater improvement is obtained when we check further down the ranked list." ></td>
	<td class="line x" title="179:204	Second, our technique outperforms that of statistical phrases." ></td>
	<td class="line x" title="180:204	However, as we use different phrases indeed, but also a different technique to match them against queries, it remains to find out whether the improvement stems from the MFS themselves, from the way they are used, or from both." ></td>
	<td class="line x" title="181:204	Thus we experimented with various linear combinations to aggregate the word term RSV and the statistical phrase RSV." ></td>
	<td class="line x" title="182:204	The results are presented in table 4." ></td>
	<td class="line x" title="183:204	The technique of gathering word and pairs features within the same vector space clearly performs better in this case." ></td>
	<td class="line x" title="184:204	Therefore, the better performance of MFS is not only due to the aggregation weigthing scheme presented in subsection 5.3." ></td>
	<td class="line x" title="185:204	This underlines their intrinsic quality as document descriptors." ></td>
	<td class="line x" title="186:204	Word Terms Words and Stat." ></td>
	<td class="line x" title="187:204	Phrases Words and MFS Average Precision@100 0.05302 0.06199 (+16.9%) 0.06713 (+26.6%) Average Precision@50 0.64419 0.62456 (-3.0%) 0.64411 (-0.0%) Average Precision@10 0.67101 0.65021 (-3.1%) 0.66293 (-1.2%) Table 3: Average Precision@n 7 Conclusions We have introduced a new type of phrases to the problem of information retrieval." ></td>
	<td class="line x" title="188:204	We have developed and presented a method to use maximal frequent sequences in information retrieval." ></td>
	<td class="line x" title="189:204	Using the INEX document collection, we compared it to a well-known technique of the state of the art." ></td>
	<td class="line x" title="190:204	Our technique outperformed that of statistical phrases, known to be performing comparably to syntactical and linguistical phrases from the literature." ></td>
	<td class="line x" title="191:204	These results are due to the allowance of a gap between words forming a sequence, offering a more realistic model of natural language." ></td>
	<td class="line x" title="192:204	Furthermore, the number of phrases to index is rather small." ></td>
	<td class="line x" title="193:204	A weak spot is the greedy algorithm to extract MFS." ></td>
	<td class="line x" title="194:204	But many improvements are under way on this side, and the partitionjoin technique mentioned in subsection 4.1 already permits to extract good approximations efficiently." ></td>
	<td class="line x" title="195:204	Our results confirm that the best improvements are obtained at the highest levels of recall." ></td>
	<td class="line x" title="196:204	Therefore, MFS would be most useful in the case of exhaustive information needs." ></td>
	<td class="line x" title="197:204	Cases where no relevant information should be missed, and 100% recall should be reached in a minimal number of hits (their inner ordering being a less serious matter)." ></td>
	<td class="line x" title="198:204	Typically, examples of such information lie in the judicial domain and in patent searching." ></td>
	<td class="line x" title="199:204	More experiments remain to be done, to find out whether similar improvements can be obtained from other document collections." ></td>
	<td class="line x" title="200:204	The INEX collection is of scientific articles and consistently uses a terminology of its own." ></td>
	<td class="line x" title="201:204	Whether similar performance would be observed from a more general document collection such as newspaper articles has to be verified." ></td>
	<td class="line x" title="202:204	The use of phrases is factual in many languages, which makes us optimistic regarding an application of this work to multilingual document corporas." ></td>
	<td class="line x" title="203:204	Thinking of the other techniques, the gap should give us robustness against the challenges of multilingualism." ></td>
	<td class="line x" title="204:204	8 Acknowledgements This work was funded by the Academy of Finland under project 50959: DoReMi Document Management, Information Retrieval and Text Mining." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W04-1113
Using Synonym Relations In Chinese Collocation Extraction
Li, Wanyin;Lu, Qin;Xu, Ruifeng;"></td>
	<td class="line x" title="1:194	Using Synonym Relations In Chinese Collocation Extraction Wanyin Li Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong cswyli@comp.polyu.edu.hk Qin Lu Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong csluqin @comp.polyu.edu.hk Ruifeng Xu Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong csrfxu@comp.polyu.edu.hk Abstract A challenging task in Chinese collocation extraction is to improve both the precision and recall rate." ></td>
	<td class="line x" title="2:194	Most lexical statistical methods including Xtract face the problem of unable to extract collocations with lower frequencies than a given threshold." ></td>
	<td class="line x" title="3:194	This paper presents a method where HowNet is used to find synonyms using a similarity function." ></td>
	<td class="line x" title="4:194	Based on such synonym information, we have successfully extracted synonymous collocations which normally cannot be extracted using the lexical statistical approach." ></td>
	<td class="line x" title="5:194	We applied synonyms mapping to each headword to extract more synonymous word bi-grams." ></td>
	<td class="line x" title="6:194	Our evaluation over 60MB tagged corpus shows that we can extract synonymous collocations that occur with very low frequency, sometimes even for collocations that occur only once in the training set." ></td>
	<td class="line n" title="7:194	Comparing to a collocation extraction system based on Xtract, we have reached the precision rate of 43% on word bi-grams for a set of 9 headwords, almost 50% improvement from precision rate of 30% in the Xtract system." ></td>
	<td class="line x" title="8:194	Furthermore, it improves the recall rate of word bi-gram collocation extraction by 30%." ></td>
	<td class="line x" title="9:194	1 Introduction A Chinese collocation is a recurrent and conventional expression of words which holds syntactic and semantic relations." ></td>
	<td class="line x" title="10:194	A widely adopted definition given by Benson (Benson 1990) stated that a collocation is an arbitrary and recurrent word combination. For example, we say warm greetings rather than hot greetings, broad daylight rather than bright daylight." ></td>
	<td class="line x" title="11:194	Similarly, in Chinese       are three nouns with similar meanings, however, we say   rather than  ,  rather than  ." ></td>
	<td class="line oc" title="12:194	Study in collocation extraction using lexical statistics has gained some insights to the issues faced in collocation extraction (Church and Hanks 1990, Smadja 1993, Choueka 1993, Lin 1998)." ></td>
	<td class="line x" title="13:194	As the lexical statistical approach is developed based on the recurrence property of collocations, only collocations with reasonably good recurrence can be extracted." ></td>
	<td class="line x" title="14:194	Collocations with low occurrence frequency cannot be extracted, thus affecting the recall rate." ></td>
	<td class="line oc" title="15:194	The precision rate using the lexical statistics approach can reach around 60% if both word bi-gram extraction and n-gram extractions are taking into account (Smadja 1993, Lin 1997 and Lu et al. 2003)." ></td>
	<td class="line x" title="16:194	The low precision rate is mainly due to the low precision rate of word bigram extractions as only about 30% 40% precision rate can be achieved for word bi-grams." ></td>
	<td class="line x" title="17:194	In this paper, we propose a different approach to find collocations with low recurrences." ></td>
	<td class="line x" title="18:194	The main idea is to make use of synonym relations to extract synonymous collocations." ></td>
	<td class="line x" title="19:194	Lin (Lin 1997) described a distributional hypothesis that if two words have similar set of collocations, they are probably similar." ></td>
	<td class="line x" title="20:194	In HowNet, Liu Qun (Liu et al. 2002) defined the word similarity as two words that can substitute each other in the context and keep the sentence consistent in syntax and semantic structure." ></td>
	<td class="line x" title="21:194	That means, naturally, two similar words are very close to each other and they can be used in place of the other in certain context." ></td>
	<td class="line x" title="22:194	For example, we may either say  or   as and are semantically close to each other." ></td>
	<td class="line x" title="23:194	We apply this lexical phenomenal after the lexical statistics based extractor to find the low frequency synonymous collocations, thus increasing recall rate." ></td>
	<td class="line x" title="24:194	The rest of this paper is organized as follows." ></td>
	<td class="line x" title="25:194	Section 2 describes related existing collocation extraction techniques based on both lexical statistics and synonymous collocation." ></td>
	<td class="line x" title="26:194	Section 3 describes our approach on collocation extraction." ></td>
	<td class="line x" title="27:194	Section 4 evaluates the proposed method." ></td>
	<td class="line x" title="28:194	Section 5 draws our conclusion and presents possible future work." ></td>
	<td class="line x" title="29:194	2 Related Work Methods have proposed to extract collocations based on lexical statistics." ></td>
	<td class="line x" title="30:194	Choueka (Choueka 1993) applied quantitative selection criteria based on frequency threshold to extract adjacent n-grams (including bi-grams)." ></td>
	<td class="line x" title="31:194	Church and Hanks (Church and Hanks 1990) employed mutual information to extract both adjacent and distant bi-grams that tend to co-occur within a fixed-size window." ></td>
	<td class="line x" title="32:194	But the method did not extend to extract n-grams." ></td>
	<td class="line oc" title="33:194	Smadja (Smadja 1993) proposed a statistical model by measuring the spread of the distribution of cooccurring pairs of words with higher strength." ></td>
	<td class="line p" title="34:194	This method successfully extracted both adjacent and distant bi-grams and n-grams." ></td>
	<td class="line n" title="35:194	However, the method failed to extract bi-grams with lower frequency." ></td>
	<td class="line x" title="36:194	The precision rate on bi-grams collocation is very low, only around high 20% and low 30%." ></td>
	<td class="line x" title="37:194	Even though, it is difficult to measure recall rate in collocation extraction (almost no report on recall estimation), It is understood that low occurrence collocations cannot be extracted." ></td>
	<td class="line o" title="38:194	Our research group has further applied the Xtract system to Chinese (Lu et al. 2003) by adjusting the parameters to optimize the algorithm for Chinese and a new weighted algorithm was developed based on mutual information to acquire word bigrams with one higher frequency word and one lower frequency word." ></td>
	<td class="line n" title="39:194	The result has achieved an estimated 5% improvement in recall rate and a 15% improvement in precision comparing to the Xtract system." ></td>
	<td class="line n" title="40:194	All of the above techniques do not take advantage of the wide range of lexical resources available including synonym information." ></td>
	<td class="line x" title="41:194	Pearce (Pearce 2001) presented a collocation extraction technique that relies on a mapping from a word to its synonyms for each of its senses." ></td>
	<td class="line x" title="42:194	The underlying intuitions is that if the difference between the occurrence counts of one synonyms pair with respect to a particular word was at least two, then this was deemed sufficient to consider them as a collocation." ></td>
	<td class="line x" title="43:194	To apply this approach, knowledge in word (concept) semantics and relations to other words must be available such as the use of WordNet." ></td>
	<td class="line x" title="44:194	Dagan (Dagan 1997) applied similaritybased smoothing method to solve the problem of data sparseness in statistical natural language processing." ></td>
	<td class="line x" title="45:194	The experiments conducted in his later works showed that this method achieved much better results than back-off smoothing methods in word sense disambiguation." ></td>
	<td class="line x" title="46:194	Similarly, Hua Wu (Wu and Zhou 2003) applied synonyms relationship between two different languages to automatically acquire English synonymous collocation." ></td>
	<td class="line x" title="47:194	This is the first time that the concept synonymous collocation is proposed." ></td>
	<td class="line x" title="48:194	A side intuition raised here is that nature language is full of synonymous collocations." ></td>
	<td class="line x" title="49:194	As many of them have low occurrences, they are failed to be retrieved by lexical statistical methods." ></td>
	<td class="line x" title="50:194	Even though there are Chinese synonym dictionaries, such as ( Tong Yi Ci Lin), the dictionaries lack structured knowledge and synonyms are too loosely defined to be used for collocation extraction." ></td>
	<td class="line x" title="51:194	HowNet developed by Dong et al (Dong and Dong 1999) is the best publicly available resource on Chinese semantics." ></td>
	<td class="line x" title="52:194	By making use of semantic similarities of words, synonyms can be defined by the closeness of their related concepts and the closeness can be calculated." ></td>
	<td class="line x" title="53:194	In Section 3, we present our method to extract synonyms from HowNet and using synonym relations to further extract collocations." ></td>
	<td class="line x" title="54:194	Sun (Sun 1997) did a preliminary Quantitative analysis on Chinese collocations based on their arbitrariness, recurrence and the syntax structure." ></td>
	<td class="line x" title="55:194	The purpose of this study is to help differentiate if a collocation is true or not according to the quantitative factors." ></td>
	<td class="line x" title="56:194	By observing the existence of synonyms information in natural language use, we consider it possible to identify different types of collocations using more semantic and syntactic information available." ></td>
	<td class="line x" title="57:194	We discuss the basic ideas in section 5 3 Our Approach Our method of extracting Chinese collocations consists of three steps." ></td>
	<td class="line x" title="58:194	Step 1: Take the output of any lexical statistical algorithm which extracts word bi-gram collocations." ></td>
	<td class="line x" title="59:194	The data is then sorted according to each headword, W h, with its coword, W c, listed." ></td>
	<td class="line x" title="60:194	Step 2: For each headword W h used to extract bigrams, we acquire its synonyms based on a similarity function using HowNet." ></td>
	<td class="line x" title="61:194	Any word in HowNet having similarity value over a threshold value is chosen as a synonym headword W s for additional extractions." ></td>
	<td class="line x" title="62:194	Step 3: For each synonym headword, W s, and the co-word W c of W h, as its synonym, if the bigram (W s, W c ) is not in the output of the lexical statistical algorithm in Step one, take this bi-gram (W s, W c ) as a collocation if the pair co-occurs in the corpus by additional search to the corpus." ></td>
	<td class="line x" title="63:194	3.1 Structure of HowNet Different from WordNet or other synonyms dictionary, HowNet describes words as a set of concepts and each concept is described by a set of primitives." ></td>
	<td class="line x" title="64:194	The following lists for the word, one of its corresponding concepts In the above record, DEF is where the primitives are specified." ></td>
	<td class="line x" title="65:194	DEF contains up to four types of primitives: the basic independent primitive, the other independent primitive, the relation primitive, and the symbol primitive, where the basic independent primitive and the other independent primitive are used to indicate the semantics of a concept and the others are used to indicate syntactical relationships." ></td>
	<td class="line x" title="66:194	The similarity model described in the next subsection will consider both of these relationships." ></td>
	<td class="line x" title="67:194	The primitives are linked by a hierarchical tree to indicate the parent-child relationships of the primitives as shown in the following example: This hierarchical structure provides a way to link one concept with any other concept in HowNet, and the closeness of concepts can be simulated by the distance between two concepts." ></td>
	<td class="line x" title="68:194	3.2 Similarity Model Based on HowNet Liu Qun (Liu 2002) defined word similarity as two words which can substitute each other in the same context and still maintain the sentence consistent syntactically and semantically." ></td>
	<td class="line x" title="69:194	This is very close to our definition of synonyms." ></td>
	<td class="line x" title="70:194	Thus we directly used their similarity function, which is stated as follows." ></td>
	<td class="line x" title="71:194	A word in HowNet is defined as a set of concepts and each concept is represented by primitives." ></td>
	<td class="line x" title="72:194	Thus, HowNet can be described by W, a collection of n words, as: W = { w 1, w 2,  w n }Each word w i is, in turn, described by a set of concepts S as: W i = { S i1, S i2, S ix }, And, each concept S i is, in turn, described by a set of primitives: S i = { p i1, p i2 p iy } For each word pair, w 1 and w 2, the similarity function is defined by )1(),(max),( 21 1,1 21 ji mjni SSSimwwSim == = where S 1i is the list of concepts associated with W 1 and S 2j is the list of concepts associated with W 2 . As any concept S i is presented by its primitives, the similarity of primitives for any p 1, and p 2 of the same type, can be expressed by the following formula:   + = ),( ),( 21 21 ppDis ppSim (2) where  is an adjustable parameter set to 1.6, and ),( 21 ppDis is the path length between p 1 and p 2 based on the semantic tree structure." ></td>
	<td class="line x" title="73:194	The above formula where  is a constant does not indicate explicitly the fact that the depth of a pair of nodes in the tree affects their similarity." ></td>
	<td class="line x" title="74:194	For two pairs of nodes (p 1, p 2 ) and (p 3, p 4 ) with the same distance, the deeper the depth is, the more commonly shared ancestros they would have which should be semantically closer to each other." ></td>
	<td class="line x" title="75:194	In following two tree structures, the pair of nodes (p 1, p 2 ) in the left tree should be more similar than (p 3, p 4 ) in the right tree." ></td>
	<td class="line x" title="76:194	root p 2 p 1 root P 3 P 4 To indicate this observation,  is modified as a function of tree depths of the nodes using the formula  =min(d(p 1 ), d(p 2 )) . Consequently, the formula (2) is rewritten as formular (2) during the experiment." ></td>
	<td class="line x" title="77:194	))(),(min(),( ))(),(min( ),( 2121 21 21 pdpdppDis pdpd ppSim + = (2) where d(p i ) is the depth of node p i in the tree . The comparison of calculating the word similarity by applying the formula (2) and (2) is shown in Section 4.4." ></td>
	<td class="line x" title="78:194	Based on the DEF description in HowNet, different primitive types play different roles only some are directly related to semantics." ></td>
	<td class="line x" title="79:194	To make use of both the semantic and syntactic information included in HowNet to describe a word, the similarity of two concepts should take into consideration of all primitive types with weighted considerations and thus the formula is defined as )3(),(),( 21 1 4 1 21 jj i j j i i ppSimSSSim  == =  where  i is a weighting factor given in (Liu 2002) with the sum of  1 +  2 +  3 +  4 being 1 and  1   2   3   4 . The distribution of the weighting factors is given for each concept a priori in HowNet to indicate the importance of primitive p i in defining the corresponding concept S. 3.3 Collocation Extraction In order to extract collocations from a corpus, and to obtain result for Step 1 of our algorithm, we used the collocation extraction algorithm developed by the research group at the Hong Kong Polytechnic University(Lu et al. 2003)." ></td>
	<td class="line o" title="80:194	The extraction of bi-gram collocation is based on the English Xtract(Smaja 1993) with improvements." ></td>
	<td class="line x" title="81:194	Based on the three Steps mentioned earlier, we will present the extractions in each step in the subsections." ></td>
	<td class="line o" title="82:194	3.3.1 Bi-gram Extraction Based on the lexical statistical model proposed by Smadja in Xtract on extracting English collocations, an improved algorithm was developed for Chinese collocation by our research group and the system is called CXtract." ></td>
	<td class="line x" title="83:194	For easy of understanding, we will explain the algorithm briefly here." ></td>
	<td class="line o" title="84:194	According to Xtract, word cooccurence is denoted by a tripplet (w h, w i, d) where w h is a given headword, w i is a co-word appeared in the corpus in a distance d within the window of [-5, 5]." ></td>
	<td class="line x" title="85:194	The frequency f i of the co-word w i in the window of [-5, 5] is defined as:  = = 5 5, j jii ff (4) where f i, j is the frequency of the co-word at distance j in the corpus within the window." ></td>
	<td class="line n" title="86:194	The average frequency of f i, denoted by i f, is given by 10/ 5 5, = = j jii ff (5) Then, the average frequency f, and the standard deviation  are defined by  = = n i i f n f 1 1 ; 2 1 )( 1  = = n i i ff n  (6) The Strength of the co-occurrence for the pair (w h, w i,), denoted by k i, is defined by  ff k i i  =  (7) Furthermore, the Spread of (w h, w i,),, denoted as U i, which characterizes the distribution of w i around w h is define as: 10 )( 2,  = iji i ff U ; (8) To eliminate the bi-grams with unlikely cooccurrence, the following sets of threshold values is defined: 0 :1 K ff kC i i   =  (9) 0 :2 UUC i  (10) )(:3 1, iiji UKffC + (11) However, the above statistical model given by Smadja fails to extract the bi-grams with a much higher frequency of w h but a relatively low frequency word of w i,, For example, in the bigram, freq ( ) is much lower than the freq ( )." ></td>
	<td class="line x" title="87:194	Therefore, we further defined a weighted mutual information to extract this kind of bi-grams:, )( ),w( 0 h R wf wf R i i i = (12) As a result, the system should return a list of triplets (w h, w i, d), where (w h, w i,) is considered collocations." ></td>
	<td class="line x" title="88:194	3.3.2 Synonyms Set For each given headword w h, before taking it as an input to extract its bi-grams directly, we fist apply the similarity formula described in Equation (1) to generate a set of synonyms headwords W syn : }),(:{ >= shssyn wwSimwW (13) Where 0 < <1 is an algorithm parameter which is adjusted based on experience." ></td>
	<td class="line x" title="89:194	We set it as 0.85 from the experiment because we would like to balance the strength of the synonyms relationship and the coverage of the synonyms set." ></td>
	<td class="line x" title="90:194	The setting of the parameter  < 0.85 weaks the similarity strength of the extracted synonyms." ></td>
	<td class="line x" title="91:194	For example, for a given collocation  , that is unlikely to include the candidates  ,  ,  ." ></td>
	<td class="line x" title="92:194	On the other hand, by setting the parameter  > 0.85 will limit the coverage of the synonyms set and hence lose valuable synonyms." ></td>
	<td class="line x" title="93:194	For example, for a given bi-gram  , we hope to include the candidate synonymous collocations such as  ,  ,  ." ></td>
	<td class="line x" title="94:194	We will show the test of  in the section 4.2." ></td>
	<td class="line x" title="95:194	This synonyms headwords set provides the possibility to extract the synonymous collocation with the lower frequency that failed to be extracted by lexical statistic." ></td>
	<td class="line x" title="96:194	3.3.3 Synonymous Collocations A phenomenal among the collocations in natural language is that there are many synonymous collocations exist." ></td>
	<td class="line x" title="97:194	For example, switch on light and turn on light,   and  ." ></td>
	<td class="line x" title="98:194	Due to the domain specification of the corpus, some of the synonymous collocations may fail to be extracted by the lexical statistic model because of their lower frequency." ></td>
	<td class="line x" title="99:194	Based on this observation, this paper takes a further step." ></td>
	<td class="line x" title="100:194	The basic idea is for a bi-gram collocation (w h, w c, d ) we select the synonyms w s of w h with the maximum similarity respect to all the concepts contained by w h, we deem (w s, w c, d ) as a collocation if its occurrence is greater than 1 in the corpus." ></td>
	<td class="line x" title="101:194	There are similar works discussed by Pearce (Pearce 2001)." ></td>
	<td class="line x" title="102:194	For a given collocation (w s, w c,, d), if w s  W syn, then we deem the triple (w s, w c,, d) as a synonymous collocation with respect to the collocation (w h, w c,, d) if the co-occurrence of (w s, w c,, d) in the corpus is greater than one." ></td>
	<td class="line x" title="103:194	Therefore, we define the collection of synonymous collocations C syn as: }1),,(:),,{( >= dwwFreqdwwC cscssyn (14) where w s  W syn . 4 Evaluation The performance of collocation is normally evaluated by precision and recall as defined below." ></td>
	<td class="line x" title="104:194	nsCollocatioextractedofnumbertotal nsCollocatioExtractedcorrectofnumber precision = (15) nsCollocatioactualofnumbertotal nsCollocatioExtractedcorrectofnumber recall = (16) To evaluate the performance of our approach, we conducted a set of experiments based on 9 selected headwords." ></td>
	<td class="line x" title="105:194	A baseline system using only lexical statistics given in 3.3.1 is used to get a set of baseline data called Set A. The output using our algorithm is called Set B. Results are checked by hand for validation on what is true collocation and what is not a true collocation." ></td>
	<td class="line x" title="106:194	Table 1." ></td>
	<td class="line x" title="107:194	Sample table for the true collocation with headword   Table 2." ></td>
	<td class="line x" title="108:194	Sample table for the bi-grams that are not true collocations Table 1 shows samples of extracted word bi-grams using our algorithm that are considered synonymous collocations for the headword  ." ></td>
	<td class="line x" title="109:194	Table 2 shows extracted bi-grams by our algorithm that are not considered true collocations." ></td>
	<td class="line x" title="110:194	4.1 Test Set Our experiment is based on a corpus of six months tagged People Daily with 11 millions number of words." ></td>
	<td class="line x" title="111:194	For word bi-gram extractions, we consider only content words, thus headwords are selected from noun, verb and adjective only." ></td>
	<td class="line x" title="112:194	For evaluation purpose, we selected randomly 3 nouns, 3 verbs and 3 adjectives with frequency of low, medium and high." ></td>
	<td class="line x" title="113:194	Thus, in Step 1 of the algorithm, 9 headwords were used to extract bigram collocations from the corpus, and 253 pairs of collocations were extracted." ></td>
	<td class="line x" title="114:194	Evaluation by hand has identified 77 true collocations in Set A test set." ></td>
	<td class="line x" title="115:194	The overall precision rate is 30% (see Table 3)." ></td>
	<td class="line x" title="116:194	Noun+Verb +Adjective Headword 9 Extracted Bi-grams 253 True collocations using lexical statistics only 77 Precision rate 30% Table 3: Statistics in test set for set A Using Step 2 of our algorithm, where  =0.85 is used, we have obtained 55 synonym headwords (include the 9 headwords)." ></td>
	<td class="line x" title="117:194	Out of these 55 synonyms, 614 bi-gram pairs were then extracted from the lexical statistics based algorithm, in which 179 are consider true collocations." ></td>
	<td class="line x" title="118:194	Then, by applying Step 3 of our algorithm, we extracted an additional 201 bi-gram pairs, among them, 178 are considered true collocations." ></td>
	<td class="line x" title="119:194	Therefore, using our algorithm, the overall precision rate has achieved 43%, an improvement of almost 50%." ></td>
	<td class="line x" title="120:194	The data is summarized in Table 4." ></td>
	<td class="line x" title="121:194	n., v, and adj." ></td>
	<td class="line x" title="122:194	Synonyms headword 55 Bi-grams (lexical statistics) 614 Non-synonym collocations (lexical statistics only) 179 Extracted synonym collocations Step 2 201 True synonym collocations using Step 2 178 Overall precision rate 43% Table 4: Statistics in test set for mode B 4.2 The choice of  We also conducted a set of experiments to choose the best value for the similarity functions threshold  . We tested the best value of  with both the precision rate and the estimated recall rate using the so called remainder bi-grams." ></td>
	<td class="line x" title="123:194	The remainder bi-grams is the total number of bi-grams extracted by the algorithm." ></td>
	<td class="line x" title="124:194	When precision goes up, the size of the result is smaller, which in a way is an indicator of less recalled collocations." ></td>
	<td class="line x" title="125:194	Figure 1 shows the precision rate and the estimated recall rate in testing the value of  . Figure 1." ></td>
	<td class="line x" title="126:194	Precision Rate vs. value of  From Figure 1, it is obvious that at  =0.85 the recall rate starts to drop more drastically without much incentive for precision." ></td>
	<td class="line x" title="127:194	Extracted Bigrams using lexical statistics Extracted Synonyms Collocations using Step 2 (1.2,1.4,12) 465 328 (1.4,1.4,12) 457 304 (1.4,1.6,12) 394 288 (1.2,1.2,12) 513 382 (1.2,1.2,14) 503 407 (1.2,1.2,16) 481 413 Table 5: Value of (K 0, K 1, U 0 )." ></td>
	<td class="line x" title="128:194	4.3 The test of (K 0, K 1, U 0 ) The original threshold for CXtract is (1.2, 1.2, 12) for the parameters (K 0, K 1, U 0 )." ></td>
	<td class="line x" title="129:194	However, with synonyms collocations, we have also conducted some experiments to see whether the parameters should be adjusted." ></td>
	<td class="line x" title="130:194	Table 5 shows the statistics to test the value of (K 0, K 1, U 0 )." ></td>
	<td class="line x" title="131:194	The similarity threshold  was fixed at 0.85 throughout the experiments." ></td>
	<td class="line x" title="132:194	The experimental shows that varying the value of (k 0, k 1 ) does not bring any benefit to our algorithm." ></td>
	<td class="line x" title="133:194	However, increasing the value of u 0 did improve the extraction of synonymous collocations." ></td>
	<td class="line x" title="134:194	Figure 2 shows that U 0 =14 is a good trade-off for the precision rate and the remainder Bi-grams." ></td>
	<td class="line x" title="135:194	The basic meaning behind the result is reasonable." ></td>
	<td class="line o" title="136:194	According to Smadja, U 0 defined in the formula (8) represents the co-occurrence distribution of the candidate collocation (w h, w c ) in the position of d (-5  d  5)." ></td>
	<td class="line x" title="137:194	For a true collocation (w h, w c,, d), its cooccurrence in the position d is much higher than in other positions which leads to a peak in the cooccurrence distribution." ></td>
	<td class="line x" title="138:194	Therefore, it is selected by the statistical algorithm based on the formula (10)." ></td>
	<td class="line x" title="139:194	Based on the physical meaning behind, one way to improve the precision rate is to increase the value of the threshold U 0." ></td>
	<td class="line x" title="140:194	A side effect to an increased value of U 0 is that the recall is decreased because some true collocations do not meet the condition of cooccurrence greater than U 0." ></td>
	<td class="line x" title="141:194	Step 2 of the new algorithm regains some true collocations lost because of a higher U 0." ></td>
	<td class="line x" title="142:194	in Step 1." ></td>
	<td class="line x" title="143:194	Figure 2." ></td>
	<td class="line x" title="144:194	Precision Rate vs. Value of U 0 4.4 The comparison of similarity calculation based on formula (2) and (2) Table 6 shows the similarity value given by formula (2) where  is a constant given the value 1.6 and by formula (2) where  is replaced by a function of the depths of the nodes." ></td>
	<td class="line x" title="145:194	Results show that (2) is more fine tuned and reflects the nature of the data better." ></td>
	<td class="line x" title="146:194	For example, and are more similar than and . and are much similar but not the same." ></td>
	<td class="line x" title="147:194	Table 6: comparison of similarity calculation 5 Conclusion and Further Work In this paper, we have presented a method to extract bi-gram collocations using lexical statistics model with synonyms information." ></td>
	<td class="line x" title="148:194	Our method reaches the precision rate of 43% for the tested data." ></td>
	<td class="line x" title="149:194	Comparing to the precision of 30% using lexical statistics only, our improvement is close to 50%." ></td>
	<td class="line x" title="150:194	In additional, the recall improved 30%." ></td>
	<td class="line x" title="151:194	The contribution is that we have made use of synonym information which is plentiful in the natural language use and it works well to supplement the shortcomings of lexical statistical method." ></td>
	<td class="line x" title="152:194	Manning claimed that the lack of valid substitution for a synonym is a characteristics of collocations in general (Manning and Schutze 1999)." ></td>
	<td class="line x" title="153:194	To extend our work, we consider the use of synonym information can be further applied to help identify collocations of different types." ></td>
	<td class="line x" title="154:194	Our preliminary study has suggested that collocation can be classified into 4 types: Type 0 Collocation: Fully fixed collocation which include some idioms, proverbs and sayings such as     and so on." ></td>
	<td class="line x" title="155:194	Type 1 Collocation: Fixed collocation in which the appearance of one word implies the cooccurrence of another one such as  ." ></td>
	<td class="line x" title="156:194	Type 2 Collocation: Strong collocation which allows very limited substitution of the components, for example,  ,  ,   and so on." ></td>
	<td class="line x" title="157:194	Type 3 Collocation: Normal collocation which allows more substitution of the components, however a limitation is still required." ></td>
	<td class="line x" title="158:194	For example,         . By using synonym information and define substitutability, we can validate whether collocations are fixed collocations, strong collocations with very limited substitutions, or general collocations that can be substituted more freely." ></td>
	<td class="line x" title="159:194	6 Acknowledgements Our great thanks to Dr. Liu Qun of the Chinese Language Research Center of Peking University for letting us share their data structure in the Synonyms Similarity Calculation." ></td>
	<td class="line x" title="160:194	This work is partially supported by the Hong Kong Polytechnic University (Project Code A-P203) and CERG Grant (Project code 5087/01E) References M. Benson, 1990." ></td>
	<td class="line x" title="161:194	Collocations and General Purpose Dictionaries." ></td>
	<td class="line x" title="162:194	International Journal of Lexicography, 3(1): 23-35 Y. Choueka, 1993." ></td>
	<td class="line x" title="163:194	Looking for Needles in a Haystack or Locating Interesting Collocation Expressions in Large Textual Database." ></td>
	<td class="line x" title="164:194	Proceedings of RIAO Conference on Useroriented Content-based Text and Image Handling: 21-24, Cambridge." ></td>
	<td class="line x" title="165:194	K. Church, and P. Hanks, 1990." ></td>
	<td class="line x" title="166:194	Word Association Norms, Mutual Information,and Lexicography." ></td>
	<td class="line x" title="167:194	Computational Linguistics, 6(1): 22-29." ></td>
	<td class="line x" title="168:194	I. Dagan, L. Lee, and F. Pereira." ></td>
	<td class="line x" title="169:194	1997." ></td>
	<td class="line x" title="170:194	Similaritybased method for word sense disambiguation." ></td>
	<td class="line x" title="171:194	Proceedings of the 35th Annual Meeting of ACL: 56-63, Madrid, Spain." ></td>
	<td class="line x" title="172:194	Z. D. Dong and Q. Dong." ></td>
	<td class="line x" title="173:194	1999." ></td>
	<td class="line x" title="174:194	Hownet, http://www.keenage.com D. K. Lin, 1997." ></td>
	<td class="line x" title="175:194	Using Syntactic Dependency as Local Context to Resolve Word Sense Ambiguity." ></td>
	<td class="line x" title="176:194	Proceedings of ACL/EACL-97: 64-71, Madrid, Spain Q. Liu, 2002." ></td>
	<td class="line x" title="177:194	The Word Similarity Calculation on <<HowNet>>." ></td>
	<td class="line x" title="178:194	Proceedings of 3 rd Conference on Chinese lexicography, TaiBei Q. Lu, Y. Li, and R. F. Xu, 2003." ></td>
	<td class="line x" title="179:194	Improving Xtract for Chinese Collocation Extraction." ></td>
	<td class="line x" title="180:194	Proceedings of IEEE International Conference on Natural Language Processing and Knowledge Engineering, Beijing C. D. Manning and H. Schutze, 1999." ></td>
	<td class="line x" title="181:194	Foundations of Statistical Natural Language Processing." ></td>
	<td class="line x" title="182:194	The MIT Press, Cambridge, Massachusetts D. Pearce, 2001." ></td>
	<td class="line x" title="183:194	Synonymy in Collocation Extraction." ></td>
	<td class="line xc" title="184:194	Proceedings of NAACL'01 Workshop on Wordnet and Other Lexical Resources: Applications, Extensions and Customizations F. Smadja, 1993." ></td>
	<td class="line x" title="185:194	Retrieving collocations from text: Xtract." ></td>
	<td class="line x" title="186:194	Computational Linguistics, 19(1): 143177 H. Wu, and M. Zhou, 2003." ></td>
	<td class="line x" title="187:194	Synonymous Collocation Extraction Using Translation Information." ></td>
	<td class="line x" title="188:194	Proceeding of the 41st Annual Meeting of ACL D. K. Lin, 1998." ></td>
	<td class="line x" title="189:194	Extracting collocations from text corpora." ></td>
	<td class="line x" title="190:194	In Proc." ></td>
	<td class="line x" title="191:194	First Workshop on Computational Terminology, Montreal, Canada." ></td>
	<td class="line x" title="192:194	M. S. Sun, C. N. Huang and J. Fang, 1997." ></td>
	<td class="line x" title="193:194	Preliminary Study on Quantitative Study on Chinese Collocations." ></td>
	<td class="line x" title="194:194	ZhongGuoYuWen, No.1, 29-38, (in Chinese) ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W04-2105
Word Lookup On The Basis Of Associations : From An Idea To A Roadmap
Zock, Michael;Bilac, Slaven;"></td>
	<td class="line x" title="1:158	Word lookup on the basis of associations: from an idea to a roadmap Michael ZOCK LIMSI-CNRS B.P. 133, 91403 Orsay, France zock@limsi.fr Slaven BILAC Tokyo Institute of Technology Ookayama 2-12-1, Meguro 152-8552, Japan sbilac@cl.cs.titech.ac.jp Abstract Word access is an obligatory step in language production." ></td>
	<td class="line x" title="2:158	In order to achieve his communicative goal, a speaker/writer needs not only to have something to say, he must also find the corresponding word(s)." ></td>
	<td class="line x" title="3:158	Yet, knowing a word, i.e. having it stored in a data-base or memory (human mind or electronic device) does not imply that one is able to access it in time." ></td>
	<td class="line x" title="4:158	This is a clearly a case where computers (electronic dictionaries) can be of great help." ></td>
	<td class="line x" title="5:158	In this paper we present our ideas of how an enhanced electronic dictionary can help people to find the word they are looking for." ></td>
	<td class="line x" title="6:158	The yet-to-be-built resource is based on the age-old notion of association: every idea, concept or word is connected." ></td>
	<td class="line x" title="7:158	In other words, we assume that people have a highly connected conceptuallexical network in their mind." ></td>
	<td class="line x" title="8:158	Finding a word amounts thus to entering the network at any point by giving the word or concept coming to their mind (source word) and then following the links (associations) leading to the word they are looking for(target word)." ></td>
	<td class="line x" title="9:158	Obviously, in order to allow for this kind of access, the resource has to be built accordingly." ></td>
	<td class="line x" title="10:158	This requires at least two things: (a) indexing words by the associations they evoke, (b) identification and labeling of the most frequent/useful associations." ></td>
	<td class="line x" title="11:158	This is precisely our goal." ></td>
	<td class="line x" title="12:158	Actually, we propose to build an associative network by enriching an existing electronic dictionary (essentially) with (syntagmatic) associations coming from a corpus, representing the average citizens shared, basic knowledge of the world (encyclopedia)." ></td>
	<td class="line x" title="13:158	Such an enhanced electronic database resembles in many respects our mental dictionary." ></td>
	<td class="line x" title="14:158	Combining the power of computers and the flexibility of the human mind (omnidirectional navigation and quick jumps), it emulates to some extent the latter in its capacity to navigate quickly and efficiently in a large data base." ></td>
	<td class="line x" title="15:158	While the notions of association and spreading activation are fairly old, their use to support word access via computer is new." ></td>
	<td class="line x" title="16:158	The resource still needs to be built, and this is not a trivial task." ></td>
	<td class="line x" title="17:158	We discuss here some of the strategies and problems involved in accomplishing it with the help of people and computers (automation)." ></td>
	<td class="line x" title="18:158	1 Introduction We all experience now and then the problem of being unable to find the word expressing the idea we have in our mind." ></td>
	<td class="line x" title="19:158	It we care and have time we may reach for a dictionary." ></td>
	<td class="line x" title="20:158	Yet, this kind of resource may be of little help, if it expects from us precisely what we are looking for : a perfectly spelled word, expressing the idea we try to convey." ></td>
	<td class="line x" title="21:158	While perfect input may be reasonable in the case of analysis (comprehension), it certainly is not in the case of synthesis (generation) where the starting point is conceptual in nature: a message, the (partial) definition of a word, a concept or a word related to the target word." ></td>
	<td class="line x" title="22:158	The language producer needs a dictionary allowing for reverse access." ></td>
	<td class="line x" title="23:158	A thesaurus does that, but only in a very limited way: the entry points are basically topical." ></td>
	<td class="line x" title="24:158	People use various methods to initiate search in their mind : words, concepts, partial descriptions, etc. If we want to mimic these functionalities by a computer, we must build the resource accordingly." ></td>
	<td class="line x" title="25:158	Let us assume that the text producer is looking for a word that he cannot access." ></td>
	<td class="line x" title="26:158	Instead he comes up with another word (or concept)1 somehow related to the former." ></td>
	<td class="line x" title="27:158	He may not know precisely how the two relate, but he knows that they are related." ></td>
	<td class="line x" title="28:158	He may also know to some extent how close their relationship is, whether a given link is relevant or not, that is, whether it can lead directly (synonym, 1We will comment below on the difference between concepts and words." ></td>
	<td class="line x" title="29:158	antonym, hyperonym) or indirectly to the target word." ></td>
	<td class="line x" title="30:158	Since the relationship between the sourceand the target word is often indirect, several lookups may be necessary: each one of them having the potential to contain either the target word (direct lookup), or a word leading towards it (indirect lookup)." ></td>
	<td class="line x" title="31:158	2 How reasonable is it to expect perfect input?" ></td>
	<td class="line x" title="32:158	The expectation of perfect input is unrealistic even in analysis,2 but clearly more so in generation." ></td>
	<td class="line x" title="33:158	The user may well be unable to provide the required information: be it because he cannot access in time the word he is looking for, even though he knows it,3 or because he does not know the word yet expressing the idea he wants to convey." ></td>
	<td class="line x" title="34:158	This latter case typically occurs when using a foreign language or when trying to use a very technical term." ></td>
	<td class="line x" title="35:158	Yet, not being able to find a word, does not imply that one does not know anything concerning the word." ></td>
	<td class="line x" title="36:158	Actually, quite often the contrary is the case." ></td>
	<td class="line x" title="37:158	Suppose, you were looking for a word expressing the following ideas: domesticated animal, producing milk suitable for making cheese." ></td>
	<td class="line x" title="38:158	Suppose further that you knew that the target word was neither cow nor sheep." ></td>
	<td class="line x" title="39:158	While none of this information is sufficient to guarantee the access of the intended word goat, the information at hand (part of the definition) could certainly be used." ></td>
	<td class="line x" title="40:158	For some concrete proposals going in this direction, see (Bilac et al. , 2004), or the OneLook reverse dictionary.4 Besides the definition information, people often have other kind of knowledge concerning the target word." ></td>
	<td class="line x" title="41:158	In particular, they know how the latter relates to other words." ></td>
	<td class="line x" title="42:158	For example, they know that goats and sheep are somehow connected, that both of them are animals, that sheep are appreciated for their wool and meet, that sheep tend to follow each other blindly, while goats manage to survive, while hardly eating anything, etc. In sum, people have in their mind lexical networks: all words, concepts or ideas they express are highly interconnected." ></td>
	<td class="line x" title="43:158	As a result, any one of the words or concepts has the potential to evoke each other." ></td>
	<td class="line x" title="44:158	The likelihood for 2Obviously, looking for pseudonym under the letter S in a dictionary wont be of great help." ></td>
	<td class="line x" title="45:158	3Temporary amnesia, known as the TOT, or tip-ofthe-tongue problem (Brown and McNeill, 1996; Zock and Fournier, 2001; Zock, 2002) 4http://www.onelook.com/reverse-dictionary." ></td>
	<td class="line x" title="46:158	shtml this to happen depends, among other things, on such factors as frequency (associative strength), saliency and distance (direct vs. indirect access)." ></td>
	<td class="line x" title="47:158	As one can see, associations are a very general and powerful mechanism." ></td>
	<td class="line x" title="48:158	No matter what we hear, read or say, any idea is likely to remind us of something else.5 This being so, we should make use of it.6 3 Search based on the relations between concepts and words If one agrees with what we have just said, one could view the mental dictionary as a huge semantic network composed of nodes (words and concepts) and links (associations), with either being able to activate the other.7 Finding a 5The idea according to which the mental dictionary (or encyclopedia) is basically an associative network, composed of nodes (words or concepts) and links (associations) is not new, neither is the idea of spreading activation." ></td>
	<td class="line x" title="49:158	Actually the very notion of association goes back at least to Aristotle (350BC), but it is also inherent in work done by philosophers (Locke, Hume), physiologists (James & Stuart Mills), psychologists (Galton, 1880; Freud, 1901; Jung and Riklin, 1906) and psycholinguists (Deese, 1965; Jenkins, 1970; Schvaneveldt, 1989)." ></td>
	<td class="line x" title="50:158	For surveys in psycholinguistics see (Hormann, 1972), or more recent work (Spitzer, 1999)." ></td>
	<td class="line x" title="51:158	The notion of association is also implicit in work on semantic networks (Quillian, 1968), hypertext (Bush, 1945), the web (Nelson, 1967), connectionism (Dell et al. , 1999) and, of course, in WordNet (Miller et al. , 1993; Fellbaum, 1998)." ></td>
	<td class="line x" title="52:158	6In the preceding sections we used several times the terms words and concepts interchangeably, as if they were the same." ></td>
	<td class="line x" title="53:158	Of course, they are very different." ></td>
	<td class="line x" title="54:158	Yet, not knowing what a concept looks like (a single node, or every node, i.e. headword of the words definition?), we think it is safer to assume that the user can communicate with the computer (dictionary) only via words." ></td>
	<td class="line x" title="55:158	Hence, concepts are represented by words, yet, since the two are connected, one can be accessed via the other, which addresses the interface problem with the computer." ></td>
	<td class="line x" title="56:158	Another point worth mentionning is the fact that associations may depend on the nature of the arguments (words vs. concepts)." ></td>
	<td class="line x" title="57:158	While in theory anything can be associated with anything (words with words, words with concepts, concepts with concepts, etc.), in practice words tend to trigger a different set of associations than concepts." ></td>
	<td class="line x" title="58:158	Also, the connectivity between words and concepts explains to some extent the power and the flexibility of the human mind." ></td>
	<td class="line x" title="59:158	Words are shorthand labels for concepts, and given the fact that the two are linked, one can make big leaps in no time and easily move from one plane (lets say the conceptual level) to the other (the linguistic counterpart)." ></td>
	<td class="line x" title="60:158	Words can be reached via concepts, but the latter can also serve as starting point to find a word." ></td>
	<td class="line x" title="61:158	Compared to the links between concepts which are a superhighway, associations between words are more like countryroads." ></td>
	<td class="line x" title="62:158	7Actually, one could question the very notion of mental dictionary which is convenient, but misleading in as it supposes a dedicated part for this task in our brain." ></td>
	<td class="line x" title="63:158	A Figure 1: Search based on propagation in a network (internal representation) word amounts thus to entering the network and following the links leading from the source node (the first word that comes to your mind) to the target word (the one you are looking for)." ></td>
	<td class="line x" title="64:158	Suppose you wanted to find the word nurse (target word), yet the only token coming to your mind were hospital." ></td>
	<td class="line x" title="65:158	In this case the system would generate internally a graph with the source word at the center and all the associated words at the periphery." ></td>
	<td class="line x" title="66:158	Put differently, the system would build internally a semantic network with hospital in the center and all its associated words as satellites (figure 1).8 Obviously, the greater the number of associations, the more complex the graph." ></td>
	<td class="line x" title="67:158	Given the diversity of situations in which a given object may occur we are likely to build many associations." ></td>
	<td class="line x" title="68:158	In other words, lexical graphs tend to bemultiply indexed mental encyclopedia, composed of polymorph information (concepts, words, meta-linguistic information) seems much more plausible to us." ></td>
	<td class="line x" title="69:158	8AKO: a kind of; ISA: subtype; TIORA: typically involved object, relation or actor." ></td>
	<td class="line x" title="70:158	come complex, too complex to be a good representation to support navigation." ></td>
	<td class="line x" title="71:158	Readability is hampered by at least two factors: high connectivity (the great number of links or associations emanating from each word), and distribution: conceptually related nodes, that is, nodes activated by the same kind of assocation are scattered around, that is, they do not necessarily occur next to each other, which is quite confusing for the user." ></td>
	<td class="line x" title="72:158	In order to solve this problem we suggest to display by category (chunks) all the words linked by the same kind of association to the source word (see figure 2)." ></td>
	<td class="line x" title="73:158	Hence, rather than displaying all the connected words as a flat list, we suggest to present them in chunks to allow for categorial search." ></td>
	<td class="line x" title="74:158	Having chosen a category, the user will be presented a list of words or categories from which he must choose." ></td>
	<td class="line x" title="75:158	If the target word is in the category chosen by the user (suppose he looked for a hyperonyme, hence he checked the ISA-bag), search stops, otherwise it goes on." ></td>
	<td class="line x" title="76:158	The user could choose either another category (eg." ></td>
	<td class="line x" title="77:158	AKO or TIORA), or a word in Figure 2: Proposed candidates, grouped according to the nature of the link the current list, which would then become the new starting point." ></td>
	<td class="line x" title="78:158	4 A resource still to be built The fact that the links are labeled has some very important consequences." ></td>
	<td class="line x" title="79:158	(a) While maintaining the power of a highly connected graph (possible cyclic navigation), it has at the interface level the simplicity of a tree: each node points only to data of the same type, i.e. same kind of association." ></td>
	<td class="line x" title="80:158	(b) Words being presented in clusters, navigation can be accomplished by clicking on the appropriate category." ></td>
	<td class="line x" title="81:158	The assumption being that the user generally knows to which category the target word belongs (or at least, he can recognize within which of the listed categories it falls), and that categorical search is in principle faster than search in a huge list of unordered (or, alphabetically ordered) words." ></td>
	<td class="line x" title="82:158	Word access, as described here, amounts to navigating in a huge associative network." ></td>
	<td class="line x" title="83:158	Of course, such a network has to be built." ></td>
	<td class="line x" title="84:158	The question is how." ></td>
	<td class="line x" title="85:158	Our proposal is to build it automatically by parsing an existing corpus containing sufficient amount of information on world knowledge (for example, an encyclopedia)." ></td>
	<td class="line x" title="86:158	This would yield a set of associations (see below),9 which still need to be labeled." ></td>
	<td class="line x" title="87:158	A rich ontology should be helpful in determining the adequate label for many, if not most of the links." ></td>
	<td class="line x" title="88:158	Unlike private information,10 which by 9The assumption being that every word co-occurring with another word in the same sentence is a candidate of an association." ></td>
	<td class="line x" title="89:158	The more frequently two words co-occur in a given corpus, the greater their associative strength." ></td>
	<td class="line x" title="90:158	10For example, the word elephant may remind you of a definition cannot and should not be put into a public dictionary,11 encyclopedic knowledge can be added in terms of associations, as this information expresses commonly shared knowledge, that is, the kind of associations most people have when encountering a given word." ></td>
	<td class="line x" title="91:158	Take for example the word elephant." ></td>
	<td class="line x" title="92:158	An electronic dictionary like Word Net associates the following gloss with the headword: large, gray, four-legged mammal, while Webster gives the following information: A mammal of the order Proboscidia, of which two living species, Elephas Indicus and E. Africanus, and several fossil species, are known." ></td>
	<td class="line x" title="93:158	They have a proboscis or trunk, and two large ivory tusks proceeding from the extremity of the upper jaw, and curving upwards." ></td>
	<td class="line x" title="94:158	The molar teeth are large and have transverse folds." ></td>
	<td class="line x" title="95:158	Elephants are the largest land animals now existing." ></td>
	<td class="line x" title="96:158	While this latter entry is already quite rich (trunk, ivory tusk, size), an encyclopedia contains even more information.12 If all this information were added to an electronic resource, it would enable us to access the same word (e.g. elephant) via many more associations than ever before." ></td>
	<td class="line x" title="97:158	By looking at the definition here above, one will notice that many associations are quite straightforward (color, size, origin, etc.), and since most of them appear frequently in a pattern-like manner it should be possible to extract them automatically (see footnote 18 below)." ></td>
	<td class="line x" title="98:158	If one agrees with these views, the remaining question is how to extract this encyclopedic information and to add it to an existing electronic resource." ></td>
	<td class="line x" title="99:158	Below we will outline some methods for extracting associated words and discuss the feasibility of using current methodology to achieve this goal." ></td>
	<td class="line x" title="100:158	5 Automatic extraction of word associations Above we outlined the need for obtaining associations between words and using them to improve dictionary accessibility." ></td>
	<td class="line x" title="101:158	While the associations can be obtained through association experiments with human subjects, this strategy is specific animal, trip or location (zoo, country in Africa)." ></td>
	<td class="line x" title="102:158	11This does not (and should not) preclude the possibility to add it to ones personal dictionary." ></td>
	<td class="line x" title="103:158	12You may consider taking a look at Wikipedia (http: //en.wikipedia.org/wiki/) which is free." ></td>
	<td class="line x" title="104:158	not very satisfying due to the high cost of running the experiments (time and money), and due to its static nature." ></td>
	<td class="line x" title="105:158	Indeed, given the costs, it is impossible to repeat these experiments to take into account the evolution of a society." ></td>
	<td class="line x" title="106:158	Hence, the goal is to automatically extract associations from large corpora." ></td>
	<td class="line x" title="107:158	This problem was addressed by a large number of researchers, but in most cases it was reduced to extraction of collocations which are a proper subset of the set of associated words." ></td>
	<td class="line x" title="108:158	While hard to define, collocations appear often enough in corpora to be extractable by statistical and information-theory based methods." ></td>
	<td class="line oc" title="109:158	There are several basic methods for evaluating associations between words: based on frequency counts (Choueka, 1988; Wettler and Rapp, 1993), information theoretic (Church and Hanks, 1990) and statistical significance (Smadja, 1993)." ></td>
	<td class="line x" title="110:158	The statistical significance often evaluate whether two words are independant using hypothesis tests such as t-score (Church et al. , 1991), the X2, the log-likelihood (Dunning, 1993) and Fishers exact test (Pedersen, 1996)." ></td>
	<td class="line x" title="111:158	Extracted sets for associated words are further pruned using numerical methods, or linguistic knowledge to obtain a subset of collocations." ></td>
	<td class="line x" title="112:158	The various extraction measures have been discussed in great detail in the literature (Manning and Schutze, 1999; McKeown and Radev, 2000), their performance has been compared (Dunning, 1993; Pedersen, 1996; Evert and Krenn, 2001), and the methods have been combined to improve overall performance (Inkpen and Hirst, 2002)." ></td>
	<td class="line x" title="113:158	Most of these methods were originally applied in large text corpora, but more recently the web has been used as a corpus (Pearce, 2001; Inkpen and Hirst, 2002)." ></td>
	<td class="line x" title="114:158	Collocation extraction methods have been used not only for English, but for many other languages: French (Ferret, 2002), German (Evert and Krenn, 2001) and Japanese (Nagao and Mori, 1994), to cite but those." ></td>
	<td class="line x" title="115:158	The most obvious question in this context is to clarify to what extent available collocation extraction techniques fulfill our needs of extracting and labeling word associations." ></td>
	<td class="line x" title="116:158	Since collocations are a subset of association, it is possible to apply collocation extraction techniques to obtain related words, ordered in terms of the relative strength of association." ></td>
	<td class="line x" title="117:158	The result of this kind of numerical extraction would be a large set of numerically weighted word pairs." ></td>
	<td class="line x" title="118:158	The problem with this approach is that the links are only labeled in terms of their relative associative strength, but not categorically, which makes it impossible to group and present them in a meaningful way for the dictionary user." ></td>
	<td class="line x" title="119:158	Clusters based only on the notion of association strength are inadequate for the kind of navigation described here above." ></td>
	<td class="line x" title="120:158	Hence another step is necessary: qualification of the links according to their types." ></td>
	<td class="line x" title="121:158	Only once this is done, a human being could use it to navigate through a large conceptual-lexical network (the dictionary) as described above." ></td>
	<td class="line x" title="122:158	Unfortunately, research on automatic link identification has been rather sparse." ></td>
	<td class="line x" title="123:158	Most attempts have been devoted to the extraction of certain types of links (usually syntactic type (Lin, 1998) or on extensions of WordNet with topical information contained in a thesaurus (Stevenson, 2002) or on the WWW (Agirre et al. , 2000)." ></td>
	<td class="line x" title="124:158	Additional methods need to be considered in order to reveal (automatically) the kind of associations holding between words and/or concepts." ></td>
	<td class="line x" title="125:158	Earlier in this paper we have suggested the use of an encyclopedia as a source of general world knowledge." ></td>
	<td class="line x" title="126:158	It should be noted, though, that there are important differences between large corpora and encyclopedias." ></td>
	<td class="line x" title="127:158	Large corpora usually contain a lot of repetitive texts on a limited number of topics (e.g. newspaper articles) which makes them very suitable for statistical methods." ></td>
	<td class="line x" title="128:158	On the other hand, while being maximally informative and comprehensive, encyclopedias are written in a highly controlled language, and their content is continually updated and re-edited, with the goal to avoid unnecessary repetition." ></td>
	<td class="line x" title="129:158	While most of the information contained in an entry is important, there is a lack of redundancy." ></td>
	<td class="line x" title="130:158	Hence, measures capable of handling word pairs with low appearance counts (e.g. log-likelihood or Fishers exact test) should be favored." ></td>
	<td class="line x" title="131:158	Also, rather than looking at individual words, one might want to look at word patterns instead." ></td>
	<td class="line x" title="132:158	6 Discussion and Conclusion We have raised and partially answered the question of how a dictionary should be indexed in order to support word access." ></td>
	<td class="line x" title="133:158	We were particularly concerned with the language producer, as his needs (and knowledge at the onset) are quite different from the ones of the language receiver (listener/reader)." ></td>
	<td class="line x" title="134:158	It seems that, in order to achieve our goal, we need to do two things: add to an existing electronic dictionary information that people tend to associate with a word, that is, build and enrich a semantic network, and provide a tool to navigate in it." ></td>
	<td class="line x" title="135:158	To this end we have suggested to label the links, as this would reduce the graph complexity and allow for type-based navigation." ></td>
	<td class="line x" title="136:158	Actually our basic proposal is to extend a resource like WordNet by adding certain links, in particular on the horizontal axis (syntagmatic relations)." ></td>
	<td class="line x" title="137:158	These links are associations, and their role consists in helping the encoder to find ideas (concepts/words) related to a given stimulus (brainstorming), or to find the word he is thinking of (word access)." ></td>
	<td class="line x" title="138:158	One problem that we are confronted with is to identify possible associations." ></td>
	<td class="line x" title="139:158	Ideally we would need a complete list, but unfortunately, this does not exist." ></td>
	<td class="line x" title="140:158	Yet, there is a lot of highly relevant information out there." ></td>
	<td class="line x" title="141:158	For example, Melcuks lexical functions (Melcuk, 1992), Fillmores FRAMENET13, work on ontologies (CYC), thesaurus (Roget), WordNets (the original version from Princeton, divers EuroWordNets, BalkaNet), HowNet14, the work done by MICRA, the FACTOTUM project15 or the Wordsmyth dictionary/thesaurus combination16." ></td>
	<td class="line x" title="142:158	Of course, one would need to make choices here and probably add links." ></td>
	<td class="line x" title="143:158	Another problem is to identify useful associations." ></td>
	<td class="line x" title="144:158	Not every possible association is necessarily plausible." ></td>
	<td class="line x" title="145:158	Hence, the idea to take as corpus something that expresses shared knowledge, for example, an encyclopedia." ></td>
	<td class="line x" title="146:158	The associations it contains can be considered as being plausible." ></td>
	<td class="line x" title="147:158	We could also collect data by watching people using a dictionary and identify search patterns.17 Next, we could run psycholinguistic experiments.18 While the typical paradigm has been to ask people to produce a response (red) to some stimulus (rose), we could ask them to identify or label the links between words (e.g. apple-fruit, lemon-yellow, etc.)." ></td>
	<td class="line x" title="148:158	The ease of la13http://www.icsi.berkeley.edu/~framenet/ 14http://www.keenage.com/html/e_index.html 15http://humanities.uchicago.edu/homes/MICRA/ 16http://www.wordsmyth.com/ 17One such pattern could be: give me the word for a bird with yellow feet and a long beak, that can swim." ></td>
	<td class="line x" title="149:158	Actually, word access problems frequently come under the form of questions like: What is the word for X that Y?, where X is usually a hypernym and Y a stereotypical, possibly partial functional/relational/case description of the target word." ></td>
	<td class="line x" title="150:158	18Actually, this has been done for decades, but with a different goal in mind (Nelson, 1967), http://cyber." ></td>
	<td class="line x" title="151:158	acomp.usf.edu/FreeAssociation/." ></td>
	<td class="line x" title="152:158	beling will probably depend upon the origin of the words (the person asked to label the link or somebody else)." ></td>
	<td class="line x" title="153:158	Another approach would be to extract collocations from a corpus and label them automatically." ></td>
	<td class="line x" title="154:158	There are tools for extracting cooccurrences (see section 5.5), and ontologies could be used to qualify some of the links between collocational elements." ></td>
	<td class="line x" title="155:158	While this approach might work fine for couples like coffeestrong, or wine-red (since an ontology would reveal that red is a kind of color, which is precisely the link type: i.e. association), one may doubt that it could reveal the nature of the link between smoke and fire." ></td>
	<td class="line x" title="156:158	Yet, most humans would immediately recognize this as a causal link." ></td>
	<td class="line x" title="157:158	As one can see, there are still quite a few serious problems to be solved." ></td>
	<td class="line x" title="158:158	Nevertheless, we do believe that these obstacles can be removed, and that the approach presented here has the potential to improve word access, making the whole process more powerful, natural and intuitive, hence efficient." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P05-1075
A Nonparametric Method For Extraction Of Candidate Phrasal Terms
Deane, Paul;"></td>
	<td class="line x" title="1:157	Proceedings of the 43rd Annual Meeting of the ACL, pages 605613, Ann Arbor, June 2005." ></td>
	<td class="line x" title="2:157	c2005 Association for Computational Linguistics A Nonparametric Method for Extraction of Candidate Phrasal Terms Paul Deane Center for Assessment, Design and Scoring Educational Testing Service pdeane@ets.org Abstract This paper introduces a new method for identifying candidate phrasal terms (also known as multiword units) which applies a nonparametric, rank-based heuristic measure." ></td>
	<td class="line x" title="3:157	Evaluation of this measure, the mutual rank ratio metric, shows that it produces better results than standard statistical measures when applied to this task." ></td>
	<td class="line x" title="4:157	1 Introduction The ordinary vocabulary of a language like English contains thousands of phrasal terms -multiword lexical units including compound nouns, technical terms, idioms, and fixed collocations." ></td>
	<td class="line x" title="5:157	The exact number of phrasal terms is difficult to determine, as new ones are coined regularly, and it is sometimes difficult to determine whether a phrase is a fixed term or a regular, compositional expression." ></td>
	<td class="line x" title="6:157	Accurate identification of phrasal terms is important in a variety of contexts, including natural language parsing, question answering systems, information retrieval systems, among others." ></td>
	<td class="line x" title="7:157	Insofar as phrasal terms function as lexical units, their component words tend to cooccur more often, to resist substitution or paraphrase, to follow fixed syntactic patterns, and to display some degree of semantic noncompositionality (Manning, 1999:183-186)." ></td>
	<td class="line x" title="8:157	However, none of these characteristics are amenable to a simple algorithmic interpretation." ></td>
	<td class="line oc" title="9:157	It is true that various term extraction systems have been developed, such as Xtract (Smadja 1993), Termight (Dagan & Church 1994), and TERMS (Justeson & Katz 1995) among others (cf.Daille 1996, Jacquemin & Tzoukermann 1994, Jacquemin, Klavans, & Toukermann 1997, Boguraev & Kennedy 1999, Lin 2001)." ></td>
	<td class="line o" title="11:157	Such systems typically rely on a combination of linguistic knowledge and statistical association measures." ></td>
	<td class="line x" title="12:157	Grammatical patterns, such as adjective-noun or noun-noun sequences are selected then ranked statistically, and the resulting ranked list is either used directly or submitted for manual filtering." ></td>
	<td class="line o" title="13:157	The linguistic filters used in typical term extraction systems have no obvious connection with the criteria that linguists would argue define a phrasal term (noncompositionality, fixed order, nonsubstitutability, etc.)." ></td>
	<td class="line o" title="14:157	They function, instead, to reduce the number of a priori improbable terms and thus improve precision." ></td>
	<td class="line n" title="15:157	The association measure does the actual work of distinguishing between terms and plausible nonterms." ></td>
	<td class="line x" title="16:157	A variety of methods have been applied, ranging from simple frequency (Justeson & Katz 1995), modified frequency measures such as c-values (Frantzi, Anadiou & Mima 2000, Maynard & Anadiou 2000) and standard statistical significance tests such as the t-test, the chi-squared test, and loglikelihood (Church and Hanks 1990, Dunning 1993), and information-based methods, e.g. pointwise mutual information (Church & Hanks 1990)." ></td>
	<td class="line x" title="17:157	Several studies of the performance of lexical association metrics suggest significant room for improvement, but also variability among tasks." ></td>
	<td class="line x" title="18:157	One series of studies (Krenn 1998, 2000; Evert & Krenn 2001, Krenn & Evert 2001; also see Evert 2004) focused on the use of association metrics to identify the best candidates in particular grammatical constructions, such as adjective-noun pairs or verb plus prepositional phrase constructions, and compared the performance of simple frequency to several common measures (the log-likelihood, the t-test, the chi-squared test, the dice coefficient, relative entropy and mutual information)." ></td>
	<td class="line x" title="19:157	In Krenn & Evert 2001, frequency outperformed mutual information though not the ttest, while in Evert and Krenn 2001, log-likelihood and the t-test gave the best results, and mutual information again performed worse than frequency." ></td>
	<td class="line x" title="20:157	However, in all these studies performance was generally low, with precision falling rapidly after the very highest ranked phrases in the list." ></td>
	<td class="line x" title="21:157	By contrast, Schone and Jurafsky (2001) evaluate the identification of phrasal terms without grammatical filtering on a 6.7 million word extract from the TREC databases, applying both WordNet and online dictionaries as gold standards." ></td>
	<td class="line x" title="22:157	Once again, the general level of performance was low, with precision falling off rapidly as larger portions 605 of the n-best list were included, but they report better performance with statistical and information theoretic measures (including mutual information) than with frequency." ></td>
	<td class="line x" title="23:157	The overall pattern appears to be one where lexical association measures in general have very low precision and recall on unfiltered data, but perform far better when combined with other features which select linguistic patterns likely to function as phrasal terms." ></td>
	<td class="line x" title="24:157	The relatively low precision of lexical association measures on unfiltered data no doubt has multiple explanations, but a logical candidate is the failure or inappropriacy of underlying statistical assumptions." ></td>
	<td class="line x" title="25:157	For instance, many of the tests assume a normal distribution, despite the highly skewed nature of natural language frequency distributions, though this is not the most important consideration except at very low n (cf.Moore 2004, Evert 2004, ch." ></td>
	<td class="line x" title="27:157	4)." ></td>
	<td class="line x" title="28:157	More importantly, statistical and information-based metrics such as the log-likelihood and mutual information measure significance or informativeness relative to the assumption that the selection of component terms is statistically independent." ></td>
	<td class="line x" title="29:157	But of course the possibilities for combinations of words are anything but random and independent." ></td>
	<td class="line x" title="30:157	Use of linguistic filters such as 'attributive adjective followed by noun' or 'verb plus modifying prepositional phrase' arguably has the effect of selecting a subset of the language for which the standard null hypothesis -that any word may freely be combined with any other word -may be much more accurate." ></td>
	<td class="line x" title="31:157	Additionally, many of the association measures are defined only for bigrams, and do not generalize well to phrasal terms of varying length." ></td>
	<td class="line x" title="32:157	The purpose of this paper is to explore whether the identification of candidate phrasal terms can be improved by adopting a heuristic which seeks to take certain of these statistical issues into account." ></td>
	<td class="line x" title="33:157	The method to be presented here, the mutual rank ratio, is a nonparametric rank-based approach which appears to perform significantly better than the standard association metrics." ></td>
	<td class="line x" title="34:157	The body of the paper is organized as follows: Section 2 will introduce the statistical considerations which provide a rationale for the mutual rank ratio heuristic and outline how it is calculated." ></td>
	<td class="line x" title="35:157	Section 3 will present the data sources and evaluation methodologies applied in the rest of the paper." ></td>
	<td class="line x" title="36:157	Section 4 will evaluate the mutual rank ratio statistic and several other lexical association measures on a larger corpus than has been used in previous evaluations." ></td>
	<td class="line x" title="37:157	As will be shown below, the mutual rank ratio statistic recognizes phrasal terms more effectively than standard statistical measures." ></td>
	<td class="line x" title="38:157	2 Statistical considerations 2.1 Highly skewed distributions As first observed e.g. by Zipf (1935, 1949) the frequency of words and other linguistic units tend to follow highly skewed distributions in which there are a large number of rare events." ></td>
	<td class="line x" title="39:157	Zipf's formulation of this relationship for single word frequency distributions (Zipf's first law) postulates that the frequency of a word is inversely proportional to its rank in the frequency distribution, or more generally if we rank words by frequency and assign rank z, where the function fz(z,N) gives the frequency of rank z for a sample of size N, Zipf's first law states that: fz(z,N) = Cz where C is a normalizing constant and is a free parameter that determines the exact degree of skew; typically with single word frequency data, approximates 1 (Baayen 2001: 14)." ></td>
	<td class="line x" title="40:157	Ideally, an association metric would be designed to maximize its statistical validity with respect to the distribution which underlies natural language text -which is if not a pure Zipfian distribution at least an LNRE (large number of rare events, cf.Baayen 2001) distribution with a very long tail, containing events which differ in probability by many orders of magnitude." ></td>
	<td class="line x" title="42:157	Unfortunately, research on LNRE distributions focuses primarily on unigram distributions, and generalizations to bigram and ngram distributions on large corpora are not as yet clearly feasible (Baayen 2001:221)." ></td>
	<td class="line x" title="43:157	Yet many of the best-performing lexical association measures, such as the t-test, assume normal distributions, (cf.Dunning 1993) or else (as with mutual information) eschew significance testing in favor of a generic information-theoretic approach." ></td>
	<td class="line x" title="45:157	Various strategies could be adopted in this situation: finding a better model of the distribution,or adopting a nonparametric method." ></td>
	<td class="line x" title="46:157	2.2 The independence assumption Even more importantly, many of the standard lexical association measures measure significance (or information content) against the default assumption that word-choices are statistically independent events." ></td>
	<td class="line x" title="47:157	This assumption is built into the highest-performing measures as observed in Evert & Krenn 2001, Krenn & Evert 2001 and Schone & Jurafsky 2001." ></td>
	<td class="line x" title="48:157	This is of course untrue, and justifiable only as a simplifying idealization in the absence of a better model." ></td>
	<td class="line x" title="49:157	The actual probability of any sequence of words is strongly influenced by the base grammatical and semantic structure of language, particularly since phrasal terms usually conform to 606 the normal rules of linguistic structure." ></td>
	<td class="line x" title="50:157	What makes a compound noun, or a verb-particle construction, into a phrasal term is not deviation from the base grammatical pattern for noun-noun or verb-particle structures, but rather a further pattern (of meaning and usage and thus heightened frequency) superimposed on the normal linguistic base." ></td>
	<td class="line x" title="51:157	There are, of course, entirely aberrant phrasal terms, but they constitute the exception rather than the rule." ></td>
	<td class="line x" title="52:157	This state of affairs poses something of a chicken-and-the-egg problem, in that statistical parsing models have to estimate probabilities from the same base data as the lexical association measures, so the usual heuristic solution as noted above is to impose a linguistic filter on the data, with the association measures being applied only to the subset thus selected." ></td>
	<td class="line x" title="53:157	The result is in effect a constrained statistical model in which the independence assumption is much more accurate." ></td>
	<td class="line x" title="54:157	For instance, if the universe of statistical possibilities is restricted to the set of sequences in which an adjective is followed by a noun, the null hypothesis that word choice is independent -i.e., that any adjective may precede any noun -is a reasonable idealization." ></td>
	<td class="line x" title="55:157	Without filtering, the independence assumption yields the much less plausible null hypothesis that any word may appear in any order." ></td>
	<td class="line x" title="56:157	It is thus worth considering whether there are any ways to bring additional information to bear on the problem of recognizing phrasal terms without presupposing statistical independence." ></td>
	<td class="line x" title="57:157	2.3 Variable length; alternative/overlapping phrases Phrasal terms vary in length." ></td>
	<td class="line x" title="58:157	Typically they range from about two to six words in length, but critically we cannot judge whether a phrase is lexical without considering both shorter and longer sequences." ></td>
	<td class="line x" title="59:157	That is, the statistical comparison that needs to be made must apply in principle to the entire set of word sequences that must be distinguished from phrasal terms, including longer sequences, subsequences, and overlapping sequences, despite the fact that these are not statistically independent events." ></td>
	<td class="line x" title="60:157	Of the association metrics mentioned thus far, only the C-Value method attempts to take direct notice of such word sequence information, and then only as a modification to the basic information provided by frequency." ></td>
	<td class="line x" title="61:157	Any solution to the problem of variable length must enable normalization allowing direct comparison of phrases of different length." ></td>
	<td class="line x" title="62:157	Ideally, the solution would also address the other issues -the independence assumption and the skewed distributions typical of natural language data." ></td>
	<td class="line x" title="63:157	2.4 Mutual expectation An interesting proposal which seeks to overcome the variable-length issue is the mutual expectation metric presented in Dias, Guillor, and Lopes (1999) and implemented in the SENTA system (Gil and Dias 2003a)." ></td>
	<td class="line x" title="64:157	In their approach, the frequency of a phrase is normalized by taking into account the relative probability of each word compared to the phrase." ></td>
	<td class="line x" title="65:157	Dias, Guillor, and Lopes take as the foundation of their approach the idea that the cohesiveness of a text unit can be measured by measuring how strongly it resists the loss of any component term." ></td>
	<td class="line x" title="66:157	This is implemented by considering, for any ngram, the set of [continuous or discontinuous] (n-1)-grams which can be formed by deleting one word from the n-gram." ></td>
	<td class="line x" title="67:157	A normalized expectation for the n-gram is then calculated as follows: 1 2 1 2 ([, ]) ([,  ]) n n p w w w FPE w w w where [w1, w2  wn] is the phrase being evaluated and FPE([w1, w2  wn]) is: 1 2 1 1 ^1 ([,  ]) [   ] n n i n i p w w w p w w wn = + where wi is the term omitted from the n-gram." ></td>
	<td class="line x" title="68:157	They then calculate mutual expectation as the product of the probability of the n-gram and its normalized expectation." ></td>
	<td class="line x" title="69:157	This statistic is of interest for two reasons: first, it provides a single statistic that can be applied to n-grams of any length; second, it is not based upon the independence assumption." ></td>
	<td class="line x" title="70:157	The core statistic, normalized expectation, is essentially frequency with a penalty if a phrase contains component parts significantly more frequent than the phrase itself." ></td>
	<td class="line x" title="71:157	It is of course an empirical question how well mutual expectation performs (and we shall examine this below) but mutual expectation is not in any sense a significance test." ></td>
	<td class="line x" title="72:157	That is, if we are examining a phrase like the east end, the conditional probability of east given [__ end] or of end given [__ east] may be relatively low (since other words can appear in that context) and yet the phrase might still be very lexicalized if the association of both words with this context were significantly stronger than their association for 607 other phrases." ></td>
	<td class="line x" title="73:157	That is, to the extent that phrasal terms follow the regular patterns of the language, a phrase might have a relatively low conditional probability (given the wide range of alternative phrases following the same basic linguistic patterns) and thus have a low mutual expectation yet still occur far more often than one would expect from chance." ></td>
	<td class="line x" title="74:157	In short, the fundamental insight -assessing how tightly each word is bound to a phrase -is worth adopting." ></td>
	<td class="line x" title="75:157	There is, however, good reason to suspect that one could improve on this method by assessing relative statistical significance for each component word without making the independence assumption." ></td>
	<td class="line x" title="76:157	In the heuristic to be outlined below, a nonparametric method is proposed." ></td>
	<td class="line x" title="77:157	This method is novel: not a modification of mutual expectation, but a new technique based on ranks in a Zipfian frequency distribution." ></td>
	<td class="line x" title="78:157	2.5 Rank ratios and mutual rank ratios This technique can be justified as follows." ></td>
	<td class="line x" title="79:157	For each component word in the n-gram, we want to know whether the n-gram is more probable for that word than we would expect given its behavior with other words." ></td>
	<td class="line x" title="80:157	Since we do not know what the expected shape of this distribution is going to be, a nonparametric method using ranks is in order, and there is some reason to think that frequency rank regardless of n-gram size will be useful." ></td>
	<td class="line x" title="81:157	In particular, Ha, Sicilia-Garcia, Ming and Smith (2002) show that Zipf's law can be extended to the combined frequency distribution of n-grams of varying length up to rank 6, which entails that the relative rank of words in such a combined distribution provide a useful estimate of relative probability." ></td>
	<td class="line x" title="82:157	The availability of new techniques for handling large sets of n-gram data (e.g. Gil & Dias 2003b) make this a relatively feasible task." ></td>
	<td class="line x" title="83:157	Thus, given a phrase like east end, we can rank how often __ end appears with east in comparison to how often other phrases appear with east.That is, if {__ end, __side, the __, toward the __, etc.} is the set of (variable length) n-gram contexts associated with east (up to a length cutoff), then the actual rank of __ end is the rank we calculate by ordering all contexts by the frequency with which the actual word appears in the context." ></td>
	<td class="line x" title="84:157	We also rank the set of contexts associated with east by their overall corpus frequency." ></td>
	<td class="line x" title="85:157	The resulting ranking is the expected rank of __ end based upon how often the competing contexts appear regardless of which word fills the context." ></td>
	<td class="line x" title="86:157	The rank ratio (RR) for the word given the context can then be defined as: RR(word,context) = ( )( ),,ER word contextAR word context where ER is the expected rank and AR is the actual rank." ></td>
	<td class="line x" title="87:157	A normalized, or mutual rank ratio for the ngram can then be defined as 2 11, [__  ] 2, [ __  ], [ 1, 2 _]( )* ( )* ( )n nw w w w n w wn RR w RR w RR w The motivation for this method is that it attempts to address each of the major issues outlined above by providing a nonparametric metric which does not make the independence assumption and allows scores to be compared across n-grams of different lengths." ></td>
	<td class="line x" title="88:157	A few notes about the details of the method are in order." ></td>
	<td class="line x" title="89:157	Actual ranks are assigned by listing all the contexts associated with each word in the corpus, and then ranking contexts by word, assigning the most frequent context for word n the rank 1, next next most frequent rank 2, etc. Tied ranks are given the median value for the ranks occupied by the tie, e.g., if two contexts with the same frequency would occupy ranks 2 and 3, they are both assigned rank 2.5." ></td>
	<td class="line x" title="90:157	Expected ranks are calculated for the same set of contexts using the same algorithm, but substituting the unconditional frequency of the (n-1)-gram for the gram's frequency with the target word.1 3 Data sources and methodology The Lexile Corpus is a collection of documents covering a wide range of reading materials such as a child might encounter at school, more or less evenly divided by Lexile (reading level) rating to cover all levels of textual complexity from kindergarten to college." ></td>
	<td class="line x" title="91:157	It contains in excess of 400 million words of running text, and has been made available to the Educational Testing Service under a research license by Metametrics Corporation." ></td>
	<td class="line x" title="92:157	This corpus was tokenized using an in-house tokenization program, toksent, which treats most punctuation marks as separate tokens but makes single tokens out of common abbreviations, numbers like 1,500, and words like o'clock." ></td>
	<td class="line x" title="93:157	It should be noted that some of the association measures are known to perform poorly if punctuation marks and common stopwords are 1 In this study the rank-ratio method was tested for bigrams and trigrams only, due to the small number of WordNet gold standard items greater than two words in length." ></td>
	<td class="line x" title="94:157	Work in progress will assess the metrics' performance on n-grams of orders four through six." ></td>
	<td class="line x" title="95:157	608 included; therefore, n-gram sequences containing punctuation marks and the 160 most frequent word forms were excluded from the analysis so as not to bias the results against them." ></td>
	<td class="line x" title="96:157	Separate lists of bigrams and trigrams were extracted and ranked according to several standard word association metrics." ></td>
	<td class="line x" title="97:157	Rank ratios were calculated from a comparison set consisting of all contexts derived by this method from bigrams and trigrams, e.g., contexts of the form word1__, ___word2, ___word1 word2, word1 ___ word3, and word1 word2 ___.2 Table 1 lists the standard lexical association measures tested in section four3." ></td>
	<td class="line x" title="98:157	The logical evaluation method for phrasal term identification is to rank n-grams using each metric and then compare the results against a gold standard containing known phrasal terms." ></td>
	<td class="line x" title="99:157	Since Schone and Jurafsky (2001) demonstrated similar results whether WordNet or online dictionaries were used as a gold standard, WordNet was selected." ></td>
	<td class="line x" title="100:157	Two separate lists were derived containing twoand three-word phrases." ></td>
	<td class="line x" title="101:157	The choice of WordNet as a gold standard tests ability to predict general dictionary headwords rather than technical terms, appropriate since the source corpus consists of nontechnical text." ></td>
	<td class="line x" title="102:157	Following Schone & Jurafsky (2001), the bigram and trigram lists were ranked by each statistic then scored against the gold standard, with results evaluated using a figure of merit (FOM) roughly characterizable as the area under the precisionrecall curve." ></td>
	<td class="line x" title="103:157	The formula is: 1 1 k i i PK = where Pi (precision at i) equals i/Hi, and Hi is the number of n-grams into the ranked n-gram list required to find the ith correct phrasal term." ></td>
	<td class="line x" title="104:157	It should be noted, however, that one of the most pressing issues with respect to phrasal terms is that they display the same skewed, long-tail distribution as ordinary words, with a large 2 Excluding the 160 most frequent words prevented evaluation of a subset of phrasal terms such as verbal idioms like act up or go on." ></td>
	<td class="line x" title="105:157	Experiments with smaller corpora during preliminary work indicated that this exclusion did not appear to bias the results." ></td>
	<td class="line oc" title="106:157	3 Schone & Jurafsky's results indicate similar results for log-likelihood & T-score, and strong parallelism among information-theoretic measures such as ChiSquared, Selectional Association (Resnik 1996), Symmetric Conditional Probability (Ferreira and Pereira Lopes, 1999) and the Z-Score (Smadja 1993)." ></td>
	<td class="line x" title="107:157	Thus it was not judged necessary to replicate results for all methods covered in Schone & Jurafsky (2001)." ></td>
	<td class="line x" title="108:157	proportion of the total displaying very low frequencies." ></td>
	<td class="line x" title="109:157	This can be measured by considering Table 1." ></td>
	<td class="line x" title="110:157	Some Lexical Association Measures the overlap between WordNet and the Lexile corpus." ></td>
	<td class="line x" title="111:157	A list of 53,764 two-word phrases were extracted from WordNet, and 7,613 three-word phrases." ></td>
	<td class="line x" title="112:157	Even though the Lexile corpus is quite large -in excess of 400 million words of running text -only 19,939 of the two-word phrases and 4 Due to the computational cost of calculating CValues over a very large corpus, C-Values were calculated over bigrams and trigrams only." ></td>
	<td class="line x" title="113:157	More sophisticated versions of the C-Value method such as NC-values were not included as these incorporate linguistic knowledge and thus fall outside the scope of the study." ></td>
	<td class="line x" title="114:157	METRIC FORMULA Frequency (Guiliano, 1964) x yf Pointwise Mutual Information [PMI] (Church & Hanks, 1990) ( )xy x y2log /P P P True Mutual Information [TMI] (Manning, 1999) ( )xy 2 xy x ylog /P P P P Chi-Squared ( 2 ) (Church and Gale, 1991) { }{ },, 2( ) i X X Y Y i j i j i j j f     T-Score (Church & Hanks, 1990) 1 2 2 2 1 2 1 2 x x s s n n  + C-Values4 (Frantzi, Anadiou & Mima 2000) 2 is not nested 2 log ( ) log ( ) 1 ( ) ( ) a a b T a f f f b P T         where is the candidate string f( ) is its frequency in the corpus T is the set of candidate terms that contain P(T ) is the number of these candidate terms 609 1,700 of the three-word phrases are attested in the Lexile corpus." ></td>
	<td class="line x" title="115:157	14,045 of the 19,939 attested twoword phrases occur at least 5 times, 11,384 occur at least 10 times, and only 5,366 occur at least 50 times; in short, the strategy of cutting off the data at a threshold sacrifices a large percent of total recall." ></td>
	<td class="line x" title="116:157	Thus one of the issues that needs to be addressed is the accuracy with which lexical association measures can be extended to deal with relatively sparse data, e.g., phrases that appear less than ten times in the source corpus." ></td>
	<td class="line x" title="117:157	A second question of interest is the effect of filtering for particular linguistic patterns." ></td>
	<td class="line x" title="118:157	This is another method of prescreening the source data which can improve precision but damage recall." ></td>
	<td class="line x" title="119:157	In the evaluation bigrams were classified as N-N and A-N sequences using a dictionary template, with the expected effect." ></td>
	<td class="line x" title="120:157	For instance, if the WordNet two word phrase list is limited only to those which could be interpreted as noun-noun or adjective noun sequences, N>=5, the total set of WordNet terms that can be retrieved is reduced to 9,757 4 Evaluation Schone and Jurafsky's (2001) study examined the performance of various association metrics on a corpus of 6.7 million words with a cutoff of N=10." ></td>
	<td class="line x" title="121:157	The resulting n-gram set had a maximum recall of 2,610 phrasal terms from the WordNet gold standard, and found the best figure of merit for any of the association metrics even with linguistic filterering to be 0.265." ></td>
	<td class="line x" title="122:157	On the significantly larger Lexile corpus N must be set higher (around N=50) to make the results comparable." ></td>
	<td class="line x" title="123:157	The statistics were also calculated for N=50, N=10 and N=5 in order to see what the effect of including more (relatively rare) n-grams would be on the overall performance for each statistic." ></td>
	<td class="line x" title="124:157	Since many of the statistics are defined without interpolation only for bigrams, and the number of WordNet trigrams at N=50 is very small, the full set of scores were only calculated on the bigram data." ></td>
	<td class="line x" title="125:157	For trigrams, in addition to rank ratio and frequency scores, extended pointwise mutual information and true mutual information scores were calculated using the formulas log (Pxyz/PxPy Pz)) and Pxyz log (Pxyz/PxPy Pz))." ></td>
	<td class="line x" title="126:157	Also, since the standard lexical association metrics cannot be calculated across different n-gram types, results for bigrams and trigrams are presented separately for purposes of comparison." ></td>
	<td class="line x" title="127:157	The results are are shown in Tables 2-5." ></td>
	<td class="line x" title="128:157	Two points should should be noted in particular." ></td>
	<td class="line x" title="129:157	First, the rank ratio statistic outperformed the other association measures tested across the board." ></td>
	<td class="line x" title="130:157	Its best performance, a score of 0.323 in the part of speech filtered condition with N=50, outdistanced METRIC POS Filtered Unfiltered RankRatio 0.323 0.196 Mutual Expectancy 0.144 0.069 TMI 0.209 0.096 PMI 0.287 0.166 Chi-sqr 0.285 0.152 T-Score 0.154 0.046 C-Values 0.065 0.048 Frequency 0.130 0.044 Table 2." ></td>
	<td class="line x" title="131:157	Bigram Scores for Lexical Association Measures with N=50 METRIC POS Filtered Unfiltered RankRatio 0.218 0.125 MutualExpectation 0.140 0.071 TMI 0.150 0.070 PMI 0.147 0.065 Chi-sqr 0.145 0.065 T-Score 0.112 0.048 C-Values 0.096 0.036 Frequency 0.093 0.034 Table 3." ></td>
	<td class="line x" title="132:157	Bigram Scores for Lexical Association Measures with N=10 METRIC POS Filtered Unfiltered RankRatio 0.188 0.110 Mutual Expectancy 0.141 0.073 TMI 0.131 0.063 PMI 0.108 0.047 Chi-sqr 0.107 0.047 T-Score 0.098 0.043 C-Values 0.084 0.031 Frequency 0.081 0.021 Table 4." ></td>
	<td class="line x" title="133:157	Bigram Scores for Lexical Association Measures with N=5 METRIC N=50 N=10 N=5 RankRatio 0.273 0.137 0.103 PMI 0.219 0.121 0.059 TMI 0.137 0.074 0.056 Frequency 0.089 0.047 0.035 Table 5." ></td>
	<td class="line x" title="134:157	Trigram scores for Lexical Association Measures at N=50, 10 and 5 without linguistic filtering." ></td>
	<td class="line x" title="135:157	610 the best score in Schone & Jurafsky's study (0.265), and when large numbers of rare bigrams were included, at N=10 and N=5, it continued to outperform the other measures." ></td>
	<td class="line x" title="136:157	Second, the results were generally consistent with those reported in the literature, and confirmed Schone & Jurafsky's observation that the information-theoretic measures (such as mutual information and chisquared) outperform frequency-based measures (such as the T-score and raw frequency.)5 4.1 Discussion One of the potential strengths of this method is that is allows for a comparison between n-grams of varying lengths." ></td>
	<td class="line x" title="137:157	The distribution of scores for the gold standard bigrams and trigrams appears to bear out the hypothesis that the numbers are comparable across n-gram length." ></td>
	<td class="line x" title="138:157	Trigrams constitute approximately four percent of the gold standard test set, and appear in roughly the same percentage across the rankings; for instance, they consistute 3.8% of the top 10,000 ngrams ranked by mutual rank ratio." ></td>
	<td class="line x" title="139:157	Comparison of trigrams with their component bigrams also seems consistent with this hypothesis; e.g., the bigram Booker T. has a higher mutual rank ratio than the trigram Booker T. Washington, which has a higher rank that the bigram T. Washington." ></td>
	<td class="line x" title="140:157	These results suggest that it would be worthwhile to examine how well the method succeeds at ranking n-grams of varying lengths, though the limitations of the current evaluation set to bigrams and trigrams prevented a full evaluation of its effectiveness across n-grams of varying length." ></td>
	<td class="line x" title="141:157	The results of this study appear to support the conclusion that the Mutual Rank Ratio performs notably better than other association measures on this task." ></td>
	<td class="line x" title="142:157	The performance is superior to the nextbest measure when N is set as low as 5 (0.110 compared to 0.073 for Mutual Expectation and 0.063 for true mutual information and less than .05 for all other metrics)." ></td>
	<td class="line x" title="143:157	While this score is still fairly low, it indicates that the measure performs relatively well even when large numbers of lowprobability n-grams are included." ></td>
	<td class="line x" title="144:157	An examination of the n-best list for the Mutual Rank ratio at N=5 supports this contention." ></td>
	<td class="line x" title="145:157	The top 10 bigrams are: 5 Schone and Jurafsky's results differ from Krenn & Evert (2001)'s results, which indicated that frequency performed better than the statistical measures in almost every case." ></td>
	<td class="line x" title="146:157	However, Krenn and Evert's data consisted of n-grams preselected to fit particular collocational patterns." ></td>
	<td class="line x" title="147:157	Frequency-based metrics seem to be particularly benefited by linguistic prefiltering." ></td>
	<td class="line x" title="148:157	Julius Caesar, Winston Churchill, potato chips, peanut butter, Frederick Douglass, Ronald Reagan, Tia Dolores, Don Quixote, cash register, Santa Claus At ranks 3,000 to 3,010, the bigrams are: Ted Williams, surgical technicians, Buffalo Bill, drug dealer, Lise Meitner, Butch Cassidy, Sandra Cisneros, Trey Granger, senior prom, Ruta Skadi At ranks 10,000 to 10,010, the bigrams are: egg beater, sperm cells, lowercase letters, methane gas, white settlers, training program, instantly recognizable, dried beef, television screens, vienna sausages In short, the n-best list returned by the mutual rank ratio statistic appears to consist primarily of phrasal terms far down the list, even when N is as low as 5." ></td>
	<td class="line x" title="149:157	False positives are typically: (i) morphological variants of established phrases; (ii) bigrams that are part of longer phrases, such as cream sundae (from ice cream sundae); (iii) examples of highly productive constructions such as an artist, three categories or January 2." ></td>
	<td class="line x" title="150:157	The results for trigrams are relatively sparse and thus less conclusive, but are consistent with the bigram results: the mutual rank ratio measure performs best, with top ranking elements consistently being phrasal terms." ></td>
	<td class="line x" title="151:157	Comparison with the n-best list for other metrics bears out the qualitative impression that the rank ratio is performing better at selecting phrasal terms even without filtering." ></td>
	<td class="line x" title="152:157	The top ten bigrams for the true mutual information metric at N=5 are: a little, did not, this is, united states, new york, know what, a good, a long, a moment, a small Ranks 3000 to 3010 are: waste time, heavily on, earlier than, daddy said, ethnic groups, tropical rain, felt sure, raw materials, gold medals, gold rush Ranks 10,000 to 10,010 are: quite close, upstairs window, object is, lord god, private schools, nat turner, fire going, bering sea,little higher, got lots The behavior is consistent with known weaknesses of true mutual information -its tendency to overvalue frequent forms." ></td>
	<td class="line x" title="153:157	Next, consider the n-best lists for loglikelihood at N=5." ></td>
	<td class="line x" title="154:157	The top ten n-grams are: sheriff poulson, simon huggett, robin redbreast, eric torrosian, colonel hillandale, colonel sapp, nurse leatheran, st. catherines, karen torrio, jenny yonge N-grams 3000 to 3010 are: comes then, stuff who, dinner get, captain see, tom see, couple get, fish see, picture go, building go, makes will, pointed way 611 N-grams 10000 to 10010 are: sayings is, writ this, llama on, undoing this, dwahro did, reno on, squirted on, hardens like, mora did, millicent is, vets did Comparison thus seems to suggest that if anything the quality of the mutual rank ratio results are being understated by the evaluation metric, as the metric is returning a large number of phrasal terms in the higher portion of the n-best list that are absent from the gold standard." ></td>
	<td class="line x" title="155:157	Conclusion This study has proposed a new method for measuring strength of lexical association for candidate phrasal terms based upon the use of Zipfian ranks over a frequency distribution combining n-grams of varying length." ></td>
	<td class="line x" title="156:157	The method is related in general philosophy of Mutual Expectation, in that it assesses the strenght of connection for each word to the combined phrase; it differs by adopting a nonparametric measure of strength of association." ></td>
	<td class="line x" title="157:157	Evaluation indicates that this method may outperform standard lexical association measures, including mutual information, chi-squared, log-likelihood, and the T-score." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W05-1006
Automatic Extraction Of Idioms Using Graph Analysis And Asymmetric Lexicosyntactic Patterns
Widdows, Dominic;Dorow, Beate;"></td>
	<td class="line x" title="1:148	Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, pages 4856, Ann Arbor, June 2005." ></td>
	<td class="line x" title="2:148	c2005 Association for Computational Linguistics Automatic Extraction of Idioms using Graph Analysis and Asymmetric Lexicosyntactic Patterns Dominic Widdows MAYA Design, Inc. Pittsburgh, Pennsylvania widdows@maya.com Beate Dorow Institute for Natural Language Processing University of Stuttgart dorowbe@IMS.Uni-Stuttgart.DE Abstract This paper describes a technique for extracting idioms from text." ></td>
	<td class="line x" title="3:148	The technique works by finding patterns such as thrills and spills, whose reversals (such as spills and thrills) are never encountered." ></td>
	<td class="line x" title="4:148	This method collects not only idioms, but also many phrases that exhibit a strong tendency to occur in one particular order, due apparently to underlying semantic issues." ></td>
	<td class="line x" title="5:148	These include hierarchical relationships, gender differences, temporal ordering, and prototype-variant effects." ></td>
	<td class="line x" title="6:148	1 Introduction Natural language is full of idiomatic and metaphorical uses." ></td>
	<td class="line x" title="7:148	However, language resources such as dictionaries and lexical knowledge bases give at best poor coverage of such phenomena." ></td>
	<td class="line x" title="8:148	In many cases, knowledge bases will mistakenly recognize a word and this can lead to more harm than good: for example, a typical mistake of blunt logic would be to assume that somebody let the cat out of the bag implied that somebody let some mammal out of some container. Idiomatic generation of natural language is, if anything, an even greater challenge than idiomatic language understanding." ></td>
	<td class="line x" title="9:148	As pointed out decades ago by Fillmore (1967), a complete knowledge of English requires not only an understanding of the semantics of the word good, but also an awareness that this special adjective (alone) can occur with the word any to construct phrases like Is this paper any good at all?, and traditional lexical resources were not designed to provide this information." ></td>
	<td class="line x" title="10:148	There are many more general examples occur: for example, the big bad wolf sounds right and the the bad big wolf sounds wrong, even though both versions are syntactically and semantically plausible." ></td>
	<td class="line x" title="11:148	Such examples are perhaps idiomatic, though we would perhaps not call them idioms, since they are compositional and can sometimes be predicted by general pattern of word-ordering." ></td>
	<td class="line x" title="12:148	In general, the goal of manually creating a complete lexicon of idioms and idiomatic usage patterns in any language is unattainable, and automatic extraction and modelling techniques have been developed to fill this ever-evolving need." ></td>
	<td class="line x" title="13:148	Firstly, automatically identifying potential idioms and bringing them to the attention of a lexicographer can be used to improve coverage and reduce the time a lexicographer must spend in searching for such examples." ></td>
	<td class="line x" title="14:148	Secondly and more ambitiously, the goal of such work is to enable computers to recognize idioms independently so that the inevitable lack of coverage in language resources does not impede their ability to respond intelligently to natural language input." ></td>
	<td class="line x" title="15:148	In attempting a first-pass at this task, the experiments described in this paper proceed as follows." ></td>
	<td class="line x" title="16:148	We focus on a particular class of idioms that can be extracted using lexicosyntactic patterns (Hearst, 1992), which are fixed patterns in text that suggest that the words occurring in them have some interesting relationship." ></td>
	<td class="line x" title="17:148	The patterns we focus on are occurrences of the form A and/or B, where A and 48 B are both nouns." ></td>
	<td class="line x" title="18:148	Examples include football and cricket and hue and cry. From this list, we extract those examples for which there is a strong preference on the ordering of the participants." ></td>
	<td class="line x" title="19:148	For example, we do see the pattern cricket and football, but rarely if ever encounter the pattern cry and hue. Using this technique, 4173 potential idioms were extracted." ></td>
	<td class="line x" title="20:148	This included a number of both true idioms, and words that have regular semantic relationships but do appear to have interesting orderings on these relationships (such as earlier before later, strong before weak, prototype before variant)." ></td>
	<td class="line x" title="21:148	The rest of this paper is organized as follows." ></td>
	<td class="line x" title="22:148	Section 2 elaborates on some of the previous works that motivate the techniques we have used." ></td>
	<td class="line x" title="23:148	Section 3 describes the precise method used to extract idioms through their asymmetric appearance in a large corpus." ></td>
	<td class="line x" title="24:148	Section 4 presents and analyses several classes of results." ></td>
	<td class="line x" title="25:148	Section 5 describes the methods attempted to filter these results into pairs of words that are more and less contextually related to one another." ></td>
	<td class="line x" title="26:148	These include a statistical method that analyses the original corpus for evidence of semantic relatedness, and a combinatoric method that relies on link-analysis on the resulting graph structure." ></td>
	<td class="line x" title="27:148	2 Previous and Related Work This section describes previous work in extracting information from text, and inferring semantic or idiomatic properties of words from the information so derived." ></td>
	<td class="line x" title="28:148	The main technique used in this paper to extract groups of words that are semantically or idiomatically related is a form of lexicosyntactic pattern recognition." ></td>
	<td class="line x" title="29:148	Lexicosyntactic patterns were pioneered by Marti Hearst (Hearst, 1992; Hearst and Schutze, 1993) in the early 1990s, to enable the addition of new information to lexical resources such as WordNet (Fellbaum, 1998)." ></td>
	<td class="line x" title="30:148	The main insight of this sort of work is that certain regular patterns in word-usage can reflect underlying semantic relationships." ></td>
	<td class="line x" title="31:148	For example, the phrase France, Germany, Italy, and other European countries suggests that France, Germany and Italy are part of the class of European countries." ></td>
	<td class="line x" title="32:148	Such hierarchical examples are quite sparse, and greater coverage was later attained by Riloff and Shepherd (1997) and Roark and Charniak (1998) in extracting relations not of hierarchy but of similarity, by finding conjunctions or co-ordinations such as cloves, cinammon, and nutmeg and cars and trucks. This work was extended by Caraballo (1999), who built classes of related words in this fashion and then reasoned that if a hierarchical relationship could be extracted for any member of this class, it could be applied to all members of the class." ></td>
	<td class="line x" title="33:148	This technique can often mistakenly reason across an ambiguous middle-term, a situation that was improved upon by Cederberg and Widdows (2003), by combining pattern-based extraction with contextual filtering using latent semantic analysis." ></td>
	<td class="line x" title="34:148	Prior work in discovering non-compositional phrases has been carried out by Lin (1999) and Baldwin et al.(2003), who also used LSA to distinguish between compositional and noncompositional verb-particle constructions and nounnoun compounds." ></td>
	<td class="line x" title="36:148	At the same time, work in analyzing idioms and asymmetry within linguistics has become more sophisticated, as discussed by Benor and Levy (2004), and many of the semantic factors underlying our results can be understood from a sophisticated theoretical perspective." ></td>
	<td class="line x" title="37:148	Other motivating and related themes of work for this paper include collocation extraction and example based machine translation." ></td>
	<td class="line oc" title="38:148	In the work of Smadja (1993) on extracting collocations, preference was given to constructions whose constituents appear in a fixed order, a similar (and more generally implemented) version of our assumption here that asymmetric constructions are more idiomatic than symmetric ones." ></td>
	<td class="line x" title="39:148	Recent advances in example-based machine translation (EBMT) have emphasized the fact that examining patterns of language use can significantly improve idiomatic language generation (Carl and Way, 2003)." ></td>
	<td class="line x" title="40:148	3 The Symmetric Graph Model as used for Lexical Acquisition and Idiom Extraction This section of the paper describes the techniques used to extract potentially idiomatic patterns from text, as deduced from previously successful experiments in lexical acquisition." ></td>
	<td class="line x" title="41:148	49 The main extraction technique is to use lexicosyntactic patterns of the form A, B and/or C to find nouns that are linked in some way." ></td>
	<td class="line x" title="42:148	For example, consider the following sentence from the British National Corpus (BNC)." ></td>
	<td class="line x" title="43:148	Ships laden with nutmeg, cinnamon, cloves or coriander once battled the Seven Seas to bring home their precious cargo." ></td>
	<td class="line x" title="44:148	Since the BNC is tagged for parts-of-speech, we know that the words highlighted in bold are nouns." ></td>
	<td class="line x" title="45:148	Since the phrase nutmeg, cinnamon, cloves or coriander fits the pattern A, B, C or D, we create nodes for each of these nouns and create links between them all." ></td>
	<td class="line x" title="46:148	When applied to the whole of the BNC, these links can be aggregated to form a graph with 99,454 nodes (nouns) and 587,475 links, as described by Widdows and Dorow (2002)." ></td>
	<td class="line x" title="47:148	This graph was originally used for lexical acquisition, since clusters of words in the graph often map to recognized semantic classes with great accuracy (> 80%, (Widdows and Dorow, 2002))." ></td>
	<td class="line x" title="48:148	However, for the sake of smoothing over sparse data, these results made the assumption that the links between nodes were symmetric, rather than directed." ></td>
	<td class="line x" title="49:148	In other words, when the pattern A and/or B was encountered, a link from A to B and a link from B to A was introduced." ></td>
	<td class="line x" title="50:148	The nature of symmetric and antisymmetric relationships is examined in detail by Widdows (2004)." ></td>
	<td class="line x" title="51:148	For the purposes of this paper, it suffices to say that the assumption of symmetry (like the assumption of transitivity) is a powerful tool for improving recall in lexical acquisition, but also leads to serious lapses in precision if the directed nature of links is overlooked, especially if symmetrized links are used to infer semantic similarity." ></td>
	<td class="line x" title="52:148	This problem was brought strikingly to our attention by the examples in Figure 1." ></td>
	<td class="line x" title="53:148	In spite of appearing to be a circle of related concepts, many of the nouns in this group are not similar at all, and many of the links in this graph are derived from very very different contexts." ></td>
	<td class="line x" title="54:148	In Figure 1, cat and mouse are linked (they are re both animals and the phrase cat and mouse is used quite often): but then mouse and keyboard are also linked because they are both objects used in computing." ></td>
	<td class="line x" title="55:148	A keyboard, as well as being a typewriter or computer keyboard, is also fiddle cat barrow bow cello flute mouse dog game kitten violin piano bass fortepiano orchestra keyboard screen monitor memory guitar rat human Figure 1: A cluster involving several idiomatic links used to mean (part of) a musical instrument such as an organ or piano, and keyboard is linked to violin." ></td>
	<td class="line x" title="56:148	A violin and a fiddle are the same instrument (as often happens with synonyms, they dont appear together often but have many neighbours in common)." ></td>
	<td class="line x" title="57:148	The unlikely circle is completed (it turns out) because of the phrase from the nursery rhyme Hey diddle diddle, The cat and the fiddle, The cow jumped over the moon; It became clear from examples such as these that idiomatic links, like ambiguous words, were a serious problem when using the graph model for lexical acquisition." ></td>
	<td class="line x" title="58:148	However, with ambiguous words, this obstacle has been gradually turned into an opportunity, since we have also developed ways to used the apparent flaws in the model to detect which words are ambiguous in the first place (Widdows, 2004, Ch 4)." ></td>
	<td class="line x" title="59:148	It is now proposed that we can take the same opportunity for certain idioms: that is, to use the properties of the graph model to work out which links arise from idiomatic usage rather than semantic similarity." ></td>
	<td class="line x" title="60:148	3.1 Idiom Extraction by Recognizing Asymmetric Patterns The link between the cat and fiddle nodes in Figure 1 arises from the phrase the cat and the fiddle. 50 Table 1: Sample of asymmetric pairs extracted from the BNC." ></td>
	<td class="line x" title="61:148	First word Second word highway byway cod haddock composer conductor wood charcoal element compound assault battery north south rock roll god goddess porgy bess middle class war aftermath god hero metal alloy salt pepper mustard cress stocking suspender bits bobs stimulus response committee subcommittee continent ocean However, no corpus examples were ever found of the converse phrase, the fiddle and the cat. In cases like these, it may be concluded that placing a symmetric link between these two nodes is a mistake." ></td>
	<td class="line x" title="62:148	Instead, a directed link may be more appropriate." ></td>
	<td class="line x" title="63:148	We therefore formed the hypothesis that if the phrase A and/or B occurs frequently in a corpus, but the phrase B and/or A is absent, then the link between A and B should be attributed to idiomatic usage rather than semantic similarity." ></td>
	<td class="line x" title="64:148	The next step was to rebuild, finding those relationships that have a strong preference for occurring in a fixed order." ></td>
	<td class="line x" title="65:148	Sure enough, several British English idioms were extracted in this way." ></td>
	<td class="line x" title="66:148	However, several other kinds of relationships were extracted as well, as shown in the sample in Table 1.1 After extracting these pairs, groups of them were gathered together into directed subgraphs.2 Some of these directed subgraphs are reporduced in the analysis in the following section." ></td>
	<td class="line x" title="67:148	1The sample chosen here was selected by the authors to be representative of some of the main types of results." ></td>
	<td class="line x" title="68:148	The complete list can be found at http://infomap.stanford." ></td>
	<td class="line x" title="69:148	edu/graphs/idioms.html." ></td>
	<td class="line x" title="70:148	2These can be viewed at http://infomap." ></td>
	<td class="line x" title="71:148	stanford.edu/graphs/directed_graphs.html 4 Analysis of Results The experimental results include representatives of several types of asymmetric relationships, including the following broad categories." ></td>
	<td class="line x" title="72:148	True Idioms There are many results that display genuinely idiomatic constructions." ></td>
	<td class="line x" title="73:148	By this, we mean phrases that have an explicitly lexicalized nature that a native speaker may be expected to recognize as having a special reference or significance." ></td>
	<td class="line x" title="74:148	Examples include the following: thrills and spills bread and circuses Punch and Judy Porgy and Bess lies and statistics cat and fiddle bow and arrow skull and crossbones This category is quite loosely defined." ></td>
	<td class="line x" title="75:148	It includes 1." ></td>
	<td class="line x" title="76:148	historic quotations such as lies, damned lies and statistics3 and bread and circuses.4 2." ></td>
	<td class="line x" title="77:148	titles of well-known works." ></td>
	<td class="line x" title="78:148	3." ></td>
	<td class="line x" title="79:148	colloquialisms." ></td>
	<td class="line x" title="80:148	4." ></td>
	<td class="line x" title="81:148	groups of objects that have become fixed nominals in their own right." ></td>
	<td class="line x" title="82:148	All of these types share the common property that any NLP system that encounters such groups, in order to behave correctly, should recognize, generate, or translate them as phrases rather than words." ></td>
	<td class="line x" title="83:148	Hierarchical Relationships Many of the asymmetric relationships follow some pattern that may be described as roughly hierarchical." ></td>
	<td class="line x" title="84:148	A cluster of examples from two domains is shown in Figure 2." ></td>
	<td class="line x" title="85:148	In chess, a rook outranks a bishop, and the phrase rook and bishop is encountered much more often than the phrase bishop and 3Attributed to Benjamin Disraeli, certainly popularized by Mark Twain." ></td>
	<td class="line x" title="86:148	4A translation of panem et circenses, from the Roman satirist Juvenal, 1st century AD." ></td>
	<td class="line x" title="87:148	51 Figure 2: Asymmetric relationships in the chess and church hierarchies Figure 3: Different beverages, showing their directed relationships rook. In the church, a cardinal outranks a bishop, a bishop outranks most of the rest of the clergy, and the clergy (in some senses) outrank the laity." ></td>
	<td class="line x" title="88:148	Sometimes these relationships coincide with figure / ground and agent / patient distinctions." ></td>
	<td class="line x" title="89:148	Examples of this kind, as well as clergy and laity, include landlord and tenant, employer and employee, teacher and pupil, and driver and passengers." ></td>
	<td class="line x" title="90:148	An interesting exception is passengers and crew, for which we have no semantic explanation." ></td>
	<td class="line x" title="91:148	Pedigree and potency appear to be two other dimensions that can be used to establish the directedness of an idiomatic construction." ></td>
	<td class="line x" title="92:148	For example, Figure 3 shows that alcoholic drinks normally appear before their cocktail mixers, but that wine outranks some stronger drinks." ></td>
	<td class="line x" title="93:148	Figure 4: Hierarchical relationships between aristocrats, some of which appear to be gender based Gender Asymmetry The relationship between corresponding concepts of different genders also appear to be heavily biased towards appearing in one direction." ></td>
	<td class="line x" title="94:148	Many of these relationships are shown in Figure 4." ></td>
	<td class="line x" title="95:148	This shows that, in cases where one class outranks another, the higher class appears first, but if the classes are identical, then the male version tends to appear before the female." ></td>
	<td class="line x" title="96:148	This pattern is repeated in many pairs of words such as host and hostess, god and goddess, etc. One exception appears to be in parenting relationships, where female precedes male, as in mother and father, mum and dad, grandma and grandpa." ></td>
	<td class="line x" title="97:148	Temporal Ordering If one word refers to an event that precedes another temporally or logically, it almost always appears first." ></td>
	<td class="line x" title="98:148	The examples in Table 2 were extracted by our experiment." ></td>
	<td class="line x" title="99:148	It has been pointed out that for cyclical events, it is perfectly possible that the order of these pairs may be reversed (e.g. , late night and early morning), though the data we extracted from the BNC showed strong tendencies in the directions given." ></td>
	<td class="line x" title="100:148	A directed subgraph showing many events in human lives in shown in Figure 5." ></td>
	<td class="line x" title="101:148	Prototype precedes Variant In cases where one participant is regarded as a pure substance and the other is a variant or mixture, the pure substance tends to come first." ></td>
	<td class="line x" title="102:148	These occur particularly in scientific writing, examples including element and compound, atoms and 52 Table 2: Pairs of events that have a strong tendency to occur in asymmetric patterns." ></td>
	<td class="line x" title="103:148	Before After spring autumn morning afternoon morning evening evening night morning night beginning end question answer shampoo conditioner marriage divorce arrival departure eggs larvae molecules, metals and alloys." ></td>
	<td class="line x" title="104:148	Also, we see apples and pears, apple and plums, and apples and oranges, suggesting that an apple is a prototypical fruit (in agreement with some of the results of prototype theory; see Rosch (1975))." ></td>
	<td class="line x" title="105:148	Another possible version of this tendency is that core precedes periphery, which may also account for asymmetric ordering of food items such as fish and chips, bangers and mash, tea and coffee (in the British National Corpus, at least)!" ></td>
	<td class="line x" title="106:148	In some cases such as meat and vegetables, a hierarchical or figure / ground distinction may also be argued." ></td>
	<td class="line x" title="107:148	Mistaken extractions Our preliminary inspection has shown that the extraction technique finds comparatively few genuine mistakes, and the reader is encouraged to follow the links provided to check this claim." ></td>
	<td class="line x" title="108:148	However, there are some genuine errors, most of which could be avoided with more sophisticated preprocessing." ></td>
	<td class="line x" title="109:148	To improve recall in our initial lexical acquisition experiments, we chose to strip off modifiers and to stem plural forms to singular forms, so that apples and green pears would give a link between apple and pear." ></td>
	<td class="line x" title="110:148	However, in many cases this is a mistake, because the bracketing should not be of the form A and (B C), but of the form (A and B) C. Using part-of-speech tags alone, we cannot recover this information." ></td>
	<td class="line x" title="111:148	One example is the phrase hardware and software vendors, from which we obtain a link between hardware and vendors, instead of a link between hardware and software." ></td>
	<td class="line x" title="112:148	A fuller degree of syntactic analysis would improve this situation." ></td>
	<td class="line x" title="113:148	For extracting semantic relationships, Figure 5: Directed graph showing that life-events are usually ordered temporally when they occur together Cederberg and Widdows (2003) demonstrated that nounphrase chunking does this work very satisfactorily, while being much more tractable than full parsing." ></td>
	<td class="line x" title="114:148	The mistaken pair middle and class shown in Table 1 is another of these mistakes, arising from phrases such as middle and upper class and middle and working class. These examples could be avoided simply by more accurate part-of-speech tagging (since the word middle should have been tagged as an adjective in these examples)." ></td>
	<td class="line x" title="115:148	This concludes our preliminary analysis of results." ></td>
	<td class="line x" title="116:148	5 Filtering using Latent Semantic Analysis and Combinatoric Analysis From the results in the previous section, the following points are clear." ></td>
	<td class="line x" title="117:148	1." ></td>
	<td class="line x" title="118:148	It is possible to extract many accurate examples of asymmetric constructions, that would be necessary knowledge for generation of naturalsounding language." ></td>
	<td class="line x" title="119:148	2." ></td>
	<td class="line x" title="120:148	Some of the pairs extracted are examples of general semantic patterns, others are examples of genuinely idiomatic phrases." ></td>
	<td class="line x" title="121:148	Even for semantically predictable phrases, the fact that the words occur in fixed patterns can be very useful for the purposes of disambiguation, as demonstrated by (Yarowsky, 1995)." ></td>
	<td class="line x" title="122:148	However, it 53 would be useful to be able to tell which of the asymmetric patterns extracted by our experiments correspond to semantically regular phrases which happen to have a conventional ordering preference, and which phrases correspond to genuine idioms." ></td>
	<td class="line x" title="123:148	This final section demonstrates two techniques for performing this filtering task, which show promising results for improving our classification, though should not yet be considered as reliable." ></td>
	<td class="line x" title="124:148	5.1 Filtering using Latent Semantic Analysis Latent semantic analysis or LSA (Landauer and Dumais, 1997) is by now a tried and tested technique for determining semantic similarity between words by analyzing large corpus (Widdows, 2004, Ch 6)." ></td>
	<td class="line x" title="125:148	Because of this, LSA can be used to determine whether a pair of words is likely to participate in a regular semantic relationship, even though LSA may not contribute specific information regarding the nature of the relationship." ></td>
	<td class="line x" title="126:148	However, once a relationship is expected, LSA can be used to predict whether this relationship is used in contexts that are typical uses of the words in question, or whether these uses appear to be anomalies such as rare senses or idioms." ></td>
	<td class="line x" title="127:148	This technique was used successfully by (Cederberg and Widdows, 2003) to improve the accuracy of hyponymy extraction." ></td>
	<td class="line x" title="128:148	It follows that it should be useful to tell the difference between regularly related words and idiomatically related words." ></td>
	<td class="line x" title="129:148	To test this hypothesis, we used an LSA model built from the BNC using the Infomap NLP software.5 This was used to measure the LSA similarity between the words in each of the pairs extracted by the techniques in Section 4." ></td>
	<td class="line x" title="130:148	In cases where a word was too infrequent to appear in the LSA model, we used folding in, which assigns a word-vector on the fly by adding together the vectors of any surrounding words of a target word that are in the model." ></td>
	<td class="line x" title="131:148	The results are shown in Table 3." ></td>
	<td class="line x" title="132:148	The hypothesis is that words whose occurrence is purely idiomatic would have a low LSA similarity score, because they are otherwise not closely related." ></td>
	<td class="line x" title="133:148	However, this hypothesis does not seem to have been confirmed, partly due to the effects of overall frequency." ></td>
	<td class="line x" title="134:148	For example, the word Porgy only occurs in the phrase 5Freely available from http://infomap-nlp." ></td>
	<td class="line x" title="135:148	sourceforge.net/ Table 3: Ordering of results from semantically similar to semantically dissimilar using LSA Word pair LSA similarity north south 0.931 middle class 0.834 porgy bess 0.766 war aftermath 0.676 salt pepper 0.672 bits bobs 0.671 mustard cress 0.603 composer conductor 0.588 cod haddock 0.565 metal alloy 0.509 highway byway 0.480 committee subcommittee 0.479 god goddess 0.456 rock roll 0.398 continent ocean 0.300 wood charcoal 0.273 stimulus response 0.261 stocking suspender 0.177 god hero 0.115 element compound 0.044 assault battery -0.068 granite cheese bread chalk limestone flint marble coal sand sandstone butter meat wine sugar margarine milk clay Figure 6: Nodes in the original symmetric graph in the vicinity of chalk and cheese Porgy and Bess, and the word bobs almost always occurs in the phrase bits and bobs. A more effective filtering technique would need to normalize to account for these effects." ></td>
	<td class="line x" title="136:148	However, there are some good results: for example, the low score between assault and battery reflects the fact that this usage, though compositional, is a rare meaning of the word battery, and the same argument can be made for element and compound." ></td>
	<td class="line x" title="137:148	Thus LSA might be a better guide for recognizing rarity in meaning of individual words than it is for idiomaticity of phrases." ></td>
	<td class="line x" title="138:148	5.2 Link analysis Another technique for determining whether a link is idiomatic or not is to check whether it connects two 54 areas of meaning that are otherwise unconnected." ></td>
	<td class="line x" title="139:148	A hallmark example of this phenomenon is the chalk and cheese example shown in Figure 6." ></td>
	<td class="line x" title="140:148	6 Note that none of the other members of the rock-types clusters is linked to any of the other foodstuffs." ></td>
	<td class="line x" title="141:148	We may be tempted to conclude that the single link between these clusters is an idiomatic phenomenon." ></td>
	<td class="line x" title="142:148	This technique shows promise, but has yet to be explored in detail." ></td>
	<td class="line x" title="143:148	6 Conclusions and Further Work It is possible to extract asymmetric constructions from text, some of which correspond to idioms which are indecomposable (in the sense that their meaning cannot be decomposed into a combination of the meanings of their constituent words)." ></td>
	<td class="line x" title="144:148	Many other phrases were extracted which exhibit a typical directionality that follows from underlying semantic principles." ></td>
	<td class="line x" title="145:148	While these are sometimes not defined as idioms (because they are still composable), knowledge of their asymmetric behaviour is necessary for a system to generate natural language utterances that would sound idiomatic to native speakers." ></td>
	<td class="line x" title="146:148	While all of this information is useful for correctly interpreting and generating natural language, further work is necessary to distinguish accurately between these different categories." ></td>
	<td class="line x" title="147:148	The first step in this process will be to manually classify the results, and evaluate the performance of different classification techniques to see if they can reliably identify different types of idiom, and also distinguish these cases from false positives that were mistakenly extracted." ></td>
	<td class="line x" title="148:148	Once some of these techniques have been evaluated, we will be in a better position to broaden our techniques by turning to larger corpora such as the Web." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E06-1026
Latent Variable Models For Semantic Orientations Of Phrases
Takamura, Hiroya;Inui, Takashi;Okumura, Manabu;"></td>
	<td class="line x" title="1:189	Latent Variable Models for Semantic Orientations of Phrases Hiroya Takamura Precision and Intelligence Laboratory Tokyo Institute of Technology takamura@pi.titech.ac.jp Takashi Inui Japan Society of the Promotion of Science tinui@lr.pi.titech.ac.jp Manabu Okumura Precision and Intelligence Laboratory Tokyo Institute of Technology oku@pi.titech.ac.jp Abstract We propose models for semantic orientations of phrases as well as classification methods based on the models." ></td>
	<td class="line x" title="2:189	Although eachphraseconsistsofmultiplewords, the semantic orientation of the phrase is not a mere sum of the orientations of the component words." ></td>
	<td class="line x" title="3:189	Some words can invert the orientation." ></td>
	<td class="line x" title="4:189	In order to capture the property of such phrases, we introduce latent variables into the models." ></td>
	<td class="line x" title="5:189	Through experiments, we show that the proposed latent variable models work well in the classification of semantic orientations of phrases and achieved nearly 82% classification accuracy." ></td>
	<td class="line x" title="6:189	1 Introduction Technologyforaffectanalysisoftextshasrecently gained attention in both academic and industrial areas." ></td>
	<td class="line x" title="7:189	It can be applied to, for example, a survey of new products or a questionnaire analysis." ></td>
	<td class="line x" title="8:189	Automatic sentiment analysis enables a fast and comprehensive investigation." ></td>
	<td class="line x" title="9:189	The most fundamental step for sentiment analysis is to acquire the semantic orientations of words: desirable or undesirable (positive or negative)." ></td>
	<td class="line x" title="10:189	For example, the word beautiful is positive, while the word dirty is negative." ></td>
	<td class="line x" title="11:189	Many researchers have developed several methods for this purpose and obtained good results (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kamps et al. , 2004; Takamura et al. , 2005; Kobayashi et al. , 2001)." ></td>
	<td class="line x" title="12:189	One of the next problems to be solved is to acquire semantic orientations of phrases, or multi-term expressions." ></td>
	<td class="line x" title="13:189	No computational model for semantically oriented phrases has been proposed so far although some researchers have used techniques developed for single words." ></td>
	<td class="line x" title="14:189	The purpose of this paperistoproposecomputationalmodelsforphrases with semantic orientations as well as classification methods based on the models." ></td>
	<td class="line x" title="15:189	Indeed the semantic orientations of phrases depend on context just as the semantic orientations of words do, but we would like to obtain the most basic orientations of phrases." ></td>
	<td class="line x" title="16:189	We believe that we can use the obtained basic orientations of phrases for affect analysis of higher linguistic units such as sentences and documents." ></td>
	<td class="line x" title="17:189	The semantic orientation of a phrase is not a mere sum of its component words." ></td>
	<td class="line x" title="18:189	Semantic orientations can emerge out of combinations of non-oriented words." ></td>
	<td class="line x" title="19:189	For example, light laptopcomputer is positively oriented although neither light nor laptop-computer has a positive orientation." ></td>
	<td class="line x" title="20:189	Besides, some words can invert the orientation of a neighboring word, such as low in low risk, where the negative orientation of risk is inverted to a positive by the adjective low." ></td>
	<td class="line x" title="21:189	This kind of non-compositional operation has to be incorporated into the model." ></td>
	<td class="line x" title="22:189	We focus on noun+adjective in this paper, since this type of phrase contains most of interesting properties of phrases, such as emergence or inversion of semantic orientations." ></td>
	<td class="line x" title="23:189	In order to capture the properties of semantic orientations of phrases, we introduce latent variables into the models, where one random variable corresponds to nouns and another random variable corresponds to adjectives." ></td>
	<td class="line x" title="24:189	The words that are similar in terms of semantic orientations, such as risk and mortality (i.e. , the positive orientation emerges when they are low), make a cluster in these models." ></td>
	<td class="line x" title="25:189	Our method is language201 independent in the sense that it uses only cooccurrence data of words and semantic orientations." ></td>
	<td class="line x" title="26:189	2 Related Work We briefly explain related work from two viewpoints: the classification of word pairs and the identification of semantic orientation." ></td>
	<td class="line x" title="27:189	2.1 Classification of Word Pairs Torisawa (2001) used a probabilistic model to identify the appropriate case for a pair of words constituting a noun and a verb with the case of the noun-verb pair unknown." ></td>
	<td class="line x" title="28:189	Their model is the same as Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 2001), which is a generative probability model of two random variables." ></td>
	<td class="line x" title="29:189	Torisawas method is similar to ours in that a latent variable model is used for word pairs." ></td>
	<td class="line x" title="30:189	However, Torisawas objective is different from ours." ></td>
	<td class="line x" title="31:189	In addition, we used not the original PLSI, but its expanded version, which is more suitable for this task of semantic orientation classification of phrases." ></td>
	<td class="line x" title="32:189	Fujita et al.(2004) addressed the task of the detection of incorrect case assignment in automatically paraphrased sentences." ></td>
	<td class="line x" title="34:189	They reduced the task to a problem of classifying pairs of a verb and a noun with a case into correct or incorrect." ></td>
	<td class="line x" title="35:189	They first obtained a latent semantic space with PLSI and adopted the nearest-neighbors method, in which they used latent variables as features." ></td>
	<td class="line x" title="36:189	Fujita et al.s method is different from ours, and also from Torisawas, in that a probabilistic model is used for feature extraction." ></td>
	<td class="line x" title="37:189	2.2 Identification of Semantic Orientations The semantic orientation classification of words has been pursued by several researchers (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kamps et al. , 2004; Takamura et al. , 2005)." ></td>
	<td class="line x" title="38:189	However, no computational model for semantically oriented phrases has been proposed to date although research for a similar purpose has been proposed." ></td>
	<td class="line x" title="39:189	Some researchers used sequences of words as features in document classification according to semantic orientation." ></td>
	<td class="line x" title="40:189	Pang et al.(2002) used bigrams." ></td>
	<td class="line x" title="42:189	Matsumoto et al.(2005) used sequential patterns and tree patterns." ></td>
	<td class="line x" title="44:189	Although such patterns were proved to be effective in document classification, the semantic orientations of the patterns themselves are not considered." ></td>
	<td class="line x" title="45:189	Suzuki et al.(2006) used the ExpectationMaximization algorithm and the naive bayes classifier to incorporate the unlabeled data in the classification of 3-term evaluative expressions." ></td>
	<td class="line x" title="47:189	They focused on the utilization of context information such as neighboring words and emoticons." ></td>
	<td class="line x" title="48:189	Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases, whichhadoriginallybeendevelopedforwordsentiment classification." ></td>
	<td class="line x" title="49:189	In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g. , phrase NEAR good) is used to determine the orientation." ></td>
	<td class="line oc" title="50:189	Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences." ></td>
	<td class="line x" title="51:189	Their method is similar to Turneys in the sense that cooccurrence with seed words is used." ></td>
	<td class="line x" title="52:189	The three methods above are based on context information." ></td>
	<td class="line x" title="53:189	In contrast, our method exploits the internal structure of the semantic orientations of phrases." ></td>
	<td class="line x" title="54:189	Inui (2004) introduced an attribute plus/minus for each word and proposed several rules that determine the semantic orientations of phrases on the basis of the plus/minus attribute values and the positive/negative attribute values of the component words." ></td>
	<td class="line x" title="55:189	For example, a rule [negative+minus=positive] determines low (minus) risk (negative) to be positive." ></td>
	<td class="line x" title="56:189	Wilson et al.(2005) worked on phrase-level semantic orientations." ></td>
	<td class="line x" title="58:189	They introduced a polarity shifter, which is almost equivalent to the plus/minus attribute above." ></td>
	<td class="line x" title="59:189	They manually created the list of polarity shifters." ></td>
	<td class="line x" title="60:189	The method that we propose in this paper is an automatic version of Inuis or Wilson et al.s idea, in the sense that the method automatically creates word clusters and their polarity shifters." ></td>
	<td class="line x" title="61:189	3 Latent Variable Models for Semantic Orientations of Phrases As mentioned in the Introduction, the semantic orientation of a phrase is not a mere sum of its component words." ></td>
	<td class="line x" title="62:189	If we know that low risk is positive, and that risk and mortality, in some sense, belong to the same semantic cluster, we can infer that low mortality is also positive." ></td>
	<td class="line x" title="63:189	Therefore, we propose to use latent variable models to extract such latent semantic clusters and to realize an accurate classification of phrases (we focus 202 N Z A N A C N Z A C N Z A C N Z A C (a) (b) (c) (d) (e) Figure 1: Graphical representations:(a) PLSI, (b) naive bayes, (c) 3-PLSI, (d) triangle, (e) U-shaped; Each node indicates a random variable." ></td>
	<td class="line x" title="64:189	Arrows indicate statistical dependency between variables." ></td>
	<td class="line x" title="65:189	N, A, Z and C respectively correspond to nouns, adjectives, latent clusters and semantic orientations." ></td>
	<td class="line x" title="66:189	on two-term phrases in this paper)." ></td>
	<td class="line x" title="67:189	The models adopted in this paper are also used for collaborative filtering by Hofmann (2004)." ></td>
	<td class="line x" title="68:189	With these models, the nouns (e.g. , risk and mortality) that become positive by reducing their degree or amount would make a cluster." ></td>
	<td class="line x" title="69:189	On the other hand, the adjectives or verbs (e.g. , reduce and decrease) that are related to reduction would also make a cluster." ></td>
	<td class="line x" title="70:189	Figure 1 shows graphical representations of statistical dependencies of models with a latent variable." ></td>
	<td class="line x" title="71:189	N, A, Z and C respectively correspond to nouns, adjectives, latent clusters and semantic orientations." ></td>
	<td class="line x" title="72:189	Figure 1-(a) is the PLSI model, which cannot be used in this task due to the absence of a variable for semantic orientations." ></td>
	<td class="line x" title="73:189	Figure 1-(b) is the naive bayes model, in which nouns and adjectives are statistically independent of each other given the semantic orientation." ></td>
	<td class="line x" title="74:189	Figure 1-(c) is, what we call, the 3-PLSI model, which is the 3observable variable version of the PLSI." ></td>
	<td class="line x" title="75:189	We call Figure 1-(d) the triangle model, since three of its four variables make a triangle." ></td>
	<td class="line x" title="76:189	We call Figure 1(e) the U-shaped model." ></td>
	<td class="line x" title="77:189	In the triangle model and the U-shaped model, adjectives directly influence semantic orientations (rating categories) through the probability P(c|az)." ></td>
	<td class="line x" title="78:189	While nouns and adjectives are associated with the same set of clusters Z in the 3-PLSI and the triangle models, only nouns are clustered in the U-shaped model." ></td>
	<td class="line x" title="79:189	In the following, we construct a probability model for the semantic orientations of phrases using each model of (b) to (e) in Figure 1." ></td>
	<td class="line x" title="80:189	We explain in detail the triangle model and the U-shaped model, which we will propose to use for this task." ></td>
	<td class="line x" title="81:189	3.1 Triangle Model Suppose that a set D of tuples of noun n, adjective a (predicate, generally) and the rating c is given : D = {(n1,a1,c1),,(n|D|,a|D|,c|D|)}, (1) where c  {1,0,1}, for example." ></td>
	<td class="line x" title="82:189	This can be easily expanded to the case ofc  {1,,5}." ></td>
	<td class="line x" title="83:189	Our purposeistopredicttheratingcforunknownpairs of n and a. According to Figure 1-(d), the generative probability of n,a,c,z is the following : P(nacz) = P(z|n)P(a|z)P(c|az)P(n)." ></td>
	<td class="line x" title="84:189	(2) Remember that for the original PLSI model, P(naz) = P(z|n)P(a|z)P(n)." ></td>
	<td class="line x" title="85:189	We use the Expectation-Maximization (EM) algorithm (Dempster et al. , 1977) to estimate the parameters of the model." ></td>
	<td class="line x" title="86:189	According to the theory of the EM algorithm, we can increase the likelihood of the model with latent variables by iteratively increasing the Q-function." ></td>
	<td class="line x" title="87:189	The Q-function (i.e. , the expected log-likelihood of the joint probability of complete data with respect to the conditional posterior of the latent variable) is expressed as : Q() = summationdisplay nac fnac summationdisplay z P(z|nac)logP(nazc|), (3) where  denotes the set of the new parameters." ></td>
	<td class="line x" title="88:189	fnac denotes the frequency of a tuple n,a,c in the data." ></td>
	<td class="line x" title="89:189	P represents the posterior computed using the current parameters." ></td>
	<td class="line x" title="90:189	The E-step (expectation step) corresponds to simple posterior computation : P(z|nac) = P(z|n)P(a|z)P(c|az)summationtext z P(z|n)P(a|z)P(c|az)." ></td>
	<td class="line x" title="91:189	(4) For derivation of update rules in the M-step (maximization step), we use a simple Lagrange method for this optimization problem with constraints : z,summationtextn P(n|z) = 1, z,summationtexta P(a|z) = 1, and a,z,summationtextc P(c|az) = 1." ></td>
	<td class="line x" title="92:189	We obtain the following update rules : P(z|n) = summationtext ac fnac P(z|nac)summationtext ac fnac, (5) 203 P(y|z) = summationtext nc fnac P(z|nac)summationtext nac fnac P(z|nac), (6) P(c|az) = summationtext n fnac P(z|nac)summationtext nc fnac P(z|nac)." ></td>
	<td class="line x" title="93:189	(7) These steps are iteratively computed until convergence." ></td>
	<td class="line x" title="94:189	IfthedifferenceofthevaluesofQ-function before and after an iteration becomes smaller than a threshold, we regard it as converged." ></td>
	<td class="line x" title="95:189	For classification of an unknown pair n,a, we compare the values of P(c|na) = summationtext z P(z|n)P(a|z)P(c|az)summationtext cz P(z|n)P(a|z)P(c|az) ." ></td>
	<td class="line x" title="96:189	(8) Then the rating categorycthat maximizeP(c|na) is selected." ></td>
	<td class="line x" title="97:189	3.2 U-shaped Model We suppose that the conditional probability of c and z given n and a is expressed as : P(cz|na) = P(c|az)P(z|n)." ></td>
	<td class="line x" title="98:189	(9) We compute parameters above using the EM algorithm with the Q-function : Q() = summationdisplay nac fnac summationdisplay z P(z|nac)logP(cz|na,).(10) We obtain the following update rules : E step P(z|nac) = P(c|az)P(z|n)summationtext z P(c|az)P(z|n), (11) M step P(c|az) = summationtext n fnac P(z|nac)summationtext nc fnac P(z|nac), (12) P(z|n) = summationtext ac fnac P(z|nac)summationtext ac fnac ." ></td>
	<td class="line x" title="99:189	(13) For classification, we use the formula : P(c|na) = summationdisplay z P(c|az)P(z|n)." ></td>
	<td class="line x" title="100:189	(14) 3.3 Other Models for Comparison We will also test the 3-PLSI model corresponding to Figure 1-(c)." ></td>
	<td class="line x" title="101:189	In addition to the latent models, we test a baseline classifier, which uses the posterior probability : P(c|na)  P(n|c)P(a|c)P(c)." ></td>
	<td class="line x" title="102:189	(15) This baseline model is equivalent to the 2-term naivebayes classifier(Mitchell, 1997)." ></td>
	<td class="line x" title="103:189	Thegraphical representation of the naive bayes model is (b) in Figure 1." ></td>
	<td class="line x" title="104:189	The parameters are estimated as : P(n|c) = 1 + fnc|N| + f c, (16) P(a|c) = 1 + fac|A| + f c, (17) where |N| and |A| are the numbers of the words for n and a, respectively." ></td>
	<td class="line x" title="105:189	Thus, we have four different models : naive bayes (baseline), 3-PLSI, triangle, and U-shaped." ></td>
	<td class="line x" title="106:189	3.4 Discussions on the EM computation, the Models and the Task In the actual EM computation, we use the tempered EM (Hofmann, 2001) instead of the standard EM explained above, because the tempered EM can avoid an inaccurate estimation of the model caused by over-confidence in computing the posterior probabilities." ></td>
	<td class="line x" title="107:189	The tempered EM can be realized by a slight modification to the E-step, which results in a new E-step : P(z|nac) = parenleftbigP(c|az)P(z|n)parenrightbig summationtext z parenleftbigP(c|az)P(z|n)parenrightbig, (18) for the U-shaped model, where  is a positive hyper-parameter, called the inverse temperature." ></td>
	<td class="line x" title="108:189	The new E-steps for the other models are similarly expressed." ></td>
	<td class="line x" title="109:189	Now we have two hyper-parameters : inverse temperature , and the number of possible values M of latent variables." ></td>
	<td class="line x" title="110:189	We determine the values of these hyper-parameters by splitting the giventrainingdatasetintotwodatasets(thetemporary training dataset 90% and the held-out dataset 10%), and by obtaining the classification accuracy for the held-out dataset, which is yielded by the classifier with the temporary training dataset." ></td>
	<td class="line x" title="111:189	We should also note that Z (or any variable) should not have incoming arrows simultaneously from N and A, because the model with such arrows has P(z|na), which usually requires an excessively large memory." ></td>
	<td class="line x" title="112:189	To work with numerical scales of the rating variable (i.e. , the difference between c = 1 and c = 1 should be larger than that of c = 1 and c = 0), Hofmann (2004) used also a GaussiandistributionforP(c|az)incollaborativefiltering." ></td>
	<td class="line x" title="113:189	However, we do not employ a Gaussian, becauseinourdataset,thenumberofratingclassesis 204 only 3, which is so small that a Gaussian distribution cannot be a good approximation of the actual probability density function." ></td>
	<td class="line x" title="114:189	We conducted preliminary experiments with the model with Gaussians, but failed to obtain good results." ></td>
	<td class="line x" title="115:189	For other datasets with more classes, Gaussians might be a good model for P(c|az)." ></td>
	<td class="line x" title="116:189	The task we address in this paper is somewhat similar to the trigram prediction task, in the sense that both are classification tasks given two words." ></td>
	<td class="line x" title="117:189	However, we should note the difference between these two tasks." ></td>
	<td class="line x" title="118:189	In our task, the actual answer given two specific words are fixed as illustrated by the fact high+salary is always positive, while the answer for the trigram prediction task is randomly distributed." ></td>
	<td class="line x" title="119:189	We are therefore interested in thesemanticorientations ofunseen pairsof words, while the main purpose of the trigram prediction is accurately estimate the probability of (possibly seen) word sequences." ></td>
	<td class="line x" title="120:189	In the proposed models, only the words that appeared in the training dataset can be classified." ></td>
	<td class="line x" title="121:189	An attempt to deal with the unseen words is an interesting task." ></td>
	<td class="line x" title="122:189	For example, we could extend our models to semi-supervised models by regarding C as a partially observable variable." ></td>
	<td class="line x" title="123:189	We could also use distributional similarity of words (e.g. , based on window-size cooccurrence) to find an observed wordthatismostsimilartothegivenunseenword." ></td>
	<td class="line x" title="124:189	However, such methods would not work for the semantic orientation classification, because those methods are designed for simple cooccurrence and cannotdistinguishsurvival-ratefrominfectionrate." ></td>
	<td class="line x" title="125:189	In fact, the similarity-based method mentioned above failed to work efficiently in our preliminary experiments." ></td>
	<td class="line x" title="126:189	To solve the problem of unseen words, we would have to use other linguistic resources such as a thesaurus or a dictionary." ></td>
	<td class="line x" title="127:189	4 Experiments 4.1 Experimental Settings We extracted pairs of a noun (subject) and an adjective (predicate), from Mainichi newspaper articles (1995) written in Japanese, and annotated the pairs with semantic orientation tags : positive, neutral or negative." ></td>
	<td class="line x" title="128:189	We thus obtained the labeled dataset consisting of 12066 pair instances (7416 different pairs)." ></td>
	<td class="line x" title="129:189	The dataset contains 4459 negative instances, 4252 neutral instances, and 3355 positiveinstances." ></td>
	<td class="line x" title="130:189	Thenumberofdistinctnounsis 4770 and the number of distinct adjectives is 384." ></td>
	<td class="line x" title="131:189	To check the inter-annotator agreement between two annotators, we calculated  statistics, which was 0.640." ></td>
	<td class="line x" title="132:189	This value is allowable, but not quite high." ></td>
	<td class="line x" title="133:189	However, positive-negative disagreement is observedforonly0.7%ofthedata." ></td>
	<td class="line x" title="134:189	Inotherwords, thisstatisticsmeansthatthetaskofextractingneutral examples, which has hardly been explored, is intrinsically difficult." ></td>
	<td class="line x" title="135:189	We employ 10-fold cross-validation to obtain the average value of the classification accuracy." ></td>
	<td class="line x" title="136:189	We split the dataset such that there is no overlapping pair (i.e. , any pair in the training dataset does not appear in the test dataset)." ></td>
	<td class="line x" title="137:189	If either of the two words in a pair in the test dataset does not appear in the training dataset, we excluded the pair from the test dataset since the problem of unknown words is not in the scope of this research." ></td>
	<td class="line x" title="138:189	Therefore, we evaluate the pairs that are not in the training dataset, but whose component words appear in the training dataset." ></td>
	<td class="line x" title="139:189	In addition to the original dataset, which we call the standard dataset, we prepared another dataset inordertoexaminethepowerofthelatentvariable model." ></td>
	<td class="line x" title="140:189	The new dataset, which we call the hard dataset, consists only of examples with 17 difficult adjectives such as high, low, large, small, heavy, and light." ></td>
	<td class="line x" title="141:189	1 The semantic orientations of pairs including these difficult words often shift depending on the noun they modify." ></td>
	<td class="line x" title="142:189	Thus, the harddatasetisasubsetofthestandarddataset." ></td>
	<td class="line x" title="143:189	The size of the hard dataset is 4787." ></td>
	<td class="line x" title="144:189	Please note that the hard dataset is used only as a test dataset." ></td>
	<td class="line x" title="145:189	For training, we always use the standard dataset in our experiments." ></td>
	<td class="line x" title="146:189	We performed experiments with all the values of  in {0.1,0.2,,1.0} and with all the values of M in {10,30,50,70,100,200,300,500}, and predicted the best values of the hyper-parameters with the held-out method in Section 3.4." ></td>
	<td class="line x" title="147:189	4.2 Results The classification accuracies of the four methods with  and M predicted by the held-out method are shown in Table 1." ></td>
	<td class="line x" title="148:189	Please note that the naive bayes method is irrelevant of  and M. The table shows that the triangle model and the U-shaped 1The complete list of the 17 Japanese adjectives with their English counterparts are : takai (high), hikui (low), ookii (large), chiisai (small), omoi (heavy), karui (light), tsuyoi (strong), yowai (weak), ooi (many), sukunai (few/little), nai (no), sugoi (terrific), hageshii (terrific), hukai (deep), asai (shallow), nagai (long), mizikai (short)." ></td>
	<td class="line x" title="149:189	205 Table 1: Accuracies with predicted  and M standard hard accuracy  M accuracy  M Naive Bayes 73.40   65.93   3-PLSI 67.02 0.73 91.7 60.51 0.80 87.4 Triangle model 81.39 0.60 174.0 77.95 0.60 191.0 U-shaped model 81.94 0.64 60.0 75.86 0.65 48.3 model achieved high accuracies and outperformed the naive bayes method." ></td>
	<td class="line x" title="150:189	This result suggests that we succeeded in capturing the internal structure of semantically oriented phrases by way of latent variables." ></td>
	<td class="line x" title="151:189	The more complex structure of the triangle model resulted in the accuracy that is higher than that of the U-shaped model." ></td>
	<td class="line x" title="152:189	The performance of the 3-PLSI method is even worse than the baseline method." ></td>
	<td class="line x" title="153:189	This result shows thatweshoulduseamodelinwhichadjectivescan directly influence the rating category." ></td>
	<td class="line x" title="154:189	Figures 2, 3, 4 show cross-validated accuracy valuesforvariousvaluesof, respectivelyyielded by the 3-PLSI model, the triangle model and the U-shapedmodelwithdifferentnumbersM ofpossible states for the latent variable." ></td>
	<td class="line x" title="155:189	As the figures show, the classification performance is sensitive to thevalueof." ></td>
	<td class="line x" title="156:189	M = 100 andM = 300 aremostly better than M = 10." ></td>
	<td class="line x" title="157:189	However, this is a tradeoff between classification performance and training time, since large values of M demand heavy computation." ></td>
	<td class="line x" title="158:189	In that sense, the U-shaped model is useful in many practical situations, since it achieved a good accuracy even with a relatively small M. To observe the overall tendency of errors, we show the contingency table of classification by the U-shaped model with the predicted values of hyperparameters, in Table 2." ></td>
	<td class="line x" title="159:189	As this table shows, most of the errors are caused by the difficulty of classifying neutral examples." ></td>
	<td class="line x" title="160:189	Only 2.26% of the errors are mix-ups of the positive orientation and the negative orientation." ></td>
	<td class="line x" title="161:189	We next investigate the causes of errors by observing those mix-ups of the positive orientation and the negative orientation." ></td>
	<td class="line x" title="162:189	One type of frequent errors is illustrated by the pair food (s price) is high, in which the word price is omitted in the actual example 2." ></td>
	<td class="line x" title="163:189	As in this expression, the attribute (price, in this case) of an example is sometimes omitted or not correctly 2This kind of ellipsis often occurs in Japanese." ></td>
	<td class="line x" title="164:189	62 64 66 68 70 72 74 76 78 80 82 84 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Accuracy (%) beta M=300 M=100 M=10 Figure 2: 3-PLSI model with standard dataset 62 64 66 68 70 72 74 76 78 80 82 84 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Accuracy (%) beta M=300 M=100 M=10 Figure 3: Triangle model with standard dataset 62 64 66 68 70 72 74 76 78 80 82 84 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Accuracy (%) beta M=300 M=100 M=10 Figure 4: U-shaped model with standard dataset 206 Table 2: Contingency table of classification result by the U-shaped model U-shaped model positive neutral negative sum positive 1856 281 69 2206 Gold standard neutral 202 2021 394 2617 negative 102 321 2335 2758 sum 2160 2623 2798 7581 identified." ></td>
	<td class="line x" title="165:189	To tackle these examples, we will need methods for correctly identifying attributes and objects." ></td>
	<td class="line x" title="166:189	Some researchers are starting to work on this problem (e.g. , Popescu and Etzioni (2005))." ></td>
	<td class="line x" title="167:189	We succeeded in addressing the data-sparseness problem by introducing a latent variable." ></td>
	<td class="line x" title="168:189	However, this problem still causes some errors." ></td>
	<td class="line x" title="169:189	Precise statistics cannot be obtained for infrequent words." ></td>
	<td class="line x" title="170:189	This problem will be solved by incorporating other resources such as thesaurus or a dictionary,orcombiningourmethodwithothermethods using external wider contexts (Suzuki et al. , 2006; Turney, 2002; Baron and Hirst, 2004)." ></td>
	<td class="line x" title="171:189	4.3 Examples of Obtained Clusters Next, wequalitativelyevaluatetheproposedmethods." ></td>
	<td class="line x" title="172:189	For several clusters z, we extract the words that occur more than twice in the whole dataset and are in top 50 according to P(z|n)." ></td>
	<td class="line x" title="173:189	The model used here as an example is the U-shaped model." ></td>
	<td class="line x" title="174:189	The experimental settings are  = 0.6 and M = 60." ></td>
	<td class="line x" title="175:189	Although some elements of clusters are composed of multiple words in English, the original Japanese counterparts are single words." ></td>
	<td class="line x" title="176:189	Cluster 1 trouble, objection, disease, complaint, anxiety, anamnesis, relapse Cluster 2 risk, mortality, infection rate, onset rate Cluster 3 bond, opinion, love, meaning, longing, will Cluster 4 vote, application, topic, supporter Cluster 5 abuse, deterioration, shock, impact, burden Cluster 6 deterioration, discrimination, load, abuse Cluster 7 relative importance, degree of influence, number, weight, sense of belonging, wave, reputation These obtained clusters match our intuition." ></td>
	<td class="line x" title="177:189	For example, in cluster 2 are the nouns that are negative when combined with high, and positive when combined with low." ></td>
	<td class="line x" title="178:189	In fact, the posterior probabilities of semantic orientations for cluster 2 are as follows : P(negative|high,cluster 2) = 0.995, P(positive|low,cluster 2) = 0.973." ></td>
	<td class="line x" title="179:189	With conventional clustering methods based on the cooccurrence of two words, cluster 2 would include the words resulting in the opposite orientation, such as success rate." ></td>
	<td class="line x" title="180:189	We succeeded in obtaining the clusters that are suitable for our task, by incorporating the new variable c for semantic orientation in the EM computation." ></td>
	<td class="line x" title="181:189	5 Conclusion We proposed models for phrases with semantic orientations as well as a classification method based on the models." ></td>
	<td class="line x" title="182:189	We introduced a latent variable into the models to capture the properties of phrases." ></td>
	<td class="line x" title="183:189	Through experiments, we showed that the proposed latent variable models work well in the classification of semantic orientations of phrases and achieved nearly 82% classification accuracy." ></td>
	<td class="line x" title="184:189	We should also note that our method is language-independent although evaluation was on a Japanese dataset." ></td>
	<td class="line x" title="185:189	We plan next to adopt a semi-supervised learning method in order to correctly classify phrases with infrequent words, as mentioned in Section 4.2." ></td>
	<td class="line x" title="186:189	We would also like to extend our method to 3or more term phrases." ></td>
	<td class="line x" title="187:189	We can also use the obtained latent variables as features for another classifier, as Fujita et al.(2004) used latent variables of PLSI for the k-nearest neighbors method." ></td>
	<td class="line x" title="189:189	One important and promising task would be the use of semantic orientations of words for phrase level classification." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E06-1043
Automatically Constructing A Lexicon Of Verb Phrase Idiomatic Combinations
Fazly, Afsaneh;Stevenson, Suzanne;"></td>
	<td class="line x" title="1:209	Automatically Constructing a Lexicon of Verb Phrase Idiomatic Combinations Afsaneh Fazly Department of Computer Science University of Toronto Toronto, ON M5S 3H5 Canada afsaneh@cs.toronto.edu Suzanne Stevenson Department of Computer Science University of Toronto Toronto, ON M5S 3H5 Canada suzanne@cs.toronto.edu Abstract We investigate the lexical and syntactic flexibility of a class of idiomatic expressions." ></td>
	<td class="line x" title="2:209	We develop measures that draw on such linguistic properties, and demonstrate that these statistical, corpus-based measures can be successfully used for distinguishing idiomatic combinations from non-idiomatic ones." ></td>
	<td class="line x" title="3:209	We also propose a means for automatically determining which syntactic forms a particular idiom can appear in, and hence should be included in its lexical representation." ></td>
	<td class="line x" title="4:209	1 Introduction The term idiom has been applied to a fuzzy category with prototypical examples such as by and large, kick the bucket, and let the cat out of the bag." ></td>
	<td class="line x" title="5:209	Providing adefinitive answerforwhatidioms are, and determining how they are learned and understood, are still subject to debate (Glucksberg, 1993; Nunberg et al. , 1994)." ></td>
	<td class="line x" title="6:209	Nonetheless, they are often defined as phrases or sentences that involve some degree of lexical, syntactic, and/or semantic idiosyncrasy." ></td>
	<td class="line x" title="7:209	Idiomatic expressions, as a part of the vast family of figurative language, are widely used both in colloquial speech and in written language." ></td>
	<td class="line x" title="8:209	Moreover, a phrase develops its idiomaticity over time (Cacciari, 1993); consequently, new idioms come into existence on a daily basis (Cowie et al. , 1983; Seaton and Macaulay, 2002)." ></td>
	<td class="line x" title="9:209	Idioms thus pose a serious challenge, both for the creation of widecoverage computational lexicons, and for the development of large-scale, linguistically plausible natural language processing (NLP) systems (Sag et al. , 2002)." ></td>
	<td class="line x" title="10:209	One problem is due to the range of syntactic idiosyncrasy of idiomatic expressions." ></td>
	<td class="line x" title="11:209	Some idioms, such as by and large, contain syntactic violations; these are often completely fixed and hence can be listed in a lexicon as words with spaces (Sag et al. , 2002)." ></td>
	<td class="line x" title="12:209	However, among those idioms that are syntactically well-formed, some exhibit limited morphosyntactic flexibility, while others may be more syntactically flexible." ></td>
	<td class="line x" title="13:209	For example, theidiom shoot the breezeundergoes verbalinflection(shot the breeze), but not internal modification orpassivization (?shoot the fun breeze, ?the breeze was shot)." ></td>
	<td class="line x" title="14:209	In contrast, the idiom spill the beans undergoes verbal inflection, internal modification, and even passivization." ></td>
	<td class="line x" title="15:209	Clearly, a words-withspaces approach does not capture the full range of behaviour of such idiomatic expressions." ></td>
	<td class="line x" title="16:209	Another barrier to the appropriate handling of idioms in a computational system is their semantic idiosyncrasy." ></td>
	<td class="line x" title="17:209	This is aparticular issue for those idioms that conform to the grammar rules of the language." ></td>
	<td class="line x" title="18:209	Such idiomatic expressions are indistinguishable on the surface from compositional (nonidiomatic) phrases, but a computational system must be capable of distinguishing the two." ></td>
	<td class="line x" title="19:209	For example, a machine translation system should translate the idiom shoot the breeze as a single unit of meaning (to chat), whereas this is not the case for the literal phrase shoot the bird." ></td>
	<td class="line x" title="20:209	In this study, we focus on a particular class of English phrasal idioms, i.e., those that involve the combination of a verb plus a noun in its direct object position." ></td>
	<td class="line x" title="21:209	Examples include shoot the breeze, pull strings, and push ones luck." ></td>
	<td class="line x" title="22:209	We refer to these as verb+noun idiomatic combinations (VNICs)." ></td>
	<td class="line x" title="23:209	The class of VNICs accommodates a large number of idiomatic expressions (Cowie et al. , 1983; Nunberg etal., 1994)." ></td>
	<td class="line x" title="25:209	Moreover, their peculiar be337 haviour signifies the need for a distinct treatment in a computational lexicon (Fellbaum, 2005)." ></td>
	<td class="line x" title="26:209	Despite this, VNICs have been granted relatively little attention within the computational linguistics community." ></td>
	<td class="line x" title="27:209	We look into two closely related problems confronting the appropriate treatment of VNICs: (i) the problem of determining their degree of flexibility; and (ii) the problem of determining their level of idiomaticity." ></td>
	<td class="line x" title="28:209	Section 2 elaborates on the lexicosyntactic flexibility of VNICs, and how this relates to their idiomaticity." ></td>
	<td class="line x" title="29:209	In Section 3, we propose two linguistically-motivated statistical measures for quantifying the degree of lexical and syntactic inflexibility (or fixedness) of verb+noun combinations." ></td>
	<td class="line x" title="30:209	Section 4 presents an evaluation of the proposed measures." ></td>
	<td class="line x" title="31:209	In Section 5, we put forward a technique for determining the syntactic variations that a VNIC can undergo, and that should be included in its lexical representation." ></td>
	<td class="line x" title="32:209	Section 6 summarizes our contributions." ></td>
	<td class="line x" title="33:209	2 Flexibility and Idiomaticity of VNICs Although syntactically well-formed, VNICs involve a certain degree of semantic idiosyncrasy." ></td>
	<td class="line x" title="34:209	Unlike compositional verb+noun combinations, the meaning of VNICs cannot be solely predicted from the meaning of their parts." ></td>
	<td class="line x" title="35:209	There is much evidence in the linguistic literature that the semantic idiosyncrasy of idiomatic combinations is reflected in their lexical and/or syntactic behaviour." ></td>
	<td class="line x" title="36:209	2.1 Lexical and Syntactic Flexibility A limited number of idioms have one (or more) lexical variants, e.g., blow ones own trumpet and toot ones own horn (examples from Cowie et al. 1983)." ></td>
	<td class="line x" title="37:209	However, most are lexically fixed (nonproductive) to a large extent." ></td>
	<td class="line x" title="38:209	Neither shoot the wind nor fling the breeze are typically recognized as variations of the idiom shoot the breeze." ></td>
	<td class="line x" title="39:209	Similarly, spill the beans has an idiomatic meaning (to reveal a secret), while spill the peas and spread the beans have only literal interpretations." ></td>
	<td class="line x" title="40:209	Idiomatic combinations are also syntactically peculiar: most VNICs cannot undergo syntactic variations and at the same time retain their idiomatic interpretations." ></td>
	<td class="line x" title="41:209	It is important, however, tonotethatVNICsdifferwithrespecttothedegree of syntactic flexibility they exhibit." ></td>
	<td class="line x" title="42:209	Some are syntactically inflexible for the most part, while others are more versatile; as illustrated in 1 and 2: 1." ></td>
	<td class="line x" title="43:209	(a) Tim and Joy shot the breeze." ></td>
	<td class="line x" title="44:209	(b) ??" ></td>
	<td class="line x" title="45:209	Tim and Joy shot a breeze." ></td>
	<td class="line x" title="46:209	(c) ??" ></td>
	<td class="line x" title="47:209	Tim and Joy shot the breezes." ></td>
	<td class="line x" title="48:209	(d) ??" ></td>
	<td class="line x" title="49:209	Tim and Joy shot the fun breeze." ></td>
	<td class="line x" title="50:209	(e) ??" ></td>
	<td class="line x" title="51:209	The breeze was shot by Tim and Joy." ></td>
	<td class="line x" title="52:209	(f) ??" ></td>
	<td class="line x" title="53:209	The breeze that Tim and Joy kicked was fun." ></td>
	<td class="line x" title="54:209	2." ></td>
	<td class="line x" title="55:209	(a) Tim spilled the beans." ></td>
	<td class="line x" title="56:209	(b) ? Tim spilled some beans." ></td>
	<td class="line x" title="57:209	(c) ??" ></td>
	<td class="line x" title="58:209	Tim spilled the bean." ></td>
	<td class="line x" title="59:209	(d) Tim spilled the official beans." ></td>
	<td class="line x" title="60:209	(e) The beans were spilled by Tim." ></td>
	<td class="line x" title="61:209	(f) The beans that Tim spilled troubled Joe." ></td>
	<td class="line x" title="62:209	Linguists have explained the lexical and syntactic flexibility of idiomatic combinations in terms of their semantic analyzability (e.g. , Glucksberg 1993; Fellbaum 1993; Nunberg et al. 1994)." ></td>
	<td class="line x" title="63:209	Semantic analyzability is inversely related to idiomaticity." ></td>
	<td class="line x" title="64:209	For example, the meaning of shoot the breeze, a highly idiomatic expression, has nothing todowith either shoot or breeze." ></td>
	<td class="line x" title="65:209	Incontrast, aless idiomatic expression, such as spill the beans, can be analyzed as spill corresponding to reveal and beans referring to secret(s)." ></td>
	<td class="line x" title="66:209	Generally, the constituents ofasemantically analyzable idiom can be mapped onto their corresponding referents in the idiomatic interpretation." ></td>
	<td class="line x" title="67:209	Hence analyzable (less idiomatic) expressions are often more open to lexical substitution and syntactic variation." ></td>
	<td class="line x" title="68:209	2.2 Our Proposal We use the observed connection between idiomaticity and (in)flexibility to devise statistical measures for automatically distinguishing idiomatic from literal verb+noun combinations." ></td>
	<td class="line x" title="69:209	While VNICs vary in their degree of flexibility (cf.1 and 2 above; see also Moon 1998), on the whole they contrast with compositional phrases, which are more lexically productive and appear in a wider range of syntactic forms." ></td>
	<td class="line x" title="71:209	We thus propose to use the degree of lexical and syntactic flexibilityofagivenverb+noun combination todetermine the level of idiomaticity of the expression." ></td>
	<td class="line x" title="72:209	It is important to note that semantic analyzability is neither a necessary nor a sufficient condition for an idiomatic combination to be lexically or syntactically flexible." ></td>
	<td class="line x" title="73:209	Other factors, such as the communicative intentions and pragmatic constraints, can motivate a speaker to use a variant in place of a canonical form (Glucksberg, 1993)." ></td>
	<td class="line x" title="74:209	Nevertheless, lexical and syntactic flexibility may well be used as partial indicators of semantic analyzability, and hence idiomaticity." ></td>
	<td class="line x" title="75:209	338 3 Automatic Recognition of VNICs Here we describe our measures for idiomaticity, whichquantify thedegreeoflexical, syntactic, and overall fixedness of a given verb+noun combination, represented as a verbnoun pair." ></td>
	<td class="line x" title="76:209	(Note that our measures quantify fixedness, not flexibility.)" ></td>
	<td class="line x" title="77:209	3.1 Measuring Lexical Fixedness AVNICislexically fixedifthereplacement ofany of its constituents by a semantically (and syntactically) similar word generally does not result in another VNIC, but in an invalid or a literal expression." ></td>
	<td class="line x" title="78:209	One way of measuring lexical fixedness of a given verb+noun combination is thus to examine theidiomaticity ofitsvariants, i.e., expressions generated by replacing one of the constituents by a similar word." ></td>
	<td class="line x" title="79:209	This approach has two main challenges: (i) it requires prior knowledge about the idiomaticity of expressions (which is what we are developing our measure to determine); (ii) it needs information on similarity among words." ></td>
	<td class="line x" title="80:209	Inspired by Lin(1999), weexamine the strength of association between the verb and noun constituents of the target combination and its variants, as an indirect cue to their idiomaticity." ></td>
	<td class="line x" title="81:209	We use the automatically-built thesaurus of Lin (1998) to find similar words to the noun of the target expression, in order to automatically generate variants." ></td>
	<td class="line x" title="82:209	Only the noun constituent is varied, since replacing the verb constituent of a VNIC with a semantically related verb is more likely to yield another VNIC, as in keep/lose ones cool (Nunberg et al. , 1994)." ></td>
	<td class="line x" title="83:209	Let a0a2a1a4a3a6a5a8a7a10a9a12a11a14a13a16a15a17a9a18a5a20a19a22a21a24a23a26a25a27a23a29a28a31a30 be the set of the a28 most similar nouns to the noun a9 of the target pair a32a34a33a36a35 a9a38a37." ></td>
	<td class="line x" title="84:209	We calculate the association strength for the target pair, and for each of its variants, a32a39a33a36a35 a9 a5 a37, using pointwise mutual information (PMI) (Church et al. , 1991): a40a42a41a44a43 a7 a33a36a35 a9a46a45a47a11a48a13 a49a51a50a53a52a55a54 a7 a33a36a35 a9 a45 a11 a54 a7 a33 a11 a54 a7a10a9 a45 a11 a13 a49a51a50a53a52 a19a56a26a57a34a58a59a19a61a60a62a7 a33a36a35 a9a46a45a47a11 a60a62a7 a33a36a35a64a63 a11a62a60a65a7 a63a66a35 a9 a45 a11 (1) where a67 a23a69a68a70a23a31a28 and a9a36a71 is the target noun; a56 is the set of all transitive verbs in the corpus; a58 is the set of all nouns appearing as the direct object of some verb; a60a2a7 a33a72a35 a9 a45 a11 is the frequency of a33 and a9 a45 occurring as a verbobject pair; a60a62a7 a33a36a35a64a63 a11 is the total frequency of the target verb with any noun in a58 ; a60a2a7 a63a66a35 a9 a45 a11 is the total frequency of the noun a9 a45 in the direct object position of any verb in a56 . Lin (1999) assumes that a target expression is non-compositional if and only if its a40a73a41a74a43 value is significantly different from that of any of the variants." ></td>
	<td class="line x" title="85:209	Instead, we propose a novel technique thatbringstogether theassociation strengths (a40a42a41a44a43 values) of the target and the variant expressions into a single measure reflecting the degree of lexical fixedness for the target pair." ></td>
	<td class="line x" title="86:209	We assume that the target pair is lexically fixed to the extent that its a40a42a41a44a43 deviates from the average a40a42a41a44a43 of its variants." ></td>
	<td class="line x" title="87:209	Our measure calculates this deviation, normalized using the samples standard deviation: a75a62a76a51a77a79a78a81a80a18a82a18a78a81a83a84a83a86a85a88a87a90a89 a7 a33a36a35 a9a91a11a48a13 a40a42a41a44a43 a7 a33a36a35 a9a12a11a62a92 a40a42a41a44a43 a93 (2) a40a73a41a74a43 is the mean and a93 the standard deviation of the sample; a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83a84a83a84a85a88a87a97a89 a7 a33a72a35 a9a12a11a38a98a100a99a94a92a102a101 a35a104a103 a101a106a105 . 3.2 Measuring Syntactic Fixedness Compared to compositional verb+noun combinations, VNICs are expected to appear in more restricted syntactic forms." ></td>
	<td class="line x" title="88:209	To quantify the syntactic fixedness of a target verbnoun pair, we thus need to: (i) identify relevant syntactic patterns, i.e., those that help distinguish VNICs from literalverb+noun combinations; (ii) translate thefrequency distribution of the target pair in the identified patterns into a measure of syntactic fixedness." ></td>
	<td class="line x" title="89:209	3.2.1 Identifying Relevant Patterns Determining a unique set of syntactic patterns appropriate for the recognition of all idiomatic combinations is difficult indeed: exactly which formsanidiomatic combination can occur inisnot entirely predictable (Sag et al. , 2002)." ></td>
	<td class="line x" title="90:209	Nonetheless, there are hypotheses about the difference in behaviour of VNICs and literal verb+noun combinations with respect to particular syntactic variations (Nunberg et al. , 1994)." ></td>
	<td class="line x" title="91:209	Linguists note that semantic analyzability is related to the referential status of the noun constituent, which is in turn related to participation in certain morphosyntactic forms." ></td>
	<td class="line x" title="92:209	In what follows, we describe three types of variation that are tolerated by literal combinations, but are prohibited by many VNICs." ></td>
	<td class="line x" title="93:209	Passivization There is much evidence in the linguistic literature that VNICs often do not undergo passivization.1 Linguists mainly attribute this to the fact that only a referential noun can appear as the surface subject of a passive construction." ></td>
	<td class="line x" title="94:209	1There are idiomatic combinations that are used only in a passivized form; we do not consider such cases in our study." ></td>
	<td class="line x" title="95:209	339 Determiner Type A strong correlation exists between the flexibility of the determiner preceding the noun in a verb+noun combination and the overall flexibility of the phrase (Fellbaum, 1993)." ></td>
	<td class="line x" title="96:209	It is however important to note that the nature of the determiner is also affected by other factors, such as the semantic properties of the noun." ></td>
	<td class="line x" title="97:209	Pluralization While the verb constituent of a VNIC is morphologically flexible, the morphological flexibility of the noun relates to its referential status." ></td>
	<td class="line x" title="98:209	A non-referential noun constituent is expected to mainly appear in just one of the singular or plural forms." ></td>
	<td class="line x" title="99:209	The pluralization of the noun is of course also affected by its semantic properties." ></td>
	<td class="line x" title="100:209	Merging the three variation types results in a pattern set, a0 a0, of a1a2a1 distinct syntactic patterns, given in Table 1.2 3.2.2 Devising a Statistical Measure Thesecond stepistodeviseastatistical measure that quantifies the degree of syntactic fixedness of a verbnoun pair, with respect to the selected set of patterns, a0 a0 . We propose a measure that compares the syntactic behaviour of the target pair with that of a typical verbnoun pair." ></td>
	<td class="line x" title="101:209	Syntactic behaviour of a typical pair is defined as the prior probability distribution over the patterns in a0 a0 . The prior probability of an individual pattern a3a5a4 a98 a0 a0 is estimated as: a54 a7a7a6a9a8a86a11a48a13 a10 a11a13a12a15a14a17a16 a10 a18a20a19a21a14a23a22 a60a2a7 a33 a3 a35 a9a46a45 a35 a3a5a4 a11 a10 a11a24a12a25a14a17a16 a10 a18a20a19a21a14a26a22 a10 a27a29a28a31a30a26a14a33a32a35a34 a60a2a7 a33 a3 a35 a9 a45 a35 a3a5a4a37a36 a11 The syntactic behaviour of the target verbnoun pair a32a34a33a72a35 a9 a37 is defined as the posterior probability distribution over the patterns, given the particular pair." ></td>
	<td class="line x" title="102:209	The posterior probability of an individual pattern a3a5a4 is estimated as: a54 a7a7a6a9a8 a19a20a38 a35a40a39 a11a48a13 a54 a7 a33a36a35 a9 a35 a3a5a4 a11 a54 a7 a33a72a35 a9a12a11 a13 a60a62a7 a33a36a35 a9 a35 a3a5a4 a11 a10 a27a29a28a31a30a41a14a17a32a35a34 a60a62a7 a33a72a35 a9 a35 a3a5a4a37a36 a11 The degree of syntactic fixedness of the target verbnoun pair is estimated as the divergence of its syntactic behaviour (the posterior distribution 2We collapse some patterns since with a larger pattern set the measure may require larger corpora to perform reliably." ></td>
	<td class="line x" title="103:209	Patterns v det:NULL na42a44a43 v det:NULL na45a20a46 v det:a/an na42a44a43 v det:the na42a44a43 v det:the na45a20a46 v det:DEM na42a44a43 v det:DEM na45a20a46 v det:POSS na42a44a43 v det:POSS na45a20a46 v det:OTHER [ n a42a44a43a48a47 a45a20a46 ] det:ANY [ n a42a44a43a48a47 a45a20a46 ] be v a45a13a49 a42a44a42 a12 a50a25a51 Table 1: Patterns for syntactic fixedness measure." ></td>
	<td class="line x" title="104:209	over the patterns), from the typical syntactic behaviour (the prior distribution)." ></td>
	<td class="line x" title="105:209	The divergence of the two probability distributions is calculated using a standard information-theoretic measure, the Kullback Leibler (KL-)divergence: a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83 a83a53a52a37a54a26a55 a7 a33a36a35 a9a91a11 a13 a56a44a7 a54 a7 a3a5a4 a19 a33a72a35 a9a12a11a91a19a51a19 a54 a7 a3a5a4 a11a86a11 a13 a10 a27a29a28a31a30a23a14a33a32a35a34 a54 a7 a3a5a4a37a36 a19 a33a36a35 a9a12a11a79a49a51a50a53a52 a54 a7 a3a57a4a37a36 a19 a33a72a35 a9a12a11 a54 a7 a3a5a4a37a36 a11 (3) KL-divergence is always non-negative and is zero if and only if the two distributions are exactly the same." ></td>
	<td class="line x" title="106:209	Thus, a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83a84a83a58a52a37a54a26a55 a7 a33a72a35 a9a91a11a73a98a100a99a67a95a35a104a103 a101a106a105 . KL-divergence is argued to be problematic because it is not a symmetric measure." ></td>
	<td class="line x" title="107:209	Nonetheless, it has proven useful in many NLP applications (Resnik, 1999; Dagan et al. , 1994)." ></td>
	<td class="line x" title="108:209	Moreover, the asymmetry is not an issue here since we are concerned with the relative distance of several posterior distributions from the same prior." ></td>
	<td class="line x" title="109:209	3.3 A Hybrid Measure of Fixedness VNICs are hypothesized to be, in most cases, both lexically and syntactically more fixed than literal verb+noun combinations (see Section 2)." ></td>
	<td class="line x" title="110:209	We thus propose a new measure of idiomaticity to be a measure of the overall fixedness of a given pair." ></td>
	<td class="line x" title="111:209	We define a75a62a76a51a77a79a78a81a80a18a82a18a78a81a83a84a83a60a59a13a61a84a87a13a62a15a63 a85a88a85 a7 a33a72a35 a9a12a11 as: a75a62a76a51a77a79a78a81a80a18a82a18a78a81a83a84a83a53a59a20a61a84a87a24a62a15a63 a85a88a85 a7 a33a36a35 a9a91a11 a13 a64 a75a2a76a94a77a79a78a81a80a72a82a96a78a81a83a84a83a53a52a65a54a21a55 a7 a33a72a35 a9a12a11 a103 a7 a1 a92a66a64a62a11 a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83 a83a86a85a88a87a90a89 a7 a33a36a35 a9a91a11 (4) where a64 weights the relative contribution of the measures in predicting idiomaticity." ></td>
	<td class="line x" title="112:209	4 Evaluation of the Fixedness Measures To evaluate our proposed fixedness measures, we determine their appropriateness asindicators ofidiomaticity." ></td>
	<td class="line x" title="113:209	We pose a classification task in which idiomatic verbnoun pairs are distinguished from literal ones." ></td>
	<td class="line x" title="114:209	We use each measure to assign scores 340 to the experimental pairs (see Section 4.2 below)." ></td>
	<td class="line x" title="115:209	We then classify the pairs by setting a threshold, here the median score, where all expressions with scores higher than the threshold are labeled as idiomatic and the rest as literal." ></td>
	<td class="line x" title="116:209	We assess the overall goodness of a measure by looking at its accuracy (Acc) and the relative reduction in error rate (RER) on the classification task described above." ></td>
	<td class="line x" title="117:209	The RER of a measure reflects the improvement in its accuracy relative to another measure (often a baseline)." ></td>
	<td class="line x" title="118:209	We consider two baselines: (i) a random baseline, a0a2a1 a82a18a80, that randomly assigns a label (literal or idiomatic) to each verbnoun pair; (ii) a more informed baseline, a40a42a41a44a43, an information-theoretic measure widely used for extracting statistically significant collocations.3 4.1 Corpus and Data Extraction We use the British National Corpus (BNC; http://www.natcorp.ox.ac.uk/) to extract verb noun pairs, along with information on the syntactic patterns they appear in." ></td>
	<td class="line x" title="119:209	We automatically parse the corpus using the Collins parser (Collins, 1999), and further process it using TGrep2 (Rohde, 2004)." ></td>
	<td class="line x" title="120:209	For each instance of a transitive verb, we use heuristics to extract the noun phrase (NP) in either the direct object position (if the sentence is active), or the subject position (if the sentence is passive)." ></td>
	<td class="line x" title="121:209	We then use NP-head extraction software4 to get the head noun of the extracted NP, its number (singular or plural), and the determiner introducing it." ></td>
	<td class="line x" title="122:209	4.2 Experimental Expressions We select our development and test expressions from verbnoun pairs that involve a member of a predefined list of (transitive) basic verbs." ></td>
	<td class="line x" title="123:209	Basic verbs, in their literal use, refer to states or acts that are central to human experience." ></td>
	<td class="line x" title="124:209	They are thus frequent, highly polysemous, and tend to combine with other words to form idiomatic combinations (Nunberg et al. , 1994)." ></td>
	<td class="line x" title="125:209	An initial list of suchverbswasselected fromseveral linguistic and psycholinguistic studies on basic vocabulary (e.g. , Pauwels 2000; Newman and Rice 2004)." ></td>
	<td class="line x" title="126:209	We further augmented this initial list with verbs that are semantically related to another verb already in the 3As in Eqn." ></td>
	<td class="line x" title="127:209	(1), our calculation of PMI here restricts the verbnoun pair to the direct object relation." ></td>
	<td class="line x" title="128:209	4We use a modified version of the software provided by Eric Joanis based on heuristics from (Collins, 1999)." ></td>
	<td class="line x" title="129:209	list; e.g., lose is added in analogy with find." ></td>
	<td class="line x" title="130:209	The final list of 28 verbs is: blow, bring, catch, cut, find, get, give, have, hear, hit, hold, keep, kick, lay, lose, make, move, place, pull, push, put, see, set, shoot, smell, take, throw, touch From the corpus, we extract all verbnoun pairs withminimum frequency of a1 a67 that contain abasic verb." ></td>
	<td class="line x" title="131:209	From these, we semi-randomly select an idiomatic and a literal subset.5 A pair is considered idiomatic if it appears in a credible idiom dictionary, such as the Oxford Dictionary of Current Idiomatic English (ODCIE) (Cowie et al. , 1983), or the Collins COBUILD Idioms Dictionary (CCID) (Seaton and Macaulay, 2002)." ></td>
	<td class="line x" title="132:209	Otherwise, the pair is considered literal." ></td>
	<td class="line x" title="133:209	We then randomly pull out a1a4a3 a67 development and a5 a67a53a67 test pairs (half idiomatic and half literal), ensuring both low and high frequency items are included." ></td>
	<td class="line x" title="134:209	Sample idioms corresponding to the extracted pairs are: kick the habit, move mountains, lose face, and keep ones word." ></td>
	<td class="line x" title="135:209	4.3 Experimental Setup Development expressions are used in devising the fixedness measures, as well as in determining the values of the parameters a28 in Eqn." ></td>
	<td class="line x" title="136:209	(2) and a64 in Eqn." ></td>
	<td class="line x" title="137:209	(4)." ></td>
	<td class="line x" title="138:209	a28 determines the maximum number of nouns similar to the target noun, to be considered in measuring the lexical fixedness of a given pair." ></td>
	<td class="line x" title="139:209	The value of this parameter is determined by performing experiments over the development data, in which a28 ranges from a1 a67 to a1 a67a53a67 by steps of a1 a67 ; a28 is set to a6 a67 based on the results." ></td>
	<td class="line x" title="140:209	We also experimented with different values of a64 ranging from a67 to a1 by steps of a7 a1 . Based on the development results, thebest value for a64 is a7a9a8 (giving moreweight to the syntactic fixedness measure)." ></td>
	<td class="line x" title="141:209	Testexpressions aresaved asunseen data forthe final evaluation." ></td>
	<td class="line x" title="142:209	We further divide the set of all testexpressions, TESTa63 a85a88a85,intotwosetscorresponding to two frequency bands: TESTa10a12a11a13a15a14 contains a6 a67 idiomatic and a6 a67 literal pairs, each with total frequency between a1 a67 and a16a66a67 (a1 a67 a23a20a60a18a17a20a19a20a21a96a7 a33a36a35 a9 a35a12a63 a11a23a22 a16a66a67 ); TESTa10a12a24a26a25a27a28a24 consists of a6 a67 idiomatic and a6 a67 literal pairs, each with total frequency of a16a66a67 or greater (a60a18a17a20a19a20a21a96a7 a33a36a35 a9 a35a91a63 a11a30a29 a16a66a67 )." ></td>
	<td class="line x" title="143:209	All frequency counts are over the entire BNC." ></td>
	<td class="line x" title="144:209	4.4 Results We first examine the performance of the individual fixedness measures, a75a2a76a94a77a79a78a81a80a72a82a96a78a81a83a84a83a104a85a88a87a90a89 and 5In selecting literal pairs, we choose those that involve a physical act corresponding to the basic semantics of the verb." ></td>
	<td class="line x" title="145:209	341 Data Set: TESTa0 a11 a11 %Acc %RER a1a3a2a5a4a7a6 50 a8a10a9a12a11 64 28 a13a15a14a17a16a19a18a20a6a19a4a7a18a22a21a23a21 a11 a24a26a25 65 30 a13a15a14a17a16a19a18a20a6a19a4a7a18a22a21a23a21a28a27a30a29a32a31 70 40 Table 2: Accuracy and relative error reduction for the two fixedness and the two baseline measures over all test pairs." ></td>
	<td class="line x" title="146:209	a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83a84a83a53a52a65a54a21a55, as well as that of the two baselines, a0a2a1 a82a72a80 and a40a73a41a74a43 ; see Table 2." ></td>
	<td class="line x" title="147:209	(Results for the overall measure are presented later in this section)." ></td>
	<td class="line x" title="148:209	As can be seen, the informed baseline, a40a42a41a44a43, shows a large improvement over the random baseline (a5 a8 a33 error reduction)." ></td>
	<td class="line x" title="149:209	This shows that one can get relatively good performance by treating verb+noun idiomatic combinations as collocations." ></td>
	<td class="line x" title="150:209	a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83a84a83a84a85a88a87a90a89 performs as well as the informed baseline (a34 a67 a33 error reduction)." ></td>
	<td class="line x" title="151:209	This result shows that, as hypothesized, lexical fixedness is areasonably good predictor of idiomaticity." ></td>
	<td class="line x" title="152:209	Nonetheless, the performance signifies a need for improvement." ></td>
	<td class="line x" title="153:209	Possibly the most beneficial enhancement would be a change in the way we acquire the similar nouns for a target noun." ></td>
	<td class="line x" title="154:209	The best performance (shown in boldface) belongs to a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83a84a83a60a52a65a54a21a55, with a16a66a67 a33 error reduction over the random baseline, and a5 a67 a33 error reduction over the informed baseline." ></td>
	<td class="line x" title="155:209	These results demonstrate that syntactic fixedness is a good indicator of idiomaticity, better than a simple measure of collocation (a40a73a41a74a43 ), or a measure of lexical fixedness." ></td>
	<td class="line x" title="156:209	These results further suggest that looking into deep linguistic properties of VNICs is both necessary and beneficial for the appropriate treatment of these expressions." ></td>
	<td class="line x" title="157:209	a40a73a41a74a43 is known to perform poorly on low frequency data." ></td>
	<td class="line x" title="158:209	To examine the effect of frequency on the measures, we analyze their performance on the two divisions of the test data, corresponding to the two frequency bands, TESTa10a12a11 a13a15a14 and TESTa10a12a24a26a25 a27a28a24 . Results are given in Table 3, with the best performance shown in boldface." ></td>
	<td class="line x" title="159:209	As expected, the performance of a40a73a41a74a43 drops substantially for low frequency items." ></td>
	<td class="line x" title="160:209	Interestingly, although it is a PMI-based measure, a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83a84a83a86a85a88a87a97a89 performs slightly better when the data is separated based on frequency." ></td>
	<td class="line x" title="161:209	The performance of a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83a84a83a60a52a65a54a21a55 improves quite a bit when it is applied to high frequency items, while it improves only slightly on the low frequency items." ></td>
	<td class="line x" title="162:209	These results show that both Fixedness measures Data Set: TESTa35a37a36a38a40a39 TESTa35a37a41a28a42 a43a44a41 %Acc %RER %Acc %RER a1a45a2a46a4a47a6 50 50 a8a48a9a12a11 56 12 70 40 a13a15a14a49a16a19a18a20a6a19a4a7a18a22a21a23a21 a11 a24a26a25 68 36 66 32 a13a15a14a49a16a19a18a20a6a19a4a7a18a22a21a23a21 a27a50a29a51a31 72 44 82 64 Table 3: Accuracy and relative error reduction for all measures over test pairs divided by frequency." ></td>
	<td class="line x" title="163:209	Data Set: TESTa0 a11 a11 %Acc %RER a13a15a14a17a16a52a18a22a6a52a4a7a18a51a21a23a21 a11 a24a53a25 65 30 a13a15a14a17a16a52a18a22a6a52a4a7a18a51a21a23a21 a27a30a29a51a31 70 40 a13a15a14a17a16a52a18a22a6a52a4a7a18a51a21a23a21 a13a40a54a28a24a26a55 a0 a11 a11 74 48 Table 4: Performance of the hybrid measure over TESTa0 a11 a11 . perform better onhomogeneous data, whileretaining comparably good performance on heterogeneous data." ></td>
	<td class="line x" title="164:209	These results reflect that our fixedness measures are not as sensitive tofrequency as a40a42a41a44a43 . Hence they can be used with a higher degree of confidence, especially when applied to data that is heterogeneous with regard to frequency." ></td>
	<td class="line x" title="165:209	This is important because while some VNICs are very common, others have very low frequency." ></td>
	<td class="line x" title="166:209	Table 4 presents the performance of the hybrid measure, a75a2a76a94a77a79a78a81a80a72a82a96a78a81a83a84a83a58a59a13a61a84a87a24a62a25a63 a85a88a85, repeating that of a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83a84a83a84a85a88a87a90a89 and a75a2a76a94a77a79a78a81a80a72a82a96a78a81a83a84a83a60a52a37a54a26a55 for comparison." ></td>
	<td class="line x" title="167:209	a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83a84a83a60a59a13a61a84a87a24a62a25a63 a85a88a85 outperforms both lexical and syntactic fixedness measures, with a substantial improvement over a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83a84a83 a85a88a87a90a89, and a small, but notable, improvement over a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83a84a83a21a52a37a54a26a55 . Each of the lexical and syntactic fixedness measures is a good indicator of idiomaticity on its own, with syntactic fixedness being a better predictor." ></td>
	<td class="line x" title="168:209	Here we demonstrate that combining them into a single measure of fixedness, while giving more weight to the better measure, results in a more effective predictor of idiomaticity." ></td>
	<td class="line x" title="169:209	5 Determining the Canonical Forms Our evaluation of the fixedness measures demonstrates their usefulness for the automatic recognition of idiomatic verbnoun pairs." ></td>
	<td class="line x" title="170:209	To represent such pairs in a lexicon, however, we must determine their canonical form(s)Cforms henceforth." ></td>
	<td class="line x" title="171:209	For example, the lexical representation of a32 shoot, breeze a37 should include shoot the breeze as a Cform." ></td>
	<td class="line x" title="172:209	Since VNICs are syntactically fixed, they are mostly expected to have a single Cform." ></td>
	<td class="line x" title="173:209	Nonetheless, there are idioms with two or more accept342 able forms." ></td>
	<td class="line x" title="174:209	For example, hold fire and hold ones fire are both listed in CCID as variations of the same idiom." ></td>
	<td class="line x" title="175:209	Our approach should thus be capable of predicting all allowable forms for a given idiomatic verbnoun pair." ></td>
	<td class="line x" title="176:209	Weexpect aVNICtooccurinitsCform(s)more frequently than it occurs in anyother syntactic patterns." ></td>
	<td class="line x" title="177:209	To discover the Cform(s) for a given idiomatic verbnoun pair, we thus examine its frequency of occurrence in each syntactic pattern in a0 a0 . Since it is possible for an idiom to have more than one Cform, we cannot simply take the most dominant pattern as the canonical one." ></td>
	<td class="line x" title="178:209	Instead, we calculate a a0 -score for the target pair a32a102a33a72a35 a9a12a37 and each pattern a3a5a4a65a36 a98 a0 a0 : a0 a36 a7 a33a72a35 a9a12a11 a13 a60a62a7 a33a72a35 a9 a35 a3a5a4a37a36 a11a62a92 a60 a93 inwhicha60 isthemeanand a93 thestandard deviation over the sample a15a84a60a62a7 a33a36a35 a9 a35 a3a57a4a65a36 a11a91a19 a3a5a4a37a36 a98 a0 a0 a30 . The statistic a0 a36 a7 a33a36a35 a9a91a11 indicates how far and in which direction the frequency of occurrence of the pair a32 a33a36a35 a9 a37 in pattern a6 a8a2a1 deviates from the samplesmean, expressed inunits ofthesamples standard deviation." ></td>
	<td class="line x" title="179:209	To decide whether a3a5a4a25a36 is a canonical pattern for the target pair, we check whether a0 a36 a7 a33a36a35 a9a91a11a4a3a6a5a8a7, where a5a9a7 is a threshold." ></td>
	<td class="line x" title="180:209	For evaluation, we set a5a9a7 to a1, based on the distribution of a10 and through examining the development data." ></td>
	<td class="line x" title="181:209	We evaluate the appropriateness of this approach in determining the Cform(s) of idiomatic pairs by verifying its predicted forms against ODCIE and CCID." ></td>
	<td class="line x" title="182:209	Specifically, for each of the a1 a67a53a67 idiomatic pairs in TESTa63 a85a88a85, we calculate the precision and recall of its predicted Cforms (those whose a0 -scores are above a5a11a7 ), compared to the Cforms listed in the two dictionaries." ></td>
	<td class="line x" title="183:209	The average precision across the 100 test pairs is 81.7%, and the average recall is 88.0% (with 69 of the pairs having 100% precision and 100% recall)." ></td>
	<td class="line x" title="184:209	Moreover, we find that for the overwhelming majority of the pairs, a8 a3 a33, the predicted Cform with the highest a0 -score appears in the dictionary entry of the pair." ></td>
	<td class="line x" title="185:209	Thus, our method of detecting Cforms performs quite well." ></td>
	<td class="line x" title="186:209	6 Discussion and Conclusions The significance of the role idioms play in language has long been recognized." ></td>
	<td class="line x" title="187:209	However, due to their peculiar behaviour, idioms have been mostly overlooked by the NLP community." ></td>
	<td class="line x" title="188:209	Recently, there has been growing awareness of the importance of identifying non-compositional multiword expressions (MWEs)." ></td>
	<td class="line x" title="189:209	Nonetheless, most research on the topic has focused on compound nouns and verb particle constructions." ></td>
	<td class="line x" title="190:209	Earlier work on idiomshaveonlytouched thesurface oftheproblem, failing to propose explicit mechanisms for appropriately handling them." ></td>
	<td class="line x" title="191:209	Here, we provide effective mechanisms for the treatment of a broadly documented and crosslinguistically frequent class of idioms, i.e., VNICs." ></td>
	<td class="line x" title="192:209	Earlier research on the lexical encoding of idioms mainly relied on the existence of human annotations, especially for detecting which syntactic variations (e.g. , passivization) an idiom can undergo (Villavicencio et al. , 2004)." ></td>
	<td class="line x" title="193:209	We propose techniques for the automatic acquisition and encoding of knowledge about the lexicosyntactic behaviour of idiomatic combinations." ></td>
	<td class="line x" title="194:209	We put forwardameans for automatically discovering the set ofsyntactic variations that aretolerated byaVNIC and that should be included in its lexical representation." ></td>
	<td class="line x" title="195:209	Moreover, weincorporate suchinformation into statistical measures that effectively predict the idiomaticity level of a given expression." ></td>
	<td class="line x" title="196:209	In this regard, our work relates to previous studies on determining the compositionality (inverse of idiomaticity) of MWEs other than idioms." ></td>
	<td class="line oc" title="197:209	Most previous work on compositionality of MWEs either treat them as collocations (Smadja, 1993), or examine the distributional similarity between the expression and its constituents (McCarthy et al. , 2003; Baldwin et al. , 2003; Bannard et al. , 2003)." ></td>
	<td class="line x" title="198:209	Lin (1999) and Wermter and Hahn (2005) go one step further and look into a linguistic property of non-compositional compoundstheir lexical fixednessto identify them." ></td>
	<td class="line x" title="199:209	Venkatapathy and Joshi (2005) combine aspects of the above-mentioned work, by incorporatinglexical fixedness, collocation-based, anddistributional similarity measures into a set of features which are used to rank verb+noun combinations according to their compositionality." ></td>
	<td class="line x" title="200:209	Our work differs from such studies in that it carefully examines several linguistic properties of VNICs that distinguish them from literal (compositional) combinations." ></td>
	<td class="line x" title="201:209	Moreover, we suggest novel techniques for translating such characteristics into measures that predict the idiomaticity level of verb+noun combinations." ></td>
	<td class="line x" title="202:209	More specifically, we propose statistical measures that quantify the degree of lexical, syntactic, and overall fixedness of such combinations." ></td>
	<td class="line x" title="203:209	We demonstrate 343 that these measures can be successfully applied to the task of automatically distinguishing idiomatic combinations from non-idiomatic ones." ></td>
	<td class="line x" title="204:209	We also show that our syntactic and overall fixedness measures substantially outperform a widely used measure of collocation, a40a42a41a44a43, even when the latter takes syntactic relations into account." ></td>
	<td class="line x" title="205:209	Others have also drawn on the notion of syntactic fixedness for idiom detection, though specific to a highly constrained type of idiom (Widdows and Dorow, 2005)." ></td>
	<td class="line x" title="206:209	Our syntactic fixedness measure looks into a broader set of patterns associated with a large class of idiomatic expressions." ></td>
	<td class="line x" title="207:209	Moreover, our approach is general and can be easily extended to other idiomatic combinations." ></td>
	<td class="line x" title="208:209	Each measure we use to identify VNICs captures a different aspect of idiomaticity: a40a73a41a74a43 reflects the statistical idiosyncrasy of VNICs, while the fixedness measures draw on their lexicosyntactic peculiarities." ></td>
	<td class="line x" title="209:209	Our ongoing work focuses on combining these measures to distinguish VNICs from other idiosyncratic verb+noun combinations that are neither purely idiomatic nor completely literal, so that we can identify linguistically plausible classes of verb+noun combinations on this continuum (Fazly and Stevenson, 2005)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-1120
Accurate Collocation Extraction Using A Multilingual Parser
Seretan, Violeta;Wehrli, Eric;"></td>
	<td class="line x" title="1:160	Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 953960, Sydney, July 2006." ></td>
	<td class="line x" title="2:160	c2006 Association for Computational Linguistics Accurate Collocation Extraction Using a Multilingual Parser Violeta Seretan Language Technology Laboratory University of Geneva 2, rue de Candolle, 1211 Geneva Violeta.Seretan@latl.unige.ch Eric Wehrli Language Technology Laboratory University of Geneva 2, rue de Candolle, 1211 Geneva Eric.Wehrli@latl.unige.ch Abstract This paper focuses on the use of advanced techniques of text analysis as support for collocation extraction." ></td>
	<td class="line x" title="3:160	A hybrid system is presented that combines statistical methods and multilingual parsing for detecting accurate collocational information from English, French, Spanish and Italian corpora." ></td>
	<td class="line x" title="4:160	The advantage of relying on full parsing over using a traditional window method (which ignores the syntactic information) is first theoretically motivated, then empirically validated by a comparative evaluation experiment." ></td>
	<td class="line x" title="5:160	1 Introduction Recentcomputationallinguisticsresearchfullyacknowledged the stringent need for a systematic and appropriate treatment of phraseological units in natural language processing applications (Sag etal.,2002)." ></td>
	<td class="line x" title="7:160	Syntagmaticrelationsbetweenwords  also called multi-word expressions, or idiosyncratic interpretations that cross word boundaries (Sag et al. , 2002, 2)  constitute an important part of the lexicon of a language: according to Jackendoff (1997), they are at least as numerous as the single words, while according to Melcuk (1998) they outnumber single words ten to one." ></td>
	<td class="line x" title="8:160	Phraseological units include a wide range of phenomena, among which we mention compound nouns (dead end), phrasal verbs (ask out), idioms (lend somebody a hand), and collocations (fierce battle, daunting task, schedule a meeting)." ></td>
	<td class="line x" title="9:160	They pose important problems for NLP applications, both text analysis and text production perspectives being concerned." ></td>
	<td class="line x" title="10:160	In particular, collocations1 are highly problematic, for at least two reasons: first, because their linguistic status and properties are unclear (as pointed out by McKeown and Radev (2000), their definition is rather vague, and the distinction from other types of expressions is not clearly drawn); second, because they are prevalent in language." ></td>
	<td class="line x" title="11:160	Melcuk(1998,24)claimsthatcollocationsmake up the lions share of the phraseme inventory, and a recent study referred in (Pearce, 2001) showed that each sentence is likely to contain at least one collocation." ></td>
	<td class="line x" title="12:160	Collocationalinformationisnotonlyuseful, but also indispensable in many applications." ></td>
	<td class="line x" title="13:160	In machinetranslation, forinstance, itisconsidered the key to producing more acceptable output (Orliac and Dillinger, 2003, 292)." ></td>
	<td class="line x" title="14:160	This article presents a system that extracts accurate collocational information from corpora by using a syntactic parser that supports several languages." ></td>
	<td class="line x" title="15:160	After describing the underlying methodology (section 2), we report several extraction results for English, French, Spanish and Italian (section3)." ></td>
	<td class="line x" title="16:160	Thenwepresentinsections4and5acomparative evaluation experiment proving that a hybrid approach leads to more accurate results than a classical approach in which syntactic information is not taken into account." ></td>
	<td class="line x" title="17:160	2 Hybrid Collocation Extraction We consider that syntactic analysis of source corpora is an inescapable precondition for collocation extraction, and that the syntactic structure of source text has to be taken into account in order to ensure the quality and interpretability of results." ></td>
	<td class="line x" title="18:160	1To put it simply, collocations are non-idiomatical, but restricted, conventional lexical combinations." ></td>
	<td class="line x" title="19:160	953 Asamatteroffact, someoftheexistingcollocation extraction systems already employ (but only to a limited extent) linguistic tools in order to support the collocation identification in text corpora." ></td>
	<td class="line x" title="20:160	Forinstance, lemmatizersareoftenusedforrecognizing all the inflected forms of a lexical item, and POS taggers are used for ruling out certain categories of words, e.g., in (Justeson and Katz, 1995)." ></td>
	<td class="line o" title="21:160	Syntactic analysis has long since been recognized as a prerequisite for collocation extraction (for instance, by Smadja2), but the traditional systems simply ignored it because of the lack, at that time, of efficient and robust parsers required for processing large corpora." ></td>
	<td class="line x" title="22:160	Oddly enough, this situation is nowadays perpetuated, in spite of the dramatic advances in parsing technology." ></td>
	<td class="line x" title="23:160	Only a few exceptions exists, e.g., (Lin, 1998; Krenn and Evert, 2001)." ></td>
	<td class="line x" title="24:160	One possible reason for this might be the way that collocations are generally understood, as a purely statistical phenomenon." ></td>
	<td class="line x" title="25:160	Some of the bestknown definitions are the following: Collocations of a given word are statements of the habitual and customary places of that word (Firth, 1957, 181); arbitrary and recurrent word combination (Benson, 1990); or sequences of lexical items that habitually co-occur (Cruse, 1986, 40)." ></td>
	<td class="line x" title="26:160	Mostoftheauthorsmakenoclaimswithrespectto the grammatical status of the collocation, although this can indirectly inferred from the examples they provide." ></td>
	<td class="line x" title="27:160	On the contrary, other definitions state explicitly that a collocation is an expression of language: co-occurrence of two or more lexical items as realizations of structural elements within a given syntactic pattern (Cowie, 1978); a sequence of two or more consecutive words, that has characteristics of a syntactic and semantic unit (Choueka, 1988)." ></td>
	<td class="line x" title="28:160	Our approach is committed to these later definitions, hence the importance we lend to using appropriate extraction methodologies, based on syntactic analysis." ></td>
	<td class="line x" title="29:160	The hybrid method we developed relies on the parser Fips (Wehrli, 2004), that implements the Government and Binding formalism and supports several languages (besides the ones mentioned in 2Ideally, in order to identify lexical relations in a corpus one would need to first parse it to verify that the words are used in a single phrase structure." ></td>
	<td class="line x" title="30:160	However, in practice, freestyle texts contain a great deal of nonstandard features over which automatic parsers would fail." ></td>
	<td class="line oc" title="31:160	This fact is being seriously challenged by current research (), and might not be true in the near future (Smadja, 1993, 151)." ></td>
	<td class="line x" title="32:160	the abstract, a few other are also partly dealt with)." ></td>
	<td class="line x" title="33:160	We will not present details about the parser here; what is relevant for this paper is the type of syntactic structures it uses." ></td>
	<td class="line x" title="34:160	Each constituent is represented by a simplified X-bar structure (without intermediate level), in which to the lexical head is attached a list of left constituents (its specifiers) and right constituents (its complements), and each of these are in turn represented by the same type of structure, recursively." ></td>
	<td class="line x" title="35:160	Generallyspeaking, acollocationextractioncan be seen as a two-stage process: I. in stage one, collocation candidates are identified from the text corpora, based on criteria which are specific to each system; II." ></td>
	<td class="line x" title="36:160	in stage two, the candidates are scored and ranked using specific association measures (a review can be found in (Manning and Schutze, 1999; Evert, 2004; Pecina, 2005))." ></td>
	<td class="line x" title="37:160	According to this description, in our approach the parser is used in the first stage of extraction, for identifying the collocation candidates." ></td>
	<td class="line x" title="38:160	A pair of lexical items is selected as a candidate only if there is a syntactic relation holding between the two items (one being the head of the current parse structure, and the other the lexical head of its specifier/complement)." ></td>
	<td class="line x" title="39:160	Therefore,thecriterionweemploy for candidate selection is the syntactic proximity, as opposed to the linear proximity used by traditional, window-based methods." ></td>
	<td class="line x" title="40:160	As the parsing goes on, the syntactic word pairs are extracted from the parse structures created, from each head-specifier or head-complement relation." ></td>
	<td class="line x" title="41:160	The pairs obtained are then partitioned according to their syntactic configuration (e.g. , noun + adjectival or nominal specifier, noun + argument, noun + adjective in predications, verb + adverbial specifier, verb + argument (subject, object), verb + adjunt, etc)." ></td>
	<td class="line x" title="42:160	Finally, the loglikelihood ratios test (henceforth LLR) (Dunning, 1993) is applied on each set of pairs." ></td>
	<td class="line x" title="43:160	We call this method hybrid, since it combines syntactic and statistical information (about word and cooccurrence frequency)." ></td>
	<td class="line x" title="44:160	The following examples  which, like all the examples in this paper, are actual extraction results  demonstrate the potential of our system to detect collocation candidates, even if subject to complex syntactic transformations." ></td>
	<td class="line x" title="45:160	954 1.a) raise question: The question of political leadership has been raised several times by previous speakers." ></td>
	<td class="line x" title="46:160	1.b) play role: What role can Canadas immigration program play in helping developing nations ? 1.c) make mistake: We could look back and probably see a lot of mistakes that all parties including Canada perhaps may have made." ></td>
	<td class="line x" title="47:160	3 Multilingual Extraction Results In this section, we present several extraction results obtained with the system presented in section 2." ></td>
	<td class="line x" title="48:160	The experiments were performed on data in the four languages, and involved the following corpora: for English and French, a subpart or the HansardCorpusofproceedingsfromtheCanadian Parliament; for Italian, documents from the Swiss Parliament; and for Spanish, a news corpus distributed by the Linguistic Data Consortium." ></td>
	<td class="line x" title="49:160	Some statistics on these corpora, some processing details and quantitative results are provided in Table 1." ></td>
	<td class="line x" title="50:160	The first row lists the corpora size (in tokens); the next three rows show some parsing statistics3, and the last rows display the number of collocation candidates extracted and of candidates for which the LLR score could be computed4." ></td>
	<td class="line x" title="51:160	Statistics English French Spanish Italian tokens 3509704 1649914 1023249 287804 sentences 197401 70342 67502 12008 compl." ></td>
	<td class="line x" title="52:160	parse 139498 50458 13245 4511 avg." ></td>
	<td class="line x" title="53:160	length 17.78 23.46 15.16 23.97 pairs 725025 370932 162802 58258 (extracted) 276670 147293 56717 37914 pairs 633345 308410 128679 47771 (scored) 251046 131384 49495 30586 Table 1: Extraction statistics In Table 2 we list the top collocations (of length two) extracted for each language." ></td>
	<td class="line x" title="54:160	We do not specificallydiscussheremultilingual issuesincollocation extraction; these are dealt with in a separate paper (Seretan and Wehrli, 2006)." ></td>
	<td class="line x" title="55:160	3The low rate of completely parsed sentences for Spanish and Italian are due to the relatively reduced coverage of the parsers of these two languages (under development)." ></td>
	<td class="line x" title="56:160	However, even if a sentence is not assigned a complete parse tree, some syntactic pairs can still be collected from the partial parses." ></td>
	<td class="line x" title="57:160	4The log-likelihood ratios score is undefined for those pairs having a cell of the contingency table equal to 0." ></td>
	<td class="line x" title="58:160	Language Key1 Key2 LLR score English federal government 7229.69 reform party 6530.69 house common 6006.84 minister finance 5829.05 acting speaker 5551.09 red book 5292.63 create job 4131.55 right Hon 4117.52 official opposition 3640.00 deputy speaker 3549.09 French premier ministre 4317.57 bloc quebecois 3946.08 discours trone 3894.04 verificateur general 3796.68 parti reformiste 3615.04 gouvernement federal 3461.88 missile croisi`ere 3147.42 Chambre commune 3083.02 livre rouge 2536.94 secretaire parlementaire 2524.68 Spanish banco central 4210.48 millon dolar 3312.68 millon peso 2335.00 libre comercio 2169.02 nuevo peso 1322.06 tasa interes 1179.62 deuda externo 1119.91 camara representante 1015.07 asamblea ordinario 992.85 papel comercial 963.95 Italian consiglio federale 3513.19 scrivere consiglio 594.54 unione europeo 479.73 servizio pubblico 452.92 milione franco 447.63 formazione continuo 388.80 iniziativa popolare 383.68 testo interpellanza 377.46 punto vista 373.24 scrivere risposta 348.77 Table 2: Top ten collocations extracted for each language The collocation pairs obtained were further processed with a procedure of long collocations extraction described elsewhere (Seretan et al. , 2003)." ></td>
	<td class="line x" title="59:160	Some examples of collocations of length 3, 4 and 5 obtained are: minister of Canadian heritage, house proceed to statement by, secretary to leader of gouvernment in house of common (En), question adresser `a ministre, programme de aide `a renovation residentielle, agent employer force susceptible causer (Fr), bolsa de comercio local, peso en cuota de fondo de inversion, permitir uso de papel de deuda esterno (Sp), consiglio federale disporre, creazione di nuovo posto di lavoro, costituire fattore penalizzante per regione (It)5." ></td>
	<td class="line x" title="60:160	5Note that the output of the procedure contains lemmas rather than inflected forms." ></td>
	<td class="line x" title="61:160	955 4 Comparative Evaluation Hypotheses 4.1 Does Parsing Really Help?" ></td>
	<td class="line x" title="62:160	Extractingcollocationsfromrawtext, withoutpreprocessing the source corpora, offers some clear advantages over linguistically-informed methods such as ours, which is based on the syntactic analysis: speed (in contrast, parsing large corpora of texts is expected to be much more time consuming), robustness (symbolic parsers are often not robust enough for processing large quantities of data), portability (no need to a priori define syntactic configurations for collocations candidates)." ></td>
	<td class="line x" title="63:160	On the other hand, these basic systems suffer from the combinatorial explosion if the candidate pairs are chosen from a large search space." ></td>
	<td class="line x" title="64:160	To cope with this problem, a candidate pair is usually chosen so that both words are inside a context (collocational) window of a small length." ></td>
	<td class="line x" title="65:160	A 5word window is the norm, while longer windows prove impractical (Dias, 2003)." ></td>
	<td class="line x" title="66:160	It has been argued that a window size of 5 is actually sufficient for capturing most of the collocational relations from texts in English." ></td>
	<td class="line x" title="67:160	But there is no evidence sustaining that the same holds for other languages, like German or the Romance ones that exhibit freer word order." ></td>
	<td class="line x" title="68:160	Therefore, as window-based systems miss the long-distance pairs, their recall is presumably lower than that of parse-based systems." ></td>
	<td class="line x" title="69:160	However, the parser could also miss relevant pairs due to inherent analysis errors." ></td>
	<td class="line x" title="70:160	As for precision, the window systems are susceptible to return more noise, produced by the grammatically unrelated pairs inside the collocational window." ></td>
	<td class="line x" title="71:160	By dividing the number of grammatical pairs by the total number of candidates considered, we obtain the overall precision with respecttogrammaticality; thisresultisexpectedto be considerably worse in the case of basic method than for the parse-based methods, just by virtue of the parsing task." ></td>
	<td class="line x" title="72:160	As for the overall precision with respect to collocability, we expect the proportional figures to be preserved." ></td>
	<td class="line x" title="73:160	This is because the parser-based methods return less, but better pairs (i.e. , only the pairs identified as grammatical), and because collocations are a subset of the grammatical pairs." ></td>
	<td class="line x" title="74:160	Summing up, the evaluation hypothesis that can be stated here is the following: parse-based methods outperform basic methods thanks to a drastic reduction of noise." ></td>
	<td class="line x" title="75:160	While unquestionable under the assumption of perfect parsing, this hypothesis has to be empirically validated in an actual setting." ></td>
	<td class="line x" title="76:160	4.2 Is More Data Better Than Better Data?" ></td>
	<td class="line x" title="77:160	The hypothesis above refers to the overall precision and recall, that is, relative to the entire list of selected candidates." ></td>
	<td class="line x" title="78:160	One might argue that these numbers are less relevant for practice than they arefromatheoretical(evaluation)perspective, and that the exact composition of the list of candidates identified is unimportant if only the top results (i.e. , those pairs situated above a threshold) are looked at by a lexicographer or an application." ></td>
	<td class="line x" title="79:160	Considering a threshold for the n-best candidates works very much in the favor of basic methods." ></td>
	<td class="line x" title="80:160	As the amount of data increases, there is a reduction of the noise among the best-scored pairs, which tend to be more grammatical because the likelihood of encountering many similar noisy pairs is lower." ></td>
	<td class="line x" title="81:160	However, as the following example shows, noisy pairs may still appear in top, if they occur often in a longer collocation: 2.a) les essais du missile de croisi`ere 2.b) essai croisi`ere The pair essai croisi`ere is marked by the basic systems as a collocation because of the recurrent association of the two words in text as part or the longer collocation essai du missile de croisi`ere." ></td>
	<td class="line x" title="82:160	It is an grammatically unrelated pair, while the correct pairs reflecting the right syntactic attachment are essai missile and missile (de) croisi`ere." ></td>
	<td class="line x" title="83:160	We mentioned that parsing helps detecting the long-distance pairs that are outside the limits of the collocational window." ></td>
	<td class="line x" title="84:160	Retrieving all such complex instances (including all the extraposition cases) certainly augment the recall of extraction systems, but this goal might seem unjustified, because the risk of not having a collocation represented at all diminishes as more and more data is processed." ></td>
	<td class="line x" title="85:160	One might think that systematically missing long-distance pairs might be very simply compensated by supplying the system with more data, and thus that larger data is a valid alternative to performing complex processing." ></td>
	<td class="line x" title="86:160	While we agree that the inclusion of more data compensates for the difficult cases, we do consider this truly helpful in deriving collocational information, for the following reasons: (1) more data means more noise for the basic methods; (2) some collocations might systematically appear in 956 a complex grammatical environment (such as passive constructions or with additional material inserted between the two items); (3) more importantly, the complex cases not taken into account alter the frequency profile of the pairs concerned." ></td>
	<td class="line x" title="87:160	These observations entitle us to believe that, evenwhenmoredataisadded, the n-bestprecision might remain lower for the basic methods with respect to the parse-based ones." ></td>
	<td class="line x" title="88:160	4.3 How Real the Counts Are?" ></td>
	<td class="line x" title="89:160	Syntactic analysis (including shallower levels of linguistic analysis traditionally used in collocation extraction, suchaslemmatization, POStagging, or chunking) has two main functions." ></td>
	<td class="line x" title="90:160	On the one hand, it guides the extraction system in the candidate selection process, in order to better pinpoint the pairs that might form collocations and to exclude the ones considered as inappropriate (e.g. , the pairs combining function words, such as a preposition followed by a determiner)." ></td>
	<td class="line x" title="91:160	On the other, parsing supports the association measures that will be applied on the selected candidates, by providing more exact frequency information on words  the inflected forms count as instances of the same lexical item  and on their co-occurrence frequency  certain pairs might count as instance of the same pair, others do not." ></td>
	<td class="line x" title="92:160	In the following example, the pair loi modifier is an instance of a subject-verb collocation in 3.a), and of a verb-object collocation type in 3.b)." ></td>
	<td class="line x" title="93:160	Basic methods are unable to distinguish between the two types, and therefore count them as equivalent." ></td>
	<td class="line x" title="94:160	3.a) Loi modifiant la Loi sur la responsabilite civile 3.b) la loi devrait etre modifiee Parsing helps to create a more realistic frequency profile for the candidate pairs, not only because of the grammaticality constraint it applies on the pairs (wrong pairs are excluded), but also because it can detect the long-distance pairs that are outside the collocational window." ></td>
	<td class="line x" title="95:160	Given that the association measures rely heavily on the frequency information, the erroneous counts have a direct influence on the ranking of candidates and, consequently, on the top candidates returned." ></td>
	<td class="line x" title="96:160	We believe that in order to achieve a good performance, extraction systems should be as close as possible to the real frequency counts and, of course, to the real syntactic interpretation provided in the source texts6." ></td>
	<td class="line x" title="97:160	Since parser-based methods rely on more accurate frequency information for words and their cooccurrence than window methods, it follows that the n-best list obtained with the first methods will probably show an increase in quality over the second." ></td>
	<td class="line x" title="98:160	To conclude this section, we enumerate the hypotheses that have been formulated so far: (1) Parse methods provide a noise-freer list of collocation candidates, in comparison with the window methods; (2) Local precision (of best-scored results) with respect to grammaticality is higher for parse methods, since in basic methods some noise still persists, even if more data is included; (3) Local precision with respect to collocability is higher for parse methods, because they use a more realistic image of word co-occurrence frequency." ></td>
	<td class="line x" title="99:160	5 Comparative Evaluation We compare our hybrid method (based on syntactic processing of texts) against the window method classically used in collocation extraction, from the point of view of their precision with respect to grammaticality and collocability." ></td>
	<td class="line x" title="100:160	5.1 The Method The n-best extraction results, for a given n (in our experiment, n varies from 50 to 500 at intervals of 50) are checked in each case for grammatical well-formedness and for lexicalization." ></td>
	<td class="line x" title="101:160	By lexicalization we mean the quality of a pair to constitute (part of) a multi-word expression  be it compound, collocation, idiom or another type of syntagmatic lexical combination." ></td>
	<td class="line x" title="102:160	We avoid giving collocability judgments since the classification of multi-word expressions cannot be made precisely and with objective criteria (McKeown and Radev, 2000)." ></td>
	<td class="line x" title="103:160	We rather distinguish between lexicalizable and trivial combinations (completely regular productions, such as big house, buy bread, that do not deserve a place in the lexicon)." ></td>
	<td class="line x" title="104:160	As in (Choueka, 1988) and (Evert, 2004), we consider that a dominant feature of collocations is that they are unpredictable for speakers and therefore have to be stored into a lexicon." ></td>
	<td class="line x" title="105:160	6To exemplify this point: the pair developpement humain (which has been detected as a collocation by the basic method) looks like a valid expression, but the source text consistently offers a different interpretation: developpement des ressources humaines." ></td>
	<td class="line x" title="106:160	957 Each collocation from the n-best list at the different levels considered is therefore annotated with one of the three flags: 1." ></td>
	<td class="line x" title="107:160	ungrammatical; 2." ></td>
	<td class="line x" title="108:160	trivial combination; 3." ></td>
	<td class="line x" title="109:160	multi-word expression (MWE)." ></td>
	<td class="line x" title="110:160	On the one side, we evaluate the results of our hybrid, parse-based method; on the other, we simulate a window method, by performing the following steps: POS-tag the source texts; filter the lexical items and retain only the open-class POS; consider all their combinations within a collocational window of length 5; and, finally, apply the log-likelihood ratios test on the pairs of each configuration type." ></td>
	<td class="line x" title="111:160	In accordance with (Evert and Kermes, 2003), we consider that the comparative evaluation of collocation extraction systems should not be done at the end of the extraction process, but separately for each stage: after the candidate selection stage, for evaluating the quality (in terms of grammaticality) of candidates proposed; and after the application of collocability measures, for evaluating the measures applied." ></td>
	<td class="line x" title="112:160	In each of these cases, different evaluation methodologies and resources are required." ></td>
	<td class="line x" title="113:160	In our case, since we used the same measure for the second stage (the log-likelihood ratios test), we could still compare the final output of basic and parse-based methods, as given by the combination of the first stage with the same collocability measure." ></td>
	<td class="line x" title="114:160	Again, similarly to Krenn and Evert (2001), we believe that the homogeneity of data is important for the collocability measures." ></td>
	<td class="line x" title="115:160	We therefore applied the LLR test on our data after first partitioning it into separate sets, according to the syntactical relation holding in each candidate pair." ></td>
	<td class="line x" title="116:160	As the data used in the basic method contains no syntacticinformation, thepartitioningwasdonebasedon POS-combination type." ></td>
	<td class="line x" title="117:160	5.2 The Data The evaluation experiment was performed on the whole French corpus used in the extraction experiment (section 2), that is, a subpart of the Hansard corpus of Canadian Parliament proceedings." ></td>
	<td class="line x" title="118:160	It contains 112 text files totalling 8.43 MB, with an average of 628.1 sentences/file and 23.46 tokens/sentence (as detected by the parser)." ></td>
	<td class="line x" title="119:160	The total number of tokens is 1, 649, 914." ></td>
	<td class="line x" title="120:160	On the one hand, the texts were parsed and 370, 932 candidate pairs were extracted using the hybrid method we presented." ></td>
	<td class="line x" title="121:160	Among the pairs extracted, 11.86% (44, 002 pairs) were multi-word expressions identified at parse-time, since present in the parsers lexicon." ></td>
	<td class="line x" title="122:160	The log-likelihood ratios test was applied on the rest of pairs." ></td>
	<td class="line x" title="123:160	A score could be associated to 308, 410 of these pairs (corresponding to 131, 384 types); for the others, the score was undefined." ></td>
	<td class="line x" title="124:160	On the other hand, the texts were POS-tagged using the same parser as in the first case." ></td>
	<td class="line x" title="125:160	If in the first case the candidate pairs were extracted during the parsing, in the second they were generated after the open-class filtering." ></td>
	<td class="line x" title="126:160	From 673, 789 POSfiltered tokens, a number of 1, 024, 888 combinations (560, 073 types) were created using the 5length window criterion, while taking care not to cross a punctuation mark." ></td>
	<td class="line x" title="127:160	A score could be associated to 1, 018, 773 token pairs (554, 202 types), which means that the candidate list is considerably larger than in the first case." ></td>
	<td class="line x" title="128:160	The processing time was more than twice longer than in the first case, because of the large amount of data to handle." ></td>
	<td class="line x" title="129:160	5.3 Results The 500 best-scored collocations retrieved with the two methods were manually checked by three human judges and annotated, as explained in 5.1, as either ungrammatical, trivial or MWE." ></td>
	<td class="line x" title="130:160	The agreement statistics on the annotations for each method are shown in Table 3." ></td>
	<td class="line x" title="131:160	Method Agr." ></td>
	<td class="line x" title="132:160	1,2,3 1,2 1,3 2,3 parse observed 285 365 362 340 k-score 55.4% 62.6% 69% 64% window observed 226 339 327 269 k-score 43.1% 63.8% 61.1% 48% Table 3: Inter-annotator agreement For reporting n-best precision results, we used as reference set the annotated pairs on which at least two of the three annotators agreed." ></td>
	<td class="line x" title="133:160	That is, from the 500 initial pairs retrieved with each method, 497 pairs were retained in the first case (parse method), and 483 pairs in the second (window method)." ></td>
	<td class="line x" title="134:160	Table 4 shows the comparative evaluation results for precision at different levels in the list of best-scored pairs, both with respect to grammaticality and to collocability (or, more exactly, the potential of a pair to constitute a MWE)." ></td>
	<td class="line x" title="135:160	The numbers show that a drastic reduction of noise is achieved by parsing the texts." ></td>
	<td class="line x" title="136:160	The error rate with 958 Precision (gram)." ></td>
	<td class="line x" title="137:160	Precision (MWE) n window parse window parse 50 94.0 96.0 80.0 72.0 100 91.0 98.0 75.0 74.0 150 87.3 98.7 72.7 73.3 200 85.5 98.5 70.5 74.0 250 82.8 98.8 67.6 69.6 300 82.3 98.7 65.0 69.3 350 80.3 98.9 63.7 67.4 400 80.0 99.0 62.5 67.0 450 79.6 99.1 61.1 66.0 500 78.3 99.0 60.1 66.0 Table 4: Comparative evaluation results respect to grammaticality is, on average, 15.9% for the window method; with parsing, it drops to 1.5% (i.e. , 10.6 times smaller)." ></td>
	<td class="line x" title="138:160	This result confirms our hypothesis regarding the local precision which was stated in section 4.2." ></td>
	<td class="line x" title="139:160	Despite the inherent parsing errors, the noise reduction is substantial." ></td>
	<td class="line x" title="140:160	It is also worth noting that we compared our method against a rather high baseline, as we made a series of choices susceptible to alleviate the candidates identification with the window-based method: we filtered out function words, we used a parser for POS-tagging (that eliminated POS-ambiguity), and we filtered out cross-punctuation pairs." ></td>
	<td class="line x" title="141:160	As for the MWE precision, the window method performs better for the first 100 pairs7); on the remaining part, the parsing-based method is on average 3.7% better." ></td>
	<td class="line x" title="142:160	The precision curve for the window method shows a more rapid degradation than it does for the other." ></td>
	<td class="line x" title="143:160	Therefore we can conclude that parsing is especially advantageous if one investigates more that the first hundred results (as it seems reasonable for large extraction experiments)." ></td>
	<td class="line x" title="144:160	In spite of the rough classification we used in annotation, we believe that the comparison performed is nonetheless meaningful since results should be first checked for grammaticality and triviality before defining more difficult tasks such as collocability." ></td>
	<td class="line x" title="145:160	6 Conclusion Inthispaper, weprovidedboththeoreticalandempirical arguments in the favor of performing syntactic analysis of texts prior to the extraction of collocations with statistical methods." ></td>
	<td class="line x" title="146:160	7A closer look at the data revealed that this might be explained by some inconsistencies between annotations." ></td>
	<td class="line x" title="147:160	Part of the extraction work that, like ours, relies on parsing was cited in section 2." ></td>
	<td class="line x" title="148:160	Most often, it concerns chunking rather than complete parsing; specific syntactic configurations (such as adjective-noun, preposition-noun-verb); and languages other than the ones we deal with (usually, English and German)." ></td>
	<td class="line oc" title="149:160	Parsing has been also used after extraction (Smadja, 1993) for filtering out invalid results." ></td>
	<td class="line n" title="150:160	We believe that this is not enough and that parsing is required prior to the application of statistical tests, for computing a realistic frequency profile for the pairs tested." ></td>
	<td class="line x" title="151:160	As for evaluation, unlike most of the existing work, we are not concerned here with comparing the performance of association measures (cf.(Evert, 2004; Pecina, 2005) for comprehensive references), but with a contrastive evaluation of syntactic-based and standard extraction methods, combined with the same statistical computation." ></td>
	<td class="line x" title="153:160	Our study finally clear the doubts on the usefulness of parsing for collocation extraction." ></td>
	<td class="line x" title="154:160	Previous work that quantified the influence of parsing onthequalityofresultssuggestedtheperformance for tagged and parsed texts is similar (Evert and Kermes, 2003)." ></td>
	<td class="line x" title="155:160	This result applies to a quite rigid syntactic pattern, namely adjective-noun in German. But a preceding study on noun-verb pairs (Breidt, 1993) came to the conclusion that good precision can only be achieved for German with parsing." ></td>
	<td class="line x" title="156:160	Its author had to simulate parsing because of the lack, at the time, of parsing tools for German. Our report, that concerns an actual system and a large data set, validates Breidts finding for a new language (French)." ></td>
	<td class="line x" title="157:160	Our experimental results confirm the hypotheses put forth in section 4, and show that parsing (even if imperfect) benefits to extraction, notably by a drastic reduction of the noise in the top of the significance list." ></td>
	<td class="line x" title="158:160	In future work, we consider investigating other levels of the significance list, extending the evaluation to other languages, comparing against shallow-parsing methods instead of the window method, and performing recall-based evaluation as well." ></td>
	<td class="line x" title="159:160	Acknowledgements We would like to thank Jorge Antonio Leoni de Leon, Mar Ndiaye, Vincenzo Pallotta and Yves Scherrer for participating to the annotation task." ></td>
	<td class="line x" title="160:160	We are also grateful to Gabrielle Musillo and to the anonymous reviewers of an earlier version of 959 this paper for useful comments and suggestions." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-1006
Multilingual Collocation Extraction: Issues And Solutions
Seretan, Violeta;Wehrli, Eric;"></td>
	<td class="line x" title="1:167	Proceedings of the Workshop on Multilingual Language Resources and Interoperability, pages 4049, Sydney, July 2006." ></td>
	<td class="line x" title="2:167	c2006 Association for Computational Linguistics MultilingualCollocationExtraction:IssuesandSolutions VioletaSeretan LanguageTechnologyLaboratory UniversityofGeneva 2,ruedeCandolle,1211Geneva Violeta.Seretan@latl.unige.ch EricWehrli LanguageTechnologyLaboratory UniversityofGeneva 2,ruedeCandolle,1211Geneva Eric.Wehrli@latl.unige.ch Abstract Althoughtraditionallyseenasa languageindependenttask, collocation extraction relies nowadays more and more on the linguistic preprocessing of texts (e.g. , lemmatization,POStagging,chunkingor parsing) prior to the applicationof statistical measures." ></td>
	<td class="line x" title="3:167	This paper provides a language-orientedreview of the existing extraction work." ></td>
	<td class="line x" title="4:167	It points out severallanguage-specificissuesrelatedtoextractionand proposesa strategy for coping withthem." ></td>
	<td class="line x" title="5:167	It thendescribesa hybrid extractionsystembasedon a multilingual parser." ></td>
	<td class="line x" title="6:167	Finally, it presentsa case-studyon theperformanceofanassociationmeasure acrossa numberoflanguages." ></td>
	<td class="line x" title="7:167	1 Introduction Collocationsare understoodin this paperas idiosyncratic syntagmatic combination of lexical items(Fontenelle,1992,222): heavyrain, light breeze, great difficulty, grow steadily, meet requirement, reach consensus, pay attention, ask a question." ></td>
	<td class="line x" title="8:167	Unlike idioms(kick the bucket, lend a hand, pullsomeones leg), theirmeaningis fairly transparentand easy to decode." ></td>
	<td class="line x" title="9:167	Yet, differently fromtheregularproductions,(bighouse, cultural activity, read a book), collocationalexpressions are highly idiosyncratic,since the lexical items a headword combineswith in order to express a given meaning is contingentupon that word (Melcuk,2003)." ></td>
	<td class="line x" title="10:167	This is apparent when comparinga collocations equivalentsacrossdifferentlanguages.The English collocationask a questiontranslatesas poserunequestioninFrench(lit.,?putaquestion), and as fare una domanda, haceruna preguntain ItalianandSpanish(lit.,tomake a question)." ></td>
	<td class="line x" title="13:167	Asit hasbeenpointedoutbymany researchers (Cruse, 1986; Benson, 1990; McKeown and Radev, 2000), collocationscannot be described bymeansof generalsyntacticandsemanticrules." ></td>
	<td class="line x" title="14:167	They are arbitraryand unpredictable,and thereforeneedtobememorizedandusedassuch.They constitutethe so-calledsemi-finishedproducts of language(Hausmann,1985)or the islandsof reliability(Lewis, 2000)on whichthe speakers buildtheirutterances." ></td>
	<td class="line x" title="15:167	2 Motivation The key importanceof collocationsin text productiontaskssuchasmachinetranslationandnatural languagegenerationhas beenstressedmany times.It hasbeenequallyshownthatcollocations areusefulina rangeofotherapplications,suchas word sense disambiguation(Brown et al. , 1991) andparsing(AlshawiandCarter, 1994)." ></td>
	<td class="line x" title="16:167	The NLP communityfully acknowledgedthe need for an appropriatetreatmentof multi-word expressionsin general(Sag et al. , 2002)." ></td>
	<td class="line x" title="17:167	Collocationsareparticularlyimportantbecauseof their prevalencein language,regardlessof the domain or genre." ></td>
	<td class="line x" title="18:167	Accordingto Jackendoff (1997, 156) and Melcuk (1998, 24), collocationsconstitute thebulkofa languages lexicon." ></td>
	<td class="line x" title="19:167	Thelastdecadeshave witnesseda considerable developmentof collocationextraction techniques, thatconcernbothmonolingualand(parallel)multilingualcorpora." ></td>
	<td class="line oc" title="20:167	We can mentionhere only part of this work: (Berry-Rogghe, 1973; Church et al. , 1989; Smadja,1993;Lin,1998;KrennandEvert,2001) for monolingualextraction, and (Kupiec, 1993; Wu,1994;Smadjaetal.,1996;KitamuraandMat40 sumoto,1996; Melamed,1997)for bilingualextractionviaalignment." ></td>
	<td class="line x" title="22:167	Traditionally, collocationextractionwas considereda language-independenttask." ></td>
	<td class="line x" title="23:167	Sincecollocationsarerecurrent,typicallexicalcombinations, a widerangeofstatisticalmethodsbasedonword co-occurrencefrequency have been heavily used for detectingthem in text corpora." ></td>
	<td class="line x" title="24:167	Amongthe mostoftenusedtypesof lexicalassociationmeasures (henceforth AMs) we mention: statistical hypothesistests(e.g. ,binomial,Poisson,Fisher,zscore,chi-squared,t-score,andlog-likelihoodratiotests),thatmeasurethesignificanceoftheassociationbetweentwowordsbasedonacontingency table listing their joint and marginal frequency, and Information-theoretic measures (Mutual Information henceforthMI  and its variants), that quantityof informationsharedby two randomvariables.A detailedreview of thestatistical methodsemployedincollocationextractioncanbe found,for instance,in (Evert, 2004)." ></td>
	<td class="line x" title="25:167	A comprehensive listofAMsisgiven(Pecina,2005)." ></td>
	<td class="line x" title="26:167	Veryoften,inadditiontotheinformationoncooccurrencefrequency, language-specificinformation is also integratedin a collocationextraction system(asit willbeseeninsection3): morphologicalinformation,inordertocount inflectedwordformsasinstancesofthesame baseform." ></td>
	<td class="line x" title="27:167	For instance,ask questions, asks question, asked questionare all instancesof thesamewordpair, askquestion; syntacticinformation,inordertorecognizea wordpairevenifsubjectto(complex)syntactic transformations:ask multiplequestions, questionasked, questionsthatonemightask." ></td>
	<td class="line x" title="28:167	Thelanguage-specificmodulesthusaimatcoping with the problemof morphosyntacticvariation,inordertoimprovetheaccuracyoffrequency information.Thisbecomestrulyimportantespeciallyfor free-word orderand for high-inflection languages,for which the token(form)-basedfrequencyfiguresbecometooskewedduetothehigh lexical dispersion." ></td>
	<td class="line x" title="29:167	Not only the data scattering modifythe frequency numbersusedby AMs,but it also altersthe performanceof AMs, if the the probabilitiesinthecontingencytablebecomevery low." ></td>
	<td class="line pc" title="30:167	Morphosyntacticinformationhas in fact been shown to significantlyimprove the extractionresults (Breidt, 1993; Smadja, 1993; Zajac et al. , 2003)." ></td>
	<td class="line oc" title="31:167	Morphologicaltoolssuch as lemmatizers andPOStaggersarebeingcommonlyusedin extractionsystems;they areemployedbothfordealingwithtext variationandfor validatingthe candidatepairs: combinationsof functionwordsare typicallyruledout (Justesonand Katz, 1995),as are the ungrammaticalcombinationsin the systemsthatmake useofparsers(ChurchandHanks, 1990;Smadja,1993;Basilietal.,1994;Lin,1998; Goldmanetal.,2001;Seretanetal.,2004)." ></td>
	<td class="line oc" title="34:167	Given the motivations for performing a linguistically-informedextraction whichwere also put forth, among others, by Church and Hanks(1990,25), Smadja(1993,151) and Heid (1994)  and given the recent developmentof linguisticanalysistools,itseemsplausiblethatthe linguisticstructurewill be more and more taken intoaccountbycollocationextractionsystems." ></td>
	<td class="line x" title="35:167	Therestofthepaperisorganizedasfollows." ></td>
	<td class="line x" title="36:167	In section3 we provide a language-orientedreview of the existingcollocationextractionwork." ></td>
	<td class="line x" title="37:167	Then wehighlight,insection4,aseriesofproblemsthat arisein thetransferof methodologyto a new language,andweproposea strategyfordealingwith them." ></td>
	<td class="line x" title="38:167	Section5 describesan extractionsystem, and,finally, section6 presentsa case-studyonthe collocationsextractedforfourlanguages,illustratingthecross-lingualvariationin theperformance ofa particularAM." ></td>
	<td class="line oc" title="39:167	3 OverviewofExtractionWork 3.1 English As one mightexpect,the bulk of the collocation extractionwork concernsthe English language: (Choueka,1988;Churchet al. ,1989;Churchand Hanks,1990; Smadja,1993; Justesonand Katz, 1995;Kjellmer, 1994;Sinclair, 1995;Lin,1998), amongmany others1." ></td>
	<td class="line x" title="40:167	Chouekas method(1988)detectsn-grams(adjacentwords)only, by simplycomputingthe cooccurrencefrequency." ></td>
	<td class="line x" title="41:167	Justesonand Katz (1995) applya POS-filteronthepairsthey extract.Asin (Kjellmer, 1994),the AM they use is the simple frequency." ></td>
	<td class="line oc" title="42:167	Smadja(1993)employsthez-scoreinconjunction with several heuristics(e.g. , the systematic occurrenceof two lexical items at the same distanceintext)andextractspredicativecollocations, 1E.g.,(Frantziet al. ,2000;Pearce,2001;Goldmanet al. , 2001;ZaiuInkpenandHirst,2002;Dias,2003;Seretanetal., 2004;Pecina,2005),andthelistcanbecontinued." ></td>
	<td class="line x" title="44:167	41 rigidnounphrasesandphrasaltemplates.Hethen uses the a parserin order to validatethe results." ></td>
	<td class="line x" title="45:167	Theparsing is shownto leadto an increasein accuracy from40%to80%." ></td>
	<td class="line x" title="46:167	(Churchet al. , 1989)and (Churchand Hanks, 1990)usePOSinformationanda parsertoextract verb-objectpairs,whichthenthey rankaccording to the mutualinformation(MI) measurethey introduce." ></td>
	<td class="line x" title="47:167	Lins(1998)isalsoahybridapproachthatrelies ona dependency parser." ></td>
	<td class="line x" title="48:167	Thecandidatesextracted arethenrankedwithMI." ></td>
	<td class="line x" title="49:167	3.2 German Germanis thesecondmostinvestigatedlanguage, thanks to the early work of Breidt (1993) and, morerecently, to thatof KrennandEvert,such as (Krennand Evert, 2001; Evert and Krenn,2001; Evert,2004)centeredonevaluation." ></td>
	<td class="line x" title="50:167	Breidt uses MI and t-score and comparesthe results accuracy when various parametersvary, such as the window size, presencevs." ></td>
	<td class="line x" title="51:167	absence of lemmatization,corpus size, and presencevs." ></td>
	<td class="line x" title="52:167	absenceof POS and syntacticinformation." ></td>
	<td class="line x" title="53:167	She focuses on N-V pairs2 and, despite the lack of syntacticanalysistoolsat the time,by simulating parsing she comes to the conclusionthat Very high precisionrates, which are an indispensable requirementforlexicalacquisition,canonlyrealisticallybeenvisagedforGermanwithparsedcorpora(Breidt,1993,82)." ></td>
	<td class="line x" title="54:167	Later, Krennand Evert (2001)used a German chunker to extractsyntacticpairssuchas P-N-V." ></td>
	<td class="line x" title="55:167	Their work put the basis of formal and systematic methodsin collocationextractionevaluation." ></td>
	<td class="line x" title="56:167	Zinsmeisterand Heid (2003; 2004) focused on N-V and A-N-Vcombinationsidentifiedusinga stochasticparser." ></td>
	<td class="line x" title="57:167	They appliedmachinelearning techniquesin combinationto the log-likelihood measure(henceforthLL)fordistinguishingtrivial compoundsfromlexicalizedones." ></td>
	<td class="line x" title="58:167	Finally, Wermter and Hahn (2004) identified PP-V combinationsusing a POS tagger and a chunker." ></td>
	<td class="line x" title="59:167	They basedtheirmethodon a linguistic criterion(that of limitedmodifiability)and compared their resultswith those obtainedusing the t-scoreandLLtests." ></td>
	<td class="line x" title="60:167	2Thefollowingabbreviationsare usedin thispaper: N noun,Vverb,Aadjective,Advadverb,Detdeterminer, Conjconjunction,P preposition." ></td>
	<td class="line x" title="61:167	3.3 French Thanks to the outstanding work of Gross on lexicon-grammar(1984), French is one of the moststudiedlanguagesin termsof distributional and transformationalpotential of words." ></td>
	<td class="line x" title="62:167	This workhasbeencarriedoutbeforethe computerera and the advent of corpuslinguistics,whileautomaticextractionwaslaterperformed,forinstance, in (Lafon,1984; Daille,1994; Bourigault, 1992; Goldmanetal.,2001)." ></td>
	<td class="line x" title="64:167	Daille (1994) aimed at extracting compound nouns,defineda prioriby meansof certainsyntacticpatterns,like N-A,N-N,N-`a-N,N-de-N,N PDetN.Sheuseda lemmatizeranda POS-tagger beforeapplyinga seriesof AMs,whichshe then evaluatedagainst a domain-specificterminology dictionaryand against a gold-standardmanually createdfromtheextractioncorpus." ></td>
	<td class="line x" title="65:167	Similarly, Bourigault (1992) extracted nounphrasesfromshallow-parsedtext,andGoldmanet al.(2001)extractedsyntacticcollocationsby usinga fullparserandapplyingtheLLtest." ></td>
	<td class="line x" title="67:167	3.4 OtherLanguages In additionto English,GermanandFrench,other languagesforwhichnotablecollocationextraction workwasperformed,areasweareawareof thefollowing:  Italian:earlyextractionworkwascarriedout byCalzolariandBindi(1990)andemployed MI." ></td>
	<td class="line oc" title="68:167	It was followedby (Basiliet al. , 1994), thatmadeuseofparsinginformation;  Korean:(Shimohataetal.,1997)usedanadjacencyn-grammodel,and(Kimetal.,1999) reliedonPOS-tagging;  Chinese:(Huanget al. , 2005)usedPOSinformation,while(Luetal.,2004)appliedextractiontechniquessimilarto Xtractsystem (Smadja,1993);  Japanese:(Ikeharaetal.,1995)wasbasedon animprovedn-grammethod." ></td>
	<td class="line x" title="73:167	As for multilingualextraction via alignment (wherecollocationsare first detectedin one languageand then matchedwith their translationin anotherlanguage),mostortheexistingworkconcern the English-Frenchlanguagepair, and the Hansardcorpusof CanadianParliamentproceedings." ></td>
	<td class="line x" title="74:167	Wu (1994)signalsa numberof problems 42 that non-Indo-Europeanlanguagespose for the existingalignmentmethodsbased on wordand sentence-length:in Chinese,forinstance,mostof thewordsarejustoneortwo characterslong,and thereareno worddelimiters.Thisresultsuggests thattheportabilityof existingalignmentmethods tonewlanguagepairsisquestionable." ></td>
	<td class="line x" title="75:167	We are not concernedhere with extractionvia alignment.We assume,instead,thatmultilingual supportin collocationextractionmeansthe customizationof the extraction procedurefor each language.Thistopicwillbeaddressedin thenext sections." ></td>
	<td class="line x" title="76:167	4 Multilingualism:WhyandHow?" ></td>
	<td class="line x" title="77:167	4.1 SomeIssues Astheprevioussectionshowed,many systemsof collocationextractionrely on the linguisticpreprocessingof sourcecorporain order to support the candidateidentificationprocess." ></td>
	<td class="line x" title="78:167	Languagespecificinformation,suchastheonederivedfrom morphologicalandsyntacticanalysis,was shown to be highlybeneficialfor extraction." ></td>
	<td class="line x" title="79:167	Moreover, the possibilityto applythe associationmeasures onsyntacticallyhomogenousmaterialisarguedto benefitextraction,as the performanceof association measuresmightvary withthe syntacticconfigurationsbecauseof the differencesin distribution(KrennandEvert,2001)." ></td>
	<td class="line x" title="80:167	The lexical distribution is thereforea relevant issuefromtheperspectiveofmultilingualcollocationextraction.Differentlanguagesshowdifferent proportionsof lexical categories (N, V, A, Adv, P, etc)." ></td>
	<td class="line x" title="81:167	whichare evenly distributed acrosssyntactictypes3." ></td>
	<td class="line x" title="82:167	Dependingon the frequency numbers,a given AMcouldbe moresuitedfor a specificsyntactic configurationin onelanguage,and less suitedfor the sameconfigurationin another." ></td>
	<td class="line x" title="83:167	Ideally, eachlanguageshouldbe assigneda suitable set of AMs to be appliedon syntacticallyhomogenousdata." ></td>
	<td class="line x" title="84:167	Another issue that is relevant in the multilingualism perspective is that of the syntactic configurationscharacterizingcollocations." ></td>
	<td class="line x" title="85:167	Severalsuchrelations(e.g. ,noun-adjectival modifier, predicate-argument)are likely to remainconstant throughlanguages,i.e. , to be judgedas collocationallyinterestingin many languages.However, 3For instance,V-P pairsare morerepresentedin English thaninotherlanguages(asphrasalverbsorverb-particleconstructions)." ></td>
	<td class="line x" title="86:167	other configurationscould be language-specific (like P-N-V in German, whose English equivalentisV-P-N).Yetotherconfigurationsmighthave nocounterpartat allinanotherlanguage(e.g. ,the FrenchP-Apair `a neuf is translatedintoEnglish asa Conj-Apair, asnew)." ></td>
	<td class="line x" title="87:167	Findingall the collocationally-relevant syntactictypesfora languageis thereforeanotherproblem that has to be solved in multilingualextraction." ></td>
	<td class="line x" title="88:167	Since a priori definingthese types based on intuitiondoesnot ensurethe necessarycoverage,analternativeproposalistoinducethemfrom POSdataanddependencyrelations,asin(Seretan, 2005)." ></td>
	<td class="line x" title="89:167	The morphoyntactic differences between languagesalso have to be taken into account." ></td>
	<td class="line x" title="90:167	With Englishasthemostinvestigatedlanguage,several hypotheseswere put forth in extractionand becamecommonplace." ></td>
	<td class="line x" title="91:167	Forinstance,usinga5-wordswindowassearch spaceforcollocationpairsisausualpractice,since this span lengthwas shown sufficientto cover a highpercentageofsyntacticco-occurrencesinEnglish." ></td>
	<td class="line x" title="92:167	But  as suggestedby otherresearchers, e.g., (Goldmanet al. , 2001), this assumption doesnotnecessaryholdforotherlanguages." ></td>
	<td class="line x" title="93:167	Similarly, the higherinflectionand the higher transformation potential shown by some languages pose additional problems in extraction, whichwereratherignoredforEnglish." ></td>
	<td class="line x" title="94:167	AsKimet al.(1999)notice,collocationextractionisparticularlydifficultin free-orderlanguageslike Korean, whereargumentsscramblefreely." ></td>
	<td class="line x" title="96:167	Breidt(1993) alsopointedouta coupleof problemsthatmakes extractionfor Germanmoredifficultthanfor English: the stronginflectionfor verbs,the variable word-order,andthepositionalambiguityofthearguments.Sheshowsthatevendistinguishingsubjectsfromobjectsisverydifficultwithoutparsing." ></td>
	<td class="line x" title="97:167	4.2 AStrategyforMultilingualExtraction Summing up the previous discussion, the customizationof collocationextractionfor a given languageneedstotake intoaccount: the syntactic configurationscharacterizing collocations, thelexicaldistributionover syntacticconfigurations, theadequacyofAMstotheseconfigurations." ></td>
	<td class="line x" title="98:167	43 These are language-specificparameterswhich needto be setin a successfulmultilingualextraction procedure." ></td>
	<td class="line x" title="99:167	Truly multilingualsystemshave not been developedyet, but we suggestthe followingstrategyforbuildingsucha system: A. parse the source corpus, extract all the syntactic pairs (e.g. , head-modifier, predicateargument)andrankthemwitha givenAM, B. analyzethe resultsandfindthe syntacticconfigurationscharacterizingcollocations, C. evaluatetheadequacy ofAMsforrankingcollocationsin each syntacticconfiguration,and find the most convenientmappingconfigurationsAMs." ></td>
	<td class="line x" title="100:167	Oncecustomizedfora language,theextraction procedureinvolves: Stage1." ></td>
	<td class="line x" title="101:167	parsing the source corpus for extracting the lexical pairs in the relevant, language-specific syntactic configurationsfoundinstepB; Stage2." ></td>
	<td class="line x" title="102:167	ranking the pairs from each syntactic classwiththeAMassignedinstepC." ></td>
	<td class="line x" title="103:167	5 AMultilingualCollocationExtractor BasedonParsing Ever sincethe collocationwas broughtto the attentionof linguistsin theframeworkof contextualism(Firth,1957; Firth,1968),it has beenpreponderantlyseenasa purestatisticalphenomenon oflexicalassociation.Infact,accordingtoa wellknowndefinition,acollocationisanarbitraryand recurrentwordcombination(Benson,1990)." ></td>
	<td class="line x" title="104:167	Thisapproachwas at thebasisof thecomputationalworkoncollocation,althoughthereexistan alternative approach the linguistic,or lexicographicone  that imposesa restrictedview on collocation,whichis seenfirstofallasanexpressionoflanguage." ></td>
	<td class="line x" title="105:167	Theexistingextractionwork(section3) shows that there is a growing interest in adoptingthe morerestricted(linguistic)view." ></td>
	<td class="line x" title="106:167	Asmentionedin section3,theimportanceofparsingforextraction was confirmedbyseveralevaluationexperiments." ></td>
	<td class="line x" title="107:167	Withtherecentdevelopmentinthefieldoflinguistic analysis,hybrid extractionsystems(i.e. , systems relyingon syntacticalanalysisfor collocationextraction)arelikelytobecometherulerather thantheexception." ></td>
	<td class="line x" title="108:167	Oursystem(Goldmanet al. ,2001;Seretanand Wehrli,2006)is  to our knowledge the first toperformthefullsyntacticanalysisassupportfor collocationextraction;similarapproachesrelyon dependency parsersoronchunking." ></td>
	<td class="line x" title="109:167	It is based on a symbolicparser that was developedover the last decade(Wehrli, 2004)and achieves a highlevel of performance,in termsof accuracy, speedandrobustness." ></td>
	<td class="line x" title="110:167	Thelanguagesit supportsare, for the timebeing,French,English, Italian, Spanishand German." ></td>
	<td class="line x" title="111:167	A few other languagesare beingalso implementedin the frameworkofa multilingualismproject." ></td>
	<td class="line x" title="112:167	Providedthatcollocationextractioncanbeseen as a two-stageprocess(where,in stage1, collocationcandidatesareidentifiedinthetextcorpora, andinstage2,theyarerankedaccordingtoagiven AM, cf.section4.2), the role of the parseris to supportthe first stage." ></td>
	<td class="line x" title="114:167	A pair of lexicalitemsis selectedasacandidateonlyifthereexistasyntacticrelationholdingbetweenthetwo items." ></td>
	<td class="line x" title="115:167	Unlike the traditional,window-basedmethods, candidateselectionis basedon syntacticproximity (as opposedto textual proximity)." ></td>
	<td class="line x" title="116:167	Another peculiarityof our systemis that candidatepairs are identifiedas the parsinggoeson; in otherapproaches, they are extracted by post-processing theoutputofsyntactictools." ></td>
	<td class="line x" title="117:167	Thecandidatepairsidentifiedareclassifiedinto syntacticallyhomogenoussets, according to the syntacticrelationsholdingbetweenthetwo items." ></td>
	<td class="line x" title="118:167	Only certain predefined syntactic relations are kept, that were judged as collocationally relevant aftermultipleexperimentsof extractionand data analysis (e.g. , adjective-noun, verb-object, subject-verb, noun-noun,verb-preposition-noun)." ></td>
	<td class="line x" title="119:167	The sets obtainedare then ranked usingthe loglikelihoodratiostest(Dunning,1993)." ></td>
	<td class="line x" title="120:167	More details about the systemand its performancecanbefoundin(SeretanandWehrli,2006)." ></td>
	<td class="line x" title="121:167	Thefollowingexamples(takenfromtheextraction experimentwe will describebelow) illustrateits potentialto detectcollocationcandidates, even if thesearesubjecttocomplex syntactictransformations: 1.a) atteindre objectif (Fr): Les objectifs fixes `a lechelleinternationale visant `a reduire les emissionsne peuventpasetreatteints`alaidede cesseulsprogrammes." ></td>
	<td class="line x" title="122:167	1.b) accogliere emendamento (It): 44 Possopertantoaccogliere in parte e in lineadi principiogli emendamentinn." ></td>
	<td class="line x" title="123:167	43-46e lemendamento n. 85." ></td>
	<td class="line x" title="124:167	1.c) reforzar cooperacion (Es): Queremos permitira los pases que lo deseen reforzar, en un contexto unitario,su cooperacion en cierto numerodesectores." ></td>
	<td class="line x" title="125:167	Thecollocationextractorispartofabiggersystem (Seretanet al. , 2004) that integrates a concordancerand a sentencealigner, and that supportsthe visualization,the manualvalidationand the managementof a multilingualterminology database." ></td>
	<td class="line x" title="126:167	Thevalidatedcollocationsare usedfor populatingthe lexiconof the parserand that of a translationsystem(Wehrli,2003)." ></td>
	<td class="line x" title="127:167	6 ACross-LingualExtraction Experiment A collocation extraction experiment concerning four different languages (English, Spanish, French,Italian)has beenconductedon a parallel subcorpusof 42 files from the EuropeanParliamentproceedings.Severalstatisticsandextraction resultsarereportedinTable1." ></td>
	<td class="line x" title="128:167	Statistics English Spanish Italian French tokens 2526403 2666764 2575858 2938118 sent/file 2329.1 2513.7 2331.6 2392.8 complete parses 63.4% 35.5% 46.8% 63.7% tokens/sent 25.8 25.3 26.3 29.2 extr." ></td>
	<td class="line x" title="129:167	pairs (tokens) 617353 568998 666122 565287 token/type 2.6 2.5 2.3 2.3 LLisdef." ></td>
	<td class="line x" title="130:167	85.9% 90.6% 83.5% 92.8% Table1: Extractionstatistics We computedthe distribution of pair tokens according to the syntactic type and noted that the most marked distributionaldifferenceamong theselanguagesconcernthefollowingtypes:N-A (7.12),A-N(4.26),V-O(2.68),V-P(4.16),N-P-N (3.81)4." ></td>
	<td class="line x" title="131:167	Unsurprisingly, theRomancelanguagesareless differentin termsof syntacticco-occurrencedistribution, and the deviationof Englishfrom the Romancemeanismorepronouncedinparticular, forN-A(9.72),V-P(5.63),A-N(5.25),N-P-N 4Thenumbersrepresentthevaluesthestandarddeviation oftherelative percentagesinthewholelistsofpairs." ></td>
	<td class="line x" title="132:167	(4.77),andV-O(3.57)." ></td>
	<td class="line x" title="133:167	Thesedistributionaldifferencesmightaccountfor the typesof collocations highlightedby a particularAM(suchas LL)in a languagevs." ></td>
	<td class="line x" title="134:167	another." ></td>
	<td class="line x" title="135:167	Figure1 displaysthe relative proportionsof 3 syntactictypes adjectivenoun,subject-verbandverb-object thatcanbe foundat differentlevels in thesignificancelistreturnedbyLL." ></td>
	<td class="line x" title="136:167	Figure1: Cross-lingualproportionsof A-N,S-V andV-Opairsatdifferentlevelsinthesignificance lists We performed a contrastive analysisof results, by carryingout a case-studyaimed at checking the LL performancevariabilityacrosslanguages." ></td>
	<td class="line x" title="137:167	Thestudyconcernedthe verb-objectcollocations having the noun policyas the directobject." ></td>
	<td class="line x" title="138:167	We specificallyfocusedonthebest-scoredcollocation extractedfromthe Frenchcorpus,namelymener unepolitique(lit.,conducta policy)." ></td>
	<td class="line x" title="140:167	We looked at the translationequivalentsof its 74 instancesidentifiedby our extractionsystem in the corpus." ></td>
	<td class="line x" title="141:167	The analysisrevealed that  at least in this particularcase  the verbal collocates of this noun are highly scattered: pursue, implement,conduct,adopt,apply, develop,have, draft, launch, run, carry out for English; practicar, llevar a cabo,desarrollar, realizar, aplicar, seguir, hacer, adoptar, ejercer for Spanish;condurre, attuare, portare avanti,perseguire, pratticare, adottare, fare forItalian(amongseveralothers)." ></td>
	<td class="line x" title="142:167	Someofthecollocates(thoselistedfirst)are more prominentlyused." ></td>
	<td class="line x" title="143:167	But generallythey are highlydispersed,andthismightindicatea bigger difficultyforLLtopinpointthebestcollocateina languagevs." ></td>
	<td class="line x" title="144:167	another." ></td>
	<td class="line x" title="145:167	Wealsoobservedthatquitefrequently(inabout 25%of thecases)thecollocationdidnotconserve itssyntacticconfiguration.Eithertheverbhere, 45 theequivalentfortheFrenchmener is omitted intranslations(like in2.bbelow): 2.a) des contradictionsexistentdansla politiquequiestmenee(Fr); 2.b) we are dealingwith contradictory policy (En), or, in a few othercases,the wholecollocation disappears,since paraphrasedwith a completely differentsyntacticconstruction: 3.a) directionqui a mene unepolitique insenseedereductiondepersonnel (Fr); 3.b) a managementthat foolishly engagedinstaff reductions(En)." ></td>
	<td class="line x" title="146:167	Inordertoquantifytheimpactsuchfactorshave on the performanceof the AM considered,we furtherscrutinizedthecollocateslistforpolitique proposedby LLtest foreach language(seeTable 2)." ></td>
	<td class="line x" title="147:167	The rank of a pair in the wholelist of verbobjectcollocationsextracted,as assignedby the LLtest,is shownin thelastcolumn.In thesesignificancelists,thecollocationswithpolitiqueasan objectconstitutea smallfraction,andfromthese, onlythetopcollocationsaredisplayedin Table2." ></td>
	<td class="line x" title="148:167	Thethresholdwasmanuallydefinedinaccordance withour intuitionthat the lower-scoredpairsobserved manifestless a collocationalstrength." ></td>
	<td class="line x" title="149:167	It happensto be situatedaroundthe LL valueof 20 foreachlanguage(andis of coursespecificto the sizeofourcorpusandtothenumberofV-Otokens identifiedtherein)." ></td>
	<td class="line x" title="150:167	If weconsidertheLLrankas thesuccessmeasurefor collocatedetection,we caninferthatthe collocatesofthewordunderinvestigationareeasier to found in French,as comparedto English, Italianor Spanish,becausethe value in the first rowofthelastcolumnissmaller." ></td>
	<td class="line x" title="151:167	Thisholdsifwe areinterestedin onlyone(themostsalient)collocatefora word." ></td>
	<td class="line x" title="152:167	If we measurethe successof retrievingall the collocates(byconsidering,forinstance,thespeed to accessthemin theresults list thehigherthe rank,thebetter),thenFrenchcanbeagainconsidered the easiestbecauseoverall, the positionsin the V-O list are higher(i.e. ,the meanof the rank columnissmaller)withrespecttoSpanish,Italian and,respectively, English." ></td>
	<td class="line x" title="153:167	This latter result corresponds,approximately, to the order given by relative proportionof V-O Language collocate freq LLscore rank French mener 74 376.8 45 politique elaborer 17 50.1 734 adapter 5 48.3 780 axer 8 41.4 955 pratiquer 9 39.7 1011 developper 13 28.1 1599 adapter 8 25.2 1867 poursuivre 11 24.4 1943 English pursue 39 214.9 122 policy implement 38 108.7 325 develop 30 81.1 473 conduct 8 28.9 2014 harmonize 9 28.2 2090 gear 5 27.7 2201 need 25 24.9 2615 apply 16 23.3 2930 Spanish practicar 17 98.7 246 poltica desarrollar 27 82.4 312 aplicar 25 65.7 431 seguir 17 33.5 1003 coordinar 8 31.0 1112 basar 11 25.1 1473 orientar 6 22.5 1707 adaptar 5 20.0 1987 construir 6 19.4 2057 Italian attuare 23 79.5 382 politica perseguire 14 46.4 735 praticare 8 37.6 976 seguire 18 30.2 1314 portare 12 29.7 1348 rivedere 9 26.0 1607 riformare 7 25.6 1639 sviluppare 12 22.1 1975 adottare 20 21.2 2087 Table2: Verbalcollocatesfortheheadwordpolicy pairs in each language(Spanish15.12%, French 15.14%, Italian 17.06%, and English 20.82%)." ></td>
	<td class="line x" title="154:167	Given thatin EnglishV-O pairsare morenumerousandtheverbsalsoparticipateinV-Pconstructions, it might seem reasonableto expect lower LLscoresforV-O collocationsin Englishvs." ></td>
	<td class="line x" title="155:167	the other3 languages." ></td>
	<td class="line x" title="156:167	Ingeneral,weexpecta correlationbetweenextractiondifficultyandthedistributionalproperties ofco-occurrencetypes." ></td>
	<td class="line x" title="157:167	7 Conclusion The paper pointed out several issues that occur in transferinga hybridcollocationextraction methodology(thatcombineslinguisticwithstatisticinformation)toa newlanguage." ></td>
	<td class="line x" title="158:167	Besides the questionable availability of language-specifictext analysistools for the new language,a numberof issuesthat are relevant to extraction proper were addressed: the changes in the distribution of (syntactic)word pairs, and the need to find, for each language, the most 46 appropriateassociationmeasureto applyforeach syntactictype (given that AMs are sensitive to distributions and syntactic types); the lack of a priori defined syntactictypes for a language; and, finally, the portabilityof some widelyused techniques(such as the window method) from English to other languages exhibiting a higher wordorderfreedom." ></td>
	<td class="line x" title="159:167	It is again in the multilingualismperspective that the inescapableneed for preprocessingthe textemerged(cf.differentresearcherscitedinsection 3): highlyinflectedlanguagesneed lemmatizers, free-word order languagesneed structural informationin order to guaranteeacceptableresults." ></td>
	<td class="line x" title="160:167	As languagetoolsbecomenowadaysmore andmoreavailable,weexpectthecollocationextraction(and terminologyacquisitionin general) to be exclusively performedin the future by relyingon linguisticanalysis." ></td>
	<td class="line x" title="161:167	We thereforebelieve thatmultilingualismis a trueconcernforcollocationextraction." ></td>
	<td class="line x" title="162:167	The paper reviewed the extractionwork in a language-orientedfashion, while mentioningthe typeof linguisticpreprocessingperformedwhenever it was the case, as well as the languagespecificissues identifiedby the authors." ></td>
	<td class="line x" title="163:167	It then proposeda strategy for implementinga multilingual extractionprocedurethat takes into account thelanguage-specificissuesidentified." ></td>
	<td class="line x" title="164:167	An extraction system for four different languages,basedonfullparsing,wasthendescribed." ></td>
	<td class="line x" title="165:167	Finally, an experimentwas carriedout as a case study, whichpointedoutseveralfactorsthatmight determinea particularAMto performdifferently acrosslanguages.Theexperimentsuggestedthat log-likelihoodratios test might highlightcertain verb-objectcollocationseasierin Frenchthan in Spanish,ItalianandEnglish(in termsof salience inthesignificancelist)." ></td>
	<td class="line x" title="166:167	Futurework needsto extendthe typeof crosslinguisticanalysisinitiatedhere, in orderto providemoreinsightson the differencesexpectedat extractionbetweenonelanguageandanotherand ontheresponsiblefactors,and,accordingly, todefinesstrategiestodealwiththem." ></td>
	<td class="line x" title="167:167	Acknowledgements Theresearchdescribedinthispaperhasbeensupportedin partby a grantfromthe SwissNational Foundation(No.101412-103999)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-2403
Automatic Extraction Of Chinese Multiword Expressions With A Statistical Tool
Piao, Scott S. L.;Sun, Guangfan;Rayson, Paul;Yuan, Qi;"></td>
	<td class="line x" title="1:299	Automatic Extraction of Chinese Multiword Expressions with a Statistical Tool Scott S.L. Piao 1 s.piao@lancaster.ac.uk Guangfan Sun 2 morgan2001_sun@sohu.com Paul Rayson 1 paul@comp.lancs.ac.uk Qi Yuan 2 yq@trans.ccidnet.com 1 UCREL Computing Department Lancaster University Lancaster, UK 2 CIPOL China Centre for Information Industry Development (CCID) Beijing, China Abstract In this paper, we report on our experiment to extract Chinese multiword expressions from corpus resources as part of a larger research effort to improve a machine translation (MT) system." ></td>
	<td class="line x" title="2:299	For existing MT systems, the issue of multiword expression (MWE) identification and accurate interpretation from source to target language remains an unsolved problem." ></td>
	<td class="line x" title="3:299	Our initial test on the Chineseto-English translation functions of Systran and CCIDs Huan-Yu-Tong MT systems reveal that, where MWEs are involved, MT tools suffer in terms of both comprehensibility and adequacy of the translated texts." ></td>
	<td class="line x" title="4:299	For MT systems to become of further practical use, they need to be enhanced with MWE processing capability." ></td>
	<td class="line x" title="5:299	As part of our study towards this goal, we test and evaluate a statistical tool, which was developed for English, for identifying and extracting Chinese MWEs." ></td>
	<td class="line x" title="6:299	In our evaluation, the tool achieved precisions ranging from 61.16% to 93.96% for different types of MWEs." ></td>
	<td class="line x" title="7:299	Such results demonstrate that it is feasible to automatically identify many Chinese MWEs using our tool, although it needs further improvement." ></td>
	<td class="line x" title="8:299	1 Introduction In real-life human communication, meaning is often conveyed by word groups, or meaning groups, rather than by single words." ></td>
	<td class="line x" title="9:299	Very often, it is difficult to interpret human speech word by word." ></td>
	<td class="line x" title="10:299	Consequently, for an MT system, it is important to identify and interpret accurate meaning of such word groups, or multiword expressions (MWE hereafter), in a source language and interpret them accurately in a target language." ></td>
	<td class="line x" title="11:299	However, accurate identification and interpretation of MWEs still remains an unsolved problem in MT research." ></td>
	<td class="line x" title="12:299	In this paper, we present our experiment on identifying Chinese MWEs using a statistical tool for MT purposes." ></td>
	<td class="line x" title="13:299	Here, by multiword expressions, we refer to word groups whose constituent words have strong collocational relations and which can be translated in the target language into stable translation equivalents, either single words or MWEs, e.g. noun phrases, prepositional phrases etc. They may include technical terminology in specific domains as well as more general fixed expressions and idioms." ></td>
	<td class="line x" title="14:299	Our observations found that existing ChineseEnglish MT systems cannot satisfactorily translate MWEs, although some may employ a machine-readable bilingual dictionary of idioms." ></td>
	<td class="line x" title="15:299	Whereas highly compositional MWEs may pose a trivial challenge to human speakers for interpretation, they present a tough challenge for fully automatic MT systems to produce even remotely fluent translations." ></td>
	<td class="line x" title="16:299	Therefore, in our context, we expand the concept of MWE to include those compositional ones which have relatively stable identifiable patterns of translations in the target language." ></td>
	<td class="line x" title="17:299	By way of illustration of the challenge, we experimented with simple Chinese sentences containing some commonly-used MWEs in SYSTRAN (http://www.systransoft.com/) and Huan-Yu-Tong (HYT henceforth) of CCID (China Centre for Information Industry Development) (Sun, 2004)." ></td>
	<td class="line x" title="18:299	The former is one of the most efficient MT systems today, claiming to be the leading provider of the worlds most scalable and modular translation architecture, while the latter is one of the most successful MT systems in China." ></td>
	<td class="line x" title="19:299	Table 1 shows the result, where SL and TL denote source and target languages respectively As shown by the samples, such 17 highly sophisticated MT tools still struggle to produce adequate English sentences Chinese Sentences English (Systran) English (HYT) nullnullnull    null null nullnull  This afternoon can practice a ball game?" ></td>
	<td class="line x" title="20:299	I hope not to be able." ></td>
	<td class="line x" title="21:299	Can practise a ball game this afternoon?" ></td>
	<td class="line x" title="22:299	I hope can not." ></td>
	<td class="line x" title="23:299	 nullnull null  null   null null You may not such do, let us pay respectively each." ></td>
	<td class="line x" title="24:299	You cannot do like that, and let us make it Dutch." ></td>
	<td class="line x" title="25:299	null null   null     nullnull nullnullnull Perhaps does not have the means to let you sit shares a table, did you mind sits separately?" ></td>
	<td class="line x" title="26:299	Perhaps no way out(ly) let you sit with table, are you situated between not mind to separate to sit?" ></td>
	<td class="line x" title="27:299	 nullnull null null null  Selects the milk coffee which ices." ></td>
	<td class="line x" title="28:299	Ice breasts coffee take is selected." ></td>
	<td class="line x" title="29:299	nullnullnullnull null null  null null  Good, I want the beer, again comes to select the coffee." ></td>
	<td class="line x" title="30:299	Alright, I want beer, and take the coffee of ordering again." ></td>
	<td class="line x" title="31:299	Table 1: Samples of Chinese-to-English translations of Systran and HYT." ></td>
	<td class="line x" title="32:299	Ignoring the eccentric English syntactic structures these tools produced, we focus on the translations of Chinese MWEs (see the italic characters in the Table 1) which have straightforward expression equivalents in English." ></td>
	<td class="line x" title="33:299	For example, in this context,   can be translated into hope not,  into go Dutch,   into together or at the same table,  into white coffee or coffee with milk,    into want some more (in addition to something already ordered)." ></td>
	<td class="line x" title="34:299	While these Chinese MWEs are highly compositional ones, when they are translated word by word, we see verbose and awkward translations (for correct translations, see the appendix)." ></td>
	<td class="line x" title="35:299	To solve such problems, we need algorithms and tools for identifying MWEs in the source language (Chinese in this case) and to accurately map them to their adequate translation equivalents in the target language (English in our case) that are appropriate for given contexts." ></td>
	<td class="line x" title="36:299	In the previous examples, an MT tool should be able to identify the Chinese MWE  and either provide the literal translation of pay for each or map it to the more idomatic expressions of go Dutch." ></td>
	<td class="line x" title="37:299	Obviously, it would involve a wide range of issues and techniques for a satisfactory solution to this problem." ></td>
	<td class="line x" title="38:299	In this paper, we focus on the sub-issue of automatically recognising and extracting Chinese MWEs." ></td>
	<td class="line x" title="39:299	Specifically, we test and evaluate a statistical tool for automatic MWE extraction in Chinese corpus data." ></td>
	<td class="line x" title="40:299	As the results of our experiment demonstrate, the tool is capable of identifying many MWEs with little language-specific knowledge." ></td>
	<td class="line x" title="41:299	Coupled with an MT system, such a tool could be useful for addressing the MWE issue." ></td>
	<td class="line oc" title="42:299	2 Related Work The issue of MWE processing has attracted much attention from the Natural Language Processing (NLP) community, including Smadja, 1993; Dagan and Church, 1994; Daille, 1995; 1995; McEnery et al. , 1997; Wu, 1997; Michiels and Dufour, 1998; Maynard and Ananiadou, 2000; Merkel and Andersson, 2000; Piao and McEnery, 2001; Sag et al. , 2001; Tanaka and Baldwin, 2003; Dias, 2003; Baldwin et al. , 2003; Nivre and Nilsson, 2004 Pereira et al,." ></td>
	<td class="line o" title="43:299	2004; Piao et al. , 2005." ></td>
	<td class="line x" title="44:299	Study in this area covers a wide range of sub-issues, including MWE identification and extraction from monolingual and multilingual corpora, classification of MWEs according to a variety of viewpoints such as types, compositionality and alignment of MWEs across different languages." ></td>
	<td class="line x" title="45:299	However studies in this area on Chinese language are limited." ></td>
	<td class="line x" title="46:299	A number of approaches have been suggested, including rule-based and statistical approaches, and have achieved success to various extents." ></td>
	<td class="line x" title="47:299	Despite this research, however, MWE processing still presents a tough challenge, and it has been receiving increasing attention, as exemplified by recent MWE-related ACL workshops." ></td>
	<td class="line x" title="48:299	Directly related to our work is the development of a statistical MWE tool at Lancaster for searching and identifying English MWEs in running text (Piao et al. , 2003, 2005)." ></td>
	<td class="line x" title="49:299	Trained on corpus data in a given domain or genre, this tool can automatically identify MWEs in running text or extract MWEs from corpus data from the similar domain/genre (see further information about this tool in section 3.1)." ></td>
	<td class="line x" title="50:299	It has been tested and compared with an English semantic tagger (Rayson et al. , 2004) and was found to be efficient in identifying domain-specific MWEs in English corpora, and complementary to the se18 mantic tagger which relies on a large manually compiled lexicon." ></td>
	<td class="line x" title="51:299	Other directly related work includes the development of the HYT MT system at CCID in Beijing, China." ></td>
	<td class="line x" title="52:299	It has been under development since 1991 (Sun, 2004) and it is one of the most successful MT systems in China." ></td>
	<td class="line x" title="53:299	However, being a mainly rule-based system, its performance degrades when processing texts from domains previously unknown to its knowledge database." ></td>
	<td class="line x" title="54:299	Recently a corpus-based approach has been adopted for its improvement, and efforts are being made to improve its capability of processing MWEs." ></td>
	<td class="line x" title="55:299	Our main interest in this study is in the application of a MWE identification tool to the improvement of MT system." ></td>
	<td class="line x" title="56:299	As far as we know, there has not been a satisfactory solution to the efficient handling of Chinese MWEs in MT systems, and our experiment contributes to a deeper understanding of this problem." ></td>
	<td class="line x" title="57:299	3 Automatic Identification and extraction of Chinese MWEs In order to test the feasibility of automatic identification and extraction of Chinese MWEs on a large scale, we used an existing statistical tool built for English and a Chinese corpus built at CCID." ></td>
	<td class="line x" title="58:299	A CCID tool is used for tokenizing and POS-tagging the Chinese corpus." ></td>
	<td class="line x" title="59:299	The result was thoroughly manually checked by Chinese experts at CCID." ></td>
	<td class="line x" title="60:299	In this paper, we aim to evaluate this existing tool from two perspectives a) its performance on MWE extraction, and b) its performance on a language other than English." ></td>
	<td class="line x" title="61:299	In the following sections, we describe our experiment in detail and discuss main issues that arose during the course of our experiment." ></td>
	<td class="line x" title="62:299	3.1 MWE extraction tool The tool we used for the experiment exploits statistical collocational information between near-context words (Piao et al. , 2005)." ></td>
	<td class="line x" title="63:299	It first collects collocates within a given scanning window, and then searches for MWEs using the collocational information as a statistical dictionary." ></td>
	<td class="line x" title="64:299	As the collocational information can be extracted on the fly from the corpus to be processed for a reasonably large corpus, this process is fully automatic." ></td>
	<td class="line x" title="65:299	To search for MWEs in a small corpus, such as a few sentences, the tool needs to be trained on other corpus data in advance." ></td>
	<td class="line x" title="66:299	With regards to the statistical measure of collocation, the option of several formulae are available, including mutual information and log likelihood, etc. Our past experience shows that log-likelihood provides an efficient metric for corpus data of moderate sizes." ></td>
	<td class="line x" title="67:299	Therefore it is used in our experiment." ></td>
	<td class="line x" title="68:299	It is calculated as follows (Scott, 2001)." ></td>
	<td class="line x" title="69:299	For a given pair of words X and Y and a search window W, let a be the number of windows in which X and Y co-occur, let b be the number of windows in which only X occurs, let c be the number of windows in which only Y occurs, and let d be the number of windows in which none of them occurs, then G 2 = 2 (alna + blnb + clnc + dlnd (a+b)ln(a+b) (a+c)ln(a+c) (b+d)ln(b+d) (c+d)ln(c+d)) + (a+b+c+d)ln(a+b+c+d)) In addition to the log-likelihood, the t-score is used to filter out insignificant co-occurrence word pairs (Fung and Church, 1994), which is calculated as follows: ),( 1 )()(),( ba baba WWprob M WprobWprobWWprob t  = In order to filter out weak collocates, a threshold is often used, i.e. in the stage of collocation extraction, any pairs of items producing word affinity scores lower than a given threshold are excluded from the MWE searching process." ></td>
	<td class="line x" title="70:299	Furthermore, in order to avoid the noise caused by functional words and some extremely frequent words, a stop word list is used to filter such words out from the process." ></td>
	<td class="line x" title="71:299	If the corpus data is POS-tagged, some simple POS patterns can be used to filter certain syntactic patterns from the candidates." ></td>
	<td class="line x" title="72:299	It can either be implemented as an internal part of the process, or as a post-process." ></td>
	<td class="line x" title="73:299	In our case, such pattern filters are mostly applied to the output of the MWE searching tool in order to allow the tool to be language-independent as much as possible." ></td>
	<td class="line x" title="74:299	Consequently, for our experiment, the major adjustment to the tool was to add a Chinese stop word list." ></td>
	<td class="line x" title="75:299	Because the tool is based on Unicode, the stop words of different languages can be kept in a single file, avoiding any need for adjusting the program itself." ></td>
	<td class="line x" title="76:299	Unless different languages involved happen to share words with the same form, this practice is safe and reliable." ></td>
	<td class="line x" title="77:299	In our particular case, because we are dealing with English and Chinese, which use widely different characters, such a practice performs well." ></td>
	<td class="line x" title="78:299	19 Another language-specific adjustment needed was to use a Chinese POS-pattern filter for selecting various patterns of the candidate MWEs (see Table 6)." ></td>
	<td class="line x" title="79:299	As pointed out previously, it was implemented as a simple pattern-matching program that is separate from the MWE tool itself, hence minimizing the modification needed for porting the tool from English to Chinese language." ></td>
	<td class="line x" title="80:299	A major advantage of this tool is its capability of identifying MWEs of various lengths which are generally representative of the given topic or domain." ></td>
	<td class="line x" title="81:299	Furthermore, for English it was found effective in extracting domain-specific multiword terms and expressions which are not included in manually compiled lexicons and dictionaries." ></td>
	<td class="line x" title="82:299	Indeed, due to the open-ended nature of such MWEs, any manually compiled lexicons, however large they may be, are unlikely to cover them exhaustively." ></td>
	<td class="line x" title="83:299	It is also efficient in finding newly emerging MWEs, particularly technical terms, that reflect the changes in the real world." ></td>
	<td class="line x" title="84:299	3.2 Experiment In this experiment, our main aim was to examine the feasibility of practical application of the MWE tool as a component of an MT system, therefore we used test data from some domains in which translation services are in strong demand." ></td>
	<td class="line x" title="85:299	We selected Chinese corpus data of approximately 696,000 tokenised words (including punctuation marks) which cover the topics of food, transportation, tourism, sports (including the Olympics) and business." ></td>
	<td class="line x" title="86:299	In our experiment, we processed the texts from different topics together." ></td>
	<td class="line x" title="87:299	These topics are related to each other under the themes of entertainment and business." ></td>
	<td class="line x" title="88:299	Therefore we assume, by mixing the data together, we could examine the performance of the MWE tool in processing data from a broad range of related domains." ></td>
	<td class="line x" title="89:299	We expect that the different features of texts from different domains will have a certain impact on the result, but the examination of such impact is beyond the scope of this paper." ></td>
	<td class="line x" title="90:299	As mentioned earlier, the Chinese word tokeniser and POS tagger used in our experiment has been developed at CCID." ></td>
	<td class="line x" title="91:299	It is an efficient tool running with accuracy of 98% for word tokenisation and 95% for POS annotation." ></td>
	<td class="line x" title="92:299	It employs a part-of-speech tagset of 15 categories shown in Table 2." ></td>
	<td class="line x" title="93:299	Although it is not a finely grained tagset, it meets the need for creating POS pattern filters for MWE extraction." ></td>
	<td class="line x" title="94:299	N Name V Verb A Adjective F Adverb R Pronoun I Preposition J Conjunction U Number S classifier (measure word) G Auxiliary verb E Accessory word L directional noun P Punctuation H Onomatopoeia X Subject-predicate phrase Table 2: CCID Chinese tagset Since function words are found to cause noise in the process of MWE identification, a Chinese stop list was collected." ></td>
	<td class="line x" title="95:299	First, a word frequency list was extracted." ></td>
	<td class="line x" title="96:299	Next, the top items were considered and we selected 70 closed class words for the stop word list." ></td>
	<td class="line x" title="97:299	When the program searches for MWEs, such words are ignored." ></td>
	<td class="line x" title="98:299	The threshold of word affinity strength is another issue to be addressed." ></td>
	<td class="line x" title="99:299	In this experiment, we used log-likelihood to measure the strength of collocation between word pairs." ></td>
	<td class="line x" title="100:299	Generally the log-likelihood score of 6.6 (p < 0.01 or 99% confidence) is recommended as the threshold (Rayson et al. , 2004), but it was found to produce too many false candidates in our case." ></td>
	<td class="line x" title="101:299	Based on our initial trials, we used a higher threshold of 30, i.e. any word pairs producing log-likelihood score less than this value are ignored in the MWE searching process." ></td>
	<td class="line x" title="102:299	Furthermore, for the sake of the reliability of the statistical score, when extracting collocates, a frequency threshold of five was used to filter out low-frequency words, i.e. word pairs with frequencies less than five were ignored." ></td>
	<td class="line x" title="103:299	An interesting issue for us in this experiment is the impact of the length of collocation searching window on the MWE identification." ></td>
	<td class="line x" title="104:299	For this purpose, we tested two search window lengths 2 and 3, and compared the results obtained by using them." ></td>
	<td class="line x" title="105:299	Our initial hypothesis was that the shorter window length may produce higher precision while the longer window length may sacrifice precision but boost the MWE coverage." ></td>
	<td class="line x" title="106:299	The output of the tool was manually checked by Chinese experts at CCID, including cross checking to guarantee the reliability of the results." ></td>
	<td class="line x" title="107:299	There were some MWE candidates on which disagreements arose." ></td>
	<td class="line x" title="108:299	In such cases, the 20 candidate was counted as false." ></td>
	<td class="line x" title="109:299	Furthermore, in order to estimate the recall, experts manually identified MWEs in the whole test corpus, so that the output of the automatic tool could be compared against it." ></td>
	<td class="line x" title="110:299	In the following section, we present a detailed report on our evaluation of the MWE tool." ></td>
	<td class="line x" title="111:299	3.3 Evaluation We first evaluated the overall precision of the tool." ></td>
	<td class="line x" title="112:299	A total of 7,142 MWE candidates (types) were obtained for window lengths of 2, of which 4,915 were accepted as true MWEs, resulting in a precision of 68.82%." ></td>
	<td class="line x" title="113:299	On the other hand, a total of 8,123 MWE candidates (types) were obtained for window lengths of 3, of which 4,968 were accepted as true MWEs, resulting in a precision of 61.16%." ></td>
	<td class="line x" title="114:299	This result is in agreement with our hypothesis that shorter search window length tends to produce higher precision." ></td>
	<td class="line x" title="115:299	Next, we estimated the recall based on the manually analysed data." ></td>
	<td class="line x" title="116:299	When we compared the accepted MWEs from the automatic result against the manually collected ones, we found that the experts tend to mark longer MWEs, which often contain the items identified by the automatic tool." ></td>
	<td class="line x" title="117:299	For example, the manually marked MWE        (development plan for the tennis sport) contains shorter MWEs    (tennis sport) and     (development plan) which were identified by the tool separately." ></td>
	<td class="line x" title="118:299	So we decided to take the partial matches into account when we estimate the recall." ></td>
	<td class="line x" title="119:299	We found that a total 14,045 MWEs were manually identified and, when the search window length was set to two and three, 1,988 and 2,044 of them match the automatic output, producing recalls of 14.15% and 14.55% respectively." ></td>
	<td class="line x" title="120:299	It should be noted that many of the manually accepted MWEs from the automatic output were not found in the manual MWE collection." ></td>
	<td class="line x" title="121:299	This discrepancy was likely caused by the manual analysis being carried out independently of the automatic tool, resulting in a lower recall than expected." ></td>
	<td class="line x" title="122:299	Table 3 lists the precisions and recalls." ></td>
	<td class="line x" title="123:299	Window length = 2 Window length = 3 Precision Recall Precision Recall 68.82% 14.15% 61.16% 14.55% Table 3: Overall precisions and recalls Furthermore, we evaluated the performance of the MWE tool from two aspects: frequency and MWE pattern." ></td>
	<td class="line x" title="124:299	Generally speaking, statistical algorithms work better on items of higher frequency as it depends on the collocational information." ></td>
	<td class="line x" title="125:299	However, our tool does not select MWEs directly from the collocates." ></td>
	<td class="line x" title="126:299	Rather, it uses the collocational information as a statistical dictionary and searches for word sequences whose constituent words have significantly strong collocational bonds between them." ></td>
	<td class="line x" title="127:299	As a result, it is capable of identifying many low-frequency MWEs." ></td>
	<td class="line x" title="128:299	Table 4 lists the breakdown of the precision for five frequency bands (window length = 2)." ></td>
	<td class="line x" title="129:299	Freq Candidates True MWEs Precision >= 100 17 9 52.94% 10 ~ 99 846 646 76.36% 3 ~ 9 2,873 2,178 75.81% 2 949 608 64.07% 1 2,457 1,474 59.99% Total 7,142 4,915 68.82% Table 4: Breakdown of precision for frequencies (window length = 2)." ></td>
	<td class="line x" title="130:299	As shown in the table above, the highest precisions were obtained for the frequency range between 3 and 99." ></td>
	<td class="line x" title="131:299	However, 2,082 of the accepted MWEs have frequencies of one or two, accounting for 42.36% of the total accepted MWEs." ></td>
	<td class="line x" title="132:299	Such a result demonstrates again that our tool is capable of identifying low-frequency items." ></td>
	<td class="line x" title="133:299	An interesting result is for the top frequency band (greater than 100)." ></td>
	<td class="line x" title="134:299	Against our general assumption that higher frequency brings higher precision, we saw the lowest precision in the table for this band." ></td>
	<td class="line x" title="135:299	Our manual examination reveals this was caused by the high frequency numbers, such as one or two in the expressions  (a/one) and    ( a kind of)." ></td>
	<td class="line x" title="136:299	This type of expression were classified as uninteresting candidates in the manual checking, resulting in higher error rates for the high frequency band." ></td>
	<td class="line x" title="137:299	When we carry out a parallel evaluation for the case of searching window length of 3, we see a similar distribution of precision across the frequency bands except that the lowest frequency band has the lowest precision, as shown by Table 5." ></td>
	<td class="line x" title="138:299	When we compare this table against Table 4, we can see, for all of the frequency bands except the top one, that the precision drops as the search window increases." ></td>
	<td class="line x" title="139:299	This further supports our earlier assumption that wider searching window tends to reduce the precision." ></td>
	<td class="line x" title="140:299	21 Freq candidates true MWEs Precision >= 100 17 9 52.94% 10 ~ 99 831 597 71.84% 3 ~ 9 3,093 2,221 71.81% 2 1,157 669 57.82% 1 3,025 1,472 48.66% Total 8,123 4,968 61.16% Table 5: Breakdown of precision for frequencies (window length = 3)." ></td>
	<td class="line x" title="141:299	In fact, not only the top frequency band, much of the errors of the total output were found to be caused by the numbers that frequently occur in the test data, e.g.  _U  _S (one),  _U  _S (two) etc. When a POS filter was used to filter them out, for the window length 2, we obtained a total 5,660 candidates, of which 4,386 were accepted as true MWEs, producing a precision of 77.49%." ></td>
	<td class="line x" title="142:299	Similarly for the window length 3, a total of 6,526 candidates were extracted in this way and 4,685 of them were accepted as true MWEs, yielding a precision of 71.79%." ></td>
	<td class="line x" title="143:299	Another factor affecting the performance of the tool is the type of MWEs." ></td>
	<td class="line x" title="144:299	In order to examine the potential impact of MWE types to the performance of the tool, we used filters to select MWEs of the following three patterns: 1) AN: Adjective + noun structure; 2) NN: Noun + noun Structure; 3) FV: Adverb + Verb." ></td>
	<td class="line x" title="145:299	Table 6 lists the precision for each of the MWE types and for search window lengths of 2 and 3." ></td>
	<td class="line x" title="146:299	Search window length = 2 Pattern Candidate True MWEs Precision A+N 236 221 93.64% N+N 644 589 91.46% F+V 345 321 93.04% total 1,225 1,131 92.33% Search window length = 3 Pattern Candidate True MWEs Precision A+N 259 233 89.96% N+N 712 635 89.19% F+V 381 358 93.96% Total 1,352 1,226 90.68% Table 6: Precisions for three types of MWEs As shown in the table, the MWE tool achieved high precisions above 91% when we use a search window of two words." ></td>
	<td class="line x" title="147:299	Even when the search window expands to three words, the tool still obtained precision around 90%." ></td>
	<td class="line x" title="148:299	In particular, the tool is efficient for the verb phrase type." ></td>
	<td class="line x" title="149:299	Such a result demonstrates that, when we constrain the search algorithm to some specific types of MWEs, we can obtain higher precisions." ></td>
	<td class="line x" title="150:299	While one may argue that rule-based parser can do the same work, it must be noted that we are not interested in all grammatical phrases, but those which reflect the features of the given domain." ></td>
	<td class="line x" title="151:299	This is achieved by combining statistical word collocation measures, a searching strategy and simple POS pattern filters." ></td>
	<td class="line x" title="152:299	Another interesting finding in our experiment is that our tool extracted clauses, such as    (What would you like to drink)?" ></td>
	<td class="line x" title="153:299	and     (Would you like a drink first?)." ></td>
	<td class="line x" title="154:299	The clauses occur only once or twice in the entire test data, but were recognized by the tool because of the strong collocational bond between their constituent words." ></td>
	<td class="line x" title="155:299	The significance of such performance is that such clauses are typical expressions which are frequently used in real-life conversation in the contexts of the canteen, tourism etc. Such a function of our tool may have practical usage in automatically collecting longer typical expressions for the given domains." ></td>
	<td class="line x" title="156:299	4 Discussion As our experiment demonstrates, our tool provides a practical means of identifying and extracting domain specific MWEs with a minimum amount of linguistic knowledge." ></td>
	<td class="line x" title="157:299	This becomes important in multilingual tasks in which it can be costly and time consuming to build comprehensive rules for several languages." ></td>
	<td class="line x" title="158:299	In particular, it is capable of detecting MWEs of various lengths, sometimes whole clauses, which are often typical of the given domains of the corpus data." ></td>
	<td class="line x" title="159:299	For example, in our experiment, the tool successfully identified several daily used long expressions in the domain of food and tourism." ></td>
	<td class="line x" title="160:299	MT systems often suffer when translating conversation." ></td>
	<td class="line x" title="161:299	An efficient MWE tool can potentially alleviate the problem by extracting typical clauses used in daily life and mapping them to adequate translations in the target language." ></td>
	<td class="line x" title="162:299	Despite the flexibility of the statistical tool, however, there is a limit to its performance in terms of precision." ></td>
	<td class="line x" title="163:299	While it is quite efficient in providing MWE candidates, its output has to be either verified by human or refined by using linguistic rules." ></td>
	<td class="line x" title="164:299	In our particular case, we improved the precision of our tool by employing simple POS pattern filters." ></td>
	<td class="line x" title="165:299	Another limitation of this tool is that currently it can only recognise continuous MWEs." ></td>
	<td class="line x" title="166:299	A more flexible searching algo22 rithm is needed to identify discontinuous MWEs, which are important for NLP tasks." ></td>
	<td class="line x" title="167:299	Besides the technical problem, a major unresolved issue we face is what constitutes MWEs." ></td>
	<td class="line x" title="168:299	Despite agreement on the core MWE types, such as idioms and highly idiosyncratic expressions, like  (Cheng-Yu) in Chinese, it is difficult to reach agreement on less fixed expressions." ></td>
	<td class="line x" title="169:299	We contend that MWEs may have different definitions for different research purposes." ></td>
	<td class="line x" title="170:299	For example, for dictionary compilation, lexicographers tend to constrain MWEs to highly noncompositional expressions (Moon, 1998: 18)." ></td>
	<td class="line x" title="171:299	This is because monolingual dictionary users can easily understand compositional MWEs and there is no need to include them in a dictionary for native speakers." ></td>
	<td class="line x" title="172:299	For lexicon compilation aimed at practical NLP tasks, however, we may apply a looser definition of MWEs." ></td>
	<td class="line x" title="173:299	For example, in the Lancaster semantic lexicon (Rayson et al. , 2004), compositional word groups such as youth club are considered as MWEs alongside non-compositional expressions such as food for thought as they depict single semantic units or concepts." ></td>
	<td class="line x" title="174:299	Furthermore, for the MT research community whose primary concern is crosslanguage interpretation, any multiword units that have stable translation equivalent(s) in a target language can be of interest." ></td>
	<td class="line x" title="175:299	As we discussed earlier, a highly idiomatic expression in a language can be translated into a highly compositional expression in another language, and vice versa." ></td>
	<td class="line x" title="176:299	In such situations, it can be more practically useful to identify and map translation equivalents between the source and target languages regardless of their level of compositionality." ></td>
	<td class="line x" title="177:299	Finally, the long Chinese clauses identified by the tool can potentially be useful for the improvement of MT systems." ></td>
	<td class="line x" title="178:299	In fact, most of them are colloquial expressions in daily conversation, and many such Chinese expressions are difficult to parse syntactically." ></td>
	<td class="line x" title="179:299	It may be more feasible to identify such expressions and map them as a whole to English equivalent expressions." ></td>
	<td class="line x" title="180:299	The same may apply to technical terms, jargon and slang." ></td>
	<td class="line x" title="181:299	In our experiment, our tool demonstrated its capability of detecting such expressions, and will prove useful in this regard." ></td>
	<td class="line x" title="182:299	5 Conclusion In this paper, we have reported on our experiment of automatic extraction of Chinese MWEs using a statistical tool originally developed for English." ></td>
	<td class="line x" title="183:299	Our statistical tool produced encouraging results, although further improvement is needed to become practically applicable for MT system in terms of recall." ></td>
	<td class="line x" title="184:299	Indeed, for some constrained types of MWEs, high precisions above 90% have been achieved." ></td>
	<td class="line x" title="185:299	This shows, enhanced with some linguistic filters, it can provide a practically useful tool for identifying and extracting MWEs." ></td>
	<td class="line x" title="186:299	Furthermore, in our experiment, our tool demonstrated its capability of multilingual processing." ></td>
	<td class="line x" title="187:299	With only minor adjustment, it can be ported to other languages." ></td>
	<td class="line x" title="188:299	Meanwhile, further study is needed for a fuller understanding of the factors affecting the performance of statistical tools, including the text styles and topic/domains of the texts, etc. Acknowledgement This work was supported by the National Natural Science Foundation of China (grant no. 60520130297) and the British Academy (grant no." ></td>
	<td class="line x" title="189:299	SG-42140)." ></td>
	<td class="line x" title="190:299	References Biber, D. , Conrad, S. , Cortes, V. , 2003." ></td>
	<td class="line x" title="191:299	Lexical bundles in speech and writing: an initial taxonomy." ></td>
	<td class="line x" title="192:299	In: Wilson, A. , Rayson P. , McEnery, T." ></td>
	<td class="line x" title="193:299	(Eds.), Corpus Linguistics by the Lune: A Festschrift for Geoffrey Leech." ></td>
	<td class="line x" title="194:299	Peter Lang, Frankfurt." ></td>
	<td class="line x" title="195:299	pp." ></td>
	<td class="line x" title="196:299	71-92." ></td>
	<td class="line x" title="197:299	Baldwin, T. , Bannard, C. , Tanaka, T. and Widdows, D. 2003 An Empirical Model of Multiword Expression Decomposability, In Proceedings of the ACL-2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, Sapporo, Japan, pp." ></td>
	<td class="line x" title="198:299	8996." ></td>
	<td class="line x" title="199:299	Dagan, I. , Church, K. , 1994." ></td>
	<td class="line x" title="200:299	Termight: identifying and translating technical terminology." ></td>
	<td class="line x" title="201:299	In: Proceedings of the 4th Conference on Applied Natural Language Processing, Stuttgart, German." ></td>
	<td class="line x" title="202:299	pp." ></td>
	<td class="line x" title="203:299	3440." ></td>
	<td class="line x" title="204:299	Daille, B. , 1995." ></td>
	<td class="line x" title="205:299	Combined approach for terminology extraction: lexical statistics and linguistic filtering." ></td>
	<td class="line x" title="206:299	Technical paper 5, UCREL, Lancaster University." ></td>
	<td class="line x" title="207:299	Dias, G. , 2003." ></td>
	<td class="line x" title="208:299	Multiword unit hybrid extraction." ></td>
	<td class="line x" title="209:299	In: Proceedings of the Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, at ACL'03, Sapporo, Japan." ></td>
	<td class="line x" title="210:299	pp." ></td>
	<td class="line x" title="211:299	41-48." ></td>
	<td class="line x" title="212:299	Dunning, T. , 1993." ></td>
	<td class="line x" title="213:299	Accurate methods for the statistics of surprise and coincidence." ></td>
	<td class="line x" title="214:299	Computational Linguistics 19 (1), 61-74." ></td>
	<td class="line x" title="215:299	Fung, P. , Church, K. , 1994." ></td>
	<td class="line x" title="216:299	K-vec: a new approach for aligning parallel texts." ></td>
	<td class="line x" title="217:299	In: Proceedings of COLING '94, Kyoto, Japan." ></td>
	<td class="line x" title="218:299	pp." ></td>
	<td class="line x" title="219:299	1996-2001." ></td>
	<td class="line x" title="220:299	Maynard, D. , Ananiadou, S. , 2000." ></td>
	<td class="line x" title="221:299	Trucks: a model for automatic multiword term recognition." ></td>
	<td class="line x" title="222:299	Journal of Natural Language Processing 8 (1), 101-126." ></td>
	<td class="line x" title="223:299	McEnery, T. , Lange, J. M., Oakes, M. , Vernonis, J , 1997." ></td>
	<td class="line x" title="224:299	The exploitation of multilingual annotated corpora for term extraction." ></td>
	<td class="line x" title="225:299	In: Garside, R. , Leech, G. , McEnery, A." ></td>
	<td class="line x" title="226:299	(Eds.), Corpus Annotation --Linguistic Information from Computer Text Corpora." ></td>
	<td class="line x" title="227:299	Longman, London & New York." ></td>
	<td class="line x" title="228:299	pp 220230." ></td>
	<td class="line x" title="229:299	Merkel, M. , Andersson, M. , 2000." ></td>
	<td class="line x" title="230:299	Knowledge-lite extraction of multi-word units with language filters and entropy thresholds." ></td>
	<td class="line x" title="231:299	In: Proceedings of 2000 Conference User-Oriented Content-Based Text and Image Handling (RIAO'00), Paris, France." ></td>
	<td class="line x" title="232:299	pp." ></td>
	<td class="line x" title="233:299	737746." ></td>
	<td class="line x" title="234:299	Michiels, A. , Dufour, N. , 1998." ></td>
	<td class="line x" title="235:299	DEFI, a tool for automatic multi-word unit recognition, meaning assignment and translation selection." ></td>
	<td class="line x" title="236:299	In: Proceedings of the First International Conference on Language Resources & Evaluation, Granada, Spain." ></td>
	<td class="line x" title="237:299	pp." ></td>
	<td class="line x" title="238:299	1179-1186." ></td>
	<td class="line x" title="239:299	Moon, R. 1998." ></td>
	<td class="line x" title="240:299	Fixed expressions and idioms in English: a corpus-based approach." ></td>
	<td class="line x" title="241:299	Clarendon Press: Oxford." ></td>
	<td class="line x" title="242:299	Nivre, J. , Nilsson, J. , 2004." ></td>
	<td class="line x" title="243:299	Multiword units in syntactic parsing." ></td>
	<td class="line x" title="244:299	In: Proceedings of LREC-04 Workshop on Methodologies & Evaluation of Multiword Units in Real-world Applications, Lisbon, Portugal." ></td>
	<td class="line x" title="245:299	pp." ></td>
	<td class="line x" title="246:299	37-46." ></td>
	<td class="line x" title="247:299	Pereira, R. , Crocker, P. , Dias, G. , 2004." ></td>
	<td class="line x" title="248:299	A parallel multikey quicksort algorithm for mining multiword units." ></td>
	<td class="line x" title="249:299	In: Proceedings of LREC-04 Workshop on Methodologies & Evaluation of Multiword Units in Real-world Applications, Lisbon, Portugal." ></td>
	<td class="line x" title="250:299	pp." ></td>
	<td class="line x" title="251:299	1723." ></td>
	<td class="line x" title="252:299	Piao, S. L., Rayson, P. , Archer, D. and McEnery, T. 2005." ></td>
	<td class="line x" title="253:299	Comparing and Combining A Semantic Tagger and A Statistical Tool for MWE Extraction." ></td>
	<td class="line x" title="254:299	Computer Speech & Language Volume 19, Issue 4, pp." ></td>
	<td class="line x" title="255:299	378-397." ></td>
	<td class="line x" title="256:299	Piao, S.L, Rayson, P. , Archer, D. , Wilson, A. and McEnery, T. 2003." ></td>
	<td class="line x" title="257:299	Extracting multiword expressions with a semantic tagger." ></td>
	<td class="line x" title="258:299	In Proceedings of the Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, at ACL'03, Sapporo, Japan, pp." ></td>
	<td class="line x" title="259:299	49-56." ></td>
	<td class="line x" title="260:299	Piao, S. , McEnery, T. , 2001." ></td>
	<td class="line x" title="261:299	Multi-word unit alignment in English-Chinese parallel corpora." ></td>
	<td class="line x" title="262:299	In: Proceedings of the Corpus Linguistics 2001, Lancaster, UK." ></td>
	<td class="line x" title="263:299	pp." ></td>
	<td class="line x" title="264:299	466-475." ></td>
	<td class="line x" title="265:299	Rayson, P. , Archer, D. , Piao, S. L., McEnery, T. 2004." ></td>
	<td class="line x" title="266:299	The UCREL semantic analysis system." ></td>
	<td class="line x" title="267:299	In proceedings of the workshop on Beyond Named Entity Recognition Semantic labelling for NLP tasks in association with LREC 2004, Lisbon, Portugal, pp." ></td>
	<td class="line x" title="268:299	7-12." ></td>
	<td class="line x" title="269:299	Rayson, P. , Berridge, D. and Francis, B. 2004." ></td>
	<td class="line x" title="270:299	Extending the Cochran rule for the comparison of word frequencies between corpora." ></td>
	<td class="line x" title="271:299	In Proceedings of the 7th International Conference on Statistical analysis of textual data (JADT 2004), Louvain-laNeuve, Belgium." ></td>
	<td class="line x" title="272:299	pp." ></td>
	<td class="line x" title="273:299	926-936." ></td>
	<td class="line x" title="274:299	Sag, I. , Baldwin, T. , Bond, F. , Copestake, A. , Dan, F. , 2001." ></td>
	<td class="line x" title="275:299	Multiword expressions: a pain in the neck for NLP." ></td>
	<td class="line x" title="276:299	LinGO Working Paper No. 2001-03, Stanford University, CA." ></td>
	<td class="line x" title="277:299	Scott, M. , 2001." ></td>
	<td class="line x" title="278:299	Mapping key words to problem and solution." ></td>
	<td class="line x" title="279:299	In: Scott, M. , Thompson, G." ></td>
	<td class="line x" title="280:299	(Eds.), Patterns of Text: in Honour of Michael Hoey." ></td>
	<td class="line x" title="281:299	Benjamins, Amsterdam." ></td>
	<td class="line x" title="282:299	pp." ></td>
	<td class="line x" title="283:299	109  127." ></td>
	<td class="line xc" title="284:299	Smadja, F. , 1993." ></td>
	<td class="line x" title="285:299	Retrieving collocations from text: Xtract." ></td>
	<td class="line x" title="286:299	Computational Linguistics 19 (1), 143-177." ></td>
	<td class="line x" title="287:299	Sun, G. 2004." ></td>
	<td class="line x" title="288:299	Design of an Interlingua-Based Chinese-English Machine Translation System." ></td>
	<td class="line x" title="289:299	In Proceedings of the 5th China-Korea Joint Symposium on Oriental Language Processing and Pattern Recognition, Qingdao, China." ></td>
	<td class="line x" title="290:299	pp." ></td>
	<td class="line x" title="291:299	129-134." ></td>
	<td class="line x" title="292:299	Tanaka, T. , Baldwin, T. , 2003." ></td>
	<td class="line x" title="293:299	Noun-noun compound machine translation: a feasibility study on shallow processing." ></td>
	<td class="line x" title="294:299	In: Proceedings of the ACL-03 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, Sapporo, Japan." ></td>
	<td class="line x" title="295:299	pp." ></td>
	<td class="line x" title="296:299	17-24." ></td>
	<td class="line x" title="297:299	Wu, D. , 1997." ></td>
	<td class="line x" title="298:299	Stochastic inversion transduction grammars and bilingual parsing of parallel corpora." ></td>
	<td class="line x" title="299:299	Computational Linguistics 23 (3), 377-401." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-2406
Collocation Extraction: Needs, Feeds And Results Of An Extraction System For German
Ritz, Julia;"></td>
	<td class="line x" title="1:230	Collocation Extraction: Needs, Feeds and Results of an Extraction System for German Julia Ritz Institut f r Maschinelle Sprachverarbeitung (IMS) Universit t Stuttgart Azenbergstr." ></td>
	<td class="line x" title="2:230	12 70174 Stuttgart Germany Julia.Ritz@ims.uni-stuttgart.de Abstract This paper provides a speci cation of requirements for collocation extraction systems, taking as an example the extraction of noun + verb collocations from German texts." ></td>
	<td class="line x" title="3:230	A hybrid approach to the extraction of habitual collocations and idioms is presented, aiming at a detailed description of collocations and their morphosyntax for natural language generation systems as well as to support learner lexicography." ></td>
	<td class="line x" title="4:230	1 Introduction Since Firth rst described collocations as habitual word combinations in the 1950ies (cf.Firth, 1968), a number of papers focusing on collocation extraction have been published (see the overviews in (Evert, 2004; Bartsch, 2004))." ></td>
	<td class="line x" title="6:230	Most studies concentrate on the extraction from English." ></td>
	<td class="line x" title="7:230	However, the procedures proposed in these studies cannot necessarily be applied to other languages as English stands out, e.g. with respect to con gurationality." ></td>
	<td class="line x" title="8:230	They rely on the fact that the syntax of English (and of all con gurational languages) provides positional clues to the grammatical function of noun phrases, and they exploit this concept by means of window-based, adjacency-based or pattern-based extraction, combined with association measures to identify co-occurrences that are more frequent than statistically expectable." ></td>
	<td class="line x" title="9:230	What these procedures do not cover is semantic-oriented de nitions like (a) and (b)." ></td>
	<td class="line x" title="10:230	a. A collocation is a combination of a free (autosematic) element (the base) and a lexically determined (synsemantic) element (the collocate, which may lose (some of) its meaning in a collocation) (adapted from (Hausmann, 1979; Hausmann, 1989; Hausmann, 2003))." ></td>
	<td class="line x" title="11:230	b. A collocation is a word combination whose semantic and/or syntactic properties cannot be fully predicted from those of its components, and which therefore has to be listed in a lexicon (Evert, 2004)." ></td>
	<td class="line oc" title="12:230	We argue that linguistic knowledge could not only improve results (Krenn, 2000b; Smadja, 1993) but is essential when extracting collocations from certain languages: this knowledge provides other applications (or a lexicon user, respectively) with a ne-grained description of how the extracted collocations are to be used in context." ></td>
	<td class="line x" title="13:230	Additional requirements resulting from the needs of dictionary users are described in (Hausmann, 2003; Heid and Gouws, 2005) and are of interest not only in lexicography but can also be transferred to the eld of natural language generation." ></td>
	<td class="line x" title="14:230	These requirements in uence the development of collocation extraction systems, which motivates this paper." ></td>
	<td class="line x" title="15:230	The structure of the paper is as follows: In chapter 2, the requirements, depending on factors like the targeted language, are presented." ></td>
	<td class="line x" title="16:230	We then discuss and suggest methods to meet the given needs." ></td>
	<td class="line x" title="17:230	A documentation of ongoing work on the extraction of noun + verb collocations from German texts is given in chapter 3." ></td>
	<td class="line x" title="18:230	Chapter 4 gives a conclusion and an outlook on work still to be done." ></td>
	<td class="line x" title="19:230	2 Collocation Extraction Tools: Requirements The development of a collocation extraction tool depends on the following conditions: 1." ></td>
	<td class="line x" title="20:230	properties of the targeted language 41 2." ></td>
	<td class="line x" title="21:230	the targeted application 3." ></td>
	<td class="line x" title="22:230	the kinds of collocations to be extracted 4." ></td>
	<td class="line x" title="23:230	the degree of detail Whereas issues 1 to 3 deal with the collocation itself, issue 4 is focused at the collocation in context, i.e. its behaviour (from a syntagmatic analysis point of view) or, respectively, its use (from a generation perspective)." ></td>
	<td class="line x" title="24:230	2.1 Language factors One of the most important factors is, of course, the targeted language and its main characteristics with respect to word formation and word order." ></td>
	<td class="line x" title="25:230	Depending on word and constituent order, the pros and cons of positional vs. relational extraction patterns need to be considered." ></td>
	<td class="line x" title="26:230	Positional patterns (based on adjacency or a window) are adequate for con gurational languages, but in languages with rather free word order, words belonging to a phrase or collocation do not necessarily occur within a prede ned span1." ></td>
	<td class="line oc" title="27:230	Extracting word combinations using relational patterns (represented by part of speech (PoS) tags or dependency rules) offers a higher level of abstraction and improves the results (cf.(Krenn, 2000b; Smadja, 1993))." ></td>
	<td class="line x" title="29:230	However, this requires part of speech tagging and possibly partial parsing." ></td>
	<td class="line x" title="30:230	A system extracting word combinations by applying relational patterns, obviously pro ts from language speci c knowledge about phrase and sentence structure and word formation." ></td>
	<td class="line x" title="31:230	One example is the order of adjective + noun pairs: in English and German, the adjective occurs left of the noun, whereas in French, the adjective can occur left or right of the noun." ></td>
	<td class="line x" title="32:230	Another example is compounding, handled differently in different languages: noun + noun in English, typically separated by a white space (e.g. death penalty) vs. noun + prepositional phrase in French (e.g. peine de mort) vs. compound noun in German (e.g. Todesstrafe)." ></td>
	<td class="line x" title="33:230	Consequently, language speci c word formation rules need to be considered when designing extraction patterns." ></td>
	<td class="line x" title="34:230	For languages with a rich inectional morphology where the individual word forms are rather rare, frequency counts and results 1In German, e.g., in usual verb second constructions with a full verb in the left sentence bracket (topological eld theory see (W llstein-Leisten et al. , 1997)), particles of particle verbs appear in the right sentence bracket." ></td>
	<td class="line x" title="35:230	The middle eld (containing arguments and possibly adjuncts of the verb) is of undetermined length." ></td>
	<td class="line x" title="36:230	of statistical analyses are little reliable." ></td>
	<td class="line x" title="37:230	To allow a grouping of words sharing the same lemma, lemmatisation is crucial." ></td>
	<td class="line x" title="38:230	2.2 Application factors Other important factors are the targeted application (i.e. analysis vs. generation) and, to some extent resulting from it, factors (3)." ></td>
	<td class="line x" title="39:230	and (4.), above." ></td>
	<td class="line x" title="40:230	Depending on the purpose of the tool (or lexicon, respectively), the collocation de nition chosen as an outline may vary, e.g. including transparent and regular collocations (cf.(Tutin, 2004)) for generation purposes, but excluding them for analysis purposes." ></td>
	<td class="line x" title="42:230	In addition, a more detailed description of the use of collocations in context (e.g. information about preferences with respect to the determiner, etc)." ></td>
	<td class="line x" title="43:230	is needed for generation purposes than for text analysis." ></td>
	<td class="line x" title="44:230	2.3 Factors of collocation de nition Collocations can be distinguished on two levels: the formal level and the content level." ></td>
	<td class="line x" title="45:230	On the formal level, a collocation can be classi ed according to the structural relation between its elements." ></td>
	<td class="line x" title="46:230	Typical patterns are shown in table 12 (taken from (Heid and Gouws, 2005))." ></td>
	<td class="line x" title="47:230	On the content level, there are regular, transparent, and opaque collocations (according to (Tutin, 2004)) and, taking de nition (b) into account, idioms as well." ></td>
	<td class="line x" title="48:230	However, as a classi cation at the content level needs detailed semantic description, we see no means of accomplishing this goal other than manually at the moment." ></td>
	<td class="line x" title="49:230	2.4 Contextual factors (Hausmann, 2003; Heid and Gouws, 2005; Evert et al. , 2004) argue that collocations have strong preferences with respect to their morphosyntax (see examples (1) and (2)) and may be combined (see example (3))." ></td>
	<td class="line x" title="50:230	The collocation in example (1) (to charge somebody) is restricted with respect to the determiner (null determiner) of the base, whereas the same base shows a strong preference for a (de nite or inde nite) determiner when used 2Abbreviations in table 1: advl adverbial prd predicative subj subject obj object pobj prepositional object dat dative case gen genitive case quant quantifying 42 No." ></td>
	<td class="line x" title="51:230	Type Example 1 N + Adj tiefer Schlaf 2 Adj + Adv tief rot 3 V + Adv tief schlafen 4 V + NPa0a2a1a4a3a6a5 Baukl tze staunen 5 V + Na7a9a8a11a10a13a12 Frage + sich stellen 6 V + Na1a14a0a14a15 Anforderungen + gen gen 7 V + Na16a17a10a13a12 Frage + aufwerfen 8 V + PPa18a19a16a17a10a13a12 zu + Darstellung + gelangen 9 V + Adja18a19a20 a1 verr ckt spielen 10 N + Na21a6a22a24a23 Einreichung des Antrags 11 Na25 a8 a0 a23 a15 + N ein Schwarm Heringe the category containing the base is underlined." ></td>
	<td class="line x" title="52:230	Table 1: Collocational patterns with a different collocate (example (2), to drop a lawsuit)." ></td>
	<td class="line x" title="53:230	Example (3) shows two collocations sharing the base can form a collocational sequence (example taken from (Heid and Gouws, 2005))." ></td>
	<td class="line x" title="54:230	(1)  Anklage erheben (2) die/eine Anklage fallenlassen (3) Kritik ben + scharfe Kritik scharfe Kritik ben For both natural language generation systems and lexicography, such information is highly relevant." ></td>
	<td class="line x" title="55:230	Therefore, the extraction of contextual information (called context parameters in the following) should be integrated into the collocation extraction process." ></td>
	<td class="line x" title="56:230	3 Extracting noun + verb collocations from German The standard architecture for collocation extraction systems contains three stages (cf.(Krenn, 2000)): a more or less detailed linguistic analysis of the corpus text (preprocessing), an extraction step and a statistic ltering of the extracted word combinations." ></td>
	<td class="line x" title="58:230	We follow this architecture (see gure 1)." ></td>
	<td class="line x" title="59:230	However, our hypothesis differs from other approaches." ></td>
	<td class="line x" title="60:230	Collocations are often restricted with respect to their morphosyntax." ></td>
	<td class="line x" title="61:230	We test to what extent they can be identi ed via these restrictions." ></td>
	<td class="line x" title="62:230	3.1 Approach In an experiment, we extracted relational word combinations (verb + subject/object pairs) from German newspaper texts." ></td>
	<td class="line x" title="63:230	The syntactic patterns for the extraction of these combinations concentrate on verbnal constructions as in example (4) and verb second constructions with a modal verb in the left sentence bracket according to the topological eld theory (see (W llstein-Leisten et al. , 1997)) as in example (5)." ></td>
	<td class="line x" title="64:230	The reason is that, in these constructions, the particle forms one word with the verb (see example (6)), as opposed to usual verb second constructions (see example (7))." ></td>
	<td class="line x" title="65:230	Thus, we need not recombine verb + particle groups that appear separatedly." ></td>
	<td class="line x" title="66:230	(4) wenn Wien einen Antrag auf Vollmitgliedschaft stellt." ></td>
	<td class="line x" title="67:230	(if Vienna an application for full membership puts) (if Vienna applies for full membership) (5)  kann Wien einen Antrag auf Vollmitgliedschaft stellen." ></td>
	<td class="line x" title="68:230	(might Vienna an application for full membership put.) (Vienna might apply for full membership.)" ></td>
	<td class="line x" title="69:230	(6)  , da er ein Schild auf stellt." ></td>
	<td class="line x" title="70:230	(that he a sign upputs) (that he puts up a sign) (7) Er stellt ein Schild auf." ></td>
	<td class="line x" title="71:230	(He puts a sign up.) (He puts up a sign.)" ></td>
	<td class="line x" title="72:230	Preprocessing As data, we used a collection of 300 million words from German newspaper texts dating from 1987 to 1993." ></td>
	<td class="line x" title="73:230	The corpus is tokenized and PoStagged by the Treetagger (Schmid, 1994), then chunk annotated by YAC (Kermes, 2003)." ></td>
	<td class="line x" title="74:230	The chunker YAC determines phrase boundaries and heads, and disambiguates agreement information as far as possible." ></td>
	<td class="line x" title="75:230	It is based on the corpus query language cqp (Christ et al. , 1999)3, which can in turn be used to query the chunk annotations." ></td>
	<td class="line x" title="76:230	Data Extraction The syntactic patterns used to extract verb + subject/object combinations are based on PoS tags and chunk information." ></td>
	<td class="line x" title="77:230	These patterns are represented using cqp macros (see gure 2)." ></td>
	<td class="line x" title="78:230	The cqp syntax largely overlaps with regular expressions." ></td>
	<td class="line x" title="79:230	3http://www.ims.unistuttgart.de/projekte/CorpusWorkbench/ 43 cqp postprocessing macros preprocessing collocation identification interpretationanalysis (STTS tagset)(newspaper texts) PoS tagging tokenizing, information agreement morphology TreeTagger phrase boundaries, extraction lexicons stat." ></td>
	<td class="line x" title="80:230	filtering NLP resources corpus data lemma and IMSLex wrt." ></td>
	<td class="line x" title="81:230	morphosyntax pairs and their features extraction of noun + verb database morphology semantic classification agreement (annotation of partial parsing YAC Figure 1: Tool architecture ( 1) MACRO n_vfin(0) ( 2) ( ( 3) [pos = '(KOUS|VMFIN)'] ( 4) []* ( 5) a26 npa27 ( 6) [!pp ( 7) & _.np_f not contains 'ne' ( 8) & _.np_f not contains 'pron' ( 9) & _.np_f not contains 'meas' (10) & _.np_h != '@card@']+ (11) a26 /npa27 (12)[pos != '($.|KOUS|VMFIN)']* (13)[pos = 'V.*']+ (14)[pos = '($.|KON)'] (15)) (16); Figure 2: sample macro Line (1) of gure 2 contains the name of the macro and the number of its parameters." ></td>
	<td class="line x" title="82:230	In line (3), a word PoS tagged KOUS (subordinating conjunction) or VMFIN ( nite modal verb) is requested, followed by an arbitrary number (*) of words without any restrictions (line (4))." ></td>
	<td class="line x" title="83:230	Line (5) indicates the start of a nominal phrase (np), line (11) its end." ></td>
	<td class="line x" title="84:230	The elements within this np (one or more words, as indicated by +) must not be part of a prepositional phrase (pp) to avoid the extraction of pp + verb (line (6), see example (8))." ></td>
	<td class="line x" title="85:230	In addition, the np must be neither a named entity (ne, see line (7)) nor a pronoun (pron, line (8)) nor an np of measure (meas, line (9), see example (9)), nor must its head be a cardinal number (card, line (10), see example (10))." ></td>
	<td class="line x" title="86:230	An arbitrary number of words may follow the np (punctuation marks (PoS tagged $.), subordinating conjunctions and nite modal verbs excluded)." ></td>
	<td class="line x" title="87:230	At least one verb is required (line (13), all PoS tags for verbs start with a capital V4)." ></td>
	<td class="line x" title="88:230	Line (14) indicates the end of the subclause or sentence." ></td>
	<td class="line x" title="89:230	(8)  kann [zur Verf gung]a28a6a28 gestellt werden." ></td>
	<td class="line x" title="90:230	(9)  weil davon j hrlich [3,5 Tonnen]a29 a28a31a30a33a32a35a34a2a36 eingef hrt werden." ></td>
	<td class="line x" title="91:230	(10)  obwohl er [1989]a29 a28a38a37a9a34a14a39a4a40 noch dort arbeitete." ></td>
	<td class="line x" title="92:230	By applying the macro to the corpus, all sequences of words matching the pattern are extracted." ></td>
	<td class="line x" title="93:230	From these sequences, the following information is made explicit (cf.(Heid and Ritz, 2005)): a41 lemma of the noun (potential base) a41 lemma of the verb (potential collocate) a41 number of the noun (singular, plural) a41 case of the noun 4The search condition is underspeci ed with respect to the niteness and the role of the verb (auxiliary, modal or full verb)." ></td>
	<td class="line x" title="95:230	Thus, line (13) matches verbal complexes." ></td>
	<td class="line x" title="96:230	It also covers cases where full verbs are accidentally PoS tagged modal or auxiliary verbs." ></td>
	<td class="line x" title="97:230	44 a42 determination of the noun (de nite, inde nite, null, demonstrative, quanti er) a42 modi cation of the noun (adjective, cardinal number, genitive np, compound noun etc.) a42 negation (yes/no) a42 auxiliaries and modal verbs a42 original phrase from the corpus For each instance found, the lemmas of noun and verb along with all the context parameters mentioned above are stored as feature value pairs in a relational data base." ></td>
	<td class="line x" title="98:230	The database can be queried via SQL." ></td>
	<td class="line x" title="99:230	See gure 3 for a sample query asking for distinct lemma pairs, ordered by frequency (in descending order), and gures 5 and 4 for more speci c queries and some of their results." ></td>
	<td class="line x" title="100:230	SELECT COUNT(*) AS f, n_lemma, v_lemma FROM comfea1 GROUP BY n_lemma, v_lemma ORDER BY f DESC; Figure 3: sample query Filtering The instances extracted in the previous step are grouped according to noun and verb lemmas, i.e. instances of the same lemma pair form one group." ></td>
	<td class="line x" title="101:230	Within these groups, a relative frequency distribution is computed for each of the features." ></td>
	<td class="line x" title="102:230	For queriability reasons, the results of this postprocessing are also stored in the database, as shown in gure 1." ></td>
	<td class="line x" title="103:230	A word combination is chosen as a collocation candidate if a preference (speci ed by a threshold of e.g. 60% of the occurrences) for a certain feature value (singular / plural, presence / absence of a determiner, de nite / inde nite / demonstrative / possessive / quantifying determiner, presence of modifying elements) is discovered." ></td>
	<td class="line x" title="104:230	3.2 Results From 300 million words, we extracted more than 1.3 million noun + verb combinations, the instances of 726,488 different lemma pairs." ></td>
	<td class="line x" title="105:230	10,934 of these lemma pairs appeared with a minimum SELECT COUNT(*) AS f, n_lemma, v_lemma FROM comfea1 WHERE neg = + GROUP BY n_lemma, v_lemma ORDER BY f DESC; f | n_lemma | v_lemma 1152| Rede | sein 748 | Angabe | machen 322 | Einigung | erzielen 228 | Chance | haben 217 | Forderung | erf llen 188 | Problem | l sen 151 | Rolle | spielen 131 | Auskunft | geben 127 | Stellungnahme | abgeben 120 | Alternative | geben 110 | Interesse | haben 110 | Angabe | best tigen 102 | Geld | haben Figure 4: sample query: word combinations from negated phrases frequency of 10." ></td>
	<td class="line x" title="106:230	Sample results are shown in gure 65." ></td>
	<td class="line x" title="107:230	We evaluated collocation candidates with a frequency of at least 100." ></td>
	<td class="line x" title="108:230	Within the 323 most frequent collocation candidates, we found 213 collocations (including 11 idioms)." ></td>
	<td class="line x" title="109:230	This corresponds to a precision of 66% (see table 26)." ></td>
	<td class="line x" title="110:230	As a comparison, a window-based study was carried out on the same (PoS-tagged) data." ></td>
	<td class="line x" title="111:230	In this study, the window was de ned in a way that up to two tokens (excluding sentence boundaries and nite full verbs) were allowed to appear between a noun (PoS tagged NN) and a nite full verb (PoS tagged VVFIN)." ></td>
	<td class="line x" title="112:230	Log-likelihood7 was used as an association measure." ></td>
	<td class="line x" title="113:230	The precision of this approach is 41%8." ></td>
	<td class="line x" title="114:230	5Abbreviations in gure 6: c rated as a collocation in evaluation i rated as an idiom in evaluation." ></td>
	<td class="line x" title="115:230	For chosing collocation candidates, a threshold of 60% is used." ></td>
	<td class="line x" title="116:230	However, additional preferences are displayed for values greater than 50%." ></td>
	<td class="line x" title="117:230	6Abbreviations in table 2: log-l window-based approach using log-likelihood feat pattern-based approach using morphosyntactic features 7www.collocations.de 8Note that partial matches, such as Verf gung + stellen 45 SELECT COUNT(*) AS f, n_lemma, v_lemma FROM comfea1 WHERE cas = Akk GROUP BY n_lemma, v_lemma ORDER BY f DESC; f | n_lemma | v_lemma 507 | Beitrag | leisten 237 | Antrag | stellen 173 | Eindruck | erwecken 173 | Weg | nden 167 | Umsatz | steigern 145 | Hut | nehmen 140 | Bericht | vorlegen 135 | Betrieb | aufnehmen 121 | Sprung | schaffen 120 | Ausschlag | geben 116 | Mut | haben 111 | Sitz | haben 106 | Weg | ebnen 105 | Zuschlag | erhalten 104 | Platz | nden 100 | Anspruch | haben 94 | Tod | feststellen 93 | Zusammenhang | geben 90 | Vertrag | unterzeichnen 90 | Riegel | vorschieben Figure 5: sample query: verb + accusative object However, the evaluation criteria from de nitions (a) and (b) remain vague or even contradictory for some of the results." ></td>
	<td class="line x" title="118:230	First, there is the problem of semantic equivalence: does the combination express more than its elements (consider example (11))?" ></td>
	<td class="line x" title="119:230	Secondly, de nitions (a) and (b) may judge the same example differently: Anteil nehmen (example (12)) is usually agreed upon to be a support verb construction, but the distinction of the noun Anteil as the base (making the main contribution to the meaning) is questionable." ></td>
	<td class="line x" title="120:230	On the other hand, its unpredictable syntactic properties (e.g. null determiner) and semantics (partial loss of meaning of the collocate nehmen) make it clear that this combination has to be listed in a lexicon." ></td>
	<td class="line x" title="121:230	(without the corresponding preposition), have been treated as correct matches in 72 cases." ></td>
	<td class="line x" title="122:230	log-l feat collocation candidates 700 323 collocations (manually veri ed) 290 213 precision 41% 66% Table 2: evaluation results For evaluation purposes, combinations judged collocations by either (or both) of the de nitions were marked as correct matches." ></td>
	<td class="line x" title="123:230	In cases like example (11), combinations were marked as correct matches if no alternative collocate existed for describing the denoted situation or event." ></td>
	<td class="line x" title="124:230	(11) Chance + haben (to have a/the chance) (12) Anteil + nehmen (to commiserate) 4 Conclusion and Outlook We presented a system for collocation extraction that takes into account the behaviour or use of collocations in context." ></td>
	<td class="line x" title="125:230	Pro ting from linguistic information (PoS tagging, chunking), the tool reaches a precision of 66% on the top 323 candidates by frequency." ></td>
	<td class="line x" title="126:230	On the same data, a windowbased approach relying only on PoS information reached a precision of 41%." ></td>
	<td class="line x" title="127:230	As the extracted word combinations as well as their context parameters (including the original evidence from the corpus) are stored in a database, the tool also supports explorative research in lexicography." ></td>
	<td class="line x" title="128:230	However, there are some enhancements worth doing: Especially when dealing with low frequencies, relative frequencies lack reliability." ></td>
	<td class="line x" title="129:230	Therefore, we suggest computing a con dence interval as proposed in (Evert, 2004b; Heid and Ritz, 2005; Ritz, 2005)." ></td>
	<td class="line x" title="130:230	As indicated in gure 1, several postprocessing steps can be added to the system, e.g. enabling a sorting of collocation candidates with compound nouns by the morphological heads of their base." ></td>
	<td class="line x" title="131:230	In order to get more data, the extraction from verb rst and verb second constructions is also possible." ></td>
	<td class="line x" title="132:230	To complete the tool, extraction patterns for collocations of different syntactic relations (cf.table 1) could be designed." ></td>
	<td class="line x" title="134:230	46 n_lemma | v_lemma | total | restrictions | Polizei | mitteilen | 5689| sg(100%), det(99.49%), def(99.49%), modif(53.84%) c | Rede | sein | 2144| sg(99.81%), det(99.16%), quant(53.59%) | Sprecher | mitteilen | 1401| sg(94.15%), det(99.21%), indef(93.36%), modif(68.31%) c | Fall | sein | 1233| sg(99.03%), det(97.32%), def(96.03%) | Kantonspolizei | mitteilen | 1094| sg(99.82%), det(59.60%), def(59.60%), modif(100%) | Beh rde | mitteilen | 952 | pl(82.14%), det(99.89%), def(99.37%), modif(63.13%) c | Stellung | nehmen | 831 | sg(99.88%), no_det(88.21%) c | Angabe | machen | 802 | pl(98.50%), det(93.64%), quant(93.27%) | Polizeisprecher | mitteilen | 737 | sg(90.91%), det(92.67%), indef(90.77%), modif(100%) i | Rolle | spielen | 724 | sg(98.62%), det(97.93%), indef(65.61%) c | Problem | l sen | 690 | det(89.13%), def(64.06%), modif(51.88%) | Zeitung | berichten | 670 | sg(94.78%), det(94.78%), def(91.79%), modif(83.28%) | Nachrichtenagentur| melden | 667 | sg(98.95%), det(98.20%), def(97.90%), modif(100%) c | Rechnung | tragen | 661 | sg(100%), no_det(98.94%) | Unternehmen | mitteilen | 655 | sg(98.16%), det(98.32%), def(98.32%), modif(66.26%) c | Chance | haben | 614 | sg(85.67%), det(86.31%) c | Beitrag | leisten | 575 | sg(96.70%), det(95.65%), indef(52.70%), modif(62.54%) | Polizei | berichten | 564 | sg(100%), det(98.40%), def(98.40%) c | Einigung | erzielen | 551 | sg(99.82%), det(92.20%), quant(58.26%) | Sprecher | sagen | 508 | sg(76.38%), det(97.83%), indef(76.18%), modif(56.69%) c | Arbeit | aufnehmen | 492 | sg(98.37%), det (98.37%), poss(76.62%) c | Ziel | erreichen | 476 | sg(78.78%), det(96.22%) | Nachrichtenagentur| berichten | 454 | sg(100%), det(98.90%), def(98.90%), modif(100%) c | Druck | aus ben | 451 | sg(100%), no_det(88.70%), modif(74.72%) c | Erfolg | haben | 438 | sg(99.32%), no_det(78.54%) | Frau | sein | 425 | sg(54.59%), det(50.12%), modif(61.41%) | Land | verlassen | 421 | sg(99.76%), det(98.57%), def(86.70%) c | Frage | stellen | 419 | sg(71.60%), det(79.00%), def(68.26%) Figure 6: sample results 47 References Sabine Bartsch." ></td>
	<td class="line x" title="135:230	2004." ></td>
	<td class="line x" title="136:230	Structural and functional properties of collocations in English." ></td>
	<td class="line x" title="137:230	A corpus study of lexical and pragmatic constraints on lexical cooccurrence Narr, T bingen." ></td>
	<td class="line x" title="138:230	Oliver Christ, Bruno M. Schulze, Anja Hofmann and Esther K nig." ></td>
	<td class="line x" title="139:230	1999." ></td>
	<td class="line x" title="140:230	The IMS Corpus Workbench: Corpus Query Processor (CQP)." ></td>
	<td class="line x" title="141:230	Users manual." ></td>
	<td class="line x" title="142:230	Institut f r maschinelle Sprachverarbeitung, Universit t Stuttgart." ></td>
	<td class="line x" title="143:230	Jonathan Crowther, Sheila Dignen and Diana Lea." ></td>
	<td class="line x" title="144:230	2002." ></td>
	<td class="line x" title="145:230	Oxford Collocations Dictionary for students of English." ></td>
	<td class="line x" title="146:230	Oxford University Press." ></td>
	<td class="line x" title="147:230	Stefan Evert." ></td>
	<td class="line x" title="148:230	2004." ></td>
	<td class="line x" title="149:230	The Statistics of Word Cooccurences Word Pairs and Collocations." ></td>
	<td class="line x" title="150:230	PhD thesis." ></td>
	<td class="line x" title="151:230	Institut f r Maschinelle Sprachverarbeitung (IMS), Universit t Stuttgart." ></td>
	<td class="line x" title="152:230	Stefan Evert, Ulrich Heid and Kristina Spranger." ></td>
	<td class="line x" title="153:230	2004." ></td>
	<td class="line x" title="154:230	Identifying morphosyntactic preferences in collocations." ></td>
	<td class="line x" title="155:230	In M. T. Lino, M. F. Xavier, F. Ferreira, R. Costa and R. Silva (eds.): Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC 2004), p. 907 910." ></td>
	<td class="line x" title="156:230	Lisbon, Portugal." ></td>
	<td class="line x" title="157:230	Stefan Evert." ></td>
	<td class="line x" title="158:230	2004." ></td>
	<td class="line x" title="159:230	The statistical analysis of morphosyntactic distributions." ></td>
	<td class="line x" title="160:230	In M. T. Lino et al.(eds.): Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC 2004), p. 1539 1542." ></td>
	<td class="line x" title="162:230	Lisbon, Portugal." ></td>
	<td class="line x" title="163:230	John R. Firth." ></td>
	<td class="line x" title="164:230	1968." ></td>
	<td class="line x" title="165:230	F.R. Palmer (ed.): Selected Papers of J.R. Firth 1952-59." ></td>
	<td class="line x" title="166:230	Longman, London." ></td>
	<td class="line x" title="167:230	Franz Josef Hausmann." ></td>
	<td class="line x" title="168:230	1979." ></td>
	<td class="line x" title="169:230	Un dictionnaire de collocations est-il possible?" ></td>
	<td class="line x" title="170:230	Travaux de Linguistique et de Litterature XVII(1), p. 187-195." ></td>
	<td class="line x" title="171:230	Centre de philologie et de littrature romanes de luniversit de Strasbourg." ></td>
	<td class="line x" title="172:230	Franz Josef Hausmann." ></td>
	<td class="line x" title="173:230	1989." ></td>
	<td class="line x" title="174:230	Le dictionnaire de collocations." ></td>
	<td class="line x" title="175:230	In F. J. Hausmann et al.(eds.): W rterb cher, Dictionaries, Dictionnaires, p. 1010 1019." ></td>
	<td class="line x" title="177:230	De Gruyter, Berlin." ></td>
	<td class="line x" title="178:230	Franz Josef Hausmann." ></td>
	<td class="line x" title="179:230	2003." ></td>
	<td class="line x" title="180:230	Was sind eigentlich Kollokationen?" ></td>
	<td class="line x" title="181:230	In K. Steyer (ed.): Wortverbindungen mehr oder weniger fest." ></td>
	<td class="line x" title="182:230	Jahrbuch des Instituts f r Deutsche Sprache 2003:309 334." ></td>
	<td class="line x" title="183:230	De Gruyter, Berlin." ></td>
	<td class="line x" title="184:230	Ulrich Heid and Rufus H. Gouws." ></td>
	<td class="line x" title="185:230	2005." ></td>
	<td class="line x" title="186:230	A model for a multifunctional electronic dictionary of collocations." ></td>
	<td class="line x" title="187:230	Institut f r Maschinelle Sprachverarbeitung (IMS), Universit t Stuttgart, submitted to EURALEX 2006." ></td>
	<td class="line x" title="188:230	Ulrich Heid and Julia Ritz." ></td>
	<td class="line x" title="189:230	2005." ></td>
	<td class="line x" title="190:230	Extracting collocations and their contents from corpora." ></td>
	<td class="line x" title="191:230	F. Kiefer et al.(eds.): Papers in Computational Lexicography: Complex 2005." ></td>
	<td class="line x" title="193:230	Hungarian Academy of Sciences, Budapest." ></td>
	<td class="line x" title="194:230	Hannah Kermes." ></td>
	<td class="line x" title="195:230	2003." ></td>
	<td class="line x" title="196:230	Off-line (and On-line) Text Analysis for Computational Lexicography." ></td>
	<td class="line x" title="197:230	Arbeitspapiere des Instituts f r Maschinelle Sprachverarbeitung (AIMS):9(3)." ></td>
	<td class="line x" title="198:230	Stuttgart." ></td>
	<td class="line x" title="199:230	Brigitte Krenn." ></td>
	<td class="line x" title="200:230	2000." ></td>
	<td class="line x" title="201:230	The Usual Suspects: Data Oriented Models for the Identi cation and Representation of Lexical Collocations." ></td>
	<td class="line x" title="202:230	PhD thesis." ></td>
	<td class="line x" title="203:230	DFKI und Universit t des Saarlandes, Saarbr cken." ></td>
	<td class="line x" title="204:230	Brigitte Krenn." ></td>
	<td class="line x" title="205:230	2000." ></td>
	<td class="line x" title="206:230	Collocation Mining: Exploiting Corpora for Collocation Identi cation and Representation." ></td>
	<td class="line x" title="207:230	W. Z hlke and E. G. SchukatTalamazzini (eds.): Proceedings of KONVENS 2000, p. 209-214." ></td>
	<td class="line x" title="208:230	Ilmenau, Deutschland." ></td>
	<td class="line x" title="209:230	Julia Ritz." ></td>
	<td class="line x" title="210:230	2005." ></td>
	<td class="line x" title="211:230	Entwicklung eines Systems zur Extraktion von Kollokationen mittels morphosyntaktischer Features." ></td>
	<td class="line x" title="212:230	Diploma Thesis." ></td>
	<td class="line x" title="213:230	Institut f r Maschinelle Sprachverarbeitung (IMS), Universit t Stuttgart." ></td>
	<td class="line x" title="214:230	Helmut Schmid." ></td>
	<td class="line x" title="215:230	1994." ></td>
	<td class="line x" title="216:230	Probabilistic part-of-speech tagging using decision trees." ></td>
	<td class="line x" title="217:230	D. Jones and H. Somers (eds.): Proceedings of the International Conference on New Methods in Language Processing (NeMLaP)." ></td>
	<td class="line x" title="218:230	Manchester, U.K. Frank Smadja." ></td>
	<td class="line x" title="219:230	1993." ></td>
	<td class="line x" title="220:230	Retrieving Collocations from Text: Xtract." ></td>
	<td class="line x" title="221:230	Computational Linguistics (19), p. 143-177." ></td>
	<td class="line x" title="222:230	Manchester, U.K. Agns Tutin." ></td>
	<td class="line x" title="223:230	2004." ></td>
	<td class="line x" title="224:230	Pour une modlisation dynamique des collocations dans les textes." ></td>
	<td class="line x" title="225:230	G. Williams and S. Vessier (eds.): Proceedings of the Eleventh EURALEX International Congress." ></td>
	<td class="line x" title="226:230	Lorient, France." ></td>
	<td class="line x" title="227:230	Angelika W llstein-Leisten, Axel Heilmann, Peter Stepan and Sten Vikner." ></td>
	<td class="line x" title="228:230	1997." ></td>
	<td class="line x" title="229:230	Deutsche Satzstruktur." ></td>
	<td class="line x" title="230:230	Stauffenburg, T bingen ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N07-1037
Extracting Semantic Orientations of Phrases from Dictionary
Takamura, Hiroya;Inui, Takashi;Okumura, Manabu;"></td>
	<td class="line x" title="1:300	Proceedings of NAACL HLT 2007, pages 292299, Rochester, NY, April 2007." ></td>
	<td class="line x" title="2:300	c2007 Association for Computational Linguistics Extracting Semantic Orientations of Phrases from Dictionary Hiroya Takamura Precision and Intelligence Laboratory Tokyo Institute of Technology takamura@pi.titech.ac.jp Takashi Inui Integrated Research Institute Tokyo Institute of Technology inui@iri.titech.ac.jp Manabu Okumura Precision and Intelligence Laboratory Tokyo Institute of Technology oku@pi.titech.ac.jp Abstract We propose a method for extracting semantic orientations of phrases (pairs of an adjective and a noun): positive, negative, or neutral." ></td>
	<td class="line x" title="3:300	Given an adjective, the semantic orientation classification of phrases can be reduced to the classification of words." ></td>
	<td class="line x" title="4:300	We construct a lexical network by connecting similar/related words." ></td>
	<td class="line x" title="5:300	In the network, each node has one of the three orientation values and the neighboring nodes tend to have the same value." ></td>
	<td class="line x" title="6:300	We adopt the Potts model for the probability model of the lexical network." ></td>
	<td class="line x" title="7:300	For each adjective, we estimate the states of the nodes, which indicate the semantic orientations of the adjective-noun pairs." ></td>
	<td class="line x" title="8:300	Unlike existing methods for phrase classification, the proposed method can classify phrases consisting of unseen words." ></td>
	<td class="line x" title="9:300	We also propose to use unlabeled data for a seed set of probability computation." ></td>
	<td class="line x" title="10:300	Empirical evaluation shows the effectiveness of the proposed method." ></td>
	<td class="line x" title="11:300	1 Introduction Technology for affect analysis of texts has recently gained attention in both academic and industrial areas." ></td>
	<td class="line x" title="12:300	It can be applied to, for example, a survey of new products or a questionnaire analysis." ></td>
	<td class="line x" title="13:300	Automatic sentiment analysis enables a fast and comprehensive investigation." ></td>
	<td class="line x" title="14:300	The most fundamental step for sentiment analysis is to acquire the semantic orientations of words: positive or negative (desirable or undesirable)." ></td>
	<td class="line x" title="15:300	For example, the word beautiful is positive, while the word dirty is negative." ></td>
	<td class="line x" title="16:300	Many researchers have developed several methods for this purpose and obtained good results." ></td>
	<td class="line x" title="17:300	One of the next problems to be solved is to acquire semantic orientations of phrases, or multi-term expressions, such as high+risk and light+laptop-computer." ></td>
	<td class="line x" title="18:300	Indeed the semantic orientations of phrases depend on context just as the semantic orientations of words do, but we would like to obtain the orientations of phrases as basic units for sentiment analysis." ></td>
	<td class="line x" title="19:300	We believe that we can use the obtained basic orientations of phrases for affect analysis of higher linguistic units such as sentences and documents." ></td>
	<td class="line x" title="20:300	A computational model for the semantic orientations of phrases has been proposed by Takamura et al.(2006)." ></td>
	<td class="line x" title="22:300	However, their method cannot deal with the words that did not appear in the training data." ></td>
	<td class="line x" title="23:300	The purpose of this paper is to propose a method for extracting semantic orientations of phrases, which is applicable also to expressions consisting of unseen words." ></td>
	<td class="line x" title="24:300	In our method, we regard this task as the noun classification problem for each adjective; the nouns that become respectively positive (negative, or neutral) when combined with a given adjective are distinguished from the other nouns." ></td>
	<td class="line x" title="25:300	We create a lexical network with words being nodes, by connecting two words if one of the two appears in the gloss of the other." ></td>
	<td class="line x" title="26:300	In the network, each node has one of the three orientation values and the neighboring nodes expectedly tend to have the same value." ></td>
	<td class="line x" title="27:300	For 292 example, the gloss of cost is a sacrifice, loss, or penalty and these words (cost, sacrifice, loss, and penalty) have the same orientation." ></td>
	<td class="line x" title="28:300	To capture this tendency of the network, we adopt the Potts model for the probability distribution of the lexical network." ></td>
	<td class="line x" title="29:300	For each adjective, we estimate the states of the nodes, which indicate the semantic orientations of the adjective-noun pairs." ></td>
	<td class="line x" title="30:300	Information from seed words is diffused to unseen nouns on the network." ></td>
	<td class="line x" title="31:300	We also propose a method for enlarging the seed set by using the output of an existing method for the seed words of the probability computation." ></td>
	<td class="line x" title="32:300	Empirical evaluation shows that our method works well both for seen and unseen nouns, and that the enlarged seed set significantly improves the classification performance of the proposed model." ></td>
	<td class="line x" title="33:300	2 Related Work The semantic orientation classification of words has been pursued by several researchers." ></td>
	<td class="line x" title="34:300	Some of them used corpora (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003), while others used dictionaries (Kobayashi et al. , 2001; Kamps et al. , 2004; Takamura et al. , 2005; Esuli and Sebastiani, 2005)." ></td>
	<td class="line x" title="35:300	Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases, which had originally been developed for word sentiment classification." ></td>
	<td class="line x" title="36:300	In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g. , phrase NEAR good) is used to determine the orientation." ></td>
	<td class="line oc" title="37:300	Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences." ></td>
	<td class="line x" title="38:300	Their method is similar to Turneys in the sense that cooccurrence with seed words is used." ></td>
	<td class="line x" title="39:300	In addition to individual seed words, Kanayama and Nasukawa (2006) used more complicated syntactic patterns that were manually created." ></td>
	<td class="line x" title="40:300	The four methods above are based on context information." ></td>
	<td class="line x" title="41:300	In contrast, our method exploits the internal structure of the semantic orientations of phrases." ></td>
	<td class="line x" title="42:300	Wilson et al.(2005) worked on phrase-level semantic orientations." ></td>
	<td class="line x" title="44:300	They introduced a polarity shifter." ></td>
	<td class="line x" title="45:300	They manually created the list of polarity shifters." ></td>
	<td class="line x" title="46:300	Inui (2004) also proposed a similar idea." ></td>
	<td class="line x" title="47:300	Takamura et al.(2006) proposed to use based on latent variable models for sentiment classification of noun-adjective pairs." ></td>
	<td class="line x" title="49:300	Their model consists of variables respectively representing nouns, adjectives, semantic orientations, and latent clusters, as well as the edges between the nodes." ></td>
	<td class="line x" title="50:300	The words that are similar in terms of semantic orientations, such as risk and mortality (i.e. , the positive orientation emerges when they are low), make a cluster in their model, which can be an automated version of Inuis or Wilson et al.s idea above." ></td>
	<td class="line x" title="51:300	However, their method cannot do anything for the words that did not appear in the labeled training data." ></td>
	<td class="line x" title="52:300	In this paper, we call their method the latent variable method (LVM)." ></td>
	<td class="line x" title="53:300	3 Potts Model If a variable can have more than two values and there is no ordering relation between the values, the network comprised of such variables is called Potts model (Wu, 1982)." ></td>
	<td class="line x" title="54:300	In this section, we explain the simplified mathematical model of Potts model, which is used for our task in Section 4." ></td>
	<td class="line x" title="55:300	The Potts system has been used as a mathematical model in several applications such as image restoration (Tanaka and Morita, 1996) and rumor transmission (Liu et al. , 2001)." ></td>
	<td class="line x" title="56:300	3.1 Introduction to the Potts Model Suppose a network consisting of nodes and weighted edges is given." ></td>
	<td class="line x" title="57:300	States of nodes are represented by c. The weight between i and j is represented by wij." ></td>
	<td class="line x" title="58:300	Let H(c) denote an energy function, which indicates a state of the whole network: H(c) = summationdisplay ij wij(ci,cj)+summationdisplay iL (ci,ai), (1) where  is a constant called the inverse-temperature, L is the set of the indices for the observed variables, ai is the state of each observed variable indexed by i, and  is a positive constant representing a weight on labeled data." ></td>
	<td class="line x" title="59:300	Function  returns 1 if two arguments are equal to each other, 0 otherwise." ></td>
	<td class="line x" title="60:300	The state is penalized if ci (i  L) is different from ai." ></td>
	<td class="line x" title="61:300	Using H(c), the probability distribution of the network is represented as P(c) = expH(c)}/Z, where Z is a normalization factor." ></td>
	<td class="line x" title="62:300	However, it is computationally difficult to exactly estimate the state of this network." ></td>
	<td class="line x" title="63:300	We resort to a 293 mean-field approximation method that is described by Nishimori (2001)." ></td>
	<td class="line x" title="64:300	In the method, P(c) is replaced by factorized function (c) = producttexti i(ci)." ></td>
	<td class="line x" title="65:300	Then we can obtain the function with the smallest value of the variational free energy: F(c) = summationdisplay c P(c)H(c) summationdisplay c P(c)logP(c) =  summationdisplay i summationdisplay ci i(ci)(ci,ai)  summationdisplay ij summationdisplay ci,cj i(ci)j(cj)wij(ci,cj)  summationdisplay i summationdisplay ci i(ci)logi(ci)." ></td>
	<td class="line x" title="66:300	(2) By minimizing F(c) under the condition that i,summationtext ci i(ci) = 1, we obtain the following fixed point equation for i  L: i(c) = exp((c,ai) +  summationtext j wijj(c))summationtext n exp((n,ai) +  summationtext j wijj(n))." ></td>
	<td class="line x" title="67:300	(3) The fixed point equation for i / L can be obtained by removing (c,ai) from above." ></td>
	<td class="line x" title="68:300	This fixed point equation is solved by an iterative computation." ></td>
	<td class="line x" title="69:300	In the actual implementation, we represent i with a linear combination of the discrete Tchebycheff polynomials (Tanaka and Morita, 1996)." ></td>
	<td class="line x" title="70:300	Details on the Potts model and its computation can be found in the literature (Nishimori, 2001)." ></td>
	<td class="line x" title="71:300	After the computation, we obtain the functionproducttext i i(ci)." ></td>
	<td class="line x" title="72:300	When the number of classes is 2, the Potts model in this formulation is equivalent to the meanfield Ising model (Nishimori, 2001)." ></td>
	<td class="line x" title="73:300	3.2 Relation to Other Models This Potts model with the mean-field approximation has relation to several other models." ></td>
	<td class="line x" title="74:300	As is often discussed (Mackay, 2003), the minimization of the variational free energy (Equation (2)) is equivalent to the obtaining the factorized model that is most similar to the maximum likelihood model in terms of the Kullback-Leibler divergence." ></td>
	<td class="line x" title="75:300	The second term of Equation (2) is the entropy of the factorized function." ></td>
	<td class="line x" title="76:300	Hence the optimization problem to be solved here is a kind of the maximum entropy model with a penalty term, which corresponds to the first term of Equation (2)." ></td>
	<td class="line x" title="77:300	We can find a similarity also to the PageRank algorithm (Brin and Page, 1998), which has been applied also to natural language processing tasks (Mihalcea, 2004; Mihalcea, 2005)." ></td>
	<td class="line x" title="78:300	In the PageRank algorithm, the pagerank score ri is updated as ri = (1d) + d summationdisplay j wijrj, (4) where d is a constant (0  d  1)." ></td>
	<td class="line x" title="79:300	This update equation consists of the first term corresponding to random jump from an arbitrary node and the second term corresponding to the random walk from the neighboring node." ></td>
	<td class="line x" title="80:300	Let us derive the first order Taylor expansion of Equation (3)." ></td>
	<td class="line x" title="81:300	We use the equation for i / L and denote the denominator by Z, for simplicity." ></td>
	<td class="line x" title="82:300	Since expx  1+x, we obtain i(c) = exp( summationtext j wijj(c)) Z  1 +  summationtext j wijj(c) Z = 1Z  + Z  summationdisplay j wijj(c)." ></td>
	<td class="line x" title="83:300	(5) Equation (5) clearly has a quite similar form as Equation (4)." ></td>
	<td class="line x" title="84:300	Thus, the PageRank algorithm can be regarded as an approximation of our model." ></td>
	<td class="line x" title="85:300	Let us clarify the difference between the two algorithms." ></td>
	<td class="line x" title="86:300	The PageRank is designed for two-class classification, while the Potts model can be used for an arbitrary number of classes." ></td>
	<td class="line x" title="87:300	In this sense, the PageRank is an approximated Ising model." ></td>
	<td class="line x" title="88:300	The PageRank is applicable to asymmetric graphs, while the theory used in this paper is based on symmetric graphs." ></td>
	<td class="line x" title="89:300	4 Potts Model for Phrasal Semantic Orientations In this section, we explain our classification method, which is applicable also to the pairs consisting of an adjective and an unseen noun." ></td>
	<td class="line x" title="90:300	4.1 Construction of Lexical Networks We construct a lexical network, which Takamura et al.(2005) call the gloss network, by linking two words if one word appears in the gloss of the other word." ></td>
	<td class="line x" title="92:300	Each link belongs to one of two groups: 294 the same-orientation links SL and the differentorientation links DL." ></td>
	<td class="line x" title="93:300	If a negation word (e.g. , nai, for Japanese) follows a word in the gloss of the other word, the link is a different-orientation link." ></td>
	<td class="line x" title="94:300	Otherwise the links is a same-orientation link1." ></td>
	<td class="line x" title="95:300	We next set weights W = (wij) to links : wij =    1 d(i)d(j) (lij  SL)  1d(i)d(j) (lij  DL) 0 otherwise, (6) where lij denotes the link between word i and word j, and d(i) denotes the degree of word i, which means the number of words linked with word i. Two words without connections are regarded as being connected by a link of weight 0." ></td>
	<td class="line x" title="96:300	4.2 Classification of Phrases Takamura et al.(2005) used the Ising model to extract semantic orientations of words (not phrases)." ></td>
	<td class="line x" title="98:300	We extend their idea and use the Potts model to extract semantic orientations of phrasal expressions." ></td>
	<td class="line x" title="99:300	Given an adjective, the decision remaining to be made in classification of phrasal expressions concerns nouns." ></td>
	<td class="line x" title="100:300	We therefore estimate the state of the nodes on the lexical network for each adjective." ></td>
	<td class="line x" title="101:300	The nouns paring with the given adjective in the training data are regarded as seed words, which we call seen words, while the words that did not appear in the training data are referred to as unseen words." ></td>
	<td class="line x" title="102:300	We use the mean-field method to estimate the state of the system." ></td>
	<td class="line x" title="103:300	If the probability i(c) of a variable being positive (negative, neutral) is the highest of the three classes, then the word corresponding to the variable is classified as a positive (negative, neutral) word." ></td>
	<td class="line x" title="104:300	We explain the reason why we use the Potts model instead of the Ising model." ></td>
	<td class="line x" title="105:300	While only two classes (i.e. , positive and negative) can be modeled by the Ising model, three classes (i.e. , positive, negative and neutral) can be modelled by the Potts model." ></td>
	<td class="line x" title="106:300	For the semantic orientations of words, all the words are sorted in the order of the average orientation value, equivalently the probability of the word being positive." ></td>
	<td class="line x" title="107:300	Therefore, even if the neutral class is 1For English data, a negation should precede a word, in order for the corresponding link to be a different-orientation link." ></td>
	<td class="line x" title="108:300	not explicitly incorporated, we can manually determine two thresholds that define respectively the positive/neutral and negative/neutral boundaries." ></td>
	<td class="line x" title="109:300	For the semantic orientations of phrasal expressions, however, it is impractical to manually determine the thresholds for each of the numerous adjectives." ></td>
	<td class="line x" title="110:300	Therefore, we have to incorporate the neutral class using the Potts model." ></td>
	<td class="line x" title="111:300	For some adjectives, the semantic orientation is constant regardless of the nouns." ></td>
	<td class="line x" title="112:300	We need not use the Potts model for those unambiguous adjectives." ></td>
	<td class="line x" title="113:300	We thus propose the following two-step classification procedure for a given noun-adjective pair < n,a >." ></td>
	<td class="line x" title="114:300	1." ></td>
	<td class="line x" title="115:300	if the semantic orientation of all the instances with a in L is c, then classify < n,a > into c. 2." ></td>
	<td class="line x" title="116:300	otherwise, use the Potts model." ></td>
	<td class="line x" title="117:300	We can also construct a probability model for each noun to deal with unseen adjectives." ></td>
	<td class="line x" title="118:300	However, we focus on the unseen nouns in this paper, because our dataset has many more nouns than adjectives." ></td>
	<td class="line x" title="119:300	4.3 Hyper-parameter Prediction The performance of the proposed method largely depends on the value of hyper-parameter ." ></td>
	<td class="line x" title="120:300	In order to make the method more practical, we propose a criterion for determining its value." ></td>
	<td class="line x" title="121:300	Takamura et al.(2005) proposed two kinds of criteria." ></td>
	<td class="line x" title="123:300	One of the two criteria is an approximated leave-one-out error rate and can be used only when a large labeled dataset is available." ></td>
	<td class="line x" title="124:300	The other is a notion from statistical physics, that is, magnetization: m = summationdisplay i xi/N." ></td>
	<td class="line x" title="125:300	(7) At a high temperature, variables are randomly oriented (paramagnetic phase, m  0)." ></td>
	<td class="line x" title="126:300	At a low temperature, most of the variables have the same direction (ferromagnetic phase, m negationslash= 0)." ></td>
	<td class="line x" title="127:300	It is known that at some intermediate temperature, ferromagnetic phase suddenly changes to paramagnetic phase." ></td>
	<td class="line x" title="128:300	This phenomenon is called phase transition." ></td>
	<td class="line x" title="129:300	Slightly before the phase transition, variables are locally polarized; strongly connected nodes have the same polarity, but not in a global way." ></td>
	<td class="line x" title="130:300	Intuitively, the state of the lexical network is locally polarized." ></td>
	<td class="line x" title="131:300	295 Therefore, they calculate values of m with several different values of  and select the value just before the phase transition." ></td>
	<td class="line x" title="132:300	Since we cannot expect a large labeled dataset to be available for each adjective, we use not the approximated leave-one-out error rate, but the magnetization-like criterion." ></td>
	<td class="line x" title="133:300	However, the magnetization above is defined for the Ising model." ></td>
	<td class="line x" title="134:300	We therefore consider that the phase transition has occurred, if a certain class c begins to be favored all over the system." ></td>
	<td class="line x" title="135:300	In practice, when the maximum of the spatial averages of the approximated probabilities maxcsummationtexti i(c)/N exceeds a threshold during increasing , we consider that the phase transition has occurred." ></td>
	<td class="line x" title="136:300	We select the value of  slightly before the phase transition." ></td>
	<td class="line x" title="137:300	4.4 Enlarging Seed Word Set We usually have only a few seed words for a given adjective." ></td>
	<td class="line x" title="138:300	Enlarging the set of seed words will increase the classification performance." ></td>
	<td class="line x" title="139:300	Therefore, we automatically classify unlabeled pairs by means of an existing method and use the classified instances as seeds." ></td>
	<td class="line x" title="140:300	As an existing classifier, we use LVM." ></td>
	<td class="line x" title="141:300	Their model can classify instances that consist of a seen noun and a seen adjective, but are unseen as a pair." ></td>
	<td class="line x" title="142:300	Although we could classify and use all the nouns that appeared in the training data (with an adjective which is different from the given one), we do not adopt such an alternative, because it will incorporate even non-collocating pairs such as green+idea into seeds, resulting in possible degradation of classification performance." ></td>
	<td class="line x" title="143:300	Therefore, we sample unseen pairs consisting of a seen noun and a seen adjective from a corpus, classify the pairs with the latent variable model, and add them to the seed set." ></td>
	<td class="line x" title="144:300	The enlarged seed set consists of pairs used in newspaper articles and does not include non-collocating pairs." ></td>
	<td class="line x" title="145:300	5 Experiments 5.1 Dataset We extracted pairs of a noun (subject) and an adjective (predicate), from Mainichi newspaper articles (1995) written in Japanese, and annotated the pairs with semantic orientation tags : positive, neutral or negative." ></td>
	<td class="line x" title="146:300	We thus obtained the labeled dataset consisting of 12066 pair instances (7416 different pairs)." ></td>
	<td class="line x" title="147:300	The dataset contains 4459 negative instances, 4252 neutral instances, and 3355 positive instances." ></td>
	<td class="line x" title="148:300	The number of distinct nouns is 4770 and the number of distinct adjectives is 384." ></td>
	<td class="line x" title="149:300	To check the interannotator agreement between two annotators, we calculated  statistics, which was 0.6402." ></td>
	<td class="line x" title="150:300	This value is allowable, but not quite high." ></td>
	<td class="line x" title="151:300	However, positivenegative disagreement is observed for only 0.7% of the data." ></td>
	<td class="line x" title="152:300	In other words, this statistics means that the task of extracting neutral examples, which has hardly been explored, is intrinsically difficult." ></td>
	<td class="line x" title="153:300	We should note that the judgment in annotation depends on which perspective the annotator takes; high+salary is positive from employees perspective, but negative from employers perspective." ></td>
	<td class="line x" title="154:300	The annotators are supposed to take a perspective subjectively." ></td>
	<td class="line x" title="155:300	Our attempt is to imitate annotators decision." ></td>
	<td class="line x" title="156:300	To construct a classifier that matches the decision of the average person, we also have to address how to create an average corpus." ></td>
	<td class="line x" title="157:300	We do not pursue this issue because it is out of the scope of the paper." ></td>
	<td class="line x" title="158:300	As unlabeled data, we extracted approximately 65,000 pairs for each iteration of the 10-fold crossvalidation, from the same news source." ></td>
	<td class="line x" title="159:300	The average number of seed nouns for each ambiguous adjective was respectively 104 in the labeled seed set and 264 in the labeled+unlabeled seed set." ></td>
	<td class="line x" title="160:300	Please note that these figures are counted for only ambiguous adjectives." ></td>
	<td class="line x" title="161:300	Usually ambiguous adjectives are more frequent than unambiguous adjectives." ></td>
	<td class="line x" title="162:300	5.2 Experimental Settings We employ 10-fold cross-validation to obtain the averaged classification accuracy." ></td>
	<td class="line x" title="163:300	We split the data such that there is no overlapping pair (i.e. , any pair in the training data does not appear in the test data)." ></td>
	<td class="line x" title="164:300	Hyperparameter  was set to 1000, which is very large since we regard the labels in the seed set is reliable." ></td>
	<td class="line x" title="165:300	For the seed words added by the classifier, lower  can be better." ></td>
	<td class="line x" title="166:300	Determining a good value for  is regarded as future work." ></td>
	<td class="line x" title="167:300	Hyperparameter  is automatically selected from 2Although Kanayama and Nasukawa (2006) that  for their dataset similar to ours was 0.83, this value cannot be directly compared with our value because their dataset includes both individual words and pairs of words." ></td>
	<td class="line x" title="168:300	296 {0.1, 0.2, , 2.5} for each adjective and each fold of the cross-validation using the prediction method described in Section 4.3." ></td>
	<td class="line x" title="169:300	5.3 Results The results of the classification experiments are summarized in Table 1." ></td>
	<td class="line x" title="170:300	The proposed method succeeded in classifying, with approximately 65% in accuracy, those phrases consisting of an ambiguous adjective and an unseen noun, which could not be classified with existing computational models such as LVM." ></td>
	<td class="line x" title="171:300	Incorporation of unlabeled data improves accuracy by 15.5 points for pairs consisting of a seen noun and an ambiguous adjective, and by 3.5 points for pairs consisting of an unseen noun and an ambiguous adjective, approximately." ></td>
	<td class="line x" title="172:300	The reason why the former obtained high increase is that pairs with an ambiguous adjective3 are usually frequent and likely to be found in the added unlabeled dataset." ></td>
	<td class="line x" title="173:300	If we regard this classification task as binary classification problems where we are to classify instances into one class or not, we obtain three accuracies: 90.76% for positive, 81.75% for neutral, and 86.85% for negative." ></td>
	<td class="line x" title="174:300	This results suggests the identification of neutral instances is relatively difficult." ></td>
	<td class="line x" title="175:300	Next we compare the proposed method with LVM." ></td>
	<td class="line x" title="176:300	The latent variable method is applicable only to instance pairs consisting of an adjective and a seen noun." ></td>
	<td class="line x" title="177:300	Therefore, we computed the accuracy for 6586 instances using the latent variable method and obtained 80.76 %." ></td>
	<td class="line x" title="178:300	The corresponding accuracy by our method was 80.93%." ></td>
	<td class="line x" title="179:300	This comparison shows that our method is better than or at least comparable to the latent variable method." ></td>
	<td class="line x" title="180:300	However, we have to note that this accuracy of the proposed method was computed using the unlabeled data classified by the latent variable method." ></td>
	<td class="line x" title="181:300	5.4 Discussion There are still 3320 (=12066-8746) word pairs which could not be classified, because there are no entries for those words in the dictionary." ></td>
	<td class="line x" title="182:300	However, the main cause of this problem is word segmenta3Seen nouns are observed in both the training and the test datasets because they are frequent." ></td>
	<td class="line x" title="183:300	Ambiguous adjectives are often-used adjectives such as large, small, high, and low." ></td>
	<td class="line x" title="184:300	tion, since many compound nouns and exceedinglysubdivided morphemes are not in dictionaries." ></td>
	<td class="line x" title="185:300	An appropriate mapping from the words found in corpus to entries of a dictionary will solve this problem." ></td>
	<td class="line x" title="186:300	We found a number of proper nouns, many of which are not in the dictionary." ></td>
	<td class="line x" title="187:300	By estimating a class of a proper noun and finding the words that matches the class in the dictionary, we can predict the semantic orientations of the proper noun based on the orientations of the found words." ></td>
	<td class="line x" title="188:300	In order to see the overall tendency of errors, we calculated the confusion matrices both for pairs of an ambiguous adjective and a seen noun, and for pairs of an ambiguous adjective and an unseen noun (Table 2)." ></td>
	<td class="line x" title="189:300	The proposed method works quite well for positive/negative classification, though it finds still some difficulty in correctly classifying neutral instances even after enhanced with the unlabeled data." ></td>
	<td class="line x" title="190:300	In order to qualitatively evaluate the method, we list several word pairs below." ></td>
	<td class="line x" title="191:300	These word pairs are classified by the Potts model with the labeled+unlabeled seed set." ></td>
	<td class="line x" title="192:300	All nouns are unseen; they did not appear in the original training dataset." ></td>
	<td class="line x" title="193:300	Please note again that the actual data is Japanese." ></td>
	<td class="line x" title="194:300	positive instances noun adjective cost low basic price low loss little intelligence high educational background high contagion not-happening version new cafe many salary high commission low negative instances noun adjective damage heavy chance little terrorist many trouble many variation little capacity small salary low disaster many disappointment big knowledge little For example, although both salary and commission are kinds of money, our method captures 297 Table 1: Classification accuracies (%) for various seed sets and test datasets." ></td>
	<td class="line x" title="195:300	Labeled seed set corresponds to the set of manually labeled pairs." ></td>
	<td class="line x" title="196:300	Labeled+unlabeled seed set corresponds to the union of labeled seed set and the set of pairs labeled by LVM." ></td>
	<td class="line x" title="197:300	Seen nouns for test are the nouns that appeared in the training data, while unseen nouns are the nouns that did not appear in the training dataset." ></td>
	<td class="line x" title="198:300	Please note that seen pairs are excluded from the test data." ></td>
	<td class="line x" title="199:300	Unambiguous adjectives corresponds to the pairs with an adjective which has a unique orientation in the original training dataset, while ambiguous adjectives corresponds to the pairs with an adjective which has more than one orientation in the original training dataset." ></td>
	<td class="line x" title="200:300	seed\test seen nouns unseen nouns total labeled 68.24 73.70 69.59 (4494/6586) (1592/2160) (6086/8746) unambiguous ambiguous unambiguous ambiguous 98.15 61.65 94.85 61.85 (1166/1188) (3328/5398) (736/776) (856/1384) labeled+unlabeled 80.93 75.88 79.68 (5330/6586) (1639/2160) (6969/8746) unambiguous ambiguous unambiguous ambiguous 98.15 77.14 94.85 65.25 (1166/1188) (4164/5398) (736/776) (903/1384) Table 2: Confusion matrices of classification result with labeled+unlabeled seed set Potts model seen nouns unseen nouns positive neutral negative sum positive neutral negative sum positive 964 254 60 1278 126 84 30 240 Gold standard neutral 198 1656 286 2140 60 427 104 591 negative 39 397 1544 1980 46 157 350 553 sum 1201 2307 1890 5398 232 668 484 1384 the difference between them; high salary is positive, while low (cheap) commission is also positive." ></td>
	<td class="line x" title="201:300	6 Conclusion We proposed a method for extracting semantic orientations of phrases (pairs of an adjective and a noun)." ></td>
	<td class="line x" title="202:300	For each adjective, we constructed a Potts system, which is actually a lexical network extracted from glosses in a dictionary." ></td>
	<td class="line x" title="203:300	We empirically showed that the proposed method works well in terms of classification accuracy." ></td>
	<td class="line x" title="204:300	Future work includes the following:  We assumed that each word has a semantic orientation." ></td>
	<td class="line x" title="205:300	However, word senses and subjectivity have strong interaction (Wiebe and Mihalcea, 2006)." ></td>
	<td class="line x" title="206:300	 The value of  must be properly set, because lower  can be better for the seed words added by the classifier,  To address word-segmentation problem discussed in Section 5.3, we can utilize the fact that the heads of compound nouns often inherit the property determining the semantic orientation when combined with an adjective." ></td>
	<td class="line x" title="207:300	 The semantic orientations of pairs consisting of a proper noun will be estimated from the named entity classes of the proper nouns such as person name and organization." ></td>
	<td class="line x" title="208:300	298 References Faye Baron and Graeme Hirst." ></td>
	<td class="line x" title="209:300	2004." ></td>
	<td class="line x" title="210:300	Collocations as cues to semantic orientation." ></td>
	<td class="line x" title="211:300	In AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications." ></td>
	<td class="line x" title="212:300	Sergey Brin and Lawrence Page." ></td>
	<td class="line x" title="213:300	1998." ></td>
	<td class="line x" title="214:300	The anatomy of a large-scale hypertextual Web search engine." ></td>
	<td class="line x" title="215:300	Computer Networks and ISDN Systems, 30(17):107117." ></td>
	<td class="line x" title="216:300	Andrea Esuli and Fabrizio Sebastiani." ></td>
	<td class="line x" title="217:300	2005." ></td>
	<td class="line x" title="218:300	Determining the semantic orientation of terms through gloss analysis." ></td>
	<td class="line x" title="219:300	In Proceedings of the 14th ACM International Conference on Information and Knowledge Management (CIKM05), pages 617624." ></td>
	<td class="line x" title="220:300	Vasileios Hatzivassiloglou and Kathleen R. McKeown." ></td>
	<td class="line x" title="221:300	1997." ></td>
	<td class="line x" title="222:300	Predicting the semantic orientation of adjectives." ></td>
	<td class="line x" title="223:300	In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and the 8th Conference of the European Chapter of the Association for Computational Linguistics, pages 174181." ></td>
	<td class="line x" title="224:300	Takashi Inui." ></td>
	<td class="line x" title="225:300	2004." ></td>
	<td class="line x" title="226:300	Acquiring Causal Knowledge from Text Using Connective Markers." ></td>
	<td class="line x" title="227:300	Ph.D. thesis, Graduate School of Information Science, Nara Institute of Science and Technology." ></td>
	<td class="line x" title="228:300	Jaap Kamps, Maarten Marx, Robert J. Mokken, and Maarten de Rijke." ></td>
	<td class="line x" title="229:300	2004." ></td>
	<td class="line x" title="230:300	Using wordnet to measure semantic orientation of adjectives." ></td>
	<td class="line x" title="231:300	In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC04), volume IV, pages 11151118." ></td>
	<td class="line x" title="232:300	Hiroshi Kanayama and Tetsuya Nasukawa." ></td>
	<td class="line x" title="233:300	2006." ></td>
	<td class="line x" title="234:300	Fully automatic lexicon expansion for domain-oriented sentiment analysis." ></td>
	<td class="line x" title="235:300	In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP06), pages 355363." ></td>
	<td class="line x" title="236:300	Nozomi Kobayashi, Takashi Inui, and Kentaro Inui." ></td>
	<td class="line x" title="237:300	2001." ></td>
	<td class="line x" title="238:300	Dictionary-based acquisition of the lexical knowledge for p/n analysis (in Japanese)." ></td>
	<td class="line x" title="239:300	In Proceedings of Japanese Society for Artificial Intelligence, SLUD-33, pages 4550." ></td>
	<td class="line x" title="240:300	Zhongzhu Liu, Jun Luo, and Chenggang Shao." ></td>
	<td class="line x" title="241:300	2001." ></td>
	<td class="line x" title="242:300	Potts model for exaggeration of a simple rumor transmitted by recreant rumormongers." ></td>
	<td class="line x" title="243:300	Physical Review E, 64:046134,1046134,9." ></td>
	<td class="line x" title="244:300	David J. C. Mackay." ></td>
	<td class="line x" title="245:300	2003." ></td>
	<td class="line x" title="246:300	Information Theory, Inference and Learning Algorithms." ></td>
	<td class="line x" title="247:300	Cambridge University Press." ></td>
	<td class="line x" title="248:300	Mainichi." ></td>
	<td class="line x" title="249:300	1995." ></td>
	<td class="line x" title="250:300	Mainichi Shimbun CD-ROM version." ></td>
	<td class="line x" title="251:300	Rada Mihalcea." ></td>
	<td class="line x" title="252:300	2004." ></td>
	<td class="line x" title="253:300	Graph-based ranking algorithms for sentence extraction, applied to text summarization." ></td>
	<td class="line x" title="254:300	In The Companion Volume to the Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, (ACL04), pages 170173." ></td>
	<td class="line x" title="255:300	Rada Mihalcea." ></td>
	<td class="line x" title="256:300	2005." ></td>
	<td class="line x" title="257:300	Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling." ></td>
	<td class="line x" title="258:300	In Proceedings of the Joint Conference on Human Language Technology / Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 411418." ></td>
	<td class="line x" title="259:300	Hidetoshi Nishimori." ></td>
	<td class="line x" title="260:300	2001." ></td>
	<td class="line x" title="261:300	Statistical Physics of Spin Glasses and Information Processing." ></td>
	<td class="line x" title="262:300	Oxford University Press." ></td>
	<td class="line x" title="263:300	Frank Z. Smadja." ></td>
	<td class="line x" title="264:300	1993." ></td>
	<td class="line x" title="265:300	Retrieving collocations from text: Xtract." ></td>
	<td class="line x" title="266:300	Computational Linguistics, 19(1):143177." ></td>
	<td class="line x" title="267:300	Hiroya Takamura, Takashi Inui, and Manabu Okumura." ></td>
	<td class="line x" title="268:300	2005." ></td>
	<td class="line x" title="269:300	Extracting semantic orientations of words using spin model." ></td>
	<td class="line x" title="270:300	In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL05), pages 133140." ></td>
	<td class="line x" title="271:300	Hiroya Takamura, Takashi Inui, and Manabu Okumura." ></td>
	<td class="line x" title="272:300	2006." ></td>
	<td class="line x" title="273:300	Latent variable models for semantic orientations of phrases." ></td>
	<td class="line x" title="274:300	In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL06)." ></td>
	<td class="line x" title="275:300	Kazuyuki Tanaka and Tohru Morita." ></td>
	<td class="line x" title="276:300	1996." ></td>
	<td class="line x" title="277:300	Application of cluster variation method to image restoration problem." ></td>
	<td class="line x" title="278:300	In Theory and Applications of the Cluster Variation and Path Probability Methods, pages 353373." ></td>
	<td class="line x" title="279:300	Plenum Press, New York." ></td>
	<td class="line x" title="280:300	Peter D. Turney and Michael L. Littman." ></td>
	<td class="line x" title="281:300	2003." ></td>
	<td class="line x" title="282:300	Measuring praise and criticism: Inference of semantic orientation from association." ></td>
	<td class="line x" title="283:300	ACM Transactions on Information Systems, 21(4):315346." ></td>
	<td class="line x" title="284:300	Peter D. Turney." ></td>
	<td class="line x" title="285:300	2002." ></td>
	<td class="line x" title="286:300	Thumbs up or thumbs down?" ></td>
	<td class="line x" title="287:300	semantic orientation applied to unsupervised classification of reviews." ></td>
	<td class="line x" title="288:300	In Proceedings 40th Annual Meeting of the Association for Computational Linguistics (ACL02), pages 417424." ></td>
	<td class="line x" title="289:300	Janyce M. Wiebe and Rada Mihalcea." ></td>
	<td class="line x" title="290:300	2006." ></td>
	<td class="line x" title="291:300	Word sense and subjectivity." ></td>
	<td class="line x" title="292:300	In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics (COLING-ACL06), pages 10651072." ></td>
	<td class="line x" title="293:300	Theresa Wilson, Janyce Wiebe, and Paul Hoffmann." ></td>
	<td class="line x" title="294:300	2005." ></td>
	<td class="line x" title="295:300	Recognizing contextual polarity in phrase-level sentiment analysis." ></td>
	<td class="line x" title="296:300	In Proceedings of joint conference on Human Language Technology / Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP05), pages 347354." ></td>
	<td class="line x" title="297:300	Fa-Yueh Wu." ></td>
	<td class="line x" title="298:300	1982." ></td>
	<td class="line x" title="299:300	The potts model." ></td>
	<td class="line x" title="300:300	Reviews of Modern Physics, 54(1):235268 ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-1511
Annotating Chinese Collocations with Multi Information
Xu, Ruifeng;Lu, Qin;Wong, Kam-Fai;Li, Wenjie;"></td>
	<td class="line x" title="1:237	Proceedings of the Linguistic Annotation Workshop, pages 6168, Prague, June 2007." ></td>
	<td class="line x" title="2:237	c2007 Association for Computational Linguistics Annotating Chinese Collocations with Multi Information Ruifeng Xu 1, Qin Lu 1, Kam-Fai Wong 2, Wenjie Li 1   1 Department of Computing, 2 Department of Systems Engineering and  Engineering Management The Hong Kong Polytechnic University, The Chinese University of Hong Kong, Kowloon, Hong Kong N.T., Hong Kong {csrfxu,csluqin,cswjli}@comp.polyu.edu.hk kfwong@se.cuhk.edu.hk  Abstract This paper presents the design and construction of an annotated Chinese collocation bank as the resource to support systematic research on Chinese collocations." ></td>
	<td class="line x" title="3:237	With the help of computational tools, the bi-gram and n-gram collocations corresponding to 3,643 headwords are manually identified." ></td>
	<td class="line x" title="4:237	Furthermore, annotations for bi-gram collocations include dependency relation, chunking relation and classification of collocation types." ></td>
	<td class="line x" title="5:237	Currently, the collocation bank annotated 23,581 bigram collocations and 2,752 n-gram collocations extracted from a 5-million-word corpus." ></td>
	<td class="line x" title="6:237	Through statistical analysis on the collocation bank, some characteristics of Chinese bigram collocations are examined which is essential to collocation research, especially for Chinese." ></td>
	<td class="line x" title="7:237	1 Introduction Collocation is a lexical phenomenon in which two or more words are habitually combined and commonly used in a language to express certain semantic meaning." ></td>
	<td class="line x" title="8:237	For example, in Chinese, people will say  - (historical baggage) rather than   - (historical luggage) even though  (baggage) and  (luggage) are synonymous." ></td>
	<td class="line x" title="9:237	However, no one can argue why  must collocate with ." ></td>
	<td class="line x" title="10:237	Briefly speaking, collocations are frequently used word combinations." ></td>
	<td class="line x" title="11:237	The collocated words always have syntactic or semantic relations but they cannot be generated directly by syntactic or semantic rules." ></td>
	<td class="line x" title="12:237	Collocation can bring out different meanings a word can carry and it plays an indispensable role in expressing the most appropriate meaning in a given context." ></td>
	<td class="line x" title="13:237	Consequently, collocation knowledge is widely employed in natural language processing tasks such as word sense disambiguation, machine translation, information retrieval and natural language generation (Manning et al. 1999)." ></td>
	<td class="line x" title="14:237	Although the importance of collocation is well known, it is difficult to compile a complete collocation dictionary." ></td>
	<td class="line oc" title="15:237	There are some existing corpus linguistic researches on automatic extraction of collocations from electronic text (Smadja 1993; Lin 1998; Xu and Lu 2006)." ></td>
	<td class="line o" title="16:237	These techniques are mainly based on statistical techniques and syntactic analysis." ></td>
	<td class="line n" title="17:237	However, the performances of automatic collocation extraction systems are not satisfactory (Pecina 2005)." ></td>
	<td class="line x" title="18:237	A problem is that collocations are word combinations that co-occur within a short context, but not all such co-occurrences are true collocations." ></td>
	<td class="line x" title="19:237	Further examinations is needed to filter out pseudo-collocations once co-occurred word pairs are identified." ></td>
	<td class="line x" title="20:237	A collocation bank with true collocations annotated is naturally an indispensable resource for collocation research." ></td>
	<td class="line x" title="21:237	(Kosho et al. 2000) presented their works of collocation annotation on Japanese text." ></td>
	<td class="line x" title="22:237	Also, the Turkish treebank, (Bedin 2003) included collocation annotation as one step in its annotation." ></td>
	<td class="line x" title="23:237	These two collocation banks provided collocation identification and co-occurrence verification information." ></td>
	<td class="line x" title="24:237	(Tutin 2005) used shallow analysis based on finite state transducers and lexicon-grammar to identify and annotate collocations in a French corpus." ></td>
	<td class="line x" title="25:237	This collocation bank further provided the lexical functions of the collocations." ></td>
	<td class="line x" title="26:237	However to this day, there is no reported Chinese collocation bank available." ></td>
	<td class="line x" title="27:237	61 In this paper, we present the design and construction of a Chinese collocation bank (acronymed CCB)." ></td>
	<td class="line x" title="28:237	This is the first attempt to build a large-scale Chinese collocation bank as a Chinese NLP resource with multiple linguistic information for each collocation including: (1) annotating the collocated words for each given headword; (2) distinguishing n-gram and bi-gram collocations for the headword; (3) for bi-gram collocations, CCB provides their syntactic dependencies, chunking relation and classification of collocation types which is proposed by (Xu and Lu 2006)." ></td>
	<td class="line x" title="29:237	In addition, we introduce the quality assurance mechanism used for CCB." ></td>
	<td class="line x" title="30:237	CCB currently contains for 3,643 common headwords taken from The Dictionary of Modern Chinese Collocations (Mei 1999) with 23,581 unique bi-gram collocations and 2,752 unique n-gram collocations extracted from a five-million-word segmented and chunked Chinese corpus (Xu and Lu, 2005)." ></td>
	<td class="line x" title="31:237	The rest of this paper is organized as follows." ></td>
	<td class="line x" title="32:237	Section 2 presents some basic concepts." ></td>
	<td class="line x" title="33:237	Section 3 describes the annotation guideline." ></td>
	<td class="line x" title="34:237	Section 4 describes the practical issues in the annotation process including corpus preparation, headword preparation, annotation flow, and the quality assurance mechanism." ></td>
	<td class="line x" title="35:237	Section 5 gives current status of CCB and characteristics analysis of the annotated collocations." ></td>
	<td class="line x" title="36:237	Section 6 concludes this paper." ></td>
	<td class="line x" title="37:237	2 Basic Concepts Although collocations are habitual expressions in natural language use and they can be easily understood by people, a precise definition of collocation is still far-reaching (Manning et al. 1999)." ></td>
	<td class="line x" title="38:237	In this study, we define a collocation as a recurrent and conventional expression of two or more content words that holds syntactic and semantic relation." ></td>
	<td class="line x" title="39:237	Content words in Chinese include noun, verb, adjective, adverb, determiner, directional word, and gerund." ></td>
	<td class="line x" title="40:237	Collocations with only two words are called bi-gram collocations and others are called ngram collocations." ></td>
	<td class="line x" title="41:237	From a linguistic view point, collocations have a number of characteristics." ></td>
	<td class="line x" title="42:237	Firstly, collocations are recurrent as they are of habitual use." ></td>
	<td class="line x" title="43:237	Collocations occur frequently in similar contexts and they appear in certain fixed patterns." ></td>
	<td class="line x" title="44:237	However, they cannot be described by the same set of syntactic or semantic rules." ></td>
	<td class="line x" title="45:237	Secondly, free word combinations which can be generated by linguistic rules are normally considered compositional." ></td>
	<td class="line x" title="46:237	In contrast, collocations should be limited compositional (Manning et al. 1999) and they usually carry additional meanings when used as a collocation." ></td>
	<td class="line x" title="47:237	Thirdly, collocations are also limited substitutable and limited modifiable." ></td>
	<td class="line x" title="48:237	Limited substitutable here means that a word cannot be freely substituted by other words with similar linguistic functions in the same context such as synonyms." ></td>
	<td class="line x" title="49:237	Also, many collocations cannot be modified freely by adding modifiers or through grammatical transformations." ></td>
	<td class="line oc" title="50:237	Lastly, collocations are domain-dependent (Smadja 1993) and language-dependent." ></td>
	<td class="line x" title="51:237	3 Annotation Guideline Design The guideline firstly determines the annotation strategy." ></td>
	<td class="line x" title="52:237	(1) The annotation of CCB follows the headword-driven strategy." ></td>
	<td class="line x" title="53:237	The annotation uses selected headwords as the starting point." ></td>
	<td class="line x" title="54:237	In each circle, the collocations corresponding to one headword are annotated." ></td>
	<td class="line x" title="55:237	Headword-driven strategy makes a more efficient annotation as it is helpful to estimate and compare the relevant collocations." ></td>
	<td class="line x" title="56:237	(2) CCB is manually annotated with the help of automatic estimation of computational features, i.e. semi-automatic software tools are used to generate parsing and chunking candidates and to estimate the classification features." ></td>
	<td class="line x" title="57:237	These data are present to the annotators for determination." ></td>
	<td class="line x" title="58:237	The use of assistive tools is helpful to produce accurate annotations with efficiency." ></td>
	<td class="line x" title="59:237	The guideline also specifies the information to be annotated and the labels used in the annotation." ></td>
	<td class="line x" title="60:237	For a given headword, CCB annotates both bigram collocations and n-gram collocations." ></td>
	<td class="line x" title="61:237	Considering the fact that n-gram collocations consisting of continuous significant bi-grams as a whole and, the n-gram annotation is based on the identification and verification of bi-gram word combinations and is prior to the annotation of bi-gram collocations." ></td>
	<td class="line x" title="62:237	For bi-gram annotation, which is the major interest in collocation research, three kinds of information are annotated." ></td>
	<td class="line x" title="63:237	The first one is the syntactic dependency of the headword and its co-word in a bi-gram collocation . A syntactic dependency normally consists of one word as the governor (or head), a dependency type and another word serves 62 as dependent (or modifier) (Lin 1998).Totally, 10 types of dependencies are annotated in CCB." ></td>
	<td class="line x" title="64:237	They are listed in Table 1 below." ></td>
	<td class="line x" title="65:237	Dependency Description Example ADA Adjective and its adverbial modifier  /d  /a greatly painful ADV Predicate and its adverbial modifier in which the predicate serves as head  /ad  /v heavily strike AN Noun and its adjective modifier  /a  /n lawful incoming CMP Predicate and its complement in which the predicate serves as head  /v  /v ineffectively treat NJX Juxtaposition structure  /a  /a fair and reasonable NN Noun and its nominal modifier  /n  /n personal safety SBV Predicate and its subject  /n  /v property transfer VO Predicate and its object in which the predicate serves as head  /v  /n change mechanism VV Serial verb constructions which indicates that there are serial actions  /v  /v trace and report OT Others Table 1." ></td>
	<td class="line x" title="66:237	The dependency categories The second one is the syntactic chunking information (a chunk is defined as a minimum non-nesting or non-overlapping phrase) (Xu and Lu, 2005)." ></td>
	<td class="line x" title="67:237	Chunking information identifies all the words for a collocation within the context of an enclosed chunk." ></td>
	<td class="line x" title="68:237	Thus, it is a way to identify its proper context at the most immediate syntactic structure." ></td>
	<td class="line x" title="69:237	11 types of syntactic chunking categories given in (Xu and 2006) are used as listed in Table 2." ></td>
	<td class="line x" title="70:237	Description Examples BNP Base noun phrase [ /n  /n]NP market economy BAP Base adjective phrase [ /a  /a]BAP fair and reasonable BVP Base verb phrase [ /a  /v]BVP successfully start BDP Base adverb phrase [ /d  /d]BDP no longer BQP Base quantifier phrase [ /m  /q]BQP  /n several thousand soldiers BTP Base time phrase [ /t  /t]BTP 8:00 in the morning BFP Base position phrase [ /ns  /f]BFP Northeast of Mongolia BNT Name of an organization [ /ns  /n]BNT Yantai University BNS Name of a place [ /ns  /ns]BNS Tongshan, Jiangsu Province BNZ Other proper noun phrase [ /nr  /n]BNZ The Nobel Prize BSV S-V structure [ /n  /a]BSV territorial integrity Table 2." ></td>
	<td class="line x" title="71:237	The chunking categories The third one is the classification of collocation types." ></td>
	<td class="line x" title="72:237	Collocations cover a wide spectrum of habitual word combinations ranging from idioms to free word combinations." ></td>
	<td class="line x" title="73:237	Some collocations are very rigid and some are more flexible." ></td>
	<td class="line x" title="74:237	(Xu and Lu 2006) proposed a scheme to classify collocations into four types according to the internal association of collocations including compositionality, nonsubstitutability, non-modifiability, and statistical significance." ></td>
	<td class="line x" title="75:237	They are, Type 0: Idiomatic Collocation Type 0 collocations are fully non-compositional as its meaning cannot be predicted from the meanings of its components such as  (climbing a tree to catch a fish, which is a metaphor for a fruitless endeavour)." ></td>
	<td class="line x" title="76:237	Some terminologies are also Type 0 collocations such as   (Blue-tooth ) which refers to a wireless communication protocol." ></td>
	<td class="line x" title="77:237	Type 0 collocations must have fixed forms." ></td>
	<td class="line x" title="78:237	Their components are non-substitutable and nonmodifiable allowing no syntactic transformation and no internal lexical variation." ></td>
	<td class="line x" title="79:237	This type of collocations has very strong internal associations and co-occurrence statistics is not important." ></td>
	<td class="line x" title="80:237	Type 1: Fixed Collocation Type 1 collocations are very limited compositional with fixed forms which are non-substitutable and non-modifiable." ></td>
	<td class="line x" title="81:237	However, this type can be compositional." ></td>
	<td class="line x" title="82:237	None of the words in a Type 1 collocation can be substituted by any other words to retain the same meaning such as in  /n  /n (diplomatic immunity)." ></td>
	<td class="line x" title="83:237	Finally, Type 1 collocations normally have strong co-occurrence statistics to support them." ></td>
	<td class="line x" title="84:237	Type 2: Strong Collocation Type 2 collocations are limitedly compositional." ></td>
	<td class="line x" title="85:237	They allow very limited substitutability." ></td>
	<td class="line x" title="86:237	In other words, their components can only be substituted by few synonyms and the newly generated word combinations have similar meaning, e.g.,  /v  /n (alliance formation) and  /v  /n (alliance formation)." ></td>
	<td class="line x" title="87:237	Furthermore, Type 2 collocations allow limited modifier insertion and the order of components must be maintained." ></td>
	<td class="line x" title="88:237	Type2 collocations normally have strong statistical support." ></td>
	<td class="line x" title="89:237	Type 3: Loose Collocation Type 3 collocations have loose restrictions." ></td>
	<td class="line x" title="90:237	They are nearly compositional." ></td>
	<td class="line x" title="91:237	Their components may be substituted by some of their synonyms and the newly generated word combinations usually have very similar meanings." ></td>
	<td class="line x" title="92:237	Type 3 collocations are modifiable meaning that they allow modifier insertions." ></td>
	<td class="line x" title="93:237	Type 3 collocations have weak internal associations and they must have statistically significant co-occurrence." ></td>
	<td class="line x" title="94:237	The classification represents the strength of internal associations of collocated words." ></td>
	<td class="line x" title="95:237	The annotation of these three kinds of information is essential to all-rounded characteristic analysis of collocations." ></td>
	<td class="line x" title="96:237	63 4 Annotation of CCB 4.1 Data Preparation CCB is based on the PolyU chunk bank (Xu and Lu, 2005) which contains chunking information on the Peoples Daily corpus with both segmentation and part-of-speech tags." ></td>
	<td class="line x" title="97:237	The accuracies of word segmentation and POS tagging are claimed to be higher than 99.9% and 99.5%, respectively (Yu et al. 2001)." ></td>
	<td class="line x" title="98:237	The use of this popular and accurate raw resource helped to reduce the cost of annotation significantly, and ensured maximal sharing of our output." ></td>
	<td class="line x" title="99:237	The set of 3, 643 headwords are selected from The Dictionary of Modern Chinese Collocation (Mei 1999) among about 6,000 headwords in the dictionary." ></td>
	<td class="line x" title="100:237	The selection was based both on the judgment by linguistic experts as well as the statistical information that they are commonly used." ></td>
	<td class="line x" title="101:237	4.2 Corpus Preprocessing The CCB annotations are represented in XML." ></td>
	<td class="line x" title="102:237	Since collocations are practical word combinations and word is the basic unit in collocation research, a preprocessing module is devised to transfer the chunked sentences in the PolyU chunk bank to word sequences with the appropriate labels to indicate the corresponding chunking information." ></td>
	<td class="line x" title="103:237	This preprocessing module indexes the words and chunks in the sentences and encodes the chunking information of each word in two steps." ></td>
	<td class="line x" title="104:237	Consider the following sample sentence extracted from the PolyU chunk bank:  /v[ /n  /n]BNP  /u[ /n  /n  /an ]BNP (ensure life and property safety of the people) The first step in preprocessing is to index each word and the chunk in the sentence by giving incremental word ids and chunk ids from left to right." ></td>
	<td class="line x" title="105:237	That is,, [W1] /v [W2] /n [W3] /n [W4] /u [W5] /n [W6] /n [W7] /an [C1]BNP [C2]BNP where, [W1] to [W7] are the words and [C1] to [C2] are chunks although chunking positions are not included in this step." ></td>
	<td class="line x" title="106:237	One Chinese word may occur in a sentence for more than one times, the unique word ids are helpful to avoid ambiguities in the collocation annotation on these words." ></td>
	<td class="line x" title="107:237	The second step is to represent the chunking information of each word." ></td>
	<td class="line x" title="108:237	Chunking boundary information is labeled by following initial/final representation scheme." ></td>
	<td class="line x" title="109:237	Four labels, O/B/I/E, are used to mark the isolated words outsides any chunks, chunk-initial words, words in the middle of chunks, and chunk-final words, respectively." ></td>
	<td class="line x" title="110:237	Finally, a label H is used to mark the identified head of chunks and N to mark the non-head words." ></td>
	<td class="line x" title="111:237	The above sample sentence is then transferred to a sequence of words with labels as shown below, <labeled> [W1][O_O_N][O] /v [W2][B_BNP_N][C1]  /n [W3][E_BNP_H][C1] /n [W4][O_O_N][O] /u [W5][B_BNP_N][C2] /n [W6][I_BNP_N][C2] /n [W7][E_BNP_N][C2] /an </labeled> For each word, the first label is the word ID. The second one is a hybrid tag for describing its chunking status." ></td>
	<td class="line x" title="112:237	The hybrid tags are ordinal with respect to the chunking status of boundary, syntactic category and head, For example, B_BNP_N indicates that current word is the beginning or a BNP and this word is not the head of this chunk." ></td>
	<td class="line x" title="113:237	The third one is the chunk ID if applicable." ></td>
	<td class="line x" title="114:237	For the word out of any chunks, a fixed chunk ID O is given." ></td>
	<td class="line x" title="115:237	4.3 Collocation Annotation Collocation annotation is conducted on one headword at a time." ></td>
	<td class="line x" title="116:237	For a given headword, an annotators examines its context to determine if its cooccurred word(s) forms a collocation with it and if so, also annotate the collocations dependency, chunking and classification information." ></td>
	<td class="line x" title="117:237	The annotation procedure, requires three passes." ></td>
	<td class="line x" title="118:237	We use a headword  /an (safe), as an illustrative example." ></td>
	<td class="line x" title="119:237	Pass 1." ></td>
	<td class="line x" title="120:237	Concordance and dependency identification In the first pass, the concordance of the given headword is performed." ></td>
	<td class="line x" title="121:237	Sentences containing the headwords are obtained, e.g. S1:  /v [ /v  /an]BVP  /u  /n (follow the principles for ensuring the safety) S2:  /v [ /n  /n]BNP  /u[ /n  /n   /an]BNP (ensure life and property safety of people) S3:  /v  /ns [ /an  /v]BVP (ensure the flood pass through Yangzi River safely) With the help of an automatic dependency parser, the annotator determines all syntactically and semantically dependent words in the chunking context of the observing headword." ></td>
	<td class="line x" title="122:237	The annotation output of S1 is given below in which XML tags are used for the dependency annotation." ></td>
	<td class="line x" title="123:237	S1:<sentence> /v [ /v  /an]BVP  /u  /n 64 <labeled> [W1][O_O_N][O] /v [W2][B_BVP_H][C1]  /v [W3][E_BNP_N][C1] /an [W4][O_O_N][O] /u [W5][O_O_N][O] /n </labeled> <dependency no='1' observing=' /an' head=' /v' head_wordid='W2' head_chunk ='B_BVP_H' head_chunkid='C1' modifier='  /an' modifier_wordid='W3' modifier _chunk='E_BVP_N' modifer_chunkid='C1' relation='VO' > </dependency> </sentence> Dependency of word combination is annotated with the tag <dependency> which includes the following attributes: -<dependency> indicates an identified dependency -no is the id of identified dependency within current sentence according to ordinal sequence -observing indicates the current observing headword -head indicates the head of the identified word dependency -head_wordid is the word id of the head -head_chunk is the hybrid tags for labeling the chunking information of the head -head_chunkid is the chunk id of the head -modifier indicates the modifier of the identified dependency -modifier_wordid is the word id of the modifier -modifier_chunk is the hybrid tags for labeling chunking information of the modifier -modifier_chunkid is the chunk id of the modifier -relation gives the syntactic dependency relations labeled according to the dependency labels listed in Table 1." ></td>
	<td class="line x" title="124:237	In S1 and S2, the word combination  /v  /an has direct dependency, and in S3, such a dependency does not exist as  /v only determines  /v and  /an depends on  /v. The quality of CCB highly depends on the accuracy of dependency annotation." ></td>
	<td class="line x" title="125:237	This is very important for effective characteristics analysis of collocations and for the collocation extraction algorithms." ></td>
	<td class="line x" title="126:237	Pass 2." ></td>
	<td class="line x" title="127:237	N-gram collocations annotation It is relatively easy to identify n-gram collocations since an n-gram collocation is of habitual and recurrent use of a series of bi-grams." ></td>
	<td class="line x" title="128:237	This means that n-gram collocations can be identified by finding consecutive occurrence of significant bi-grams in certain position." ></td>
	<td class="line x" title="129:237	In the second pass, the annotators focus on the sentences where the headword has more than one dependency." ></td>
	<td class="line x" title="130:237	The percentage of all appearances of each dependent word at each position around the headword is estimated with the help of a program (Xu and Lu, 2006)." ></td>
	<td class="line x" title="131:237	Finally, word dependencies frequently co-occurring in consecutive positions in a fixed order are extracted as n-gram collocations." ></td>
	<td class="line x" title="132:237	For the headword, an n-gram collocation  /n  /n  /an is identified since the cooccurrence percentage of dependency  /-NN-  /an and dependency  /n-NN- /an is 0.74 is greater than a empirical threshold suggest in (Xu and Lu, 2006)." ></td>
	<td class="line x" title="133:237	This n-gram is annotated in S2 as follows: <ncolloc observing=' /an' w1=' /n' w2=' /n' w3=' /an' start_wordid='5'> </ncolloc> where, -<ncolloc> indicates an n-gram collocation -w1, w2,wn give the components of the n-gram collocation according to the ordinal sequence." ></td>
	<td class="line x" title="134:237	-start_wordid indicates the word id of the first component of the n-gram collocation." ></td>
	<td class="line x" title="135:237	Since n-gram collocation is regarded as a whole, its internal dependencies are ignored in the output file of pass 2." ></td>
	<td class="line x" title="136:237	That is, if the dependencies of several components are associated with an n-gram collocation in one sentence, the n-gram collocation is annotated and these dependencies are filtered out so as not to disturb the bi-gram dependencies." ></td>
	<td class="line x" title="137:237	Pass 3." ></td>
	<td class="line x" title="138:237	Bi-gram collocations annotation In this pass, all the word dependencies are examined to identify bi-gram collocations." ></td>
	<td class="line x" title="139:237	Furthermore, if a dependent word combination is regarded as a collocation by the annotators, it will be further labeled based on the type determined." ></td>
	<td class="line x" title="140:237	The identification is based on expert knowledge combined with the use of several computational features as discussed in (Xu and Lu, 2006)." ></td>
	<td class="line x" title="141:237	An assistive tool is developed to estimate the computational features." ></td>
	<td class="line x" title="142:237	We use the program to obtain feature data based on two sets of data." ></td>
	<td class="line x" title="143:237	The first data set is the annotated dependencies in the 5-million-word corpus which is obtained through Pass 1 and Pass 2 annotations." ></td>
	<td class="line x" title="144:237	Because the dependent word combinations are manually identified and annotated in the first pass, the statistical significance is helpful to identify whether the word combination is a collocation and to determine its type." ></td>
	<td class="line x" title="145:237	However, data sparseness problem must be considered since 5-million-word is not large enough." ></td>
	<td class="line x" title="146:237	Thus, another set of statistical data are 65 collected from a 100-million segmented and tagged corpus (Xu and Lu, 2006)." ></td>
	<td class="line x" title="147:237	With this large corpus, data sparseness is no longer a serious problem." ></td>
	<td class="line x" title="148:237	But, the collected statistics are quite noisy since they are directly retrieved from text without any verification." ></td>
	<td class="line x" title="149:237	By analyzing the statistical features from both sets, the annotator can use his/her professional judgment to determine whether a bi-gram is a collocation and its collocation type." ></td>
	<td class="line x" title="150:237	In the example sentences, two collocations are identified." ></td>
	<td class="line x" title="151:237	Firstly,  /an  /v is classified as a Type 1 collocation as they have only one peak cooccurrence, very low substitution ratio and their co-occurrence order nearly never altered." ></td>
	<td class="line x" title="152:237	Secondly,  /v  /an is identified as a collocation." ></td>
	<td class="line x" title="153:237	They have frequent co-occurrences and they are always co-occurred in fixed order among the verified dependencies." ></td>
	<td class="line x" title="154:237	However, their co-occurrences are distributed evenly and they have two peak cooccurrences." ></td>
	<td class="line x" title="155:237	Therefore,  /v  /an is classified as a Type 3 collocation." ></td>
	<td class="line x" title="156:237	These bi-gram collocations are annotated as illustrated below, <bcolloc observing=' /an' col=' /v' head=' /v' type= '1' relation='ADV'> <dependency no='1' observing=' /an' head=' /v' head_wordid='W4' head_chunk ='E_BVP_H' head_chunkid='C1' modifier='  /an' modifier_wordid='W3' modifier _chunk='B_BVP_N' modifer_chunkid='C1' relation='ADV' ></dependency></bcolloc> where, -<bcolloc> indicates a bi-gram collocation." ></td>
	<td class="line x" title="157:237	-col is for the collocated word." ></td>
	<td class="line x" title="158:237	-head indicates the head of an identified collocation -type is the classified collocation type." ></td>
	<td class="line x" title="159:237	-relation gives the syntactic dependency relations of this bi-gram collocation." ></td>
	<td class="line x" title="160:237	Note that the dependency annotations within the bi-gram collocations are reserved." ></td>
	<td class="line x" title="161:237	4.4 Quality Assurance The annotators of CCB are three post-graduate students majoring in linguistics." ></td>
	<td class="line x" title="162:237	In the first annotation stage, 20% headwords of the whole set was annotated in duplicates by all three of them." ></td>
	<td class="line x" title="163:237	Their outputs were checked by a program." ></td>
	<td class="line x" title="164:237	Annotated collocation including classified dependencies and types accepted by at least two annotators are reserved in the final data as the Golden Standard while the others are considered incorrect." ></td>
	<td class="line x" title="165:237	The inconsistencies between different annotators were discussed to clarify any misunderstanding in order to come up with the most appropriate annotations." ></td>
	<td class="line x" title="166:237	In the second annotation stage, 80% of the whole annotations were then divided into three parts and separately distributed to the annotators with 5% duplicate headwords were distributed blindly." ></td>
	<td class="line x" title="167:237	The duplicate annotation data were used to estimate the annotation consistency between annotators." ></td>
	<td class="line x" title="168:237	5 Collocation Characteristic Analysis 5.1 Progress and Quality of CCB Up to now, the first version of CCB is completed." ></td>
	<td class="line x" title="169:237	We have obtained 23,581 unique bi-gram collocations and 2,752 unique n-gram collocations corresponding to the 3,643 observing headwords." ></td>
	<td class="line x" title="170:237	Meanwhile, their occurrences in the corpus are annotated and verified." ></td>
	<td class="line x" title="171:237	With the help of a computer program, the annotators manually classified bigram collocations into three types." ></td>
	<td class="line x" title="172:237	The numbers of Type 0/1, Type 2 and Type 3 collocations are 152, 3,982 and 19,447, respectively." ></td>
	<td class="line x" title="173:237	For the 3,643 headwords in The Dictionary of Modern Chinese Collocations (Mei 1999) with 35,742 bi-gram collocations, 20,035 collocations appear in the corpus." ></td>
	<td class="line x" title="174:237	We call this collection as Meis Collocation Collection (MCC)." ></td>
	<td class="line x" title="175:237	There are 19,967 common entries in MCC and CCB, which means 99.7% collocations in MCC appear in CCB indicating a good linguistic consistency." ></td>
	<td class="line x" title="176:237	Furthermore, 3,614 additional collocations are found in CCB which enriches the static collocation dictionary." ></td>
	<td class="line x" title="177:237	5.2 Dependencies Numbers Statistics of Collocations Firstly, we study the statistics of how many types of dependencies a bi-gram collocation may have." ></td>
	<td class="line x" title="178:237	The numbers of dependency types with respect to different collocation types are listed in Table 3." ></td>
	<td class="line x" title="179:237	Collocations 1 type 2 types >2 types Total Type 0/1 152 0 0 152 Type 2 3970 12 0 3982 Type 3 17282 2130 35 19447 Total 21404 2142 35 23581 Table 3." ></td>
	<td class="line x" title="180:237	Collocation classification versus number of dependency types 66 It is observed that about 90% bi-gram collocations have only one dependency type." ></td>
	<td class="line x" title="181:237	This indicates that a collocation normally has only one fixed syntactic dependency." ></td>
	<td class="line x" title="182:237	It is also observed that about 10% bigram collocations have more than one dependency type, especially Type 3 collocations." ></td>
	<td class="line x" title="183:237	For example, two types of dependencies are identified in the bigram collocation  /an- /n. They are  /an-AN- /n (a safe nation) which indicates the dependency of a noun and its nominal modifier where  /n serves as the head, and  /n-NN /an (national security) which indicates the dependency of a noun and its nominal modifier where  /an serves as the head." ></td>
	<td class="line x" title="184:237	It is attributed to the fact that the use of Chinese words is flexible." ></td>
	<td class="line x" title="185:237	A Chinese word may support different part-of-speech." ></td>
	<td class="line x" title="186:237	A collocation with different dependencies results in different distribution trends and most of these collocations are classified as Type 3." ></td>
	<td class="line x" title="187:237	On the other hand, Type 0/1 and Type 2 collocations seldom have more than one dependency type." ></td>
	<td class="line x" title="188:237	5.3 Syntactic Dependency Statistics of Collocations The statistics of the 10 types of syntactic dependencies with respect to different types of bi-gram collocations are shown in Table 4." ></td>
	<td class="line x" title="189:237	No." ></td>
	<td class="line x" title="190:237	is the number of collocations with a given dependency type D and a given collocation type T. The percentage of No." ></td>
	<td class="line x" title="191:237	among all collocations with the same collocation type T is labeled as P_T, and the percentage of No." ></td>
	<td class="line x" title="192:237	among all of the collocations with the same dependency D is labeled as P_D." ></td>
	<td class="line x" title="193:237	Type 0/1 Type 2 Type 3 Total No." ></td>
	<td class="line x" title="194:237	P_T P_D No." ></td>
	<td class="line x" title="195:237	P_T P_D No." ></td>
	<td class="line x" title="196:237	P_T P_D No." ></td>
	<td class="line x" title="197:237	P_T ADA 1 0.7 0.1 212 5.3 11.5 1637 7.6 88.5 1850 7.2 ADV 9 5.9 0.3 322 8.1 11.2 2555 11.8 88.5 2886 11.2 AN 20 13.2 0.4 871 21.8 15.4 4771 22.0 84.3 5662 22.0 CMP 12 7.9 2.2 144 3.6 26.9 379 1.8 70.8 535 2.1 NJX 8 5.3 3.2 42 1.1 16.9 198 0.9 79.8 248 1.0 NN 44 28.9 0.9 1036 25.9 21.6 3722 17.2 77.5 4802 18.6 SBV 4 2.6 0.2 285 7.1 11.1 2279 10.5 88.7 2568 10.0 VO 26 17.1 0.5 652 16.3 12.5 4545 21.0 87.0 5223 20.2 VV 3 2.0 0.2 227 5.7 13.4 1464 6.8 86.4 1694 6.6 OT 25 16.4 7.7 203 5.1 62.5 97 0.4 29.8 325 1.3 Total 152 100.0 0.6 3994 100.0 15.5 21647 100.0 83.9 25793 100.0 Table 4." ></td>
	<td class="line x" title="198:237	The statistics of collocations with different collocation type and dependency Corresponding to 23,581 bi-gram collocations, 25,793 types of dependencies are identified (some collocations have more than one types of dependency)." ></td>
	<td class="line x" title="199:237	In which, about 82% belongs to five major dependency types." ></td>
	<td class="line x" title="200:237	They are AN, VO, NN, ADV and SBV." ></td>
	<td class="line x" title="201:237	It is note-worthy that the percentage of NN collocation is much higher than that in English." ></td>
	<td class="line x" title="202:237	This is because nouns are more often used in parallel to serve as one syntactic component in Chinese sentences than in English." ></td>
	<td class="line x" title="203:237	The percentages of Type 0/1, Type 2 and Type 3 collocations in CCB are 0.6%, 16.9% and 82.5%, respectively." ></td>
	<td class="line x" title="204:237	However, the collocations with different types of dependencies have shown their own characteristics with respect to different collocation types." ></td>
	<td class="line x" title="205:237	The collocations with CMP, NJX and NN dependencies on average have higher percentage to be classified into Type 0/1 and Type 2 collocations." ></td>
	<td class="line x" title="206:237	This indicates that CMP, NJX and NN collocations in Chinese are always used in fixed patterns and these kinds of collocations are not freely modifiable and substitutable." ></td>
	<td class="line x" title="207:237	In the contrary, many ADV and AN collocations are classified as Type 3." ></td>
	<td class="line x" title="208:237	This is partially due to the special usage of auxiliary words in Chinese." ></td>
	<td class="line x" title="209:237	Many AN Chinese collocations can be inserted by a meaningless auxiliary word  /u and many ADV Chinese collocations can be inserted by an auxiliary word  /u. This means that many AN and ADV collocations can be modified and thus, they always have two peak cooccurrences." ></td>
	<td class="line x" title="210:237	Therefore, they are classified as Type 3 collocations." ></td>
	<td class="line x" title="211:237	7.7% and 62.5% of the collocations with dependency OT are classified as Type 0/1 and Type2 collocations, respectively." ></td>
	<td class="line x" title="212:237	Such percentages are much higher than the average." ></td>
	<td class="line x" title="213:237	This is attributed by the fact that some Type 0/1 and Type 2 collocations have strong semantic relations rather than syntactic relations and thus their dependencies are difficult to label." ></td>
	<td class="line x" title="214:237	5.4 Chunking Statistics of Collocations The chunking characteristic for the collocations with different types and different dependencies are examined." ></td>
	<td class="line x" title="215:237	In most cases, Type 0/1/2 collocations co-occur within one chunk or between neighboring chunks." ></td>
	<td class="line x" title="216:237	Therefore, their chunking characteristics are not discussed in detail." ></td>
	<td class="line x" title="217:237	The percentage of the occurrences of Type 3 collocations with different chunking distances are given in Table 5." ></td>
	<td class="line x" title="218:237	If a collocation co-occurs within one chunk, the chunking distance is 0." ></td>
	<td class="line x" title="219:237	If a collocation co-occurs between neighboring chunks, or between neighboring words, or between a word and a neighboring chunk, the chunking distance is 1, and so on." ></td>
	<td class="line x" title="220:237	67 ADA ADV AN CMP NJX NN SBV VO VV OT 0 chunk 56.8 53.1 65.7 48.5 70.2 62.4 46.5 41.1 47.2 86.4 1 chunk 38.2 43.7 28.5 37.2 15.4 27.9 41.2 35.7 41.1 13.5 2 chunks 5.0 3.2 3.7 14.2 14.4 9.7 11.0 17.6 9.6 0.1 >2chunks 0.0 0.0 2.1 0.1 0.0 0.0 1.3 5.6 2.1 0.0 Table 5." ></td>
	<td class="line x" title="221:237	Chunking distances of Type 3 collocations It is shown that the co-occurrence of collocations decreases with increased chunking distance." ></td>
	<td class="line x" title="222:237	Yet, the behavior for decrease is different for collocations with different dependencies." ></td>
	<td class="line x" title="223:237	Generally speaking, the ADA, ADV, CMP, NJX, NN and OT collocations seldom co-occur cross two words or two chunks." ></td>
	<td class="line x" title="224:237	Furthermore, the occurrences of AN, NJX and OT collocations quickly drops when the chunking distance is greater than 0, i.e. these collocations tends to co-occur within the same chunk." ></td>
	<td class="line x" title="225:237	In the contrary, the co-occurrences of ADA, ADV, CMP, SBV and VV collocations corresponding to chunking distance equals 0 and 1 decrease steadily." ></td>
	<td class="line x" title="226:237	It means that these four kinds of collocations are more evenly distributed within the same chunk or between neighboring words or chunks." ></td>
	<td class="line x" title="227:237	The occurrences of VO collocations corresponding to chunking distance from 0 to 3 with a much flatter reduction." ></td>
	<td class="line x" title="228:237	This indicates that a verb may govern its object in a long range." ></td>
	<td class="line x" title="229:237	6 Conclusions This paper describes the design and construction of a manually annotated Chinese collocation bank." ></td>
	<td class="line x" title="230:237	Following a set of well-designed annotation guideline, the collocations corresponding to 3,643 headwords are identified from a chunked fivemillion word corpus." ></td>
	<td class="line x" title="231:237	2,752 unique n-gram collocations and 23,581 unique bi-gram collocations are annotated." ></td>
	<td class="line x" title="232:237	Furthermore, each bi-gram collocation is annotated with its syntactic dependency information, classification information and chunking information." ></td>
	<td class="line x" title="233:237	Based on CCB, characteristics of collocations with different types and different dependencies are examined." ></td>
	<td class="line x" title="234:237	The obtained result is essential for improving research related to Chinese collocation." ></td>
	<td class="line x" title="235:237	Also, CCB may be used as a standard answer set for evaluating the performance of different collocation extraction algorithms." ></td>
	<td class="line x" title="236:237	In the future, collocations of all unvisited headwords will be annotated to produce a complete 5-million-word Chinese collocation bank." ></td>
	<td class="line x" title="237:237	Acknowledgement This research is supported by The Hong Kong Polytechnic University (A-P203), CERG Grant (5087/01E) and Chinese University of Hong Kong under the Direct Grant Scheme project (2050330) and Strategic Grant Scheme project (4410001)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1030
Retrieving Bilingual Verb-Noun Collocations by Integrating Cross-Language Category Hierarchies
Fukumoto, Fumiyo;Suzuki, Yoshimi;Yamashita, Kazuyuki;"></td>
	<td class="line x" title="1:209	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 233240 Manchester, August 2008 Retrieving Bilingual Verbnoun Collocations by Integrating Cross-Language Category Hierarchies Fumiyo Fukumoto Yoshimi Suzuki Kazuyuki Yamashita  Interdisciplinary Graduate School of Medicine and Engineering Univ. of Yamanashi {fukumoto, ysuzuki}@yamanashi.ac.jp  The Center for Educational Research Faculty of Education and Human Sciences Univ. of Yamanashi kazuyuki@yamanashi.ac.jp Abstract This paper presents a method of retrieving bilingual collocations of a verb and its objective noun from cross-lingual documents with similar contents." ></td>
	<td class="line x" title="2:209	Relevant documents are obtained by integrating crosslanguage hierarchies." ></td>
	<td class="line x" title="3:209	The results showed a 15.1% improvement over the baseline nonhierarchy model, and a 6.0% improvement over use of relevant documents retrieved from a single hierarchy." ></td>
	<td class="line x" title="4:209	Moreover, we found that some of the retrieved collocations were domain-specific." ></td>
	<td class="line x" title="5:209	1 Introduction A bilingual lexicon is important for cross-lingual NLP applications, such as CLIR, and multilingual topic tracking." ></td>
	<td class="line x" title="6:209	Much of the previous work on finding bilingual lexicons has made use of comparable corpora, which exhibit various degrees of parallelism." ></td>
	<td class="line x" title="7:209	Fung et al.(2004) described corpora ranging from noisy parallel, to comparable, and finally to very non-parallel." ></td>
	<td class="line x" title="9:209	Obviously, the latter are easy to collect because very non-parallel corpora consist of sets of documents in two different languages from the same period of dates." ></td>
	<td class="line x" title="10:209	However, a good solution is required to produce a higher quality of lexicon retrieval." ></td>
	<td class="line x" title="11:209	In this paper, we focus on English and Japanese bilingual verbobjective noun collocations which we call verbnoun collocations and retrieve them using very non-parallel corpora." ></td>
	<td class="line x" title="12:209	The method first finds cross-lingual relevant document pairs with similar contents from non-parallel corpora, and c2008." ></td>
	<td class="line x" title="13:209	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="14:209	Some rights reserved." ></td>
	<td class="line x" title="15:209	then we estimate bilingual verbnoun collocations within these relevant documents." ></td>
	<td class="line x" title="16:209	Relevant documents are defined here as pairs of English and Japanese documents that report identical or closely related contents, e.g., a pair of documents describing an aircraft crash and the ensuing investigation to compensate the victims families or any safety measures proposed as a result of the crash." ></td>
	<td class="line x" title="17:209	In the task of retrieving cross-lingual relevant documents, it is crucial to identify an event as something occurs at some specific place and time associated with some specific action." ></td>
	<td class="line x" title="18:209	One solution is to use a topic, i.e., category in the hierarchical structure, such as Internet directories." ></td>
	<td class="line x" title="19:209	Although a topic is not an event, it can be a broader class of event." ></td>
	<td class="line x" title="20:209	Therefore, it is helpful for retrieving relevant documents, and thus bilingual verbnoun collocations." ></td>
	<td class="line x" title="21:209	Consider the Reuters96 and Mainichi newspaper documents shown in Figure 1." ></td>
	<td class="line x" title="22:209	The documents report on the same event, Russian space station collides with cargo craft, were published within two days of each other, and have overlapping content." ></td>
	<td class="line x" title="23:209	Moreover, as indicated by the double-headed arrows in the figure, there are a number of bilingual collocations." ></td>
	<td class="line x" title="24:209	However, as shown in Figure 1, the Reuters document is classified into Science and Technology, while the Mainichi document is classified into Space Navigation." ></td>
	<td class="line x" title="25:209	This is natural because categories in the hierarchical structures are defined by different human experts." ></td>
	<td class="line x" title="26:209	Therefore, a hierarchy tends to have some bias in both defining hierarchical structure and classifying documents, and as a result some hierarchies written in one language are coarse-grained, while others written in other languages are fine-grained." ></td>
	<td class="line x" title="27:209	Our attempt using the results of integrating different hierarchies for retrieving relevant documents was postulated to be able to solve this defect of the differences in 233 Figure 1: Relevant document pairs hierarchies, and to improve the efficiency and efficacy of retrieving collocations." ></td>
	<td class="line x" title="28:209	2 System Description The method consists of three steps: integrating category hierarchies, retrieving cross-lingual relevant documents, and retrieving collocations from relevant documents." ></td>
	<td class="line x" title="29:209	2.1 Integrating Hierarchies The method for integrating different category hierarchies does not simply merge two different hierarchies into a large hierarchy, but instead retrieves pairs of categories, where each category is relevant to each other." ></td>
	<td class="line x" title="30:209	1 The procedure consists of two substeps: Cross-language text classification (CLTC) and estimating category correspondences." ></td>
	<td class="line x" title="31:209	2.1.1 Cross-language text classification The corpora we used are the Reuters96 and the RWCP of the Mainichi Japanese newspapers." ></td>
	<td class="line x" title="32:209	In the CLTC task, we used English and Japanese data to train the Reuters96 categorical hierarchy and the Mainichi UDC code hierarchy (Mainichi hierarchy), respectively." ></td>
	<td class="line x" title="33:209	In the Reuters96 hierarchy, the system was trained using labeled English documents, and classified translated labeled Japanese 1 The reason for retrieving pairs of categories is that each categorical hierarchy is defined by individual human experts, and different linguists often identify different numbers of categories for the same concepts." ></td>
	<td class="line x" title="34:209	Therefore, it is impossible to handle full integration of hierarchies." ></td>
	<td class="line x" title="35:209	Figure 2: Cross-language text classification documents." ></td>
	<td class="line x" title="36:209	Similarly, for Mainichi hierarchy, the system was trained using labeled Japanese documents, and classified translated labeled English documents." ></td>
	<td class="line x" title="37:209	We used Japanese-English and English-Japanese MT software." ></td>
	<td class="line x" title="38:209	We used a learning model, Support Vector Machines (SVMs) (Vapnik, 1995), to classify documents, as SVMs have been shown to be effective for text classification." ></td>
	<td class="line x" title="39:209	We used the One-againstthe-Rest version of the SVMs at each level of a hierarchy." ></td>
	<td class="line x" title="40:209	We classify test documents using a hierarchy by learning separate classifiers at each internal node of the hierarchy." ></td>
	<td class="line x" title="41:209	We used a Boolean function b(L 1 )&&&&b(L m ), where b(L i ) is a decision threshold value of the i-th hierarchical level." ></td>
	<td class="line x" title="42:209	The process is repeated by greedily selecting subbranches until a leaf is reached." ></td>
	<td class="line x" title="43:209	We classified translated Mainichi documents with Mainichi category m into Reuters categories using SVMs classifiers." ></td>
	<td class="line x" title="44:209	Similarly, each translated Reuters document with category r was classified into Mainichi categories." ></td>
	<td class="line x" title="45:209	Figure 2 illustrates the classification of Reuters and Mainichi documents." ></td>
	<td class="line x" title="46:209	A document with Mainichi category m1 is classified into Reuters category r12, and a document with Reuters category r1 is classified into Mainichi category m21." ></td>
	<td class="line x" title="47:209	As a result, we obtained category pairs, e.g.,(r12, m1), and (m21, r1), from the documents assigned to the categories in each hierarchy." ></td>
	<td class="line x" title="48:209	2.1.2 Estimating category correspondences The assumption of category correspondences is that semantically similar categories, such as Equity markets and Bond markets exhibit similar statistical properties than dissimilar categories, such as Equity markets and Sports." ></td>
	<td class="line x" title="49:209	We applied  2 statistics to the results of CLTC." ></td>
	<td class="line x" title="50:209	Let us take a look at the Reuters96 hierarchy." ></td>
	<td class="line x" title="51:209	Sup234 pose that the translated Mainichi document with Mainichi category m  M (where M is a set of Mainichi categories) is assigned to Reuters category r  R (R is a set of Reuters96 categories)." ></td>
	<td class="line x" title="52:209	We can retrieve Reuters and Mainichi category pairs, and estimate category correspondences according to the  2 statistics shown in Eq." ></td>
	<td class="line x" title="53:209	(1)." ></td>
	<td class="line x" title="54:209	 2 (r, m)= f(r, m) E(r, m) E(r, m) (1) where E(r, m)=S r  S m S R , S r = summationdisplay kM f(r, k),S R = summationdisplay rR S r . Here, the co-occurrence frequency of r and m, f(r, m) is equal to the number of category m documents assigned to r. Similar to the Reuters hierarchy, we can estimate category correspondences from Mainichi hierarchy, and extract a pair (r, m) according to the  2 value." ></td>
	<td class="line x" title="55:209	We note that the similarity obtained by each hierarchy does not have a fixed range." ></td>
	<td class="line x" title="56:209	Thus, we apply the normalization strategy shown in Eq." ></td>
	<td class="line x" title="57:209	(2) to the results obtained by each hierarchy to bring the similarity value into the range [0,1]." ></td>
	<td class="line x" title="58:209	 2 new (r, m)=  2 old (r, m)  2 min (r, m)  2 max (r, m)  2 min (r, m) ." ></td>
	<td class="line x" title="59:209	(2) Let SP r and SP m are a set of pairs obtained by Reuters hierarchy and Mainichi hierarchy, respectively." ></td>
	<td class="line x" title="60:209	We construct the set of r and m category pairs, SP (r,m) = {(r,m) | (r,m)  SP r  SP m }, where each pair is sorted in descending order of  2 value." ></td>
	<td class="line x" title="61:209	For each pair of SP (r,m) , if the value of  2 is higher than a lower bound L  2, two categories, r and m, are regarded as similar." ></td>
	<td class="line x" title="62:209	2 2.2 Retrieval of Relevant Documents We used the results of category correspondences from the Reuters and Mainichi hierarchies to retrieve relevant documents." ></td>
	<td class="line x" title="63:209	Recall that we used English and Japanese documents with quite different hierarchical structures." ></td>
	<td class="line x" title="64:209	The task thus consists of two criteria: retrieving relevant documents based on English (we call this Int hi & Eng) and in Japanese (Int hi&Jap)." ></td>
	<td class="line x" title="65:209	Let d r i (1  i  s) be a Reuters document that is classified into the Reuters category r. Let d m j (1  j  t) be a Mainichi 2 We set  2 value of each element of SP (r,m) to a higher value of either (r,m)  SP r or (r,m)  SP m . Figure 3: Retrieving relevant documents document that belongs to the Mainichi category m. Here, s and t are the number of documents classified into r and m, respectively." ></td>
	<td class="line x" title="66:209	Each Reuters document d r i is translated into a Japanese document d r mt i by an MT system." ></td>
	<td class="line x" title="67:209	Each Mainichi document d m j is translated into an English document d m mt j . Retrieving relevant documents itself is quite simple." ></td>
	<td class="line x" title="68:209	As illustrated in Figure 3, in Int hi & Eng with a set of similar categories consisting of r and m, for each Reuters and translated Mainichi document, we calculate BM25 similarities between them." ></td>
	<td class="line x" title="69:209	BM25(d r i ,d m mt j )= summationdisplay wd m mt j w (1) (k 1 +1)tf K + tf (k 3 +1)qtf k 3 + qtf , (3) where w is a word within d m mt j , and w (1) is the weight of w, w (1) =log (Nn+0.5) (n+0.5) . N is the number of Reuters documents within the same category r, and n is the number of documents which contains w. K refers to k 1 ((1  b)+b dl avdl )." ></td>
	<td class="line x" title="70:209	k 1 , b, and k 3 are parameters and set to 1, 1, and 1,000, respectively." ></td>
	<td class="line x" title="71:209	dl is the document length of d r i and avdl is the average document length in words." ></td>
	<td class="line x" title="72:209	tf and qtf are the frequency of occurrence of w in d r i , and d m mt j , respectively." ></td>
	<td class="line x" title="73:209	If the similarity value between them is higher than a lower bound L  ,we regarded these as relevant documents." ></td>
	<td class="line x" title="74:209	The procedure is applied to all documents belonging to the sets of similar categories." ></td>
	<td class="line x" title="75:209	Int hi & Jap is the same as Int hi & Eng except for the use of d r mt i and d m j for comparison." ></td>
	<td class="line x" title="76:209	We compared the performance of these tasks, and found that Int hi & Eng was better than Int hi & Jap." ></td>
	<td class="line x" title="77:209	In section 3, we show results with Int hi & Eng due to lack of space." ></td>
	<td class="line x" title="78:209	235 2.3 Acquisition of Bilingual Collocations The final step is to estimate bilingual correspondences from relevant documents." ></td>
	<td class="line x" title="79:209	All Japanese documents were parsed using the syntactic analyzer CaboCha (Kudo and Matsumoto, 2003)." ></td>
	<td class="line x" title="80:209	English documents were parsed with the syntactic analyzer (Lin, 1993)." ></td>
	<td class="line x" title="81:209	In both English and Japanese, we extracted all the dependency triplets(obj, n, v)." ></td>
	<td class="line x" title="82:209	Here, n refers to a noun which is an object(obj) of a verb v in a sentence." ></td>
	<td class="line x" title="83:209	3 Hereafter, we describe the Reuters English dependency triplet as vn r , and that of Mainichi as vn m . The method to retrieve bilingual correspondences consists of two sub-steps: document-based retrieval and sentencebased retrieval." ></td>
	<td class="line x" title="84:209	2.3.1 Document-based retrieval We extract vn r and vn m pairs from the results of relevant documents: {vn r ,vn m } s.t.  d r i owner vn r ,  d m j owner vn m BM25(d r i ,d m mt j )  L  ." ></td>
	<td class="line x" title="85:209	(4) Next, we estimate the bilingual correspondences according to the  2 (vn r ,vn m ) statistics shown in Eq." ></td>
	<td class="line x" title="86:209	(1)." ></td>
	<td class="line x" title="87:209	In Eq." ></td>
	<td class="line x" title="88:209	(1), we replace r by vn r and m by vn m . f(r, m) is replaced by f(vn r ,vn m ), i.e., the co-occurrence frequency of vn r and vn m . 2.3.2 Sentence-based retrieval We note that bilingual correspondences obtained by document-based retrieval are not reliable." ></td>
	<td class="line x" title="89:209	This is because many verbnoun collocations appear in a pair of relevant documents, as can be seen from Figure 1." ></td>
	<td class="line x" title="90:209	Therefore, we applied sentence-based retrieval to the results obtained by document-based retrieval." ></td>
	<td class="line x" title="91:209	First, we extract vn r and vn m pairs the  2 values of which are higher than 0." ></td>
	<td class="line x" title="92:209	Next, for each vn r and vn m pair, we assign sentence-based similarity: S sim(vn r ,vn m )= max S vn r Set r ,S vn m Set m sim(S vn r ,S vn m ) ." ></td>
	<td class="line x" title="93:209	(5) Here, Set r and Set m are a set of sentences that include vn r and vn m , respectively." ></td>
	<td class="line x" title="94:209	The similarity between S vn r and S vn m is shown in Eq." ></td>
	<td class="line x" title="95:209	(6)." ></td>
	<td class="line x" title="96:209	3 We used the particle wo as an object relationship in Japanese." ></td>
	<td class="line x" title="97:209	sim(S vn r ,S vn m )= co(S vn r S mt vn m ) | S vn r | + | S mt vn m |2co(S vn r S mt vn m )+2 , (6) where |X| is the number of content words in a sentence X, and co(S vn r  S mt vn m ) refers to the number of content words that appear in both S vn r and S mt vn m . S mt vn m is a translation result of S vm m . We retrieved vn r and vn m as a bilingual lexicon that satisfies: {vn r ,vn m } =argmax {vn r primeBP(vnm),vnm} S sim(vn r prime,vn m ) , (7) where BP(vn m ) is a set of bilingual verbnoun pairs, each of which includes vn m on the Japanese side." ></td>
	<td class="line x" title="98:209	3 Experiments 3.1 Integrating hierarchies 3.1.1 Experimental setup We used Reuters96 and UDC code hierarchies." ></td>
	<td class="line x" title="99:209	The Reuters96 corpus from 20th Aug. 1996 to 19th Aug. 1997 consists of 806,791 documents organized into coarse-grained categories, i.e., 126 categories with a four-level hierarchy." ></td>
	<td class="line x" title="100:209	The RWCP corpus labeled with UDC codes selected from 1994 Mainichi newspaper consists of 27,755 documents organized into a fine-grained categories, i.e., 9,951 categories with a seven-level hierarchy (RWCP., 1998)." ></td>
	<td class="line x" title="101:209	We used Japanese-English and English-Japanese MT software (Internet Honyakuno-Ousama for Linux, Ver.5, IBM Corp.) for CLTC." ></td>
	<td class="line x" title="102:209	We divided both Reuters96 (from 20th Aug. 1996 to 19th May 1997) and RWCP corpora into two equal sets: a training set to train SVM classifiers, and a test set for TC to generate pairs of similar categories." ></td>
	<td class="line x" title="103:209	We divided the test set into two parts: the first was used to estimate thresholds, i.e., a decision threshold b used in CLTC, and lower bound L  2; and the second was used to generate pairs of similar categories using the threshold." ></td>
	<td class="line x" title="104:209	We chose b = 0 for each level of a hierarchy." ></td>
	<td class="line x" title="105:209	The lower bound L  2 was .003." ></td>
	<td class="line x" title="106:209	We selected 109 categories from Reuters and 4,739 categories from Mainichi, which have at least five documents in each set." ></td>
	<td class="line x" title="107:209	We used content words for both English and Japanese documents." ></td>
	<td class="line x" title="108:209	We compared the results obtained by hierarchical approach to those obtained by the flat 236 Table 1: Performance of category correspondences Hierarchy Flat Prec Rec F1 Prec Rec F1 Mai & Reu .503 .463 .482 .462 .389 .422 Reu .342 .329 .335 .240 .296 .265 Mai .157 .293 .204 .149 .277 .194 non-hierarchical approach." ></td>
	<td class="line x" title="109:209	Moreover, in the hierarchical approach, we applied a Boolean function to each test document." ></td>
	<td class="line x" title="110:209	For evaluation of category correspondences, we used F1-score (F1) which is a measure that balances precision (Prec) and recall (Rec)." ></td>
	<td class="line x" title="111:209	Let Cor be a set of correct category pairs." ></td>
	<td class="line x" title="112:209	4 The precise definitions of the precision and recall of the task are given below: Prec = |{(r, m) | (r, m)  Cor, 2 (r, m)  L  2}| |{(r, m) |  2 (r, m)  L  2}| Rec = |{(r, m) | (r, m)  Cor, 2 (r, m)  L  2}| |{(r, m) | (r, m)  Cor}| 3.1.2 Results Table 1 shows F1 of category correspondences with L  2 = .003." ></td>
	<td class="line x" title="113:209	Mai & Reu shows the results obtained by our method." ></td>
	<td class="line x" title="114:209	Mai and Reu show the results using only one hierarchy." ></td>
	<td class="line x" title="115:209	For example, Mai shows the results in which both Mainichi and translated Reuters documents are classified into categories with Mainichi hierarchy, and estimated category correspondences." ></td>
	<td class="line x" title="116:209	Integrating hierarchies is more effective than only a single hierarchy." ></td>
	<td class="line x" title="117:209	Moreover, we found advantages in the F1 for the hierarchical approach (Hierarchy in Table 1) in comparison with a baseline flat approach (Flat)." ></td>
	<td class="line x" title="118:209	We note that the result of Mai was worse than that of Reu in both approaches." ></td>
	<td class="line x" title="119:209	One reason is that the accuracy of TC." ></td>
	<td class="line x" title="120:209	The micro-average F1 of TC for Reuters hierarchy was .815, while that of Mainichi was .673, as Mainichi hierarchy consists of many categories, and the number of training data for each category were smaller than those of Reuters." ></td>
	<td class="line x" title="121:209	The results obtained by our method depend on the performance of TC." ></td>
	<td class="line x" title="122:209	Therefore, it will be necessary to examine some semi-supervised learning techniques to improve classification accuracy." ></td>
	<td class="line x" title="123:209	4 The classification was determined to be correct if the two human judges agreed on the evaluation." ></td>
	<td class="line x" title="124:209	Table 2: Data for retrieving documents Jap  Eng( 3) Total # of doc." ></td>
	<td class="line x" title="125:209	Total # of Jap Eng relevant doc." ></td>
	<td class="line x" title="126:209	26/06/97 391 15,482 513 3.2 Relevant document retrieval 3.2.1 Experimental setup The training data for choosing the lower bound L  used in the relevant document retrieval is Reuters and RWCP from 13th to 21st Jun. 1997." ></td>
	<td class="line x" title="127:209	The difference in dates between them is less than  3 days." ></td>
	<td class="line x" title="128:209	For example, when the date of the RWCP document is 18th Jun., the corresponding Reuters date is from 15th to 21st Jun. We chose L  that maximized the average F1 among them." ></td>
	<td class="line x" title="129:209	Table 2 shows the test data, i.e., the total number of collected documents and the number of related documents collected manually for the evaluation." ></td>
	<td class="line x" title="130:209	5 We implemented the following approaches including related work, and compared these results with those obtained by our methods, Int hi & Eng." ></td>
	<td class="line x" title="131:209	1." ></td>
	<td class="line x" title="132:209	No hierarchy: Categories with each hierarchy are not used in the approach." ></td>
	<td class="line x" title="133:209	The approach is the same as the method reported by Collier et al.(1998) except for term weights and similarities." ></td>
	<td class="line x" title="135:209	We calculate similarities between Reuters and translated Mainichi documents, where the difference in dates is less than  3 days." ></td>
	<td class="line x" title="136:209	(No hi & Eng)." ></td>
	<td class="line x" title="137:209	2." ></td>
	<td class="line x" title="138:209	Hierarchy: The approach uses only Reuters hierarchy (we call this Reu Hierarchy)." ></td>
	<td class="line x" title="139:209	Reuters documents and translated Mainichi documents are classified into categories with Reuters hierarchy." ></td>
	<td class="line x" title="140:209	We calculate BM25 between Reuters and Mainichi documents within the same category." ></td>
	<td class="line x" title="141:209	The procedure is applied for all categories of the hierarchies." ></td>
	<td class="line x" title="142:209	The judgment of relevant documents was the same as our method: if the value of similarity between two documents is higher than a lower bound L  , we regarded them as relevant documents." ></td>
	<td class="line x" title="143:209	3.2.2 Results The retrieval results are shown in Table 3 and Figure 4." ></td>
	<td class="line x" title="144:209	Table 3 shows best performance of each method against L  . As can be seen clearly from Table 3 and Figure 4, the results with integrating hierarchies improved overall performance." ></td>
	<td class="line x" title="145:209	5 The classification was determined by two human." ></td>
	<td class="line x" title="146:209	237 Table 3: Retrieval performance Prec Rec F1-score L  No hi & Eng .417 .322 .363 40 Reu Hierarchy .356 .544 .430 20 Int hi & Eng .839 .585 .689 20 Figure 4: F1 of retrieving relevant documents Table 4 shows the total number of document pairs (P), Reuters (E), and Mainichi documents (J), which satisfied the similarity lower bound L  .As shown in Table 4, the number of retrieved pairs by non-hierarchy approach was much greater than that of Int hi & Eng at all L  values." ></td>
	<td class="line x" title="147:209	This is because pairs are retrieved by using only the BM25." ></td>
	<td class="line x" title="148:209	Therefore, many of the document pairs retrieved do not have closely related contents, even if L  is set to a higher value." ></td>
	<td class="line x" title="149:209	The results of a single hierarchy showed recall of .544, while that of the integrating hierarchies was .585 at the same L  value (20), as shown in Table 3." ></td>
	<td class="line x" title="150:209	This is because in the single hierarchy method, there are some translated Mainichi documents that are not correctly classified into categories with the Reuters hierarchy." ></td>
	<td class="line x" title="151:209	For example, Hashimoto remarks on fx rates in Mainichi documents should be classified into Reuters category Forex markets, but it was classified into Government." ></td>
	<td class="line x" title="152:209	As a result, U.S. Treasury has no comment on Hashimoto fx remarks in Reuters category Forex markets and the document Hashimoto are not retrieved by a single hierarchy approach." ></td>
	<td class="line x" title="153:209	In contrast, in the integrating method, these two documents are classified correctly into a pair of similar categories, i.e., the U.S Treasury is classified into Reuters category Forex markets, and the Hashimoto is classified into Mainichi category Money and banking." ></td>
	<td class="line x" title="154:209	These observations show that our method contributes to the retrieval of relevant documents." ></td>
	<td class="line x" title="155:209	Table 4: # of documents vs L  Approach Lower Bound L  100 80 60 40 20 p 188 319 630 1,229 3,000 No hi & Eng E 150 272 543 987 2,053 J 13 16 19 22 25 p 12 17 25 47 186 Reu Hierarchy E 8 12 19 36 142 J 8 10 12 18 25 p 46 61 83 135 218 Int hi & Eng E 32 43 60 99 158 J 4 4 5 7 9 Table 5: # of J/E document pairs with L  Approach & (L  ) pairs Eng Jap No hi & Eng (40) 3,042,166 428,042 70,080 Reu Hierarchy (20) 27,181,243 43,0181 99,452 Int hi & Eng (20) 81,904,243 45,965 654,787 3.3 Bilingual Verbnoun Collocations Finally, we report the results of bilingual verb noun collocations." ></td>
	<td class="line x" title="156:209	3.3.1 Experimental setup The data for relevant document retrieval was the Reuters and Mainichi corpora from the same period, i.e., 20th Aug. 1996 to 19th Aug. 1997." ></td>
	<td class="line x" title="157:209	The total number of Reuters documents was 806,791, and that of Mainichi was 119,822." ></td>
	<td class="line x" title="158:209	As the number of Reuters documents was far greater than that of Mainichi documents, we estimated collocations from the results of cross-lingually retrieving relevant English documents with Japanese query documents." ></td>
	<td class="line x" title="159:209	The difference in dates between them was less than  3 days." ></td>
	<td class="line x" title="160:209	Table 5 shows retrieved relevant documents that showed best performance of each method against L  . From these data, we extracted bilingual verb-noun collocations." ></td>
	<td class="line x" title="161:209	3.3.2 Results Table 6 shows the numbers of English and Japanese monolingual verbnoun collocations, those of candidate collocations against which bilingual correspondences were estimated, and those of correct collocations." ></td>
	<td class="line x" title="162:209	D & S of candidate collocations indicates the number of collocations when we applied both documentand sentencebased retrieval." ></td>
	<td class="line x" title="163:209	Doc indicates the number of collocations when we applied only document-based retrieval." ></td>
	<td class="line x" title="164:209	D & S and Doc of correct collocations show the number of correct collocations in the topmost 1,000 according to sentence similarity and the  2 statistics, respectively." ></td>
	<td class="line x" title="165:209	As shown in 238 Table 6, the results obtained by integrating hierarchies showed a 15.1% (32.8 17.7) improvement over the baseline non-hierarchy model, and a 6.0% (32.8 26.8) improvement over use of a single hierarchy." ></td>
	<td class="line x" title="166:209	We manually compared those 328 bilingual collocations with an existing bilingual lexicon where 78 of them (23.8%) were not included in it." ></td>
	<td class="line x" title="167:209	6 Moreover, 168 of 328 (51.2%) were not correctly translated by Japanese-English MT software." ></td>
	<td class="line x" title="168:209	7 These observations clearly support the usefulness of the method." ></td>
	<td class="line x" title="169:209	It is very important to compare the column rate for the numbers of candidate collocations with that for the numbers of correct collocations." ></td>
	<td class="line x" title="170:209	In all approaches, sentence-based retrieval was effective in removing useless collocations, especially in our method, about 1.5% of the size obtained by Doc was retrieved, while about 4.6(328/72) times the number of correct collocations were obtained in the topmost 1,000 collocations." ></td>
	<td class="line x" title="171:209	These observations showed that sentencebased retrieval contributes to a marked reduction in the number of useless collocations without a decrease in accuracy." ></td>
	<td class="line x" title="172:209	The last column in Table 6 shows the results using Inverse Rank Score (IRS), which is a measure of system performance by considering the rank of correct bilingual collocations within the candidate collocations." ></td>
	<td class="line x" title="173:209	It is the sum of the inverse rank of each matching collocations, e.g., correct collocations by manual evaluation matches at ranks 2 and 4giveanIRSof 1 2 + 1 4 = 0.75." ></td>
	<td class="line x" title="174:209	With at most 1,000 collocations, the maximum IRS score is 7.485, and the higher the IRS value, the better the system performance." ></td>
	<td class="line x" title="175:209	As shown in Table 6, the performance by integrating hierarchies was much better than that of the non-hierarchical approach, and slightly better than those obtained by a single hierarchy." ></td>
	<td class="line x" title="176:209	However, correct retrieved collocations were different from each other." ></td>
	<td class="line x" title="177:209	Table 7 lists examples of bilingual collocations obtained by a single hierarchy and integrating hierarchies." ></td>
	<td class="line x" title="178:209	The category is Sport." ></td>
	<td class="line x" title="179:209	8 (x,y) of category pair in Table 7 refer to Reuters and Mainichi category correspondences." ></td>
	<td class="line x" title="180:209	Examples in Table 7 denote only English verb 6 We used an existing bilingual lexicon, Eijiro on the Web, 1.91 million words, (http://www.alc.co.jp) for evaluation." ></td>
	<td class="line x" title="181:209	If collocations were not included, the estimation was determined by two human judges." ></td>
	<td class="line x" title="182:209	7 The number of words in the Japanese-English dictionary (Internet Honyaku-no-Ousama for Linux, Ver.5, IBM Corp.) was about 250,000." ></td>
	<td class="line x" title="183:209	8 We obtained 98 category pairs in the Sport category." ></td>
	<td class="line x" title="184:209	noun collocations." ></td>
	<td class="line x" title="185:209	It is interesting to note that 12 of 154 collocations, such as earn medal and block shot obtained by integrating hierarchies were also obtained by a single hierarchy approach." ></td>
	<td class="line x" title="186:209	However, other collocations such as get strikeout and make birdie which were obtained in a particular category (Sport, Baseball) and (Sport, Golf), did not appear in either of the results using a single hierarchy or a non hierarchical approach." ></td>
	<td class="line x" title="187:209	These observations again clearly support the usefulness of our method." ></td>
	<td class="line x" title="188:209	4 Previous Work Much of the previous work on finding bilingual lexicons used comparable corpora." ></td>
	<td class="line x" title="189:209	One attempt involved directly retrieving bilingual lexicons from corpora." ></td>
	<td class="line x" title="190:209	One approach focused on extracting word translations (Gaussier et al., 2004)." ></td>
	<td class="line x" title="191:209	The techniques were based on the idea that semantically similar words appear in similar contexts." ></td>
	<td class="line x" title="192:209	Unlike parallel corpora, the position of a word in a document is useless for translation into the other language." ></td>
	<td class="line x" title="193:209	In these techniques, the frequency of words in the monolingual document is calculated and their contextual similarity is measured across languages." ></td>
	<td class="line x" title="194:209	Another approach focused on sentence extraction (Fung and Cheung, 2004)." ></td>
	<td class="line x" title="195:209	One limitation of all these methods is that they need to control the experimental evaluation to avoid estimation of every bilingual lexicon appearing in comparable corpora." ></td>
	<td class="line x" title="196:209	The alternative consists of two steps: first, crosslingual relevant documents are retrieved from comparable corpora, then bilingual term correspondences within these relevant documents are estimated." ></td>
	<td class="line x" title="197:209	Thus, the accuracy depends on the performance of relevant documents retrieval." ></td>
	<td class="line x" title="198:209	Much of the previous work in finding relevant documents used MT systems or existing bilingual lexicons to translate one language into another." ></td>
	<td class="line x" title="199:209	Document pairs are then retrieved using some measure of document similarity." ></td>
	<td class="line x" title="200:209	Another approach to retrieving relevant documents involves the collection of relevant document URLs from the WWW (Resnik and Smith, 2003)." ></td>
	<td class="line x" title="201:209	Utsuro et al.(2003) proposed a method for acquiring bilingual lexicons that involved retrieval of relevant English and Japanese documents from news sites on the WWW." ></td>
	<td class="line x" title="203:209	Our work is also applicable to retrieval of relevant documents on the web because it estimates every bilingual lexicon only appearing in 239 Table 6: Numbers of monolingual and bilingual verbnoun collocations Approach & (L  ) #of Candidate collocations # of Correct collocations Inverse (top 1,000) rank score Monolingual patterns # of collocations rate # of collocations rate (D & S/ (D&S/ (top 1,000) Jap Eng D&S Doc Doc) D&S Doc Doc) D&S Doc No hi & Eng (40) 25,163 44,762 25,163 6,976,214 .361 177 62 2.9 1.35 0.71 Reu Hierarchy (20) 10,576 37,022 10,576 1,272,102 .831 268 64 4.2 2.24 1.41 Int hi & Eng (20) 8,347 21,524 8,347 560,472 1.489 328 72 4.6 2.33 1.46 Table 7: Examples of bilingual verbnoun collocations Approach & (L  ) Category or # of collocations # of correct Examples (English) category pair D&S Doc collocations(%) Reu Hierarchy (20) Sport 262 19,391 36(13.7) create chance, earn medal, feel pressure block shot, establish record, take chance (Sport, Baseball) 110 8,838 24(21.8) get strikeout, leave base, throw pitch (Sport, Relay) 177 3,418 18(10.2) lead ranking, run km, win athletic (Sport, Tennis) 115 2,656 32(27.8) lose prize money, play exhibition game Int hi & Eng (20) (Sport, Golf) 131 2,654 28(21.4) make birdie, have birdie, hole putt, miss putt (Sport, Soccer) 86 1,317 34(39.5) block shot, score defender, give free kick (Sport, Sumo) 75 773 2(2.7) lead sumo, set championship (Sport, Ski jump) 68 661 10(14.7) postpone downhill, earn medal (Sport, Football) 37 461 6(16.2) play football, lease football stadium a set of smaller documents belonging to pairs of similar categories." ></td>
	<td class="line x" title="204:209	Munteanu and Marcu (2006) proposed a method for extracting parallel subsentential fragments from very non-parallel bilingual corpora." ></td>
	<td class="line x" title="205:209	The method is based on the fact that very non-parallel corpora has none or few good sentence pairs, while existing methods for exploiting comparable corpora look for parallel data at the sentence level." ></td>
	<td class="line x" title="206:209	Their methodology is the first aimed at detecting sub-sentential correspondences, while they have not reported that the method is also applicable for large amount of data with good performance, especially in the case of large-scale evaluation such as that presented in this paper." ></td>
	<td class="line x" title="207:209	5 Conclusion We have developed an approach to bilingual verb noun collocations from non-parallel corpora." ></td>
	<td class="line x" title="208:209	The results showed the effectiveness of the method." ></td>
	<td class="line oc" title="209:209	Future work will include: (i) applying the method to retrieve other types of collocations (Smadja, 1993), and (ii) evaluating the method using Internet directories." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="I08-1014
Determining the Unithood of Word Sequences Using a Probabilistic Approach
Wong, Wilson;Liu, Wei;Bennamoun, Mohammed;"></td>
	<td class="line x" title="1:157	Determining the Unithood of Word Sequences using a Probabilistic Approach Wilson Wong, Wei Liu and Mohammed Bennamoun School of Computer Science and Software Engineering University of Western Australia Crawley WA 6009 {wilson,wei,bennamou}@csse.uwa.edu.au Abstract Most research related to unithood were conducted as part of a larger effort for the determination of termhood." ></td>
	<td class="line x" title="2:157	Consequently, novelties are rare in this small sub-field of term extraction." ></td>
	<td class="line x" title="3:157	In addition, existing work were mostly empirically motivated and derived." ></td>
	<td class="line x" title="4:157	We propose a new probabilistically-derived measure, independent of any influences of termhood, that provides dedicated measures to gather linguistic evidence from parsed text and statistical evidence from Google search engine for the measurement of unithood." ></td>
	<td class="line x" title="5:157	Our comparative study using 1,825 test cases against an existing empiricallyderived function revealed an improvement in terms of precision, recall and accuracy." ></td>
	<td class="line x" title="6:157	1 Introduction Automatic term recognition, also referred to as term extraction or terminology mining, is the process of extracting lexical units from text and filtering them for the purpose of identifying terms which characterise certain domains of interest." ></td>
	<td class="line x" title="7:157	This process involves the determination of two factors: unithood and termhood." ></td>
	<td class="line x" title="8:157	Unithood concerns with whether or not a sequence of words should be combined to form a more stable lexical unit." ></td>
	<td class="line x" title="9:157	On the other hand, termhood measures the degree to which these stable lexical units are related to domain-specific concepts." ></td>
	<td class="line x" title="10:157	Unithood is only relevant to complex terms (i.e. multi-word terms) while termhood (Wong et al., 2007a) deals with both simple terms (i.e. singleword terms) and complex terms." ></td>
	<td class="line x" title="11:157	Recent reviews by (Wong et al., 2007b) show that existing research on unithood are mostly carried out as a prerequisite to the determination of termhood." ></td>
	<td class="line x" title="12:157	As a result, there is only a small number of existing measures dedicated to determining unithood." ></td>
	<td class="line x" title="13:157	Besides the lack of dedicated attention in this sub-field of term extraction, the existing measures are usually derived from term or document frequency, and are modified as per need." ></td>
	<td class="line x" title="14:157	As such, the significance of the different weights that compose the measures usually assume an empirical viewpoint." ></td>
	<td class="line x" title="15:157	Obviously, such methods are at most inspired by, but not derived from formal models (Kageura and Umino, 1996)." ></td>
	<td class="line x" title="16:157	The three objectives of this paper are (1) to separate the measurement of unithood from the determination of termhood, (2) to devise a probabilisticallyderived measure which requires only one threshold for determining the unithood of word sequences using non-static textual resources, and (3) to demonstrate the superior performance of the new probabilistically-derived measure against existing empirical measures." ></td>
	<td class="line x" title="17:157	In regards to the first objective, we will derive our probabilistic measure free from any influence of termhood determination." ></td>
	<td class="line x" title="18:157	Following this, our unithood measure will be an independent tool that is applicable not only to term extraction, but many other tasks in information extraction and text mining." ></td>
	<td class="line x" title="19:157	Concerning the second objective, we will devise our new measure, known as the Odds of Unithood (OU), which are derived using Bayes Theorem and founded on a few elementary probabilities." ></td>
	<td class="line x" title="20:157	The probabilities are estimated using Google page counts in an attempt to eliminate problems related to the use of static corpora." ></td>
	<td class="line x" title="21:157	Moreover, only 103 one threshold, namely, OUT is required to control the functioning of OU." ></td>
	<td class="line x" title="22:157	Regarding the third objective, we will compare our new OU against an existing empirically-derived measure called Unithood (UH) (Wong et al., 2007b) in terms of their precision, recall and accuracy." ></td>
	<td class="line x" title="23:157	In Section 2, we provide a brief review on some of existing techniques for measuring unithood." ></td>
	<td class="line x" title="24:157	In Section 3, we present our new probabilistic approach, the measures involved, and the theoretical and intuitive justification behind every aspect of our measures." ></td>
	<td class="line x" title="25:157	In Section 4, we summarize some findings from our evaluations." ></td>
	<td class="line x" title="26:157	Finally, we conclude this paper with an outlook to future work in Section 5." ></td>
	<td class="line x" title="27:157	2 Related Works Some of the most common measures of unithood include pointwise mutual information (MI) (Church and Hanks, 1990) and log-likelihood ratio (Dunning, 1994)." ></td>
	<td class="line x" title="28:157	In mutual information, the co-occurrence frequencies of the constituents of complex terms are utilised to measure their dependency." ></td>
	<td class="line x" title="29:157	The mutual information for two words a and b is defined as: MI(a,b) = log2 p(a,b)p(a)p(b) (1) where p(a) and p(b) are the probabilities of occurrence of a and b. Many measures that apply statistical techniques assuming strict normal distribution, and independence between the word occurrences (Franz, 1997) do not fare well." ></td>
	<td class="line x" title="30:157	For handling extremely uncommon words or small sized corpus, log-likelihood ratio delivers the best precision (Kurz and Xu, 2002)." ></td>
	<td class="line x" title="31:157	Log-likelihood ratio attempts to quantify how much more likely one pair of words is to occur compared to the others." ></td>
	<td class="line x" title="32:157	Despite its potential, How to apply this statistic measure to quantify structural dependency of a word sequence remains an interesting issue to explore. (Kit, 2002)." ></td>
	<td class="line x" title="33:157	(Seretan et al., 2004) tested mutual information, loglikelihood ratio and t-tests to examine the use of results from web search engines for determining the collocational strength of word pairs." ></td>
	<td class="line x" title="34:157	However, no performance results were presented." ></td>
	<td class="line x" title="35:157	(Wong et al., 2007b) presented a hybrid approach inspired by mutual information in Equation 1, and C-value in Equation 3." ></td>
	<td class="line x" title="36:157	The authors employ Google page counts for the computation of statistical evidences to replace the use of frequencies obtained from static corpora." ></td>
	<td class="line x" title="37:157	Using the page counts, the authors proposed a function known as Unithood (UH) for determining the mergeability of two lexical units ax and ay to produce a stable sequence of words s. The word sequences are organised as a set W = {s,ax,ay} where s = axbay is a term candidate, b can be any preposition, the coordinating conjunction and or an empty string, and ax and ay can either be noun phrases in the form AdjN+ or another s (i.e. defining a new s in terms of other s)." ></td>
	<td class="line x" title="38:157	The authors define UH as: UH(ax,ay) =            1 if (MI(ax,ay) > MI+)  (MI+  MI(ax,ay)  MI ID(ax,s)  IDT  ID(ay,s)  IDT  IDR+  IDR(ax,ay)  IDR) 0 otherwise (2) where MI+, MI, IDT , IDR+ and IDR are thresholds for determining mergeability decisions, and MI(ax,ay) is the mutual information between ax and ay, while ID(ax,s), ID(ay,s) and IDR(ax,ay) are measures of lexical independence of ax and ay from s. For brevity, let z be either ax or ay, and the independence measure ID(z,s) is then defined as: ID(z,s) = braceleftBigg log10(nz ns) if(nz > ns) 0 otherwise where nz and ns is the Google page count for z and s respectively." ></td>
	<td class="line x" title="39:157	On the other hand, IDR(ax,ay) = ID(ax,s) ID(ay,s)." ></td>
	<td class="line x" title="40:157	Intuitively, UH(ax,ay) states that the twolexical units a x and ay can only be merged in two cases, namely, 1) if ax and ay has extremely high mutual information (i.e. higher than a certain threshold MI+), or 2) if ax and ay achieve average mutual information (i.e. within the acceptable range of two thresholds MI+ and MI) due to both of their extremely high independence (i.e. higher than the threshold IDT ) from s." ></td>
	<td class="line x" title="41:157	(Frantzi, 1997) proposed a measure known as Cvalue for extracting complex terms." ></td>
	<td class="line x" title="42:157	The measure 104 is based upon the claim that a substring of a term candidate is a candidate itself given that it demonstrates adequate independence from the longer version it appears in." ></td>
	<td class="line x" title="43:157	For example, E. coli food poisoning, E. coli and food poisoning are acceptable as valid complex term candidates." ></td>
	<td class="line x" title="44:157	However, E. coli food is not." ></td>
	<td class="line x" title="45:157	Therefore, some measures are required to gauge the strength of word combinations to decide whether two word sequences should be merged or not." ></td>
	<td class="line x" title="46:157	Given a word sequence a to be examined for unithood, the Cvalue is defined as: Cvalue(a) = braceleftBigglog 2 |a|fa if |a| = g log2 |a|(fa  summationtext lLa fl |La| ) otherwise (3) where |a| is the number of words in a, La is the set of longer term candidates that contain a, g is the longest n-gram considered, fa is the frequency of occurrence of a, and a / La. While certain researchers (Kit, 2002) consider Cvalue as a termhood measure, others (Nakagawa and Mori, 2002) accept it as a measure for unithood." ></td>
	<td class="line x" title="47:157	One can observe that longer candidates tend to gain higher weights due to the inclusion of log2 |a| in Equation 3." ></td>
	<td class="line x" title="48:157	In addition, the weights computed using Equation 3 are purely dependent on the frequency of a. 3 A Probabilistically-derived Measure for Unithood Determination We propose a probabilistically-derived measure for determining the unithood of word pairs (i.e. potential term candidates) extracted using the headdriven left-right filter (Wong, 2005; Wong et al., 2007b) and Stanford Parser (Klein and Manning, 2003)." ></td>
	<td class="line x" title="49:157	These word pairs will appear in the form of (ax,ay)  A with ax and ay located immediately next to each other (i.e. x + 1 = y), or separated by a preposition or coordinating conjunction and (i.e. x+2 = y)." ></td>
	<td class="line x" title="50:157	Obviously, ax has to appear before ay in the sentence or in other words, x < y for all pairs where x and y are the word offsets produced by the Stanford Parser." ></td>
	<td class="line x" title="51:157	The pairs in A will remain as potential term candidates until their unithood have been examined." ></td>
	<td class="line x" title="52:157	Once the unithood of the pairs in A have been determined, they will be referred to as term candidates." ></td>
	<td class="line x" title="53:157	Formally, the unithood of any two lexical units ax and ay can be defined as Definition 1 The unithood of two lexical units is the degree of strength or stability of syntagmatic combinations and collocations (Kageura and Umino, 1996) between them." ></td>
	<td class="line x" title="54:157	It is obvious that the problem of measuring the unithood of any pair of words is the determination of their degree of collocational strength as mentioned in Definition 1." ></td>
	<td class="line x" title="55:157	In practical terms, the degree mentioned above will provide us with a way to determine if the units ax and ay should be combined to form s, or left alone as separate units." ></td>
	<td class="line x" title="56:157	The collocational strength of ax and ay that exceeds a certain threshold will demonstrate to us that s is able to form a stable unit and hence, a better term candidate than ax and ay separated." ></td>
	<td class="line x" title="57:157	It is worth pointing that the size (i.e. number of words) of ax and ay is not limited to 1." ></td>
	<td class="line x" title="58:157	For example, we can have ax=National Institute, b=of and ay=Allergy and Infectious Diseases." ></td>
	<td class="line x" title="59:157	In addition, the size of ax and ay has no effect on the determination of their unithood using our approach." ></td>
	<td class="line x" title="60:157	As we have discussed in Section 2, most of the conventional practices employ frequency of occurrence from local corpora, and some statistical tests or information-theoretic measures to determine the coupling strength between elements in W = {s,ax,ay}." ></td>
	<td class="line x" title="61:157	Two of the main problems associated with such approaches are:  Data sparseness is a problem that is welldocumented by many researchers (Keller et al., 2002)." ></td>
	<td class="line x" title="62:157	It is inherent to the use of local corpora that can lead to poor estimation of parameters or weights; and  Assumption of independence and normality of word distribution are two of the many problems in language modelling (Franz, 1997)." ></td>
	<td class="line x" title="63:157	While the independence assumption reduces text to simply a bag of words, the assumption of normal distribution of words will often lead to incorrect conclusions during statistical tests." ></td>
	<td class="line x" title="64:157	As a general solution, we innovatively employ results from web search engines for use in a probabilistic framework for measuring unithood." ></td>
	<td class="line x" title="65:157	As an attempt to address the first problem, we utilise page counts by Google for estimating the probability of occurrences of the lexical units in W. 105 We consider the World Wide Web as a large general corpus and the Google search engine as a gateway for accessing the documents in the general corpus." ></td>
	<td class="line x" title="66:157	Our choice of using Google to obtain the page count was merely motivated by its extensive coverage." ></td>
	<td class="line x" title="67:157	In fact, it is possible to employ any search engines on the World Wide Web for this research." ></td>
	<td class="line x" title="68:157	As for the second issue, we attempt to address the problem of determining the degree of collocational strength in terms of probabilities estimated using Google page count." ></td>
	<td class="line x" title="69:157	We begin by defining the sample space, N as the set of all documents indexed by Google search engine." ></td>
	<td class="line x" title="70:157	We can estimate the index size of Google, |N| using function words as predictors." ></td>
	<td class="line x" title="71:157	Function words such as a, is and with, as opposed to content words, appear with frequencies that are relatively stable over many different genres." ></td>
	<td class="line x" title="72:157	Next, we perform random draws (i.e. trial) of documents from N. For each lexical unit w  W, there will be a corresponding set of outcomes (i.e. events) from the draw." ></td>
	<td class="line x" title="73:157	There will be three basic sets which are of interest to us: Definition 2 Basic events corresponding to each w  W:  X is the event that ax occurs in the document  Y is the event that ay occurs in the document  S is the event that s occurs in the document It should be obvious to the readers that since the documents in S have to contain all two units ax and ay, S is a subset of X  Y or S  X  Y . It is worth noting that even though S  X  Y , it is highly unlikely that S = X  Y since the two portions ax and ay may exist in the same document without being conjoined by b. Next, subscribing to the frequency interpretation of probability, we can obtain the probability of the events in Definition 2 in terms of Google page count: P(X) = nx|N| (4) P(Y) = ny|N| P(S) = ns|N| where nx, ny and ns is the page count returned as the result of Google search using the term [+ax], [+ay] and [+s], respectively." ></td>
	<td class="line x" title="74:157	The pair of quotes that encapsulates the search terms is the phrase operator, while the character + is the required operator supported by the Google search engine." ></td>
	<td class="line x" title="75:157	As discussed earlier, the independence assumption required by certain information-theoretic measures and other Bayesian approaches may not always be valid, especially when we are dealing with linguistics." ></td>
	<td class="line x" title="76:157	As such, P(X  Y) negationslash= P(X)P(Y) since the occurrences of ax and ay in documents are inevitably governed by some hidden variables and hence, not independent." ></td>
	<td class="line x" title="77:157	Following this, we define the probabilities for two new sets which result from applying some set operations on the basic events in Definition 2: P(X Y) = nxy|N| (5) P(X Y \S) = P(X Y)P(S) where nxy is the page count returned by Google for the search using [+ax +ay]." ></td>
	<td class="line x" title="78:157	Defining P(XY) in terms of observable page counts, rather than a combination of two independent events will allow us to avoid any unnecessary assumption of independence." ></td>
	<td class="line x" title="79:157	Next, referring back to our main problem discussed in Definition 1, we are required to estimate the strength of collocation of the two units ax and ay." ></td>
	<td class="line x" title="80:157	Since there is no standard metric for such measurement, we propose to address the problem from a probabilistic perspective." ></td>
	<td class="line x" title="81:157	We introduce the probability that s is a stable lexical unit given the evidence s possesses: Definition 3 Probability of unithood: P(U|E) = P(E|U)P(U)P(E) where U is the event that s is a stable lexical unit and E is the evidences belonging to s. P(U|E) is the posterior probability that s is a stable unit given the evidence E. P(U) is the prior probability that s is a unit without any evidence, and P(E) is the prior probability of evidences held by s. As we shall see later, these two prior probabilities will be immaterial in the final computation of unithood." ></td>
	<td class="line x" title="82:157	Since s can either be a stable unit or not, we can state that, P(U|E) = 1P(U|E) (6) 106 where U is the event that s is not a stable lexical unit." ></td>
	<td class="line oc" title="83:157	Since Odds = P/(1  P), we multiply both sides of Definition 3 by (1P(U|E))1 to obtain, P(U|E) 1P(U|E) = P(E|U)P(U) P(E)(1P(U|E)) (7) By substituting Equation 6 in Equation 7 and later, applying the multiplication rule P(U|E)P(E) = P(E|U)P(U) to it, we will obtain: P(U|E) P(U|E) = P(E|U)P(U) P(E|U)P(U) (8) We proceed to take the log of the odds in Equation 8 (i.e. logit) to get: log P(E|U)P(E|U) = log P(U|E)P(U|E) log P(U)P(U) (9) While it is obvious that certain words tend to cooccur more frequently than others (i.e. idioms and collocations), such phenomena are largely arbitrary (Smadja, 1993)." ></td>
	<td class="line x" title="84:157	This makes the task of deciding on what constitutes an acceptable collocation difficult." ></td>
	<td class="line x" title="85:157	The only way to objectively identify stable lexical units is through observations in samples of the language (e.g. text corpus) (McKeown and Radev, 2000)." ></td>
	<td class="line x" title="86:157	In other words, assigning the apriori probability of collocational strength without empirical evidence is both subjective and difficult." ></td>
	<td class="line x" title="87:157	As such, we are left with the option to assume that the probability of s being a stable unit and not being a stable unit without evidence is the same (i.e. P(U) = P(U) = 0.5)." ></td>
	<td class="line x" title="88:157	As a result, the second term in Equation 9 evaluates to 0: log P(U|E)P(U|E) = log P(E|U)P(E|U) (10) We introduce a new measure for determining the odds of s being a stable unit known as Odds of Unithood (OU): Definition 4 Odds of unithood OU(s) = log P(E|U)P(E|U) Assuming that the evidences in E are independent of one another, we can evaluate OU(s) in terms of: OU(s) = log producttext i P(ei|U)producttext i P(ei|U) (11) = summationdisplay i log P(ei|U)P(e i|U) (a) The area with darker shade is the set X  Y \ S. Computing the ratio of P(S) and the probability of this area will give us the first evidence." ></td>
	<td class="line x" title="89:157	(b) The area with darker shade is the set S." ></td>
	<td class="line x" title="90:157	Computing the ratio of P(S) and the probability of this area (i.e. P(S) = 1P(S)) will give us the second evidence." ></td>
	<td class="line x" title="91:157	Figure 1: The probability of the areas with darker shade are the denominators required by the evidences e1 and e2 for the estimation of OU(s)." ></td>
	<td class="line x" title="92:157	where ei are individual evidences possessed by s. With the introduction of Definition 4, we can examine the degree of collocational strength of ax and ay in forming s, mentioned in Definition 1 in terms of OU(s)." ></td>
	<td class="line x" title="93:157	With the base of the log in Definition 4 more than 1, the upper and lower bound of OU(s) would be + and , respectively." ></td>
	<td class="line x" title="94:157	OU(s) = + and OU(s) =  corresponds to the highest and the lowest degree of stability of the two units ax and ay appearing as s, respectively." ></td>
	<td class="line x" title="95:157	A high1 OU(s) would indicate the suitability for the two units ax and ay to be merged to form s. Ultimately, we have reduced the vague problem of the determination of unithood introduced in Definition 1 into a practical and computable solution in Definition 4." ></td>
	<td class="line x" title="96:157	The evidences that we propose to employ for determining unithood are based on the occurrences of s, or the event S if the readers recall from Definition 2." ></td>
	<td class="line x" title="97:157	We are interested in two types of occurrences of s, namely, the occurrence of s given that ax and ay have already occurred or X  Y , and the occurrence of s as it is in our sample space, N. We refer to the first evidence e1 as local occurrence, while the second one e2 as global occurrence." ></td>
	<td class="line x" title="98:157	We will discuss the intuitive justification behind each type of occurrences." ></td>
	<td class="line x" title="99:157	Each evidence ei captures the occurrences of s within a different confinement." ></td>
	<td class="line x" title="100:157	We will estimate these evidences in terms of the elementary probabilities already defined in Equations 4 and 5." ></td>
	<td class="line x" title="101:157	The first evidence e1 captures the probability of occurrences of s within the confinement of ax and ay 1A subjective issue that may be determined using a threshold 107 or XY . As such, P(e1|U) can be interpreted as the probability of s occurring within X Y as a stable unit or P(S|X  Y)." ></td>
	<td class="line x" title="102:157	On the other hand, P(e1|U) captures the probability of s occurring in X Y not as a unit." ></td>
	<td class="line x" title="103:157	In other words, P(e1|U) is the probability of s not occurring in X  Y , or equivalently, equal to P((X Y \S)|(X Y))." ></td>
	<td class="line x" title="104:157	The set X Y \S is shown as the area with darker shade in Figure 1(a)." ></td>
	<td class="line x" title="105:157	Let us define the odds based on the first evidence as: OL = P(e1|U)P(e 1|U) (12) Substituting P(e1|U) = P(S|X  Y) and P(e1|U) = P((X  Y \ S)|(X  Y)) into Equation 12 will give us: OL = P(S|X Y)P((X Y \S)|(X Y)) = P(S (X Y))P(X Y) P(X Y)P((X Y \S)(X Y)) = P(S (X Y))P((X Y \S)(X Y)) and since S  (XY) and (XY \S)  (XY), OL = P(S)P(X Y \S) if(P(X Y \S) negationslash= 0) and OL = 1 if P(X Y \S) = 0." ></td>
	<td class="line x" title="106:157	The second evidence e2 captures the probability of occurrences of s without confinement." ></td>
	<td class="line x" title="107:157	If s is a stable unit, then its probability of occurrence in the sample space would simply be P(S)." ></td>
	<td class="line x" title="108:157	On the other hand, if s occurs not as a unit, then its probability of non-occurrence is 1P(S)." ></td>
	<td class="line x" title="109:157	The complement of S, which is the set S is shown as the area with darker shade in Figure 1(b)." ></td>
	<td class="line x" title="110:157	Let us define the odds based on the second evidence as: OG = P(e2|U)P(e 2|U) (13) Substituting P(e2|U) = P(S) and P(e2|U) = 1  P(S) into Equation 13 will give us: OG = P(S)1P(S) Intuitively, the first evidence attempts to capture the extent to which the existence of the two lexical units ax and ay is attributable to s. Referring back to OL, whenever the denominator P(XY \S) becomes less than P(S), we can deduce that ax and ay actually exist together as s more than in other forms." ></td>
	<td class="line x" title="111:157	At one extreme when P(X  Y \ S) = 0, we can conclude that the co-occurrence of ax and ay is exclusively for s. As such, we can also refer to OL as a measure of exclusivity for the use of ax and ay with respect to s. This first evidence is a good indication for the unithood of s since the more the existence of ax and ay is attributed to s, the stronger the collocational strength of s becomes." ></td>
	<td class="line x" title="112:157	Concerning the second evidence, OG attempts to capture the extent to which s occurs in general usage (i.e. World Wide Web)." ></td>
	<td class="line x" title="113:157	We can consider OG as a measure of pervasiveness for the use of s. As s becomes more widely used in text, the numerator in OG will increase." ></td>
	<td class="line x" title="114:157	This provides a good indication on the unithood of s since the more s appears in usage, the likelier it becomes that s is a stable unit instead of an occurrence by chance when ax and ay are located next to each other." ></td>
	<td class="line x" title="115:157	As a result, the derivation of OU(s) using OL and OG will ensure a comprehensive way of determining unithood." ></td>
	<td class="line x" title="116:157	Finally, expanding OU(s) in Equation 11 using Equations 12 and 13 will give us: OU(s) = logOL + logOG (14) = log P(S)P(X Y \S) + log P(S)1P(S) As such, the decision on whether ax and ay should be merged to form s can be made based solely on the Odds of Unithood (OU) defined in Equation 14." ></td>
	<td class="line x" title="117:157	We will merge ax and ay if their odds of unithood exceeds a certain threshold, OUT . 4 Evaluations and Discussions For this evaluation, we employed 500 news articles from Reuters in the health domain gathered between December 2006 to May 2007." ></td>
	<td class="line x" title="118:157	These 500 articles are fed into the Stanford Parser whose output is then used by our head-driven left-right filter (Wong, 2005; Wong et al., 2007b) to extract word sequences in the form of nouns and noun phrases." ></td>
	<td class="line x" title="119:157	Pairs of word sequences (i.e. ax and ay) located immediately next to each other, or separated by a preposition or the conjunction and in the same sentence are mea108 sured for their unithood." ></td>
	<td class="line x" title="120:157	Using the 500 news articles, we managed to obtain 1,825 pairs of words to be tested for unithood." ></td>
	<td class="line x" title="121:157	We performed a comparative study of our new probabilistic approach against the empiricallyderived unithood function described in Equation 2." ></td>
	<td class="line x" title="122:157	Two experiments were conducted." ></td>
	<td class="line x" title="123:157	In the first one, we assessed our probabilistically-derived measure OU(s) as described in Equation 14 where the decisions on whether or not to merge the 1,825 pairs are done automatically." ></td>
	<td class="line x" title="124:157	These decisions are known as the actual results." ></td>
	<td class="line x" title="125:157	At the same time, we inspected the same list manually to decide on the merging of all the pairs." ></td>
	<td class="line x" title="126:157	These decisions are known as the ideal results." ></td>
	<td class="line x" title="127:157	The threshold OUT employed for our evaluation is determined empirically through experiments and is set to 8.39." ></td>
	<td class="line x" title="128:157	However, since only one threshold is involved in deciding mergeability, training algorithms and data sets may be employed to automatically decide on an optimal number." ></td>
	<td class="line x" title="129:157	This option is beyond the scope of this paper." ></td>
	<td class="line x" title="130:157	The actual and ideal results for this first experiment are organised into a contingency table (not shown here) for identifying the true and the false positives, and the true and the false negatives." ></td>
	<td class="line x" title="131:157	In the second experiment, we conducted the same assessment as carried out in the first one but the decisions to merge the 1,825 pairs are based on the UH(ax,ay) function described in Equation 2." ></td>
	<td class="line x" title="132:157	The thresholds required for this function are based on the values suggested by (Wong et al., 2007b), namely, MI+ = 0.9, MI = 0.02, IDT = 6, IDR+ = 1.35, and IDR = 0.93." ></td>
	<td class="line x" title="133:157	Table 1: The performance of OU(s) (from Experiment 1) and UH(ax,ay) (from Experiment 2) in terms of precision, recall and accuracy." ></td>
	<td class="line x" title="134:157	The last column shows the difference in the performance of Experiment 1 and 2." ></td>
	<td class="line x" title="135:157	Using the results from the contingency tables, we computed the precision, recall and accuracy for the two measures under evaluation." ></td>
	<td class="line x" title="136:157	Table 1 summarises the performance of OU(s) and UH(ax,ay) in determining the unithood of 1,825 pairs of lexical units." ></td>
	<td class="line x" title="137:157	One will notice that our new measure OU(s) outperformed the empirically-derived function UH(ax,ay) in all aspects, with an improvement of 2.63%, 3.33% and 2.74% for precision, recall and accuracy, respectively." ></td>
	<td class="line x" title="138:157	Our new measure achieved a 100% precision with a lower recall at 95.83%." ></td>
	<td class="line x" title="139:157	As with any measures that employ thresholds as a cutoff point in accepting or rejecting certain decisions, we can improve the recall of OU(s) by decreasing the threshold OUT . In this way, there will be less false negatives (i.e. pairs which are supposed to be merged but are not) and hence, increases the recall rate." ></td>
	<td class="line x" title="140:157	Unfortunately, recall will improve at the expense of precision since the number of false positives will definitely increase from the existing 0." ></td>
	<td class="line x" title="141:157	Since our application (i.e. ontology learning) requires perfect precision in determining the unithood of word sequences, OU(s) is the ideal candidate." ></td>
	<td class="line x" title="142:157	Moreover, with only one threshold (i.e. OUT ) required in controlling the function of OU(s), we are able to reduce the amount of time and effort spent on optimising our results." ></td>
	<td class="line x" title="143:157	5 Conclusion and Future Work In this paper, we highlighted the significance of unithood and that its measurement should be given equal attention by researchers in term extraction." ></td>
	<td class="line x" title="144:157	We focused on the development of a new approach that is independent of influences of termhood measurement." ></td>
	<td class="line x" title="145:157	We proposed a new probabilistically-derived measure which provide a dedicated way to determine the unithood of word sequences." ></td>
	<td class="line x" title="146:157	We refer to this measure as the Odds of Unithood (OU)." ></td>
	<td class="line x" title="147:157	OU is derived using Bayes Theorem and is founded upon two evidences, namely, local occurrence and global occurrence." ></td>
	<td class="line x" title="148:157	Elementary probabilities estimated using page counts from the Google search engine are utilised to quantify the two evidences." ></td>
	<td class="line x" title="149:157	The new probabilistically-derived measure OU is then evaluated against an existing empirical function known as Unithood (UH)." ></td>
	<td class="line x" title="150:157	Our new measure OU achieved a precision and a recall of 100% and 95.83% respectively, with an accuracy at 97.26% in measuring the unithood of 1,825 test cases." ></td>
	<td class="line x" title="151:157	OU outperformed UH by 2.63%, 3.33% and 2.74% in terms of precision, 109 recall and accuracy, respectively." ></td>
	<td class="line x" title="152:157	Moreover, our new measure requires only one threshold, as compared to five in UH to control the mergeability decision." ></td>
	<td class="line x" title="153:157	More work is required to establish the coverage and the depth of the World Wide Web with regards to the determination of unithood." ></td>
	<td class="line x" title="154:157	While the Web has demonstrated reasonable strength in handling general news articles, we have yet to study its appropriateness in dealing with unithood determination for technical text (i.e. the depth of the Web)." ></td>
	<td class="line x" title="155:157	Similarly, it remains a question the extent to which the Web is able to satisfy the requirement of unithood determination for a wider range of genres (i.e. the coverage of the Web)." ></td>
	<td class="line x" title="156:157	Studies on the effect of noises (e.g. keyword spamming) and multiple word senses on unithood determination using the Web is another future research direction." ></td>
	<td class="line x" title="157:157	Acknowledgement This research was supported by the Australian Endeavour International Postgraduate Research Scholarship, and the Research Grant 2006 by the University of Western Australia." ></td>
</tr></table>
</div
</body></html>
